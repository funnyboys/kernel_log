commit 52cd0d972fa6491928add05f11f97a4a59babe92
Merge: d2d5439df22f 49b3deaad345
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 12 11:05:52 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull more KVM updates from Paolo Bonzini:
     "The guest side of the asynchronous page fault work has been delayed to
      5.9 in order to sync with Thomas's interrupt entry rework, but here's
      the rest of the KVM updates for this merge window.
    
      MIPS:
       - Loongson port
    
      PPC:
       - Fixes
    
      ARM:
       - Fixes
    
      x86:
       - KVM_SET_USER_MEMORY_REGION optimizations
       - Fixes
       - Selftest fixes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (62 commits)
      KVM: x86: do not pass poisoned hva to __kvm_set_memory_region
      KVM: selftests: fix sync_with_host() in smm_test
      KVM: async_pf: Inject 'page ready' event only if 'page not present' was previously injected
      KVM: async_pf: Cleanup kvm_setup_async_pf()
      kvm: i8254: remove redundant assignment to pointer s
      KVM: x86: respect singlestep when emulating instruction
      KVM: selftests: Don't probe KVM_CAP_HYPERV_ENLIGHTENED_VMCS when nested VMX is unsupported
      KVM: selftests: do not substitute SVM/VMX check with KVM_CAP_NESTED_STATE check
      KVM: nVMX: Consult only the "basic" exit reason when routing nested exit
      KVM: arm64: Move hyp_symbol_addr() to kvm_asm.h
      KVM: arm64: Synchronize sysreg state on injecting an AArch32 exception
      KVM: arm64: Make vcpu_cp1x() work on Big Endian hosts
      KVM: arm64: Remove host_cpu_context member from vcpu structure
      KVM: arm64: Stop sparse from moaning at __hyp_this_cpu_ptr
      KVM: arm64: Handle PtrAuth traps early
      KVM: x86: Unexport x86_fpu_cache and make it static
      KVM: selftests: Ignore KVM 5-level paging support for VM_MODE_PXXV48_4K
      KVM: arm64: Save the host's PtrAuth keys in non-preemptible context
      KVM: arm64: Stop save/restoring ACTLR_EL1
      KVM: arm64: Add emulation for 32bit guests accessing ACTLR2
      ...

commit 2a18b7e7cd8882f626316c340c6f2fca49b5fa12
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Jun 10 19:55:32 2020 +0200

    KVM: async_pf: Inject 'page ready' event only if 'page not present' was previously injected
    
    'Page not present' event may or may not get injected depending on
    guest's state. If the event wasn't injected, there is no need to
    inject the corresponding 'page ready' event as the guest may get
    confused. E.g. Linux thinks that the corresponding 'page not present'
    event wasn't delivered *yet* and allocates a 'dummy entry' for it.
    This entry is never freed.
    
    Note, 'wakeup all' events have no corresponding 'page not present'
    event and always get injected.
    
    s390 seems to always be able to inject 'page not present', the
    change is effectively a nop.
    
    Suggested-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200610175532.779793-2-vkuznets@redhat.com>
    Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=208081
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 06bde4bad205..33fea4488ef3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3923,11 +3923,13 @@ static void __kvm_inject_pfault_token(struct kvm_vcpu *vcpu, bool start_token,
 	}
 }
 
-void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
+bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work)
 {
 	trace_kvm_s390_pfault_init(vcpu, work->arch.pfault_token);
 	__kvm_inject_pfault_token(vcpu, true, work->arch.pfault_token);
+
+	return true;
 }
 
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5c546d27fdae..d0ff26d157bc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -763,9 +763,9 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 			r = -EINVAL;
 		else {
 			r = 0;
-			down_write(&kvm->mm->mmap_sem);
+			mmap_write_lock(kvm->mm);
 			kvm->mm->context.allow_gmap_hpage_1m = 1;
-			up_write(&kvm->mm->mmap_sem);
+			mmap_write_unlock(kvm->mm);
 			/*
 			 * We might have to create fake 4k page
 			 * tables. To avoid that the hardware works on
@@ -1815,7 +1815,7 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	if (!keys)
 		return -ENOMEM;
 
-	down_read(&current->mm->mmap_sem);
+	mmap_read_lock(current->mm);
 	srcu_idx = srcu_read_lock(&kvm->srcu);
 	for (i = 0; i < args->count; i++) {
 		hva = gfn_to_hva(kvm, args->start_gfn + i);
@@ -1829,7 +1829,7 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 			break;
 	}
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
-	up_read(&current->mm->mmap_sem);
+	mmap_read_unlock(current->mm);
 
 	if (!r) {
 		r = copy_to_user((uint8_t __user *)args->skeydata_addr, keys,
@@ -1873,7 +1873,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 		goto out;
 
 	i = 0;
-	down_read(&current->mm->mmap_sem);
+	mmap_read_lock(current->mm);
 	srcu_idx = srcu_read_lock(&kvm->srcu);
         while (i < args->count) {
 		unlocked = false;
@@ -1900,7 +1900,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 			i++;
 	}
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
-	up_read(&current->mm->mmap_sem);
+	mmap_read_unlock(current->mm);
 out:
 	kvfree(keys);
 	return r;
@@ -2089,14 +2089,14 @@ static int kvm_s390_get_cmma_bits(struct kvm *kvm,
 	if (!values)
 		return -ENOMEM;
 
-	down_read(&kvm->mm->mmap_sem);
+	mmap_read_lock(kvm->mm);
 	srcu_idx = srcu_read_lock(&kvm->srcu);
 	if (peek)
 		ret = kvm_s390_peek_cmma(kvm, args, values, bufsize);
 	else
 		ret = kvm_s390_get_cmma(kvm, args, values, bufsize);
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
-	up_read(&kvm->mm->mmap_sem);
+	mmap_read_unlock(kvm->mm);
 
 	if (kvm->arch.migration_mode)
 		args->remaining = atomic64_read(&kvm->arch.cmma_dirty_pages);
@@ -2146,7 +2146,7 @@ static int kvm_s390_set_cmma_bits(struct kvm *kvm,
 		goto out;
 	}
 
-	down_read(&kvm->mm->mmap_sem);
+	mmap_read_lock(kvm->mm);
 	srcu_idx = srcu_read_lock(&kvm->srcu);
 	for (i = 0; i < args->count; i++) {
 		hva = gfn_to_hva(kvm, args->start_gfn + i);
@@ -2161,12 +2161,12 @@ static int kvm_s390_set_cmma_bits(struct kvm *kvm,
 		set_pgste_bits(kvm->mm, hva, mask, pgstev);
 	}
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
-	up_read(&kvm->mm->mmap_sem);
+	mmap_read_unlock(kvm->mm);
 
 	if (!kvm->mm->context.uses_cmm) {
-		down_write(&kvm->mm->mmap_sem);
+		mmap_write_lock(kvm->mm);
 		kvm->mm->context.uses_cmm = 1;
-		up_write(&kvm->mm->mmap_sem);
+		mmap_write_unlock(kvm->mm);
 	}
 out:
 	vfree(bits);
@@ -2239,9 +2239,9 @@ static int kvm_s390_handle_pv(struct kvm *kvm, struct kvm_pv_cmd *cmd)
 		if (r)
 			break;
 
-		down_write(&current->mm->mmap_sem);
+		mmap_write_lock(current->mm);
 		r = gmap_mark_unmergeable();
-		up_write(&current->mm->mmap_sem);
+		mmap_write_unlock(current->mm);
 		if (r)
 			break;
 

commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 023f2b07f97f..5c546d27fdae 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -31,11 +31,11 @@
 #include <linux/bitmap.h>
 #include <linux/sched/signal.h>
 #include <linux/string.h>
+#include <linux/pgtable.h>
 
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/stp.h>
-#include <linux/pgtable.h>
 #include <asm/gmap.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 22058ea9b8eb..023f2b07f97f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -35,7 +35,7 @@
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/stp.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/gmap.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>

commit 23fc02e36e4f657af242e59175c891b27c704935
Merge: 4e3a16ee9148 bfa50e1427e4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 8 12:05:31 2020 -0700

    Merge tag 's390-5.8-1' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Vasily Gorbik:
    
     - Add support for multi-function devices in pci code.
    
     - Enable PF-VF linking for architectures using the pdev->no_vf_scan
       flag (currently just s390).
    
     - Add reipl from NVMe support.
    
     - Get rid of critical section cleanup in entry.S.
    
     - Refactor PNSO CHSC (perform network subchannel operation) in cio and
       qeth.
    
     - QDIO interrupts and error handling fixes and improvements, more
       refactoring changes.
    
     - Align ioremap() with generic code.
    
     - Accept requests without the prefetch bit set in vfio-ccw.
    
     - Enable path handling via two new regions in vfio-ccw.
    
     - Other small fixes and improvements all over the code.
    
    * tag 's390-5.8-1' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (52 commits)
      vfio-ccw: make vfio_ccw_regops variables declarations static
      vfio-ccw: Add trace for CRW event
      vfio-ccw: Wire up the CRW irq and CRW region
      vfio-ccw: Introduce a new CRW region
      vfio-ccw: Refactor IRQ handlers
      vfio-ccw: Introduce a new schib region
      vfio-ccw: Refactor the unregister of the async regions
      vfio-ccw: Register a chp_event callback for vfio-ccw
      vfio-ccw: Introduce new helper functions to free/destroy regions
      vfio-ccw: document possible errors
      vfio-ccw: Enable transparent CCW IPL from DASD
      s390/pci: Log new handle in clp_disable_fh()
      s390/cio, s390/qeth: cleanup PNSO CHSC
      s390/qdio: remove q->first_to_kick
      s390/qdio: fix up qdio_start_irq() kerneldoc
      s390: remove critical section cleanup from entry.S
      s390: add machine check SIGP
      s390/pci: ioremap() align with generic code
      s390/ap: introduce new ap function ap_get_qdev()
      Documentation/s390: Update / remove developerWorks web links
      ...

commit 7c0ade6c9023b2b90b757e2927b306bec1cc4ca6
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon May 25 16:41:18 2020 +0200

    KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
    
    An innocent reader of the following x86 KVM code:
    
    bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu)
    {
            if (!(vcpu->arch.apf.msr_val & KVM_ASYNC_PF_ENABLED))
                    return true;
    ...
    
    may get very confused: if APF mechanism is not enabled, why do we report
    that we 'can inject async page present'? In reality, upon injection
    kvm_arch_async_page_present() will check the same condition again and,
    in case APF is disabled, will just drop the item. This is fine as the
    guest which deliberately disabled APF doesn't expect to get any APF
    notifications.
    
    Rename kvm_arch_can_inject_async_page_present() to
    kvm_arch_can_dequeue_async_page_present() to make it clear what we are
    checking: if the item can be dequeued (meaning either injected or just
    dropped).
    
    On s390 kvm_arch_can_inject_async_page_present() always returns 'true' so
    the rename doesn't matter much.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200525144125.143875-4-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a560a368f92c..06bde4bad205 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3943,7 +3943,7 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 	/* s390 will always inject the page directly */
 }
 
-bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu)
+bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
 {
 	/*
 	 * s390 will always inject the page directly,

commit 0b0ed657fe003fdf4df3766b898e8869950aa1ce
Author: Sven Schnelle <svens@linux.ibm.com>
Date:   Thu Feb 20 12:09:36 2020 +0100

    s390: remove critical section cleanup from entry.S
    
    The current code is rather complex and caused a lot of subtle
    and hard to debug bugs in the past. Simplify the code by calling
    the system_call handler with interrupts disabled, save
    machine state, and re-enable them later.
    
    This requires significant changes to the machine check handling code
    as well. When the machine check interrupt arrived while being in kernel
    mode the new code will signal pending machine checks with a SIGP external
    call. When userspace was interrupted, the handler will switch to the
    kernel stack and directly execute s390_handle_mcck().
    
    Signed-off-by: Sven Schnelle <svens@linux.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 19a81024fe16..5722daf0b603 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3995,9 +3995,6 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 	if (need_resched())
 		schedule();
 
-	if (test_cpu_flag(CIF_MCCK_PENDING))
-		s390_handle_mcck();
-
 	if (!kvm_is_ucontrol(vcpu->kvm)) {
 		rc = kvm_s390_deliver_pending_interrupts(vcpu);
 		if (rc)

commit cb953129bfe5c0f2da835a0469930873fb7e71df
Author: David Matlack <dmatlack@google.com>
Date:   Fri May 8 11:22:40 2020 -0700

    kvm: add halt-polling cpu usage stats
    
    Two new stats for exposing halt-polling cpu usage:
    halt_poll_success_ns
    halt_poll_fail_ns
    
    Thus sum of these 2 stats is the total cpu time spent polling. "success"
    means the VCPU polled until a virtual interrupt was delivered. "fail"
    means the VCPU had to schedule out (either because the maximum poll time
    was reached or it needed to yield the CPU).
    
    To avoid touching every arch's kvm_vcpu_stat struct, only update and
    export halt-polling cpu usage stats if we're on x86.
    
    Exporting cpu usage as a u64 and in nanoseconds means we will overflow at
    ~500 years, which seems reasonably large.
    
    Signed-off-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Jon Cargille <jcargill@google.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    
    Message-Id: <20200508182240.68440-1-jcargill@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 389ff1b7cd43..a560a368f92c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -75,6 +75,8 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	VCPU_STAT("halt_poll_invalid", halt_poll_invalid),
 	VCPU_STAT("halt_no_poll_steal", halt_no_poll_steal),
 	VCPU_STAT("halt_wakeup", halt_wakeup),
+	VCPU_STAT("halt_poll_success_ns", halt_poll_success_ns),
+	VCPU_STAT("halt_poll_fail_ns", halt_poll_fail_ns),
 	VCPU_STAT("instruction_lctlg", instruction_lctlg),
 	VCPU_STAT("instruction_lctl", instruction_lctl),
 	VCPU_STAT("instruction_stctl", instruction_stctl),

commit 4aef2ec9022b217f74d0f4c9b84081f07cc223d9
Merge: 7c67f54661fc 37486135d3a7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 13 12:14:05 2020 -0400

    Merge branch 'kvm-amd-fixes' into HEAD

commit b9b2782cd55aaf5a65573df99376cf391941cb66
Author: Peter Xu <peterx@redhat.com>
Date:   Tue May 5 11:47:50 2020 -0400

    KVM: X86: Declare KVM_CAP_SET_GUEST_DEBUG properly
    
    KVM_CAP_SET_GUEST_DEBUG should be supported for x86 however it's not declared
    as supported.  My wild guess is that userspaces like QEMU are using "#ifdef
    KVM_CAP_SET_GUEST_DEBUG" to check for the capability instead, but that could be
    wrong because the compilation host may not be the runtime host.
    
    The userspace might still want to keep the old "#ifdef" though to not break the
    guest debug on old kernels.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Message-Id: <20200505154750.126300-1-peterx@redhat.com>
    [Do the same for PPC and s390. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5dcf9ff12828..d05bb040fd42 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -545,6 +545,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_AIS:
 	case KVM_CAP_S390_AIS_MIGRATION:
 	case KVM_CAP_S390_VCPU_RESETS:
+	case KVM_CAP_SET_GUEST_DEBUG:
 		r = 1;
 		break;
 	case KVM_CAP_S390_HPAGE_1M:

commit 1b94f6f81007b4afaea3480ec018bc9236148961
Author: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
Date:   Thu Apr 16 13:10:57 2020 +0800

    KVM: Remove redundant argument to kvm_arch_vcpu_ioctl_run
    
    In earlier versions of kvm, 'kvm_run' was an independent structure
    and was not included in the vcpu structure. At present, 'kvm_run'
    is already included in the vcpu structure, so the parameter
    'kvm_run' is redundant.
    
    This patch simplifies the function definition, removes the extra
    'kvm_run' parameter, and extracts it from the 'kvm_vcpu' structure
    if necessary.
    
    Signed-off-by: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
    Message-Id: <20200416051057.26526-1-tianjia.zhang@linux.alibaba.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5307929a6c89..75471b646fd7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4333,8 +4333,9 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		store_regs_fmt2(vcpu, kvm_run);
 }
 
-int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
+	struct kvm_run *kvm_run = vcpu->run;
 	int rc;
 
 	if (kvm_run->immediate_exit)

commit 812756a82ea51e3c7ff7ba5e6fa3f34345234bc7
Author: Emanuele Giuseppe Esposito <eesposit@redhat.com>
Date:   Tue Apr 14 17:56:25 2020 +0200

    kvm_host: unify VM_STAT and VCPU_STAT definitions in a single place
    
    The macros VM_STAT and VCPU_STAT are redundantly implemented in multiple
    files, each used by a different architecure to initialize the debugfs
    entries for statistics. Since they all have the same purpose, they can be
    unified in a single common definition in include/linux/kvm_host.h
    
    Signed-off-by: Emanuele Giuseppe Esposito <eesposit@redhat.com>
    Message-Id: <20200414155625.20559-1-eesposit@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5dcf9ff12828..5307929a6c89 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -57,110 +57,107 @@
 #define VCPU_IRQS_MAX_BUF (sizeof(struct kvm_s390_irq) * \
 			   (KVM_MAX_VCPUS + LOCAL_IRQS))
 
-#define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
-#define VM_STAT(x) offsetof(struct kvm, stat.x), KVM_STAT_VM
-
 struct kvm_stats_debugfs_item debugfs_entries[] = {
-	{ "userspace_handled", VCPU_STAT(exit_userspace) },
-	{ "exit_null", VCPU_STAT(exit_null) },
-	{ "exit_validity", VCPU_STAT(exit_validity) },
-	{ "exit_stop_request", VCPU_STAT(exit_stop_request) },
-	{ "exit_external_request", VCPU_STAT(exit_external_request) },
-	{ "exit_io_request", VCPU_STAT(exit_io_request) },
-	{ "exit_external_interrupt", VCPU_STAT(exit_external_interrupt) },
-	{ "exit_instruction", VCPU_STAT(exit_instruction) },
-	{ "exit_pei", VCPU_STAT(exit_pei) },
-	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
-	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
-	{ "exit_operation_exception", VCPU_STAT(exit_operation_exception) },
-	{ "halt_successful_poll", VCPU_STAT(halt_successful_poll) },
-	{ "halt_attempted_poll", VCPU_STAT(halt_attempted_poll) },
-	{ "halt_poll_invalid", VCPU_STAT(halt_poll_invalid) },
-	{ "halt_no_poll_steal", VCPU_STAT(halt_no_poll_steal) },
-	{ "halt_wakeup", VCPU_STAT(halt_wakeup) },
-	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
-	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
-	{ "instruction_stctl", VCPU_STAT(instruction_stctl) },
-	{ "instruction_stctg", VCPU_STAT(instruction_stctg) },
-	{ "deliver_ckc", VCPU_STAT(deliver_ckc) },
-	{ "deliver_cputm", VCPU_STAT(deliver_cputm) },
-	{ "deliver_emergency_signal", VCPU_STAT(deliver_emergency_signal) },
-	{ "deliver_external_call", VCPU_STAT(deliver_external_call) },
-	{ "deliver_service_signal", VCPU_STAT(deliver_service_signal) },
-	{ "deliver_virtio", VCPU_STAT(deliver_virtio) },
-	{ "deliver_stop_signal", VCPU_STAT(deliver_stop_signal) },
-	{ "deliver_prefix_signal", VCPU_STAT(deliver_prefix_signal) },
-	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
-	{ "deliver_program", VCPU_STAT(deliver_program) },
-	{ "deliver_io", VCPU_STAT(deliver_io) },
-	{ "deliver_machine_check", VCPU_STAT(deliver_machine_check) },
-	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
-	{ "inject_ckc", VCPU_STAT(inject_ckc) },
-	{ "inject_cputm", VCPU_STAT(inject_cputm) },
-	{ "inject_external_call", VCPU_STAT(inject_external_call) },
-	{ "inject_float_mchk", VM_STAT(inject_float_mchk) },
-	{ "inject_emergency_signal", VCPU_STAT(inject_emergency_signal) },
-	{ "inject_io", VM_STAT(inject_io) },
-	{ "inject_mchk", VCPU_STAT(inject_mchk) },
-	{ "inject_pfault_done", VM_STAT(inject_pfault_done) },
-	{ "inject_program", VCPU_STAT(inject_program) },
-	{ "inject_restart", VCPU_STAT(inject_restart) },
-	{ "inject_service_signal", VM_STAT(inject_service_signal) },
-	{ "inject_set_prefix", VCPU_STAT(inject_set_prefix) },
-	{ "inject_stop_signal", VCPU_STAT(inject_stop_signal) },
-	{ "inject_pfault_init", VCPU_STAT(inject_pfault_init) },
-	{ "inject_virtio", VM_STAT(inject_virtio) },
-	{ "instruction_epsw", VCPU_STAT(instruction_epsw) },
-	{ "instruction_gs", VCPU_STAT(instruction_gs) },
-	{ "instruction_io_other", VCPU_STAT(instruction_io_other) },
-	{ "instruction_lpsw", VCPU_STAT(instruction_lpsw) },
-	{ "instruction_lpswe", VCPU_STAT(instruction_lpswe) },
-	{ "instruction_pfmf", VCPU_STAT(instruction_pfmf) },
-	{ "instruction_ptff", VCPU_STAT(instruction_ptff) },
-	{ "instruction_stidp", VCPU_STAT(instruction_stidp) },
-	{ "instruction_sck", VCPU_STAT(instruction_sck) },
-	{ "instruction_sckpf", VCPU_STAT(instruction_sckpf) },
-	{ "instruction_spx", VCPU_STAT(instruction_spx) },
-	{ "instruction_stpx", VCPU_STAT(instruction_stpx) },
-	{ "instruction_stap", VCPU_STAT(instruction_stap) },
-	{ "instruction_iske", VCPU_STAT(instruction_iske) },
-	{ "instruction_ri", VCPU_STAT(instruction_ri) },
-	{ "instruction_rrbe", VCPU_STAT(instruction_rrbe) },
-	{ "instruction_sske", VCPU_STAT(instruction_sske) },
-	{ "instruction_ipte_interlock", VCPU_STAT(instruction_ipte_interlock) },
-	{ "instruction_essa", VCPU_STAT(instruction_essa) },
-	{ "instruction_stsi", VCPU_STAT(instruction_stsi) },
-	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
-	{ "instruction_tb", VCPU_STAT(instruction_tb) },
-	{ "instruction_tpi", VCPU_STAT(instruction_tpi) },
-	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
-	{ "instruction_tsch", VCPU_STAT(instruction_tsch) },
-	{ "instruction_sthyi", VCPU_STAT(instruction_sthyi) },
-	{ "instruction_sie", VCPU_STAT(instruction_sie) },
-	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
-	{ "instruction_sigp_sense_running", VCPU_STAT(instruction_sigp_sense_running) },
-	{ "instruction_sigp_external_call", VCPU_STAT(instruction_sigp_external_call) },
-	{ "instruction_sigp_emergency", VCPU_STAT(instruction_sigp_emergency) },
-	{ "instruction_sigp_cond_emergency", VCPU_STAT(instruction_sigp_cond_emergency) },
-	{ "instruction_sigp_start", VCPU_STAT(instruction_sigp_start) },
-	{ "instruction_sigp_stop", VCPU_STAT(instruction_sigp_stop) },
-	{ "instruction_sigp_stop_store_status", VCPU_STAT(instruction_sigp_stop_store_status) },
-	{ "instruction_sigp_store_status", VCPU_STAT(instruction_sigp_store_status) },
-	{ "instruction_sigp_store_adtl_status", VCPU_STAT(instruction_sigp_store_adtl_status) },
-	{ "instruction_sigp_set_arch", VCPU_STAT(instruction_sigp_arch) },
-	{ "instruction_sigp_set_prefix", VCPU_STAT(instruction_sigp_prefix) },
-	{ "instruction_sigp_restart", VCPU_STAT(instruction_sigp_restart) },
-	{ "instruction_sigp_cpu_reset", VCPU_STAT(instruction_sigp_cpu_reset) },
-	{ "instruction_sigp_init_cpu_reset", VCPU_STAT(instruction_sigp_init_cpu_reset) },
-	{ "instruction_sigp_unknown", VCPU_STAT(instruction_sigp_unknown) },
-	{ "instruction_diag_10", VCPU_STAT(diagnose_10) },
-	{ "instruction_diag_44", VCPU_STAT(diagnose_44) },
-	{ "instruction_diag_9c", VCPU_STAT(diagnose_9c) },
-	{ "diag_9c_ignored", VCPU_STAT(diagnose_9c_ignored) },
-	{ "instruction_diag_258", VCPU_STAT(diagnose_258) },
-	{ "instruction_diag_308", VCPU_STAT(diagnose_308) },
-	{ "instruction_diag_500", VCPU_STAT(diagnose_500) },
-	{ "instruction_diag_other", VCPU_STAT(diagnose_other) },
+	VCPU_STAT("userspace_handled", exit_userspace),
+	VCPU_STAT("exit_null", exit_null),
+	VCPU_STAT("exit_validity", exit_validity),
+	VCPU_STAT("exit_stop_request", exit_stop_request),
+	VCPU_STAT("exit_external_request", exit_external_request),
+	VCPU_STAT("exit_io_request", exit_io_request),
+	VCPU_STAT("exit_external_interrupt", exit_external_interrupt),
+	VCPU_STAT("exit_instruction", exit_instruction),
+	VCPU_STAT("exit_pei", exit_pei),
+	VCPU_STAT("exit_program_interruption", exit_program_interruption),
+	VCPU_STAT("exit_instr_and_program_int", exit_instr_and_program),
+	VCPU_STAT("exit_operation_exception", exit_operation_exception),
+	VCPU_STAT("halt_successful_poll", halt_successful_poll),
+	VCPU_STAT("halt_attempted_poll", halt_attempted_poll),
+	VCPU_STAT("halt_poll_invalid", halt_poll_invalid),
+	VCPU_STAT("halt_no_poll_steal", halt_no_poll_steal),
+	VCPU_STAT("halt_wakeup", halt_wakeup),
+	VCPU_STAT("instruction_lctlg", instruction_lctlg),
+	VCPU_STAT("instruction_lctl", instruction_lctl),
+	VCPU_STAT("instruction_stctl", instruction_stctl),
+	VCPU_STAT("instruction_stctg", instruction_stctg),
+	VCPU_STAT("deliver_ckc", deliver_ckc),
+	VCPU_STAT("deliver_cputm", deliver_cputm),
+	VCPU_STAT("deliver_emergency_signal", deliver_emergency_signal),
+	VCPU_STAT("deliver_external_call", deliver_external_call),
+	VCPU_STAT("deliver_service_signal", deliver_service_signal),
+	VCPU_STAT("deliver_virtio", deliver_virtio),
+	VCPU_STAT("deliver_stop_signal", deliver_stop_signal),
+	VCPU_STAT("deliver_prefix_signal", deliver_prefix_signal),
+	VCPU_STAT("deliver_restart_signal", deliver_restart_signal),
+	VCPU_STAT("deliver_program", deliver_program),
+	VCPU_STAT("deliver_io", deliver_io),
+	VCPU_STAT("deliver_machine_check", deliver_machine_check),
+	VCPU_STAT("exit_wait_state", exit_wait_state),
+	VCPU_STAT("inject_ckc", inject_ckc),
+	VCPU_STAT("inject_cputm", inject_cputm),
+	VCPU_STAT("inject_external_call", inject_external_call),
+	VM_STAT("inject_float_mchk", inject_float_mchk),
+	VCPU_STAT("inject_emergency_signal", inject_emergency_signal),
+	VM_STAT("inject_io", inject_io),
+	VCPU_STAT("inject_mchk", inject_mchk),
+	VM_STAT("inject_pfault_done", inject_pfault_done),
+	VCPU_STAT("inject_program", inject_program),
+	VCPU_STAT("inject_restart", inject_restart),
+	VM_STAT("inject_service_signal", inject_service_signal),
+	VCPU_STAT("inject_set_prefix", inject_set_prefix),
+	VCPU_STAT("inject_stop_signal", inject_stop_signal),
+	VCPU_STAT("inject_pfault_init", inject_pfault_init),
+	VM_STAT("inject_virtio", inject_virtio),
+	VCPU_STAT("instruction_epsw", instruction_epsw),
+	VCPU_STAT("instruction_gs", instruction_gs),
+	VCPU_STAT("instruction_io_other", instruction_io_other),
+	VCPU_STAT("instruction_lpsw", instruction_lpsw),
+	VCPU_STAT("instruction_lpswe", instruction_lpswe),
+	VCPU_STAT("instruction_pfmf", instruction_pfmf),
+	VCPU_STAT("instruction_ptff", instruction_ptff),
+	VCPU_STAT("instruction_stidp", instruction_stidp),
+	VCPU_STAT("instruction_sck", instruction_sck),
+	VCPU_STAT("instruction_sckpf", instruction_sckpf),
+	VCPU_STAT("instruction_spx", instruction_spx),
+	VCPU_STAT("instruction_stpx", instruction_stpx),
+	VCPU_STAT("instruction_stap", instruction_stap),
+	VCPU_STAT("instruction_iske", instruction_iske),
+	VCPU_STAT("instruction_ri", instruction_ri),
+	VCPU_STAT("instruction_rrbe", instruction_rrbe),
+	VCPU_STAT("instruction_sske", instruction_sske),
+	VCPU_STAT("instruction_ipte_interlock", instruction_ipte_interlock),
+	VCPU_STAT("instruction_essa", instruction_essa),
+	VCPU_STAT("instruction_stsi", instruction_stsi),
+	VCPU_STAT("instruction_stfl", instruction_stfl),
+	VCPU_STAT("instruction_tb", instruction_tb),
+	VCPU_STAT("instruction_tpi", instruction_tpi),
+	VCPU_STAT("instruction_tprot", instruction_tprot),
+	VCPU_STAT("instruction_tsch", instruction_tsch),
+	VCPU_STAT("instruction_sthyi", instruction_sthyi),
+	VCPU_STAT("instruction_sie", instruction_sie),
+	VCPU_STAT("instruction_sigp_sense", instruction_sigp_sense),
+	VCPU_STAT("instruction_sigp_sense_running", instruction_sigp_sense_running),
+	VCPU_STAT("instruction_sigp_external_call", instruction_sigp_external_call),
+	VCPU_STAT("instruction_sigp_emergency", instruction_sigp_emergency),
+	VCPU_STAT("instruction_sigp_cond_emergency", instruction_sigp_cond_emergency),
+	VCPU_STAT("instruction_sigp_start", instruction_sigp_start),
+	VCPU_STAT("instruction_sigp_stop", instruction_sigp_stop),
+	VCPU_STAT("instruction_sigp_stop_store_status", instruction_sigp_stop_store_status),
+	VCPU_STAT("instruction_sigp_store_status", instruction_sigp_store_status),
+	VCPU_STAT("instruction_sigp_store_adtl_status", instruction_sigp_store_adtl_status),
+	VCPU_STAT("instruction_sigp_set_arch", instruction_sigp_arch),
+	VCPU_STAT("instruction_sigp_set_prefix", instruction_sigp_prefix),
+	VCPU_STAT("instruction_sigp_restart", instruction_sigp_restart),
+	VCPU_STAT("instruction_sigp_cpu_reset", instruction_sigp_cpu_reset),
+	VCPU_STAT("instruction_sigp_init_cpu_reset", instruction_sigp_init_cpu_reset),
+	VCPU_STAT("instruction_sigp_unknown", instruction_sigp_unknown),
+	VCPU_STAT("instruction_diag_10", diagnose_10),
+	VCPU_STAT("instruction_diag_44", diagnose_44),
+	VCPU_STAT("instruction_diag_9c", diagnose_9c),
+	VCPU_STAT("diag_9c_ignored", diagnose_9c_ignored),
+	VCPU_STAT("instruction_diag_258", diagnose_258),
+	VCPU_STAT("instruction_diag_308", diagnose_308),
+	VCPU_STAT("instruction_diag_500", diagnose_500),
+	VCPU_STAT("instruction_diag_other", diagnose_other),
 	{ NULL }
 };
 

commit 97daa028f3f621adff2c4f7b15fe0874e5b5bd6c
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Apr 7 23:40:59 2020 -0700

    KVM: s390: Return last valid slot if approx index is out-of-bounds
    
    Return the index of the last valid slot from gfn_to_memslot_approx() if
    its binary search loop yielded an out-of-bounds index.  The index can
    be out-of-bounds if the specified gfn is less than the base of the
    lowest memslot (which is also the last valid memslot).
    
    Note, the sole caller, kvm_s390_get_cmma(), ensures used_slots is
    non-zero.
    
    Fixes: afdad61615cc3 ("KVM: s390: Fix storage attributes migration with memory slots")
    Cc: stable@vger.kernel.org # 4.19.x: 0774a964ef56: KVM: Fix out of range accesses to memslots
    Cc: stable@vger.kernel.org # 4.19.x
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200408064059.8957-3-sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 19a81024fe16..5dcf9ff12828 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1939,6 +1939,9 @@ static int gfn_to_memslot_approx(struct kvm_memslots *slots, gfn_t gfn)
 			start = slot + 1;
 	}
 
+	if (start >= slots->used_slots)
+		return slots->used_slots - 1;
+
 	if (gfn >= memslots[start].base_gfn &&
 	    gfn < memslots[start].base_gfn + memslots[start].npages) {
 		atomic_set(&slots->lru_slot, start);

commit 8c1b724ddb218f221612d4c649bc9c7819d8d7a6
Merge: f14a9532ee30 514ccc194971
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 15:13:15 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - GICv4.1 support
    
       - 32bit host removal
    
      PPC:
       - secure (encrypted) using under the Protected Execution Framework
         ultravisor
    
      s390:
       - allow disabling GISA (hardware interrupt injection) and protected
         VMs/ultravisor support.
    
      x86:
       - New dirty bitmap flag that sets all bits in the bitmap when dirty
         page logging is enabled; this is faster because it doesn't require
         bulk modification of the page tables.
    
       - Initial work on making nested SVM event injection more similar to
         VMX, and less buggy.
    
       - Various cleanups to MMU code (though the big ones and related
         optimizations were delayed to 5.8). Instead of using cr3 in
         function names which occasionally means eptp, KVM too has
         standardized on "pgd".
    
       - A large refactoring of CPUID features, which now use an array that
         parallels the core x86_features.
    
       - Some removal of pointer chasing from kvm_x86_ops, which will also
         be switched to static calls as soon as they are available.
    
       - New Tigerlake CPUID features.
    
       - More bugfixes, optimizations and cleanups.
    
      Generic:
       - selftests: cleanups, new MMU notifier stress test, steal-time test
    
       - CSV output for kvm_stat"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (277 commits)
      x86/kvm: fix a missing-prototypes "vmread_error"
      KVM: x86: Fix BUILD_BUG() in __cpuid_entry_get_reg() w/ CONFIG_UBSAN=y
      KVM: VMX: Add a trampoline to fix VMREAD error handling
      KVM: SVM: Annotate svm_x86_ops as __initdata
      KVM: VMX: Annotate vmx_x86_ops as __initdata
      KVM: x86: Drop __exit from kvm_x86_ops' hardware_unsetup()
      KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection
      KVM: x86: Set kvm_x86_ops only after ->hardware_setup() completes
      KVM: VMX: Configure runtime hooks using vmx_x86_ops
      KVM: VMX: Move hardware_setup() definition below vmx_x86_ops
      KVM: x86: Move init-only kvm_x86_ops to separate struct
      KVM: Pass kvm_init()'s opaque param to additional arch funcs
      s390/gmap: return proper error code on ksm unsharing
      KVM: selftests: Fix cosmetic copy-paste error in vm_mem_region_move()
      KVM: Fix out of range accesses to memslots
      KVM: X86: Micro-optimize IPI fastpath delay
      KVM: X86: Delay read msr data iff writes ICR MSR
      KVM: PPC: Book3S HV: Add a capability for enabling secure guests
      KVM: arm64: GICv4.1: Expose HW-based SGIs in debugfs
      KVM: arm64: GICv4.1: Allow non-trapping WFI when using HW SGIs
      ...

commit b990408537388e9174b642ad36cdef6c47c64d3a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 13:25:55 2020 -0700

    KVM: Pass kvm_init()'s opaque param to additional arch funcs
    
    Pass @opaque to kvm_arch_hardware_setup() and
    kvm_arch_check_processor_compat() to allow architecture specific code to
    reference @opaque without having to stash it away in a temporary global
    variable.  This will enable x86 to separate its vendor specific callback
    ops, which are passed via @opaque, into "init" and "runtime" ops without
    having to stash away the "init" ops.
    
    No functional change intended.
    
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Tested-by: Cornelia Huck <cohuck@redhat.com> #s390
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321202603.19355-2-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6b2649b3d4f3..f6268dfb8362 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -235,7 +235,7 @@ int kvm_arch_hardware_enable(void)
 	return 0;
 }
 
-int kvm_arch_check_processor_compat(void)
+int kvm_arch_check_processor_compat(void *opaque)
 {
 	return 0;
 }
@@ -302,7 +302,7 @@ static struct notifier_block kvm_clock_notifier = {
 	.notifier_call = kvm_clock_sync,
 };
 
-int kvm_arch_hardware_setup(void)
+int kvm_arch_hardware_setup(void *opaque)
 {
 	gmap_notifier.notifier_call = kvm_gmap_notifier;
 	gmap_register_pte_notifier(&gmap_notifier);

commit 8bf8961332bd6393716297e1a2b628d88300b2f2
Merge: 4b547a869db9 f3dd18d444c7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 26 05:58:49 2020 -0400

    Merge tag 'kvm-s390-next-5.7-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: cleanups for 5.7
    
    - mark sie control block as 512 byte aligned
    - use fallthrough;

commit 0774a964ef561b7170d8d1b1bfe6f88002b6d219
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 13:55:40 2020 -0700

    KVM: Fix out of range accesses to memslots
    
    Reset the LRU slot if it becomes invalid when deleting a memslot to fix
    an out-of-bounds/use-after-free access when searching through memslots.
    
    Explicitly check for there being no used slots in search_memslots(), and
    in the caller of s390's approximation variant.
    
    Fixes: 36947254e5f9 ("KVM: Dynamically size memslot array based on number of used slots")
    Reported-by: Qian Cai <cai@lca.pw>
    Cc: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 807ed6d722dd..cb15fdda1fee 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2002,6 +2002,9 @@ static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,
 	struct kvm_memslots *slots = kvm_memslots(kvm);
 	struct kvm_memory_slot *ms;
 
+	if (unlikely(!slots->used_slots))
+		return 0;
+
 	cur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);
 	ms = gfn_to_memslot(kvm, cur_gfn);
 	args->count = 0;

commit 3b684a420bd8a2689ae260ddc47b5f041f93f5f5
Author: Joe Perches <joe@perches.com>
Date:   Tue Mar 10 21:51:32 2020 -0700

    KVM: s390: Use fallthrough;
    
    Convert the various uses of fallthrough comments to fallthrough;
    
    Done via script
    Link: https://lore.kernel.org/lkml/b56602fcf79f849e733e7b521bb0e17895d390fa.1582230379.git.joe@perches.com
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Link: https://lore.kernel.org/r/d63c86429f3e5aa806aa3e185c97d213904924a5.1583896348.git.joe@perches.com
    [borntrager@de.ibm.com: Fix link to tool and subject]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6b1842a9feed..d590f32f13d3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3752,7 +3752,7 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 		rc = kvm_s390_pv_set_cpu_state(vcpu, PV_CPU_STATE_OPR_LOAD);
 		break;
 	case KVM_MP_STATE_CHECK_STOP:
-		/* fall through - CHECK_STOP and LOAD are not supported yet */
+		fallthrough;	/* CHECK_STOP and LOAD are not supported yet */
 	default:
 		rc = -ENXIO;
 	}
@@ -4985,7 +4985,7 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 					old->npages * PAGE_SIZE);
 		if (rc)
 			break;
-		/* FALLTHROUGH */
+		fallthrough;
 	case KVM_MR_CREATE:
 		rc = gmap_map_segment(kvm->arch.gmap, mem->userspace_addr,
 				      mem->guest_phys_addr, mem->memory_size);

commit 1c482452d5db0f52e4e8eed95bd7314eec537d78
Merge: 6d05a965addb cc674ef252f4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Mar 16 18:19:34 2020 +0100

    Merge tag 'kvm-s390-next-5.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Features and Enhancements for 5.7 part1
    
    1. Allow to disable gisa
    2. protected virtual machines
      Protected VMs (PVM) are KVM VMs, where KVM can't access the VM's
      state like guest memory and guest registers anymore. Instead the
      PVMs are mostly managed by a new entity called Ultravisor (UV),
      which provides an API, so KVM and the PV can request management
      actions.
    
      PVMs are encrypted at rest and protected from hypervisor access
      while running.  They switch from a normal operation into protected
      mode, so we can still use the standard boot process to load a
      encrypted blob and then move it into protected mode.
    
      Rebooting is only possible by passing through the unprotected/normal
      mode and switching to protected again.
    
      One mm related patch will go via Andrews mm tree ( mm/gup/writeback:
      add callbacks for inaccessible pages)

commit 2a49f61dfcdc25ec06b41f7466ccb94a7a9d2624
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:30 2020 -0800

    KVM: Ensure validity of memslot with respect to kvm_get_dirty_log()
    
    Rework kvm_get_dirty_log() so that it "returns" the associated memslot
    on success.  A future patch will rework memslot handling such that
    id_to_memslot() can return NULL, returning the memslot makes it more
    obvious that the validity of the memslot has been verified, i.e.
    precludes the need to add validity checks in the arch code that are
    technically unnecessary.
    
    To maintain ordering in s390, move the call to kvm_arch_sync_dirty_log()
    from s390's kvm_vm_ioctl_get_dirty_log() to the new kvm_get_dirty_log().
    This is a nop for PPC, the only other arch that doesn't select
    KVM_GENERIC_DIRTYLOG_READ_PROTECT, as its sync_dirty_log() is empty.
    
    Ideally, moving the sync_dirty_log() call would be done in a separate
    patch, but it can't be done in a follow-on patch because that would
    temporarily break s390's ordering.  Making the move in a preparatory
    patch would be functionally correct, but would create an odd scenario
    where the moved sync_dirty_log() would operate on a "different" memslot
    due to consuming the result of a different id_to_memslot().  The
    memslot couldn't actually be different as slots_lock is held, but the
    code is confusing enough as it is, i.e. moving sync_dirty_log() in this
    patch is the lesser of all evils.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2adbc2fde382..fb081c5715b2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -611,9 +611,8 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 {
 	int r;
 	unsigned long n;
-	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot;
-	int is_dirty = 0;
+	int is_dirty;
 
 	if (kvm_is_ucontrol(kvm))
 		return -EINVAL;
@@ -624,14 +623,7 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 	if (log->slot >= KVM_USER_MEM_SLOTS)
 		goto out;
 
-	slots = kvm_memslots(kvm);
-	memslot = id_to_memslot(slots, log->slot);
-	r = -ENOENT;
-	if (!memslot->dirty_bitmap)
-		goto out;
-
-	kvm_arch_sync_dirty_log(kvm, memslot);
-	r = kvm_get_dirty_log(kvm, log, &is_dirty);
+	r = kvm_get_dirty_log(kvm, log, &is_dirty, &memslot);
 	if (r)
 		goto out;
 

commit 0dff084607bd555d6f74db2af8406a9da9f0fc3a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:29 2020 -0800

    KVM: Provide common implementation for generic dirty log functions
    
    Move the implementations of KVM_GET_DIRTY_LOG and KVM_CLEAR_DIRTY_LOG
    for CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT into common KVM code.
    The arch specific implemenations are extremely similar, differing
    only in whether the dirty log needs to be sync'd from hardware (x86)
    and how the TLBs are flushed.  Add new arch hooks to handle sync
    and TLB flush; the sync will also be used for non-generic dirty log
    support in a future patch (s390).
    
    The ulterior motive for providing a common implementation is to
    eliminate the dependency between arch and common code with respect to
    the memslot referenced by the dirty log, i.e. to make it obvious in the
    code that the validity of the memslot is guaranteed, as a future patch
    will rework memslot handling such that id_to_memslot() can return NULL.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 78f92c005f93..2adbc2fde382 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -570,8 +570,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	return r;
 }
 
-static void kvm_s390_sync_dirty_log(struct kvm *kvm,
-				    struct kvm_memory_slot *memslot)
+void kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot)
 {
 	int i;
 	gfn_t cur_gfn, last_gfn;
@@ -631,7 +630,7 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 	if (!memslot->dirty_bitmap)
 		goto out;
 
-	kvm_s390_sync_dirty_log(kvm, memslot);
+	kvm_arch_sync_dirty_log(kvm, memslot);
 	r = kvm_get_dirty_log(kvm, log, &is_dirty);
 	if (r)
 		goto out;

commit 9d4c197c0e94c372ceffd2ffc53a23518f301ed9
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:24 2020 -0800

    KVM: Drop "const" attribute from old memslot in commit_memory_region()
    
    Drop the "const" attribute from @old in kvm_arch_commit_memory_region()
    to allow arch specific code to free arch specific resources in the old
    memslot without having to cast away the attribute.  Freeing resources in
    kvm_arch_commit_memory_region() paves the way for simplifying
    kvm_free_memslot() by eliminating the last usage of its @dont param.
    
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6638024e440d..78f92c005f93 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4532,7 +4532,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
-				const struct kvm_memory_slot *old,
+				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
 				enum kvm_mr_change change)
 {

commit 414de7abbf809f046511269797d9f2310b88e036
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:20 2020 -0800

    KVM: Drop kvm_arch_create_memslot()
    
    Remove kvm_arch_create_memslot() now that all arch implementations are
    effectively nops.  Removing kvm_arch_create_memslot() eliminates the
    possibility for arch specific code to allocate memory prior to setting
    a memslot, which sets the stage for simplifying kvm_free_memslot().
    
    Cc: Janosch Frank <frankja@linux.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d7ff30e45589..6638024e440d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4507,12 +4507,6 @@ vm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 	return VM_FAULT_SIGBUS;
 }
 
-int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
-			    unsigned long npages)
-{
-	return 0;
-}
-
 /* Section: memory related */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,

commit e93fc7b4544a5475cfdbc22f87e89f9829bf801c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 3 03:10:57 2020 -0500

    KVM: s390: Also reset registers in sync regs for initial cpu reset
    
    When we do the initial CPU reset we must not only clear the registers
    in the internal data structures but also in kvm_run sync_regs. For
    modern userspace sync_regs is the only place that it looks at.
    
    Fixes: 7de3f1423ff9 ("KVM: s390: Add new reset vcpu API")
    Acked-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d7ff30e45589..c2e6d4ba4e23 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3268,7 +3268,10 @@ static void kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 	/* Initial reset is a superset of the normal reset */
 	kvm_arch_vcpu_ioctl_normal_reset(vcpu);
 
-	/* this equals initial cpu reset in pop, but we don't switch to ESA */
+	/*
+	 * This equals initial cpu reset in pop, but we don't switch to ESA.
+	 * We do not only reset the internal data, but also ...
+	 */
 	vcpu->arch.sie_block->gpsw.mask = 0;
 	vcpu->arch.sie_block->gpsw.addr = 0;
 	kvm_s390_set_prefix(vcpu, 0);
@@ -3278,6 +3281,19 @@ static void kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 	memset(vcpu->arch.sie_block->gcr, 0, sizeof(vcpu->arch.sie_block->gcr));
 	vcpu->arch.sie_block->gcr[0] = CR0_INITIAL_MASK;
 	vcpu->arch.sie_block->gcr[14] = CR14_INITIAL_MASK;
+
+	/* ... the data in sync regs */
+	memset(vcpu->run->s.regs.crs, 0, sizeof(vcpu->run->s.regs.crs));
+	vcpu->run->s.regs.ckc = 0;
+	vcpu->run->s.regs.crs[0] = CR0_INITIAL_MASK;
+	vcpu->run->s.regs.crs[14] = CR14_INITIAL_MASK;
+	vcpu->run->psw_addr = 0;
+	vcpu->run->psw_mask = 0;
+	vcpu->run->s.regs.todpr = 0;
+	vcpu->run->s.regs.cputm = 0;
+	vcpu->run->s.regs.ckc = 0;
+	vcpu->run->s.regs.pp = 0;
+	vcpu->run->s.regs.gbea = 1;
 	vcpu->run->s.regs.fpc = 0;
 	vcpu->arch.sie_block->gbea = 1;
 	vcpu->arch.sie_block->pp = 0;

commit cc674ef252f4750bdcea1560ff491081bb960954
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Thu Feb 27 10:10:31 2020 +0100

    KVM: s390: introduce module parameter kvm.use_gisa
    
    The boolean module parameter "kvm.use_gisa" controls if newly
    created guests will use the GISA facility if provided by the
    host system. The default is yes.
    
      # cat /sys/module/kvm/parameters/use_gisa
      Y
    
    The parameter can be changed on the fly.
    
      # echo N > /sys/module/kvm/parameters/use_gisa
    
    Already running guests are not affected by this change.
    
    The kvm s390 debug feature shows if a guest is running with GISA.
    
      # grep gisa /sys/kernel/debug/s390dbf/kvm-$pid/sprintf
      00 01582725059:843303 3 - 08 00000000e119bc01  gisa 0x00000000c9ac2642 initialized
      00 01582725059:903840 3 - 11 000000004391ee22  00[0000000000000000-0000000000000000]: AIV gisa format-1 enabled for cpu 000
      ...
      00 01582725059:916847 3 - 08 0000000094fff572  gisa 0x00000000c9ac2642 cleared
    
    In general, that value should not be changed as the GISA facility
    enhances interruption delivery performance.
    
    A reason to switch the GISA facility off might be a performance
    comparison run or debugging.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Link: https://lore.kernel.org/r/20200227091031.102993-1-mimu@linux.ibm.com
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f4cd436ba979..6b1842a9feed 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -185,6 +185,11 @@ static u8 halt_poll_max_steal = 10;
 module_param(halt_poll_max_steal, byte, 0644);
 MODULE_PARM_DESC(halt_poll_max_steal, "Maximum percentage of steal time to allow polling");
 
+/* if set to true, the GISA will be initialized and used if available */
+static bool use_gisa  = true;
+module_param(use_gisa, bool, 0644);
+MODULE_PARM_DESC(use_gisa, "Use the GISA if the host supports it.");
+
 /*
  * For now we handle at most 16 double words as this is what the s390 base
  * kernel handles and stores in the prefix page. If we ever need to go beyond
@@ -2732,7 +2737,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.use_skf = sclp.has_skey;
 	spin_lock_init(&kvm->arch.start_stop_lock);
 	kvm_s390_vsie_init(kvm);
-	kvm_s390_gisa_init(kvm);
+	if (use_gisa)
+		kvm_s390_gisa_init(kvm);
 	KVM_EVENT(3, "vm 0x%pK created by pid %u", kvm, current->pid);
 
 	return 0;

commit 13da9ae1cdbf1ec4ea36b7612e606681c27cca13
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Feb 18 15:08:07 2020 -0500

    KVM: s390: protvirt: introduce and enable KVM_CAP_S390_PROTECTED
    
    Now that everything is in place, we can announce the feature.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bb060064cce0..f4cd436ba979 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -574,6 +574,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_BPB:
 		r = test_facility(82);
 		break;
+	case KVM_CAP_S390_PROTECTED:
+		r = is_prot_virt_host();
+		break;
 	default:
 		r = 0;
 	}

commit 8a8378fa61571eb308428780dee063c4580edb2a
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Thu Jan 9 04:37:50 2020 -0500

    KVM: s390: protvirt: Add UV cpu reset calls
    
    For protected VMs, the VCPU resets are done by the Ultravisor, as KVM
    has no access to the VCPU registers.
    
    Note that the ultravisor will only accept a call for the exact reset
    that has been requested.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fed025ea6f62..bb060064cce0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4748,6 +4748,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 	int idx;
 	long r;
+	u16 rc, rrc;
 
 	vcpu_load(vcpu);
 
@@ -4769,14 +4770,33 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	case KVM_S390_CLEAR_RESET:
 		r = 0;
 		kvm_arch_vcpu_ioctl_clear_reset(vcpu);
+		if (kvm_s390_pv_cpu_is_protected(vcpu)) {
+			r = uv_cmd_nodata(kvm_s390_pv_cpu_get_handle(vcpu),
+					  UVC_CMD_CPU_RESET_CLEAR, &rc, &rrc);
+			VCPU_EVENT(vcpu, 3, "PROTVIRT RESET CLEAR VCPU: rc %x rrc %x",
+				   rc, rrc);
+		}
 		break;
 	case KVM_S390_INITIAL_RESET:
 		r = 0;
 		kvm_arch_vcpu_ioctl_initial_reset(vcpu);
+		if (kvm_s390_pv_cpu_is_protected(vcpu)) {
+			r = uv_cmd_nodata(kvm_s390_pv_cpu_get_handle(vcpu),
+					  UVC_CMD_CPU_RESET_INITIAL,
+					  &rc, &rrc);
+			VCPU_EVENT(vcpu, 3, "PROTVIRT RESET INITIAL VCPU: rc %x rrc %x",
+				   rc, rrc);
+		}
 		break;
 	case KVM_S390_NORMAL_RESET:
 		r = 0;
 		kvm_arch_vcpu_ioctl_normal_reset(vcpu);
+		if (kvm_s390_pv_cpu_is_protected(vcpu)) {
+			r = uv_cmd_nodata(kvm_s390_pv_cpu_get_handle(vcpu),
+					  UVC_CMD_CPU_RESET, &rc, &rrc);
+			VCPU_EVENT(vcpu, 3, "PROTVIRT RESET NORMAL VCPU: rc %x rrc %x",
+				   rc, rrc);
+		}
 		break;
 	case KVM_SET_ONE_REG:
 	case KVM_GET_ONE_REG: {

commit 72f218208fa63806ebcfca7e793b095d346ee0a4
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Jan 30 11:18:28 2020 -0500

    KVM: s390: protvirt: do not inject interrupts after start
    
    As PSW restart is handled by the ultravisor (and we only get a start
    notification) we must re-check the PSW after a start before injecting
    interrupts.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 66ba6ca714fb..fed025ea6f62 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4515,6 +4515,13 @@ int kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 	}
 
 	kvm_s390_clear_cpuflags(vcpu, CPUSTAT_STOPPED);
+	/*
+	 * The real PSW might have changed due to a RESTART interpreted by the
+	 * ultravisor. We block all interrupts and let the next sie exit
+	 * refresh our view.
+	 */
+	if (kvm_s390_pv_cpu_is_protected(vcpu))
+		vcpu->arch.sie_block->gpsw.mask &= ~PSW_INT_MASK;
 	/*
 	 * Another VCPU might have used IBS while we were offline.
 	 * Let's play safe and flush the VCPU at startup.

commit 3adae0b4ca64c08a6c05a54be0becf9d127d39dc
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Dec 13 08:26:06 2019 -0500

    KVM: s390: protvirt: Mask PSW interrupt bits for interception 104 and 112
    
    We're not allowed to inject interrupts on intercepts that leave the
    guest state in an "in-between" state where the next SIE entry will do a
    continuation, namely secure instruction interception (104) and secure
    prefix interception (112).
    As our PSW is just a copy of the real one that will be replaced on the
    next exit, we can mask out the interrupt bits in the PSW to make sure
    that we do not inject anything.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 028ce4e74393..66ba6ca714fb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4093,6 +4093,7 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 	return vcpu_post_run_fault_in_sie(vcpu);
 }
 
+#define PSW_INT_MASK (PSW_MASK_EXT | PSW_MASK_IO | PSW_MASK_MCHECK)
 static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc, exit_reason;
@@ -4129,6 +4130,16 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 			memcpy(vcpu->run->s.regs.gprs,
 			       sie_page->pv_grregs,
 			       sizeof(sie_page->pv_grregs));
+			/*
+			 * We're not allowed to inject interrupts on intercepts
+			 * that leave the guest state in an "in-between" state
+			 * where the next SIE entry will do a continuation.
+			 * Fence interrupts in our "internal" PSW.
+			 */
+			if (vcpu->arch.sie_block->icptcode == ICPT_PV_INSTR ||
+			    vcpu->arch.sie_block->icptcode == ICPT_PV_PREF) {
+				vcpu->arch.sie_block->gpsw.mask &= ~PSW_INT_MASK;
+			}
 		}
 		local_irq_disable();
 		__enable_cpu_timer_accounting(vcpu);

commit 7c36a3fcf444ced8efc3da106cc7215227d60fde
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Mon Sep 2 08:34:44 2019 +0200

    KVM: s390: protvirt: Support cmd 5 operation state
    
    Code 5 for the set cpu state UV call tells the UV to load a PSW from
    the SE header (first IPL) or from guest location 0x0 (diag 308 subcode
    0/1). Also it sets the cpu into operating state afterwards, so we can
    start it.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 80e16bd72d48..028ce4e74393 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3736,6 +3736,12 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 		rc = kvm_s390_vcpu_start(vcpu);
 		break;
 	case KVM_MP_STATE_LOAD:
+		if (!kvm_s390_pv_cpu_is_protected(vcpu)) {
+			rc = -ENXIO;
+			break;
+		}
+		rc = kvm_s390_pv_set_cpu_state(vcpu, PV_CPU_STATE_OPR_LOAD);
+		break;
 	case KVM_MP_STATE_CHECK_STOP:
 		/* fall through - CHECK_STOP and LOAD are not supported yet */
 	default:

commit fe28c7868f68b82e4517efb1dc3c22e2297df340
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Wed May 15 13:24:30 2019 +0200

    KVM: s390: protvirt: Report CPU state to Ultravisor
    
    VCPU states have to be reported to the ultravisor for SIGP
    interpretation, kdump, kexec and reboot.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 16531b251eab..80e16bd72d48 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2456,6 +2456,8 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	case KVM_S390_PV_COMMAND: {
 		struct kvm_pv_cmd args;
 
+		/* protvirt means user sigp */
+		kvm->arch.user_cpu_state_ctrl = 1;
 		r = 0;
 		if (!is_prot_virt_host()) {
 			r = -EINVAL;
@@ -3728,10 +3730,10 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 
 	switch (mp_state->mp_state) {
 	case KVM_MP_STATE_STOPPED:
-		kvm_s390_vcpu_stop(vcpu);
+		rc = kvm_s390_vcpu_stop(vcpu);
 		break;
 	case KVM_MP_STATE_OPERATING:
-		kvm_s390_vcpu_start(vcpu);
+		rc = kvm_s390_vcpu_start(vcpu);
 		break;
 	case KVM_MP_STATE_LOAD:
 	case KVM_MP_STATE_CHECK_STOP:
@@ -4316,6 +4318,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	kvm_sigset_activate(vcpu);
 
+	/*
+	 * no need to check the return value of vcpu_start as it can only have
+	 * an error for protvirt, but protvirt means user cpu state
+	 */
 	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm)) {
 		kvm_s390_vcpu_start(vcpu);
 	} else if (is_vcpu_stopped(vcpu)) {
@@ -4453,18 +4459,27 @@ static void __enable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
 	kvm_s390_sync_request(KVM_REQ_ENABLE_IBS, vcpu);
 }
 
-void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
+int kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 {
-	int i, online_vcpus, started_vcpus = 0;
+	int i, online_vcpus, r = 0, started_vcpus = 0;
 
 	if (!is_vcpu_stopped(vcpu))
-		return;
+		return 0;
 
 	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 1);
 	/* Only one cpu at a time may enter/leave the STOPPED state. */
 	spin_lock(&vcpu->kvm->arch.start_stop_lock);
 	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
 
+	/* Let's tell the UV that we want to change into the operating state */
+	if (kvm_s390_pv_cpu_is_protected(vcpu)) {
+		r = kvm_s390_pv_set_cpu_state(vcpu, PV_CPU_STATE_OPR);
+		if (r) {
+			spin_unlock(&vcpu->kvm->arch.start_stop_lock);
+			return r;
+		}
+	}
+
 	for (i = 0; i < online_vcpus; i++) {
 		if (!is_vcpu_stopped(vcpu->kvm->vcpus[i]))
 			started_vcpus++;
@@ -4489,22 +4504,31 @@ void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 	 */
 	kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 	spin_unlock(&vcpu->kvm->arch.start_stop_lock);
-	return;
+	return 0;
 }
 
-void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
+int kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 {
-	int i, online_vcpus, started_vcpus = 0;
+	int i, online_vcpus, r = 0, started_vcpus = 0;
 	struct kvm_vcpu *started_vcpu = NULL;
 
 	if (is_vcpu_stopped(vcpu))
-		return;
+		return 0;
 
 	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 0);
 	/* Only one cpu at a time may enter/leave the STOPPED state. */
 	spin_lock(&vcpu->kvm->arch.start_stop_lock);
 	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
 
+	/* Let's tell the UV that we want to change into the stopped state */
+	if (kvm_s390_pv_cpu_is_protected(vcpu)) {
+		r = kvm_s390_pv_set_cpu_state(vcpu, PV_CPU_STATE_STP);
+		if (r) {
+			spin_unlock(&vcpu->kvm->arch.start_stop_lock);
+			return r;
+		}
+	}
+
 	/* SIGP STOP and SIGP STOP AND STORE STATUS has been fully processed */
 	kvm_s390_clear_stop_irq(vcpu);
 
@@ -4527,7 +4551,7 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 	}
 
 	spin_unlock(&vcpu->kvm->arch.start_stop_lock);
-	return;
+	return 0;
 }
 
 static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,

commit e0d2773d487c2a41c99d9e256d51cc0a859aa9ab
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Thu May 9 13:07:21 2019 +0200

    KVM: s390: protvirt: UV calls in support of diag308 0, 1
    
    diag 308 subcode 0 and 1 require several KVM and Ultravisor interactions.
    Specific to these "soft" reboots are
    
    * The "unshare all" UVC
    * The "prepare for reset" UVC
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Acked-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index abe295077d00..16531b251eab 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2328,6 +2328,28 @@ static int kvm_s390_handle_pv(struct kvm *kvm, struct kvm_pv_cmd *cmd)
 			     cmd->rrc);
 		break;
 	}
+	case KVM_PV_PREP_RESET: {
+		r = -EINVAL;
+		if (!kvm_s390_pv_is_protected(kvm))
+			break;
+
+		r = uv_cmd_nodata(kvm_s390_pv_get_handle(kvm),
+				  UVC_CMD_PREPARE_RESET, &cmd->rc, &cmd->rrc);
+		KVM_UV_EVENT(kvm, 3, "PROTVIRT PREP RESET: rc %x rrc %x",
+			     cmd->rc, cmd->rrc);
+		break;
+	}
+	case KVM_PV_UNSHARE_ALL: {
+		r = -EINVAL;
+		if (!kvm_s390_pv_is_protected(kvm))
+			break;
+
+		r = uv_cmd_nodata(kvm_s390_pv_get_handle(kvm),
+				  UVC_CMD_SET_UNSHARE_ALL, &cmd->rc, &cmd->rrc);
+		KVM_UV_EVENT(kvm, 3, "PROTVIRT UNSHARE: rc %x rrc %x",
+			     cmd->rc, cmd->rrc);
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit 811ea797118a8caf54b54fc5c30e0b6c90c8abf3
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Jun 14 13:11:21 2019 +0200

    KVM: s390: protvirt: Only sync fmt4 registers
    
    A lot of the registers are controlled by the Ultravisor and never
    visible to KVM. Also some registers are overlayed, like gbea is with
    sidad, which might leak data to userspace.
    
    Hence we sync a minimal set of registers for both SIE formats and then
    check and sync format 2 registers if necessary.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1c7bbc2497a2..abe295077d00 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4113,7 +4113,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	return rc;
 }
 
-static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
+static void sync_regs_fmt2(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	struct runtime_instr_cb *riccb;
 	struct gs_cb *gscb;
@@ -4122,16 +4122,7 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	gscb = (struct gs_cb *) &kvm_run->s.regs.gscb;
 	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
 	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
-	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX)
-		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
-	if (kvm_run->kvm_dirty_regs & KVM_SYNC_CRS) {
-		memcpy(&vcpu->arch.sie_block->gcr, &kvm_run->s.regs.crs, 128);
-		/* some control register changes require a tlb flush */
-		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
-	}
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_ARCH0) {
-		kvm_s390_set_cpu_timer(vcpu, kvm_run->s.regs.cputm);
-		vcpu->arch.sie_block->ckc = kvm_run->s.regs.ckc;
 		vcpu->arch.sie_block->todpr = kvm_run->s.regs.todpr;
 		vcpu->arch.sie_block->pp = kvm_run->s.regs.pp;
 		vcpu->arch.sie_block->gbea = kvm_run->s.regs.gbea;
@@ -4172,6 +4163,36 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
 		vcpu->arch.sie_block->fpf |= kvm_run->s.regs.bpbc ? FPF_BPBC : 0;
 	}
+	if (MACHINE_HAS_GS) {
+		preempt_disable();
+		__ctl_set_bit(2, 4);
+		if (current->thread.gs_cb) {
+			vcpu->arch.host_gscb = current->thread.gs_cb;
+			save_gs_cb(vcpu->arch.host_gscb);
+		}
+		if (vcpu->arch.gs_enabled) {
+			current->thread.gs_cb = (struct gs_cb *)
+						&vcpu->run->s.regs.gscb;
+			restore_gs_cb(current->thread.gs_cb);
+		}
+		preempt_enable();
+	}
+	/* SIE will load etoken directly from SDNX and therefore kvm_run */
+}
+
+static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
+{
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX)
+		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_CRS) {
+		memcpy(&vcpu->arch.sie_block->gcr, &kvm_run->s.regs.crs, 128);
+		/* some control register changes require a tlb flush */
+		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
+	}
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_ARCH0) {
+		kvm_s390_set_cpu_timer(vcpu, kvm_run->s.regs.cputm);
+		vcpu->arch.sie_block->ckc = kvm_run->s.regs.ckc;
+	}
 	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	/* save host (userspace) fprs/vrs */
@@ -4186,23 +4207,47 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (test_fp_ctl(current->thread.fpu.fpc))
 		/* User space provided an invalid FPC, let's clear it */
 		current->thread.fpu.fpc = 0;
+
+	/* Sync fmt2 only data */
+	if (likely(!kvm_s390_pv_cpu_is_protected(vcpu))) {
+		sync_regs_fmt2(vcpu, kvm_run);
+	} else {
+		/*
+		 * In several places we have to modify our internal view to
+		 * not do things that are disallowed by the ultravisor. For
+		 * example we must not inject interrupts after specific exits
+		 * (e.g. 112 prefix page not secure). We do this by turning
+		 * off the machine check, external and I/O interrupt bits
+		 * of our PSW copy. To avoid getting validity intercepts, we
+		 * do only accept the condition code from userspace.
+		 */
+		vcpu->arch.sie_block->gpsw.mask &= ~PSW_MASK_CC;
+		vcpu->arch.sie_block->gpsw.mask |= kvm_run->psw_mask &
+						   PSW_MASK_CC;
+	}
+
+	kvm_run->kvm_dirty_regs = 0;
+}
+
+static void store_regs_fmt2(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
+{
+	kvm_run->s.regs.todpr = vcpu->arch.sie_block->todpr;
+	kvm_run->s.regs.pp = vcpu->arch.sie_block->pp;
+	kvm_run->s.regs.gbea = vcpu->arch.sie_block->gbea;
+	kvm_run->s.regs.bpbc = (vcpu->arch.sie_block->fpf & FPF_BPBC) == FPF_BPBC;
 	if (MACHINE_HAS_GS) {
-		preempt_disable();
 		__ctl_set_bit(2, 4);
-		if (current->thread.gs_cb) {
-			vcpu->arch.host_gscb = current->thread.gs_cb;
-			save_gs_cb(vcpu->arch.host_gscb);
-		}
-		if (vcpu->arch.gs_enabled) {
-			current->thread.gs_cb = (struct gs_cb *)
-						&vcpu->run->s.regs.gscb;
-			restore_gs_cb(current->thread.gs_cb);
-		}
+		if (vcpu->arch.gs_enabled)
+			save_gs_cb(current->thread.gs_cb);
+		preempt_disable();
+		current->thread.gs_cb = vcpu->arch.host_gscb;
+		restore_gs_cb(vcpu->arch.host_gscb);
 		preempt_enable();
+		if (!vcpu->arch.host_gscb)
+			__ctl_clear_bit(2, 4);
+		vcpu->arch.host_gscb = NULL;
 	}
-	/* SIE will load etoken directly from SDNX and therefore kvm_run */
-
-	kvm_run->kvm_dirty_regs = 0;
+	/* SIE will save etoken directly into SDNX and therefore kvm_run */
 }
 
 static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
@@ -4213,13 +4258,9 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	memcpy(&kvm_run->s.regs.crs, &vcpu->arch.sie_block->gcr, 128);
 	kvm_run->s.regs.cputm = kvm_s390_get_cpu_timer(vcpu);
 	kvm_run->s.regs.ckc = vcpu->arch.sie_block->ckc;
-	kvm_run->s.regs.todpr = vcpu->arch.sie_block->todpr;
-	kvm_run->s.regs.pp = vcpu->arch.sie_block->pp;
-	kvm_run->s.regs.gbea = vcpu->arch.sie_block->gbea;
 	kvm_run->s.regs.pft = vcpu->arch.pfault_token;
 	kvm_run->s.regs.pfs = vcpu->arch.pfault_select;
 	kvm_run->s.regs.pfc = vcpu->arch.pfault_compare;
-	kvm_run->s.regs.bpbc = (vcpu->arch.sie_block->fpf & FPF_BPBC) == FPF_BPBC;
 	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_access_regs(vcpu->arch.host_acrs);
 	/* Save guest register state */
@@ -4228,19 +4269,8 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	/* Restore will be done lazily at return */
 	current->thread.fpu.fpc = vcpu->arch.host_fpregs.fpc;
 	current->thread.fpu.regs = vcpu->arch.host_fpregs.regs;
-	if (MACHINE_HAS_GS) {
-		__ctl_set_bit(2, 4);
-		if (vcpu->arch.gs_enabled)
-			save_gs_cb(current->thread.gs_cb);
-		preempt_disable();
-		current->thread.gs_cb = vcpu->arch.host_gscb;
-		restore_gs_cb(vcpu->arch.host_gscb);
-		preempt_enable();
-		if (!vcpu->arch.host_gscb)
-			__ctl_clear_bit(2, 4);
-		vcpu->arch.host_gscb = NULL;
-	}
-	/* SIE will save etoken directly into SDNX and therefore kvm_run */
+	if (likely(!kvm_s390_pv_cpu_is_protected(vcpu)))
+		store_regs_fmt2(vcpu, kvm_run);
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)

commit 0f3035047140b3dc18fc5a028ed5f273f24b5539
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Mon Feb 10 04:27:47 2020 -0500

    KVM: s390: protvirt: Do only reset registers that are accessible
    
    For protected VMs the hypervisor can not access guest breaking event
    address, program parameter, bpbc and todpr. Do not reset those fields
    as the control block does not provide access to these fields.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 797b4031ed4d..1c7bbc2497a2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3502,14 +3502,21 @@ static void kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 	kvm_s390_set_prefix(vcpu, 0);
 	kvm_s390_set_cpu_timer(vcpu, 0);
 	vcpu->arch.sie_block->ckc = 0;
-	vcpu->arch.sie_block->todpr = 0;
 	memset(vcpu->arch.sie_block->gcr, 0, sizeof(vcpu->arch.sie_block->gcr));
 	vcpu->arch.sie_block->gcr[0] = CR0_INITIAL_MASK;
 	vcpu->arch.sie_block->gcr[14] = CR14_INITIAL_MASK;
 	vcpu->run->s.regs.fpc = 0;
-	vcpu->arch.sie_block->gbea = 1;
-	vcpu->arch.sie_block->pp = 0;
-	vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
+	/*
+	 * Do not reset these registers in the protected case, as some of
+	 * them are overlayed and they are not accessible in this case
+	 * anyway.
+	 */
+	if (!kvm_s390_pv_cpu_is_protected(vcpu)) {
+		vcpu->arch.sie_block->gbea = 1;
+		vcpu->arch.sie_block->pp = 0;
+		vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
+		vcpu->arch.sie_block->todpr = 0;
+	}
 }
 
 static void kvm_arch_vcpu_ioctl_clear_reset(struct kvm_vcpu *vcpu)

commit 68cf7b1f137e61cea71925e48bc0c6d7bcfc637c
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Jun 14 13:11:21 2019 +0200

    KVM: s390: protvirt: disallow one_reg
    
    A lot of the registers are controlled by the Ultravisor and never
    visible to KVM. Some fields in the sie control block are overlayed, like
    gbea. As no known userspace uses the ONE_REG interface on s390 if sync
    regs are available, no functionality is lost if it is disabled for
    protected guests.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index efbbcd2948a3..797b4031ed4d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4674,6 +4674,9 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	case KVM_SET_ONE_REG:
 	case KVM_GET_ONE_REG: {
 		struct kvm_one_reg reg;
+		r = -EINVAL;
+		if (kvm_s390_pv_cpu_is_protected(vcpu))
+			break;
 		r = -EFAULT;
 		if (copy_from_user(&reg, argp, sizeof(reg)))
 			break;

commit 19e1227768863a1469797c13ef8fea1af7beac2c
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Tue Apr 2 09:21:06 2019 +0200

    KVM: S390: protvirt: Introduce instruction data area bounce buffer
    
    Now that we can't access guest memory anymore, we have a dedicated
    satellite block that's a bounce buffer for instruction data.
    
    We re-use the memop interface to copy the instruction data to / from
    userspace. This lets us re-use a lot of QEMU code which used that
    interface to make logical guest memory accesses which are not possible
    anymore in protected mode anyway.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bd62312fdc0e..efbbcd2948a3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4495,12 +4495,40 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+static long kvm_s390_guest_sida_op(struct kvm_vcpu *vcpu,
+				   struct kvm_s390_mem_op *mop)
+{
+	void __user *uaddr = (void __user *)mop->buf;
+	int r = 0;
+
+	if (mop->flags || !mop->size)
+		return -EINVAL;
+	if (mop->size + mop->sida_offset < mop->size)
+		return -EINVAL;
+	if (mop->size + mop->sida_offset > sida_size(vcpu->arch.sie_block))
+		return -E2BIG;
+
+	switch (mop->op) {
+	case KVM_S390_MEMOP_SIDA_READ:
+		if (copy_to_user(uaddr, (void *)(sida_origin(vcpu->arch.sie_block) +
+				 mop->sida_offset), mop->size))
+			r = -EFAULT;
+
+		break;
+	case KVM_S390_MEMOP_SIDA_WRITE:
+		if (copy_from_user((void *)(sida_origin(vcpu->arch.sie_block) +
+				   mop->sida_offset), uaddr, mop->size))
+			r = -EFAULT;
+		break;
+	}
+	return r;
+}
 static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 				  struct kvm_s390_mem_op *mop)
 {
 	void __user *uaddr = (void __user *)mop->buf;
 	void *tmpbuf = NULL;
-	int r, srcu_idx;
+	int r = 0;
 	const u64 supported_flags = KVM_S390_MEMOP_F_INJECT_EXCEPTION
 				    | KVM_S390_MEMOP_F_CHECK_ONLY;
 
@@ -4510,14 +4538,15 @@ static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 	if (mop->size > MEM_OP_MAX_SIZE)
 		return -E2BIG;
 
+	if (kvm_s390_pv_cpu_is_protected(vcpu))
+		return -EINVAL;
+
 	if (!(mop->flags & KVM_S390_MEMOP_F_CHECK_ONLY)) {
 		tmpbuf = vmalloc(mop->size);
 		if (!tmpbuf)
 			return -ENOMEM;
 	}
 
-	srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
-
 	switch (mop->op) {
 	case KVM_S390_MEMOP_LOGICAL_READ:
 		if (mop->flags & KVM_S390_MEMOP_F_CHECK_ONLY) {
@@ -4543,12 +4572,8 @@ static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 		}
 		r = write_guest(vcpu, mop->gaddr, mop->ar, tmpbuf, mop->size);
 		break;
-	default:
-		r = -EINVAL;
 	}
 
-	srcu_read_unlock(&vcpu->kvm->srcu, srcu_idx);
-
 	if (r > 0 && (mop->flags & KVM_S390_MEMOP_F_INJECT_EXCEPTION) != 0)
 		kvm_s390_inject_prog_irq(vcpu, &vcpu->arch.pgm);
 
@@ -4556,6 +4581,31 @@ static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+static long kvm_s390_guest_memsida_op(struct kvm_vcpu *vcpu,
+				      struct kvm_s390_mem_op *mop)
+{
+	int r, srcu_idx;
+
+	srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
+
+	switch (mop->op) {
+	case KVM_S390_MEMOP_LOGICAL_READ:
+	case KVM_S390_MEMOP_LOGICAL_WRITE:
+		r = kvm_s390_guest_mem_op(vcpu, mop);
+		break;
+	case KVM_S390_MEMOP_SIDA_READ:
+	case KVM_S390_MEMOP_SIDA_WRITE:
+		/* we are locked against sida going away by the vcpu->mutex */
+		r = kvm_s390_guest_sida_op(vcpu, mop);
+		break;
+	default:
+		r = -EINVAL;
+	}
+
+	srcu_read_unlock(&vcpu->kvm->srcu, srcu_idx);
+	return r;
+}
+
 long kvm_arch_vcpu_async_ioctl(struct file *filp,
 			       unsigned int ioctl, unsigned long arg)
 {
@@ -4686,7 +4736,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		struct kvm_s390_mem_op mem_op;
 
 		if (copy_from_user(&mem_op, argp, sizeof(mem_op)) == 0)
-			r = kvm_s390_guest_mem_op(vcpu, &mem_op);
+			r = kvm_s390_guest_memsida_op(vcpu, &mem_op);
 		else
 			r = -EFAULT;
 		break;

commit c8aac2344d663ec9c635ccec368341602f255f4c
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Wed May 8 15:52:00 2019 +0200

    KVM: s390: protvirt: Add new gprs location handling
    
    Guest registers for protected guests are stored at offset 0x380.  We
    will copy those to the usual places.  Long term we could refactor this
    or use register access functions.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2881151fd773..bd62312fdc0e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4059,6 +4059,7 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc, exit_reason;
+	struct sie_page *sie_page = (struct sie_page *)vcpu->arch.sie_block;
 
 	/*
 	 * We try to hold kvm->srcu during most of vcpu_run (except when run-
@@ -4080,8 +4081,18 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		guest_enter_irqoff();
 		__disable_cpu_timer_accounting(vcpu);
 		local_irq_enable();
+		if (kvm_s390_pv_cpu_is_protected(vcpu)) {
+			memcpy(sie_page->pv_grregs,
+			       vcpu->run->s.regs.gprs,
+			       sizeof(sie_page->pv_grregs));
+		}
 		exit_reason = sie64a(vcpu->arch.sie_block,
 				     vcpu->run->s.regs.gprs);
+		if (kvm_s390_pv_cpu_is_protected(vcpu)) {
+			memcpy(vcpu->run->s.regs.gprs,
+			       sie_page->pv_grregs,
+			       sizeof(sie_page->pv_grregs));
+		}
 		local_irq_disable();
 		__enable_cpu_timer_accounting(vcpu);
 		guest_exit_irqoff();

commit 0890ddea1a90e57114b5704cd560192c743f3d2e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Feb 3 09:13:37 2020 +0100

    KVM: s390: protvirt: Add SCLP interrupt handling
    
    The sclp interrupt is kind of special. The ultravisor polices that we
    do not inject an sclp interrupt with payload if no sccb is outstanding.
    On the other hand we have "asynchronous" event interrupts, e.g. for
    console input.
    We separate both variants into sclp interrupt and sclp event interrupt.
    The sclp interrupt is masked until a previous servc instruction has
    finished (sie exit 108).
    
    [frankja@linux.ibm.com: factoring out write_sclp]
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bf61f48e9a3d..2881151fd773 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2247,6 +2247,9 @@ static int kvm_s390_handle_pv(struct kvm *kvm, struct kvm_pv_cmd *cmd)
 		r = kvm_s390_cpus_to_pv(kvm, &cmd->rc, &cmd->rrc);
 		if (r)
 			kvm_s390_pv_deinit_vm(kvm, &dummy, &dummy);
+
+		/* we need to block service interrupts from now on */
+		set_bit(IRQ_PEND_EXT_SERVICE, &kvm->arch.float_int.masked_irqs);
 		break;
 	}
 	case KVM_PV_DISABLE: {
@@ -2263,6 +2266,9 @@ static int kvm_s390_handle_pv(struct kvm *kvm, struct kvm_pv_cmd *cmd)
 		if (r)
 			break;
 		r = kvm_s390_pv_deinit_vm(kvm, &cmd->rc, &cmd->rrc);
+
+		/* no need to block service interrupts any more */
+		clear_bit(IRQ_PEND_EXT_SERVICE, &kvm->arch.float_int.masked_irqs);
 		break;
 	}
 	case KVM_PV_SET_SEC_PARMS: {

commit fa0c5eabbdd33012b369cf75d6a39389cc9ae707
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Tue Jul 16 13:08:37 2019 +0200

    KVM: s390: protvirt: Secure memory is not mergeable
    
    KSM will not work on secure pages, because when the kernel reads a
    secure page, it will be encrypted and hence no two pages will look the
    same.
    
    Let's mark the guest pages as unmergeable when we transition to secure
    mode.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 87258bebb955..bf61f48e9a3d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2234,6 +2234,12 @@ static int kvm_s390_handle_pv(struct kvm *kvm, struct kvm_pv_cmd *cmd)
 		if (r)
 			break;
 
+		down_write(&current->mm->mmap_sem);
+		r = gmap_mark_unmergeable();
+		up_write(&current->mm->mmap_sem);
+		if (r)
+			break;
+
 		r = kvm_s390_pv_init_vm(kvm, &cmd->rc, &cmd->rrc);
 		if (r)
 			break;

commit 29b40f105ec8d555984c1f72dc9133b122e51903
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Mon Sep 30 04:19:18 2019 -0400

    KVM: s390: protvirt: Add initial vm and cpu lifecycle handling
    
    This contains 3 main changes:
    1. changes in SIE control block handling for secure guests
    2. helper functions for create/destroy/unpack secure guests
    3. KVM_S390_PV_COMMAND ioctl to allow userspace dealing with secure
    machines
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7e4a982bfea3..87258bebb955 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -44,6 +44,7 @@
 #include <asm/cpacf.h>
 #include <asm/timex.h>
 #include <asm/ap.h>
+#include <asm/uv.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
 
@@ -234,8 +235,10 @@ int kvm_arch_check_processor_compat(void)
 	return 0;
 }
 
+/* forward declarations */
 static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
 			      unsigned long end);
+static int sca_switch_to_extended(struct kvm *kvm);
 
 static void kvm_clock_sync_scb(struct kvm_s390_sie_block *scb, u64 delta)
 {
@@ -2165,6 +2168,160 @@ static int kvm_s390_set_cmma_bits(struct kvm *kvm,
 	return r;
 }
 
+static int kvm_s390_cpus_from_pv(struct kvm *kvm, u16 *rcp, u16 *rrcp)
+{
+	struct kvm_vcpu *vcpu;
+	u16 rc, rrc;
+	int ret = 0;
+	int i;
+
+	/*
+	 * We ignore failures and try to destroy as many CPUs as possible.
+	 * At the same time we must not free the assigned resources when
+	 * this fails, as the ultravisor has still access to that memory.
+	 * So kvm_s390_pv_destroy_cpu can leave a "wanted" memory leak
+	 * behind.
+	 * We want to return the first failure rc and rrc, though.
+	 */
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		mutex_lock(&vcpu->mutex);
+		if (kvm_s390_pv_destroy_cpu(vcpu, &rc, &rrc) && !ret) {
+			*rcp = rc;
+			*rrcp = rrc;
+			ret = -EIO;
+		}
+		mutex_unlock(&vcpu->mutex);
+	}
+	return ret;
+}
+
+static int kvm_s390_cpus_to_pv(struct kvm *kvm, u16 *rc, u16 *rrc)
+{
+	int i, r = 0;
+	u16 dummy;
+
+	struct kvm_vcpu *vcpu;
+
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		mutex_lock(&vcpu->mutex);
+		r = kvm_s390_pv_create_cpu(vcpu, rc, rrc);
+		mutex_unlock(&vcpu->mutex);
+		if (r)
+			break;
+	}
+	if (r)
+		kvm_s390_cpus_from_pv(kvm, &dummy, &dummy);
+	return r;
+}
+
+static int kvm_s390_handle_pv(struct kvm *kvm, struct kvm_pv_cmd *cmd)
+{
+	int r = 0;
+	u16 dummy;
+	void __user *argp = (void __user *)cmd->data;
+
+	switch (cmd->cmd) {
+	case KVM_PV_ENABLE: {
+		r = -EINVAL;
+		if (kvm_s390_pv_is_protected(kvm))
+			break;
+
+		/*
+		 *  FMT 4 SIE needs esca. As we never switch back to bsca from
+		 *  esca, we need no cleanup in the error cases below
+		 */
+		r = sca_switch_to_extended(kvm);
+		if (r)
+			break;
+
+		r = kvm_s390_pv_init_vm(kvm, &cmd->rc, &cmd->rrc);
+		if (r)
+			break;
+
+		r = kvm_s390_cpus_to_pv(kvm, &cmd->rc, &cmd->rrc);
+		if (r)
+			kvm_s390_pv_deinit_vm(kvm, &dummy, &dummy);
+		break;
+	}
+	case KVM_PV_DISABLE: {
+		r = -EINVAL;
+		if (!kvm_s390_pv_is_protected(kvm))
+			break;
+
+		r = kvm_s390_cpus_from_pv(kvm, &cmd->rc, &cmd->rrc);
+		/*
+		 * If a CPU could not be destroyed, destroy VM will also fail.
+		 * There is no point in trying to destroy it. Instead return
+		 * the rc and rrc from the first CPU that failed destroying.
+		 */
+		if (r)
+			break;
+		r = kvm_s390_pv_deinit_vm(kvm, &cmd->rc, &cmd->rrc);
+		break;
+	}
+	case KVM_PV_SET_SEC_PARMS: {
+		struct kvm_s390_pv_sec_parm parms = {};
+		void *hdr;
+
+		r = -EINVAL;
+		if (!kvm_s390_pv_is_protected(kvm))
+			break;
+
+		r = -EFAULT;
+		if (copy_from_user(&parms, argp, sizeof(parms)))
+			break;
+
+		/* Currently restricted to 8KB */
+		r = -EINVAL;
+		if (parms.length > PAGE_SIZE * 2)
+			break;
+
+		r = -ENOMEM;
+		hdr = vmalloc(parms.length);
+		if (!hdr)
+			break;
+
+		r = -EFAULT;
+		if (!copy_from_user(hdr, (void __user *)parms.origin,
+				    parms.length))
+			r = kvm_s390_pv_set_sec_parms(kvm, hdr, parms.length,
+						      &cmd->rc, &cmd->rrc);
+
+		vfree(hdr);
+		break;
+	}
+	case KVM_PV_UNPACK: {
+		struct kvm_s390_pv_unp unp = {};
+
+		r = -EINVAL;
+		if (!kvm_s390_pv_is_protected(kvm))
+			break;
+
+		r = -EFAULT;
+		if (copy_from_user(&unp, argp, sizeof(unp)))
+			break;
+
+		r = kvm_s390_pv_unpack(kvm, unp.addr, unp.size, unp.tweak,
+				       &cmd->rc, &cmd->rrc);
+		break;
+	}
+	case KVM_PV_VERIFY: {
+		r = -EINVAL;
+		if (!kvm_s390_pv_is_protected(kvm))
+			break;
+
+		r = uv_cmd_nodata(kvm_s390_pv_get_handle(kvm),
+				  UVC_CMD_VERIFY_IMG, &cmd->rc, &cmd->rrc);
+		KVM_UV_EVENT(kvm, 3, "PROTVIRT VERIFY: rc %x rrc %x", cmd->rc,
+			     cmd->rrc);
+		break;
+	}
+	default:
+		r = -ENOTTY;
+	}
+	return r;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg)
 {
@@ -2262,6 +2419,31 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		mutex_unlock(&kvm->slots_lock);
 		break;
 	}
+	case KVM_S390_PV_COMMAND: {
+		struct kvm_pv_cmd args;
+
+		r = 0;
+		if (!is_prot_virt_host()) {
+			r = -EINVAL;
+			break;
+		}
+		if (copy_from_user(&args, argp, sizeof(args))) {
+			r = -EFAULT;
+			break;
+		}
+		if (args.flags) {
+			r = -EINVAL;
+			break;
+		}
+		mutex_lock(&kvm->lock);
+		r = kvm_s390_handle_pv(kvm, &args);
+		mutex_unlock(&kvm->lock);
+		if (copy_to_user(argp, &args, sizeof(args))) {
+			r = -EFAULT;
+			break;
+		}
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}
@@ -2525,6 +2707,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
+	u16 rc, rrc;
+
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
 	trace_kvm_s390_destroy_vcpu(vcpu->vcpu_id);
 	kvm_s390_clear_local_irqs(vcpu);
@@ -2537,6 +2721,9 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 	if (vcpu->kvm->arch.use_cmma)
 		kvm_s390_vcpu_unsetup_cmma(vcpu);
+	/* We can not hold the vcpu mutex here, we are already dying */
+	if (kvm_s390_pv_cpu_get_handle(vcpu))
+		kvm_s390_pv_destroy_cpu(vcpu, &rc, &rrc);
 	free_page((unsigned long)(vcpu->arch.sie_block));
 }
 
@@ -2558,10 +2745,20 @@ static void kvm_free_vcpus(struct kvm *kvm)
 
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
+	u16 rc, rrc;
+
 	kvm_free_vcpus(kvm);
 	sca_dispose(kvm);
-	debug_unregister(kvm->arch.dbf);
 	kvm_s390_gisa_destroy(kvm);
+	/*
+	 * We are already at the end of life and kvm->lock is not taken.
+	 * This is ok as the file descriptor is closed by now and nobody
+	 * can mess with the pv state. To avoid lockdep_assert_held from
+	 * complaining we do not use kvm_s390_pv_is_protected.
+	 */
+	if (kvm_s390_pv_get_handle(kvm))
+		kvm_s390_pv_deinit_vm(kvm, &rc, &rrc);
+	debug_unregister(kvm->arch.dbf);
 	free_page((unsigned long)kvm->arch.sie_page2);
 	if (!kvm_is_ucontrol(kvm))
 		gmap_remove(kvm->arch.gmap);
@@ -2657,6 +2854,9 @@ static int sca_switch_to_extended(struct kvm *kvm)
 	unsigned int vcpu_idx;
 	u32 scaol, scaoh;
 
+	if (kvm->arch.use_esca)
+		return 0;
+
 	new_sca = alloc_pages_exact(sizeof(*new_sca), GFP_KERNEL|__GFP_ZERO);
 	if (!new_sca)
 		return -ENOMEM;
@@ -2908,6 +3108,7 @@ static void kvm_s390_vcpu_setup_model(struct kvm_vcpu *vcpu)
 static int kvm_s390_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	int rc = 0;
+	u16 uvrc, uvrrc;
 
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |
 						    CPUSTAT_SM |
@@ -2975,6 +3176,14 @@ static int kvm_s390_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	kvm_s390_vcpu_crypto_setup(vcpu);
 
+	mutex_lock(&vcpu->kvm->lock);
+	if (kvm_s390_pv_is_protected(vcpu->kvm)) {
+		rc = kvm_s390_pv_create_cpu(vcpu, &uvrc, &uvrrc);
+		if (rc)
+			kvm_s390_vcpu_unsetup_cmma(vcpu);
+	}
+	mutex_unlock(&vcpu->kvm->lock);
+
 	return rc;
 }
 
@@ -4540,6 +4749,9 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	if (mem->guest_phys_addr + mem->memory_size > kvm->arch.mem_limit)
 		return -EINVAL;
 
+	/* When we are protected, we should not change the memory slots */
+	if (kvm_s390_pv_get_handle(kvm))
+		return -EINVAL;
 	return 0;
 }
 

commit 3e6c556899d02e04d3d65f0e12adfbe05a557832
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Wed Oct 2 04:46:58 2019 -0400

    KVM: s390: protvirt: Add UV debug trace
    
    Let's have some debug traces which stay around for longer than the
    guest.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d7ff30e45589..7e4a982bfea3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2,7 +2,7 @@
 /*
  * hosting IBM Z kernel virtual machines (s390x)
  *
- * Copyright IBM Corp. 2008, 2018
+ * Copyright IBM Corp. 2008, 2020
  *
  *    Author(s): Carsten Otte <cotte@de.ibm.com>
  *               Christian Borntraeger <borntraeger@de.ibm.com>
@@ -220,6 +220,7 @@ static struct kvm_s390_vm_cpu_subfunc kvm_s390_available_subfunc;
 static struct gmap_notifier gmap_notifier;
 static struct gmap_notifier vsie_gmap_notifier;
 debug_info_t *kvm_s390_dbf;
+debug_info_t *kvm_s390_dbf_uv;
 
 /* Section: not file related */
 int kvm_arch_hardware_enable(void)
@@ -460,7 +461,12 @@ int kvm_arch_init(void *opaque)
 	if (!kvm_s390_dbf)
 		return -ENOMEM;
 
-	if (debug_register_view(kvm_s390_dbf, &debug_sprintf_view))
+	kvm_s390_dbf_uv = debug_register("kvm-uv", 32, 1, 7 * sizeof(long));
+	if (!kvm_s390_dbf_uv)
+		goto out;
+
+	if (debug_register_view(kvm_s390_dbf, &debug_sprintf_view) ||
+	    debug_register_view(kvm_s390_dbf_uv, &debug_sprintf_view))
 		goto out;
 
 	kvm_s390_cpu_feat_init();
@@ -487,6 +493,7 @@ void kvm_arch_exit(void)
 {
 	kvm_s390_gib_destroy();
 	debug_unregister(kvm_s390_dbf);
+	debug_unregister(kvm_s390_dbf_uv);
 }
 
 /* Section: device related */

commit ef09f4f463350ef4c174158648d91bb7ebfd10db
Merge: df7e8818926e b2ff728bae9b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Feb 5 16:15:05 2020 +0100

    Merge tag 'kvm-s390-next-5.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Fixes and cleanups for 5.6
    - fix register corruption
    - ENOTSUPP/EOPNOTSUPP mixed
    - reset cleanups/fixes
    - selftests

commit 7de3f1423ff9431f3bd5023bb78d1e062314e7f0
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Jan 31 05:02:02 2020 -0500

    KVM: s390: Add new reset vcpu API
    
    The architecture states that we need to reset local IRQs for all CPU
    resets. Because the old reset interface did not support the normal CPU
    reset we never did that on a normal reset.
    
    Let's implement an interface for the missing normal and clear resets
    and reset all local IRQs, registers and control structures as stated
    in the architecture.
    
    Userspace might already reset the registers via the vcpu run struct,
    but as we need the interface for the interrupt clearing part anyway,
    we implement the resets fully and don't rely on userspace to reset the
    rest.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Link: https://lore.kernel.org/r/20200131100205.74720-4-frankja@linux.ibm.com
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bb072866bd69..e39f6ef97b09 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -529,6 +529,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_CMMA_MIGRATION:
 	case KVM_CAP_S390_AIS:
 	case KVM_CAP_S390_AIS_MIGRATION:
+	case KVM_CAP_S390_VCPU_RESETS:
 		r = 1;
 		break;
 	case KVM_CAP_S390_HPAGE_1M:
@@ -2844,29 +2845,6 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 
 }
 
-static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
-{
-	/* this equals initial cpu reset in pop, but we don't switch to ESA */
-	vcpu->arch.sie_block->gpsw.mask = 0;
-	vcpu->arch.sie_block->gpsw.addr = 0;
-	kvm_s390_set_prefix(vcpu, 0);
-	kvm_s390_set_cpu_timer(vcpu, 0);
-	vcpu->arch.sie_block->ckc = 0;
-	vcpu->arch.sie_block->todpr = 0;
-	memset(vcpu->arch.sie_block->gcr, 0, sizeof(vcpu->arch.sie_block->gcr));
-	vcpu->arch.sie_block->gcr[0] = CR0_INITIAL_MASK;
-	vcpu->arch.sie_block->gcr[14] = CR14_INITIAL_MASK;
-	vcpu->run->s.regs.fpc = 0;
-	vcpu->arch.sie_block->gbea = 1;
-	vcpu->arch.sie_block->pp = 0;
-	vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
-	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
-	kvm_clear_async_pf_completion_queue(vcpu);
-	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm))
-		kvm_s390_vcpu_stop(vcpu);
-	kvm_s390_clear_local_irqs(vcpu);
-}
-
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 	mutex_lock(&vcpu->kvm->lock);
@@ -3281,10 +3259,53 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 	return r;
 }
 
-static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
+static void kvm_arch_vcpu_ioctl_normal_reset(struct kvm_vcpu *vcpu)
 {
-	kvm_s390_vcpu_initial_reset(vcpu);
-	return 0;
+	vcpu->arch.sie_block->gpsw.mask &= ~PSW_MASK_RI;
+	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
+	memset(vcpu->run->s.regs.riccb, 0, sizeof(vcpu->run->s.regs.riccb));
+
+	kvm_clear_async_pf_completion_queue(vcpu);
+	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm))
+		kvm_s390_vcpu_stop(vcpu);
+	kvm_s390_clear_local_irqs(vcpu);
+}
+
+static void kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
+{
+	/* Initial reset is a superset of the normal reset */
+	kvm_arch_vcpu_ioctl_normal_reset(vcpu);
+
+	/* this equals initial cpu reset in pop, but we don't switch to ESA */
+	vcpu->arch.sie_block->gpsw.mask = 0;
+	vcpu->arch.sie_block->gpsw.addr = 0;
+	kvm_s390_set_prefix(vcpu, 0);
+	kvm_s390_set_cpu_timer(vcpu, 0);
+	vcpu->arch.sie_block->ckc = 0;
+	vcpu->arch.sie_block->todpr = 0;
+	memset(vcpu->arch.sie_block->gcr, 0, sizeof(vcpu->arch.sie_block->gcr));
+	vcpu->arch.sie_block->gcr[0] = CR0_INITIAL_MASK;
+	vcpu->arch.sie_block->gcr[14] = CR14_INITIAL_MASK;
+	vcpu->run->s.regs.fpc = 0;
+	vcpu->arch.sie_block->gbea = 1;
+	vcpu->arch.sie_block->pp = 0;
+	vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
+}
+
+static void kvm_arch_vcpu_ioctl_clear_reset(struct kvm_vcpu *vcpu)
+{
+	struct kvm_sync_regs *regs = &vcpu->run->s.regs;
+
+	/* Clear reset is a superset of the initial reset */
+	kvm_arch_vcpu_ioctl_initial_reset(vcpu);
+
+	memset(&regs->gprs, 0, sizeof(regs->gprs));
+	memset(&regs->vrs, 0, sizeof(regs->vrs));
+	memset(&regs->acrs, 0, sizeof(regs->acrs));
+	memset(&regs->gscb, 0, sizeof(regs->gscb));
+
+	regs->etoken = 0;
+	regs->etoken_extension = 0;
 }
 
 int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
@@ -4357,8 +4378,17 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		r = kvm_arch_vcpu_ioctl_set_initial_psw(vcpu, psw);
 		break;
 	}
+	case KVM_S390_CLEAR_RESET:
+		r = 0;
+		kvm_arch_vcpu_ioctl_clear_reset(vcpu);
+		break;
 	case KVM_S390_INITIAL_RESET:
-		r = kvm_arch_vcpu_ioctl_initial_reset(vcpu);
+		r = 0;
+		kvm_arch_vcpu_ioctl_initial_reset(vcpu);
+		break;
+	case KVM_S390_NORMAL_RESET:
+		r = 0;
+		kvm_arch_vcpu_ioctl_normal_reset(vcpu);
 		break;
 	case KVM_SET_ONE_REG:
 	case KVM_GET_ONE_REG: {

commit cca00ebb8ad3b3a5eb6e60b1ac7e9211f66af477
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Jan 31 05:02:01 2020 -0500

    KVM: s390: Cleanup initial cpu reset
    
    The code seems to be quite old and uses lots of unneeded spaces for
    alignment, which doesn't really help with readability.
    
    Let's:
    * Get rid of the extra spaces
    * Remove the ULs as they are not needed on 0s
    * Define constants for the CR 0 and 14 initial values
    * Use the sizeof of the gcr array to memset it to 0
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Link: https://lore.kernel.org/r/20200131100205.74720-3-frankja@linux.ibm.com
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 876802894b35..bb072866bd69 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2847,19 +2847,15 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 {
 	/* this equals initial cpu reset in pop, but we don't switch to ESA */
-	vcpu->arch.sie_block->gpsw.mask = 0UL;
-	vcpu->arch.sie_block->gpsw.addr = 0UL;
+	vcpu->arch.sie_block->gpsw.mask = 0;
+	vcpu->arch.sie_block->gpsw.addr = 0;
 	kvm_s390_set_prefix(vcpu, 0);
 	kvm_s390_set_cpu_timer(vcpu, 0);
-	vcpu->arch.sie_block->ckc       = 0UL;
-	vcpu->arch.sie_block->todpr     = 0;
-	memset(vcpu->arch.sie_block->gcr, 0, 16 * sizeof(__u64));
-	vcpu->arch.sie_block->gcr[0]  = CR0_UNUSED_56 |
-					CR0_INTERRUPT_KEY_SUBMASK |
-					CR0_MEASUREMENT_ALERT_SUBMASK;
-	vcpu->arch.sie_block->gcr[14] = CR14_UNUSED_32 |
-					CR14_UNUSED_33 |
-					CR14_EXTERNAL_DAMAGE_SUBMASK;
+	vcpu->arch.sie_block->ckc = 0;
+	vcpu->arch.sie_block->todpr = 0;
+	memset(vcpu->arch.sie_block->gcr, 0, sizeof(vcpu->arch.sie_block->gcr));
+	vcpu->arch.sie_block->gcr[0] = CR0_INITIAL_MASK;
+	vcpu->arch.sie_block->gcr[14] = CR14_INITIAL_MASK;
 	vcpu->run->s.regs.fpc = 0;
 	vcpu->arch.sie_block->gbea = 1;
 	vcpu->arch.sie_block->pp = 0;

commit 55680890ea78be0df5e1384989f1be835043c084
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jan 31 05:02:00 2020 -0500

    KVM: s390: do not clobber registers during guest reset/store status
    
    The initial CPU reset clobbers the userspace fpc and the store status
    ioctl clobbers the guest acrs + fpr.  As these calls are only done via
    ioctl (and not via vcpu_run), no CPU context is loaded, so we can (and
    must) act directly on the sync regs, not on the thread context.
    
    Cc: stable@kernel.org
    Fixes: e1788bb995be ("KVM: s390: handle floating point registers in the run ioctl not in vcpu_put/load")
    Fixes: 31d8b8d41a7e ("KVM: s390: handle access registers in the run ioctl not in vcpu_put/load")
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Link: https://lore.kernel.org/r/20200131100205.74720-2-frankja@linux.ibm.com
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d9e6bf3d54f0..876802894b35 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2860,9 +2860,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->gcr[14] = CR14_UNUSED_32 |
 					CR14_UNUSED_33 |
 					CR14_EXTERNAL_DAMAGE_SUBMASK;
-	/* make sure the new fpc will be lazily loaded */
-	save_fpu_regs();
-	current->thread.fpu.fpc = 0;
+	vcpu->run->s.regs.fpc = 0;
 	vcpu->arch.sie_block->gbea = 1;
 	vcpu->arch.sie_block->pp = 0;
 	vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
@@ -4351,7 +4349,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	switch (ioctl) {
 	case KVM_S390_STORE_STATUS:
 		idx = srcu_read_lock(&vcpu->kvm->srcu);
-		r = kvm_s390_vcpu_store_status(vcpu, arg);
+		r = kvm_s390_store_status_unloaded(vcpu, arg);
 		srcu_read_unlock(&vcpu->kvm->srcu, idx);
 		break;
 	case KVM_S390_SET_INITIAL_PSW: {

commit ddd259c9aaba08244dba8877687ee856f79c4f45
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:28 2019 -0800

    KVM: Drop kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit()
    
    Remove kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit() now that all
    arch specific implementations are nops.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a1bb47c7ba1e..8646c99217f2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2700,11 +2700,6 @@ static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
 	return rc == 0 && id < KVM_S390_ESCA_CPU_SLOTS;
 }
 
-int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
-{
-	return 0;
-}
-
 /* needs disabled preemption to protect from TOD sync and vcpu_load/put */
 static void __start_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 {

commit afede96df55e9cba948c8cc8a682e962244285b4
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:22 2019 -0800

    KVM: Drop kvm_arch_vcpu_setup()
    
    Remove kvm_arch_vcpu_setup() now that all arch specific implementations
    are nops.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dca3d6aac2bb..a1bb47c7ba1e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2931,11 +2931,6 @@ static void kvm_s390_vcpu_setup_model(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->fac = (u32)(u64) model->fac_list;
 }
 
-int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
-{
-	return 0;
-}
-
 static int kvm_s390_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	int rc = 0;

commit ff72bb55cbfd060172cfbafafe4838ce92ab080f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:20 2019 -0800

    KVM: s390: Manually invoke vcpu setup during kvm_arch_vcpu_create()
    
    Rename kvm_arch_vcpu_setup() to kvm_s390_vcpu_setup() and manually call
    the new function during kvm_arch_vcpu_create().  Define an empty
    kvm_arch_vcpu_setup() as it's still required for compilation.  This
    is effectively a nop as kvm_arch_vcpu_create() and kvm_arch_vcpu_setup()
    are called back-to-back by common KVM code.  Obsoleting
    kvm_arch_vcpu_setup() paves the way for its removal.
    
    Note, gmap_remove() is now called if setup fails, as s390 was previously
    freeing it via kvm_arch_vcpu_destroy(), which is called by common KVM
    code if kvm_arch_vcpu_setup() fails.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9cba1e5d033b..dca3d6aac2bb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2932,6 +2932,11 @@ static void kvm_s390_vcpu_setup_model(struct kvm_vcpu *vcpu)
 }
 
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+static int kvm_s390_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	int rc = 0;
 
@@ -3070,8 +3075,14 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 		 vcpu->vcpu_id, vcpu, vcpu->arch.sie_block);
 	trace_kvm_s390_create_vcpu(vcpu->vcpu_id, vcpu, vcpu->arch.sie_block);
 
+	rc = kvm_s390_vcpu_setup(vcpu);
+	if (rc)
+		goto out_ucontrol_uninit;
 	return 0;
 
+out_ucontrol_uninit:
+	if (kvm_is_ucontrol(vcpu->kvm))
+		gmap_remove(vcpu->arch.gmap);
 out_free_sie_block:
 	free_page((unsigned long)(vcpu->arch.sie_block));
 	return rc;

commit e529ef66e6b53b34f9b8caac55950c8a55c79dac
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:15 2019 -0800

    KVM: Move vcpu alloc and init invocation to common code
    
    Now that all architectures tightly couple vcpu allocation/free with the
    mandatory calls to kvm_{un}init_vcpu(), move the sequences verbatim to
    common KVM code.
    
    Move both allocation and initialization in a single patch to eliminate
    thrash in arch specific code.  The bisection benefits of moving the two
    pieces in separate patches is marginal at best, whereas the odds of
    introducing a transient arch specific bug are non-zero.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8543d338a06a..9cba1e5d033b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2530,9 +2530,6 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	if (vcpu->kvm->arch.use_cmma)
 		kvm_s390_vcpu_unsetup_cmma(vcpu);
 	free_page((unsigned long)(vcpu->arch.sie_block));
-
-	kvm_vcpu_uninit(vcpu);
-	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
 static void kvm_free_vcpus(struct kvm *kvm)
@@ -3014,29 +3011,15 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return 0;
 }
 
-struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
-				      unsigned int id)
+int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
-	struct kvm_vcpu *vcpu;
 	struct sie_page *sie_page;
 	int rc;
 
-	rc = -ENOMEM;
-
-	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
-	if (!vcpu)
-		goto out;
-
-	rc = kvm_vcpu_init(vcpu, kvm, id);
-	if (rc)
-		goto out_free_cpu;
-
-	rc = -ENOMEM;
-
 	BUILD_BUG_ON(sizeof(struct sie_page) != 4096);
 	sie_page = (struct sie_page *) get_zeroed_page(GFP_KERNEL);
 	if (!sie_page)
-		goto out_uninit_vcpu;
+		return -ENOMEM;
 
 	vcpu->arch.sie_block = &sie_page->sie_block;
 	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
@@ -3045,9 +3028,9 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.sie_block->mso = 0;
 	vcpu->arch.sie_block->msl = sclp.hamax;
 
-	vcpu->arch.sie_block->icpua = id;
+	vcpu->arch.sie_block->icpua = vcpu->vcpu_id;
 	spin_lock_init(&vcpu->arch.local_int.lock);
-	vcpu->arch.sie_block->gd = (u32)(u64)kvm->arch.gisa_int.origin;
+	vcpu->arch.sie_block->gd = (u32)(u64)vcpu->kvm->arch.gisa_int.origin;
 	if (vcpu->arch.sie_block->gd && sclp.has_gisaf)
 		vcpu->arch.sie_block->gd |= GISA_FORMAT1;
 	seqcount_init(&vcpu->arch.cputm_seqcount);
@@ -3083,19 +3066,15 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 			goto out_free_sie_block;
 	}
 
-	VM_EVENT(kvm, 3, "create cpu %d at 0x%pK, sie block at 0x%pK", id, vcpu,
-		 vcpu->arch.sie_block);
-	trace_kvm_s390_create_vcpu(id, vcpu, vcpu->arch.sie_block);
+	VM_EVENT(vcpu->kvm, 3, "create cpu %d at 0x%pK, sie block at 0x%pK",
+		 vcpu->vcpu_id, vcpu, vcpu->arch.sie_block);
+	trace_kvm_s390_create_vcpu(vcpu->vcpu_id, vcpu, vcpu->arch.sie_block);
+
+	return 0;
 
-	return vcpu;
 out_free_sie_block:
 	free_page((unsigned long)(vcpu->arch.sie_block));
-out_uninit_vcpu:
-	kvm_vcpu_uninit(vcpu);
-out_free_cpu:
-	kmem_cache_free(kvm_vcpu_cache, vcpu);
-out:
-	return ERR_PTR(rc);
+	return rc;
 }
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)

commit 4543bdc08857e8026475a477e7ba88e461f38271
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:14 2019 -0800

    KVM: Introduce kvm_vcpu_destroy()
    
    Add kvm_vcpu_destroy() and wire up all architectures to call the common
    function instead of their arch specific implementation.  The common
    destruction function will be used by future patches to move allocation
    and initialization of vCPUs to common KVM code, i.e. to free resources
    that are allocated by arch agnostic code.
    
    No functional change intended.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1f8ba074cbd6..8543d338a06a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2541,7 +2541,7 @@ static void kvm_free_vcpus(struct kvm *kvm)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm)
-		kvm_arch_vcpu_destroy(vcpu);
+		kvm_vcpu_destroy(vcpu);
 
 	mutex_lock(&kvm->lock);
 	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)

commit a2017f17fa175b812ce7de302316f67e8f2b7db0
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:11 2019 -0800

    KVM: s390: Invoke kvm_vcpu_init() before allocating sie_page
    
    Now that s390's implementation of kvm_arch_vcpu_init() is empty, move
    the call to kvm_vcpu_init() above the allocation of the sie_page.  This
    paves the way for moving vcpu allocation and initialization into common
    KVM code without any associated functional change.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0049b621e56a..1f8ba074cbd6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3027,10 +3027,16 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	if (!vcpu)
 		goto out;
 
+	rc = kvm_vcpu_init(vcpu, kvm, id);
+	if (rc)
+		goto out_free_cpu;
+
+	rc = -ENOMEM;
+
 	BUILD_BUG_ON(sizeof(struct sie_page) != 4096);
 	sie_page = (struct sie_page *) get_zeroed_page(GFP_KERNEL);
 	if (!sie_page)
-		goto out_free_cpu;
+		goto out_uninit_vcpu;
 
 	vcpu->arch.sie_block = &sie_page->sie_block;
 	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
@@ -3046,10 +3052,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		vcpu->arch.sie_block->gd |= GISA_FORMAT1;
 	seqcount_init(&vcpu->arch.cputm_seqcount);
 
-	rc = kvm_vcpu_init(vcpu, kvm, id);
-	if (rc)
-		goto out_free_sie_block;
-
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
 	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
@@ -3078,7 +3080,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	if (kvm_is_ucontrol(vcpu->kvm)) {
 		rc = __kvm_ucontrol_vcpu_init(vcpu);
 		if (rc)
-			goto out_uninit_vcpu;
+			goto out_free_sie_block;
 	}
 
 	VM_EVENT(kvm, 3, "create cpu %d at 0x%pK, sie block at 0x%pK", id, vcpu,
@@ -3086,10 +3088,10 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	trace_kvm_s390_create_vcpu(id, vcpu, vcpu->arch.sie_block);
 
 	return vcpu;
-out_uninit_vcpu:
-	kvm_vcpu_uninit(vcpu);
 out_free_sie_block:
 	free_page((unsigned long)(vcpu->arch.sie_block));
+out_uninit_vcpu:
+	kvm_vcpu_uninit(vcpu);
 out_free_cpu:
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 out:

commit 321f8ee559d697d69efa81e8b6d4ea1e487c8bcc
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:10 2019 -0800

    KVM: s390: Move guts of kvm_arch_vcpu_init() into kvm_arch_vcpu_create()
    
    Move all of kvm_arch_vcpu_init(), which is invoked at the very end of
    kvm_vcpu_init(), into kvm_arch_vcpu_create() in preparation of moving
    the call to kvm_vcpu_init().  Moving kvm_vcpu_init() is itself a
    preparatory step for moving allocation and initialization to common KVM
    code.
    
    No functional change inteded.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 57c6838dff37..0049b621e56a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2705,34 +2705,6 @@ static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
 
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
-	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
-	kvm_clear_async_pf_completion_queue(vcpu);
-	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
-				    KVM_SYNC_GPRS |
-				    KVM_SYNC_ACRS |
-				    KVM_SYNC_CRS |
-				    KVM_SYNC_ARCH0 |
-				    KVM_SYNC_PFAULT;
-	kvm_s390_set_prefix(vcpu, 0);
-	if (test_kvm_facility(vcpu->kvm, 64))
-		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
-	if (test_kvm_facility(vcpu->kvm, 82))
-		vcpu->run->kvm_valid_regs |= KVM_SYNC_BPBC;
-	if (test_kvm_facility(vcpu->kvm, 133))
-		vcpu->run->kvm_valid_regs |= KVM_SYNC_GSCB;
-	if (test_kvm_facility(vcpu->kvm, 156))
-		vcpu->run->kvm_valid_regs |= KVM_SYNC_ETOKEN;
-	/* fprs can be synchronized via vrs, even if the guest has no vx. With
-	 * MACHINE_HAS_VX, (load|store)_fpu_regs() will work with vrs format.
-	 */
-	if (MACHINE_HAS_VX)
-		vcpu->run->kvm_valid_regs |= KVM_SYNC_VRS;
-	else
-		vcpu->run->kvm_valid_regs |= KVM_SYNC_FPRS;
-
-	if (kvm_is_ucontrol(vcpu->kvm))
-		return __kvm_ucontrol_vcpu_init(vcpu);
-
 	return 0;
 }
 
@@ -3077,11 +3049,45 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
 		goto out_free_sie_block;
+
+	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
+	kvm_clear_async_pf_completion_queue(vcpu);
+	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
+				    KVM_SYNC_GPRS |
+				    KVM_SYNC_ACRS |
+				    KVM_SYNC_CRS |
+				    KVM_SYNC_ARCH0 |
+				    KVM_SYNC_PFAULT;
+	kvm_s390_set_prefix(vcpu, 0);
+	if (test_kvm_facility(vcpu->kvm, 64))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
+	if (test_kvm_facility(vcpu->kvm, 82))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_BPBC;
+	if (test_kvm_facility(vcpu->kvm, 133))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_GSCB;
+	if (test_kvm_facility(vcpu->kvm, 156))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_ETOKEN;
+	/* fprs can be synchronized via vrs, even if the guest has no vx. With
+	 * MACHINE_HAS_VX, (load|store)_fpu_regs() will work with vrs format.
+	 */
+	if (MACHINE_HAS_VX)
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_VRS;
+	else
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_FPRS;
+
+	if (kvm_is_ucontrol(vcpu->kvm)) {
+		rc = __kvm_ucontrol_vcpu_init(vcpu);
+		if (rc)
+			goto out_uninit_vcpu;
+	}
+
 	VM_EVENT(kvm, 3, "create cpu %d at 0x%pK, sie block at 0x%pK", id, vcpu,
 		 vcpu->arch.sie_block);
 	trace_kvm_s390_create_vcpu(id, vcpu, vcpu->arch.sie_block);
 
 	return vcpu;
+out_uninit_vcpu:
+	kvm_vcpu_uninit(vcpu);
 out_free_sie_block:
 	free_page((unsigned long)(vcpu->arch.sie_block));
 out_free_cpu:

commit 897cc38eaab96d006ab17edd0f50a2f432f584cf
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:09 2019 -0800

    KVM: Add kvm_arch_vcpu_precreate() to handle pre-allocation issues
    
    Add a pre-allocation arch hook to handle checks that are currently done
    by arch specific code prior to allocating the vCPU object.  This paves
    the way for moving the allocation to common KVM code.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d9e6bf3d54f0..57c6838dff37 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3035,15 +3035,19 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	return rc;
 }
 
+int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
+{
+	if (!kvm_is_ucontrol(kvm) && !sca_can_add_vcpu(kvm, id))
+		return -EINVAL;
+	return 0;
+}
+
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 				      unsigned int id)
 {
 	struct kvm_vcpu *vcpu;
 	struct sie_page *sie_page;
-	int rc = -EINVAL;
-
-	if (!kvm_is_ucontrol(kvm) && !sca_can_add_vcpu(kvm, id))
-		goto out;
+	int rc;
 
 	rc = -ENOMEM;
 

commit fe289ebb65eee604f08fd1f8ab33ded3e955a7df
Merge: 7ee30bc132c6 c7b7de631246
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 18 13:16:46 2019 +0100

    Merge tag 'kvm-s390-next-5.5-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: small fixes and enhancements
    
    - selftest improvements
    - yield improvements
    - cleanups

commit 8474e5cac07e7f21dd8c0b3620640db30115db56
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Feb 15 13:47:20 2019 +0100

    KVM: s390: count invalid yields
    
    To analyze some performance issues with lock contention and scheduling
    it is nice to know when diag9c did not result in any action or when
    no action was tried.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 40af442b2e15..3b5ebf48f802 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -155,6 +155,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_diag_10", VCPU_STAT(diagnose_10) },
 	{ "instruction_diag_44", VCPU_STAT(diagnose_44) },
 	{ "instruction_diag_9c", VCPU_STAT(diagnose_9c) },
+	{ "diag_9c_ignored", VCPU_STAT(diagnose_9c_ignored) },
 	{ "instruction_diag_258", VCPU_STAT(diagnose_258) },
 	{ "instruction_diag_308", VCPU_STAT(diagnose_308) },
 	{ "instruction_diag_500", VCPU_STAT(diagnose_500) },

commit d0dea733f60efe94257d08ae6eba81d0b511d0a9
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Oct 2 14:34:37 2019 +0200

    KVM: s390: mark __insn32_query() as __always_inline
    
    __insn32_query() will not compile if the compiler decides to not
    inline it, since it contains an inline assembly with an "i" constraint
    with variable contents.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 74e1a22786e8..d047e846e1b9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -332,7 +332,7 @@ static inline int plo_test_bit(unsigned char nr)
 	return cc == 0;
 }
 
-static inline void __insn32_query(unsigned int opcode, u8 *query)
+static __always_inline void __insn32_query(unsigned int opcode, u8 *query)
 {
 	register unsigned long r0 asm("0") = 0;	/* query function */
 	register unsigned long r1 asm("1") = (unsigned long) query;

commit b1c41ac3ce569b04644bb1e3fd28926604637da3
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Oct 2 14:24:47 2019 +0200

    KVM: s390: fix __insn32_query() inline assembly
    
    The inline assembly constraints of __insn32_query() tell the compiler
    that only the first byte of "query" is being written to. Intended was
    probably that 32 bytes are written to.
    
    Fix and simplify the code and just use a "memory" clobber.
    
    Fixes: d668139718a9 ("KVM: s390: provide query function for instructions returning 32 byte")
    Cc: stable@vger.kernel.org # v5.2+
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f6db0f1bc867..74e1a22786e8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -332,7 +332,7 @@ static inline int plo_test_bit(unsigned char nr)
 	return cc == 0;
 }
 
-static inline void __insn32_query(unsigned int opcode, u8 query[32])
+static inline void __insn32_query(unsigned int opcode, u8 *query)
 {
 	register unsigned long r0 asm("0") = 0;	/* query function */
 	register unsigned long r1 asm("1") = (unsigned long) query;
@@ -340,9 +340,9 @@ static inline void __insn32_query(unsigned int opcode, u8 query[32])
 	asm volatile(
 		/* Parameter regs are ignored */
 		"	.insn	rrf,%[opc] << 16,2,4,6,0\n"
-		: "=m" (*query)
+		:
 		: "d" (r0), "a" (r1), [opc] "i" (opcode)
-		: "cc");
+		: "cc", "memory");
 }
 
 #define INSN_SORTL 0xb938

commit f76f6371643b563a7168a6ba5713ce93caa36ecc
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Wed Oct 2 03:56:27 2019 -0400

    KVM: s390: Cleanup kvm_arch_init error path
    
    Both kvm_s390_gib_destroy and debug_unregister test if the needed
    pointers are not NULL and hence can be called unconditionally.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Link: https://lore.kernel.org/kvm/20191002075627.3582-1-frankja@linux.ibm.com
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f6db0f1bc867..40af442b2e15 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -453,16 +453,14 @@ static void kvm_s390_cpu_feat_init(void)
 
 int kvm_arch_init(void *opaque)
 {
-	int rc;
+	int rc = -ENOMEM;
 
 	kvm_s390_dbf = debug_register("kvm-trace", 32, 1, 7 * sizeof(long));
 	if (!kvm_s390_dbf)
 		return -ENOMEM;
 
-	if (debug_register_view(kvm_s390_dbf, &debug_sprintf_view)) {
-		rc = -ENOMEM;
-		goto out_debug_unreg;
-	}
+	if (debug_register_view(kvm_s390_dbf, &debug_sprintf_view))
+		goto out;
 
 	kvm_s390_cpu_feat_init();
 
@@ -470,19 +468,17 @@ int kvm_arch_init(void *opaque)
 	rc = kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
 	if (rc) {
 		pr_err("A FLIC registration call failed with rc=%d\n", rc);
-		goto out_debug_unreg;
+		goto out;
 	}
 
 	rc = kvm_s390_gib_init(GAL_ISC);
 	if (rc)
-		goto out_gib_destroy;
+		goto out;
 
 	return 0;
 
-out_gib_destroy:
-	kvm_s390_gib_destroy();
-out_debug_unreg:
-	debug_unregister(kvm_s390_dbf);
+out:
+	kvm_arch_exit();
 	return rc;
 }
 

commit fe38bd6862074c0a2b9be7f31f043aaa70b2af5f
Merge: 404e634fdb96 fb3925d06c28
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 18 09:49:13 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "s390:
       - ioctl hardening
       - selftests
    
      ARM:
       - ITS translation cache
       - support for 512 vCPUs
       - various cleanups and bugfixes
    
      PPC:
       - various minor fixes and preparation
    
      x86:
       - bugfixes all over the place (posted interrupts, SVM, emulation
         corner cases, blocked INIT)
       - some IPI optimizations"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (75 commits)
      KVM: X86: Use IPI shorthands in kvm guest when support
      KVM: x86: Fix INIT signal handling in various CPU states
      KVM: VMX: Introduce exit reason for receiving INIT signal on guest-mode
      KVM: VMX: Stop the preemption timer during vCPU reset
      KVM: LAPIC: Micro optimize IPI latency
      kvm: Nested KVM MMUs need PAE root too
      KVM: x86: set ctxt->have_exception in x86_decode_insn()
      KVM: x86: always stop emulation on page fault
      KVM: nVMX: trace nested VM-Enter failures detected by H/W
      KVM: nVMX: add tracepoint for failed nested VM-Enter
      x86: KVM: svm: Fix a check in nested_svm_vmrun()
      KVM: x86: Return to userspace with internal error on unexpected exit reason
      KVM: x86: Add kvm_emulate_{rd,wr}msr() to consolidate VXM/SVM code
      KVM: x86: Refactor up kvm_{g,s}et_msr() to simplify callers
      doc: kvm: Fix return description of KVM_SET_MSRS
      KVM: X86: Tune PLE Window tracepoint
      KVM: VMX: Change ple_window type to unsigned int
      KVM: X86: Remove tailing newline for tracepoints
      KVM: X86: Trace vcpu_id for vmexit
      KVM: x86: Manually calculate reserved bits when loading PDPTRS
      ...

commit a9c20bb0206ae9384bd470a6832dd8913730add9
Merge: 002c5f73c508 53936b5bf35e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Sep 14 09:25:30 2019 +0200

    Merge tag 'kvm-s390-master-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kvm-master
    
    KVM: s390: Fixes for 5.3
    
    - prevent a user triggerable oops in the migration code
    - do not leak kernel stack content

commit 53936b5bf35e140ae27e4bbf0447a61063f400da
Author: Thomas Huth <thuth@redhat.com>
Date:   Thu Sep 12 13:54:38 2019 +0200

    KVM: s390: Do not leak kernel stack data in the KVM_S390_INTERRUPT ioctl
    
    When the userspace program runs the KVM_S390_INTERRUPT ioctl to inject
    an interrupt, we convert them from the legacy struct kvm_s390_interrupt
    to the new struct kvm_s390_irq via the s390int_to_s390irq() function.
    However, this function does not take care of all types of interrupts
    that we can inject into the guest later (see do_inject_vcpu()). Since we
    do not clear out the s390irq values before calling s390int_to_s390irq(),
    there is a chance that we copy random data from the kernel stack which
    could be leaked to the userspace later.
    
    Specifically, the problem exists with the KVM_S390_INT_PFAULT_INIT
    interrupt: s390int_to_s390irq() does not handle it, and the function
    __inject_pfault_init() later copies irq->u.ext which contains the
    random kernel stack data. This data can then be leaked either to
    the guest memory in __deliver_pfault_init(), or the userspace might
    retrieve it directly with the KVM_S390_GET_IRQ_STATE ioctl.
    
    Fix it by handling that interrupt type in s390int_to_s390irq(), too,
    and by making sure that the s390irq struct is properly pre-initialized.
    And while we're at it, make sure that s390int_to_s390irq() now
    directly returns -EINVAL for unknown interrupt types, so that we
    immediately get a proper error code in case we add more interrupt
    types to do_inject_vcpu() without updating s390int_to_s390irq()
    sometime in the future.
    
    Cc: stable@vger.kernel.org
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Link: https://lore.kernel.org/kvm/20190912115438.25761-1-thuth@redhat.com
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b298fcc9ec56..0d697b436f2b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4335,7 +4335,7 @@ long kvm_arch_vcpu_async_ioctl(struct file *filp,
 	}
 	case KVM_S390_INTERRUPT: {
 		struct kvm_s390_interrupt s390int;
-		struct kvm_s390_irq s390irq;
+		struct kvm_s390_irq s390irq = {};
 
 		if (copy_from_user(&s390int, argp, sizeof(s390int)))
 			return -EFAULT;

commit 13a17cc0526f08d1df9507f7484176371cd263a0
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Wed Sep 11 03:52:18 2019 -0400

    KVM: s390: kvm_s390_vm_start_migration: check dirty_bitmap before using it as target for memset()
    
    If userspace doesn't set KVM_MEM_LOG_DIRTY_PAGES on memslot before calling
    kvm_s390_vm_start_migration(), kernel will oops with:
    
      Unable to handle kernel pointer dereference in virtual kernel address space
      Failing address: 0000000000000000 TEID: 0000000000000483
      Fault in home space mode while using kernel ASCE.
      AS:0000000002a2000b R2:00000001bff8c00b R3:00000001bff88007 S:00000001bff91000 P:000000000000003d
      Oops: 0004 ilc:2 [#1] SMP
      ...
      Call Trace:
      ([<001fffff804ec552>] kvm_s390_vm_set_attr+0x347a/0x3828 [kvm])
       [<001fffff804ecfc0>] kvm_arch_vm_ioctl+0x6c0/0x1998 [kvm]
       [<001fffff804b67e4>] kvm_vm_ioctl+0x51c/0x11a8 [kvm]
       [<00000000008ba572>] do_vfs_ioctl+0x1d2/0xe58
       [<00000000008bb284>] ksys_ioctl+0x8c/0xb8
       [<00000000008bb2e2>] sys_ioctl+0x32/0x40
       [<000000000175552c>] system_call+0x2b8/0x2d8
      INFO: lockdep is turned off.
      Last Breaking-Event-Address:
       [<0000000000dbaf60>] __memset+0xc/0xa0
    
    due to ms->dirty_bitmap being NULL, which might crash the host.
    
    Make sure that ms->dirty_bitmap is set before using it or
    return -EINVAL otherwise.
    
    Cc: <stable@vger.kernel.org>
    Fixes: afdad61615cc ("KVM: s390: Fix storage attributes migration with memory slots")
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Link: https://lore.kernel.org/kvm/20190911075218.29153-1-imammedo@redhat.com/
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Claudio Imbrenda <imbrenda@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3f520cd837fb..b298fcc9ec56 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1018,6 +1018,8 @@ static int kvm_s390_vm_start_migration(struct kvm *kvm)
 	/* mark all the pages in active slots as dirty */
 	for (slotnr = 0; slotnr < slots->used_slots; slotnr++) {
 		ms = slots->memslots + slotnr;
+		if (!ms->dirty_bitmap)
+			return -EINVAL;
 		/*
 		 * The second half of the bitmap is only used on x86,
 		 * and would be wasted otherwise, so we put it to good

commit 17a81bdb4ee441bcbf09ec76f530197c3788d610
Merge: 95c065400a12 81cb736c0c92
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Sep 11 18:06:15 2019 +0200

    Merge tag 'kvm-s390-next-5.4-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    * More selftests
    * Improved KVM_S390_MEM_OP ioctl input checking
    * Add kvm_valid_regs and kvm_dirty_regs invalid bit checking

commit 200824f55eef409830a2d1703d6f81542620c2f0
Author: Thomas Huth <thuth@redhat.com>
Date:   Wed Sep 4 10:51:59 2019 +0200

    KVM: s390: Disallow invalid bits in kvm_valid_regs and kvm_dirty_regs
    
    If unknown bits are set in kvm_valid_regs or kvm_dirty_regs, this
    clearly indicates that something went wrong in the KVM userspace
    application. The x86 variant of KVM already contains a check for
    bad bits, so let's do the same on s390x now, too.
    
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Link: https://lore.kernel.org/lkml/20190904085200.29021-2-thuth@redhat.com/
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d41c091546b7..a3d4527ac23b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4008,6 +4008,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (kvm_run->immediate_exit)
 		return -EINTR;
 
+	if (kvm_run->kvm_valid_regs & ~KVM_SYNC_S390_VALID_FIELDS ||
+	    kvm_run->kvm_dirty_regs & ~KVM_SYNC_S390_VALID_FIELDS)
+		return -EINVAL;
+
 	vcpu_load(vcpu);
 
 	if (guestdbg_exit_pending(vcpu)) {

commit a13b03bbb4575b350b46090af4dfd30e735aaed1
Author: Thomas Huth <thuth@redhat.com>
Date:   Thu Aug 29 14:25:17 2019 +0200

    KVM: s390: Test for bad access register and size at the start of S390_MEM_OP
    
    If the KVM_S390_MEM_OP ioctl is called with an access register >= 16,
    then there is certainly a bug in the calling userspace application.
    We check for wrong access registers, but only if the vCPU was already
    in the access register mode before (i.e. the SIE block has recorded
    it). The check is also buried somewhere deep in the calling chain (in
    the function ar_translation()), so this is somewhat hard to find.
    
    It's better to always report an error to the userspace in case this
    field is set wrong, and it's safer in the KVM code if we block wrong
    values here early instead of relying on a check somewhere deep down
    the calling chain, so let's add another check to kvm_s390_guest_mem_op()
    directly.
    
    We also should check that the "size" is non-zero here (thanks to Janosch
    Frank for the hint!). If we do not check the size, we could call vmalloc()
    with this 0 value, and this will cause a kernel warning.
    
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Link: https://lkml.kernel.org/r/20190829122517.31042-1-thuth@redhat.com
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3f520cd837fb..d41c091546b7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4265,7 +4265,7 @@ static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 	const u64 supported_flags = KVM_S390_MEMOP_F_INJECT_EXCEPTION
 				    | KVM_S390_MEMOP_F_CHECK_ONLY;
 
-	if (mop->flags & ~supported_flags)
+	if (mop->flags & ~supported_flags || mop->ar >= NUM_ACRS || !mop->size)
 		return -EINVAL;
 
 	if (mop->size > MEM_OP_MAX_SIZE)

commit 741cbbae0768b828be2d48331eb371a4f08bbea8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Aug 3 08:14:25 2019 +0200

    KVM: remove kvm_arch_has_vcpu_debugfs()
    
    There is no need for this function as all arches have to implement
    kvm_arch_create_vcpu_debugfs() no matter what.  A #define symbol
    let us actually simplify the code.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3f520cd837fb..f329dcb3f44c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2516,16 +2516,6 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	return rc;
 }
 
-bool kvm_arch_has_vcpu_debugfs(void)
-{
-	return false;
-}
-
-int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
-{
-	return 0;
-}
-
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");

commit 39d7530d7494b4e47ba1856e741f513dafd17e3d
Merge: 16c97650a56a a45ff5994c9c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 12 15:35:14 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for chained PMU counters in guests
       - improved SError handling
       - handle Neoverse N1 erratum #1349291
       - allow side-channel mitigation status to be migrated
       - standardise most AArch64 system register accesses to msr_s/mrs_s
       - fix host MPIDR corruption on 32bit
       - selftests ckleanups
    
      x86:
       - PMU event {white,black}listing
       - ability for the guest to disable host-side interrupt polling
       - fixes for enlightened VMCS (Hyper-V pv nested virtualization),
       - new hypercall to yield to IPI target
       - support for passing cstate MSRs through to the guest
       - lots of cleanups and optimizations
    
      Generic:
       - Some txt->rST conversions for the documentation"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (128 commits)
      Documentation: virtual: Add toctree hooks
      Documentation: kvm: Convert cpuid.txt to .rst
      Documentation: virtual: Convert paravirt_ops.txt to .rst
      KVM: x86: Unconditionally enable irqs in guest context
      KVM: x86: PMU Event Filter
      kvm: x86: Fix -Wmissing-prototypes warnings
      KVM: Properly check if "page" is valid in kvm_vcpu_unmap
      KVM: arm/arm64: Initialise host's MPIDRs by reading the actual register
      KVM: LAPIC: Retry tune per-vCPU timer_advance_ns if adaptive tuning goes insane
      kvm: LAPIC: write down valid APIC registers
      KVM: arm64: Migrate _elx sysreg accessors to msr_s/mrs_s
      KVM: doc: Add API documentation on the KVM_REG_ARM_WORKAROUNDS register
      KVM: arm/arm64: Add save/restore support for firmware workaround state
      arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
      KVM: arm/arm64: Support chained PMU counters
      KVM: arm/arm64: Remove pmc->bitmask
      KVM: arm/arm64: Re-create event when setting counter value
      KVM: arm/arm64: Extract duplicated code to own function
      KVM: arm/arm64: Rename kvm_pmu_{enable/disable}_counter functions
      KVM: LAPIC: ARBPRI is a reserved register for x2APIC
      ...

commit 05f31e3bf6b34fe6e4922868d132f6455f81d5bf
Author: Pierre Morel <pmorel@linux.ibm.com>
Date:   Tue May 21 17:34:37 2019 +0200

    s390: ap: kvm: Enable PQAP/AQIC facility for the guest
    
    AP Queue Interruption Control (AQIC) facility gives
    the guest the possibility to control interruption for
    the Cryptographic Adjunct Processor queues.
    
    Signed-off-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: Tony Krowiak <akrowiak@linux.ibm.com>
    Acked-by: Harald Freudenberger <freude@linux.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Halil Pasic <pasic@linux.ibm.com>
    [ Modified while picking: we may not expose STFLE facility 65
    unconditionally because AIV is a pre-requirement.]
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 28ebd647784c..1c4113f0f2a8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2461,6 +2461,9 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		set_kvm_facility(kvm->arch.model.fac_list, 147);
 	}
 
+	if (css_general_characteristics.aiv && test_facility(65))
+		set_kvm_facility(kvm->arch.model.fac_mask, 65);
+
 	kvm->arch.model.cpuid = kvm_s390_get_initial_cpuid();
 	kvm->arch.model.ibc = sclp.ibc & 0x0fff;
 

commit 0d9ce162cf46c99628cc5da9510b959c7976735b
Author: Junaid Shahid <junaids@google.com>
Date:   Thu Jan 3 17:14:28 2019 -0800

    kvm: Convert kvm_lock to a mutex
    
    It doesn't seem as if there is any particular need for kvm_lock to be a
    spinlock, so convert the lock to a mutex so that sleepable functions (in
    particular cond_resched()) can be called while holding it.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7936af0a971f..0fef9192f6ac 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2423,13 +2423,13 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(alloc_flags);
 	if (!kvm->arch.sca)
 		goto out_err;
-	spin_lock(&kvm_lock);
+	mutex_lock(&kvm_lock);
 	sca_offset += 16;
 	if (sca_offset + sizeof(struct bsca_block) > PAGE_SIZE)
 		sca_offset = 0;
 	kvm->arch.sca = (struct bsca_block *)
 			((char *) kvm->arch.sca + sca_offset);
-	spin_unlock(&kvm_lock);
+	mutex_unlock(&kvm_lock);
 
 	sprintf(debug_name, "kvm-%u", current->pid);
 

commit f257d6dcda0187693407e0c2e5dab69bdab3223f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Apr 19 22:18:17 2019 -0700

    KVM: Directly return result from kvm_arch_check_processor_compat()
    
    Add a wrapper to invoke kvm_arch_check_processor_compat() so that the
    boilerplate ugliness of checking virtualization support on all CPUs is
    hidden from the arch specific code.  x86's implementation in particular
    is quite heinous, as it unnecessarily propagates the out-param pattern
    into kvm_x86_ops.
    
    While the x86 specific issue could be resolved solely by changing
    kvm_x86_ops, make the change for all architectures as returning a value
    directly is prettier and technically more robust, e.g. s390 doesn't set
    the out param, which could lead to subtle breakage in the (highly
    unlikely) scenario where the out-param was not pre-initialized by the
    caller.
    
    Opportunistically annotate svm_check_processor_compat() with __init.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 28ebd647784c..7936af0a971f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -227,6 +227,11 @@ int kvm_arch_hardware_enable(void)
 	return 0;
 }
 
+int kvm_arch_check_processor_compat(void)
+{
+	return 0;
+}
+
 static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
 			      unsigned long end);
 

commit a86cb413f4bf273a9d341a3ab2c2ca44e12eb317
Author: Thomas Huth <thuth@redhat.com>
Date:   Thu May 23 18:43:08 2019 +0200

    KVM: s390: Do not report unusabled IDs via KVM_CAP_MAX_VCPU_ID
    
    KVM_CAP_MAX_VCPU_ID is currently always reporting KVM_MAX_VCPU_ID on all
    architectures. However, on s390x, the amount of usable CPUs is determined
    during runtime - it is depending on the features of the machine the code
    is running on. Since we are using the vcpu_id as an index into the SCA
    structures that are defined by the hardware (see e.g. the sca_add_vcpu()
    function), it is not only the amount of CPUs that is limited by the hard-
    ware, but also the range of IDs that we can use.
    Thus KVM_CAP_MAX_VCPU_ID must be determined during runtime on s390x, too.
    So the handling of KVM_CAP_MAX_VCPU_ID has to be moved from the common
    code into the architecture specific code, and on s390x we have to return
    the same value here as for KVM_CAP_MAX_VCPUS.
    This problem has been discovered with the kvm_create_max_vcpus selftest.
    With this change applied, the selftest now passes on s390x, too.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Message-Id: <20190523164309.13345-9-thuth@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e5e8eb29e68e..28ebd647784c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -539,6 +539,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	case KVM_CAP_NR_VCPUS:
 	case KVM_CAP_MAX_VCPUS:
+	case KVM_CAP_MAX_VCPU_ID:
 		r = KVM_S390_BSCA_CPU_SLOTS;
 		if (!kvm_s390_use_sca_entries())
 			r = KVM_MAX_VCPUS;

commit 19ec166c3f39fe1d3789888a74cc95544ac266d4
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 24 16:06:23 2019 +0200

    KVM: s390: fix memory slot handling for KVM_SET_USER_MEMORY_REGION
    
    kselftests exposed a problem in the s390 handling for memory slots.
    Right now we only do proper memory slot handling for creation of new
    memory slots. Neither MOVE, nor DELETION are handled properly. Let us
    implement those.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ac6163c334d6..e5e8eb29e68e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4524,21 +4524,28 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_memory_slot *new,
 				enum kvm_mr_change change)
 {
-	int rc;
-
-	/* If the basics of the memslot do not change, we do not want
-	 * to update the gmap. Every update causes several unnecessary
-	 * segment translation exceptions. This is usually handled just
-	 * fine by the normal fault handler + gmap, but it will also
-	 * cause faults on the prefix page of running guest CPUs.
-	 */
-	if (old->userspace_addr == mem->userspace_addr &&
-	    old->base_gfn * PAGE_SIZE == mem->guest_phys_addr &&
-	    old->npages * PAGE_SIZE == mem->memory_size)
-		return;
+	int rc = 0;
 
-	rc = gmap_map_segment(kvm->arch.gmap, mem->userspace_addr,
-		mem->guest_phys_addr, mem->memory_size);
+	switch (change) {
+	case KVM_MR_DELETE:
+		rc = gmap_unmap_segment(kvm->arch.gmap, old->base_gfn * PAGE_SIZE,
+					old->npages * PAGE_SIZE);
+		break;
+	case KVM_MR_MOVE:
+		rc = gmap_unmap_segment(kvm->arch.gmap, old->base_gfn * PAGE_SIZE,
+					old->npages * PAGE_SIZE);
+		if (rc)
+			break;
+		/* FALLTHROUGH */
+	case KVM_MR_CREATE:
+		rc = gmap_map_segment(kvm->arch.gmap, mem->userspace_addr,
+				      mem->guest_phys_addr, mem->memory_size);
+		break;
+	case KVM_MR_FLAGS_ONLY:
+		break;
+	default:
+		WARN(1, "Unknown KVM MR CHANGE: %d\n", change);
+	}
 	if (rc)
 		pr_warn("failed to commit memory region\n");
 	return;

commit b41fb528dd8784a87414e3af5579674badefa76b
Author: Wei Yongjun <weiyongjun1@huawei.com>
Date:   Sat May 4 06:51:45 2019 +0000

    KVM: s390: fix typo in parameter description
    
    Fix typo in parameter description.
    
    Fixes: 8b905d28ee17 ("KVM: s390: provide kvm_arch_no_poll function")
    Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
    Message-Id: <20190504065145.53665-1-weiyongjun1@huawei.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8d6d75db8de6..ac6163c334d6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -181,7 +181,7 @@ MODULE_PARM_DESC(hpage, "1m huge page backing support");
 /* maximum percentage of steal time for polling.  >100 is treated like 100 */
 static u8 halt_poll_max_steal = 10;
 module_param(halt_poll_max_steal, byte, 0644);
-MODULE_PARM_DESC(hpage, "Maximum percentage of steal time to allow polling");
+MODULE_PARM_DESC(halt_poll_max_steal, "Maximum percentage of steal time to allow polling");
 
 /*
  * For now we handle at most 16 double words as this is what the s390 base

commit da8f0d97b2a02ebc98eb380d9e59c7fb653d4ad8
Merge: c110ae578ca0 b2d0371d2e37
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Apr 30 21:29:14 2019 +0200

    Merge tag 'kvm-s390-next-5.2-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Features and fixes for 5.2
    
    - VSIE crypto fixes
    - new guest features for gen15
    - disable halt polling for nested virtualization with overcommit

commit 8b905d28ee17795c939813b2f6947829774619b9
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 5 05:30:02 2019 -0500

    KVM: s390: provide kvm_arch_no_poll function
    
    We do track the current steal time of the host CPUs. Let us use
    this value to disable halt polling if the steal time goes beyond
    a configured value.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index eb68ada1334b..ed280a4fea07 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -75,6 +75,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "halt_successful_poll", VCPU_STAT(halt_successful_poll) },
 	{ "halt_attempted_poll", VCPU_STAT(halt_attempted_poll) },
 	{ "halt_poll_invalid", VCPU_STAT(halt_poll_invalid) },
+	{ "halt_no_poll_steal", VCPU_STAT(halt_no_poll_steal) },
 	{ "halt_wakeup", VCPU_STAT(halt_wakeup) },
 	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
@@ -177,6 +178,11 @@ static int hpage;
 module_param(hpage, int, 0444);
 MODULE_PARM_DESC(hpage, "1m huge page backing support");
 
+/* maximum percentage of steal time for polling.  >100 is treated like 100 */
+static u8 halt_poll_max_steal = 10;
+module_param(halt_poll_max_steal, byte, 0644);
+MODULE_PARM_DESC(hpage, "Maximum percentage of steal time to allow polling");
+
 /*
  * For now we handle at most 16 double words as this is what the s390 base
  * kernel handles and stores in the prefix page. If we ever need to go beyond
@@ -3166,6 +3172,17 @@ static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
 	}
 }
 
+bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
+{
+	/* do not poll with more than halt_poll_max_steal percent of steal time */
+	if (S390_lowcore.avg_steal_timer * 100 / (TICK_USEC << 12) >=
+	    halt_poll_max_steal) {
+		vcpu->stat.halt_no_poll_steal++;
+		return true;
+	}
+	return false;
+}
+
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
 	/* kvm common code refers to this, but never calls it */

commit 8ec2fa52eac53bff7ef1cedbc4ad8af650ec937c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Apr 3 03:00:35 2019 -0400

    KVM: s390: enable MSA9 keywrapping functions depending on cpu model
    
    Instead of adding a new machine option to disable/enable the keywrapping
    options of pckmo (like for AES and DEA) we can now use the CPU model to
    decide. As ECC is also wrapped with the AES key we need that to be
    enabled.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 38ca8324a91a..eb68ada1334b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2890,6 +2890,25 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 	vcpu->arch.enabled_gmap = vcpu->arch.gmap;
 }
 
+static bool kvm_has_pckmo_subfunc(struct kvm *kvm, unsigned long nr)
+{
+	if (test_bit_inv(nr, (unsigned long *)&kvm->arch.model.subfuncs.pckmo) &&
+	    test_bit_inv(nr, (unsigned long *)&kvm_s390_available_subfunc.pckmo))
+		return true;
+	return false;
+}
+
+static bool kvm_has_pckmo_ecc(struct kvm *kvm)
+{
+	/* At least one ECC subfunction must be present */
+	return kvm_has_pckmo_subfunc(kvm, 32) ||
+	       kvm_has_pckmo_subfunc(kvm, 33) ||
+	       kvm_has_pckmo_subfunc(kvm, 34) ||
+	       kvm_has_pckmo_subfunc(kvm, 40) ||
+	       kvm_has_pckmo_subfunc(kvm, 41);
+
+}
+
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -2902,13 +2921,19 @@ static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->crycbd = vcpu->kvm->arch.crypto.crycbd;
 	vcpu->arch.sie_block->ecb3 &= ~(ECB3_AES | ECB3_DEA);
 	vcpu->arch.sie_block->eca &= ~ECA_APIE;
+	vcpu->arch.sie_block->ecd &= ~ECD_ECC;
 
 	if (vcpu->kvm->arch.crypto.apie)
 		vcpu->arch.sie_block->eca |= ECA_APIE;
 
 	/* Set up protected key support */
-	if (vcpu->kvm->arch.crypto.aes_kw)
+	if (vcpu->kvm->arch.crypto.aes_kw) {
 		vcpu->arch.sie_block->ecb3 |= ECB3_AES;
+		/* ecc is also wrapped with AES key */
+		if (kvm_has_pckmo_ecc(vcpu->kvm))
+			vcpu->arch.sie_block->ecd |= ECD_ECC;
+	}
+
 	if (vcpu->kvm->arch.crypto.dea_kw)
 		vcpu->arch.sie_block->ecb3 |= ECB3_DEA;
 }

commit 4f45b90e1c03466202fca7f62eaf32243f220830
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Dec 28 10:46:04 2018 +0100

    KVM: s390: add deflate conversion facilty to cpu model
    
    This enables stfle.151 and adds the subfunctions for DFLTCC. Bit 151 is
    added to the list of facilities that will be enabled when there is no
    cpu model involved as DFLTCC requires no additional handling from
    userspace, e.g. for migration.
    
    Please note that a cpu model enabled user space can and will have the
    final decision on the facility bits for a guests.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 757f76bba9ea..38ca8324a91a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -335,6 +335,7 @@ static inline void __insn32_query(unsigned int opcode, u8 query[32])
 }
 
 #define INSN_SORTL 0xb938
+#define INSN_DFLTCC 0xb939
 
 static void kvm_s390_cpu_feat_init(void)
 {
@@ -390,6 +391,9 @@ static void kvm_s390_cpu_feat_init(void)
 	if (test_facility(150)) /* SORTL */
 		__insn32_query(INSN_SORTL, kvm_s390_available_subfunc.sortl);
 
+	if (test_facility(151)) /* DFLTCC */
+		__insn32_query(INSN_DFLTCC, kvm_s390_available_subfunc.dfltcc);
+
 	if (MACHINE_HAS_ESOP)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
 	/*
@@ -1361,6 +1365,11 @@ static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
 		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[1],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[2],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[3]);
+	VM_EVENT(kvm, 3, "SET: guest DFLTCC subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[1],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[2],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[3]);
 
 	return 0;
 }
@@ -1537,6 +1546,11 @@ static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
 		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[1],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[2],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[3]);
+	VM_EVENT(kvm, 3, "GET: guest DFLTCC subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[1],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[2],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.dfltcc)[3]);
 
 	return 0;
 }
@@ -1600,6 +1614,11 @@ static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
 		 ((unsigned long *) &kvm_s390_available_subfunc.sortl)[1],
 		 ((unsigned long *) &kvm_s390_available_subfunc.sortl)[2],
 		 ((unsigned long *) &kvm_s390_available_subfunc.sortl)[3]);
+	VM_EVENT(kvm, 3, "GET: host  DFLTCC subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.dfltcc)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.dfltcc)[1],
+		 ((unsigned long *) &kvm_s390_available_subfunc.dfltcc)[2],
+		 ((unsigned long *) &kvm_s390_available_subfunc.dfltcc)[3]);
 
 	return 0;
 }

commit 173aec2d5a9fa5f40e462661a8283fcafe04764f
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Dec 28 10:59:06 2018 +0100

    KVM: s390: add enhanced sort facilty to cpu model
    
    This enables stfle.150 and adds the subfunctions for SORTL. Bit 150 is
    added to the list of facilities that will be enabled when there is no
    cpu model involved as sortl requires no additional handling from
    userspace, e.g. for migration.
    
    Please note that a cpu model enabled user space can and will have the
    final decision on the facility bits for a guests.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 336e591d94eb..757f76bba9ea 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -334,6 +334,8 @@ static inline void __insn32_query(unsigned int opcode, u8 query[32])
 		: "cc");
 }
 
+#define INSN_SORTL 0xb938
+
 static void kvm_s390_cpu_feat_init(void)
 {
 	int i;
@@ -385,6 +387,9 @@ static void kvm_s390_cpu_feat_init(void)
 		__cpacf_query(CPACF_KDSA, (cpacf_mask_t *)
 			      kvm_s390_available_subfunc.kdsa);
 
+	if (test_facility(150)) /* SORTL */
+		__insn32_query(INSN_SORTL, kvm_s390_available_subfunc.sortl);
+
 	if (MACHINE_HAS_ESOP)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
 	/*
@@ -1351,6 +1356,11 @@ static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
 	VM_EVENT(kvm, 3, "SET: guest KDSA   subfunc 0x%16.16lx.%16.16lx",
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[0],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[1]);
+	VM_EVENT(kvm, 3, "SET: guest SORTL  subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[1],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[2],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[3]);
 
 	return 0;
 }
@@ -1522,6 +1532,11 @@ static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
 	VM_EVENT(kvm, 3, "GET: guest KDSA   subfunc 0x%16.16lx.%16.16lx",
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[0],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[1]);
+	VM_EVENT(kvm, 3, "GET: guest SORTL  subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[1],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[2],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.sortl)[3]);
 
 	return 0;
 }
@@ -1580,6 +1595,11 @@ static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
 	VM_EVENT(kvm, 3, "GET: host  KDSA   subfunc 0x%16.16lx.%16.16lx",
 		 ((unsigned long *) &kvm_s390_available_subfunc.kdsa)[0],
 		 ((unsigned long *) &kvm_s390_available_subfunc.kdsa)[1]);
+	VM_EVENT(kvm, 3, "GET: host  SORTL  subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.sortl)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.sortl)[1],
+		 ((unsigned long *) &kvm_s390_available_subfunc.sortl)[2],
+		 ((unsigned long *) &kvm_s390_available_subfunc.sortl)[3]);
 
 	return 0;
 }

commit d668139718a9e2260702777bd8d86d71c30b6539
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Feb 20 03:04:07 2019 -0500

    KVM: s390: provide query function for instructions returning 32 byte
    
    Some of the new features have a 32byte response for the query function.
    Provide a new wrapper similar to __cpacf_query. We might want to factor
    this out if other users come up, as of today there is none. So let us
    keep the function within KVM.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0dad61ccde3d..336e591d94eb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -321,6 +321,19 @@ static inline int plo_test_bit(unsigned char nr)
 	return cc == 0;
 }
 
+static inline void __insn32_query(unsigned int opcode, u8 query[32])
+{
+	register unsigned long r0 asm("0") = 0;	/* query function */
+	register unsigned long r1 asm("1") = (unsigned long) query;
+
+	asm volatile(
+		/* Parameter regs are ignored */
+		"	.insn	rrf,%[opc] << 16,2,4,6,0\n"
+		: "=m" (*query)
+		: "d" (r0), "a" (r1), [opc] "i" (opcode)
+		: "cc");
+}
+
 static void kvm_s390_cpu_feat_init(void)
 {
 	int i;

commit 13209ad0395c4de7fa48108b1dac72e341d5c089
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Dec 28 09:33:35 2018 +0100

    KVM: s390: add MSA9 to cpumodel
    
    This enables stfle.155 and adds the subfunctions for KDSA. Bit 155 is
    added to the list of facilities that will be enabled when there is no
    cpu model involved as MSA9 requires no additional handling from
    userspace, e.g. for migration.
    
    Please note that a cpu model enabled user space can and will have the
    final decision on the facility bits for a guests.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d3f3e63bb164..0dad61ccde3d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -368,6 +368,10 @@ static void kvm_s390_cpu_feat_init(void)
 		__cpacf_query(CPACF_KMA, (cpacf_mask_t *)
 			      kvm_s390_available_subfunc.kma);
 
+	if (test_facility(155)) /* MSA9 */
+		__cpacf_query(CPACF_KDSA, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kdsa);
+
 	if (MACHINE_HAS_ESOP)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
 	/*
@@ -1331,6 +1335,9 @@ static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
 	VM_EVENT(kvm, 3, "SET: guest KMA    subfunc 0x%16.16lx.%16.16lx",
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[0],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KDSA   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[1]);
 
 	return 0;
 }
@@ -1499,6 +1506,9 @@ static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
 	VM_EVENT(kvm, 3, "GET: guest KMA    subfunc 0x%16.16lx.%16.16lx",
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[0],
 		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KDSA   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kdsa)[1]);
 
 	return 0;
 }
@@ -1554,6 +1564,9 @@ static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
 	VM_EVENT(kvm, 3, "GET: host  KMA    subfunc 0x%16.16lx.%16.16lx",
 		 ((unsigned long *) &kvm_s390_available_subfunc.kma)[0],
 		 ((unsigned long *) &kvm_s390_available_subfunc.kma)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KDSA   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kdsa)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kdsa)[1]);
 
 	return 0;
 }

commit d5cb6ab1e3d4d7e0648a167f6290e89f6e86964e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Dec 28 09:45:58 2018 +0100

    KVM: s390: add vector BCD enhancements facility to cpumodel
    
    If vector support is enabled, the vector BCD enhancements facility
    might also be enabled.
    We can directly forward this facility to the guest if available
    and VX is requested by user space.
    
    Please note that user space can and will have the final decision
    on the facility bits for a guests.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 03ddbe5e62bc..d3f3e63bb164 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -661,6 +661,10 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 				set_kvm_facility(kvm->arch.model.fac_mask, 148);
 				set_kvm_facility(kvm->arch.model.fac_list, 148);
 			}
+			if (test_facility(152)) {
+				set_kvm_facility(kvm->arch.model.fac_mask, 152);
+				set_kvm_facility(kvm->arch.model.fac_list, 152);
+			}
 			r = 0;
 		} else
 			r = -EINVAL;

commit 7832e91cd33f21f3cf82b003478c292915a1ec14
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Dec 28 09:43:37 2018 +0100

    KVM: s390: add vector enhancements facility 2 to cpumodel
    
    If vector support is enabled, the vector enhancements facility 2
    might also be enabled.
    We can directly forward this facility to the guest if available
    and VX is requested by user space.
    
    Please note that user space can and will have the final decision
    on the facility bits for a guests.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4638303ba6a8..03ddbe5e62bc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -657,6 +657,10 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 				set_kvm_facility(kvm->arch.model.fac_mask, 135);
 				set_kvm_facility(kvm->arch.model.fac_list, 135);
 			}
+			if (test_facility(148)) {
+				set_kvm_facility(kvm->arch.model.fac_mask, 148);
+				set_kvm_facility(kvm->arch.model.fac_list, 148);
+			}
 			r = 0;
 		} else
 			r = -EINVAL;

commit c110ae578ca0a10064dfbda3d786d6a733b9fe69
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 28 17:24:03 2019 +0100

    kvm: move KVM_CAP_NR_MEMSLOTS to common code
    
    All architectures except MIPS were defining it in the same way,
    and memory slots are handled entirely by common code so there
    is no point in keeping the definition per-architecture.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4638303ba6a8..28f35d2b06cb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -513,9 +513,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		else if (sclp.has_esca && sclp.has_64bscao)
 			r = KVM_S390_ESCA_CPU_SLOTS;
 		break;
-	case KVM_CAP_NR_MEMSLOTS:
-		r = KVM_USER_MEM_SLOTS;
-		break;
 	case KVM_CAP_S390_COW:
 		r = MACHINE_HAS_ESOP;
 		break;

commit 11ba5961a2156a4f210627ed8421387e2531b100
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Feb 20 11:38:42 2019 -0500

    KVM: s390: add debug logging for cpu model subfunctions
    
    As userspace can now get/set the subfunctions we want to trace those.
    This will allow to also check QEMUs cpu model vs. what the real
    hardware provides.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Janosch Frank <frankja@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 82a95afa6629..4638303ba6a8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1279,6 +1279,51 @@ static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
 	}
 	mutex_unlock(&kvm->lock);
 
+	VM_EVENT(kvm, 3, "SET: guest PLO    subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[1],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[2],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[3]);
+	VM_EVENT(kvm, 3, "SET: guest PTFF   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ptff)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ptff)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KMAC   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmac)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmac)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KMC    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmc)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmc)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KM     subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.km)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.km)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KIMD   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kimd)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kimd)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KLMD   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.klmd)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.klmd)[1]);
+	VM_EVENT(kvm, 3, "SET: guest PCKMO  subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pckmo)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pckmo)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KMCTR  subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmctr)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmctr)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KMF    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmf)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmf)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KMO    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmo)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmo)[1]);
+	VM_EVENT(kvm, 3, "SET: guest PCC    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pcc)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pcc)[1]);
+	VM_EVENT(kvm, 3, "SET: guest PPNO   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ppno)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ppno)[1]);
+	VM_EVENT(kvm, 3, "SET: guest KMA    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[1]);
+
 	return 0;
 }
 
@@ -1402,6 +1447,51 @@ static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
 	    sizeof(struct kvm_s390_vm_cpu_subfunc)))
 		return -EFAULT;
 
+	VM_EVENT(kvm, 3, "GET: guest PLO    subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[1],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[2],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.plo)[3]);
+	VM_EVENT(kvm, 3, "GET: guest PTFF   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ptff)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ptff)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KMAC   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmac)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmac)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KMC    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmc)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmc)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KM     subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.km)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.km)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KIMD   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kimd)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kimd)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KLMD   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.klmd)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.klmd)[1]);
+	VM_EVENT(kvm, 3, "GET: guest PCKMO  subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pckmo)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pckmo)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KMCTR  subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmctr)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmctr)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KMF    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmf)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmf)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KMO    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmo)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kmo)[1]);
+	VM_EVENT(kvm, 3, "GET: guest PCC    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pcc)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.pcc)[1]);
+	VM_EVENT(kvm, 3, "GET: guest PPNO   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ppno)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.ppno)[1]);
+	VM_EVENT(kvm, 3, "GET: guest KMA    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[0],
+		 ((unsigned long *) &kvm->arch.model.subfuncs.kma)[1]);
+
 	return 0;
 }
 
@@ -1411,6 +1501,52 @@ static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
 	if (copy_to_user((void __user *)attr->addr, &kvm_s390_available_subfunc,
 	    sizeof(struct kvm_s390_vm_cpu_subfunc)))
 		return -EFAULT;
+
+	VM_EVENT(kvm, 3, "GET: host  PLO    subfunc 0x%16.16lx.%16.16lx.%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.plo)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.plo)[1],
+		 ((unsigned long *) &kvm_s390_available_subfunc.plo)[2],
+		 ((unsigned long *) &kvm_s390_available_subfunc.plo)[3]);
+	VM_EVENT(kvm, 3, "GET: host  PTFF   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.ptff)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.ptff)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KMAC   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmac)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmac)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KMC    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmc)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmc)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KM     subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.km)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.km)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KIMD   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kimd)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kimd)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KLMD   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.klmd)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.klmd)[1]);
+	VM_EVENT(kvm, 3, "GET: host  PCKMO  subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.pckmo)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.pckmo)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KMCTR  subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmctr)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmctr)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KMF    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmf)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmf)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KMO    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmo)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kmo)[1]);
+	VM_EVENT(kvm, 3, "GET: host  PCC    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.pcc)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.pcc)[1]);
+	VM_EVENT(kvm, 3, "GET: host  PPNO   subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.ppno)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.ppno)[1]);
+	VM_EVENT(kvm, 3, "GET: host  KMA    subfunc 0x%16.16lx.%16.16lx",
+		 ((unsigned long *) &kvm_s390_available_subfunc.kma)[0],
+		 ((unsigned long *) &kvm_s390_available_subfunc.kma)[1]);
+
 	return 0;
 }
 

commit 346fa2f891c71a9b98014f8f62c15f4c7dd95ec1
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Feb 18 07:48:25 2019 -0500

    KVM: s390: implement subfunction processor calls
    
    While we will not implement interception for query functions yet, we can
    and should disable functions that have a control bit based on the given
    CPU model.
    
    Let us start with enabling the subfunction interface.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2e47c724679e..82a95afa6629 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1266,11 +1266,20 @@ static int kvm_s390_set_processor_feat(struct kvm *kvm,
 static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
 					  struct kvm_device_attr *attr)
 {
-	/*
-	 * Once supported by kernel + hw, we have to store the subfunctions
-	 * in kvm->arch and remember that user space configured them.
-	 */
-	return -ENXIO;
+	mutex_lock(&kvm->lock);
+	if (kvm->created_vcpus) {
+		mutex_unlock(&kvm->lock);
+		return -EBUSY;
+	}
+
+	if (copy_from_user(&kvm->arch.model.subfuncs, (void __user *)attr->addr,
+			   sizeof(struct kvm_s390_vm_cpu_subfunc))) {
+		mutex_unlock(&kvm->lock);
+		return -EFAULT;
+	}
+	mutex_unlock(&kvm->lock);
+
+	return 0;
 }
 
 static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
@@ -1389,12 +1398,11 @@ static int kvm_s390_get_machine_feat(struct kvm *kvm,
 static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
 					  struct kvm_device_attr *attr)
 {
-	/*
-	 * Once we can actually configure subfunctions (kernel + hw support),
-	 * we have to check if they were already set by user space, if so copy
-	 * them from kvm->arch.
-	 */
-	return -ENXIO;
+	if (copy_to_user((void __user *)attr->addr, &kvm->arch.model.subfuncs,
+	    sizeof(struct kvm_s390_vm_cpu_subfunc)))
+		return -EFAULT;
+
+	return 0;
 }
 
 static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
@@ -1405,6 +1413,7 @@ static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
 		return -EFAULT;
 	return 0;
 }
+
 static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret = -ENXIO;
@@ -1522,10 +1531,9 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 		case KVM_S390_VM_CPU_PROCESSOR_FEAT:
 		case KVM_S390_VM_CPU_MACHINE_FEAT:
 		case KVM_S390_VM_CPU_MACHINE_SUBFUNC:
+		case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
 			ret = 0;
 			break;
-		/* configuring subfunctions is not supported yet */
-		case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
 		default:
 			ret = -ENXIO;
 			break;
@@ -2227,6 +2235,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		kvm->arch.model.fac_list[i] = S390_lowcore.stfle_fac_list[i] &
 					      kvm_s390_fac_base[i];
 	}
+	kvm->arch.model.subfuncs = kvm_s390_available_subfunc;
 
 	/* we are always in czam mode - even on pre z14 machines */
 	set_kvm_facility(kvm->arch.model.fac_mask, 138);

commit b1d1e76ed9ee5a0f7671da257d4f0595d1f5162e
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Thu Jan 31 09:52:45 2019 +0100

    KVM: s390: start using the GIB
    
    By initializing the GIB, it will be used by the kvm host.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: Halil Pasic <pasic@linux.ibm.com>
    Message-Id: <20190131085247.13826-15-mimu@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0099fbda2e98..2e47c724679e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -435,8 +435,15 @@ int kvm_arch_init(void *opaque)
 		pr_err("A FLIC registration call failed with rc=%d\n", rc);
 		goto out_debug_unreg;
 	}
+
+	rc = kvm_s390_gib_init(GAL_ISC);
+	if (rc)
+		goto out_gib_destroy;
+
 	return 0;
 
+out_gib_destroy:
+	kvm_s390_gib_destroy();
 out_debug_unreg:
 	debug_unregister(kvm_s390_dbf);
 	return rc;

commit 9f30f62163786a0b80e0886046b5c66e714e7e71
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Thu Jan 31 09:52:44 2019 +0100

    KVM: s390: add gib_alert_irq_handler()
    
    The patch implements a handler for GIB alert interruptions
    on the host. Its task is to alert guests that interrupts are
    pending for them.
    
    A GIB alert interrupt statistic counter is added as well:
    
    $ cat /proc/interrupts
              CPU0       CPU1
      ...
      GAL:      23         37   [I/O] GIB Alert
      ...
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Acked-by: Halil Pasic <pasic@linux.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Message-Id: <20190131085247.13826-14-mimu@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0de062e989e2..0099fbda2e98 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3460,6 +3460,8 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 		kvm_s390_patch_guest_per_regs(vcpu);
 	}
 
+	clear_bit(vcpu->vcpu_id, vcpu->kvm->arch.gisa_int.kicked_mask);
+
 	vcpu->arch.sie_block->icptcode = 0;
 	cpuflags = atomic_read(&vcpu->arch.sie_block->cpuflags);
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x", cpuflags);

commit 25c84dbaec6a5079ab64cf8b633ec811f8e43fdd
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Thu Jan 31 09:52:41 2019 +0100

    KVM: s390: add kvm reference to struct sie_page2
    
    Adding the kvm reference to struct sie_page2 will allow to
    determine the kvm a given gisa belongs to:
    
      container_of(gisa, struct sie_page2, gisa)->kvm
    
    This functionality will be required to process a gisa in
    gib alert interruption context.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Halil Pasic <pasic@linux.ibm.com>
    Message-Id: <20190131085247.13826-11-mimu@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ede89172b8f4..0de062e989e2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2210,6 +2210,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.sie_page2)
 		goto out_err;
 
+	kvm->arch.sie_page2->kvm = kvm;
 	kvm->arch.model.fac_list = kvm->arch.sie_page2->fac_list;
 
 	for (i = 0; i < kvm_s390_fac_size(); i++) {

commit 1282c21eb3dac324b8531bf2e0e0fbd9d5b6516b
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Thu Jan 31 09:52:40 2019 +0100

    KVM: s390: add the GIB and its related life-cyle functions
    
    The Guest Information Block (GIB) links the GISA of all guests
    that have adapter interrupts pending. These interrupts cannot be
    delivered because all vcpus of these guests are currently in WAIT
    state or have masked the respective Interruption Sub Class (ISC).
    If enabled, a GIB alert is issued on the host to schedule these
    guests to run suitable vcpus to consume the pending interruptions.
    
    This mechanism allows to process adapter interrupts for currently
    not running guests.
    
    The GIB is created during host initialization and associated with
    the Adapter Interruption Facility in case an Adapter Interruption
    Virtualization Facility is available.
    
    The GIB initialization and thus the activation of the related code
    will be done in an upcoming patch of this series.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Acked-by: Halil Pasic <pasic@linux.ibm.com>
    Message-Id: <20190131085247.13826-10-mimu@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5eaffb3e1738..ede89172b8f4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -444,6 +444,7 @@ int kvm_arch_init(void *opaque)
 
 void kvm_arch_exit(void)
 {
+	kvm_s390_gib_destroy();
 	debug_unregister(kvm_s390_dbf);
 }
 

commit 982cff425959901dfb4df7433622e5c0510f9d37
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Thu Jan 31 09:52:38 2019 +0100

    KVM: s390: introduce struct kvm_s390_gisa_interrupt
    
    Use this struct analog to the kvm interruption structs
    for kvm emulated floating and local interruptions.
    
    GIB handling will add further fields to this structure as
    required.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Halil Pasic <pasic@linux.ibm.com>
    Message-Id: <20190131085247.13826-8-mimu@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 11b4be3dad15..5eaffb3e1738 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2812,7 +2812,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	vcpu->arch.sie_block->icpua = id;
 	spin_lock_init(&vcpu->arch.local_int.lock);
-	vcpu->arch.sie_block->gd = (u32)(u64)kvm->arch.gisa;
+	vcpu->arch.sie_block->gd = (u32)(u64)kvm->arch.gisa_int.origin;
 	if (vcpu->arch.sie_block->gd && sclp.has_gisaf)
 		vcpu->arch.sie_block->gd |= GISA_FORMAT1;
 	seqcount_init(&vcpu->arch.cputm_seqcount);

commit 8d43d57036679a8635952c9ef54989a7b48e8c00
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Mon Dec 10 11:15:16 2018 +0100

    KVM: s390: clarify kvm related kernel message
    
    As suggested by our ID dept. here are some kernel message
    updates.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7f4bc58a53b9..11b4be3dad15 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -432,7 +432,7 @@ int kvm_arch_init(void *opaque)
 	/* Register floating interrupt controller interface. */
 	rc = kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
 	if (rc) {
-		pr_err("Failed to register FLIC rc=%d\n", rc);
+		pr_err("A FLIC registration call failed with rc=%d\n", rc);
 		goto out_debug_unreg;
 	}
 	return 0;
@@ -4293,12 +4293,12 @@ static int __init kvm_s390_init(void)
 	int i;
 
 	if (!sclp.has_sief2) {
-		pr_info("SIE not available\n");
+		pr_info("SIE is not available\n");
 		return -ENODEV;
 	}
 
 	if (nested && hpage) {
-		pr_info("nested (vSIE) and hpage (huge page backing) can currently not be activated concurrently");
+		pr_info("A KVM host that supports nesting cannot back its KVM guests with huge pages\n");
 		return -EINVAL;
 	}
 

commit e9f2e05a5f19ce2a21d79c765fd6bb7c03d4aaeb
Merge: 8c5e14f438b8 7aedd9d48f2a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Dec 19 22:17:09 2018 +0100

    Merge tag 'kvm-s390-next-4.21-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Fixes for 4.21
    
    Just two small fixes.

commit 7aedd9d48f2a8332c2c51a40d2dca8c097f0dcdb
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Mon Dec 3 10:20:22 2018 +0100

    KVM: s390: fix kmsg component kvm-s390
    
    Relocate #define statement for kvm related kernel messages
    before the include of printk to become effective.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d8d8e0788157..530b4175f56d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -11,6 +11,9 @@
  *               Jason J. Herne <jjherne@us.ibm.com>
  */
 
+#define KMSG_COMPONENT "kvm-s390"
+#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
+
 #include <linux/compiler.h>
 #include <linux/err.h>
 #include <linux/fs.h>
@@ -44,10 +47,6 @@
 #include "kvm-s390.h"
 #include "gaccess.h"
 
-#define KMSG_COMPONENT "kvm-s390"
-#undef pr_fmt
-#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
-
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 #include "trace-s390.h"

commit 308c3e6673b012beecb96ef04cc65f4a0e7cdd99
Author: Michael Mueller <mimu@linux.ibm.com>
Date:   Fri Nov 30 15:32:06 2018 +0100

    KVM: s390: unregister debug feature on failing arch init
    
    Make sure the debug feature and its allocated resources get
    released upon unsuccessful architecture initialization.
    
    A related indication of the issue will be reported as kernel
    message.
    
    Signed-off-by: Michael Mueller <mimu@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20181130143215.69496-2-mimu@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fe24150ff666..d8d8e0788157 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -417,19 +417,30 @@ static void kvm_s390_cpu_feat_init(void)
 
 int kvm_arch_init(void *opaque)
 {
+	int rc;
+
 	kvm_s390_dbf = debug_register("kvm-trace", 32, 1, 7 * sizeof(long));
 	if (!kvm_s390_dbf)
 		return -ENOMEM;
 
 	if (debug_register_view(kvm_s390_dbf, &debug_sprintf_view)) {
-		debug_unregister(kvm_s390_dbf);
-		return -ENOMEM;
+		rc = -ENOMEM;
+		goto out_debug_unreg;
 	}
 
 	kvm_s390_cpu_feat_init();
 
 	/* Register floating interrupt controller interface. */
-	return kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
+	rc = kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
+	if (rc) {
+		pr_err("Failed to register FLIC rc=%d\n", rc);
+		goto out_debug_unreg;
+	}
+	return 0;
+
+out_debug_unreg:
+	debug_unregister(kvm_s390_dbf);
+	return rc;
 }
 
 void kvm_arch_exit(void)

commit e5d83c74a5800c2a1fa3ba982c1c4b2b39ae6db2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Feb 16 10:40:56 2017 +0100

    kvm: make KVM_CAP_ENABLE_CAP_VM architecture agnostic
    
    The first such capability to be handled in virt/kvm/ will be manual
    dirty page reprotection.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fe24150ff666..16c300bdf2c8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -464,7 +464,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_CSS_SUPPORT:
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
-	case KVM_CAP_ENABLE_CAP_VM:
 	case KVM_CAP_S390_IRQCHIP:
 	case KVM_CAP_VM_ATTRIBUTES:
 	case KVM_CAP_MP_STATE:
@@ -607,7 +606,7 @@ static void icpt_operexc_on_all_vcpus(struct kvm *kvm)
 	}
 }
 
-static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
+int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 {
 	int r;
 
@@ -1933,14 +1932,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_s390_inject_vm(kvm, &s390int);
 		break;
 	}
-	case KVM_ENABLE_CAP: {
-		struct kvm_enable_cap cap;
-		r = -EFAULT;
-		if (copy_from_user(&cap, argp, sizeof(cap)))
-			break;
-		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
-		break;
-	}
 	case KVM_CREATE_IRQCHIP: {
 		struct kvm_irq_routing_entry routing;
 

commit 3d0d0d9b1d805e39456a9d49443d847092cb21ab
Merge: 7dd2157cb61a ed3054a30258
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Oct 13 12:00:26 2018 +0200

    Merge tag 'kvm-s390-next-4.20-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390/vfio-ap: Fixes and enhancements for vfio-ap
    
    - add tracing
    - fix a locking bug
    - make local functions and data static

commit ed3054a3025879c7d3f64de7a58b7f6427e0d3a0
Merge: 55d09dd4c860 46623ab3194a
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Oct 8 12:14:54 2018 +0200

    Merge branch 'apv11' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kernelorgnext

commit 0e237e44699465139c07f969b051f83066a2ec1d
Author: Pierre Morel <pmorel@linux.ibm.com>
Date:   Fri Oct 5 10:31:09 2018 +0200

    KVM: s390: Tracing APCB changes
    
    kvm_arch_crypto_set_masks is a new function to centralize
    the setup the APCB masks inside the CRYCB SIE satellite.
    
    To trace APCB mask changes, we add KVM_EVENT() tracing to
    both kvm_arch_crypto_set_masks and kvm_arch_crypto_clear_masks.
    
    Signed-off-by: Pierre Morel <pmorel@linux.ibm.com>
    Message-Id: <1538728270-10340-2-git-send-email-pmorel@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 734d87d88eb3..22a320a9a00d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2064,6 +2064,46 @@ static void kvm_s390_set_crycb_format(struct kvm *kvm)
 		kvm->arch.crypto.crycbd |= CRYCB_FORMAT1;
 }
 
+void kvm_arch_crypto_set_masks(struct kvm *kvm, unsigned long *apm,
+			       unsigned long *aqm, unsigned long *adm)
+{
+	struct kvm_s390_crypto_cb *crycb = kvm->arch.crypto.crycb;
+
+	mutex_lock(&kvm->lock);
+	kvm_s390_vcpu_block_all(kvm);
+
+	switch (kvm->arch.crypto.crycbd & CRYCB_FORMAT_MASK) {
+	case CRYCB_FORMAT2: /* APCB1 use 256 bits */
+		memcpy(crycb->apcb1.apm, apm, 32);
+		VM_EVENT(kvm, 3, "SET CRYCB: apm %016lx %016lx %016lx %016lx",
+			 apm[0], apm[1], apm[2], apm[3]);
+		memcpy(crycb->apcb1.aqm, aqm, 32);
+		VM_EVENT(kvm, 3, "SET CRYCB: aqm %016lx %016lx %016lx %016lx",
+			 aqm[0], aqm[1], aqm[2], aqm[3]);
+		memcpy(crycb->apcb1.adm, adm, 32);
+		VM_EVENT(kvm, 3, "SET CRYCB: adm %016lx %016lx %016lx %016lx",
+			 adm[0], adm[1], adm[2], adm[3]);
+		break;
+	case CRYCB_FORMAT1:
+	case CRYCB_FORMAT0: /* Fall through both use APCB0 */
+		memcpy(crycb->apcb0.apm, apm, 8);
+		memcpy(crycb->apcb0.aqm, aqm, 2);
+		memcpy(crycb->apcb0.adm, adm, 2);
+		VM_EVENT(kvm, 3, "SET CRYCB: apm %016lx aqm %04x adm %04x",
+			 apm[0], *((unsigned short *)aqm),
+			 *((unsigned short *)adm));
+		break;
+	default:	/* Can not happen */
+		break;
+	}
+
+	/* recreate the shadow crycb for each vcpu */
+	kvm_s390_sync_request_broadcast(kvm, KVM_REQ_VSIE_RESTART);
+	kvm_s390_vcpu_unblock_all(kvm);
+	mutex_unlock(&kvm->lock);
+}
+EXPORT_SYMBOL_GPL(kvm_arch_crypto_set_masks);
+
 void kvm_arch_crypto_clear_masks(struct kvm *kvm)
 {
 	mutex_lock(&kvm->lock);
@@ -2074,6 +2114,7 @@ void kvm_arch_crypto_clear_masks(struct kvm *kvm)
 	memset(&kvm->arch.crypto.crycb->apcb1, 0,
 	       sizeof(kvm->arch.crypto.crycb->apcb1));
 
+	VM_EVENT(kvm, 3, "%s", "CLR CRYCB:");
 	/* recreate the shadow crycb for each vcpu */
 	kvm_s390_sync_request_broadcast(kvm, KVM_REQ_VSIE_RESTART);
 	kvm_s390_vcpu_unblock_all(kvm);

commit 8e41bd54317b04f2bf03012a4ca8ab7360c9beef
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Oct 4 14:42:43 2018 +0200

    KVM: s390: fix locking for crypto setting error path
    
    We need to unlock the kvm->lock mutex in the error case.
    
    Reported-by: smatch
    Fixes: 37940fb0b6a2c4bf101 ("KVM: s390: device attrs to enable/disable AP interpretation")
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a6230b00c1df..734d87d88eb3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -859,8 +859,10 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 	mutex_lock(&kvm->lock);
 	switch (attr->attr) {
 	case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
-		if (!test_kvm_facility(kvm, 76))
+		if (!test_kvm_facility(kvm, 76)) {
+			mutex_unlock(&kvm->lock);
 			return -EINVAL;
+		}
 		get_random_bytes(
 			kvm->arch.crypto.crycb->aes_wrapping_key_mask,
 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
@@ -868,8 +870,10 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 		VM_EVENT(kvm, 3, "%s", "ENABLE: AES keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
-		if (!test_kvm_facility(kvm, 76))
+		if (!test_kvm_facility(kvm, 76)) {
+			mutex_unlock(&kvm->lock);
 			return -EINVAL;
+		}
 		get_random_bytes(
 			kvm->arch.crypto.crycb->dea_wrapping_key_mask,
 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
@@ -877,16 +881,20 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 		VM_EVENT(kvm, 3, "%s", "ENABLE: DEA keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
-		if (!test_kvm_facility(kvm, 76))
+		if (!test_kvm_facility(kvm, 76)) {
+			mutex_unlock(&kvm->lock);
 			return -EINVAL;
+		}
 		kvm->arch.crypto.aes_kw = 0;
 		memset(kvm->arch.crypto.crycb->aes_wrapping_key_mask, 0,
 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 		VM_EVENT(kvm, 3, "%s", "DISABLE: AES keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
-		if (!test_kvm_facility(kvm, 76))
+		if (!test_kvm_facility(kvm, 76)) {
+			mutex_unlock(&kvm->lock);
 			return -EINVAL;
+		}
 		kvm->arch.crypto.dea_kw = 0;
 		memset(kvm->arch.crypto.crycb->dea_wrapping_key_mask, 0,
 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));

commit dd5bd0a65ff6f22a32b35ca3fa1bcf7a6bc7104f
Merge: 7e7126846c95 55d09dd4c860
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Oct 4 17:12:45 2018 +0200

    Merge tag 'kvm-s390-next-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Features for 4.20
    - Initial version of AP crypto virtualization via vfio-mdev
    - Set the host program identifier
    - Optimize page table locking

commit 55d09dd4c86060fbbc74ab2b1bfaed401cd0163a
Merge: af4bf6c3d9b4 492a6be197c0
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Oct 1 08:53:23 2018 +0200

    Merge branch 'apv11' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kernelorgnext

commit 67d49d52ae502eaea8858fbcb97e3c2891f78da9
Author: Collin Walling <walling@linux.ibm.com>
Date:   Fri Aug 31 12:51:19 2018 -0400

    KVM: s390: set host program identifier
    
    A host program identifier (HPID) provides information regarding the
    underlying host environment. A level-2 (VM) guest will have an HPID
    denoting Linux/KVM, which is set during VCPU setup. A level-3 (VM on a
    VM) and beyond guest will have an HPID denoting KVM vSIE, which is set
    for all shadow control blocks, overriding the original value of the
    HPID.
    
    Signed-off-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Message-Id: <1535734279-10204-4-git-send-email-walling@linux.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f69333fd2fa3..a1e8205d4b6b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2685,6 +2685,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 
+	vcpu->arch.sie_block->hpid = HPID_KVM;
+
 	kvm_s390_vcpu_crypto_setup(vcpu);
 
 	return rc;

commit 37940fb0b6a2c4bf101481365c42f250694ffd15
Author: Tony Krowiak <akrowiak@linux.ibm.com>
Date:   Tue Sep 25 19:16:39 2018 -0400

    KVM: s390: device attrs to enable/disable AP interpretation
    
    Introduces two new VM crypto device attributes (KVM_S390_VM_CRYPTO)
    to enable or disable AP instruction interpretation from userspace
    via the KVM_SET_DEVICE_ATTR ioctl:
    
    * The KVM_S390_VM_CRYPTO_ENABLE_APIE attribute enables hardware
      interpretation of AP instructions executed on the guest.
    
    * The KVM_S390_VM_CRYPTO_DISABLE_APIE attribute disables hardware
      interpretation of AP instructions executed on the guest. In this
      case the instructions will be intercepted and pass through to
      the guest.
    
    Signed-off-by: Tony Krowiak <akrowiak@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Message-Id: <20180925231641.4954-25-akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c94ef2d0dbe4..a6230b00c1df 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -856,12 +856,11 @@ void kvm_s390_vcpu_crypto_reset_all(struct kvm *kvm)
 
 static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 {
-	if (!test_kvm_facility(kvm, 76))
-		return -EINVAL;
-
 	mutex_lock(&kvm->lock);
 	switch (attr->attr) {
 	case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
+		if (!test_kvm_facility(kvm, 76))
+			return -EINVAL;
 		get_random_bytes(
 			kvm->arch.crypto.crycb->aes_wrapping_key_mask,
 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
@@ -869,6 +868,8 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 		VM_EVENT(kvm, 3, "%s", "ENABLE: AES keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
+		if (!test_kvm_facility(kvm, 76))
+			return -EINVAL;
 		get_random_bytes(
 			kvm->arch.crypto.crycb->dea_wrapping_key_mask,
 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
@@ -876,17 +877,35 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 		VM_EVENT(kvm, 3, "%s", "ENABLE: DEA keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
+		if (!test_kvm_facility(kvm, 76))
+			return -EINVAL;
 		kvm->arch.crypto.aes_kw = 0;
 		memset(kvm->arch.crypto.crycb->aes_wrapping_key_mask, 0,
 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 		VM_EVENT(kvm, 3, "%s", "DISABLE: AES keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
+		if (!test_kvm_facility(kvm, 76))
+			return -EINVAL;
 		kvm->arch.crypto.dea_kw = 0;
 		memset(kvm->arch.crypto.crycb->dea_wrapping_key_mask, 0,
 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 		VM_EVENT(kvm, 3, "%s", "DISABLE: DEA keywrapping support");
 		break;
+	case KVM_S390_VM_CRYPTO_ENABLE_APIE:
+		if (!ap_instructions_available()) {
+			mutex_unlock(&kvm->lock);
+			return -EOPNOTSUPP;
+		}
+		kvm->arch.crypto.apie = 1;
+		break;
+	case KVM_S390_VM_CRYPTO_DISABLE_APIE:
+		if (!ap_instructions_available()) {
+			mutex_unlock(&kvm->lock);
+			return -EOPNOTSUPP;
+		}
+		kvm->arch.crypto.apie = 0;
+		break;
 	default:
 		mutex_unlock(&kvm->lock);
 		return -ENXIO;
@@ -1495,6 +1514,10 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 		case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
 			ret = 0;
 			break;
+		case KVM_S390_VM_CRYPTO_ENABLE_APIE:
+		case KVM_S390_VM_CRYPTO_DISABLE_APIE:
+			ret = ap_instructions_available() ? 0 : -ENXIO;
+			break;
 		default:
 			ret = -ENXIO;
 			break;
@@ -2601,6 +2624,7 @@ static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.sie_block->crycbd = vcpu->kvm->arch.crypto.crycbd;
 	vcpu->arch.sie_block->ecb3 &= ~(ECB3_AES | ECB3_DEA);
+	vcpu->arch.sie_block->eca &= ~ECA_APIE;
 
 	if (vcpu->kvm->arch.crypto.apie)
 		vcpu->arch.sie_block->eca |= ECA_APIE;

commit 6cc571b1b1e8b6fbcf69411d115cf9d9be866276
Author: Pierre Morel <pmorel@linux.ibm.com>
Date:   Tue Sep 25 19:16:30 2018 -0400

    KVM: s390: Clear Crypto Control Block when using vSIE
    
    When we clear the Crypto Control Block (CRYCB) used by a guest
    level 2, the vSIE shadow CRYCB for guest level 3 must be updated
    before the guest uses it.
    
    We achieve this by using the KVM_REQ_VSIE_RESTART synchronous
    request for each vCPU belonging to the guest to force the reload
    of the shadow CRYCB before rerunning the guest level 3.
    
    Signed-off-by: Pierre Morel <pmorel@linux.ibm.com>
    Signed-off-by: Tony Krowiak <akrowiak@linux.ibm.com>
    Message-Id: <20180925231641.4954-16-akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 75aa5aa4a926..c94ef2d0dbe4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2043,6 +2043,8 @@ void kvm_arch_crypto_clear_masks(struct kvm *kvm)
 	memset(&kvm->arch.crypto.crycb->apcb1, 0,
 	       sizeof(kvm->arch.crypto.crycb->apcb1));
 
+	/* recreate the shadow crycb for each vcpu */
+	kvm_s390_sync_request_broadcast(kvm, KVM_REQ_VSIE_RESTART);
 	kvm_s390_vcpu_unblock_all(kvm);
 	mutex_unlock(&kvm->lock);
 }

commit 42104598ef2e8c3ce532ebec891c9edec161e508
Author: Tony Krowiak <akrowiak@linux.ibm.com>
Date:   Tue Sep 25 19:16:25 2018 -0400

    KVM: s390: interface to clear CRYCB masks
    
    Introduces a new KVM function to clear the APCB0 and APCB1 in the guest's
    CRYCB. This effectively clears all bits of the APM, AQM and ADM masks
    configured for the guest. The VCPUs are taken out of SIE to ensure the
    VCPUs do not get out of sync.
    
    Signed-off-by: Tony Krowiak <akrowiak@linux.ibm.com>
    Acked-by: Halil Pasic <pasic@linux.ibm.com>
    Tested-by: Michael Mueller <mimu@linux.ibm.com>
    Tested-by: Farhan Ali <alifm@linux.ibm.com>
    Tested-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Message-Id: <20180925231641.4954-11-akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d62f6d840f8c..75aa5aa4a926 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2033,6 +2033,21 @@ static void kvm_s390_set_crycb_format(struct kvm *kvm)
 		kvm->arch.crypto.crycbd |= CRYCB_FORMAT1;
 }
 
+void kvm_arch_crypto_clear_masks(struct kvm *kvm)
+{
+	mutex_lock(&kvm->lock);
+	kvm_s390_vcpu_block_all(kvm);
+
+	memset(&kvm->arch.crypto.crycb->apcb0, 0,
+	       sizeof(kvm->arch.crypto.crycb->apcb0));
+	memset(&kvm->arch.crypto.crycb->apcb1, 0,
+	       sizeof(kvm->arch.crypto.crycb->apcb1));
+
+	kvm_s390_vcpu_unblock_all(kvm);
+	mutex_unlock(&kvm->lock);
+}
+EXPORT_SYMBOL_GPL(kvm_arch_crypto_clear_masks);
+
 static u64 kvm_s390_get_initial_cpuid(void)
 {
 	struct cpuid cpuid;

commit e585b24aeb445493b6a581d6c42fa58f663e93e0
Author: Tony Krowiak <akrowiak@linux.ibm.com>
Date:   Tue Sep 25 19:16:18 2018 -0400

    KVM: s390: refactor crypto initialization
    
    This patch refactors the code that initializes and sets up the
    crypto configuration for a guest. The following changes are
    implemented via this patch:
    
    1. Introduces a flag indicating AP instructions executed on
       the guest shall be interpreted by the firmware. This flag
       is used to set a bit in the guest's state description
       indicating AP instructions are to be interpreted.
    
    2. Replace code implementing AP interfaces with code supplied
       by the AP bus to query the AP configuration.
    
    Signed-off-by: Tony Krowiak <akrowiak@linux.ibm.com>
    Reviewed-by: Halil Pasic <pasic@linux.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Tested-by: Michael Mueller <mimu@linux.ibm.com>
    Tested-by: Farhan Ali <alifm@linux.ibm.com>
    Message-Id: <20180925231641.4954-4-akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 876fbb2b0c04..d62f6d840f8c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -40,6 +40,7 @@
 #include <asm/sclp.h>
 #include <asm/cpacf.h>
 #include <asm/timex.h>
+#include <asm/ap.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
 
@@ -1995,49 +1996,37 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
-static int kvm_s390_query_ap_config(u8 *config)
-{
-	u32 fcn_code = 0x04000000UL;
-	u32 cc = 0;
-
-	memset(config, 0, 128);
-	asm volatile(
-		"lgr 0,%1\n"
-		"lgr 2,%2\n"
-		".long 0xb2af0000\n"		/* PQAP(QCI) */
-		"0: ipm %0\n"
-		"srl %0,28\n"
-		"1:\n"
-		EX_TABLE(0b, 1b)
-		: "+r" (cc)
-		: "r" (fcn_code), "r" (config)
-		: "cc", "0", "2", "memory"
-	);
-
-	return cc;
-}
-
 static int kvm_s390_apxa_installed(void)
 {
-	u8 config[128];
-	int cc;
+	struct ap_config_info info;
 
-	if (test_facility(12)) {
-		cc = kvm_s390_query_ap_config(config);
-
-		if (cc)
-			pr_err("PQAP(QCI) failed with cc=%d", cc);
-		else
-			return config[0] & 0x40;
+	if (ap_instructions_available()) {
+		if (ap_qci(&info) == 0)
+			return info.apxa;
 	}
 
 	return 0;
 }
 
+/*
+ * The format of the crypto control block (CRYCB) is specified in the 3 low
+ * order bits of the CRYCB designation (CRYCBD) field as follows:
+ * Format 0: Neither the message security assist extension 3 (MSAX3) nor the
+ *	     AP extended addressing (APXA) facility are installed.
+ * Format 1: The APXA facility is not installed but the MSAX3 facility is.
+ * Format 2: Both the APXA and MSAX3 facilities are installed
+ */
 static void kvm_s390_set_crycb_format(struct kvm *kvm)
 {
 	kvm->arch.crypto.crycbd = (__u32)(unsigned long) kvm->arch.crypto.crycb;
 
+	/* Clear the CRYCB format bits - i.e., set format 0 by default */
+	kvm->arch.crypto.crycbd &= ~(CRYCB_FORMAT_MASK);
+
+	/* Check whether MSAX3 is installed */
+	if (!test_kvm_facility(kvm, 76))
+		return;
+
 	if (kvm_s390_apxa_installed())
 		kvm->arch.crypto.crycbd |= CRYCB_FORMAT2;
 	else
@@ -2055,12 +2044,12 @@ static u64 kvm_s390_get_initial_cpuid(void)
 
 static void kvm_s390_crypto_init(struct kvm *kvm)
 {
-	if (!test_kvm_facility(kvm, 76))
-		return;
-
 	kvm->arch.crypto.crycb = &kvm->arch.sie_page2->crycb;
 	kvm_s390_set_crycb_format(kvm);
 
+	if (!test_kvm_facility(kvm, 76))
+		return;
+
 	/* Enable AES/DEA protected key functions by default */
 	kvm->arch.crypto.aes_kw = 1;
 	kvm->arch.crypto.dea_kw = 1;
@@ -2586,17 +2575,24 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
 {
-	if (!test_kvm_facility(vcpu->kvm, 76))
+	/*
+	 * If the AP instructions are not being interpreted and the MSAX3
+	 * facility is not configured for the guest, there is nothing to set up.
+	 */
+	if (!vcpu->kvm->arch.crypto.apie && !test_kvm_facility(vcpu->kvm, 76))
 		return;
 
+	vcpu->arch.sie_block->crycbd = vcpu->kvm->arch.crypto.crycbd;
 	vcpu->arch.sie_block->ecb3 &= ~(ECB3_AES | ECB3_DEA);
 
+	if (vcpu->kvm->arch.crypto.apie)
+		vcpu->arch.sie_block->eca |= ECA_APIE;
+
+	/* Set up protected key support */
 	if (vcpu->kvm->arch.crypto.aes_kw)
 		vcpu->arch.sie_block->ecb3 |= ECB3_AES;
 	if (vcpu->kvm->arch.crypto.dea_kw)
 		vcpu->arch.sie_block->ecb3 |= ECB3_DEA;
-
-	vcpu->arch.sie_block->crycbd = vcpu->kvm->arch.crypto.crycbd;
 }
 
 void kvm_s390_vcpu_unsetup_cmma(struct kvm_vcpu *vcpu)

commit 3194cdb71190a74d46ae456efef10ecfc6f1e062
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Sep 25 19:16:17 2018 -0400

    KVM: s390: introduce and use KVM_REQ_VSIE_RESTART
    
    When we change the crycb (or execution controls), we also have to make sure
    that the vSIE shadow datastructures properly consider the changed
    values before rerunning the vSIE. We can achieve that by simply using a
    VCPU request now.
    
    This has to be a synchronous request (== handled before entering the
    (v)SIE again).
    
    The request will make sure that the vSIE handler is left, and that the
    request will be processed (NOP), therefore forcing a reload of all
    vSIE data (including rebuilding the crycb) when re-entering the vSIE
    interception handler the next time.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Tony Krowiak <akrowiak@linux.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Message-Id: <20180925231641.4954-3-akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0b5aff0e3984..876fbb2b0c04 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -844,8 +844,11 @@ void kvm_s390_vcpu_crypto_reset_all(struct kvm *kvm)
 
 	kvm_s390_vcpu_block_all(kvm);
 
-	kvm_for_each_vcpu(i, vcpu, kvm)
+	kvm_for_each_vcpu(i, vcpu, kvm) {
 		kvm_s390_vcpu_crypto_setup(vcpu);
+		/* recreate the shadow crycb by leaving the VSIE handler */
+		kvm_s390_sync_request(KVM_REQ_VSIE_RESTART, vcpu);
+	}
 
 	kvm_s390_vcpu_unblock_all(kvm);
 }
@@ -3203,6 +3206,8 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 
 	/* nothing to do, just clear the request */
 	kvm_clear_request(KVM_REQ_UNHALT, vcpu);
+	/* we left the vsie handler, nothing to do, just clear the request */
+	kvm_clear_request(KVM_REQ_VSIE_RESTART, vcpu);
 
 	return 0;
 }

commit 9ea597286570b50f94af3f785d099e5a5859a745
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Sep 25 19:16:16 2018 -0400

    KVM: s390: vsie: simulate VCPU SIE entry/exit
    
    VCPU requests and VCPU blocking right now don't take care of the vSIE
    (as it was not necessary until now). But we want to have synchronous VCPU
    requests that will also be handled before running the vSIE again.
    
    So let's simulate a SIE entry of the VCPU when calling the sie during
    vSIE handling and check for PROG_ flags. The existing infrastructure
    (e.g. exit_sie()) will then detect that the SIE (in form of the vSIE) is
    running and properly kick the vSIE CPU, resulting in it leaving the vSIE
    loop and therefore the vSIE interception handler, allowing it to handle
    VCPU requests.
    
    E.g. if we want to modify the crycb of the VCPU and make sure that any
    masks also get applied to the VSIE crycb shadow (which uses masks from the
    VCPU crycb), we will need a way to hinder the vSIE from running and make
    sure to process the updated crycb before reentering the vSIE again.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Tony Krowiak <akrowiak@linux.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Message-Id: <20180925231641.4954-2-akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f69333fd2fa3..0b5aff0e3984 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2768,18 +2768,25 @@ static void kvm_s390_vcpu_request(struct kvm_vcpu *vcpu)
 	exit_sie(vcpu);
 }
 
+bool kvm_s390_vcpu_sie_inhibited(struct kvm_vcpu *vcpu)
+{
+	return atomic_read(&vcpu->arch.sie_block->prog20) &
+	       (PROG_BLOCK_SIE | PROG_REQUEST);
+}
+
 static void kvm_s390_vcpu_request_handled(struct kvm_vcpu *vcpu)
 {
 	atomic_andnot(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
 }
 
 /*
- * Kick a guest cpu out of SIE and wait until SIE is not running.
+ * Kick a guest cpu out of (v)SIE and wait until (v)SIE is not running.
  * If the CPU is not running (e.g. waiting as idle) the function will
  * return immediately. */
 void exit_sie(struct kvm_vcpu *vcpu)
 {
 	kvm_s390_set_cpuflags(vcpu, CPUSTAT_STOP_INT);
+	kvm_s390_vsie_kick(vcpu);
 	while (vcpu->arch.sie_block->prog0c & PROG_IN_SIE)
 		cpu_relax();
 }

commit 40ebdb8e59df36e2cc71810bd021a0808b16c956
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Wed Aug 1 11:48:28 2018 +0100

    KVM: s390: Make huge pages unavailable in ucontrol VMs
    
    We currently do not notify all gmaps when using gmap_pmdp_xchg(), due
    to locking constraints. This makes ucontrol VMs, which is the only VM
    type that creates multiple gmaps, incompatible with huge pages. Also
    we would need to hold the guest_table_lock of all gmaps that have this
    vmaddr maped to synchronize access to the pmd.
    
    ucontrol VMs are rather exotic and creating a new locking concept is
    no easy task. Hence we return EINVAL when trying to active
    KVM_CAP_S390_HPAGE_1M and report it as being not available when
    checking for it.
    
    Fixes: a4499382 ("KVM: s390: Add huge page enablement control")
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Claudio Imbrenda <imbrenda@linux.ibm.com>
    Message-Id: <20180801112508.138159-1-frankja@linux.ibm.com>
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f69333fd2fa3..ac5da6b0b862 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -481,7 +481,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	case KVM_CAP_S390_HPAGE_1M:
 		r = 0;
-		if (hpage)
+		if (hpage && !kvm_is_ucontrol(kvm))
 			r = 1;
 		break;
 	case KVM_CAP_S390_MEM_OP:
@@ -691,7 +691,7 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		mutex_lock(&kvm->lock);
 		if (kvm->created_vcpus)
 			r = -EBUSY;
-		else if (!hpage || kvm->arch.use_cmma)
+		else if (!hpage || kvm->arch.use_cmma || kvm_is_ucontrol(kvm))
 			r = -EINVAL;
 		else {
 			r = 0;

commit df88f3181f10565c6e3a89eb6f0f9e6afaaf15f1
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Thu Aug 30 16:14:18 2018 +0200

    KVM: s390: Properly lock mm context allow_gmap_hpage_1m setting
    
    We have to do down_write on the mm semaphore to set a bitfield in the
    mm context.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Fixes: a4499382 ("KVM: s390: Add huge page enablement control")
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 91ad4a9425c0..f69333fd2fa3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -695,7 +695,9 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 			r = -EINVAL;
 		else {
 			r = 0;
+			down_write(&kvm->mm->mmap_sem);
 			kvm->mm->context.allow_gmap_hpage_1m = 1;
+			up_write(&kvm->mm->mmap_sem);
 			/*
 			 * We might have to create fake 4k page
 			 * tables. To avoid that the hardware works on

commit 2375846193663a1282c0ef7093640ed3210dc09f
Merge: 57cb198cfdd2 a449938297e5
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Mon Jul 30 23:20:00 2018 +0200

    Merge tag 'hlp_stage1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kvms390/next
    
    KVM: s390: initial host large page support
    
    - must be enabled via module parameter hpage=1
    - cannot be used together with nested
    - does support migration
    - does support hugetlbfs
    - no THP yet

commit a449938297e55e7e8958f8b48583f7d342da1930
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Jul 13 11:28:31 2018 +0100

    KVM: s390: Add huge page enablement control
    
    General KVM huge page support on s390 has to be enabled via the
    kvm.hpage module parameter. Either nested or hpage can be enabled, as
    we currently do not support vSIE for huge backed guests. Once the vSIE
    support is added we will either drop the parameter or enable it as
    default.
    
    For a guest the feature has to be enabled through the new
    KVM_CAP_S390_HPAGE_1M capability and the hpage module
    parameter. Enabling it means that cmm can't be enabled for the vm and
    disables pfmf and storage key interpretation.
    
    This is due to the fact that in some cases, in upcoming patches, we
    have to split huge pages in the guest mapping to be able to set more
    granular memory protection on 4k pages. These split pages have fake
    page tables that are not visible to the Linux memory management which
    subsequently will not manage its PGSTEs, while the SIE will. Disabling
    these features lets us manage PGSTE data in a consistent matter and
    solve that problem.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 662f4d8046db..f9d90337e64a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -172,6 +172,10 @@ static int nested;
 module_param(nested, int, S_IRUGO);
 MODULE_PARM_DESC(nested, "Nested virtualization support");
 
+/* allow 1m huge page guest backing, if !nested */
+static int hpage;
+module_param(hpage, int, 0444);
+MODULE_PARM_DESC(hpage, "1m huge page backing support");
 
 /*
  * For now we handle at most 16 double words as this is what the s390 base
@@ -475,6 +479,11 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_AIS_MIGRATION:
 		r = 1;
 		break;
+	case KVM_CAP_S390_HPAGE_1M:
+		r = 0;
+		if (hpage)
+			r = 1;
+		break;
 	case KVM_CAP_S390_MEM_OP:
 		r = MEM_OP_MAX_SIZE;
 		break;
@@ -678,6 +687,27 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_GS %s",
 			 r ? "(not available)" : "(success)");
 		break;
+	case KVM_CAP_S390_HPAGE_1M:
+		mutex_lock(&kvm->lock);
+		if (kvm->created_vcpus)
+			r = -EBUSY;
+		else if (!hpage || kvm->arch.use_cmma)
+			r = -EINVAL;
+		else {
+			r = 0;
+			kvm->mm->context.allow_gmap_hpage_1m = 1;
+			/*
+			 * We might have to create fake 4k page
+			 * tables. To avoid that the hardware works on
+			 * stale PGSTEs, we emulate these instructions.
+			 */
+			kvm->arch.use_skf = 0;
+			kvm->arch.use_pfmfi = 0;
+		}
+		mutex_unlock(&kvm->lock);
+		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_HPAGE %s",
+			 r ? "(not available)" : "(success)");
+		break;
 	case KVM_CAP_S390_USER_STSI:
 		VM_EVENT(kvm, 3, "%s", "ENABLE: CAP_S390_USER_STSI");
 		kvm->arch.user_stsi = 1;
@@ -725,10 +755,13 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		if (!sclp.has_cmma)
 			break;
 
-		ret = -EBUSY;
 		VM_EVENT(kvm, 3, "%s", "ENABLE: CMMA support");
 		mutex_lock(&kvm->lock);
-		if (!kvm->created_vcpus) {
+		if (kvm->created_vcpus)
+			ret = -EBUSY;
+		else if (kvm->mm->context.allow_gmap_hpage_1m)
+			ret = -EINVAL;
+		else {
 			kvm->arch.use_cmma = 1;
 			/* Not compatible with cmma. */
 			kvm->arch.use_pfmfi = 0;
@@ -4102,6 +4135,11 @@ static int __init kvm_s390_init(void)
 		return -ENODEV;
 	}
 
+	if (nested && hpage) {
+		pr_info("nested (vSIE) and hpage (huge page backing) can currently not be activated concurrently");
+		return -EINVAL;
+	}
+
 	for (i = 0; i < 16; i++)
 		kvm_s390_fac_base[i] |=
 			S390_lowcore.stfle_fac_list[i] & nonhyp_mask(i);

commit bd096f6443194e57382686a3ac5f2ce4e82b55d7
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Wed Jul 18 13:40:22 2018 +0100

    KVM: s390: Add skey emulation fault handling
    
    When doing skey emulation for huge guests, we now need to fault in
    pmds, as we don't have PGSTES anymore to store them when we do not
    have valid table entries.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4cff5e31ca36..662f4d8046db 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1551,6 +1551,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	uint8_t *keys;
 	uint64_t hva;
 	int srcu_idx, i, r = 0;
+	bool unlocked;
 
 	if (args->flags != 0)
 		return -EINVAL;
@@ -1575,9 +1576,11 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	if (r)
 		goto out;
 
+	i = 0;
 	down_read(&current->mm->mmap_sem);
 	srcu_idx = srcu_read_lock(&kvm->srcu);
-	for (i = 0; i < args->count; i++) {
+        while (i < args->count) {
+		unlocked = false;
 		hva = gfn_to_hva(kvm, args->start_gfn + i);
 		if (kvm_is_error_hva(hva)) {
 			r = -EFAULT;
@@ -1591,8 +1594,14 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 		}
 
 		r = set_guest_storage_key(current->mm, hva, keys[i], 0);
-		if (r)
-			break;
+		if (r) {
+			r = fixup_user_fault(current, current->mm, hva,
+					     FAULT_FLAG_WRITE, &unlocked);
+			if (r)
+				break;
+		}
+		if (!r)
+			i++;
 	}
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
 	up_read(&current->mm->mmap_sem);

commit 0959e168678d2d95648317e1e5e46bcb358272eb
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Tue Jul 17 13:21:22 2018 +0100

    s390/mm: Add huge page dirty sync support
    
    To do dirty loging with huge pages, we protect huge pmds in the
    gmap. When they are written to, we unprotect them and mark them dirty.
    
    We introduce the function gmap_test_and_clear_dirty_pmd which handles
    dirty sync for huge pages.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Acked-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3b7a5151b6a5..4cff5e31ca36 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -511,19 +511,30 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 }
 
 static void kvm_s390_sync_dirty_log(struct kvm *kvm,
-					struct kvm_memory_slot *memslot)
+				    struct kvm_memory_slot *memslot)
 {
+	int i;
 	gfn_t cur_gfn, last_gfn;
-	unsigned long address;
+	unsigned long gaddr, vmaddr;
 	struct gmap *gmap = kvm->arch.gmap;
+	DECLARE_BITMAP(bitmap, _PAGE_ENTRIES);
 
-	/* Loop over all guest pages */
+	/* Loop over all guest segments */
+	cur_gfn = memslot->base_gfn;
 	last_gfn = memslot->base_gfn + memslot->npages;
-	for (cur_gfn = memslot->base_gfn; cur_gfn <= last_gfn; cur_gfn++) {
-		address = gfn_to_hva_memslot(memslot, cur_gfn);
+	for (; cur_gfn <= last_gfn; cur_gfn += _PAGE_ENTRIES) {
+		gaddr = gfn_to_gpa(cur_gfn);
+		vmaddr = gfn_to_hva_memslot(memslot, cur_gfn);
+		if (kvm_is_error_hva(vmaddr))
+			continue;
+
+		bitmap_zero(bitmap, _PAGE_ENTRIES);
+		gmap_sync_dirty_log_pmd(gmap, bitmap, gaddr, vmaddr);
+		for (i = 0; i < _PAGE_ENTRIES; i++) {
+			if (test_bit(i, bitmap))
+				mark_page_dirty(kvm, cur_gfn + i);
+		}
 
-		if (test_and_clear_guest_dirty(gmap->mm, address))
-			mark_page_dirty(kvm, cur_gfn);
 		if (fatal_signal_pending(current))
 			return;
 		cond_resched();

commit a3da7b4a3be51f37f434f14e11e60491f098b6ea
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Mar 8 16:08:49 2018 +0000

    KVM: s390: add etoken support for guests
    
    We want to provide facility 156 (etoken facility) to our
    guests. This includes migration support (via sync regs) and
    VSIE changes. The tokens are being reset on clear reset. This
    has to be implemented by userspace (via sync regs).
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b350ec14d882..0cc8a48fc732 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2355,6 +2355,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_BPBC;
 	if (test_kvm_facility(vcpu->kvm, 133))
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_GSCB;
+	if (test_kvm_facility(vcpu->kvm, 156))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_ETOKEN;
 	/* fprs can be synchronized via vrs, even if the guest has no vx. With
 	 * MACHINE_HAS_VX, (load|store)_fpu_regs() will work with vrs format.
 	 */
@@ -2604,7 +2606,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	}
 	if (test_kvm_facility(vcpu->kvm, 139))
 		vcpu->arch.sie_block->ecd |= ECD_MEF;
-
+	if (test_kvm_facility(vcpu->kvm, 156))
+		vcpu->arch.sie_block->ecd |= ECD_ETOKENF;
 	if (vcpu->arch.sie_block->gd) {
 		vcpu->arch.sie_block->eca |= ECA_AIV;
 		VCPU_EVENT(vcpu, 3, "AIV gisa format-%u enabled for cpu %03u",
@@ -3522,6 +3525,7 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		}
 		preempt_enable();
 	}
+	/* SIE will load etoken directly from SDNX and therefore kvm_run */
 
 	kvm_run->kvm_dirty_regs = 0;
 }
@@ -3561,7 +3565,7 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 			__ctl_clear_bit(2, 4);
 		vcpu->arch.host_gscb = NULL;
 	}
-
+	/* SIE will save etoken directly into SDNX and therefore kvm_run */
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)

commit afdad61615cc37fef91bc70af7ee5b293785fbd6
Author: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
Date:   Mon Apr 30 18:33:25 2018 +0200

    KVM: s390: Fix storage attributes migration with memory slots
    
    This is a fix for several issues that were found in the original code
    for storage attributes migration.
    
    Now no bitmap is allocated to keep track of dirty storage attributes;
    the extra bits of the per-memslot bitmap that are always present anyway
    are now used for this purpose.
    
    The code has also been refactored a little to improve readability.
    
    Fixes: 190df4a212a ("KVM: s390: CMMA tracking, ESSA emulation, migration mode")
    Fixes: 4036e3874a1 ("KVM: s390: ioctls to get and set guest storage attributes")
    Acked-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Message-Id: <1525106005-13931-3-git-send-email-imbrenda@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3b7a5151b6a5..b350ec14d882 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -862,54 +862,37 @@ static void kvm_s390_sync_request_broadcast(struct kvm *kvm, int req)
  */
 static int kvm_s390_vm_start_migration(struct kvm *kvm)
 {
-	struct kvm_s390_migration_state *mgs;
 	struct kvm_memory_slot *ms;
-	/* should be the only one */
 	struct kvm_memslots *slots;
-	unsigned long ram_pages;
+	unsigned long ram_pages = 0;
 	int slotnr;
 
 	/* migration mode already enabled */
-	if (kvm->arch.migration_state)
+	if (kvm->arch.migration_mode)
 		return 0;
-
 	slots = kvm_memslots(kvm);
 	if (!slots || !slots->used_slots)
 		return -EINVAL;
 
-	mgs = kzalloc(sizeof(*mgs), GFP_KERNEL);
-	if (!mgs)
-		return -ENOMEM;
-	kvm->arch.migration_state = mgs;
-
-	if (kvm->arch.use_cmma) {
+	if (!kvm->arch.use_cmma) {
+		kvm->arch.migration_mode = 1;
+		return 0;
+	}
+	/* mark all the pages in active slots as dirty */
+	for (slotnr = 0; slotnr < slots->used_slots; slotnr++) {
+		ms = slots->memslots + slotnr;
 		/*
-		 * Get the first slot. They are reverse sorted by base_gfn, so
-		 * the first slot is also the one at the end of the address
-		 * space. We have verified above that at least one slot is
-		 * present.
+		 * The second half of the bitmap is only used on x86,
+		 * and would be wasted otherwise, so we put it to good
+		 * use here to keep track of the state of the storage
+		 * attributes.
 		 */
-		ms = slots->memslots;
-		/* round up so we only use full longs */
-		ram_pages = roundup(ms->base_gfn + ms->npages, BITS_PER_LONG);
-		/* allocate enough bytes to store all the bits */
-		mgs->pgste_bitmap = vmalloc(ram_pages / 8);
-		if (!mgs->pgste_bitmap) {
-			kfree(mgs);
-			kvm->arch.migration_state = NULL;
-			return -ENOMEM;
-		}
-
-		mgs->bitmap_size = ram_pages;
-		atomic64_set(&mgs->dirty_pages, ram_pages);
-		/* mark all the pages in active slots as dirty */
-		for (slotnr = 0; slotnr < slots->used_slots; slotnr++) {
-			ms = slots->memslots + slotnr;
-			bitmap_set(mgs->pgste_bitmap, ms->base_gfn, ms->npages);
-		}
-
-		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_START_MIGRATION);
+		memset(kvm_second_dirty_bitmap(ms), 0xff, kvm_dirty_bitmap_bytes(ms));
+		ram_pages += ms->npages;
 	}
+	atomic64_set(&kvm->arch.cmma_dirty_pages, ram_pages);
+	kvm->arch.migration_mode = 1;
+	kvm_s390_sync_request_broadcast(kvm, KVM_REQ_START_MIGRATION);
 	return 0;
 }
 
@@ -919,21 +902,12 @@ static int kvm_s390_vm_start_migration(struct kvm *kvm)
  */
 static int kvm_s390_vm_stop_migration(struct kvm *kvm)
 {
-	struct kvm_s390_migration_state *mgs;
-
 	/* migration mode already disabled */
-	if (!kvm->arch.migration_state)
+	if (!kvm->arch.migration_mode)
 		return 0;
-	mgs = kvm->arch.migration_state;
-	kvm->arch.migration_state = NULL;
-
-	if (kvm->arch.use_cmma) {
+	kvm->arch.migration_mode = 0;
+	if (kvm->arch.use_cmma)
 		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_STOP_MIGRATION);
-		/* We have to wait for the essa emulation to finish */
-		synchronize_srcu(&kvm->srcu);
-		vfree(mgs->pgste_bitmap);
-	}
-	kfree(mgs);
 	return 0;
 }
 
@@ -961,7 +935,7 @@ static int kvm_s390_vm_set_migration(struct kvm *kvm,
 static int kvm_s390_vm_get_migration(struct kvm *kvm,
 				     struct kvm_device_attr *attr)
 {
-	u64 mig = (kvm->arch.migration_state != NULL);
+	u64 mig = kvm->arch.migration_mode;
 
 	if (attr->attr != KVM_S390_VM_MIGRATION_STATUS)
 		return -ENXIO;
@@ -1599,6 +1573,134 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 /* for consistency */
 #define KVM_S390_CMMA_SIZE_MAX ((u32)KVM_S390_SKEYS_MAX)
 
+/*
+ * Similar to gfn_to_memslot, but returns the index of a memslot also when the
+ * address falls in a hole. In that case the index of one of the memslots
+ * bordering the hole is returned.
+ */
+static int gfn_to_memslot_approx(struct kvm_memslots *slots, gfn_t gfn)
+{
+	int start = 0, end = slots->used_slots;
+	int slot = atomic_read(&slots->lru_slot);
+	struct kvm_memory_slot *memslots = slots->memslots;
+
+	if (gfn >= memslots[slot].base_gfn &&
+	    gfn < memslots[slot].base_gfn + memslots[slot].npages)
+		return slot;
+
+	while (start < end) {
+		slot = start + (end - start) / 2;
+
+		if (gfn >= memslots[slot].base_gfn)
+			end = slot;
+		else
+			start = slot + 1;
+	}
+
+	if (gfn >= memslots[start].base_gfn &&
+	    gfn < memslots[start].base_gfn + memslots[start].npages) {
+		atomic_set(&slots->lru_slot, start);
+	}
+
+	return start;
+}
+
+static int kvm_s390_peek_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,
+			      u8 *res, unsigned long bufsize)
+{
+	unsigned long pgstev, hva, cur_gfn = args->start_gfn;
+
+	args->count = 0;
+	while (args->count < bufsize) {
+		hva = gfn_to_hva(kvm, cur_gfn);
+		/*
+		 * We return an error if the first value was invalid, but we
+		 * return successfully if at least one value was copied.
+		 */
+		if (kvm_is_error_hva(hva))
+			return args->count ? 0 : -EFAULT;
+		if (get_pgste(kvm->mm, hva, &pgstev) < 0)
+			pgstev = 0;
+		res[args->count++] = (pgstev >> 24) & 0x43;
+		cur_gfn++;
+	}
+
+	return 0;
+}
+
+static unsigned long kvm_s390_next_dirty_cmma(struct kvm_memslots *slots,
+					      unsigned long cur_gfn)
+{
+	int slotidx = gfn_to_memslot_approx(slots, cur_gfn);
+	struct kvm_memory_slot *ms = slots->memslots + slotidx;
+	unsigned long ofs = cur_gfn - ms->base_gfn;
+
+	if (ms->base_gfn + ms->npages <= cur_gfn) {
+		slotidx--;
+		/* If we are above the highest slot, wrap around */
+		if (slotidx < 0)
+			slotidx = slots->used_slots - 1;
+
+		ms = slots->memslots + slotidx;
+		ofs = 0;
+	}
+	ofs = find_next_bit(kvm_second_dirty_bitmap(ms), ms->npages, ofs);
+	while ((slotidx > 0) && (ofs >= ms->npages)) {
+		slotidx--;
+		ms = slots->memslots + slotidx;
+		ofs = find_next_bit(kvm_second_dirty_bitmap(ms), ms->npages, 0);
+	}
+	return ms->base_gfn + ofs;
+}
+
+static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,
+			     u8 *res, unsigned long bufsize)
+{
+	unsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;
+	struct kvm_memslots *slots = kvm_memslots(kvm);
+	struct kvm_memory_slot *ms;
+
+	cur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);
+	ms = gfn_to_memslot(kvm, cur_gfn);
+	args->count = 0;
+	args->start_gfn = cur_gfn;
+	if (!ms)
+		return 0;
+	next_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);
+	mem_end = slots->memslots[0].base_gfn + slots->memslots[0].npages;
+
+	while (args->count < bufsize) {
+		hva = gfn_to_hva(kvm, cur_gfn);
+		if (kvm_is_error_hva(hva))
+			return 0;
+		/* Decrement only if we actually flipped the bit to 0 */
+		if (test_and_clear_bit(cur_gfn - ms->base_gfn, kvm_second_dirty_bitmap(ms)))
+			atomic64_dec(&kvm->arch.cmma_dirty_pages);
+		if (get_pgste(kvm->mm, hva, &pgstev) < 0)
+			pgstev = 0;
+		/* Save the value */
+		res[args->count++] = (pgstev >> 24) & 0x43;
+		/* If the next bit is too far away, stop. */
+		if (next_gfn > cur_gfn + KVM_S390_MAX_BIT_DISTANCE)
+			return 0;
+		/* If we reached the previous "next", find the next one */
+		if (cur_gfn == next_gfn)
+			next_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);
+		/* Reached the end of memory or of the buffer, stop */
+		if ((next_gfn >= mem_end) ||
+		    (next_gfn - args->start_gfn >= bufsize))
+			return 0;
+		cur_gfn++;
+		/* Reached the end of the current memslot, take the next one. */
+		if (cur_gfn - ms->base_gfn >= ms->npages) {
+			ms = gfn_to_memslot(kvm, cur_gfn);
+			if (!ms)
+				return 0;
+		}
+	}
+	return 0;
+}
+
 /*
  * This function searches for the next page with dirty CMMA attributes, and
  * saves the attributes in the buffer up to either the end of the buffer or
@@ -1610,22 +1712,18 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 static int kvm_s390_get_cmma_bits(struct kvm *kvm,
 				  struct kvm_s390_cmma_log *args)
 {
-	struct kvm_s390_migration_state *s = kvm->arch.migration_state;
-	unsigned long bufsize, hva, pgstev, i, next, cur;
-	int srcu_idx, peek, r = 0, rr;
-	u8 *res;
-
-	cur = args->start_gfn;
-	i = next = pgstev = 0;
+	unsigned long bufsize;
+	int srcu_idx, peek, ret;
+	u8 *values;
 
-	if (unlikely(!kvm->arch.use_cmma))
+	if (!kvm->arch.use_cmma)
 		return -ENXIO;
 	/* Invalid/unsupported flags were specified */
 	if (args->flags & ~KVM_S390_CMMA_PEEK)
 		return -EINVAL;
 	/* Migration mode query, and we are not doing a migration */
 	peek = !!(args->flags & KVM_S390_CMMA_PEEK);
-	if (!peek && !s)
+	if (!peek && !kvm->arch.migration_mode)
 		return -EINVAL;
 	/* CMMA is disabled or was not used, or the buffer has length zero */
 	bufsize = min(args->count, KVM_S390_CMMA_SIZE_MAX);
@@ -1633,74 +1731,35 @@ static int kvm_s390_get_cmma_bits(struct kvm *kvm,
 		memset(args, 0, sizeof(*args));
 		return 0;
 	}
-
-	if (!peek) {
-		/* We are not peeking, and there are no dirty pages */
-		if (!atomic64_read(&s->dirty_pages)) {
-			memset(args, 0, sizeof(*args));
-			return 0;
-		}
-		cur = find_next_bit(s->pgste_bitmap, s->bitmap_size,
-				    args->start_gfn);
-		if (cur >= s->bitmap_size)	/* nothing found, loop back */
-			cur = find_next_bit(s->pgste_bitmap, s->bitmap_size, 0);
-		if (cur >= s->bitmap_size) {	/* again! (very unlikely) */
-			memset(args, 0, sizeof(*args));
-			return 0;
-		}
-		next = find_next_bit(s->pgste_bitmap, s->bitmap_size, cur + 1);
+	/* We are not peeking, and there are no dirty pages */
+	if (!peek && !atomic64_read(&kvm->arch.cmma_dirty_pages)) {
+		memset(args, 0, sizeof(*args));
+		return 0;
 	}
 
-	res = vmalloc(bufsize);
-	if (!res)
+	values = vmalloc(bufsize);
+	if (!values)
 		return -ENOMEM;
 
-	args->start_gfn = cur;
-
 	down_read(&kvm->mm->mmap_sem);
 	srcu_idx = srcu_read_lock(&kvm->srcu);
-	while (i < bufsize) {
-		hva = gfn_to_hva(kvm, cur);
-		if (kvm_is_error_hva(hva)) {
-			r = -EFAULT;
-			break;
-		}
-		/* decrement only if we actually flipped the bit to 0 */
-		if (!peek && test_and_clear_bit(cur, s->pgste_bitmap))
-			atomic64_dec(&s->dirty_pages);
-		r = get_pgste(kvm->mm, hva, &pgstev);
-		if (r < 0)
-			pgstev = 0;
-		/* save the value */
-		res[i++] = (pgstev >> 24) & 0x43;
-		/*
-		 * if the next bit is too far away, stop.
-		 * if we reached the previous "next", find the next one
-		 */
-		if (!peek) {
-			if (next > cur + KVM_S390_MAX_BIT_DISTANCE)
-				break;
-			if (cur == next)
-				next = find_next_bit(s->pgste_bitmap,
-						     s->bitmap_size, cur + 1);
-		/* reached the end of the bitmap or of the buffer, stop */
-			if ((next >= s->bitmap_size) ||
-			    (next >= args->start_gfn + bufsize))
-				break;
-		}
-		cur++;
-	}
+	if (peek)
+		ret = kvm_s390_peek_cmma(kvm, args, values, bufsize);
+	else
+		ret = kvm_s390_get_cmma(kvm, args, values, bufsize);
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
 	up_read(&kvm->mm->mmap_sem);
-	args->count = i;
-	args->remaining = s ? atomic64_read(&s->dirty_pages) : 0;
 
-	rr = copy_to_user((void __user *)args->values, res, args->count);
-	if (rr)
-		r = -EFAULT;
+	if (kvm->arch.migration_mode)
+		args->remaining = atomic64_read(&kvm->arch.cmma_dirty_pages);
+	else
+		args->remaining = 0;
 
-	vfree(res);
-	return r;
+	if (copy_to_user((void __user *)args->values, values, args->count))
+		ret = -EFAULT;
+
+	vfree(values);
+	return ret;
 }
 
 /*
@@ -2139,10 +2198,6 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_s390_destroy_adapters(kvm);
 	kvm_s390_clear_float_irqs(kvm);
 	kvm_s390_vsie_destroy(kvm);
-	if (kvm->arch.migration_state) {
-		vfree(kvm->arch.migration_state->pgste_bitmap);
-		kfree(kvm->arch.migration_state);
-	}
 	KVM_EVENT(3, "vm 0x%pK destroyed", kvm);
 }
 

commit b08fc5277aaa1d8ea15470d38bf36f19dfb0e125
Merge: 4597fcff0704 9d2a789c1db7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 18:28:00 2018 -0700

    Merge tag 'overflow-v4.18-rc1-part2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull more overflow updates from Kees Cook:
     "The rest of the overflow changes for v4.18-rc1.
    
      This includes the explicit overflow fixes from Silvio, further
      struct_size() conversions from Matthew, and a bug fix from Dan.
    
      But the bulk of it is the treewide conversions to use either the
      2-factor argument allocators (e.g. kmalloc(a * b, ...) into
      kmalloc_array(a, b, ...) or the array_size() macros (e.g. vmalloc(a *
      b) into vmalloc(array_size(a, b)).
    
      Coccinelle was fighting me on several fronts, so I've done a bunch of
      manual whitespace updates in the patches as well.
    
      Summary:
    
       - Error path bug fix for overflow tests (Dan)
    
       - Additional struct_size() conversions (Matthew, Kees)
    
       - Explicitly reported overflow fixes (Silvio, Kees)
    
       - Add missing kvcalloc() function (Kees)
    
       - Treewide conversions of allocators to use either 2-factor argument
         variant when available, or array_size() and array3_size() as needed
         (Kees)"
    
    * tag 'overflow-v4.18-rc1-part2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux: (26 commits)
      treewide: Use array_size in f2fs_kvzalloc()
      treewide: Use array_size() in f2fs_kzalloc()
      treewide: Use array_size() in f2fs_kmalloc()
      treewide: Use array_size() in sock_kmalloc()
      treewide: Use array_size() in kvzalloc_node()
      treewide: Use array_size() in vzalloc_node()
      treewide: Use array_size() in vzalloc()
      treewide: Use array_size() in vmalloc()
      treewide: devm_kzalloc() -> devm_kcalloc()
      treewide: devm_kmalloc() -> devm_kmalloc_array()
      treewide: kvzalloc() -> kvcalloc()
      treewide: kvmalloc() -> kvmalloc_array()
      treewide: kzalloc_node() -> kcalloc_node()
      treewide: kzalloc() -> kcalloc()
      treewide: kmalloc() -> kmalloc_array()
      mm: Introduce kvcalloc()
      video: uvesafb: Fix integer overflow in allocation
      UBIFS: Fix potential integer overflow in allocation
      leds: Use struct_size() in allocation
      Convert intel uncore to struct_size
      ...

commit 42bc47b35320e0e587a88e437e18f80f9c5bcbb2
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jun 12 14:27:11 2018 -0700

    treewide: Use array_size() in vmalloc()
    
    The vmalloc() function has no 2-factor argument form, so multiplication
    factors need to be wrapped in array_size(). This patch replaces cases of:
    
            vmalloc(a * b)
    
    with:
            vmalloc(array_size(a, b))
    
    as well as handling cases of:
    
            vmalloc(a * b * c)
    
    with:
    
            vmalloc(array3_size(a, b, c))
    
    This does, however, attempt to ignore constant size factors like:
    
            vmalloc(4 * 1024)
    
    though any constants defined via macros get caught up in the conversion.
    
    Any factors with a sizeof() of "unsigned char", "char", and "u8" were
    dropped, since they're redundant.
    
    The Coccinelle script used for this was:
    
    // Fix redundant parens around sizeof().
    @@
    type TYPE;
    expression THING, E;
    @@
    
    (
      vmalloc(
    -       (sizeof(TYPE)) * E
    +       sizeof(TYPE) * E
      , ...)
    |
      vmalloc(
    -       (sizeof(THING)) * E
    +       sizeof(THING) * E
      , ...)
    )
    
    // Drop single-byte sizes and redundant parens.
    @@
    expression COUNT;
    typedef u8;
    typedef __u8;
    @@
    
    (
      vmalloc(
    -       sizeof(u8) * (COUNT)
    +       COUNT
      , ...)
    |
      vmalloc(
    -       sizeof(__u8) * (COUNT)
    +       COUNT
      , ...)
    |
      vmalloc(
    -       sizeof(char) * (COUNT)
    +       COUNT
      , ...)
    |
      vmalloc(
    -       sizeof(unsigned char) * (COUNT)
    +       COUNT
      , ...)
    |
      vmalloc(
    -       sizeof(u8) * COUNT
    +       COUNT
      , ...)
    |
      vmalloc(
    -       sizeof(__u8) * COUNT
    +       COUNT
      , ...)
    |
      vmalloc(
    -       sizeof(char) * COUNT
    +       COUNT
      , ...)
    |
      vmalloc(
    -       sizeof(unsigned char) * COUNT
    +       COUNT
      , ...)
    )
    
    // 2-factor product with sizeof(type/expression) and identifier or constant.
    @@
    type TYPE;
    expression THING;
    identifier COUNT_ID;
    constant COUNT_CONST;
    @@
    
    (
      vmalloc(
    -       sizeof(TYPE) * (COUNT_ID)
    +       array_size(COUNT_ID, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE) * COUNT_ID
    +       array_size(COUNT_ID, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE) * (COUNT_CONST)
    +       array_size(COUNT_CONST, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE) * COUNT_CONST
    +       array_size(COUNT_CONST, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * (COUNT_ID)
    +       array_size(COUNT_ID, sizeof(THING))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * COUNT_ID
    +       array_size(COUNT_ID, sizeof(THING))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * (COUNT_CONST)
    +       array_size(COUNT_CONST, sizeof(THING))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * COUNT_CONST
    +       array_size(COUNT_CONST, sizeof(THING))
      , ...)
    )
    
    // 2-factor product, only identifiers.
    @@
    identifier SIZE, COUNT;
    @@
    
      vmalloc(
    -       SIZE * COUNT
    +       array_size(COUNT, SIZE)
      , ...)
    
    // 3-factor product with 1 sizeof(type) or sizeof(expression), with
    // redundant parens removed.
    @@
    expression THING;
    identifier STRIDE, COUNT;
    type TYPE;
    @@
    
    (
      vmalloc(
    -       sizeof(TYPE) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      vmalloc(
    -       sizeof(THING) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    )
    
    // 3-factor product with 2 sizeof(variable), with redundant parens removed.
    @@
    expression THING1, THING2;
    identifier COUNT;
    type TYPE1, TYPE2;
    @@
    
    (
      vmalloc(
    -       sizeof(TYPE1) * sizeof(TYPE2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      vmalloc(
    -       sizeof(THING1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      vmalloc(
    -       sizeof(THING1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    |
      vmalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    )
    
    // 3-factor product, only identifiers, with redundant parens removed.
    @@
    identifier STRIDE, SIZE, COUNT;
    @@
    
    (
      vmalloc(
    -       (COUNT) * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vmalloc(
    -       COUNT * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vmalloc(
    -       COUNT * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vmalloc(
    -       (COUNT) * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vmalloc(
    -       COUNT * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vmalloc(
    -       (COUNT) * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vmalloc(
    -       (COUNT) * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vmalloc(
    -       COUNT * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    )
    
    // Any remaining multi-factor products, first at least 3-factor products
    // when they're not all constants...
    @@
    expression E1, E2, E3;
    constant C1, C2, C3;
    @@
    
    (
      vmalloc(C1 * C2 * C3, ...)
    |
      vmalloc(
    -       E1 * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    )
    
    // And then all remaining 2 factors products when they're not all constants.
    @@
    expression E1, E2;
    constant C1, C2;
    @@
    
    (
      vmalloc(C1 * C2, ...)
    |
      vmalloc(
    -       E1 * E2
    +       array_size(E1, E2)
      , ...)
    )
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 64c986243018..3f6625c64341 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1725,7 +1725,7 @@ static int kvm_s390_set_cmma_bits(struct kvm *kvm,
 	if (args->count == 0)
 		return 0;
 
-	bits = vmalloc(sizeof(*bits) * args->count);
+	bits = vmalloc(array_size(sizeof(*bits), args->count));
 	if (!bits)
 		return -ENOMEM;
 

commit 1499fa809e9e6713952ef84a7e9d51606881681f
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Thu Apr 19 00:49:58 2018 +0530

    kvm: Change return type to vm_fault_t
    
    Use new return type vm_fault_t for fault handler. For
    now, this is just documenting that the function returns
    a VM_FAULT value rather than an errno. Once all instances
    are converted, vm_fault_t will become a distinct type.
    
    commit 1c8f422059ae ("mm: change return type to vm_fault_t")
    
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Reviewed-by: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e521f7699032..7142508ca6e1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3993,7 +3993,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	return r;
 }
 
-int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
+vm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 {
 #ifdef CONFIG_KVM_S390_UCONTROL
 	if ((vmf->pgoff == KVM_S390_SIE_PAGE_OFFSET)

commit 33d1b2729e409e8327dec2d13a9144dfa76a947c
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Apr 27 14:36:13 2018 +0200

    KVM: s390: generalize kvm_s390_get_tod_clock_ext()
    
    Move the Multiple-epoch facility handling into it and rename it to
    kvm_s390_get_tod_clock().
    
    This leaves us with:
    - kvm_s390_set_tod_clock()
    - kvm_s390_get_tod_clock()
    - kvm_s390_get_tod_clock_fast()
    
    So all Multiple-epoch facility is hidden in these functions.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fd7ce3ab45eb..e521f7699032 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1040,8 +1040,8 @@ static int kvm_s390_set_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 	return ret;
 }
 
-static void kvm_s390_get_tod_clock_ext(struct kvm *kvm,
-					struct kvm_s390_vm_tod_clock *gtod)
+static void kvm_s390_get_tod_clock(struct kvm *kvm,
+				   struct kvm_s390_vm_tod_clock *gtod)
 {
 	struct kvm_s390_tod_clock_ext htod;
 
@@ -1050,10 +1050,12 @@ static void kvm_s390_get_tod_clock_ext(struct kvm *kvm,
 	get_tod_clock_ext((char *)&htod);
 
 	gtod->tod = htod.tod + kvm->arch.epoch;
-	gtod->epoch_idx = htod.epoch_idx + kvm->arch.epdx;
-
-	if (gtod->tod < htod.tod)
-		gtod->epoch_idx += 1;
+	gtod->epoch_idx = 0;
+	if (test_kvm_facility(kvm, 139)) {
+		gtod->epoch_idx = htod.epoch_idx + kvm->arch.epdx;
+		if (gtod->tod < htod.tod)
+			gtod->epoch_idx += 1;
+	}
 
 	preempt_enable();
 }
@@ -1063,12 +1065,7 @@ static int kvm_s390_get_tod_ext(struct kvm *kvm, struct kvm_device_attr *attr)
 	struct kvm_s390_vm_tod_clock gtod;
 
 	memset(&gtod, 0, sizeof(gtod));
-
-	if (test_kvm_facility(kvm, 139))
-		kvm_s390_get_tod_clock_ext(kvm, &gtod);
-	else
-		gtod.tod = kvm_s390_get_tod_clock_fast(kvm);
-
+	kvm_s390_get_tod_clock(kvm, &gtod);
 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
 		return -EFAULT;
 

commit 9ac96d759fa2de2386a4fccab80880f99d1161d2
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Apr 27 14:36:12 2018 +0200

    KVM: s390: no need to inititalize kvm->arch members to 0
    
    KVM is allocated with kzalloc(), so these members are already 0.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 60bb3b7243d9..fd7ce3ab45eb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1989,10 +1989,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	rc = -ENOMEM;
 
-	kvm->arch.use_esca = 0; /* start with basic SCA */
 	if (!sclp.has_64bscao)
 		alloc_flags |= GFP_DMA;
 	rwlock_init(&kvm->arch.sca_lock);
+	/* start with basic SCA */
 	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(alloc_flags);
 	if (!kvm->arch.sca)
 		goto out_err;
@@ -2043,8 +2043,6 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm_s390_crypto_init(kvm);
 
 	mutex_init(&kvm->arch.float_int.ais_lock);
-	kvm->arch.float_int.simm = 0;
-	kvm->arch.float_int.nimm = 0;
 	spin_lock_init(&kvm->arch.float_int.lock);
 	for (i = 0; i < FIRQ_LIST_COUNT; i++)
 		INIT_LIST_HEAD(&kvm->arch.float_int.lists[i]);
@@ -2070,12 +2068,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		kvm->arch.gmap->pfault_enabled = 0;
 	}
 
-	kvm->arch.css_support = 0;
-	kvm->arch.use_irqchip = 0;
 	kvm->arch.use_pfmfi = sclp.has_pfmfi;
 	kvm->arch.use_skf = sclp.has_skey;
-	kvm->arch.epoch = 0;
-
 	spin_lock_init(&kvm->arch.start_stop_lock);
 	kvm_s390_vsie_init(kvm);
 	kvm_s390_gisa_init(kvm);

commit b9224cd7381aea7380e230d7488d8672143600e4
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Apr 30 17:55:24 2018 +0200

    KVM: s390: introduce defines for control registers
    
    In KVM code we use masks to test/set control registers.
    
    Let's define the ones we use in arch/s390/include/asm/ctl_reg.h and
    replace all occurrences in KVM code.
    
    As we will be needing the define for Clock-comparator sign control soon,
    let's also add it.
    
    Suggested-by: Collin L. Walling <walling@linux.ibm.com>
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Collin Walling <walling@linux.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Janosch Frank <frankja@linux.ibm.com>
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d9799946722e..60bb3b7243d9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2441,8 +2441,12 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->ckc       = 0UL;
 	vcpu->arch.sie_block->todpr     = 0;
 	memset(vcpu->arch.sie_block->gcr, 0, 16 * sizeof(__u64));
-	vcpu->arch.sie_block->gcr[0]  = 0xE0UL;
-	vcpu->arch.sie_block->gcr[14] = 0xC2000000UL;
+	vcpu->arch.sie_block->gcr[0]  = CR0_UNUSED_56 |
+					CR0_INTERRUPT_KEY_SUBMASK |
+					CR0_MEASUREMENT_ALERT_SUBMASK;
+	vcpu->arch.sie_block->gcr[14] = CR14_UNUSED_32 |
+					CR14_UNUSED_33 |
+					CR14_EXTERNAL_DAMAGE_SUBMASK;
 	/* make sure the new fpc will be lazily loaded */
 	save_fpu_regs();
 	current->thread.fpu.fpc = 0;
@@ -3200,7 +3204,7 @@ static int kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu)
 		return 0;
 	if (kvm_s390_vcpu_has_irq(vcpu, 0))
 		return 0;
-	if (!(vcpu->arch.sie_block->gcr[0] & 0x200ul))
+	if (!(vcpu->arch.sie_block->gcr[0] & CR0_SERVICE_SIGNAL_SUBMASK))
 		return 0;
 	if (!vcpu->arch.gmap->pfault_enabled)
 		return 0;

commit 20c922f04b17aa51a75e514eca8fcbfa337a002d
Author: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
Date:   Sun Apr 22 11:37:03 2018 -0400

    KVM: s390: reset crypto attributes for all vcpus
    
    Introduces a new function to reset the crypto attributes for all
    vcpus whether they are running or not. Each vcpu in KVM will
    be removed from SIE prior to resetting the crypto attributes in its
    SIE state description. After all vcpus have had their crypto attributes
    reset the vcpus will be restored to SIE.
    
    This function is incorporated into the kvm_s390_vm_set_crypto(kvm)
    function to fix a reported issue whereby the crypto key wrapping
    attributes could potentially get out of synch for running vcpus.
    
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reported-by: Halil Pasic <pasic@linux.vnet.ibm.com>
    Signed-off-by: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 007db8faafa5..d9799946722e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -791,11 +791,21 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu);
 
-static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
+void kvm_s390_vcpu_crypto_reset_all(struct kvm *kvm)
 {
 	struct kvm_vcpu *vcpu;
 	int i;
 
+	kvm_s390_vcpu_block_all(kvm);
+
+	kvm_for_each_vcpu(i, vcpu, kvm)
+		kvm_s390_vcpu_crypto_setup(vcpu);
+
+	kvm_s390_vcpu_unblock_all(kvm);
+}
+
+static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
+{
 	if (!test_kvm_facility(kvm, 76))
 		return -EINVAL;
 
@@ -832,10 +842,7 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 		return -ENXIO;
 	}
 
-	kvm_for_each_vcpu(i, vcpu, kvm) {
-		kvm_s390_vcpu_crypto_setup(vcpu);
-		exit_sie(vcpu);
-	}
+	kvm_s390_vcpu_crypto_reset_all(kvm);
 	mutex_unlock(&kvm->lock);
 	return 0;
 }

commit 55531b7431db789766ac952391e95c170db48581
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Thu Feb 15 16:33:47 2018 +0100

    KVM: s390: Add storage key facility interpretation control
    
    Up to now we always expected to have the storage key facility
    available for our (non-VSIE) KVM guests. For huge page support, we
    need to be able to disable it, so let's introduce that now.
    
    We add the use_skf variable to manage KVM storage key facility
    usage. Also we rename use_skey in the mm context struct to uses_skeys
    to make it more clear that it is an indication that the vm actively
    uses storage keys.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: Farhan Ali <alifm@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 64c986243018..007db8faafa5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1493,7 +1493,7 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 		return -EINVAL;
 
 	/* Is this guest using storage keys? */
-	if (!mm_use_skey(current->mm))
+	if (!mm_uses_skeys(current->mm))
 		return KVM_S390_GET_SKEYS_NONE;
 
 	/* Enforce sane limit on memory allocation */
@@ -2066,6 +2066,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.css_support = 0;
 	kvm->arch.use_irqchip = 0;
 	kvm->arch.use_pfmfi = sclp.has_pfmfi;
+	kvm->arch.use_skf = sclp.has_skey;
 	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);

commit ccc40c53c07ba22cfe711cd625f9f0c77f272ae6
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Mar 8 12:48:05 2018 +0000

    KVM: s390: provide counters for all interrupt injects/delivery
    
    For testing the exitless interrupt support it turned out useful to
    have separate counters for inject and delivery of I/O interrupt.
    While at it do the same for all interrupt types. For timer
    related interrupts (clock comparator and cpu timer) we even had
    no delivery counters. Fix this as well. On this way some counters
    are being renamed to have a similar name.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 41615c900348..64c986243018 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -57,6 +57,7 @@
 			   (KVM_MAX_VCPUS + LOCAL_IRQS))
 
 #define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
+#define VM_STAT(x) offsetof(struct kvm, stat.x), KVM_STAT_VM
 
 struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "userspace_handled", VCPU_STAT(exit_userspace) },
@@ -79,17 +80,34 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
 	{ "instruction_stctl", VCPU_STAT(instruction_stctl) },
 	{ "instruction_stctg", VCPU_STAT(instruction_stctg) },
+	{ "deliver_ckc", VCPU_STAT(deliver_ckc) },
+	{ "deliver_cputm", VCPU_STAT(deliver_cputm) },
 	{ "deliver_emergency_signal", VCPU_STAT(deliver_emergency_signal) },
 	{ "deliver_external_call", VCPU_STAT(deliver_external_call) },
 	{ "deliver_service_signal", VCPU_STAT(deliver_service_signal) },
-	{ "deliver_virtio_interrupt", VCPU_STAT(deliver_virtio_interrupt) },
+	{ "deliver_virtio", VCPU_STAT(deliver_virtio) },
 	{ "deliver_stop_signal", VCPU_STAT(deliver_stop_signal) },
 	{ "deliver_prefix_signal", VCPU_STAT(deliver_prefix_signal) },
 	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
-	{ "deliver_program_interruption", VCPU_STAT(deliver_program_int) },
-	{ "deliver_io_interrupt", VCPU_STAT(deliver_io_int) },
+	{ "deliver_program", VCPU_STAT(deliver_program) },
+	{ "deliver_io", VCPU_STAT(deliver_io) },
 	{ "deliver_machine_check", VCPU_STAT(deliver_machine_check) },
 	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
+	{ "inject_ckc", VCPU_STAT(inject_ckc) },
+	{ "inject_cputm", VCPU_STAT(inject_cputm) },
+	{ "inject_external_call", VCPU_STAT(inject_external_call) },
+	{ "inject_float_mchk", VM_STAT(inject_float_mchk) },
+	{ "inject_emergency_signal", VCPU_STAT(inject_emergency_signal) },
+	{ "inject_io", VM_STAT(inject_io) },
+	{ "inject_mchk", VCPU_STAT(inject_mchk) },
+	{ "inject_pfault_done", VM_STAT(inject_pfault_done) },
+	{ "inject_program", VCPU_STAT(inject_program) },
+	{ "inject_restart", VCPU_STAT(inject_restart) },
+	{ "inject_service_signal", VM_STAT(inject_service_signal) },
+	{ "inject_set_prefix", VCPU_STAT(inject_set_prefix) },
+	{ "inject_stop_signal", VCPU_STAT(inject_stop_signal) },
+	{ "inject_pfault_init", VCPU_STAT(inject_pfault_init) },
+	{ "inject_virtio", VM_STAT(inject_virtio) },
 	{ "instruction_epsw", VCPU_STAT(instruction_epsw) },
 	{ "instruction_gs", VCPU_STAT(instruction_gs) },
 	{ "instruction_io_other", VCPU_STAT(instruction_io_other) },

commit 32de0749097ea5be5af7f06bc26cf4e06958ac0f
Author: QingFeng Hao <haoqf@linux.vnet.ibm.com>
Date:   Fri Mar 2 11:56:47 2018 +0100

    KVM: add machine check counter to kvm_stat
    
    This counter can be used for administration, debug or test purposes.
    
    Suggested-by: Vladislav Mironov <mironov@de.ibm.com>
    Signed-off-by: QingFeng Hao <haoqf@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 41adcfe169a9..41615c900348 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -88,6 +88,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
 	{ "deliver_program_interruption", VCPU_STAT(deliver_program_int) },
 	{ "deliver_io_interrupt", VCPU_STAT(deliver_io_int) },
+	{ "deliver_machine_check", VCPU_STAT(deliver_machine_check) },
 	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
 	{ "instruction_epsw", VCPU_STAT(instruction_epsw) },
 	{ "instruction_gs", VCPU_STAT(instruction_gs) },

commit a5e0acea9ed644d622281a4a8fde94a2e24ec7e8
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Feb 23 07:57:33 2018 +0000

    KVM: s390: add exit io request stats and simplify code
    
    We want to count IO exit requests in kvm_stat. At the same time
    we can get rid of the handle_noop function.
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 957be1c51a4b..41adcfe169a9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -64,6 +64,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_validity", VCPU_STAT(exit_validity) },
 	{ "exit_stop_request", VCPU_STAT(exit_stop_request) },
 	{ "exit_external_request", VCPU_STAT(exit_external_request) },
+	{ "exit_io_request", VCPU_STAT(exit_io_request) },
 	{ "exit_external_interrupt", VCPU_STAT(exit_external_interrupt) },
 	{ "exit_instruction", VCPU_STAT(exit_instruction) },
 	{ "exit_pei", VCPU_STAT(exit_pei) },

commit c9f0a2b87f9e95197c2f81920a82bcef6ad3db99
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Fri Feb 16 12:16:14 2018 +0100

    KVM: s390: Refactor host cmma and pfmfi interpretation controls
    
    use_cmma in kvm_arch means that the KVM hypervisor is allowed to use
    cmma, whereas use_cmma in the mm context means cmm has been used before.
    Let's rename the context one to uses_cmm, as the vm does use
    collaborative memory management but the host uses the cmm assist
    (interpretation facility).
    
    Also let's introduce use_pfmfi, so we can remove the pfmfi disablement
    when we activate cmma and rather not activate it in the first place.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Message-Id: <1518779775-256056-2-git-send-email-frankja@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 56061cf82f01..957be1c51a4b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -699,6 +699,8 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		mutex_lock(&kvm->lock);
 		if (!kvm->created_vcpus) {
 			kvm->arch.use_cmma = 1;
+			/* Not compatible with cmma. */
+			kvm->arch.use_pfmfi = 0;
 			ret = 0;
 		}
 		mutex_unlock(&kvm->lock);
@@ -1603,7 +1605,7 @@ static int kvm_s390_get_cmma_bits(struct kvm *kvm,
 		return -EINVAL;
 	/* CMMA is disabled or was not used, or the buffer has length zero */
 	bufsize = min(args->count, KVM_S390_CMMA_SIZE_MAX);
-	if (!bufsize || !kvm->mm->context.use_cmma) {
+	if (!bufsize || !kvm->mm->context.uses_cmm) {
 		memset(args, 0, sizeof(*args));
 		return 0;
 	}
@@ -1680,7 +1682,7 @@ static int kvm_s390_get_cmma_bits(struct kvm *kvm,
 /*
  * This function sets the CMMA attributes for the given pages. If the input
  * buffer has zero length, no action is taken, otherwise the attributes are
- * set and the mm->context.use_cmma flag is set.
+ * set and the mm->context.uses_cmm flag is set.
  */
 static int kvm_s390_set_cmma_bits(struct kvm *kvm,
 				  const struct kvm_s390_cmma_log *args)
@@ -1730,9 +1732,9 @@ static int kvm_s390_set_cmma_bits(struct kvm *kvm,
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
 	up_read(&kvm->mm->mmap_sem);
 
-	if (!kvm->mm->context.use_cmma) {
+	if (!kvm->mm->context.uses_cmm) {
 		down_write(&kvm->mm->mmap_sem);
-		kvm->mm->context.use_cmma = 1;
+		kvm->mm->context.uses_cmm = 1;
 		up_write(&kvm->mm->mmap_sem);
 	}
 out:
@@ -2043,6 +2045,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm->arch.css_support = 0;
 	kvm->arch.use_irqchip = 0;
+	kvm->arch.use_pfmfi = sclp.has_pfmfi;
 	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
@@ -2469,8 +2472,6 @@ int kvm_s390_vcpu_setup_cmma(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->cbrlo = get_zeroed_page(GFP_KERNEL);
 	if (!vcpu->arch.sie_block->cbrlo)
 		return -ENOMEM;
-
-	vcpu->arch.sie_block->ecb2 &= ~ECB2_PFMFI;
 	return 0;
 }
 
@@ -2506,7 +2507,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	if (test_kvm_facility(vcpu->kvm, 73))
 		vcpu->arch.sie_block->ecb |= ECB_TE;
 
-	if (test_kvm_facility(vcpu->kvm, 8) && sclp.has_pfmfi)
+	if (test_kvm_facility(vcpu->kvm, 8) && vcpu->kvm->arch.use_pfmfi)
 		vcpu->arch.sie_block->ecb2 |= ECB2_PFMFI;
 	if (test_kvm_facility(vcpu->kvm, 130))
 		vcpu->arch.sie_block->ecb2 |= ECB2_IEP;
@@ -3038,7 +3039,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 
 	if (kvm_check_request(KVM_REQ_START_MIGRATION, vcpu)) {
 		/*
-		 * Disable CMMA virtualization; we will emulate the ESSA
+		 * Disable CMM virtualization; we will emulate the ESSA
 		 * instruction manually, in order to provide additional
 		 * functionalities needed for live migration.
 		 */
@@ -3048,11 +3049,11 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 
 	if (kvm_check_request(KVM_REQ_STOP_MIGRATION, vcpu)) {
 		/*
-		 * Re-enable CMMA virtualization if CMMA is available and
-		 * was used.
+		 * Re-enable CMM virtualization if CMMA is available and
+		 * CMM has been used.
 		 */
 		if ((vcpu->kvm->arch.use_cmma) &&
-		    (vcpu->kvm->mm->context.use_cmma))
+		    (vcpu->kvm->mm->context.uses_cmm))
 			vcpu->arch.sie_block->ecb2 |= ECB2_CMMA;
 		goto retry;
 	}

commit c3b9e3e1ea1c1d1524b56b6734711db2a6fc2163
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Feb 9 16:26:29 2018 +0000

    KVM: s390: implement CPU model only facilities
    
    Some facilities should only be provided to the guest, if they are
    enabled by a CPU model. This allows us to avoid capabilities and
    to simply fall back to the cpumodel for deciding about a facility
    without enabling it for older QEMUs or QEMUs without a CPU
    model.
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 339ac0964590..56061cf82f01 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -152,13 +152,33 @@ static int nested;
 module_param(nested, int, S_IRUGO);
 MODULE_PARM_DESC(nested, "Nested virtualization support");
 
-/* upper facilities limit for kvm */
-unsigned long kvm_s390_fac_list_mask[16] = { FACILITIES_KVM };
 
-unsigned long kvm_s390_fac_list_mask_size(void)
+/*
+ * For now we handle at most 16 double words as this is what the s390 base
+ * kernel handles and stores in the prefix page. If we ever need to go beyond
+ * this, this requires changes to code, but the external uapi can stay.
+ */
+#define SIZE_INTERNAL 16
+
+/*
+ * Base feature mask that defines default mask for facilities. Consists of the
+ * defines in FACILITIES_KVM and the non-hypervisor managed bits.
+ */
+static unsigned long kvm_s390_fac_base[SIZE_INTERNAL] = { FACILITIES_KVM };
+/*
+ * Extended feature mask. Consists of the defines in FACILITIES_KVM_CPUMODEL
+ * and defines the facilities that can be enabled via a cpu model.
+ */
+static unsigned long kvm_s390_fac_ext[SIZE_INTERNAL] = { FACILITIES_KVM_CPUMODEL };
+
+static unsigned long kvm_s390_fac_size(void)
 {
-	BUILD_BUG_ON(ARRAY_SIZE(kvm_s390_fac_list_mask) > S390_ARCH_FAC_MASK_SIZE_U64);
-	return ARRAY_SIZE(kvm_s390_fac_list_mask);
+	BUILD_BUG_ON(SIZE_INTERNAL > S390_ARCH_FAC_MASK_SIZE_U64);
+	BUILD_BUG_ON(SIZE_INTERNAL > S390_ARCH_FAC_LIST_SIZE_U64);
+	BUILD_BUG_ON(SIZE_INTERNAL * sizeof(unsigned long) >
+		sizeof(S390_lowcore.stfle_fac_list));
+
+	return SIZE_INTERNAL;
 }
 
 /* available cpu features supported by kvm */
@@ -1967,20 +1987,15 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.sie_page2)
 		goto out_err;
 
-	/* Populate the facility mask initially. */
-	memcpy(kvm->arch.model.fac_mask, S390_lowcore.stfle_fac_list,
-	       sizeof(S390_lowcore.stfle_fac_list));
-	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
-		if (i < kvm_s390_fac_list_mask_size())
-			kvm->arch.model.fac_mask[i] &= kvm_s390_fac_list_mask[i];
-		else
-			kvm->arch.model.fac_mask[i] = 0UL;
-	}
-
-	/* Populate the facility list initially. */
 	kvm->arch.model.fac_list = kvm->arch.sie_page2->fac_list;
-	memcpy(kvm->arch.model.fac_list, kvm->arch.model.fac_mask,
-	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+
+	for (i = 0; i < kvm_s390_fac_size(); i++) {
+		kvm->arch.model.fac_mask[i] = S390_lowcore.stfle_fac_list[i] &
+					      (kvm_s390_fac_base[i] |
+					       kvm_s390_fac_ext[i]);
+		kvm->arch.model.fac_list[i] = S390_lowcore.stfle_fac_list[i] &
+					      kvm_s390_fac_base[i];
+	}
 
 	/* we are always in czam mode - even on pre z14 machines */
 	set_kvm_facility(kvm->arch.model.fac_mask, 138);
@@ -4044,7 +4059,7 @@ static int __init kvm_s390_init(void)
 	}
 
 	for (i = 0; i < 16; i++)
-		kvm_s390_fac_list_mask[i] |=
+		kvm_s390_fac_base[i] |=
 			S390_lowcore.stfle_fac_list[i] & nonhyp_mask(i);
 
 	return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);

commit f07afa0462b76a5b9c4f3a43d5ac24fdb86a90c2
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Mar 6 14:27:58 2018 +0100

    KVM: s390: fix memory overwrites when not using SCA entries
    
    Even if we don't have extended SCA support, we can have more than 64 CPUs
    if we don't enable any HW features that might use the SCA entries.
    
    Now, this works just fine, but we missed a return, which is why we
    would actually store the SCA entries. If we have more than 64 CPUs, this
    means writing outside of the basic SCA - bad.
    
    Let's fix this. This allows > 64 CPUs when running nested (under vSIE)
    without random crashes.
    
    Fixes: a6940674c384 ("KVM: s390: allow 255 VCPUs when sca entries aren't used")
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180306132758.21034-1-david@redhat.com>
    Cc: stable@vger.kernel.org
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index df19f158347e..339ac0964590 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2147,6 +2147,7 @@ static void sca_add_vcpu(struct kvm_vcpu *vcpu)
 		/* we still need the basic sca for the ipte control */
 		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
+		return;
 	}
 	read_lock(&vcpu->kvm->arch.sca_lock);
 	if (vcpu->kvm->arch.use_esca) {

commit 09a0fb67536a49af19f2bfc632100e9de91fe526
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Feb 28 18:44:34 2018 +0000

    KVM: s390: provide io interrupt kvm_stat
    
    We already count io interrupts, but we forgot to print them.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Fixes: d8346b7d9b ("KVM: s390: Support for I/O interrupts.")
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 77d7818130db..df19f158347e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -86,6 +86,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "deliver_prefix_signal", VCPU_STAT(deliver_prefix_signal) },
 	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
 	{ "deliver_program_interruption", VCPU_STAT(deliver_program_int) },
+	{ "deliver_io_interrupt", VCPU_STAT(deliver_io_int) },
 	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
 	{ "instruction_epsw", VCPU_STAT(instruction_epsw) },
 	{ "instruction_gs", VCPU_STAT(instruction_gs) },

commit 0e7def5fb0dc53ddbb9f62a497d15f1e11ccdc36
Author: David Hildenbrand <david@redhat.com>
Date:   Wed Feb 7 12:46:43 2018 +0100

    KVM: s390: provide only a single function for setting the tod (fix SCK)
    
    Right now, SET CLOCK called in the guest does not properly take care of
    the epoch index, as the call goes via the old kvm_s390_set_tod_clock()
    interface. So the epoch index is neither reset to 0, if required, nor
    properly set to e.g. 0xff on negative values.
    
    Fix this by providing a single kvm_s390_set_tod_clock() function. Move
    Multiple-epoch facility handling into it.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180207114647.6220-3-david@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Fixes: 8fa1696ea781 ("KVM: s390: Multiple Epoch Facility support")
    Cc: stable@vger.kernel.org
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b07aa16dcf06..77d7818130db 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -928,12 +928,9 @@ static int kvm_s390_set_tod_ext(struct kvm *kvm, struct kvm_device_attr *attr)
 	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
 		return -EFAULT;
 
-	if (test_kvm_facility(kvm, 139))
-		kvm_s390_set_tod_clock_ext(kvm, &gtod);
-	else if (gtod.epoch_idx == 0)
-		kvm_s390_set_tod_clock(kvm, gtod.tod);
-	else
+	if (!test_kvm_facility(kvm, 139) && gtod.epoch_idx)
 		return -EINVAL;
+	kvm_s390_set_tod_clock(kvm, &gtod);
 
 	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x, TOD base: 0x%llx",
 		gtod.epoch_idx, gtod.tod);
@@ -958,13 +955,14 @@ static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 
 static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 {
-	u64 gtod;
+	struct kvm_s390_vm_tod_clock gtod = { 0 };
 
-	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
+	if (copy_from_user(&gtod.tod, (void __user *)attr->addr,
+			   sizeof(gtod.tod)))
 		return -EFAULT;
 
-	kvm_s390_set_tod_clock(kvm, gtod);
-	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx", gtod);
+	kvm_s390_set_tod_clock(kvm, &gtod);
+	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx", gtod.tod);
 	return 0;
 }
 
@@ -3048,8 +3046,8 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-void kvm_s390_set_tod_clock_ext(struct kvm *kvm,
-				 const struct kvm_s390_vm_tod_clock *gtod)
+void kvm_s390_set_tod_clock(struct kvm *kvm,
+			    const struct kvm_s390_vm_tod_clock *gtod)
 {
 	struct kvm_vcpu *vcpu;
 	struct kvm_s390_tod_clock_ext htod;
@@ -3061,10 +3059,12 @@ void kvm_s390_set_tod_clock_ext(struct kvm *kvm,
 	get_tod_clock_ext((char *)&htod);
 
 	kvm->arch.epoch = gtod->tod - htod.tod;
-	kvm->arch.epdx = gtod->epoch_idx - htod.epoch_idx;
-
-	if (kvm->arch.epoch > gtod->tod)
-		kvm->arch.epdx -= 1;
+	kvm->arch.epdx = 0;
+	if (test_kvm_facility(kvm, 139)) {
+		kvm->arch.epdx = gtod->epoch_idx - htod.epoch_idx;
+		if (kvm->arch.epoch > gtod->tod)
+			kvm->arch.epdx -= 1;
+	}
 
 	kvm_s390_vcpu_block_all(kvm);
 	kvm_for_each_vcpu(i, vcpu, kvm) {
@@ -3077,22 +3077,6 @@ void kvm_s390_set_tod_clock_ext(struct kvm *kvm,
 	mutex_unlock(&kvm->lock);
 }
 
-void kvm_s390_set_tod_clock(struct kvm *kvm, u64 tod)
-{
-	struct kvm_vcpu *vcpu;
-	int i;
-
-	mutex_lock(&kvm->lock);
-	preempt_disable();
-	kvm->arch.epoch = tod - get_tod_clock();
-	kvm_s390_vcpu_block_all(kvm);
-	kvm_for_each_vcpu(i, vcpu, kvm)
-		vcpu->arch.sie_block->epoch = kvm->arch.epoch;
-	kvm_s390_vcpu_unblock_all(kvm);
-	preempt_enable();
-	mutex_unlock(&kvm->lock);
-}
-
 /**
  * kvm_arch_fault_in_page - fault-in guest page if necessary
  * @vcpu: The corresponding virtual cpu

commit 1575767ef3cf5326701d2ae3075b7732cbc855e4
Author: David Hildenbrand <david@redhat.com>
Date:   Wed Feb 7 12:46:45 2018 +0100

    KVM: s390: consider epoch index on TOD clock syncs
    
    For now, we don't take care of over/underflows. Especially underflows
    are critical:
    
    Assume the epoch is currently 0 and we get a sync request for delta=1,
    meaning the TOD is moved forward by 1 and we have to fix it up by
    subtracting 1 from the epoch. Right now, this will leave the epoch
    index untouched, resulting in epoch=-1, epoch_idx=0, which is wrong.
    
    We have to take care of over and underflows, also for the VSIE case. So
    let's factor out calculation into a separate function.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180207114647.6220-5-david@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Fixes: 8fa1696ea781 ("KVM: s390: Multiple Epoch Facility support")
    Cc: stable@vger.kernel.org
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [use u8 for idx]

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5b7fe80cda56..b07aa16dcf06 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -179,6 +179,28 @@ int kvm_arch_hardware_enable(void)
 static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
 			      unsigned long end);
 
+static void kvm_clock_sync_scb(struct kvm_s390_sie_block *scb, u64 delta)
+{
+	u8 delta_idx = 0;
+
+	/*
+	 * The TOD jumps by delta, we have to compensate this by adding
+	 * -delta to the epoch.
+	 */
+	delta = -delta;
+
+	/* sign-extension - we're adding to signed values below */
+	if ((s64)delta < 0)
+		delta_idx = -1;
+
+	scb->epoch += delta;
+	if (scb->ecd & ECD_MEF) {
+		scb->epdx += delta_idx;
+		if (scb->epoch < delta)
+			scb->epdx += 1;
+	}
+}
+
 /*
  * This callback is executed during stop_machine(). All CPUs are therefore
  * temporarily stopped. In order not to change guest behavior, we have to
@@ -194,13 +216,17 @@ static int kvm_clock_sync(struct notifier_block *notifier, unsigned long val,
 	unsigned long long *delta = v;
 
 	list_for_each_entry(kvm, &vm_list, vm_list) {
-		kvm->arch.epoch -= *delta;
 		kvm_for_each_vcpu(i, vcpu, kvm) {
-			vcpu->arch.sie_block->epoch -= *delta;
+			kvm_clock_sync_scb(vcpu->arch.sie_block, *delta);
+			if (i == 0) {
+				kvm->arch.epoch = vcpu->arch.sie_block->epoch;
+				kvm->arch.epdx = vcpu->arch.sie_block->epdx;
+			}
 			if (vcpu->arch.cputm_enabled)
 				vcpu->arch.cputm_start += *delta;
 			if (vcpu->arch.vsie_block)
-				vcpu->arch.vsie_block->epoch -= *delta;
+				kvm_clock_sync_scb(vcpu->arch.vsie_block,
+						   *delta);
 		}
 	}
 	return NOTIFY_OK;

commit d16b52cb9cdb6f06dea8ab2f0a428e7d7f0b0a81
Author: David Hildenbrand <david@redhat.com>
Date:   Wed Feb 7 12:46:44 2018 +0100

    KVM: s390: consider epoch index on hotplugged CPUs
    
    We must copy both, the epoch and the epoch_idx.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180207114647.6220-4-david@redhat.com>
    Fixes: 8fa1696ea781 ("KVM: s390: Multiple Epoch Facility support")
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Fixes: 8fa1696ea781 ("KVM: s390: Multiple Epoch Facility support")
    Cc: stable@vger.kernel.org
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ba4c7092335a..5b7fe80cda56 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2389,6 +2389,7 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 	mutex_lock(&vcpu->kvm->lock);
 	preempt_disable();
 	vcpu->arch.sie_block->epoch = vcpu->kvm->arch.epoch;
+	vcpu->arch.sie_block->epdx = vcpu->kvm->arch.epdx;
 	preempt_enable();
 	mutex_unlock(&vcpu->kvm->lock);
 	if (!kvm_is_ucontrol(vcpu->kvm)) {

commit 7bf14c28ee776be567855bd39ed8ff795ea19f55
Merge: 87cedc6be559 5fa4ec9cb2e6
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Thu Feb 1 15:04:17 2018 +0100

    Merge branch 'x86/hyperv' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Topic branch for stable KVM clockource under Hyper-V.
    
    Thanks to Christoffer Dall for resolving the ARM conflict.

commit 92ea2b3381d7dc4223375f4337dcadb1fd556576
Merge: d7231e75f73f 4b9f952577fb
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Tue Jan 30 17:42:40 2018 +0100

    Merge tag 'kvm-s390-next-4.16-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
    
    KVM: s390: Fixes and features for 4.16 part 2
    
    - exitless interrupts for emulated devices (Michael Mueller)
    - cleanup of cpuflag handling (David Hildenbrand)
    - kvm stat counter improvements (Christian Borntraeger)
    - vsie improvements (David Hildenbrand)
    - mm cleanup (Janosch Frank)

commit 4b9f952577fb40875a2a163d80515a8daa0d6bef
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Fri Jun 23 13:51:25 2017 +0200

    KVM: s390: introduce the format-1 GISA
    
    The patch modifies the previously defined GISA data structure to be
    able to store two GISA formats, format-0 and format-1. Additionally,
    it verifies the availability of the GISA format facility and enables
    the use of a format-1 GISA in the SIE control block accordingly.
    
    A format-1 can do everything that format-0 can and we will need it
    for real HW passthrough. As there are systems with only format-0
    we keep both variants.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2c5e25b39435..f85a405e2595 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2531,6 +2531,8 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.sie_block->icpua = id;
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	vcpu->arch.sie_block->gd = (u32)(u64)kvm->arch.gisa;
+	if (vcpu->arch.sie_block->gd && sclp.has_gisaf)
+		vcpu->arch.sie_block->gd |= GISA_FORMAT1;
 	seqcount_init(&vcpu->arch.cputm_seqcount);
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);

commit d7c5cb0105ddeff56694f4c6222ee7221824bad3
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Mon Jun 12 14:15:19 2017 +0200

    KVM: s390: exploit GISA and AIV for emulated interrupts
    
    The adapter interruption virtualization (AIV) facility is an
    optional facility that comes with functionality expected to increase
    the performance of adapter interrupt handling for both emulated and
    passed-through adapter interrupts. With AIV, adapter interrupts can be
    delivered to the guest without exiting SIE.
    
    This patch provides some preparations for using AIV for emulated adapter
    interrupts (including virtio) if it's available. When using AIV, the
    interrupts are delivered at the so called GISA by setting the bit
    corresponding to its Interruption Subclass (ISC) in the Interruption
    Pending Mask (IPM) instead of inserting a node into the floating interrupt
    list.
    
    To keep the change reasonably small, the handling of this new state is
    deferred in get_all_floating_irqs and handle_tpi. This patch concentrates
    on the code handling enqueuement of emulated adapter interrupts, and their
    delivery to the guest.
    
    Note that care is still required for adapter interrupts using AIV,
    because there is no guarantee that AIV is going to deliver the adapter
    interrupts pending at the GISA (consider all vcpus idle). When delivering
    GISA adapter interrupts by the host (usual mechanism) special attention
    is required to honor interrupt priorities.
    
    Empirical results show that the time window between making an interrupt
    pending at the GISA and doing kvm_s390_deliver_pending_interrupts is
    sufficient for a guest with at least moderate cpu activity to get adapter
    interrupts delivered within the SIE, and potentially save some SIE exits
    (if not other deliverable interrupts).
    
    The code will be activated with a follow-up patch.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2fbdb1601089..2c5e25b39435 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1999,6 +1999,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
 	kvm_s390_vsie_init(kvm);
+	kvm_s390_gisa_init(kvm);
 	KVM_EVENT(3, "vm 0x%pK created by pid %u", kvm, current->pid);
 
 	return 0;
@@ -2061,6 +2062,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_free_vcpus(kvm);
 	sca_dispose(kvm);
 	debug_unregister(kvm->arch.dbf);
+	kvm_s390_gisa_destroy(kvm);
 	free_page((unsigned long)kvm->arch.sie_page2);
 	if (!kvm_is_ucontrol(kvm))
 		gmap_remove(kvm->arch.gmap);
@@ -2471,6 +2473,11 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	if (test_kvm_facility(vcpu->kvm, 139))
 		vcpu->arch.sie_block->ecd |= ECD_MEF;
 
+	if (vcpu->arch.sie_block->gd) {
+		vcpu->arch.sie_block->eca |= ECA_AIV;
+		VCPU_EVENT(vcpu, 3, "AIV gisa format-%u enabled for cpu %03u",
+			   vcpu->arch.sie_block->gd & 0x3, vcpu->vcpu_id);
+	}
 	vcpu->arch.sie_block->sdnxo = ((unsigned long) &vcpu->run->s.regs.sdnx)
 					| SDNXC;
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
@@ -2523,6 +2530,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	vcpu->arch.sie_block->icpua = id;
 	spin_lock_init(&vcpu->arch.local_int.lock);
+	vcpu->arch.sie_block->gd = (u32)(u64)kvm->arch.gisa;
 	seqcount_init(&vcpu->arch.cputm_seqcount);
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);

commit 19114beb73f774e466d9e39b8e8b961812c9f881
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Tue May 30 14:26:02 2017 +0200

    KVM: s390: define GISA format-0 data structure
    
    In preperation to support pass-through adapter interrupts, the Guest
    Interruption State Area (GISA) and the Adapter Interruption Virtualization
    (AIV) features will be introduced here.
    
    This patch introduces format-0 GISA (that is defines the struct describing
    the GISA, allocates storage for it, and introduces fields for the
    GISA address in kvm_s390_sie_block and kvm_s390_vsie).
    
    As the GISA requires storage below 2GB, it is put in sie_page2, which is
    already allocated in ZONE_DMA. In addition, The GISA requires alignment to
    its integral boundary. This is already naturally aligned via the
    padding in the sie_page2.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.vnet.ibm.com>
    Reviewed-by: Halil Pasic <pasic@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3dd209de2e0c..2fbdb1601089 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1928,6 +1928,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.dbf)
 		goto out_err;
 
+	BUILD_BUG_ON(sizeof(struct sie_page2) != 4096);
 	kvm->arch.sie_page2 =
 	     (struct sie_page2 *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!kvm->arch.sie_page2)

commit 8d5fb0dc4ec069ea02395593e9b6b2b39a92457e
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Jan 23 18:05:31 2018 +0100

    KVM: s390: introduce and use kvm_s390_test_cpuflags()
    
    Use it just like kvm_s390_set_cpuflags() and kvm_s390_clear_cpuflags().
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180123170531.13687-5-david@redhat.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3eb97c4212cf..3dd209de2e0c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2875,7 +2875,7 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 
 static bool ibs_enabled(struct kvm_vcpu *vcpu)
 {
-	return atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_IBS;
+	return kvm_s390_test_cpuflags(vcpu, CPUSTAT_IBS);
 }
 
 static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)

commit 9daecfc66015530ee5d2d84cce5d341f0fffd0ab
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Jan 23 18:05:30 2018 +0100

    KVM: s390: introduce and use kvm_s390_clear_cpuflags()
    
    Use it just like kvm_s390_set_cpuflags().
    
    Suggested-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180123170531.13687-4-david@redhat.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9dc996acb52c..3eb97c4212cf 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2340,7 +2340,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	vcpu->cpu = -1;
 	if (vcpu->arch.cputm_enabled && !is_vcpu_idle(vcpu))
 		__stop_cpu_timer_accounting(vcpu);
-	atomic_andnot(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_clear_cpuflags(vcpu, CPUSTAT_RUNNING);
 	vcpu->arch.enabled_gmap = gmap_get_enabled();
 	gmap_disable(vcpu->arch.enabled_gmap);
 
@@ -2827,14 +2827,14 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 		if (dbg->control & KVM_GUESTDBG_USE_HW_BP)
 			rc = kvm_s390_import_bp_data(vcpu, dbg);
 	} else {
-		atomic_andnot(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+		kvm_s390_clear_cpuflags(vcpu, CPUSTAT_P);
 		vcpu->arch.guestdbg.last_bp = 0;
 	}
 
 	if (rc) {
 		vcpu->guest_debug = 0;
 		kvm_s390_clear_bp_data(vcpu);
-		atomic_andnot(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+		kvm_s390_clear_cpuflags(vcpu, CPUSTAT_P);
 	}
 
 	return rc;
@@ -2919,8 +2919,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	if (kvm_check_request(KVM_REQ_DISABLE_IBS, vcpu)) {
 		if (ibs_enabled(vcpu)) {
 			trace_kvm_s390_enable_disable_ibs(vcpu->vcpu_id, 0);
-			atomic_andnot(CPUSTAT_IBS,
-					  &vcpu->arch.sie_block->cpuflags);
+			kvm_s390_clear_cpuflags(vcpu, CPUSTAT_IBS);
 		}
 		goto retry;
 	}
@@ -3564,7 +3563,7 @@ void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 		__disable_ibs_on_all_vcpus(vcpu->kvm);
 	}
 
-	atomic_andnot(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_clear_cpuflags(vcpu, CPUSTAT_STOPPED);
 	/*
 	 * Another VCPU might have used IBS while we were offline.
 	 * Let's play safe and flush the VCPU at startup.

commit ef8f4f49fcfa56f7399db9886fcbb89f9f92a340
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Jan 23 18:05:29 2018 +0100

    KVM: s390: reuse kvm_s390_set_cpuflags()
    
    Use it in all places where we set cpuflags.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180123170531.13687-3-david@redhat.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8a3a1d50ef99..9dc996acb52c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2329,7 +2329,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 
 	gmap_enable(vcpu->arch.enabled_gmap);
-	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_set_cpuflags(vcpu, CPUSTAT_RUNNING);
 	if (vcpu->arch.cputm_enabled && !is_vcpu_idle(vcpu))
 		__start_cpu_timer_accounting(vcpu);
 	vcpu->cpu = cpu;
@@ -2436,9 +2436,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 						    CPUSTAT_STOPPED);
 
 	if (test_kvm_facility(vcpu->kvm, 78))
-		atomic_or(CPUSTAT_GED2, &vcpu->arch.sie_block->cpuflags);
+		kvm_s390_set_cpuflags(vcpu, CPUSTAT_GED2);
 	else if (test_kvm_facility(vcpu->kvm, 8))
-		atomic_or(CPUSTAT_GED, &vcpu->arch.sie_block->cpuflags);
+		kvm_s390_set_cpuflags(vcpu, CPUSTAT_GED);
 
 	kvm_s390_vcpu_setup_model(vcpu);
 
@@ -2475,7 +2475,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
 
 	if (sclp.has_kss)
-		atomic_or(CPUSTAT_KSS, &vcpu->arch.sie_block->cpuflags);
+		kvm_s390_set_cpuflags(vcpu, CPUSTAT_KSS);
 	else
 		vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 
@@ -2578,7 +2578,7 @@ static void kvm_s390_vcpu_request_handled(struct kvm_vcpu *vcpu)
  * return immediately. */
 void exit_sie(struct kvm_vcpu *vcpu)
 {
-	atomic_or(CPUSTAT_STOP_INT, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_set_cpuflags(vcpu, CPUSTAT_STOP_INT);
 	while (vcpu->arch.sie_block->prog0c & PROG_IN_SIE)
 		cpu_relax();
 }
@@ -2822,7 +2822,7 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 	if (dbg->control & KVM_GUESTDBG_ENABLE) {
 		vcpu->guest_debug = dbg->control;
 		/* enforce guest PER */
-		atomic_or(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+		kvm_s390_set_cpuflags(vcpu, CPUSTAT_P);
 
 		if (dbg->control & KVM_GUESTDBG_USE_HW_BP)
 			rc = kvm_s390_import_bp_data(vcpu, dbg);
@@ -2911,8 +2911,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	if (kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu)) {
 		if (!ibs_enabled(vcpu)) {
 			trace_kvm_s390_enable_disable_ibs(vcpu->vcpu_id, 1);
-			atomic_or(CPUSTAT_IBS,
-					&vcpu->arch.sie_block->cpuflags);
+			kvm_s390_set_cpuflags(vcpu, CPUSTAT_IBS);
 		}
 		goto retry;
 	}
@@ -3591,7 +3590,7 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 	/* SIGP STOP and SIGP STOP AND STORE STATUS has been fully processed */
 	kvm_s390_clear_stop_irq(vcpu);
 
-	atomic_or(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_set_cpuflags(vcpu, CPUSTAT_STOPPED);
 	__disable_ibs_on_vcpu(vcpu);
 
 	for (i = 0; i < online_vcpus; i++) {

commit a37cb07a30f0a181bc45c6970e486ac2992e9cde
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Jan 23 13:28:40 2018 +0100

    KVM: s390: add vcpu stat counters for many instruction
    
    The overall instruction counter is larger than the sum of the
    single counters. We should try to catch all instruction handlers
    to make this match the summary counter.
    Let us add sck,tb,sske,iske,rrbe,tb,tpi,tsch,lpsw,pswe....
    and remove other unused ones.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ce8ea8b94478..8a3a1d50ef99 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2,7 +2,7 @@
 /*
  * hosting IBM Z kernel virtual machines (s390x)
  *
- * Copyright IBM Corp. 2008, 2017
+ * Copyright IBM Corp. 2008, 2018
  *
  *    Author(s): Carsten Otte <cotte@de.ibm.com>
  *               Christian Borntraeger <borntraeger@de.ibm.com>
@@ -87,19 +87,31 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
 	{ "deliver_program_interruption", VCPU_STAT(deliver_program_int) },
 	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
+	{ "instruction_epsw", VCPU_STAT(instruction_epsw) },
+	{ "instruction_gs", VCPU_STAT(instruction_gs) },
+	{ "instruction_io_other", VCPU_STAT(instruction_io_other) },
+	{ "instruction_lpsw", VCPU_STAT(instruction_lpsw) },
+	{ "instruction_lpswe", VCPU_STAT(instruction_lpswe) },
 	{ "instruction_pfmf", VCPU_STAT(instruction_pfmf) },
+	{ "instruction_ptff", VCPU_STAT(instruction_ptff) },
 	{ "instruction_stidp", VCPU_STAT(instruction_stidp) },
+	{ "instruction_sck", VCPU_STAT(instruction_sck) },
+	{ "instruction_sckpf", VCPU_STAT(instruction_sckpf) },
 	{ "instruction_spx", VCPU_STAT(instruction_spx) },
 	{ "instruction_stpx", VCPU_STAT(instruction_stpx) },
 	{ "instruction_stap", VCPU_STAT(instruction_stap) },
-	{ "instruction_storage_key", VCPU_STAT(instruction_storage_key) },
+	{ "instruction_iske", VCPU_STAT(instruction_iske) },
+	{ "instruction_ri", VCPU_STAT(instruction_ri) },
+	{ "instruction_rrbe", VCPU_STAT(instruction_rrbe) },
+	{ "instruction_sske", VCPU_STAT(instruction_sske) },
 	{ "instruction_ipte_interlock", VCPU_STAT(instruction_ipte_interlock) },
-	{ "instruction_stsch", VCPU_STAT(instruction_stsch) },
-	{ "instruction_chsc", VCPU_STAT(instruction_chsc) },
 	{ "instruction_essa", VCPU_STAT(instruction_essa) },
 	{ "instruction_stsi", VCPU_STAT(instruction_stsi) },
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
+	{ "instruction_tb", VCPU_STAT(instruction_tb) },
+	{ "instruction_tpi", VCPU_STAT(instruction_tpi) },
 	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
+	{ "instruction_tsch", VCPU_STAT(instruction_tsch) },
 	{ "instruction_sthyi", VCPU_STAT(instruction_sthyi) },
 	{ "instruction_sie", VCPU_STAT(instruction_sie) },
 	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
@@ -124,6 +136,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_diag_258", VCPU_STAT(diagnose_258) },
 	{ "instruction_diag_308", VCPU_STAT(diagnose_308) },
 	{ "instruction_diag_500", VCPU_STAT(diagnose_500) },
+	{ "instruction_diag_other", VCPU_STAT(diagnose_other) },
 	{ NULL }
 };
 

commit bda646dd182a90ba4239fc62b71eb8b73126fa77
Merge: 0c5b9b5d9adb 1de1ea7efeb9
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Wed Jan 24 16:25:53 2018 +0100

    Merge tag 'kvm-s390-master-4.15-3' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
    
    KVM: s390: another fix for cmma migration
    
    This fixes races and potential use after free in the
    cmma migration code.

commit 866c138c32547b690e4cab3cd209c763508a95ab
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jan 24 12:27:01 2018 +0100

    KVM: s390: diagnoses are instructions as well
    
    Make the diagnose counters also appear as instruction counters.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index cc5a1b4d74b7..ce8ea8b94478 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -118,12 +118,12 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_sigp_cpu_reset", VCPU_STAT(instruction_sigp_cpu_reset) },
 	{ "instruction_sigp_init_cpu_reset", VCPU_STAT(instruction_sigp_init_cpu_reset) },
 	{ "instruction_sigp_unknown", VCPU_STAT(instruction_sigp_unknown) },
-	{ "diagnose_10", VCPU_STAT(diagnose_10) },
-	{ "diagnose_44", VCPU_STAT(diagnose_44) },
-	{ "diagnose_9c", VCPU_STAT(diagnose_9c) },
-	{ "diagnose_258", VCPU_STAT(diagnose_258) },
-	{ "diagnose_308", VCPU_STAT(diagnose_308) },
-	{ "diagnose_500", VCPU_STAT(diagnose_500) },
+	{ "instruction_diag_10", VCPU_STAT(diagnose_10) },
+	{ "instruction_diag_44", VCPU_STAT(diagnose_44) },
+	{ "instruction_diag_9c", VCPU_STAT(diagnose_9c) },
+	{ "instruction_diag_258", VCPU_STAT(diagnose_258) },
+	{ "instruction_diag_308", VCPU_STAT(diagnose_308) },
+	{ "instruction_diag_500", VCPU_STAT(diagnose_500) },
 	{ NULL }
 };
 

commit 1de1ea7efeb9e8543212210e34518b4049ccd285
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Dec 22 10:54:20 2017 +0100

    KVM: s390: add proper locking for CMMA migration bitmap
    
    Some parts of the cmma migration bitmap is already protected
    with the kvm->lock (e.g. the migration start). On the other
    hand the read of the cmma bits is not protected against a
    concurrent free, neither is the emulation of the ESSA instruction.
    Let's extend the locking to all related ioctls by using
    the slots lock for
    - kvm_s390_vm_start_migration
    - kvm_s390_vm_stop_migration
    - kvm_s390_set_cmma_bits
    - kvm_s390_get_cmma_bits
    
    In addition to that, we use synchronize_srcu before freeing
    the migration structure as all users hold kvm->srcu for read.
    (e.g. the ESSA handler).
    
    Reported-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: stable@vger.kernel.org # 4.13+
    Fixes: 190df4a212a7 (KVM: s390: CMMA tracking, ESSA emulation, migration mode)
    Reviewed-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index abcd24fdde3f..52880e980a33 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -766,7 +766,7 @@ static void kvm_s390_sync_request_broadcast(struct kvm *kvm, int req)
 
 /*
  * Must be called with kvm->srcu held to avoid races on memslots, and with
- * kvm->lock to avoid races with ourselves and kvm_s390_vm_stop_migration.
+ * kvm->slots_lock to avoid races with ourselves and kvm_s390_vm_stop_migration.
  */
 static int kvm_s390_vm_start_migration(struct kvm *kvm)
 {
@@ -822,7 +822,7 @@ static int kvm_s390_vm_start_migration(struct kvm *kvm)
 }
 
 /*
- * Must be called with kvm->lock to avoid races with ourselves and
+ * Must be called with kvm->slots_lock to avoid races with ourselves and
  * kvm_s390_vm_start_migration.
  */
 static int kvm_s390_vm_stop_migration(struct kvm *kvm)
@@ -837,6 +837,8 @@ static int kvm_s390_vm_stop_migration(struct kvm *kvm)
 
 	if (kvm->arch.use_cmma) {
 		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_STOP_MIGRATION);
+		/* We have to wait for the essa emulation to finish */
+		synchronize_srcu(&kvm->srcu);
 		vfree(mgs->pgste_bitmap);
 	}
 	kfree(mgs);
@@ -846,14 +848,12 @@ static int kvm_s390_vm_stop_migration(struct kvm *kvm)
 static int kvm_s390_vm_set_migration(struct kvm *kvm,
 				     struct kvm_device_attr *attr)
 {
-	int idx, res = -ENXIO;
+	int res = -ENXIO;
 
-	mutex_lock(&kvm->lock);
+	mutex_lock(&kvm->slots_lock);
 	switch (attr->attr) {
 	case KVM_S390_VM_MIGRATION_START:
-		idx = srcu_read_lock(&kvm->srcu);
 		res = kvm_s390_vm_start_migration(kvm);
-		srcu_read_unlock(&kvm->srcu, idx);
 		break;
 	case KVM_S390_VM_MIGRATION_STOP:
 		res = kvm_s390_vm_stop_migration(kvm);
@@ -861,7 +861,7 @@ static int kvm_s390_vm_set_migration(struct kvm *kvm,
 	default:
 		break;
 	}
-	mutex_unlock(&kvm->lock);
+	mutex_unlock(&kvm->slots_lock);
 
 	return res;
 }
@@ -1751,7 +1751,9 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = -EFAULT;
 		if (copy_from_user(&args, argp, sizeof(args)))
 			break;
+		mutex_lock(&kvm->slots_lock);
 		r = kvm_s390_get_cmma_bits(kvm, &args);
+		mutex_unlock(&kvm->slots_lock);
 		if (!r) {
 			r = copy_to_user(argp, &args, sizeof(args));
 			if (r)
@@ -1765,7 +1767,9 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = -EFAULT;
 		if (copy_from_user(&args, argp, sizeof(args)))
 			break;
+		mutex_lock(&kvm->slots_lock);
 		r = kvm_s390_set_cmma_bits(kvm, &args);
+		mutex_unlock(&kvm->slots_lock);
 		break;
 	}
 	default:

commit 35b3fde6203b932b2b1a5b53b3d8808abc9c4f60
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jan 17 14:44:34 2018 +0100

    KVM: s390: wire up bpb feature
    
    The new firmware interfaces for branch prediction behaviour changes
    are transparently available for the guest. Nevertheless, there is
    new state attached that should be migrated and properly resetted.
    Provide a mechanism for handling reset, migration and VSIE.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [Changed capability number to 152. - Radim]
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2c93cbbcd15e..2598cf243b86 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -421,6 +421,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_GS:
 		r = test_facility(133);
 		break;
+	case KVM_CAP_S390_BPB:
+		r = test_facility(82);
+		break;
 	default:
 		r = 0;
 	}
@@ -2198,6 +2201,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	kvm_s390_set_prefix(vcpu, 0);
 	if (test_kvm_facility(vcpu->kvm, 64))
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
+	if (test_kvm_facility(vcpu->kvm, 82))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_BPBC;
 	if (test_kvm_facility(vcpu->kvm, 133))
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_GSCB;
 	/* fprs can be synchronized via vrs, even if the guest has no vx. With
@@ -2339,6 +2344,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	current->thread.fpu.fpc = 0;
 	vcpu->arch.sie_block->gbea = 1;
 	vcpu->arch.sie_block->pp = 0;
+	vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
 	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm))
@@ -3298,6 +3304,11 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		vcpu->arch.sie_block->ecd |= ECD_HOSTREGMGMT;
 		vcpu->arch.gs_enabled = 1;
 	}
+	if ((kvm_run->kvm_dirty_regs & KVM_SYNC_BPBC) &&
+	    test_kvm_facility(vcpu->kvm, 82)) {
+		vcpu->arch.sie_block->fpf &= ~FPF_BPBC;
+		vcpu->arch.sie_block->fpf |= kvm_run->s.regs.bpbc ? FPF_BPBC : 0;
+	}
 	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	/* save host (userspace) fprs/vrs */
@@ -3344,6 +3355,7 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	kvm_run->s.regs.pft = vcpu->arch.pfault_token;
 	kvm_run->s.regs.pfs = vcpu->arch.pfault_select;
 	kvm_run->s.regs.pfc = vcpu->arch.pfault_compare;
+	kvm_run->s.regs.bpbc = (vcpu->arch.sie_block->fpf & FPF_BPBC) == FPF_BPBC;
 	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_access_regs(vcpu->arch.host_acrs);
 	/* Save guest register state */

commit 7cd918047a8bcb29501e6130e0e7eca98c78fe4d
Merge: 6b6977117f50 a9f6c9a92f37
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Tue Jan 16 16:41:27 2018 +0100

    Merge tag 'kvm-s390-next-4.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
    
    KVM: s390: Fixes and features for 4.16
    
    - add the virtio-ccw transport for kvmconfig
    - more debug tracing for cpu model
    - cleanups and fixes

commit a9f6c9a92f3771000493f6bbacbd7677b46d8706
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Jan 8 20:37:47 2018 +0100

    KVM: s390: cleanup struct kvm_s390_float_interrupt
    
    "wq" is not used at all. "cpuflags" can be access directly via the vcpu,
    just as "float_int" via vcpu->kvm.
    While at it, reuse _set_cpuflag() to make the code look nicer.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20180108193747.10818-1-david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d9f94bf24148..cc5a1b4d74b7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2509,9 +2509,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	vcpu->arch.sie_block->icpua = id;
 	spin_lock_init(&vcpu->arch.local_int.lock);
-	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
-	vcpu->arch.local_int.wq = &vcpu->wq;
-	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
 	seqcount_init(&vcpu->arch.cputm_seqcount);
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);

commit 2f8311c912ab67084ce3657096df601c87f49a58
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Nov 16 12:30:15 2017 +0100

    KVM: s390: add debug tracing for cpu features of CPU model
    
    The cpu model already traces the cpu facilities, the ibc and
    guest CPU ids. We should do the same for the cpu features (on
    success only).
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Halil Pasic <pasic@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f579bf5c2620..d9f94bf24148 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1084,7 +1084,6 @@ static int kvm_s390_set_processor_feat(struct kvm *kvm,
 				       struct kvm_device_attr *attr)
 {
 	struct kvm_s390_vm_cpu_feat data;
-	int ret = -EBUSY;
 
 	if (copy_from_user(&data, (void __user *)attr->addr, sizeof(data)))
 		return -EFAULT;
@@ -1094,13 +1093,18 @@ static int kvm_s390_set_processor_feat(struct kvm *kvm,
 		return -EINVAL;
 
 	mutex_lock(&kvm->lock);
-	if (!kvm->created_vcpus) {
-		bitmap_copy(kvm->arch.cpu_feat, (unsigned long *) data.feat,
-			    KVM_S390_VM_CPU_FEAT_NR_BITS);
-		ret = 0;
+	if (kvm->created_vcpus) {
+		mutex_unlock(&kvm->lock);
+		return -EBUSY;
 	}
+	bitmap_copy(kvm->arch.cpu_feat, (unsigned long *) data.feat,
+		    KVM_S390_VM_CPU_FEAT_NR_BITS);
 	mutex_unlock(&kvm->lock);
-	return ret;
+	VM_EVENT(kvm, 3, "SET: guest feat: 0x%16.16llx.0x%16.16llx.0x%16.16llx",
+			 data.feat[0],
+			 data.feat[1],
+			 data.feat[2]);
+	return 0;
 }
 
 static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
@@ -1202,6 +1206,10 @@ static int kvm_s390_get_processor_feat(struct kvm *kvm,
 		    KVM_S390_VM_CPU_FEAT_NR_BITS);
 	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
 		return -EFAULT;
+	VM_EVENT(kvm, 3, "GET: guest feat: 0x%16.16llx.0x%16.16llx.0x%16.16llx",
+			 data.feat[0],
+			 data.feat[1],
+			 data.feat[2]);
 	return 0;
 }
 
@@ -1215,6 +1223,10 @@ static int kvm_s390_get_machine_feat(struct kvm *kvm,
 		    KVM_S390_VM_CPU_FEAT_NR_BITS);
 	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
 		return -EFAULT;
+	VM_EVENT(kvm, 3, "GET: host feat:  0x%16.16llx.0x%16.16llx.0x%16.16llx",
+			 data.feat[0],
+			 data.feat[1],
+			 data.feat[2]);
 	return 0;
 }
 

commit 241e3ec0faf5ab1a0d9b1f6c43eefa919fb9c112
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Nov 16 15:12:52 2017 +0100

    KVM: s390: use created_vcpus in more places
    
    commit a03825bbd0c3 ("KVM: s390: use kvm->created_vcpus") introduced
    kvm->created_vcpus to avoid races with the existing kvm->online_vcpus
    scheme. One place was "forgotten" and one new place was "added".
    Let's fix those.
    
    Reported-by: Halil Pasic <pasic@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Halil Pasic <pasic@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Fixes: 4e0b1ab72b8a ("KVM: s390: gs support for kvm guests")
    Fixes: a03825bbd0c3 ("KVM: s390: use kvm->created_vcpus")

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ec8b68e97d3c..f579bf5c2620 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -573,7 +573,7 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 	case KVM_CAP_S390_GS:
 		r = -EINVAL;
 		mutex_lock(&kvm->lock);
-		if (atomic_read(&kvm->online_vcpus)) {
+		if (kvm->created_vcpus) {
 			r = -EBUSY;
 		} else if (test_facility(133)) {
 			set_kvm_facility(kvm->arch.model.fac_mask, 133);
@@ -1094,7 +1094,7 @@ static int kvm_s390_set_processor_feat(struct kvm *kvm,
 		return -EINVAL;
 
 	mutex_lock(&kvm->lock);
-	if (!atomic_read(&kvm->online_vcpus)) {
+	if (!kvm->created_vcpus) {
 		bitmap_copy(kvm->arch.cpu_feat, (unsigned long *) data.feat,
 			    KVM_S390_VM_CPU_FEAT_NR_BITS);
 		ret = 0;

commit bb4945e60dd0b5afb0e92bc8006ce560948fbc39
Merge: 0cb5b30698fd c2cf265d8608
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Sat Jan 6 17:26:37 2018 +0100

    Merge tag 'kvm-s390-master-4.15-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
    
    KVM: s390: fixes for cmma migration
    
    Two fixes for potential bitmap overruns in the cmma migration
    code.

commit 32aa144fc32abfcbf7140f473dfbd94c5b9b4105
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Dec 15 13:14:31 2017 +0100

    KVM: s390: fix cmma migration for multiple memory slots
    
    When multiple memory slots are present the cmma migration code
    does not allocate enough memory for the bitmap. The memory slots
    are sorted in reverse order, so we must use gfn and size of
    slot[0] instead of the last one.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Cc: stable@vger.kernel.org # 4.13+
    Fixes: 190df4a212a7 (KVM: s390: CMMA tracking, ESSA emulation, migration mode)
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index efa439f6ffb3..abcd24fdde3f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -792,11 +792,12 @@ static int kvm_s390_vm_start_migration(struct kvm *kvm)
 
 	if (kvm->arch.use_cmma) {
 		/*
-		 * Get the last slot. They should be sorted by base_gfn, so the
-		 * last slot is also the one at the end of the address space.
-		 * We have verified above that at least one slot is present.
+		 * Get the first slot. They are reverse sorted by base_gfn, so
+		 * the first slot is also the one at the end of the address
+		 * space. We have verified above that at least one slot is
+		 * present.
 		 */
-		ms = slots->memslots + slots->used_slots - 1;
+		ms = slots->memslots;
 		/* round up so we only use full longs */
 		ram_pages = roundup(ms->base_gfn + ms->npages, BITS_PER_LONG);
 		/* allocate enough bytes to store all the bits */

commit 5cb0944c0c66004c0d9006a7f0fba5782ae38f69
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Dec 12 17:41:34 2017 +0100

    KVM: introduce kvm_arch_vcpu_async_ioctl
    
    After the vcpu_load/vcpu_put pushdown, the handling of asynchronous VCPU
    ioctl is already much clearer in that it is obvious that they bypass
    vcpu_load and vcpu_put.
    
    However, it is still not perfect in that the different state of the VCPU
    mutex is still hidden in the caller.  Separate those ioctls into a new
    function kvm_arch_vcpu_async_ioctl that returns -ENOIOCTLCMD for more
    "traditional" synchronous ioctls.
    
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Suggested-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9700d71cb691..40f0ae5a883f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3725,13 +3725,11 @@ static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 	return r;
 }
 
-long kvm_arch_vcpu_ioctl(struct file *filp,
-			 unsigned int ioctl, unsigned long arg)
+long kvm_arch_vcpu_async_ioctl(struct file *filp,
+			       unsigned int ioctl, unsigned long arg)
 {
 	struct kvm_vcpu *vcpu = filp->private_data;
 	void __user *argp = (void __user *)arg;
-	int idx;
-	long r;
 
 	switch (ioctl) {
 	case KVM_S390_IRQ: {
@@ -3752,6 +3750,16 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		return kvm_s390_inject_vcpu(vcpu, &s390irq);
 	}
 	}
+	return -ENOIOCTLCMD;
+}
+
+long kvm_arch_vcpu_ioctl(struct file *filp,
+			 unsigned int ioctl, unsigned long arg)
+{
+	struct kvm_vcpu *vcpu = filp->private_data;
+	void __user *argp = (void __user *)arg;
+	int idx;
+	long r;
 
 	vcpu_load(vcpu);
 

commit 9b062471e52a1692c5563ba1535c84d708e2ff6f
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:36 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl
    
    Move the calls to vcpu_load() and vcpu_put() in to the architecture
    specific implementations of kvm_arch_vcpu_ioctl() which dispatches
    further architecture-specific ioctls on to other functions.
    
    Some architectures support asynchronous vcpu ioctls which cannot call
    vcpu_load() or take the vcpu->mutex, because that would prevent
    concurrent execution with a running VCPU, which is the intended purpose
    of these ioctls, for example because they inject interrupts.
    
    We repeat the separate checks for these specifics in the architecture
    code for MIPS, S390 and PPC, and avoid taking the vcpu->mutex and
    calling vcpu_load for these ioctls.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3bd2f83e1fa9..9700d71cb691 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3737,24 +3737,25 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	case KVM_S390_IRQ: {
 		struct kvm_s390_irq s390irq;
 
-		r = -EFAULT;
 		if (copy_from_user(&s390irq, argp, sizeof(s390irq)))
-			break;
-		r = kvm_s390_inject_vcpu(vcpu, &s390irq);
-		break;
+			return -EFAULT;
+		return kvm_s390_inject_vcpu(vcpu, &s390irq);
 	}
 	case KVM_S390_INTERRUPT: {
 		struct kvm_s390_interrupt s390int;
 		struct kvm_s390_irq s390irq;
 
-		r = -EFAULT;
 		if (copy_from_user(&s390int, argp, sizeof(s390int)))
-			break;
+			return -EFAULT;
 		if (s390int_to_s390irq(&s390int, &s390irq))
 			return -EINVAL;
-		r = kvm_s390_inject_vcpu(vcpu, &s390irq);
-		break;
+		return kvm_s390_inject_vcpu(vcpu, &s390irq);
 	}
+	}
+
+	vcpu_load(vcpu);
+
+	switch (ioctl) {
 	case KVM_S390_STORE_STATUS:
 		idx = srcu_read_lock(&vcpu->kvm->srcu);
 		r = kvm_s390_vcpu_store_status(vcpu, arg);
@@ -3879,6 +3880,8 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	default:
 		r = -ENOTTY;
 	}
+
+	vcpu_put(vcpu);
 	return r;
 }
 

commit 6a96bc7fa0cdd96bac2b8298d708a94f8de6f6d4
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:35 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_set_fpu
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_set_fpu().
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 39327888bb06..3bd2f83e1fa9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2749,15 +2749,24 @@ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 
 int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
-	if (test_fp_ctl(fpu->fpc))
-		return -EINVAL;
+	int ret = 0;
+
+	vcpu_load(vcpu);
+
+	if (test_fp_ctl(fpu->fpc)) {
+		ret = -EINVAL;
+		goto out;
+	}
 	vcpu->run->s.regs.fpc = fpu->fpc;
 	if (MACHINE_HAS_VX)
 		convert_fp_to_vx((__vector128 *) vcpu->run->s.regs.vrs,
 				 (freg_t *) fpu->fprs);
 	else
 		memcpy(vcpu->run->s.regs.fprs, &fpu->fprs, sizeof(fpu->fprs));
-	return 0;
+
+out:
+	vcpu_put(vcpu);
+	return ret;
 }
 
 int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)

commit 1393123e1e24aba96413d351b9546086ea07504d
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:34 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_get_fpu
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_get_fpu().
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1c82dda07bc5..39327888bb06 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2762,6 +2762,8 @@ int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 
 int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
+	vcpu_load(vcpu);
+
 	/* make sure we have the latest values */
 	save_fpu_regs();
 	if (MACHINE_HAS_VX)
@@ -2770,6 +2772,8 @@ int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 	else
 		memcpy(fpu->fprs, vcpu->run->s.regs.fprs, sizeof(fpu->fprs));
 	fpu->fpc = vcpu->run->s.regs.fpc;
+
+	vcpu_put(vcpu);
 	return 0;
 }
 

commit 66b5656222990f1a536f5900ccd98539f9cf231f
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:33 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_set_guest_debug
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_set_guest_debug().
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4e6304d45ca5..1c82dda07bc5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2801,13 +2801,19 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 {
 	int rc = 0;
 
+	vcpu_load(vcpu);
+
 	vcpu->guest_debug = 0;
 	kvm_s390_clear_bp_data(vcpu);
 
-	if (dbg->control & ~VALID_GUESTDBG_FLAGS)
-		return -EINVAL;
-	if (!sclp.has_gpere)
-		return -EINVAL;
+	if (dbg->control & ~VALID_GUESTDBG_FLAGS) {
+		rc = -EINVAL;
+		goto out;
+	}
+	if (!sclp.has_gpere) {
+		rc = -EINVAL;
+		goto out;
+	}
 
 	if (dbg->control & KVM_GUESTDBG_ENABLE) {
 		vcpu->guest_debug = dbg->control;
@@ -2827,6 +2833,8 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 		atomic_andnot(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
 	}
 
+out:
+	vcpu_put(vcpu);
 	return rc;
 }
 

commit e83dff5edf0c3f014e4b4ac5e1c86dbe797687c7
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:31 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_set_mpstate
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_set_mpstate().
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4297f5094a88..4e6304d45ca5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2850,6 +2850,8 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 {
 	int rc = 0;
 
+	vcpu_load(vcpu);
+
 	/* user space knows about this interface - let it control the state */
 	vcpu->kvm->arch.user_cpu_state_ctrl = 1;
 
@@ -2867,6 +2869,7 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 		rc = -ENXIO;
 	}
 
+	vcpu_put(vcpu);
 	return rc;
 }
 

commit fd2325612c1493c85cce89ea16b2396baca83311
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:30 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_get_mpstate
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_get_mpstate().
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d27a1dd0986b..4297f5094a88 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2833,9 +2833,16 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state)
 {
+	int ret;
+
+	vcpu_load(vcpu);
+
 	/* CHECK_STOP and LOAD are not supported yet */
-	return is_vcpu_stopped(vcpu) ? KVM_MP_STATE_STOPPED :
-				       KVM_MP_STATE_OPERATING;
+	ret = is_vcpu_stopped(vcpu) ? KVM_MP_STATE_STOPPED :
+				      KVM_MP_STATE_OPERATING;
+
+	vcpu_put(vcpu);
+	return ret;
 }
 
 int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,

commit b4ef9d4e8cb8938e6c0aa3be672b0aeeb791ecf3
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:29 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_set_sregs
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_set_sregs().
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 87544beff507..d27a1dd0986b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2726,8 +2726,12 @@ int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs)
 {
+	vcpu_load(vcpu);
+
 	memcpy(&vcpu->run->s.regs.acrs, &sregs->acrs, sizeof(sregs->acrs));
 	memcpy(&vcpu->arch.sie_block->gcr, &sregs->crs, sizeof(sregs->crs));
+
+	vcpu_put(vcpu);
 	return 0;
 }
 

commit bcdec41cefbea525ad424050650acb0f2eed1378
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:28 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_get_sregs
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_get_sregs().
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ab08823a854b..87544beff507 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2734,8 +2734,12 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs)
 {
+	vcpu_load(vcpu);
+
 	memcpy(&sregs->acrs, &vcpu->run->s.regs.acrs, sizeof(sregs->acrs));
 	memcpy(&sregs->crs, &vcpu->arch.sie_block->gcr, sizeof(sregs->crs));
+
+	vcpu_put(vcpu);
 	return 0;
 }
 

commit 875656fe0c8473c544860d557ca1512753d6aeef
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:27 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_set_regs
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_set_regs().
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 44317813f498..ab08823a854b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2709,7 +2709,9 @@ static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
+	vcpu_load(vcpu);
 	memcpy(&vcpu->run->s.regs.gprs, &regs->gprs, sizeof(regs->gprs));
+	vcpu_put(vcpu);
 	return 0;
 }
 

commit 1fc9b76b3dd2c57ca0fe42742043a5c3cbdc41c1
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:26 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_get_regs
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_get_regs().
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7972598e60d0..44317813f498 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2715,7 +2715,9 @@ int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 
 int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
+	vcpu_load(vcpu);
 	memcpy(&regs->gprs, &vcpu->run->s.regs.gprs, sizeof(regs->gprs));
+	vcpu_put(vcpu);
 	return 0;
 }
 

commit accb757d798c9b4d85cfe3e5972134c586525168
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:25 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_run
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_run().
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com> # s390 parts
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [Rebased. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ec8b68e97d3c..7972598e60d0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3373,9 +3373,12 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (kvm_run->immediate_exit)
 		return -EINTR;
 
+	vcpu_load(vcpu);
+
 	if (guestdbg_exit_pending(vcpu)) {
 		kvm_s390_prepare_debug_exit(vcpu);
-		return 0;
+		rc = 0;
+		goto out;
 	}
 
 	kvm_sigset_activate(vcpu);
@@ -3385,7 +3388,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	} else if (is_vcpu_stopped(vcpu)) {
 		pr_err_ratelimited("can't run stopped vcpu %d\n",
 				   vcpu->vcpu_id);
-		return -EINVAL;
+		rc = -EINVAL;
+		goto out;
 	}
 
 	sync_regs(vcpu, kvm_run);
@@ -3415,6 +3419,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	kvm_sigset_deactivate(vcpu);
 
 	vcpu->stat.exit_userspace++;
+out:
+	vcpu_put(vcpu);
 	return rc;
 }
 

commit d29899a30f987de46bcdab3ca4513de2479629e5
Merge: 2895db67b01c ca76ec9ca871
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Wed Dec 6 15:55:44 2017 +0100

    Merge tag 'kvm-s390-master-4.15-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
    
    KVM: s390: Fixes for 4.15
    
    - SPDX tags
    - Fence storage key accesses from problem state
    - Make sure that irq_state.flags is not used in the future

commit bb64da9aba89765fee74b395967b18a7d6c364e9
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Nov 21 16:02:52 2017 +0100

    KVM: s390: mark irq_state.flags as non-usable
    
    Old kernels did not check for zero in the irq_state.flags field and old
    QEMUs did not zero the flag/reserved fields when calling
    KVM_S390_*_IRQ_STATE.  Let's add comments to prevent future uses of
    these fields.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 34375eed93ee..efa439f6ffb3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1,8 +1,8 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
- * hosting zSeries kernel virtual machines
+ * hosting IBM Z kernel virtual machines (s390x)
  *
- * Copyright IBM Corp. 2008, 2009
+ * Copyright IBM Corp. 2008, 2017
  *
  *    Author(s): Carsten Otte <cotte@de.ibm.com>
  *               Christian Borntraeger <borntraeger@de.ibm.com>
@@ -3808,6 +3808,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			r = -EINVAL;
 			break;
 		}
+		/* do not use irq_state.flags, it will break old QEMUs */
 		r = kvm_s390_set_irq_state(vcpu,
 					   (void __user *) irq_state.buf,
 					   irq_state.len);
@@ -3823,6 +3824,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			r = -EINVAL;
 			break;
 		}
+		/* do not use irq_state.flags, it will break old QEMUs */
 		r = kvm_s390_get_irq_state(vcpu,
 					   (__u8 __user *)  irq_state.buf,
 					   irq_state.len);

commit 940f89a5a37789b94f332755767c556a64b004e4
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Nov 24 15:00:39 2017 +0100

    KVM: s390: Remove redundant license text
    
    Now that the SPDX tag is in all arch/s390/kvm/ files, that identifies
    the license in a specific and legally-defined manner.  So the extra GPL
    text wording can be removed as it is no longer needed at all.
    
    This is done on a quest to remove the 700+ different ways that files in
    the kernel describe the GPL license text.  And there's unneeded stuff
    like the address (sometimes incorrect) for the FSF which is never
    needed.
    
    No copyright headers or other non-license-description text was removed.
    
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Cornelia Huck <cohuck@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Message-Id: <20171124140043.10062-9-gregkh@linuxfoundation.org>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Acked-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0d45e32dd868..34375eed93ee 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -4,10 +4,6 @@
  *
  * Copyright IBM Corp. 2008, 2009
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License (version 2 only)
- * as published by the Free Software Foundation.
- *
  *    Author(s): Carsten Otte <cotte@de.ibm.com>
  *               Christian Borntraeger <borntraeger@de.ibm.com>
  *               Heiko Carstens <heiko.carstens@de.ibm.com>

commit d809aa238744ae5b7520b73ac5411862ccfdc1bc
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Nov 24 15:00:33 2017 +0100

    KVM: s390: add SPDX identifiers to the remaining files
    
    It's good to have SPDX identifiers in all files to make it easier to
    audit the kernel tree for correct licenses.
    
    Update the arch/s390/kvm/ files with the correct SPDX license
    identifier based on the license text in the file itself.  The SPDX
    identifier is a legally binding shorthand, which can be used instead of
    the full boiler plate text.
    
    This work is based on a script and data from Thomas Gleixner, Philippe
    Ombredanne, and Kate Stewart.
    
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Cornelia Huck <cohuck@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Message-Id: <20171124140043.10062-3-gregkh@linuxfoundation.org>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 98ad8b9e0360..0d45e32dd868 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * hosting zSeries kernel virtual machines
  *

commit 20b7035c66bacc909ae3ffe92c1a1ea7db99fe4f
Author: Jan H. Schnherr <jschoenh@amazon.de>
Date:   Fri Nov 24 22:39:01 2017 +0100

    KVM: Let KVM_SET_SIGNAL_MASK work as advertised
    
    KVM API says for the signal mask you set via KVM_SET_SIGNAL_MASK, that
    "any unblocked signal received [...] will cause KVM_RUN to return with
    -EINTR" and that "the signal will only be delivered if not blocked by
    the original signal mask".
    
    This, however, is only true, when the calling task has a signal handler
    registered for a signal. If not, signal evaluation is short-circuited for
    SIG_IGN and SIG_DFL, and the signal is either ignored without KVM_RUN
    returning or the whole process is terminated.
    
    Make KVM_SET_SIGNAL_MASK behave as advertised by utilizing logic similar
    to that in do_sigtimedwait() to avoid short-circuiting of signals.
    
    Signed-off-by: Jan H. Schnherr <jschoenh@amazon.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 98ad8b9e0360..9614aea5839b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3372,7 +3372,6 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	int rc;
-	sigset_t sigsaved;
 
 	if (kvm_run->immediate_exit)
 		return -EINTR;
@@ -3382,8 +3381,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		return 0;
 	}
 
-	if (vcpu->sigset_active)
-		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
+	kvm_sigset_activate(vcpu);
 
 	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm)) {
 		kvm_s390_vcpu_start(vcpu);
@@ -3417,8 +3415,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	disable_cpu_timer_accounting(vcpu);
 	store_regs(vcpu, kvm_run);
 
-	if (vcpu->sigset_active)
-		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
+	kvm_sigset_deactivate(vcpu);
 
 	vcpu->stat.exit_userspace++;
 	return rc;

commit 974aa5630b318938273d7efe7a2cf031c7b927db
Merge: 441692aafc17 a6014f1ab708
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 13:00:24 2017 -0800

    Merge tag 'kvm-4.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krm:
     "First batch of KVM changes for 4.15
    
      Common:
       - Python 3 support in kvm_stat
       - Accounting of slabs to kmemcg
    
      ARM:
       - Optimized arch timer handling for KVM/ARM
       - Improvements to the VGIC ITS code and introduction of an ITS reset
         ioctl
       - Unification of the 32-bit fault injection logic
       - More exact external abort matching logic
    
      PPC:
       - Support for running hashed page table (HPT) MMU mode on a host that
         is using the radix MMU mode; single threaded mode on POWER 9 is
         added as a pre-requisite
       - Resolution of merge conflicts with the last second 4.14 HPT fixes
       - Fixes and cleanups
    
      s390:
       - Some initial preparation patches for exitless interrupts and crypto
       - New capability for AIS migration
       - Fixes
    
      x86:
       - Improved emulation of LAPIC timer mode changes, MCi_STATUS MSRs,
         and after-reset state
       - Refined dependencies for VMX features
       - Fixes for nested SMI injection
       - A lot of cleanups"
    
    * tag 'kvm-4.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (89 commits)
      KVM: s390: provide a capability for AIS state migration
      KVM: s390: clear_io_irq() requests are not expected for adapter interrupts
      KVM: s390: abstract conversion between isc and enum irq_types
      KVM: s390: vsie: use common code functions for pinning
      KVM: s390: SIE considerations for AP Queue virtualization
      KVM: s390: document memory ordering for kvm_s390_vcpu_wakeup
      KVM: PPC: Book3S HV: Cosmetic post-merge cleanups
      KVM: arm/arm64: fix the incompatible matching for external abort
      KVM: arm/arm64: Unify 32bit fault injection
      KVM: arm/arm64: vgic-its: Implement KVM_DEV_ARM_ITS_CTRL_RESET
      KVM: arm/arm64: Document KVM_DEV_ARM_ITS_CTRL_RESET
      KVM: arm/arm64: vgic-its: Free caches when GITS_BASER Valid bit is cleared
      KVM: arm/arm64: vgic-its: New helper functions to free the caches
      KVM: arm/arm64: vgic-its: Remove kvm_its_unmap_device
      arm/arm64: KVM: Load the timer state when enabling the timer
      KVM: arm/arm64: Rework kvm_timer_should_fire
      KVM: arm/arm64: Get rid of kvm_timer_flush_hwstate
      KVM: arm/arm64: Avoid phys timer emulation in vcpu entry/exit
      KVM: arm/arm64: Move phys_timer_emulate function
      KVM: arm/arm64: Use kvm_arm_timer_set/get_reg for guest register traps
      ...

commit da9a1446d248f673a8560ce46251ff620214ab7b
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Nov 9 10:00:45 2017 +0100

    KVM: s390: provide a capability for AIS state migration
    
    The AIS capability was introduced in 4.12, while the interface to
    migrate the state was added in 4.13. Unfortunately it is not possible
    for userspace to detect the migration capability without creating a flic
    kvm device. As in QEMU the cpu model detection runs on the "none"
    machine this will result in cpu model issues regarding the "ais"
    capability.
    
    To get the "ais" capability properly let's add a new KVM capability that
    tells userspace that AIS states can be migrated.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Halil Pasic <pasic@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index de6a5b790da0..8f4b655f65d7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -395,6 +395,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_USER_INSTR0:
 	case KVM_CAP_S390_CMMA_MIGRATION:
 	case KVM_CAP_S390_AIS:
+	case KVM_CAP_S390_AIS_MIGRATION:
 		r = 1;
 		break;
 	case KVM_CAP_S390_MEM_OP:

commit 3bdf5679c9e46088a0d677e795afc92b5f77940f
Merge: 91a1fad759ff 3d8757b87d7f
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Oct 9 11:16:49 2017 +0200

    Merge branch 'sthyi' into features
    
    Add the store-hypervisor-information code into features
    using a tip branch for parallel merging into the KVM tree.

commit 9fb6c9b3fea1b1d1c6f14178373e8f7235f3b681
Author: QingFeng Hao <haoqf@linux.vnet.ibm.com>
Date:   Fri Sep 29 12:41:51 2017 +0200

    s390/sthyi: add cache to store hypervisor info
    
    STHYI requires extensive locking in the higher hypervisors and is
    very computational/memory expensive. Therefore we cache the retrieved
    hypervisor info whose valid period is 1s with mutex to allow concurrent
    access. rw semaphore can't benefit here due to cache line bounce.
    
    Signed-off-by: QingFeng Hao <haoqf@linux.vnet.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 40d0a1a97889..de6a5b790da0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1884,8 +1884,6 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	rc = -ENOMEM;
 
-	ratelimit_state_init(&kvm->arch.sthyi_limit, 5 * HZ, 500);
-
 	kvm->arch.use_esca = 0; /* start with basic SCA */
 	if (!sclp.has_64bscao)
 		alloc_flags |= GFP_DMA;

commit bb59c2da3fb4ecc83258ca6b2ecde70d80c33465
Author: Alice Frosi <alice@linux.vnet.ibm.com>
Date:   Thu Sep 14 12:35:45 2017 +0200

    s390/runtime_instrumentation: clean up struct runtime_instr_cb
    
    Update runtime_instr_cb structure to be consistent with the runtime
    instrumentation documentation.
    
    Signed-off-by: Alice Frosi <alice@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 40d0a1a97889..3f8316c2cb2a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3283,7 +3283,7 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	 */
 	if ((kvm_run->kvm_dirty_regs & KVM_SYNC_RICCB) &&
 	    test_kvm_facility(vcpu->kvm, 64) &&
-	    riccb->valid &&
+	    riccb->v &&
 	    !(vcpu->arch.sie_block->ecb3 & ECB3_RI)) {
 		VCPU_EVENT(vcpu, 3, "%s", "ENABLE: RI (sync_regs)");
 		vcpu->arch.sie_block->ecb3 |= ECB3_RI;

commit 6e0ff1b4dbe2dcd31d903f691b2bb887206405f1
Merge: 712b12d72451 c95c895303ed
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Thu Sep 7 16:46:46 2017 +0200

    Merge tag 'kvm-s390-next-4.14-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
    
    KVM: s390: Fixes and features for 4.14
    
    - merge of topic branch tlb-flushing from the s390 tree to get the
      no-dat base features
    - merge of kvm/master to avoid conflicts with additional sthyi fixes
    - wire up the no-dat enhancements in KVM
    - multiple epoch facility (z14 feature)
    - Configuration z/Architecture Mode
    - more sthyi fixes
    - gdb server range checking fix
    - small code cleanups

commit 1935222dc2f22ea885e7a7ece6d1be586aafe4a1
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Aug 29 16:31:08 2017 +0200

    KVM: s390: we are always in czam mode
    
    Independent of the underlying hardware, kvm will now always handle
    SIGP SET ARCHITECTURE as if czam were enabled. Therefore, let's not
    only forward that bit but always set it.
    
    While at it, add a comment regarding STHYI.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20170829143108.14703-1-david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 84c069afc02f..39115f5a38df 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1927,6 +1927,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	memcpy(kvm->arch.model.fac_list, kvm->arch.model.fac_mask,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
+	/* we are always in czam mode - even on pre z14 machines */
+	set_kvm_facility(kvm->arch.model.fac_mask, 138);
+	set_kvm_facility(kvm->arch.model.fac_list, 138);
+	/* we emulate STHYI in kvm */
 	set_kvm_facility(kvm->arch.model.fac_mask, 74);
 	set_kvm_facility(kvm->arch.model.fac_list, 74);
 	if (MACHINE_HAS_TLB_GUEST) {

commit 1bab1c02aff73eb5c3001e97f48e64ab6a80988a
Author: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
Date:   Mon Aug 29 15:56:55 2016 +0200

    KVM: s390: expose no-DAT to guest and migration support
    
    The STFLE bit 147 indicates whether the ESSA no-DAT operation code is
    valid, the bit is not normally provided to the host; the host is
    instead provided with an SCLP bit that indicates whether guests can
    support the feature.
    
    This patch:
    * enables the STFLE bit in the guest if the corresponding SCLP bit is
      present in the host.
    * adds support for migrating the no-DAT bit in the PGSTEs
    * fixes the software interpretation of the ESSA instruction that is
      used when migrating, both for the new operation code and for the old
      "set stable", as per specifications.
    
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e65b7637cc45..84c069afc02f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1574,7 +1574,7 @@ static int kvm_s390_get_cmma_bits(struct kvm *kvm,
 		if (r < 0)
 			pgstev = 0;
 		/* save the value */
-		res[i++] = (pgstev >> 24) & 0x3;
+		res[i++] = (pgstev >> 24) & 0x43;
 		/*
 		 * if the next bit is too far away, stop.
 		 * if we reached the previous "next", find the next one
@@ -1652,7 +1652,7 @@ static int kvm_s390_set_cmma_bits(struct kvm *kvm,
 
 		pgstev = bits[i];
 		pgstev = pgstev << 24;
-		mask &= _PGSTE_GPS_USAGE_MASK;
+		mask &= _PGSTE_GPS_USAGE_MASK | _PGSTE_GPS_NODAT;
 		set_pgste_bits(kvm->mm, hva, mask, pgstev);
 	}
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
@@ -1929,6 +1929,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	set_kvm_facility(kvm->arch.model.fac_mask, 74);
 	set_kvm_facility(kvm->arch.model.fac_list, 74);
+	if (MACHINE_HAS_TLB_GUEST) {
+		set_kvm_facility(kvm->arch.model.fac_mask, 147);
+		set_kvm_facility(kvm->arch.model.fac_list, 147);
+	}
 
 	kvm->arch.model.cpuid = kvm_s390_get_initial_cpuid();
 	kvm->arch.model.ibc = sclp.ibc & 0x0fff;

commit 8fa1696ea78162ca3112a26879d9379483443c85
Author: Collin L. Walling <walling@linux.vnet.ibm.com>
Date:   Tue Jul 26 15:29:44 2016 -0400

    KVM: s390: Multiple Epoch Facility support
    
    Allow for the enablement of MEF and the support for the extended
    epoch in SIE and VSIE for the extended guest TOD-Clock.
    
    A new interface is used for getting/setting a guest's extended TOD-Clock
    that uses a single ioctl invocation, KVM_S390_VM_TOD_EXT.  Since the
    host time is a moving target that might see an epoch switch or STP sync
    checks we need an atomic ioctl and cannot use the exisiting two
    interfaces. The old method of getting and setting the guest TOD-Clock is
    still retained and is used when the old ioctls are called.
    
    Signed-off-by: Collin L. Walling <walling@linux.vnet.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Reviewed-by: Jason J. Herne <jjherne@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index af09d3437631..e65b7637cc45 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -130,6 +130,12 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ NULL }
 };
 
+struct kvm_s390_tod_clock_ext {
+	__u8 epoch_idx;
+	__u64 tod;
+	__u8 reserved[7];
+} __packed;
+
 /* allow nested virtualization in KVM (if enabled by user space) */
 static int nested;
 module_param(nested, int, S_IRUGO);
@@ -874,6 +880,26 @@ static int kvm_s390_vm_get_migration(struct kvm *kvm,
 	return 0;
 }
 
+static int kvm_s390_set_tod_ext(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_tod_clock gtod;
+
+	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
+		return -EFAULT;
+
+	if (test_kvm_facility(kvm, 139))
+		kvm_s390_set_tod_clock_ext(kvm, &gtod);
+	else if (gtod.epoch_idx == 0)
+		kvm_s390_set_tod_clock(kvm, gtod.tod);
+	else
+		return -EINVAL;
+
+	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x, TOD base: 0x%llx",
+		gtod.epoch_idx, gtod.tod);
+
+	return 0;
+}
+
 static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	u8 gtod_high;
@@ -909,6 +935,9 @@ static int kvm_s390_set_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 		return -EINVAL;
 
 	switch (attr->attr) {
+	case KVM_S390_VM_TOD_EXT:
+		ret = kvm_s390_set_tod_ext(kvm, attr);
+		break;
 	case KVM_S390_VM_TOD_HIGH:
 		ret = kvm_s390_set_tod_high(kvm, attr);
 		break;
@@ -922,6 +951,43 @@ static int kvm_s390_set_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 	return ret;
 }
 
+static void kvm_s390_get_tod_clock_ext(struct kvm *kvm,
+					struct kvm_s390_vm_tod_clock *gtod)
+{
+	struct kvm_s390_tod_clock_ext htod;
+
+	preempt_disable();
+
+	get_tod_clock_ext((char *)&htod);
+
+	gtod->tod = htod.tod + kvm->arch.epoch;
+	gtod->epoch_idx = htod.epoch_idx + kvm->arch.epdx;
+
+	if (gtod->tod < htod.tod)
+		gtod->epoch_idx += 1;
+
+	preempt_enable();
+}
+
+static int kvm_s390_get_tod_ext(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_tod_clock gtod;
+
+	memset(&gtod, 0, sizeof(gtod));
+
+	if (test_kvm_facility(kvm, 139))
+		kvm_s390_get_tod_clock_ext(kvm, &gtod);
+	else
+		gtod.tod = kvm_s390_get_tod_clock_fast(kvm);
+
+	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
+		return -EFAULT;
+
+	VM_EVENT(kvm, 3, "QUERY: TOD extension: 0x%x, TOD base: 0x%llx",
+		gtod.epoch_idx, gtod.tod);
+	return 0;
+}
+
 static int kvm_s390_get_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	u8 gtod_high = 0;
@@ -954,6 +1020,9 @@ static int kvm_s390_get_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 		return -EINVAL;
 
 	switch (attr->attr) {
+	case KVM_S390_VM_TOD_EXT:
+		ret = kvm_s390_get_tod_ext(kvm, attr);
+		break;
 	case KVM_S390_VM_TOD_HIGH:
 		ret = kvm_s390_get_tod_high(kvm, attr);
 		break;
@@ -2369,6 +2438,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= ECA_VX;
 		vcpu->arch.sie_block->ecd |= ECD_HOSTREGMGMT;
 	}
+	if (test_kvm_facility(vcpu->kvm, 139))
+		vcpu->arch.sie_block->ecd |= ECD_MEF;
+
 	vcpu->arch.sie_block->sdnxo = ((unsigned long) &vcpu->run->s.regs.sdnx)
 					| SDNXC;
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
@@ -2855,6 +2927,35 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+void kvm_s390_set_tod_clock_ext(struct kvm *kvm,
+				 const struct kvm_s390_vm_tod_clock *gtod)
+{
+	struct kvm_vcpu *vcpu;
+	struct kvm_s390_tod_clock_ext htod;
+	int i;
+
+	mutex_lock(&kvm->lock);
+	preempt_disable();
+
+	get_tod_clock_ext((char *)&htod);
+
+	kvm->arch.epoch = gtod->tod - htod.tod;
+	kvm->arch.epdx = gtod->epoch_idx - htod.epoch_idx;
+
+	if (kvm->arch.epoch > gtod->tod)
+		kvm->arch.epdx -= 1;
+
+	kvm_s390_vcpu_block_all(kvm);
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		vcpu->arch.sie_block->epoch = kvm->arch.epoch;
+		vcpu->arch.sie_block->epdx  = kvm->arch.epdx;
+	}
+
+	kvm_s390_vcpu_unblock_all(kvm);
+	preempt_enable();
+	mutex_unlock(&kvm->lock);
+}
+
 void kvm_s390_set_tod_clock(struct kvm *kvm, u64 tod)
 {
 	struct kvm_vcpu *vcpu;

commit 0546c63d988d347fc734fc8c624692b30f95f633
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Tue Aug 8 12:05:34 2017 +0800

    KVM: s390: implements the kvm_arch_vcpu_in_kernel()
    
    This implements kvm_arch_vcpu_in_kernel() for s390.  DIAG is a privileged
    operation, so it cannot be called from problem state (user mode).
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5e07322034b9..9f23a9e81a91 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2449,7 +2449,7 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 
 bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
 {
-	return false;
+	return !(vcpu->arch.sie_block->gpsw.mask & PSW_MASK_PSTATE);
 }
 
 void kvm_s390_vcpu_block(struct kvm_vcpu *vcpu)

commit 199b5763d329b43c88f6ad539db8a6c6b42f8edb
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Tue Aug 8 12:05:32 2017 +0800

    KVM: add spinlock optimization framework
    
    If a vcpu exits due to request a user mode spinlock, then
    the spinlock-holder may be preempted in user mode or kernel mode.
    (Note that not all architectures trap spin loops in user mode,
    only AMD x86 and ARM/ARM64 currently do).
    
    But if a vcpu exits in kernel mode, then the holder must be
    preempted in kernel mode, so we should choose a vcpu in kernel mode
    as a more likely candidate for the lock holder.
    
    This introduces kvm_arch_vcpu_in_kernel() to decide whether the
    vcpu is in kernel-mode when it's preempted.  kvm_vcpu_on_spin's
    new argument says the same of the spinning VCPU.
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index af09d3437631..5e07322034b9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2447,6 +2447,11 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 	return kvm_s390_vcpu_has_irq(vcpu, 0);
 }
 
+bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
+{
+	return false;
+}
+
 void kvm_s390_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	atomic_or(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);

commit 4f899147424a189b0ad1fdd6f35784ed5a642e83
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Jul 10 13:35:48 2017 +0200

    KVM: s390: take srcu lock when getting/setting storage keys
    
    The following warning was triggered by missing srcu locks around
    the storage key handling functions.
    
    =============================
    WARNING: suspicious RCU usage
    4.12.0+ #56 Not tainted
    -----------------------------
    ./include/linux/kvm_host.h:572 suspicious rcu_dereference_check() usage!
    rcu_scheduler_active = 2, debug_locks = 1
     1 lock held by live_migration/4936:
      #0:  (&mm->mmap_sem){++++++}, at: [<0000000000141be0>]
    kvm_arch_vm_ioctl+0x6b8/0x22d0
    
     CPU: 8 PID: 4936 Comm: live_migration Not tainted 4.12.0+ #56
     Hardware name: IBM 2964 NC9 704 (LPAR)
     Call Trace:
     ([<000000000011378a>] show_stack+0xea/0xf0)
      [<000000000055cc4c>] dump_stack+0x94/0xd8
      [<000000000012ee70>] gfn_to_memslot+0x1a0/0x1b8
      [<0000000000130b76>] gfn_to_hva+0x2e/0x48
      [<0000000000141c3c>] kvm_arch_vm_ioctl+0x714/0x22d0
      [<000000000013306c>] kvm_vm_ioctl+0x11c/0x7b8
      [<000000000037e2c0>] do_vfs_ioctl+0xa8/0x6c8
      [<000000000037e984>] SyS_ioctl+0xa4/0xb8
      [<00000000008b20a4>] system_call+0xc4/0x27c
     1 lock held by live_migration/4936:
      #0:  (&mm->mmap_sem){++++++}, at: [<0000000000141be0>]
    kvm_arch_vm_ioctl+0x6b8/0x22d0
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Pierre Morel<pmorel@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3f2884e99ed4..af09d3437631 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1324,7 +1324,7 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 {
 	uint8_t *keys;
 	uint64_t hva;
-	int i, r = 0;
+	int srcu_idx, i, r = 0;
 
 	if (args->flags != 0)
 		return -EINVAL;
@@ -1342,6 +1342,7 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 		return -ENOMEM;
 
 	down_read(&current->mm->mmap_sem);
+	srcu_idx = srcu_read_lock(&kvm->srcu);
 	for (i = 0; i < args->count; i++) {
 		hva = gfn_to_hva(kvm, args->start_gfn + i);
 		if (kvm_is_error_hva(hva)) {
@@ -1353,6 +1354,7 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 		if (r)
 			break;
 	}
+	srcu_read_unlock(&kvm->srcu, srcu_idx);
 	up_read(&current->mm->mmap_sem);
 
 	if (!r) {
@@ -1370,7 +1372,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 {
 	uint8_t *keys;
 	uint64_t hva;
-	int i, r = 0;
+	int srcu_idx, i, r = 0;
 
 	if (args->flags != 0)
 		return -EINVAL;
@@ -1396,6 +1398,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 		goto out;
 
 	down_read(&current->mm->mmap_sem);
+	srcu_idx = srcu_read_lock(&kvm->srcu);
 	for (i = 0; i < args->count; i++) {
 		hva = gfn_to_hva(kvm, args->start_gfn + i);
 		if (kvm_is_error_hva(hva)) {
@@ -1413,6 +1416,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 		if (r)
 			break;
 	}
+	srcu_read_unlock(&kvm->srcu, srcu_idx);
 	up_read(&current->mm->mmap_sem);
 out:
 	kvfree(keys);

commit c136b84393d4e340e1b53fc7f737dd5827b19ee5
Merge: e0f25a3f2d05 1372324b328c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 6 18:38:31 2017 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "PPC:
       - Better machine check handling for HV KVM
       - Ability to support guests with threads=2, 4 or 8 on POWER9
       - Fix for a race that could cause delayed recognition of signals
       - Fix for a bug where POWER9 guests could sleep with interrupts pending.
    
      ARM:
       - VCPU request overhaul
       - allow timer and PMU to have their interrupt number selected from userspace
       - workaround for Cavium erratum 30115
       - handling of memory poisonning
       - the usual crop of fixes and cleanups
    
      s390:
       - initial machine check forwarding
       - migration support for the CMMA page hinting information
       - cleanups and fixes
    
      x86:
       - nested VMX bugfixes and improvements
       - more reliable NMI window detection on AMD
       - APIC timer optimizations
    
      Generic:
       - VCPU request overhaul + documentation of common code patterns
       - kvm_stat improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (124 commits)
      Update my email address
      kvm: vmx: allow host to access guest MSR_IA32_BNDCFGS
      x86: kvm: mmu: use ept a/d in vmcs02 iff used in vmcs12
      kvm: x86: mmu: allow A/D bits to be disabled in an mmu
      x86: kvm: mmu: make spte mmio mask more explicit
      x86: kvm: mmu: dead code thanks to access tracking
      KVM: PPC: Book3S: Fix typo in XICS-on-XIVE state saving code
      KVM: PPC: Book3S HV: Close race with testing for signals on guest entry
      KVM: PPC: Book3S HV: Simplify dynamic micro-threading code
      KVM: x86: remove ignored type attribute
      KVM: LAPIC: Fix lapic timer injection delay
      KVM: lapic: reorganize restart_apic_timer
      KVM: lapic: reorganize start_hv_timer
      kvm: nVMX: Check memory operand to INVVPID
      KVM: s390: Inject machine check into the nested guest
      KVM: s390: Inject machine check into the guest
      tools/kvm_stat: add new interactive command 'b'
      tools/kvm_stat: add new command line switch '-i'
      tools/kvm_stat: fix error on interactive command 'g'
      KVM: SVM: suppress unnecessary NMI singlestep on GIF=0 and nested exit
      ...

commit e0f3e8f14da868047c524a0cf11e08b95fd1b008
Merge: e5859eb84576 9e293b5a7062
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 15:39:36 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "The bulk of the s390 patches for 4.13. Some new things but mostly bug
      fixes and cleanups. Noteworthy changes:
    
       - The SCM block driver is converted to blk-mq
    
       - Switch s390 to 5 level page tables. The virtual address space for a
         user space process can now have up to 16EB-4KB.
    
       - Introduce a ELF phdr flag for qemu to avoid the global
         vm.alloc_pgste which forces all processes to large page tables
    
       - A couple of PCI improvements to improve error recovery
    
       - Included is the merge of the base support for proper machine checks
         for KVM"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (52 commits)
      s390/dasd: Fix faulty ENODEV for RO sysfs attribute
      s390/pci: recognize name clashes with uids
      s390/pci: provide more debug information
      s390/pci: fix handling of PEC 306
      s390/pci: improve pci hotplug
      s390/pci: introduce clp_get_state
      s390/pci: improve error handling during fmb (de)registration
      s390/pci: improve unreg_ioat error handling
      s390/pci: improve error handling during interrupt deregistration
      s390/pci: don't cleanup in arch_setup_msi_irqs
      KVM: s390: Backup the guest's machine check info
      s390/nmi: s390: New low level handling for machine check happening in guest
      s390/fpu: export save_fpu_regs for all configs
      s390/kvm: avoid global config of vm.alloc_pgste=1
      s390: rename struct psw_bits members
      s390: rename psw_bits enums
      s390/mm: use correct address space when enabling DAT
      s390/cio: introduce io_subchannel_type
      s390/ipl: revert Load Normal semantics for LPAR CCW-type re-IPL
      s390/dumpstack: remove raw stack dump
      ...

commit 04a7ea04d508b925e7f829305b358157d58b4f82
Merge: c853354429f7 d38338e396ee
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jun 30 12:38:26 2017 +0200

    Merge tag 'kvmarm-for-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM updates for 4.13
    
    - vcpu request overhaul
    - allow timer and PMU to have their interrupt number
      selected from userspace
    - workaround for Cavium erratum 30115
    - handling of memory poisonning
    - the usual crop of fixes and cleanups
    
    Conflicts:
            arch/s390/include/asm/kvm_host.h

commit 4d62fcc0b692e3b4058d7d138114c27cd8b011f7
Author: QingFeng Hao <haoqf@linux.vnet.ibm.com>
Date:   Wed Jun 7 12:03:05 2017 +0200

    KVM: s390: Inject machine check into the guest
    
    If the exit flag of SIE indicates that a machine check has happened
    during guest's running and needs to be injected, inject it to the guest
    accordingly.
    But some machine checks, e.g. Channel Report Pending (CRW), refer to
    host conditions only (the guest's channel devices are not managed by
    the kernel directly) and are therefore not injected into the guest.
    External Damage (ED) is also not reinjected into the guest because ETR
    conditions are gone in Linux and STP conditions are not enabled in the
    guest, and ED contains only these 8 ETR and STP conditions.
    In general, instruction-processing damage, system recovery,
    storage error, service-processor damage and channel subsystem damage
    will be reinjected into the guest, and the remain (System damage,
    timing-facility damage, warning, ED and CRW) will be handled on the host.
    
    Signed-off-by: QingFeng Hao <haoqf@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 90434760cda5..a0f6b599ce6b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -3041,6 +3041,9 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 
 static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 {
+	struct mcck_volatile_info *mcck_info;
+	struct sie_page *sie_page;
+
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
 	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
@@ -3051,6 +3054,15 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 	vcpu->run->s.regs.gprs[14] = vcpu->arch.sie_block->gg14;
 	vcpu->run->s.regs.gprs[15] = vcpu->arch.sie_block->gg15;
 
+	if (exit_reason == -EINTR) {
+		VCPU_EVENT(vcpu, 3, "%s", "machine check");
+		sie_page = container_of(vcpu->arch.sie_block,
+					struct sie_page, sie_block);
+		mcck_info = &sie_page->mcck_info;
+		kvm_s390_reinject_machine_check(vcpu, mcck_info);
+		return 0;
+	}
+
 	if (vcpu->arch.sie_block->icptcode > 0) {
 		int rc = kvm_handle_sie_intercept(vcpu);
 

commit aec3b2c5f929c31d5517e36e2068617179d0fefd
Merge: 1cae025577f4 da72ca4d4090
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jun 28 12:42:02 2017 +0200

    Merge tag 'nmiforkvm' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kernelorgnext
    
    s390,kvm: provide plumbing for machines checks when running guests
    
    This provides the basic plumbing for handling machine checks when
    running guests

commit da72ca4d4090a8ab0e6b0a23682ef42d39d7ae00
Author: QingFeng Hao <haoqf@linux.vnet.ibm.com>
Date:   Wed Jun 7 11:41:19 2017 +0200

    KVM: s390: Backup the guest's machine check info
    
    When a machine check happens in the guest, related mcck info (mcic,
    external damage code, ...) is stored in the vcpu's lowcore on the host.
    Then the machine check handler's low-level part is executed, followed
    by the high-level part.
    
    If the high-level part's execution is interrupted by a new machine check
    happening on the same vcpu on the host, the mcck info in the lowcore is
    overwritten with the new machine check's data.
    
    If the high-level part's execution is scheduled to a different cpu,
    the mcck info in the lowcore is uncertain.
    
    Therefore, for both cases, the further reinjection to the guest will use
    the wrong data.
    Let's backup the mcck info in the lowcore to the sie page
    for further reinjection, so that the right data will be used.
    
    Add new member into struct sie_page to store related machine check's
    info of mcic, failing storage address and external damage code.
    
    Signed-off-by: QingFeng Hao <haoqf@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 689ac48361c6..0457e03199c5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2069,6 +2069,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	if (!vcpu)
 		goto out;
 
+	BUILD_BUG_ON(sizeof(struct sie_page) != 4096);
 	sie_page = (struct sie_page *) get_zeroed_page(GFP_KERNEL);
 	if (!sie_page)
 		goto out_free_cpu;

commit 4036e3874a1ce41a4f7267289f9a0d8e5cd49408
Author: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
Date:   Thu Aug 4 17:58:47 2016 +0200

    KVM: s390: ioctls to get and set guest storage attributes
    
    * Add the struct used in the ioctls to get and set CMMA attributes.
    * Add the two functions needed to get and set the CMMA attributes for
      guest pages.
    * Add the two ioctls that use the aforementioned functions.
    
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c2b391499374..e100a7ff35c7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -30,8 +30,8 @@
 #include <linux/vmalloc.h>
 #include <linux/bitmap.h>
 #include <linux/sched/signal.h>
-
 #include <linux/string.h>
+
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/stp.h>
@@ -387,6 +387,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_SKEYS:
 	case KVM_CAP_S390_IRQ_STATE:
 	case KVM_CAP_S390_USER_INSTR0:
+	case KVM_CAP_S390_CMMA_MIGRATION:
 	case KVM_CAP_S390_AIS:
 		r = 1;
 		break;
@@ -1419,6 +1420,182 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	return r;
 }
 
+/*
+ * Base address and length must be sent at the start of each block, therefore
+ * it's cheaper to send some clean data, as long as it's less than the size of
+ * two longs.
+ */
+#define KVM_S390_MAX_BIT_DISTANCE (2 * sizeof(void *))
+/* for consistency */
+#define KVM_S390_CMMA_SIZE_MAX ((u32)KVM_S390_SKEYS_MAX)
+
+/*
+ * This function searches for the next page with dirty CMMA attributes, and
+ * saves the attributes in the buffer up to either the end of the buffer or
+ * until a block of at least KVM_S390_MAX_BIT_DISTANCE clean bits is found;
+ * no trailing clean bytes are saved.
+ * In case no dirty bits were found, or if CMMA was not enabled or used, the
+ * output buffer will indicate 0 as length.
+ */
+static int kvm_s390_get_cmma_bits(struct kvm *kvm,
+				  struct kvm_s390_cmma_log *args)
+{
+	struct kvm_s390_migration_state *s = kvm->arch.migration_state;
+	unsigned long bufsize, hva, pgstev, i, next, cur;
+	int srcu_idx, peek, r = 0, rr;
+	u8 *res;
+
+	cur = args->start_gfn;
+	i = next = pgstev = 0;
+
+	if (unlikely(!kvm->arch.use_cmma))
+		return -ENXIO;
+	/* Invalid/unsupported flags were specified */
+	if (args->flags & ~KVM_S390_CMMA_PEEK)
+		return -EINVAL;
+	/* Migration mode query, and we are not doing a migration */
+	peek = !!(args->flags & KVM_S390_CMMA_PEEK);
+	if (!peek && !s)
+		return -EINVAL;
+	/* CMMA is disabled or was not used, or the buffer has length zero */
+	bufsize = min(args->count, KVM_S390_CMMA_SIZE_MAX);
+	if (!bufsize || !kvm->mm->context.use_cmma) {
+		memset(args, 0, sizeof(*args));
+		return 0;
+	}
+
+	if (!peek) {
+		/* We are not peeking, and there are no dirty pages */
+		if (!atomic64_read(&s->dirty_pages)) {
+			memset(args, 0, sizeof(*args));
+			return 0;
+		}
+		cur = find_next_bit(s->pgste_bitmap, s->bitmap_size,
+				    args->start_gfn);
+		if (cur >= s->bitmap_size)	/* nothing found, loop back */
+			cur = find_next_bit(s->pgste_bitmap, s->bitmap_size, 0);
+		if (cur >= s->bitmap_size) {	/* again! (very unlikely) */
+			memset(args, 0, sizeof(*args));
+			return 0;
+		}
+		next = find_next_bit(s->pgste_bitmap, s->bitmap_size, cur + 1);
+	}
+
+	res = vmalloc(bufsize);
+	if (!res)
+		return -ENOMEM;
+
+	args->start_gfn = cur;
+
+	down_read(&kvm->mm->mmap_sem);
+	srcu_idx = srcu_read_lock(&kvm->srcu);
+	while (i < bufsize) {
+		hva = gfn_to_hva(kvm, cur);
+		if (kvm_is_error_hva(hva)) {
+			r = -EFAULT;
+			break;
+		}
+		/* decrement only if we actually flipped the bit to 0 */
+		if (!peek && test_and_clear_bit(cur, s->pgste_bitmap))
+			atomic64_dec(&s->dirty_pages);
+		r = get_pgste(kvm->mm, hva, &pgstev);
+		if (r < 0)
+			pgstev = 0;
+		/* save the value */
+		res[i++] = (pgstev >> 24) & 0x3;
+		/*
+		 * if the next bit is too far away, stop.
+		 * if we reached the previous "next", find the next one
+		 */
+		if (!peek) {
+			if (next > cur + KVM_S390_MAX_BIT_DISTANCE)
+				break;
+			if (cur == next)
+				next = find_next_bit(s->pgste_bitmap,
+						     s->bitmap_size, cur + 1);
+		/* reached the end of the bitmap or of the buffer, stop */
+			if ((next >= s->bitmap_size) ||
+			    (next >= args->start_gfn + bufsize))
+				break;
+		}
+		cur++;
+	}
+	srcu_read_unlock(&kvm->srcu, srcu_idx);
+	up_read(&kvm->mm->mmap_sem);
+	args->count = i;
+	args->remaining = s ? atomic64_read(&s->dirty_pages) : 0;
+
+	rr = copy_to_user((void __user *)args->values, res, args->count);
+	if (rr)
+		r = -EFAULT;
+
+	vfree(res);
+	return r;
+}
+
+/*
+ * This function sets the CMMA attributes for the given pages. If the input
+ * buffer has zero length, no action is taken, otherwise the attributes are
+ * set and the mm->context.use_cmma flag is set.
+ */
+static int kvm_s390_set_cmma_bits(struct kvm *kvm,
+				  const struct kvm_s390_cmma_log *args)
+{
+	unsigned long hva, mask, pgstev, i;
+	uint8_t *bits;
+	int srcu_idx, r = 0;
+
+	mask = args->mask;
+
+	if (!kvm->arch.use_cmma)
+		return -ENXIO;
+	/* invalid/unsupported flags */
+	if (args->flags != 0)
+		return -EINVAL;
+	/* Enforce sane limit on memory allocation */
+	if (args->count > KVM_S390_CMMA_SIZE_MAX)
+		return -EINVAL;
+	/* Nothing to do */
+	if (args->count == 0)
+		return 0;
+
+	bits = vmalloc(sizeof(*bits) * args->count);
+	if (!bits)
+		return -ENOMEM;
+
+	r = copy_from_user(bits, (void __user *)args->values, args->count);
+	if (r) {
+		r = -EFAULT;
+		goto out;
+	}
+
+	down_read(&kvm->mm->mmap_sem);
+	srcu_idx = srcu_read_lock(&kvm->srcu);
+	for (i = 0; i < args->count; i++) {
+		hva = gfn_to_hva(kvm, args->start_gfn + i);
+		if (kvm_is_error_hva(hva)) {
+			r = -EFAULT;
+			break;
+		}
+
+		pgstev = bits[i];
+		pgstev = pgstev << 24;
+		mask &= _PGSTE_GPS_USAGE_MASK;
+		set_pgste_bits(kvm->mm, hva, mask, pgstev);
+	}
+	srcu_read_unlock(&kvm->srcu, srcu_idx);
+	up_read(&kvm->mm->mmap_sem);
+
+	if (!kvm->mm->context.use_cmma) {
+		down_write(&kvm->mm->mmap_sem);
+		kvm->mm->context.use_cmma = 1;
+		up_write(&kvm->mm->mmap_sem);
+	}
+out:
+	vfree(bits);
+	return r;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg)
 {
@@ -1497,6 +1674,29 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_s390_set_skeys(kvm, &args);
 		break;
 	}
+	case KVM_S390_GET_CMMA_BITS: {
+		struct kvm_s390_cmma_log args;
+
+		r = -EFAULT;
+		if (copy_from_user(&args, argp, sizeof(args)))
+			break;
+		r = kvm_s390_get_cmma_bits(kvm, &args);
+		if (!r) {
+			r = copy_to_user(argp, &args, sizeof(args));
+			if (r)
+				r = -EFAULT;
+		}
+		break;
+	}
+	case KVM_S390_SET_CMMA_BITS: {
+		struct kvm_s390_cmma_log args;
+
+		r = -EFAULT;
+		if (copy_from_user(&args, argp, sizeof(args)))
+			break;
+		r = kvm_s390_set_cmma_bits(kvm, &args);
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit 190df4a212a708fdd18f6cabfdd82594c91fdf25
Author: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
Date:   Thu Aug 4 17:54:42 2016 +0200

    KVM: s390: CMMA tracking, ESSA emulation, migration mode
    
    * Add a migration state bitmap to keep track of which pages have dirty
      CMMA information.
    * Disable CMMA by default, so we can track if it's used or not. Enable
      it on first use like we do for storage keys (unless we are doing a
      migration).
    * Creates a VM attribute to enter and leave migration mode.
    * In migration mode, CMMA is disabled in the SIE block, so ESSA is
      always interpreted and emulated in software.
    * Free the migration state on VM destroy.
    
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 689ac48361c6..c2b391499374 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -31,6 +31,7 @@
 #include <linux/bitmap.h>
 #include <linux/sched/signal.h>
 
+#include <linux/string.h>
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/stp.h>
@@ -750,6 +751,129 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 	return 0;
 }
 
+static void kvm_s390_sync_request_broadcast(struct kvm *kvm, int req)
+{
+	int cx;
+	struct kvm_vcpu *vcpu;
+
+	kvm_for_each_vcpu(cx, vcpu, kvm)
+		kvm_s390_sync_request(req, vcpu);
+}
+
+/*
+ * Must be called with kvm->srcu held to avoid races on memslots, and with
+ * kvm->lock to avoid races with ourselves and kvm_s390_vm_stop_migration.
+ */
+static int kvm_s390_vm_start_migration(struct kvm *kvm)
+{
+	struct kvm_s390_migration_state *mgs;
+	struct kvm_memory_slot *ms;
+	/* should be the only one */
+	struct kvm_memslots *slots;
+	unsigned long ram_pages;
+	int slotnr;
+
+	/* migration mode already enabled */
+	if (kvm->arch.migration_state)
+		return 0;
+
+	slots = kvm_memslots(kvm);
+	if (!slots || !slots->used_slots)
+		return -EINVAL;
+
+	mgs = kzalloc(sizeof(*mgs), GFP_KERNEL);
+	if (!mgs)
+		return -ENOMEM;
+	kvm->arch.migration_state = mgs;
+
+	if (kvm->arch.use_cmma) {
+		/*
+		 * Get the last slot. They should be sorted by base_gfn, so the
+		 * last slot is also the one at the end of the address space.
+		 * We have verified above that at least one slot is present.
+		 */
+		ms = slots->memslots + slots->used_slots - 1;
+		/* round up so we only use full longs */
+		ram_pages = roundup(ms->base_gfn + ms->npages, BITS_PER_LONG);
+		/* allocate enough bytes to store all the bits */
+		mgs->pgste_bitmap = vmalloc(ram_pages / 8);
+		if (!mgs->pgste_bitmap) {
+			kfree(mgs);
+			kvm->arch.migration_state = NULL;
+			return -ENOMEM;
+		}
+
+		mgs->bitmap_size = ram_pages;
+		atomic64_set(&mgs->dirty_pages, ram_pages);
+		/* mark all the pages in active slots as dirty */
+		for (slotnr = 0; slotnr < slots->used_slots; slotnr++) {
+			ms = slots->memslots + slotnr;
+			bitmap_set(mgs->pgste_bitmap, ms->base_gfn, ms->npages);
+		}
+
+		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_START_MIGRATION);
+	}
+	return 0;
+}
+
+/*
+ * Must be called with kvm->lock to avoid races with ourselves and
+ * kvm_s390_vm_start_migration.
+ */
+static int kvm_s390_vm_stop_migration(struct kvm *kvm)
+{
+	struct kvm_s390_migration_state *mgs;
+
+	/* migration mode already disabled */
+	if (!kvm->arch.migration_state)
+		return 0;
+	mgs = kvm->arch.migration_state;
+	kvm->arch.migration_state = NULL;
+
+	if (kvm->arch.use_cmma) {
+		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_STOP_MIGRATION);
+		vfree(mgs->pgste_bitmap);
+	}
+	kfree(mgs);
+	return 0;
+}
+
+static int kvm_s390_vm_set_migration(struct kvm *kvm,
+				     struct kvm_device_attr *attr)
+{
+	int idx, res = -ENXIO;
+
+	mutex_lock(&kvm->lock);
+	switch (attr->attr) {
+	case KVM_S390_VM_MIGRATION_START:
+		idx = srcu_read_lock(&kvm->srcu);
+		res = kvm_s390_vm_start_migration(kvm);
+		srcu_read_unlock(&kvm->srcu, idx);
+		break;
+	case KVM_S390_VM_MIGRATION_STOP:
+		res = kvm_s390_vm_stop_migration(kvm);
+		break;
+	default:
+		break;
+	}
+	mutex_unlock(&kvm->lock);
+
+	return res;
+}
+
+static int kvm_s390_vm_get_migration(struct kvm *kvm,
+				     struct kvm_device_attr *attr)
+{
+	u64 mig = (kvm->arch.migration_state != NULL);
+
+	if (attr->attr != KVM_S390_VM_MIGRATION_STATUS)
+		return -ENXIO;
+
+	if (copy_to_user((void __user *)attr->addr, &mig, sizeof(mig)))
+		return -EFAULT;
+	return 0;
+}
+
 static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	u8 gtod_high;
@@ -1090,6 +1214,9 @@ static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_CRYPTO:
 		ret = kvm_s390_vm_set_crypto(kvm, attr);
 		break;
+	case KVM_S390_VM_MIGRATION:
+		ret = kvm_s390_vm_set_migration(kvm, attr);
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -1112,6 +1239,9 @@ static int kvm_s390_vm_get_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_CPU_MODEL:
 		ret = kvm_s390_get_cpu_model(kvm, attr);
 		break;
+	case KVM_S390_VM_MIGRATION:
+		ret = kvm_s390_vm_get_migration(kvm, attr);
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -1179,6 +1309,9 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 			break;
 		}
 		break;
+	case KVM_S390_VM_MIGRATION:
+		ret = 0;
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -1633,6 +1766,10 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_s390_destroy_adapters(kvm);
 	kvm_s390_clear_float_irqs(kvm);
 	kvm_s390_vsie_destroy(kvm);
+	if (kvm->arch.migration_state) {
+		vfree(kvm->arch.migration_state->pgste_bitmap);
+		kfree(kvm->arch.migration_state);
+	}
 	KVM_EVENT(3, "vm 0x%pK destroyed", kvm);
 }
 
@@ -1977,7 +2114,6 @@ int kvm_s390_vcpu_setup_cmma(struct kvm_vcpu *vcpu)
 	if (!vcpu->arch.sie_block->cbrlo)
 		return -ENOMEM;
 
-	vcpu->arch.sie_block->ecb2 |= ECB2_CMMA;
 	vcpu->arch.sie_block->ecb2 &= ~ECB2_PFMFI;
 	return 0;
 }
@@ -2489,6 +2625,27 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 		goto retry;
 	}
 
+	if (kvm_check_request(KVM_REQ_START_MIGRATION, vcpu)) {
+		/*
+		 * Disable CMMA virtualization; we will emulate the ESSA
+		 * instruction manually, in order to provide additional
+		 * functionalities needed for live migration.
+		 */
+		vcpu->arch.sie_block->ecb2 &= ~ECB2_CMMA;
+		goto retry;
+	}
+
+	if (kvm_check_request(KVM_REQ_STOP_MIGRATION, vcpu)) {
+		/*
+		 * Re-enable CMMA virtualization if CMMA is available and
+		 * was used.
+		 */
+		if ((vcpu->kvm->arch.use_cmma) &&
+		    (vcpu->kvm->mm->context.use_cmma))
+			vcpu->arch.sie_block->ecb2 |= ECB2_CMMA;
+		goto retry;
+	}
+
 	/* nothing to do, just clear the request */
 	kvm_clear_request(KVM_REQ_UNHALT, vcpu);
 

commit 2fa6e1e12a024b48b2c7ea39f50205246e027da7
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Sun Jun 4 14:43:52 2017 +0200

    KVM: add kvm_request_pending
    
    A first step in vcpu->requests encapsulation.  Additionally, we now
    use READ_ONCE() when accessing vcpu->requests, which ensures we
    always load vcpu->requests when it's accessed.  This is important as
    other threads can change it any time.  Also, READ_ONCE() documents
    that vcpu->requests is used with other threads, likely requiring
    memory barriers, which it does.
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    [ Documented the new use of READ_ONCE() and converted another check
      in arch/mips/kvm/vz.c ]
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 689ac48361c6..ad41e0fa3a21 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2440,7 +2440,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 {
 retry:
 	kvm_s390_vcpu_request_handled(vcpu);
-	if (!vcpu->requests)
+	if (!kvm_request_pending(vcpu))
 		return 0;
 	/*
 	 * We use MMU_RELOAD just to re-arm the ipte notifier for the

commit 1ba15b24f07c44f68067959522639226c963e212
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed May 31 10:18:55 2017 +0200

    KVM: s390: fix ais handling vs cpu model
    
    If ais is disabled via cpumodel, we must act accordingly, even if
    KVM_CAP_S390_AIS was enabled.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Pierre Morel <pmorel@linux.vnet.ibm.com>
    Reviewed-by: Yi Min Zhao <zyimin@linux.vnet.ibm.com>
    Reviewed-by: Halil Pasic <pasic@linux.vnet.ibm.com>
    Reviewed-by: Eric Farman <farman@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 689ac48361c6..f28e2e776931 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -558,7 +558,6 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		} else {
 			set_kvm_facility(kvm->arch.model.fac_mask, 72);
 			set_kvm_facility(kvm->arch.model.fac_list, 72);
-			kvm->arch.float_int.ais_enabled = 1;
 			r = 0;
 		}
 		mutex_unlock(&kvm->lock);
@@ -1533,7 +1532,6 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	mutex_init(&kvm->arch.float_int.ais_lock);
 	kvm->arch.float_int.simm = 0;
 	kvm->arch.float_int.nimm = 0;
-	kvm->arch.float_int.ais_enabled = 0;
 	spin_lock_init(&kvm->arch.float_int.lock);
 	for (i = 0; i < FIRQ_LIST_COUNT; i++)
 		INIT_LIST_HEAD(&kvm->arch.float_int.lists[i]);

commit bf5f89463f5b3109a72ed13ca62b57e90213387d
Merge: 2d3e4866dea9 4d2b5bcab53f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 8 18:17:56 2017 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
    
     - the rest of MM
    
     - various misc things
    
     - procfs updates
    
     - lib/ updates
    
     - checkpatch updates
    
     - kdump/kexec updates
    
     - add kvmalloc helpers, use them
    
     - time helper updates for Y2038 issues. We're almost ready to remove
       current_fs_time() but that awaits a btrfs merge.
    
     - add tracepoints to DAX
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (114 commits)
      drivers/staging/ccree/ssi_hash.c: fix build with gcc-4.4.4
      selftests/vm: add a test for virtual address range mapping
      dax: add tracepoint to dax_insert_mapping()
      dax: add tracepoint to dax_writeback_one()
      dax: add tracepoints to dax_writeback_mapping_range()
      dax: add tracepoints to dax_load_hole()
      dax: add tracepoints to dax_pfn_mkwrite()
      dax: add tracepoints to dax_iomap_pte_fault()
      mtd: nand: nandsim: convert to memalloc_noreclaim_*()
      treewide: convert PF_MEMALLOC manipulations to new helpers
      mm: introduce memalloc_noreclaim_{save,restore}
      mm: prevent potential recursive reclaim due to clearing PF_MEMALLOC
      mm/huge_memory.c: deposit a pgtable for DAX PMD faults when required
      mm/huge_memory.c: use zap_deposited_table() more
      time: delete CURRENT_TIME_SEC and CURRENT_TIME
      gfs2: replace CURRENT_TIME with current_time
      apparmorfs: replace CURRENT_TIME with current_time()
      lustre: replace CURRENT_TIME macro
      fs: ubifs: replace CURRENT_TIME_SEC with current_time
      fs: ufs: use ktime_get_real_ts64() for birthtime
      ...

commit 752ade68cbd81d0321dfecc188f655a945551b25
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:27 2017 -0700

    treewide: use kv[mz]alloc* rather than opencoded variants
    
    There are many code paths opencoding kvmalloc.  Let's use the helper
    instead.  The main difference to kvmalloc is that those users are
    usually not considering all the aspects of the memory allocator.  E.g.
    allocation requests <= 32kB (with 4kB pages) are basically never failing
    and invoke OOM killer to satisfy the allocation.  This sounds too
    disruptive for something that has a reasonable fallback - the vmalloc.
    On the other hand those requests might fallback to vmalloc even when the
    memory allocator would succeed after several more reclaim/compaction
    attempts previously.  There is no guarantee something like that happens
    though.
    
    This patch converts many of those places to kv[mz]alloc* helpers because
    they are more conservative.
    
    Link: http://lkml.kernel.org/r/20170306103327.2766-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com> # Xen bits
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Andreas Dilger <andreas.dilger@intel.com> # Lustre
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com> # KVM/s390
    Acked-by: Dan Williams <dan.j.williams@intel.com> # nvdim
    Acked-by: David Sterba <dsterba@suse.com> # btrfs
    Acked-by: Ilya Dryomov <idryomov@gmail.com> # Ceph
    Acked-by: Tariq Toukan <tariqt@mellanox.com> # mlx4
    Acked-by: Leon Romanovsky <leonro@mellanox.com> # mlx5
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Anton Vorontsov <anton@enomsg.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Ben Skeggs <bskeggs@redhat.com>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Cc: Santosh Raspatur <santosh@chelsio.com>
    Cc: Hariprasad S <hariprasad@chelsio.com>
    Cc: Yishai Hadas <yishaih@mellanox.com>
    Cc: Oleg Drokin <oleg.drokin@intel.com>
    Cc: "Yan, Zheng" <zyan@redhat.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d5c5c911821a..323297e55e80 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1166,10 +1166,7 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
 		return -EINVAL;
 
-	keys = kmalloc_array(args->count, sizeof(uint8_t),
-			     GFP_KERNEL | __GFP_NOWARN);
-	if (!keys)
-		keys = vmalloc(sizeof(uint8_t) * args->count);
+	keys = kvmalloc_array(args->count, sizeof(uint8_t), GFP_KERNEL);
 	if (!keys)
 		return -ENOMEM;
 
@@ -1211,10 +1208,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
 		return -EINVAL;
 
-	keys = kmalloc_array(args->count, sizeof(uint8_t),
-			     GFP_KERNEL | __GFP_NOWARN);
-	if (!keys)
-		keys = vmalloc(sizeof(uint8_t) * args->count);
+	keys = kvmalloc_array(args->count, sizeof(uint8_t), GFP_KERNEL);
 	if (!keys)
 		return -ENOMEM;
 

commit 2d3e4866dea96b0506395b47bfefb234f2088dac
Merge: 9c6ee01ed5bb 2e5b0bd9cc61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 8 12:37:56 2017 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - HYP mode stub supports kexec/kdump on 32-bit
       - improved PMU support
       - virtual interrupt controller performance improvements
       - support for userspace virtual interrupt controller (slower, but
         necessary for KVM on the weird Broadcom SoCs used by the Raspberry
         Pi 3)
    
      MIPS:
       - basic support for hardware virtualization (ImgTec P5600/P6600/I6400
         and Cavium Octeon III)
    
      PPC:
       - in-kernel acceleration for VFIO
    
      s390:
       - support for guests without storage keys
       - adapter interruption suppression
    
      x86:
       - usual range of nVMX improvements, notably nested EPT support for
         accessed and dirty bits
       - emulation of CPL3 CPUID faulting
    
      generic:
       - first part of VCPU thread request API
       - kvm_stat improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (227 commits)
      kvm: nVMX: Don't validate disabled secondary controls
      KVM: put back #ifndef CONFIG_S390 around kvm_vcpu_kick
      Revert "KVM: Support vCPU-based gfn->hva cache"
      tools/kvm: fix top level makefile
      KVM: x86: don't hold kvm->lock in KVM_SET_GSI_ROUTING
      KVM: Documentation: remove VM mmap documentation
      kvm: nVMX: Remove superfluous VMX instruction fault checks
      KVM: x86: fix emulation of RSM and IRET instructions
      KVM: mark requests that need synchronization
      KVM: return if kvm_vcpu_wake_up() did wake up the VCPU
      KVM: add explicit barrier to kvm_vcpu_kick
      KVM: perform a wake_up in kvm_make_all_cpus_request
      KVM: mark requests that do not need a wakeup
      KVM: remove #ifndef CONFIG_S390 around kvm_vcpu_wake_up
      KVM: x86: always use kvm_make_request instead of set_bit
      KVM: add kvm_{test,clear}_request to replace {test,clear}_bit
      s390: kvm: Cpu model support for msa6, msa7 and msa8
      KVM: x86: remove irq disablement around KVM_SET_CLOCK/KVM_GET_CLOCK
      kvm: better MWAIT emulation for guests
      KVM: x86: virtualize cpuid faulting
      ...

commit 72875d8a4d92f6f37e051be522b2252fd49bd50e
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Wed Apr 26 22:32:19 2017 +0200

    KVM: add kvm_{test,clear}_request to replace {test,clear}_bit
    
    Users were expected to use kvm_check_request() for testing and clearing,
    but request have expanded their use since then and some users want to
    only test or do a faster clear.
    
    Make sure that requests are not directly accessed with bit operations.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7eb1275cc265..4bafb0a0c8b5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2496,7 +2496,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	}
 
 	/* nothing to do, just clear the request */
-	clear_bit(KVM_REQ_UNHALT, &vcpu->requests);
+	kvm_clear_request(KVM_REQ_UNHALT, vcpu);
 
 	return 0;
 }

commit e000b8e0968dd7bfa09c444607ce1e48e57aafd3
Author: Jason J. Herne <jjherne@linux.vnet.ibm.com>
Date:   Mon Mar 20 09:57:42 2017 -0400

    s390: kvm: Cpu model support for msa6, msa7 and msa8
    
    msa6 and msa7 require no changes.
    msa8 adds kma instruction and feature area.
    
    Signed-off-by: Jason J. Herne <jjherne@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8771fef112a1..7eb1275cc265 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -276,6 +276,10 @@ static void kvm_s390_cpu_feat_init(void)
 		__cpacf_query(CPACF_PPNO, (cpacf_mask_t *)
 			      kvm_s390_available_subfunc.ppno);
 
+	if (test_facility(146)) /* MSA8 */
+		__cpacf_query(CPACF_KMA, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kma);
+
 	if (MACHINE_HAS_ESOP)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
 	/*

commit 985a9d20daa67e1983910fcf6e4f348ce8eed086
Author: Harald Freudenberger <freude@linux.vnet.ibm.com>
Date:   Fri Feb 24 10:11:54 2017 +0100

    s390/crypto: Renaming PPNO to PRNO.
    
    The PPNO (Perform Pseudorandom Number Operation) instruction
    has been renamed to PRNO (Perform Random Number Operation).
    To avoid confusion and conflicts with future extensions with
    this instruction (like e.g. provide a true random number
    generator) this patch renames all occurences in cpacf.h and
    adjusts the only exploiter code which is the prng device
    driver and one line in the s390 kvm feature check.
    
    Signed-off-by: Harald Freudenberger <freude@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 28983836c0f7..d5c5c911821a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -273,7 +273,7 @@ static void kvm_s390_cpu_feat_init(void)
 			      kvm_s390_available_subfunc.pcc);
 	}
 	if (test_facility(57)) /* MSA5 */
-		__cpacf_query(CPACF_PPNO, (cpacf_mask_t *)
+		__cpacf_query(CPACF_PRNO, (cpacf_mask_t *)
 			      kvm_s390_available_subfunc.ppno);
 
 	if (MACHINE_HAS_ESOP)

commit ee71d16d22bb268c1f6a64ef6d3654ace5f1e8c7
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Apr 20 14:43:51 2017 +0200

    s390/mm: make TASK_SIZE independent from the number of page table levels
    
    The TASK_SIZE for a process should be maximum possible size of the address
    space, 2GB for a 31-bit process and 8PB for a 64-bit process. The number
    of page table levels required for a given memory layout is a consequence
    of the mapped memory areas and their location.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fd6cd05bb6a7..28983836c0f7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1512,9 +1512,9 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		kvm->arch.mem_limit = KVM_S390_NO_MEM_LIMIT;
 	} else {
 		if (sclp.hamax == U64_MAX)
-			kvm->arch.mem_limit = TASK_MAX_SIZE;
+			kvm->arch.mem_limit = TASK_SIZE_MAX;
 		else
-			kvm->arch.mem_limit = min_t(unsigned long, TASK_MAX_SIZE,
+			kvm->arch.mem_limit = min_t(unsigned long, TASK_SIZE_MAX,
 						    sclp.hamax + 1);
 		kvm->arch.gmap = gmap_create(current->mm, kvm->arch.mem_limit - 1);
 		if (!kvm->arch.gmap)

commit 730cd632c4e485b90f97fe998e300045da094938
Author: Farhan Ali <alifm@linux.vnet.ibm.com>
Date:   Fri Feb 24 16:12:56 2017 -0500

    KVM: s390: Support keyless subset guest mode
    
    If the KSS facility is available on the machine, we also make it
    available for our KVM guests.
    
    The KSS facility bypasses storage key management as long as the guest
    does not issue a related instruction. When that happens, the control is
    returned to the host, which has to turn off KSS for a guest vcpu
    before retrying the instruction.
    
    Signed-off-by: Corey S. McQuay <csmcquay@linux.vnet.ibm.com>
    Signed-off-by: Farhan Ali <alifm@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 11b7d6638991..8771fef112a1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -300,6 +300,8 @@ static void kvm_s390_cpu_feat_init(void)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_CEI);
 	if (sclp.has_ibs)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IBS);
+	if (sclp.has_kss)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_KSS);
 	/*
 	 * KVM_S390_VM_CPU_FEAT_SKEY: Wrong shadow of PTE.I bits will make
 	 * all skey handling functions read/set the skey from the PGSTE
@@ -2034,7 +2036,11 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->sdnxo = ((unsigned long) &vcpu->run->s.regs.sdnx)
 					| SDNXC;
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
-	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
+
+	if (sclp.has_kss)
+		atomic_or(CPUSTAT_KSS, &vcpu->arch.sie_block->cpuflags);
+	else
+		vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 
 	if (vcpu->kvm->arch.use_cmma) {
 		rc = kvm_s390_vcpu_setup_cmma(vcpu);

commit 47a4693e1d3eb09e523c223753fb5a97721f49b8
Author: Yi Min Zhao <zyimin@linux.vnet.ibm.com>
Date:   Fri Mar 10 09:29:38 2017 +0100

    KVM: s390: introduce AIS capability
    
    Introduce a cap to enable AIS facility bit, and add documentation
    for this capability.
    
    Signed-off-by: Yi Min Zhao <zyimin@linux.vnet.ibm.com>
    Signed-off-by: Fei Li <sherrylf@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 977cc1660a83..11b7d6638991 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -380,6 +380,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_SKEYS:
 	case KVM_CAP_S390_IRQ_STATE:
 	case KVM_CAP_S390_USER_INSTR0:
+	case KVM_CAP_S390_AIS:
 		r = 1;
 		break;
 	case KVM_CAP_S390_MEM_OP:
@@ -544,6 +545,20 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_RI %s",
 			 r ? "(not available)" : "(success)");
 		break;
+	case KVM_CAP_S390_AIS:
+		mutex_lock(&kvm->lock);
+		if (kvm->created_vcpus) {
+			r = -EBUSY;
+		} else {
+			set_kvm_facility(kvm->arch.model.fac_mask, 72);
+			set_kvm_facility(kvm->arch.model.fac_list, 72);
+			kvm->arch.float_int.ais_enabled = 1;
+			r = 0;
+		}
+		mutex_unlock(&kvm->lock);
+		VM_EVENT(kvm, 3, "ENABLE: AIS %s",
+			 r ? "(not available)" : "(success)");
+		break;
 	case KVM_CAP_S390_GS:
 		r = -EINVAL;
 		mutex_lock(&kvm->lock);

commit 519783935451764b397f2a712de5ea778ff77fdf
Author: Fei Li <sherrylf@linux.vnet.ibm.com>
Date:   Fri Feb 17 17:06:26 2017 +0800

    KVM: s390: introduce ais mode modify function
    
    Provide an interface for userspace to modify AIS
    (adapter-interruption-suppression) mode state, and add documentation
    for the interface. Allowed target modes are ALL-Interruptions mode
    and SINGLE-Interruption mode.
    
    We introduce the 'simm' and 'nimm' fields in kvm_s390_float_interrupt
    to store interruption modes for each ISC. Each bit in 'simm' and
    'nimm' targets to one ISC, and collaboratively indicate three modes:
    ALL-Interruptions, SINGLE-Interruption and NO-Interruptions. This
    interface can initiate most transitions between the states; transition
    from SINGLE-Interruption to NO-Interruptions via adapter interrupt
    injection will be introduced in a following patch. The meaningful
    combinations are as follows:
    
        interruption mode | simm bit | nimm bit
        ------------------|----------|----------
                 ALL      |    0     |     0
               SINGLE     |    1     |     0
                 NO       |    1     |     1
    
    Besides, add tracepoint to track AIS mode transitions.
    
    Co-Authored-By: Yi Min Zhao <zyimin@linux.vnet.ibm.com>
    Signed-off-by: Yi Min Zhao <zyimin@linux.vnet.ibm.com>
    Signed-off-by: Fei Li <sherrylf@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f83f18b77f3d..977cc1660a83 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1515,6 +1515,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm_s390_crypto_init(kvm);
 
+	mutex_init(&kvm->arch.float_int.ais_lock);
+	kvm->arch.float_int.simm = 0;
+	kvm->arch.float_int.nimm = 0;
+	kvm->arch.float_int.ais_enabled = 0;
 	spin_lock_init(&kvm->arch.float_int.lock);
 	for (i = 0; i < FIRQ_LIST_COUNT; i++)
 		INIT_LIST_HEAD(&kvm->arch.float_int.lists[i]);

commit 4e0b1ab72b8af961bcaca9ec1475279c1cd9579c
Author: Fan Zhang <zhangfan@linux.vnet.ibm.com>
Date:   Tue Nov 29 07:17:55 2016 +0100

    KVM: s390: gs support for kvm guests
    
    This patch adds guarded storage support for KVM guest. We need to
    setup the necessary control blocks, the kvm_run structure for the
    new registers, the necessary wrappers for VSIE, as well as the
    machine check save areas.
    GS is enabled lazily and the register saving and reloading is done in
    KVM code.  As this feature adds new content for migration, we provide
    a new capability for enablement (KVM_CAP_S390_GS).
    
    Signed-off-by: Fan Zhang <zhangfan@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 976373c03138..f83f18b77f3d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -405,6 +405,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_RI:
 		r = test_facility(64);
 		break;
+	case KVM_CAP_S390_GS:
+		r = test_facility(133);
+		break;
 	default:
 		r = 0;
 	}
@@ -541,6 +544,20 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_RI %s",
 			 r ? "(not available)" : "(success)");
 		break;
+	case KVM_CAP_S390_GS:
+		r = -EINVAL;
+		mutex_lock(&kvm->lock);
+		if (atomic_read(&kvm->online_vcpus)) {
+			r = -EBUSY;
+		} else if (test_facility(133)) {
+			set_kvm_facility(kvm->arch.model.fac_mask, 133);
+			set_kvm_facility(kvm->arch.model.fac_list, 133);
+			r = 0;
+		}
+		mutex_unlock(&kvm->lock);
+		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_GS %s",
+			 r ? "(not available)" : "(success)");
+		break;
 	case KVM_CAP_S390_USER_STSI:
 		VM_EVENT(kvm, 3, "%s", "ENABLE: CAP_S390_USER_STSI");
 		kvm->arch.user_stsi = 1;
@@ -1749,6 +1766,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	kvm_s390_set_prefix(vcpu, 0);
 	if (test_kvm_facility(vcpu->kvm, 64))
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
+	if (test_kvm_facility(vcpu->kvm, 133))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_GSCB;
 	/* fprs can be synchronized via vrs, even if the guest has no vx. With
 	 * MACHINE_HAS_VX, (load|store)_fpu_regs() will work with vrs format.
 	 */
@@ -1993,6 +2012,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= ECA_VX;
 		vcpu->arch.sie_block->ecd |= ECD_HOSTREGMGMT;
 	}
+	vcpu->arch.sie_block->sdnxo = ((unsigned long) &vcpu->run->s.regs.sdnx)
+					| SDNXC;
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 
@@ -2720,8 +2741,10 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	struct runtime_instr_cb *riccb;
+	struct gs_cb *gscb;
 
 	riccb = (struct runtime_instr_cb *) &kvm_run->s.regs.riccb;
+	gscb = (struct gs_cb *) &kvm_run->s.regs.gscb;
 	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
 	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX)
@@ -2756,6 +2779,19 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		VCPU_EVENT(vcpu, 3, "%s", "ENABLE: RI (sync_regs)");
 		vcpu->arch.sie_block->ecb3 |= ECB3_RI;
 	}
+	/*
+	 * If userspace sets the gscb (e.g. after migration) to non-zero,
+	 * we should enable GS here instead of doing the lazy enablement.
+	 */
+	if ((kvm_run->kvm_dirty_regs & KVM_SYNC_GSCB) &&
+	    test_kvm_facility(vcpu->kvm, 133) &&
+	    gscb->gssm &&
+	    !vcpu->arch.gs_enabled) {
+		VCPU_EVENT(vcpu, 3, "%s", "ENABLE: GS (sync_regs)");
+		vcpu->arch.sie_block->ecb |= ECB_GS;
+		vcpu->arch.sie_block->ecd |= ECD_HOSTREGMGMT;
+		vcpu->arch.gs_enabled = 1;
+	}
 	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	/* save host (userspace) fprs/vrs */
@@ -2770,6 +2806,20 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (test_fp_ctl(current->thread.fpu.fpc))
 		/* User space provided an invalid FPC, let's clear it */
 		current->thread.fpu.fpc = 0;
+	if (MACHINE_HAS_GS) {
+		preempt_disable();
+		__ctl_set_bit(2, 4);
+		if (current->thread.gs_cb) {
+			vcpu->arch.host_gscb = current->thread.gs_cb;
+			save_gs_cb(vcpu->arch.host_gscb);
+		}
+		if (vcpu->arch.gs_enabled) {
+			current->thread.gs_cb = (struct gs_cb *)
+						&vcpu->run->s.regs.gscb;
+			restore_gs_cb(current->thread.gs_cb);
+		}
+		preempt_enable();
+	}
 
 	kvm_run->kvm_dirty_regs = 0;
 }
@@ -2796,6 +2846,18 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	/* Restore will be done lazily at return */
 	current->thread.fpu.fpc = vcpu->arch.host_fpregs.fpc;
 	current->thread.fpu.regs = vcpu->arch.host_fpregs.regs;
+	if (MACHINE_HAS_GS) {
+		__ctl_set_bit(2, 4);
+		if (vcpu->arch.gs_enabled)
+			save_gs_cb(current->thread.gs_cb);
+		preempt_disable();
+		current->thread.gs_cb = vcpu->arch.host_gscb;
+		restore_gs_cb(vcpu->arch.host_gscb);
+		preempt_enable();
+		if (!vcpu->arch.host_gscb)
+			__ctl_clear_bit(2, 4);
+		vcpu->arch.host_gscb = NULL;
+	}
 
 }
 

commit 0c9d86833dfdafaf580cc49735661d002ef07ee3
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Mar 13 11:48:28 2017 +0100

    KVM: s390: use defines for execution controls
    
    Let's replace the bitmasks by defines. Reconstructed from code, comments
    and commit messages.
    
    Tried to keep the defines short and map them to feature names. In case
    they don't completely map to features, keep them in the stye of ICTL
    defines.
    
    This effectively drops all "U" from the existing numbers. I think this
    should be fine (as similarly done for e.g. ICTL defines).
    
    I am not 100% sure about the ECA_MVPGI and ECA_PROTEXCI bits as they are
    always used in pairs.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20170313104828.13362-1-david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [some renames, add one missing place]

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5fda4686e817..976373c03138 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1646,7 +1646,7 @@ static void sca_add_vcpu(struct kvm_vcpu *vcpu)
 		sca->cpu[vcpu->vcpu_id].sda = (__u64) vcpu->arch.sie_block;
 		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca & ~0x3fU;
-		vcpu->arch.sie_block->ecb2 |= 0x04U;
+		vcpu->arch.sie_block->ecb2 |= ECB2_ESCA;
 		set_bit_inv(vcpu->vcpu_id, (unsigned long *) sca->mcn);
 	} else {
 		struct bsca_block *sca = vcpu->kvm->arch.sca;
@@ -1700,7 +1700,7 @@ static int sca_switch_to_extended(struct kvm *kvm)
 	kvm_for_each_vcpu(vcpu_idx, vcpu, kvm) {
 		vcpu->arch.sie_block->scaoh = scaoh;
 		vcpu->arch.sie_block->scaol = scaol;
-		vcpu->arch.sie_block->ecb2 |= 0x04U;
+		vcpu->arch.sie_block->ecb2 |= ECB2_ESCA;
 	}
 	kvm->arch.sca = new_sca;
 	kvm->arch.use_esca = 1;
@@ -1939,8 +1939,8 @@ int kvm_s390_vcpu_setup_cmma(struct kvm_vcpu *vcpu)
 	if (!vcpu->arch.sie_block->cbrlo)
 		return -ENOMEM;
 
-	vcpu->arch.sie_block->ecb2 |= 0x80;
-	vcpu->arch.sie_block->ecb2 &= ~0x08;
+	vcpu->arch.sie_block->ecb2 |= ECB2_CMMA;
+	vcpu->arch.sie_block->ecb2 &= ~ECB2_PFMFI;
 	return 0;
 }
 
@@ -1970,28 +1970,28 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	/* pgste_set_pte has special handling for !MACHINE_HAS_ESOP */
 	if (MACHINE_HAS_ESOP)
-		vcpu->arch.sie_block->ecb |= 0x02;
+		vcpu->arch.sie_block->ecb |= ECB_HOSTPROTINT;
 	if (test_kvm_facility(vcpu->kvm, 9))
-		vcpu->arch.sie_block->ecb |= 0x04;
+		vcpu->arch.sie_block->ecb |= ECB_SRSI;
 	if (test_kvm_facility(vcpu->kvm, 73))
-		vcpu->arch.sie_block->ecb |= 0x10;
+		vcpu->arch.sie_block->ecb |= ECB_TE;
 
 	if (test_kvm_facility(vcpu->kvm, 8) && sclp.has_pfmfi)
-		vcpu->arch.sie_block->ecb2 |= 0x08;
+		vcpu->arch.sie_block->ecb2 |= ECB2_PFMFI;
 	if (test_kvm_facility(vcpu->kvm, 130))
-		vcpu->arch.sie_block->ecb2 |= 0x20;
-	vcpu->arch.sie_block->eca = 0x1002000U;
+		vcpu->arch.sie_block->ecb2 |= ECB2_IEP;
+	vcpu->arch.sie_block->eca = ECA_MVPGI | ECA_PROTEXCI;
 	if (sclp.has_cei)
-		vcpu->arch.sie_block->eca |= 0x80000000U;
+		vcpu->arch.sie_block->eca |= ECA_CEI;
 	if (sclp.has_ib)
-		vcpu->arch.sie_block->eca |= 0x40000000U;
+		vcpu->arch.sie_block->eca |= ECA_IB;
 	if (sclp.has_siif)
-		vcpu->arch.sie_block->eca |= 1;
+		vcpu->arch.sie_block->eca |= ECA_SII;
 	if (sclp.has_sigpif)
-		vcpu->arch.sie_block->eca |= 0x10000000U;
+		vcpu->arch.sie_block->eca |= ECA_SIGPI;
 	if (test_kvm_facility(vcpu->kvm, 129)) {
-		vcpu->arch.sie_block->eca |= 0x00020000;
-		vcpu->arch.sie_block->ecd |= 0x20000000;
+		vcpu->arch.sie_block->eca |= ECA_VX;
+		vcpu->arch.sie_block->ecd |= ECD_HOSTREGMGMT;
 	}
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
@@ -2752,9 +2752,9 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if ((kvm_run->kvm_dirty_regs & KVM_SYNC_RICCB) &&
 	    test_kvm_facility(vcpu->kvm, 64) &&
 	    riccb->valid &&
-	    !(vcpu->arch.sie_block->ecb3 & 0x01)) {
+	    !(vcpu->arch.sie_block->ecb3 & ECB3_RI)) {
 		VCPU_EVENT(vcpu, 3, "%s", "ENABLE: RI (sync_regs)");
-		vcpu->arch.sie_block->ecb3 |= 0x01;
+		vcpu->arch.sie_block->ecb3 |= ECB3_RI;
 	}
 	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);

commit 4d5f2c04c8a462a1c7ad3fdc138515c84fa65379
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Feb 9 17:15:41 2017 +0100

    KVM: s390: log runtime instrumentation enablement
    
    We handle runtime instrumentation enablement either lazy or via
    sync_regs on migration. Make sure to add a debug log entry for that
    per CPU on the first occurrence.
    
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fd6cd05bb6a7..5fda4686e817 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2719,6 +2719,9 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 
 static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
+	struct runtime_instr_cb *riccb;
+
+	riccb = (struct runtime_instr_cb *) &kvm_run->s.regs.riccb;
 	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
 	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX)
@@ -2747,12 +2750,11 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	 * we should enable RI here instead of doing the lazy enablement.
 	 */
 	if ((kvm_run->kvm_dirty_regs & KVM_SYNC_RICCB) &&
-	    test_kvm_facility(vcpu->kvm, 64)) {
-		struct runtime_instr_cb *riccb =
-			(struct runtime_instr_cb *) &kvm_run->s.regs.riccb;
-
-		if (riccb->valid)
-			vcpu->arch.sie_block->ecb3 |= 0x01;
+	    test_kvm_facility(vcpu->kvm, 64) &&
+	    riccb->valid &&
+	    !(vcpu->arch.sie_block->ecb3 & 0x01)) {
+		VCPU_EVENT(vcpu, 3, "%s", "ENABLE: RI (sync_regs)");
+		vcpu->arch.sie_block->ecb3 |= 0x01;
 	}
 	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);

commit 174cd4b1e5fbd0d74c68cf3a74f5bd4923485512
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 19:15:33 2017 +0100

    sched/headers: Prepare to move signal wakeup & sigpending methods from <linux/sched.h> into <linux/sched/signal.h>
    
    Fix up affected files that include this signal functionality via sched.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f5694838234d..fd6cd05bb6a7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -29,6 +29,8 @@
 #include <linux/timer.h>
 #include <linux/vmalloc.h>
 #include <linux/bitmap.h>
+#include <linux/sched/signal.h>
+
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/stp.h>

commit fd7e9a88348472521d999434ee02f25735c7dadf
Merge: 5066e4a34081 dd0fd8bca185
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 18:22:53 2017 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "4.11 is going to be a relatively large release for KVM, with a little
      over 200 commits and noteworthy changes for most architectures.
    
      ARM:
       - GICv3 save/restore
       - cache flushing fixes
       - working MSI injection for GICv3 ITS
       - physical timer emulation
    
      MIPS:
       - various improvements under the hood
       - support for SMP guests
       - a large rewrite of MMU emulation. KVM MIPS can now use MMU
         notifiers to support copy-on-write, KSM, idle page tracking,
         swapping, ballooning and everything else. KVM_CAP_READONLY_MEM is
         also supported, so that writes to some memory regions can be
         treated as MMIO. The new MMU also paves the way for hardware
         virtualization support.
    
      PPC:
       - support for POWER9 using the radix-tree MMU for host and guest
       - resizable hashed page table
       - bugfixes.
    
      s390:
       - expose more features to the guest
       - more SIMD extensions
       - instruction execution protection
       - ESOP2
    
      x86:
       - improved hashing in the MMU
       - faster PageLRU tracking for Intel CPUs without EPT A/D bits
       - some refactoring of nested VMX entry/exit code, preparing for live
         migration support of nested hypervisors
       - expose yet another AVX512 CPUID bit
       - host-to-guest PTP support
       - refactoring of interrupt injection, with some optimizations thrown
         in and some duct tape removed.
       - remove lazy FPU handling
       - optimizations of user-mode exits
       - optimizations of vcpu_is_preempted() for KVM guests
    
      generic:
       - alternative signaling mechanism that doesn't pound on
         tsk->sighand->siglock"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (195 commits)
      x86/kvm: Provide optimized version of vcpu_is_preempted() for x86-64
      x86/paravirt: Change vcp_is_preempted() arg type to long
      KVM: VMX: use correct vmcs_read/write for guest segment selector/base
      x86/kvm/vmx: Defer TR reload after VM exit
      x86/asm/64: Drop __cacheline_aligned from struct x86_hw_tss
      x86/kvm/vmx: Simplify segment_base()
      x86/kvm/vmx: Get rid of segment_base() on 64-bit kernels
      x86/kvm/vmx: Don't fetch the TSS base from the GDT
      x86/asm: Define the kernel TSS limit in a macro
      kvm: fix page struct leak in handle_vmon
      KVM: PPC: Book3S HV: Disable HPT resizing on POWER9 for now
      KVM: Return an error code only as a constant in kvm_get_dirty_log()
      KVM: Return an error code only as a constant in kvm_get_dirty_log_protect()
      KVM: Return directly after a failed copy_from_user() in kvm_vm_compat_ioctl()
      KVM: x86: remove code for lazy FPU handling
      KVM: race-free exit from KVM_RUN without POSIX signals
      KVM: PPC: Book3S HV: Turn "KVM guest htab" message into a debug message
      KVM: PPC: Book3S PR: Ratelimit copy data failure error messages
      KVM: Support vCPU-based gfn->hva cache
      KVM: use separate generations for each address space
      ...

commit ff47d8c05019d6e7753cef270d6399cb5a33be57
Merge: 3051bf36c25d d24b98e3a9c6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:20:04 2017 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
    
     - New entropy generation for the pseudo random number generator.
    
     - Early boot printk output via sclp to help debug crashes on boot. This
       needs to be enabled with a kernel parameter.
    
     - Add proper no-execute support with a bit in the page table entry.
    
     - Bug fixes and cleanups.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (65 commits)
      s390/syscall: fix single stepped system calls
      s390/zcrypt: make ap_bus explicitly non-modular
      s390/zcrypt: Removed unneeded debug feature directory creation.
      s390: add missing "do {} while (0)" loop constructs to multiline macros
      s390/mm: add cond_resched call to kernel page table dumper
      s390: get rid of MACHINE_HAS_PFMF and MACHINE_HAS_HPAGE
      s390/mm: make memory_block_size_bytes available for !MEMORY_HOTPLUG
      s390: replace ACCESS_ONCE with READ_ONCE
      s390: Audit and remove any remaining unnecessary uses of module.h
      s390: mm: Audit and remove any unnecessary uses of module.h
      s390: kernel: Audit and remove any unnecessary uses of module.h
      s390/kdump: Use "LINUX" ELF note name instead of "CORE"
      s390: add no-execute support
      s390: report new vector facilities
      s390: use correct input data address for setup_randomness
      s390/sclp: get rid of common response code handling
      s390/sclp: don't add new lines to each printed string
      s390/sclp: make early sclp code readable
      s390/sclp: disable early sclp code as soon as the base sclp driver is active
      s390/sclp: move early printk code to drivers
      ...

commit 460df4c1fc7c00829050c08d6368dc6e6beef307
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Feb 8 11:50:15 2017 +0100

    KVM: race-free exit from KVM_RUN without POSIX signals
    
    The purpose of the KVM_SET_SIGNAL_MASK API is to let userspace "kick"
    a VCPU out of KVM_RUN through a POSIX signal.  A signal is attached
    to a dummy signal handler; by blocking the signal outside KVM_RUN and
    unblocking it inside, this possible race is closed:
    
              VCPU thread                     service thread
       --------------------------------------------------------------
            check flag
                                              set flag
                                              raise signal
            (signal handler does nothing)
            KVM_RUN
    
    However, one issue with KVM_SET_SIGNAL_MASK is that it has to take
    tsk->sighand->siglock on every KVM_RUN.  This lock is often on a
    remote NUMA node, because it is on the node of a thread's creator.
    Taking this lock can be very expensive if there are many userspace
    exits (as is the case for SMP Windows VMs without Hyper-V reference
    time counter).
    
    As an alternative, we can put the flag directly in kvm_run so that
    KVM can see it:
    
              VCPU thread                     service thread
       --------------------------------------------------------------
                                              raise signal
            signal handler
              set run->immediate_exit
            KVM_RUN
              check run->immediate_exit
    
    Reviewed-by: Radim Krm <rkrcmar@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 502de74ea984..99e35fe0dea8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -370,6 +370,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_IRQCHIP:
 	case KVM_CAP_VM_ATTRIBUTES:
 	case KVM_CAP_MP_STATE:
+	case KVM_CAP_IMMEDIATE_EXIT:
 	case KVM_CAP_S390_INJECT_IRQ:
 	case KVM_CAP_S390_USER_SIGP:
 	case KVM_CAP_S390_USER_STSI:
@@ -2798,6 +2799,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	int rc;
 	sigset_t sigsaved;
 
+	if (kvm_run->immediate_exit)
+		return -EINTR;
+
 	if (guestdbg_exit_pending(vcpu)) {
 		kvm_s390_prepare_debug_exit(vcpu);
 		return 0;

commit d321796753f5305a00a5f4996dbbb996994df45c
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Feb 9 15:20:25 2017 -0500

    s390: Audit and remove any remaining unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each change instance
    for the presence of either and replace as needed.  An instance
    where module_param was used without moduleparam.h was also fixed,
    as well as implicit use of ptrace.h and string.h headers.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bec71e902be3..95562ee5cc44 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -23,6 +23,7 @@
 #include <linux/kvm_host.h>
 #include <linux/mman.h>
 #include <linux/module.h>
+#include <linux/moduleparam.h>
 #include <linux/random.h>
 #include <linux/slab.h>
 #include <linux/timer.h>

commit e1e8a9624f7ba8ead4f056ff558ed070e86fa747
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Thu Feb 2 16:39:31 2017 +0100

    KVM: s390: Disable dirty log retrieval for UCONTROL guests
    
    User controlled KVM guests do not support the dirty log, as they have
    no single gmap that we can check for changes.
    
    As they have no single gmap, kvm->arch.gmap is NULL and all further
    referencing to it for dirty checking will result in a NULL
    dereference.
    
    Let's return -EINVAL if a caller tries to sync dirty logs for a
    UCONTROL guest.
    
    Fixes: 15f36eb ("KVM: s390: Add proper dirty bitmap support to S390 kvm.")
    Cc: <stable@vger.kernel.org> # 3.16+
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reported-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dabd3b15bf11..502de74ea984 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -442,6 +442,9 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 	struct kvm_memory_slot *memslot;
 	int is_dirty = 0;
 
+	if (kvm_is_ucontrol(kvm))
+		return -EINVAL;
+
 	mutex_lock(&kvm->slots_lock);
 
 	r = -EINVAL;

commit a8c39dd77cb9fad0d0e5c5e6581851bdcbc1e6f6
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jan 18 16:01:02 2017 +0100

    KVM: s390: Add debug logging to basic cpu model interface
    
    Let's log something for changes in facilities, cpuid and ibc now that we
    have a cpu model in QEMU. All of these calls are pretty seldom, so we
    will not spill the log, the they will help to understand pontential
    guest issues, for example if some instructions are fenced off.
    
    As the s390 debug feature has a limited amount of parameters and
    strings must not go away we limit the facility printing to 3 double
    words, instead of building that list dynamically. This should be enough
    for several years. If we ever exceed 3 double words then the logging
    will be incomplete but no functional impact will happen.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4078ba630689..dabd3b15bf11 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -829,6 +829,13 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 		}
 		memcpy(kvm->arch.model.fac_list, proc->fac_list,
 		       S390_ARCH_FAC_LIST_SIZE_BYTE);
+		VM_EVENT(kvm, 3, "SET: guest ibc: 0x%4.4x, guest cpuid: 0x%16.16llx",
+			 kvm->arch.model.ibc,
+			 kvm->arch.model.cpuid);
+		VM_EVENT(kvm, 3, "SET: guest faclist: 0x%16.16llx.%16.16llx.%16.16llx",
+			 kvm->arch.model.fac_list[0],
+			 kvm->arch.model.fac_list[1],
+			 kvm->arch.model.fac_list[2]);
 	} else
 		ret = -EFAULT;
 	kfree(proc);
@@ -902,6 +909,13 @@ static int kvm_s390_get_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 	proc->ibc = kvm->arch.model.ibc;
 	memcpy(&proc->fac_list, kvm->arch.model.fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+	VM_EVENT(kvm, 3, "GET: guest ibc: 0x%4.4x, guest cpuid: 0x%16.16llx",
+		 kvm->arch.model.ibc,
+		 kvm->arch.model.cpuid);
+	VM_EVENT(kvm, 3, "GET: guest faclist: 0x%16.16llx.%16.16llx.%16.16llx",
+		 kvm->arch.model.fac_list[0],
+		 kvm->arch.model.fac_list[1],
+		 kvm->arch.model.fac_list[2]);
 	if (copy_to_user((void __user *)attr->addr, proc, sizeof(*proc)))
 		ret = -EFAULT;
 	kfree(proc);
@@ -925,6 +939,17 @@ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
 	       sizeof(S390_lowcore.stfle_fac_list));
+	VM_EVENT(kvm, 3, "GET: host ibc:  0x%4.4x, host cpuid:  0x%16.16llx",
+		 kvm->arch.model.ibc,
+		 kvm->arch.model.cpuid);
+	VM_EVENT(kvm, 3, "GET: host facmask:  0x%16.16llx.%16.16llx.%16.16llx",
+		 mach->fac_mask[0],
+		 mach->fac_mask[1],
+		 mach->fac_mask[2]);
+	VM_EVENT(kvm, 3, "GET: host faclist:  0x%16.16llx.%16.16llx.%16.16llx",
+		 mach->fac_list[0],
+		 mach->fac_list[1],
+		 mach->fac_list[2]);
 	if (copy_to_user((void __user *)attr->addr, mach, sizeof(*mach)))
 		ret = -EFAULT;
 	kfree(mach);

commit af0f339a6c18468421a67f361a60f14bc5144345
Merge: 4bead2a423ea 04478197416e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Jan 30 11:19:20 2017 +0100

    Merge tag 'kvm-s390-master-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kernelorgnext
    
    avoid merge conflicts, pull update for master
    also into next.

commit 3fa8cad7402cfe982a60d746609e89aafe15d131
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue May 24 12:00:49 2016 +0200

    KVM: s390: prepare to read random guest instructions
    
    We will have to read instructions not residing at the current PSW
    address.
    
    Reviewed-by: Eric Farman <farman@linux.vnet.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: David Hildenbrand <david@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 69401b8d4521..66e73f4ed64b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2588,7 +2588,7 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 	 * to look up the current opcode to get the length of the instruction
 	 * to be able to forward the PSW.
 	 */
-	rc = read_guest_instr(vcpu, &opcode, 1);
+	rc = read_guest_instr(vcpu, vcpu->arch.sie_block->gpsw.addr, &opcode, 1);
 	ilen = insn_length(opcode);
 	if (rc < 0) {
 		return rc;

commit 2f87d942be9d0f86e44fbcbd473264c26c7f1809
Author: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
Date:   Fri Jun 3 14:37:17 2016 +0200

    KVM: s390: Introduce BCD Vector Instructions to the guest
    
    We can directly forward the vector BCD instructions to the guest
    if available and VX is requested by user space.
    
    Please note that user space will have to take care of the final state
    of the facility bit when migrating to older machines.
    
    Signed-off-by: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1fd4b854efdc..69401b8d4521 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -505,6 +505,10 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		} else if (MACHINE_HAS_VX) {
 			set_kvm_facility(kvm->arch.model.fac_mask, 129);
 			set_kvm_facility(kvm->arch.model.fac_list, 129);
+			if (test_facility(134)) {
+				set_kvm_facility(kvm->arch.model.fac_mask, 134);
+				set_kvm_facility(kvm->arch.model.fac_list, 134);
+			}
 			if (test_facility(135)) {
 				set_kvm_facility(kvm->arch.model.fac_mask, 135);
 				set_kvm_facility(kvm->arch.model.fac_list, 135);

commit 53743aa7f14671dea6f3567ddca2f7d97454f3fe
Author: Maxim Samoylov <max7255@linux.vnet.ibm.com>
Date:   Wed Feb 10 10:31:23 2016 +0100

    KVM: s390: Introduce Vector Enhancements facility 1 to the guest
    
    We can directly forward the vector enhancement facility 1 to the guest
    if available and VX is requested by user space.
    
    Please note that user space will have to take care of the final state
    of the facility bit when migrating to older machines.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Maxim Samoylov <max7255@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4f74511015b8..1fd4b854efdc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -505,6 +505,10 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		} else if (MACHINE_HAS_VX) {
 			set_kvm_facility(kvm->arch.model.fac_mask, 129);
 			set_kvm_facility(kvm->arch.model.fac_list, 129);
+			if (test_facility(135)) {
+				set_kvm_facility(kvm->arch.model.fac_mask, 135);
+				set_kvm_facility(kvm->arch.model.fac_list, 135);
+			}
 			r = 0;
 		} else
 			r = -EINVAL;

commit d051ae531324fb5130366d47e05bf8eadeb95535
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Dec 13 14:25:32 2016 +0100

    KVM: s390: get rid of bogus cc initialization
    
    The plo inline assembly has a cc output operand that is always written
    to and is also as such an operand declared. Therefore the compiler is
    free to omit the rather pointless and misleading initialization.
    
    Get rid of this.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e5130818fb03..4f74511015b8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -217,7 +217,7 @@ static void allow_cpu_feat(unsigned long nr)
 static inline int plo_test_bit(unsigned char nr)
 {
 	register unsigned long r0 asm("0") = (unsigned long) nr | 0x100;
-	int cc = 3; /* subfunction not available */
+	int cc;
 
 	asm volatile(
 		/* Parameter registers are ignored for "test bit" */

commit cd1836f583d78bdd15ef748f4d85bf007569c7ad
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Thu Aug 4 09:57:36 2016 +0200

    KVM: s390: instruction-execution-protection support
    
    The new Instruction Execution Protection needs to be enabled before
    the guest can use it. Therefore we pass the IEP facility bit to the
    guest and enable IEP interpretation.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bec71e902be3..e5130818fb03 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1938,6 +1938,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	if (test_kvm_facility(vcpu->kvm, 8) && sclp.has_pfmfi)
 		vcpu->arch.sie_block->ecb2 |= 0x08;
+	if (test_kvm_facility(vcpu->kvm, 130))
+		vcpu->arch.sie_block->ecb2 |= 0x20;
 	vcpu->arch.sie_block->eca = 0x1002000U;
 	if (sclp.has_cei)
 		vcpu->arch.sie_block->eca |= 0x80000000U;

commit 04478197416e3a302e9ebc917ba1aa884ef9bfab
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Jan 12 16:25:15 2017 +0100

    KVM: s390: do not expose random data via facility bitmap
    
    kvm_s390_get_machine() populates the facility bitmap by copying bytes
    from the host results that are stored in a 256 byte array in the prefix
    page. The KVM code does use the size of the target buffer (2k), thus
    copying and exposing unrelated kernel memory (mostly machine check
    related logout data).
    
    Let's use the size of the source buffer instead.  This is ok, as the
    target buffer will always be greater or equal than the source buffer as
    the KVM internal buffers (and thus S390_ARCH_FAC_LIST_SIZE_BYTE) cover
    the maximum possible size that is allowed by STFLE, which is 256
    doublewords. All structures are zero allocated so we can leave bytes
    256-2047 unchanged.
    
    Add a similar fix for kvm_arch_init_vm().
    
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    [found with smatch]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    CC: stable@vger.kernel.org
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bec71e902be3..6484a250021e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -916,7 +916,7 @@ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 	memcpy(&mach->fac_mask, kvm->arch.model.fac_mask,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
-	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+	       sizeof(S390_lowcore.stfle_fac_list));
 	if (copy_to_user((void __user *)attr->addr, mach, sizeof(*mach)))
 		ret = -EFAULT;
 	kfree(mach);
@@ -1437,7 +1437,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	/* Populate the facility mask initially. */
 	memcpy(kvm->arch.model.fac_mask, S390_lowcore.stfle_fac_list,
-	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+	       sizeof(S390_lowcore.stfle_fac_list));
 	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
 		if (i < kvm_s390_fac_list_mask_size())
 			kvm->arch.model.fac_mask[i] &= kvm_s390_fac_list_mask[i];

commit e1788bb995befed5831c99cee6527dfb080a46c0
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Nov 22 09:29:38 2016 +0100

    KVM: s390: handle floating point registers in the run ioctl not in vcpu_put/load
    
    Right now we switch the host fprs/vrs in kvm_arch_vcpu_load and switch
    back in kvm_arch_vcpu_put. This process is already optimized
    since commit 9977e886cbbc7 ("s390/kernel: lazy restore fpu registers")
    avoiding double save/restores on schedule. We still reload the pointers
    and test the guest fpc on each context switch, though.
    
    We can minimize the cost of vcpu_load/put by doing the test in the
    VCPU_RUN ioctl itself. As most VCPU threads almost never exit to
    userspace in the common fast path, this allows to avoid this overhead
    for the common case (eventfd driven I/O, all exits including sleep
    handled in the kernel) - making kvm_arch_vcpu_load/put basically
    disappear in perf top.
    
    Also adapt the fpu get/set ioctls.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4105e1ea8dda..bec71e902be3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1812,19 +1812,6 @@ __u64 kvm_s390_get_cpu_timer(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
-	/* Save host register state */
-	save_fpu_regs();
-	vcpu->arch.host_fpregs.fpc = current->thread.fpu.fpc;
-	vcpu->arch.host_fpregs.regs = current->thread.fpu.regs;
-
-	if (MACHINE_HAS_VX)
-		current->thread.fpu.regs = vcpu->run->s.regs.vrs;
-	else
-		current->thread.fpu.regs = vcpu->run->s.regs.fprs;
-	current->thread.fpu.fpc = vcpu->run->s.regs.fpc;
-	if (test_fp_ctl(current->thread.fpu.fpc))
-		/* User space provided an invalid FPC, let's clear it */
-		current->thread.fpu.fpc = 0;
 
 	gmap_enable(vcpu->arch.enabled_gmap);
 	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
@@ -1842,13 +1829,6 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	vcpu->arch.enabled_gmap = gmap_get_enabled();
 	gmap_disable(vcpu->arch.enabled_gmap);
 
-	/* Save guest register state */
-	save_fpu_regs();
-	vcpu->run->s.regs.fpc = current->thread.fpu.fpc;
-
-	/* Restore host register state */
-	current->thread.fpu.fpc = vcpu->arch.host_fpregs.fpc;
-	current->thread.fpu.regs = vcpu->arch.host_fpregs.regs;
 }
 
 static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
@@ -2251,11 +2231,9 @@ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 
 int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
-	/* make sure the new values will be lazily loaded */
-	save_fpu_regs();
 	if (test_fp_ctl(fpu->fpc))
 		return -EINVAL;
-	current->thread.fpu.fpc = fpu->fpc;
+	vcpu->run->s.regs.fpc = fpu->fpc;
 	if (MACHINE_HAS_VX)
 		convert_fp_to_vx((__vector128 *) vcpu->run->s.regs.vrs,
 				 (freg_t *) fpu->fprs);
@@ -2273,7 +2251,7 @@ int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 				 (__vector128 *) vcpu->run->s.regs.vrs);
 	else
 		memcpy(fpu->fprs, vcpu->run->s.regs.fprs, sizeof(fpu->fprs));
-	fpu->fpc = current->thread.fpu.fpc;
+	fpu->fpc = vcpu->run->s.regs.fpc;
 	return 0;
 }
 
@@ -2736,6 +2714,18 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	}
 	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);
+	/* save host (userspace) fprs/vrs */
+	save_fpu_regs();
+	vcpu->arch.host_fpregs.fpc = current->thread.fpu.fpc;
+	vcpu->arch.host_fpregs.regs = current->thread.fpu.regs;
+	if (MACHINE_HAS_VX)
+		current->thread.fpu.regs = vcpu->run->s.regs.vrs;
+	else
+		current->thread.fpu.regs = vcpu->run->s.regs.fprs;
+	current->thread.fpu.fpc = vcpu->run->s.regs.fpc;
+	if (test_fp_ctl(current->thread.fpu.fpc))
+		/* User space provided an invalid FPC, let's clear it */
+		current->thread.fpu.fpc = 0;
 
 	kvm_run->kvm_dirty_regs = 0;
 }
@@ -2756,6 +2746,13 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	kvm_run->s.regs.pfc = vcpu->arch.pfault_compare;
 	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_access_regs(vcpu->arch.host_acrs);
+	/* Save guest register state */
+	save_fpu_regs();
+	vcpu->run->s.regs.fpc = current->thread.fpu.fpc;
+	/* Restore will be done lazily at return */
+	current->thread.fpu.fpc = vcpu->arch.host_fpregs.fpc;
+	current->thread.fpu.regs = vcpu->arch.host_fpregs.regs;
+
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)

commit 31d8b8d41a7e3e8db081972a63ef1de276ef8ab4
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Nov 10 14:22:02 2016 +0100

    KVM: s390: handle access registers in the run ioctl not in vcpu_put/load
    
    Right now we save the host access registers in kvm_arch_vcpu_load
    and load them in kvm_arch_vcpu_put. Vice versa for the guest access
    registers. On schedule this means, that we load/save access registers
    multiple times.
    
    e.g. VCPU_RUN with just one reschedule and then return does
    
    [from user space via VCPU_RUN]
    - save the host registers in kvm_arch_vcpu_load (via ioctl)
    - load the guest registers in kvm_arch_vcpu_load (via ioctl)
    - do guest stuff
    - decide to schedule/sleep
    - save the guest registers in kvm_arch_vcpu_put (via sched)
    - load the host registers in kvm_arch_vcpu_put (via sched)
    - save the host registers in switch_to (via sched)
    - schedule
    - return
    - load the host registers in switch_to (via sched)
    - save the host registers in kvm_arch_vcpu_load (via sched)
    - load the guest registers in kvm_arch_vcpu_load (via sched)
    - do guest stuff
    - decide to go to userspace
    - save the guest registers in kvm_arch_vcpu_put (via ioctl)
    - load the host registers in kvm_arch_vcpu_put (via ioctl)
    [back to user space]
    
    As the kernel does not use access registers, we can avoid
    this reloading and simply piggy back on switch_to (let it save
    the guest values instead of host values in thread.acrs) by
    moving the host/guest switch into the VCPU_RUN ioctl function.
    We now do
    
    [from user space via VCPU_RUN]
    - save the host registers in kvm_arch_vcpu_ioctl_run
    - load the guest registers in kvm_arch_vcpu_ioctl_run
    - do guest stuff
    - decide to schedule/sleep
    - save the guest registers in switch_to
    - schedule
    - return
    - load the guest registers in switch_to (via sched)
    - do guest stuff
    - decide to go to userspace
    - save the guest registers in kvm_arch_vcpu_ioctl_run
    - load the host registers in kvm_arch_vcpu_ioctl_run
    
    This seems to save about 10% of the vcpu_put/load functions
    according to perf.
    
    As vcpu_load no longer switches the acrs, We can also loading
    the acrs in kvm_arch_vcpu_ioctl_set_sregs.
    
    Suggested-by: Fan Zhang <zhangfan@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9c7a1ecfe6bd..4105e1ea8dda 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1826,8 +1826,6 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		/* User space provided an invalid FPC, let's clear it */
 		current->thread.fpu.fpc = 0;
 
-	save_access_regs(vcpu->arch.host_acrs);
-	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.enabled_gmap);
 	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	if (vcpu->arch.cputm_enabled && !is_vcpu_idle(vcpu))
@@ -1851,9 +1849,6 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	/* Restore host register state */
 	current->thread.fpu.fpc = vcpu->arch.host_fpregs.fpc;
 	current->thread.fpu.regs = vcpu->arch.host_fpregs.regs;
-
-	save_access_regs(vcpu->run->s.regs.acrs);
-	restore_access_regs(vcpu->arch.host_acrs);
 }
 
 static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
@@ -2243,7 +2238,6 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 {
 	memcpy(&vcpu->run->s.regs.acrs, &sregs->acrs, sizeof(sregs->acrs));
 	memcpy(&vcpu->arch.sie_block->gcr, &sregs->crs, sizeof(sregs->crs));
-	restore_access_regs(vcpu->run->s.regs.acrs);
 	return 0;
 }
 
@@ -2740,6 +2734,8 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		if (riccb->valid)
 			vcpu->arch.sie_block->ecb3 |= 0x01;
 	}
+	save_access_regs(vcpu->arch.host_acrs);
+	restore_access_regs(vcpu->run->s.regs.acrs);
 
 	kvm_run->kvm_dirty_regs = 0;
 }
@@ -2758,6 +2754,8 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	kvm_run->s.regs.pft = vcpu->arch.pfault_token;
 	kvm_run->s.regs.pfs = vcpu->arch.pfault_select;
 	kvm_run->s.regs.pfc = vcpu->arch.pfault_compare;
+	save_access_regs(vcpu->run->s.regs.acrs);
+	restore_access_regs(vcpu->arch.host_acrs);
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
@@ -2874,7 +2872,7 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 {
 	/*
 	 * The guest FPRS and ACRS are in the host FPRS/ACRS due to the lazy
-	 * copying in vcpu load/put. Lets update our copies before we save
+	 * switch in the run ioctl. Let's update our copies before we save
 	 * it into the save area
 	 */
 	save_fpu_regs();

commit 6218590bcb452c3da7517d02b588d4d0a8628f73
Merge: 14986a34e128 d9ab710b8531
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 6 10:49:01 2016 -0700

    Merge tag 'kvm-4.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krm:
     "All architectures:
       - move `make kvmconfig` stubs from x86
       - use 64 bits for debugfs stats
    
      ARM:
       - Important fixes for not using an in-kernel irqchip
       - handle SError exceptions and present them to guests if appropriate
       - proxying of GICV access at EL2 if guest mappings are unsafe
       - GICv3 on AArch32 on ARMv8
       - preparations for GICv3 save/restore, including ABI docs
       - cleanups and a bit of optimizations
    
      MIPS:
       - A couple of fixes in preparation for supporting MIPS EVA host
         kernels
       - MIPS SMP host & TLB invalidation fixes
    
      PPC:
       - Fix the bug which caused guests to falsely report lockups
       - other minor fixes
       - a small optimization
    
      s390:
       - Lazy enablement of runtime instrumentation
       - up to 255 CPUs for nested guests
       - rework of machine check deliver
       - cleanups and fixes
    
      x86:
       - IOMMU part of AMD's AVIC for vmexit-less interrupt delivery
       - Hyper-V TSC page
       - per-vcpu tsc_offset in debugfs
       - accelerated INS/OUTS in nVMX
       - cleanups and fixes"
    
    * tag 'kvm-4.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (140 commits)
      KVM: MIPS: Drop dubious EntryHi optimisation
      KVM: MIPS: Invalidate TLB by regenerating ASIDs
      KVM: MIPS: Split kernel/user ASID regeneration
      KVM: MIPS: Drop other CPU ASIDs on guest MMU changes
      KVM: arm/arm64: vgic: Don't flush/sync without a working vgic
      KVM: arm64: Require in-kernel irqchip for PMU support
      KVM: PPC: Book3s PR: Allow access to unprivileged MMCR2 register
      KVM: PPC: Book3S PR: Support 64kB page size on POWER8E and POWER8NVL
      KVM: PPC: Book3S: Remove duplicate setting of the B field in tlbie
      KVM: PPC: BookE: Fix a sanity check
      KVM: PPC: Book3S HV: Take out virtual core piggybacking code
      KVM: PPC: Book3S: Treat VTB as a per-subcore register, not per-thread
      ARM: gic-v3: Work around definition of gic_write_bpr1
      KVM: nVMX: Fix the NMI IDT-vectoring handling
      KVM: VMX: Enable MSR-BASED TPR shadow even if APICv is inactive
      KVM: nVMX: Fix reload apic access page warning
      kvmconfig: add virtio-gpu to config fragment
      config: move x86 kvm_guest.config to a common location
      arm64: KVM: Remove duplicating init code for setting VMID
      ARM: KVM: Support vgic-v3
      ...

commit e46cae441824999c858d482ca1f661cf4292c02f
Merge: 02bafd96f3a5 6cd997db911f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 4 14:05:52 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "The new features and main improvements in this merge for v4.9
    
       - Support for the UBSAN sanitizer
    
       - Set HAVE_EFFICIENT_UNALIGNED_ACCESS, it improves the code in some
         places
    
       - Improvements for the in-kernel fpu code, in particular the overhead
         for multiple consecutive in kernel fpu users is recuded
    
       - Add a SIMD implementation for the RAID6 gen and xor operations
    
       - Add RAID6 recovery based on the XC instruction
    
       - The PCI DMA flush logic has been improved to increase the speed of
         the map / unmap operations
    
       - The time synchronization code has seen some updates
    
      And bug fixes all over the place"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (48 commits)
      s390/con3270: fix insufficient space padding
      s390/con3270: fix use of uninitialised data
      MAINTAINERS: update DASD maintainer
      s390/cio: fix accidental interrupt enabling during resume
      s390/dasd: add missing \n to end of dev_err messages
      s390/config: Enable config options for Docker
      s390/dasd: make query host access interruptible
      s390/dasd: fix panic during offline processing
      s390/dasd: fix hanging offline processing
      s390/pci_dma: improve lazy flush for unmap
      s390/pci_dma: split dma_update_trans
      s390/pci_dma: improve map_sg
      s390/pci_dma: simplify dma address calculation
      s390/pci_dma: remove dma address range check
      iommu/s390: simplify registration of I/O address translation parameters
      s390: migrate exception table users off module.h and onto extable.h
      s390: export header for CLP ioctl
      s390/vmur: fix irq pointer dereference in int handler
      s390/dasd: add missing KOBJ_CHANGE event for unformatted devices
      s390: enable UBSAN
      ...

commit 235539b48a2357da28f52d66d04bec04f3dcb9dd
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Wed Sep 7 14:47:23 2016 -0400

    kvm: add stubs for arch specific debugfs support
    
    Two stubs are added:
    
     o kvm_arch_has_vcpu_debugfs(): must return true if the arch
       supports creating debugfs entries in the vcpu debugfs dir
       (which will be implemented by the next commit)
    
     o kvm_arch_create_vcpu_debugfs(): code that creates debugfs
       entries in the vcpu debugfs dir
    
    For x86, this commit introduces a new file to avoid growing
    arch/x86/kvm/x86.c even more.
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ac6c056df4b9..0a9ce9dd7825 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1489,6 +1489,16 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	return rc;
 }
 
+bool kvm_arch_has_vcpu_debugfs(void)
+{
+	return false;
+}
+
+int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");

commit b0eb91ae630a4f2771790e306bb4df323127a397
Merge: 0624a8eb82ef f6c1d359be6b
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Sep 8 13:41:08 2016 +0200

    Merge remote-tracking branch 'kvms390/s390forkvm' into kvms390next

commit a6940674c384ebf56aa0c44f417032de2b67100c
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Aug 8 22:39:32 2016 +0200

    KVM: s390: allow 255 VCPUs when sca entries aren't used
    
    If the SCA entries aren't used by the hardware (no SIGPIF), we
    can simply not set the entries, stick to the basic sca and allow more
    than 64 VCPUs.
    
    To hinder any other facility from using these entries, let's properly
    provoke intercepts by not setting the MCN and keeping the entries
    unset.
    
    This effectively allows when running KVM under KVM (vSIE) or under z/VM to
    provide more than 64 VCPUs to a guest. Let's limit it to 255 for now, to
    not run into problems if the CPU numbers are limited somewhere else.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 892abf4653a8..3a628eb3156f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -376,7 +376,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_NR_VCPUS:
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_S390_BSCA_CPU_SLOTS;
-		if (sclp.has_esca && sclp.has_64bscao)
+		if (!kvm_s390_use_sca_entries())
+			r = KVM_MAX_VCPUS;
+		else if (sclp.has_esca && sclp.has_64bscao)
 			r = KVM_S390_ESCA_CPU_SLOTS;
 		break;
 	case KVM_CAP_NR_MEMSLOTS:
@@ -1553,6 +1555,8 @@ static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
 
 static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 {
+	if (!kvm_s390_use_sca_entries())
+		return;
 	read_lock(&vcpu->kvm->arch.sca_lock);
 	if (vcpu->kvm->arch.use_esca) {
 		struct esca_block *sca = vcpu->kvm->arch.sca;
@@ -1570,6 +1574,13 @@ static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 
 static void sca_add_vcpu(struct kvm_vcpu *vcpu)
 {
+	if (!kvm_s390_use_sca_entries()) {
+		struct bsca_block *sca = vcpu->kvm->arch.sca;
+
+		/* we still need the basic sca for the ipte control */
+		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
+		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
+	}
 	read_lock(&vcpu->kvm->arch.sca_lock);
 	if (vcpu->kvm->arch.use_esca) {
 		struct esca_block *sca = vcpu->kvm->arch.sca;
@@ -1650,6 +1661,11 @@ static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
 {
 	int rc;
 
+	if (!kvm_s390_use_sca_entries()) {
+		if (id < KVM_MAX_VCPUS)
+			return true;
+		return false;
+	}
 	if (id < KVM_S390_BSCA_CPU_SLOTS)
 		return true;
 	if (!sclp.has_esca || !sclp.has_64bscao)

commit 80cd8763388b52fa9129cbb4b57a3615a55afd40
Author: Fan Zhang <zhangfan@linux.vnet.ibm.com>
Date:   Mon Aug 15 04:53:22 2016 +0200

    KVM: s390: lazy enable RI
    
    Only enable runtime instrumentation if the guest issues an RI related
    instruction or if userspace changes the riccb to a valid state.
    This makes entry/exit a tiny bit faster.
    
    Initial patch by Christian Borntraeger
    Signed-off-by: Fan Zhang <zhangfan@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dead20ae9c00..892abf4653a8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1938,8 +1938,6 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= 1;
 	if (sclp.has_sigpif)
 		vcpu->arch.sie_block->eca |= 0x10000000U;
-	if (test_kvm_facility(vcpu->kvm, 64))
-		vcpu->arch.sie_block->ecb3 |= 0x01;
 	if (test_kvm_facility(vcpu->kvm, 129)) {
 		vcpu->arch.sie_block->eca |= 0x00020000;
 		vcpu->arch.sie_block->ecd |= 0x20000000;
@@ -2694,6 +2692,19 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		if (vcpu->arch.pfault_token == KVM_S390_PFAULT_TOKEN_INVALID)
 			kvm_clear_async_pf_completion_queue(vcpu);
 	}
+	/*
+	 * If userspace sets the riccb (e.g. after migration) to a valid state,
+	 * we should enable RI here instead of doing the lazy enablement.
+	 */
+	if ((kvm_run->kvm_dirty_regs & KVM_SYNC_RICCB) &&
+	    test_kvm_facility(vcpu->kvm, 64)) {
+		struct runtime_instr_cb *riccb =
+			(struct runtime_instr_cb *) &kvm_run->s.regs.riccb;
+
+		if (riccb->valid)
+			vcpu->arch.sie_block->ecb3 |= 0x01;
+	}
+
 	kvm_run->kvm_dirty_regs = 0;
 }
 

commit ff5dc1492a11a6c90955ab34063be1cddc54ec00
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Oct 14 16:57:56 2015 +0200

    KVM: s390: fix delivery of vector regs during machine checks
    
    Vector registers are only to be stored if the facility is available
    and if the guest has set up the machine check extended save area.
    
    If anything goes wrong while writing the vector registers, the vector
    registers are to be marked as invalid. Please note that we are allowed
    to write the registers although they are marked as invalid.
    
    Machine checks and "store status" SIGP orders are two different concepts,
    let's correctly separate these. As the SIGP part is completely handled in
    user space, we can drop it.
    
    This patch is based on a patch from Cornelia Huck.
    
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f142215ed30d..dead20ae9c00 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2837,38 +2837,6 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return kvm_s390_store_status_unloaded(vcpu, addr);
 }
 
-/*
- * store additional status at address
- */
-int kvm_s390_store_adtl_status_unloaded(struct kvm_vcpu *vcpu,
-					unsigned long gpa)
-{
-	/* Only bits 0-53 are used for address formation */
-	if (!(gpa & ~0x3ff))
-		return 0;
-
-	return write_guest_abs(vcpu, gpa & ~0x3ff,
-			       (void *)&vcpu->run->s.regs.vrs, 512);
-}
-
-int kvm_s390_vcpu_store_adtl_status(struct kvm_vcpu *vcpu, unsigned long addr)
-{
-	if (!test_kvm_facility(vcpu->kvm, 129))
-		return 0;
-
-	/*
-	 * The guest VXRS are in the host VXRs due to the lazy
-	 * copying in vcpu load/put. We can simply call save_fpu_regs()
-	 * to save the current register state because we are in the
-	 * middle of a load/put cycle.
-	 *
-	 * Let's update our copies before we save it into the save area.
-	 */
-	save_fpu_regs();
-
-	return kvm_s390_store_adtl_status_unloaded(vcpu, addr);
-}
-
 static void __disable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
 {
 	kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu);

commit 69c0e360f990c2dc737681f40a361195066cef02
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Aug 18 12:59:46 2016 +0200

    s390/crypto: cpacf function detection
    
    The CPACF code makes some assumptions about the availablity of hardware
    support. E.g. if the machine supports KM(AES-256) without chaining it is
    assumed that KMC(AES-256) with chaining is available as well. For the
    existing CPUs this is true but the architecturally correct way is to
    check each CPACF functions on its own. This is what the query function
    of each instructions is all about.
    
    Reviewed-by: Harald Freudenberger <freude@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fd2f1201204e..d6e7e527f0bf 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -245,22 +245,33 @@ static void kvm_s390_cpu_feat_init(void)
 		     PTFF_QAF);
 
 	if (test_facility(17)) { /* MSA */
-		__cpacf_query(CPACF_KMAC, kvm_s390_available_subfunc.kmac);
-		__cpacf_query(CPACF_KMC, kvm_s390_available_subfunc.kmc);
-		__cpacf_query(CPACF_KM, kvm_s390_available_subfunc.km);
-		__cpacf_query(CPACF_KIMD, kvm_s390_available_subfunc.kimd);
-		__cpacf_query(CPACF_KLMD, kvm_s390_available_subfunc.klmd);
+		__cpacf_query(CPACF_KMAC, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kmac);
+		__cpacf_query(CPACF_KMC, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kmc);
+		__cpacf_query(CPACF_KM, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.km);
+		__cpacf_query(CPACF_KIMD, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kimd);
+		__cpacf_query(CPACF_KLMD, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.klmd);
 	}
 	if (test_facility(76)) /* MSA3 */
-		__cpacf_query(CPACF_PCKMO, kvm_s390_available_subfunc.pckmo);
+		__cpacf_query(CPACF_PCKMO, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.pckmo);
 	if (test_facility(77)) { /* MSA4 */
-		__cpacf_query(CPACF_KMCTR, kvm_s390_available_subfunc.kmctr);
-		__cpacf_query(CPACF_KMF, kvm_s390_available_subfunc.kmf);
-		__cpacf_query(CPACF_KMO, kvm_s390_available_subfunc.kmo);
-		__cpacf_query(CPACF_PCC, kvm_s390_available_subfunc.pcc);
+		__cpacf_query(CPACF_KMCTR, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kmctr);
+		__cpacf_query(CPACF_KMF, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kmf);
+		__cpacf_query(CPACF_KMO, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.kmo);
+		__cpacf_query(CPACF_PCC, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.pcc);
 	}
 	if (test_facility(57)) /* MSA5 */
-		__cpacf_query(CPACF_PPNO, kvm_s390_available_subfunc.ppno);
+		__cpacf_query(CPACF_PPNO, (cpacf_mask_t *)
+			      kvm_s390_available_subfunc.ppno);
 
 	if (MACHINE_HAS_ESOP)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);

commit 88bf46bfde1eaa5f27bc5df75912b1ab7c7baf7e
Merge: 2ccb5bf0e2f1 f6c1d359be6b
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Aug 26 16:27:15 2016 +0200

    Merge branch 's390forkvm' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
    
    Pull facility mask patch from the KVM tree.
    
    * tag 's390forkvm' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux
      KVM: s390: generate facility mask from readable list

commit f6c1d359be6bb0aa0715b4b75d9ecf63bdb07c4a
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Aug 16 10:31:10 2016 +0200

    KVM: s390: generate facility mask from readable list
    
    Automatically generate the KVM facility mask out of a readable list.
    Manually changing the masks is very error prone, especially if the
    special IBM bit numbering has to be considered.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3f3ae4865d57..4f484e09eeb9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -132,10 +132,7 @@ module_param(nested, int, S_IRUGO);
 MODULE_PARM_DESC(nested, "Nested virtualization support");
 
 /* upper facilities limit for kvm */
-unsigned long kvm_s390_fac_list_mask[16] = {
-	0xffe6000000000000UL,
-	0x005e000000000000UL,
-};
+unsigned long kvm_s390_fac_list_mask[16] = { FACILITIES_KVM };
 
 unsigned long kvm_s390_fac_list_mask_size(void)
 {

commit a7d4b8f2565ad0dfdff9a222d1d87990c73b36e8
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Aug 16 14:38:24 2016 +0200

    KVM: s390: don't use current->thread.fpu.* when accessing registers
    
    As the meaning of these variables and pointers seems to change more
    frequently, let's directly access our save area, instead of going via
    current->thread.
    
    Right now, this is broken for set/get_fpu. They simply overwrite the
    host registers, as the pointers to the current save area were turned
    into the static host save area.
    
    Cc: stable@vger.kernel.org # 4.7
    Fixes: 3f6813b9a5e0 ("s390/fpu: allocate 'struct fpu' with the task_struct")
    Reported-by: Hao QingFeng <haoqf@linux.vnet.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f142215ed30d..607ec91966c7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2231,9 +2231,10 @@ int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 		return -EINVAL;
 	current->thread.fpu.fpc = fpu->fpc;
 	if (MACHINE_HAS_VX)
-		convert_fp_to_vx(current->thread.fpu.vxrs, (freg_t *)fpu->fprs);
+		convert_fp_to_vx((__vector128 *) vcpu->run->s.regs.vrs,
+				 (freg_t *) fpu->fprs);
 	else
-		memcpy(current->thread.fpu.fprs, &fpu->fprs, sizeof(fpu->fprs));
+		memcpy(vcpu->run->s.regs.fprs, &fpu->fprs, sizeof(fpu->fprs));
 	return 0;
 }
 
@@ -2242,9 +2243,10 @@ int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 	/* make sure we have the latest values */
 	save_fpu_regs();
 	if (MACHINE_HAS_VX)
-		convert_vx_to_fp((freg_t *)fpu->fprs, current->thread.fpu.vxrs);
+		convert_vx_to_fp((freg_t *) fpu->fprs,
+				 (__vector128 *) vcpu->run->s.regs.vrs);
 	else
-		memcpy(fpu->fprs, current->thread.fpu.fprs, sizeof(fpu->fprs));
+		memcpy(fpu->fprs, vcpu->run->s.regs.fprs, sizeof(fpu->fprs));
 	fpu->fpc = current->thread.fpu.fpc;
 	return 0;
 }

commit aca411a4b17a4aebe14ecdf253373db5b7ee6058
Author: Julius Niedworok <jniedwor@linux.vnet.ibm.com>
Date:   Wed Aug 3 16:39:55 2016 +0200

    KVM: s390: reset KVM_REQ_MMU_RELOAD if mapping the prefix failed
    
    When triggering KVM_RUN without a user memory region being mapped
    (KVM_SET_USER_MEMORY_REGION) a validity intercept occurs. This could
    happen, if the user memory region was not mapped initially or if it
    was unmapped after the vcpu is initialized. The function
    kvm_s390_handle_requests checks for the KVM_REQ_MMU_RELOAD bit. The
    check function always clears this bit. If gmap_mprotect_notify
    returns an error code, the mapping failed, but the KVM_REQ_MMU_RELOAD
    was not set anymore. So the next time kvm_s390_handle_requests is
    called, the execution would fall trough the check for
    KVM_REQ_MMU_RELOAD. The bit needs to be resetted, if
    gmap_mprotect_notify returns an error code. Resetting the bit with
    kvm_make_request(KVM_REQ_MMU_RELOAD, vcpu) fixes the bug.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Julius Niedworok <jniedwor@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e63f6ed0a936..f142215ed30d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2362,8 +2362,10 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 		rc = gmap_mprotect_notify(vcpu->arch.gmap,
 					  kvm_s390_get_prefix(vcpu),
 					  PAGE_SIZE * 2, PROT_WRITE);
-		if (rc)
+		if (rc) {
+			kvm_make_request(KVM_REQ_MMU_RELOAD, vcpu);
 			return rc;
+		}
 		goto retry;
 	}
 

commit 75a4615c95a2de4fa592c1312fb8446e74b4e5eb
Author: Julius Niedworok <jniedwor@linux.vnet.ibm.com>
Date:   Wed Aug 3 16:39:54 2016 +0200

    KVM: s390: set the prefix initially properly
    
    When KVM_RUN is triggered on a VCPU without an initial reset, a
    validity intercept occurs.
    Setting the prefix will set the KVM_REQ_MMU_RELOAD bit initially,
    thus preventing the bug.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Julius Niedworok <jniedwor@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3f3ae4865d57..e63f6ed0a936 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1672,6 +1672,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 				    KVM_SYNC_CRS |
 				    KVM_SYNC_ARCH0 |
 				    KVM_SYNC_PFAULT;
+	kvm_s390_set_prefix(vcpu, 0);
 	if (test_kvm_facility(vcpu->kvm, 64))
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
 	/* fprs can be synchronized via vrs, even if the guest has no vx. With

commit 221bb8a46e230b9824204ae86537183d9991ff2a
Merge: f7b32e4c021f 23528bb21ee2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 16:11:27 2016 -0400

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
    
     - ARM: GICv3 ITS emulation and various fixes.  Removal of the
       old VGIC implementation.
    
     - s390: support for trapping software breakpoints, nested
       virtualization (vSIE), the STHYI opcode, initial extensions
       for CPU model support.
    
     - MIPS: support for MIPS64 hosts (32-bit guests only) and lots
       of cleanups, preliminary to this and the upcoming support for
       hardware virtualization extensions.
    
     - x86: support for execute-only mappings in nested EPT; reduced
       vmexit latency for TSC deadline timer (by about 30%) on Intel
       hosts; support for more than 255 vCPUs.
    
     - PPC: bugfixes.
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (302 commits)
      KVM: PPC: Introduce KVM_CAP_PPC_HTM
      MIPS: Select HAVE_KVM for MIPS64_R{2,6}
      MIPS: KVM: Reset CP0_PageMask during host TLB flush
      MIPS: KVM: Fix ptr->int cast via KVM_GUEST_KSEGX()
      MIPS: KVM: Sign extend MFC0/RDHWR results
      MIPS: KVM: Fix 64-bit big endian dynamic translation
      MIPS: KVM: Fail if ebase doesn't fit in CP0_EBase
      MIPS: KVM: Use 64-bit CP0_EBase when appropriate
      MIPS: KVM: Set CP0_Status.KX on MIPS64
      MIPS: KVM: Make entry code MIPS64 friendly
      MIPS: KVM: Use kmap instead of CKSEG0ADDR()
      MIPS: KVM: Use virt_to_phys() to get commpage PFN
      MIPS: Fix definition of KSEGX() for 64-bit
      KVM: VMX: Add VMCS to CPU's loaded VMCSs before VMPTRLD
      kvm: x86: nVMX: maintain internal copy of current VMCS
      KVM: PPC: Book3S HV: Save/restore TM state in H_CEDE
      KVM: PPC: Book3S HV: Pull out TM state save/restore into separate procedures
      KVM: arm64: vgic-its: Simplify MAPI error handling
      KVM: arm64: vgic-its: Make vgic_its_cmd_handle_mapi similar to other handlers
      KVM: arm64: vgic-its: Turn device_id validation into generic ID validation
      ...

commit 015cd867e566e3a27b5e8062eb24eeaa4d77297f
Merge: 85802a49a85c 64a40c84001e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 26 12:22:51 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "There are a couple of new things for s390 with this merge request:
    
       - a new scheduling domain "drawer" is added to reflect the unusual
         topology found on z13 machines.  Performance tests showed up to 8
         percent gain with the additional domain.
    
       - the new crc-32 checksum crypto module uses the vector-galois-field
         multiply and sum SIMD instruction to speed up crc-32 and crc-32c.
    
       - proper __ro_after_init support, this requires RO_AFTER_INIT_DATA in
         the generic vmlinux.lds linker script definitions.
    
       - kcov instrumentation support.  A prerequisite for that is the
         inline assembly basic block cleanup, which is the reason for the
         net/iucv/iucv.c change.
    
       - support for 2GB pages is added to the hugetlbfs backend.
    
      Then there are two removals:
    
       - the oprofile hardware sampling support is dead code and is removed.
         The oprofile user space uses the perf interface nowadays.
    
       - the ETR clock synchronization is removed, this has been superseeded
         be the STP clock synchronization.  And it always has been
         "interesting" code..
    
      And the usual bug fixes and cleanups"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (82 commits)
      s390/pci: Delete an unnecessary check before the function call "pci_dev_put"
      s390/smp: clean up a condition
      s390/cio/chp : Remove deprecated create_singlethread_workqueue
      s390/chsc: improve channel path descriptor determination
      s390/chsc: sanitize fmt check for chp_desc determination
      s390/cio: make fmt1 channel path descriptor optional
      s390/chsc: fix ioctl CHSC_INFO_CU command
      s390/cio/device_ops: fix kernel doc
      s390/cio: allow to reset channel measurement block
      s390/console: Make preferred console handling more consistent
      s390/mm: fix gmap tlb flush issues
      s390/mm: add support for 2GB hugepages
      s390: have unique symbol for __switch_to address
      s390/cpuinfo: show maximum thread id
      s390/ptrace: clarify bits in the per_struct
      s390: stack address vs thread_info
      s390: remove pointless load within __switch_to
      s390: enable kcov support
      s390/cpumf: use basic block for ecctr inline assembly
      s390/hypfs: use basic block for diag inline assembly
      ...

commit 6502a34cfd6695929086187f63fe670cc3050e68
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Jun 21 14:19:51 2016 +0200

    KVM: s390: allow user space to handle instr 0x0000
    
    We will use illegal instruction 0x0000 for handling 2 byte sw breakpoints
    from user space. As it can be enabled dynamically via a capability,
    let's move setting of ICTL_OPEREXC to the post creation step, so we avoid
    any races when enabling that capability just while adding new cpus.
    
    Acked-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d42428c11794..63ac7c1641a7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -364,6 +364,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_USER_STSI:
 	case KVM_CAP_S390_SKEYS:
 	case KVM_CAP_S390_IRQ_STATE:
+	case KVM_CAP_S390_USER_INSTR0:
 		r = 1;
 		break;
 	case KVM_CAP_S390_MEM_OP:
@@ -456,6 +457,16 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 	return r;
 }
 
+static void icpt_operexc_on_all_vcpus(struct kvm *kvm)
+{
+	unsigned int i;
+	struct kvm_vcpu *vcpu;
+
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		kvm_s390_sync_request(KVM_REQ_ICPT_OPEREXC, vcpu);
+	}
+}
+
 static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 {
 	int r;
@@ -507,6 +518,12 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		kvm->arch.user_stsi = 1;
 		r = 0;
 		break;
+	case KVM_CAP_S390_USER_INSTR0:
+		VM_EVENT(kvm, 3, "%s", "ENABLE: CAP_S390_USER_INSTR0");
+		kvm->arch.user_instr0 = 1;
+		icpt_operexc_on_all_vcpus(kvm);
+		r = 0;
+		break;
 	default:
 		r = -EINVAL;
 		break;
@@ -1836,6 +1853,8 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 		vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 		sca_add_vcpu(vcpu);
 	}
+	if (test_kvm_facility(vcpu->kvm, 74) || vcpu->kvm->arch.user_instr0)
+		vcpu->arch.sie_block->ictl |= ICTL_OPEREXC;
 	/* make vcpu_load load the right gmap on the first trigger */
 	vcpu->arch.enabled_gmap = vcpu->arch.gmap;
 }
@@ -1923,8 +1942,6 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	}
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
-	if (test_kvm_facility(vcpu->kvm, 74))
-		vcpu->arch.sie_block->ictl |= ICTL_OPEREXC;
 
 	if (vcpu->kvm->arch.use_cmma) {
 		rc = kvm_s390_vcpu_setup_cmma(vcpu);
@@ -2369,6 +2386,11 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 		goto retry;
 	}
 
+	if (kvm_check_request(KVM_REQ_ICPT_OPEREXC, vcpu)) {
+		vcpu->arch.sie_block->ictl |= ICTL_OPEREXC;
+		goto retry;
+	}
+
 	/* nothing to do, just clear the request */
 	clear_bit(KVM_REQ_UNHALT, &vcpu->requests);
 

commit 6edaa5307f3f51e4e56dc4c63f68a69d88c6ddf5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jun 15 15:18:26 2016 +0200

    KVM: remove kvm_guest_enter/exit wrappers
    
    Use the functions from context_tracking.h directly.
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 03eeeb0ded24..d42428c11794 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2623,14 +2623,14 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		 * guest_enter and guest_exit should be no uaccess.
 		 */
 		local_irq_disable();
-		__kvm_guest_enter();
+		guest_enter_irqoff();
 		__disable_cpu_timer_accounting(vcpu);
 		local_irq_enable();
 		exit_reason = sie64a(vcpu->arch.sie_block,
 				     vcpu->run->s.regs.gprs);
 		local_irq_disable();
 		__enable_cpu_timer_accounting(vcpu);
-		__kvm_guest_exit();
+		guest_exit_irqoff();
 		local_irq_enable();
 		vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
 

commit a411edf1320ed8fa3b4560902ac4e033c4a72bcf
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Feb 2 15:41:22 2016 +0100

    KVM: s390: vsie: add module parameter "nested"
    
    Let's be careful first and allow nested virtualization only if enabled
    by the system administrator. In addition, user space still has to
    explicitly enable it via SCLP features for it to work.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 31cf22f7d846..03eeeb0ded24 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -125,6 +125,11 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ NULL }
 };
 
+/* allow nested virtualization in KVM (if enabled by user space) */
+static int nested;
+module_param(nested, int, S_IRUGO);
+MODULE_PARM_DESC(nested, "Nested virtualization support");
+
 /* upper facilities limit for kvm */
 unsigned long kvm_s390_fac_list_mask[16] = {
 	0xffe6000000000000UL,
@@ -264,7 +269,7 @@ static void kvm_s390_cpu_feat_init(void)
 	 * 64bit SCAO (SCA passthrough) and IDTE (for gmap_shadow unshadowing).
 	 */
 	if (!sclp.has_sief2 || !MACHINE_HAS_ESOP || !sclp.has_64bscao ||
-	    !test_facility(3))
+	    !test_facility(3) || !nested)
 		return;
 	allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIEF2);
 	if (sclp.has_64bscao)

commit 5d3876a8bf4607b72cbe754278d19c68990b57a8
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Apr 13 17:06:50 2016 +0200

    KVM: s390: vsie: add indication for future features
    
    We have certain SIE features that we cannot support for now.
    Let's add these features, so user space can directly prepare to enable
    them, so we don't have to update yet another component.
    
    In addition, add a comment block, telling why it is for now not possible to
    forward/enable these features.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6fdf1f7647d7..31cf22f7d846 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -281,6 +281,24 @@ static void kvm_s390_cpu_feat_init(void)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_CEI);
 	if (sclp.has_ibs)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IBS);
+	/*
+	 * KVM_S390_VM_CPU_FEAT_SKEY: Wrong shadow of PTE.I bits will make
+	 * all skey handling functions read/set the skey from the PGSTE
+	 * instead of the real storage key.
+	 *
+	 * KVM_S390_VM_CPU_FEAT_CMMA: Wrong shadow of PTE.I bits will make
+	 * pages being detected as preserved although they are resident.
+	 *
+	 * KVM_S390_VM_CPU_FEAT_PFMFI: Wrong shadow of PTE.I bits will
+	 * have the same effect as for KVM_S390_VM_CPU_FEAT_SKEY.
+	 *
+	 * For KVM_S390_VM_CPU_FEAT_SKEY, KVM_S390_VM_CPU_FEAT_CMMA and
+	 * KVM_S390_VM_CPU_FEAT_PFMFI, all PTE.I and PGSTE bits have to be
+	 * correctly shadowed. We can do that for the PGSTE but not for PTE.I.
+	 *
+	 * KVM_S390_VM_CPU_FEAT_SIGPIF: Wrong SCB addresses in the SCA. We
+	 * cannot easily shadow the SCA because of the ipte lock.
+	 */
 }
 
 int kvm_arch_init(void *opaque)

commit 91473b487dd58af6384c5c3db13de50defa2c106
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Oct 29 10:30:36 2015 +0100

    KVM: s390: vsie: correctly set and handle guest TOD
    
    Guest 2 sets up the epoch of guest 3 from his point of view. Therefore,
    we have to add the guest 2 epoch to the guest 3 epoch. We also have to take
    care of guest 2 epoch changes on STP syncs. This will work just fine by
    also updating the guest 3 epoch when a vsie_block has been set for a VCPU.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8ba7a98a50cf..6fdf1f7647d7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -176,6 +176,8 @@ static int kvm_clock_sync(struct notifier_block *notifier, unsigned long val,
 			vcpu->arch.sie_block->epoch -= *delta;
 			if (vcpu->arch.cputm_enabled)
 				vcpu->arch.cputm_start += *delta;
+			if (vcpu->arch.vsie_block)
+				vcpu->arch.vsie_block->epoch -= *delta;
 		}
 	}
 	return NOTIFY_OK;

commit 7fd7f39daa3da822122124730437c4f37e4d82de
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 16:56:23 2015 +0100

    KVM: s390: vsie: support IBS interpretation
    
    We can easily enable ibs for guest 2, so he can use it for guest 3.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1c1188ba1042..8ba7a98a50cf 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -277,6 +277,8 @@ static void kvm_s390_cpu_feat_init(void)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IB);
 	if (sclp.has_cei)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_CEI);
+	if (sclp.has_ibs)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IBS);
 }
 
 int kvm_arch_init(void *opaque)

commit 13ee3f678b1117d7511a2c5e10549f7c37f4cadf
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 16:54:37 2015 +0100

    KVM: s390: vsie: support conditional-external-interception
    
    We can easily enable cei for guest 2, so he can use it for guest 3.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5ec598ca7660..1c1188ba1042 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -275,6 +275,8 @@ static void kvm_s390_cpu_feat_init(void)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GSLS);
 	if (sclp.has_ib)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IB);
+	if (sclp.has_cei)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_CEI);
 }
 
 int kvm_arch_init(void *opaque)

commit 5630a8e82b1ee4d13daa500c045603c5b4801fd9
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 16:53:51 2015 +0100

    KVM: s390: vsie: support intervention-bypass
    
    We can easily enable intervention bypass for guest 2, so it can use it
    for guest 3.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ce9813afd502..5ec598ca7660 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -273,6 +273,8 @@ static void kvm_s390_cpu_feat_init(void)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GPERE);
 	if (sclp.has_gsls)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GSLS);
+	if (sclp.has_ib)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IB);
 }
 
 int kvm_arch_init(void *opaque)

commit a1b7b9b286c0157748922526ecb353e550209833
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 16:41:33 2015 +0100

    KVM: s390: vsie: support guest-storage-limit-suppression
    
    We can easily forward guest-storage-limit-suppression if available.
    
    One thing to care about is keeping the prefix properly mapped when
    gsls in toggled on/off or the mso changes in between. Therefore we better
    remap the prefix on any mso changes just like we already do with the
    prefix.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 175752877c0d..ce9813afd502 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -271,6 +271,8 @@ static void kvm_s390_cpu_feat_init(void)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIIF);
 	if (sclp.has_gpere)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GPERE);
+	if (sclp.has_gsls)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GSLS);
 }
 
 int kvm_arch_init(void *opaque)

commit 77d18f6d47fbeaaceb15df9ab928757d5bb96ec6
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 16:32:35 2015 +0100

    KVM: s390: vsie: support guest-PER-enhancement
    
    We can easily forward the guest-PER-enhancement facility to guest 2 if
    available.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d735612f9081..175752877c0d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -269,6 +269,8 @@ static void kvm_s390_cpu_feat_init(void)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_64BSCAO);
 	if (sclp.has_siif)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIIF);
+	if (sclp.has_gpere)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GPERE);
 }
 
 int kvm_arch_init(void *opaque)

commit 0615a326e066b580cf26d16a092ea54997dd6cbb
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Nov 25 09:59:49 2015 +0100

    KVM: s390: vsie: support shared IPTE-interlock facility
    
    As we forward the whole SCA provided by guest 2, we can directly forward
    SIIF if available.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e0c5a57bf58b..d735612f9081 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -267,6 +267,8 @@ static void kvm_s390_cpu_feat_init(void)
 	allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIEF2);
 	if (sclp.has_64bscao)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_64BSCAO);
+	if (sclp.has_siif)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIIF);
 }
 
 int kvm_arch_init(void *opaque)

commit 19c439b564b05939b83876a687bd48389d0aebb5
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Nov 25 11:02:26 2015 +0100

    KVM: s390: vsie: support 64-bit-SCAO
    
    Let's provide the 64-bit-SCAO facility to guest 2, so he can set up a SCA
    for guest 3 that has a 64 bit address. Please note that we already require
    the 64 bit SCAO for our vsie implementation, in order to forward the SCA
    directly (by pinning the page).
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3fb124226e97..e0c5a57bf58b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -265,6 +265,8 @@ static void kvm_s390_cpu_feat_init(void)
 	    !test_facility(3))
 		return;
 	allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIEF2);
+	if (sclp.has_64bscao)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_64BSCAO);
 }
 
 int kvm_arch_init(void *opaque)

commit a3508fbe9dc6dd3bece0c7bf889cc085a011738c
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Jul 8 13:19:48 2015 +0200

    KVM: s390: vsie: initial support for nested virtualization
    
    This patch adds basic support for nested virtualization on s390x, called
    VSIE (virtual SIE) and allows it to be used by the guest if the necessary
    facilities are supported by the hardware and enabled for the guest.
    
    In order to make this work, we have to shadow the sie control block
    provided by guest 2. In order to gain some performance, we have to
    reuse the same shadow blocks as good as possible. For now, we allow
    as many shadow blocks as we have VCPUs (that way, every VCPU can run the
    VSIE concurrently).
    
    We have to watch out for the prefix getting unmapped out of our shadow
    gmap and properly get the VCPU out of VSIE in that case, to fault the
    prefix pages back in. We use the PROG_REQUEST bit for that purpose.
    
    This patch is based on an initial prototype by Tobias Elpelt.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a890f7d20711..3fb124226e97 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -99,6 +99,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
 	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
 	{ "instruction_sthyi", VCPU_STAT(instruction_sthyi) },
+	{ "instruction_sie", VCPU_STAT(instruction_sie) },
 	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
 	{ "instruction_sigp_sense_running", VCPU_STAT(instruction_sigp_sense_running) },
 	{ "instruction_sigp_external_call", VCPU_STAT(instruction_sigp_external_call) },
@@ -142,6 +143,7 @@ static DECLARE_BITMAP(kvm_s390_available_cpu_feat, KVM_S390_VM_CPU_FEAT_NR_BITS)
 static struct kvm_s390_vm_cpu_subfunc kvm_s390_available_subfunc;
 
 static struct gmap_notifier gmap_notifier;
+static struct gmap_notifier vsie_gmap_notifier;
 debug_info_t *kvm_s390_dbf;
 
 /* Section: not file related */
@@ -187,6 +189,8 @@ int kvm_arch_hardware_setup(void)
 {
 	gmap_notifier.notifier_call = kvm_gmap_notifier;
 	gmap_register_pte_notifier(&gmap_notifier);
+	vsie_gmap_notifier.notifier_call = kvm_s390_vsie_gmap_notifier;
+	gmap_register_pte_notifier(&vsie_gmap_notifier);
 	atomic_notifier_chain_register(&s390_epoch_delta_notifier,
 				       &kvm_clock_notifier);
 	return 0;
@@ -195,6 +199,7 @@ int kvm_arch_hardware_setup(void)
 void kvm_arch_hardware_unsetup(void)
 {
 	gmap_unregister_pte_notifier(&gmap_notifier);
+	gmap_unregister_pte_notifier(&vsie_gmap_notifier);
 	atomic_notifier_chain_unregister(&s390_epoch_delta_notifier,
 					 &kvm_clock_notifier);
 }
@@ -252,6 +257,14 @@ static void kvm_s390_cpu_feat_init(void)
 
 	if (MACHINE_HAS_ESOP)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
+	/*
+	 * We need SIE support, ESOP (PROT_READ protection for gmap_shadow),
+	 * 64bit SCAO (SCA passthrough) and IDTE (for gmap_shadow unshadowing).
+	 */
+	if (!sclp.has_sief2 || !MACHINE_HAS_ESOP || !sclp.has_64bscao ||
+	    !test_facility(3))
+		return;
+	allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIEF2);
 }
 
 int kvm_arch_init(void *opaque)
@@ -1406,6 +1419,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
+	kvm_s390_vsie_init(kvm);
 	KVM_EVENT(3, "vm 0x%pK created by pid %u", kvm, current->pid);
 
 	return 0;
@@ -1463,6 +1477,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 		gmap_remove(kvm->arch.gmap);
 	kvm_s390_destroy_adapters(kvm);
 	kvm_s390_clear_float_irqs(kvm);
+	kvm_s390_vsie_destroy(kvm);
 	KVM_EVENT(3, "vm 0x%pK destroyed", kvm);
 }
 

commit 37d9df98b71afdf3baf41ee5451b6206c13328c6
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Mar 11 16:47:33 2015 +0100

    KVM: s390: backup the currently enabled gmap when scheduled out
    
    Nested virtualization will have to enable own gmaps. Current code
    would enable the wrong gmap whenever scheduled out and back in,
    therefore resulting in the wrong gmap being enabled.
    
    This patch reenables the last enabled gmap, therefore avoiding having to
    touch vcpu->arch.gmap when enabling a different gmap.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 45a8316ba1eb..a890f7d20711 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1719,7 +1719,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 
 	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);
-	gmap_enable(vcpu->arch.gmap);
+	gmap_enable(vcpu->arch.enabled_gmap);
 	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	if (vcpu->arch.cputm_enabled && !is_vcpu_idle(vcpu))
 		__start_cpu_timer_accounting(vcpu);
@@ -1732,7 +1732,8 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.cputm_enabled && !is_vcpu_idle(vcpu))
 		__stop_cpu_timer_accounting(vcpu);
 	atomic_andnot(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
-	gmap_disable(vcpu->arch.gmap);
+	vcpu->arch.enabled_gmap = gmap_get_enabled();
+	gmap_disable(vcpu->arch.enabled_gmap);
 
 	/* Save guest register state */
 	save_fpu_regs();
@@ -1781,7 +1782,8 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 		vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 		sca_add_vcpu(vcpu);
 	}
-
+	/* make vcpu_load load the right gmap on the first trigger */
+	vcpu->arch.enabled_gmap = vcpu->arch.gmap;
 }
 
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)

commit 65d0b0d4bcc67b596d8e7286c3bebf24c59ade6a
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 27 16:29:34 2015 +0200

    KVM: s390: fast path for shadow gmaps in gmap notifier
    
    The default kvm gmap notifier doesn't have to handle shadow gmaps.
    So let's just directly exit in case we get notified about one.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9dd52980605c..45a8316ba1eb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1986,6 +1986,8 @@ static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
 	unsigned long prefix;
 	int i;
 
+	if (gmap_is_shadow(gmap))
+		return;
 	if (start >= 1UL << 31)
 		/* We are only interested in prefix pages */
 		return;

commit 6ea427bbbd4078297bb1dbd6c5cb83f3f48aac46
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:55:04 2016 +0100

    s390/mm: add reference counter to gmap structure
    
    Let's use a reference counter mechanism to control the lifetime of
    gmap structures. This will be needed for further changes related to
    gmap shadows.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b6e7f66f0f01..9dd52980605c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -532,20 +532,20 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		if (!new_limit)
 			return -EINVAL;
 
-		/* gmap_alloc takes last usable address */
+		/* gmap_create takes last usable address */
 		if (new_limit != KVM_S390_NO_MEM_LIMIT)
 			new_limit -= 1;
 
 		ret = -EBUSY;
 		mutex_lock(&kvm->lock);
 		if (!kvm->created_vcpus) {
-			/* gmap_alloc will round the limit up */
-			struct gmap *new = gmap_alloc(current->mm, new_limit);
+			/* gmap_create will round the limit up */
+			struct gmap *new = gmap_create(current->mm, new_limit);
 
 			if (!new) {
 				ret = -ENOMEM;
 			} else {
-				gmap_free(kvm->arch.gmap);
+				gmap_remove(kvm->arch.gmap);
 				new->private = kvm;
 				kvm->arch.gmap = new;
 				ret = 0;
@@ -1394,7 +1394,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		else
 			kvm->arch.mem_limit = min_t(unsigned long, TASK_MAX_SIZE,
 						    sclp.hamax + 1);
-		kvm->arch.gmap = gmap_alloc(current->mm, kvm->arch.mem_limit - 1);
+		kvm->arch.gmap = gmap_create(current->mm, kvm->arch.mem_limit - 1);
 		if (!kvm->arch.gmap)
 			goto out_err;
 		kvm->arch.gmap->private = kvm;
@@ -1427,7 +1427,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		sca_del_vcpu(vcpu);
 
 	if (kvm_is_ucontrol(vcpu->kvm))
-		gmap_free(vcpu->arch.gmap);
+		gmap_remove(vcpu->arch.gmap);
 
 	if (vcpu->kvm->arch.use_cmma)
 		kvm_s390_vcpu_unsetup_cmma(vcpu);
@@ -1460,7 +1460,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	debug_unregister(kvm->arch.dbf);
 	free_page((unsigned long)kvm->arch.sie_page2);
 	if (!kvm_is_ucontrol(kvm))
-		gmap_free(kvm->arch.gmap);
+		gmap_remove(kvm->arch.gmap);
 	kvm_s390_destroy_adapters(kvm);
 	kvm_s390_clear_float_irqs(kvm);
 	KVM_EVENT(3, "vm 0x%pK destroyed", kvm);
@@ -1469,7 +1469,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 /* Section: vcpu related */
 static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
 {
-	vcpu->arch.gmap = gmap_alloc(current->mm, -1UL);
+	vcpu->arch.gmap = gmap_create(current->mm, -1UL);
 	if (!vcpu->arch.gmap)
 		return -ENOMEM;
 	vcpu->arch.gmap->private = vcpu->kvm;

commit b2d73b2a0ad1c758cb0c1acb01a911744b845942
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:54:42 2016 +0100

    s390/mm: extended gmap pte notifier
    
    The current gmap pte notifier forces a pte into to a read-write state.
    If the pte is invalidated the gmap notifier is called to inform KVM
    that the mapping will go away.
    
    Extend this approach to allow read-write, read-only and no-access
    as possible target states and call the pte notifier for any change
    to the pte.
    
    This mechanism is used to temporarily set specific access rights for
    a pte without doing the heavy work of a true mprotect call.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 67f1b6b4c060..b6e7f66f0f01 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -21,6 +21,7 @@
 #include <linux/init.h>
 #include <linux/kvm.h>
 #include <linux/kvm_host.h>
+#include <linux/mman.h>
 #include <linux/module.h>
 #include <linux/random.h>
 #include <linux/slab.h>
@@ -185,7 +186,7 @@ static struct notifier_block kvm_clock_notifier = {
 int kvm_arch_hardware_setup(void)
 {
 	gmap_notifier.notifier_call = kvm_gmap_notifier;
-	gmap_register_ipte_notifier(&gmap_notifier);
+	gmap_register_pte_notifier(&gmap_notifier);
 	atomic_notifier_chain_register(&s390_epoch_delta_notifier,
 				       &kvm_clock_notifier);
 	return 0;
@@ -193,7 +194,7 @@ int kvm_arch_hardware_setup(void)
 
 void kvm_arch_hardware_unsetup(void)
 {
-	gmap_unregister_ipte_notifier(&gmap_notifier);
+	gmap_unregister_pte_notifier(&gmap_notifier);
 	atomic_notifier_chain_unregister(&s390_epoch_delta_notifier,
 					 &kvm_clock_notifier);
 }
@@ -2272,16 +2273,16 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 		return 0;
 	/*
 	 * We use MMU_RELOAD just to re-arm the ipte notifier for the
-	 * guest prefix page. gmap_ipte_notify will wait on the ptl lock.
+	 * guest prefix page. gmap_mprotect_notify will wait on the ptl lock.
 	 * This ensures that the ipte instruction for this request has
 	 * already finished. We might race against a second unmapper that
 	 * wants to set the blocking bit. Lets just retry the request loop.
 	 */
 	if (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu)) {
 		int rc;
-		rc = gmap_ipte_notify(vcpu->arch.gmap,
-				      kvm_s390_get_prefix(vcpu),
-				      PAGE_SIZE * 2);
+		rc = gmap_mprotect_notify(vcpu->arch.gmap,
+					  kvm_s390_get_prefix(vcpu),
+					  PAGE_SIZE * 2, PROT_WRITE);
 		if (rc)
 			return rc;
 		goto retry;

commit 414d3b07496604a4372466a6b474ca24291a143c
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:52:54 2016 +0100

    s390/kvm: page table invalidation notifier
    
    Pass an address range to the page table invalidation notifier
    for KVM. This allows to notify changes that affect a larger
    virtual memory area, e.g. for 1MB pages.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0dcf9b8fc12c..67f1b6b4c060 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -150,7 +150,8 @@ int kvm_arch_hardware_enable(void)
 	return 0;
 }
 
-static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address);
+static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
+			      unsigned long end);
 
 /*
  * This callback is executed during stop_machine(). All CPUs are therefore
@@ -1976,16 +1977,23 @@ void kvm_s390_sync_request(int req, struct kvm_vcpu *vcpu)
 	kvm_s390_vcpu_request(vcpu);
 }
 
-static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address)
+static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
+			      unsigned long end)
 {
-	int i;
 	struct kvm *kvm = gmap->private;
 	struct kvm_vcpu *vcpu;
+	unsigned long prefix;
+	int i;
 
+	if (start >= 1UL << 31)
+		/* We are only interested in prefix pages */
+		return;
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		/* match against both prefix pages */
-		if (kvm_s390_get_prefix(vcpu) == (address & ~0x1000UL)) {
-			VCPU_EVENT(vcpu, 2, "gmap notifier for %lx", address);
+		prefix = kvm_s390_get_prefix(vcpu);
+		if (prefix <= end && start <= prefix + 2*PAGE_SIZE - 1) {
+			VCPU_EVENT(vcpu, 2, "gmap notifier for %lx-%lx",
+				   start, end);
 			kvm_s390_sync_request(KVM_REQ_MMU_RELOAD, vcpu);
 		}
 	}

commit a03825bbd0c39feeba605912cdbc28e79e4e01e1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jun 13 14:50:04 2016 +0200

    KVM: s390: use kvm->created_vcpus
    
    The new created_vcpus field avoids possible races between enabling
    capabilities and creating VCPUs.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 49c60393a15c..0dcf9b8fc12c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -422,7 +422,7 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		break;
 	case KVM_CAP_S390_VECTOR_REGISTERS:
 		mutex_lock(&kvm->lock);
-		if (atomic_read(&kvm->online_vcpus)) {
+		if (kvm->created_vcpus) {
 			r = -EBUSY;
 		} else if (MACHINE_HAS_VX) {
 			set_kvm_facility(kvm->arch.model.fac_mask, 129);
@@ -437,7 +437,7 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 	case KVM_CAP_S390_RI:
 		r = -EINVAL;
 		mutex_lock(&kvm->lock);
-		if (atomic_read(&kvm->online_vcpus)) {
+		if (kvm->created_vcpus) {
 			r = -EBUSY;
 		} else if (test_facility(64)) {
 			set_kvm_facility(kvm->arch.model.fac_mask, 64);
@@ -492,7 +492,7 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		ret = -EBUSY;
 		VM_EVENT(kvm, 3, "%s", "ENABLE: CMMA support");
 		mutex_lock(&kvm->lock);
-		if (atomic_read(&kvm->online_vcpus) == 0) {
+		if (!kvm->created_vcpus) {
 			kvm->arch.use_cmma = 1;
 			ret = 0;
 		}
@@ -536,7 +536,7 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 
 		ret = -EBUSY;
 		mutex_lock(&kvm->lock);
-		if (atomic_read(&kvm->online_vcpus) == 0) {
+		if (!kvm->created_vcpus) {
 			/* gmap_alloc will round the limit up */
 			struct gmap *new = gmap_alloc(current->mm, new_limit);
 
@@ -713,7 +713,7 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 	int ret = 0;
 
 	mutex_lock(&kvm->lock);
-	if (atomic_read(&kvm->online_vcpus)) {
+	if (kvm->created_vcpus) {
 		ret = -EBUSY;
 		goto out;
 	}

commit fd5ada04030cb584251c381cb70daa41e984ae82
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue May 31 15:06:51 2016 +0200

    s390/time: remove ETR support
    
    The External-Time-Reference (ETR) clock synchronization interface has
    been superseded by Server-Time-Protocol (STP). Remove the outdated
    ETR interface.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6d8ec3ac9dd8..08c39e769747 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -28,7 +28,7 @@
 #include <linux/vmalloc.h>
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
-#include <asm/etr.h>
+#include <asm/stp.h>
 #include <asm/pgtable.h>
 #include <asm/gmap.h>
 #include <asm/nmi.h>

commit fe69eabf8deb85ae8b2958830ea3b2911e332820
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 9 13:08:07 2016 +0200

    KVM: s390: storage keys fit into a char
    
    No need to convert the storage key into an unsigned long, the target
    function expects a char as argument.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ad166c6698e0..49c60393a15c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1120,8 +1120,7 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 			break;
 		}
 
-		r = set_guest_storage_key(current->mm, hva,
-					  (unsigned long)keys[i], 0);
+		r = set_guest_storage_key(current->mm, hva, keys[i], 0);
 		if (r)
 			break;
 	}

commit 154c8c19c35b6da94a623cb793458e203572083d
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 9 11:22:34 2016 +0200

    s390/mm: return key via pointer in get_guest_storage_key
    
    Let's just split returning the key and reporting errors. This makes calling
    code easier and avoids bugs as happened already.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d0156d7969e0..ad166c6698e0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1029,7 +1029,6 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 {
 	uint8_t *keys;
 	uint64_t hva;
-	unsigned long curkey;
 	int i, r = 0;
 
 	if (args->flags != 0)
@@ -1058,12 +1057,9 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 			break;
 		}
 
-		curkey = get_guest_storage_key(current->mm, hva);
-		if (IS_ERR_VALUE(curkey)) {
-			r = curkey;
+		r = get_guest_storage_key(current->mm, hva, &keys[i]);
+		if (r)
 			break;
-		}
-		keys[i] = curkey;
 	}
 	up_read(&current->mm->mmap_sem);
 

commit d3ed1ceeace311af9973d17a07a114bfaf0ca1b1
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:53:35 2016 +0100

    s390/mm: set and get guest storage key mmap locking
    
    Move the mmap semaphore locking out of set_guest_storage_key
    and get_guest_storage_key. This makes the two functions more
    like the other ptep_xxx operations and allows to avoid repeated
    semaphore operations if multiple keys are read or written.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9d0e4d0487f4..d0156d7969e0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1050,26 +1050,30 @@ static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	if (!keys)
 		return -ENOMEM;
 
+	down_read(&current->mm->mmap_sem);
 	for (i = 0; i < args->count; i++) {
 		hva = gfn_to_hva(kvm, args->start_gfn + i);
 		if (kvm_is_error_hva(hva)) {
 			r = -EFAULT;
-			goto out;
+			break;
 		}
 
 		curkey = get_guest_storage_key(current->mm, hva);
 		if (IS_ERR_VALUE(curkey)) {
 			r = curkey;
-			goto out;
+			break;
 		}
 		keys[i] = curkey;
 	}
+	up_read(&current->mm->mmap_sem);
+
+	if (!r) {
+		r = copy_to_user((uint8_t __user *)args->skeydata_addr, keys,
+				 sizeof(uint8_t) * args->count);
+		if (r)
+			r = -EFAULT;
+	}
 
-	r = copy_to_user((uint8_t __user *)args->skeydata_addr, keys,
-			 sizeof(uint8_t) * args->count);
-	if (r)
-		r = -EFAULT;
-out:
 	kvfree(keys);
 	return r;
 }
@@ -1106,24 +1110,26 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	if (r)
 		goto out;
 
+	down_read(&current->mm->mmap_sem);
 	for (i = 0; i < args->count; i++) {
 		hva = gfn_to_hva(kvm, args->start_gfn + i);
 		if (kvm_is_error_hva(hva)) {
 			r = -EFAULT;
-			goto out;
+			break;
 		}
 
 		/* Lowest order bit is reserved */
 		if (keys[i] & 0x01) {
 			r = -EINVAL;
-			goto out;
+			break;
 		}
 
 		r = set_guest_storage_key(current->mm, hva,
 					  (unsigned long)keys[i], 0);
 		if (r)
-			goto out;
+			break;
 	}
+	up_read(&current->mm->mmap_sem);
 out:
 	kvfree(keys);
 	return r;

commit f597d24eee2dd9486edaac7a1821f35bc4d349c2
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Apr 22 16:26:49 2016 +0200

    KVM: s390: turn on tx even without ctx
    
    Constrained transactional execution is an addon of transactional execution.
    
    Let's enable the assist also if only TX is enabled for the guest.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4e764faed524..9d0e4d0487f4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1842,7 +1842,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->ecb |= 0x02;
 	if (test_kvm_facility(vcpu->kvm, 9))
 		vcpu->arch.sie_block->ecb |= 0x04;
-	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))
+	if (test_kvm_facility(vcpu->kvm, 73))
 		vcpu->arch.sie_block->ecb |= 0x10;
 
 	if (test_kvm_facility(vcpu->kvm, 8) && sclp.has_pfmfi)

commit bdab09f3d81c3fac6314012ca0eff1206ea067ab
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Apr 12 11:07:49 2016 +0200

    KVM: s390: enable host-protection-interruption only with ESOP
    
    host-protection-interruption control was introduced with ESOP. So let's
    enable it only if we have ESOP and add an explanatory comment why
    we can live without it.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ad93b40bfdc0..4e764faed524 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1837,7 +1837,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	kvm_s390_vcpu_setup_model(vcpu);
 
-	vcpu->arch.sie_block->ecb = 0x02;
+	/* pgste_set_pte has special handling for !MACHINE_HAS_ESOP */
+	if (MACHINE_HAS_ESOP)
+		vcpu->arch.sie_block->ecb |= 0x02;
 	if (test_kvm_facility(vcpu->kvm, 9))
 		vcpu->arch.sie_block->ecb |= 0x04;
 	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))

commit 09a400e78eaf02d8ab8e836edf864e1025c8e2d7
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 4 15:57:08 2016 +0200

    KVM: s390: enable ibs only if available
    
    Let's enable interlock-and-broadcast suppression only if the facility is
    actually available.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d987eb8af059..ad93b40bfdc0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2789,6 +2789,8 @@ static void __disable_ibs_on_all_vcpus(struct kvm *kvm)
 
 static void __enable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
 {
+	if (!sclp.has_ibs)
+		return;
 	kvm_check_request(KVM_REQ_DISABLE_IBS, vcpu);
 	kvm_s390_sync_request(KVM_REQ_ENABLE_IBS, vcpu);
 }

commit 873b425e4c2fd0ba6617d67a45fbf119b65575b4
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 4 15:53:47 2016 +0200

    KVM: s390: enable PFMFI only if available
    
    Let's enable interpretation of PFMFI only if the facility is
    actually available. Emulation code still works in case the guest is
    offered EDAT-1.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1a239a6748fe..d987eb8af059 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1843,7 +1843,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))
 		vcpu->arch.sie_block->ecb |= 0x10;
 
-	if (test_kvm_facility(vcpu->kvm, 8))
+	if (test_kvm_facility(vcpu->kvm, 8) && sclp.has_pfmfi)
 		vcpu->arch.sie_block->ecb2 |= 0x08;
 	vcpu->arch.sie_block->eca = 0x1002000U;
 	if (sclp.has_cei)

commit 48ee7d3a7f8f3ca90dfc5e1103e68c0044051acc
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 4 15:49:34 2016 +0200

    KVM: s390: enable cei only if available
    
    Let's only enable conditional-external-interruption if the facility is
    actually available.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 340fb405bc23..1a239a6748fe 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1845,7 +1845,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	if (test_kvm_facility(vcpu->kvm, 8))
 		vcpu->arch.sie_block->ecb2 |= 0x08;
-	vcpu->arch.sie_block->eca = 0x81002000U;
+	vcpu->arch.sie_block->eca = 0x1002000U;
+	if (sclp.has_cei)
+		vcpu->arch.sie_block->eca |= 0x80000000U;
 	if (sclp.has_ib)
 		vcpu->arch.sie_block->eca |= 0x40000000U;
 	if (sclp.has_siif)

commit 11ad65b79e8c27cdafe404e33938da270a55858a
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 4 15:46:26 2016 +0200

    KVM: s390: enable ib only if available
    
    Let's enable intervention bypass only if the facility is acutally
    available.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2a239554eb89..340fb405bc23 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1845,7 +1845,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	if (test_kvm_facility(vcpu->kvm, 8))
 		vcpu->arch.sie_block->ecb2 |= 0x08;
-	vcpu->arch.sie_block->eca   = 0xC1002000U;
+	vcpu->arch.sie_block->eca = 0x81002000U;
+	if (sclp.has_ib)
+		vcpu->arch.sie_block->eca |= 0x40000000U;
 	if (sclp.has_siif)
 		vcpu->arch.sie_block->eca |= 1;
 	if (sclp.has_sigpif)

commit efed110446226c725268a1f980806d799990a979
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Apr 16 12:32:41 2015 +0200

    KVM: s390: handle missing guest-storage-limit-suppression
    
    If guest-storage-limit-suppression is not available, we would for now
    have a valid guest address space with size 0. So let's simply set the
    origin to 0 and the limit to hamax.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f695c6e08337..2a239554eb89 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1897,6 +1897,10 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.sie_block = &sie_page->sie_block;
 	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
 
+	/* the real guest size will always be smaller than msl */
+	vcpu->arch.sie_block->mso = 0;
+	vcpu->arch.sie_block->msl = sclp.hamax;
+
 	vcpu->arch.sie_block->icpua = id;
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;

commit f9cbd9b02539330ddd349df583fcfc2db8a23b90
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Mar 3 09:48:47 2016 +0100

    KVM: s390: provide CMMA attributes only if available
    
    Let's not provide the device attribute for cmma enabling and clearing
    if the hardware doesn't support it.
    
    This also helps getting rid of the undocumented return value "-EINVAL"
    in case CMMA is not available when trying to enable it.
    
    Also properly document the meaning of -EINVAL for CMMA clearing.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 005e664f6360..f695c6e08337 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -485,7 +485,7 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 	unsigned int idx;
 	switch (attr->attr) {
 	case KVM_S390_VM_MEM_ENABLE_CMMA:
-		ret = -EINVAL;
+		ret = -ENXIO;
 		if (!sclp.has_cmma)
 			break;
 
@@ -499,6 +499,9 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		mutex_unlock(&kvm->lock);
 		break;
 	case KVM_S390_VM_MEM_CLR_CMMA:
+		ret = -ENXIO;
+		if (!sclp.has_cmma)
+			break;
 		ret = -EINVAL;
 		if (!kvm->arch.use_cmma)
 			break;
@@ -964,6 +967,8 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 		switch (attr->attr) {
 		case KVM_S390_VM_MEM_ENABLE_CMMA:
 		case KVM_S390_VM_MEM_CLR_CMMA:
+			ret = sclp.has_cmma ? 0 : -ENXIO;
+			break;
 		case KVM_S390_VM_MEM_LIMIT_SIZE:
 			ret = 0;
 			break;

commit c24cc9c8a6ca798427d3ff46b55df8403361df3e
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 13:53:04 2015 +0100

    KVM: s390: enable CMMA if the interpration is available
    
    Now that we can detect if collaborative-memory-management interpretation
    is available, replace the heuristic by a real hardware detection.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e477c8e5b5c1..005e664f6360 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -485,9 +485,8 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 	unsigned int idx;
 	switch (attr->attr) {
 	case KVM_S390_VM_MEM_ENABLE_CMMA:
-		/* enable CMMA only for z10 and later (EDAT_1) */
 		ret = -EINVAL;
-		if (!MACHINE_IS_LPAR || !MACHINE_HAS_EDAT1)
+		if (!sclp.has_cmma)
 			break;
 
 		ret = -EBUSY;

commit 89b5b4de33902a57cb9c8f2d06de4ffbc921de15
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 13:47:13 2015 +0100

    KVM: s390: guestdbg: signal missing hardware support
    
    Without guest-PER enhancement, we can't provide any debugging support.
    Therefore act like kernel support is missing.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index efb902cdd1d2..e477c8e5b5c1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2179,6 +2179,8 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 
 	if (dbg->control & ~VALID_GUESTDBG_FLAGS)
 		return -EINVAL;
+	if (!sclp.has_gpere)
+		return -EINVAL;
 
 	if (dbg->control & KVM_GUESTDBG_ENABLE) {
 		vcpu->guest_debug = dbg->control;

commit 76a6dd7241ae03c47f44a9605dcd525f31b2124a
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Nov 24 13:33:49 2015 +0100

    KVM: s390: handle missing 64-bit-SCAO facility
    
    Without that facility, we may only use scaol. So fallback
    to DMA allocation in that case, so we won't overwrite random memory
    via the SIE.
    
    Also disallow ESCA, so we don't have to handle that allocation case.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f746a35e3950..efb902cdd1d2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -317,8 +317,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	case KVM_CAP_NR_VCPUS:
 	case KVM_CAP_MAX_VCPUS:
-		r = sclp.has_esca ? KVM_S390_ESCA_CPU_SLOTS
-				  : KVM_S390_BSCA_CPU_SLOTS;
+		r = KVM_S390_BSCA_CPU_SLOTS;
+		if (sclp.has_esca && sclp.has_64bscao)
+			r = KVM_S390_ESCA_CPU_SLOTS;
 		break;
 	case KVM_CAP_NR_MEMSLOTS:
 		r = KVM_USER_MEM_SLOTS;
@@ -1295,6 +1296,7 @@ static void sca_dispose(struct kvm *kvm)
 
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
+	gfp_t alloc_flags = GFP_KERNEL;
 	int i, rc;
 	char debug_name[16];
 	static unsigned long sca_offset;
@@ -1319,8 +1321,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	ratelimit_state_init(&kvm->arch.sthyi_limit, 5 * HZ, 500);
 
 	kvm->arch.use_esca = 0; /* start with basic SCA */
+	if (!sclp.has_64bscao)
+		alloc_flags |= GFP_DMA;
 	rwlock_init(&kvm->arch.sca_lock);
-	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(GFP_KERNEL);
+	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(alloc_flags);
 	if (!kvm->arch.sca)
 		goto out_err;
 	spin_lock(&kvm_lock);
@@ -1567,7 +1571,7 @@ static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
 
 	if (id < KVM_S390_BSCA_CPU_SLOTS)
 		return true;
-	if (!sclp.has_esca)
+	if (!sclp.has_esca || !sclp.has_64bscao)
 		return false;
 
 	mutex_lock(&kvm->lock);

commit 0a763c780b7cb830c250d00ead975778ab948f40
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed May 18 16:03:47 2016 +0200

    KVM: s390: interface to query and configure cpu subfunctions
    
    We have certain instructions that indicate available subfunctions via
    a query subfunction (crypto functions and ptff), or via a test bit
    function (plo).
    
    By exposing these "subfunction blocks" to user space, we allow user space
    to
    1) query available subfunctions and make sure subfunctions won't get lost
       during migration - e.g. properly indicate them via a CPU model
    2) change the subfunctions to be reported to the guest (even adding
       unavailable ones)
    
    This mechanism works just like the way we indicate the stfl(e) list to
    user space.
    
    This way, user space could even emulate some subfunctions in QEMU in the
    future. If this is ever applicable, we have to make sure later on, that
    unsupported subfunctions result in an intercept to QEMU.
    
    Please note that support to indicate them to the guest is still missing
    and requires hardware support. Usually, the IBC takes already care of these
    subfunctions for migration safety. QEMU should make sure to always set
    these bits properly according to the machine generation to be emulated.
    
    Available subfunctions are only valid in combination with STFLE bits
    retrieved via KVM_S390_VM_CPU_MACHINE and enabled via
    KVM_S390_VM_CPU_PROCESSOR. If the applicable bits are available, the
    indicated subfunctions are guaranteed to be correct.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2b5c14da3227..f746a35e3950 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -36,6 +36,8 @@
 #include <asm/switch_to.h>
 #include <asm/isc.h>
 #include <asm/sclp.h>
+#include <asm/cpacf.h>
+#include <asm/etr.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
 
@@ -135,6 +137,8 @@ unsigned long kvm_s390_fac_list_mask_size(void)
 
 /* available cpu features supported by kvm */
 static DECLARE_BITMAP(kvm_s390_available_cpu_feat, KVM_S390_VM_CPU_FEAT_NR_BITS);
+/* available subfunctions indicated via query / "test bit" */
+static struct kvm_s390_vm_cpu_subfunc kvm_s390_available_subfunc;
 
 static struct gmap_notifier gmap_notifier;
 debug_info_t *kvm_s390_dbf;
@@ -198,8 +202,52 @@ static void allow_cpu_feat(unsigned long nr)
 	set_bit_inv(nr, kvm_s390_available_cpu_feat);
 }
 
+static inline int plo_test_bit(unsigned char nr)
+{
+	register unsigned long r0 asm("0") = (unsigned long) nr | 0x100;
+	int cc = 3; /* subfunction not available */
+
+	asm volatile(
+		/* Parameter registers are ignored for "test bit" */
+		"	plo	0,0,0,0(0)\n"
+		"	ipm	%0\n"
+		"	srl	%0,28\n"
+		: "=d" (cc)
+		: "d" (r0)
+		: "cc");
+	return cc == 0;
+}
+
 static void kvm_s390_cpu_feat_init(void)
 {
+	int i;
+
+	for (i = 0; i < 256; ++i) {
+		if (plo_test_bit(i))
+			kvm_s390_available_subfunc.plo[i >> 3] |= 0x80 >> (i & 7);
+	}
+
+	if (test_facility(28)) /* TOD-clock steering */
+		etr_ptff(kvm_s390_available_subfunc.ptff, ETR_PTFF_QAF);
+
+	if (test_facility(17)) { /* MSA */
+		__cpacf_query(CPACF_KMAC, kvm_s390_available_subfunc.kmac);
+		__cpacf_query(CPACF_KMC, kvm_s390_available_subfunc.kmc);
+		__cpacf_query(CPACF_KM, kvm_s390_available_subfunc.km);
+		__cpacf_query(CPACF_KIMD, kvm_s390_available_subfunc.kimd);
+		__cpacf_query(CPACF_KLMD, kvm_s390_available_subfunc.klmd);
+	}
+	if (test_facility(76)) /* MSA3 */
+		__cpacf_query(CPACF_PCKMO, kvm_s390_available_subfunc.pckmo);
+	if (test_facility(77)) { /* MSA4 */
+		__cpacf_query(CPACF_KMCTR, kvm_s390_available_subfunc.kmctr);
+		__cpacf_query(CPACF_KMF, kvm_s390_available_subfunc.kmf);
+		__cpacf_query(CPACF_KMO, kvm_s390_available_subfunc.kmo);
+		__cpacf_query(CPACF_PCC, kvm_s390_available_subfunc.pcc);
+	}
+	if (test_facility(57)) /* MSA5 */
+		__cpacf_query(CPACF_PPNO, kvm_s390_available_subfunc.ppno);
+
 	if (MACHINE_HAS_ESOP)
 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
 }
@@ -717,6 +765,16 @@ static int kvm_s390_set_processor_feat(struct kvm *kvm,
 	return ret;
 }
 
+static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
+					  struct kvm_device_attr *attr)
+{
+	/*
+	 * Once supported by kernel + hw, we have to store the subfunctions
+	 * in kvm->arch and remember that user space configured them.
+	 */
+	return -ENXIO;
+}
+
 static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret = -ENXIO;
@@ -728,6 +786,9 @@ static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_CPU_PROCESSOR_FEAT:
 		ret = kvm_s390_set_processor_feat(kvm, attr);
 		break;
+	case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
+		ret = kvm_s390_set_processor_subfunc(kvm, attr);
+		break;
 	}
 	return ret;
 }
@@ -801,6 +862,25 @@ static int kvm_s390_get_machine_feat(struct kvm *kvm,
 	return 0;
 }
 
+static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
+					  struct kvm_device_attr *attr)
+{
+	/*
+	 * Once we can actually configure subfunctions (kernel + hw support),
+	 * we have to check if they were already set by user space, if so copy
+	 * them from kvm->arch.
+	 */
+	return -ENXIO;
+}
+
+static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
+					struct kvm_device_attr *attr)
+{
+	if (copy_to_user((void __user *)attr->addr, &kvm_s390_available_subfunc,
+	    sizeof(struct kvm_s390_vm_cpu_subfunc)))
+		return -EFAULT;
+	return 0;
+}
 static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret = -ENXIO;
@@ -818,6 +898,12 @@ static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_CPU_MACHINE_FEAT:
 		ret = kvm_s390_get_machine_feat(kvm, attr);
 		break;
+	case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
+		ret = kvm_s390_get_processor_subfunc(kvm, attr);
+		break;
+	case KVM_S390_VM_CPU_MACHINE_SUBFUNC:
+		ret = kvm_s390_get_machine_subfunc(kvm, attr);
+		break;
 	}
 	return ret;
 }
@@ -903,8 +989,11 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 		case KVM_S390_VM_CPU_MACHINE:
 		case KVM_S390_VM_CPU_PROCESSOR_FEAT:
 		case KVM_S390_VM_CPU_MACHINE_FEAT:
+		case KVM_S390_VM_CPU_MACHINE_SUBFUNC:
 			ret = 0;
 			break;
+		/* configuring subfunctions is not supported yet */
+		case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
 		default:
 			ret = -ENXIO;
 			break;

commit 22be5a133169e855097936438417ab1b672ad43f
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Jan 21 13:22:54 2016 +0100

    KVM: s390: forward ESOP if available
    
    ESOP guarantees that during a protection exception, bit 61 of real location
    168-175 will only be set to 1 if it was because of ALCP or DATP. If the
    exception is due to LAP or KCP, the bit will always be set to 0.
    
    The old SOP definition allowed bit 61 to be unpredictable in case of LAP
    or KCP in some conditions. So ESOP replaces this unpredictability by
    a guarantee.
    
    Therefore, we can directly forward ESOP if it is available on our machine.
    We don't have to do anything when ESOP is disabled - the guest will simply
    expect unpredictable values. Our guest access functions are already
    handling ESOP properly.
    
    Please note that future functionality in KVM will require knowledge about
    ESOP being enabled for a guest or not.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6960468f28ad..2b5c14da3227 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -193,6 +193,17 @@ void kvm_arch_hardware_unsetup(void)
 					 &kvm_clock_notifier);
 }
 
+static void allow_cpu_feat(unsigned long nr)
+{
+	set_bit_inv(nr, kvm_s390_available_cpu_feat);
+}
+
+static void kvm_s390_cpu_feat_init(void)
+{
+	if (MACHINE_HAS_ESOP)
+		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
+}
+
 int kvm_arch_init(void *opaque)
 {
 	kvm_s390_dbf = debug_register("kvm-trace", 32, 1, 7 * sizeof(long));
@@ -204,6 +215,8 @@ int kvm_arch_init(void *opaque)
 		return -ENOMEM;
 	}
 
+	kvm_s390_cpu_feat_init();
+
 	/* Register floating interrupt controller interface. */
 	return kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
 }

commit 15c9705f0c8af2d19dede9866aec364746b269ef
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Mar 19 17:36:43 2015 +0100

    KVM: s390: interface to query and configure cpu features
    
    For now, we only have an interface to query and configure facilities
    indicated via STFL(E). However, we also have features indicated via
    SCLP, that have to be indicated to the guest by user space and usually
    require KVM support.
    
    This patch allows user space to query and configure available cpu features
    for the guest.
    
    Please note that disabling a feature doesn't necessarily mean that it is
    completely disabled (e.g. ESOP is mostly handled by the SIE). We will try
    our best to disable it.
    
    Most features (e.g. SCLP) can't directly be forwarded, as most of them need
    in addition to hardware support, support in KVM. As we later on want to
    turn these features in KVM explicitly on/off (to simulate different
    behavior), we have to filter all features provided by the hardware and
    make them configurable.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 44297ff53b44..6960468f28ad 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -26,6 +26,7 @@
 #include <linux/slab.h>
 #include <linux/timer.h>
 #include <linux/vmalloc.h>
+#include <linux/bitmap.h>
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/etr.h>
@@ -132,6 +133,9 @@ unsigned long kvm_s390_fac_list_mask_size(void)
 	return ARRAY_SIZE(kvm_s390_fac_list_mask);
 }
 
+/* available cpu features supported by kvm */
+static DECLARE_BITMAP(kvm_s390_available_cpu_feat, KVM_S390_VM_CPU_FEAT_NR_BITS);
+
 static struct gmap_notifier gmap_notifier;
 debug_info_t *kvm_s390_dbf;
 
@@ -677,6 +681,29 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 	return ret;
 }
 
+static int kvm_s390_set_processor_feat(struct kvm *kvm,
+				       struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_cpu_feat data;
+	int ret = -EBUSY;
+
+	if (copy_from_user(&data, (void __user *)attr->addr, sizeof(data)))
+		return -EFAULT;
+	if (!bitmap_subset((unsigned long *) data.feat,
+			   kvm_s390_available_cpu_feat,
+			   KVM_S390_VM_CPU_FEAT_NR_BITS))
+		return -EINVAL;
+
+	mutex_lock(&kvm->lock);
+	if (!atomic_read(&kvm->online_vcpus)) {
+		bitmap_copy(kvm->arch.cpu_feat, (unsigned long *) data.feat,
+			    KVM_S390_VM_CPU_FEAT_NR_BITS);
+		ret = 0;
+	}
+	mutex_unlock(&kvm->lock);
+	return ret;
+}
+
 static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret = -ENXIO;
@@ -685,6 +712,9 @@ static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_CPU_PROCESSOR:
 		ret = kvm_s390_set_processor(kvm, attr);
 		break;
+	case KVM_S390_VM_CPU_PROCESSOR_FEAT:
+		ret = kvm_s390_set_processor_feat(kvm, attr);
+		break;
 	}
 	return ret;
 }
@@ -733,6 +763,31 @@ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 	return ret;
 }
 
+static int kvm_s390_get_processor_feat(struct kvm *kvm,
+				       struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_cpu_feat data;
+
+	bitmap_copy((unsigned long *) data.feat, kvm->arch.cpu_feat,
+		    KVM_S390_VM_CPU_FEAT_NR_BITS);
+	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
+		return -EFAULT;
+	return 0;
+}
+
+static int kvm_s390_get_machine_feat(struct kvm *kvm,
+				     struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_cpu_feat data;
+
+	bitmap_copy((unsigned long *) data.feat,
+		    kvm_s390_available_cpu_feat,
+		    KVM_S390_VM_CPU_FEAT_NR_BITS);
+	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
+		return -EFAULT;
+	return 0;
+}
+
 static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret = -ENXIO;
@@ -744,6 +799,12 @@ static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_CPU_MACHINE:
 		ret = kvm_s390_get_machine(kvm, attr);
 		break;
+	case KVM_S390_VM_CPU_PROCESSOR_FEAT:
+		ret = kvm_s390_get_processor_feat(kvm, attr);
+		break;
+	case KVM_S390_VM_CPU_MACHINE_FEAT:
+		ret = kvm_s390_get_machine_feat(kvm, attr);
+		break;
 	}
 	return ret;
 }
@@ -827,6 +888,8 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 		switch (attr->attr) {
 		case KVM_S390_VM_CPU_PROCESSOR:
 		case KVM_S390_VM_CPU_MACHINE:
+		case KVM_S390_VM_CPU_PROCESSOR_FEAT:
+		case KVM_S390_VM_CPU_MACHINE_FEAT:
 			ret = 0;
 			break;
 		default:

commit 7d0a5e62411a9223512c6af2e4c08a2d7c00fa2e
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Tue May 10 15:03:42 2016 +0200

    KVM: s390: Limit sthyi execution
    
    Store hypervisor information is a valid instruction not only in
    supervisor state but also in problem state, i.e. the guest's
    userspace. Its execution is not only computational and memory
    intensive, but also has to get hold of the ipte lock to write to the
    guest's memory.
    
    This lock is not intended to be held often and long, especially not
    from the untrusted guest userspace. Therefore we apply rate limiting
    of sthyi executions per VM.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Acked-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1c10254119b3..44297ff53b44 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1151,6 +1151,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	rc = -ENOMEM;
 
+	ratelimit_state_init(&kvm->arch.sthyi_limit, 5 * HZ, 500);
+
 	kvm->arch.use_esca = 0; /* start with basic SCA */
 	rwlock_init(&kvm->arch.sca_lock);
 	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(GFP_KERNEL);

commit 95ca2cb57985b07f5b136405f80a5106f5b06641
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Mon May 23 15:11:58 2016 +0200

    KVM: s390: Add sthyi emulation
    
    Store Hypervisor Information is an emulated z/VM instruction that
    provides a guest with basic information about the layers it is running
    on. This includes information about the cpu configuration of both the
    machine and the lpar, as well as their names, machine model and
    machine type. This information enables an application to determine the
    maximum capacity of CPs and IFLs available to software.
    
    The instruction is available whenever the facility bit 74 is set,
    otherwise executing it results in an operation exception.
    
    It is important to check the validity flags in the sections before
    using data from any structure member. It is not guaranteed that all
    members will be valid on all machines / machine configurations.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f0addece729e..1c10254119b3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -94,6 +94,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_stsi", VCPU_STAT(instruction_stsi) },
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
 	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
+	{ "instruction_sthyi", VCPU_STAT(instruction_sthyi) },
 	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
 	{ "instruction_sigp_sense_running", VCPU_STAT(instruction_sigp_sense_running) },
 	{ "instruction_sigp_external_call", VCPU_STAT(instruction_sigp_external_call) },
@@ -1189,6 +1190,9 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	memcpy(kvm->arch.model.fac_list, kvm->arch.model.fac_mask,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
+	set_kvm_facility(kvm->arch.model.fac_mask, 74);
+	set_kvm_facility(kvm->arch.model.fac_list, 74);
+
 	kvm->arch.model.cpuid = kvm_s390_get_initial_cpuid();
 	kvm->arch.model.ibc = sclp.ibc & 0x0fff;
 
@@ -1679,6 +1683,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	}
 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
+	if (test_kvm_facility(vcpu->kvm, 74))
+		vcpu->arch.sie_block->ictl |= ICTL_OPEREXC;
 
 	if (vcpu->kvm->arch.use_cmma) {
 		rc = kvm_s390_vcpu_setup_cmma(vcpu);

commit a011eeb2a3d6cd778eb63bea0bf149ebbe658ab5
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Mon May 9 14:14:01 2016 +0200

    KVM: s390: Add operation exception interception handler
    
    This commit introduces code that handles operation exception
    interceptions. With this handler we can emulate instructions by using
    illegal opcodes.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6d8ec3ac9dd8..f0addece729e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -63,6 +63,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_instruction", VCPU_STAT(exit_instruction) },
 	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
+	{ "exit_operation_exception", VCPU_STAT(exit_operation_exception) },
 	{ "halt_successful_poll", VCPU_STAT(halt_successful_poll) },
 	{ "halt_attempted_poll", VCPU_STAT(halt_attempted_poll) },
 	{ "halt_poll_invalid", VCPU_STAT(halt_poll_invalid) },

commit 9ec6de19235889ab0118e970ee732cb33c9efc06
Author: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
Date:   Fri May 6 16:33:06 2016 +0300

    KVM: s390: Add stats for PEI events
    
    Add partial execution intercepted events in kvm_stats_debugfs.
    
    Signed-off-by: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c9ae53924a69..43f2a2b80490 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -61,6 +61,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_external_request", VCPU_STAT(exit_external_request) },
 	{ "exit_external_interrupt", VCPU_STAT(exit_external_interrupt) },
 	{ "exit_instruction", VCPU_STAT(exit_instruction) },
+	{ "exit_pei", VCPU_STAT(exit_pei) },
 	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
 	{ "halt_successful_poll", VCPU_STAT(halt_successful_poll) },

commit 0487c44d1e1070b636ba3f3a12494ec456c2d005
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Jun 10 09:22:31 2016 +0200

    KVM: s390: ignore IBC if zero
    
    Looks like we forgot about the special IBC value of 0 meaning "no IBC".
    Let's fix that, otherwise it gets rounded up and suddenly an IBC is active
    with the lowest possible machine.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Fixes: commit 053dd2308d81 ("KVM: s390: force ibc into valid range")
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6d8ec3ac9dd8..c9ae53924a69 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -657,7 +657,7 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 		kvm->arch.model.cpuid = proc->cpuid;
 		lowest_ibc = sclp.ibc >> 16 & 0xfff;
 		unblocked_ibc = sclp.ibc & 0xfff;
-		if (lowest_ibc) {
+		if (lowest_ibc && proc->ibc) {
 			if (proc->ibc > unblocked_ibc)
 				kvm->arch.model.ibc = unblocked_ibc;
 			else if (proc->ibc < lowest_ibc)

commit 3491caf2755e9f312666712510d80b00c81ff247
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 13 12:16:35 2016 +0200

    KVM: halt_polling: provide a way to qualify wakeups during poll
    
    Some wakeups should not be considered a sucessful poll. For example on
    s390 I/O interrupts are usually floating, which means that _ALL_ CPUs
    would be considered runnable - letting all vCPUs poll all the time for
    transactional like workload, even if one vCPU would be enough.
    This can result in huge CPU usage for large guests.
    This patch lets architectures provide a way to qualify wakeups if they
    should be considered a good/bad wakeups in regard to polls.
    
    For s390 the implementation will fence of halt polling for anything but
    known good, single vCPU events. The s390 implementation for floating
    interrupts does a wakeup for one vCPU, but the interrupt will be delivered
    by whatever CPU checks first for a pending interrupt. We prefer the
    woken up CPU by marking the poll of this CPU as "good" poll.
    This code will also mark several other wakeup reasons like IPI or
    expired timers as "good". This will of course also mark some events as
    not sucessful. As  KVM on z runs always as a 2nd level hypervisor,
    we prefer to not poll, unless we are really sure, though.
    
    This patch successfully limits the CPU usage for cases like uperf 1byte
    transactional ping pong workload or wakeup heavy workload like OLTP
    while still providing a proper speedup.
    
    This also introduced a new vcpu stat "halt_poll_no_tuning" that marks
    wakeups that are considered not good for polling.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Radim Krm <rkrcmar@redhat.com> (for an earlier version)
    Cc: David Matlack <dmatlack@google.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    [Rename config symbol. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c597201a5ca9..6d8ec3ac9dd8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -65,6 +65,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
 	{ "halt_successful_poll", VCPU_STAT(halt_successful_poll) },
 	{ "halt_attempted_poll", VCPU_STAT(halt_attempted_poll) },
+	{ "halt_poll_invalid", VCPU_STAT(halt_poll_invalid) },
 	{ "halt_wakeup", VCPU_STAT(halt_wakeup) },
 	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
@@ -2992,6 +2993,11 @@ static inline unsigned long nonhyp_mask(int i)
 	return 0x0000ffffffffffffUL >> (nonhyp_fai << 4);
 }
 
+void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu)
+{
+	vcpu->valid_wakeup = false;
+}
+
 static int __init kvm_s390_init(void)
 {
 	int i;

commit 60a37709ce603da62d0b60699b713558b4cfd513
Author: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
Date:   Fri Apr 1 15:38:57 2016 +0300

    KVM: s390: Populate mask of non-hypervisor managed facility bits
    
    When a guest is initializing, KVM provides facility bits that can be
    successfully used by the guest. It's done by applying
    kvm_s390_fac_list_mask mask on host facility bits stored by the STFLE
    instruction. Facility bits can be one of two kinds: it's either a
    hypervisor managed bit or non-hypervisor managed.
    
    The hardware provides information which bits need special handling.
    Let's automatically passthrough to guests new facility bits, that
    don't require hypervisor support.
    
    Signed-off-by: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Eric Farman <farman@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5fe660528f1a..c597201a5ca9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -118,9 +118,9 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 };
 
 /* upper facilities limit for kvm */
-unsigned long kvm_s390_fac_list_mask[] = {
-	0xffe6ffffffffffffUL,
-	0x005effffffffffffUL,
+unsigned long kvm_s390_fac_list_mask[16] = {
+	0xffe6000000000000UL,
+	0x005e000000000000UL,
 };
 
 unsigned long kvm_s390_fac_list_mask_size(void)
@@ -2985,13 +2985,26 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 	return;
 }
 
+static inline unsigned long nonhyp_mask(int i)
+{
+	unsigned int nonhyp_fai = (sclp.hmfai << i * 2) >> 30;
+
+	return 0x0000ffffffffffffUL >> (nonhyp_fai << 4);
+}
+
 static int __init kvm_s390_init(void)
 {
+	int i;
+
 	if (!sclp.has_sief2) {
 		pr_info("SIE not available\n");
 		return -ENODEV;
 	}
 
+	for (i = 0; i < 16; i++)
+		kvm_s390_fac_list_mask[i] |=
+			S390_lowcore.stfle_fac_list[i] & nonhyp_mask(i);
+
 	return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
 }
 

commit ed8dda0bf74b4fb8e73d8880c78effabd3285fd8
Author: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
Date:   Thu Mar 31 13:48:52 2016 +0300

    KVM: s390: Enable all facility bits that are known good for passthrough
    
    Some facility bits are in a range that is defined to be "ok for guests
    without any necessary hypervisor changes". Enable those bits.
    
    Signed-off-by: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index eb52b3624b41..5fe660528f1a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -119,8 +119,8 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 
 /* upper facilities limit for kvm */
 unsigned long kvm_s390_fac_list_mask[] = {
-	0xffe6fffbfcfdfc40UL,
-	0x005e800000000000UL,
+	0xffe6ffffffffffffUL,
+	0x005effffffffffffUL,
 };
 
 unsigned long kvm_s390_fac_list_mask_size(void)

commit 053dd2308d8174566af4ac0059397c3e4042ebfb
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 4 13:59:42 2016 +0200

    KVM: s390: force ibc into valid range
    
    Some hardware variants will round the ibc value up/down themselves,
    others will report a validity intercept. Let's always round it up/down.
    
    This patch will also make sure that the ibc is set to 0 in case we don't
    have ibc support (lowest_ibc == 0).
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0d2f24210198..eb52b3624b41 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -638,6 +638,7 @@ static int kvm_s390_get_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	struct kvm_s390_vm_cpu_processor *proc;
+	u16 lowest_ibc, unblocked_ibc;
 	int ret = 0;
 
 	mutex_lock(&kvm->lock);
@@ -653,7 +654,16 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 	if (!copy_from_user(proc, (void __user *)attr->addr,
 			    sizeof(*proc))) {
 		kvm->arch.model.cpuid = proc->cpuid;
-		kvm->arch.model.ibc = proc->ibc;
+		lowest_ibc = sclp.ibc >> 16 & 0xfff;
+		unblocked_ibc = sclp.ibc & 0xfff;
+		if (lowest_ibc) {
+			if (proc->ibc > unblocked_ibc)
+				kvm->arch.model.ibc = unblocked_ibc;
+			else if (proc->ibc < lowest_ibc)
+				kvm->arch.model.ibc = lowest_ibc;
+			else
+				kvm->arch.model.ibc = proc->ibc;
+		}
 		memcpy(kvm->arch.model.fac_list, proc->fac_list,
 		       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	} else

commit 9bb0ec099756078118d8079e09f2bf3d6e7e00b6
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 4 14:27:51 2016 +0200

    KVM: s390: cleanup cpuid handling
    
    We only have one cpuid for all VCPUs, so let's directly use the one in the
    cpu model. Also always store it directly as u64, no need for struct cpuid.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e1e375c8010c..0d2f24210198 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -652,8 +652,7 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 	}
 	if (!copy_from_user(proc, (void __user *)attr->addr,
 			    sizeof(*proc))) {
-		memcpy(&kvm->arch.model.cpu_id, &proc->cpuid,
-		       sizeof(struct cpuid));
+		kvm->arch.model.cpuid = proc->cpuid;
 		kvm->arch.model.ibc = proc->ibc;
 		memcpy(kvm->arch.model.fac_list, proc->fac_list,
 		       S390_ARCH_FAC_LIST_SIZE_BYTE);
@@ -687,7 +686,7 @@ static int kvm_s390_get_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 		ret = -ENOMEM;
 		goto out;
 	}
-	memcpy(&proc->cpuid, &kvm->arch.model.cpu_id, sizeof(struct cpuid));
+	proc->cpuid = kvm->arch.model.cpuid;
 	proc->ibc = kvm->arch.model.ibc;
 	memcpy(&proc->fac_list, kvm->arch.model.fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
@@ -1081,10 +1080,13 @@ static void kvm_s390_set_crycb_format(struct kvm *kvm)
 		kvm->arch.crypto.crycbd |= CRYCB_FORMAT1;
 }
 
-static void kvm_s390_get_cpu_id(struct cpuid *cpu_id)
+static u64 kvm_s390_get_initial_cpuid(void)
 {
-	get_cpu_id(cpu_id);
-	cpu_id->version = 0xff;
+	struct cpuid cpuid;
+
+	get_cpu_id(&cpuid);
+	cpuid.version = 0xff;
+	return *((u64 *) &cpuid);
 }
 
 static void kvm_s390_crypto_init(struct kvm *kvm)
@@ -1175,7 +1177,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	memcpy(kvm->arch.model.fac_list, kvm->arch.model.fac_mask,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
-	kvm_s390_get_cpu_id(&kvm->arch.model.cpu_id);
+	kvm->arch.model.cpuid = kvm_s390_get_initial_cpuid();
 	kvm->arch.model.ibc = sclp.ibc & 0x0fff;
 
 	kvm_s390_crypto_init(kvm);
@@ -1624,7 +1626,6 @@ static void kvm_s390_vcpu_setup_model(struct kvm_vcpu *vcpu)
 {
 	struct kvm_s390_cpu_model *model = &vcpu->kvm->arch.model;
 
-	vcpu->arch.cpu_id = model->cpu_id;
 	vcpu->arch.sie_block->ibc = model->ibc;
 	if (test_kvm_facility(vcpu->kvm, 7))
 		vcpu->arch.sie_block->fac = (u32)(u64) model->fac_list;

commit bd50e8ec8f668de472b3bf782ac95866056257dd
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Mar 4 12:23:55 2016 +0100

    KVM: s390: enable SRS only if enabled for the guest
    
    If we don't have SIGP SENSE RUNNING STATUS enabled for the guest, let's
    not enable interpretation so we can correctly report an invalid order.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 13b3f5eb49be..e1e375c8010c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1645,7 +1645,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	kvm_s390_vcpu_setup_model(vcpu);
 
-	vcpu->arch.sie_block->ecb   = 6;
+	vcpu->arch.sie_block->ecb = 0x02;
+	if (test_kvm_facility(vcpu->kvm, 9))
+		vcpu->arch.sie_block->ecb |= 0x04;
 	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))
 		vcpu->arch.sie_block->ecb |= 0x10;
 

commit d6af0b491b1f675e5e5a12c5587f804c698a6ba5
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Mar 4 11:55:56 2016 +0100

    KVM: s390: enable PFMFI only if guest has EDAT1
    
    Only enable PFMF interpretation if the necessary facility (EDAT1) is
    available, otherwise the pfmf handler in priv.c will inject an exception
    
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 668c087513e5..13b3f5eb49be 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1649,7 +1649,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))
 		vcpu->arch.sie_block->ecb |= 0x10;
 
-	vcpu->arch.sie_block->ecb2  = 8;
+	if (test_kvm_facility(vcpu->kvm, 8))
+		vcpu->arch.sie_block->ecb2 |= 0x08;
 	vcpu->arch.sie_block->eca   = 0xC1002000U;
 	if (sclp.has_siif)
 		vcpu->arch.sie_block->eca |= 1;

commit 72aafdf01d826ad9e6fbd52bf689be937679f5d6
Merge: 1c8e85b17ac0 80c544ded25a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 10:47:45 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
    
     - Add the CPU id for the new z13s machine
    
     - Add a s390 specific XOR template for RAID-5 checksumming based on the
       XC instruction.  Remove all other alternatives, XC is always faster
    
     - The merge of our four different stack tracers into a single one
    
     - Tidy up the code related to page tables, several large inline
       functions are now out-of-line.  Bloat-o-meter reports ~11K text size
       reduction
    
     - A binary interface for the priviledged CLP instruction to retrieve
       the hardware view of the installed PCI functions
    
     - Improvements for the dasd format code
    
     - Bug fixes and cleanups
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (31 commits)
      s390/pci: enforce fmb page boundary rule
      s390: fix floating pointer register corruption (again)
      s390/cpumf: add missing lpp magic initialization
      s390: Fix misspellings in comments
      s390/mm: split arch/s390/mm/pgtable.c
      s390/mm: uninline pmdp_xxx functions from pgtable.h
      s390/mm: uninline ptep_xxx functions from pgtable.h
      s390/pci: add ioctl interface for CLP
      s390: Use pr_warn instead of pr_warning
      s390/dasd: remove casts to dasd_*_private
      s390/dasd: Refactor dasd format functions
      s390/dasd: Simplify code in format logic
      s390/dasd: Improve dasd format code
      s390/percpu: remove this_cpu_cmpxchg_double_4
      s390/cpumf: Improve guest detection heuristics
      s390/fault: merge report_user_fault implementations
      s390/dis: use correct escape sequence for '%' character
      s390/kvm: simplify set_guest_storage_key
      s390/oprofile: add z13/z13s model numbers
      s390: add z13s model number to z13 elf platform
      ...

commit 10dc3747661bea9215417b659449bb7b8ed3df2c
Merge: 047486d8e7c2 f958ee745f70
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 09:55:35 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "One of the largest releases for KVM...  Hardly any generic
      changes, but lots of architecture-specific updates.
    
      ARM:
       - VHE support so that we can run the kernel at EL2 on ARMv8.1 systems
       - PMU support for guests
       - 32bit world switch rewritten in C
       - various optimizations to the vgic save/restore code.
    
      PPC:
       - enabled KVM-VFIO integration ("VFIO device")
       - optimizations to speed up IPIs between vcpus
       - in-kernel handling of IOMMU hypercalls
       - support for dynamic DMA windows (DDW).
    
      s390:
       - provide the floating point registers via sync regs;
       - separated instruction vs.  data accesses
       - dirty log improvements for huge guests
       - bugfixes and documentation improvements.
    
      x86:
       - Hyper-V VMBus hypercall userspace exit
       - alternative implementation of lowest-priority interrupts using
         vector hashing (for better VT-d posted interrupt support)
       - fixed guest debugging with nested virtualizations
       - improved interrupt tracking in the in-kernel IOAPIC
       - generic infrastructure for tracking writes to guest
         memory - currently its only use is to speedup the legacy shadow
         paging (pre-EPT) case, but in the future it will be used for
         virtual GPUs as well
       - much cleanup (LAPIC, kvmclock, MMU, PIT), including ubsan fixes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (217 commits)
      KVM: x86: remove eager_fpu field of struct kvm_vcpu_arch
      KVM: x86: disable MPX if host did not enable MPX XSAVE features
      arm64: KVM: vgic-v3: Only wipe LRs on vcpu exit
      arm64: KVM: vgic-v3: Reset LRs at boot time
      arm64: KVM: vgic-v3: Do not save an LR known to be empty
      arm64: KVM: vgic-v3: Save maintenance interrupt state only if required
      arm64: KVM: vgic-v3: Avoid accessing ICH registers
      KVM: arm/arm64: vgic-v2: Make GICD_SGIR quicker to hit
      KVM: arm/arm64: vgic-v2: Only wipe LRs on vcpu exit
      KVM: arm/arm64: vgic-v2: Reset LRs at boot time
      KVM: arm/arm64: vgic-v2: Do not save an LR known to be empty
      KVM: arm/arm64: vgic-v2: Move GICH_ELRSR saving to its own function
      KVM: arm/arm64: vgic-v2: Save maintenance interrupt state only if required
      KVM: arm/arm64: vgic-v2: Avoid accessing GICH registers
      KVM: s390: allocate only one DMA page per VM
      KVM: s390: enable STFLE interpretation only if enabled for the guest
      KVM: s390: wake up when the VCPU cpu timer expires
      KVM: s390: step the VCPU timer while in enabled wait
      KVM: s390: protect VCPU cpu timer with a seqcount
      KVM: s390: step VCPU cpu timer during kvm_run ioctl
      ...

commit 1e133ab296f3ff8d9e58a5e758291ed39ba72ad7
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:49:57 2016 +0100

    s390/mm: split arch/s390/mm/pgtable.c
    
    The pgtable.c file is quite big, before it grows any larger split it
    into pgtable.c, pgalloc.c and gmap.c. In addition move the gmap related
    header definitions into the new gmap.h header and all of the pgste
    helpers from pgtable.h to pgtable.c.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 616e0a16ee88..be1f0288443e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -30,6 +30,7 @@
 #include <asm/lowcore.h>
 #include <asm/etr.h>
 #include <asm/pgtable.h>
+#include <asm/gmap.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>
 #include <asm/isc.h>
@@ -280,7 +281,7 @@ static void kvm_s390_sync_dirty_log(struct kvm *kvm,
 	for (cur_gfn = memslot->base_gfn; cur_gfn <= last_gfn; cur_gfn++) {
 		address = gfn_to_hva_memslot(memslot, cur_gfn);
 
-		if (pgste_test_and_clear_dirty(gmap->mm, address))
+		if (test_and_clear_guest_dirty(gmap->mm, address))
 			mark_page_dirty(kvm, cur_gfn);
 	}
 	up_read(&gmap->mm->mmap_sem);

commit ebde765c0e85f48534f98779b22349bf00761b61
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:08:09 2016 +0100

    s390/mm: uninline ptep_xxx functions from pgtable.h
    
    The code in the various ptep_xxx functions has grown quite large,
    consolidate them to four out-of-line functions:
      ptep_xchg_direct to exchange a pte with another with immediate flushing
      ptep_xchg_lazy to exchange a pte with another in a batched update
      ptep_modify_prot_start to begin a protection flags update
      ptep_modify_prot_commit to commit a protection flags update
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4af21c771f9b..616e0a16ee88 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -280,7 +280,7 @@ static void kvm_s390_sync_dirty_log(struct kvm *kvm,
 	for (cur_gfn = memslot->base_gfn; cur_gfn <= last_gfn; cur_gfn++) {
 		address = gfn_to_hva_memslot(memslot, cur_gfn);
 
-		if (gmap_test_and_clear_dirty(address, gmap))
+		if (pgste_test_and_clear_dirty(gmap->mm, address))
 			mark_page_dirty(kvm, cur_gfn);
 	}
 	up_read(&gmap->mm->mmap_sem);

commit c54f0d6ae057444453f5167e66ed999e8cf26936
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Dec 2 08:53:52 2015 +0100

    KVM: s390: allocate only one DMA page per VM
    
    We can fit the 2k for the STFLE interpretation and the crypto
    control block into one DMA page. As we now only have to allocate
    one DMA page, we can clean up the code a bit.
    
    As a nice side effect, this also fixes a problem with crycbd alignment in
    case special allocation debug options are enabled, debugged by Sascha
    Silbe.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b6a065403bdc..c186d55b87ac 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -355,8 +355,8 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		if (atomic_read(&kvm->online_vcpus)) {
 			r = -EBUSY;
 		} else if (MACHINE_HAS_VX) {
-			set_kvm_facility(kvm->arch.model.fac->mask, 129);
-			set_kvm_facility(kvm->arch.model.fac->list, 129);
+			set_kvm_facility(kvm->arch.model.fac_mask, 129);
+			set_kvm_facility(kvm->arch.model.fac_list, 129);
 			r = 0;
 		} else
 			r = -EINVAL;
@@ -370,8 +370,8 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		if (atomic_read(&kvm->online_vcpus)) {
 			r = -EBUSY;
 		} else if (test_facility(64)) {
-			set_kvm_facility(kvm->arch.model.fac->mask, 64);
-			set_kvm_facility(kvm->arch.model.fac->list, 64);
+			set_kvm_facility(kvm->arch.model.fac_mask, 64);
+			set_kvm_facility(kvm->arch.model.fac_list, 64);
 			r = 0;
 		}
 		mutex_unlock(&kvm->lock);
@@ -654,7 +654,7 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 		memcpy(&kvm->arch.model.cpu_id, &proc->cpuid,
 		       sizeof(struct cpuid));
 		kvm->arch.model.ibc = proc->ibc;
-		memcpy(kvm->arch.model.fac->list, proc->fac_list,
+		memcpy(kvm->arch.model.fac_list, proc->fac_list,
 		       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	} else
 		ret = -EFAULT;
@@ -688,7 +688,8 @@ static int kvm_s390_get_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 	}
 	memcpy(&proc->cpuid, &kvm->arch.model.cpu_id, sizeof(struct cpuid));
 	proc->ibc = kvm->arch.model.ibc;
-	memcpy(&proc->fac_list, kvm->arch.model.fac->list, S390_ARCH_FAC_LIST_SIZE_BYTE);
+	memcpy(&proc->fac_list, kvm->arch.model.fac_list,
+	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	if (copy_to_user((void __user *)attr->addr, proc, sizeof(*proc)))
 		ret = -EFAULT;
 	kfree(proc);
@@ -708,7 +709,7 @@ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 	}
 	get_cpu_id((struct cpuid *) &mach->cpuid);
 	mach->ibc = sclp.ibc;
-	memcpy(&mach->fac_mask, kvm->arch.model.fac->mask,
+	memcpy(&mach->fac_mask, kvm->arch.model.fac_mask,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
@@ -1085,16 +1086,12 @@ static void kvm_s390_get_cpu_id(struct cpuid *cpu_id)
 	cpu_id->version = 0xff;
 }
 
-static int kvm_s390_crypto_init(struct kvm *kvm)
+static void kvm_s390_crypto_init(struct kvm *kvm)
 {
 	if (!test_kvm_facility(kvm, 76))
-		return 0;
-
-	kvm->arch.crypto.crycb = kzalloc(sizeof(*kvm->arch.crypto.crycb),
-					 GFP_KERNEL | GFP_DMA);
-	if (!kvm->arch.crypto.crycb)
-		return -ENOMEM;
+		return;
 
+	kvm->arch.crypto.crycb = &kvm->arch.sie_page2->crycb;
 	kvm_s390_set_crycb_format(kvm);
 
 	/* Enable AES/DEA protected key functions by default */
@@ -1104,8 +1101,6 @@ static int kvm_s390_crypto_init(struct kvm *kvm)
 			 sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 	get_random_bytes(kvm->arch.crypto.crycb->dea_wrapping_key_mask,
 			 sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
-
-	return 0;
 }
 
 static void sca_dispose(struct kvm *kvm)
@@ -1159,37 +1154,30 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.dbf)
 		goto out_err;
 
-	/*
-	 * The architectural maximum amount of facilities is 16 kbit. To store
-	 * this amount, 2 kbyte of memory is required. Thus we need a full
-	 * page to hold the guest facility list (arch.model.fac->list) and the
-	 * facility mask (arch.model.fac->mask). Its address size has to be
-	 * 31 bits and word aligned.
-	 */
-	kvm->arch.model.fac =
-		(struct kvm_s390_fac *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
-	if (!kvm->arch.model.fac)
+	kvm->arch.sie_page2 =
+	     (struct sie_page2 *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
+	if (!kvm->arch.sie_page2)
 		goto out_err;
 
 	/* Populate the facility mask initially. */
-	memcpy(kvm->arch.model.fac->mask, S390_lowcore.stfle_fac_list,
+	memcpy(kvm->arch.model.fac_mask, S390_lowcore.stfle_fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
 		if (i < kvm_s390_fac_list_mask_size())
-			kvm->arch.model.fac->mask[i] &= kvm_s390_fac_list_mask[i];
+			kvm->arch.model.fac_mask[i] &= kvm_s390_fac_list_mask[i];
 		else
-			kvm->arch.model.fac->mask[i] = 0UL;
+			kvm->arch.model.fac_mask[i] = 0UL;
 	}
 
 	/* Populate the facility list initially. */
-	memcpy(kvm->arch.model.fac->list, kvm->arch.model.fac->mask,
+	kvm->arch.model.fac_list = kvm->arch.sie_page2->fac_list;
+	memcpy(kvm->arch.model.fac_list, kvm->arch.model.fac_mask,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
 	kvm_s390_get_cpu_id(&kvm->arch.model.cpu_id);
 	kvm->arch.model.ibc = sclp.ibc & 0x0fff;
 
-	if (kvm_s390_crypto_init(kvm) < 0)
-		goto out_err;
+	kvm_s390_crypto_init(kvm);
 
 	spin_lock_init(&kvm->arch.float_int.lock);
 	for (i = 0; i < FIRQ_LIST_COUNT; i++)
@@ -1225,8 +1213,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	return 0;
 out_err:
-	kfree(kvm->arch.crypto.crycb);
-	free_page((unsigned long)kvm->arch.model.fac);
+	free_page((unsigned long)kvm->arch.sie_page2);
 	debug_unregister(kvm->arch.dbf);
 	sca_dispose(kvm);
 	KVM_EVENT(3, "creation of vm failed: %d", rc);
@@ -1272,10 +1259,9 @@ static void kvm_free_vcpus(struct kvm *kvm)
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvm_free_vcpus(kvm);
-	free_page((unsigned long)kvm->arch.model.fac);
 	sca_dispose(kvm);
 	debug_unregister(kvm->arch.dbf);
-	kfree(kvm->arch.crypto.crycb);
+	free_page((unsigned long)kvm->arch.sie_page2);
 	if (!kvm_is_ucontrol(kvm))
 		gmap_free(kvm->arch.gmap);
 	kvm_s390_destroy_adapters(kvm);
@@ -1640,7 +1626,7 @@ static void kvm_s390_vcpu_setup_model(struct kvm_vcpu *vcpu)
 	vcpu->arch.cpu_id = model->cpu_id;
 	vcpu->arch.sie_block->ibc = model->ibc;
 	if (test_kvm_facility(vcpu->kvm, 7))
-		vcpu->arch.sie_block->fac = (int) (long) model->fac->list;
+		vcpu->arch.sie_block->fac = (u32)(u64) model->fac_list;
 }
 
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)

commit 80bc79dc0b18b17510ceb1e2d2d1999104af03c9
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Dec 2 09:43:29 2015 +0100

    KVM: s390: enable STFLE interpretation only if enabled for the guest
    
    Not setting the facility list designation disables STFLE interpretation,
    this is what we want if the guest was told to not have it.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b54daed49c2c..b6a065403bdc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1639,7 +1639,8 @@ static void kvm_s390_vcpu_setup_model(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.cpu_id = model->cpu_id;
 	vcpu->arch.sie_block->ibc = model->ibc;
-	vcpu->arch.sie_block->fac = (int) (long) model->fac->list;
+	if (test_kvm_facility(vcpu->kvm, 7))
+		vcpu->arch.sie_block->fac = (int) (long) model->fac->list;
 }
 
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)

commit 5ebda31686af6bb70affdcc5777ebc7ed81c0eac
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Feb 22 13:52:27 2016 +0100

    KVM: s390: step the VCPU timer while in enabled wait
    
    The cpu timer is a mean to measure task execution time. We want
    to account everything for a VCPU for which it is responsible. Therefore,
    if the VCPU wants to sleep, it shall be accounted for it.
    
    We can easily get this done by not disabling cpu timer accounting when
    scheduled out while sleeping because of enabled wait.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 38223c4603c7..b54daed49c2c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1538,7 +1538,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
-	if (vcpu->arch.cputm_enabled)
+	if (vcpu->arch.cputm_enabled && !is_vcpu_idle(vcpu))
 		__start_cpu_timer_accounting(vcpu);
 	vcpu->cpu = cpu;
 }
@@ -1546,7 +1546,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	vcpu->cpu = -1;
-	if (vcpu->arch.cputm_enabled)
+	if (vcpu->arch.cputm_enabled && !is_vcpu_idle(vcpu))
 		__stop_cpu_timer_accounting(vcpu);
 	atomic_andnot(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);

commit 9c23a1318eb12fcf76d9f663d2c3d88598e62a55
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Feb 17 21:53:33 2016 +0100

    KVM: s390: protect VCPU cpu timer with a seqcount
    
    For now, only the owning VCPU thread (that has loaded the VCPU) can get a
    consistent cpu timer value when calculating the delta. However, other
    threads might also be interested in a more recent, consistent value. Of
    special interest will be the timer callback of a VCPU that executes without
    having the VCPU loaded and could run in parallel with the VCPU thread.
    
    The cpu timer has a nice property: it is only updated by the owning VCPU
    thread. And speaking about accounting, a consistent value can only be
    calculated by looking at cputm_start and the cpu timer itself in
    one shot, otherwise the result might be wrong.
    
    As we only have one writing thread at a time (owning VCPU thread), we can
    use a seqcount instead of a seqlock and retry if the VCPU refreshed its
    cpu timer. This avoids any heavy locking and only introduces a counter
    update/check plus a handful of smp_wmb().
    
    The owning VCPU thread should never have to retry on reads, and also for
    other threads this might be a very rare scenario.
    
    Please note that we have to use the raw_* variants for locking the seqcount
    as lockdep will produce false warnings otherwise. The rq->lock held during
    vcpu_load/put is also acquired from hardirq context. Lockdep cannot know
    that we avoid potential deadlocks by disabling preemption and thereby
    disable concurrent write locking attempts (via vcpu_put/load).
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 76b99149dc65..38223c4603c7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1435,15 +1435,19 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 static void __start_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 {
 	WARN_ON_ONCE(vcpu->arch.cputm_start != 0);
+	raw_write_seqcount_begin(&vcpu->arch.cputm_seqcount);
 	vcpu->arch.cputm_start = get_tod_clock_fast();
+	raw_write_seqcount_end(&vcpu->arch.cputm_seqcount);
 }
 
 /* needs disabled preemption to protect from TOD sync and vcpu_load/put */
 static void __stop_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 {
 	WARN_ON_ONCE(vcpu->arch.cputm_start == 0);
+	raw_write_seqcount_begin(&vcpu->arch.cputm_seqcount);
 	vcpu->arch.sie_block->cputm -= get_tod_clock_fast() - vcpu->arch.cputm_start;
 	vcpu->arch.cputm_start = 0;
+	raw_write_seqcount_end(&vcpu->arch.cputm_seqcount);
 }
 
 /* needs disabled preemption to protect from TOD sync and vcpu_load/put */
@@ -1480,28 +1484,37 @@ static void disable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 void kvm_s390_set_cpu_timer(struct kvm_vcpu *vcpu, __u64 cputm)
 {
 	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
+	raw_write_seqcount_begin(&vcpu->arch.cputm_seqcount);
 	if (vcpu->arch.cputm_enabled)
 		vcpu->arch.cputm_start = get_tod_clock_fast();
 	vcpu->arch.sie_block->cputm = cputm;
+	raw_write_seqcount_end(&vcpu->arch.cputm_seqcount);
 	preempt_enable();
 }
 
 /* update and get the cpu timer - can also be called from other VCPU threads */
 __u64 kvm_s390_get_cpu_timer(struct kvm_vcpu *vcpu)
 {
+	unsigned int seq;
 	__u64 value;
-	int me;
 
 	if (unlikely(!vcpu->arch.cputm_enabled))
 		return vcpu->arch.sie_block->cputm;
 
-	me = get_cpu(); /* also protects from TOD sync and vcpu_load/put */
-	value = vcpu->arch.sie_block->cputm;
-	if (likely(me == vcpu->cpu)) {
-		/* the VCPU itself will always read consistent values */
-		value -= get_tod_clock_fast() - vcpu->arch.cputm_start;
-	}
-	put_cpu();
+	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
+	do {
+		seq = raw_read_seqcount(&vcpu->arch.cputm_seqcount);
+		/*
+		 * If the writer would ever execute a read in the critical
+		 * section, e.g. in irq context, we have a deadlock.
+		 */
+		WARN_ON_ONCE((seq & 1) && smp_processor_id() == vcpu->cpu);
+		value = vcpu->arch.sie_block->cputm;
+		/* if cputm_start is 0, accounting is being started/stopped */
+		if (likely(vcpu->arch.cputm_start))
+			value -= get_tod_clock_fast() - vcpu->arch.cputm_start;
+	} while (read_seqcount_retry(&vcpu->arch.cputm_seqcount, seq & ~1));
+	preempt_enable();
 	return value;
 }
 
@@ -1704,6 +1717,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
 	vcpu->arch.local_int.wq = &vcpu->wq;
 	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
+	seqcount_init(&vcpu->arch.cputm_seqcount);
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)

commit db0758b29709815d93a963e31e2ec87ecf74f8bd
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Feb 15 09:42:25 2016 +0100

    KVM: s390: step VCPU cpu timer during kvm_run ioctl
    
    Architecturally we should only provide steal time if we are scheduled
    away, and not if the host interprets a guest exit. We have to step
    the guest CPU timer in these cases.
    
    In the first shot, we will step the VCPU timer only during the kvm_run
    ioctl. Therefore all time spent e.g. in interception handlers or on irq
    delivery will be accounted for that VCPU.
    
    We have to take care of a few special cases:
    - Other VCPUs can test for pending irqs. We can only report a consistent
      value for the VCPU thread itself when adding the delta.
    - We have to take care of STP sync, therefore we have to extend
      kvm_clock_sync() and disable preemption accordingly
    - During any call to disable/enable/start/stop we could get premeempted
      and therefore get start/stop calls. Therefore we have to make sure we
      don't get into an inconsistent state.
    
    Whenever a VCPU is scheduled out, sleeping, in user space or just about
    to enter the SIE, the guest cpu timer isn't stepped.
    
    Please note that all primitives are prepared to be called from both
    environments (cpu timer accounting enabled or not), although not completely
    used in this patch yet (e.g. kvm_s390_set_cpu_timer() will never be called
    while cpu timer accounting is enabled).
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2118a2250ac7..76b99149dc65 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -158,6 +158,8 @@ static int kvm_clock_sync(struct notifier_block *notifier, unsigned long val,
 		kvm->arch.epoch -= *delta;
 		kvm_for_each_vcpu(i, vcpu, kvm) {
 			vcpu->arch.sie_block->epoch -= *delta;
+			if (vcpu->arch.cputm_enabled)
+				vcpu->arch.cputm_start += *delta;
 		}
 	}
 	return NOTIFY_OK;
@@ -1429,16 +1431,78 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
+static void __start_cpu_timer_accounting(struct kvm_vcpu *vcpu)
+{
+	WARN_ON_ONCE(vcpu->arch.cputm_start != 0);
+	vcpu->arch.cputm_start = get_tod_clock_fast();
+}
+
+/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
+static void __stop_cpu_timer_accounting(struct kvm_vcpu *vcpu)
+{
+	WARN_ON_ONCE(vcpu->arch.cputm_start == 0);
+	vcpu->arch.sie_block->cputm -= get_tod_clock_fast() - vcpu->arch.cputm_start;
+	vcpu->arch.cputm_start = 0;
+}
+
+/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
+static void __enable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
+{
+	WARN_ON_ONCE(vcpu->arch.cputm_enabled);
+	vcpu->arch.cputm_enabled = true;
+	__start_cpu_timer_accounting(vcpu);
+}
+
+/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
+static void __disable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
+{
+	WARN_ON_ONCE(!vcpu->arch.cputm_enabled);
+	__stop_cpu_timer_accounting(vcpu);
+	vcpu->arch.cputm_enabled = false;
+}
+
+static void enable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
+{
+	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
+	__enable_cpu_timer_accounting(vcpu);
+	preempt_enable();
+}
+
+static void disable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
+{
+	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
+	__disable_cpu_timer_accounting(vcpu);
+	preempt_enable();
+}
+
 /* set the cpu timer - may only be called from the VCPU thread itself */
 void kvm_s390_set_cpu_timer(struct kvm_vcpu *vcpu, __u64 cputm)
 {
+	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
+	if (vcpu->arch.cputm_enabled)
+		vcpu->arch.cputm_start = get_tod_clock_fast();
 	vcpu->arch.sie_block->cputm = cputm;
+	preempt_enable();
 }
 
-/* get the cpu timer - can also be called from other VCPU threads */
+/* update and get the cpu timer - can also be called from other VCPU threads */
 __u64 kvm_s390_get_cpu_timer(struct kvm_vcpu *vcpu)
 {
-	return vcpu->arch.sie_block->cputm;
+	__u64 value;
+	int me;
+
+	if (unlikely(!vcpu->arch.cputm_enabled))
+		return vcpu->arch.sie_block->cputm;
+
+	me = get_cpu(); /* also protects from TOD sync and vcpu_load/put */
+	value = vcpu->arch.sie_block->cputm;
+	if (likely(me == vcpu->cpu)) {
+		/* the VCPU itself will always read consistent values */
+		value -= get_tod_clock_fast() - vcpu->arch.cputm_start;
+	}
+	put_cpu();
+	return value;
 }
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
@@ -1461,12 +1525,16 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+	if (vcpu->arch.cputm_enabled)
+		__start_cpu_timer_accounting(vcpu);
 	vcpu->cpu = cpu;
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	vcpu->cpu = -1;
+	if (vcpu->arch.cputm_enabled)
+		__stop_cpu_timer_accounting(vcpu);
 	atomic_andnot(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
 
@@ -2277,10 +2345,12 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		 */
 		local_irq_disable();
 		__kvm_guest_enter();
+		__disable_cpu_timer_accounting(vcpu);
 		local_irq_enable();
 		exit_reason = sie64a(vcpu->arch.sie_block,
 				     vcpu->run->s.regs.gprs);
 		local_irq_disable();
+		__enable_cpu_timer_accounting(vcpu);
 		__kvm_guest_exit();
 		local_irq_enable();
 		vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
@@ -2358,6 +2428,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	}
 
 	sync_regs(vcpu, kvm_run);
+	enable_cpu_timer_accounting(vcpu);
 
 	might_fault();
 	rc = __vcpu_run(vcpu);
@@ -2377,6 +2448,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = 0;
 	}
 
+	disable_cpu_timer_accounting(vcpu);
 	store_regs(vcpu, kvm_run);
 
 	if (vcpu->sigset_active)

commit 4287f247f6cfaea0ed73b5104e94cd737e1ac0ae
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Feb 15 09:40:12 2016 +0100

    KVM: s390: abstract access to the VCPU cpu timer
    
    We want to manually step the cpu timer in certain scenarios in the future.
    Let's abstract any access to the cpu timer, so we can hide the complexity
    internally.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bd5edb138479..2118a2250ac7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1429,6 +1429,18 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/* set the cpu timer - may only be called from the VCPU thread itself */
+void kvm_s390_set_cpu_timer(struct kvm_vcpu *vcpu, __u64 cputm)
+{
+	vcpu->arch.sie_block->cputm = cputm;
+}
+
+/* get the cpu timer - can also be called from other VCPU threads */
+__u64 kvm_s390_get_cpu_timer(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.sie_block->cputm;
+}
+
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Save host register state */
@@ -1476,7 +1488,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->gpsw.mask = 0UL;
 	vcpu->arch.sie_block->gpsw.addr = 0UL;
 	kvm_s390_set_prefix(vcpu, 0);
-	vcpu->arch.sie_block->cputm     = 0UL;
+	kvm_s390_set_cpu_timer(vcpu, 0);
 	vcpu->arch.sie_block->ckc       = 0UL;
 	vcpu->arch.sie_block->todpr     = 0;
 	memset(vcpu->arch.sie_block->gcr, 0, 16 * sizeof(__u64));
@@ -1723,7 +1735,7 @@ static int kvm_arch_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
 			     (u64 __user *)reg->addr);
 		break;
 	case KVM_REG_S390_CPU_TIMER:
-		r = put_user(vcpu->arch.sie_block->cputm,
+		r = put_user(kvm_s390_get_cpu_timer(vcpu),
 			     (u64 __user *)reg->addr);
 		break;
 	case KVM_REG_S390_CLOCK_COMP:
@@ -1761,6 +1773,7 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 					   struct kvm_one_reg *reg)
 {
 	int r = -EINVAL;
+	__u64 val;
 
 	switch (reg->id) {
 	case KVM_REG_S390_TODPR:
@@ -1772,8 +1785,9 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 			     (u64 __user *)reg->addr);
 		break;
 	case KVM_REG_S390_CPU_TIMER:
-		r = get_user(vcpu->arch.sie_block->cputm,
-			     (u64 __user *)reg->addr);
+		r = get_user(val, (u64 __user *)reg->addr);
+		if (!r)
+			kvm_s390_set_cpu_timer(vcpu, val);
 		break;
 	case KVM_REG_S390_CLOCK_COMP:
 		r = get_user(vcpu->arch.sie_block->ckc,
@@ -2290,7 +2304,7 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 	}
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_ARCH0) {
-		vcpu->arch.sie_block->cputm = kvm_run->s.regs.cputm;
+		kvm_s390_set_cpu_timer(vcpu, kvm_run->s.regs.cputm);
 		vcpu->arch.sie_block->ckc = kvm_run->s.regs.ckc;
 		vcpu->arch.sie_block->todpr = kvm_run->s.regs.todpr;
 		vcpu->arch.sie_block->pp = kvm_run->s.regs.pp;
@@ -2312,7 +2326,7 @@ static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	kvm_run->psw_addr = vcpu->arch.sie_block->gpsw.addr;
 	kvm_run->s.regs.prefix = kvm_s390_get_prefix(vcpu);
 	memcpy(&kvm_run->s.regs.crs, &vcpu->arch.sie_block->gcr, 128);
-	kvm_run->s.regs.cputm = vcpu->arch.sie_block->cputm;
+	kvm_run->s.regs.cputm = kvm_s390_get_cpu_timer(vcpu);
 	kvm_run->s.regs.ckc = vcpu->arch.sie_block->ckc;
 	kvm_run->s.regs.todpr = vcpu->arch.sie_block->todpr;
 	kvm_run->s.regs.pp = vcpu->arch.sie_block->pp;
@@ -2383,7 +2397,7 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 	unsigned char archmode = 1;
 	freg_t fprs[NUM_FPRS];
 	unsigned int px;
-	u64 clkcomp;
+	u64 clkcomp, cputm;
 	int rc;
 
 	px = kvm_s390_get_prefix(vcpu);
@@ -2417,8 +2431,9 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 			      &vcpu->run->s.regs.fpc, 4);
 	rc |= write_guest_abs(vcpu, gpa + __LC_TOD_PROGREG_SAVE_AREA,
 			      &vcpu->arch.sie_block->todpr, 4);
+	cputm = kvm_s390_get_cpu_timer(vcpu);
 	rc |= write_guest_abs(vcpu, gpa + __LC_CPU_TIMER_SAVE_AREA,
-			      &vcpu->arch.sie_block->cputm, 8);
+			      &cputm, 8);
 	clkcomp = vcpu->arch.sie_block->ckc >> 8;
 	rc |= write_guest_abs(vcpu, gpa + __LC_CLOCK_COMP_SAVE_AREA,
 			      &clkcomp, 8);

commit 01a745ac8b6c5d323a37194c242f7c77f3402469
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Feb 12 20:41:56 2016 +0100

    KVM: s390: store cpu id in vcpu->cpu when scheduled in
    
    By storing the cpu id, we have a way to verify if the current cpu is
    owning a VCPU.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 28bd5ea1b08f..bd5edb138479 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1449,10 +1449,12 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+	vcpu->cpu = cpu;
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
+	vcpu->cpu = -1;
 	atomic_andnot(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
 

commit 9522b37f5a8c7bfabe46eecadf2e130f1103f337
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Mar 8 12:24:30 2016 +0100

    KVM: s390: correct fprs on SIGP (STOP AND) STORE STATUS
    
    With MACHINE_HAS_VX, we convert the floating point registers from the
    vector registeres when storing the status. For other VCPUs, these are
    stored to vcpu->run->s.regs.vrs, but we are using current->thread.fpu.vxrs,
    which resolves to the currently loaded VCPU.
    
    So kvm_s390_store_status_unloaded() currently writes the wrong floating
    point registers (converted from the vector registers) when called from
    another VCPU on a z13.
    
    This is only the case for old user space not handling SIGP STORE STATUS and
    SIGP STOP AND STORE STATUS, but relying on the kernel implementation. All
    other calls come from the loaded VCPU via kvm_s390_store_status().
    
    Fixes: 9abc2a08a7d6 (KVM: s390: fix memory overwrites when vx is disabled)
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: stable@vger.kernel.org # v4.4+
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4af21c771f9b..03dfe9c667f4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2381,7 +2381,7 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 
 	/* manually convert vector registers if necessary */
 	if (MACHINE_HAS_VX) {
-		convert_vx_to_fp(fprs, current->thread.fpu.vxrs);
+		convert_vx_to_fp(fprs, (__vector128 *) vcpu->run->s.regs.vrs);
 		rc = write_guest_abs(vcpu, gpa + __LC_FPREGS_SAVE_AREA,
 				     fprs, 128);
 	} else {

commit 1763f8d09d522b3ac998229dcf038476e88b78fc
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Feb 3 11:12:34 2016 +0100

    KVM: s390: bail out early on fatal signal in dirty logging
    
    A KVM_GET_DIRTY_LOG ioctl might take a long time.
    This can result in fatal signals seemingly being ignored.
    Lets bail out during the dirty bit sync, if a fatal signal
    is pending.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bb99ca28eb66..28bd5ea1b08f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -281,6 +281,8 @@ static void kvm_s390_sync_dirty_log(struct kvm *kvm,
 
 		if (gmap_test_and_clear_dirty(address, gmap))
 			mark_page_dirty(kvm, cur_gfn);
+		if (fatal_signal_pending(current))
+			return;
 		cond_resched();
 	}
 }

commit 70c88a00fbf65990b6268dabd305113460fe94c1
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Feb 2 15:15:56 2016 +0100

    KVM: s390: do not block CPU on dirty logging
    
    When doing dirty logging on huge guests (e.g.600GB) we sometimes
    get rcu stall timeouts with backtraces like
    
    [ 2753.194083] ([<0000000000112fb2>] show_trace+0x12a/0x130)
    [ 2753.194092]  [<0000000000113024>] show_stack+0x6c/0xe8
    [ 2753.194094]  [<00000000001ee6a8>] rcu_pending+0x358/0xa48
    [ 2753.194099]  [<00000000001f20cc>] rcu_check_callbacks+0x84/0x168
    [ 2753.194102]  [<0000000000167654>] update_process_times+0x54/0x80
    [ 2753.194107]  [<00000000001bdb5c>] tick_sched_handle.isra.16+0x4c/0x60
    [ 2753.194113]  [<00000000001bdbd8>] tick_sched_timer+0x68/0x90
    [ 2753.194115]  [<0000000000182a88>] __run_hrtimer+0x88/0x1f8
    [ 2753.194119]  [<00000000001838ba>] hrtimer_interrupt+0x122/0x2b0
    [ 2753.194121]  [<000000000010d034>] do_extint+0x16c/0x170
    [ 2753.194123]  [<00000000005e206e>] ext_skip+0x38/0x3e
    [ 2753.194129]  [<000000000012157c>] gmap_test_and_clear_dirty+0xcc/0x118
    [ 2753.194134] ([<00000000001214ea>] gmap_test_and_clear_dirty+0x3a/0x118)
    [ 2753.194137]  [<0000000000132da4>] kvm_vm_ioctl_get_dirty_log+0xd4/0x1b0
    [ 2753.194143]  [<000000000012ac12>] kvm_vm_ioctl+0x21a/0x548
    [ 2753.194146]  [<00000000002b57f6>] do_vfs_ioctl+0x30e/0x518
    [ 2753.194149]  [<00000000002b5a9c>] SyS_ioctl+0x9c/0xb0
    [ 2753.194151]  [<00000000005e1ae6>] sysc_tracego+0x14/0x1a
    [ 2753.194153]  [<000003ffb75f3972>] 0x3ffb75f3972
    
    We should do a cond_resched in here.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d4bcd863b24a..bb99ca28eb66 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -281,6 +281,7 @@ static void kvm_s390_sync_dirty_log(struct kvm *kvm,
 
 		if (gmap_test_and_clear_dirty(address, gmap))
 			mark_page_dirty(kvm, cur_gfn);
+		cond_resched();
 	}
 }
 

commit ab99a1cc7a405fed5148cf6a6fc26eec75b8a7a7
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Feb 3 10:18:41 2016 +0100

    KVM: s390: do not take mmap_sem on dirty log query
    
    Dirty log query can take a long time for huge guests.
    Holding the mmap_sem for very long times  can cause some unwanted
    latencies.
    Turns out that we do not need to hold the mmap semaphore.
    We hold the slots_lock for gfn->hva translation and walk the page
    tables with that address, so no need to look at the VMAs. KVM also
    holds a reference to the mm, which should prevent other things
    going away. During the walk we take the necessary ptl locks.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d0dcf73f36bc..d4bcd863b24a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -274,7 +274,6 @@ static void kvm_s390_sync_dirty_log(struct kvm *kvm,
 	unsigned long address;
 	struct gmap *gmap = kvm->arch.gmap;
 
-	down_read(&gmap->mm->mmap_sem);
 	/* Loop over all guest pages */
 	last_gfn = memslot->base_gfn + memslot->npages;
 	for (cur_gfn = memslot->base_gfn; cur_gfn <= last_gfn; cur_gfn++) {
@@ -283,7 +282,6 @@ static void kvm_s390_sync_dirty_log(struct kvm *kvm,
 		if (gmap_test_and_clear_dirty(address, gmap))
 			mark_page_dirty(kvm, cur_gfn);
 	}
-	up_read(&gmap->mm->mmap_sem);
 }
 
 /* Section: vm related */

commit 9b0d721a07a2d92c79362dda8e6d896b2c107ce6
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Jan 12 17:40:54 2016 +0100

    KVM: s390: instruction-fetching exceptions on SIE faults
    
    On instruction-fetch exceptions, we have to forward the PSW by any
    valid ilc and correctly use that ilc when injecting the irq. Injection
    will already take care of rewinding the PSW if we injected a nullifying
    program irq, so we don't need special handling prior to injection.
    
    Until now, autodetection would have guessed an ilc of 0.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 12cec63eda27..d0dcf73f36bc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2181,9 +2181,17 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 	 * to be able to forward the PSW.
 	 */
 	rc = read_guest_instr(vcpu, &opcode, 1);
-	if (rc)
-		return kvm_s390_inject_prog_cond(vcpu, rc);
 	ilen = insn_length(opcode);
+	if (rc < 0) {
+		return rc;
+	} else if (rc) {
+		/* Instruction-Fetching Exceptions - we can't detect the ilen.
+		 * Forward by arbitrary ilc, injection will take care of
+		 * nullification if necessary.
+		 */
+		pgm_info = vcpu->arch.pgm;
+		ilen = 4;
+	}
 	pgm_info.flags = ilen | KVM_S390_PGM_FLAGS_ILC_VALID;
 	kvm_s390_forward_psw(vcpu, ilen);
 	return kvm_s390_inject_prog_irq(vcpu, &pgm_info);

commit 5631792053f094a8e2f01d5ddcc2550ad4da22f0
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Jan 12 17:37:58 2016 +0100

    KVM: s390: provide prog irq ilc on SIE faults
    
    On SIE faults, the ilc cannot be detected automatically, as the icptcode
    is 0. The ilc indicated in the program irq will always be 0. Therefore we
    have to manually specify the ilc in order to tell the guest which ilen was
    used when forwarding the PSW.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index aa51a8d5179f..12cec63eda27 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2163,7 +2163,10 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 
 static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 {
-	u8 opcode;
+	struct kvm_s390_pgm_info pgm_info = {
+		.code = PGM_ADDRESSING,
+	};
+	u8 opcode, ilen;
 	int rc;
 
 	VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
@@ -2180,9 +2183,10 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 	rc = read_guest_instr(vcpu, &opcode, 1);
 	if (rc)
 		return kvm_s390_inject_prog_cond(vcpu, rc);
-	kvm_s390_forward_psw(vcpu, insn_length(opcode));
-
-	return kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
+	ilen = insn_length(opcode);
+	pgm_info.flags = ilen | KVM_S390_PGM_FLAGS_ILC_VALID;
+	kvm_s390_forward_psw(vcpu, ilen);
+	return kvm_s390_inject_prog_irq(vcpu, &pgm_info);
 }
 
 static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)

commit 659773227506f7b971cfd52e1aa146c896405187
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Nov 16 16:17:45 2015 +0100

    KVM: s390: read the correct opcode on SIE faults
    
    Let's use our fresh new function read_guest_instr() to access
    guest storage via the correct addressing schema.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 85e169b8e90d..aa51a8d5179f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2163,7 +2163,6 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 
 static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 {
-	psw_t *psw = &vcpu->arch.sie_block->gpsw;
 	u8 opcode;
 	int rc;
 
@@ -2178,7 +2177,7 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 	 * to look up the current opcode to get the length of the instruction
 	 * to be able to forward the PSW.
 	 */
-	rc = read_guest(vcpu, psw->addr, 0, &opcode, 1);
+	rc = read_guest_instr(vcpu, &opcode, 1);
 	if (rc)
 		return kvm_s390_inject_prog_cond(vcpu, rc);
 	kvm_s390_forward_psw(vcpu, insn_length(opcode));

commit 92c9632119b67f3e201240f6813cd0343bfb0141
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Nov 16 15:42:11 2015 +0100

    KVM: s390: gaccess: introduce access modes
    
    We will need special handling when fetching instructions, so let's
    introduce new guest access modes GACC_FETCH and GACC_STORE instead
    of a write flag. An additional patch will then introduce GACC_IFETCH.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index cd84a3eeb214..85e169b8e90d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2610,7 +2610,8 @@ static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 	switch (mop->op) {
 	case KVM_S390_MEMOP_LOGICAL_READ:
 		if (mop->flags & KVM_S390_MEMOP_F_CHECK_ONLY) {
-			r = check_gva_range(vcpu, mop->gaddr, mop->ar, mop->size, false);
+			r = check_gva_range(vcpu, mop->gaddr, mop->ar,
+					    mop->size, GACC_FETCH);
 			break;
 		}
 		r = read_guest(vcpu, mop->gaddr, mop->ar, tmpbuf, mop->size);
@@ -2621,7 +2622,8 @@ static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
 		break;
 	case KVM_S390_MEMOP_LOGICAL_WRITE:
 		if (mop->flags & KVM_S390_MEMOP_F_CHECK_ONLY) {
-			r = check_gva_range(vcpu, mop->gaddr, mop->ar, mop->size, true);
+			r = check_gva_range(vcpu, mop->gaddr, mop->ar,
+					    mop->size, GACC_STORE);
 			break;
 		}
 		if (copy_from_user(tmpbuf, uaddr, mop->size)) {

commit 0e8bc06a2fbb4d6b688baa8e2416cd07f9453595
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Nov 4 13:47:58 2015 +0100

    KVM: s390: PSW forwarding / rewinding / ilc rework
    
    We have some confusion about ilc vs. ilen in our current code. So let's
    correctly use the term ilen when dealing with (ilc << 1).
    
    Program irq injection didn't take care of the correct ilc in case of
    irqs triggered by EXECUTE functions, let's provide one function
    kvm_s390_get_ilen() to take care of all that.
    
    Also, manually specifying in intercept handlers the size of the
    instruction (and sometimes overwriting that value for EXECUTE internally)
    doesn't make too much sense. So also provide the functions:
    - kvm_s390_retry_instr to retry the currently intercepted instruction
    - kvm_s390_rewind_psw to rewind the PSW without internal overwrites
    - kvm_s390_forward_psw to forward the PSW
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2270fe4c8b71..cd84a3eeb214 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2181,7 +2181,7 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 	rc = read_guest(vcpu, psw->addr, 0, &opcode, 1);
 	if (rc)
 		return kvm_s390_inject_prog_cond(vcpu, rc);
-	psw->addr = __rewind_psw(*psw, -insn_length(opcode));
+	kvm_s390_forward_psw(vcpu, insn_length(opcode));
 
 	return kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
 }

commit 6fd8e67dd83437118cf46a4a9c224142004c3d51
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Jan 18 14:46:34 2016 +0100

    KVM: s390: sync of fp registers via kvm_run
    
    As we already store the floating point registers in the vector save area
    in floating point register format when we don't have MACHINE_HAS_VX, we can
    directly expose them to user space using a new sync flag.
    
    The floating point registers will be valid when KVM_SYNC_FPRS is set. The
    fpc will also be valid when KVM_SYNC_FPRS is set.
    
    Either KVM_SYNC_FPRS or KVM_SYNC_VRS will be enabled, never both.
    
    Let's also change two positions where we access vrs, making the code easier
    to read and one comment superfluous.
    
    Suggested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8fcdf9836321..2270fe4c8b71 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1419,6 +1419,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	 */
 	if (MACHINE_HAS_VX)
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_VRS;
+	else
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_FPRS;
 
 	if (kvm_is_ucontrol(vcpu->kvm))
 		return __kvm_ucontrol_vcpu_init(vcpu);
@@ -1433,10 +1435,10 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	vcpu->arch.host_fpregs.fpc = current->thread.fpu.fpc;
 	vcpu->arch.host_fpregs.regs = current->thread.fpu.regs;
 
-	/* Depending on MACHINE_HAS_VX, data stored to vrs either
-	 * has vector register or floating point register format.
-	 */
-	current->thread.fpu.regs = vcpu->run->s.regs.vrs;
+	if (MACHINE_HAS_VX)
+		current->thread.fpu.regs = vcpu->run->s.regs.vrs;
+	else
+		current->thread.fpu.regs = vcpu->run->s.regs.fprs;
 	current->thread.fpu.fpc = vcpu->run->s.regs.fpc;
 	if (test_fp_ctl(current->thread.fpu.fpc))
 		/* User space provided an invalid FPC, let's clear it */
@@ -2389,7 +2391,7 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 				     fprs, 128);
 	} else {
 		rc = write_guest_abs(vcpu, gpa + __LC_FPREGS_SAVE_AREA,
-				     vcpu->run->s.regs.vrs, 128);
+				     vcpu->run->s.regs.fprs, 128);
 	}
 	rc |= write_guest_abs(vcpu, gpa + __LC_GPREGS_SAVE_AREA,
 			      vcpu->run->s.regs.gprs, 128);

commit f6aa6dc44948739be39c20bbcbbe8cff5d77fe18
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Jan 15 14:11:46 2016 +0100

    KVM: s390: allow sync of fp registers via vregs
    
    If we have MACHINE_HAS_VX, the floating point registers are stored
    in the vector register format, event if the guest isn't enabled for vector
    registers. So we can allow KVM_SYNC_VRS as soon as MACHINE_HAS_VX is
    available.
    
    This can in return be used by user space to support floating point
    registers via struct kvm_run when the machine has vector registers.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4af21c771f9b..8fcdf9836321 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1414,7 +1414,10 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 				    KVM_SYNC_PFAULT;
 	if (test_kvm_facility(vcpu->kvm, 64))
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
-	if (test_kvm_facility(vcpu->kvm, 129))
+	/* fprs can be synchronized via vrs, even if the guest has no vx. With
+	 * MACHINE_HAS_VX, (load|store)_fpu_regs() will work with vrs format.
+	 */
+	if (MACHINE_HAS_VX)
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_VRS;
 
 	if (kvm_is_ucontrol(vcpu->kvm))

commit 9abc2a08a7d665b02bdde974fd6c44aae86e923e
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Jan 14 22:12:47 2016 +0100

    KVM: s390: fix memory overwrites when vx is disabled
    
    The kernel now always uses vector registers when available, however KVM
    has special logic if support is really enabled for a guest. If support
    is disabled, guest_fpregs.fregs will only contain memory for the fpu.
    The kernel, however, will store vector registers into that area,
    resulting in crazy memory overwrites.
    
    Simply extending that area is not enough, because the format of the
    registers also changes. We would have to do additional conversions, making
    the code even more complex. Therefore let's directly use one place for
    the vector/fpu registers + fpc (in kvm_run). We just have to convert the
    data properly when accessing it. This makes current code much easier.
    
    Please note that vector/fpu registers are now always stored to
    vcpu->run->s.regs.vrs. Although this data is visible to QEMU and
    used for migration, we only guarantee valid values to user space  when
    KVM_SYNC_VRS is set. As that is only the case when we have vector
    register support, we are on the safe side.
    
    Fixes: b5510d9b68c3 ("s390/fpu: always enable the vector facility if it is available")
    Cc: stable@vger.kernel.org # v4.4 d9a3a09af54d s390/kvm: remove dependency on struct save_area definition
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [adopt to d9a3a09af54d]

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 797dbc4e11c3..4af21c771f9b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1244,7 +1244,6 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 	if (vcpu->kvm->arch.use_cmma)
 		kvm_s390_vcpu_unsetup_cmma(vcpu);
-	kfree(vcpu->arch.guest_fpregs.fprs);
 	free_page((unsigned long)(vcpu->arch.sie_block));
 
 	kvm_vcpu_uninit(vcpu);
@@ -1424,44 +1423,18 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-/*
- * Backs up the current FP/VX register save area on a particular
- * destination.  Used to switch between different register save
- * areas.
- */
-static inline void save_fpu_to(struct fpu *dst)
-{
-	dst->fpc = current->thread.fpu.fpc;
-	dst->regs = current->thread.fpu.regs;
-}
-
-/*
- * Switches the FP/VX register save area from which to lazy
- * restore register contents.
- */
-static inline void load_fpu_from(struct fpu *from)
-{
-	current->thread.fpu.fpc = from->fpc;
-	current->thread.fpu.regs = from->regs;
-}
-
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Save host register state */
 	save_fpu_regs();
-	save_fpu_to(&vcpu->arch.host_fpregs);
-
-	if (test_kvm_facility(vcpu->kvm, 129)) {
-		current->thread.fpu.fpc = vcpu->run->s.regs.fpc;
-		/*
-		 * Use the register save area in the SIE-control block
-		 * for register restore and save in kvm_arch_vcpu_put()
-		 */
-		current->thread.fpu.vxrs =
-			(__vector128 *)&vcpu->run->s.regs.vrs;
-	} else
-		load_fpu_from(&vcpu->arch.guest_fpregs);
+	vcpu->arch.host_fpregs.fpc = current->thread.fpu.fpc;
+	vcpu->arch.host_fpregs.regs = current->thread.fpu.regs;
 
+	/* Depending on MACHINE_HAS_VX, data stored to vrs either
+	 * has vector register or floating point register format.
+	 */
+	current->thread.fpu.regs = vcpu->run->s.regs.vrs;
+	current->thread.fpu.fpc = vcpu->run->s.regs.fpc;
 	if (test_fp_ctl(current->thread.fpu.fpc))
 		/* User space provided an invalid FPC, let's clear it */
 		current->thread.fpu.fpc = 0;
@@ -1477,19 +1450,13 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	atomic_andnot(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
 
+	/* Save guest register state */
 	save_fpu_regs();
+	vcpu->run->s.regs.fpc = current->thread.fpu.fpc;
 
-	if (test_kvm_facility(vcpu->kvm, 129))
-		/*
-		 * kvm_arch_vcpu_load() set up the register save area to
-		 * the &vcpu->run->s.regs.vrs and, thus, the vector registers
-		 * are already saved.  Only the floating-point control must be
-		 * copied.
-		 */
-		vcpu->run->s.regs.fpc = current->thread.fpu.fpc;
-	else
-		save_fpu_to(&vcpu->arch.guest_fpregs);
-	load_fpu_from(&vcpu->arch.host_fpregs);
+	/* Restore host register state */
+	current->thread.fpu.fpc = vcpu->arch.host_fpregs.fpc;
+	current->thread.fpu.regs = vcpu->arch.host_fpregs.regs;
 
 	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_access_regs(vcpu->arch.host_acrs);
@@ -1507,8 +1474,9 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	memset(vcpu->arch.sie_block->gcr, 0, 16 * sizeof(__u64));
 	vcpu->arch.sie_block->gcr[0]  = 0xE0UL;
 	vcpu->arch.sie_block->gcr[14] = 0xC2000000UL;
-	vcpu->arch.guest_fpregs.fpc = 0;
-	asm volatile("lfpc %0" : : "Q" (vcpu->arch.guest_fpregs.fpc));
+	/* make sure the new fpc will be lazily loaded */
+	save_fpu_regs();
+	current->thread.fpu.fpc = 0;
 	vcpu->arch.sie_block->gbea = 1;
 	vcpu->arch.sie_block->pp = 0;
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
@@ -1649,27 +1617,14 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.local_int.wq = &vcpu->wq;
 	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
 
-	/*
-	 * Allocate a save area for floating-point registers.  If the vector
-	 * extension is available, register contents are saved in the SIE
-	 * control block.  The allocated save area is still required in
-	 * particular places, for example, in kvm_s390_vcpu_store_status().
-	 */
-	vcpu->arch.guest_fpregs.fprs = kzalloc(sizeof(freg_t) * __NUM_FPRS,
-					       GFP_KERNEL);
-	if (!vcpu->arch.guest_fpregs.fprs)
-		goto out_free_sie_block;
-
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
-		goto out_free_fprs;
+		goto out_free_sie_block;
 	VM_EVENT(kvm, 3, "create cpu %d at 0x%pK, sie block at 0x%pK", id, vcpu,
 		 vcpu->arch.sie_block);
 	trace_kvm_s390_create_vcpu(id, vcpu, vcpu->arch.sie_block);
 
 	return vcpu;
-out_free_fprs:
-	kfree(vcpu->arch.guest_fpregs.fprs);
 out_free_sie_block:
 	free_page((unsigned long)(vcpu->arch.sie_block));
 out_free_cpu:
@@ -1882,19 +1837,27 @@ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 
 int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
+	/* make sure the new values will be lazily loaded */
+	save_fpu_regs();
 	if (test_fp_ctl(fpu->fpc))
 		return -EINVAL;
-	memcpy(vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
-	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
-	save_fpu_regs();
-	load_fpu_from(&vcpu->arch.guest_fpregs);
+	current->thread.fpu.fpc = fpu->fpc;
+	if (MACHINE_HAS_VX)
+		convert_fp_to_vx(current->thread.fpu.vxrs, (freg_t *)fpu->fprs);
+	else
+		memcpy(current->thread.fpu.fprs, &fpu->fprs, sizeof(fpu->fprs));
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
-	memcpy(&fpu->fprs, vcpu->arch.guest_fpregs.fprs, sizeof(fpu->fprs));
-	fpu->fpc = vcpu->arch.guest_fpregs.fpc;
+	/* make sure we have the latest values */
+	save_fpu_regs();
+	if (MACHINE_HAS_VX)
+		convert_vx_to_fp((freg_t *)fpu->fprs, current->thread.fpu.vxrs);
+	else
+		memcpy(fpu->fprs, current->thread.fpu.fprs, sizeof(fpu->fprs));
+	fpu->fpc = current->thread.fpu.fpc;
 	return 0;
 }
 
@@ -2399,6 +2362,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 {
 	unsigned char archmode = 1;
+	freg_t fprs[NUM_FPRS];
 	unsigned int px;
 	u64 clkcomp;
 	int rc;
@@ -2414,8 +2378,16 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 		gpa = px;
 	} else
 		gpa -= __LC_FPREGS_SAVE_AREA;
-	rc = write_guest_abs(vcpu, gpa + __LC_FPREGS_SAVE_AREA,
-			     vcpu->arch.guest_fpregs.fprs, 128);
+
+	/* manually convert vector registers if necessary */
+	if (MACHINE_HAS_VX) {
+		convert_vx_to_fp(fprs, current->thread.fpu.vxrs);
+		rc = write_guest_abs(vcpu, gpa + __LC_FPREGS_SAVE_AREA,
+				     fprs, 128);
+	} else {
+		rc = write_guest_abs(vcpu, gpa + __LC_FPREGS_SAVE_AREA,
+				     vcpu->run->s.regs.vrs, 128);
+	}
 	rc |= write_guest_abs(vcpu, gpa + __LC_GPREGS_SAVE_AREA,
 			      vcpu->run->s.regs.gprs, 128);
 	rc |= write_guest_abs(vcpu, gpa + __LC_PSW_SAVE_AREA,
@@ -2423,7 +2395,7 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 	rc |= write_guest_abs(vcpu, gpa + __LC_PREFIX_SAVE_AREA,
 			      &px, 4);
 	rc |= write_guest_abs(vcpu, gpa + __LC_FP_CREG_SAVE_AREA,
-			      &vcpu->arch.guest_fpregs.fpc, 4);
+			      &vcpu->run->s.regs.fpc, 4);
 	rc |= write_guest_abs(vcpu, gpa + __LC_TOD_PROGREG_SAVE_AREA,
 			      &vcpu->arch.sie_block->todpr, 4);
 	rc |= write_guest_abs(vcpu, gpa + __LC_CPU_TIMER_SAVE_AREA,
@@ -2446,19 +2418,7 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	 * it into the save area
 	 */
 	save_fpu_regs();
-	if (test_kvm_facility(vcpu->kvm, 129)) {
-		/*
-		 * If the vector extension is available, the vector registers
-		 * which overlaps with floating-point registers are saved in
-		 * the SIE-control block.  Hence, extract the floating-point
-		 * registers and the FPC value and store them in the
-		 * guest_fpregs structure.
-		 */
-		vcpu->arch.guest_fpregs.fpc = current->thread.fpu.fpc;
-		convert_vx_to_fp(vcpu->arch.guest_fpregs.fprs,
-				 current->thread.fpu.vxrs);
-	} else
-		save_fpu_to(&vcpu->arch.guest_fpregs);
+	vcpu->run->s.regs.fpc = current->thread.fpu.fpc;
 	save_access_regs(vcpu->run->s.regs.acrs);
 
 	return kvm_s390_store_status_unloaded(vcpu, addr);

commit 9c7ebb613bffea2feef4ec562ba1dbcaa810942b
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Jan 22 14:55:56 2016 +0100

    KVM: s390: fix guest fprs memory leak
    
    fprs is never freed, therefore resulting in a memory leak if
    kvm_vcpu_init() fails or the vcpu is destroyed.
    
    Fixes: 9977e886cbbc ("s390/kernel: lazy restore fpu registers")
    Cc: stable@vger.kernel.org # v4.3+
    Reported-by: Eric Farman <farman@linux.vnet.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Eric Farman <farman@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 835d60bedb54..797dbc4e11c3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1244,6 +1244,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 	if (vcpu->kvm->arch.use_cmma)
 		kvm_s390_vcpu_unsetup_cmma(vcpu);
+	kfree(vcpu->arch.guest_fpregs.fprs);
 	free_page((unsigned long)(vcpu->arch.sie_block));
 
 	kvm_vcpu_uninit(vcpu);
@@ -1661,12 +1662,14 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
-		goto out_free_sie_block;
+		goto out_free_fprs;
 	VM_EVENT(kvm, 3, "create cpu %d at 0x%pK, sie block at 0x%pK", id, vcpu,
 		 vcpu->arch.sie_block);
 	trace_kvm_s390_create_vcpu(id, vcpu, vcpu->arch.sie_block);
 
 	return vcpu;
+out_free_fprs:
+	kfree(vcpu->arch.guest_fpregs.fprs);
 out_free_sie_block:
 	free_page((unsigned long)(vcpu->arch.sie_block));
 out_free_cpu:

commit cbd88cd4c07f9361914ab7fd7e21c9227986fe68
Merge: 928b3f12e5fe c2ab7282f0fc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 13 13:16:16 2016 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "Among the traditional bug fixes and cleanups are some improvements:
    
       - A tool to generated the facility lists, generating the bit fields
         by hand has been a source of bugs in the past
    
       - The spinlock loop is reordered to avoid bursts of hypervisor calls
    
       - Add support for the open-for-business interface to the service
         element
    
       - The get_cpu call is added to the vdso
    
       - A set of tracepoints is defined for the common I/O layer
    
       - The deprecated sclp_cpi module is removed
    
       - Update default configuration"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (56 commits)
      s390/sclp: fix possible control register corruption
      s390: fix normalization bug in exception table sorting
      s390/configs: update default configurations
      s390/vdso: optimize getcpu system call
      s390: drop smp_mb in vdso_init
      s390: rename struct _lowcore to struct lowcore
      s390/mem_detect: use unsigned longs
      s390/ptrace: get rid of long longs in psw_bits
      s390/sysinfo: add missing SYSIB 1.2.2 multithreading fields
      s390: get rid of CONFIG_SCHED_MC and CONFIG_SCHED_BOOK
      s390/Kconfig: remove pointless 64 bit dependencies
      s390/dasd: fix failfast for disconnected devices
      s390/con3270: testing return kzalloc retval
      s390/hmcdrv: constify hmcdrv_ftp_ops structs
      s390/cio: add NULL test
      s390/cio: Change I/O instructions from inline to normal functions
      s390/cio: Introduce common I/O layer tracepoints
      s390/cio: Consolidate inline assemblies and related data definitions
      s390/cio: Fix incorrect xsch opcode specification
      s390/cio: Remove unused inline assemblies
      ...

commit c6e5f166373a20a433c38a34bd16f2b62bd0864c
Author: Fan Zhang <zhangfan@linux.vnet.ibm.com>
Date:   Thu Jan 7 18:24:29 2016 +0800

    KVM: s390: implement the RI support of guest
    
    This patch adds runtime instrumentation support for KVM guest. We need to
    setup a save area for the runtime instrumentation-controls control block(RICCB)
    and implement the necessary interfaces to live migrate the guest settings.
    
    We setup the sie control block in a way, that the runtime
    instrumentation instructions of a guest are handled by hardware.
    
    We also add a capability KVM_CAP_S390_RI to make this feature opt-in as
    it needs migration support.
    
    Signed-off-by: Fan Zhang <zhangfan@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9f8eea3e2987..5927c61d322a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -258,6 +258,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_VECTOR_REGISTERS:
 		r = MACHINE_HAS_VX;
 		break;
+	case KVM_CAP_S390_RI:
+		r = test_facility(64);
+		break;
 	default:
 		r = 0;
 	}
@@ -358,6 +361,20 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_VECTOR_REGISTERS %s",
 			 r ? "(not available)" : "(success)");
 		break;
+	case KVM_CAP_S390_RI:
+		r = -EINVAL;
+		mutex_lock(&kvm->lock);
+		if (atomic_read(&kvm->online_vcpus)) {
+			r = -EBUSY;
+		} else if (test_facility(64)) {
+			set_kvm_facility(kvm->arch.model.fac->mask, 64);
+			set_kvm_facility(kvm->arch.model.fac->list, 64);
+			r = 0;
+		}
+		mutex_unlock(&kvm->lock);
+		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_RI %s",
+			 r ? "(not available)" : "(success)");
+		break;
 	case KVM_CAP_S390_USER_STSI:
 		VM_EVENT(kvm, 3, "%s", "ENABLE: CAP_S390_USER_STSI");
 		kvm->arch.user_stsi = 1;
@@ -1395,6 +1412,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 				    KVM_SYNC_CRS |
 				    KVM_SYNC_ARCH0 |
 				    KVM_SYNC_PFAULT;
+	if (test_kvm_facility(vcpu->kvm, 64))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
 	if (test_kvm_facility(vcpu->kvm, 129))
 		vcpu->run->kvm_valid_regs |= KVM_SYNC_VRS;
 
@@ -1578,10 +1597,13 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= 1;
 	if (sclp.has_sigpif)
 		vcpu->arch.sie_block->eca |= 0x10000000U;
+	if (test_kvm_facility(vcpu->kvm, 64))
+		vcpu->arch.sie_block->ecb3 |= 0x01;
 	if (test_kvm_facility(vcpu->kvm, 129)) {
 		vcpu->arch.sie_block->eca |= 0x00020000;
 		vcpu->arch.sie_block->ecd |= 0x20000000;
 	}
+	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 
 	if (vcpu->kvm->arch.use_cmma) {

commit c57ee5faf4503b0cd586c3af663262a3d5599fc1
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Tue Jan 5 18:20:24 2016 +0200

    kvm/s390: drop unpaired smp_mb
    
    smp_mb on vcpu destroy isn't paired with anything, violating pairing
    rules, and seems to be useless.
    
    Drop it.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Message-Id: <1452010811-25486-1-git-send-email-mst@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 940e9ff231a3..9f8eea3e2987 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1221,7 +1221,6 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kvm_clear_async_pf_completion_queue(vcpu);
 	if (!kvm_is_ucontrol(vcpu->kvm))
 		sca_del_vcpu(vcpu);
-	smp_mb();
 
 	if (kvm_is_ucontrol(vcpu->kvm))
 		gmap_free(vcpu->arch.gmap);

commit 32e6b236d26946eb076d1450bfb8f9978f15d6b9
Author: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
Date:   Mon Dec 1 17:24:42 2014 +0100

    KVM: s390: consider system MHA for guest storage
    
    Verify that the guest maximum storage address is below the MHA (maximum
    host address) value allowed on the host.
    
    Acked-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    [adopt to match recent limit,size changes]
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8aa5e55a8ecd..940e9ff231a3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1184,7 +1184,11 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		kvm->arch.gmap = NULL;
 		kvm->arch.mem_limit = KVM_S390_NO_MEM_LIMIT;
 	} else {
-		kvm->arch.mem_limit = TASK_MAX_SIZE;
+		if (sclp.hamax == U64_MAX)
+			kvm->arch.mem_limit = TASK_MAX_SIZE;
+		else
+			kvm->arch.mem_limit = min_t(unsigned long, TASK_MAX_SIZE,
+						    sclp.hamax + 1);
 		kvm->arch.gmap = gmap_alloc(current->mm, kvm->arch.mem_limit - 1);
 		if (!kvm->arch.gmap)
 			goto out_err;

commit a3a92c31bf0b57ad0ca7f092a6f3a57168ba9ae2
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Mon Dec 1 17:24:42 2014 +0100

    KVM: s390: fix mismatch between user and in-kernel guest limit
    
    While the userspace interface requests the maximum size the gmap code
    expects to get a maximum address.
    
    This error resulted in bigger page tables than necessary for some guest
    sizes, e.g. a 2GB guest used 3 levels instead of 2.
    
    At the same time we introduce KVM_S390_NO_MEM_LIMIT, which allows in a
    bright future that a guest spans the complete 64 bit address space.
    
    We also switch to TASK_MAX_SIZE for the initial memory size, this is a
    cosmetic change as the previous size also resulted in a 4 level pagetable
    creation.
    
    Reported-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c14845c3a6f8..8aa5e55a8ecd 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -378,8 +378,8 @@ static int kvm_s390_get_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 	case KVM_S390_VM_MEM_LIMIT_SIZE:
 		ret = 0;
 		VM_EVENT(kvm, 3, "QUERY: max guest memory: %lu bytes",
-			 kvm->arch.gmap->asce_end);
-		if (put_user(kvm->arch.gmap->asce_end, (u64 __user *)attr->addr))
+			 kvm->arch.mem_limit);
+		if (put_user(kvm->arch.mem_limit, (u64 __user *)attr->addr))
 			ret = -EFAULT;
 		break;
 	default:
@@ -431,9 +431,17 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		if (get_user(new_limit, (u64 __user *)attr->addr))
 			return -EFAULT;
 
-		if (new_limit > kvm->arch.gmap->asce_end)
+		if (kvm->arch.mem_limit != KVM_S390_NO_MEM_LIMIT &&
+		    new_limit > kvm->arch.mem_limit)
 			return -E2BIG;
 
+		if (!new_limit)
+			return -EINVAL;
+
+		/* gmap_alloc takes last usable address */
+		if (new_limit != KVM_S390_NO_MEM_LIMIT)
+			new_limit -= 1;
+
 		ret = -EBUSY;
 		mutex_lock(&kvm->lock);
 		if (atomic_read(&kvm->online_vcpus) == 0) {
@@ -450,7 +458,9 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 			}
 		}
 		mutex_unlock(&kvm->lock);
-		VM_EVENT(kvm, 3, "SET: max guest memory: %lu bytes", new_limit);
+		VM_EVENT(kvm, 3, "SET: max guest address: %lu", new_limit);
+		VM_EVENT(kvm, 3, "New guest asce: 0x%pK",
+			 (void *) kvm->arch.gmap->asce);
 		break;
 	}
 	default:
@@ -1172,8 +1182,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	if (type & KVM_VM_S390_UCONTROL) {
 		kvm->arch.gmap = NULL;
+		kvm->arch.mem_limit = KVM_S390_NO_MEM_LIMIT;
 	} else {
-		kvm->arch.gmap = gmap_alloc(current->mm, (1UL << 44) - 1);
+		kvm->arch.mem_limit = TASK_MAX_SIZE;
+		kvm->arch.gmap = gmap_alloc(current->mm, kvm->arch.mem_limit - 1);
 		if (!kvm->arch.gmap)
 			goto out_err;
 		kvm->arch.gmap->private = kvm;
@@ -2829,6 +2841,9 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	if (mem->memory_size & 0xffffful)
 		return -EINVAL;
 
+	if (mem->guest_phys_addr + mem->memory_size > kvm->arch.mem_limit)
+		return -EINVAL;
+
 	return 0;
 }
 

commit 8335713ad08caf2c3dfcb5bc2c93d7e0276142d4
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Dec 8 16:55:27 2015 +0100

    KVM: s390: obey kptr_restrict in traces
    
    The s390dbf and trace events provide a debugfs interface.
    If kptr_restrict is active, we should not expose kernel
    pointers. We can fence the debugfs output by using %pK
    instead of %p.
    
    Cc: Kees Cook <keescook@chromium.org>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6dec01d6c1c4..c14845c3a6f8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1185,7 +1185,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
-	KVM_EVENT(3, "vm 0x%p created by pid %u", kvm, current->pid);
+	KVM_EVENT(3, "vm 0x%pK created by pid %u", kvm, current->pid);
 
 	return 0;
 out_err:
@@ -1245,7 +1245,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 		gmap_free(kvm->arch.gmap);
 	kvm_s390_destroy_adapters(kvm);
 	kvm_s390_clear_float_irqs(kvm);
-	KVM_EVENT(3, "vm 0x%p destroyed", kvm);
+	KVM_EVENT(3, "vm 0x%pK destroyed", kvm);
 }
 
 /* Section: vcpu related */
@@ -1349,7 +1349,8 @@ static int sca_switch_to_extended(struct kvm *kvm)
 
 	free_page((unsigned long)old_sca);
 
-	VM_EVENT(kvm, 2, "Switched to ESCA (%p -> %p)", old_sca, kvm->arch.sca);
+	VM_EVENT(kvm, 2, "Switched to ESCA (0x%pK -> 0x%pK)",
+		 old_sca, kvm->arch.sca);
 	return 0;
 }
 
@@ -1624,7 +1625,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
 		goto out_free_sie_block;
-	VM_EVENT(kvm, 3, "create cpu %d at %p, sie block at %p", id, vcpu,
+	VM_EVENT(kvm, 3, "create cpu %d at 0x%pK, sie block at 0x%pK", id, vcpu,
 		 vcpu->arch.sie_block);
 	trace_kvm_s390_create_vcpu(id, vcpu, vcpu->arch.sie_block);
 

commit 7ec7c8c70b26de90f61be7523a6ad14df911219f
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Dec 2 14:27:03 2015 +0100

    KVM: s390: use assignment instead of memcpy
    
    Replace two memcpy with proper assignment.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6857262eb7ae..6dec01d6c1c4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2120,7 +2120,8 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 	 */
 	kvm_check_async_pf_completion(vcpu);
 
-	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->run->s.regs.gprs[14], 16);
+	vcpu->arch.sie_block->gg14 = vcpu->run->s.regs.gprs[14];
+	vcpu->arch.sie_block->gg15 = vcpu->run->s.regs.gprs[15];
 
 	if (need_resched())
 		schedule();
@@ -2185,7 +2186,8 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 	if (guestdbg_enabled(vcpu))
 		kvm_s390_restore_guest_per_regs(vcpu);
 
-	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
+	vcpu->run->s.regs.gprs[14] = vcpu->arch.sie_block->gg14;
+	vcpu->run->s.regs.gprs[15] = vcpu->arch.sie_block->gg15;
 
 	if (vcpu->arch.sie_block->icptcode > 0) {
 		int rc = kvm_handle_sie_intercept(vcpu);

commit 2f8a43d45d14ad62b105ed99151b453c12df7149
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Nov 27 11:00:54 2015 +0100

    KVM: s390: remove redudant assigment of error code
    
    rc already contains -ENOMEM, no need to assign it twice.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 77724ce0cb82..6857262eb7ae 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1618,10 +1618,8 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	 */
 	vcpu->arch.guest_fpregs.fprs = kzalloc(sizeof(freg_t) * __NUM_FPRS,
 					       GFP_KERNEL);
-	if (!vcpu->arch.guest_fpregs.fprs) {
-		rc = -ENOMEM;
+	if (!vcpu->arch.guest_fpregs.fprs)
 		goto out_free_sie_block;
-	}
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)

commit a6aacc3f87dfd44425fc17ea9875a5c2ad917227
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Nov 24 14:28:12 2015 +0100

    KVM: s390: remove pointless test_facility(2) check
    
    This evaluates always to 'true'.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 49d33190bd98..77724ce0cb82 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1027,7 +1027,7 @@ static int kvm_s390_apxa_installed(void)
 	u8 config[128];
 	int cc;
 
-	if (test_facility(2) && test_facility(12)) {
+	if (test_facility(12)) {
 		cc = kvm_s390_query_ap_config(config);
 
 		if (cc)

commit 07197fd05fa3eb2e8a5aae92938ca5d07bcac9fc
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Jan 30 16:01:38 2015 +0100

    KVM: s390: don't load kvm without virtualization support
    
    If we don't have support for virtualization (SIE), e.g. when running under
    a hypervisor not supporting execution of the SIE instruction, we should
    immediately abort loading the kvm module, as the SIE instruction cannot
    be enabled dynamically.
    
    Currently, the SIE instructions fails with an exception on a non-SIE
    host, resulting in the guest making no progress, instead of failing hard.
    
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 539d3857a59a..49d33190bd98 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2859,6 +2859,11 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 
 static int __init kvm_s390_init(void)
 {
+	if (!sclp.has_sief2) {
+		pr_info("SIE not available\n");
+		return -ENODEV;
+	}
+
 	return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
 }
 

commit 4215825eeb1f704d1bd327ca01fb753b53ea34d3
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Oct 12 12:57:22 2015 +0200

    KVM: s390: don't switch to ESCA for ucontrol
    
    sca_add_vpcu is not called for ucontrol guests. We must also not
    apply the sca checking for sca_can_add_vcpu as ucontrol guests
    do not have to follow the sca limits.
    
    As common code already checks that id < KVM_MAX_VCPUS all other
    data structures are safe as well.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d9d71bb91404..539d3857a59a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1588,7 +1588,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	struct sie_page *sie_page;
 	int rc = -EINVAL;
 
-	if (!sca_can_add_vcpu(kvm, id))
+	if (!kvm_is_ucontrol(kvm) && !sca_can_add_vcpu(kvm, id))
 		goto out;
 
 	rc = -ENOMEM;

commit eaa78f343255dabac963ab8c7644367844d18e00
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Oct 12 16:29:01 2015 +0200

    KVM: s390: cleanup sca_add_vcpu
    
    Now that we already have kvm and the VCPU id set for the VCPU, we can
    convert sda_add_vcpu to look much more like sda_del_vcpu.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7e0092bea6e9..d9d71bb91404 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1276,27 +1276,26 @@ static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 	read_unlock(&vcpu->kvm->arch.sca_lock);
 }
 
-static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
-			unsigned int id)
+static void sca_add_vcpu(struct kvm_vcpu *vcpu)
 {
-	read_lock(&kvm->arch.sca_lock);
-	if (kvm->arch.use_esca) {
-		struct esca_block *sca = kvm->arch.sca;
+	read_lock(&vcpu->kvm->arch.sca_lock);
+	if (vcpu->kvm->arch.use_esca) {
+		struct esca_block *sca = vcpu->kvm->arch.sca;
 
-		sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+		sca->cpu[vcpu->vcpu_id].sda = (__u64) vcpu->arch.sie_block;
 		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca & ~0x3fU;
 		vcpu->arch.sie_block->ecb2 |= 0x04U;
-		set_bit_inv(id, (unsigned long *) sca->mcn);
+		set_bit_inv(vcpu->vcpu_id, (unsigned long *) sca->mcn);
 	} else {
-		struct bsca_block *sca = kvm->arch.sca;
+		struct bsca_block *sca = vcpu->kvm->arch.sca;
 
-		sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+		sca->cpu[vcpu->vcpu_id].sda = (__u64) vcpu->arch.sie_block;
 		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
-		set_bit_inv(id, (unsigned long *) &sca->mcn);
+		set_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
 	}
-	read_unlock(&kvm->arch.sca_lock);
+	read_unlock(&vcpu->kvm->arch.sca_lock);
 }
 
 /* Basic SCA to Extended SCA data copy routines */
@@ -1492,7 +1491,7 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 	mutex_unlock(&vcpu->kvm->lock);
 	if (!kvm_is_ucontrol(vcpu->kvm)) {
 		vcpu->arch.gmap = vcpu->kvm->arch.gmap;
-		sca_add_vcpu(vcpu, vcpu->kvm, vcpu->vcpu_id);
+		sca_add_vcpu(vcpu);
 	}
 
 }

commit 10ce32d5b07470c5eb0fa821000c789073128b3f
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Oct 12 12:41:41 2015 +0200

    KVM: s390: always set/clear the SCA sda field
    
    Let's always set and clear the sda when enabling/disabling a VCPU.
    Dealing with sda being set to something else makes no sense anymore
    as we enable a VCPU in the SCA now after it has been registered at
    the VM.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2ba5978829f6..7e0092bea6e9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1266,14 +1266,12 @@ static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 		struct esca_block *sca = vcpu->kvm->arch.sca;
 
 		clear_bit_inv(vcpu->vcpu_id, (unsigned long *) sca->mcn);
-		if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
-			sca->cpu[vcpu->vcpu_id].sda = 0;
+		sca->cpu[vcpu->vcpu_id].sda = 0;
 	} else {
 		struct bsca_block *sca = vcpu->kvm->arch.sca;
 
 		clear_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
-		if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
-			sca->cpu[vcpu->vcpu_id].sda = 0;
+		sca->cpu[vcpu->vcpu_id].sda = 0;
 	}
 	read_unlock(&vcpu->kvm->arch.sca_lock);
 }
@@ -1285,8 +1283,7 @@ static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
 	if (kvm->arch.use_esca) {
 		struct esca_block *sca = kvm->arch.sca;
 
-		if (!sca->cpu[id].sda)
-			sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+		sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
 		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca & ~0x3fU;
 		vcpu->arch.sie_block->ecb2 |= 0x04U;
@@ -1294,8 +1291,7 @@ static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
 	} else {
 		struct bsca_block *sca = kvm->arch.sca;
 
-		if (!sca->cpu[id].sda)
-			sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+		sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
 		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
 		set_bit_inv(id, (unsigned long *) &sca->mcn);

commit 2550882449299fd55c8214529cc0777b789db0f7
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Oct 12 16:27:23 2015 +0200

    KVM: s390: fix SCA related races and double use
    
    If something goes wrong in kvm_arch_vcpu_create, the VCPU has already
    been added to the sca but will never be removed. Trying to create VCPUs
    with duplicate ids (e.g. after a failed attempt) is problematic.
    
    Also, when creating multiple VCPUs in parallel, we could theoretically
    forget to set the correct SCA when the switch to ESCA happens just
    before the VCPU is registered.
    
    Let's add the VCPU to the SCA in kvm_arch_vcpu_postcreate, where we can
    be sure that no duplicate VCPU with the same id is around and the VCPU
    has already been registered at the VM. We also have to make sure to update
    ECB at that point.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5c58127b7527..2ba5978829f6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1289,6 +1289,7 @@ static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
 			sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
 		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca & ~0x3fU;
+		vcpu->arch.sie_block->ecb2 |= 0x04U;
 		set_bit_inv(id, (unsigned long *) sca->mcn);
 	} else {
 		struct bsca_block *sca = kvm->arch.sca;
@@ -1493,8 +1494,11 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->epoch = vcpu->kvm->arch.epoch;
 	preempt_enable();
 	mutex_unlock(&vcpu->kvm->lock);
-	if (!kvm_is_ucontrol(vcpu->kvm))
+	if (!kvm_is_ucontrol(vcpu->kvm)) {
 		vcpu->arch.gmap = vcpu->kvm->arch.gmap;
+		sca_add_vcpu(vcpu, vcpu->kvm, vcpu->vcpu_id);
+	}
+
 }
 
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
@@ -1558,8 +1562,6 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->ecb |= 0x10;
 
 	vcpu->arch.sie_block->ecb2  = 8;
-	if (vcpu->kvm->arch.use_esca)
-		vcpu->arch.sie_block->ecb2 |= 4;
 	vcpu->arch.sie_block->eca   = 0xC1002000U;
 	if (sclp.has_siif)
 		vcpu->arch.sie_block->eca |= 1;
@@ -1608,9 +1610,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
 
 	vcpu->arch.sie_block->icpua = id;
-	if (!kvm_is_ucontrol(kvm))
-		sca_add_vcpu(vcpu, kvm, id);
-
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
 	vcpu->arch.local_int.wq = &vcpu->wq;

commit 5f3fe620a56f2f5c79e89522107f2476a45ed6ce
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Oct 12 12:34:19 2015 +0200

    KVM: s390: we always have a SCA
    
    Having no sca can never happen, even when something goes wrong when
    switching to ESCA. Otherwise we would have a serious bug.
    Let's remove this superfluous check.
    
    Acked-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 16c19fb0af8d..5c58127b7527 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1608,13 +1608,8 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
 
 	vcpu->arch.sie_block->icpua = id;
-	if (!kvm_is_ucontrol(kvm)) {
-		if (!kvm->arch.sca) {
-			WARN_ON_ONCE(1);
-			goto out_free_cpu;
-		}
+	if (!kvm_is_ucontrol(kvm))
 		sca_add_vcpu(vcpu, kvm, id);
-	}
 
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;

commit fe0edcb7311e80a349ad5f2b15f2d91a5606ed7f
Author: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
Date:   Wed Apr 22 18:37:40 2015 +0200

    KVM: s390: Enable up to 248 VCPUs per VM
    
    This patch allows s390 to have more than 64 VCPUs for a guest (up to
    248 for memory usage considerations), if supported by the underlaying
    hardware (sclp.has_esca).
    
    Signed-off-by: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5e884aa890cb..16c19fb0af8d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -246,7 +246,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	case KVM_CAP_NR_VCPUS:
 	case KVM_CAP_MAX_VCPUS:
-		r = KVM_MAX_VCPUS;
+		r = sclp.has_esca ? KVM_S390_ESCA_CPU_SLOTS
+				  : KVM_S390_BSCA_CPU_SLOTS;
 		break;
 	case KVM_CAP_NR_MEMSLOTS:
 		r = KVM_USER_MEM_SLOTS;

commit 5e0443152367ab9fef597a41a4e09a32df2bf887
Author: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
Date:   Wed Apr 22 18:08:39 2015 +0200

    KVM: s390: Introduce switching code
    
    This patch adds code that performs transparent switch to Extended
    SCA on addition of 65th VCPU in a VM. Disposal of ESCA is added too.
    The entier ESCA functionality, however, is still not enabled.
    The enablement will be provided in a separate patch.
    
    This patch also uses read/write lock protection of SCA and its subfields for
    possible disposal at the BSCA-to-ESCA transition. While only Basic SCA needs such
    a protection (for the swap), any SCA access is now guarded.
    
    Signed-off-by: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 41b3fed51591..5e884aa890cb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1080,7 +1080,7 @@ static int kvm_s390_crypto_init(struct kvm *kvm)
 static void sca_dispose(struct kvm *kvm)
 {
 	if (kvm->arch.use_esca)
-		BUG(); /* not implemented yet */
+		free_pages_exact(kvm->arch.sca, sizeof(struct esca_block));
 	else
 		free_page((unsigned long)(kvm->arch.sca));
 	kvm->arch.sca = NULL;
@@ -1110,6 +1110,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	rc = -ENOMEM;
 
 	kvm->arch.use_esca = 0; /* start with basic SCA */
+	rwlock_init(&kvm->arch.sca_lock);
 	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(GFP_KERNEL);
 	if (!kvm->arch.sca)
 		goto out_err;
@@ -1259,6 +1260,7 @@ static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
 
 static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 {
+	read_lock(&vcpu->kvm->arch.sca_lock);
 	if (vcpu->kvm->arch.use_esca) {
 		struct esca_block *sca = vcpu->kvm->arch.sca;
 
@@ -1272,11 +1274,13 @@ static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 		if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
 			sca->cpu[vcpu->vcpu_id].sda = 0;
 	}
+	read_unlock(&vcpu->kvm->arch.sca_lock);
 }
 
 static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
 			unsigned int id)
 {
+	read_lock(&kvm->arch.sca_lock);
 	if (kvm->arch.use_esca) {
 		struct esca_block *sca = kvm->arch.sca;
 
@@ -1294,11 +1298,78 @@ static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
 		set_bit_inv(id, (unsigned long *) &sca->mcn);
 	}
+	read_unlock(&kvm->arch.sca_lock);
+}
+
+/* Basic SCA to Extended SCA data copy routines */
+static inline void sca_copy_entry(struct esca_entry *d, struct bsca_entry *s)
+{
+	d->sda = s->sda;
+	d->sigp_ctrl.c = s->sigp_ctrl.c;
+	d->sigp_ctrl.scn = s->sigp_ctrl.scn;
+}
+
+static void sca_copy_b_to_e(struct esca_block *d, struct bsca_block *s)
+{
+	int i;
+
+	d->ipte_control = s->ipte_control;
+	d->mcn[0] = s->mcn;
+	for (i = 0; i < KVM_S390_BSCA_CPU_SLOTS; i++)
+		sca_copy_entry(&d->cpu[i], &s->cpu[i]);
+}
+
+static int sca_switch_to_extended(struct kvm *kvm)
+{
+	struct bsca_block *old_sca = kvm->arch.sca;
+	struct esca_block *new_sca;
+	struct kvm_vcpu *vcpu;
+	unsigned int vcpu_idx;
+	u32 scaol, scaoh;
+
+	new_sca = alloc_pages_exact(sizeof(*new_sca), GFP_KERNEL|__GFP_ZERO);
+	if (!new_sca)
+		return -ENOMEM;
+
+	scaoh = (u32)((u64)(new_sca) >> 32);
+	scaol = (u32)(u64)(new_sca) & ~0x3fU;
+
+	kvm_s390_vcpu_block_all(kvm);
+	write_lock(&kvm->arch.sca_lock);
+
+	sca_copy_b_to_e(new_sca, old_sca);
+
+	kvm_for_each_vcpu(vcpu_idx, vcpu, kvm) {
+		vcpu->arch.sie_block->scaoh = scaoh;
+		vcpu->arch.sie_block->scaol = scaol;
+		vcpu->arch.sie_block->ecb2 |= 0x04U;
+	}
+	kvm->arch.sca = new_sca;
+	kvm->arch.use_esca = 1;
+
+	write_unlock(&kvm->arch.sca_lock);
+	kvm_s390_vcpu_unblock_all(kvm);
+
+	free_page((unsigned long)old_sca);
+
+	VM_EVENT(kvm, 2, "Switched to ESCA (%p -> %p)", old_sca, kvm->arch.sca);
+	return 0;
 }
 
 static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
 {
-	return id < KVM_MAX_VCPUS;
+	int rc;
+
+	if (id < KVM_S390_BSCA_CPU_SLOTS)
+		return true;
+	if (!sclp.has_esca)
+		return false;
+
+	mutex_lock(&kvm->lock);
+	rc = kvm->arch.use_esca ? 0 : sca_switch_to_extended(kvm);
+	mutex_unlock(&kvm->lock);
+
+	return rc == 0 && id < KVM_S390_ESCA_CPU_SLOTS;
 }
 
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)

commit 7d43bafcff17c7fb07270999d3cf002f1ed6bd3f
Author: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
Date:   Wed Apr 22 17:09:44 2015 +0200

    KVM: s390: Make provisions for ESCA utilization
    
    This patch updates the routines (sca_*) to provide transparent access
    to and manipulation on the data for both Basic and Extended SCA in use.
    The kvm.arch.sca is generalized to (void *) to handle BSCA/ESCA cases.
    Also the kvm.arch.use_esca flag is provided.
    The actual functionality is kept the same.
    
    Signed-off-by: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c2683529b25c..41b3fed51591 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1077,6 +1077,15 @@ static int kvm_s390_crypto_init(struct kvm *kvm)
 	return 0;
 }
 
+static void sca_dispose(struct kvm *kvm)
+{
+	if (kvm->arch.use_esca)
+		BUG(); /* not implemented yet */
+	else
+		free_page((unsigned long)(kvm->arch.sca));
+	kvm->arch.sca = NULL;
+}
+
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int i, rc;
@@ -1100,6 +1109,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	rc = -ENOMEM;
 
+	kvm->arch.use_esca = 0; /* start with basic SCA */
 	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(GFP_KERNEL);
 	if (!kvm->arch.sca)
 		goto out_err;
@@ -1180,7 +1190,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kfree(kvm->arch.crypto.crycb);
 	free_page((unsigned long)kvm->arch.model.fac);
 	debug_unregister(kvm->arch.dbf);
-	free_page((unsigned long)(kvm->arch.sca));
+	sca_dispose(kvm);
 	KVM_EVENT(3, "creation of vm failed: %d", rc);
 	return rc;
 }
@@ -1226,7 +1236,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvm_free_vcpus(kvm);
 	free_page((unsigned long)kvm->arch.model.fac);
-	free_page((unsigned long)(kvm->arch.sca));
+	sca_dispose(kvm);
 	debug_unregister(kvm->arch.dbf);
 	kfree(kvm->arch.crypto.crycb);
 	if (!kvm_is_ucontrol(kvm))
@@ -1249,23 +1259,41 @@ static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
 
 static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 {
-	struct bsca_block *sca = vcpu->kvm->arch.sca;
+	if (vcpu->kvm->arch.use_esca) {
+		struct esca_block *sca = vcpu->kvm->arch.sca;
+
+		clear_bit_inv(vcpu->vcpu_id, (unsigned long *) sca->mcn);
+		if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
+			sca->cpu[vcpu->vcpu_id].sda = 0;
+	} else {
+		struct bsca_block *sca = vcpu->kvm->arch.sca;
 
-	clear_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
-	if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
-		sca->cpu[vcpu->vcpu_id].sda = 0;
+		clear_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
+		if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
+			sca->cpu[vcpu->vcpu_id].sda = 0;
+	}
 }
 
 static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
 			unsigned int id)
 {
-	struct bsca_block *sca = kvm->arch.sca;
+	if (kvm->arch.use_esca) {
+		struct esca_block *sca = kvm->arch.sca;
 
-	if (!sca->cpu[id].sda)
-		sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
-	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
-	vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
-	set_bit_inv(id, (unsigned long *) &sca->mcn);
+		if (!sca->cpu[id].sda)
+			sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
+		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca & ~0x3fU;
+		set_bit_inv(id, (unsigned long *) sca->mcn);
+	} else {
+		struct bsca_block *sca = kvm->arch.sca;
+
+		if (!sca->cpu[id].sda)
+			sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
+		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
+		set_bit_inv(id, (unsigned long *) &sca->mcn);
+	}
 }
 
 static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
@@ -1458,6 +1486,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->ecb |= 0x10;
 
 	vcpu->arch.sie_block->ecb2  = 8;
+	if (vcpu->kvm->arch.use_esca)
+		vcpu->arch.sie_block->ecb2 |= 4;
 	vcpu->arch.sie_block->eca   = 0xC1002000U;
 	if (sclp.has_siif)
 		vcpu->arch.sie_block->eca |= 1;

commit bc784ccee5eb9ae1e737927eb9d8a0fbf7601abc
Author: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
Date:   Thu Apr 23 16:09:06 2015 +0200

    KVM: s390: Introduce new structures
    
    This patch adds new structures and updates some existing ones to
    provide the base for Extended SCA functionality.
    
    The old sca_* structures were renamed to bsca_* to keep things uniform.
    
    The access to fields of SIGP controls were turned into bitfields instead
    of hardcoded bitmasks.
    
    Signed-off-by: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8ddd48848a83..c2683529b25c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1100,14 +1100,15 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	rc = -ENOMEM;
 
-	kvm->arch.sca = (struct sca_block *) get_zeroed_page(GFP_KERNEL);
+	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(GFP_KERNEL);
 	if (!kvm->arch.sca)
 		goto out_err;
 	spin_lock(&kvm_lock);
 	sca_offset += 16;
-	if (sca_offset + sizeof(struct sca_block) > PAGE_SIZE)
+	if (sca_offset + sizeof(struct bsca_block) > PAGE_SIZE)
 		sca_offset = 0;
-	kvm->arch.sca = (struct sca_block *) ((char *) kvm->arch.sca + sca_offset);
+	kvm->arch.sca = (struct bsca_block *)
+			((char *) kvm->arch.sca + sca_offset);
 	spin_unlock(&kvm_lock);
 
 	sprintf(debug_name, "kvm-%u", current->pid);
@@ -1190,9 +1191,8 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	trace_kvm_s390_destroy_vcpu(vcpu->vcpu_id);
 	kvm_s390_clear_local_irqs(vcpu);
 	kvm_clear_async_pf_completion_queue(vcpu);
-	if (!kvm_is_ucontrol(vcpu->kvm)) {
+	if (!kvm_is_ucontrol(vcpu->kvm))
 		sca_del_vcpu(vcpu);
-	}
 	smp_mb();
 
 	if (kvm_is_ucontrol(vcpu->kvm))
@@ -1249,7 +1249,7 @@ static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
 
 static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 {
-	struct sca_block *sca = vcpu->kvm->arch.sca;
+	struct bsca_block *sca = vcpu->kvm->arch.sca;
 
 	clear_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
 	if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
@@ -1259,7 +1259,7 @@ static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
 			unsigned int id)
 {
-	struct sca_block *sca = kvm->arch.sca;
+	struct bsca_block *sca = kvm->arch.sca;
 
 	if (!sca->cpu[id].sda)
 		sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;

commit a6e2f683e7691949d33ca9392e7807cfa9aca34e
Author: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
Date:   Tue Apr 21 15:31:59 2015 +0200

    KVM: s390: Provide SCA-aware helpers for VCPU add/del
    
    This patch provides SCA-aware helpers to create/delete a VCPU.
    This is to prepare for upcoming introduction of Extended SCA support.
    
    Signed-off-by: Eugene (jno) Dvurechenski <jno@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5c36c8e7e937..8ddd48848a83 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -283,6 +283,8 @@ static void kvm_s390_sync_dirty_log(struct kvm *kvm,
 }
 
 /* Section: vm related */
+static void sca_del_vcpu(struct kvm_vcpu *vcpu);
+
 /*
  * Get (and clear) the dirty memory log for a memory slot.
  */
@@ -1189,11 +1191,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kvm_s390_clear_local_irqs(vcpu);
 	kvm_clear_async_pf_completion_queue(vcpu);
 	if (!kvm_is_ucontrol(vcpu->kvm)) {
-		clear_bit(63 - vcpu->vcpu_id,
-			  (unsigned long *) &vcpu->kvm->arch.sca->mcn);
-		if (vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda ==
-		    (__u64) vcpu->arch.sie_block)
-			vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda = 0;
+		sca_del_vcpu(vcpu);
 	}
 	smp_mb();
 
@@ -1249,6 +1247,32 @@ static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+static void sca_del_vcpu(struct kvm_vcpu *vcpu)
+{
+	struct sca_block *sca = vcpu->kvm->arch.sca;
+
+	clear_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
+	if (sca->cpu[vcpu->vcpu_id].sda == (__u64) vcpu->arch.sie_block)
+		sca->cpu[vcpu->vcpu_id].sda = 0;
+}
+
+static void sca_add_vcpu(struct kvm_vcpu *vcpu, struct kvm *kvm,
+			unsigned int id)
+{
+	struct sca_block *sca = kvm->arch.sca;
+
+	if (!sca->cpu[id].sda)
+		sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
+	vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
+	set_bit_inv(id, (unsigned long *) &sca->mcn);
+}
+
+static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
+{
+	return id < KVM_MAX_VCPUS;
+}
+
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
@@ -1465,7 +1489,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	struct sie_page *sie_page;
 	int rc = -EINVAL;
 
-	if (id >= KVM_MAX_VCPUS)
+	if (!sca_can_add_vcpu(kvm, id))
 		goto out;
 
 	rc = -ENOMEM;
@@ -1487,13 +1511,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 			WARN_ON_ONCE(1);
 			goto out_free_cpu;
 		}
-		if (!kvm->arch.sca->cpu[id].sda)
-			kvm->arch.sca->cpu[id].sda =
-				(__u64) vcpu->arch.sie_block;
-		vcpu->arch.sie_block->scaoh =
-			(__u32)(((__u64)kvm->arch.sca) >> 32);
-		vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
-		set_bit(63 - id, (unsigned long *) &kvm->arch.sca->mcn);
+		sca_add_vcpu(vcpu, kvm, id);
 	}
 
 	spin_lock_init(&vcpu->arch.local_int.lock);

commit 71f116bfedfdd6763f2caf842bf40a6506759029
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Oct 19 16:24:28 2015 +0200

    KVM: s390: rewrite vcpu_post_run and drop out early
    
    Let's rewrite this function to better reflect how we actually handle
    exit_code. By dropping out early we can save a few cycles. This
    especially speeds up sie exits caused by host irqs.
    
    Also, let's move the special -EOPNOTSUPP for intercepts to
    the place where it belongs and convert it to -EREMOTE.
    
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 846589281b04..5c36c8e7e937 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2071,8 +2071,6 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 
 static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 {
-	int rc = -1;
-
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
 	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
@@ -2080,40 +2078,35 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 	if (guestdbg_enabled(vcpu))
 		kvm_s390_restore_guest_per_regs(vcpu);
 
-	if (exit_reason >= 0) {
-		rc = 0;
+	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
+
+	if (vcpu->arch.sie_block->icptcode > 0) {
+		int rc = kvm_handle_sie_intercept(vcpu);
+
+		if (rc != -EOPNOTSUPP)
+			return rc;
+		vcpu->run->exit_reason = KVM_EXIT_S390_SIEIC;
+		vcpu->run->s390_sieic.icptcode = vcpu->arch.sie_block->icptcode;
+		vcpu->run->s390_sieic.ipa = vcpu->arch.sie_block->ipa;
+		vcpu->run->s390_sieic.ipb = vcpu->arch.sie_block->ipb;
+		return -EREMOTE;
+	} else if (exit_reason != -EFAULT) {
+		vcpu->stat.exit_null++;
+		return 0;
 	} else if (kvm_is_ucontrol(vcpu->kvm)) {
 		vcpu->run->exit_reason = KVM_EXIT_S390_UCONTROL;
 		vcpu->run->s390_ucontrol.trans_exc_code =
 						current->thread.gmap_addr;
 		vcpu->run->s390_ucontrol.pgm_code = 0x10;
-		rc = -EREMOTE;
-
+		return -EREMOTE;
 	} else if (current->thread.gmap_pfault) {
 		trace_kvm_s390_major_guest_pfault(vcpu);
 		current->thread.gmap_pfault = 0;
-		if (kvm_arch_setup_async_pf(vcpu)) {
-			rc = 0;
-		} else {
-			gpa_t gpa = current->thread.gmap_addr;
-			rc = kvm_arch_fault_in_page(vcpu, gpa, 1);
-		}
-	}
-
-	if (rc == -1)
-		rc = vcpu_post_run_fault_in_sie(vcpu);
-
-	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
-
-	if (rc == 0) {
-		if (kvm_is_ucontrol(vcpu->kvm))
-			/* Don't exit for host interrupts. */
-			rc = vcpu->arch.sie_block->icptcode ? -EOPNOTSUPP : 0;
-		else
-			rc = kvm_handle_sie_intercept(vcpu);
+		if (kvm_arch_setup_async_pf(vcpu))
+			return 0;
+		return kvm_arch_fault_in_page(vcpu, current->thread.gmap_addr, 1);
 	}
-
-	return rc;
+	return vcpu_post_run_fault_in_sie(vcpu);
 }
 
 static int __vcpu_run(struct kvm_vcpu *vcpu)
@@ -2233,18 +2226,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = 0;
 	}
 
-	if (rc == -EOPNOTSUPP) {
-		/* intercept cannot be handled in-kernel, prepare kvm-run */
-		kvm_run->exit_reason         = KVM_EXIT_S390_SIEIC;
-		kvm_run->s390_sieic.icptcode = vcpu->arch.sie_block->icptcode;
-		kvm_run->s390_sieic.ipa      = vcpu->arch.sie_block->ipa;
-		kvm_run->s390_sieic.ipb      = vcpu->arch.sie_block->ipb;
-		rc = 0;
-	}
-
 	if (rc == -EREMOTE) {
-		/* intercept was handled, but userspace support is needed
-		 * kvm_run has been prepared by the handler */
+		/* userspace support is needed, kvm_run has been prepared */
 		rc = 0;
 	}
 

commit d9a3a09af54d01ab8b0c320580f4f95328d4a7ac
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Oct 23 09:02:32 2015 +0200

    s390/kvm: remove dependency on struct save_area definition
    
    Replace the offsets based on the struct area_area with the offset
    constants from asm-offsets.c based on the struct _lowcore.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 846589281b04..713a91a0622b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2270,37 +2270,37 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 	u64 clkcomp;
 	int rc;
 
+	px = kvm_s390_get_prefix(vcpu);
 	if (gpa == KVM_S390_STORE_STATUS_NOADDR) {
 		if (write_guest_abs(vcpu, 163, &archmode, 1))
 			return -EFAULT;
-		gpa = SAVE_AREA_BASE;
+		gpa = 0;
 	} else if (gpa == KVM_S390_STORE_STATUS_PREFIXED) {
 		if (write_guest_real(vcpu, 163, &archmode, 1))
 			return -EFAULT;
-		gpa = kvm_s390_real_to_abs(vcpu, SAVE_AREA_BASE);
-	}
-	rc = write_guest_abs(vcpu, gpa + offsetof(struct save_area, fp_regs),
+		gpa = px;
+	} else
+		gpa -= __LC_FPREGS_SAVE_AREA;
+	rc = write_guest_abs(vcpu, gpa + __LC_FPREGS_SAVE_AREA,
 			     vcpu->arch.guest_fpregs.fprs, 128);
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, gp_regs),
+	rc |= write_guest_abs(vcpu, gpa + __LC_GPREGS_SAVE_AREA,
 			      vcpu->run->s.regs.gprs, 128);
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, psw),
+	rc |= write_guest_abs(vcpu, gpa + __LC_PSW_SAVE_AREA,
 			      &vcpu->arch.sie_block->gpsw, 16);
-	px = kvm_s390_get_prefix(vcpu);
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, pref_reg),
+	rc |= write_guest_abs(vcpu, gpa + __LC_PREFIX_SAVE_AREA,
 			      &px, 4);
-	rc |= write_guest_abs(vcpu,
-			      gpa + offsetof(struct save_area, fp_ctrl_reg),
+	rc |= write_guest_abs(vcpu, gpa + __LC_FP_CREG_SAVE_AREA,
 			      &vcpu->arch.guest_fpregs.fpc, 4);
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, tod_reg),
+	rc |= write_guest_abs(vcpu, gpa + __LC_TOD_PROGREG_SAVE_AREA,
 			      &vcpu->arch.sie_block->todpr, 4);
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, timer),
+	rc |= write_guest_abs(vcpu, gpa + __LC_CPU_TIMER_SAVE_AREA,
 			      &vcpu->arch.sie_block->cputm, 8);
 	clkcomp = vcpu->arch.sie_block->ckc >> 8;
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, clk_cmp),
+	rc |= write_guest_abs(vcpu, gpa + __LC_CLOCK_COMP_SAVE_AREA,
 			      &clkcomp, 8);
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, acc_regs),
+	rc |= write_guest_abs(vcpu, gpa + __LC_AREGS_SAVE_AREA,
 			      &vcpu->run->s.regs.acrs, 64);
-	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, ctrl_regs),
+	rc |= write_guest_abs(vcpu, gpa + __LC_CREGS_SAVE_AREA,
 			      &vcpu->arch.sie_block->gcr, 128);
 	return rc ? -EFAULT : 0;
 }

commit 5967c17b118a2bd1dd1d554cc4eee16233e52bec
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Nov 6 12:08:48 2015 +0100

    KVM: s390: enable SIMD only when no VCPUs were created
    
    We should never allow to enable/disable any facilities for the guest
    when other VCPUs were already created.
    
    kvm_arch_vcpu_(load|put) relies on SIMD not changing during runtime.
    If somebody would create and run VCPUs and then decides to enable
    SIMD, undefined behaviour could be possible (e.g. vector save area
    not being set up).
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: stable@vger.kernel.org # 4.1+

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8fe2f1c722dc..846589281b04 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -342,12 +342,16 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		r = 0;
 		break;
 	case KVM_CAP_S390_VECTOR_REGISTERS:
-		if (MACHINE_HAS_VX) {
+		mutex_lock(&kvm->lock);
+		if (atomic_read(&kvm->online_vcpus)) {
+			r = -EBUSY;
+		} else if (MACHINE_HAS_VX) {
 			set_kvm_facility(kvm->arch.model.fac->mask, 129);
 			set_kvm_facility(kvm->arch.model.fac->list, 129);
 			r = 0;
 		} else
 			r = -EINVAL;
+		mutex_unlock(&kvm->lock);
 		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_VECTOR_REGISTERS %s",
 			 r ? "(not available)" : "(success)");
 		break;

commit 933425fb0010bd02bd459b41e63082756818ffce
Merge: a3e7531535a0 a3eaa8649e4c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 16:26:26 2015 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "First batch of KVM changes for 4.4.
    
      s390:
         A bunch of fixes and optimizations for interrupt and time handling.
    
      PPC:
         Mostly bug fixes.
    
      ARM:
         No big features, but many small fixes and prerequisites including:
    
          - a number of fixes for the arch-timer
    
          - introducing proper level-triggered semantics for the arch-timers
    
          - a series of patches to synchronously halt a guest (prerequisite
            for IRQ forwarding)
    
          - some tracepoint improvements
    
          - a tweak for the EL2 panic handlers
    
          - some more VGIC cleanups getting rid of redundant state
    
      x86:
         Quite a few changes:
    
          - support for VT-d posted interrupts (i.e. PCI devices can inject
            interrupts directly into vCPUs).  This introduces a new
            component (in virt/lib/) that connects VFIO and KVM together.
            The same infrastructure will be used for ARM interrupt
            forwarding as well.
    
          - more Hyper-V features, though the main one Hyper-V synthetic
            interrupt controller will have to wait for 4.5.  These will let
            KVM expose Hyper-V devices.
    
          - nested virtualization now supports VPID (same as PCID but for
            vCPUs) which makes it quite a bit faster
    
          - for future hardware that supports NVDIMM, there is support for
            clflushopt, clwb, pcommit
    
          - support for "split irqchip", i.e.  LAPIC in kernel +
            IOAPIC/PIC/PIT in userspace, which reduces the attack surface of
            the hypervisor
    
          - obligatory smattering of SMM fixes
    
          - on the guest side, stable scheduler clock support was rewritten
            to not require help from the hypervisor"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (123 commits)
      KVM: VMX: Fix commit which broke PML
      KVM: x86: obey KVM_X86_QUIRK_CD_NW_CLEARED in kvm_set_cr0()
      KVM: x86: allow RSM from 64-bit mode
      KVM: VMX: fix SMEP and SMAP without EPT
      KVM: x86: move kvm_set_irq_inatomic to legacy device assignment
      KVM: device assignment: remove pointless #ifdefs
      KVM: x86: merge kvm_arch_set_irq with kvm_set_msi_inatomic
      KVM: x86: zero apic_arb_prio on reset
      drivers/hv: share Hyper-V SynIC constants with userspace
      KVM: x86: handle SMBASE as physical address in RSM
      KVM: x86: add read_phys to x86_emulate_ops
      KVM: x86: removing unused variable
      KVM: don't pointlessly leave KVM_COMPAT=y in non-KVM configs
      KVM: arm/arm64: Merge vgic_set_lr() and vgic_sync_lr_elrsr()
      KVM: arm/arm64: Clean up vgic_retire_lr() and surroundings
      KVM: arm/arm64: Optimize away redundant LR tracking
      KVM: s390: use simple switch statement as multiplexer
      KVM: s390: drop useless newline in debugging data
      KVM: s390: SCA must not cross page boundaries
      KVM: arm: Do not indent the arguments of DECLARE_BITMAP
      ...

commit 58c383c62e1a4379cee531b56e4293211f2d5ded
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Oct 12 13:27:29 2015 +0200

    KVM: s390: drop useless newline in debugging data
    
    the s390 debug feature does not need newlines. In fact it will
    result in empty lines. Get rid of 4 leftovers.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 35596177fad2..07a6aa896c12 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -514,7 +514,7 @@ static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 
 	if (gtod_high != 0)
 		return -EINVAL;
-	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x\n", gtod_high);
+	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x", gtod_high);
 
 	return 0;
 }
@@ -527,7 +527,7 @@ static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 		return -EFAULT;
 
 	kvm_s390_set_tod_clock(kvm, gtod);
-	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx\n", gtod);
+	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx", gtod);
 	return 0;
 }
 
@@ -559,7 +559,7 @@ static int kvm_s390_get_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 	if (copy_to_user((void __user *)attr->addr, &gtod_high,
 					 sizeof(gtod_high)))
 		return -EFAULT;
-	VM_EVENT(kvm, 3, "QUERY: TOD extension: 0x%x\n", gtod_high);
+	VM_EVENT(kvm, 3, "QUERY: TOD extension: 0x%x", gtod_high);
 
 	return 0;
 }
@@ -571,7 +571,7 @@ static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 	gtod = kvm_s390_get_tod_clock_fast(kvm);
 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
 		return -EFAULT;
-	VM_EVENT(kvm, 3, "QUERY: TOD base: 0x%llx\n", gtod);
+	VM_EVENT(kvm, 3, "QUERY: TOD base: 0x%llx", gtod);
 
 	return 0;
 }

commit c5c2c393468576bad6d10b2b5fefff8cd25df3f4
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Oct 26 08:41:29 2015 +0100

    KVM: s390: SCA must not cross page boundaries
    
    We seemed to have missed a few corner cases in commit f6c137ff00a4
    ("KVM: s390: randomize sca address").
    
    The SCA has a maximum size of 2112 bytes. By setting the sca_offset to
    some unlucky numbers, we exceed the page.
    
    0x7c0 (1984) -> Fits exactly
    0x7d0 (2000) -> 16 bytes out
    0x7e0 (2016) -> 32 bytes out
    0x7f0 (2032) -> 48 bytes out
    
    One VCPU entry is 32 bytes long.
    
    For the last two cases, we actually write data to the other page.
    1. The address of the VCPU.
    2. Injection/delivery/clearing of SIGP externall calls via SIGP IF.
    
    Especially the 2. happens regularly. So this could produce two problems:
    1. The guest losing/getting external calls.
    2. Random memory overwrites in the host.
    
    So this problem happens on every 127 + 128 created VM with 64 VCPUs.
    
    Cc: stable@vger.kernel.org # v3.15+
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 618c85411a51..35596177fad2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1098,7 +1098,9 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.sca)
 		goto out_err;
 	spin_lock(&kvm_lock);
-	sca_offset = (sca_offset + 16) & 0x7f0;
+	sca_offset += 16;
+	if (sca_offset + sizeof(struct sca_block) > PAGE_SIZE)
+		sca_offset = 0;
 	kvm->arch.sca = (struct sca_block *) ((char *) kvm->arch.sca + sca_offset);
 	spin_unlock(&kvm_lock);
 

commit b5510d9b68c33964abd938148f407ad3789e369f
Author: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
Date:   Tue Sep 29 10:04:41 2015 +0200

    s390/fpu: always enable the vector facility if it is available
    
    If the kernel detects that the s390 hardware supports the vector
    facility, it is enabled by default at an early stage.  To force
    it off, use the novx kernel parameter.  Note that there is a small
    time window, where the vector facility is enabled before it is
    forced to be off.
    
    With enabling the vector facility by default, the FPU save and
    restore functions can be improved.  They do not longer require
    to manage expensive control register updates to enable or disable
    the vector enablement control for particular processes.
    
    Signed-off-by: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0a67c40eece9..c6b4063fce29 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1292,7 +1292,6 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 static inline void save_fpu_to(struct fpu *dst)
 {
 	dst->fpc = current->thread.fpu.fpc;
-	dst->flags = current->thread.fpu.flags;
 	dst->regs = current->thread.fpu.regs;
 }
 
@@ -1303,7 +1302,6 @@ static inline void save_fpu_to(struct fpu *dst)
 static inline void load_fpu_from(struct fpu *from)
 {
 	current->thread.fpu.fpc = from->fpc;
-	current->thread.fpu.flags = from->flags;
 	current->thread.fpu.regs = from->regs;
 }
 
@@ -1315,15 +1313,12 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 
 	if (test_kvm_facility(vcpu->kvm, 129)) {
 		current->thread.fpu.fpc = vcpu->run->s.regs.fpc;
-		current->thread.fpu.flags = FPU_USE_VX;
 		/*
 		 * Use the register save area in the SIE-control block
 		 * for register restore and save in kvm_arch_vcpu_put()
 		 */
 		current->thread.fpu.vxrs =
 			(__vector128 *)&vcpu->run->s.regs.vrs;
-		/* Always enable the vector extension for KVM */
-		__ctl_set_vx();
 	} else
 		load_fpu_from(&vcpu->arch.guest_fpregs);
 
@@ -2326,7 +2321,6 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 		 * registers and the FPC value and store them in the
 		 * guest_fpregs structure.
 		 */
-		WARN_ON(!is_vx_task(current));	  /* XXX remove later */
 		vcpu->arch.guest_fpregs.fpc = current->thread.fpu.fpc;
 		convert_vx_to_fp(vcpu->arch.guest_fpregs.fprs,
 				 current->thread.fpu.vxrs);

commit 60417fcc2b0235dfe3dcd589c56dbe3ea1a64c54
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Sep 29 16:20:36 2015 +0200

    KVM: s390: factor out reading of the guest TOD clock
    
    Let's factor this out and always use get_tod_clock_fast() when
    reading the guest TOD.
    
    STORE CLOCK FAST does not do serialization and, therefore, might
    result in some fuzziness between different processors in a way
    that subsequent calls on different CPUs might have time stamps that
    are earlier. This semantics is fine though for all KVM use cases.
    To make it obvious that the new function has STORE CLOCK FAST
    semantics we name it kvm_s390_get_tod_clock_fast.
    
    With this patch, we only have a handful of places were we
    have to care about STP sync (using preempt_disable() logic).
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 87bd602f326c..618c85411a51 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -568,9 +568,7 @@ static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	u64 gtod;
 
-	preempt_disable();
-	gtod = get_tod_clock() + kvm->arch.epoch;
-	preempt_enable();
+	gtod = kvm_s390_get_tod_clock_fast(kvm);
 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
 		return -EFAULT;
 	VM_EVENT(kvm, 3, "QUERY: TOD base: 0x%llx\n", gtod);

commit 25ed16759660cdfccd4a3cb7d30cce8a797b542a
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue May 12 09:49:14 2015 +0200

    KVM: s390: factor out and fix setting of guest TOD clock
    
    Let's move that whole logic into one function. We now always use unsigned
    values when calculating the epoch (to avoid over/underflow defined).
    Also, we always have to get all VCPUs out of SIE before doing the update
    to avoid running differing VCPUs with different TODs.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a0907795f31d..87bd602f326c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -521,22 +521,12 @@ static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 
 static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 {
-	struct kvm_vcpu *cur_vcpu;
-	unsigned int vcpu_idx;
 	u64 gtod;
 
 	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
 		return -EFAULT;
 
-	mutex_lock(&kvm->lock);
-	preempt_disable();
-	kvm->arch.epoch = gtod - get_tod_clock();
-	kvm_s390_vcpu_block_all(kvm);
-	kvm_for_each_vcpu(vcpu_idx, cur_vcpu, kvm)
-		cur_vcpu->arch.sie_block->epoch = kvm->arch.epoch;
-	kvm_s390_vcpu_unblock_all(kvm);
-	preempt_enable();
-	mutex_unlock(&kvm->lock);
+	kvm_s390_set_tod_clock(kvm, gtod);
 	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx\n", gtod);
 	return 0;
 }
@@ -1906,6 +1896,22 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+void kvm_s390_set_tod_clock(struct kvm *kvm, u64 tod)
+{
+	struct kvm_vcpu *vcpu;
+	int i;
+
+	mutex_lock(&kvm->lock);
+	preempt_disable();
+	kvm->arch.epoch = tod - get_tod_clock();
+	kvm_s390_vcpu_block_all(kvm);
+	kvm_for_each_vcpu(i, vcpu, kvm)
+		vcpu->arch.sie_block->epoch = kvm->arch.epoch;
+	kvm_s390_vcpu_unblock_all(kvm);
+	preempt_enable();
+	mutex_unlock(&kvm->lock);
+}
+
 /**
  * kvm_arch_fault_in_page - fault-in guest page if necessary
  * @vcpu: The corresponding virtual cpu

commit 5a3d883a59b3fe8dc8775c7a79200a5b11a6761e
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Sep 29 16:27:24 2015 +0200

    KVM: s390: switch to get_tod_clock() and fix STP sync races
    
    Nobody except early.c makes use of store_tod_clock() to handle the
    cc. So if we would get a cc != 0, we would be in more trouble.
    
    Let's replace all users with get_tod_clock(). Returning a cc
    on an ioctl sounded strange either way.
    
    We can now also easily move the get_tod_clock() call into the
    preempt_disable() section. This is in fact necessary to make the
    STP sync work as expected. Otherwise the host TOD could change
    and we would end up with a wrong epoch calculation.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0a67c40eece9..a0907795f31d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -523,19 +523,14 @@ static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	struct kvm_vcpu *cur_vcpu;
 	unsigned int vcpu_idx;
-	u64 host_tod, gtod;
-	int r;
+	u64 gtod;
 
 	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
 		return -EFAULT;
 
-	r = store_tod_clock(&host_tod);
-	if (r)
-		return r;
-
 	mutex_lock(&kvm->lock);
 	preempt_disable();
-	kvm->arch.epoch = gtod - host_tod;
+	kvm->arch.epoch = gtod - get_tod_clock();
 	kvm_s390_vcpu_block_all(kvm);
 	kvm_for_each_vcpu(vcpu_idx, cur_vcpu, kvm)
 		cur_vcpu->arch.sie_block->epoch = kvm->arch.epoch;
@@ -581,15 +576,10 @@ static int kvm_s390_get_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 
 static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 {
-	u64 host_tod, gtod;
-	int r;
-
-	r = store_tod_clock(&host_tod);
-	if (r)
-		return r;
+	u64 gtod;
 
 	preempt_disable();
-	gtod = host_tod + kvm->arch.epoch;
+	gtod = get_tod_clock() + kvm->arch.epoch;
 	preempt_enable();
 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
 		return -EFAULT;

commit 9bf9fde2c98ba8362ea1d41d8bd8b32a23776e67
Author: Jason J. Herne <jjherne@linux.vnet.ibm.com>
Date:   Wed Sep 16 09:13:50 2015 -0400

    KVM: s390: Replace incorrect atomic_or with atomic_andnot
    
    The offending commit accidentally replaces an atomic_clear with an
    atomic_or instead of an atomic_andnot in kvm_s390_vcpu_request_handled.
    The symptom is that kvm guests on s390 hang on startup.
    This patch simply replaces the incorrect atomic_or with atomic_andnot
    
    Fixes: 805de8f43c20 (atomic: Replace atomic_{set,clear}_mask() usage)
    Signed-off-by: Jason J. Herne <jjherne@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2f807ab1725e..0a67c40eece9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1575,7 +1575,7 @@ static void kvm_s390_vcpu_request(struct kvm_vcpu *vcpu)
 
 static void kvm_s390_vcpu_request_handled(struct kvm_vcpu *vcpu)
 {
-	atomic_or(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
+	atomic_andnot(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
 }
 
 /*

commit 62bea5bff486644ecf363fe8a1a2f6f32c614a49
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 15 18:27:57 2015 +0200

    KVM: add halt_attempted_poll to VCPU stats
    
    This new statistic can help diagnosing VCPUs that, for any reason,
    trigger bad behavior of halt_poll_ns autotuning.
    
    For example, say halt_poll_ns = 480000, and wakeups are spaced exactly
    like 479us, 481us, 479us, 481us. Then KVM always fails polling and wastes
    10+20+40+80+160+320+480 = 1110 microseconds out of every
    479+481+479+481+479+481+479 = 3359 microseconds. The VCPU then
    is consuming about 30% more CPU than it would use without
    polling.  This would show as an abnormally high number of
    attempted polling compared to the successful polls.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com<
    Reviewed-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c91eb941b444..2f807ab1725e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -63,6 +63,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
 	{ "halt_successful_poll", VCPU_STAT(halt_successful_poll) },
+	{ "halt_attempted_poll", VCPU_STAT(halt_attempted_poll) },
 	{ "halt_wakeup", VCPU_STAT(halt_wakeup) },
 	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },

commit ca520cab25e0e8da717c596ccaa2c2b3650cfa09
Merge: 4c12ab7e5e2e d420acd816c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 3 15:46:07 2015 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking and atomic updates from Ingo Molnar:
     "Main changes in this cycle are:
    
       - Extend atomic primitives with coherent logic op primitives
         (atomic_{or,and,xor}()) and deprecate the old partial APIs
         (atomic_{set,clear}_mask())
    
         The old ops were incoherent with incompatible signatures across
         architectures and with incomplete support.  Now every architecture
         supports the primitives consistently (by Peter Zijlstra)
    
       - Generic support for 'relaxed atomics':
    
           - _acquire/release/relaxed() flavours of xchg(), cmpxchg() and {add,sub}_return()
           - atomic_read_acquire()
           - atomic_set_release()
    
         This came out of porting qwrlock code to arm64 (by Will Deacon)
    
       - Clean up the fragile static_key APIs that were causing repeat bugs,
         by introducing a new one:
    
           DEFINE_STATIC_KEY_TRUE(name);
           DEFINE_STATIC_KEY_FALSE(name);
    
         which define a key of different types with an initial true/false
         value.
    
         Then allow:
    
           static_branch_likely()
           static_branch_unlikely()
    
         to take a key of either type and emit the right instruction for the
         case.  To be able to know the 'type' of the static key we encode it
         in the jump entry (by Peter Zijlstra)
    
       - Static key self-tests (by Jason Baron)
    
       - qrwlock optimizations (by Waiman Long)
    
       - small futex enhancements (by Davidlohr Bueso)
    
       - ... and misc other changes"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (63 commits)
      jump_label/x86: Work around asm build bug on older/backported GCCs
      locking, ARM, atomics: Define our SMP atomics in terms of _relaxed() operations
      locking, include/llist: Use linux/atomic.h instead of asm/cmpxchg.h
      locking/qrwlock: Make use of _{acquire|release|relaxed}() atomics
      locking/qrwlock: Implement queue_write_unlock() using smp_store_release()
      locking/lockref: Remove homebrew cmpxchg64_relaxed() macro definition
      locking, asm-generic: Add _{relaxed|acquire|release}() variants for 'atomic_long_t'
      locking, asm-generic: Rework atomic-long.h to avoid bulk code duplication
      locking/atomics: Add _{acquire|release|relaxed}() variants of some atomic operations
      locking, compiler.h: Cast away attributes in the WRITE_ONCE() magic
      locking/static_keys: Make verify_keys() static
      jump label, locking/static_keys: Update docs
      locking/static_keys: Provide a selftest
      jump_label: Provide a self-test
      s390/uaccess, locking/static_keys: employ static_branch_likely()
      x86, tsc, locking/static_keys: Employ static_branch_likely()
      locking/static_keys: Add selftest
      locking/static_keys: Add a new static_key interface
      locking/static_keys: Rework update logic
      locking/static_keys: Add static_key_{en,dis}able() helpers
      ...

commit 9c6a019c6edf8591e34ae9da51bac7684131d905
Merge: 7c01919130ef e4ec73510812
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 31 15:11:53 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "The big one is support for fake NUMA, splitting a really large machine
      in more manageable piece improves performance in some cases, e.g. for
      a KVM host.
    
      The FICON Link Incident handling has been improved, this helps the
      operator to identify degraded or non-operational FICON connections.
    
      The save and restore of floating point and vector registers has been
      overhauled to allow the future use of vector registers in the kernel.
    
      A few small enhancement, magic sys-requests for the vt220 console via
      SCLP, some more assembler code has been converted to C, the PCI error
      handling is improved.
    
      And the usual cleanup and bug fixing"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (59 commits)
      s390/jump_label: Use %*ph to print small buffers
      s390/sclp_vt220: support magic sysrequests
      s390/ctrlchar: improve handling of magic sysrequests
      s390/numa: remove superfluous ARCH_WANT defines
      s390/3270: redraw screen on unsolicited device end
      s390/dcssblk: correct out of bounds array indexes
      s390/mm: simplify page table alloc/free code
      s390/pci: move debug messages to debugfs
      s390/nmi: initialize control register 0 earlier
      s390/zcrypt: use msleep() instead of mdelay()
      s390/hmcdrv: fix interrupt registration
      s390/setup: fix novx parameter
      s390/uaccess: remove uaccess_primary kernel parameter
      s390: remove unneeded sizeof(void *) comparisons
      s390/facilities: remove transactional-execution bits
      s390/numa: re-add DIE sched_domain_topology_level
      s390/dasd: enhance CUIR scope detection
      s390/dasd: fix failing path verification
      s390/vdso: emit a GNU hash
      s390/numa: make core to node mapping data dynamic
      ...

commit 44e98edcd11a48619b342d8f442d447b094ab2fc
Merge: 64291f7db5bd 4d283ec908e6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 31 08:27:44 2015 -0700

    Merge tag 'kvm-4.3-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "A very small release for x86 and s390 KVM.
    
       - s390: timekeeping changes, cleanups and fixes
    
       - x86: support for Hyper-V MSRs to report crashes, and a bunch of
         cleanups.
    
      One interesting feature that was planned for 4.3 (emulating the local
      APIC in kernel while keeping the IOAPIC and 8254 in userspace) had to
      be delayed because Intel complained about my reading of the manual"
    
    * tag 'kvm-4.3-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (42 commits)
      x86/kvm: Rename VMX's segment access rights defines
      KVM: x86/vPMU: Fix unnecessary signed extension for AMD PERFCTRn
      kvm: x86: Fix error handling in the function kvm_lapic_sync_from_vapic
      KVM: s390: Fix assumption that kvm_set_irq_routing is always run successfully
      KVM: VMX: drop ept misconfig check
      KVM: MMU: fully check zero bits for sptes
      KVM: MMU: introduce is_shadow_zero_bits_set()
      KVM: MMU: introduce the framework to check zero bits on sptes
      KVM: MMU: split reset_rsvds_bits_mask_ept
      KVM: MMU: split reset_rsvds_bits_mask
      KVM: MMU: introduce rsvd_bits_validate
      KVM: MMU: move FNAME(is_rsvd_bits_set) to mmu.c
      KVM: MMU: fix validation of mmio page fault
      KVM: MTRR: Use default type for non-MTRR-covered gfn before WARN_ON
      KVM: s390: host STP toleration for VMs
      KVM: x86: clean/fix memory barriers in irqchip_in_kernel
      KVM: document memory barriers for kvm->vcpus/kvm->online_vcpus
      KVM: x86: remove unnecessary memory barriers for shared MSRs
      KVM: move code related to KVM_SET_BOOT_CPU_ID to x86
      KVM: s390: log capability enablement and vm attribute changes
      ...

commit 152b28392a8d9dd08e789b48b602eb75eef436fa
Author: Nicholas Krause <xerofoify@gmail.com>
Date:   Thu Aug 6 13:05:54 2015 -0400

    KVM: s390: Fix assumption that kvm_set_irq_routing is always run successfully
    
    This fixes the assumption that kvm_set_irq_routing is always run
    successfully by instead making it equal to the variable r which
    we use for returning in the function kvm_arch_vm_ioctl instead
    of making r equal to zero when calling this particular function
    and incorrectly making the caller of kvm_arch_vm_ioctl think
    the function has run successfully.
    
    Signed-off-by: Nicholas Krause <xerofoify@gmail.com>
    Message-Id: <1438880754-27149-1-git-send-email-xerofoify@gmail.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4bdb860b5c49..397b88dc1b2c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -962,8 +962,7 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		if (kvm->arch.use_irqchip) {
 			/* Set up dummy routing. */
 			memset(&routing, 0, sizeof(routing));
-			kvm_set_irq_routing(kvm, &routing, 0, 0);
-			r = 0;
+			r = kvm_set_irq_routing(kvm, &routing, 0, 0);
 		}
 		break;
 	}

commit fdf036507f1fc036d5a06753e9e8b13f46de73e8
Author: Fan Zhang <zhangfan@linux.vnet.ibm.com>
Date:   Wed May 13 10:58:41 2015 +0200

    KVM: s390: host STP toleration for VMs
    
    If the host has STP enabled, the TOD of the host will be changed during
    synchronization phases. These are performed during a stop_machine() call.
    
    As the guest TOD is based on the host TOD, we have to make sure that:
    - no VCPU is in the SIE (implicitly guaranteed via stop_machine())
    - manual guest TOD calculations are not affected
    
    "Epoch" is the guest TOD clock delta to the host TOD clock. We have to
    adjust that value during the STP synchronization and make sure that code
    that accesses the epoch won't get interrupted in between (via disabling
    preemption).
    
    Signed-off-by: Fan Zhang <zhangfan@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 924b1ae86caf..4bdb860b5c49 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -28,6 +28,7 @@
 #include <linux/vmalloc.h>
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
+#include <asm/etr.h>
 #include <asm/pgtable.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>
@@ -138,16 +139,47 @@ int kvm_arch_hardware_enable(void)
 
 static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address);
 
+/*
+ * This callback is executed during stop_machine(). All CPUs are therefore
+ * temporarily stopped. In order not to change guest behavior, we have to
+ * disable preemption whenever we touch the epoch of kvm and the VCPUs,
+ * so a CPU won't be stopped while calculating with the epoch.
+ */
+static int kvm_clock_sync(struct notifier_block *notifier, unsigned long val,
+			  void *v)
+{
+	struct kvm *kvm;
+	struct kvm_vcpu *vcpu;
+	int i;
+	unsigned long long *delta = v;
+
+	list_for_each_entry(kvm, &vm_list, vm_list) {
+		kvm->arch.epoch -= *delta;
+		kvm_for_each_vcpu(i, vcpu, kvm) {
+			vcpu->arch.sie_block->epoch -= *delta;
+		}
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block kvm_clock_notifier = {
+	.notifier_call = kvm_clock_sync,
+};
+
 int kvm_arch_hardware_setup(void)
 {
 	gmap_notifier.notifier_call = kvm_gmap_notifier;
 	gmap_register_ipte_notifier(&gmap_notifier);
+	atomic_notifier_chain_register(&s390_epoch_delta_notifier,
+				       &kvm_clock_notifier);
 	return 0;
 }
 
 void kvm_arch_hardware_unsetup(void)
 {
 	gmap_unregister_ipte_notifier(&gmap_notifier);
+	atomic_notifier_chain_unregister(&s390_epoch_delta_notifier,
+					 &kvm_clock_notifier);
 }
 
 int kvm_arch_init(void *opaque)
@@ -501,11 +533,13 @@ static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 		return r;
 
 	mutex_lock(&kvm->lock);
+	preempt_disable();
 	kvm->arch.epoch = gtod - host_tod;
 	kvm_s390_vcpu_block_all(kvm);
 	kvm_for_each_vcpu(vcpu_idx, cur_vcpu, kvm)
 		cur_vcpu->arch.sie_block->epoch = kvm->arch.epoch;
 	kvm_s390_vcpu_unblock_all(kvm);
+	preempt_enable();
 	mutex_unlock(&kvm->lock);
 	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx\n", gtod);
 	return 0;
@@ -553,7 +587,9 @@ static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 	if (r)
 		return r;
 
+	preempt_disable();
 	gtod = host_tod + kvm->arch.epoch;
+	preempt_enable();
 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
 		return -EFAULT;
 	VM_EVENT(kvm, 3, "QUERY: TOD base: 0x%llx\n", gtod);
@@ -1314,7 +1350,9 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 	mutex_lock(&vcpu->kvm->lock);
+	preempt_disable();
 	vcpu->arch.sie_block->epoch = vcpu->kvm->arch.epoch;
+	preempt_enable();
 	mutex_unlock(&vcpu->kvm->lock);
 	if (!kvm_is_ucontrol(vcpu->kvm))
 		vcpu->arch.gmap = vcpu->kvm->arch.gmap;

commit d0164ee20d98847d3c777a0ae90e678e7ac1e416
Author: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
Date:   Mon Jun 29 16:43:06 2015 +0200

    s390/kernel: remove save_fpu_regs() parameter and use __LC_CURRENT instead
    
    All calls to save_fpu_regs() specify the fpu structure of the current task
    pointer as parameter.  The task pointer of the current task can also be
    retrieved from the CPU lowcore directly.  Remove the parameter definition,
    load the __LC_CURRENT task pointer from the CPU lowcore, and rebase the FPU
    structure onto the task structure.  Apply the same approach for the
    load_fpu_regs() function.
    
    Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c0cceaf4a92e..1903f0212bd0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1224,7 +1224,7 @@ static inline void load_fpu_from(struct fpu *from)
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Save host register state */
-	save_fpu_regs(&current->thread.fpu);
+	save_fpu_regs();
 	save_fpu_to(&vcpu->arch.host_fpregs);
 
 	if (test_kvm_facility(vcpu->kvm, 129)) {
@@ -1256,7 +1256,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
 
-	save_fpu_regs(&current->thread.fpu);
+	save_fpu_regs();
 
 	if (test_kvm_facility(vcpu->kvm, 129))
 		/*
@@ -1671,7 +1671,7 @@ int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 		return -EINVAL;
 	memcpy(vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
 	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
-	save_fpu_regs(&current->thread.fpu);
+	save_fpu_regs();
 	load_fpu_from(&vcpu->arch.guest_fpregs);
 	return 0;
 }
@@ -2241,7 +2241,7 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	 * copying in vcpu load/put. Lets update our copies before we save
 	 * it into the save area
 	 */
-	save_fpu_regs(&current->thread.fpu);
+	save_fpu_regs();
 	if (test_kvm_facility(vcpu->kvm, 129)) {
 		/*
 		 * If the vector extension is available, the vector registers
@@ -2288,7 +2288,7 @@ int kvm_s390_vcpu_store_adtl_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	 *
 	 * Let's update our copies before we save it into the save area.
 	 */
-	save_fpu_regs(&current->thread.fpu);
+	save_fpu_regs();
 
 	return kvm_s390_store_adtl_status_unloaded(vcpu, addr);
 }

commit 586b7ccdb7143b6a9b975d2c6ad52b6ca5c162b9
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Jul 28 15:03:05 2015 +0200

    KVM: s390: Fix hang VCPU hang/loop regression
    
    commit 785dbef407d8 ("KVM: s390: optimize round trip time in request
    handling") introduced a regression. This regression was seen with
    CPU hotplug in the guest and switching between 1 or 2 CPUs. This will
    set/reset the IBS control via synced request.
    
    Whenever we make a synced request, we first set the vcpu->requests
    bit and then block the vcpu. The handler, on the other hand, unblocks
    itself, processes vcpu->requests (by clearing them) and unblocks itself
    once again.
    
    Now, if the requester sleeps between setting of vcpu->requests and
    blocking, the handler will clear the vcpu->requests bit and try to
    unblock itself (although no bit is set). When the requester wakes up,
    it blocks the VCPU and we have a blocked VCPU without requests.
    
    Solution is to always unset the block bit.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Fixes: 785dbef407d8 ("KVM: s390: optimize round trip time in request handling")

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2078f92d15ac..f32f843a3631 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1742,10 +1742,10 @@ static bool ibs_enabled(struct kvm_vcpu *vcpu)
 
 static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 {
-	if (!vcpu->requests)
-		return 0;
 retry:
 	kvm_s390_vcpu_request_handled(vcpu);
+	if (!vcpu->requests)
+		return 0;
 	/*
 	 * We use MMU_RELOAD just to re-arm the ipte notifier for the
 	 * guest prefix page. gmap_ipte_notify will wait on the ptl lock.

commit c92ea7b9f7d256cabf7ee08a7627a5227e356dec
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jul 22 15:52:10 2015 +0200

    KVM: s390: log capability enablement and vm attribute changes
    
    Depending on user space, some capabilities and vm attributes are
    enabled at runtime. Let's log those events and while we're at it,
    log querying the vm attributes as well.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 762103653a29..924b1ae86caf 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -299,10 +299,12 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 
 	switch (cap->cap) {
 	case KVM_CAP_S390_IRQCHIP:
+		VM_EVENT(kvm, 3, "%s", "ENABLE: CAP_S390_IRQCHIP");
 		kvm->arch.use_irqchip = 1;
 		r = 0;
 		break;
 	case KVM_CAP_S390_USER_SIGP:
+		VM_EVENT(kvm, 3, "%s", "ENABLE: CAP_S390_USER_SIGP");
 		kvm->arch.user_sigp = 1;
 		r = 0;
 		break;
@@ -313,8 +315,11 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 			r = 0;
 		} else
 			r = -EINVAL;
+		VM_EVENT(kvm, 3, "ENABLE: CAP_S390_VECTOR_REGISTERS %s",
+			 r ? "(not available)" : "(success)");
 		break;
 	case KVM_CAP_S390_USER_STSI:
+		VM_EVENT(kvm, 3, "%s", "ENABLE: CAP_S390_USER_STSI");
 		kvm->arch.user_stsi = 1;
 		r = 0;
 		break;
@@ -332,6 +337,8 @@ static int kvm_s390_get_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 	switch (attr->attr) {
 	case KVM_S390_VM_MEM_LIMIT_SIZE:
 		ret = 0;
+		VM_EVENT(kvm, 3, "QUERY: max guest memory: %lu bytes",
+			 kvm->arch.gmap->asce_end);
 		if (put_user(kvm->arch.gmap->asce_end, (u64 __user *)attr->addr))
 			ret = -EFAULT;
 		break;
@@ -354,6 +361,7 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 			break;
 
 		ret = -EBUSY;
+		VM_EVENT(kvm, 3, "%s", "ENABLE: CMMA support");
 		mutex_lock(&kvm->lock);
 		if (atomic_read(&kvm->online_vcpus) == 0) {
 			kvm->arch.use_cmma = 1;
@@ -366,6 +374,7 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		if (!kvm->arch.use_cmma)
 			break;
 
+		VM_EVENT(kvm, 3, "%s", "RESET: CMMA states");
 		mutex_lock(&kvm->lock);
 		idx = srcu_read_lock(&kvm->srcu);
 		s390_reset_cmma(kvm->arch.gmap->mm);
@@ -401,6 +410,7 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 			}
 		}
 		mutex_unlock(&kvm->lock);
+		VM_EVENT(kvm, 3, "SET: max guest memory: %lu bytes", new_limit);
 		break;
 	}
 	default:
@@ -427,22 +437,26 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 			kvm->arch.crypto.crycb->aes_wrapping_key_mask,
 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 		kvm->arch.crypto.aes_kw = 1;
+		VM_EVENT(kvm, 3, "%s", "ENABLE: AES keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
 		get_random_bytes(
 			kvm->arch.crypto.crycb->dea_wrapping_key_mask,
 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 		kvm->arch.crypto.dea_kw = 1;
+		VM_EVENT(kvm, 3, "%s", "ENABLE: DEA keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
 		kvm->arch.crypto.aes_kw = 0;
 		memset(kvm->arch.crypto.crycb->aes_wrapping_key_mask, 0,
 			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
+		VM_EVENT(kvm, 3, "%s", "DISABLE: AES keywrapping support");
 		break;
 	case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
 		kvm->arch.crypto.dea_kw = 0;
 		memset(kvm->arch.crypto.crycb->dea_wrapping_key_mask, 0,
 			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
+		VM_EVENT(kvm, 3, "%s", "DISABLE: DEA keywrapping support");
 		break;
 	default:
 		mutex_unlock(&kvm->lock);
@@ -467,6 +481,7 @@ static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 
 	if (gtod_high != 0)
 		return -EINVAL;
+	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x\n", gtod_high);
 
 	return 0;
 }
@@ -492,6 +507,7 @@ static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 		cur_vcpu->arch.sie_block->epoch = kvm->arch.epoch;
 	kvm_s390_vcpu_unblock_all(kvm);
 	mutex_unlock(&kvm->lock);
+	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx\n", gtod);
 	return 0;
 }
 
@@ -523,6 +539,7 @@ static int kvm_s390_get_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 	if (copy_to_user((void __user *)attr->addr, &gtod_high,
 					 sizeof(gtod_high)))
 		return -EFAULT;
+	VM_EVENT(kvm, 3, "QUERY: TOD extension: 0x%x\n", gtod_high);
 
 	return 0;
 }
@@ -539,6 +556,7 @@ static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 	gtod = host_tod + kvm->arch.epoch;
 	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
 		return -EFAULT;
+	VM_EVENT(kvm, 3, "QUERY: TOD base: 0x%llx\n", gtod);
 
 	return 0;
 }
@@ -2360,6 +2378,7 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	case KVM_CAP_S390_CSS_SUPPORT:
 		if (!vcpu->kvm->arch.css_support) {
 			vcpu->kvm->arch.css_support = 1;
+			VM_EVENT(vcpu->kvm, 3, "%s", "ENABLE: CSS support");
 			trace_kvm_s390_enable_css(vcpu->kvm);
 		}
 		r = 0;

commit 78f2613168eca83a218272aa12b680a365ee58d6
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jul 22 15:50:58 2015 +0200

    KVM: s390: Provide global debug log
    
    In addition to the per VM debug logs, let's provide a global
    one for KVM-wide events, like new guests or fatal errors.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4c1b5cbea1a6..762103653a29 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -127,6 +127,7 @@ unsigned long kvm_s390_fac_list_mask_size(void)
 }
 
 static struct gmap_notifier gmap_notifier;
+debug_info_t *kvm_s390_dbf;
 
 /* Section: not file related */
 int kvm_arch_hardware_enable(void)
@@ -151,10 +152,24 @@ void kvm_arch_hardware_unsetup(void)
 
 int kvm_arch_init(void *opaque)
 {
+	kvm_s390_dbf = debug_register("kvm-trace", 32, 1, 7 * sizeof(long));
+	if (!kvm_s390_dbf)
+		return -ENOMEM;
+
+	if (debug_register_view(kvm_s390_dbf, &debug_sprintf_view)) {
+		debug_unregister(kvm_s390_dbf);
+		return -ENOMEM;
+	}
+
 	/* Register floating interrupt controller interface. */
 	return kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
 }
 
+void kvm_arch_exit(void)
+{
+	debug_unregister(kvm_s390_dbf);
+}
+
 /* Section: device related */
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg)
@@ -1100,7 +1115,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	mutex_init(&kvm->arch.ipte_mutex);
 
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
-	VM_EVENT(kvm, 3, "%s", "vm created");
+	VM_EVENT(kvm, 3, "vm created with type %lu", type);
 
 	if (type & KVM_VM_S390_UCONTROL) {
 		kvm->arch.gmap = NULL;
@@ -1117,6 +1132,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
+	KVM_EVENT(3, "vm 0x%p created by pid %u", kvm, current->pid);
 
 	return 0;
 out_err:
@@ -1124,6 +1140,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	free_page((unsigned long)kvm->arch.model.fac);
 	debug_unregister(kvm->arch.dbf);
 	free_page((unsigned long)(kvm->arch.sca));
+	KVM_EVENT(3, "creation of vm failed: %d", rc);
 	return rc;
 }
 
@@ -1180,6 +1197,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 		gmap_free(kvm->arch.gmap);
 	kvm_s390_destroy_adapters(kvm);
 	kvm_s390_clear_float_irqs(kvm);
+	KVM_EVENT(3, "vm 0x%p destroyed", kvm);
 }
 
 /* Section: vcpu related */

commit 1cb9cf726efeb77e05ee4f27f32700c46ecb1b8a
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Jul 20 15:04:48 2015 +0200

    KVM: s390: improve debug feature usage
    
    We do not use the exception logger, so the 2nd area is unused.
    Just have one area that is bigger (32 pages).
    At the same time we can limit the debug feature size to 7
    longs, as the largest user has 3 parameters + string + boiler
    plate (vCPU, PSW mask, PSW addr)
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b623267d60cc..4c1b5cbea1a6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1057,7 +1057,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	sprintf(debug_name, "kvm-%u", current->pid);
 
-	kvm->arch.dbf = debug_register(debug_name, 8, 2, 8 * sizeof(long));
+	kvm->arch.dbf = debug_register(debug_name, 32, 1, 7 * sizeof(long));
 	if (!kvm->arch.dbf)
 		goto out_err;
 

commit 175a5c9e79b0eb822c158a6535f44a8460b40664
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Jul 7 15:19:32 2015 +0200

    KVM: s390: add kvm stat counter for all diagnoses
    
    Sometimes kvm stat counters are the only performance metric to check
    after something went wrong. Let's add additional counters for some
    diagnoses.
    
    In addition do the count for diag 10 all the time, even if we inject
    a program interrupt.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Jens Freimann <jfrei@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3772947ac194..b623267d60cc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -108,6 +108,9 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "diagnose_10", VCPU_STAT(diagnose_10) },
 	{ "diagnose_44", VCPU_STAT(diagnose_44) },
 	{ "diagnose_9c", VCPU_STAT(diagnose_9c) },
+	{ "diagnose_258", VCPU_STAT(diagnose_258) },
+	{ "diagnose_308", VCPU_STAT(diagnose_308) },
+	{ "diagnose_500", VCPU_STAT(diagnose_500) },
 	{ NULL }
 };
 

commit c348915585af12a7fc6453524d9835830dba574b
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Jun 18 13:17:11 2015 +0200

    KVM: s390: only reset CMMA state if it was enabled before
    
    There is no point in resetting the CMMA state if it was never enabled.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 641639b2fbd0..3772947ac194 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -344,6 +344,10 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 		mutex_unlock(&kvm->lock);
 		break;
 	case KVM_S390_VM_MEM_CLR_CMMA:
+		ret = -EINVAL;
+		if (!kvm->arch.use_cmma)
+			break;
+
 		mutex_lock(&kvm->lock);
 		idx = srcu_read_lock(&kvm->srcu);
 		s390_reset_cmma(kvm->arch.gmap->mm);

commit e6db1d61c7cc05dd3c8b787a0bd47dcb94800300
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu May 7 15:41:57 2015 +0200

    KVM: s390: clean up cmma_enable check
    
    As we already only enable CMMA when userspace requests it, we can
    safely move the additional checks to the request handler and avoid
    doing them multiple times. This also tells userspace if CMMA is
    available.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b77dcfe867be..641639b2fbd0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -330,6 +330,11 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 	unsigned int idx;
 	switch (attr->attr) {
 	case KVM_S390_VM_MEM_ENABLE_CMMA:
+		/* enable CMMA only for z10 and later (EDAT_1) */
+		ret = -EINVAL;
+		if (!MACHINE_IS_LPAR || !MACHINE_HAS_EDAT1)
+			break;
+
 		ret = -EBUSY;
 		mutex_lock(&kvm->lock);
 		if (atomic_read(&kvm->online_vcpus) == 0) {
@@ -1133,7 +1138,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	if (kvm_is_ucontrol(vcpu->kvm))
 		gmap_free(vcpu->arch.gmap);
 
-	if (kvm_s390_cmma_enabled(vcpu->kvm))
+	if (vcpu->kvm->arch.use_cmma)
 		kvm_s390_vcpu_unsetup_cmma(vcpu);
 	free_page((unsigned long)(vcpu->arch.sie_block));
 
@@ -1344,7 +1349,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	}
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 
-	if (kvm_s390_cmma_enabled(vcpu->kvm)) {
+	if (vcpu->kvm->arch.use_cmma) {
 		rc = kvm_s390_vcpu_setup_cmma(vcpu);
 		if (rc)
 			return rc;
@@ -1725,18 +1730,6 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return rc;
 }
 
-bool kvm_s390_cmma_enabled(struct kvm *kvm)
-{
-	if (!MACHINE_IS_LPAR)
-		return false;
-	/* only enable for z10 and later */
-	if (!MACHINE_HAS_EDAT1)
-		return false;
-	if (!kvm->arch.use_cmma)
-		return false;
-	return true;
-}
-
 static bool ibs_enabled(struct kvm_vcpu *vcpu)
 {
 	return atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_IBS;

commit 14d4a425e7df6aaaa3287108ce191ccb6644c264
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu May 7 15:16:13 2015 +0200

    KVM: s390: propagate error from enable storage key
    
    As enabling storage keys might fail, we should forward the error.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2078f92d15ac..b77dcfe867be 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -821,7 +821,9 @@ static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 	}
 
 	/* Enable storage key handling for the guest */
-	s390_enable_skey();
+	r = s390_enable_skey();
+	if (r)
+		goto out;
 
 	for (i = 0; i < args->count; i++) {
 		hva = gfn_to_hva(kvm, args->start_gfn + i);

commit 805de8f43c20ba8b479bb598b543fa86b20067f6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 24 01:12:32 2015 +0200

    atomic: Replace atomic_{set,clear}_mask() usage
    
    Replace the deprecated atomic_{set,clear}_mask() usage with the now
    ubiquous atomic_{or,andnot}() functions.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2078f92d15ac..b73302fb0507 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1215,12 +1215,12 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	}
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
-	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+	atomic_or(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
-	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+	atomic_andnot(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
 	if (test_kvm_facility(vcpu->kvm, 129)) {
 		save_fp_ctl(&vcpu->run->s.regs.fpc);
@@ -1320,9 +1320,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 						    CPUSTAT_STOPPED);
 
 	if (test_kvm_facility(vcpu->kvm, 78))
-		atomic_set_mask(CPUSTAT_GED2, &vcpu->arch.sie_block->cpuflags);
+		atomic_or(CPUSTAT_GED2, &vcpu->arch.sie_block->cpuflags);
 	else if (test_kvm_facility(vcpu->kvm, 8))
-		atomic_set_mask(CPUSTAT_GED, &vcpu->arch.sie_block->cpuflags);
+		atomic_or(CPUSTAT_GED, &vcpu->arch.sie_block->cpuflags);
 
 	kvm_s390_vcpu_setup_model(vcpu);
 
@@ -1422,24 +1422,24 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 
 void kvm_s390_vcpu_block(struct kvm_vcpu *vcpu)
 {
-	atomic_set_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
+	atomic_or(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
 	exit_sie(vcpu);
 }
 
 void kvm_s390_vcpu_unblock(struct kvm_vcpu *vcpu)
 {
-	atomic_clear_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
+	atomic_andnot(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
 }
 
 static void kvm_s390_vcpu_request(struct kvm_vcpu *vcpu)
 {
-	atomic_set_mask(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
+	atomic_or(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
 	exit_sie(vcpu);
 }
 
 static void kvm_s390_vcpu_request_handled(struct kvm_vcpu *vcpu)
 {
-	atomic_clear_mask(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
+	atomic_or(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
 }
 
 /*
@@ -1448,7 +1448,7 @@ static void kvm_s390_vcpu_request_handled(struct kvm_vcpu *vcpu)
  * return immediately. */
 void exit_sie(struct kvm_vcpu *vcpu)
 {
-	atomic_set_mask(CPUSTAT_STOP_INT, &vcpu->arch.sie_block->cpuflags);
+	atomic_or(CPUSTAT_STOP_INT, &vcpu->arch.sie_block->cpuflags);
 	while (vcpu->arch.sie_block->prog0c & PROG_IN_SIE)
 		cpu_relax();
 }
@@ -1672,19 +1672,19 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 	if (dbg->control & KVM_GUESTDBG_ENABLE) {
 		vcpu->guest_debug = dbg->control;
 		/* enforce guest PER */
-		atomic_set_mask(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+		atomic_or(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
 
 		if (dbg->control & KVM_GUESTDBG_USE_HW_BP)
 			rc = kvm_s390_import_bp_data(vcpu, dbg);
 	} else {
-		atomic_clear_mask(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+		atomic_andnot(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
 		vcpu->arch.guestdbg.last_bp = 0;
 	}
 
 	if (rc) {
 		vcpu->guest_debug = 0;
 		kvm_s390_clear_bp_data(vcpu);
-		atomic_clear_mask(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+		atomic_andnot(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
 	}
 
 	return rc;
@@ -1771,7 +1771,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	if (kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu)) {
 		if (!ibs_enabled(vcpu)) {
 			trace_kvm_s390_enable_disable_ibs(vcpu->vcpu_id, 1);
-			atomic_set_mask(CPUSTAT_IBS,
+			atomic_or(CPUSTAT_IBS,
 					&vcpu->arch.sie_block->cpuflags);
 		}
 		goto retry;
@@ -1780,7 +1780,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	if (kvm_check_request(KVM_REQ_DISABLE_IBS, vcpu)) {
 		if (ibs_enabled(vcpu)) {
 			trace_kvm_s390_enable_disable_ibs(vcpu->vcpu_id, 0);
-			atomic_clear_mask(CPUSTAT_IBS,
+			atomic_andnot(CPUSTAT_IBS,
 					  &vcpu->arch.sie_block->cpuflags);
 		}
 		goto retry;
@@ -2280,7 +2280,7 @@ void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 		__disable_ibs_on_all_vcpus(vcpu->kvm);
 	}
 
-	atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	atomic_andnot(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 	/*
 	 * Another VCPU might have used IBS while we were offline.
 	 * Let's play safe and flush the VCPU at startup.
@@ -2306,7 +2306,7 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 	/* SIGP STOP and SIGP STOP AND STORE STATUS has been fully processed */
 	kvm_s390_clear_stop_irq(vcpu);
 
-	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	atomic_or(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 	__disable_ibs_on_vcpu(vcpu);
 
 	for (i = 0; i < online_vcpus; i++) {

commit 9977e886cbbc758b4b601a160b5825ba573b5ca8
Author: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
Date:   Wed Jun 10 12:53:42 2015 +0200

    s390/kernel: lazy restore fpu registers
    
    Improve the save and restore behavior of FPU register contents to use the
    vector extension within the kernel.
    
    The kernel does not use floating-point or vector registers and, therefore,
    saving and restoring the FPU register contents are performed for handling
    signals or switching processes only.  To prepare for using vector
    instructions and vector registers within the kernel, enhance the save
    behavior and implement a lazy restore at return to user space from a
    system call or interrupt.
    
    To implement the lazy restore, the save_fpu_regs() sets a CPU information
    flag, CIF_FPU, to indicate that the FPU registers must be restored.
    Saving and setting CIF_FPU is performed in an atomic fashion to be
    interrupt-safe.  When the kernel wants to use the vector extension or
    wants to change the FPU register state for a task during signal handling,
    the save_fpu_regs() must be called first.  The CIF_FPU flag is also set at
    process switch.  At return to user space, the FPU state is restored.  In
    particular, the FPU state includes the floating-point or vector register
    contents, as well as, vector-enablement and floating-point control.  The
    FPU state restore and clearing CIF_FPU is also performed in an atomic
    fashion.
    
    For KVM, the restore of the FPU register state is performed when restoring
    the general-purpose guest registers before the SIE instructions is started.
    Because the path towards the SIE instruction is interruptible, the CIF_FPU
    flag must be checked again right before going into SIE.  If set, the guest
    registers must be reloaded again by re-entering the outer SIE loop.  This
    is the same behavior as if the SIE critical section is interrupted.
    
    Signed-off-by: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fc7bc7118b23..c0cceaf4a92e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1198,27 +1198,54 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * Backs up the current FP/VX register save area on a particular
+ * destination.  Used to switch between different register save
+ * areas.
+ */
+static inline void save_fpu_to(struct fpu *dst)
+{
+	dst->fpc = current->thread.fpu.fpc;
+	dst->flags = current->thread.fpu.flags;
+	dst->regs = current->thread.fpu.regs;
+}
+
+/*
+ * Switches the FP/VX register save area from which to lazy
+ * restore register contents.
+ */
+static inline void load_fpu_from(struct fpu *from)
+{
+	current->thread.fpu.fpc = from->fpc;
+	current->thread.fpu.flags = from->flags;
+	current->thread.fpu.regs = from->regs;
+}
+
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
-	__u32 fpc;
+	/* Save host register state */
+	save_fpu_regs(&current->thread.fpu);
+	save_fpu_to(&vcpu->arch.host_fpregs);
 
-	save_fp_ctl(&vcpu->arch.host_fpregs.fpc);
-	if (test_kvm_facility(vcpu->kvm, 129))
-		save_vx_regs((__vector128 *)&vcpu->arch.host_vregs->vrs);
-	else
-		save_fp_regs(vcpu->arch.host_fpregs.fprs);
-	save_access_regs(vcpu->arch.host_acrs);
 	if (test_kvm_facility(vcpu->kvm, 129)) {
-		fpc = vcpu->run->s.regs.fpc;
-		restore_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
-	} else {
-		fpc = vcpu->arch.guest_fpregs.fpc;
-		restore_fp_regs(vcpu->arch.guest_fpregs.fprs);
-	}
-	if (test_fp_ctl(fpc))
+		current->thread.fpu.fpc = vcpu->run->s.regs.fpc;
+		current->thread.fpu.flags = FPU_USE_VX;
+		/*
+		 * Use the register save area in the SIE-control block
+		 * for register restore and save in kvm_arch_vcpu_put()
+		 */
+		current->thread.fpu.vxrs =
+			(__vector128 *)&vcpu->run->s.regs.vrs;
+		/* Always enable the vector extension for KVM */
+		__ctl_set_vx();
+	} else
+		load_fpu_from(&vcpu->arch.guest_fpregs);
+
+	if (test_fp_ctl(current->thread.fpu.fpc))
 		/* User space provided an invalid FPC, let's clear it */
-		fpc = 0;
-	restore_fp_ctl(&fpc);
+		current->thread.fpu.fpc = 0;
+
+	save_access_regs(vcpu->arch.host_acrs);
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
@@ -1228,19 +1255,22 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
-	if (test_kvm_facility(vcpu->kvm, 129)) {
-		save_fp_ctl(&vcpu->run->s.regs.fpc);
-		save_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
-	} else {
-		save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
-		save_fp_regs(vcpu->arch.guest_fpregs.fprs);
-	}
-	save_access_regs(vcpu->run->s.regs.acrs);
-	restore_fp_ctl(&vcpu->arch.host_fpregs.fpc);
+
+	save_fpu_regs(&current->thread.fpu);
+
 	if (test_kvm_facility(vcpu->kvm, 129))
-		restore_vx_regs((__vector128 *)&vcpu->arch.host_vregs->vrs);
+		/*
+		 * kvm_arch_vcpu_load() set up the register save area to
+		 * the &vcpu->run->s.regs.vrs and, thus, the vector registers
+		 * are already saved.  Only the floating-point control must be
+		 * copied.
+		 */
+		vcpu->run->s.regs.fpc = current->thread.fpu.fpc;
 	else
-		restore_fp_regs(vcpu->arch.host_fpregs.fprs);
+		save_fpu_to(&vcpu->arch.guest_fpregs);
+	load_fpu_from(&vcpu->arch.host_fpregs);
+
+	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_access_regs(vcpu->arch.host_acrs);
 }
 
@@ -1383,7 +1413,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	vcpu->arch.sie_block = &sie_page->sie_block;
 	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
-	vcpu->arch.host_vregs = &sie_page->vregs;
 
 	vcpu->arch.sie_block->icpua = id;
 	if (!kvm_is_ucontrol(kvm)) {
@@ -1405,6 +1434,19 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.local_int.wq = &vcpu->wq;
 	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
 
+	/*
+	 * Allocate a save area for floating-point registers.  If the vector
+	 * extension is available, register contents are saved in the SIE
+	 * control block.  The allocated save area is still required in
+	 * particular places, for example, in kvm_s390_vcpu_store_status().
+	 */
+	vcpu->arch.guest_fpregs.fprs = kzalloc(sizeof(freg_t) * __NUM_FPRS,
+					       GFP_KERNEL);
+	if (!vcpu->arch.guest_fpregs.fprs) {
+		rc = -ENOMEM;
+		goto out_free_sie_block;
+	}
+
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
 		goto out_free_sie_block;
@@ -1627,16 +1669,16 @@ int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
 	if (test_fp_ctl(fpu->fpc))
 		return -EINVAL;
-	memcpy(&vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
+	memcpy(vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
 	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
-	restore_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
-	restore_fp_regs(vcpu->arch.guest_fpregs.fprs);
+	save_fpu_regs(&current->thread.fpu);
+	load_fpu_from(&vcpu->arch.guest_fpregs);
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
-	memcpy(&fpu->fprs, &vcpu->arch.guest_fpregs.fprs, sizeof(fpu->fprs));
+	memcpy(&fpu->fprs, vcpu->arch.guest_fpregs.fprs, sizeof(fpu->fprs));
 	fpu->fpc = vcpu->arch.guest_fpregs.fpc;
 	return 0;
 }
@@ -2199,8 +2241,21 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	 * copying in vcpu load/put. Lets update our copies before we save
 	 * it into the save area
 	 */
-	save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
-	save_fp_regs(vcpu->arch.guest_fpregs.fprs);
+	save_fpu_regs(&current->thread.fpu);
+	if (test_kvm_facility(vcpu->kvm, 129)) {
+		/*
+		 * If the vector extension is available, the vector registers
+		 * which overlaps with floating-point registers are saved in
+		 * the SIE-control block.  Hence, extract the floating-point
+		 * registers and the FPC value and store them in the
+		 * guest_fpregs structure.
+		 */
+		WARN_ON(!is_vx_task(current));	  /* XXX remove later */
+		vcpu->arch.guest_fpregs.fpc = current->thread.fpu.fpc;
+		convert_vx_to_fp(vcpu->arch.guest_fpregs.fprs,
+				 current->thread.fpu.vxrs);
+	} else
+		save_fpu_to(&vcpu->arch.guest_fpregs);
 	save_access_regs(vcpu->run->s.regs.acrs);
 
 	return kvm_s390_store_status_unloaded(vcpu, addr);
@@ -2227,10 +2282,13 @@ int kvm_s390_vcpu_store_adtl_status(struct kvm_vcpu *vcpu, unsigned long addr)
 
 	/*
 	 * The guest VXRS are in the host VXRs due to the lazy
-	 * copying in vcpu load/put. Let's update our copies before we save
-	 * it into the save area.
+	 * copying in vcpu load/put. We can simply call save_fpu_regs()
+	 * to save the current register state because we are in the
+	 * middle of a load/put cycle.
+	 *
+	 * Let's update our copies before we save it into the save area.
 	 */
-	save_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
+	save_fpu_regs(&current->thread.fpu);
 
 	return kvm_s390_store_adtl_status_unloaded(vcpu, addr);
 }

commit 96b2d7a83a27fbae10fc57c39577a7e2689d9f0a
Author: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
Date:   Fri Jun 12 13:53:51 2015 +0200

    s390/kvm: validate the floating-point control before restoring it
    
    The kvm_arch_vcpu_load() does not validate whether the floating-point
    control (FPC) is valid.  Further, the return code of the restore is not
    checked too.  If the FPC is invalid, the restore fails and the host FPC
    value might remain.  The correct behavior would be to clear the FPC if it
    is not valid.  Hence, validate the FPC value and, optionally, reset the
    value before restoring it.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2078f92d15ac..fc7bc7118b23 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1200,6 +1200,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
+	__u32 fpc;
+
 	save_fp_ctl(&vcpu->arch.host_fpregs.fpc);
 	if (test_kvm_facility(vcpu->kvm, 129))
 		save_vx_regs((__vector128 *)&vcpu->arch.host_vregs->vrs);
@@ -1207,12 +1209,16 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		save_fp_regs(vcpu->arch.host_fpregs.fprs);
 	save_access_regs(vcpu->arch.host_acrs);
 	if (test_kvm_facility(vcpu->kvm, 129)) {
-		restore_fp_ctl(&vcpu->run->s.regs.fpc);
+		fpc = vcpu->run->s.regs.fpc;
 		restore_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
 	} else {
-		restore_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+		fpc = vcpu->arch.guest_fpregs.fpc;
 		restore_fp_regs(vcpu->arch.guest_fpregs.fprs);
 	}
+	if (test_fp_ctl(fpc))
+		/* User space provided an invalid FPC, let's clear it */
+		fpc = 0;
+	restore_fp_ctl(&fpc);
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);

commit 4e241557fc1cb560bd9e77ca1b4a9352732a5427
Merge: 08d183e3c1f6 f2ae45edbca7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 09:36:49 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull first batch of KVM updates from Paolo Bonzini:
     "The bulk of the changes here is for x86.  And for once it's not for
      silicon that no one owns: these are really new features for everyone.
    
      Details:
    
       - ARM:
            several features are in progress but missed the 4.2 deadline.
            So here is just a smattering of bug fixes, plus enabling the
            VFIO integration.
    
       - s390:
            Some fixes/refactorings/optimizations, plus support for 2GB
            pages.
    
       - x86:
            * host and guest support for marking kvmclock as a stable
              scheduler clock.
            * support for write combining.
            * support for system management mode, needed for secure boot in
              guests.
            * a bunch of cleanups required for the above
            * support for virtualized performance counters on AMD
            * legacy PCI device assignment is deprecated and defaults to "n"
              in Kconfig; VFIO replaces it
    
            On top of this there are also bug fixes and eager FPU context
            loading for FPU-heavy guests.
    
       - Common code:
            Support for multiple address spaces; for now it is used only for
            x86 SMM but the s390 folks also have plans"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (124 commits)
      KVM: s390: clear floating interrupt bitmap and parameters
      KVM: x86/vPMU: Enable PMU handling for AMD PERFCTRn and EVNTSELn MSRs
      KVM: x86/vPMU: Implement AMD vPMU code for KVM
      KVM: x86/vPMU: Define kvm_pmu_ops to support vPMU function dispatch
      KVM: x86/vPMU: introduce kvm_pmu_msr_idx_to_pmc
      KVM: x86/vPMU: reorder PMU functions
      KVM: x86/vPMU: whitespace and stylistic adjustments in PMU code
      KVM: x86/vPMU: use the new macros to go between PMC, PMU and VCPU
      KVM: x86/vPMU: introduce pmu.h header
      KVM: x86/vPMU: rename a few PMU functions
      KVM: MTRR: do not map huge page for non-consistent range
      KVM: MTRR: simplify kvm_mtrr_get_guest_memory_type
      KVM: MTRR: introduce mtrr_for_each_mem_type
      KVM: MTRR: introduce fixed_mtrr_addr_* functions
      KVM: MTRR: sort variable MTRRs
      KVM: MTRR: introduce var_mtrr_range
      KVM: MTRR: introduce fixed_mtrr_segment table
      KVM: MTRR: improve kvm_mtrr_get_guest_memory_type
      KVM: MTRR: do not split 64 bits MSR content
      KVM: MTRR: clean up mtrr default type
      ...

commit f71f81d70a264ac9924deb9a95a7e0a4578e7648
Merge: b7e60c5aedd2 ea2cdd27dce6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jun 3 14:51:02 2015 +0200

    Merge tag 'kvm-s390-next-20150602' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kvm-next
    
    KVM: s390: Fix and cleanup for 4.2 (kvm/next)
    
    One small fix for a commit targetted for 4.2 and one cleanup
    regarding our printks.

commit ea2cdd27dce66dc498c623adacd807ea3a350443
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed May 20 13:24:02 2015 +0200

    KVM: s390: introduce KMSG_COMPONENT for kvm-s390
    
    Let's remove "kvm-s390" from our printk messages and make use
    of pr_fmt instead.
    
    Also replace one printk() occurrence by a equivalent pr_warn
    on the way.
    
    Suggested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f37557bc2512..9cb6cfaac986 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -36,6 +36,10 @@
 #include "kvm-s390.h"
 #include "gaccess.h"
 
+#define KMSG_COMPONENT "kvm-s390"
+#undef pr_fmt
+#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
+
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 #include "trace-s390.h"
@@ -2086,7 +2090,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm)) {
 		kvm_s390_vcpu_start(vcpu);
 	} else if (is_vcpu_stopped(vcpu)) {
-		pr_err_ratelimited("kvm-s390: can't run stopped vcpu %d\n",
+		pr_err_ratelimited("can't run stopped vcpu %d\n",
 				   vcpu->vcpu_id);
 		return -EINVAL;
 	}
@@ -2617,7 +2621,7 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 	rc = gmap_map_segment(kvm->arch.gmap, mem->userspace_addr,
 		mem->guest_phys_addr, mem->memory_size);
 	if (rc)
-		printk(KERN_WARNING "kvm-s390: failed to commit memory region\n");
+		pr_warn("failed to commit memory region\n");
 	return;
 }
 

commit 61a6df54b64913defc64c87a05d5d258a2020fb5
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue May 12 08:41:40 2015 +0200

    KVM: s390: call exit_sie() directly on vcpu block/request
    
    Thinking about it, I can't find a real use case where we want
    to block a VCPU and not kick it out of SIE. (except if we want
    to do the same in batch for multiple VCPUs - but that's a micro
    optimization)
    
    So let's simply perform the exit_sie() calls directly when setting
    the other magic block bits in the SIE.
    
    Otherwise e.g. kvm_s390_set_tod_low() still has other VCPUs running
    after that call, working with a wrong epoch.
    
    Fixes: 27406cd50c ("KVM: s390: provide functions for blocking all CPUs")
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6bc69ab5b5b4..f37557bc2512 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1417,6 +1417,7 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 void kvm_s390_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	atomic_set_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
+	exit_sie(vcpu);
 }
 
 void kvm_s390_vcpu_unblock(struct kvm_vcpu *vcpu)
@@ -1427,6 +1428,7 @@ void kvm_s390_vcpu_unblock(struct kvm_vcpu *vcpu)
 static void kvm_s390_vcpu_request(struct kvm_vcpu *vcpu)
 {
 	atomic_set_mask(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
+	exit_sie(vcpu);
 }
 
 static void kvm_s390_vcpu_request_handled(struct kvm_vcpu *vcpu)
@@ -1450,7 +1452,6 @@ void kvm_s390_sync_request(int req, struct kvm_vcpu *vcpu)
 {
 	kvm_make_request(req, vcpu);
 	kvm_s390_vcpu_request(vcpu);
-	exit_sie(vcpu);
 }
 
 static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address)

commit f36f3f2846b5578d62910ee0b6dbef59fdd1cfa4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:20:23 2015 +0200

    KVM: add "new" argument to kvm_arch_commit_memory_region
    
    This lets the function access the new memory slot without going through
    kvm_memslots and id_to_memslot.  It will simplify the code when more
    than one address space will be supported.
    
    Unfortunately, the "const"ness of the new argument must be casted
    away in two places.  Fixing KVM to accept const struct kvm_memory_slot
    pointers would require modifications in pretty much all architectures,
    and is left for later.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 994f9c37f25f..8ad4b9a5667f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2602,6 +2602,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
+				const struct kvm_memory_slot *new,
 				enum kvm_mr_change change)
 {
 	int rc;

commit 09170a49422bd786be3eac5cec1955257c5a34b7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:59:39 2015 +0200

    KVM: const-ify uses of struct kvm_userspace_memory_region
    
    Architecture-specific helpers are not supposed to muck with
    struct kvm_userspace_memory_region contents.  Add const to
    enforce this.
    
    In order to eliminate the only write in __kvm_set_memory_region,
    the cleaning of deleted slots is pulled up from update_memslots
    to __kvm_set_memory_region.
    
    Reviewed-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a05107e9b2bf..994f9c37f25f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2582,7 +2582,7 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 /* Section: memory related */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
-				   struct kvm_userspace_memory_region *mem,
+				   const struct kvm_userspace_memory_region *mem,
 				   enum kvm_mr_change change)
 {
 	/* A few sanity checks. We can have memory slots which have to be
@@ -2600,7 +2600,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 }
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
-				struct kvm_userspace_memory_region *mem,
+				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
 				enum kvm_mr_change change)
 {

commit 9f6b8029787bb37170d4535e9fc09158f634282c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun May 17 16:20:07 2015 +0200

    KVM: use kvm_memslots whenever possible
    
    kvm_memslots provides lockdep checking.  Use it consistently instead of
    explicit dereferencing of kvm->memslots.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d461f8a15c07..a05107e9b2bf 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -236,6 +236,7 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 {
 	int r;
 	unsigned long n;
+	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot;
 	int is_dirty = 0;
 
@@ -245,7 +246,8 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 	if (log->slot >= KVM_USER_MEM_SLOTS)
 		goto out;
 
-	memslot = id_to_memslot(kvm->memslots, log->slot);
+	slots = kvm_memslots(kvm);
+	memslot = id_to_memslot(slots, log->slot);
 	r = -ENOENT;
 	if (!memslot->dirty_bitmap)
 		goto out;

commit 37c5f6c86cf5cda66c71c3bb1672e3b09d81c6da
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed May 6 13:18:59 2015 +0200

    s390/sclp: unify basic sclp access by exposing "struct sclp"
    
    Let's unify basic access to sclp fields by storing the data in an external
    struct in asm/sclp.h.
    
    The values can now directly be accessed by other components, so there is
    no need for most accessor functions and external variables anymore.
    
    The mtid, mtid_max and facility part will be cleaned up separately.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8cd8e7b288c5..c4e81b26c1b0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -604,7 +604,7 @@ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 		goto out;
 	}
 	get_cpu_id((struct cpuid *) &mach->cpuid);
-	mach->ibc = sclp_get_ibc();
+	mach->ibc = sclp.ibc;
 	memcpy(&mach->fac_mask, kvm->arch.model.fac->mask,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
@@ -1068,7 +1068,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
 	kvm_s390_get_cpu_id(&kvm->arch.model.cpu_id);
-	kvm->arch.model.ibc = sclp_get_ibc() & 0x0fff;
+	kvm->arch.model.ibc = sclp.ibc & 0x0fff;
 
 	if (kvm_s390_crypto_init(kvm) < 0)
 		goto out_err;
@@ -1321,9 +1321,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.sie_block->ecb2  = 8;
 	vcpu->arch.sie_block->eca   = 0xC1002000U;
-	if (sclp_has_siif())
+	if (sclp.has_siif)
 		vcpu->arch.sie_block->eca |= 1;
-	if (sclp_has_sigpif())
+	if (sclp.has_sigpif)
 		vcpu->arch.sie_block->eca |= 0x10000000U;
 	if (test_kvm_facility(vcpu->kvm, 129)) {
 		vcpu->arch.sie_block->eca |= 0x00020000;

commit cba3d2765e9fcbce45fa3930c4d5f4da7652e752
Merge: 31fd9880a1c5 06b36753a646
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 11 14:06:32 2015 +0200

    Merge tag 'kvm-s390-next-20150508' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Fixes and features for 4.2 (kvm/next)
    
    Mostly a bunch of fixes, reworks and optimizations for s390.
    There is one new feature (EDAT-2 inside the guest), which boils
    down to 2GB pages.

commit 27406cd50cf6653196eea8ebbb15a75596da01ca
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Apr 14 12:17:34 2015 +0200

    KVM: s390: provide functions for blocking all CPUs
    
    Some updates to the control blocks need to be done in a way that
    ensures that no CPU is within SIE. Provide wrappers around the
    s390_vcpu_block functions and adopt the TOD migration code to
    update in a guaranteed fashion. Also rename these functions to
    have the kvm_s390_ prefix as everything else.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9bc57afe54ba..6bc69ab5b5b4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -454,10 +454,10 @@ static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 
 	mutex_lock(&kvm->lock);
 	kvm->arch.epoch = gtod - host_tod;
-	kvm_for_each_vcpu(vcpu_idx, cur_vcpu, kvm) {
+	kvm_s390_vcpu_block_all(kvm);
+	kvm_for_each_vcpu(vcpu_idx, cur_vcpu, kvm)
 		cur_vcpu->arch.sie_block->epoch = kvm->arch.epoch;
-		exit_sie(cur_vcpu);
-	}
+	kvm_s390_vcpu_unblock_all(kvm);
 	mutex_unlock(&kvm->lock);
 	return 0;
 }
@@ -1414,12 +1414,12 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 	return kvm_s390_vcpu_has_irq(vcpu, 0);
 }
 
-void s390_vcpu_block(struct kvm_vcpu *vcpu)
+void kvm_s390_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	atomic_set_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
 }
 
-void s390_vcpu_unblock(struct kvm_vcpu *vcpu)
+void kvm_s390_vcpu_unblock(struct kvm_vcpu *vcpu)
 {
 	atomic_clear_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
 }

commit 8e23654687c97df1c0e4b1def245a05001eaf640
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Apr 9 13:49:04 2015 +0200

    KVM: s390: make exit_sie_sync more robust
    
    exit_sie_sync is used to kick CPUs out of SIE and prevent reentering at
    any point in time. This is used to reload the prefix pages and to
    set the IBS stuff in a way that guarantees that after this function
    returns we are no longer in SIE. All current users trigger KVM requests.
    
    The request must be set before we block the CPUs to avoid races. Let's
    make this implicit by adding the request into a new function
    kvm_s390_sync_requests that replaces exit_sie_sync and split out
    s390_vcpu_block and s390_vcpu_unblock, that can be used to keep
    CPUs out of SIE independent of requests.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 142d9b40528d..9bc57afe54ba 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1424,6 +1424,16 @@ void s390_vcpu_unblock(struct kvm_vcpu *vcpu)
 	atomic_clear_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
 }
 
+static void kvm_s390_vcpu_request(struct kvm_vcpu *vcpu)
+{
+	atomic_set_mask(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
+}
+
+static void kvm_s390_vcpu_request_handled(struct kvm_vcpu *vcpu)
+{
+	atomic_clear_mask(PROG_REQUEST, &vcpu->arch.sie_block->prog20);
+}
+
 /*
  * Kick a guest cpu out of SIE and wait until SIE is not running.
  * If the CPU is not running (e.g. waiting as idle) the function will
@@ -1435,10 +1445,11 @@ void exit_sie(struct kvm_vcpu *vcpu)
 		cpu_relax();
 }
 
-/* Kick a guest cpu out of SIE and prevent SIE-reentry */
-void exit_sie_sync(struct kvm_vcpu *vcpu)
+/* Kick a guest cpu out of SIE to process a request synchronously */
+void kvm_s390_sync_request(int req, struct kvm_vcpu *vcpu)
 {
-	s390_vcpu_block(vcpu);
+	kvm_make_request(req, vcpu);
+	kvm_s390_vcpu_request(vcpu);
 	exit_sie(vcpu);
 }
 
@@ -1452,8 +1463,7 @@ static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address)
 		/* match against both prefix pages */
 		if (kvm_s390_get_prefix(vcpu) == (address & ~0x1000UL)) {
 			VCPU_EVENT(vcpu, 2, "gmap notifier for %lx", address);
-			kvm_make_request(KVM_REQ_MMU_RELOAD, vcpu);
-			exit_sie_sync(vcpu);
+			kvm_s390_sync_request(KVM_REQ_MMU_RELOAD, vcpu);
 		}
 	}
 }
@@ -1728,7 +1738,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	if (!vcpu->requests)
 		return 0;
 retry:
-	s390_vcpu_unblock(vcpu);
+	kvm_s390_vcpu_request_handled(vcpu);
 	/*
 	 * We use MMU_RELOAD just to re-arm the ipte notifier for the
 	 * guest prefix page. gmap_ipte_notify will wait on the ptl lock.
@@ -2213,8 +2223,7 @@ int kvm_s390_vcpu_store_adtl_status(struct kvm_vcpu *vcpu, unsigned long addr)
 static void __disable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
 {
 	kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu);
-	kvm_make_request(KVM_REQ_DISABLE_IBS, vcpu);
-	exit_sie_sync(vcpu);
+	kvm_s390_sync_request(KVM_REQ_DISABLE_IBS, vcpu);
 }
 
 static void __disable_ibs_on_all_vcpus(struct kvm *kvm)
@@ -2230,8 +2239,7 @@ static void __disable_ibs_on_all_vcpus(struct kvm *kvm)
 static void __enable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
 {
 	kvm_check_request(KVM_REQ_DISABLE_IBS, vcpu);
-	kvm_make_request(KVM_REQ_ENABLE_IBS, vcpu);
-	exit_sie_sync(vcpu);
+	kvm_s390_sync_request(KVM_REQ_ENABLE_IBS, vcpu);
 }
 
 void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)

commit 53df84f8de312586e1c05a8f496f614ca814eeff
Author: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
Date:   Wed Feb 18 11:13:03 2015 +0100

    KVM: s390: Enable guest EDAT2 support
    
    1. Enable EDAT2 in the list of KVM facilities
    
    2. Handle 2G frames in pfmf instruction
    If we support EDAT2, we may enable handling of 2G frames if not in 24
    bit mode.
    
    3. Enable EDAT2 in sie_block
    If the EDAT2 facility is available we enable GED2 mode control in the
    sie_block.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2da36c432aea..142d9b40528d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -110,7 +110,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 /* upper facilities limit for kvm */
 unsigned long kvm_s390_fac_list_mask[] = {
 	0xffe6fffbfcfdfc40UL,
-	0x005c800000000000UL,
+	0x005e800000000000UL,
 };
 
 unsigned long kvm_s390_fac_list_mask_size(void)
@@ -1313,7 +1313,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 						    CPUSTAT_SM |
 						    CPUSTAT_STOPPED);
 
-	if (test_kvm_facility(vcpu->kvm, 8))
+	if (test_kvm_facility(vcpu->kvm, 78))
+		atomic_set_mask(CPUSTAT_GED2, &vcpu->arch.sie_block->cpuflags);
+	else if (test_kvm_facility(vcpu->kvm, 8))
 		atomic_set_mask(CPUSTAT_GED, &vcpu->arch.sie_block->cpuflags);
 
 	kvm_s390_vcpu_setup_model(vcpu);

commit a4a4f1916abbbc3148d79a37cf3fe4f3f6c604d9
Author: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
Date:   Tue Mar 31 14:39:49 2015 +0200

    KVM: s390: make EDAT1 depend on host support
    
    We should only enable EDAT1 for the guest if the host actually supports
    it and the cpu model for the guest has EDAT-1 enabled.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Guenther Hutzl <hutzl@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f5282e62f87f..2da36c432aea 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1311,8 +1311,11 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |
 						    CPUSTAT_SM |
-						    CPUSTAT_STOPPED |
-						    CPUSTAT_GED);
+						    CPUSTAT_STOPPED);
+
+	if (test_kvm_facility(vcpu->kvm, 8))
+		atomic_set_mask(CPUSTAT_GED, &vcpu->arch.sie_block->cpuflags);
+
 	kvm_s390_vcpu_setup_model(vcpu);
 
 	vcpu->arch.sie_block->ecb   = 6;

commit 785dbef407d8b3c82348aba2689e493c752c62ec
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Apr 16 16:58:22 2015 +0200

    KVM: s390: optimize round trip time in request handling
    
    The fast path for a sie exit is that no kvm reqest is pending.
    Make an early check to skip all single bit checks.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8cd8e7b288c5..f5282e62f87f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1720,6 +1720,8 @@ static bool ibs_enabled(struct kvm_vcpu *vcpu)
 
 static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 {
+	if (!vcpu->requests)
+		return 0;
 retry:
 	s390_vcpu_unblock(vcpu);
 	/*

commit 0097d12e504b3ce57b68810737ad6a5a64a98c68
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Apr 30 13:43:30 2015 +0200

    KVM: provide irq_unsafe kvm_guest_{enter|exit}
    
    Several kvm architectures disable interrupts before kvm_guest_enter.
    kvm_guest_enter then uses local_irq_save/restore to disable interrupts
    again or for the first time. Lets provide underscore versions of
    kvm_guest_{enter|exit} that assume being called locked.
    kvm_guest_enter now disables interrupts for the full function and
    thus we can remove the check for preemptible.
    
    This patch then adopts s390/kvm to use local_irq_disable/enable calls
    which are slighty cheaper that local_irq_save/restore and call these
    new functions.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8cd8e7b288c5..2be391bb8557 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1993,12 +1993,14 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		 * As PF_VCPU will be used in fault handler, between
 		 * guest_enter and guest_exit should be no uaccess.
 		 */
-		preempt_disable();
-		kvm_guest_enter();
-		preempt_enable();
+		local_irq_disable();
+		__kvm_guest_enter();
+		local_irq_enable();
 		exit_reason = sie64a(vcpu->arch.sie_block,
 				     vcpu->run->s.regs.gprs);
-		kvm_guest_exit();
+		local_irq_disable();
+		__kvm_guest_exit();
+		local_irq_enable();
 		vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
 
 		rc = vcpu_post_run(vcpu, exit_reason);

commit 95fce4fa0850da8f85ecf8d069ab3fc6c8bc1478
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Apr 15 12:27:04 2015 +0200

    KVM: s390: disable RRBM again
    
    commit b273921356df ("KVM: s390: enable more features that need
    no hypervisor changes") also enabled RRBM. Turns out that this
    instruction does need some KVM code, so lets disable that bit
    again.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Fixes: b273921356df ("KVM: s390: enable more features that need no hypervisor changes")
    Message-Id: <1429093624-49611-2-git-send-email-borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index afa2bd750ffc..8cd8e7b288c5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -110,7 +110,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 /* upper facilities limit for kvm */
 unsigned long kvm_s390_fac_list_mask[] = {
 	0xffe6fffbfcfdfc40UL,
-	0x205c800000000000UL,
+	0x005c800000000000UL,
 };
 
 unsigned long kvm_s390_fac_list_mask_size(void)

commit 900360131066f192c82311a098d03d6ac6429e20
Merge: 4541fec3104b ca3f0874723f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 09:47:01 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "First batch of KVM changes for 4.1
    
      The most interesting bit here is irqfd/ioeventfd support for ARM and
      ARM64.
    
      Summary:
    
      ARM/ARM64:
         fixes for live migration, irqfd and ioeventfd support (enabling
         vhost, too), page aging
    
      s390:
         interrupt handling rework, allowing to inject all local interrupts
         via new ioctl and to get/set the full local irq state for migration
         and introspection.  New ioctls to access memory by virtual address,
         and to get/set the guest storage keys.  SIMD support.
    
      MIPS:
         FPU and MIPS SIMD Architecture (MSA) support.  Includes some
         patches from Ralf Baechle's MIPS tree.
    
      x86:
         bugfixes (notably for pvclock, the others are small) and cleanups.
         Another small latency improvement for the TSC deadline timer"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (146 commits)
      KVM: use slowpath for cross page cached accesses
      kvm: mmu: lazy collapse small sptes into large sptes
      KVM: x86: Clear CR2 on VCPU reset
      KVM: x86: DR0-DR3 are not clear on reset
      KVM: x86: BSP in MSR_IA32_APICBASE is writable
      KVM: x86: simplify kvm_apic_map
      KVM: x86: avoid logical_map when it is invalid
      KVM: x86: fix mixed APIC mode broadcast
      KVM: x86: use MDA for interrupt matching
      kvm/ppc/mpic: drop unused IRQ_testbit
      KVM: nVMX: remove unnecessary double caching of MAXPHYADDR
      KVM: nVMX: checks for address bits beyond MAXPHYADDR on VM-entry
      KVM: x86: cache maxphyaddr CPUID leaf in struct kvm_vcpu
      KVM: vmx: pass error code with internal error #2
      x86: vdso: fix pvclock races with task migration
      KVM: remove kvm_read_hva and kvm_read_hva_atomic
      KVM: x86: optimize delivery of TSC deadline timer interrupt
      KVM: x86: extract blocking logic from __vcpu_run
      kvm: x86: fix x86 eflags fixed bit
      KVM: s390: migrate vcpu interrupt state
      ...

commit 816c7667ea97c61884e014cfeedaede5b67b0e58
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Mon Nov 24 17:13:46 2014 +0100

    KVM: s390: migrate vcpu interrupt state
    
    This patch adds support to migrate vcpu interrupts. Two new vcpu ioctls
    are added which get/set the complete status of pending interrupts in one
    go. The ioctls are marked as available with the new capability
    KVM_CAP_S390_IRQ_STATE.
    
    We can not use a ONEREG, as the number of pending local interrupts is not
    constant and depends on the number of CPUs.
    
    To retrieve the interrupt state we add an ioctl KVM_S390_GET_IRQ_STATE.
    Its input parameter is a pointer to a struct kvm_s390_irq_state which
    has a buffer and length.  For all currently pending interrupts, we copy
    a struct kvm_s390_irq into the buffer and pass it to userspace.
    
    To store interrupt state into a buffer provided by userspace, we add an
    ioctl KVM_S390_SET_IRQ_STATE. It passes a struct kvm_s390_irq_state into
    the kernel and injects all interrupts contained in the buffer.
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8bc25d405edf..3040b14751b8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -41,6 +41,9 @@
 #include "trace-s390.h"
 
 #define MEM_OP_MAX_SIZE 65536	/* Maximum transfer size for KVM_S390_MEM_OP */
+#define LOCAL_IRQS 32
+#define VCPU_IRQS_MAX_BUF (sizeof(struct kvm_s390_irq) * \
+			   (KVM_MAX_VCPUS + LOCAL_IRQS))
 
 #define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
 
@@ -181,6 +184,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_USER_SIGP:
 	case KVM_CAP_S390_USER_STSI:
 	case KVM_CAP_S390_SKEYS:
+	case KVM_CAP_S390_IRQ_STATE:
 		r = 1;
 		break;
 	case KVM_CAP_S390_MEM_OP:
@@ -2500,6 +2504,38 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			r = -EFAULT;
 		break;
 	}
+	case KVM_S390_SET_IRQ_STATE: {
+		struct kvm_s390_irq_state irq_state;
+
+		r = -EFAULT;
+		if (copy_from_user(&irq_state, argp, sizeof(irq_state)))
+			break;
+		if (irq_state.len > VCPU_IRQS_MAX_BUF ||
+		    irq_state.len == 0 ||
+		    irq_state.len % sizeof(struct kvm_s390_irq) > 0) {
+			r = -EINVAL;
+			break;
+		}
+		r = kvm_s390_set_irq_state(vcpu,
+					   (void __user *) irq_state.buf,
+					   irq_state.len);
+		break;
+	}
+	case KVM_S390_GET_IRQ_STATE: {
+		struct kvm_s390_irq_state irq_state;
+
+		r = -EFAULT;
+		if (copy_from_user(&irq_state, argp, sizeof(irq_state)))
+			break;
+		if (irq_state.len == 0) {
+			r = -EINVAL;
+			break;
+		}
+		r = kvm_s390_get_irq_state(vcpu,
+					   (__u8 __user *)  irq_state.buf,
+					   irq_state.len);
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit 47b43c52ee4b0425449d1b2b1eedca7f6b7a578a
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Tue Nov 11 20:57:06 2014 +0100

    KVM: s390: add ioctl to inject local interrupts
    
    We have introduced struct kvm_s390_irq a while ago which allows to
    inject all kinds of interrupts as defined in the Principles of
    Operation.
    Add ioctl to inject interrupts with the extended struct kvm_s390_irq
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dbc9ca34d9da..8bc25d405edf 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -177,6 +177,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_IRQCHIP:
 	case KVM_CAP_VM_ATTRIBUTES:
 	case KVM_CAP_MP_STATE:
+	case KVM_CAP_S390_INJECT_IRQ:
 	case KVM_CAP_S390_USER_SIGP:
 	case KVM_CAP_S390_USER_STSI:
 	case KVM_CAP_S390_SKEYS:
@@ -2391,6 +2392,15 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	long r;
 
 	switch (ioctl) {
+	case KVM_S390_IRQ: {
+		struct kvm_s390_irq s390irq;
+
+		r = -EFAULT;
+		if (copy_from_user(&s390irq, argp, sizeof(s390irq)))
+			break;
+		r = kvm_s390_inject_vcpu(vcpu, &s390irq);
+		break;
+	}
 	case KVM_S390_INTERRUPT: {
 		struct kvm_s390_interrupt s390int;
 		struct kvm_s390_irq s390irq;

commit 6d3da241416e6088f83a7ff1f37fb6bb518d9bc8
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Wed Jul 3 15:18:35 2013 +0200

    KVM: s390: deliver floating interrupts in order of priority
    
    This patch makes interrupt handling compliant to the z/Architecture
    Principles of Operation with regard to interrupt priorities.
    
    Add a bitmap for pending floating interrupts. Each bit relates to a
    interrupt type and its list. A turned on bit indicates that a list
    contains items (interrupts) which need to be delivered.  When delivering
    interrupts on a cpu we can merge the existing bitmap for cpu-local
    interrupts and floating interrupts and have a single mechanism for
    delivery.
    Currently we have one list for all kinds of floating interrupts and a
    corresponding spin lock. This patch adds a separate list per
    interrupt type. An exception to this are service signal and machine check
    interrupts, as there can be only one pending interrupt at a time.
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a1308859887d..dbc9ca34d9da 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -31,6 +31,7 @@
 #include <asm/pgtable.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>
+#include <asm/isc.h>
 #include <asm/sclp.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
@@ -1069,7 +1070,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		goto out_err;
 
 	spin_lock_init(&kvm->arch.float_int.lock);
-	INIT_LIST_HEAD(&kvm->arch.float_int.list);
+	for (i = 0; i < FIRQ_LIST_COUNT; i++)
+		INIT_LIST_HEAD(&kvm->arch.float_int.lists[i]);
 	init_waitqueue_head(&kvm->arch.ipte_wq);
 	mutex_init(&kvm->arch.ipte_mutex);
 

commit a3ed8dae6e3db479ca275883ba7fe994170b0ae6
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Mar 18 13:54:31 2015 +0100

    KVM: s390: enable more features that need no hypervisor changes
    
    After some review about what these facilities do, the following
    facilities will work under KVM and can, therefore, be reported
    to the guest if the cpu model and the host cpu provide this bit.
    
    There are plans underway to make the whole bit thing more readable,
    but its not yet finished. So here are some last bit changes and
    we enhance the KVM mask with:
    
    9 The sense-running-status facility is installed in the
      z/Architecture architectural mode.
      ---> handled by SIE or KVM
    
    10 The conditional-SSKE facility is installed in the
       z/Architecture architectural mode.
      ---> handled by SIE. KVM will retry SIE
    
    13 The IPTE-range facility is installed in the
       z/Architecture architectural mode.
      ---> handled by SIE. KVM will retry SIE
    
    36 The enhanced-monitor facility is installed in the
       z/Architecture architectural mode.
      ---> handled by SIE
    
    47 The CMPSC-enhancement facility is installed in the
       z/Architecture architectural mode.
      ---> handled by SIE
    
    48 The decimal-floating-point zoned-conversion facility
       is installed in the z/Architecture architectural mode.
      ---> handled by SIE
    
    49 The execution-hint, load-and-trap, miscellaneous-
       instruction-extensions and processor-assist
      ---> handled by SIE
    
    51 The local-TLB-clearing facility is installed in the
       z/Architecture architectural mode.
      ---> handled by SIE
    
    52 The interlocked-access facility 2 is installed.
      ---> handled by SIE
    
    53 The load/store-on-condition facility 2 and load-and-
       zero-rightmost-byte facility are installed in the
       z/Architecture architectural mode.
      ---> handled by SIE
    
    57 The message-security-assist-extension-5 facility is
      installed in the z/Architecture architectural mode.
      ---> handled by SIE
    
    66 The reset-reference-bits-multiple facility is installed
      in the z/Architecture architectural mode.
      ---> handled by SIE. KVM will retry SIE
    
    80 The decimal-floating-point packed-conversion
       facility is installed in the z/Architecture architectural
       mode.
      ---> handled by SIE
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9072127bd51b..a1308859887d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -105,8 +105,8 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 
 /* upper facilities limit for kvm */
 unsigned long kvm_s390_fac_list_mask[] = {
-	0xff82fffbf4fc2000UL,
-	0x005c000000000000UL,
+	0xffe6fffbfcfdfc40UL,
+	0x205c800000000000UL,
 };
 
 unsigned long kvm_s390_fac_list_mask_size(void)

commit 18280d8b4bcd4a2b174ee3cd748166c6190acacb
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Mon Mar 16 16:05:41 2015 +0100

    KVM: s390: represent SIMD cap in kvm facility
    
    The patch represents capability KVM_CAP_S390_VECTOR_REGISTERS by means
    of the SIMD facility bit. This allows to a) disable the use of SIMD when
    used in conjunction with a not-SIMD-aware QEMU, b) to enable SIMD when
    used with a SIMD-aware version of QEMU and c) finally by means of a QEMU
    version using the future cpu model ioctls.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Eric Farman <farman@linux.vnet.ibm.com>
    Tested-by: Eric Farman <farman@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 42b8a2595237..9072127bd51b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -278,8 +278,12 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		r = 0;
 		break;
 	case KVM_CAP_S390_VECTOR_REGISTERS:
-		kvm->arch.use_vectors = MACHINE_HAS_VX;
-		r = MACHINE_HAS_VX ? 0 : -EINVAL;
+		if (MACHINE_HAS_VX) {
+			set_kvm_facility(kvm->arch.model.fac->mask, 129);
+			set_kvm_facility(kvm->arch.model.fac->list, 129);
+			r = 0;
+		} else
+			r = -EINVAL;
 		break;
 	case KVM_CAP_S390_USER_STSI:
 		kvm->arch.user_stsi = 1;
@@ -1084,7 +1088,6 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm->arch.css_support = 0;
 	kvm->arch.use_irqchip = 0;
-	kvm->arch.use_vectors = 0;
 	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
@@ -1186,12 +1189,12 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	save_fp_ctl(&vcpu->arch.host_fpregs.fpc);
-	if (vcpu->kvm->arch.use_vectors)
+	if (test_kvm_facility(vcpu->kvm, 129))
 		save_vx_regs((__vector128 *)&vcpu->arch.host_vregs->vrs);
 	else
 		save_fp_regs(vcpu->arch.host_fpregs.fprs);
 	save_access_regs(vcpu->arch.host_acrs);
-	if (vcpu->kvm->arch.use_vectors) {
+	if (test_kvm_facility(vcpu->kvm, 129)) {
 		restore_fp_ctl(&vcpu->run->s.regs.fpc);
 		restore_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
 	} else {
@@ -1207,7 +1210,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
-	if (vcpu->kvm->arch.use_vectors) {
+	if (test_kvm_facility(vcpu->kvm, 129)) {
 		save_fp_ctl(&vcpu->run->s.regs.fpc);
 		save_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
 	} else {
@@ -1216,7 +1219,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	}
 	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_fp_ctl(&vcpu->arch.host_fpregs.fpc);
-	if (vcpu->kvm->arch.use_vectors)
+	if (test_kvm_facility(vcpu->kvm, 129))
 		restore_vx_regs((__vector128 *)&vcpu->arch.host_vregs->vrs);
 	else
 		restore_fp_regs(vcpu->arch.host_fpregs.fprs);
@@ -1316,7 +1319,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= 1;
 	if (sclp_has_sigpif())
 		vcpu->arch.sie_block->eca |= 0x10000000U;
-	if (vcpu->kvm->arch.use_vectors) {
+	if (test_kvm_facility(vcpu->kvm, 129)) {
 		vcpu->arch.sie_block->eca |= 0x00020000;
 		vcpu->arch.sie_block->ecd |= 0x20000000;
 	}

commit 400ac6cd73633f61e42a035b910c3db2b590b9d5
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Tue Mar 17 11:03:07 2015 +0100

    KVM: s390: drop SIMD bit from kvm_s390_fac_list_mask
    
    Setting the SIMD bit in the KVM mask is an issue because it makes the
    facility visible but not usable to the guest, thus it needs to be
    removed again.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Eric Farman <farman@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0dc22baa0a57..42b8a2595237 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -107,7 +107,6 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 unsigned long kvm_s390_fac_list_mask[] = {
 	0xff82fffbf4fc2000UL,
 	0x005c000000000000UL,
-	0x4000000000000000UL,
 };
 
 unsigned long kvm_s390_fac_list_mask_size(void)

commit 30ee2a984f07b00895e0e01d78859b3aff9307c7
Author: Jason J. Herne <jjherne@linux.vnet.ibm.com>
Date:   Tue Sep 23 09:23:01 2014 -0400

    KVM: s390: Create ioctl for Getting/Setting guest storage keys
    
    Provide the KVM_S390_GET_SKEYS and KVM_S390_SET_SKEYS ioctl which can be used
    to get/set guest storage keys. This functionality is needed for live migration
    of s390 guests that use storage keys.
    
    Signed-off-by: Jason J. Herne <jjherne@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fdfa10662700..0dc22baa0a57 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -179,6 +179,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_MP_STATE:
 	case KVM_CAP_S390_USER_SIGP:
 	case KVM_CAP_S390_USER_STSI:
+	case KVM_CAP_S390_SKEYS:
 		r = 1;
 		break;
 	case KVM_CAP_S390_MEM_OP:
@@ -729,6 +730,108 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	return ret;
 }
 
+static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
+{
+	uint8_t *keys;
+	uint64_t hva;
+	unsigned long curkey;
+	int i, r = 0;
+
+	if (args->flags != 0)
+		return -EINVAL;
+
+	/* Is this guest using storage keys? */
+	if (!mm_use_skey(current->mm))
+		return KVM_S390_GET_SKEYS_NONE;
+
+	/* Enforce sane limit on memory allocation */
+	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
+		return -EINVAL;
+
+	keys = kmalloc_array(args->count, sizeof(uint8_t),
+			     GFP_KERNEL | __GFP_NOWARN);
+	if (!keys)
+		keys = vmalloc(sizeof(uint8_t) * args->count);
+	if (!keys)
+		return -ENOMEM;
+
+	for (i = 0; i < args->count; i++) {
+		hva = gfn_to_hva(kvm, args->start_gfn + i);
+		if (kvm_is_error_hva(hva)) {
+			r = -EFAULT;
+			goto out;
+		}
+
+		curkey = get_guest_storage_key(current->mm, hva);
+		if (IS_ERR_VALUE(curkey)) {
+			r = curkey;
+			goto out;
+		}
+		keys[i] = curkey;
+	}
+
+	r = copy_to_user((uint8_t __user *)args->skeydata_addr, keys,
+			 sizeof(uint8_t) * args->count);
+	if (r)
+		r = -EFAULT;
+out:
+	kvfree(keys);
+	return r;
+}
+
+static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
+{
+	uint8_t *keys;
+	uint64_t hva;
+	int i, r = 0;
+
+	if (args->flags != 0)
+		return -EINVAL;
+
+	/* Enforce sane limit on memory allocation */
+	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
+		return -EINVAL;
+
+	keys = kmalloc_array(args->count, sizeof(uint8_t),
+			     GFP_KERNEL | __GFP_NOWARN);
+	if (!keys)
+		keys = vmalloc(sizeof(uint8_t) * args->count);
+	if (!keys)
+		return -ENOMEM;
+
+	r = copy_from_user(keys, (uint8_t __user *)args->skeydata_addr,
+			   sizeof(uint8_t) * args->count);
+	if (r) {
+		r = -EFAULT;
+		goto out;
+	}
+
+	/* Enable storage key handling for the guest */
+	s390_enable_skey();
+
+	for (i = 0; i < args->count; i++) {
+		hva = gfn_to_hva(kvm, args->start_gfn + i);
+		if (kvm_is_error_hva(hva)) {
+			r = -EFAULT;
+			goto out;
+		}
+
+		/* Lowest order bit is reserved */
+		if (keys[i] & 0x01) {
+			r = -EINVAL;
+			goto out;
+		}
+
+		r = set_guest_storage_key(current->mm, hva,
+					  (unsigned long)keys[i], 0);
+		if (r)
+			goto out;
+	}
+out:
+	kvfree(keys);
+	return r;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg)
 {
@@ -788,6 +891,26 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_s390_vm_has_attr(kvm, &attr);
 		break;
 	}
+	case KVM_S390_GET_SKEYS: {
+		struct kvm_s390_skeys args;
+
+		r = -EFAULT;
+		if (copy_from_user(&args, argp,
+				   sizeof(struct kvm_s390_skeys)))
+			break;
+		r = kvm_s390_get_skeys(kvm, &args);
+		break;
+	}
+	case KVM_S390_SET_SKEYS: {
+		struct kvm_s390_skeys args;
+
+		r = -EFAULT;
+		if (copy_from_user(&args, argp,
+				   sizeof(struct kvm_s390_skeys)))
+			break;
+		r = kvm_s390_set_skeys(kvm, &args);
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit e44fc8c9dab215ac0e398622a05574cffd5f5184
Author: Ekaterina Tumanova <tumanova@linux.vnet.ibm.com>
Date:   Fri Jan 30 16:55:56 2015 +0100

    KVM: s390: introduce post handlers for STSI
    
    The Store System Information (STSI) instruction currently collects all
    information it relays to the caller in the kernel. Some information,
    however, is only available in user space. An example of this is the
    guest name: The kernel always sets "KVMGuest", but user space knows the
    actual guest name.
    
    This patch introduces a new exit, KVM_EXIT_S390_STSI, guarded by a
    capability that can be enabled by user space if it wants to be able to
    insert such data. User space will be provided with the target buffer
    and the requested STSI function code.
    
    Reviewed-by: Eric Farman <farman@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Ekaterina Tumanova <tumanova@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b7ecef98b668..fdfa10662700 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -178,6 +178,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_VM_ATTRIBUTES:
 	case KVM_CAP_MP_STATE:
 	case KVM_CAP_S390_USER_SIGP:
+	case KVM_CAP_S390_USER_STSI:
 		r = 1;
 		break;
 	case KVM_CAP_S390_MEM_OP:
@@ -280,6 +281,10 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		kvm->arch.use_vectors = MACHINE_HAS_VX;
 		r = MACHINE_HAS_VX ? 0 : -EINVAL;
 		break;
+	case KVM_CAP_S390_USER_STSI:
+		kvm->arch.user_stsi = 1;
+		r = 0;
+		break;
 	default:
 		r = -EINVAL;
 		break;

commit 41408c28f283b49202ae374b1c42bc8e9b33a048
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Fri Feb 6 15:01:21 2015 +0100

    KVM: s390: Add MEMOP ioctls for reading/writing guest memory
    
    On s390, we've got to make sure to hold the IPTE lock while accessing
    logical memory. So let's add an ioctl for reading and writing logical
    memory to provide this feature for userspace, too.
    The maximum transfer size of this call is limited to 64kB to prevent
    that the guest can trigger huge copy_from/to_user transfers. QEMU
    currently only requests up to one or two pages so far, so 16*4kB seems
    to be a reasonable limit here.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 610e90afadf2..b7ecef98b668 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -25,6 +25,7 @@
 #include <linux/random.h>
 #include <linux/slab.h>
 #include <linux/timer.h>
+#include <linux/vmalloc.h>
 #include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/pgtable.h>
@@ -38,6 +39,8 @@
 #include "trace.h"
 #include "trace-s390.h"
 
+#define MEM_OP_MAX_SIZE 65536	/* Maximum transfer size for KVM_S390_MEM_OP */
+
 #define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
 
 struct kvm_stats_debugfs_item debugfs_entries[] = {
@@ -177,6 +180,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_USER_SIGP:
 		r = 1;
 		break;
+	case KVM_CAP_S390_MEM_OP:
+		r = MEM_OP_MAX_SIZE;
+		break;
 	case KVM_CAP_NR_VCPUS:
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
@@ -2185,6 +2191,65 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+static long kvm_s390_guest_mem_op(struct kvm_vcpu *vcpu,
+				  struct kvm_s390_mem_op *mop)
+{
+	void __user *uaddr = (void __user *)mop->buf;
+	void *tmpbuf = NULL;
+	int r, srcu_idx;
+	const u64 supported_flags = KVM_S390_MEMOP_F_INJECT_EXCEPTION
+				    | KVM_S390_MEMOP_F_CHECK_ONLY;
+
+	if (mop->flags & ~supported_flags)
+		return -EINVAL;
+
+	if (mop->size > MEM_OP_MAX_SIZE)
+		return -E2BIG;
+
+	if (!(mop->flags & KVM_S390_MEMOP_F_CHECK_ONLY)) {
+		tmpbuf = vmalloc(mop->size);
+		if (!tmpbuf)
+			return -ENOMEM;
+	}
+
+	srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
+
+	switch (mop->op) {
+	case KVM_S390_MEMOP_LOGICAL_READ:
+		if (mop->flags & KVM_S390_MEMOP_F_CHECK_ONLY) {
+			r = check_gva_range(vcpu, mop->gaddr, mop->ar, mop->size, false);
+			break;
+		}
+		r = read_guest(vcpu, mop->gaddr, mop->ar, tmpbuf, mop->size);
+		if (r == 0) {
+			if (copy_to_user(uaddr, tmpbuf, mop->size))
+				r = -EFAULT;
+		}
+		break;
+	case KVM_S390_MEMOP_LOGICAL_WRITE:
+		if (mop->flags & KVM_S390_MEMOP_F_CHECK_ONLY) {
+			r = check_gva_range(vcpu, mop->gaddr, mop->ar, mop->size, true);
+			break;
+		}
+		if (copy_from_user(tmpbuf, uaddr, mop->size)) {
+			r = -EFAULT;
+			break;
+		}
+		r = write_guest(vcpu, mop->gaddr, mop->ar, tmpbuf, mop->size);
+		break;
+	default:
+		r = -EINVAL;
+	}
+
+	srcu_read_unlock(&vcpu->kvm->srcu, srcu_idx);
+
+	if (r > 0 && (mop->flags & KVM_S390_MEMOP_F_INJECT_EXCEPTION) != 0)
+		kvm_s390_inject_prog_irq(vcpu, &vcpu->arch.pgm);
+
+	vfree(tmpbuf);
+	return r;
+}
+
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
@@ -2284,6 +2349,15 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		r = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);
 		break;
 	}
+	case KVM_S390_MEM_OP: {
+		struct kvm_s390_mem_op mem_op;
+
+		if (copy_from_user(&mem_op, argp, sizeof(mem_op)) == 0)
+			r = kvm_s390_guest_mem_op(vcpu, &mem_op);
+		else
+			r = -EFAULT;
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit 8ae04b8f500b9f46652c63431bf658223d875597
Author: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
Date:   Mon Jan 19 13:24:51 2015 +0300

    KVM: s390: Guest's memory access functions get access registers
    
    In access register mode, the write_guest() read_guest() and other
    functions will invoke the access register translation, which
    requires an ar, designated by one of the instruction fields.
    
    Signed-off-by: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4075acb7c517..610e90afadf2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1776,7 +1776,7 @@ static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
 	 * to look up the current opcode to get the length of the instruction
 	 * to be able to forward the PSW.
 	 */
-	rc = read_guest(vcpu, psw->addr, &opcode, 1);
+	rc = read_guest(vcpu, psw->addr, 0, &opcode, 1);
 	if (rc)
 		return kvm_s390_inject_prog_cond(vcpu, rc);
 	psw->addr = __rewind_psw(*psw, -insn_length(opcode));

commit 40f5b735e867b8fd3e6090f5a184950c68d227bb
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Mar 12 13:55:53 2015 +0100

    KVM: s390: cleanup jump lables in kvm_arch_init_vm
    
    As all cleanup functions can handle their respective NULL case
    there is no need to have more than one error jump label.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 02e03c862a60..4075acb7c517 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -897,7 +897,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm->arch.dbf = debug_register(debug_name, 8, 2, 8 * sizeof(long));
 	if (!kvm->arch.dbf)
-		goto out_nodbf;
+		goto out_err;
 
 	/*
 	 * The architectural maximum amount of facilities is 16 kbit. To store
@@ -909,7 +909,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.model.fac =
 		(struct kvm_s390_fac *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!kvm->arch.model.fac)
-		goto out_nofac;
+		goto out_err;
 
 	/* Populate the facility mask initially. */
 	memcpy(kvm->arch.model.fac->mask, S390_lowcore.stfle_fac_list,
@@ -929,7 +929,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.model.ibc = sclp_get_ibc() & 0x0fff;
 
 	if (kvm_s390_crypto_init(kvm) < 0)
-		goto out_crypto;
+		goto out_err;
 
 	spin_lock_init(&kvm->arch.float_int.lock);
 	INIT_LIST_HEAD(&kvm->arch.float_int.list);
@@ -944,7 +944,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	} else {
 		kvm->arch.gmap = gmap_alloc(current->mm, (1UL << 44) - 1);
 		if (!kvm->arch.gmap)
-			goto out_nogmap;
+			goto out_err;
 		kvm->arch.gmap->private = kvm;
 		kvm->arch.gmap->pfault_enabled = 0;
 	}
@@ -957,15 +957,11 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	spin_lock_init(&kvm->arch.start_stop_lock);
 
 	return 0;
-out_nogmap:
+out_err:
 	kfree(kvm->arch.crypto.crycb);
-out_crypto:
 	free_page((unsigned long)kvm->arch.model.fac);
-out_nofac:
 	debug_unregister(kvm->arch.dbf);
-out_nodbf:
 	free_page((unsigned long)(kvm->arch.sca));
-out_err:
 	return rc;
 }
 

commit dc9be0fac70a2ad86e31a81372bb0bdfb6945353
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 5 11:54:46 2015 +0100

    kvm: move advertising of KVM_CAP_IRQFD to common code
    
    POWER supports irqfds but forgot to advertise them.  Some userspace does
    not check for the capability, but others check it---thus they work on
    x86 and s390 but not POWER.
    
    To avoid that other architectures in the future make the same mistake, let
    common code handle KVM_CAP_IRQFD the same way as KVM_CAP_IRQFD_RESAMPLE.
    
    Reported-and-tested-by: Greg Kurz <gkurz@linux.vnet.ibm.com>
    Cc: stable@vger.kernel.org
    Fixes: 297e21053a52f060944e9f0de4c64fad9bcd72fc
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f6579cfde2df..19e17bd7aec0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -165,7 +165,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_S390_CSS_SUPPORT:
-	case KVM_CAP_IRQFD:
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
 	case KVM_CAP_ENABLE_CAP_VM:

commit 13211ea7b47db3d8ee2ff258a9a973a6d3aa3d43
Author: Eric Farman <farman@linux.vnet.ibm.com>
Date:   Wed Apr 30 13:39:46 2014 -0400

    KVM: s390: Enable vector support for capable guest
    
    We finally have all the pieces in place, so let's include the
    vector facility bit in the mask of available hardware facilities
    for the guest to recognize.  Also, enable the vector functionality
    in the guest control blocks, to avoid a possible vector data
    exception that would otherwise occur when a vector instruction
    is issued by the guest operating system.
    
    Signed-off-by: Eric Farman <farman@linux.vnet.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0c045cfdce9b..02e03c862a60 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -104,6 +104,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 unsigned long kvm_s390_fac_list_mask[] = {
 	0xff82fffbf4fc2000UL,
 	0x005c000000000000UL,
+	0x4000000000000000UL,
 };
 
 unsigned long kvm_s390_fac_list_mask_size(void)
@@ -1186,6 +1187,10 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= 1;
 	if (sclp_has_sigpif())
 		vcpu->arch.sie_block->eca |= 0x10000000U;
+	if (vcpu->kvm->arch.use_vectors) {
+		vcpu->arch.sie_block->eca |= 0x00020000;
+		vcpu->arch.sie_block->ecd |= 0x20000000;
+	}
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 
 	if (kvm_s390_cmma_enabled(vcpu->kvm)) {

commit bc17de7c966504b287a1dceb76a523d8b7816731
Author: Eric Farman <farman@linux.vnet.ibm.com>
Date:   Mon Apr 14 16:01:09 2014 -0400

    KVM: s390: Machine Check
    
    Store additional status in the machine check handler, in order to
    collect status (such as vector registers) that is not defined by
    store status.
    
    Signed-off-by: Eric Farman <farman@linux.vnet.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c0ae03aa0dff..0c045cfdce9b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2031,6 +2031,35 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return kvm_s390_store_status_unloaded(vcpu, addr);
 }
 
+/*
+ * store additional status at address
+ */
+int kvm_s390_store_adtl_status_unloaded(struct kvm_vcpu *vcpu,
+					unsigned long gpa)
+{
+	/* Only bits 0-53 are used for address formation */
+	if (!(gpa & ~0x3ff))
+		return 0;
+
+	return write_guest_abs(vcpu, gpa & ~0x3ff,
+			       (void *)&vcpu->run->s.regs.vrs, 512);
+}
+
+int kvm_s390_vcpu_store_adtl_status(struct kvm_vcpu *vcpu, unsigned long addr)
+{
+	if (!test_kvm_facility(vcpu->kvm, 129))
+		return 0;
+
+	/*
+	 * The guest VXRS are in the host VXRs due to the lazy
+	 * copying in vcpu load/put. Let's update our copies before we save
+	 * it into the save area.
+	 */
+	save_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
+
+	return kvm_s390_store_adtl_status_unloaded(vcpu, addr);
+}
+
 static void __disable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
 {
 	kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu);

commit cd7b4b61063eb55ab7a5f96523e028c9e0914694
Author: Eric Farman <farman@linux.vnet.ibm.com>
Date:   Thu Feb 12 09:06:34 2015 -0500

    KVM: s390: Add new SIGP order to kernel counters
    
    The new SIGP order Store Additional Status at Address is totally
    handled by user space, but we should still record the occurrence
    of this order in the kernel code.
    
    Signed-off-by: Eric Farman <farman@linux.vnet.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a8fe3ab76d68..c0ae03aa0dff 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -87,6 +87,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_sigp_stop", VCPU_STAT(instruction_sigp_stop) },
 	{ "instruction_sigp_stop_store_status", VCPU_STAT(instruction_sigp_stop_store_status) },
 	{ "instruction_sigp_store_status", VCPU_STAT(instruction_sigp_store_status) },
+	{ "instruction_sigp_store_adtl_status", VCPU_STAT(instruction_sigp_store_adtl_status) },
 	{ "instruction_sigp_set_arch", VCPU_STAT(instruction_sigp_arch) },
 	{ "instruction_sigp_set_prefix", VCPU_STAT(instruction_sigp_prefix) },
 	{ "instruction_sigp_restart", VCPU_STAT(instruction_sigp_restart) },

commit 68c557501b008515cb86c9a36c75f4e82e14a819
Author: Eric Farman <farman@linux.vnet.ibm.com>
Date:   Mon Jun 9 10:57:26 2014 -0400

    KVM: s390: Allocate and save/restore vector registers
    
    Define and allocate space for both the host and guest views of
    the vector registers for a given vcpu.  The 32 vector registers
    occupy 128 bits each (512 bytes total), but architecturally are
    paired with 512 additional bytes of reserved space for future
    expansion.
    
    The kvm_sync_regs structs containing the registers are union'ed
    with 1024 bytes of padding in the common kvm_run struct.  The
    addition of 1024 bytes of new register information clearly exceeds
    the existing union, so an expansion of that padding is required.
    
    When changing environments, we need to appropriately save and
    restore the vector registers viewed by both the host and guest,
    into and out of the sync_regs space.
    
    The floating point registers overlay the upper half of vector
    registers 0-15, so there's a bit of data duplication here that
    needs to be carefully avoided.
    
    Signed-off-by: Eric Farman <farman@linux.vnet.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f3517e716fa4..a8fe3ab76d68 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -185,6 +185,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_COW:
 		r = MACHINE_HAS_ESOP;
 		break;
+	case KVM_CAP_S390_VECTOR_REGISTERS:
+		r = MACHINE_HAS_VX;
+		break;
 	default:
 		r = 0;
 	}
@@ -265,6 +268,10 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		kvm->arch.user_sigp = 1;
 		r = 0;
 		break;
+	case KVM_CAP_S390_VECTOR_REGISTERS:
+		kvm->arch.use_vectors = MACHINE_HAS_VX;
+		r = MACHINE_HAS_VX ? 0 : -EINVAL;
+		break;
 	default:
 		r = -EINVAL;
 		break;
@@ -942,6 +949,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm->arch.css_support = 0;
 	kvm->arch.use_irqchip = 0;
+	kvm->arch.use_vectors = 0;
 	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
@@ -1035,6 +1043,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 				    KVM_SYNC_CRS |
 				    KVM_SYNC_ARCH0 |
 				    KVM_SYNC_PFAULT;
+	if (test_kvm_facility(vcpu->kvm, 129))
+		vcpu->run->kvm_valid_regs |= KVM_SYNC_VRS;
 
 	if (kvm_is_ucontrol(vcpu->kvm))
 		return __kvm_ucontrol_vcpu_init(vcpu);
@@ -1045,10 +1055,18 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	save_fp_ctl(&vcpu->arch.host_fpregs.fpc);
-	save_fp_regs(vcpu->arch.host_fpregs.fprs);
+	if (vcpu->kvm->arch.use_vectors)
+		save_vx_regs((__vector128 *)&vcpu->arch.host_vregs->vrs);
+	else
+		save_fp_regs(vcpu->arch.host_fpregs.fprs);
 	save_access_regs(vcpu->arch.host_acrs);
-	restore_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
-	restore_fp_regs(vcpu->arch.guest_fpregs.fprs);
+	if (vcpu->kvm->arch.use_vectors) {
+		restore_fp_ctl(&vcpu->run->s.regs.fpc);
+		restore_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
+	} else {
+		restore_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+		restore_fp_regs(vcpu->arch.guest_fpregs.fprs);
+	}
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
@@ -1058,11 +1076,19 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
-	save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
-	save_fp_regs(vcpu->arch.guest_fpregs.fprs);
+	if (vcpu->kvm->arch.use_vectors) {
+		save_fp_ctl(&vcpu->run->s.regs.fpc);
+		save_vx_regs((__vector128 *)&vcpu->run->s.regs.vrs);
+	} else {
+		save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+		save_fp_regs(vcpu->arch.guest_fpregs.fprs);
+	}
 	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_fp_ctl(&vcpu->arch.host_fpregs.fpc);
-	restore_fp_regs(vcpu->arch.host_fpregs.fprs);
+	if (vcpu->kvm->arch.use_vectors)
+		restore_vx_regs((__vector128 *)&vcpu->arch.host_vregs->vrs);
+	else
+		restore_fp_regs(vcpu->arch.host_fpregs.fprs);
 	restore_access_regs(vcpu->arch.host_acrs);
 }
 
@@ -1196,6 +1222,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	vcpu->arch.sie_block = &sie_page->sie_block;
 	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
+	vcpu->arch.host_vregs = &sie_page->vregs;
 
 	vcpu->arch.sie_block->icpua = id;
 	if (!kvm_is_ucontrol(kvm)) {

commit 91520f1af8a01d349d19911238fc3dbed3fa58d2
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Fri Feb 27 14:32:11 2015 +0100

    KVM: s390: perform vcpu model setup in a function
    
    The function kvm_s390_vcpu_setup_model() now performs all cpu model realated
    setup tasks for a vcpu. Besides cpuid and ibc initialization, facility list
    assignment takes place during the setup step as well. The model setup has been
    pulled to the begin of vcpu setup to allow kvm facility tests.
    
    There is no need to protect the cpu model setup with a lock since the attributes
    can't be changed anymore as soon the first vcpu is online.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7ac40aa70cf8..f3517e716fa4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1130,6 +1130,15 @@ int kvm_s390_vcpu_setup_cmma(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+static void kvm_s390_vcpu_setup_model(struct kvm_vcpu *vcpu)
+{
+	struct kvm_s390_cpu_model *model = &vcpu->kvm->arch.model;
+
+	vcpu->arch.cpu_id = model->cpu_id;
+	vcpu->arch.sie_block->ibc = model->ibc;
+	vcpu->arch.sie_block->fac = (int) (long) model->fac->list;
+}
+
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	int rc = 0;
@@ -1138,6 +1147,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 						    CPUSTAT_SM |
 						    CPUSTAT_STOPPED |
 						    CPUSTAT_GED);
+	kvm_s390_vcpu_setup_model(vcpu);
+
 	vcpu->arch.sie_block->ecb   = 6;
 	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))
 		vcpu->arch.sie_block->ecb |= 0x10;
@@ -1158,11 +1169,6 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 
-	mutex_lock(&vcpu->kvm->lock);
-	vcpu->arch.cpu_id = vcpu->kvm->arch.model.cpu_id;
-	vcpu->arch.sie_block->ibc = vcpu->kvm->arch.model.ibc;
-	mutex_unlock(&vcpu->kvm->lock);
-
 	kvm_s390_vcpu_crypto_setup(vcpu);
 
 	return rc;
@@ -1205,7 +1211,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
 		set_bit(63 - id, (unsigned long *) &kvm->arch.sca->mcn);
 	}
-	vcpu->arch.sie_block->fac = (int) (long) kvm->arch.model.fac->list;
 
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;

commit 492d8642eaefbd47f6fb0e8265f058c02720e5c8
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Tue Feb 10 16:11:01 2015 +0100

    KVM: s390: Forward PSW to next instruction for addressing exceptions
    
    When the SIE exited by a DAT access exceptions which we can not
    resolve, the guest tried to access a page which is out of bounds
    and can not be paged-in. In this case we have to signal the bad
    access by injecting an address exception. However, address exceptions
    are either suppressing or terminating, i.e. the PSW has to point to
    the next instruction when the exception is delivered. Since the
    originating DAT access exception is nullifying, the PSW still
    points to the offending instruction instead, so we've got to forward
    the PSW to the next instruction.
    Having fixed this issue, we can now also enable the TPROT
    interpretation facility again which had been disabled because
    of this problem.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f6579cfde2df..7ac40aa70cf8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1148,8 +1148,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= 1;
 	if (sclp_has_sigpif())
 		vcpu->arch.sie_block->eca |= 0x10000000U;
-	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE |
-				      ICTL_TPROT;
+	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 
 	if (kvm_s390_cmma_enabled(vcpu->kvm)) {
 		rc = kvm_s390_vcpu_setup_cmma(vcpu);
@@ -1726,6 +1725,31 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+static int vcpu_post_run_fault_in_sie(struct kvm_vcpu *vcpu)
+{
+	psw_t *psw = &vcpu->arch.sie_block->gpsw;
+	u8 opcode;
+	int rc;
+
+	VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
+	trace_kvm_s390_sie_fault(vcpu);
+
+	/*
+	 * We want to inject an addressing exception, which is defined as a
+	 * suppressing or terminating exception. However, since we came here
+	 * by a DAT access exception, the PSW still points to the faulting
+	 * instruction since DAT exceptions are nullifying. So we've got
+	 * to look up the current opcode to get the length of the instruction
+	 * to be able to forward the PSW.
+	 */
+	rc = read_guest(vcpu, psw->addr, &opcode, 1);
+	if (rc)
+		return kvm_s390_inject_prog_cond(vcpu, rc);
+	psw->addr = __rewind_psw(*psw, -insn_length(opcode));
+
+	return kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
+}
+
 static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 {
 	int rc = -1;
@@ -1757,11 +1781,8 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 		}
 	}
 
-	if (rc == -1) {
-		VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
-		trace_kvm_s390_sie_fault(vcpu);
-		rc = kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
-	}
+	if (rc == -1)
+		rc = vcpu_post_run_fault_in_sie(vcpu);
 
 	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
 

commit fb5bf93f84c277546473be35543ed7890f6e6742
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Fri Feb 27 14:25:10 2015 +0100

    KVM: s390: non-LPAR case obsolete during facilities mask init
    
    With patch "include guest facilities in kvm facility test" it is no
    longer necessary to have special handling for the non-LPAR case.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5a02be4628f1..f6579cfde2df 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -902,24 +902,9 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.model.fac)
 		goto out_nofac;
 
+	/* Populate the facility mask initially. */
 	memcpy(kvm->arch.model.fac->mask, S390_lowcore.stfle_fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
-
-	/*
-	 * If this KVM host runs *not* in a LPAR, relax the facility bits
-	 * of the kvm facility mask by all missing facilities. This will allow
-	 * to determine the right CPU model by means of the remaining facilities.
-	 * Live guest migration must prohibit the migration of KVMs running in
-	 * a LPAR to non LPAR hosts.
-	 */
-	if (!MACHINE_IS_LPAR)
-		for (i = 0; i < kvm_s390_fac_list_mask_size(); i++)
-			kvm_s390_fac_list_mask[i] &= kvm->arch.model.fac->mask[i];
-
-	/*
-	 * Apply the kvm facility mask to limit the kvm supported/tolerated
-	 * facility list.
-	 */
 	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
 		if (i < kvm_s390_fac_list_mask_size())
 			kvm->arch.model.fac->mask[i] &= kvm_s390_fac_list_mask[i];

commit 981467c930bdfa4be59acbbc9f3a80eb9e3167a8
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Tue Feb 24 13:51:04 2015 +0100

    KVM: s390: include guest facilities in kvm facility test
    
    Most facility related decisions in KVM have to take into account:
    
    - the facilities offered by the underlying run container (LPAR/VM)
    - the facilities supported by the KVM code itself
    - the facilities requested by a guest VM
    
    This patch adds the KVM driver requested facilities to the test routine.
    
    It additionally renames struct s390_model_fac to kvm_s390_fac and its field
    names to be more meaningful.
    
    The semantics of the facilities stored in the KVM architecture structure
    is changed. The address arch.model.fac->list now points to the guest
    facility list and arch.model.fac->mask points to the KVM facility mask.
    
    This patch fixes the behaviour of KVM for some facilities for guests
    that ignore the guest visible facility bits, e.g. guests could use
    transactional memory intructions on hosts supporting them even if the
    chosen cpu model would not offer them.
    
    The userspace interface is not affected by this change.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 76894c8db4d7..5a02be4628f1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -522,7 +522,7 @@ static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 		memcpy(&kvm->arch.model.cpu_id, &proc->cpuid,
 		       sizeof(struct cpuid));
 		kvm->arch.model.ibc = proc->ibc;
-		memcpy(kvm->arch.model.fac->kvm, proc->fac_list,
+		memcpy(kvm->arch.model.fac->list, proc->fac_list,
 		       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	} else
 		ret = -EFAULT;
@@ -556,7 +556,7 @@ static int kvm_s390_get_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 	}
 	memcpy(&proc->cpuid, &kvm->arch.model.cpu_id, sizeof(struct cpuid));
 	proc->ibc = kvm->arch.model.ibc;
-	memcpy(&proc->fac_list, kvm->arch.model.fac->kvm, S390_ARCH_FAC_LIST_SIZE_BYTE);
+	memcpy(&proc->fac_list, kvm->arch.model.fac->list, S390_ARCH_FAC_LIST_SIZE_BYTE);
 	if (copy_to_user((void __user *)attr->addr, proc, sizeof(*proc)))
 		ret = -EFAULT;
 	kfree(proc);
@@ -576,8 +576,8 @@ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 	}
 	get_cpu_id((struct cpuid *) &mach->cpuid);
 	mach->ibc = sclp_get_ibc();
-	memcpy(&mach->fac_mask, kvm_s390_fac_list_mask,
-	       kvm_s390_fac_list_mask_size() * sizeof(u64));
+	memcpy(&mach->fac_mask, kvm->arch.model.fac->mask,
+	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	if (copy_to_user((void __user *)attr->addr, mach, sizeof(*mach)))
@@ -893,16 +893,16 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	/*
 	 * The architectural maximum amount of facilities is 16 kbit. To store
 	 * this amount, 2 kbyte of memory is required. Thus we need a full
-	 * page to hold the active copy (arch.model.fac->sie) and the current
-	 * facilities set (arch.model.fac->kvm). Its address size has to be
+	 * page to hold the guest facility list (arch.model.fac->list) and the
+	 * facility mask (arch.model.fac->mask). Its address size has to be
 	 * 31 bits and word aligned.
 	 */
 	kvm->arch.model.fac =
-		(struct s390_model_fac *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
+		(struct kvm_s390_fac *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 	if (!kvm->arch.model.fac)
 		goto out_nofac;
 
-	memcpy(kvm->arch.model.fac->kvm, S390_lowcore.stfle_fac_list,
+	memcpy(kvm->arch.model.fac->mask, S390_lowcore.stfle_fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
 	/*
@@ -914,7 +914,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	 */
 	if (!MACHINE_IS_LPAR)
 		for (i = 0; i < kvm_s390_fac_list_mask_size(); i++)
-			kvm_s390_fac_list_mask[i] &= kvm->arch.model.fac->kvm[i];
+			kvm_s390_fac_list_mask[i] &= kvm->arch.model.fac->mask[i];
 
 	/*
 	 * Apply the kvm facility mask to limit the kvm supported/tolerated
@@ -922,11 +922,15 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	 */
 	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
 		if (i < kvm_s390_fac_list_mask_size())
-			kvm->arch.model.fac->kvm[i] &= kvm_s390_fac_list_mask[i];
+			kvm->arch.model.fac->mask[i] &= kvm_s390_fac_list_mask[i];
 		else
-			kvm->arch.model.fac->kvm[i] = 0UL;
+			kvm->arch.model.fac->mask[i] = 0UL;
 	}
 
+	/* Populate the facility list initially. */
+	memcpy(kvm->arch.model.fac->list, kvm->arch.model.fac->mask,
+	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+
 	kvm_s390_get_cpu_id(&kvm->arch.model.cpu_id);
 	kvm->arch.model.ibc = sclp_get_ibc() & 0x0fff;
 
@@ -1172,8 +1176,6 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 
 	mutex_lock(&vcpu->kvm->lock);
 	vcpu->arch.cpu_id = vcpu->kvm->arch.model.cpu_id;
-	memcpy(vcpu->kvm->arch.model.fac->sie, vcpu->kvm->arch.model.fac->kvm,
-	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	vcpu->arch.sie_block->ibc = vcpu->kvm->arch.model.ibc;
 	mutex_unlock(&vcpu->kvm->lock);
 
@@ -1219,7 +1221,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
 		set_bit(63 - id, (unsigned long *) &kvm->arch.sca->mcn);
 	}
-	vcpu->arch.sie_block->fac = (int) (long) kvm->arch.model.fac->sie;
+	vcpu->arch.sie_block->fac = (int) (long) kvm->arch.model.fac->list;
 
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;

commit 94422ee880afc4af050bac172ea39af8e2130034
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Thu Feb 26 12:12:40 2015 +0100

    KVM: s390: fix in memory copy of facility lists
    
    The facility lists were not fully copied.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 18965f91d39e..76894c8db4d7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -579,7 +579,7 @@ static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 	memcpy(&mach->fac_mask, kvm_s390_fac_list_mask,
 	       kvm_s390_fac_list_mask_size() * sizeof(u64));
 	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
-	       S390_ARCH_FAC_LIST_SIZE_U64);
+	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 	if (copy_to_user((void __user *)attr->addr, mach, sizeof(*mach)))
 		ret = -EFAULT;
 	kfree(mach);
@@ -903,7 +903,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		goto out_nofac;
 
 	memcpy(kvm->arch.model.fac->kvm, S390_lowcore.stfle_fac_list,
-	       S390_ARCH_FAC_LIST_SIZE_U64);
+	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
 	/*
 	 * If this KVM host runs *not* in a LPAR, relax the facility bits

commit 86044c8c14b618b11558d3cba96aa0548c81274d
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Feb 26 13:53:47 2015 +0100

    KVM: s390/cpacf: Fix kernel bug under z/VM
    
    Under z/VM PQAP might trigger an operation exception if no crypto cards
    are defined via APVIRTUAL or APDEDICATED.
    
    [  386.098666] Kernel BUG at 0000000000135c56 [verbose debug info unavailable]
    [  386.098693] illegal operation: 0001 ilc:2 [#1] SMP
    [...]
    [  386.098751] Krnl PSW : 0704c00180000000 0000000000135c56 (kvm_s390_apxa_installed+0x46/0x98)
    [...]
    [  386.098804]  [<000000000013627c>] kvm_arch_init_vm+0x29c/0x358
    [  386.098806]  [<000000000012d008>] kvm_dev_ioctl+0xc0/0x460
    [  386.098809]  [<00000000002c639a>] do_vfs_ioctl+0x332/0x508
    [  386.098811]  [<00000000002c660e>] SyS_ioctl+0x9e/0xb0
    [  386.098814]  [<000000000070476a>] system_call+0xd6/0x258
    [  386.098815]  [<000003fffc7400a2>] 0x3fffc7400a2
    
    Lets add an extable entry and provide a zeroed config in that case.
    
    Reported-by: Stefan Zimmermann <stzi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Tested-by: Stefan Zimmermann <stzi@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b4d2030c22eb..18965f91d39e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -778,15 +778,18 @@ long kvm_arch_vm_ioctl(struct file *filp,
 static int kvm_s390_query_ap_config(u8 *config)
 {
 	u32 fcn_code = 0x04000000UL;
-	u32 cc;
+	u32 cc = 0;
 
+	memset(config, 0, 128);
 	asm volatile(
 		"lgr 0,%1\n"
 		"lgr 2,%2\n"
 		".long 0xb2af0000\n"		/* PQAP(QCI) */
-		"ipm %0\n"
+		"0: ipm %0\n"
 		"srl %0,28\n"
-		: "=r" (cc)
+		"1:\n"
+		EX_TABLE(0b, 1b)
+		: "+r" (cc)
 		: "r" (fcn_code), "r" (config)
 		: "cc", "0", "2", "memory"
 	);

commit ed6f76b464ab53e59adc7ec6cc8428d3d6ade1a5
Author: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
Date:   Tue Feb 24 14:06:57 2015 -0500

    KVM: s390/cpacf: Enable key wrapping by default
    
    z/VM and LPAR enable key wrapping by default, lets do the same on KVM.
    
    Signed-off-by: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0c3623927563..b4d2030c22eb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -839,9 +839,13 @@ static int kvm_s390_crypto_init(struct kvm *kvm)
 
 	kvm_s390_set_crycb_format(kvm);
 
-	/* Disable AES/DEA protected key functions by default */
-	kvm->arch.crypto.aes_kw = 0;
-	kvm->arch.crypto.dea_kw = 0;
+	/* Enable AES/DEA protected key functions by default */
+	kvm->arch.crypto.aes_kw = 1;
+	kvm->arch.crypto.dea_kw = 1;
+	get_random_bytes(kvm->arch.crypto.crycb->aes_wrapping_key_mask,
+			 sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
+	get_random_bytes(kvm->arch.crypto.crycb->dea_wrapping_key_mask,
+			 sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 
 	return 0;
 }

commit 658b6eda2042c0fe0e3f8acd7ffd11fc6f280119
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Mon Feb 2 15:49:35 2015 +0100

    KVM: s390: add cpu model support
    
    This patch enables cpu model support in kvm/s390 via the vm attribute
    interface.
    
    During KVM initialization, the host properties cpuid, IBC value and the
    facility list are stored in the architecture specific cpu model structure.
    
    During vcpu setup, these properties are taken to initialize the related SIE
    state. This mechanism allows to adjust the properties from user space and thus
    to implement different selectable cpu models.
    
    This patch uses the IBC functionality to block instructions that have not
    been implemented at the requested CPU type and GA level compared to the
    full host capability.
    
    Userspace has to initialize the cpu model before vcpu creation. A cpu model
    change of running vcpus is not possible.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8c538a1a23c1..0c3623927563 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -502,6 +502,106 @@ static int kvm_s390_get_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 	return ret;
 }
 
+static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_cpu_processor *proc;
+	int ret = 0;
+
+	mutex_lock(&kvm->lock);
+	if (atomic_read(&kvm->online_vcpus)) {
+		ret = -EBUSY;
+		goto out;
+	}
+	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
+	if (!proc) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	if (!copy_from_user(proc, (void __user *)attr->addr,
+			    sizeof(*proc))) {
+		memcpy(&kvm->arch.model.cpu_id, &proc->cpuid,
+		       sizeof(struct cpuid));
+		kvm->arch.model.ibc = proc->ibc;
+		memcpy(kvm->arch.model.fac->kvm, proc->fac_list,
+		       S390_ARCH_FAC_LIST_SIZE_BYTE);
+	} else
+		ret = -EFAULT;
+	kfree(proc);
+out:
+	mutex_unlock(&kvm->lock);
+	return ret;
+}
+
+static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret = -ENXIO;
+
+	switch (attr->attr) {
+	case KVM_S390_VM_CPU_PROCESSOR:
+		ret = kvm_s390_set_processor(kvm, attr);
+		break;
+	}
+	return ret;
+}
+
+static int kvm_s390_get_processor(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_cpu_processor *proc;
+	int ret = 0;
+
+	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
+	if (!proc) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	memcpy(&proc->cpuid, &kvm->arch.model.cpu_id, sizeof(struct cpuid));
+	proc->ibc = kvm->arch.model.ibc;
+	memcpy(&proc->fac_list, kvm->arch.model.fac->kvm, S390_ARCH_FAC_LIST_SIZE_BYTE);
+	if (copy_to_user((void __user *)attr->addr, proc, sizeof(*proc)))
+		ret = -EFAULT;
+	kfree(proc);
+out:
+	return ret;
+}
+
+static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	struct kvm_s390_vm_cpu_machine *mach;
+	int ret = 0;
+
+	mach = kzalloc(sizeof(*mach), GFP_KERNEL);
+	if (!mach) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	get_cpu_id((struct cpuid *) &mach->cpuid);
+	mach->ibc = sclp_get_ibc();
+	memcpy(&mach->fac_mask, kvm_s390_fac_list_mask,
+	       kvm_s390_fac_list_mask_size() * sizeof(u64));
+	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
+	       S390_ARCH_FAC_LIST_SIZE_U64);
+	if (copy_to_user((void __user *)attr->addr, mach, sizeof(*mach)))
+		ret = -EFAULT;
+	kfree(mach);
+out:
+	return ret;
+}
+
+static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret = -ENXIO;
+
+	switch (attr->attr) {
+	case KVM_S390_VM_CPU_PROCESSOR:
+		ret = kvm_s390_get_processor(kvm, attr);
+		break;
+	case KVM_S390_VM_CPU_MACHINE:
+		ret = kvm_s390_get_machine(kvm, attr);
+		break;
+	}
+	return ret;
+}
+
 static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret;
@@ -513,6 +613,9 @@ static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_TOD:
 		ret = kvm_s390_set_tod(kvm, attr);
 		break;
+	case KVM_S390_VM_CPU_MODEL:
+		ret = kvm_s390_set_cpu_model(kvm, attr);
+		break;
 	case KVM_S390_VM_CRYPTO:
 		ret = kvm_s390_vm_set_crypto(kvm, attr);
 		break;
@@ -535,6 +638,9 @@ static int kvm_s390_vm_get_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_TOD:
 		ret = kvm_s390_get_tod(kvm, attr);
 		break;
+	case KVM_S390_VM_CPU_MODEL:
+		ret = kvm_s390_get_cpu_model(kvm, attr);
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -571,6 +677,17 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 			break;
 		}
 		break;
+	case KVM_S390_VM_CPU_MODEL:
+		switch (attr->attr) {
+		case KVM_S390_VM_CPU_PROCESSOR:
+		case KVM_S390_VM_CPU_MACHINE:
+			ret = 0;
+			break;
+		default:
+			ret = -ENXIO;
+			break;
+		}
+		break;
 	case KVM_S390_VM_CRYPTO:
 		switch (attr->attr) {
 		case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
@@ -781,6 +898,17 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	memcpy(kvm->arch.model.fac->kvm, S390_lowcore.stfle_fac_list,
 	       S390_ARCH_FAC_LIST_SIZE_U64);
 
+	/*
+	 * If this KVM host runs *not* in a LPAR, relax the facility bits
+	 * of the kvm facility mask by all missing facilities. This will allow
+	 * to determine the right CPU model by means of the remaining facilities.
+	 * Live guest migration must prohibit the migration of KVMs running in
+	 * a LPAR to non LPAR hosts.
+	 */
+	if (!MACHINE_IS_LPAR)
+		for (i = 0; i < kvm_s390_fac_list_mask_size(); i++)
+			kvm_s390_fac_list_mask[i] &= kvm->arch.model.fac->kvm[i];
+
 	/*
 	 * Apply the kvm facility mask to limit the kvm supported/tolerated
 	 * facility list.
@@ -793,6 +921,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	}
 
 	kvm_s390_get_cpu_id(&kvm->arch.model.cpu_id);
+	kvm->arch.model.ibc = sclp_get_ibc() & 0x0fff;
 
 	if (kvm_s390_crypto_init(kvm) < 0)
 		goto out_crypto;
@@ -1034,9 +1163,12 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 
+	mutex_lock(&vcpu->kvm->lock);
 	vcpu->arch.cpu_id = vcpu->kvm->arch.model.cpu_id;
 	memcpy(vcpu->kvm->arch.model.fac->sie, vcpu->kvm->arch.model.fac->kvm,
 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+	vcpu->arch.sie_block->ibc = vcpu->kvm->arch.model.ibc;
+	mutex_unlock(&vcpu->kvm->lock);
 
 	kvm_s390_vcpu_crypto_setup(vcpu);
 

commit 9d8d578605b4fca37bd2230bbacb3ad0ee48e7e4
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Mon Feb 2 15:42:51 2015 +0100

    KVM: s390: use facilities and cpu_id per KVM
    
    The patch introduces facilities and cpu_ids per virtual machine.
    Different virtual machines may want to expose different facilities and
    cpu ids to the guest, so let's make them per-vm instead of global.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index deac47378f77..8c538a1a23c1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -30,7 +30,6 @@
 #include <asm/pgtable.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>
-#include <asm/facility.h>
 #include <asm/sclp.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
@@ -100,15 +99,20 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ NULL }
 };
 
-unsigned long *vfacilities;
-static struct gmap_notifier gmap_notifier;
+/* upper facilities limit for kvm */
+unsigned long kvm_s390_fac_list_mask[] = {
+	0xff82fffbf4fc2000UL,
+	0x005c000000000000UL,
+};
 
-/* test availability of vfacility */
-int test_vfacility(unsigned long nr)
+unsigned long kvm_s390_fac_list_mask_size(void)
 {
-	return __test_facility(nr, (void *) vfacilities);
+	BUILD_BUG_ON(ARRAY_SIZE(kvm_s390_fac_list_mask) > S390_ARCH_FAC_MASK_SIZE_U64);
+	return ARRAY_SIZE(kvm_s390_fac_list_mask);
 }
 
+static struct gmap_notifier gmap_notifier;
+
 /* Section: not file related */
 int kvm_arch_hardware_enable(void)
 {
@@ -351,7 +355,7 @@ static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 	struct kvm_vcpu *vcpu;
 	int i;
 
-	if (!test_vfacility(76))
+	if (!test_kvm_facility(kvm, 76))
 		return -EINVAL;
 
 	mutex_lock(&kvm->lock);
@@ -700,9 +704,15 @@ static void kvm_s390_set_crycb_format(struct kvm *kvm)
 		kvm->arch.crypto.crycbd |= CRYCB_FORMAT1;
 }
 
+static void kvm_s390_get_cpu_id(struct cpuid *cpu_id)
+{
+	get_cpu_id(cpu_id);
+	cpu_id->version = 0xff;
+}
+
 static int kvm_s390_crypto_init(struct kvm *kvm)
 {
-	if (!test_vfacility(76))
+	if (!test_kvm_facility(kvm, 76))
 		return 0;
 
 	kvm->arch.crypto.crycb = kzalloc(sizeof(*kvm->arch.crypto.crycb),
@@ -721,7 +731,7 @@ static int kvm_s390_crypto_init(struct kvm *kvm)
 
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
-	int rc;
+	int i, rc;
 	char debug_name[16];
 	static unsigned long sca_offset;
 
@@ -756,6 +766,34 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.dbf)
 		goto out_nodbf;
 
+	/*
+	 * The architectural maximum amount of facilities is 16 kbit. To store
+	 * this amount, 2 kbyte of memory is required. Thus we need a full
+	 * page to hold the active copy (arch.model.fac->sie) and the current
+	 * facilities set (arch.model.fac->kvm). Its address size has to be
+	 * 31 bits and word aligned.
+	 */
+	kvm->arch.model.fac =
+		(struct s390_model_fac *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
+	if (!kvm->arch.model.fac)
+		goto out_nofac;
+
+	memcpy(kvm->arch.model.fac->kvm, S390_lowcore.stfle_fac_list,
+	       S390_ARCH_FAC_LIST_SIZE_U64);
+
+	/*
+	 * Apply the kvm facility mask to limit the kvm supported/tolerated
+	 * facility list.
+	 */
+	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
+		if (i < kvm_s390_fac_list_mask_size())
+			kvm->arch.model.fac->kvm[i] &= kvm_s390_fac_list_mask[i];
+		else
+			kvm->arch.model.fac->kvm[i] = 0UL;
+	}
+
+	kvm_s390_get_cpu_id(&kvm->arch.model.cpu_id);
+
 	if (kvm_s390_crypto_init(kvm) < 0)
 		goto out_crypto;
 
@@ -787,6 +825,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 out_nogmap:
 	kfree(kvm->arch.crypto.crycb);
 out_crypto:
+	free_page((unsigned long)kvm->arch.model.fac);
+out_nofac:
 	debug_unregister(kvm->arch.dbf);
 out_nodbf:
 	free_page((unsigned long)(kvm->arch.sca));
@@ -839,6 +879,7 @@ static void kvm_free_vcpus(struct kvm *kvm)
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvm_free_vcpus(kvm);
+	free_page((unsigned long)kvm->arch.model.fac);
 	free_page((unsigned long)(kvm->arch.sca));
 	debug_unregister(kvm->arch.dbf);
 	kfree(kvm->arch.crypto.crycb);
@@ -934,7 +975,7 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
 {
-	if (!test_vfacility(76))
+	if (!test_kvm_facility(vcpu->kvm, 76))
 		return;
 
 	vcpu->arch.sie_block->ecb3 &= ~(ECB3_AES | ECB3_DEA);
@@ -973,7 +1014,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 						    CPUSTAT_STOPPED |
 						    CPUSTAT_GED);
 	vcpu->arch.sie_block->ecb   = 6;
-	if (test_vfacility(50) && test_vfacility(73))
+	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))
 		vcpu->arch.sie_block->ecb |= 0x10;
 
 	vcpu->arch.sie_block->ecb2  = 8;
@@ -982,7 +1023,6 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->eca |= 1;
 	if (sclp_has_sigpif())
 		vcpu->arch.sie_block->eca |= 0x10000000U;
-	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE |
 				      ICTL_TPROT;
 
@@ -993,8 +1033,10 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	}
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
-	get_cpu_id(&vcpu->arch.cpu_id);
-	vcpu->arch.cpu_id.version = 0xff;
+
+	vcpu->arch.cpu_id = vcpu->kvm->arch.model.cpu_id;
+	memcpy(vcpu->kvm->arch.model.fac->sie, vcpu->kvm->arch.model.fac->kvm,
+	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 
 	kvm_s390_vcpu_crypto_setup(vcpu);
 
@@ -1038,6 +1080,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
 		set_bit(63 - id, (unsigned long *) &kvm->arch.sca->mcn);
 	}
+	vcpu->arch.sie_block->fac = (int) (long) kvm->arch.model.fac->sie;
 
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
@@ -2103,30 +2146,11 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 
 static int __init kvm_s390_init(void)
 {
-	int ret;
-	ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
-	if (ret)
-		return ret;
-
-	/*
-	 * guests can ask for up to 255+1 double words, we need a full page
-	 * to hold the maximum amount of facilities. On the other hand, we
-	 * only set facilities that are known to work in KVM.
-	 */
-	vfacilities = (unsigned long *) get_zeroed_page(GFP_KERNEL|GFP_DMA);
-	if (!vfacilities) {
-		kvm_exit();
-		return -ENOMEM;
-	}
-	memcpy(vfacilities, S390_lowcore.stfle_fac_list, 16);
-	vfacilities[0] &= 0xff82fffbf4fc2000UL;
-	vfacilities[1] &= 0x005c000000000000UL;
-	return 0;
+	return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
 }
 
 static void __exit kvm_s390_exit(void)
 {
-	free_page((unsigned long) vfacilities);
 	kvm_exit();
 }
 

commit 45c9b47c5883d02abab6c7c7788e3d97a2f158e1
Author: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
Date:   Tue Jan 13 11:33:26 2015 -0500

    KVM: s390/CPACF: Choose crypto control block format
    
    We need to specify a different format for the crypto control block
    depending on whether the APXA facility is installed or not. Let's
    test for it by executing the PQAP(QCI) function and use either a
    format-1 or a format-2 crypto control block accordingly. This is a
    host only change for z13 and does not affect the guest view.
    
    Signed-off-by: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3acf08ba88e4..deac47378f77 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -654,6 +654,52 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
+static int kvm_s390_query_ap_config(u8 *config)
+{
+	u32 fcn_code = 0x04000000UL;
+	u32 cc;
+
+	asm volatile(
+		"lgr 0,%1\n"
+		"lgr 2,%2\n"
+		".long 0xb2af0000\n"		/* PQAP(QCI) */
+		"ipm %0\n"
+		"srl %0,28\n"
+		: "=r" (cc)
+		: "r" (fcn_code), "r" (config)
+		: "cc", "0", "2", "memory"
+	);
+
+	return cc;
+}
+
+static int kvm_s390_apxa_installed(void)
+{
+	u8 config[128];
+	int cc;
+
+	if (test_facility(2) && test_facility(12)) {
+		cc = kvm_s390_query_ap_config(config);
+
+		if (cc)
+			pr_err("PQAP(QCI) failed with cc=%d", cc);
+		else
+			return config[0] & 0x40;
+	}
+
+	return 0;
+}
+
+static void kvm_s390_set_crycb_format(struct kvm *kvm)
+{
+	kvm->arch.crypto.crycbd = (__u32)(unsigned long) kvm->arch.crypto.crycb;
+
+	if (kvm_s390_apxa_installed())
+		kvm->arch.crypto.crycbd |= CRYCB_FORMAT2;
+	else
+		kvm->arch.crypto.crycbd |= CRYCB_FORMAT1;
+}
+
 static int kvm_s390_crypto_init(struct kvm *kvm)
 {
 	if (!test_vfacility(76))
@@ -664,8 +710,7 @@ static int kvm_s390_crypto_init(struct kvm *kvm)
 	if (!kvm->arch.crypto.crycb)
 		return -ENOMEM;
 
-	kvm->arch.crypto.crycbd = (__u32) (unsigned long) kvm->arch.crypto.crycb |
-				  CRYCB_FORMAT1;
+	kvm_s390_set_crycb_format(kvm);
 
 	/* Disable AES/DEA protected key functions by default */
 	kvm->arch.crypto.aes_kw = 0;

commit c23f397cc4e440742b2b27690694c9346f638800
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Jan 29 14:09:54 2015 +0100

    KVM: s390: reenable LPP facility
    
    commit 7be81a46695d ("KVM: s390/facilities: allow TOD-CLOCK steering
    facility bit") accidentially disabled the "load program parameter"
    facility bit during rebase for upstream submission (my fault).
    
    Re-add that bit.
    
    As this is only for a performance measurement helper instruction
    (used by KVM itself) cc stable is not necessary see
    http://www-01.ibm.com/support/docview.wss?uid=isg26fcd1cc32246f4c8852574ce0044734a
    (SA23-2260 The Load-Program-Parameter and CPU-Measurement Facilities)
    for details about LPP and its usecase.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Fixes: 7be81a46695d ("KVM: s390/facilities: allow TOD-CLOCK steering")

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1dbab2340a66..3acf08ba88e4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -2074,7 +2074,7 @@ static int __init kvm_s390_init(void)
 		return -ENOMEM;
 	}
 	memcpy(vfacilities, S390_lowcore.stfle_fac_list, 16);
-	vfacilities[0] &= 0xff82fffbf47c2000UL;
+	vfacilities[0] &= 0xff82fffbf4fc2000UL;
 	vfacilities[1] &= 0x005c000000000000UL;
 	return 0;
 }

commit f7819512996361280b86259222456fcf15aad926
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Feb 4 18:20:58 2015 +0100

    kvm: add halt_poll_ns module parameter
    
    This patch introduces a new module parameter for the KVM module; when it
    is present, KVM attempts a bit of polling on every HLT before scheduling
    itself out via kvm_vcpu_block.
    
    This parameter helps a lot for latency-bound workloads---in particular
    I tested it with O_DSYNC writes with a battery-backed disk in the host.
    In this case, writes are fast (because the data doesn't have to go all
    the way to the platters) but they cannot be merged by either the host or
    the guest.  KVM's performance here is usually around 30% of bare metal,
    or 50% if you use cache=directsync or cache=writethrough (these
    parameters avoid that the guest sends pointless flush requests, and
    at the same time they are not slow because of the battery-backed cache).
    The bad performance happens because on every halt the host CPU decides
    to halt itself too.  When the interrupt comes, the vCPU thread is then
    migrated to a new physical CPU, and in general the latency is horrible
    because the vCPU thread has to be scheduled back in.
    
    With this patch performance reaches 60-65% of bare metal and, more
    important, 99% of what you get if you use idle=poll in the guest.  This
    means that the tunable gets rid of this particular bottleneck, and more
    work can be done to improve performance in the kernel or QEMU.
    
    Of course there is some price to pay; every time an otherwise idle vCPUs
    is interrupted by an interrupt, it will poll unnecessarily and thus
    impose a little load on the host.  The above results were obtained with
    a mostly random value of the parameter (500000), and the load was around
    1.5-2.5% CPU usage on one of the host's core for each idle guest vCPU.
    
    The patch also adds a new stat, /sys/kernel/debug/kvm/halt_successful_poll,
    that can be used to tune the parameter.  It counts how many HLT
    instructions received an interrupt during the polling period; each
    successful poll avoids that Linux schedules the VCPU thread out and back
    in, and may also avoid a likely trip to C1 and back for the physical CPU.
    
    While the VM is idle, a Linux 4 VCPU VM halts around 10 times per second.
    Of these halts, almost all are failed polls.  During the benchmark,
    instead, basically all halts end within the polling period, except a more
    or less constant stream of 50 per second coming from vCPUs that are not
    running the benchmark.  The wasted time is thus very low.  Things may
    be slightly different for Windows VMs, which have a ~10 ms timer tick.
    
    The effect is also visible on Marcelo's recently-introduced latency
    test for the TSC deadline timer.  Though of course a non-RT kernel has
    awful latency bounds, the latency of the timer is around 8000-10000 clock
    cycles compared to 20000-120000 without setting halt_poll_ns.  For the TSC
    deadline timer, thus, the effect is both a smaller average latency and
    a smaller variance.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b2371c0fd1f8..1dbab2340a66 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -51,6 +51,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_instruction", VCPU_STAT(exit_instruction) },
 	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
+	{ "halt_successful_poll", VCPU_STAT(halt_successful_poll) },
 	{ "halt_wakeup", VCPU_STAT(halt_wakeup) },
 	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },

commit a374e892c3421f81a71e85d5a8c526959221f279
Author: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
Date:   Wed Sep 3 10:13:53 2014 +0200

    KVM: s390/cpacf: Enable/disable protected key functions for kvm guest
    
    Created new KVM device attributes for indicating whether the AES and
    DES/TDES protected key functions are available for programs running
    on the KVM guest.  The attributes are used to set up the controls in
    the guest SIE block that specify whether programs running on the
    guest will be given access to the protected key functions available
    on the s390 hardware.
    
    Signed-off-by: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [split MSA4/protected key into two patches]

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9cf899e9a5d4..b2371c0fd1f8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -22,6 +22,7 @@
 #include <linux/kvm.h>
 #include <linux/kvm_host.h>
 #include <linux/module.h>
+#include <linux/random.h>
 #include <linux/slab.h>
 #include <linux/timer.h>
 #include <asm/asm-offsets.h>
@@ -342,6 +343,53 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 	return ret;
 }
 
+static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu);
+
+static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	struct kvm_vcpu *vcpu;
+	int i;
+
+	if (!test_vfacility(76))
+		return -EINVAL;
+
+	mutex_lock(&kvm->lock);
+	switch (attr->attr) {
+	case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
+		get_random_bytes(
+			kvm->arch.crypto.crycb->aes_wrapping_key_mask,
+			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
+		kvm->arch.crypto.aes_kw = 1;
+		break;
+	case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
+		get_random_bytes(
+			kvm->arch.crypto.crycb->dea_wrapping_key_mask,
+			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
+		kvm->arch.crypto.dea_kw = 1;
+		break;
+	case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
+		kvm->arch.crypto.aes_kw = 0;
+		memset(kvm->arch.crypto.crycb->aes_wrapping_key_mask, 0,
+			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
+		break;
+	case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
+		kvm->arch.crypto.dea_kw = 0;
+		memset(kvm->arch.crypto.crycb->dea_wrapping_key_mask, 0,
+			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
+		break;
+	default:
+		mutex_unlock(&kvm->lock);
+		return -ENXIO;
+	}
+
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		kvm_s390_vcpu_crypto_setup(vcpu);
+		exit_sie(vcpu);
+	}
+	mutex_unlock(&kvm->lock);
+	return 0;
+}
+
 static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	u8 gtod_high;
@@ -460,6 +508,9 @@ static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_TOD:
 		ret = kvm_s390_set_tod(kvm, attr);
 		break;
+	case KVM_S390_VM_CRYPTO:
+		ret = kvm_s390_vm_set_crypto(kvm, attr);
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -515,6 +566,19 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 			break;
 		}
 		break;
+	case KVM_S390_VM_CRYPTO:
+		switch (attr->attr) {
+		case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
+		case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
+		case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
+		case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
+			ret = 0;
+			break;
+		default:
+			ret = -ENXIO;
+			break;
+		}
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -602,6 +666,10 @@ static int kvm_s390_crypto_init(struct kvm *kvm)
 	kvm->arch.crypto.crycbd = (__u32) (unsigned long) kvm->arch.crypto.crycb |
 				  CRYCB_FORMAT1;
 
+	/* Disable AES/DEA protected key functions by default */
+	kvm->arch.crypto.aes_kw = 0;
+	kvm->arch.crypto.dea_kw = 0;
+
 	return 0;
 }
 
@@ -823,6 +891,13 @@ static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
 	if (!test_vfacility(76))
 		return;
 
+	vcpu->arch.sie_block->ecb3 &= ~(ECB3_AES | ECB3_DEA);
+
+	if (vcpu->kvm->arch.crypto.aes_kw)
+		vcpu->arch.sie_block->ecb3 |= ECB3_AES;
+	if (vcpu->kvm->arch.crypto.dea_kw)
+		vcpu->arch.sie_block->ecb3 |= ECB3_DEA;
+
 	vcpu->arch.sie_block->crycbd = vcpu->kvm->arch.crypto.crycbd;
 }
 

commit 72f250206f0f291190ab7f54e4d92ab211779929
Author: Jason J. Herne <jjherne@linux.vnet.ibm.com>
Date:   Tue Nov 25 09:46:02 2014 -0500

    KVM: s390: Provide guest TOD Clock Get/Set Controls
    
    Provide controls for setting/getting the guest TOD clock based on the VM
    attribute interface.
    
    Provide TOD and TOD_HIGH vm attributes on s390 for managing guest Time Of
    Day clock value.
    
    TOD_HIGH is presently always set to 0. In the future it will contain a high
    order expansion of the tod clock value after it overflows the 64-bits of
    the TOD.
    
    Signed-off-by: Jason J. Herne <jjherne@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3677b8ca647f..9cf899e9a5d4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -342,6 +342,113 @@ static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *att
 	return ret;
 }
 
+static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	u8 gtod_high;
+
+	if (copy_from_user(&gtod_high, (void __user *)attr->addr,
+					   sizeof(gtod_high)))
+		return -EFAULT;
+
+	if (gtod_high != 0)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	struct kvm_vcpu *cur_vcpu;
+	unsigned int vcpu_idx;
+	u64 host_tod, gtod;
+	int r;
+
+	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
+		return -EFAULT;
+
+	r = store_tod_clock(&host_tod);
+	if (r)
+		return r;
+
+	mutex_lock(&kvm->lock);
+	kvm->arch.epoch = gtod - host_tod;
+	kvm_for_each_vcpu(vcpu_idx, cur_vcpu, kvm) {
+		cur_vcpu->arch.sie_block->epoch = kvm->arch.epoch;
+		exit_sie(cur_vcpu);
+	}
+	mutex_unlock(&kvm->lock);
+	return 0;
+}
+
+static int kvm_s390_set_tod(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret;
+
+	if (attr->flags)
+		return -EINVAL;
+
+	switch (attr->attr) {
+	case KVM_S390_VM_TOD_HIGH:
+		ret = kvm_s390_set_tod_high(kvm, attr);
+		break;
+	case KVM_S390_VM_TOD_LOW:
+		ret = kvm_s390_set_tod_low(kvm, attr);
+		break;
+	default:
+		ret = -ENXIO;
+		break;
+	}
+	return ret;
+}
+
+static int kvm_s390_get_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	u8 gtod_high = 0;
+
+	if (copy_to_user((void __user *)attr->addr, &gtod_high,
+					 sizeof(gtod_high)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	u64 host_tod, gtod;
+	int r;
+
+	r = store_tod_clock(&host_tod);
+	if (r)
+		return r;
+
+	gtod = host_tod + kvm->arch.epoch;
+	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int kvm_s390_get_tod(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret;
+
+	if (attr->flags)
+		return -EINVAL;
+
+	switch (attr->attr) {
+	case KVM_S390_VM_TOD_HIGH:
+		ret = kvm_s390_get_tod_high(kvm, attr);
+		break;
+	case KVM_S390_VM_TOD_LOW:
+		ret = kvm_s390_get_tod_low(kvm, attr);
+		break;
+	default:
+		ret = -ENXIO;
+		break;
+	}
+	return ret;
+}
+
 static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret;
@@ -350,6 +457,9 @@ static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_MEM_CTRL:
 		ret = kvm_s390_set_mem_control(kvm, attr);
 		break;
+	case KVM_S390_VM_TOD:
+		ret = kvm_s390_set_tod(kvm, attr);
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -366,6 +476,9 @@ static int kvm_s390_vm_get_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_MEM_CTRL:
 		ret = kvm_s390_get_mem_control(kvm, attr);
 		break;
+	case KVM_S390_VM_TOD:
+		ret = kvm_s390_get_tod(kvm, attr);
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -391,6 +504,17 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 			break;
 		}
 		break;
+	case KVM_S390_VM_TOD:
+		switch (attr->attr) {
+		case KVM_S390_VM_TOD_LOW:
+		case KVM_S390_VM_TOD_HIGH:
+			ret = 0;
+			break;
+		default:
+			ret = -ENXIO;
+			break;
+		}
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -541,6 +665,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm->arch.css_support = 0;
 	kvm->arch.use_irqchip = 0;
+	kvm->arch.epoch = 0;
 
 	spin_lock_init(&kvm->arch.start_stop_lock);
 
@@ -686,6 +811,9 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
+	mutex_lock(&vcpu->kvm->lock);
+	vcpu->arch.sie_block->epoch = vcpu->kvm->arch.epoch;
+	mutex_unlock(&vcpu->kvm->lock);
 	if (!kvm_is_ucontrol(vcpu->kvm))
 		vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 }

commit 2444b352c3acf54897b0e2803a7c4e66699f9f43
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Oct 9 14:10:13 2014 +0200

    KVM: s390: forward most SIGP orders to user space
    
    Most SIGP orders are handled partially in kernel and partially in
    user space. In order to:
    - Get a correct SIGP SET PREFIX handler that informs user space
    - Avoid race conditions between concurrently executed SIGP orders
    - Serialize SIGP orders per VCPU
    
    We need to handle all "slow" SIGP orders in user space. The remaining
    ones to be handled completely in kernel are:
    - SENSE
    - SENSE RUNNING
    - EXTERNAL CALL
    - EMERGENCY SIGNAL
    - CONDITIONAL EMERGENCY SIGNAL
    According to the PoP, they have to be fast. They can be executed
    without conflicting to the actions of other pending/concurrently
    executing orders (e.g. STOP vs. START).
    
    This patch introduces a new capability that will - when enabled -
    forward all but the mentioned SIGP orders to user space. The
    instruction counters in the kernel are still updated.
    
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bfb2b990da9b..3677b8ca647f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -166,6 +166,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_S390_IRQCHIP:
 	case KVM_CAP_VM_ATTRIBUTES:
 	case KVM_CAP_MP_STATE:
+	case KVM_CAP_S390_USER_SIGP:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -254,6 +255,10 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		kvm->arch.use_irqchip = 1;
 		r = 0;
 		break;
+	case KVM_CAP_S390_USER_SIGP:
+		kvm->arch.user_sigp = 1;
+		r = 0;
+		break;
 	default:
 		r = -EINVAL;
 		break;

commit 9fbd80828cef1b1bba5a293609a021047bb86a7e
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Oct 9 15:01:38 2014 +0200

    KVM: s390: clear the pfault queue if user space sets the invalid token
    
    We need a way to clear the async pfault queue from user space (e.g.
    for resets and SIGP SET ARCHITECTURE).
    
    This patch simply clears the queue as soon as user space sets the
    invalid pfault token. The definition of the invalid token is moved
    to uapi.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9bdc9f927e82..bfb2b990da9b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -936,6 +936,8 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 	case KVM_REG_S390_PFTOKEN:
 		r = get_user(vcpu->arch.pfault_token,
 			     (u64 __user *)reg->addr);
+		if (vcpu->arch.pfault_token == KVM_S390_PFAULT_TOKEN_INVALID)
+			kvm_clear_async_pf_completion_queue(vcpu);
 		break;
 	case KVM_REG_S390_PFCOMPARE:
 		r = get_user(vcpu->arch.pfault_compare,
@@ -1408,6 +1410,8 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		vcpu->arch.pfault_token = kvm_run->s.regs.pft;
 		vcpu->arch.pfault_select = kvm_run->s.regs.pfs;
 		vcpu->arch.pfault_compare = kvm_run->s.regs.pfc;
+		if (vcpu->arch.pfault_token == KVM_S390_PFAULT_TOKEN_INVALID)
+			kvm_clear_async_pf_completion_queue(vcpu);
 	}
 	kvm_run->kvm_dirty_regs = 0;
 }

commit ea5f496925755bc5e13adbdcfcb841bc2848913b
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Oct 14 15:29:30 2014 +0200

    KVM: s390: only one external call may be pending at a time
    
    Only one external call may be pending at a vcpu at a time. For this
    reason, we have to detect whether the SIGP externcal call interpretation
    facility is available. If so, all external calls have to be injected
    using this mechanism.
    
    SIGP EXTERNAL CALL orders have to return whether another external
    call is already pending. This check was missing until now.
    
    SIGP SENSE hasn't returned yet in all conditions whether an external
    call was pending.
    
    If a SIGP EXTERNAL CALL irq is to be injected and one is already
    pending, -EBUSY is returned.
    
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6a5ed333628c..9bdc9f927e82 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -723,9 +723,11 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->ecb |= 0x10;
 
 	vcpu->arch.sie_block->ecb2  = 8;
-	vcpu->arch.sie_block->eca   = 0xD1002000U;
+	vcpu->arch.sie_block->eca   = 0xC1002000U;
 	if (sclp_has_siif())
 		vcpu->arch.sie_block->eca |= 1;
+	if (sclp_has_sigpif())
+		vcpu->arch.sie_block->eca |= 0x10000000U;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE |
 				      ICTL_TPROT;

commit 9a022067ad75b117f1c1e5cbf6a592022cf0a749
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Aug 5 17:40:47 2014 +0200

    KVM: s390: a VCPU may only stop when no interrupts are left pending
    
    As a SIGP STOP is an interrupt with the least priority, it may only result
    in stop of the vcpu when no other interrupts are left pending.
    
    To detect whether a non-stop irq is pending, we need a way to mask out
    stop irqs from the general kvm_cpu_has_interrupt() function. For this
    reason, the existing function (with an outdated name) is replaced by
    kvm_s390_vcpu_has_irq() which allows to mask out pending stop irqs.
    
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b987b5674625..6a5ed333628c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -806,7 +806,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 {
-	return kvm_cpu_has_interrupt(vcpu);
+	return kvm_s390_vcpu_has_irq(vcpu, 0);
 }
 
 void s390_vcpu_block(struct kvm_vcpu *vcpu)
@@ -1241,7 +1241,7 @@ static int kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu)
 		return 0;
 	if (psw_extint_disabled(vcpu))
 		return 0;
-	if (kvm_cpu_has_interrupt(vcpu))
+	if (kvm_s390_vcpu_has_irq(vcpu, 0))
 		return 0;
 	if (!(vcpu->arch.sie_block->gcr[0] & 0x200ul))
 		return 0;

commit 6cddd432e3da5e25eccbc13844d03c871674a62e
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Oct 15 16:48:53 2014 +0200

    KVM: s390: handle stop irqs without action_bits
    
    This patch removes the famous action_bits and moves the handling of
    SIGP STOP AND STORE STATUS directly into the SIGP STOP interrupt.
    
    The new local interrupt infrastructure is used to track pending stop
    requests.
    
    STOP irqs are the only irqs that don't get actively delivered. They
    remain pending until the stop function is executed (=stop intercept).
    
    If another STOP irq is already pending, -EBUSY will now be returned
    (needed for the SIGP handling code).
    
    Migration of pending SIGP STOP (AND STORE STATUS) orders should now
    be supported out of the box.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 37ef06c19c31..b987b5674625 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1624,15 +1624,10 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 	spin_lock(&vcpu->kvm->arch.start_stop_lock);
 	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
 
-	/* Need to lock access to action_bits to avoid a SIGP race condition */
-	spin_lock(&vcpu->arch.local_int.lock);
-	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
-
 	/* SIGP STOP and SIGP STOP AND STORE STATUS has been fully processed */
-	vcpu->arch.local_int.action_bits &=
-				 ~(ACTION_STOP_ON_STOP | ACTION_STORE_ON_STOP);
-	spin_unlock(&vcpu->arch.local_int.lock);
+	kvm_s390_clear_stop_irq(vcpu);
 
+	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 	__disable_ibs_on_vcpu(vcpu);
 
 	for (i = 0; i < online_vcpus; i++) {

commit 0ac96caf0f9381088c673a16d910b1d329670edf
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Dec 12 15:17:31 2014 +0100

    KVM: s390: base hrtimer on a monotonic clock
    
    The hrtimer that handles the wait with enabled timer interrupts
    should not be disturbed by changes of the host time.
    
    This patch changes our hrtimer to be based on a monotonic clock.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 14cdf1c3b995..37ef06c19c31 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -735,7 +735,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		if (rc)
 			return rc;
 	}
-	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
+	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 	get_cpu_id(&vcpu->arch.cpu_id);
 	vcpu->arch.cpu_id.version = 0xff;

commit 8c0a7ce60676caa89c204a1a3b748e065f3d7182
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Fri Oct 31 14:10:41 2014 +0100

    KVM: s390: Allow userspace to limit guest memory size
    
    With commit c6c956b80bdf ("KVM: s390/mm: support gmap page tables with less
    than 5 levels") we are able to define a limit for the guest memory size.
    
    As we round up the guest size in respect to the levels of page tables
    we get to guest limits of: 2048 MB, 4096 GB, 8192 TB and 16384 PB.
    We currently limit the guest size to 16 TB, which means we end up
    creating a page table structure supporting guest sizes up to 8192 TB.
    
    This patch introduces an interface that allows userspace to tune
    this limit. This may bring performance improvements for small guests.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 529f7905b341..14cdf1c3b995 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -261,7 +261,24 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 	return r;
 }
 
-static int kvm_s390_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
+static int kvm_s390_get_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret;
+
+	switch (attr->attr) {
+	case KVM_S390_VM_MEM_LIMIT_SIZE:
+		ret = 0;
+		if (put_user(kvm->arch.gmap->asce_end, (u64 __user *)attr->addr))
+			ret = -EFAULT;
+		break;
+	default:
+		ret = -ENXIO;
+		break;
+	}
+	return ret;
+}
+
+static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret;
 	unsigned int idx;
@@ -283,6 +300,36 @@ static int kvm_s390_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
 		mutex_unlock(&kvm->lock);
 		ret = 0;
 		break;
+	case KVM_S390_VM_MEM_LIMIT_SIZE: {
+		unsigned long new_limit;
+
+		if (kvm_is_ucontrol(kvm))
+			return -EINVAL;
+
+		if (get_user(new_limit, (u64 __user *)attr->addr))
+			return -EFAULT;
+
+		if (new_limit > kvm->arch.gmap->asce_end)
+			return -E2BIG;
+
+		ret = -EBUSY;
+		mutex_lock(&kvm->lock);
+		if (atomic_read(&kvm->online_vcpus) == 0) {
+			/* gmap_alloc will round the limit up */
+			struct gmap *new = gmap_alloc(current->mm, new_limit);
+
+			if (!new) {
+				ret = -ENOMEM;
+			} else {
+				gmap_free(kvm->arch.gmap);
+				new->private = kvm;
+				kvm->arch.gmap = new;
+				ret = 0;
+			}
+		}
+		mutex_unlock(&kvm->lock);
+		break;
+	}
 	default:
 		ret = -ENXIO;
 		break;
@@ -296,7 +343,7 @@ static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 
 	switch (attr->group) {
 	case KVM_S390_VM_MEM_CTRL:
-		ret = kvm_s390_mem_control(kvm, attr);
+		ret = kvm_s390_set_mem_control(kvm, attr);
 		break;
 	default:
 		ret = -ENXIO;
@@ -308,7 +355,18 @@ static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 
 static int kvm_s390_vm_get_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 {
-	return -ENXIO;
+	int ret;
+
+	switch (attr->group) {
+	case KVM_S390_VM_MEM_CTRL:
+		ret = kvm_s390_get_mem_control(kvm, attr);
+		break;
+	default:
+		ret = -ENXIO;
+		break;
+	}
+
+	return ret;
 }
 
 static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
@@ -320,6 +378,7 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 		switch (attr->attr) {
 		case KVM_S390_VM_MEM_ENABLE_CMMA:
 		case KVM_S390_VM_MEM_CLR_CMMA:
+		case KVM_S390_VM_MEM_LIMIT_SIZE:
 			ret = 0;
 			break;
 		default:

commit dafd032a15f8f165a7d10e00217d3b74f70a7b0f
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Tue Dec 2 16:53:21 2014 +0100

    KVM: s390: move vcpu specific initalization to a later point
    
    As we will allow in a later patch to recreate gmaps with new limits,
    we need to make sure that vcpus get their reference for that gmap
    after they increased the online_vcpu counter, so there is no possible race.
    
    While we are doing this, we also can simplify the vcpu_init function, by
    moving ucontrol specifics to an own function.
    That way we also start now setting the kvm_valid_regs for the ucontrol path.
    
    Reviewed-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ec004f80ee45..529f7905b341 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -546,25 +546,30 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 }
 
 /* Section: vcpu related */
+static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
+{
+	vcpu->arch.gmap = gmap_alloc(current->mm, -1UL);
+	if (!vcpu->arch.gmap)
+		return -ENOMEM;
+	vcpu->arch.gmap->private = vcpu->kvm;
+
+	return 0;
+}
+
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
-	if (kvm_is_ucontrol(vcpu->kvm)) {
-		vcpu->arch.gmap = gmap_alloc(current->mm, -1UL);
-		if (!vcpu->arch.gmap)
-			return -ENOMEM;
-		vcpu->arch.gmap->private = vcpu->kvm;
-		return 0;
-	}
-
-	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
 				    KVM_SYNC_GPRS |
 				    KVM_SYNC_ACRS |
 				    KVM_SYNC_CRS |
 				    KVM_SYNC_ARCH0 |
 				    KVM_SYNC_PFAULT;
+
+	if (kvm_is_ucontrol(vcpu->kvm))
+		return __kvm_ucontrol_vcpu_init(vcpu);
+
 	return 0;
 }
 
@@ -617,6 +622,8 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
+	if (!kvm_is_ucontrol(vcpu->kvm))
+		vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 }
 
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)

commit 31928aa5863e71535ee942f506ca9ac8ce1c4315
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Dec 4 15:47:07 2014 +0100

    KVM: remove unneeded return value of vcpu_postcreate
    
    The return value of kvm_arch_vcpu_postcreate is not checked in its
    caller.  This is okay, because only x86 provides vcpu_postcreate right
    now and it could only fail if vcpu_load failed.  But that is not
    possible during KVM_CREATE_VCPU (kvm_arch_vcpu_load is void, too), so
    just get rid of the unchecked return value.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3e09801e3104..ec004f80ee45 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -615,9 +615,8 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	kvm_s390_clear_local_irqs(vcpu);
 }
 
-int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
+void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
-	return 0;
 }
 
 static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)

commit 66dcff86ba40eebb5133cccf450878f2bba102ef
Merge: 91ed9e8a32d9 2c4aa55a6af0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 18 16:05:28 2014 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM update from Paolo Bonzini:
     "3.19 changes for KVM:
    
       - spring cleaning: removed support for IA64, and for hardware-
         assisted virtualization on the PPC970
    
       - ARM, PPC, s390 all had only small fixes
    
      For x86:
       - small performance improvements (though only on weird guests)
       - usual round of hardware-compliancy fixes from Nadav
       - APICv fixes
       - XSAVES support for hosts and guests.  XSAVES hosts were broken
         because the (non-KVM) XSAVES patches inadvertently changed the KVM
         userspace ABI whenever XSAVES was enabled; hence, this part is
         going to stable.  Guest support is just a matter of exposing the
         feature and CPUID leaves support"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (179 commits)
      KVM: move APIC types to arch/x86/
      KVM: PPC: Book3S: Enable in-kernel XICS emulation by default
      KVM: PPC: Book3S HV: Improve H_CONFER implementation
      KVM: PPC: Book3S HV: Fix endianness of instruction obtained from HEIR register
      KVM: PPC: Book3S HV: Remove code for PPC970 processors
      KVM: PPC: Book3S HV: Tracepoints for KVM HV guest interactions
      KVM: PPC: Book3S HV: Simplify locking around stolen time calculations
      arch: powerpc: kvm: book3s_paired_singles.c: Remove unused function
      arch: powerpc: kvm: book3s_pr.c: Remove unused function
      arch: powerpc: kvm: book3s.c: Remove some unused functions
      arch: powerpc: kvm: book3s_32_mmu.c: Remove unused function
      KVM: PPC: Book3S HV: Check wait conditions before sleeping in kvmppc_vcore_blocked
      KVM: PPC: Book3S HV: ptes are big endian
      KVM: PPC: Book3S HV: Fix inaccuracies in ICP emulation for H_IPI
      KVM: PPC: Book3S HV: Fix KSM memory corruption
      KVM: PPC: Book3S HV: Fix an issue where guest is paused on receiving HMI
      KVM: PPC: Book3S HV: Fix computation of tlbie operand
      KVM: PPC: Book3S HV: Add missing HPTE unlock
      KVM: PPC: BookE: Improve irq inject tracepoint
      arm/arm64: KVM: Require in-kernel vgic for the arch timers
      ...

commit 383d0b050106abecb82f43101cac94fa423af5cd
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Tue Jul 29 15:11:49 2014 +0200

    KVM: s390: handle pending local interrupts via bitmap
    
    This patch adapts handling of local interrupts to be more compliant with
    the z/Architecture Principles of Operation and introduces a data
    structure
    which allows more efficient handling of interrupts.
    
    * get rid of li->active flag, use bitmap instead
    * Keep interrupts in a bitmap instead of a list
    * Deliver interrupts in the order of their priority as defined in the
      PoP
    * Use a second bitmap for sigp emergency requests, as a CPU can have
      one request pending from every other CPU in the system.
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 06878bdf0c6b..f66591eee9ca 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -719,7 +719,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	}
 
 	spin_lock_init(&vcpu->arch.local_int.lock);
-	INIT_LIST_HEAD(&vcpu->arch.local_int.list);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
 	vcpu->arch.local_int.wq = &vcpu->wq;
 	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
@@ -1122,13 +1121,15 @@ static void __kvm_inject_pfault_token(struct kvm_vcpu *vcpu, bool start_token,
 				      unsigned long token)
 {
 	struct kvm_s390_interrupt inti;
-	inti.parm64 = token;
+	struct kvm_s390_irq irq;
 
 	if (start_token) {
-		inti.type = KVM_S390_INT_PFAULT_INIT;
-		WARN_ON_ONCE(kvm_s390_inject_vcpu(vcpu, &inti));
+		irq.u.ext.ext_params2 = token;
+		irq.type = KVM_S390_INT_PFAULT_INIT;
+		WARN_ON_ONCE(kvm_s390_inject_vcpu(vcpu, &irq));
 	} else {
 		inti.type = KVM_S390_INT_PFAULT_DONE;
+		inti.parm64 = token;
 		WARN_ON_ONCE(kvm_s390_inject_vm(vcpu->kvm, &inti));
 	}
 }
@@ -1622,11 +1623,14 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	switch (ioctl) {
 	case KVM_S390_INTERRUPT: {
 		struct kvm_s390_interrupt s390int;
+		struct kvm_s390_irq s390irq;
 
 		r = -EFAULT;
 		if (copy_from_user(&s390int, argp, sizeof(s390int)))
 			break;
-		r = kvm_s390_inject_vcpu(vcpu, &s390int);
+		if (s390int_to_s390irq(&s390int, &s390irq))
+			return -EINVAL;
+		r = kvm_s390_inject_vcpu(vcpu, &s390irq);
 		break;
 	}
 	case KVM_S390_STORE_STATUS:

commit 42cb0c9ff92eba2168d1b8f69d6e62d2af608a13
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri May 23 12:25:11 2014 +0200

    KVM: s390: sigp: instruction counters for all sigp orders
    
    This patch introduces instruction counters for all known sigp orders and also a
    separate one for unknown orders that are passed to user space.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3e83d4b357b9..06878bdf0c6b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -81,10 +81,17 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_sigp_sense_running", VCPU_STAT(instruction_sigp_sense_running) },
 	{ "instruction_sigp_external_call", VCPU_STAT(instruction_sigp_external_call) },
 	{ "instruction_sigp_emergency", VCPU_STAT(instruction_sigp_emergency) },
+	{ "instruction_sigp_cond_emergency", VCPU_STAT(instruction_sigp_cond_emergency) },
+	{ "instruction_sigp_start", VCPU_STAT(instruction_sigp_start) },
 	{ "instruction_sigp_stop", VCPU_STAT(instruction_sigp_stop) },
+	{ "instruction_sigp_stop_store_status", VCPU_STAT(instruction_sigp_stop_store_status) },
+	{ "instruction_sigp_store_status", VCPU_STAT(instruction_sigp_store_status) },
 	{ "instruction_sigp_set_arch", VCPU_STAT(instruction_sigp_arch) },
 	{ "instruction_sigp_set_prefix", VCPU_STAT(instruction_sigp_prefix) },
 	{ "instruction_sigp_restart", VCPU_STAT(instruction_sigp_restart) },
+	{ "instruction_sigp_cpu_reset", VCPU_STAT(instruction_sigp_cpu_reset) },
+	{ "instruction_sigp_init_cpu_reset", VCPU_STAT(instruction_sigp_init_cpu_reset) },
+	{ "instruction_sigp_unknown", VCPU_STAT(instruction_sigp_unknown) },
 	{ "diagnose_10", VCPU_STAT(diagnose_10) },
 	{ "diagnose_44", VCPU_STAT(diagnose_44) },
 	{ "diagnose_9c", VCPU_STAT(diagnose_9c) },

commit a6b7e459ff6d569227980f711664f927100c73a8
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Wed Oct 1 14:48:42 2014 +0200

    KVM: s390: Make the simple ipte mutex specific to a VM instead of global
    
    The ipte-locking should be done for each VM seperately, not globally.
    This way we avoid possible congestions when the simple ipte-lock is used
    and multiple VMs are running.
    
    Suggested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 55aade49b6d1..3e83d4b357b9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -453,6 +453,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	spin_lock_init(&kvm->arch.float_int.lock);
 	INIT_LIST_HEAD(&kvm->arch.float_int.list);
 	init_waitqueue_head(&kvm->arch.ipte_wq);
+	mutex_init(&kvm->arch.ipte_mutex);
 
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 	VM_EVENT(kvm, 3, "%s", "vm created");

commit a13cff318cafbd493b8d5d679e5f3f761084c4fe
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Oct 23 12:07:14 2014 +0200

    s390/mm: recfactor global pgste updates
    
    Replace the s390 specific page table walker for the pgste updates
    with a call to the common code walk_page_range function.
    There are now two pte modification functions, one for the reset
    of the CMMA state and another one for the initialization of the
    storage keys.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 55aade49b6d1..6b049ee75a56 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -271,7 +271,7 @@ static int kvm_s390_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
 	case KVM_S390_VM_MEM_CLR_CMMA:
 		mutex_lock(&kvm->lock);
 		idx = srcu_read_lock(&kvm->srcu);
-		page_table_reset_pgste(kvm->arch.gmap->mm, 0, TASK_SIZE, false);
+		s390_reset_cmma(kvm->arch.gmap->mm);
 		srcu_read_unlock(&kvm->srcu, idx);
 		mutex_unlock(&kvm->lock);
 		ret = 0;

commit ce2e4f0b75a567d25375b52476662c724304e476
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Jul 11 10:00:43 2014 +0200

    KVM: s390: count vcpu wakeups in stat.halt_wakeup
    
    This patch introduces the halt_wakeup counter used by common code and uses it to
    count vcpu wakeups done in s390 arch specific code.
    
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0d5aa88e238b..55aade49b6d1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -50,6 +50,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_instruction", VCPU_STAT(exit_instruction) },
 	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
+	{ "halt_wakeup", VCPU_STAT(halt_wakeup) },
 	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
 	{ "instruction_stctl", VCPU_STAT(instruction_stctl) },

commit 7be81a46695d2088f848653f2bbec06bb178adce
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Sep 19 15:55:20 2014 +0200

    KVM: s390/facilities: allow TOD-CLOCK steering facility bit
    
    There is nothing to do for KVM to support TOD-CLOCK steering.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 56a411c0245a..0d5aa88e238b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1786,7 +1786,7 @@ static int __init kvm_s390_init(void)
 		return -ENOMEM;
 	}
 	memcpy(vfacilities, S390_lowcore.stfle_fac_list, 16);
-	vfacilities[0] &= 0xff82fff3f4fc2000UL;
+	vfacilities[0] &= 0xff82fffbf47c2000UL;
 	vfacilities[1] &= 0x005c000000000000UL;
 	return 0;
 }

commit 84877d93336de21a6251db00b841468a83c65906
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Tue Sep 2 10:27:35 2014 +0100

    KVM: s390: register flic ops dynamically
    
    Using the new kvm_register_device_ops() interface makes us get rid of
    an #ifdef in common code.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b95d4a481b0c..56a411c0245a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -122,7 +122,8 @@ void kvm_arch_hardware_unsetup(void)
 
 int kvm_arch_init(void *opaque)
 {
-	return 0;
+	/* Register floating interrupt controller interface. */
+	return kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
 }
 
 /* Section: device related */

commit 0349985add77ef5c9da8a75f4a9855977f4197d9
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Aug 25 12:38:57 2014 +0200

    KVM: s390: Limit guest size to 16TB
    
    Currently we fill up a full 5 level page table to hold the guest
    mapping. Since commit "support gmap page tables with less than 5
    levels" we can do better.
    Having more than 4 TB might be useful for some testing scenarios,
    so let's just limit ourselves to 16TB guest size.
    Having more than that is totally untested as I do not have enough
    swap space/memory.
    
    We continue to allow ucontrol the full size.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2037738d01a0..b95d4a481b0c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -458,7 +458,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (type & KVM_VM_S390_UCONTROL) {
 		kvm->arch.gmap = NULL;
 	} else {
-		kvm->arch.gmap = gmap_alloc(current->mm, -1UL);
+		kvm->arch.gmap = gmap_alloc(current->mm, (1UL << 44) - 1);
 		if (!kvm->arch.gmap)
 			goto out_nogmap;
 		kvm->arch.gmap->private = kvm;

commit 5102ee879539ebd2e0de1eb93290e3d691973e79
Author: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
Date:   Fri Jun 27 14:46:01 2014 -0400

    KVM: CPACF: Enable MSA4 instructions for kvm guest
    
    We have to provide a per guest crypto block for the CPUs to
    enable MSA4 instructions. According to icainfo on z196 or
    later this enables CCM-AES-128, CMAC-AES-128, CMAC-AES-192
    and CMAC-AES-256.
    
    Signed-off-by: Tony Krowiak <akrowiak@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [split MSA4/protected key into two patches]

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 628e992eeded..2037738d01a0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -392,6 +392,22 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
+static int kvm_s390_crypto_init(struct kvm *kvm)
+{
+	if (!test_vfacility(76))
+		return 0;
+
+	kvm->arch.crypto.crycb = kzalloc(sizeof(*kvm->arch.crypto.crycb),
+					 GFP_KERNEL | GFP_DMA);
+	if (!kvm->arch.crypto.crycb)
+		return -ENOMEM;
+
+	kvm->arch.crypto.crycbd = (__u32) (unsigned long) kvm->arch.crypto.crycb |
+				  CRYCB_FORMAT1;
+
+	return 0;
+}
+
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int rc;
@@ -429,6 +445,9 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (!kvm->arch.dbf)
 		goto out_nodbf;
 
+	if (kvm_s390_crypto_init(kvm) < 0)
+		goto out_crypto;
+
 	spin_lock_init(&kvm->arch.float_int.lock);
 	INIT_LIST_HEAD(&kvm->arch.float_int.list);
 	init_waitqueue_head(&kvm->arch.ipte_wq);
@@ -453,6 +472,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	return 0;
 out_nogmap:
+	kfree(kvm->arch.crypto.crycb);
+out_crypto:
 	debug_unregister(kvm->arch.dbf);
 out_nodbf:
 	free_page((unsigned long)(kvm->arch.sca));
@@ -507,6 +528,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_free_vcpus(kvm);
 	free_page((unsigned long)(kvm->arch.sca));
 	debug_unregister(kvm->arch.dbf);
+	kfree(kvm->arch.crypto.crycb);
 	if (!kvm_is_ucontrol(kvm))
 		gmap_free(kvm->arch.gmap);
 	kvm_s390_destroy_adapters(kvm);
@@ -588,6 +610,14 @@ int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu)
+{
+	if (!test_vfacility(76))
+		return;
+
+	vcpu->arch.sie_block->crycbd = vcpu->kvm->arch.crypto.crycbd;
+}
+
 void kvm_s390_vcpu_unsetup_cmma(struct kvm_vcpu *vcpu)
 {
 	free_page(vcpu->arch.sie_block->cbrlo);
@@ -634,6 +664,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 	get_cpu_id(&vcpu->arch.cpu_id);
 	vcpu->arch.cpu_id.version = 0xff;
+
+	kvm_s390_vcpu_crypto_setup(vcpu);
+
 	return rc;
 }
 

commit 13a34e067eab24fec882e1834fbf2cc31911d474
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:03 2014 +0200

    KVM: remove garbage arg to *hardware_{en,dis}able
    
    In the beggining was on_each_cpu(), which required an unused argument to
    kvm_arch_ops.hardware_{en,dis}able, but this was soon forgotten.
    
    Remove unnecessary arguments that stem from this.
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b8fe1ae777db..628e992eeded 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -100,7 +100,7 @@ int test_vfacility(unsigned long nr)
 }
 
 /* Section: not file related */
-int kvm_arch_hardware_enable(void *garbage)
+int kvm_arch_hardware_enable(void)
 {
 	/* every s390 is virtualization enabled ;-) */
 	return 0;

commit 0865e636aef751966e6e0f8950a26bc7391e923c
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:02 2014 +0200

    KVM: static inline empty kvm_arch functions
    
    Using static inline is going to save few bytes and cycles.
    For example on powerpc, the difference is 700 B after stripping.
    (5 kB before)
    
    This patch also deals with two overlooked empty functions:
    kvm_arch_flush_shadow was not removed from arch/mips/kvm/mips.c
      2df72e9bc KVM: split kvm_arch_flush_shadow
    and kvm_arch_sched_in never made it into arch/ia64/kvm/kvm-ia64.c.
      e790d9ef6 KVM: add kvm_arch_sched_in
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 36209969bf98..b8fe1ae777db 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -106,10 +106,6 @@ int kvm_arch_hardware_enable(void *garbage)
 	return 0;
 }
 
-void kvm_arch_hardware_disable(void *garbage)
-{
-}
-
 static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address);
 
 int kvm_arch_hardware_setup(void)
@@ -124,19 +120,11 @@ void kvm_arch_hardware_unsetup(void)
 	gmap_unregister_ipte_notifier(&gmap_notifier);
 }
 
-void kvm_arch_check_processor_compat(void *rtn)
-{
-}
-
 int kvm_arch_init(void *opaque)
 {
 	return 0;
 }
 
-void kvm_arch_exit(void)
-{
-}
-
 /* Section: device related */
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg)
@@ -514,10 +502,6 @@ static void kvm_free_vcpus(struct kvm *kvm)
 	mutex_unlock(&kvm->lock);
 }
 
-void kvm_arch_sync_events(struct kvm *kvm)
-{
-}
-
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvm_free_vcpus(kvm);
@@ -552,15 +536,6 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
-{
-	/* Nothing todo */
-}
-
-void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
-{
-}
-
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	save_fp_ctl(&vcpu->arch.host_fpregs.fpc);
@@ -1708,21 +1683,12 @@ int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 	return VM_FAULT_SIGBUS;
 }
 
-void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
-			   struct kvm_memory_slot *dont)
-{
-}
-
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 			    unsigned long npages)
 {
 	return 0;
 }
 
-void kvm_arch_memslots_updated(struct kvm *kvm)
-{
-}
-
 /* Section: memory related */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
@@ -1768,15 +1734,6 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 	return;
 }
 
-void kvm_arch_flush_shadow_all(struct kvm *kvm)
-{
-}
-
-void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
-				   struct kvm_memory_slot *slot)
-{
-}
-
 static int __init kvm_s390_init(void)
 {
 	int ret;

commit a7428c3ded540fb10dd32fd6c45352cb314f3569
Merge: 54ad89b05ec4 f079e9521464
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Aug 26 14:31:44 2014 +0200

    Merge tag 'kvm-s390-next-20140825' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Fixes and features for 3.18 part 1
    
    1. The usual cleanups: get rid of duplicate code, use defines, factor
       out the sync_reg handling, additional docs for sync_regs, better
       error handling on interrupt injection
    2. We use KVM_REQ_TLB_FLUSH instead of open coding tlb flushes
    3. Additional registers for kvm_run sync regs. This is usually not
       needed in the fast path due to eventfd/irqfd, but kvm stat claims
       that we reduced the overhead of console output by ~50% on my system
    4. A rework of the gmap infrastructure. This is the 2nd step towards
       host large page support (after getting rid of the storage key
       dependency). We introduces two radix trees to store the guest-to-host
       and host-to-guest translations. This gets us rid of most of
       the page-table walks in the gmap code. Only one in __gmap_link is left,
       this one is required to link the shadow page table to the process page
       table. Finally this contains the plumbing to support gmap page tables
       with less than 5 levels.

commit c6c956b80bdf151cf41d3e7e5c54755d930a212c
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Jul 1 14:36:04 2014 +0200

    KVM: s390/mm: support gmap page tables with less than 5 levels
    
    Add an addressing limit to the gmap address spaces and only allocate
    the page table levels that are needed for the given limit. The limit
    is fixed and can not be changed after a gmap has been created.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 543c24baf1eb..82065dc7948d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -451,7 +451,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	if (type & KVM_VM_S390_UCONTROL) {
 		kvm->arch.gmap = NULL;
 	} else {
-		kvm->arch.gmap = gmap_alloc(current->mm);
+		kvm->arch.gmap = gmap_alloc(current->mm, -1UL);
 		if (!kvm->arch.gmap)
 			goto out_nogmap;
 		kvm->arch.gmap->private = kvm;
@@ -535,7 +535,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
 	if (kvm_is_ucontrol(vcpu->kvm)) {
-		vcpu->arch.gmap = gmap_alloc(current->mm);
+		vcpu->arch.gmap = gmap_alloc(current->mm, -1UL);
 		if (!vcpu->arch.gmap)
 			return -ENOMEM;
 		vcpu->arch.gmap->private = vcpu->kvm;

commit 527e30b41d8b86e9ae7f5b740de416958c0e574e
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Apr 30 16:04:25 2014 +0200

    KVM: s390/mm: use radix trees for guest to host mappings
    
    Store the target address for the gmap segments in a radix tree
    instead of using invalid segment table entries. gmap_translate
    becomes a simple radix_tree_lookup, gmap_fault is split into the
    address translation with gmap_translate and the part that does
    the linking of the gmap shadow page table with the process page
    table.
    A second radix tree is used to keep the pointers to the segment
    table entries for segments that are mapped in the guest address
    space. On unmap of a segment the pointer is retrieved from the
    radix tree and is used to carry out the segment invalidation in
    the gmap shadow page table. As the radix tree can only store one
    pointer, each host segment may only be mapped to exactly one
    guest location.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5c877c8e4494..543c24baf1eb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1092,18 +1092,8 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
  */
 long kvm_arch_fault_in_page(struct kvm_vcpu *vcpu, gpa_t gpa, int writable)
 {
-	struct mm_struct *mm = current->mm;
-	hva_t hva;
-	long rc;
-
-	hva = gmap_fault(vcpu->arch.gmap, gpa);
-	if (IS_ERR_VALUE(hva))
-		return (long)hva;
-	down_read(&mm->mmap_sem);
-	rc = get_user_pages(current, mm, hva, 1, writable, 0, NULL, NULL);
-	up_read(&mm->mmap_sem);
-
-	return rc < 0 ? rc : 0;
+	return gmap_fault(vcpu->arch.gmap, gpa,
+			  writable ? FAULT_FLAG_WRITE : 0);
 }
 
 static void __kvm_inject_pfault_token(struct kvm_vcpu *vcpu, bool start_token,
@@ -1683,9 +1673,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 #endif
 	case KVM_S390_VCPU_FAULT: {
-		r = gmap_fault(vcpu->arch.gmap, arg);
-		if (!IS_ERR_VALUE(r))
-			r = 0;
+		r = gmap_fault(vcpu->arch.gmap, arg, 0);
 		break;
 	}
 	case KVM_ENABLE_CAP:

commit 7cd4b90a737e2e6f41be4ac8b1df847fec67f3da
Merge: 7b46268d2954 ab3f285f227f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Aug 25 15:37:00 2014 +0200

    Merge tag 'kvm-s390-20140825' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    Here are two fixes for s390 KVM code that prevent:
    1. a malicious user to trigger a kernel BUG
    2. a malicious user to change the storage key of read-only pages

commit 6e0a0431bf7d90ed0b8a0a974ad219617a70cc22
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Apr 29 09:34:41 2014 +0200

    KVM: s390/mm: cleanup gmap function arguments, variable names
    
    Make the order of arguments for the gmap calls more consistent,
    if the gmap pointer is passed it is always the first argument.
    In addition distinguish between guest address and user address
    by naming the variables gaddr for a guest address and vmaddr for
    a user address.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c2caa175320c..5c877c8e4494 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1096,7 +1096,7 @@ long kvm_arch_fault_in_page(struct kvm_vcpu *vcpu, gpa_t gpa, int writable)
 	hva_t hva;
 	long rc;
 
-	hva = gmap_fault(gpa, vcpu->arch.gmap);
+	hva = gmap_fault(vcpu->arch.gmap, gpa);
 	if (IS_ERR_VALUE(hva))
 		return (long)hva;
 	down_read(&mm->mmap_sem);
@@ -1683,7 +1683,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 #endif
 	case KVM_S390_VCPU_FAULT: {
-		r = gmap_fault(arg, vcpu->arch.gmap);
+		r = gmap_fault(vcpu->arch.gmap, arg);
 		if (!IS_ERR_VALUE(r))
 			r = 0;
 		break;

commit 7939503147f89f0799ddc89afec2aeae57dd7e2c
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Thu Apr 17 10:10:30 2014 +0200

    KVM: s390: return -EFAULT if lowcore is not mapped during irq delivery
    
    Currently we just kill the userspace process and exit the thread
    immediatly without making sure that we don't hold any locks etc.
    
    Improve this by making KVM_RUN return -EFAULT if the lowcore is not
    mapped during interrupt delivery. To achieve this we need to pass
    the return code of guest memory access routines used in interrupt
    delivery all the way back to the KVM_RUN ioctl.
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 56193becdca5..c2caa175320c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1198,8 +1198,11 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 	if (test_cpu_flag(CIF_MCCK_PENDING))
 		s390_handle_mcck();
 
-	if (!kvm_is_ucontrol(vcpu->kvm))
-		kvm_s390_deliver_pending_interrupts(vcpu);
+	if (!kvm_is_ucontrol(vcpu->kvm)) {
+		rc = kvm_s390_deliver_pending_interrupts(vcpu);
+		if (rc)
+			return rc;
+	}
 
 	rc = kvm_s390_handle_requests(vcpu);
 	if (rc)

commit d3d692c82e4ed79ae7c85f8825ccfdb7d11819da
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Jul 29 08:53:36 2014 +0200

    KVM: s390: implement KVM_REQ_TLB_FLUSH and make use of it
    
    Use the KVM_REQ_TLB_FLUSH request in order to trigger tlb flushes instead
    of manipulating the SIE control block whenever we need it. Also trigger it for
    a control register sync directly instead of (ab)using kvm_s390_set_prefix().
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ab7cd645c127..56193becdca5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1051,6 +1051,11 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 		goto retry;
 	}
 
+	if (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu)) {
+		vcpu->arch.sie_block->ihcpu = 0xffff;
+		goto retry;
+	}
+
 	if (kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu)) {
 		if (!ibs_enabled(vcpu)) {
 			trace_kvm_s390_enable_disable_ibs(vcpu->vcpu_id, 1);
@@ -1306,7 +1311,8 @@ static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_CRS) {
 		memcpy(&vcpu->arch.sie_block->gcr, &kvm_run->s.regs.crs, 128);
-		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
+		/* some control register changes require a tlb flush */
+		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 	}
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_ARCH0) {
 		vcpu->arch.sie_block->cputm = kvm_run->s.regs.cputm;
@@ -1519,7 +1525,7 @@ void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 	 * Another VCPU might have used IBS while we were offline.
 	 * Let's play safe and flush the VCPU at startup.
 	 */
-	vcpu->arch.sie_block->ihcpu  = 0xffff;
+	kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 	spin_unlock(&vcpu->kvm->arch.start_stop_lock);
 	return;
 }

commit b028ee3edd54d338dd811aeafd670a2c682be558
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Jul 17 10:47:43 2014 +0200

    KVM: s390: synchronize more registers with kvm_run
    
    In order to reduce the number of syscalls when dropping to user space, this
    patch enables the synchronization of the following "registers" with kvm_run:
    - ARCH0: CPU timer, clock comparator, TOD programmable register,
             guest breaking-event register, program parameter
    - PFAULT: pfault parameters (token, select, compare)
    
    The registers are grouped to reduce the overhead when syncing.
    
    As this grows the number of sync registers quite a bit, let's move the code
    synchronizing registers with kvm_run from kvm_arch_vcpu_ioctl_run() into
    separate helper routines.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f00d0b02f868..ab7cd645c127 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -546,7 +546,9 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
 				    KVM_SYNC_GPRS |
 				    KVM_SYNC_ACRS |
-				    KVM_SYNC_CRS;
+				    KVM_SYNC_CRS |
+				    KVM_SYNC_ARCH0 |
+				    KVM_SYNC_PFAULT;
 	return 0;
 }
 
@@ -1296,6 +1298,47 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	return rc;
 }
 
+static void sync_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
+{
+	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
+	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX)
+		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_CRS) {
+		memcpy(&vcpu->arch.sie_block->gcr, &kvm_run->s.regs.crs, 128);
+		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
+	}
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_ARCH0) {
+		vcpu->arch.sie_block->cputm = kvm_run->s.regs.cputm;
+		vcpu->arch.sie_block->ckc = kvm_run->s.regs.ckc;
+		vcpu->arch.sie_block->todpr = kvm_run->s.regs.todpr;
+		vcpu->arch.sie_block->pp = kvm_run->s.regs.pp;
+		vcpu->arch.sie_block->gbea = kvm_run->s.regs.gbea;
+	}
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PFAULT) {
+		vcpu->arch.pfault_token = kvm_run->s.regs.pft;
+		vcpu->arch.pfault_select = kvm_run->s.regs.pfs;
+		vcpu->arch.pfault_compare = kvm_run->s.regs.pfc;
+	}
+	kvm_run->kvm_dirty_regs = 0;
+}
+
+static void store_regs(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
+{
+	kvm_run->psw_mask = vcpu->arch.sie_block->gpsw.mask;
+	kvm_run->psw_addr = vcpu->arch.sie_block->gpsw.addr;
+	kvm_run->s.regs.prefix = kvm_s390_get_prefix(vcpu);
+	memcpy(&kvm_run->s.regs.crs, &vcpu->arch.sie_block->gcr, 128);
+	kvm_run->s.regs.cputm = vcpu->arch.sie_block->cputm;
+	kvm_run->s.regs.ckc = vcpu->arch.sie_block->ckc;
+	kvm_run->s.regs.todpr = vcpu->arch.sie_block->todpr;
+	kvm_run->s.regs.pp = vcpu->arch.sie_block->pp;
+	kvm_run->s.regs.gbea = vcpu->arch.sie_block->gbea;
+	kvm_run->s.regs.pft = vcpu->arch.pfault_token;
+	kvm_run->s.regs.pfs = vcpu->arch.pfault_select;
+	kvm_run->s.regs.pfc = vcpu->arch.pfault_compare;
+}
+
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	int rc;
@@ -1317,15 +1360,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		return -EINVAL;
 	}
 
-	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
-	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
-	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX)
-		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
-	if (kvm_run->kvm_dirty_regs & KVM_SYNC_CRS) {
-		memcpy(&vcpu->arch.sie_block->gcr, &kvm_run->s.regs.crs, 128);
-		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
-	}
-	kvm_run->kvm_dirty_regs = 0;
+	sync_regs(vcpu, kvm_run);
 
 	might_fault();
 	rc = __vcpu_run(vcpu);
@@ -1355,10 +1390,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = 0;
 	}
 
-	kvm_run->psw_mask     = vcpu->arch.sie_block->gpsw.mask;
-	kvm_run->psw_addr     = vcpu->arch.sie_block->gpsw.addr;
-	kvm_run->s.regs.prefix = kvm_s390_get_prefix(vcpu);
-	memcpy(&kvm_run->s.regs.crs, &vcpu->arch.sie_block->gcr, 128);
+	store_regs(vcpu, kvm_run);
 
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);

commit fbfa304963fa8bf990dac1d05a77800d1e123b66
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Jul 29 08:22:33 2014 +0200

    KVM: s390: clear kvm_dirty_regs when dropping to user space
    
    We should make sure that all kvm_dirty_regs bits are cleared before dropping
    to user space. Until now, some would remain pending.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 81b0e11521e4..f00d0b02f868 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1319,15 +1319,13 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
 	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
-	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX) {
-		kvm_run->kvm_dirty_regs &= ~KVM_SYNC_PREFIX;
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX)
 		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
-	}
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_CRS) {
-		kvm_run->kvm_dirty_regs &= ~KVM_SYNC_CRS;
 		memcpy(&vcpu->arch.sie_block->gcr, &kvm_run->s.regs.crs, 128);
 		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
 	}
+	kvm_run->kvm_dirty_regs = 0;
 
 	might_fault();
 	rc = __vcpu_run(vcpu);

commit 614a80e474b227cace52fd6e3c790554db8a396e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Aug 6 16:17:58 2014 +0200

    KVM: s390: Fix user triggerable bug in dead code
    
    In the early days, we had some special handling for the
    KVM_EXIT_S390_SIEIC exit, but this was gone in 2009 with commit
    d7b0b5eb3000 (KVM: s390: Make psw available on all exits, not
    just a subset).
    
    Now this switch statement is just a sanity check for userspace
    not messing with the kvm_run structure. Unfortunately, this
    allows userspace to trigger a kernel BUG. Let's just remove
    this switch statement.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: stable@vger.kernel.org

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ce81eb2ab76a..81b0e11521e4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1317,19 +1317,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		return -EINVAL;
 	}
 
-	switch (kvm_run->exit_reason) {
-	case KVM_EXIT_S390_SIEIC:
-	case KVM_EXIT_UNKNOWN:
-	case KVM_EXIT_INTR:
-	case KVM_EXIT_S390_RESET:
-	case KVM_EXIT_S390_UCONTROL:
-	case KVM_EXIT_S390_TSCH:
-	case KVM_EXIT_DEBUG:
-		break;
-	default:
-		BUG();
-	}
-
 	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
 	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
 	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX) {

commit e790d9ef6405633b007339d746b709aed43a928d
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Thu Aug 21 18:08:05 2014 +0200

    KVM: add kvm_arch_sched_in
    
    Introduce preempt notifiers for architecture specific code.
    Advantage over creating a new notifier in every arch is slightly simpler
    code and guaranteed call order with respect to kvm_sched_in.
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ce81eb2ab76a..a3c324ec4370 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -555,6 +555,10 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 	/* Nothing todo */
 }
 
+void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
+{
+}
+
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	save_fp_ctl(&vcpu->arch.host_fpregs.fpc);

commit cc568ead3ce8e0284e7e2cc77bd1dafb03ba4ca1
Merge: 5d5768660539 8e6afa36e754
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Aug 5 09:55:22 2014 +0200

    Merge tag 'signed-kvm-ppc-next' of git://github.com/agraf/linux-2.6 into kvm
    
    Patch queue for ppc - 2014-08-01
    
    Highlights in this release include:
    
      - BookE: Rework instruction fetch, not racy anymore now
      - BookE HV: Fix ONE_REG accessors for some in-hardware registers
      - Book3S: Good number of LE host fixes, enable HV on LE
      - Book3S: Some misc bug fixes
      - Book3S HV: Add in-guest debug support
      - Book3S HV: Preload cache lines on context switch
      - Remove 440 support
    
    Alexander Graf (31):
          KVM: PPC: Book3s PR: Disable AIL mode with OPAL
          KVM: PPC: Book3s HV: Fix tlbie compile error
          KVM: PPC: Book3S PR: Handle hyp doorbell exits
          KVM: PPC: Book3S PR: Fix ABIv2 on LE
          KVM: PPC: Book3S PR: Fix sparse endian checks
          PPC: Add asm helpers for BE 32bit load/store
          KVM: PPC: Book3S HV: Make HTAB code LE host aware
          KVM: PPC: Book3S HV: Access guest VPA in BE
          KVM: PPC: Book3S HV: Access host lppaca and shadow slb in BE
          KVM: PPC: Book3S HV: Access XICS in BE
          KVM: PPC: Book3S HV: Fix ABIv2 on LE
          KVM: PPC: Book3S HV: Enable for little endian hosts
          KVM: PPC: Book3S: Move vcore definition to end of kvm_arch struct
          KVM: PPC: Deflect page write faults properly in kvmppc_st
          KVM: PPC: Book3S: Stop PTE lookup on write errors
          KVM: PPC: Book3S: Add hack for split real mode
          KVM: PPC: Book3S: Make magic page properly 4k mappable
          KVM: PPC: Remove 440 support
          KVM: Rename and add argument to check_extension
          KVM: Allow KVM_CHECK_EXTENSION on the vm fd
          KVM: PPC: Book3S: Provide different CAPs based on HV or PR mode
          KVM: PPC: Implement kvmppc_xlate for all targets
          KVM: PPC: Move kvmppc_ld/st to common code
          KVM: PPC: Remove kvmppc_bad_hva()
          KVM: PPC: Use kvm_read_guest in kvmppc_ld
          KVM: PPC: Handle magic page in kvmppc_ld/st
          KVM: PPC: Separate loadstore emulation from priv emulation
          KVM: PPC: Expose helper functions for data/inst faults
          KVM: PPC: Remove DCR handling
          KVM: PPC: HV: Remove generic instruction emulation
          KVM: PPC: PR: Handle FSCR feature deselects
    
    Alexey Kardashevskiy (1):
          KVM: PPC: Book3S: Fix LPCR one_reg interface
    
    Aneesh Kumar K.V (4):
          KVM: PPC: BOOK3S: PR: Fix PURR and SPURR emulation
          KVM: PPC: BOOK3S: PR: Emulate virtual timebase register
          KVM: PPC: BOOK3S: PR: Emulate instruction counter
          KVM: PPC: BOOK3S: HV: Update compute_tlbie_rb to handle 16MB base page
    
    Anton Blanchard (2):
          KVM: PPC: Book3S HV: Fix ABIv2 indirect branch issue
          KVM: PPC: Assembly functions exported to modules need _GLOBAL_TOC()
    
    Bharat Bhushan (10):
          kvm: ppc: bookehv: Added wrapper macros for shadow registers
          kvm: ppc: booke: Use the shared struct helpers of SRR0 and SRR1
          kvm: ppc: booke: Use the shared struct helpers of SPRN_DEAR
          kvm: ppc: booke: Add shared struct helpers of SPRN_ESR
          kvm: ppc: booke: Use the shared struct helpers for SPRN_SPRG0-7
          kvm: ppc: Add SPRN_EPR get helper function
          kvm: ppc: bookehv: Save restore SPRN_SPRG9 on guest entry exit
          KVM: PPC: Booke-hv: Add one reg interface for SPRG9
          KVM: PPC: Remove comment saying SPRG1 is used for vcpu pointer
          KVM: PPC: BOOKEHV: rename e500hv_spr to bookehv_spr
    
    Michael Neuling (1):
          KVM: PPC: Book3S HV: Add H_SET_MODE hcall handling
    
    Mihai Caraman (8):
          KVM: PPC: e500mc: Enhance tlb invalidation condition on vcpu schedule
          KVM: PPC: e500: Fix default tlb for victim hint
          KVM: PPC: e500: Emulate power management control SPR
          KVM: PPC: e500mc: Revert "add load inst fixup"
          KVM: PPC: Book3e: Add TLBSEL/TSIZE defines for MAS0/1
          KVM: PPC: Book3s: Remove kvmppc_read_inst() function
          KVM: PPC: Allow kvmppc_get_last_inst() to fail
          KVM: PPC: Bookehv: Get vcpu's last instruction for emulation
    
    Paul Mackerras (4):
          KVM: PPC: Book3S: Controls for in-kernel sPAPR hypercall handling
          KVM: PPC: Book3S: Allow only implemented hcalls to be enabled or disabled
          KVM: PPC: Book3S PR: Take SRCU read lock around RTAS kvm_read_guest() call
          KVM: PPC: Book3S: Make kvmppc_ld return a more accurate error indication
    
    Stewart Smith (2):
          Split out struct kvmppc_vcore creation to separate function
          Use the POWER8 Micro Partition Prefetch Engine in KVM HV on POWER8
    
    Conflicts:
            Documentation/virtual/kvm/api.txt

commit 784aa3d7fb6f729c06d5836c9d9569f58e4d05ae
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jul 14 18:27:35 2014 +0200

    KVM: Rename and add argument to check_extension
    
    In preparation to make the check_extension function available to VM scope
    we add a struct kvm * argument to the function header and rename the function
    accordingly. It will still be called from the /dev/kvm fd, but with a NULL
    argument for struct kvm *.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2f3e14fe91a4..00268cacdf4c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -146,7 +146,7 @@ long kvm_arch_dev_ioctl(struct file *filp,
 	return -EINVAL;
 }
 
-int kvm_dev_ioctl_check_extension(long ext)
+int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r;
 

commit 78599d90041e0af5702eb45081a77b65d2941a00
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Tue Jul 15 09:54:39 2014 +0200

    KVM: s390: advertise KVM_CAP_S390_IRQCHIP
    
    We should advertise all capabilities, including those that can
    be enabled.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dd902e64c033..339b34a02fb8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -166,6 +166,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
 	case KVM_CAP_ENABLE_CAP_VM:
+	case KVM_CAP_S390_IRQCHIP:
 	case KVM_CAP_VM_ATTRIBUTES:
 	case KVM_CAP_MP_STATE:
 		r = 1;

commit ea74c0ea1b24a6978a6ebc80ba4dbc7b7848b32d
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri May 16 12:08:29 2014 +0200

    KVM: s390: remove the tasklet used by the hrtimer
    
    We can get rid of the tasklet used for waking up a VCPU in the hrtimer
    code but wakeup the VCPU directly.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b29a03132ecb..dd902e64c033 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -649,8 +649,6 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 			return rc;
 	}
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
-	tasklet_init(&vcpu->arch.tasklet, kvm_s390_tasklet,
-		     (unsigned long) vcpu);
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 	get_cpu_id(&vcpu->arch.cpu_id);
 	vcpu->arch.cpu_id.version = 0xff;

commit 433b9ee43c233790c0ae7c02785d6d73fd4d4455
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue May 6 16:11:14 2014 +0200

    KVM: s390: remove _bh locking from start_stop_lock
    
    The start_stop_lock is no longer acquired when in atomic context, therefore we
    can convert it into an ordinary spin_lock.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a7bda180fe65..b29a03132ecb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1478,7 +1478,7 @@ void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 
 	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 1);
 	/* Only one cpu at a time may enter/leave the STOPPED state. */
-	spin_lock_bh(&vcpu->kvm->arch.start_stop_lock);
+	spin_lock(&vcpu->kvm->arch.start_stop_lock);
 	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
 
 	for (i = 0; i < online_vcpus; i++) {
@@ -1504,7 +1504,7 @@ void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 	 * Let's play safe and flush the VCPU at startup.
 	 */
 	vcpu->arch.sie_block->ihcpu  = 0xffff;
-	spin_unlock_bh(&vcpu->kvm->arch.start_stop_lock);
+	spin_unlock(&vcpu->kvm->arch.start_stop_lock);
 	return;
 }
 
@@ -1518,7 +1518,7 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 
 	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 0);
 	/* Only one cpu at a time may enter/leave the STOPPED state. */
-	spin_lock_bh(&vcpu->kvm->arch.start_stop_lock);
+	spin_lock(&vcpu->kvm->arch.start_stop_lock);
 	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
 
 	/* Need to lock access to action_bits to avoid a SIGP race condition */
@@ -1547,7 +1547,7 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 		__enable_ibs_on_vcpu(started_vcpu);
 	}
 
-	spin_unlock_bh(&vcpu->kvm->arch.start_stop_lock);
+	spin_unlock(&vcpu->kvm->arch.start_stop_lock);
 	return;
 }
 

commit 4ae3c0815fb63cbed1afcd5bacc7705c6d1b9fec
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri May 16 10:23:53 2014 +0200

    KVM: s390: remove _bh locking from local_int.lock
    
    local_int.lock is not used in a bottom-half handler anymore, therefore we can
    turn it into an ordinary spin_lock at all occurrences.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ecb135702313..a7bda180fe65 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1522,13 +1522,13 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
 
 	/* Need to lock access to action_bits to avoid a SIGP race condition */
-	spin_lock_bh(&vcpu->arch.local_int.lock);
+	spin_lock(&vcpu->arch.local_int.lock);
 	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 
 	/* SIGP STOP and SIGP STOP AND STORE STATUS has been fully processed */
 	vcpu->arch.local_int.action_bits &=
 				 ~(ACTION_STOP_ON_STOP | ACTION_STORE_ON_STOP);
-	spin_unlock_bh(&vcpu->arch.local_int.lock);
+	spin_unlock(&vcpu->arch.local_int.lock);
 
 	__disable_ibs_on_vcpu(vcpu);
 

commit 0759d0681cae279e77ebb4b76175e330360b01d9
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue May 13 16:54:32 2014 +0200

    KVM: s390: cleanup handle_wait by reusing kvm_vcpu_block
    
    This patch cleans up the code in handle_wait by reusing the common code
    function kvm_vcpu_block.
    
    signal_pending(), kvm_cpu_has_pending_timer() and kvm_arch_vcpu_runnable() are
    sufficient for checking if we need to wake-up that VCPU. kvm_vcpu_block
    uses these functions, so no checks are lost.
    
    The flag "timer_due" can be removed - kvm_cpu_has_pending_timer() tests whether
    the timer is pending, thus the vcpu is correctly woken up.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fdf88f7a539c..ecb135702313 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1068,6 +1068,9 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 		goto retry;
 	}
 
+	/* nothing to do, just clear the request */
+	clear_bit(KVM_REQ_UNHALT, &vcpu->requests);
+
 	return 0;
 }
 

commit 6352e4d2dd9a349024a41356148eced553e1dce4
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Apr 10 17:35:00 2014 +0200

    KVM: s390: implement KVM_(S|G)ET_MP_STATE for user space state control
    
    This patch
    - adds s390 specific MP states to linux headers and documents them
    - implements the KVM_{SET,GET}_MP_STATE ioctls
    - enables KVM_CAP_MP_STATE
    - allows user space to control the VCPU state on s390.
    
    If user space sets the VCPU state using the ioctl KVM_SET_MP_STATE, we can disable
    manual changing of the VCPU state and trust user space to do the right thing.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 342895350825..fdf88f7a539c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -167,6 +167,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_DEVICE_CTRL:
 	case KVM_CAP_ENABLE_CAP_VM:
 	case KVM_CAP_VM_ATTRIBUTES:
+	case KVM_CAP_MP_STATE:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -595,7 +596,8 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->pp = 0;
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
-	kvm_s390_vcpu_stop(vcpu);
+	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm))
+		kvm_s390_vcpu_stop(vcpu);
 	kvm_s390_clear_local_irqs(vcpu);
 }
 
@@ -980,13 +982,34 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state)
 {
-	return -EINVAL; /* not implemented yet */
+	/* CHECK_STOP and LOAD are not supported yet */
+	return is_vcpu_stopped(vcpu) ? KVM_MP_STATE_STOPPED :
+				       KVM_MP_STATE_OPERATING;
 }
 
 int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state)
 {
-	return -EINVAL; /* not implemented yet */
+	int rc = 0;
+
+	/* user space knows about this interface - let it control the state */
+	vcpu->kvm->arch.user_cpu_state_ctrl = 1;
+
+	switch (mp_state->mp_state) {
+	case KVM_MP_STATE_STOPPED:
+		kvm_s390_vcpu_stop(vcpu);
+		break;
+	case KVM_MP_STATE_OPERATING:
+		kvm_s390_vcpu_start(vcpu);
+		break;
+	case KVM_MP_STATE_LOAD:
+	case KVM_MP_STATE_CHECK_STOP:
+		/* fall through - CHECK_STOP and LOAD are not supported yet */
+	default:
+		rc = -ENXIO;
+	}
+
+	return rc;
 }
 
 bool kvm_s390_cmma_enabled(struct kvm *kvm)
@@ -1284,7 +1307,13 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
-	kvm_s390_vcpu_start(vcpu);
+	if (!kvm_s390_user_cpu_state_ctrl(vcpu->kvm)) {
+		kvm_s390_vcpu_start(vcpu);
+	} else if (is_vcpu_stopped(vcpu)) {
+		pr_err_ratelimited("kvm-s390: can't run stopped vcpu %d\n",
+				   vcpu->vcpu_id);
+		return -EINVAL;
+	}
 
 	switch (kvm_run->exit_reason) {
 	case KVM_EXIT_S390_SIEIC:

commit 7a42fdc20f1ff31bb47b05a4283c17129d0ecca3
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 5 16:26:19 2014 +0200

    KVM: s390: remove __cpu_is_stopped and expose is_vcpu_stopped
    
    The function "__cpu_is_stopped" is not used any more. Let's remove it and
    expose the function "is_vcpu_stopped" instead, which is actually what we want.
    
    This patch also converts an open coded check for CPUSTAT_STOPPED to
    is_vcpu_stopped().
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c5077899de5b..342895350825 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -926,7 +926,7 @@ static int kvm_arch_vcpu_ioctl_set_initial_psw(struct kvm_vcpu *vcpu, psw_t psw)
 {
 	int rc = 0;
 
-	if (!(atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_STOPPED))
+	if (!is_vcpu_stopped(vcpu))
 		rc = -EBUSY;
 	else {
 		vcpu->run->psw_mask = psw.mask;
@@ -1413,11 +1413,6 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return kvm_s390_store_status_unloaded(vcpu, addr);
 }
 
-static inline int is_vcpu_stopped(struct kvm_vcpu *vcpu)
-{
-	return atomic_read(&(vcpu)->arch.sie_block->cpuflags) & CPUSTAT_STOPPED;
-}
-
 static void __disable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
 {
 	kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu);

commit 32f5ff63ff9c87195d06896e6ab4086b6369546a
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 14 12:40:03 2014 +0200

    KVM: s390: move finalization of SIGP STOP orders to kvm_s390_vcpu_stop
    
    Let's move the finalization of SIGP STOP and SIGP STOP AND STORE STATUS orders to
    the point where the VCPU is actually stopped.
    
    This change is needed to prepare for a user space driven VCPU state change. The
    action_bits may only be cleared when setting the cpu state to STOPPED while
    holding the local irq lock.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2f3e14fe91a4..c5077899de5b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1494,7 +1494,15 @@ void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 	spin_lock_bh(&vcpu->kvm->arch.start_stop_lock);
 	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
 
+	/* Need to lock access to action_bits to avoid a SIGP race condition */
+	spin_lock_bh(&vcpu->arch.local_int.lock);
 	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+
+	/* SIGP STOP and SIGP STOP AND STORE STATUS has been fully processed */
+	vcpu->arch.local_int.action_bits &=
+				 ~(ACTION_STOP_ON_STOP | ACTION_STORE_ON_STOP);
+	spin_unlock_bh(&vcpu->arch.local_int.lock);
+
 	__disable_ibs_on_vcpu(vcpu);
 
 	for (i = 0; i < online_vcpus; i++) {

commit b05d59dfceaea72565b1648af929b037b0f96d7f
Merge: daf342af2f78 820b3fcdeb80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 4 08:47:12 2014 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm into next
    
    Pull KVM updates from Paolo Bonzini:
     "At over 200 commits, covering almost all supported architectures, this
      was a pretty active cycle for KVM.  Changes include:
    
       - a lot of s390 changes: optimizations, support for migration, GDB
         support and more
    
       - ARM changes are pretty small: support for the PSCI 0.2 hypercall
         interface on both the guest and the host (the latter acked by
         Catalin)
    
       - initial POWER8 and little-endian host support
    
       - support for running u-boot on embedded POWER targets
    
       - pretty large changes to MIPS too, completing the userspace
         interface and improving the handling of virtualized timer hardware
    
       - for x86, a larger set of changes is scheduled for 3.17.  Still, we
         have a few emulator bugfixes and support for running nested
         fully-virtualized Xen guests (para-virtualized Xen guests have
         always worked).  And some optimizations too.
    
      The only missing architecture here is ia64.  It's not a coincidence
      that support for KVM on ia64 is scheduled for removal in 3.17"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (203 commits)
      KVM: add missing cleanup_srcu_struct
      KVM: PPC: Book3S PR: Rework SLB switching code
      KVM: PPC: Book3S PR: Use SLB entry 0
      KVM: PPC: Book3S HV: Fix machine check delivery to guest
      KVM: PPC: Book3S HV: Work around POWER8 performance monitor bugs
      KVM: PPC: Book3S HV: Make sure we don't miss dirty pages
      KVM: PPC: Book3S HV: Fix dirty map for hugepages
      KVM: PPC: Book3S HV: Put huge-page HPTEs in rmap chain for base address
      KVM: PPC: Book3S HV: Fix check for running inside guest in global_invalidates()
      KVM: PPC: Book3S: Move KVM_REG_PPC_WORT to an unused register number
      KVM: PPC: Book3S: Add ONE_REG register names that were missed
      KVM: PPC: Add CAP to indicate hcall fixes
      KVM: PPC: MPIC: Reset IRQ source private members
      KVM: PPC: Graciously fail broken LE hypercalls
      PPC: ePAPR: Fix hypercall on LE guest
      KVM: PPC: BOOK3S: Remove open coded make_dsisr in alignment handler
      KVM: PPC: BOOK3S: Always use the saved DAR value
      PPC: KVM: Make NX bit available with magic page
      KVM: PPC: Disable NX for old magic page using guests
      KVM: PPC: BOOK3S: HV: Add mixed page-size support for guest
      ...

commit 8f5759aeb88a47448cd92ab55a016d013b154a98
Merge: e5c4ecdc55b6 63aef00b55d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 3 10:26:41 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux into next
    
    Pull first set of s390 updates from Martin Schwidefsky:
     "The biggest change in this patchset is conversion from the bootmem
      bitmaps to the memblock code.  This conversion requires two common
      code patches to introduce the 'physmem' memblock list.
    
      We experimented with ticket spinlocks but in the end decided against
      them as they perform poorly on virtualized systems.  But the spinlock
      cleanup and some small improvements are included.
    
      The uaccess code got another optimization, the get_user/put_user calls
      are now inline again for kernel compiles targeted at z10 or newer
      machines.  This makes the text segment shorter and the code gets a
      little bit faster.
    
      And as always some bug fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (31 commits)
      s390/lowcore: replace lowcore irb array with a per-cpu variable
      s390/lowcore: reserve 96 bytes for IRB in lowcore
      s390/facilities: remove extract-cpu-time facility check
      s390: require mvcos facility for z10 and newer machines
      s390/boot: fix boot of compressed kernel built with gcc 4.9
      s390/cio: remove weird assignment during argument evaluation
      s390/time: cast tv_nsec to u64 prior to shift in update_vsyscall
      s390/oprofile: make return of 0 explicit
      s390/spinlock: refactor arch_spin_lock_wait[_flags]
      s390/rwlock: add missing local_irq_restore calls
      s390/spinlock,rwlock: always to a load-and-test first
      s390/cio: fix multiple structure definitions
      s390/spinlock: fix system hang with spin_retry <= 0
      s390/appldata: add slab.h for kzalloc/kfree
      s390/uaccess: provide inline variants of get_user/put_user
      s390/pci: add some new arch specific pci attributes
      s390/pci: use pdev->dev.groups for attribute creation
      s390/pci: use macro for attribute creation
      s390/pci: improve state check when processing hotplug events
      s390: split TIF bits into CIF, PIF and TIF bits
      ...

commit 5a5e65361f01b44caa51ba202e6720d458829fc5
Author: Matthew Rosato <mjrosato@linux.vnet.ibm.com>
Date:   Tue Jan 29 11:48:20 2013 -0500

    KVM: s390: Intercept the tprot instruction
    
    Based on original patch from Jeng-fang (Nick) Wang
    
    When standby memory is specified for a guest Linux, but no virtual memory has
    been allocated on the Qemu host backing that guest, the guest memory detection
    process encounters a memory access exception which is not thrown from the KVM
    handle_tprot() instruction-handler function. The access exception comes from
    sie64a returning EFAULT, which then passes an addressing exception to the guest.
    Unfortunately this does not the proper PSW fixup (nullifying vs.
    suppressing) so the guest will get a fault for the wrong address.
    
    Let's just intercept the tprot instruction all the time to do the right thing
    and not go the page fault handler path for standby memory. tprot is only used
    by Linux during startup so some exits should be ok.
    Without this patch, standby memory cannot be used with KVM.
    
    Signed-off-by: Nick Wang <jfwang@us.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Tested-by: Matthew Rosato <mjrosato@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 06d1888f9179..43e191b25789 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -637,7 +637,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	if (sclp_has_siif())
 		vcpu->arch.sie_block->eca |= 1;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
-	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
+	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE |
+				      ICTL_TPROT;
+
 	if (kvm_s390_cmma_enabled(vcpu->kvm)) {
 		rc = kvm_s390_vcpu_setup_cmma(vcpu);
 		if (rc)

commit 2de3bfc25ab39109883075a75134c37759fa1b93
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue May 20 17:25:20 2014 +0200

    KVM: s390: check the given debug flags, not the set ones
    
    This patch fixes a minor bug when updating the guest debug settings.
    We should check the given debug flags, not the already set ones.
    Doesn't do any harm but too many (for now unused) flags could be set internally
    without error.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e519860c6031..06d1888f9179 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -950,7 +950,7 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 	vcpu->guest_debug = 0;
 	kvm_s390_clear_bp_data(vcpu);
 
-	if (vcpu->guest_debug & ~VALID_GUESTDBG_FLAGS)
+	if (dbg->control & ~VALID_GUESTDBG_FLAGS)
 		return -EINVAL;
 
 	if (dbg->control & KVM_GUESTDBG_ENABLE) {

commit d3a73acbc26a4a81a01a35fd162973e53d0386f5
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Apr 15 12:55:07 2014 +0200

    s390: split TIF bits into CIF, PIF and TIF bits
    
    The oi and ni instructions used in entry[64].S to set and clear bits
    in the thread-flags are not guaranteed to be atomic in regard to other
    CPUs. Split the TIF bits into CPU, pt_regs and thread-info specific
    bits. Updates on the TIF bits are done with atomic instructions,
    updates on CPU and pt_regs bits are done with non-atomic instructions.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b3ecb8f5b6ce..020659745df0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -906,7 +906,7 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 	if (need_resched())
 		schedule();
 
-	if (test_thread_flag(TIF_MCCK_PENDING))
+	if (test_cpu_flag(CIF_MCCK_PENDING))
 		s390_handle_mcck();
 
 	if (!kvm_is_ucontrol(vcpu->kvm))

commit fda902cb8347da121025c4079b9e87748228a27e
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Tue May 13 16:58:30 2014 +0200

    KVM: s390: split SIE state guest prefix field
    
    This patch splits the SIE state guest prefix at offset 4
    into a prefix bit field. Additionally it provides the
    access functions:
    
     - kvm_s390_get_prefix()
     - kvm_s390_set_prefix()
    
    to access the prefix per vcpu.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2c243124a4e2..e519860c6031 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -753,7 +753,7 @@ static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address)
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		/* match against both prefix pages */
-		if (vcpu->arch.sie_block->prefix == (address & ~0x1000UL)) {
+		if (kvm_s390_get_prefix(vcpu) == (address & ~0x1000UL)) {
 			VCPU_EVENT(vcpu, 2, "gmap notifier for %lx", address);
 			kvm_make_request(KVM_REQ_MMU_RELOAD, vcpu);
 			exit_sie_sync(vcpu);
@@ -1017,7 +1017,7 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	if (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu)) {
 		int rc;
 		rc = gmap_ipte_notify(vcpu->arch.gmap,
-				      vcpu->arch.sie_block->prefix,
+				      kvm_s390_get_prefix(vcpu),
 				      PAGE_SIZE * 2);
 		if (rc)
 			return rc;
@@ -1338,7 +1338,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	kvm_run->psw_mask     = vcpu->arch.sie_block->gpsw.mask;
 	kvm_run->psw_addr     = vcpu->arch.sie_block->gpsw.addr;
-	kvm_run->s.regs.prefix = vcpu->arch.sie_block->prefix;
+	kvm_run->s.regs.prefix = kvm_s390_get_prefix(vcpu);
 	memcpy(&kvm_run->s.regs.crs, &vcpu->arch.sie_block->gcr, 128);
 
 	if (vcpu->sigset_active)
@@ -1357,6 +1357,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 {
 	unsigned char archmode = 1;
+	unsigned int px;
 	u64 clkcomp;
 	int rc;
 
@@ -1375,8 +1376,9 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 			      vcpu->run->s.regs.gprs, 128);
 	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, psw),
 			      &vcpu->arch.sie_block->gpsw, 16);
+	px = kvm_s390_get_prefix(vcpu);
 	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, pref_reg),
-			      &vcpu->arch.sie_block->prefix, 4);
+			      &px, 4);
 	rc |= write_guest_abs(vcpu,
 			      gpa + offsetof(struct save_area, fp_ctrl_reg),
 			      &vcpu->arch.guest_fpregs.fpc, 4);

commit 4953919feedaeb6d0161ecea920c35d1d1f639d3
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Feb 21 08:59:59 2014 +0100

    KVM: s390: interpretive execution of SIGP EXTERNAL CALL
    
    If the sigp interpretation facility is installed, most SIGP EXTERNAL CALL
    operations will be interpreted instead of intercepted. A partial execution
    interception will occurr at the sending cpu only if the target cpu is in the
    wait state ("W" bit in the cpuflags set). Instruction interception will only
    happen in error cases (e.g. cpu addr invalid).
    
    As a sending cpu might set the external call interrupt pending flags at the
    target cpu at every point in time, we can't handle this kind of interrupt using
    our kvm interrupt injection mechanism. The injection will be done automatically
    by the SIE when preparing the start of the target cpu.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    CC: Thomas Huth <thuth@linux.vnet.ibm.com>
    [Adopt external call injection to check for sigp interpretion]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d91feb2f03ea..2c243124a4e2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -633,7 +633,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->ecb |= 0x10;
 
 	vcpu->arch.sie_block->ecb2  = 8;
-	vcpu->arch.sie_block->eca   = 0xC1002000U;
+	vcpu->arch.sie_block->eca   = 0xD1002000U;
 	if (sclp_has_siif())
 		vcpu->arch.sie_block->eca |= 1;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;

commit fa576c583d877d667d9acaed909a3dfc6b03e138
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Tue May 6 17:20:16 2014 +0200

    KVM: s390: Introduce helper function for faulting-in a guest page
    
    Rework the function kvm_arch_fault_in_sync() to become a proper helper
    function for faulting-in a guest page. Now it takes the guest address as
    a parameter and does not ignore the possible error code from gmap_fault()
    anymore (which could cause undetected error conditions before).
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0a01744cbdd9..d91feb2f03ea 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1045,15 +1045,30 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-static long kvm_arch_fault_in_sync(struct kvm_vcpu *vcpu)
+/**
+ * kvm_arch_fault_in_page - fault-in guest page if necessary
+ * @vcpu: The corresponding virtual cpu
+ * @gpa: Guest physical address
+ * @writable: Whether the page should be writable or not
+ *
+ * Make sure that a guest page has been faulted-in on the host.
+ *
+ * Return: Zero on success, negative error code otherwise.
+ */
+long kvm_arch_fault_in_page(struct kvm_vcpu *vcpu, gpa_t gpa, int writable)
 {
-	long rc;
-	hva_t fault = gmap_fault(current->thread.gmap_addr, vcpu->arch.gmap);
 	struct mm_struct *mm = current->mm;
+	hva_t hva;
+	long rc;
+
+	hva = gmap_fault(gpa, vcpu->arch.gmap);
+	if (IS_ERR_VALUE(hva))
+		return (long)hva;
 	down_read(&mm->mmap_sem);
-	rc = get_user_pages(current, mm, fault, 1, 1, 0, NULL, NULL);
+	rc = get_user_pages(current, mm, hva, 1, writable, 0, NULL, NULL);
 	up_read(&mm->mmap_sem);
-	return rc;
+
+	return rc < 0 ? rc : 0;
 }
 
 static void __kvm_inject_pfault_token(struct kvm_vcpu *vcpu, bool start_token,
@@ -1191,9 +1206,12 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 	} else if (current->thread.gmap_pfault) {
 		trace_kvm_s390_major_guest_pfault(vcpu);
 		current->thread.gmap_pfault = 0;
-		if (kvm_arch_setup_async_pf(vcpu) ||
-		    (kvm_arch_fault_in_sync(vcpu) >= 0))
+		if (kvm_arch_setup_async_pf(vcpu)) {
 			rc = 0;
+		} else {
+			gpa_t gpa = current->thread.gmap_addr;
+			rc = kvm_arch_fault_in_page(vcpu, gpa, 1);
+		}
 	}
 
 	if (rc == -1) {

commit ebc3226202d5956a5963185222982d435378b899
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Fri May 9 15:00:46 2014 +0200

    KVM: s390: announce irqfd capability
    
    s390 has acquired irqfd support with commit "KVM: s390: irq routing for
    adapter interrupts" (84223598778ba08041f4297fda485df83414d57e) but
    failed to announce it. Let's fix that.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b3ecb8f5b6ce..9ae6664ff08c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -158,6 +158,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_S390_CSS_SUPPORT:
+	case KVM_CAP_IRQFD:
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
 	case KVM_CAP_ENABLE_CAP_VM:

commit 8ad357551797b1edc184fb9f6a4f80a6fa626459
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Mar 14 11:00:21 2014 +0100

    KVM: s390: enable IBS for single running VCPUs
    
    This patch enables the IBS facility when a single VCPU is running.
    The facility is dynamically turned on/off as soon as other VCPUs
    enter/leave the stopped state.
    
    When this facility is operating, some instructions can be executed
    faster for single-cpu guests.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6c972d229ace..0a01744cbdd9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -458,6 +458,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.css_support = 0;
 	kvm->arch.use_irqchip = 0;
 
+	spin_lock_init(&kvm->arch.start_stop_lock);
+
 	return 0;
 out_nogmap:
 	debug_unregister(kvm->arch.dbf);
@@ -996,8 +998,15 @@ bool kvm_s390_cmma_enabled(struct kvm *kvm)
 	return true;
 }
 
+static bool ibs_enabled(struct kvm_vcpu *vcpu)
+{
+	return atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_IBS;
+}
+
 static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 {
+retry:
+	s390_vcpu_unblock(vcpu);
 	/*
 	 * We use MMU_RELOAD just to re-arm the ipte notifier for the
 	 * guest prefix page. gmap_ipte_notify will wait on the ptl lock.
@@ -1005,15 +1014,34 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	 * already finished. We might race against a second unmapper that
 	 * wants to set the blocking bit. Lets just retry the request loop.
 	 */
-	while (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu)) {
+	if (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu)) {
 		int rc;
 		rc = gmap_ipte_notify(vcpu->arch.gmap,
 				      vcpu->arch.sie_block->prefix,
 				      PAGE_SIZE * 2);
 		if (rc)
 			return rc;
-		s390_vcpu_unblock(vcpu);
+		goto retry;
+	}
+
+	if (kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu)) {
+		if (!ibs_enabled(vcpu)) {
+			trace_kvm_s390_enable_disable_ibs(vcpu->vcpu_id, 1);
+			atomic_set_mask(CPUSTAT_IBS,
+					&vcpu->arch.sie_block->cpuflags);
+		}
+		goto retry;
 	}
+
+	if (kvm_check_request(KVM_REQ_DISABLE_IBS, vcpu)) {
+		if (ibs_enabled(vcpu)) {
+			trace_kvm_s390_enable_disable_ibs(vcpu->vcpu_id, 0);
+			atomic_clear_mask(CPUSTAT_IBS,
+					  &vcpu->arch.sie_block->cpuflags);
+		}
+		goto retry;
+	}
+
 	return 0;
 }
 
@@ -1362,16 +1390,107 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return kvm_s390_store_status_unloaded(vcpu, addr);
 }
 
+static inline int is_vcpu_stopped(struct kvm_vcpu *vcpu)
+{
+	return atomic_read(&(vcpu)->arch.sie_block->cpuflags) & CPUSTAT_STOPPED;
+}
+
+static void __disable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
+{
+	kvm_check_request(KVM_REQ_ENABLE_IBS, vcpu);
+	kvm_make_request(KVM_REQ_DISABLE_IBS, vcpu);
+	exit_sie_sync(vcpu);
+}
+
+static void __disable_ibs_on_all_vcpus(struct kvm *kvm)
+{
+	unsigned int i;
+	struct kvm_vcpu *vcpu;
+
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		__disable_ibs_on_vcpu(vcpu);
+	}
+}
+
+static void __enable_ibs_on_vcpu(struct kvm_vcpu *vcpu)
+{
+	kvm_check_request(KVM_REQ_DISABLE_IBS, vcpu);
+	kvm_make_request(KVM_REQ_ENABLE_IBS, vcpu);
+	exit_sie_sync(vcpu);
+}
+
 void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
 {
+	int i, online_vcpus, started_vcpus = 0;
+
+	if (!is_vcpu_stopped(vcpu))
+		return;
+
 	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 1);
+	/* Only one cpu at a time may enter/leave the STOPPED state. */
+	spin_lock_bh(&vcpu->kvm->arch.start_stop_lock);
+	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
+
+	for (i = 0; i < online_vcpus; i++) {
+		if (!is_vcpu_stopped(vcpu->kvm->vcpus[i]))
+			started_vcpus++;
+	}
+
+	if (started_vcpus == 0) {
+		/* we're the only active VCPU -> speed it up */
+		__enable_ibs_on_vcpu(vcpu);
+	} else if (started_vcpus == 1) {
+		/*
+		 * As we are starting a second VCPU, we have to disable
+		 * the IBS facility on all VCPUs to remove potentially
+		 * oustanding ENABLE requests.
+		 */
+		__disable_ibs_on_all_vcpus(vcpu->kvm);
+	}
+
 	atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	/*
+	 * Another VCPU might have used IBS while we were offline.
+	 * Let's play safe and flush the VCPU at startup.
+	 */
+	vcpu->arch.sie_block->ihcpu  = 0xffff;
+	spin_unlock_bh(&vcpu->kvm->arch.start_stop_lock);
+	return;
 }
 
 void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
 {
+	int i, online_vcpus, started_vcpus = 0;
+	struct kvm_vcpu *started_vcpu = NULL;
+
+	if (is_vcpu_stopped(vcpu))
+		return;
+
 	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 0);
+	/* Only one cpu at a time may enter/leave the STOPPED state. */
+	spin_lock_bh(&vcpu->kvm->arch.start_stop_lock);
+	online_vcpus = atomic_read(&vcpu->kvm->online_vcpus);
+
 	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	__disable_ibs_on_vcpu(vcpu);
+
+	for (i = 0; i < online_vcpus; i++) {
+		if (!is_vcpu_stopped(vcpu->kvm->vcpus[i])) {
+			started_vcpus++;
+			started_vcpu = vcpu->kvm->vcpus[i];
+		}
+	}
+
+	if (started_vcpus == 1) {
+		/*
+		 * As we only have one VCPU left, we want to enable the
+		 * IBS facility for that VCPU to speed it up.
+		 */
+		__enable_ibs_on_vcpu(started_vcpu);
+	}
+
+	spin_unlock_bh(&vcpu->kvm->arch.start_stop_lock);
+	return;
 }
 
 static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,

commit 6852d7b69b4949234c3a8ae1f279f6a4c6563662
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Mar 14 10:59:29 2014 +0100

    KVM: s390: introduce kvm_s390_vcpu_{start,stop}
    
    This patch introduces two new functions to set/clear the CPUSTAT_STOPPED bit and
    makes use of it at all applicable places. These functions prepare the additional
    execution of code when starting/stopping a vcpu.
    
    The CPUSTAT_STOPPED bit should not be touched outside of these functions.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b32c42cbc706..6c972d229ace 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -592,7 +592,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->pp = 0;
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
-	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_vcpu_stop(vcpu);
 	kvm_s390_clear_local_irqs(vcpu);
 }
 
@@ -1235,7 +1235,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
-	atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_vcpu_start(vcpu);
 
 	switch (kvm_run->exit_reason) {
 	case KVM_EXIT_S390_SIEIC:
@@ -1362,6 +1362,18 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return kvm_s390_store_status_unloaded(vcpu, addr);
 }
 
+void kvm_s390_vcpu_start(struct kvm_vcpu *vcpu)
+{
+	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 1);
+	atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+}
+
+void kvm_s390_vcpu_stop(struct kvm_vcpu *vcpu)
+{
+	trace_kvm_s390_vcpu_start_stop(vcpu->vcpu_id, 0);
+	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+}
+
 static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 				     struct kvm_enable_cap *cap)
 {

commit 67335e63c9ef59e97b45a08b4a6a93767762031d
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 25 17:09:08 2014 +0100

    KVM: s390: Drop pending interrupts on guest exit
    
    On hard exits (abort, sigkill) we have have some kvm_s390_interrupt_info
    structures hanging around. Delete those on exit to avoid memory leaks.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    CC: stable@vger.kernel.org
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e6bbfe1a9474..b32c42cbc706 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -471,6 +471,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
 	trace_kvm_s390_destroy_vcpu(vcpu->vcpu_id);
+	kvm_s390_clear_local_irqs(vcpu);
 	kvm_clear_async_pf_completion_queue(vcpu);
 	if (!kvm_is_ucontrol(vcpu->kvm)) {
 		clear_bit(63 - vcpu->vcpu_id,
@@ -520,6 +521,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	if (!kvm_is_ucontrol(kvm))
 		gmap_free(kvm->arch.gmap);
 	kvm_s390_destroy_adapters(kvm);
+	kvm_s390_clear_float_irqs(kvm);
 }
 
 /* Section: vcpu related */

commit 27291e2165b6de70c476b7b675308113edd69a60
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Jan 23 12:26:52 2014 +0100

    KVM: s390: hardware support for guest debugging
    
    This patch adds support to debug the guest using the PER facility on s390.
    Single-stepping, hardware breakpoints and hardware watchpoints are supported. In
    order to use the PER facility of the guest without it noticing it, the control
    registers of the guest have to be patched and access to them has to be
    intercepted(stctl, stctg, lctl, lctlg).
    
    All PER program interrupts have to be intercepted and only the relevant PER
    interrupts for the guest have to be given back. Special care has to be taken
    about repeated exits on the same hardware breakpoint. The intervention of the
    host in the guests PER configuration is not fully transparent. PER instruction
    nullification can not be used by the guest and too many storage alteration
    events may be reported to the guest (if it is activated for special address
    ranges only) when the host concurrently debugging it.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7ae8c26065fb..e6bbfe1a9474 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -934,10 +934,40 @@ int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
+#define VALID_GUESTDBG_FLAGS (KVM_GUESTDBG_SINGLESTEP | \
+			      KVM_GUESTDBG_USE_HW_BP | \
+			      KVM_GUESTDBG_ENABLE)
+
 int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 					struct kvm_guest_debug *dbg)
 {
-	return -EINVAL; /* not implemented yet */
+	int rc = 0;
+
+	vcpu->guest_debug = 0;
+	kvm_s390_clear_bp_data(vcpu);
+
+	if (vcpu->guest_debug & ~VALID_GUESTDBG_FLAGS)
+		return -EINVAL;
+
+	if (dbg->control & KVM_GUESTDBG_ENABLE) {
+		vcpu->guest_debug = dbg->control;
+		/* enforce guest PER */
+		atomic_set_mask(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+
+		if (dbg->control & KVM_GUESTDBG_USE_HW_BP)
+			rc = kvm_s390_import_bp_data(vcpu, dbg);
+	} else {
+		atomic_clear_mask(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+		vcpu->arch.guestdbg.last_bp = 0;
+	}
+
+	if (rc) {
+		vcpu->guest_debug = 0;
+		kvm_s390_clear_bp_data(vcpu);
+		atomic_clear_mask(CPUSTAT_P, &vcpu->arch.sie_block->cpuflags);
+	}
+
+	return rc;
 }
 
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
@@ -1095,6 +1125,11 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 	if (rc)
 		return rc;
 
+	if (guestdbg_enabled(vcpu)) {
+		kvm_s390_backup_guest_per_regs(vcpu);
+		kvm_s390_patch_guest_per_regs(vcpu);
+	}
+
 	vcpu->arch.sie_block->icptcode = 0;
 	cpuflags = atomic_read(&vcpu->arch.sie_block->cpuflags);
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x", cpuflags);
@@ -1111,6 +1146,9 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 		   vcpu->arch.sie_block->icptcode);
 	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
 
+	if (guestdbg_enabled(vcpu))
+		kvm_s390_restore_guest_per_regs(vcpu);
+
 	if (exit_reason >= 0) {
 		rc = 0;
 	} else if (kvm_is_ucontrol(vcpu->kvm)) {
@@ -1176,7 +1214,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
 
 		rc = vcpu_post_run(vcpu, exit_reason);
-	} while (!signal_pending(current) && !rc);
+	} while (!signal_pending(current) && !guestdbg_exit_pending(vcpu) && !rc);
 
 	srcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);
 	return rc;
@@ -1187,6 +1225,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	int rc;
 	sigset_t sigsaved;
 
+	if (guestdbg_exit_pending(vcpu)) {
+		kvm_s390_prepare_debug_exit(vcpu);
+		return 0;
+	}
+
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
@@ -1199,6 +1242,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	case KVM_EXIT_S390_RESET:
 	case KVM_EXIT_S390_UCONTROL:
 	case KVM_EXIT_S390_TSCH:
+	case KVM_EXIT_DEBUG:
 		break;
 	default:
 		BUG();
@@ -1224,6 +1268,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = -EINTR;
 	}
 
+	if (guestdbg_exit_pending(vcpu) && !rc)  {
+		kvm_s390_prepare_debug_exit(vcpu);
+		rc = 0;
+	}
+
 	if (rc == -EOPNOTSUPP) {
 		/* intercept cannot be handled in-kernel, prepare kvm-run */
 		kvm_run->exit_reason         = KVM_EXIT_S390_SIEIC;

commit aba0750889d012f84a719112997abb7be11bba4b
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Jan 23 10:47:13 2014 +0100

    KVM: s390: emulate stctl and stctg
    
    Introduce the methods to emulate the stctl and stctg instruction. Added tracing
    code.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ae7c1265fcc1..7ae8c26065fb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -52,6 +52,8 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
 	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
+	{ "instruction_stctl", VCPU_STAT(instruction_stctl) },
+	{ "instruction_stctg", VCPU_STAT(instruction_stctg) },
 	{ "deliver_emergency_signal", VCPU_STAT(deliver_emergency_signal) },
 	{ "deliver_external_call", VCPU_STAT(deliver_external_call) },
 	{ "deliver_service_signal", VCPU_STAT(deliver_service_signal) },

commit d0bce6054a1759f1b2c86bf553801c77dcaca745
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 1 16:45:58 2014 +0100

    KVM: s390: convert kvm_s390_store_status_unloaded()
    
    Convert kvm_s390_store_status_unloaded() to new guest access functions.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6cee19c7ad74..ae7c1265fcc1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1249,83 +1249,50 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	return rc;
 }
 
-static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, void *from,
-		       unsigned long n, int prefix)
-{
-	if (prefix)
-		return copy_to_guest(vcpu, guestdest, from, n);
-	else
-		return copy_to_guest_absolute(vcpu, guestdest, from, n);
-}
-
 /*
  * store status at address
  * we use have two special cases:
  * KVM_S390_STORE_STATUS_NOADDR: -> 0x1200 on 64 bit
  * KVM_S390_STORE_STATUS_PREFIXED: -> prefix
  */
-int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long addr)
+int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long gpa)
 {
 	unsigned char archmode = 1;
-	int prefix;
 	u64 clkcomp;
+	int rc;
 
-	if (addr == KVM_S390_STORE_STATUS_NOADDR) {
-		if (copy_to_guest_absolute(vcpu, 163ul, &archmode, 1))
+	if (gpa == KVM_S390_STORE_STATUS_NOADDR) {
+		if (write_guest_abs(vcpu, 163, &archmode, 1))
 			return -EFAULT;
-		addr = SAVE_AREA_BASE;
-		prefix = 0;
-	} else if (addr == KVM_S390_STORE_STATUS_PREFIXED) {
-		if (copy_to_guest(vcpu, 163ul, &archmode, 1))
+		gpa = SAVE_AREA_BASE;
+	} else if (gpa == KVM_S390_STORE_STATUS_PREFIXED) {
+		if (write_guest_real(vcpu, 163, &archmode, 1))
 			return -EFAULT;
-		addr = SAVE_AREA_BASE;
-		prefix = 1;
-	} else
-		prefix = 0;
-
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, fp_regs),
-			vcpu->arch.guest_fpregs.fprs, 128, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, gp_regs),
-			vcpu->run->s.regs.gprs, 128, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, psw),
-			&vcpu->arch.sie_block->gpsw, 16, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, pref_reg),
-			&vcpu->arch.sie_block->prefix, 4, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu,
-			addr + offsetof(struct save_area, fp_ctrl_reg),
-			&vcpu->arch.guest_fpregs.fpc, 4, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, tod_reg),
-			&vcpu->arch.sie_block->todpr, 4, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, timer),
-			&vcpu->arch.sie_block->cputm, 8, prefix))
-		return -EFAULT;
-
+		gpa = kvm_s390_real_to_abs(vcpu, SAVE_AREA_BASE);
+	}
+	rc = write_guest_abs(vcpu, gpa + offsetof(struct save_area, fp_regs),
+			     vcpu->arch.guest_fpregs.fprs, 128);
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, gp_regs),
+			      vcpu->run->s.regs.gprs, 128);
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, psw),
+			      &vcpu->arch.sie_block->gpsw, 16);
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, pref_reg),
+			      &vcpu->arch.sie_block->prefix, 4);
+	rc |= write_guest_abs(vcpu,
+			      gpa + offsetof(struct save_area, fp_ctrl_reg),
+			      &vcpu->arch.guest_fpregs.fpc, 4);
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, tod_reg),
+			      &vcpu->arch.sie_block->todpr, 4);
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, timer),
+			      &vcpu->arch.sie_block->cputm, 8);
 	clkcomp = vcpu->arch.sie_block->ckc >> 8;
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, clk_cmp),
-			&clkcomp, 8, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area, acc_regs),
-			&vcpu->run->s.regs.acrs, 64, prefix))
-		return -EFAULT;
-
-	if (__guestcopy(vcpu,
-			addr + offsetof(struct save_area, ctrl_regs),
-			&vcpu->arch.sie_block->gcr, 128, prefix))
-		return -EFAULT;
-	return 0;
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, clk_cmp),
+			      &clkcomp, 8);
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, acc_regs),
+			      &vcpu->run->s.regs.acrs, 64);
+	rc |= write_guest_abs(vcpu, gpa + offsetof(struct save_area, ctrl_regs),
+			      &vcpu->arch.sie_block->gcr, 128);
+	return rc ? -EFAULT : 0;
 }
 
 int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)

commit 81480cc19c56efb573c63d89c46797d34fe58f27
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 1 16:36:07 2014 +0100

    KVM: s390: convert pfault code
    
    Convert pfault code to new guest access functions.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e1dfe2461d4b..6cee19c7ad74 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1058,8 +1058,9 @@ static int kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu)
 	if (!vcpu->arch.gmap->pfault_enabled)
 		return 0;
 
-	hva = gmap_fault(current->thread.gmap_addr, vcpu->arch.gmap);
-	if (copy_from_guest(vcpu, &arch.pfault_token, vcpu->arch.pfault_token, 8))
+	hva = gfn_to_hva(vcpu->kvm, gpa_to_gfn(current->thread.gmap_addr));
+	hva += current->thread.gmap_addr & ~PAGE_MASK;
+	if (read_guest_real(vcpu, vcpu->arch.pfault_token, &arch.pfault_token, 8))
 		return 0;
 
 	rc = kvm_setup_async_pf(vcpu, current->thread.gmap_addr, hva, &arch);

commit 8a242234b4bfed37f7fbd9b0b16f8088f31ca140
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jan 10 14:33:28 2014 +0100

    KVM: s390: make use of ipte lock
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 927ba7361da9..e1dfe2461d4b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -67,6 +67,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_stpx", VCPU_STAT(instruction_stpx) },
 	{ "instruction_stap", VCPU_STAT(instruction_stap) },
 	{ "instruction_storage_key", VCPU_STAT(instruction_storage_key) },
+	{ "instruction_ipte_interlock", VCPU_STAT(instruction_ipte_interlock) },
 	{ "instruction_stsch", VCPU_STAT(instruction_stsch) },
 	{ "instruction_chsc", VCPU_STAT(instruction_chsc) },
 	{ "instruction_essa", VCPU_STAT(instruction_essa) },
@@ -437,6 +438,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	spin_lock_init(&kvm->arch.float_int.lock);
 	INIT_LIST_HEAD(&kvm->arch.float_int.list);
+	init_waitqueue_head(&kvm->arch.ipte_wq);
 
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 	VM_EVENT(kvm, 3, "%s", "vm created");

commit 217a440683b51463f53e397cfdda27d7e92bf275
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Dec 30 12:54:14 2013 +0100

    KVM: s390/sclp: correctly set eca siif bit
    
    Check if siif is available before setting.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4b1df682e5c3..927ba7361da9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -625,7 +625,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 		vcpu->arch.sie_block->ecb |= 0x10;
 
 	vcpu->arch.sie_block->ecb2  = 8;
-	vcpu->arch.sie_block->eca   = 0xC1002001U;
+	vcpu->arch.sie_block->eca   = 0xC1002000U;
+	if (sclp_has_siif())
+		vcpu->arch.sie_block->eca |= 1;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 	if (kvm_s390_cmma_enabled(vcpu->kvm)) {

commit 280ef0f1f982e4bd9a22ee3627ca06dccde59a64
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Dec 17 09:08:28 2013 +0100

    KVM: s390: export test_vfacility()
    
    Make test_vfacility() available for other files. This is needed for the
    new guest access functions, which test if certain facilities are available
    for a guest.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fe2396cc0856..4b1df682e5c3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -91,7 +91,7 @@ unsigned long *vfacilities;
 static struct gmap_notifier gmap_notifier;
 
 /* test availability of vfacility */
-static inline int test_vfacility(unsigned long nr)
+int test_vfacility(unsigned long nr)
 {
 	return __test_facility(nr, (void *) vfacilities);
 }

commit 4f718eab262868aeab83774ec4ce25e6a9906657
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Wed Apr 9 13:13:00 2014 +0200

    KVM: s390: Exploiting generic userspace interface for cmma
    
    To enable CMMA and to reset its state we use the vm kvm_device ioctls,
    encapsulating attributes within the KVM_S390_VM_MEM_CTRL group.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fc2fe49488c0..fe2396cc0856 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -258,11 +258,43 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 	return r;
 }
 
+static int kvm_s390_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret;
+	unsigned int idx;
+	switch (attr->attr) {
+	case KVM_S390_VM_MEM_ENABLE_CMMA:
+		ret = -EBUSY;
+		mutex_lock(&kvm->lock);
+		if (atomic_read(&kvm->online_vcpus) == 0) {
+			kvm->arch.use_cmma = 1;
+			ret = 0;
+		}
+		mutex_unlock(&kvm->lock);
+		break;
+	case KVM_S390_VM_MEM_CLR_CMMA:
+		mutex_lock(&kvm->lock);
+		idx = srcu_read_lock(&kvm->srcu);
+		page_table_reset_pgste(kvm->arch.gmap->mm, 0, TASK_SIZE, false);
+		srcu_read_unlock(&kvm->srcu, idx);
+		mutex_unlock(&kvm->lock);
+		ret = 0;
+		break;
+	default:
+		ret = -ENXIO;
+		break;
+	}
+	return ret;
+}
+
 static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 {
 	int ret;
 
 	switch (attr->group) {
+	case KVM_S390_VM_MEM_CTRL:
+		ret = kvm_s390_mem_control(kvm, attr);
+		break;
 	default:
 		ret = -ENXIO;
 		break;
@@ -281,6 +313,17 @@ static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 	int ret;
 
 	switch (attr->group) {
+	case KVM_S390_VM_MEM_CTRL:
+		switch (attr->attr) {
+		case KVM_S390_VM_MEM_ENABLE_CMMA:
+		case KVM_S390_VM_MEM_CLR_CMMA:
+			ret = 0;
+			break;
+		default:
+			ret = -ENXIO;
+			break;
+		}
+		break;
 	default:
 		ret = -ENXIO;
 		break;

commit b31605c12f4edda7a40ad9f86985739666298c31
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Tue Mar 25 13:47:11 2014 +0100

    KVM: s390: make cmma usage conditionally
    
    When userspace reset the guest without notifying kvm, the CMMA state
    of the pages might be unused, resulting in guest data corruption.
    To avoid this, CMMA must be enabled only if userspace understands
    the implications.
    
    CMMA must be enabled before vCPU creation. It can't be switched off
    once enabled.  All subsequently created vCPUs will be enabled for
    CMMA according to the CMMA state of the VM.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [remove now unnecessary calls to page_table_reset_pgste]

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c335a2efa5de..fc2fe49488c0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -437,9 +437,8 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	if (kvm_is_ucontrol(vcpu->kvm))
 		gmap_free(vcpu->arch.gmap);
 
-	if (vcpu->arch.sie_block->cbrlo)
-		__free_page(__pfn_to_page(
-				vcpu->arch.sie_block->cbrlo >> PAGE_SHIFT));
+	if (kvm_s390_cmma_enabled(vcpu->kvm))
+		kvm_s390_vcpu_unsetup_cmma(vcpu);
 	free_page((unsigned long)(vcpu->arch.sie_block));
 
 	kvm_vcpu_uninit(vcpu);
@@ -553,9 +552,26 @@ int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+void kvm_s390_vcpu_unsetup_cmma(struct kvm_vcpu *vcpu)
+{
+	free_page(vcpu->arch.sie_block->cbrlo);
+	vcpu->arch.sie_block->cbrlo = 0;
+}
+
+int kvm_s390_vcpu_setup_cmma(struct kvm_vcpu *vcpu)
+{
+	vcpu->arch.sie_block->cbrlo = get_zeroed_page(GFP_KERNEL);
+	if (!vcpu->arch.sie_block->cbrlo)
+		return -ENOMEM;
+
+	vcpu->arch.sie_block->ecb2 |= 0x80;
+	vcpu->arch.sie_block->ecb2 &= ~0x08;
+	return 0;
+}
+
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
-	struct page *cbrl;
+	int rc = 0;
 
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |
 						    CPUSTAT_SM |
@@ -569,13 +585,10 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
-	if (kvm_enabled_cmma()) {
-		cbrl = alloc_page(GFP_KERNEL | __GFP_ZERO);
-		if (cbrl) {
-			vcpu->arch.sie_block->ecb2 |= 0x80;
-			vcpu->arch.sie_block->ecb2 &= ~0x08;
-			vcpu->arch.sie_block->cbrlo = page_to_phys(cbrl);
-		}
+	if (kvm_s390_cmma_enabled(vcpu->kvm)) {
+		rc = kvm_s390_vcpu_setup_cmma(vcpu);
+		if (rc)
+			return rc;
 	}
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 	tasklet_init(&vcpu->arch.tasklet, kvm_s390_tasklet,
@@ -583,7 +596,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 	get_cpu_id(&vcpu->arch.cpu_id);
 	vcpu->arch.cpu_id.version = 0xff;
-	return 0;
+	return rc;
 }
 
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
@@ -890,6 +903,18 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
+bool kvm_s390_cmma_enabled(struct kvm *kvm)
+{
+	if (!MACHINE_IS_LPAR)
+		return false;
+	/* only enable for z10 and later */
+	if (!MACHINE_HAS_EDAT1)
+		return false;
+	if (!kvm->arch.use_cmma)
+		return false;
+	return true;
+}
+
 static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -1072,16 +1097,6 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 	return rc;
 }
 
-bool kvm_enabled_cmma(void)
-{
-	if (!MACHINE_IS_LPAR)
-		return false;
-	/* only enable for z10 and later */
-	if (!MACHINE_HAS_EDAT1)
-		return false;
-	return true;
-}
-
 static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc, exit_reason;

commit f2061656209fb9a5d54bbb1999f0a633438504e7
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Wed Apr 9 13:13:00 2014 +0200

    KVM: s390: Per-vm kvm device controls
    
    We sometimes need to get/set attributes specific to a virtual machine
    and so need something else than ONE_REG.
    
    Let's copy the KVM_DEVICE approach, and define the respective ioctls
    for the vm file descriptor.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Acked-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 346a3478dd00..c335a2efa5de 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -162,6 +162,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
 	case KVM_CAP_ENABLE_CAP_VM:
+	case KVM_CAP_VM_ATTRIBUTES:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -257,11 +258,43 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 	return r;
 }
 
+static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret;
+
+	switch (attr->group) {
+	default:
+		ret = -ENXIO;
+		break;
+	}
+
+	return ret;
+}
+
+static int kvm_s390_vm_get_attr(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	return -ENXIO;
+}
+
+static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
+{
+	int ret;
+
+	switch (attr->group) {
+	default:
+		ret = -ENXIO;
+		break;
+	}
+
+	return ret;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg)
 {
 	struct kvm *kvm = filp->private_data;
 	void __user *argp = (void __user *)arg;
+	struct kvm_device_attr attr;
 	int r;
 
 	switch (ioctl) {
@@ -294,6 +327,27 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		}
 		break;
 	}
+	case KVM_SET_DEVICE_ATTR: {
+		r = -EFAULT;
+		if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
+			break;
+		r = kvm_s390_vm_set_attr(kvm, &attr);
+		break;
+	}
+	case KVM_GET_DEVICE_ATTR: {
+		r = -EFAULT;
+		if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
+			break;
+		r = kvm_s390_vm_get_attr(kvm, &attr);
+		break;
+	}
+	case KVM_HAS_DEVICE_ATTR: {
+		r = -EFAULT;
+		if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
+			break;
+		r = kvm_s390_vm_has_attr(kvm, &attr);
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit 15f36ebd34b5b296c274ef67024f14c2d394a507
Author: Jason J. Herne <jjherne@us.ibm.com>
Date:   Thu Aug 2 10:10:17 2012 -0400

    KVM: s390: Add proper dirty bitmap support to S390 kvm.
    
    Replace the kvm_s390_sync_dirty_log() stub with code to construct the KVM
    dirty_bitmap from S390 memory change bits.  Also add code to properly clear
    the dirty_bitmap size when clearing the bitmap.
    
    Signed-off-by: Jason J. Herne <jjherne@us.ibm.com>
    CC: Dominik Dingel <dingel@linux.vnet.ibm.com>
    [Dominik Dingel: use gmap_test_and_clear_dirty, locking fixes]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b767ec97368a..346a3478dd00 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -11,6 +11,7 @@
  *               Christian Borntraeger <borntraeger@de.ibm.com>
  *               Heiko Carstens <heiko.carstens@de.ibm.com>
  *               Christian Ehrhardt <ehrhardt@de.ibm.com>
+ *               Jason J. Herne <jjherne@us.ibm.com>
  */
 
 #include <linux/compiler.h>
@@ -179,6 +180,25 @@ int kvm_dev_ioctl_check_extension(long ext)
 	return r;
 }
 
+static void kvm_s390_sync_dirty_log(struct kvm *kvm,
+					struct kvm_memory_slot *memslot)
+{
+	gfn_t cur_gfn, last_gfn;
+	unsigned long address;
+	struct gmap *gmap = kvm->arch.gmap;
+
+	down_read(&gmap->mm->mmap_sem);
+	/* Loop over all guest pages */
+	last_gfn = memslot->base_gfn + memslot->npages;
+	for (cur_gfn = memslot->base_gfn; cur_gfn <= last_gfn; cur_gfn++) {
+		address = gfn_to_hva_memslot(memslot, cur_gfn);
+
+		if (gmap_test_and_clear_dirty(address, gmap))
+			mark_page_dirty(kvm, cur_gfn);
+	}
+	up_read(&gmap->mm->mmap_sem);
+}
+
 /* Section: vm related */
 /*
  * Get (and clear) the dirty memory log for a memory slot.
@@ -186,7 +206,36 @@ int kvm_dev_ioctl_check_extension(long ext)
 int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 			       struct kvm_dirty_log *log)
 {
-	return 0;
+	int r;
+	unsigned long n;
+	struct kvm_memory_slot *memslot;
+	int is_dirty = 0;
+
+	mutex_lock(&kvm->slots_lock);
+
+	r = -EINVAL;
+	if (log->slot >= KVM_USER_MEM_SLOTS)
+		goto out;
+
+	memslot = id_to_memslot(kvm->memslots, log->slot);
+	r = -ENOENT;
+	if (!memslot->dirty_bitmap)
+		goto out;
+
+	kvm_s390_sync_dirty_log(kvm, memslot);
+	r = kvm_get_dirty_log(kvm, log, &is_dirty);
+	if (r)
+		goto out;
+
+	/* Clear the dirty log */
+	if (is_dirty) {
+		n = kvm_dirty_bitmap_bytes(memslot);
+		memset(memslot->dirty_bitmap, 0, n);
+	}
+	r = 0;
+out:
+	mutex_unlock(&kvm->slots_lock);
+	return r;
 }
 
 static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)

commit 693ffc0802db41911ada95a3e77546f0ed1e7d00
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Tue Jan 14 18:11:14 2014 +0100

    KVM: s390: Don't enable skeys by default
    
    The first invocation of storage key operations on a given cpu will be intercepted.
    
    On these intercepts we will enable storage keys for the guest and remove the
    previously added intercepts.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b3ecb8f5b6ce..b767ec97368a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -465,6 +465,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->ecb2  = 8;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
+	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
 	if (kvm_enabled_cmma()) {
 		cbrl = alloc_page(GFP_KERNEL | __GFP_ZERO);
 		if (cbrl) {

commit 7cbb39d4d4d530dff12f2ff06ed6c85c504ba91a
Merge: 64056a94256e 7227fc066660
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 2 14:50:10 2014 -0700

    Merge tag 'kvm-3.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "PPC and ARM do not have much going on this time.  Most of the cool
      stuff, instead, is in s390 and (after a few releases) x86.
    
      ARM has some caching fixes and PPC has transactional memory support in
      guests.  MIPS has some fixes, with more probably coming in 3.16 as
      QEMU will soon get support for MIPS KVM.
    
      For x86 there are optimizations for debug registers, which trigger on
      some Windows games, and other important fixes for Windows guests.  We
      now expose to the guest Broadwell instruction set extensions and also
      Intel MPX.  There's also a fix/workaround for OS X guests, nested
      virtualization features (preemption timer), and a couple kvmclock
      refinements.
    
      For s390, the main news is asynchronous page faults, together with
      improvements to IRQs (floating irqs and adapter irqs) that speed up
      virtio devices"
    
    * tag 'kvm-3.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (96 commits)
      KVM: PPC: Book3S HV: Save/restore host PMU registers that are new in POWER8
      KVM: PPC: Book3S HV: Fix decrementer timeouts with non-zero TB offset
      KVM: PPC: Book3S HV: Don't use kvm_memslots() in real mode
      KVM: PPC: Book3S HV: Return ENODEV error rather than EIO
      KVM: PPC: Book3S: Trim top 4 bits of physical address in RTAS code
      KVM: PPC: Book3S HV: Add get/set_one_reg for new TM state
      KVM: PPC: Book3S HV: Add transactional memory support
      KVM: Specify byte order for KVM_EXIT_MMIO
      KVM: vmx: fix MPX detection
      KVM: PPC: Book3S HV: Fix KVM hang with CONFIG_KVM_XICS=n
      KVM: PPC: Book3S: Introduce hypervisor call H_GET_TCE
      KVM: PPC: Book3S HV: Fix incorrect userspace exit on ioeventfd write
      KVM: s390: clear local interrupts at cpu initial reset
      KVM: s390: Fix possible memory leak in SIGP functions
      KVM: s390: fix calculation of idle_mask array size
      KVM: s390: randomize sca address
      KVM: ioapic: reinject pending interrupts on KVM_SET_IRQCHIP
      KVM: Bump KVM_MAX_IRQ_ROUTES for s390
      KVM: s390: irq routing for adapter interrupts.
      KVM: s390: adapter interrupt sources
      ...

commit f7b9ddb8a5e8a25954217fa25d114bd8cf4491a4
Merge: ea2108c93080 2ed10cc15e7e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Mar 25 15:44:06 2014 +0100

    Merge tag 'kvm-s390-20140325' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kvm-next
    
    3 fixes
    - memory leak on certain SIGP conditions
    - wrong size for idle bitmap (always too big)
    - clear local interrupts on initial CPU reset
    
    1 performance improvement
    - improve performance with many guests on certain workloads

commit 2ed10cc15e7edf2daf22ce807a877a1266e97711
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Tue Feb 11 13:48:07 2014 +0100

    KVM: s390: clear local interrupts at cpu initial reset
    
    Empty list of local interrupts when vcpu goes through initial reset
    to provide a clean state
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a02979f1e1af..83b79447de55 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -395,6 +395,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
 	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
+	kvm_s390_clear_local_irqs(vcpu);
 }
 
 int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)

commit f6c137ff00a478ae619deea8650829dd2f8e71b9
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Mar 19 11:18:29 2014 +0100

    KVM: s390: randomize sca address
    
    We allocate a page for the 2k sca, so lets use the space to improve
    hit rate of some internal cpu caches. No need to change the freeing
    of the page, as this will shift away the page offset bits anyway.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7337c577e949..a02979f1e1af 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -215,6 +215,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int rc;
 	char debug_name[16];
+	static unsigned long sca_offset;
 
 	rc = -EINVAL;
 #ifdef CONFIG_KVM_S390_UCONTROL
@@ -236,6 +237,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.sca = (struct sca_block *) get_zeroed_page(GFP_KERNEL);
 	if (!kvm->arch.sca)
 		goto out_err;
+	spin_lock(&kvm_lock);
+	sca_offset = (sca_offset + 16) & 0x7f0;
+	kvm->arch.sca = (struct sca_block *) ((char *) kvm->arch.sca + sca_offset);
+	spin_unlock(&kvm_lock);
 
 	sprintf(debug_name, "kvm-%u", current->pid);
 

commit 84223598778ba08041f4297fda485df83414d57e
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Mon Jul 15 13:36:01 2013 +0200

    KVM: s390: irq routing for adapter interrupts.
    
    Introduce a new interrupt class for s390 adapter interrupts and enable
    irqfds for s390.
    
    This is depending on a new s390 specific vm capability, KVM_CAP_S390_IRQCHIP,
    that needs to be enabled by userspace.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2e6fbb0b4f68..ce5b659ec531 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -196,6 +196,10 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
 		return -EINVAL;
 
 	switch (cap->cap) {
+	case KVM_CAP_S390_IRQCHIP:
+		kvm->arch.use_irqchip = 1;
+		r = 0;
+		break;
 	default:
 		r = -EINVAL;
 		break;
@@ -228,6 +232,18 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
 		break;
 	}
+	case KVM_CREATE_IRQCHIP: {
+		struct kvm_irq_routing_entry routing;
+
+		r = -EINVAL;
+		if (kvm->arch.use_irqchip) {
+			/* Set up dummy routing. */
+			memset(&routing, 0, sizeof(routing));
+			kvm_set_irq_routing(kvm, &routing, 0, 0);
+			r = 0;
+		}
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}
@@ -284,6 +300,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	}
 
 	kvm->arch.css_support = 0;
+	kvm->arch.use_irqchip = 0;
 
 	return 0;
 out_nogmap:

commit 841b91c584b6d1e2a2cb508bd2d0236cd37e1750
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Mon Jul 15 13:36:01 2013 +0200

    KVM: s390: adapter interrupt sources
    
    Add a new interface to register/deregister sources of adapter interrupts
    identified by an unique id via the flic. Adapters may also be maskable
    and carry a list of pinned pages.
    
    These adapters will be used by irq routing later.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9f1e99f12d4f..2e6fbb0b4f68 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -343,6 +343,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	debug_unregister(kvm->arch.dbf);
 	if (!kvm_is_ucontrol(kvm))
 		gmap_free(kvm->arch.gmap);
+	kvm_s390_destroy_adapters(kvm);
 }
 
 /* Section: vcpu related */

commit d938dc55225a7212e7f31c5a8571da304cc3de16
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Wed Oct 23 18:26:34 2013 +0200

    KVM: Add per-vm capability enablement.
    
    Allow KVM_ENABLE_CAP to act on a vm as well as on a vcpu. This makes more
    sense when the caller wants to enable a vm-related capability.
    
    s390 will be the first user; wire it up.
    
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7337c577e949..9f1e99f12d4f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -159,6 +159,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_S390_CSS_SUPPORT:
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
+	case KVM_CAP_ENABLE_CAP_VM:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -187,6 +188,21 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 	return 0;
 }
 
+static int kvm_vm_ioctl_enable_cap(struct kvm *kvm, struct kvm_enable_cap *cap)
+{
+	int r;
+
+	if (cap->flags)
+		return -EINVAL;
+
+	switch (cap->cap) {
+	default:
+		r = -EINVAL;
+		break;
+	}
+	return r;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg)
 {
@@ -204,6 +220,14 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_s390_inject_vm(kvm, &s390int);
 		break;
 	}
+	case KVM_ENABLE_CAP: {
+		struct kvm_enable_cap cap;
+		r = -EFAULT;
+		if (copy_from_user(&cap, argp, sizeof(cap)))
+			break;
+		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit 2955c83f72801245afd0fe5c560cc75b82bea9aa
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Mar 6 16:01:38 2014 +0100

    KVM: s390: Optimize ucontrol path
    
    Since commit 7c470539c95630c1f2a10f109e96f249730b75eb
    (s390/kvm: avoid automatic sie reentry) we will run through the C code
    of KVM on host interrupts instead of just reentering the guest. This
    will result in additional ucontrol exits (at least HZ per second). Let
    handle a 0 intercept in the kernel and dont return to userspace,
    even if in ucontrol mode.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    CC: stable@vger.kernel.org

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 62683576f8e2..7337c577e949 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -896,7 +896,8 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 
 	if (rc == 0) {
 		if (kvm_is_ucontrol(vcpu->kvm))
-			rc = -EOPNOTSUPP;
+			/* Don't exit for host interrupts. */
+			rc = vcpu->arch.sie_block->icptcode ? -EOPNOTSUPP : 0;
 		else
 			rc = kvm_handle_sie_intercept(vcpu);
 	}

commit fed495d25e0137028c37678a14cd1d8466c02dd3
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Mon Mar 10 15:23:34 2014 +0100

    KVM: s390: Removing untriggerable BUG_ONs
    
    The BUG_ON in kvm-s390.c is unreachable, as we get the vcpu per common code,
    which itself does this from the private_data field of the file descriptor,
    and there is no KVM_UNCREATE_VCPU.
    
    The __{set,unset}_cpu_idle BUG_ONs are not triggerable because the vcpu
    creation code already checks against KVM_MAX_VCPUS.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9136f8d40850..62683576f8e2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -949,8 +949,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 
-	BUG_ON(kvm_get_vcpu(vcpu->kvm, vcpu->vcpu_id) == NULL);
-
 	switch (kvm_run->exit_reason) {
 	case KVM_EXIT_S390_SIEIC:
 	case KVM_EXIT_UNKNOWN:

commit 1ee0bc559dc34fe36a29494faf7b7c91533bd31c
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Tue Feb 25 15:36:45 2014 +0100

    KVM: s390: get rid of local_int array
    
    We can use kvm_get_vcpu() now and don't need the
    local_int array in the floating_int struct anymore.
    This also means we don't have to hold the float_int.lock
    in some places.
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a3e4c07ec3a5..9136f8d40850 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -460,11 +460,8 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	INIT_LIST_HEAD(&vcpu->arch.local_int.list);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
-	spin_lock(&kvm->arch.float_int.lock);
-	kvm->arch.float_int.local_int[id] = &vcpu->arch.local_int;
 	vcpu->arch.local_int.wq = &vcpu->wq;
 	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
-	spin_unlock(&kvm->arch.float_int.lock);
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
@@ -952,7 +949,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 
-	BUG_ON(vcpu->kvm->arch.float_int.local_int[vcpu->vcpu_id] == NULL);
+	BUG_ON(kvm_get_vcpu(vcpu->kvm, vcpu->vcpu_id) == NULL);
 
 	switch (kvm_run->exit_reason) {
 	case KVM_EXIT_S390_SIEIC:

commit afa45ff521130cee79a50b565693388be8c8c9c2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Feb 10 15:39:23 2014 +0100

    KVM: s390: expose gbea register to userspace
    
    For migration/reset we want to expose the guest breaking event
    address register to userspace. Lets use ONE_REG for that purpose.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Jason J. Herne <jjherne@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0262936e18be..a3e4c07ec3a5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -576,6 +576,10 @@ static int kvm_arch_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
 		r = put_user(vcpu->arch.sie_block->pp,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_GBEA:
+		r = put_user(vcpu->arch.sie_block->gbea,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}
@@ -621,6 +625,10 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 		r = get_user(vcpu->arch.sie_block->pp,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_GBEA:
+		r = get_user(vcpu->arch.sie_block->gbea,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}

commit 672550fb682e9935e1a318bf4ac3f611a057dee1
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Feb 10 15:32:19 2014 +0100

    KVM: s390: Provide access to program parameter
    
    commit d208c79d63e06457eef077af770d23dc4cde4d43 (KVM: s390: Enable
    the LPP facility for guests) enabled the LPP instruction for guests.
    We should expose the program parameter as a pseudo register for
    migration/reset etc. Lets also reset this value on initial CPU
    reset.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: Jason J. Herne <jjherne@linux.vnet.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 18959bb75c51..0262936e18be 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -386,6 +386,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.guest_fpregs.fpc = 0;
 	asm volatile("lfpc %0" : : "Q" (vcpu->arch.guest_fpregs.fpc));
 	vcpu->arch.sie_block->gbea = 1;
+	vcpu->arch.sie_block->pp = 0;
 	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 	kvm_clear_async_pf_completion_queue(vcpu);
 	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
@@ -571,6 +572,10 @@ static int kvm_arch_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
 		r = put_user(vcpu->arch.pfault_select,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_PP:
+		r = put_user(vcpu->arch.sie_block->pp,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}
@@ -612,6 +617,10 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 		r = get_user(vcpu->arch.pfault_select,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_PP:
+		r = get_user(vcpu->arch.sie_block->pp,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}

commit f87618e870d03ac114dd5496b23f6f628af54152
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Wed Feb 26 16:14:17 2014 +0100

    KVM: s390: implementation of kvm_arch_vcpu_runnable()
    
    A vcpu is defined to be runnable if an interrupt is pending.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a5da2cc798c8..18959bb75c51 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -483,9 +483,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 {
-	/* kvm common code refers to this, but never calls it */
-	BUG();
-	return 0;
+	return kvm_cpu_has_interrupt(vcpu);
 }
 
 void s390_vcpu_block(struct kvm_vcpu *vcpu)

commit b31288fa83b2bcc8834e1e208e9526b8bd5ce361
Author: Konstantin Weitz <konstantin.weitz@gmail.com>
Date:   Wed Apr 17 17:36:29 2013 +0200

    s390/kvm: support collaborative memory management
    
    This patch enables Collaborative Memory Management (CMM) for kvm
    on s390. CMM allows the guest to inform the host about page usage
    (see arch/s390/mm/cmm.c). The host uses this information to avoid
    swapping in unused pages in the page fault handler. Further, a CPU
    provided list of unused invalid pages is processed to reclaim swap
    space of not yet accessed unused pages.
    
    [ Martin Schwidefsky: patch reordering and cleanup ]
    
    Signed-off-by: Konstantin Weitz <konstantin.weitz@gmail.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e0676f390d57..10b5db3c9bc4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -68,6 +68,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_storage_key", VCPU_STAT(instruction_storage_key) },
 	{ "instruction_stsch", VCPU_STAT(instruction_stsch) },
 	{ "instruction_chsc", VCPU_STAT(instruction_chsc) },
+	{ "instruction_essa", VCPU_STAT(instruction_essa) },
 	{ "instruction_stsi", VCPU_STAT(instruction_stsi) },
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
 	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
@@ -283,7 +284,11 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	if (kvm_is_ucontrol(vcpu->kvm))
 		gmap_free(vcpu->arch.gmap);
 
+	if (vcpu->arch.sie_block->cbrlo)
+		__free_page(__pfn_to_page(
+				vcpu->arch.sie_block->cbrlo >> PAGE_SHIFT));
 	free_page((unsigned long)(vcpu->arch.sie_block));
+
 	kvm_vcpu_uninit(vcpu);
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
@@ -390,6 +395,8 @@ int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
+	struct page *cbrl;
+
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |
 						    CPUSTAT_SM |
 						    CPUSTAT_STOPPED |
@@ -401,6 +408,14 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->ecb2  = 8;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
+	if (kvm_enabled_cmma()) {
+		cbrl = alloc_page(GFP_KERNEL | __GFP_ZERO);
+		if (cbrl) {
+			vcpu->arch.sie_block->ecb2 |= 0x80;
+			vcpu->arch.sie_block->ecb2 &= ~0x08;
+			vcpu->arch.sie_block->cbrlo = page_to_phys(cbrl);
+		}
+	}
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 	tasklet_init(&vcpu->arch.tasklet, kvm_s390_tasklet,
 		     (unsigned long) vcpu);
@@ -761,6 +776,16 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 	return rc;
 }
 
+bool kvm_enabled_cmma(void)
+{
+	if (!MACHINE_IS_LPAR)
+		return false;
+	/* only enable for z10 and later */
+	if (!MACHINE_HAS_EDAT1)
+		return false;
+	return true;
+}
+
 static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc, exit_reason;

commit 536336c21697551ceca44bdffb9f53e6cc5f2f20
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Mon Sep 30 10:55:33 2013 +0200

    KVM: async_pf: Exploit one reg interface for pfault
    
    To enable pfault after live migration we need to expose pfault_token,
    pfault_select and pfault_compare, as one reg registers to userspace.
    
    So that qemu is able to transfer this between the source and the target.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d8e9f04977db..a5da2cc798c8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -561,6 +561,18 @@ static int kvm_arch_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
 		r = put_user(vcpu->arch.sie_block->ckc,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_PFTOKEN:
+		r = put_user(vcpu->arch.pfault_token,
+			     (u64 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_PFCOMPARE:
+		r = put_user(vcpu->arch.pfault_compare,
+			     (u64 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_PFSELECT:
+		r = put_user(vcpu->arch.pfault_select,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}
@@ -590,6 +602,18 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 		r = get_user(vcpu->arch.sie_block->ckc,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_PFTOKEN:
+		r = get_user(vcpu->arch.pfault_token,
+			     (u64 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_PFCOMPARE:
+		r = get_user(vcpu->arch.pfault_compare,
+			     (u64 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_PFSELECT:
+		r = get_user(vcpu->arch.pfault_select,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}

commit 3c038e6be0e299d4d3762d0a9a29f02de6e04991
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Mon Oct 7 17:11:48 2013 +0200

    KVM: async_pf: Async page fault support on s390
    
    This patch enables async page faults for s390 kvm guests.
    It provides the userspace API to enable and disable_wait this feature.
    The disable_wait will enforce that the feature is off by waiting on it.
    Also it includes the diagnose code, called by the guest to enable async page faults.
    
    The async page faults will use an already existing guest interface for this
    purpose, as described in "CP Programming Services (SC24-6084)".
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 9eec794caa7f..d8e9f04977db 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -152,6 +152,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #ifdef CONFIG_KVM_S390_UCONTROL
 	case KVM_CAP_S390_UCONTROL:
 #endif
+	case KVM_CAP_ASYNC_PF:
 	case KVM_CAP_SYNC_REGS:
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_ENABLE_CAP:
@@ -273,6 +274,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
 	trace_kvm_s390_destroy_vcpu(vcpu->vcpu_id);
+	kvm_clear_async_pf_completion_queue(vcpu);
 	if (!kvm_is_ucontrol(vcpu->kvm)) {
 		clear_bit(63 - vcpu->vcpu_id,
 			  (unsigned long *) &vcpu->kvm->arch.sca->mcn);
@@ -322,6 +324,8 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 /* Section: vcpu related */
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
+	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
+	kvm_clear_async_pf_completion_queue(vcpu);
 	if (kvm_is_ucontrol(vcpu->kvm)) {
 		vcpu->arch.gmap = gmap_alloc(current->mm);
 		if (!vcpu->arch.gmap)
@@ -382,6 +386,8 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.guest_fpregs.fpc = 0;
 	asm volatile("lfpc %0" : : "Q" (vcpu->arch.guest_fpregs.fpc));
 	vcpu->arch.sie_block->gbea = 1;
+	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
+	kvm_clear_async_pf_completion_queue(vcpu);
 	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 }
 
@@ -713,10 +719,89 @@ static long kvm_arch_fault_in_sync(struct kvm_vcpu *vcpu)
 	return rc;
 }
 
+static void __kvm_inject_pfault_token(struct kvm_vcpu *vcpu, bool start_token,
+				      unsigned long token)
+{
+	struct kvm_s390_interrupt inti;
+	inti.parm64 = token;
+
+	if (start_token) {
+		inti.type = KVM_S390_INT_PFAULT_INIT;
+		WARN_ON_ONCE(kvm_s390_inject_vcpu(vcpu, &inti));
+	} else {
+		inti.type = KVM_S390_INT_PFAULT_DONE;
+		WARN_ON_ONCE(kvm_s390_inject_vm(vcpu->kvm, &inti));
+	}
+}
+
+void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
+				     struct kvm_async_pf *work)
+{
+	trace_kvm_s390_pfault_init(vcpu, work->arch.pfault_token);
+	__kvm_inject_pfault_token(vcpu, true, work->arch.pfault_token);
+}
+
+void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
+				 struct kvm_async_pf *work)
+{
+	trace_kvm_s390_pfault_done(vcpu, work->arch.pfault_token);
+	__kvm_inject_pfault_token(vcpu, false, work->arch.pfault_token);
+}
+
+void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
+			       struct kvm_async_pf *work)
+{
+	/* s390 will always inject the page directly */
+}
+
+bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu)
+{
+	/*
+	 * s390 will always inject the page directly,
+	 * but we still want check_async_completion to cleanup
+	 */
+	return true;
+}
+
+static int kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu)
+{
+	hva_t hva;
+	struct kvm_arch_async_pf arch;
+	int rc;
+
+	if (vcpu->arch.pfault_token == KVM_S390_PFAULT_TOKEN_INVALID)
+		return 0;
+	if ((vcpu->arch.sie_block->gpsw.mask & vcpu->arch.pfault_select) !=
+	    vcpu->arch.pfault_compare)
+		return 0;
+	if (psw_extint_disabled(vcpu))
+		return 0;
+	if (kvm_cpu_has_interrupt(vcpu))
+		return 0;
+	if (!(vcpu->arch.sie_block->gcr[0] & 0x200ul))
+		return 0;
+	if (!vcpu->arch.gmap->pfault_enabled)
+		return 0;
+
+	hva = gmap_fault(current->thread.gmap_addr, vcpu->arch.gmap);
+	if (copy_from_guest(vcpu, &arch.pfault_token, vcpu->arch.pfault_token, 8))
+		return 0;
+
+	rc = kvm_setup_async_pf(vcpu, current->thread.gmap_addr, hva, &arch);
+	return rc;
+}
+
 static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 {
 	int rc, cpuflags;
 
+	/*
+	 * On s390 notifications for arriving pages will be delivered directly
+	 * to the guest but the house keeping for completed pfaults is
+	 * handled outside the worker.
+	 */
+	kvm_check_async_pf_completion(vcpu);
+
 	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->run->s.regs.gprs[14], 16);
 
 	if (need_resched())
@@ -758,8 +843,10 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 		rc = -EREMOTE;
 
 	} else if (current->thread.gmap_pfault) {
+		trace_kvm_s390_major_guest_pfault(vcpu);
 		current->thread.gmap_pfault = 0;
-		if (kvm_arch_fault_in_sync(vcpu) >= 0)
+		if (kvm_arch_setup_async_pf(vcpu) ||
+		    (kvm_arch_fault_in_sync(vcpu) >= 0))
 			rc = 0;
 	}
 

commit 24eb3a824c4f3ccfaa2305dc1d9d9e2a708828c5
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Mon Jun 17 16:25:18 2013 +0200

    KVM: s390: Add FAULT_FLAG_RETRY_NOWAIT for guest fault
    
    In the case of a fault, we will retry to exit sie64 but with gmap fault
    indication for this thread set. This makes it possible to handle async
    page faults.
    
    Based on a patch from Martin Schwidefsky.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 782420f3c4d5..9eec794caa7f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -255,6 +255,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		if (!kvm->arch.gmap)
 			goto out_nogmap;
 		kvm->arch.gmap->private = kvm;
+		kvm->arch.gmap->pfault_enabled = 0;
 	}
 
 	kvm->arch.css_support = 0;
@@ -701,6 +702,17 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+static long kvm_arch_fault_in_sync(struct kvm_vcpu *vcpu)
+{
+	long rc;
+	hva_t fault = gmap_fault(current->thread.gmap_addr, vcpu->arch.gmap);
+	struct mm_struct *mm = current->mm;
+	down_read(&mm->mmap_sem);
+	rc = get_user_pages(current, mm, fault, 1, 1, 0, NULL, NULL);
+	up_read(&mm->mmap_sem);
+	return rc;
+}
+
 static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 {
 	int rc, cpuflags;
@@ -730,7 +742,7 @@ static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 
 static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 {
-	int rc;
+	int rc = -1;
 
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
@@ -744,7 +756,14 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 						current->thread.gmap_addr;
 		vcpu->run->s390_ucontrol.pgm_code = 0x10;
 		rc = -EREMOTE;
-	} else {
+
+	} else if (current->thread.gmap_pfault) {
+		current->thread.gmap_pfault = 0;
+		if (kvm_arch_fault_in_sync(vcpu) >= 0)
+			rc = 0;
+	}
+
+	if (rc == -1) {
 		VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
 		trace_kvm_s390_sie_fault(vcpu);
 		rc = kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);

commit c05c4186bbe4e99d64e8a36f7ca7f480da5d109f
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Mon Oct 7 16:13:45 2013 +0200

    KVM: s390: add floating irq controller
    
    This patch adds a floating irq controller as a kvm_device.
    It will be necessary for migration of floating interrupts as well
    as for hardening the reset code by allowing user space to explicitly
    remove all pending floating interrupts.
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e0676f390d57..782420f3c4d5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -157,6 +157,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_S390_CSS_SUPPORT:
 	case KVM_CAP_IOEVENTFD:
+	case KVM_CAP_DEVICE_CTRL:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:

commit c760f5e29d92adf5184589f1e616a4be146fb57c
Merge: 0dc3fd0249a2 19e4735bd7f0
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 23 11:38:13 2014 +0100

    Merge tag 'kvm-s390-20140117' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kvm-queue
    
    This deals with 2 guest features that need enablement in the kvm host:
    - transactional execution
    - lpp sampling support
    
    In addition there is also a fix to the virtio-ccw guest driver. This will
    enable future features

commit 699bde3b6c95319749a8e1b7aa2b3f6bee84bff8
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Jan 20 12:34:13 2014 +0100

    KVM: s390: Fix memory access error detection
    
    Seems that commit 210b1607012cc9034841a393e0591b2c86d9e26c
    (KVM: s390: Removed SIE_INTERCEPT_UCONTROL) lost a hunk when we
    reworked our patch queue to rework the async_fp code. We now
    ignore faults on the sie instruction (guest accesses non-existing
    memory) instead of sending a fault into the guest. This leads to
    hang situations with the old virtio transport that checks for
    descriptor memory after guest memory. Instead of bailing out this
    code now goes wild...
    Lets re-add the check.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1bb1ddaf93c0..7635c00a1479 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -738,6 +738,10 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 						current->thread.gmap_addr;
 		vcpu->run->s390_ucontrol.pgm_code = 0x10;
 		rc = -EREMOTE;
+	} else {
+		VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
+		trace_kvm_s390_sie_fault(vcpu);
+		rc = kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
 	}
 
 	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);

commit d208c79d63e06457eef077af770d23dc4cde4d43
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Thu Dec 12 13:40:40 2013 +0100

    KVM: s390: Enable the LPP facility for guests
    
    The Load-Program-Parameter Facility is available for guests without
    any further ado, so we should indicate its availability by setting
    facility bit 40 if it is supported by the host.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0084c2c272e3..597114b7ab99 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1183,7 +1183,7 @@ static int __init kvm_s390_init(void)
 		return -ENOMEM;
 	}
 	memcpy(vfacilities, S390_lowcore.stfle_fac_list, 16);
-	vfacilities[0] &= 0xff82fff3f47c2000UL;
+	vfacilities[0] &= 0xff82fff3f4fc2000UL;
 	vfacilities[1] &= 0x005c000000000000UL;
 	return 0;
 }

commit 7feb6bb8e6dbd129c11fc93bf206daa156bf1c0f
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Fri Jun 28 13:30:24 2013 +0200

    KVM: s390: enable Transactional Execution
    
    This patch enables transactional execution for KVM guests
    on s390 systems zec12 or later.
    
    We rework the allocation of the page containing the sie_block
    to also back the Interception Transaction Diagnostic Block.
    If available the TE facilities will be enabled.
    
    Setting bit 73 and 50 in vfacilities bitmask reveals the HW
    facilities Transactional Memory and Constraint Transactional
    Memory respectively to the KVM guest.
    
    Furthermore, the patch restores the Program-Interruption TDB
    from the Interception TDB in case a program interception has
    occurred and the ITDB has a valid format.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1bb1ddaf93c0..0084c2c272e3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -395,6 +395,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 						    CPUSTAT_STOPPED |
 						    CPUSTAT_GED);
 	vcpu->arch.sie_block->ecb   = 6;
+	if (test_vfacility(50) && test_vfacility(73))
+		vcpu->arch.sie_block->ecb |= 0x10;
+
 	vcpu->arch.sie_block->ecb2  = 8;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
@@ -411,6 +414,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 				      unsigned int id)
 {
 	struct kvm_vcpu *vcpu;
+	struct sie_page *sie_page;
 	int rc = -EINVAL;
 
 	if (id >= KVM_MAX_VCPUS)
@@ -422,12 +426,13 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	if (!vcpu)
 		goto out;
 
-	vcpu->arch.sie_block = (struct kvm_s390_sie_block *)
-					get_zeroed_page(GFP_KERNEL);
-
-	if (!vcpu->arch.sie_block)
+	sie_page = (struct sie_page *) get_zeroed_page(GFP_KERNEL);
+	if (!sie_page)
 		goto out_free_cpu;
 
+	vcpu->arch.sie_block = &sie_page->sie_block;
+	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
+
 	vcpu->arch.sie_block->icpua = id;
 	if (!kvm_is_ucontrol(kvm)) {
 		if (!kvm->arch.sca) {
@@ -1178,8 +1183,8 @@ static int __init kvm_s390_init(void)
 		return -ENOMEM;
 	}
 	memcpy(vfacilities, S390_lowcore.stfle_fac_list, 16);
-	vfacilities[0] &= 0xff82fff3f47c0000UL;
-	vfacilities[1] &= 0x001c000000000000UL;
+	vfacilities[0] &= 0xff82fff3f47c2000UL;
+	vfacilities[1] &= 0x005c000000000000UL;
 	return 0;
 }
 

commit 178bd789775ab29233e0553155253ec8d73af71f
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Wed Nov 13 20:28:18 2013 +0100

    KVM: s390: Fix clock comparator field for STORE STATUS
    
    Only the most 7 significant bytes of the clock comparator must be
    saved to the status area, and the byte at offset 304 has to be zero.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 55eb8dec2a77..1bb1ddaf93c0 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -877,6 +877,7 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long addr)
 {
 	unsigned char archmode = 1;
 	int prefix;
+	u64 clkcomp;
 
 	if (addr == KVM_S390_STORE_STATUS_NOADDR) {
 		if (copy_to_guest_absolute(vcpu, 163ul, &archmode, 1))
@@ -920,8 +921,9 @@ int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long addr)
 			&vcpu->arch.sie_block->cputm, 8, prefix))
 		return -EFAULT;
 
+	clkcomp = vcpu->arch.sie_block->ckc >> 8;
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, clk_cmp),
-			&vcpu->arch.sie_block->ckc, 8, prefix))
+			&clkcomp, 8, prefix))
 		return -EFAULT;
 
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, acc_regs),

commit e879892c725217a4af1012f31ae56be762473216
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Wed Nov 6 15:46:33 2013 +0100

    KVM: s390: Always store status during SIGP STOP_AND_STORE_STATUS
    
    The SIGP order STOP_AND_STORE_STATUS is defined to stop a CPU and store
    its status. However, we only stored the status if the CPU was still
    running, so make sure that the status is now also stored if the CPU was
    already stopped. This fixes the problem that the CPU information was
    not stored correctly in kdump files, rendering them unreadable.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7f4783525144..55eb8dec2a77 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -873,7 +873,7 @@ static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, void *from,
  * KVM_S390_STORE_STATUS_NOADDR: -> 0x1200 on 64 bit
  * KVM_S390_STORE_STATUS_PREFIXED: -> prefix
  */
-int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
+int kvm_s390_store_status_unloaded(struct kvm_vcpu *vcpu, unsigned long addr)
 {
 	unsigned char archmode = 1;
 	int prefix;
@@ -891,15 +891,6 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	} else
 		prefix = 0;
 
-	/*
-	 * The guest FPRS and ACRS are in the host FPRS/ACRS due to the lazy
-	 * copying in vcpu load/put. Lets update our copies before we save
-	 * it into the save area
-	 */
-	save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
-	save_fp_regs(vcpu->arch.guest_fpregs.fprs);
-	save_access_regs(vcpu->run->s.regs.acrs);
-
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, fp_regs),
 			vcpu->arch.guest_fpregs.fprs, 128, prefix))
 		return -EFAULT;
@@ -944,6 +935,20 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return 0;
 }
 
+int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
+{
+	/*
+	 * The guest FPRS and ACRS are in the host FPRS/ACRS due to the lazy
+	 * copying in vcpu load/put. Lets update our copies before we save
+	 * it into the save area
+	 */
+	save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+	save_fp_regs(vcpu->arch.guest_fpregs.fprs);
+	save_access_regs(vcpu->run->s.regs.acrs);
+
+	return kvm_s390_store_status_unloaded(vcpu, addr);
+}
+
 static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 				     struct kvm_enable_cap *cap)
 {

commit 210b1607012cc9034841a393e0591b2c86d9e26c
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Thu Sep 19 16:26:18 2013 +0200

    KVM: s390: Removed SIE_INTERCEPT_UCONTROL
    
    The SIE_INTERCEPT_UCONTROL can be removed by moving the related code
    from kvm_arch_vcpu_ioctl_run() to vcpu_post_run().
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 569494e01ec6..7f4783525144 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -732,14 +732,12 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 
 	if (exit_reason >= 0) {
 		rc = 0;
-	} else {
-		if (kvm_is_ucontrol(vcpu->kvm)) {
-			rc = SIE_INTERCEPT_UCONTROL;
-		} else {
-			VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
-			trace_kvm_s390_sie_fault(vcpu);
-			rc = kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
-		}
+	} else if (kvm_is_ucontrol(vcpu->kvm)) {
+		vcpu->run->exit_reason = KVM_EXIT_S390_UCONTROL;
+		vcpu->run->s390_ucontrol.trans_exc_code =
+						current->thread.gmap_addr;
+		vcpu->run->s390_ucontrol.pgm_code = 0x10;
+		rc = -EREMOTE;
 	}
 
 	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
@@ -833,16 +831,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = -EINTR;
 	}
 
-#ifdef CONFIG_KVM_S390_UCONTROL
-	if (rc == SIE_INTERCEPT_UCONTROL) {
-		kvm_run->exit_reason = KVM_EXIT_S390_UCONTROL;
-		kvm_run->s390_ucontrol.trans_exc_code =
-			current->thread.gmap_addr;
-		kvm_run->s390_ucontrol.pgm_code = 0x10;
-		rc = 0;
-	}
-#endif
-
 	if (rc == -EOPNOTSUPP) {
 		/* intercept cannot be handled in-kernel, prepare kvm-run */
 		kvm_run->exit_reason         = KVM_EXIT_S390_SIEIC;

commit f080480488028bcc25357f85e8ae54ccc3bb7173
Merge: eda670c626a4 e504c9098ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 15 13:51:36 2013 +0900

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Paolo Bonzini:
     "Here are the 3.13 KVM changes.  There was a lot of work on the PPC
      side: the HV and emulation flavors can now coexist in a single kernel
      is probably the most interesting change from a user point of view.
    
      On the x86 side there are nested virtualization improvements and a few
      bugfixes.
    
      ARM got transparent huge page support, improved overcommit, and
      support for big endian guests.
    
      Finally, there is a new interface to connect KVM with VFIO.  This
      helps with devices that use NoSnoop PCI transactions, letting the
      driver in the guest execute WBINVD instructions.  This includes some
      nVidia cards on Windows, that fail to start without these patches and
      the corresponding userspace changes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (146 commits)
      kvm, vmx: Fix lazy FPU on nested guest
      arm/arm64: KVM: PSCI: propagate caller endianness to the incoming vcpu
      arm/arm64: KVM: MMIO support for BE guest
      kvm, cpuid: Fix sparse warning
      kvm: Delete prototype for non-existent function kvm_check_iopl
      kvm: Delete prototype for non-existent function complete_pio
      hung_task: add method to reset detector
      pvclock: detect watchdog reset at pvclock read
      kvm: optimize out smp_mb after srcu_read_unlock
      srcu: API for barrier after srcu read unlock
      KVM: remove vm mmap method
      KVM: IOMMU: hva align mapping page size
      KVM: x86: trace cpuid emulation when called from emulator
      KVM: emulator: cleanup decode_register_operand() a bit
      KVM: emulator: check rex prefix inside decode_register()
      KVM: x86: fix emulation of "movzbl %bpl, %eax"
      kvm_host: typo fix
      KVM: x86: emulate SAHF instruction
      MAINTAINERS: add tree for kvm.git
      Documentation/kvm: add a 00-INDEX file
      ...

commit 4725c86055f5bbdcdfe47199c0715881893a2c79
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Oct 15 16:08:34 2013 +0200

    s390: fix save and restore of the floating-point-control register
    
    The FPC_VALID_MASK has been used to check the validity of the value
    to be loaded into the floating-point-control register. With the
    introduction of the floating-point extension facility and the
    decimal-floating-point additional bits have been defined which need
    to be checked in a non straight forward way. So far these bits have
    been ignored which can cause an incorrect results for decimal-
    floating-point operations, e.g. an incorrect rounding mode to be
    set after signal return.
    
    The static check with the FPC_VALID_MASK is replaced with a trial
    load of the floating-point-control value, see test_fp_ctl.
    
    In addition an information leak with the padding word between the
    floating-point-control word and the floating-point registers in
    the s390_fp_regs is fixed.
    
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 776dafe918db..ed8064cb5c49 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -343,10 +343,11 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
-	save_fp_regs(&vcpu->arch.host_fpregs);
+	save_fp_ctl(&vcpu->arch.host_fpregs.fpc);
+	save_fp_regs(vcpu->arch.host_fpregs.fprs);
 	save_access_regs(vcpu->arch.host_acrs);
-	vcpu->arch.guest_fpregs.fpc &= FPC_VALID_MASK;
-	restore_fp_regs(&vcpu->arch.guest_fpregs);
+	restore_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+	restore_fp_regs(vcpu->arch.guest_fpregs.fprs);
 	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
@@ -356,9 +357,11 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
-	save_fp_regs(&vcpu->arch.guest_fpregs);
+	save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+	save_fp_regs(vcpu->arch.guest_fpregs.fprs);
 	save_access_regs(vcpu->run->s.regs.acrs);
-	restore_fp_regs(&vcpu->arch.host_fpregs);
+	restore_fp_ctl(&vcpu->arch.host_fpregs.fpc);
+	restore_fp_regs(vcpu->arch.host_fpregs.fprs);
 	restore_access_regs(vcpu->arch.host_acrs);
 }
 
@@ -618,9 +621,12 @@ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 
 int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
+	if (test_fp_ctl(fpu->fpc))
+		return -EINVAL;
 	memcpy(&vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
-	vcpu->arch.guest_fpregs.fpc = fpu->fpc & FPC_VALID_MASK;
-	restore_fp_regs(&vcpu->arch.guest_fpregs);
+	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
+	restore_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+	restore_fp_regs(vcpu->arch.guest_fpregs.fprs);
 	return 0;
 }
 
@@ -876,7 +882,8 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	 * copying in vcpu load/put. Lets update our copies before we save
 	 * it into the save area
 	 */
-	save_fp_regs(&vcpu->arch.guest_fpregs);
+	save_fp_ctl(&vcpu->arch.guest_fpregs.fpc);
+	save_fp_regs(vcpu->arch.guest_fpregs.fprs);
 	save_access_regs(vcpu->run->s.regs.acrs);
 
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, fp_regs),

commit 5587027ce9d59a57aecaa190be1c8e560aaff45d
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:00 2013 +0530

    kvm: Add struct kvm arg to memslot APIs
    
    We will use that in the later patch to find the kvm ops handler
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1e4e7b97337a..bedda67cc222 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1089,12 +1089,13 @@ int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 	return VM_FAULT_SIGBUS;
 }
 
-void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont)
 {
 }
 
-int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
+int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
+			    unsigned long npages)
 {
 	return 0;
 }

commit 800c1065c3207a1b04d4ac457a57a37ce19eeb21
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Thu Sep 12 10:33:45 2013 +0200

    KVM: s390: Lock kvm->srcu at the appropriate places
    
    The kvm->srcu lock has to be held while accessing the memory of
    guests and during certain other actions. This patch now adds
    the locks to the __vcpu_run function so that all affected code
    is protected now (and additionally to the KVM_S390_STORE_STATUS
    ioctl, which can be called out-of-band and needs a separate lock).
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8eec7abc5664..1e4e7b97337a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -752,11 +752,18 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc, exit_reason;
 
+	/*
+	 * We try to hold kvm->srcu during most of vcpu_run (except when run-
+	 * ning the guest), so that memslots (and other stuff) are protected
+	 */
+	vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
+
 	do {
 		rc = vcpu_pre_run(vcpu);
 		if (rc)
 			break;
 
+		srcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);
 		/*
 		 * As PF_VCPU will be used in fault handler, between
 		 * guest_enter and guest_exit should be no uaccess.
@@ -767,10 +774,12 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		exit_reason = sie64a(vcpu->arch.sie_block,
 				     vcpu->run->s.regs.gprs);
 		kvm_guest_exit();
+		vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
 
 		rc = vcpu_post_run(vcpu, exit_reason);
 	} while (!signal_pending(current) && !rc);
 
+	srcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);
 	return rc;
 }
 
@@ -968,6 +977,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 {
 	struct kvm_vcpu *vcpu = filp->private_data;
 	void __user *argp = (void __user *)arg;
+	int idx;
 	long r;
 
 	switch (ioctl) {
@@ -981,7 +991,9 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_S390_STORE_STATUS:
+		idx = srcu_read_lock(&vcpu->kvm->srcu);
 		r = kvm_s390_vcpu_store_status(vcpu, arg);
+		srcu_read_unlock(&vcpu->kvm->srcu, idx);
 		break;
 	case KVM_S390_SET_INITIAL_PSW: {
 		psw_t psw;

commit a76ccff6f5ed89153bf58ef4215b5512a0316877
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Thu Sep 12 10:33:44 2013 +0200

    KVM: s390: Push run loop into __vcpu_run
    
    Moved the do-while loop from kvm_arch_vcpu_ioctl_run into __vcpu_run
    and the calling of kvm_handle_sie_intercept() into vcpu_post_run()
    (so we can add the srcu locks in a proper way in the next patch).
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 69c7592e80d9..8eec7abc5664 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -738,6 +738,13 @@ static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
 
 	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
 
+	if (rc == 0) {
+		if (kvm_is_ucontrol(vcpu->kvm))
+			rc = -EOPNOTSUPP;
+		else
+			rc = kvm_handle_sie_intercept(vcpu);
+	}
+
 	return rc;
 }
 
@@ -745,21 +752,24 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc, exit_reason;
 
-	rc = vcpu_pre_run(vcpu);
-	if (rc)
-		return rc;
-
-	/*
-	 * As PF_VCPU will be used in fault handler, between guest_enter
-	 * and guest_exit should be no uaccess.
-	 */
-	preempt_disable();
-	kvm_guest_enter();
-	preempt_enable();
-	exit_reason = sie64a(vcpu->arch.sie_block, vcpu->run->s.regs.gprs);
-	kvm_guest_exit();
+	do {
+		rc = vcpu_pre_run(vcpu);
+		if (rc)
+			break;
 
-	rc = vcpu_post_run(vcpu, exit_reason);
+		/*
+		 * As PF_VCPU will be used in fault handler, between
+		 * guest_enter and guest_exit should be no uaccess.
+		 */
+		preempt_disable();
+		kvm_guest_enter();
+		preempt_enable();
+		exit_reason = sie64a(vcpu->arch.sie_block,
+				     vcpu->run->s.regs.gprs);
+		kvm_guest_exit();
+
+		rc = vcpu_post_run(vcpu, exit_reason);
+	} while (!signal_pending(current) && !rc);
 
 	return rc;
 }
@@ -801,16 +811,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	}
 
 	might_fault();
-
-	do {
-		rc = __vcpu_run(vcpu);
-		if (rc)
-			break;
-		if (kvm_is_ucontrol(vcpu->kvm))
-			rc = -EOPNOTSUPP;
-		else
-			rc = kvm_handle_sie_intercept(vcpu);
-	} while (!signal_pending(current) && !rc);
+	rc = __vcpu_run(vcpu);
 
 	if (signal_pending(current) && !rc) {
 		kvm_run->exit_reason = KVM_EXIT_INTR;

commit 3fb4c40f0715f382ba7092c5d9a1804d45818039
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Thu Sep 12 10:33:43 2013 +0200

    KVM: s390: Split up __vcpu_run into three parts
    
    In preparation for the following patch (which will change the indentation
    of __vcpu_run quite a bit), this patch puts most of the code from __vcpu_run
    into separate functions. The first function handles the code that runs
    before the SIE instruction and the other one handles the code that runs
    afterwards.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e3e7ff77ba44..69c7592e80d9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -689,9 +689,9 @@ static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-static int __vcpu_run(struct kvm_vcpu *vcpu)
+static int vcpu_pre_run(struct kvm_vcpu *vcpu)
 {
-	int rc;
+	int rc, cpuflags;
 
 	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->run->s.regs.gprs[14], 16);
 
@@ -709,28 +709,24 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		return rc;
 
 	vcpu->arch.sie_block->icptcode = 0;
-	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
-		   atomic_read(&vcpu->arch.sie_block->cpuflags));
-	trace_kvm_s390_sie_enter(vcpu,
-				 atomic_read(&vcpu->arch.sie_block->cpuflags));
+	cpuflags = atomic_read(&vcpu->arch.sie_block->cpuflags);
+	VCPU_EVENT(vcpu, 6, "entering sie flags %x", cpuflags);
+	trace_kvm_s390_sie_enter(vcpu, cpuflags);
 
-	/*
-	 * As PF_VCPU will be used in fault handler, between guest_enter
-	 * and guest_exit should be no uaccess.
-	 */
-	preempt_disable();
-	kvm_guest_enter();
-	preempt_enable();
-	rc = sie64a(vcpu->arch.sie_block, vcpu->run->s.regs.gprs);
-	kvm_guest_exit();
+	return 0;
+}
+
+static int vcpu_post_run(struct kvm_vcpu *vcpu, int exit_reason)
+{
+	int rc;
 
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
 	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
 
-	if (rc > 0)
+	if (exit_reason >= 0) {
 		rc = 0;
-	if (rc < 0) {
+	} else {
 		if (kvm_is_ucontrol(vcpu->kvm)) {
 			rc = SIE_INTERCEPT_UCONTROL;
 		} else {
@@ -741,6 +737,30 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	}
 
 	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
+
+	return rc;
+}
+
+static int __vcpu_run(struct kvm_vcpu *vcpu)
+{
+	int rc, exit_reason;
+
+	rc = vcpu_pre_run(vcpu);
+	if (rc)
+		return rc;
+
+	/*
+	 * As PF_VCPU will be used in fault handler, between guest_enter
+	 * and guest_exit should be no uaccess.
+	 */
+	preempt_disable();
+	kvm_guest_enter();
+	preempt_enable();
+	exit_reason = sie64a(vcpu->arch.sie_block, vcpu->run->s.regs.gprs);
+	kvm_guest_exit();
+
+	rc = vcpu_post_run(vcpu, exit_reason);
+
 	return rc;
 }
 

commit 6b948a7276b61ba0bb5a102bd240d2473a751506
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Thu Sep 12 10:33:42 2013 +0200

    KVM: s390: Remove dead "rerun vcpu" code
    
    The need for SIE_INTERCEPT_RERUNVCPU has been removed long ago already,
    with the following commit:
            f7850c92884b40915001e332a0a33ed4f10158e8
            [S390] remove kvm mmu reload on s390
    Since the remainders are dead code, they are now removed by this patch.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 776dafe918db..e3e7ff77ba44 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -749,7 +749,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	int rc;
 	sigset_t sigsaved;
 
-rerun_vcpu:
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
@@ -793,9 +792,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 			rc = kvm_handle_sie_intercept(vcpu);
 	} while (!signal_pending(current) && !rc);
 
-	if (rc == SIE_INTERCEPT_RERUNVCPU)
-		goto rerun_vcpu;
-
 	if (signal_pending(current) && !rc) {
 		kvm_run->exit_reason = KVM_EXIT_INTR;
 		rc = -EINTR;

commit ae7a835cc546fc67df90edaaa0c48ae2b22a29fe
Merge: cf39c8e5352b 6b9e4fa07443
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 18:15:06 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Gleb Natapov:
     "The highlights of the release are nested EPT and pv-ticketlocks
      support (hypervisor part, guest part, which is most of the code, goes
      through tip tree).  Apart of that there are many fixes for all arches"
    
    Fix up semantic conflicts as discussed in the pull request thread..
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (88 commits)
      ARM: KVM: Add newlines to panic strings
      ARM: KVM: Work around older compiler bug
      ARM: KVM: Simplify tracepoint text
      ARM: KVM: Fix kvm_set_pte assignment
      ARM: KVM: vgic: Bump VGIC_NR_IRQS to 256
      ARM: KVM: Bugfix: vgic_bytemap_get_reg per cpu regs
      ARM: KVM: vgic: fix GICD_ICFGRn access
      ARM: KVM: vgic: simplify vgic_get_target_reg
      KVM: MMU: remove unused parameter
      KVM: PPC: Book3S PR: Rework kvmppc_mmu_book3s_64_xlate()
      KVM: PPC: Book3S PR: Make instruction fetch fallback work for system calls
      KVM: PPC: Book3S PR: Don't corrupt guest state when kernel uses VMX
      KVM: x86: update masterclock when kvmclock_offset is calculated (v2)
      KVM: PPC: Book3S: Fix compile error in XICS emulation
      KVM: PPC: Book3S PR: return appropriate error when allocation fails
      arch: powerpc: kvm: add signed type cast for comparation
      KVM: x86: add comments where MMIO does not return to the emulator
      KVM: vmx: count exits to userspace during invalid guest emulation
      KVM: rename __kvm_io_bus_sort_cmp to kvm_io_bus_cmp
      kvm: optimize away THP checks in kvm_is_mmio_pfn()
      ...

commit 78c4b59f721fb0c30e8520f1c8e78fbf47bddfdf
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Fri Jul 26 15:04:04 2013 +0200

    KVM: s390: declare virtual HW facilities
    
    The patch renames the array holding the HW facility bitmaps.
    This allows to interprete the variable as set of virtual
    machine specific "virtual" facilities. The basic idea is
    to make virtual facilities externally managable in future.
    An availability test for virtual facilites has been added
    as well.
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a3d797b689a3..ac8e6670c551 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -28,6 +28,7 @@
 #include <asm/pgtable.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>
+#include <asm/facility.h>
 #include <asm/sclp.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
@@ -84,9 +85,15 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ NULL }
 };
 
-static unsigned long long *facilities;
+unsigned long *vfacilities;
 static struct gmap_notifier gmap_notifier;
 
+/* test availability of vfacility */
+static inline int test_vfacility(unsigned long nr)
+{
+	return __test_facility(nr, (void *) vfacilities);
+}
+
 /* Section: not file related */
 int kvm_arch_hardware_enable(void *garbage)
 {
@@ -387,7 +394,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->ecb   = 6;
 	vcpu->arch.sie_block->ecb2  = 8;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
-	vcpu->arch.sie_block->fac   = (int) (long) facilities;
+	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 	tasklet_init(&vcpu->arch.tasklet, kvm_s390_tasklet,
 		     (unsigned long) vcpu);
@@ -1126,20 +1133,20 @@ static int __init kvm_s390_init(void)
 	 * to hold the maximum amount of facilities. On the other hand, we
 	 * only set facilities that are known to work in KVM.
 	 */
-	facilities = (unsigned long long *) get_zeroed_page(GFP_KERNEL|GFP_DMA);
-	if (!facilities) {
+	vfacilities = (unsigned long *) get_zeroed_page(GFP_KERNEL|GFP_DMA);
+	if (!vfacilities) {
 		kvm_exit();
 		return -ENOMEM;
 	}
-	memcpy(facilities, S390_lowcore.stfle_fac_list, 16);
-	facilities[0] &= 0xff82fff3f47c0000ULL;
-	facilities[1] &= 0x001c000000000000ULL;
+	memcpy(vfacilities, S390_lowcore.stfle_fac_list, 16);
+	vfacilities[0] &= 0xff82fff3f47c0000UL;
+	vfacilities[1] &= 0x001c000000000000UL;
 	return 0;
 }
 
 static void __exit kvm_s390_exit(void)
 {
-	free_page((unsigned long) facilities);
+	free_page((unsigned long) vfacilities);
 	kvm_exit();
 }
 

commit 2b29a9fdcb92bfc6b6f4c412d71505869de61a56
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Fri Jul 26 15:04:00 2013 +0200

    KVM: s390: move kvm_guest_enter,exit closer to sie
    
    Any uaccess between guest_enter and guest_exit could trigger a page fault,
    the page fault handler would handle it as a guest fault and translate a
    user address as guest address.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    CC: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ba694d2ba51e..34c1c9a90be2 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -702,14 +702,25 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		return rc;
 
 	vcpu->arch.sie_block->icptcode = 0;
-	preempt_disable();
-	kvm_guest_enter();
-	preempt_enable();
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
 	trace_kvm_s390_sie_enter(vcpu,
 				 atomic_read(&vcpu->arch.sie_block->cpuflags));
+
+	/*
+	 * As PF_VCPU will be used in fault handler, between guest_enter
+	 * and guest_exit should be no uaccess.
+	 */
+	preempt_disable();
+	kvm_guest_enter();
+	preempt_enable();
 	rc = sie64a(vcpu->arch.sie_block, vcpu->run->s.regs.gprs);
+	kvm_guest_exit();
+
+	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
+		   vcpu->arch.sie_block->icptcode);
+	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
+
 	if (rc > 0)
 		rc = 0;
 	if (rc < 0) {
@@ -721,10 +732,6 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 			rc = kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
 		}
 	}
-	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
-		   vcpu->arch.sie_block->icptcode);
-	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
-	kvm_guest_exit();
 
 	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
 	return rc;

commit e59dbe09f8e6fb8f6ee19dc79d1a2f14299e4cd2
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Thu Jul 4 13:40:29 2013 +0900

    KVM: Introduce kvm_arch_memslots_updated()
    
    This is called right after the memslots is updated, i.e. when the result
    of update_memslots() gets installed in install_new_memslots().  Since
    the memslots needs to be updated twice when we delete or move a memslot,
    kvm_arch_commit_memory_region() does not correspond to this exactly.
    
    In the following patch, x86 will use this new API to check if the mmio
    generation has reached its maximum value, in which case mmio sptes need
    to be flushed out.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Acked-by: Alexander Graf <agraf@suse.de>
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ba694d2ba51e..a3d797b689a3 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1056,6 +1056,10 @@ int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 	return 0;
 }
 
+void kvm_arch_memslots_updated(struct kvm *kvm)
+{
+}
+
 /* Section: memory related */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,

commit d0321a24bf10e2299a997c4747b924f79f70a232
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jun 12 13:54:55 2013 +0200

    KVM: s390: Use common waitqueue
    
    Lets use the common waitqueue for kvm cpus on s390. By itself it is
    just a cleanup, but it should also improve the accuracy of diag 0x44
    which is implemented via kvm_vcpu_on_spin. kvm_vcpu_on_spin has
    an explicit check for waiting on the waitqueue to optimize the
    yielding.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a3183651ff45..ba694d2ba51e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -438,7 +438,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
 	spin_lock(&kvm->arch.float_int.lock);
 	kvm->arch.float_int.local_int[id] = &vcpu->arch.local_int;
-	init_waitqueue_head(&vcpu->arch.local_int.wq);
+	vcpu->arch.local_int.wq = &vcpu->wq;
 	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
 	spin_unlock(&kvm->arch.float_int.lock);
 

commit b110feaf4d0bbc31802589ea6b956389afdabcee
Author: Michael Mueller <mimu@linux.vnet.ibm.com>
Date:   Wed Jun 12 13:54:54 2013 +0200

    KVM: s390: code cleanup to use common vcpu slab cache
    
    cleanup of arch specific code to use common code provided vcpu slab cache
    instead of kzalloc() provided memory
    
    Signed-off-by: Michael Mueller <mimu@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 426e259b6a69..a3183651ff45 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -278,7 +278,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 	free_page((unsigned long)(vcpu->arch.sie_block));
 	kvm_vcpu_uninit(vcpu);
-	kfree(vcpu);
+	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
 static void kvm_free_vcpus(struct kvm *kvm)
@@ -408,7 +408,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	rc = -ENOMEM;
 
-	vcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);
+	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
 	if (!vcpu)
 		goto out;
 
@@ -453,7 +453,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 out_free_sie_block:
 	free_page((unsigned long)(vcpu->arch.sie_block));
 out_free_cpu:
-	kfree(vcpu);
+	kmem_cache_free(kvm_vcpu_cache, vcpu);
 out:
 	return ERR_PTR(rc);
 }

commit 69d0d3a3160690cf64ea3bf484ca1f9d7a1bf798
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jun 12 13:54:53 2013 +0200

    KVM: s390: guest large pages
    
    This patch enables kvm to give large pages to the guest. The heavy
    lifting is done by the hardware, the host only has to take care
    of the PFMF instruction, which is also part of EDAT-1.
    
    We also support the non-quiescing key setting facility if the host
    supports it, to behave similar to the interpretation of sske.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3b597e590a75..426e259b6a69 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -59,6 +59,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
 	{ "deliver_program_interruption", VCPU_STAT(deliver_program_int) },
 	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
+	{ "instruction_pfmf", VCPU_STAT(instruction_pfmf) },
 	{ "instruction_stidp", VCPU_STAT(instruction_stidp) },
 	{ "instruction_spx", VCPU_STAT(instruction_spx) },
 	{ "instruction_stpx", VCPU_STAT(instruction_stpx) },
@@ -381,8 +382,10 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |
 						    CPUSTAT_SM |
-						    CPUSTAT_STOPPED);
+						    CPUSTAT_STOPPED |
+						    CPUSTAT_GED);
 	vcpu->arch.sie_block->ecb   = 6;
+	vcpu->arch.sie_block->ecb2  = 8;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) facilities;
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
@@ -1125,7 +1128,7 @@ static int __init kvm_s390_init(void)
 		return -ENOMEM;
 	}
 	memcpy(facilities, S390_lowcore.stfle_fac_list, 16);
-	facilities[0] &= 0xff00fff3f47c0000ULL;
+	facilities[0] &= 0xff82fff3f47c0000ULL;
 	facilities[1] &= 0x001c000000000000ULL;
 	return 0;
 }

commit 566af9404bf57267ea4fc29ca6b4628a17ee3ea7
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Mon May 27 18:42:33 2013 +0200

    KVM: s390: Add "devname:kvm" alias.
    
    Providing a "devname:kvm" module alias enables automatic loading of
    the kvm module when /dev/kvm is opened.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 93444c4dae5a..3b597e590a75 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1138,3 +1138,12 @@ static void __exit kvm_s390_exit(void)
 
 module_init(kvm_s390_init);
 module_exit(kvm_s390_exit);
+
+/*
+ * Enable autoloading of the kvm module.
+ * Note that we add the module alias here instead of virt/kvm/kvm_main.c
+ * since x86 takes a different approach.
+ */
+#include <linux/miscdevice.h>
+MODULE_ALIAS_MISCDEV(KVM_MINOR);
+MODULE_ALIAS("devname:kvm");

commit 7c470539c95630c1f2a10f109e96f249730b75eb
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri May 17 14:41:37 2013 +0200

    s390/kvm: avoid automatic sie reentry
    
    Do not automatically restart the sie instruction in entry64.S after an
    interrupt, return to the caller with a reason code instead. That allows
    to deal with RCU and other conditions in C code.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 08227c1e816f..93444c4dae5a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -707,7 +707,9 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	trace_kvm_s390_sie_enter(vcpu,
 				 atomic_read(&vcpu->arch.sie_block->cpuflags));
 	rc = sie64a(vcpu->arch.sie_block, vcpu->run->s.regs.gprs);
-	if (rc) {
+	if (rc > 0)
+		rc = 0;
+	if (rc < 0) {
 		if (kvm_is_ucontrol(vcpu->kvm)) {
 			rc = SIE_INTERCEPT_UCONTROL;
 		} else {

commit 2c70fe4416d5f6d092b20ebf7d7654835e09c109
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 17 14:41:36 2013 +0200

    s390/kvm: Kick guests out of sie if prefix page host pte is touched
    
    The guest prefix pages must be mapped writeable all the time
    while SIE is running, otherwise the guest might see random
    behaviour. (pinned at the pte level) Turns out that mlocking is
    not enough, the page table entry (not the page) might change or
    become r/o. This patch uses the gmap notifiers to kick guest
    cpus out of SIE.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ef4ef21f2c73..08227c1e816f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -84,6 +84,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 };
 
 static unsigned long long *facilities;
+static struct gmap_notifier gmap_notifier;
 
 /* Section: not file related */
 int kvm_arch_hardware_enable(void *garbage)
@@ -96,13 +97,18 @@ void kvm_arch_hardware_disable(void *garbage)
 {
 }
 
+static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address);
+
 int kvm_arch_hardware_setup(void)
 {
+	gmap_notifier.notifier_call = kvm_gmap_notifier;
+	gmap_register_ipte_notifier(&gmap_notifier);
 	return 0;
 }
 
 void kvm_arch_hardware_unsetup(void)
 {
+	gmap_unregister_ipte_notifier(&gmap_notifier);
 }
 
 void kvm_arch_check_processor_compat(void *rtn)
@@ -239,6 +245,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		kvm->arch.gmap = gmap_alloc(current->mm);
 		if (!kvm->arch.gmap)
 			goto out_nogmap;
+		kvm->arch.gmap->private = kvm;
 	}
 
 	kvm->arch.css_support = 0;
@@ -309,6 +316,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 		vcpu->arch.gmap = gmap_alloc(current->mm);
 		if (!vcpu->arch.gmap)
 			return -ENOMEM;
+		vcpu->arch.gmap->private = vcpu->kvm;
 		return 0;
 	}
 
@@ -482,6 +490,22 @@ void exit_sie_sync(struct kvm_vcpu *vcpu)
 	exit_sie(vcpu);
 }
 
+static void kvm_gmap_notifier(struct gmap *gmap, unsigned long address)
+{
+	int i;
+	struct kvm *kvm = gmap->private;
+	struct kvm_vcpu *vcpu;
+
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/* match against both prefix pages */
+		if (vcpu->arch.sie_block->prefix == (address & ~0x1000UL)) {
+			VCPU_EVENT(vcpu, 2, "gmap notifier for %lx", address);
+			kvm_make_request(KVM_REQ_MMU_RELOAD, vcpu);
+			exit_sie_sync(vcpu);
+		}
+	}
+}
+
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
 	/* kvm common code refers to this, but never calls it */
@@ -634,6 +658,27 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
+static int kvm_s390_handle_requests(struct kvm_vcpu *vcpu)
+{
+	/*
+	 * We use MMU_RELOAD just to re-arm the ipte notifier for the
+	 * guest prefix page. gmap_ipte_notify will wait on the ptl lock.
+	 * This ensures that the ipte instruction for this request has
+	 * already finished. We might race against a second unmapper that
+	 * wants to set the blocking bit. Lets just retry the request loop.
+	 */
+	while (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu)) {
+		int rc;
+		rc = gmap_ipte_notify(vcpu->arch.gmap,
+				      vcpu->arch.sie_block->prefix,
+				      PAGE_SIZE * 2);
+		if (rc)
+			return rc;
+		s390_vcpu_unblock(vcpu);
+	}
+	return 0;
+}
+
 static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc;
@@ -649,6 +694,10 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	if (!kvm_is_ucontrol(vcpu->kvm))
 		kvm_s390_deliver_pending_interrupts(vcpu);
 
+	rc = kvm_s390_handle_requests(vcpu);
+	if (rc)
+		return rc;
+
 	vcpu->arch.sie_block->icptcode = 0;
 	preempt_disable();
 	kvm_guest_enter();

commit 49b99e1e0dedbd6cc93b2d2776b60fb7151ff3d7
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 17 14:41:35 2013 +0200

    s390/kvm: Provide a way to prevent reentering SIE
    
    Lets provide functions to prevent KVM from reentering SIE and
    to kick cpus out of SIE. We cannot use the common kvm_vcpu_kick code,
    since we need to kick out guests in places that hold architecture
    specific locks (e.g. pgste lock) which might be necessary on the
    other cpus - so no waiting possible.
    
    So lets provide a bit in a private field of the sie control block
    that acts as a gate keeper, after we claimed we are in SIE.
    Please note that we do not reuse prog0c, since we want to access
    that bit without atomic ops.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c1c7c683fa26..ef4ef21f2c73 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -454,6 +454,34 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+void s390_vcpu_block(struct kvm_vcpu *vcpu)
+{
+	atomic_set_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
+}
+
+void s390_vcpu_unblock(struct kvm_vcpu *vcpu)
+{
+	atomic_clear_mask(PROG_BLOCK_SIE, &vcpu->arch.sie_block->prog20);
+}
+
+/*
+ * Kick a guest cpu out of SIE and wait until SIE is not running.
+ * If the CPU is not running (e.g. waiting as idle) the function will
+ * return immediately. */
+void exit_sie(struct kvm_vcpu *vcpu)
+{
+	atomic_set_mask(CPUSTAT_STOP_INT, &vcpu->arch.sie_block->cpuflags);
+	while (vcpu->arch.sie_block->prog0c & PROG_IN_SIE)
+		cpu_relax();
+}
+
+/* Kick a guest cpu out of SIE and prevent SIE-reentry */
+void exit_sie_sync(struct kvm_vcpu *vcpu)
+{
+	s390_vcpu_block(vcpu);
+	exit_sie(vcpu);
+}
+
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
 	/* kvm common code refers to this, but never calls it */

commit e1e2e605c2ad6791ce6346b22443ce611709fa65
Author: Nick Wang <jfwang@us.ibm.com>
Date:   Mon Mar 25 17:22:58 2013 +0100

    KVM: s390: Enable KVM_CAP_NR_MEMSLOTS on s390
    
    Return KVM_USER_MEM_SLOTS in kvm_dev_ioctl_check_extension().
    
    Signed-off-by: Nick Wang <jfwang@us.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b322ff15751d..c1c7c683fa26 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -149,6 +149,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
 		break;
+	case KVM_CAP_NR_MEMSLOTS:
+		r = KVM_USER_MEM_SLOTS;
+		break;
 	case KVM_CAP_S390_COW:
 		r = MACHINE_HAS_ESOP;
 		break;

commit dd2887e7c36d0be986ef17a9dbec904e3e334566
Author: Nick Wang <jfwang@us.ibm.com>
Date:   Mon Mar 25 17:22:57 2013 +0100

    KVM: s390: Remove the sanity checks for kvm memory slot
    
    To model the standby memory with memory_region_add_subregion
    and friends, the guest would have one or more regions of ram.
    Remove the check allowing only one memory slot and the check
    requiring the real address of memory slot starts at zero.
    
    Signed-off-by: Nick Wang <jfwang@us.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d05a59c1eea7..b322ff15751d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -977,18 +977,10 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_userspace_memory_region *mem,
 				   enum kvm_mr_change change)
 {
-	/* A few sanity checks. We can have exactly one memory slot which has
-	   to start at guest virtual zero and which has to be located at a
-	   page boundary in userland and which has to end at a page boundary.
-	   The memory in userland is ok to be fragmented into various different
-	   vmas. It is okay to mmap() and munmap() stuff in this slot after
-	   doing this call at any time */
-
-	if (mem->slot)
-		return -EINVAL;
-
-	if (mem->guest_phys_addr)
-		return -EINVAL;
+	/* A few sanity checks. We can have memory slots which have to be
+	   located/ended at a segment boundary (1MB). The memory in userland is
+	   ok to be fragmented into various different vmas. It is okay to mmap()
+	   and munmap() stuff in this slot after doing this call at any time */
 
 	if (mem->userspace_addr & 0xffffful)
 		return -EINVAL;

commit db4a29cb6ac7b2fda505923bdbc58fc35a719f62
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Mar 25 17:22:53 2013 +0100

    KVM: s390: fix and enforce return code handling for irq injections
    
    kvm_s390_inject_program_int() and friends may fail if no memory is available.
    This must be reported to the calling functions, so that this gets passed
    down to user space which should fix the situation.
    Alternatively we end up with guest state corruption.
    
    So fix this and enforce return value checking by adding a __must_check
    annotation to all of these function prototypes.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f241e3315ebb..d05a59c1eea7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -633,8 +633,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		} else {
 			VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
 			trace_kvm_s390_sie_fault(vcpu);
-			kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
-			rc = 0;
+			rc = kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
 		}
 	}
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",

commit 2cef4deb4018c02fb3cd08f76c8a988f7ddee480
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Mar 25 17:22:48 2013 +0100

    KVM: s390: Dont do a gmap update on minor memslot changes
    
    Some memslot updates dont affect the gmap implementation,
    e.g. setting/unsetting dirty tracking. Since a gmap update
    will cause tlb flushes and segment table invalidations we
    want to avoid that.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 33161b4a8280..f241e3315ebb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1007,6 +1007,16 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 {
 	int rc;
 
+	/* If the basics of the memslot do not change, we do not want
+	 * to update the gmap. Every update causes several unnecessary
+	 * segment translation exceptions. This is usually handled just
+	 * fine by the normal fault handler + gmap, but it will also
+	 * cause faults on the prefix page of running guest CPUs.
+	 */
+	if (old->userspace_addr == mem->userspace_addr &&
+	    old->base_gfn * PAGE_SIZE == mem->guest_phys_addr &&
+	    old->npages * PAGE_SIZE == mem->memory_size)
+		return;
 
 	rc = gmap_map_segment(kvm->arch.gmap, mem->userspace_addr,
 		mem->guest_phys_addr, mem->memory_size);

commit 10ccaa1e7057d8a9dc3e9ce833af40ec8187b25e
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Thu Feb 28 12:33:21 2013 +0100

    KVM: s390: Wire up ioeventfd.
    
    Enable ioeventfd support on s390 and hook up diagnose 500 virtio-ccw
    notifications.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6cae4ad647a9..33161b4a8280 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -142,6 +142,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_S390_CSS_SUPPORT:
+	case KVM_CAP_IOEVENTFD:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:

commit 8482644aea11e0647867732319ccf35879a9acc2
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:45:25 2013 +0900

    KVM: set_memory_region: Refactor commit_memory_region()
    
    This patch makes the parameter old a const pointer to the old memory
    slot and adds a new parameter named change to know the change being
    requested: the former is for removing extra copying and the latter is
    for cleaning up the code.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4288780c86b8..6cae4ad647a9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1001,7 +1001,8 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
-				struct kvm_memory_slot old)
+				const struct kvm_memory_slot *old,
+				enum kvm_mr_change change)
 {
 	int rc;
 

commit 7b6195a91d60909a2834ab7181e2b9476e6fe749
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:44:34 2013 +0900

    KVM: set_memory_region: Refactor prepare_memory_region()
    
    This patch drops the parameter old, a copy of the old memory slot, and
    adds a new parameter named change to know the change being requested.
    
    This not only cleans up the code but also removes extra copying of the
    memory slot structure.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 07ac302ce246..4288780c86b8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -974,8 +974,8 @@ int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 /* Section: memory related */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
-				   struct kvm_memory_slot old,
-				   struct kvm_userspace_memory_region *mem)
+				   struct kvm_userspace_memory_region *mem,
+				   enum kvm_mr_change change)
 {
 	/* A few sanity checks. We can have exactly one memory slot which has
 	   to start at guest virtual zero and which has to be located at a

commit 462fce46065ec4b200c08619c047b9e5a8fd154a
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:41:56 2013 +0900

    KVM: set_memory_region: Drop user_alloc from prepare/commit_memory_region()
    
    X86 does not use this any more.  The remaining user, s390's !user_alloc
    check, can be simply removed since KVM_SET_MEMORY_REGION ioctl is no
    longer supported.
    
    Note: fixed powerpc's indentations with spaces to suppress checkpatch
    errors.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4cf35a0a79e7..07ac302ce246 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -975,8 +975,7 @@ int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
 				   struct kvm_memory_slot old,
-				   struct kvm_userspace_memory_region *mem,
-				   bool user_alloc)
+				   struct kvm_userspace_memory_region *mem)
 {
 	/* A few sanity checks. We can have exactly one memory slot which has
 	   to start at guest virtual zero and which has to be located at a
@@ -997,16 +996,12 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	if (mem->memory_size & 0xffffful)
 		return -EINVAL;
 
-	if (!user_alloc)
-		return -EINVAL;
-
 	return 0;
 }
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
-				struct kvm_memory_slot old,
-				bool user_alloc)
+				struct kvm_memory_slot old)
 {
 	int rc;
 

commit 89f883372fa60f604d136924baf3e89ff1870e9e
Merge: 9e2d59ad580d 6b73a96065e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 24 13:07:18 2013 -0800

    Merge tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "KVM updates for the 3.9 merge window, including x86 real mode
      emulation fixes, stronger memory slot interface restrictions, mmu_lock
      spinlock hold time reduction, improved handling of large page faults
      on shadow, initial APICv HW acceleration support, s390 channel IO
      based virtio, amongst others"
    
    * tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (143 commits)
      Revert "KVM: MMU: lazily drop large spte"
      x86: pvclock kvm: align allocation size to page size
      KVM: nVMX: Remove redundant get_vmcs12 from nested_vmx_exit_handled_msr
      x86 emulator: fix parity calculation for AAD instruction
      KVM: PPC: BookE: Handle alignment interrupts
      booke: Added DBCR4 SPR number
      KVM: PPC: booke: Allow multiple exception types
      KVM: PPC: booke: use vcpu reference from thread_struct
      KVM: Remove user_alloc from struct kvm_memory_slot
      KVM: VMX: disable apicv by default
      KVM: s390: Fix handling of iscs.
      KVM: MMU: cleanup __direct_map
      KVM: MMU: remove pt_access in mmu_set_spte
      KVM: MMU: cleanup mapping-level
      KVM: MMU: lazily drop large spte
      KVM: VMX: cleanup vmx_set_cr0().
      KVM: VMX: add missing exit names to VMX_EXIT_REASONS array
      KVM: VMX: disable SMEP feature when guest is in non-paging mode
      KVM: Remove duplicate text in api.txt
      Revert "KVM: MMU: split kvm_mmu_free_page"
      ...

commit abf09bed3cceadd809f0356065c2ada6cee90d4a
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Nov 7 13:17:37 2012 +0100

    s390/mm: implement software dirty bits
    
    The s390 architecture is unique in respect to dirty page detection,
    it uses the change bit in the per-page storage key to track page
    modifications. All other architectures track dirty bits by means
    of page table entries. This property of s390 has caused numerous
    problems in the past, e.g. see git commit ef5d437f71afdf4a
    "mm: fix XFS oops due to dirty pages without buffers on s390".
    
    To avoid future issues in regard to per-page dirty bits convert
    s390 to a fault based software dirty bit detection mechanism. All
    user page table entries which are marked as clean will be hardware
    read-only, even if the pte is supposed to be writable. A write by
    the user process will trigger a protection fault which will cause
    the user pte to be marked as dirty and the hardware read-only bit
    is removed.
    
    With this change the dirty bit in the storage key is irrelevant
    for Linux as a host, but the storage key is still required for
    KVM guests. The effect is that page_test_and_clear_dirty and the
    related code can be removed. The referenced bit in the storage
    key is still used by the page_test_and_clear_young primitive to
    provide page age information.
    
    For page cache pages of mappings with mapping_cap_account_dirty
    there will not be any change in behavior as the dirty bit tracking
    already uses read-only ptes to control the amount of dirty pages.
    Only for swap cache pages and pages of mappings without
    mapping_cap_account_dirty there can be additional protection faults.
    To avoid an excessive number of additional faults the mk_pte
    primitive checks for PageDirty if the pgprot value allows for writes
    and pre-dirties the pte. That avoids all additional faults for
    tmpfs and shmem pages until these pages are added to the swap cache.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f090e819bf71..2923781590a6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -147,7 +147,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 		r = KVM_MAX_VCPUS;
 		break;
 	case KVM_CAP_S390_COW:
-		r = sclp_get_fac85() & 0x2;
+		r = MACHINE_HAS_ESOP;
 		break;
 	default:
 		r = 0;

commit 15bc8d8457875f495c59d933b05770ba88d1eacb
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jan 25 15:34:15 2013 +0100

    s390/kvm: Fix store status for ACRS/FPRS
    
    On store status we need to copy the current state of registers
    into a save area. Currently we might save stale versions:
    The sie state descriptor doesnt have fields for guest ACRS,FPRS,
    those registers are simply stored in the host registers. The host
    program must copy these away if needed. We do that in vcpu_put/load.
    
    If we now do a store status in KVM code between vcpu_put/load, the
    saved values are not up-to-date. Lets collect the ACRS/FPRS before
    saving them.
    
    This also fixes some strange problems with hotplug and virtio-ccw,
    since the low level machine check handler (on hotplug a machine check
    will happen) will revalidate all registers with the content of the
    save area.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    CC: stable@vger.kernel.org
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5b01f0953900..4377d1886631 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -770,6 +770,14 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	} else
 		prefix = 0;
 
+	/*
+	 * The guest FPRS and ACRS are in the host FPRS/ACRS due to the lazy
+	 * copying in vcpu load/put. Lets update our copies before we save
+	 * it into the save area
+	 */
+	save_fp_regs(&vcpu->arch.guest_fpregs);
+	save_access_regs(vcpu->run->s.regs.acrs);
+
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, fp_regs),
 			vcpu->arch.guest_fpregs.fprs, 128, prefix))
 		return -EFAULT;

commit 83987ace22f541964bd0dbf83a3f6b0014ba3e1e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Jan 8 16:23:10 2013 +0100

    s390/kvm: Fix BUG in include/linux/kvm_host.h:745
    
    commit b080935c8638e08134629d0a9ebdf35669bec14d
        kvm: Directly account vtime to system on guest switch
    
    also removed the irq_disable/enable around kvm guest switch, which
    is correct in itself. Unfortunately, there is a BUG ON that (correctly)
    checks for preemptible to cover the call to rcu later on.
    (Introduced with commit 8fa2206821953a50a3a02ea33fcfb3ced2fd9997
        KVM: make guest mode entry to be rcu quiescent state)
    
    This check might trigger depending on the kernel config.
    Lets make sure that no preemption happens during kvm_guest_enter.
    We can enable preemption again after the call to
    rcu_virt_note_context_switch returns.
    
    Please note that we continue to run s390 guests with interrupts
    enabled.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    CC: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c9011bfaabbe..f090e819bf71 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -613,7 +613,9 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		kvm_s390_deliver_pending_interrupts(vcpu);
 
 	vcpu->arch.sie_block->icptcode = 0;
+	preempt_disable();
 	kvm_guest_enter();
+	preempt_enable();
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
 	trace_kvm_s390_sie_enter(vcpu,

commit fa6b7fe9928d50444c29b29c8563746c6b0c6299
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Thu Dec 20 15:32:12 2012 +0100

    KVM: s390: Add support for channel I/O instructions.
    
    Add a new capability, KVM_CAP_S390_CSS_SUPPORT, which will pass
    intercepts for channel I/O instructions to userspace. Only I/O
    instructions interacting with I/O interrupts need to be handled
    in-kernel:
    
    - TEST PENDING INTERRUPTION (tpi) dequeues and stores pending
      interrupts entirely in-kernel.
    - TEST SUBCHANNEL (tsch) dequeues pending interrupts in-kernel
      and exits via KVM_EXIT_S390_TSCH to userspace for subchannel-
      related processing.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Reviewed-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5ff26033825c..5b01f0953900 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -141,6 +141,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_SYNC_REGS:
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_ENABLE_CAP:
+	case KVM_CAP_S390_CSS_SUPPORT:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -235,6 +236,9 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 		if (!kvm->arch.gmap)
 			goto out_nogmap;
 	}
+
+	kvm->arch.css_support = 0;
+
 	return 0;
 out_nogmap:
 	debug_unregister(kvm->arch.dbf);
@@ -658,6 +662,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	case KVM_EXIT_INTR:
 	case KVM_EXIT_S390_RESET:
 	case KVM_EXIT_S390_UCONTROL:
+	case KVM_EXIT_S390_TSCH:
 		break;
 	default:
 		BUG();
@@ -818,6 +823,13 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		return -EINVAL;
 
 	switch (cap->cap) {
+	case KVM_CAP_S390_CSS_SUPPORT:
+		if (!vcpu->kvm->arch.css_support) {
+			vcpu->kvm->arch.css_support = 1;
+			trace_kvm_s390_enable_css(vcpu->kvm);
+		}
+		r = 0;
+		break;
 	default:
 		r = -EINVAL;
 		break;

commit d6712df95bcfea597fc3ea2405ec13e8b69a7b8c
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Thu Dec 20 15:32:11 2012 +0100

    KVM: s390: Base infrastructure for enabling capabilities.
    
    Make s390 support KVM_ENABLE_CAP.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Acked-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f718bc65835c..5ff26033825c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -140,6 +140,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #endif
 	case KVM_CAP_SYNC_REGS:
 	case KVM_CAP_ONE_REG:
+	case KVM_CAP_ENABLE_CAP:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -808,6 +809,22 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return 0;
 }
 
+static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
+				     struct kvm_enable_cap *cap)
+{
+	int r;
+
+	if (cap->flags)
+		return -EINVAL;
+
+	switch (cap->cap) {
+	default:
+		r = -EINVAL;
+		break;
+	}
+	return r;
+}
+
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
@@ -894,6 +911,15 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			r = 0;
 		break;
 	}
+	case KVM_ENABLE_CAP:
+	{
+		struct kvm_enable_cap cap;
+		r = -EFAULT;
+		if (copy_from_user(&cap, argp, sizeof(cap)))
+			break;
+		r = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}

commit f82a8cfe9354f5cdea55ebeceba3fd19051d3ee8
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:21 2012 -0700

    KVM: struct kvm_memory_slot.user_alloc -> bool
    
    There's no need for this to be an int, it holds a boolean.
    Move to the end of the struct for alignment.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c9011bfaabbe..f718bc65835c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -928,7 +928,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
 				   struct kvm_memory_slot old,
 				   struct kvm_userspace_memory_region *mem,
-				   int user_alloc)
+				   bool user_alloc)
 {
 	/* A few sanity checks. We can have exactly one memory slot which has
 	   to start at guest virtual zero and which has to be located at a
@@ -958,7 +958,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				struct kvm_memory_slot old,
-				int user_alloc)
+				bool user_alloc)
 {
 	int rc;
 

commit 66cdd0ceaf65a18996f561b770eedde1d123b019
Merge: 896ea17d3da5 58b7825bc324
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 13 15:31:08 2012 -0800

    Merge tag 'kvm-3.8-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "Considerable KVM/PPC work, x86 kvmclock vsyscall support,
      IA32_TSC_ADJUST MSR emulation, amongst others."
    
    Fix up trivial conflict in kernel/sched/core.c due to cross-cpu
    migration notifier added next to rq migration call-back.
    
    * tag 'kvm-3.8-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (156 commits)
      KVM: emulator: fix real mode segment checks in address linearization
      VMX: remove unneeded enable_unrestricted_guest check
      KVM: VMX: fix DPL during entry to protected mode
      x86/kexec: crash_vmclear_local_vmcss needs __rcu
      kvm: Fix irqfd resampler list walk
      KVM: VMX: provide the vmclear function and a bitmap to support VMCLEAR in kdump
      x86/kexec: VMCLEAR VMCSs loaded on all cpus if necessary
      KVM: MMU: optimize for set_spte
      KVM: PPC: booke: Get/set guest EPCR register using ONE_REG interface
      KVM: PPC: bookehv: Add EPCR support in mtspr/mfspr emulation
      KVM: PPC: bookehv: Add guest computation mode for irq delivery
      KVM: PPC: Make EPCR a valid field for booke64 and bookehv
      KVM: PPC: booke: Extend MAS2 EPN mask for 64-bit
      KVM: PPC: e500: Mask MAS2 EPN high 32-bits in 32/64 tlbwe emulation
      KVM: PPC: Mask ea's high 32-bits in 32/64 instr emulation
      KVM: PPC: e500: Add emulation helper for getting instruction ea
      KVM: PPC: bookehv64: Add support for interrupt handling
      KVM: PPC: bookehv: Remove GET_VCPU macro from exception handler
      KVM: PPC: booke: Fix get_tb() compile error on 64-bit
      KVM: PPC: e500: Silence bogus GCC warning in tlb code
      ...

commit 42897d866b120547777ae1fd316680ec53356d9c
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:29:02 2012 -0200

    KVM: x86: add kvm_arch_vcpu_postcreate callback, move TSC initialization
    
    TSC initialization will soon make use of online_vcpus.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 38883f0bf27e..731ddeee32e4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -355,6 +355,11 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 }
 
+int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |

commit b080935c8638e08134629d0a9ebdf35669bec14d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 5 23:07:19 2012 +0200

    kvm: Directly account vtime to system on guest switch
    
    Switching to or from guest context is done on ioctl context.
    So by the time we call kvm_guest_enter() or kvm_guest_exit()
    we know we are not running the idle task.
    
    As a result, we can directly account the cputime using
    vtime_account_system().
    
    There are two good reasons to do this:
    
    * We avoid some useless checks on guest switch. It optimizes
    a bit this fast path.
    
    * In the case of CONFIG_IRQ_TIME_ACCOUNTING, calling vtime_account()
    checks for irq time to account. This is pointless since we know
    we are not in an irq on guest switch. This is wasting cpu cycles
    for no good reason. vtime_account_system() OTOH is a no-op in
    this config option.
    
    * We can remove the irq disable/enable around kvm guest switch in s390.
    
    A further optimization may consist in introducing a vtime_account_guest()
    that directly calls account_guest_time().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Joerg Roedel <joerg.roedel@amd.com>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: Xiantao Zhang <xiantao.zhang@intel.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Cornelia Huck <cornelia.huck@de.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ecced9d18986..d91a95568002 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -608,9 +608,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 		kvm_s390_deliver_pending_interrupts(vcpu);
 
 	vcpu->arch.sie_block->icptcode = 0;
-	local_irq_disable();
 	kvm_guest_enter();
-	local_irq_enable();
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
 	trace_kvm_s390_sie_enter(vcpu,
@@ -629,9 +627,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
 	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
-	local_irq_disable();
 	kvm_guest_exit();
-	local_irq_enable();
 
 	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
 	return rc;

commit 87cac8f879a5ecd7109dbe688087e8810b3364eb
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 2 16:25:38 2012 +0200

    s390/kvm: dont announce RRBM support
    
    Newer kernels (linux-next with the transparent huge page patches)
    use rrbm if the feature is announced via feature bit 66.
    RRBM will cause intercepts, so KVM does not handle it right now,
    causing an illegal instruction in the guest.
    The  easy solution is to disable the feature bit for the guest.
    
    This fixes bugs like:
    Kernel BUG at 0000000000124c2a [verbose debug info unavailable]
    illegal operation: 0001 [#1] SMP
    Modules linked in: virtio_balloon virtio_net ipv6 autofs4
    CPU: 0 Not tainted 3.5.4 #1
    Process fmempig (pid: 659, task: 000000007b712fd0, ksp: 000000007bed3670)
    Krnl PSW : 0704d00180000000 0000000000124c2a (pmdp_clear_flush_young+0x5e/0x80)
         R:0 T:1 IO:1 EX:1 Key:0 M:1 W:0 P:0 AS:3 CC:1 PM:0 EA:3
         00000000003cc000 0000000000000004 0000000000000000 0000000079800000
         0000000000040000 0000000000000000 000000007bed3918 000000007cf40000
         0000000000000001 000003fff7f00000 000003d281a94000 000000007bed383c
         000000007bed3918 00000000005ecbf8 00000000002314a6 000000007bed36e0
     Krnl Code:>0000000000124c2a: b9810025          ogr     %r2,%r5
               0000000000124c2e: 41343000           la      %r3,0(%r4,%r3)
               0000000000124c32: a716fffa           brct    %r1,124c26
               0000000000124c36: b9010022           lngr    %r2,%r2
               0000000000124c3a: e3d0f0800004       lg      %r13,128(%r15)
               0000000000124c40: eb22003f000c       srlg    %r2,%r2,63
    [ 2150.713198] Call Trace:
    [ 2150.713223] ([<00000000002312c4>] page_referenced_one+0x6c/0x27c)
    [ 2150.713749]  [<0000000000233812>] page_referenced+0x32a/0x410
    [...]
    
    CC: stable@vger.kernel.org
    CC: Alex Graf <agraf@suse.de>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ecced9d18986..38883f0bf27e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -997,7 +997,7 @@ static int __init kvm_s390_init(void)
 	}
 	memcpy(facilities, S390_lowcore.stfle_fac_list, 16);
 	facilities[0] &= 0xff00fff3f47c0000ULL;
-	facilities[1] &= 0x201c000000000000ULL;
+	facilities[1] &= 0x001c000000000000ULL;
 	return 0;
 }
 

commit 2df72e9bc4c505d8357012f2924589f3d16f9d44
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Aug 24 15:54:57 2012 -0300

    KVM: split kvm_arch_flush_shadow
    
    Introducing kvm_arch_flush_shadow_memslot, to invalidate the
    translations of a single memory slot.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e83df7f0fedd..ecced9d18986 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -969,7 +969,12 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 	return;
 }
 
-void kvm_arch_flush_shadow(struct kvm *kvm)
+void kvm_arch_flush_shadow_all(struct kvm *kvm)
+{
+}
+
+void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
+				   struct kvm_memory_slot *slot)
 {
 }
 

commit ade38c311a0ad8c32e902fe1d0ae74d0d44bc71e
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Mon Jul 23 17:20:30 2012 +0200

    KVM: s390: Add implementation-specific trace events
    
    Introduce a new trace system, kvm-s390, for some kvm/s390 specific
    trace points:
    
    - injection of interrupts
    - delivery of interrupts to the guest
    - creation/destruction of kvm machines and vcpus
    - stop actions for vcpus
    - reset requests for userspace
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4613602e123e..e83df7f0fedd 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -34,6 +34,7 @@
 
 #define CREATE_TRACE_POINTS
 #include "trace.h"
+#include "trace-s390.h"
 
 #define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
 
@@ -245,6 +246,7 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
+	trace_kvm_s390_destroy_vcpu(vcpu->vcpu_id);
 	if (!kvm_is_ucontrol(vcpu->kvm)) {
 		clear_bit(63 - vcpu->vcpu_id,
 			  (unsigned long *) &vcpu->kvm->arch.sca->mcn);
@@ -420,6 +422,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		goto out_free_sie_block;
 	VM_EVENT(kvm, 3, "create cpu %d at %p, sie block at %p", id, vcpu,
 		 vcpu->arch.sie_block);
+	trace_kvm_s390_create_vcpu(id, vcpu, vcpu->arch.sie_block);
 
 	return vcpu;
 out_free_sie_block:

commit 5786fffa96ae7c3f8111e29fb62e073a45efc557
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Mon Jul 23 17:20:29 2012 +0200

    KVM: s390: Add architectural trace events
    
    Add trace events for several s390 architecture specifics:
    
    - SIE entry/exit
    - common intercepts
    - common instructions (sigp/diagnose)
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d470ccbfabae..4613602e123e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -32,6 +32,9 @@
 #include "kvm-s390.h"
 #include "gaccess.h"
 
+#define CREATE_TRACE_POINTS
+#include "trace.h"
+
 #define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
 
 struct kvm_stats_debugfs_item debugfs_entries[] = {
@@ -607,18 +610,22 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
+	trace_kvm_s390_sie_enter(vcpu,
+				 atomic_read(&vcpu->arch.sie_block->cpuflags));
 	rc = sie64a(vcpu->arch.sie_block, vcpu->run->s.regs.gprs);
 	if (rc) {
 		if (kvm_is_ucontrol(vcpu->kvm)) {
 			rc = SIE_INTERCEPT_UCONTROL;
 		} else {
 			VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
+			trace_kvm_s390_sie_fault(vcpu);
 			kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
 			rc = 0;
 		}
 	}
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
+	trace_kvm_s390_sie_exit(vcpu, vcpu->arch.sie_block->icptcode);
 	local_irq_disable();
 	kvm_guest_exit();
 	local_irq_enable();

commit 5fecc9d8f59e765c2a48379dd7c6f5cf88c7d75a
Merge: 3c4cfadef6a1 1a577b72475d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 24 12:01:20 2012 -0700

    Merge tag 'kvm-3.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Avi Kivity:
     "Highlights include
      - full big real mode emulation on pre-Westmere Intel hosts (can be
        disabled with emulate_invalid_guest_state=0)
      - relatively small ppc and s390 updates
      - PCID/INVPCID support in guests
      - EOI avoidance; 3.6 guests should perform better on 3.6 hosts on
        interrupt intensive workloads)
      - Lockless write faults during live migration
      - EPT accessed/dirty bits support for new Intel processors"
    
    Fix up conflicts in:
     - Documentation/virtual/kvm/api.txt:
    
       Stupid subchapter numbering, added next to each other.
    
     - arch/powerpc/kvm/booke_interrupts.S:
    
       PPC asm changes clashing with the KVM fixes
    
     - arch/s390/include/asm/sigp.h, arch/s390/kvm/sigp.c:
    
       Duplicated commits through the kvm tree and the s390 tree, with
       subsequent edits in the KVM tree.
    
    * tag 'kvm-3.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (93 commits)
      KVM: fix race with level interrupts
      x86, hyper: fix build with !CONFIG_KVM_GUEST
      Revert "apic: fix kvm build on UP without IOAPIC"
      KVM guest: switch to apic_set_eoi_write, apic_write
      apic: add apic_set_eoi_write for PV use
      KVM: VMX: Implement PCID/INVPCID for guests with EPT
      KVM: Add x86_hyper_kvm to complete detect_hypervisor_platform check
      KVM: PPC: Critical interrupt emulation support
      KVM: PPC: e500mc: Fix tlbilx emulation for 64-bit guests
      KVM: PPC64: booke: Set interrupt computation mode for 64-bit host
      KVM: PPC: bookehv: Add ESR flag to Data Storage Interrupt
      KVM: PPC: bookehv64: Add support for std/ld emulation.
      booke: Added crit/mc exception handler for e500v2
      booke/bookehv: Add host crit-watchdog exception support
      KVM: MMU: document mmu-lock and fast page fault
      KVM: MMU: fix kvm_mmu_pagetable_walk tracepoint
      KVM: MMU: trace fast page fault
      KVM: MMU: fast path of handling guest page fault
      KVM: MMU: introduce SPTE_MMU_WRITEABLE bit
      KVM: MMU: fold tlb flush judgement into mmu_spte_update
      ...

commit a53c8fab3f87c995c30ac226a03af95361243144
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jul 20 11:15:04 2012 +0200

    s390/comments: unify copyright messages and remove file names
    
    Remove the file name from the comment at top of many files. In most
    cases the file name was wrong anyway, so it's rather pointless.
    
    Also unify the IBM copyright statement. We did have a lot of sightly
    different statements and wanted to change them one after another
    whenever a file gets touched. However that never happened. Instead
    people start to take the old/"wrong" statements to use as a template
    for new files.
    So unify all of them in one go.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 664766d0c83c..c552d1f4103f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1,7 +1,7 @@
 /*
- * s390host.c --  hosting zSeries kernel virtual machines
+ * hosting zSeries kernel virtual machines
  *
- * Copyright IBM Corp. 2008,2009
+ * Copyright IBM Corp. 2008, 2009
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License (version 2 only)

commit 61bde82caee95426bf1ad53fefc8dc691b8ba37c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Jun 11 16:06:57 2012 +0200

    KVM: s390: Set CPU in stopped state on initial cpu reset
    
    The initial cpu reset sets the cpu in the stopped state.
    Several places check for the cpu state (e.g. sigp set prefix) and
    not setting the STOPPED state triggered errors with newer guest
    kernels after reboot.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 664766d0c83c..ace93603d861 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -347,6 +347,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.guest_fpregs.fpc = 0;
 	asm volatile("lfpc %0" : : "Q" (vcpu->arch.guest_fpregs.fpc));
 	vcpu->arch.sie_block->gbea = 1;
+	atomic_set_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 }
 
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)

commit 46a6dd1c87c4ff17202574127daf70cc0bb38d6d
Author: Jason J. herne <jjherne@us.ibm.com>
Date:   Tue May 15 14:15:28 2012 +0200

    KVM: s390: onereg for timer related registers
    
    Enhance the KVM ONE_REG capability within S390 to allow
    getting/setting the following special cpu registers: clock comparator
    and the cpu timer. These are needed for migration.
    
    Signed-off-by: Jason J. herne <jjherne@us.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index cc4c01322ce4..664766d0c83c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -454,6 +454,14 @@ static int kvm_arch_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
 		r = put_user(vcpu->arch.sie_block->epoch,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_CPU_TIMER:
+		r = put_user(vcpu->arch.sie_block->cputm,
+			     (u64 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_CLOCK_COMP:
+		r = put_user(vcpu->arch.sie_block->ckc,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}
@@ -475,6 +483,14 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 		r = get_user(vcpu->arch.sie_block->epoch,
 			     (u64 __user *)reg->addr);
 		break;
+	case KVM_REG_S390_CPU_TIMER:
+		r = get_user(vcpu->arch.sie_block->cputm,
+			     (u64 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_CLOCK_COMP:
+		r = get_user(vcpu->arch.sie_block->ckc,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}

commit 29b7c71b5ecf2caaa4c2105ecc0094826db8a8a8
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue May 15 14:15:27 2012 +0200

    KVM: s390: epoch difference and TOD programmable field
    
    This patch makes vcpu epoch difference and the TOD programmable
    field accessible from userspace. This is needed in order to
    implement a couple of instructions that deal with the time of
    day clock on s390, such as SET CLOCK and for migration.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 894b3e47b8c1..cc4c01322ce4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -446,6 +446,14 @@ static int kvm_arch_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
 	int r = -EINVAL;
 
 	switch (reg->id) {
+	case KVM_REG_S390_TODPR:
+		r = put_user(vcpu->arch.sie_block->todpr,
+			     (u32 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_EPOCHDIFF:
+		r = put_user(vcpu->arch.sie_block->epoch,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}
@@ -459,6 +467,14 @@ static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 	int r = -EINVAL;
 
 	switch (reg->id) {
+	case KVM_REG_S390_TODPR:
+		r = get_user(vcpu->arch.sie_block->todpr,
+			     (u32 __user *)reg->addr);
+		break;
+	case KVM_REG_S390_EPOCHDIFF:
+		r = get_user(vcpu->arch.sie_block->epoch,
+			     (u64 __user *)reg->addr);
+		break;
 	default:
 		break;
 	}

commit 14eebd917d154f3181d2cb219e40b18eec314db7
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue May 15 14:15:26 2012 +0200

    KVM: s390: KVM_GET/SET_ONEREG for s390
    
    This patch enables KVM_CAP_ONE_REG for s390 and implements stubs
    for KVM_GET/SET_ONE_REG. This is based on the ppc implementation.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5c761bffa02d..894b3e47b8c1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -135,6 +135,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_S390_UCONTROL:
 #endif
 	case KVM_CAP_SYNC_REGS:
+	case KVM_CAP_ONE_REG:
 		r = 1;
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -439,6 +440,31 @@ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+static int kvm_arch_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
+					   struct kvm_one_reg *reg)
+{
+	int r = -EINVAL;
+
+	switch (reg->id) {
+	default:
+		break;
+	}
+
+	return r;
+}
+
+static int kvm_arch_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
+					   struct kvm_one_reg *reg)
+{
+	int r = -EINVAL;
+
+	switch (reg->id) {
+	default:
+		break;
+	}
+
+	return r;
+}
 
 static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 {
@@ -770,6 +796,18 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	case KVM_S390_INITIAL_RESET:
 		r = kvm_arch_vcpu_ioctl_initial_reset(vcpu);
 		break;
+	case KVM_SET_ONE_REG:
+	case KVM_GET_ONE_REG: {
+		struct kvm_one_reg reg;
+		r = -EFAULT;
+		if (copy_from_user(&reg, argp, sizeof(reg)))
+			break;
+		if (ioctl == KVM_SET_ONE_REG)
+			r = kvm_arch_vcpu_ioctl_set_one_reg(vcpu, &reg);
+		else
+			r = kvm_arch_vcpu_ioctl_get_one_reg(vcpu, &reg);
+		break;
+	}
 #ifdef CONFIG_KVM_S390_UCONTROL
 	case KVM_S390_UCAS_MAP: {
 		struct kvm_s390_ucas_mapping ucasmap;

commit 1526bf9ccf310f1d35c1275b8b477a249d25aaf2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue May 15 14:15:25 2012 +0200

    KVM: s390: add capability indicating COW support
    
    Currently qemu/kvm on s390 uses a guest mapping that does not
    allow the guest backing page table to be write-protected to
    support older systems. On those older systems a host write
    protection fault will be delivered to the guest.
    
    Newer systems allow to write-protect the guest backing memory
    and let the fault be delivered to the host, thus allowing COW.
    
    Use a capability bit to tell qemu if that is possible.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index e5e3800b0125..5c761bffa02d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -28,6 +28,7 @@
 #include <asm/pgtable.h>
 #include <asm/nmi.h>
 #include <asm/switch_to.h>
+#include <asm/sclp.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
 
@@ -140,6 +141,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
 		break;
+	case KVM_CAP_S390_COW:
+		r = sclp_get_fac85() & 0x2;
+		break;
 	default:
 		r = 0;
 	}

commit e726b1bd64b0b8945c171d2d4bf749fba9fc0800
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed May 2 10:50:38 2012 +0200

    KVM: s390: implement KVM_CAP_NR/MAX_VCPUS
    
    Let userspace know the number of max and supported cpus for kvm on s390.
    Return KVM_MAX_VCPUS (currently 64) for both values.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fd98914a36f1..e5e3800b0125 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -136,6 +136,10 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_SYNC_REGS:
 		r = 1;
 		break;
+	case KVM_CAP_NR_VCPUS:
+	case KVM_CAP_MAX_VCPUS:
+		r = KVM_MAX_VCPUS;
+		break;
 	default:
 		r = 0;
 	}

commit 41628d334361670d825fb03c04568f5ef9f084dc
Author: Konstantin Weitz <WEITZKON@de.ibm.com>
Date:   Wed Apr 25 15:30:38 2012 +0200

    KVM: s390: Implement the directed yield (diag 9c) hypervisor call for KVM
    
    This patch implements the directed yield hypercall found on other
    System z hypervisors. It delegates execution time to the virtual cpu
    specified in the instruction's parameter.
    
    Useful to avoid long spinlock waits in the guest.
    
    Christian Borntraeger: moved common code in virt/kvm/
    
    Signed-off-by: Konstantin Weitz <WEITZKON@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d30c8350b949..fd98914a36f1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -74,6 +74,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_sigp_restart", VCPU_STAT(instruction_sigp_restart) },
 	{ "diagnose_10", VCPU_STAT(diagnose_10) },
 	{ "diagnose_44", VCPU_STAT(diagnose_44) },
+	{ "diagnose_9c", VCPU_STAT(diagnose_9c) },
 	{ NULL }
 };
 

commit b6d33834bd4e8bdf4a199812e31b3e36da53c794
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Thu Mar 8 16:44:24 2012 -0500

    KVM: Factor out kvm_vcpu_kick to arch-generic code
    
    The kvm_vcpu_kick function performs roughly the same funcitonality on
    most all architectures, so we shouldn't have separate copies.
    
    PowerPC keeps a pointer to interchanging waitqueues on the vcpu_arch
    structure and to accomodate this special need a
    __KVM_HAVE_ARCH_VCPU_GET_WQ define and accompanying function
    kvm_arch_vcpu_wq have been defined. For all other architectures this
    is a generic inline that just returns &vcpu->wq;
    
    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 217ce44395a4..d30c8350b949 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -423,6 +423,14 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
+{
+	/* kvm common code refers to this, but never calls it */
+	BUG();
+	return 0;
+}
+
+
 static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 {
 	kvm_s390_vcpu_initial_reset(vcpu);

commit 0195c00244dc2e9f522475868fa278c473ba7339
Merge: f21ce8f8447c 141124c02059
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 15:58:21 2012 -0700

    Merge tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system
    
    Pull "Disintegrate and delete asm/system.h" from David Howells:
     "Here are a bunch of patches to disintegrate asm/system.h into a set of
      separate bits to relieve the problem of circular inclusion
      dependencies.
    
      I've built all the working defconfigs from all the arches that I can
      and made sure that they don't break.
    
      The reason for these patches is that I recently encountered a circular
      dependency problem that came about when I produced some patches to
      optimise get_order() by rewriting it to use ilog2().
    
      This uses bitops - and on the SH arch asm/bitops.h drags in
      asm-generic/get_order.h by a circuituous route involving asm/system.h.
    
      The main difficulty seems to be asm/system.h.  It holds a number of
      low level bits with no/few dependencies that are commonly used (eg.
      memory barriers) and a number of bits with more dependencies that
      aren't used in many places (eg.  switch_to()).
    
      These patches break asm/system.h up into the following core pieces:
    
        (1) asm/barrier.h
    
            Move memory barriers here.  This already done for MIPS and Alpha.
    
        (2) asm/switch_to.h
    
            Move switch_to() and related stuff here.
    
        (3) asm/exec.h
    
            Move arch_align_stack() here.  Other process execution related bits
            could perhaps go here from asm/processor.h.
    
        (4) asm/cmpxchg.h
    
            Move xchg() and cmpxchg() here as they're full word atomic ops and
            frequently used by atomic_xchg() and atomic_cmpxchg().
    
        (5) asm/bug.h
    
            Move die() and related bits.
    
        (6) asm/auxvec.h
    
            Move AT_VECTOR_SIZE_ARCH here.
    
      Other arch headers are created as needed on a per-arch basis."
    
    Fixed up some conflicts from other header file cleanups and moving code
    around that has happened in the meantime, so David's testing is somewhat
    weakened by that.  We'll find out anything that got broken and fix it..
    
    * tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system: (38 commits)
      Delete all instances of asm/system.h
      Remove all #inclusions of asm/system.h
      Add #includes needed to permit the removal of asm/system.h
      Move all declarations of free_initmem() to linux/mm.h
      Disintegrate asm/system.h for OpenRISC
      Split arch_align_stack() out from asm-generic/system.h
      Split the switch_to() wrapper out of asm-generic/system.h
      Move the asm-generic/system.h xchg() implementation to asm-generic/cmpxchg.h
      Create asm-generic/barrier.h
      Make asm-generic/cmpxchg.h #include asm-generic/cmpxchg-local.h
      Disintegrate asm/system.h for Xtensa
      Disintegrate asm/system.h for Unicore32 [based on ver #3, changed by gxt]
      Disintegrate asm/system.h for Tile
      Disintegrate asm/system.h for Sparc
      Disintegrate asm/system.h for SH
      Disintegrate asm/system.h for Score
      Disintegrate asm/system.h for S390
      Disintegrate asm/system.h for PowerPC
      Disintegrate asm/system.h for PA-RISC
      Disintegrate asm/system.h for MN10300
      ...

commit a0616cdebcfd575dcd4c46102d1b52fbb827fc29
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for S390
    
    Disintegrate asm/system.h for S390.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-s390@vger.kernel.org

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d1c445732451..d56de16a651a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -27,7 +27,7 @@
 #include <asm/lowcore.h>
 #include <asm/pgtable.h>
 #include <asm/nmi.h>
-#include <asm/system.h>
+#include <asm/switch_to.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
 

commit db3fe4eb45f3555d91a7124e18cf3a2f2a30eb90
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Feb 8 13:02:18 2012 +0900

    KVM: Introduce kvm_memory_slot::arch and move lpage_info into it
    
    Some members of kvm_memory_slot are not used by every architecture.
    
    This patch is the first step to make this difference clear by
    introducing kvm_memory_slot::arch;  lpage_info is moved into it.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index cf3c0a91d046..17ad69d596fd 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -814,6 +814,16 @@ int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 	return VM_FAULT_SIGBUS;
 }
 
+void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+			   struct kvm_memory_slot *dont)
+{
+}
+
+int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
+{
+	return 0;
+}
+
 /* Section: memory related */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,

commit 9eed0735ca6a5cf386a4998ad4b6d52d1e29353f
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Feb 6 10:59:07 2012 +0100

    KVM: s390: provide control registers via kvm_run
    
    There are several cases were we need the control registers for
    userspace. Lets also provide those in kvm_run.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 121316ef2e15..cf3c0a91d046 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -291,7 +291,8 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
 				    KVM_SYNC_GPRS |
-				    KVM_SYNC_ACRS;
+				    KVM_SYNC_ACRS |
+				    KVM_SYNC_CRS;
 	return 0;
 }
 
@@ -580,6 +581,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		kvm_run->kvm_dirty_regs &= ~KVM_SYNC_PREFIX;
 		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
 	}
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_CRS) {
+		kvm_run->kvm_dirty_regs &= ~KVM_SYNC_CRS;
+		memcpy(&vcpu->arch.sie_block->gcr, &kvm_run->s.regs.crs, 128);
+		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
+	}
 
 	might_fault();
 
@@ -629,6 +635,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	kvm_run->psw_mask     = vcpu->arch.sie_block->gpsw.mask;
 	kvm_run->psw_addr     = vcpu->arch.sie_block->gpsw.addr;
 	kvm_run->s.regs.prefix = vcpu->arch.sie_block->prefix;
+	memcpy(&kvm_run->s.regs.crs, &vcpu->arch.sie_block->gcr, 128);
 
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);

commit 851755871c1f3184f4124c466e85881f17fa3226
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Feb 6 10:59:02 2012 +0100

    KVM: s390: Sanitize fpc registers for KVM_SET_FPU
    
    commit 7eef87dc99e419b1cc051e4417c37e4744d7b661 (KVM: s390: fix
    register setting) added a load of the floating point control register
    to the KVM_SET_FPU path. Lets make sure that the fpc is valid.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0b91679369bc..121316ef2e15 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -460,7 +460,7 @@ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
 	memcpy(&vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
-	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
+	vcpu->arch.guest_fpregs.fpc = fpu->fpc & FPC_VALID_MASK;
 	restore_fp_regs(&vcpu->arch.guest_fpregs);
 	return 0;
 }

commit 59674c1a6a35d56ae5197cbc9abe7bfec6762ba9
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jan 11 11:20:33 2012 +0100

    KVM: s390: provide access guest registers via kvm_run
    
    This patch adds the access registers to the kvm_run structure.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 80b12bac6a5b..0b91679369bc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -289,7 +289,9 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	}
 
 	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
-	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX | KVM_SYNC_GPRS;
+	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
+				    KVM_SYNC_GPRS |
+				    KVM_SYNC_ACRS;
 	return 0;
 }
 
@@ -304,7 +306,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	save_access_regs(vcpu->arch.host_acrs);
 	vcpu->arch.guest_fpregs.fpc &= FPC_VALID_MASK;
 	restore_fp_regs(&vcpu->arch.guest_fpregs);
-	restore_access_regs(vcpu->arch.guest_acrs);
+	restore_access_regs(vcpu->run->s.regs.acrs);
 	gmap_enable(vcpu->arch.gmap);
 	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 }
@@ -314,7 +316,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
 	save_fp_regs(&vcpu->arch.guest_fpregs);
-	save_access_regs(vcpu->arch.guest_acrs);
+	save_access_regs(vcpu->run->s.regs.acrs);
 	restore_fp_regs(&vcpu->arch.host_fpregs);
 	restore_access_regs(vcpu->arch.host_acrs);
 }
@@ -441,16 +443,16 @@ int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs)
 {
-	memcpy(&vcpu->arch.guest_acrs, &sregs->acrs, sizeof(sregs->acrs));
+	memcpy(&vcpu->run->s.regs.acrs, &sregs->acrs, sizeof(sregs->acrs));
 	memcpy(&vcpu->arch.sie_block->gcr, &sregs->crs, sizeof(sregs->crs));
-	restore_access_regs(vcpu->arch.guest_acrs);
+	restore_access_regs(vcpu->run->s.regs.acrs);
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs)
 {
-	memcpy(&sregs->acrs, &vcpu->arch.guest_acrs, sizeof(sregs->acrs));
+	memcpy(&sregs->acrs, &vcpu->run->s.regs.acrs, sizeof(sregs->acrs));
 	memcpy(&sregs->crs, &vcpu->arch.sie_block->gcr, sizeof(sregs->crs));
 	return 0;
 }
@@ -702,7 +704,7 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 		return -EFAULT;
 
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, acc_regs),
-			&vcpu->arch.guest_acrs, 64, prefix))
+			&vcpu->run->s.regs.acrs, 64, prefix))
 		return -EFAULT;
 
 	if (__guestcopy(vcpu,

commit 5a32c1af56b3c74212b1de2a1d1658c303dd3516
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jan 11 11:20:32 2012 +0100

    KVM: s390: provide general purpose guest registers via kvm_run
    
    This patch adds the general purpose registers to the kvm_run structure.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6962c1b9eec6..80b12bac6a5b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -289,7 +289,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	}
 
 	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
-	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX;
+	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX | KVM_SYNC_GPRS;
 	return 0;
 }
 
@@ -428,13 +428,13 @@ static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	memcpy(&vcpu->arch.guest_gprs, &regs->gprs, sizeof(regs->gprs));
+	memcpy(&vcpu->run->s.regs.gprs, &regs->gprs, sizeof(regs->gprs));
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	memcpy(&regs->gprs, &vcpu->arch.guest_gprs, sizeof(regs->gprs));
+	memcpy(&regs->gprs, &vcpu->run->s.regs.gprs, sizeof(regs->gprs));
 	return 0;
 }
 
@@ -511,7 +511,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc;
 
-	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->arch.guest_gprs[14], 16);
+	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->run->s.regs.gprs[14], 16);
 
 	if (need_resched())
 		schedule();
@@ -528,7 +528,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
-	rc = sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs);
+	rc = sie64a(vcpu->arch.sie_block, vcpu->run->s.regs.gprs);
 	if (rc) {
 		if (kvm_is_ucontrol(vcpu->kvm)) {
 			rc = SIE_INTERCEPT_UCONTROL;
@@ -544,7 +544,7 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	kvm_guest_exit();
 	local_irq_enable();
 
-	memcpy(&vcpu->arch.guest_gprs[14], &vcpu->arch.sie_block->gg14, 16);
+	memcpy(&vcpu->run->s.regs.gprs[14], &vcpu->arch.sie_block->gg14, 16);
 	return rc;
 }
 
@@ -673,7 +673,7 @@ int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 		return -EFAULT;
 
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, gp_regs),
-			vcpu->arch.guest_gprs, 128, prefix))
+			vcpu->run->s.regs.gprs, 128, prefix))
 		return -EFAULT;
 
 	if (__guestcopy(vcpu, addr + offsetof(struct save_area, psw),

commit 60b413c9248495ea400e80e08e4d1e28ed7ee05e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jan 11 11:20:31 2012 +0100

    KVM: s390: provide the prefix register via kvm_run
    
    Add the prefix register to the synced register field in kvm_run.
    While we need the prefix register most of the time read-only, this
    patch also adds handling for guest dirtying of the prefix register.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1868b89a840f..6962c1b9eec6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -132,6 +132,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #ifdef CONFIG_KVM_S390_UCONTROL
 	case KVM_CAP_S390_UCONTROL:
 #endif
+	case KVM_CAP_SYNC_REGS:
 		r = 1;
 		break;
 	default:
@@ -288,6 +289,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	}
 
 	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
+	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX;
 	return 0;
 }
 
@@ -572,6 +574,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
 	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
+	if (kvm_run->kvm_dirty_regs & KVM_SYNC_PREFIX) {
+		kvm_run->kvm_dirty_regs &= ~KVM_SYNC_PREFIX;
+		kvm_s390_set_prefix(vcpu, kvm_run->s.regs.prefix);
+	}
 
 	might_fault();
 
@@ -620,6 +626,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	kvm_run->psw_mask     = vcpu->arch.sie_block->gpsw.mask;
 	kvm_run->psw_addr     = vcpu->arch.sie_block->gpsw.addr;
+	kvm_run->s.regs.prefix = vcpu->arch.sie_block->prefix;
 
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);

commit 8d26cf7b40b1648c39e77a113dac07ad31363120
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Jan 11 11:19:32 2012 +0100

    KVM: s390: rework code that sets the prefix
    
    There are several places in the kvm module, which set the prefix register.
    Since we need to flush the cpu, lets combine this operation into a helper
    function. This helper will also explicitely mask out the unused bits.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a33b44487540..1868b89a840f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -322,8 +322,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	/* this equals initial cpu reset in pop, but we don't switch to ESA */
 	vcpu->arch.sie_block->gpsw.mask = 0UL;
 	vcpu->arch.sie_block->gpsw.addr = 0UL;
-	vcpu->arch.sie_block->prefix    = 0UL;
-	vcpu->arch.sie_block->ihcpu     = 0xffff;
+	kvm_s390_set_prefix(vcpu, 0);
 	vcpu->arch.sie_block->cputm     = 0UL;
 	vcpu->arch.sie_block->ckc       = 0UL;
 	vcpu->arch.sie_block->todpr     = 0;

commit 3e6afcf1d8ce6bca6bfa62a49c1d70d2f16162eb
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:30 2012 +0100

    KVM: s390: Fix return code for unknown ioctl numbers
    
    This patch fixes the return code of kvm_arch_vcpu_ioctl in case
    of an unkown ioctl number.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a1061b361c43..a33b44487540 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -781,7 +781,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		break;
 	}
 	default:
-		r = -EINVAL;
+		r = -ENOTTY;
 	}
 	return r;
 }

commit 1efd0f595ab9d10fef1486dfdef952107c91f3db
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:29 2012 +0100

    KVM: s390: ucontrol: announce capability for user controlled vms
    
    This patch announces a new capability KVM_CAP_S390_UCONTROL that
    indicates that kvm can now support virtual machines that are
    controlled by userspace.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index abf784d8c680..a1061b361c43 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -129,6 +129,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_S390_PSW:
 	case KVM_CAP_S390_GMAP:
 	case KVM_CAP_SYNC_MMU:
+#ifdef CONFIG_KVM_S390_UCONTROL
+	case KVM_CAP_S390_UCONTROL:
+#endif
 		r = 1;
 		break;
 	default:

commit 58f9460ba1cc9de67e6591bfd08dccded43d27bd
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:27 2012 +0100

    KVM: s390: ucontrol: disable sca
    
    This patch makes sure user controlled virtual machines do not use a
    system control area (sca). This is needed in order to create
    virtual machines with more cpus than the size of the sca [64].
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8489edf80c89..abf784d8c680 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -228,10 +228,13 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
-	clear_bit(63 - vcpu->vcpu_id, (unsigned long *) &vcpu->kvm->arch.sca->mcn);
-	if (vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda ==
-		(__u64) vcpu->arch.sie_block)
-		vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda = 0;
+	if (!kvm_is_ucontrol(vcpu->kvm)) {
+		clear_bit(63 - vcpu->vcpu_id,
+			  (unsigned long *) &vcpu->kvm->arch.sca->mcn);
+		if (vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda ==
+		    (__u64) vcpu->arch.sie_block)
+			vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda = 0;
+	}
 	smp_mb();
 
 	if (kvm_is_ucontrol(vcpu->kvm))
@@ -368,12 +371,19 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		goto out_free_cpu;
 
 	vcpu->arch.sie_block->icpua = id;
-	BUG_ON(!kvm->arch.sca);
-	if (!kvm->arch.sca->cpu[id].sda)
-		kvm->arch.sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
-	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)kvm->arch.sca) >> 32);
-	vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
-	set_bit(63 - id, (unsigned long *) &kvm->arch.sca->mcn);
+	if (!kvm_is_ucontrol(kvm)) {
+		if (!kvm->arch.sca) {
+			WARN_ON_ONCE(1);
+			goto out_free_cpu;
+		}
+		if (!kvm->arch.sca->cpu[id].sda)
+			kvm->arch.sca->cpu[id].sda =
+				(__u64) vcpu->arch.sie_block;
+		vcpu->arch.sie_block->scaoh =
+			(__u32)(((__u64)kvm->arch.sca) >> 32);
+		vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
+		set_bit(63 - id, (unsigned long *) &kvm->arch.sca->mcn);
+	}
 
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	INIT_LIST_HEAD(&vcpu->arch.local_int.list);

commit ccc7910fe564d99415def7c041fa261e62a43011
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:26 2012 +0100

    KVM: s390: ucontrol: interface to inject faults on a vcpu page table
    
    This patch allows the user to fault in pages on a virtual cpus
    address space for user controlled virtual machines. Typically this
    is superfluous because userspace can just create a mapping and
    let the kernel's page fault logic take are of it. There is one
    exception: SIE won't start if the lowcore is not present. Normally
    the kernel takes care of this [handle_validity() in
    arch/s390/kvm/intercept.c] but since the kernel does not handle
    intercepts for user controlled virtual machines, userspace needs to
    be able to handle this condition.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5b5c28e471df..8489edf80c89 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -761,6 +761,12 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		break;
 	}
 #endif
+	case KVM_S390_VCPU_FAULT: {
+		r = gmap_fault(arg, vcpu->arch.gmap);
+		if (!IS_ERR_VALUE(r))
+			r = 0;
+		break;
+	}
 	default:
 		r = -EINVAL;
 	}

commit d6b6d166864fa97ca3b1ed1a5c62fd3b53d4606f
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:25 2012 +0100

    KVM: s390: ucontrol: disable in-kernel irq stack
    
    This patch disables the in-kernel interrupt stack for KVM virtual
    machines that are controlled by user. Userspace has to take care
    of handling interrupts on its own.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 02510946864e..5b5c28e471df 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -505,7 +505,8 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 	if (test_thread_flag(TIF_MCCK_PENDING))
 		s390_handle_mcck();
 
-	kvm_s390_deliver_pending_interrupts(vcpu);
+	if (!kvm_is_ucontrol(vcpu->kvm))
+		kvm_s390_deliver_pending_interrupts(vcpu);
 
 	vcpu->arch.sie_block->icptcode = 0;
 	local_irq_disable();

commit c0d744a9c8aae81b624e0650e6fbbbb83db1a145
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:24 2012 +0100

    KVM: s390: ucontrol: disable in-kernel handling of SIE intercepts
    
    This patch disables in-kernel handling of SIE intercepts for user
    controlled virtual machines. All intercepts are passed to userspace
    via KVM_EXIT_SIE exit reason just like SIE intercepts that cannot be
    handled in-kernel for regular KVM guests.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d6bc65aeb950..02510946864e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -566,7 +566,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = __vcpu_run(vcpu);
 		if (rc)
 			break;
-		rc = kvm_handle_sie_intercept(vcpu);
+		if (kvm_is_ucontrol(vcpu->kvm))
+			rc = -EOPNOTSUPP;
+		else
+			rc = kvm_handle_sie_intercept(vcpu);
 	} while (!signal_pending(current) && !rc);
 
 	if (rc == SIE_INTERCEPT_RERUNVCPU)

commit 5b1c1493afe8d69909f9df3221bb2fffdf479f4a
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:23 2012 +0100

    KVM: s390: ucontrol: export SIE control block to user
    
    This patch exports the s390 SIE hardware control block to userspace
    via the mapping of the vcpu file descriptor. In order to do so,
    a new arch callback named kvm_arch_vcpu_fault  is introduced for all
    architectures. It allows to map architecture specific pages.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index af05328aca25..d6bc65aeb950 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -763,6 +763,19 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	return r;
 }
 
+int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
+{
+#ifdef CONFIG_KVM_S390_UCONTROL
+	if ((vmf->pgoff == KVM_S390_SIE_PAGE_OFFSET)
+		 && (kvm_is_ucontrol(vcpu->kvm))) {
+		vmf->page = virt_to_page(vcpu->arch.sie_block);
+		get_page(vmf->page);
+		return 0;
+	}
+#endif
+	return VM_FAULT_SIGBUS;
+}
+
 /* Section: memory related */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,

commit e168bf8de33e16a909df2401af1f7d419c5780de
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:22 2012 +0100

    KVM: s390: ucontrol: export page faults to user
    
    This patch introduces a new exit reason in the kvm_run structure
    named KVM_EXIT_S390_UCONTROL. This exit indicates, that a virtual cpu
    has regognized a fault on the host page table. The idea is that
    userspace can handle this fault by mapping memory at the fault
    location into the cpu's address space and then continue to run the
    virtual cpu.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 2d3248895def..af05328aca25 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -493,8 +493,10 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
-static void __vcpu_run(struct kvm_vcpu *vcpu)
+static int __vcpu_run(struct kvm_vcpu *vcpu)
 {
+	int rc;
+
 	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->arch.guest_gprs[14], 16);
 
 	if (need_resched())
@@ -511,9 +513,15 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
-	if (sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs)) {
-		VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
-		kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
+	rc = sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs);
+	if (rc) {
+		if (kvm_is_ucontrol(vcpu->kvm)) {
+			rc = SIE_INTERCEPT_UCONTROL;
+		} else {
+			VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
+			kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
+			rc = 0;
+		}
 	}
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
@@ -522,6 +530,7 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 
 	memcpy(&vcpu->arch.guest_gprs[14], &vcpu->arch.sie_block->gg14, 16);
+	return rc;
 }
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
@@ -542,6 +551,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	case KVM_EXIT_UNKNOWN:
 	case KVM_EXIT_INTR:
 	case KVM_EXIT_S390_RESET:
+	case KVM_EXIT_S390_UCONTROL:
 		break;
 	default:
 		BUG();
@@ -553,7 +563,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	might_fault();
 
 	do {
-		__vcpu_run(vcpu);
+		rc = __vcpu_run(vcpu);
+		if (rc)
+			break;
 		rc = kvm_handle_sie_intercept(vcpu);
 	} while (!signal_pending(current) && !rc);
 
@@ -565,6 +577,16 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = -EINTR;
 	}
 
+#ifdef CONFIG_KVM_S390_UCONTROL
+	if (rc == SIE_INTERCEPT_UCONTROL) {
+		kvm_run->exit_reason = KVM_EXIT_S390_UCONTROL;
+		kvm_run->s390_ucontrol.trans_exc_code =
+			current->thread.gmap_addr;
+		kvm_run->s390_ucontrol.pgm_code = 0x10;
+		rc = 0;
+	}
+#endif
+
 	if (rc == -EOPNOTSUPP) {
 		/* intercept cannot be handled in-kernel, prepare kvm-run */
 		kvm_run->exit_reason         = KVM_EXIT_S390_SIEIC;

commit 27e0393f15fc8bc855c6a888387ff5ffd2181089
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:21 2012 +0100

    KVM: s390: ucontrol: per vcpu address spaces
    
    This patch introduces two ioctls for virtual cpus, that are only
    valid for kernel virtual machines that are controlled by userspace.
    Each virtual cpu has its individual address space in this mode of
    operation, and each address space is backed by the gmap
    implementation just like the address space for regular KVM guests.
    KVM_S390_UCAS_MAP allows to map a part of the user's virtual address
    space to the vcpu. Starting offset and length in both the user and
    the vcpu address space need to be aligned to 1M.
    KVM_S390_UCAS_UNMAP can be used to unmap a range of memory from a
    virtual cpu in a similar way.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f0937552175b..2d3248895def 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -233,6 +233,10 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		(__u64) vcpu->arch.sie_block)
 		vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda = 0;
 	smp_mb();
+
+	if (kvm_is_ucontrol(vcpu->kvm))
+		gmap_free(vcpu->arch.gmap);
+
 	free_page((unsigned long)(vcpu->arch.sie_block));
 	kvm_vcpu_uninit(vcpu);
 	kfree(vcpu);
@@ -263,12 +267,20 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_free_vcpus(kvm);
 	free_page((unsigned long)(kvm->arch.sca));
 	debug_unregister(kvm->arch.dbf);
-	gmap_free(kvm->arch.gmap);
+	if (!kvm_is_ucontrol(kvm))
+		gmap_free(kvm->arch.gmap);
 }
 
 /* Section: vcpu related */
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
+	if (kvm_is_ucontrol(vcpu->kvm)) {
+		vcpu->arch.gmap = gmap_alloc(current->mm);
+		if (!vcpu->arch.gmap)
+			return -ENOMEM;
+		return 0;
+	}
+
 	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 	return 0;
 }
@@ -687,6 +699,42 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	case KVM_S390_INITIAL_RESET:
 		r = kvm_arch_vcpu_ioctl_initial_reset(vcpu);
 		break;
+#ifdef CONFIG_KVM_S390_UCONTROL
+	case KVM_S390_UCAS_MAP: {
+		struct kvm_s390_ucas_mapping ucasmap;
+
+		if (copy_from_user(&ucasmap, argp, sizeof(ucasmap))) {
+			r = -EFAULT;
+			break;
+		}
+
+		if (!kvm_is_ucontrol(vcpu->kvm)) {
+			r = -EINVAL;
+			break;
+		}
+
+		r = gmap_map_segment(vcpu->arch.gmap, ucasmap.user_addr,
+				     ucasmap.vcpu_addr, ucasmap.length);
+		break;
+	}
+	case KVM_S390_UCAS_UNMAP: {
+		struct kvm_s390_ucas_mapping ucasmap;
+
+		if (copy_from_user(&ucasmap, argp, sizeof(ucasmap))) {
+			r = -EFAULT;
+			break;
+		}
+
+		if (!kvm_is_ucontrol(vcpu->kvm)) {
+			r = -EINVAL;
+			break;
+		}
+
+		r = gmap_unmap_segment(vcpu->arch.gmap, ucasmap.vcpu_addr,
+			ucasmap.length);
+		break;
+	}
+#endif
 	default:
 		r = -EINVAL;
 	}

commit e08b96371625aaa84cb03f51acc4c8e0be27403a
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:20 2012 +0100

    KVM: s390: add parameter for KVM_CREATE_VM
    
    This patch introduces a new config option for user controlled kernel
    virtual machines. It introduces a parameter to KVM_CREATE_VM that
    allows to set bits that alter the capabilities of the newly created
    virtual machine.
    The parameter is passed to kvm_arch_init_vm for all architectures.
    The only valid modifier bit for now is KVM_VM_S390_UCONTROL.
    This requires CAP_SYS_ADMIN privileges and creates a user controlled
    virtual machine on s390 architectures.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d1c445732451..f0937552175b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -171,11 +171,22 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
-int kvm_arch_init_vm(struct kvm *kvm)
+int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int rc;
 	char debug_name[16];
 
+	rc = -EINVAL;
+#ifdef CONFIG_KVM_S390_UCONTROL
+	if (type & ~KVM_VM_S390_UCONTROL)
+		goto out_err;
+	if ((type & KVM_VM_S390_UCONTROL) && (!capable(CAP_SYS_ADMIN)))
+		goto out_err;
+#else
+	if (type)
+		goto out_err;
+#endif
+
 	rc = s390_enable_sie();
 	if (rc)
 		goto out_err;
@@ -198,10 +209,13 @@ int kvm_arch_init_vm(struct kvm *kvm)
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 	VM_EVENT(kvm, 3, "%s", "vm created");
 
-	kvm->arch.gmap = gmap_alloc(current->mm);
-	if (!kvm->arch.gmap)
-		goto out_nogmap;
-
+	if (type & KVM_VM_S390_UCONTROL) {
+		kvm->arch.gmap = NULL;
+	} else {
+		kvm->arch.gmap = gmap_alloc(current->mm);
+		if (!kvm->arch.gmap)
+			goto out_nogmap;
+	}
 	return 0;
 out_nogmap:
 	debug_unregister(kvm->arch.dbf);

commit 52e16b185fdd0aeb0522c203e944873e38ac1575
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Nov 17 11:00:44 2011 +0100

    KVM: s390: announce SYNC_MMU
    
    KVM on s390 always had a sync mmu. Any mapping change in userspace
    mapping was always reflected immediately in the guest mapping.
    - In older code the guest mapping was just an offset
    - In newer code the last level page table is shared
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0891926ea953..d1c445732451 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -128,6 +128,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	switch (ext) {
 	case KVM_CAP_S390_PSW:
 	case KVM_CAP_S390_GMAP:
+	case KVM_CAP_SYNC_MMU:
 		r = 1;
 		break;
 	default:

commit bd59d3a4444e5f941c863db1a7715edf7289e29a
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Thu Nov 17 11:00:42 2011 +0100

    KVM: s390: handle SIGP sense running intercepts
    
    SIGP sense running may cause an intercept on higher level
    virtualization, so handle it by checking the CPUSTAT_RUNNING flag.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 630ab65b488b..0891926ea953 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -65,6 +65,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
 	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
 	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
+	{ "instruction_sigp_sense_running", VCPU_STAT(instruction_sigp_sense_running) },
 	{ "instruction_sigp_external_call", VCPU_STAT(instruction_sigp_external_call) },
 	{ "instruction_sigp_emergency", VCPU_STAT(instruction_sigp_emergency) },
 	{ "instruction_sigp_stop", VCPU_STAT(instruction_sigp_stop) },

commit 9e6dabeffd1d0ec2aa19aa076c4886067238d442
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Thu Nov 17 11:00:41 2011 +0100

    KVM: s390: Fix RUNNING flag misinterpretation
    
    CPUSTAT_RUNNING was implemented signifying that a vcpu is not stopped.
    This is not, however, what the architecture says: RUNNING should be
    set when the host is acting on the behalf of the guest operating
    system.
    
    CPUSTAT_RUNNING has been changed to be set in kvm_arch_vcpu_load()
    and to be unset in kvm_arch_vcpu_put().
    
    For signifying stopped state of a vcpu, a host-controlled bit has
    been used and is set/unset basically on the reverse as the old
    CPUSTAT_RUNNING bit (including pushing it down into stop handling
    proper in handle_stop()).
    
    Cc: stable@kernel.org
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0bd3bea1e4cd..630ab65b488b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -270,10 +270,12 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	restore_fp_regs(&vcpu->arch.guest_fpregs);
 	restore_access_regs(vcpu->arch.guest_acrs);
 	gmap_enable(vcpu->arch.gmap);
+	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
+	atomic_clear_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 	gmap_disable(vcpu->arch.gmap);
 	save_fp_regs(&vcpu->arch.guest_fpregs);
 	save_access_regs(vcpu->arch.guest_acrs);
@@ -301,7 +303,9 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
-	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH | CPUSTAT_SM);
+	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |
+						    CPUSTAT_SM |
+						    CPUSTAT_STOPPED);
 	vcpu->arch.sie_block->ecb   = 6;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) facilities;
@@ -428,7 +432,7 @@ static int kvm_arch_vcpu_ioctl_set_initial_psw(struct kvm_vcpu *vcpu, psw_t psw)
 {
 	int rc = 0;
 
-	if (atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_RUNNING)
+	if (!(atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_STOPPED))
 		rc = -EBUSY;
 	else {
 		vcpu->run->psw_mask = psw.mask;
@@ -501,7 +505,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
-	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+	atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 
 	BUG_ON(vcpu->kvm->arch.float_int.local_int[vcpu->vcpu_id] == NULL);
 

commit 32087d4eeca14b82660dab288b1d659963b954bd
Merge: b1c907f3b267 07ea815b22b9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 31 16:14:20 2011 -0700

    Merge branch 'for-linus' of git://git390.marist.edu/pub/scm/linux-2.6
    
    * 'for-linus' of git://git390.marist.edu/pub/scm/linux-2.6: (54 commits)
      [S390] Remove error checking from copy_oldmem_page()
      [S390] qdio: prevent dsci access without adapter interrupts
      [S390] irqstats: split IPI interrupt accounting
      [S390] add missing __tlb_flush_global() for !CONFIG_SMP
      [S390] sparse: fix sparse symbol shadow warning
      [S390] sparse: fix sparse NULL pointer warnings
      [S390] sparse: fix sparse warnings with __user pointers
      [S390] sparse: fix sparse warnings in math-emu
      [S390] sparse: fix sparse warnings about missing prototypes
      [S390] sparse: fix sparse ANSI-C warnings
      [S390] sparse: fix sparse static warnings
      [S390] sparse: fix access past end of array warnings
      [S390] dasd: prevent path verification before resume
      [S390] qdio: remove multicast polling
      [S390] qdio: reset outbound SBAL error states
      [S390] qdio: EQBS retry after CCQ 96
      [S390] qdio: add timestamp for last queue scan time
      [S390] Introduce get_clock_fast()
      [S390] kvm: Handle diagnose 0x10 (release pages)
      [S390] take mmap_sem when walking guest page table
      ...

commit 388186bc920d9200202e4d25de66fa95b1b8fc68
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Sun Oct 30 15:17:03 2011 +0100

    [S390] kvm: Handle diagnose 0x10 (release pages)
    
    Linux on System z uses a ballooner based on diagnose 0x10. (aka as
    collaborative memory management). This patch implements diagnose
    0x10 on the guest address space.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dc2b580e27bc..189d6bdcac08 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -69,6 +69,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_sigp_set_arch", VCPU_STAT(instruction_sigp_arch) },
 	{ "instruction_sigp_set_prefix", VCPU_STAT(instruction_sigp_prefix) },
 	{ "instruction_sigp_restart", VCPU_STAT(instruction_sigp_restart) },
+	{ "diagnose_10", VCPU_STAT(diagnose_10) },
 	{ "diagnose_44", VCPU_STAT(diagnose_44) },
 	{ NULL }
 };

commit 7697e71f72b45a1bd0abe70918c383100fcc8514
Author: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
Date:   Tue Oct 18 12:27:15 2011 +0200

    KVM: s390: implement sigp external call
    
    Implement sigp external call, which might be required for guests that
    issue an external call instead of an emergency signal for IPI.
    
    This fixes an issue with "KVM: unknown SIGP: 0x02" when booting
    such an SMP guest.
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 29635678b5ec..9610ba41b974 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -46,6 +46,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
 	{ "deliver_emergency_signal", VCPU_STAT(deliver_emergency_signal) },
+	{ "deliver_external_call", VCPU_STAT(deliver_external_call) },
 	{ "deliver_service_signal", VCPU_STAT(deliver_service_signal) },
 	{ "deliver_virtio_interrupt", VCPU_STAT(deliver_virtio_interrupt) },
 	{ "deliver_stop_signal", VCPU_STAT(deliver_stop_signal) },
@@ -64,6 +65,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
 	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
 	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
+	{ "instruction_sigp_external_call", VCPU_STAT(instruction_sigp_external_call) },
 	{ "instruction_sigp_emergency", VCPU_STAT(instruction_sigp_emergency) },
 	{ "instruction_sigp_stop", VCPU_STAT(instruction_sigp_stop) },
 	{ "instruction_sigp_set_arch", VCPU_STAT(instruction_sigp_arch) },

commit 7eef87dc99e419b1cc051e4417c37e4744d7b661
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue Oct 18 12:27:14 2011 +0200

    KVM: s390: fix register setting
    
    KVM common code does vcpu_load prior to calling our arch ioctls and
    vcpu_put after we're done here. Via the kvm_arch_vcpu_load/put
    callbacks we do load the fpu and access register state into the
    processor, which saves us moving the state on every SIE exit the
    kernel handles. However this breaks register setting from userspace,
    because of the following sequence:
    1a. vcpu load stores userspace register content
    1b. vcpu load loads guest register content
    2.  kvm_arch_vcpu_ioctl_set_fpu/sregs updates saved guest register content
    3a. vcpu put stores the guest registers and overwrites the new content
    3b. vcpu put loads the userspace register set again
    
    This patch loads the new guest register state into the cpu, so that the correct
    (new) set of guest registers will be stored in step 3a.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 397f0cbc8b76..29635678b5ec 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -394,6 +394,7 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 {
 	memcpy(&vcpu->arch.guest_acrs, &sregs->acrs, sizeof(sregs->acrs));
 	memcpy(&vcpu->arch.sie_block->gcr, &sregs->crs, sizeof(sregs->crs));
+	restore_access_regs(vcpu->arch.guest_acrs);
 	return 0;
 }
 
@@ -409,6 +410,7 @@ int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
 	memcpy(&vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
 	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
+	restore_fp_regs(&vcpu->arch.guest_fpregs);
 	return 0;
 }
 

commit b290411a1321dd937dce4aaa812e5d8fae8a14a5
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue Oct 18 12:27:13 2011 +0200

    KVM: s390: fix return value of kvm_arch_init_vm
    
    This patch fixes the return value of kvm_arch_init_vm in case a memory
    allocation goes wrong.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0cba935d1282..397f0cbc8b76 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -175,6 +175,8 @@ int kvm_arch_init_vm(struct kvm *kvm)
 	if (rc)
 		goto out_err;
 
+	rc = -ENOMEM;
+
 	kvm->arch.sca = (struct sca_block *) get_zeroed_page(GFP_KERNEL);
 	if (!kvm->arch.sca)
 		goto out_err;

commit 4d47555a80495657161a7e71ec3014ff2021e450
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue Oct 18 12:27:12 2011 +0200

    KVM: s390: check cpu_id prior to using it
    
    We use the cpu id provided by userspace as array index here. Thus we
    clearly need to check it first. Ooops.
    
    CC: <stable@vger.kernel.org>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dc2b580e27bc..0cba935d1282 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -312,11 +312,17 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 				      unsigned int id)
 {
-	struct kvm_vcpu *vcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);
-	int rc = -ENOMEM;
+	struct kvm_vcpu *vcpu;
+	int rc = -EINVAL;
+
+	if (id >= KVM_MAX_VCPUS)
+		goto out;
+
+	rc = -ENOMEM;
 
+	vcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);
 	if (!vcpu)
-		goto out_nomem;
+		goto out;
 
 	vcpu->arch.sie_block = (struct kvm_s390_sie_block *)
 					get_zeroed_page(GFP_KERNEL);
@@ -352,7 +358,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	free_page((unsigned long)(vcpu->arch.sie_block));
 out_free_cpu:
 	kfree(vcpu);
-out_nomem:
+out:
 	return ERR_PTR(rc);
 }
 

commit b6cf8788a3382c2000743a0e393bcc8aeb0601cb
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Sep 20 17:07:29 2011 +0200

    [S390] kvm: extension capability for new address space layout
    
    598841ca9919d008b520114d8a4378c4ce4e40a1 ([S390] use gmap address
    spaces for kvm guest images) changed kvm on s390 to use a separate
    address space for kvm guests. We can now put KVM guests anywhere
    in the user address mode with a size up to 8PB - as long as the
    memory is 1MB-aligned. This change was done without KVM extension
    capability bit.
    The change was added after 3.0, but we still have a chance to add
    a feature bit before 3.1 (keeping the releases in a sane state).
    We use number 71 to avoid collisions with other pending kvm patches
    as requested by Alexander Graf.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Avi Kivity <avi@redhat.com>
    Cc: Alexander Graf <agraf@suse.de>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b4eced131e5c..dc2b580e27bc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -123,6 +123,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 
 	switch (ext) {
 	case KVM_CAP_S390_PSW:
+	case KVM_CAP_S390_GMAP:
 		r = 1;
 		break;
 	default:

commit 480e5926ce3bb61ec229be2dab08bdce8abb8d2e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Sep 20 17:07:28 2011 +0200

    [S390] kvm: fix address mode switching
    
    598841ca9919d008b520114d8a4378c4ce4e40a1 ([S390] use gmap address
    spaces for kvm guest images) changed kvm to use a separate address
    space for kvm guests. This address space was switched in __vcpu_run
    In some cases (preemption, page fault) there is the possibility that
    this address space switch is lost.
    The typical symptom was a huge amount of validity intercepts or
    random guest addressing exceptions.
    Fix this by doing the switch in sie_loop and sie_exit and saving the
    address space in the gmap structure itself. Also use the preempt
    notifier.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f17296e4fc89..b4eced131e5c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -263,10 +263,12 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	vcpu->arch.guest_fpregs.fpc &= FPC_VALID_MASK;
 	restore_fp_regs(&vcpu->arch.guest_fpregs);
 	restore_access_regs(vcpu->arch.guest_acrs);
+	gmap_enable(vcpu->arch.gmap);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
+	gmap_disable(vcpu->arch.gmap);
 	save_fp_regs(&vcpu->arch.guest_fpregs);
 	save_access_regs(vcpu->arch.guest_acrs);
 	restore_fp_regs(&vcpu->arch.host_fpregs);
@@ -461,7 +463,6 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	local_irq_disable();
 	kvm_guest_enter();
 	local_irq_enable();
-	gmap_enable(vcpu->arch.gmap);
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
 	if (sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs)) {
@@ -470,7 +471,6 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	}
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
-	gmap_disable(vcpu->arch.gmap);
 	local_irq_disable();
 	kvm_guest_exit();
 	local_irq_enable();

commit f7850c92884b40915001e332a0a33ed4f10158e8
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Sun Jul 24 10:48:23 2011 +0200

    [S390] remove kvm mmu reload on s390
    
    This patch removes the mmu reload logic for kvm on s390. Via Martin's
    new gmap interface, we can safely add or remove memory slots while
    guest CPUs are in-flight. Thus, the mmu reload logic is not needed
    anymore.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5a99f342fd0b..f17296e4fc89 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -294,7 +294,6 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH | CPUSTAT_SM);
-	set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests);
 	vcpu->arch.sie_block->ecb   = 6;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) facilities;
@@ -485,10 +484,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	sigset_t sigsaved;
 
 rerun_vcpu:
-	if (vcpu->requests)
-		if (test_and_clear_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
-			kvm_s390_vcpu_set_mem(vcpu);
-
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
@@ -701,21 +696,13 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot old,
 				int user_alloc)
 {
-	int i, rc;
-	struct kvm_vcpu *vcpu;
+	int rc;
 
 
 	rc = gmap_map_segment(kvm->arch.gmap, mem->userspace_addr,
 		mem->guest_phys_addr, mem->memory_size);
 	if (rc)
-		return;
-
-	/* request update of sie control block for all available vcpus */
-	kvm_for_each_vcpu(i, vcpu, kvm) {
-		if (test_and_set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
-			continue;
-		kvm_s390_inject_sigp_stop(vcpu, ACTION_RELOADVCPU_ON_STOP);
-	}
+		printk(KERN_WARNING "kvm-s390: failed to commit memory region\n");
 	return;
 }
 

commit 092670cd90eb88c33661de21f1b4ee08d2597171
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Sun Jul 24 10:48:22 2011 +0200

    [S390] Use gmap translation for accessing guest memory
    
    This patch removes kvm-s390 internal assumption of a linear mapping
    of guest address space to user space. Previously, guest memory was
    translated to user addresses using a fixed offset (gmsor). The new
    code uses gmap_fault to resolve guest addresses.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3ebb4ba83d9d..5a99f342fd0b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -549,7 +549,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	return rc;
 }
 
-static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, const void *from,
+static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, void *from,
 		       unsigned long n, int prefix)
 {
 	if (prefix)
@@ -566,7 +566,7 @@ static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, const void *from,
  */
 int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 {
-	const unsigned char archmode = 1;
+	unsigned char archmode = 1;
 	int prefix;
 
 	if (addr == KVM_S390_STORE_STATUS_NOADDR) {

commit 598841ca9919d008b520114d8a4378c4ce4e40a1
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Sun Jul 24 10:48:21 2011 +0200

    [S390] use gmap address spaces for kvm guest images
    
    This patch switches kvm from using (Qemu's) user address space to
    Martin's gmap address space. This way QEMU does not have to use a
    linker script in order to fit large guests at low addresses in its
    address space.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 123ebea72282..3ebb4ba83d9d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -190,7 +190,13 @@ int kvm_arch_init_vm(struct kvm *kvm)
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 	VM_EVENT(kvm, 3, "%s", "vm created");
 
+	kvm->arch.gmap = gmap_alloc(current->mm);
+	if (!kvm->arch.gmap)
+		goto out_nogmap;
+
 	return 0;
+out_nogmap:
+	debug_unregister(kvm->arch.dbf);
 out_nodbf:
 	free_page((unsigned long)(kvm->arch.sca));
 out_err:
@@ -235,11 +241,13 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_free_vcpus(kvm);
 	free_page((unsigned long)(kvm->arch.sca));
 	debug_unregister(kvm->arch.dbf);
+	gmap_free(kvm->arch.gmap);
 }
 
 /* Section: vcpu related */
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
+	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 	return 0;
 }
 
@@ -285,7 +293,7 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
-	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH);
+	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH | CPUSTAT_SM);
 	set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests);
 	vcpu->arch.sie_block->ecb   = 6;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
@@ -454,6 +462,7 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	local_irq_disable();
 	kvm_guest_enter();
 	local_irq_enable();
+	gmap_enable(vcpu->arch.gmap);
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
 	if (sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs)) {
@@ -462,6 +471,7 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	}
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
+	gmap_disable(vcpu->arch.gmap);
 	local_irq_disable();
 	kvm_guest_exit();
 	local_irq_enable();
@@ -479,13 +489,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		if (test_and_clear_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
 			kvm_s390_vcpu_set_mem(vcpu);
 
-	/* verify, that memory has been registered */
-	if (!vcpu->arch.sie_block->gmslm) {
-		vcpu_put(vcpu);
-		VCPU_EVENT(vcpu, 3, "%s", "no memory registered to run vcpu");
-		return -EINVAL;
-	}
-
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
@@ -681,10 +684,10 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	if (mem->guest_phys_addr)
 		return -EINVAL;
 
-	if (mem->userspace_addr & (PAGE_SIZE - 1))
+	if (mem->userspace_addr & 0xffffful)
 		return -EINVAL;
 
-	if (mem->memory_size & (PAGE_SIZE - 1))
+	if (mem->memory_size & 0xffffful)
 		return -EINVAL;
 
 	if (!user_alloc)
@@ -698,15 +701,22 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot old,
 				int user_alloc)
 {
-	int i;
+	int i, rc;
 	struct kvm_vcpu *vcpu;
 
+
+	rc = gmap_map_segment(kvm->arch.gmap, mem->userspace_addr,
+		mem->guest_phys_addr, mem->memory_size);
+	if (rc)
+		return;
+
 	/* request update of sie control block for all available vcpus */
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		if (test_and_set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
 			continue;
 		kvm_s390_inject_sigp_stop(vcpu, ACTION_RELOADVCPU_ON_STOP);
 	}
+	return;
 }
 
 void kvm_arch_flush_shadow(struct kvm *kvm)

commit bb25b9ba3e33e941dc48048d0a784e6a05e5648a
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Sun Jul 24 10:48:17 2011 +0200

    [S390] kvm: handle tprot intercepts
    
    When running a kvm guest we can get intercepts for tprot, if the host
    page table is read-only or not populated. This patch implements the
    most common case (linux memory detection).
    This also allows host copy on write for guest memory on newer systems.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 67345ae7ce8d..123ebea72282 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -62,6 +62,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_chsc", VCPU_STAT(instruction_chsc) },
 	{ "instruction_stsi", VCPU_STAT(instruction_stsi) },
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
+	{ "instruction_tprot", VCPU_STAT(instruction_tprot) },
 	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
 	{ "instruction_sigp_emergency", VCPU_STAT(instruction_sigp_emergency) },
 	{ "instruction_sigp_stop", VCPU_STAT(instruction_sigp_stop) },

commit 9950f8be3f379e36be73be958ec5cf6c15eac0b2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Jun 6 14:14:39 2011 +0200

    [S390] kvm-s390: fix stfle facilities numbers >=64
    
    Currently KVM masks out the known good facilities only for the first
    double word, but passed the 2nd double word without filtering. This
    breaks some code on newer systems:
    
    [    0.593966] ------------[ cut here ]------------
    [    0.594086] WARNING: at arch/s390/oprofile/hwsampler.c:696
    [    0.594213] Modules linked in:
    [    0.594321] Modules linked in:
    [    0.594439] CPU: 0 Not tainted 3.0.0-rc1 #46
    [    0.594564] Process swapper (pid: 1, task: 00000001effa8038, ksp: 00000001effafab8)
    [    0.594735] Krnl PSW : 0704100180000000 00000000004ab89a (hwsampler_setup+0x75a/0x7b8)
    [    0.594910]            R:0 T:1 IO:1 EX:1 Key:0 M:1 W:0 P:0 AS:0 CC:1 PM:0 EA:3
    [    0.595120] Krnl GPRS: ffffffff00000000 00000000ffffffea ffffffffffffffea 00000000004a98f8
    [    0.595351]            00000000004aa002 0000000000000001 000000000080e720 000000000088b9f8
    [    0.595522]            000000000080d3e8 0000000000000000 0000000000000000 000000000080e464
    [    0.595725]            0000000000000000 00000000005db198 00000000004ab3a2 00000001effafd98
    [    0.595901] Krnl Code: 00000000004ab88c: c0e5000673ca        brasl   %r14,57a020
    [    0.596071]            00000000004ab892: a7f4fc77            brc     15,4ab180
    [    0.596276]            00000000004ab896: a7f40001            brc     15,4ab898
    [    0.596454]           >00000000004ab89a: a7c8ffa1            lhi     %r12,-95
    [    0.596657]            00000000004ab89e: a7f4fc71            brc     15,4ab180
    [    0.596854]            00000000004ab8a2: a7f40001            brc     15,4ab8a4
    [    0.597029]            00000000004ab8a6: a7f4ff22            brc     15,4ab6ea
    [    0.597230]            00000000004ab8aa: c0200011009a        larl    %r2,6cb9de
    [    0.597441] Call Trace:
    [    0.597511] ([<00000000004ab3a2>] hwsampler_setup+0x262/0x7b8)
    [    0.597676]  [<0000000000875812>] oprofile_arch_init+0x32/0xd0
    [    0.597834]  [<0000000000875788>] oprofile_init+0x28/0x74
    [    0.597991]  [<00000000001001be>] do_one_initcall+0x3a/0x170
    [    0.598151]  [<000000000084fa22>] kernel_init+0x142/0x1ec
    [    0.598314]  [<000000000057db16>] kernel_thread_starter+0x6/0xc
    [    0.598468]  [<000000000057db10>] kernel_thread_starter+0x0/0xc
    [    0.598606] Last Breaking-Event-Address:
    [    0.598707]  [<00000000004ab896>] hwsampler_setup+0x756/0x7b8
    [    0.598863] ---[ end trace ce3179037f4e3e5b ]---
    
    So lets also mask the 2nd double word. Facilites 66,76,76,77 should be fine.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 30ca85cce314..67345ae7ce8d 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -731,6 +731,7 @@ static int __init kvm_s390_init(void)
 	}
 	memcpy(facilities, S390_lowcore.stfle_fac_list, 16);
 	facilities[0] &= 0xff00fff3f47c0000ULL;
+	facilities[1] &= 0x201c000000000000ULL;
 	return 0;
 }
 

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index bade533ba288..30ca85cce314 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -721,7 +721,7 @@ static int __init kvm_s390_init(void)
 
 	/*
 	 * guests can ask for up to 255+1 double words, we need a full page
-	 * to hold the maximum amount of facilites. On the other hand, we
+	 * to hold the maximum amount of facilities. On the other hand, we
 	 * only set facilities that are known to work in KVM.
 	 */
 	facilities = (unsigned long long *) get_zeroed_page(GFP_KERNEL|GFP_DMA);

commit d89f5eff70a31237ffa1e21c51d23ca532110aea
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Nov 9 17:02:49 2010 +0100

    KVM: Clean up vm creation and release
    
    IA64 support forces us to abstract the allocation of the kvm structure.
    But instead of mixing this up with arch-specific initialization and
    doing the same on destruction, split both steps. This allows to move
    generic destruction calls into generic code.
    
    It also fixes error clean-up on failures of kvm_create_vm for IA64.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 985d825494f1..bade533ba288 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -164,24 +164,18 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
-struct kvm *kvm_arch_create_vm(void)
+int kvm_arch_init_vm(struct kvm *kvm)
 {
-	struct kvm *kvm;
 	int rc;
 	char debug_name[16];
 
 	rc = s390_enable_sie();
 	if (rc)
-		goto out_nokvm;
-
-	rc = -ENOMEM;
-	kvm = kzalloc(sizeof(struct kvm), GFP_KERNEL);
-	if (!kvm)
-		goto out_nokvm;
+		goto out_err;
 
 	kvm->arch.sca = (struct sca_block *) get_zeroed_page(GFP_KERNEL);
 	if (!kvm->arch.sca)
-		goto out_nosca;
+		goto out_err;
 
 	sprintf(debug_name, "kvm-%u", current->pid);
 
@@ -195,13 +189,11 @@ struct kvm *kvm_arch_create_vm(void)
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 	VM_EVENT(kvm, 3, "%s", "vm created");
 
-	return kvm;
+	return 0;
 out_nodbf:
 	free_page((unsigned long)(kvm->arch.sca));
-out_nosca:
-	kfree(kvm);
-out_nokvm:
-	return ERR_PTR(rc);
+out_err:
+	return rc;
 }
 
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
@@ -240,11 +232,8 @@ void kvm_arch_sync_events(struct kvm *kvm)
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvm_free_vcpus(kvm);
-	kvm_free_physmem(kvm);
 	free_page((unsigned long)(kvm->arch.sca));
 	debug_unregister(kvm->arch.dbf);
-	cleanup_srcu_struct(&kvm->srcu);
-	kfree(kvm);
 }
 
 /* Section: vcpu related */

commit 14375bc4eb8dd0fb0e765390650564c35bb31068
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Oct 25 16:10:51 2010 +0200

    [S390] cleanup facility list handling
    
    Store the facility list once at system startup with stfl/stfle and
    reuse the result for all facility tests.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 76482b65ba3e..985d825494f1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -740,7 +740,7 @@ static int __init kvm_s390_init(void)
 		kvm_exit();
 		return -ENOMEM;
 	}
-	stfle(facilities, 1);
+	memcpy(facilities, S390_lowcore.stfle_fac_list, 16);
 	facilities[0] &= 0xff00fff3f47c0000ULL;
 	return 0;
 }

commit 6d00d00bf23b86b352e2d075cfe751acd1908278
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Oct 25 16:10:48 2010 +0200

    [S390] kvm: Enable z196 instruction facilities
    
    Enable PFPO, floating point extension, distinct-operands,
    fast-BCR-serialization, high-word, interlocked-access, load/store-
    on-condition, and population-count facilities for guests.
    (bits 37, 44 and 45).
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4fe68650535c..76482b65ba3e 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -741,7 +741,7 @@ static int __init kvm_s390_init(void)
 		return -ENOMEM;
 	}
 	stfle(facilities, 1);
-	facilities[0] &= 0xff00fff3f0700000ULL;
+	facilities[0] &= 0xff00fff3f47c0000ULL;
 	return 0;
 }
 

commit a1f4d39500ad8ed61825eff061debff42386ab5b
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Jun 21 11:44:20 2010 +0300

    KVM: Remove memory alias support
    
    As advertised in feature-removal-schedule.txt.  Equivalent support is provided
    by overlapping memory regions.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 08a3b35d30be..4fe68650535c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -723,11 +723,6 @@ void kvm_arch_flush_shadow(struct kvm *kvm)
 {
 }
 
-gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
-{
-	return gfn;
-}
-
 static int __init kvm_s390_init(void)
 {
 	int ret;

commit fc34531db3cf8c422f2ff7cf4ef507a3ca672cd2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Jun 17 23:16:20 2010 +0200

    KVM: s390: Don't exit SIE on SIGP sense running
    
    Newer (guest) kernels use sigp sense running in their spinlock
    implementation to check if the other cpu is running before yielding
    the processor. This revealed some wrong guest settings, causing
    unnecessary exits for every sigp sense running.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0079b8c2a9a6..08a3b35d30be 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -207,6 +207,7 @@ struct kvm *kvm_arch_create_vm(void)
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
+	clear_bit(63 - vcpu->vcpu_id, (unsigned long *) &vcpu->kvm->arch.sca->mcn);
 	if (vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda ==
 		(__u64) vcpu->arch.sie_block)
 		vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda = 0;
@@ -296,7 +297,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH);
 	set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests);
-	vcpu->arch.sie_block->ecb   = 2;
+	vcpu->arch.sie_block->ecb   = 6;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) facilities;
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
@@ -329,6 +330,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		kvm->arch.sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
 	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)kvm->arch.sca) >> 32);
 	vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
+	set_bit(63 - id, (unsigned long *) &kvm->arch.sca->mcn);
 
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	INIT_LIST_HEAD(&vcpu->arch.local_int.list);

commit 971eb77f87bfd352a56a24651922d0383db76219
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Sat Jun 12 08:54:13 2010 +0200

    KVM: s390: Fix build failure due to centralized vcpu locking patches
    
    This patch fixes
    ERROR: "__kvm_s390_vcpu_store_status" [arch/s390/kvm/kvm.ko] undefined!
    
    triggered by
    commit 3268c56840dcee78c3e928336550f4e1861504c4 (kvm.git)
    Author: Avi Kivity <avi@redhat.com>
    Date:   Thu May 13 12:21:46 2010 +0300
        KVM: s390: Centrally lock arch specific vcpu ioctls
    
    Reported-by: Sachin Sant <sachinp@in.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0cb0da7822d5..0079b8c2a9a6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -569,7 +569,7 @@ static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, const void *from,
  * KVM_S390_STORE_STATUS_NOADDR: -> 0x1200 on 64 bit
  * KVM_S390_STORE_STATUS_PREFIXED: -> prefix
  */
-static int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
+int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 {
 	const unsigned char archmode = 1;
 	int prefix;

commit 93736624635235cc5372ffca6d62816d02170724
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 13 12:35:17 2010 +0300

    KVM: Consolidate arch specific vcpu ioctl locking
    
    Now that all arch specific ioctls have centralized locking, it is easy to
    move it to the central dispatcher.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fd169f658bf8..0cb0da7822d5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -638,16 +638,16 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 	long r;
 
-	if (ioctl == KVM_S390_INTERRUPT) {
+	switch (ioctl) {
+	case KVM_S390_INTERRUPT: {
 		struct kvm_s390_interrupt s390int;
 
+		r = -EFAULT;
 		if (copy_from_user(&s390int, argp, sizeof(s390int)))
-			return -EFAULT;
-		return kvm_s390_inject_vcpu(vcpu, &s390int);
+			break;
+		r = kvm_s390_inject_vcpu(vcpu, &s390int);
+		break;
 	}
-
-	vcpu_load(vcpu);
-	switch (ioctl) {
 	case KVM_S390_STORE_STATUS:
 		r = kvm_s390_vcpu_store_status(vcpu, arg);
 		break;
@@ -666,7 +666,6 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	default:
 		r = -EINVAL;
 	}
-	vcpu_put(vcpu);
 	return r;
 }
 

commit bc923cc93b7719576f20687e4cb07751601fbbb3
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 13 12:21:46 2010 +0300

    KVM: s390: Centrally lock arch specific vcpu ioctls
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0d0e0add0201..fd169f658bf8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -363,9 +363,7 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 
 static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 {
-	vcpu_load(vcpu);
 	kvm_s390_vcpu_initial_reset(vcpu);
-	vcpu_put(vcpu);
 	return 0;
 }
 
@@ -415,14 +413,12 @@ static int kvm_arch_vcpu_ioctl_set_initial_psw(struct kvm_vcpu *vcpu, psw_t psw)
 {
 	int rc = 0;
 
-	vcpu_load(vcpu);
 	if (atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_RUNNING)
 		rc = -EBUSY;
 	else {
 		vcpu->run->psw_mask = psw.mask;
 		vcpu->run->psw_addr = psw.addr;
 	}
-	vcpu_put(vcpu);
 	return rc;
 }
 
@@ -573,7 +569,7 @@ static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, const void *from,
  * KVM_S390_STORE_STATUS_NOADDR: -> 0x1200 on 64 bit
  * KVM_S390_STORE_STATUS_PREFIXED: -> prefix
  */
-int __kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
+static int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 {
 	const unsigned char archmode = 1;
 	int prefix;
@@ -635,45 +631,43 @@ int __kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	return 0;
 }
 
-static int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
-{
-	int rc;
-
-	vcpu_load(vcpu);
-	rc = __kvm_s390_vcpu_store_status(vcpu, addr);
-	vcpu_put(vcpu);
-	return rc;
-}
-
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
 	struct kvm_vcpu *vcpu = filp->private_data;
 	void __user *argp = (void __user *)arg;
+	long r;
 
-	switch (ioctl) {
-	case KVM_S390_INTERRUPT: {
+	if (ioctl == KVM_S390_INTERRUPT) {
 		struct kvm_s390_interrupt s390int;
 
 		if (copy_from_user(&s390int, argp, sizeof(s390int)))
 			return -EFAULT;
 		return kvm_s390_inject_vcpu(vcpu, &s390int);
 	}
+
+	vcpu_load(vcpu);
+	switch (ioctl) {
 	case KVM_S390_STORE_STATUS:
-		return kvm_s390_vcpu_store_status(vcpu, arg);
+		r = kvm_s390_vcpu_store_status(vcpu, arg);
+		break;
 	case KVM_S390_SET_INITIAL_PSW: {
 		psw_t psw;
 
+		r = -EFAULT;
 		if (copy_from_user(&psw, argp, sizeof(psw)))
-			return -EFAULT;
-		return kvm_arch_vcpu_ioctl_set_initial_psw(vcpu, psw);
+			break;
+		r = kvm_arch_vcpu_ioctl_set_initial_psw(vcpu, psw);
+		break;
 	}
 	case KVM_S390_INITIAL_RESET:
-		return kvm_arch_vcpu_ioctl_initial_reset(vcpu);
+		r = kvm_arch_vcpu_ioctl_initial_reset(vcpu);
+		break;
 	default:
-		;
+		r = -EINVAL;
 	}
-	return -EINVAL;
+	vcpu_put(vcpu);
+	return r;
 }
 
 /* Section: memory related */

commit 2122ff5eab8faec853e43f6de886e8dc8f31e317
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 13 11:25:04 2010 +0300

    KVM: move vcpu locking to dispatcher for generic vcpu ioctls
    
    All vcpu ioctls need to be locked, so instead of locking each one specifically
    we lock at the generic dispatcher.
    
    This patch only updates generic ioctls and leaves arch specific ioctls alone.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ae3705816878..0d0e0add0201 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -371,55 +371,43 @@ static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	vcpu_load(vcpu);
 	memcpy(&vcpu->arch.guest_gprs, &regs->gprs, sizeof(regs->gprs));
-	vcpu_put(vcpu);
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	vcpu_load(vcpu);
 	memcpy(&regs->gprs, &vcpu->arch.guest_gprs, sizeof(regs->gprs));
-	vcpu_put(vcpu);
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs)
 {
-	vcpu_load(vcpu);
 	memcpy(&vcpu->arch.guest_acrs, &sregs->acrs, sizeof(sregs->acrs));
 	memcpy(&vcpu->arch.sie_block->gcr, &sregs->crs, sizeof(sregs->crs));
-	vcpu_put(vcpu);
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs)
 {
-	vcpu_load(vcpu);
 	memcpy(&sregs->acrs, &vcpu->arch.guest_acrs, sizeof(sregs->acrs));
 	memcpy(&sregs->crs, &vcpu->arch.sie_block->gcr, sizeof(sregs->crs));
-	vcpu_put(vcpu);
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
-	vcpu_load(vcpu);
 	memcpy(&vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
 	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
-	vcpu_put(vcpu);
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 {
-	vcpu_load(vcpu);
 	memcpy(&fpu->fprs, &vcpu->arch.guest_fpregs.fprs, sizeof(fpu->fprs));
 	fpu->fpc = vcpu->arch.guest_fpregs.fpc;
-	vcpu_put(vcpu);
 	return 0;
 }
 
@@ -498,8 +486,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	int rc;
 	sigset_t sigsaved;
 
-	vcpu_load(vcpu);
-
 rerun_vcpu:
 	if (vcpu->requests)
 		if (test_and_clear_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
@@ -568,8 +554,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
 
-	vcpu_put(vcpu);
-
 	vcpu->stat.exit_userspace++;
 	return rc;
 }

commit c2f0e8c803ceba530060ec9bb9c74a06c2c3d833
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Jun 8 18:58:09 2010 +0200

    [S390] appldata/extmem/kvm: add missing GFP_KERNEL flag
    
    Add missing GFP flag to memory allocations. The part in cio only
    changes a comment.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8093e6f47f49..ae3705816878 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -761,7 +761,7 @@ static int __init kvm_s390_init(void)
 	 * to hold the maximum amount of facilites. On the other hand, we
 	 * only set facilities that are known to work in KVM.
 	 */
-	facilities = (unsigned long long *) get_zeroed_page(GFP_DMA);
+	facilities = (unsigned long long *) get_zeroed_page(GFP_KERNEL|GFP_DMA);
 	if (!facilities) {
 		kvm_exit();
 		return -ENOMEM;

commit 0ee75bead83da4791e5cbf659806c54d8ee40f12
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 28 15:39:01 2010 +0300

    KVM: Let vcpu structure alignment be determined at runtime
    
    vmx and svm vcpus have different contents and therefore may have different
    alignmment requirements.  Let each specify its required alignment.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ee7c713686ce..8093e6f47f49 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -752,7 +752,7 @@ gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
 static int __init kvm_s390_init(void)
 {
 	int ret;
-	ret = kvm_init(NULL, sizeof(struct kvm_vcpu), THIS_MODULE);
+	ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
 	if (ret)
 		return ret;
 

commit 7b06bf2ffa15e119c7439ed0b024d44f66d7b605
Author: Wei Yongjun <yjwei@cn.fujitsu.com>
Date:   Tue Mar 9 14:37:53 2010 +0800

    KVM: s390: Fix possible memory leak of in kvm_arch_vcpu_create()
    
    This patch fixed possible memory leak in kvm_arch_vcpu_create()
    under s390, which would happen when kvm_arch_vcpu_create() fails.
    
    Signed-off-by: Wei Yongjun <yjwei@cn.fujitsu.com>
    Acked-by: Carsten Otte <cotte@de.ibm.com>
    Cc: stable@kernel.org
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 49292869a5cd..ee7c713686ce 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -341,11 +341,13 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
-		goto out_free_cpu;
+		goto out_free_sie_block;
 	VM_EVENT(kvm, 3, "create cpu %d at %p, sie block at %p", id, vcpu,
 		 vcpu->arch.sie_block);
 
 	return vcpu;
+out_free_sie_block:
+	free_page((unsigned long)(vcpu->arch.sie_block));
 out_free_cpu:
 	kfree(vcpu);
 out_nomem:

commit 647492047763c3ee8fe51ecf9a04f39040aa495b
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Jan 19 12:45:23 2010 -0200

    KVM: fix cleanup_srcu_struct on vm destruction
    
    cleanup_srcu_struct on VM destruction remains broken:
    
    BUG: unable to handle kernel paging request at ffffffffffffffff
    IP: [<ffffffff802533d2>] srcu_read_lock+0x16/0x21
    RIP: 0010:[<ffffffff802533d2>]  [<ffffffff802533d2>] srcu_read_lock+0x16/0x21
    Call Trace:
     [<ffffffffa05354c4>] kvm_arch_vcpu_uninit+0x1b/0x48 [kvm]
     [<ffffffffa05339c6>] kvm_vcpu_uninit+0x9/0x15 [kvm]
     [<ffffffffa0569f7d>] vmx_free_vcpu+0x7f/0x8f [kvm_intel]
     [<ffffffffa05357b5>] kvm_arch_destroy_vm+0x78/0x111 [kvm]
     [<ffffffffa053315b>] kvm_put_kvm+0xd4/0xfe [kvm]
    
    Move it to kvm_arch_destroy_vm.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Reported-by: Jan Kiszka <jan.kiszka@siemens.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c8002193d9d4..49292869a5cd 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -242,6 +242,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_free_physmem(kvm);
 	free_page((unsigned long)(kvm->arch.sca));
 	debug_unregister(kvm->arch.dbf);
+	cleanup_srcu_struct(&kvm->srcu);
 	kfree(kvm);
 }
 

commit f7784b8ec9b6a041fa828cfbe9012fe51933f5ac
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:18 2009 -0200

    KVM: split kvm_arch_set_memory_region into prepare and commit
    
    Required for SRCU convertion later.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3fa0a10e4668..c8002193d9d4 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -690,14 +690,12 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 }
 
 /* Section: memory related */
-int kvm_arch_set_memory_region(struct kvm *kvm,
-				struct kvm_userspace_memory_region *mem,
-				struct kvm_memory_slot old,
-				int user_alloc)
+int kvm_arch_prepare_memory_region(struct kvm *kvm,
+				   struct kvm_memory_slot *memslot,
+				   struct kvm_memory_slot old,
+				   struct kvm_userspace_memory_region *mem,
+				   int user_alloc)
 {
-	int i;
-	struct kvm_vcpu *vcpu;
-
 	/* A few sanity checks. We can have exactly one memory slot which has
 	   to start at guest virtual zero and which has to be located at a
 	   page boundary in userland and which has to end at a page boundary.
@@ -720,14 +718,23 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 	if (!user_alloc)
 		return -EINVAL;
 
+	return 0;
+}
+
+void kvm_arch_commit_memory_region(struct kvm *kvm,
+				struct kvm_userspace_memory_region *mem,
+				struct kvm_memory_slot old,
+				int user_alloc)
+{
+	int i;
+	struct kvm_vcpu *vcpu;
+
 	/* request update of sie control block for all available vcpus */
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		if (test_and_set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
 			continue;
 		kvm_s390_inject_sigp_stop(vcpu, ACTION_RELOADVCPU_ON_STOP);
 	}
-
-	return 0;
 }
 
 void kvm_arch_flush_shadow(struct kvm *kvm)

commit cbb870c8221147ae337612e04b2bb0211f31a74b
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Feb 26 22:37:43 2010 +0100

    [S390] Cleanup struct _lowcore usage and defines.
    
    Use asm offsets to make sure the offset defines to struct _lowcore and
    its layout don't get out of sync.
    Also add a BUILD_BUG_ON() which checks that the size of the structure
    is sane.
    And while being at it change those sites which use odd casts to access
    the current lowcore. These should use S390_lowcore instead.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 7f19340481b7..3fa0a10e4668 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -23,6 +23,7 @@
 #include <linux/module.h>
 #include <linux/slab.h>
 #include <linux/timer.h>
+#include <asm/asm-offsets.h>
 #include <asm/lowcore.h>
 #include <asm/pgtable.h>
 #include <asm/nmi.h>

commit b8e660b83d505148c83ddc6d5ba69c29d0eab1e3
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Feb 26 22:37:41 2010 +0100

    [S390] Replace ENOTSUPP usage with EOPNOTSUPP
    
    ENOTSUPP is not supposed to leak to userspace so lets just use
    EOPNOTSUPP everywhere.
    Doesn't fix a bug, but makes future reviews easier.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 70e799eefec9..7f19340481b7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -543,7 +543,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = -EINTR;
 	}
 
-	if (rc == -ENOTSUPP) {
+	if (rc == -EOPNOTSUPP) {
 		/* intercept cannot be handled in-kernel, prepare kvm-run */
 		kvm_run->exit_reason         = KVM_EXIT_S390_SIEIC;
 		kvm_run->s390_sieic.icptcode = vcpu->arch.sie_block->icptcode;

commit f64ca21714f114a78b6c19ceabc7745b29913efc
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Feb 26 22:37:32 2010 +0100

    [S390] zfcpdump: remove cross arch dump support
    
    Remove support to be able to dump 31 bit systems with a 64 bit dumper.
    This is mostly useless since no distro ships 31 bit kernels together
    with a 64 bit dumper.
    We also get rid of a bit of hacky code.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f8bcaefd7d34..70e799eefec9 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -603,45 +603,45 @@ int __kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
 	} else
 		prefix = 0;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, fp_regs),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, fp_regs),
 			vcpu->arch.guest_fpregs.fprs, 128, prefix))
 		return -EFAULT;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, gp_regs),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, gp_regs),
 			vcpu->arch.guest_gprs, 128, prefix))
 		return -EFAULT;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, psw),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, psw),
 			&vcpu->arch.sie_block->gpsw, 16, prefix))
 		return -EFAULT;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, pref_reg),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, pref_reg),
 			&vcpu->arch.sie_block->prefix, 4, prefix))
 		return -EFAULT;
 
 	if (__guestcopy(vcpu,
-			addr + offsetof(struct save_area_s390x, fp_ctrl_reg),
+			addr + offsetof(struct save_area, fp_ctrl_reg),
 			&vcpu->arch.guest_fpregs.fpc, 4, prefix))
 		return -EFAULT;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, tod_reg),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, tod_reg),
 			&vcpu->arch.sie_block->todpr, 4, prefix))
 		return -EFAULT;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, timer),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, timer),
 			&vcpu->arch.sie_block->cputm, 8, prefix))
 		return -EFAULT;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, clk_cmp),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, clk_cmp),
 			&vcpu->arch.sie_block->ckc, 8, prefix))
 		return -EFAULT;
 
-	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, acc_regs),
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area, acc_regs),
 			&vcpu->arch.guest_acrs, 64, prefix))
 		return -EFAULT;
 
 	if (__guestcopy(vcpu,
-			addr + offsetof(struct save_area_s390x, ctrl_regs),
+			addr + offsetof(struct save_area, ctrl_regs),
 			&vcpu->arch.sie_block->gcr, 128, prefix))
 		return -EFAULT;
 	return 0;

commit d7b0b5eb3000c6fb902f08c619fcd673a23d8fab
Author: Carsten Otte <carsteno@de.ibm.com>
Date:   Thu Nov 19 14:21:16 2009 +0100

    KVM: s390: Make psw available on all exits, not just a subset
    
    This patch moves s390 processor status word into the base kvm_run
    struct and keeps it up-to date on all userspace exits.
    
    The userspace ABI is broken by this, however there are no applications
    in the wild using this.  A capability check is provided so users can
    verify the updated API exists.
    
    Cc: stable@kernel.org
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 544505893c9f..f8bcaefd7d34 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -117,10 +117,16 @@ long kvm_arch_dev_ioctl(struct file *filp,
 
 int kvm_dev_ioctl_check_extension(long ext)
 {
+	int r;
+
 	switch (ext) {
+	case KVM_CAP_S390_PSW:
+		r = 1;
+		break;
 	default:
-		return 0;
+		r = 0;
 	}
+	return r;
 }
 
 /* Section: vm related */
@@ -420,8 +426,10 @@ static int kvm_arch_vcpu_ioctl_set_initial_psw(struct kvm_vcpu *vcpu, psw_t psw)
 	vcpu_load(vcpu);
 	if (atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_RUNNING)
 		rc = -EBUSY;
-	else
-		vcpu->arch.sie_block->gpsw = psw;
+	else {
+		vcpu->run->psw_mask = psw.mask;
+		vcpu->run->psw_addr = psw.addr;
+	}
 	vcpu_put(vcpu);
 	return rc;
 }
@@ -509,9 +517,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	switch (kvm_run->exit_reason) {
 	case KVM_EXIT_S390_SIEIC:
-		vcpu->arch.sie_block->gpsw.mask = kvm_run->s390_sieic.mask;
-		vcpu->arch.sie_block->gpsw.addr = kvm_run->s390_sieic.addr;
-		break;
 	case KVM_EXIT_UNKNOWN:
 	case KVM_EXIT_INTR:
 	case KVM_EXIT_S390_RESET:
@@ -520,6 +525,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		BUG();
 	}
 
+	vcpu->arch.sie_block->gpsw.mask = kvm_run->psw_mask;
+	vcpu->arch.sie_block->gpsw.addr = kvm_run->psw_addr;
+
 	might_fault();
 
 	do {
@@ -539,8 +547,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		/* intercept cannot be handled in-kernel, prepare kvm-run */
 		kvm_run->exit_reason         = KVM_EXIT_S390_SIEIC;
 		kvm_run->s390_sieic.icptcode = vcpu->arch.sie_block->icptcode;
-		kvm_run->s390_sieic.mask     = vcpu->arch.sie_block->gpsw.mask;
-		kvm_run->s390_sieic.addr     = vcpu->arch.sie_block->gpsw.addr;
 		kvm_run->s390_sieic.ipa      = vcpu->arch.sie_block->ipa;
 		kvm_run->s390_sieic.ipb      = vcpu->arch.sie_block->ipb;
 		rc = 0;
@@ -552,6 +558,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = 0;
 	}
 
+	kvm_run->psw_mask     = vcpu->arch.sie_block->gpsw.mask;
+	kvm_run->psw_addr     = vcpu->arch.sie_block->gpsw.addr;
+
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
 

commit 10474ae8945ce08622fd1f3464e55bd817bf2376
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Sep 15 11:37:46 2009 +0200

    KVM: Activate Virtualization On Demand
    
    X86 CPUs need to have some magic happening to enable the virtualization
    extensions on them. This magic can result in unpleasant results for
    users, like blocking other VMMs from working (vmx) or using invalid TLB
    entries (svm).
    
    Currently KVM activates virtualization when the respective kernel module
    is loaded. This blocks us from autoloading KVM modules without breaking
    other VMMs.
    
    To circumvent this problem at least a bit, this patch introduces on
    demand activation of virtualization. This means, that instead
    virtualization is enabled on creation of the first virtual machine
    and disabled on destruction of the last one.
    
    So using this, KVM can be easily autoloaded, while keeping other
    hypervisors usable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 00e2ce8e91f5..544505893c9f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -74,9 +74,10 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 static unsigned long long *facilities;
 
 /* Section: not file related */
-void kvm_arch_hardware_enable(void *garbage)
+int kvm_arch_hardware_enable(void *garbage)
 {
 	/* every s390 is virtualization enabled ;-) */
+	return 0;
 }
 
 void kvm_arch_hardware_disable(void *garbage)

commit 367e1319b229110a27c53221c2fa32a6aa86d4a9
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Aug 26 14:57:07 2009 +0300

    KVM: Return -ENOTTY on unrecognized ioctls
    
    Not the incorrect -EINVAL.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 07ced89740d7..00e2ce8e91f5 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -150,7 +150,7 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		break;
 	}
 	default:
-		r = -EINVAL;
+		r = -ENOTTY;
 	}
 
 	return r;

commit 988a2cae6a3c0dea6df59808a935a9a697bfc28c
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Jun 9 15:56:29 2009 +0300

    KVM: Use macro to iterate over vcpus.
    
    [christian: remove unused variables on s390]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 098bfa6fbdf6..07ced89740d7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -211,13 +211,17 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 static void kvm_free_vcpus(struct kvm *kvm)
 {
 	unsigned int i;
+	struct kvm_vcpu *vcpu;
 
-	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
-		if (kvm->vcpus[i]) {
-			kvm_arch_vcpu_destroy(kvm->vcpus[i]);
-			kvm->vcpus[i] = NULL;
-		}
-	}
+	kvm_for_each_vcpu(i, vcpu, kvm)
+		kvm_arch_vcpu_destroy(vcpu);
+
+	mutex_lock(&kvm->lock);
+	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
+		kvm->vcpus[i] = NULL;
+
+	atomic_set(&kvm->online_vcpus, 0);
+	mutex_unlock(&kvm->lock);
 }
 
 void kvm_arch_sync_events(struct kvm *kvm)
@@ -314,8 +318,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	BUG_ON(!kvm->arch.sca);
 	if (!kvm->arch.sca->cpu[id].sda)
 		kvm->arch.sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
-	else
-		BUG_ON(!kvm->vcpus[id]); /* vcpu does already exist */
 	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)kvm->arch.sca) >> 32);
 	vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
 
@@ -683,6 +685,7 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 				int user_alloc)
 {
 	int i;
+	struct kvm_vcpu *vcpu;
 
 	/* A few sanity checks. We can have exactly one memory slot which has
 	   to start at guest virtual zero and which has to be located at a
@@ -707,14 +710,10 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 		return -EINVAL;
 
 	/* request update of sie control block for all available vcpus */
-	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
-		if (kvm->vcpus[i]) {
-			if (test_and_set_bit(KVM_REQ_MMU_RELOAD,
-						&kvm->vcpus[i]->requests))
-				continue;
-			kvm_s390_inject_sigp_stop(kvm->vcpus[i],
-						  ACTION_RELOADVCPU_ON_STOP);
-		}
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		if (test_and_set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
+			continue;
+		kvm_s390_inject_sigp_stop(vcpu, ACTION_RELOADVCPU_ON_STOP);
 	}
 
 	return 0;

commit 628eb9b8a8f3ef31d8316112a4596b1a21b38159
Author: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
Date:   Mon May 25 13:40:51 2009 +0200

    KVM: s390: streamline memslot handling
    
    This patch relocates the variables kvm-s390 uses to track guest mem addr/size.
    As discussed dropping the variables at struct kvm_arch level allows to use the
    common vcpu->request based mechanism to reload guest memory if e.g. changes
    via set_memory_region.
    
    The kick mechanism introduced in this series is used to ensure running vcpus
    leave guest state to catch the update.
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5c1c30259002..098bfa6fbdf6 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1,7 +1,7 @@
 /*
  * s390host.c --  hosting zSeries kernel virtual machines
  *
- * Copyright IBM Corp. 2008
+ * Copyright IBM Corp. 2008,2009
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License (version 2 only)
@@ -10,6 +10,7 @@
  *    Author(s): Carsten Otte <cotte@de.ibm.com>
  *               Christian Borntraeger <borntraeger@de.ibm.com>
  *               Heiko Carstens <heiko.carstens@de.ibm.com>
+ *               Christian Ehrhardt <ehrhardt@de.ibm.com>
  */
 
 #include <linux/compiler.h>
@@ -278,16 +279,10 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->gbea = 1;
 }
 
-/* The current code can have up to 256 pages for virtio */
-#define VIRTIODESCSPACE (256ul * 4096ul)
-
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH);
-	vcpu->arch.sie_block->gmslm = vcpu->kvm->arch.guest_memsize +
-				      vcpu->kvm->arch.guest_origin +
-				      VIRTIODESCSPACE - 1ul;
-	vcpu->arch.sie_block->gmsor = vcpu->kvm->arch.guest_origin;
+	set_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests);
 	vcpu->arch.sie_block->ecb   = 2;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	vcpu->arch.sie_block->fac   = (int) (long) facilities;
@@ -491,9 +486,14 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	vcpu_load(vcpu);
 
 rerun_vcpu:
+	if (vcpu->requests)
+		if (test_and_clear_bit(KVM_REQ_MMU_RELOAD, &vcpu->requests))
+			kvm_s390_vcpu_set_mem(vcpu);
+
 	/* verify, that memory has been registered */
-	if (!vcpu->kvm->arch.guest_memsize) {
+	if (!vcpu->arch.sie_block->gmslm) {
 		vcpu_put(vcpu);
+		VCPU_EVENT(vcpu, 3, "%s", "no memory registered to run vcpu");
 		return -EINVAL;
 	}
 
@@ -691,7 +691,7 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 	   vmas. It is okay to mmap() and munmap() stuff in this slot after
 	   doing this call at any time */
 
-	if (mem->slot || kvm->arch.guest_memsize)
+	if (mem->slot)
 		return -EINVAL;
 
 	if (mem->guest_phys_addr)
@@ -706,36 +706,18 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 	if (!user_alloc)
 		return -EINVAL;
 
-	/* lock all vcpus */
-	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
-		if (!kvm->vcpus[i])
-			continue;
-		if (!mutex_trylock(&kvm->vcpus[i]->mutex))
-			goto fail_out;
-	}
-
-	kvm->arch.guest_origin = mem->userspace_addr;
-	kvm->arch.guest_memsize = mem->memory_size;
-
-	/* update sie control blocks, and unlock all vcpus */
+	/* request update of sie control block for all available vcpus */
 	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
 		if (kvm->vcpus[i]) {
-			kvm->vcpus[i]->arch.sie_block->gmsor =
-				kvm->arch.guest_origin;
-			kvm->vcpus[i]->arch.sie_block->gmslm =
-				kvm->arch.guest_memsize +
-				kvm->arch.guest_origin +
-				VIRTIODESCSPACE - 1ul;
-			mutex_unlock(&kvm->vcpus[i]->mutex);
+			if (test_and_set_bit(KVM_REQ_MMU_RELOAD,
+						&kvm->vcpus[i]->requests))
+				continue;
+			kvm_s390_inject_sigp_stop(kvm->vcpus[i],
+						  ACTION_RELOADVCPU_ON_STOP);
 		}
 	}
 
 	return 0;
-
-fail_out:
-	for (; i >= 0; i--)
-		mutex_unlock(&kvm->vcpus[i]->mutex);
-	return -EINVAL;
 }
 
 void kvm_arch_flush_shadow(struct kvm *kvm)

commit b1d16c495d9e6fe48e7df2e1d18cafc6555a116a
Author: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
Date:   Wed May 20 15:34:56 2009 +0200

    KVM: s390: fix signal handling
    
    If signal pending is true we exit without updating kvm_run, userspace
    currently just does nothing and jumps to kvm_run again.
    Since we did not set an exit_reason we might end up with a random one
    (whatever was the last exit). Therefore it was possible to e.g. jump to
    the psw position the last real interruption set.
    Setting the INTR exit reason ensures that no old psw data is swapped
    in on reentry.
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1d65f6277166..5c1c30259002 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -527,8 +527,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	if (rc == SIE_INTERCEPT_RERUNVCPU)
 		goto rerun_vcpu;
 
-	if (signal_pending(current) && !rc)
+	if (signal_pending(current) && !rc) {
+		kvm_run->exit_reason = KVM_EXIT_INTR;
 		rc = -EINTR;
+	}
 
 	if (rc == -ENOTSUPP) {
 		/* intercept cannot be handled in-kernel, prepare kvm-run */

commit 9ace903d171db7dc2fed96e44ac62b6f4c3ccb3d
Author: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
Date:   Wed May 20 15:34:55 2009 +0200

    KVM: s390: infrastructure to kick vcpus out of guest state
    
    To ensure vcpu's come out of guest context in certain cases this patch adds a
    s390 specific way to kick them out of guest context. Currently it kicks them
    out to rerun the vcpu_run path in the s390 code, but the mechanism itself is
    expandable and with a new flag we could also add e.g. kicks to userspace etc.
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 90d9d1ba258b..1d65f6277166 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -490,6 +490,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	vcpu_load(vcpu);
 
+rerun_vcpu:
 	/* verify, that memory has been registered */
 	if (!vcpu->kvm->arch.guest_memsize) {
 		vcpu_put(vcpu);
@@ -509,6 +510,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		vcpu->arch.sie_block->gpsw.addr = kvm_run->s390_sieic.addr;
 		break;
 	case KVM_EXIT_UNKNOWN:
+	case KVM_EXIT_INTR:
 	case KVM_EXIT_S390_RESET:
 		break;
 	default:
@@ -522,6 +524,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		rc = kvm_handle_sie_intercept(vcpu);
 	} while (!signal_pending(current) && !rc);
 
+	if (rc == SIE_INTERCEPT_RERUNVCPU)
+		goto rerun_vcpu;
+
 	if (signal_pending(current) && !rc)
 		rc = -EINTR;
 

commit ef50f7ac7e234f9696555e41eab3de69c3d86166
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Jun 23 17:24:07 2009 +0200

    KVM: s390: Allow stfle instruction in the guest
    
    2.6.31-rc introduced an architecture level set checker based on facility
    bits. e.g. if the kernel is compiled to run only on z9, several facility
    bits are checked very early and the kernel refuses to boot if a z9 specific
    facility is missing.
    Until now kvm on s390 did not implement the store facility extended (STFLE)
    instruction. A 2.6.31-rc kernel that was compiled for z9 or higher did not
    boot in kvm. This patch implements stfle.
    
    This patch should go in before 2.6.31.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c18b21d6991c..90d9d1ba258b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -25,6 +25,7 @@
 #include <asm/lowcore.h>
 #include <asm/pgtable.h>
 #include <asm/nmi.h>
+#include <asm/system.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
 
@@ -69,6 +70,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ NULL }
 };
 
+static unsigned long long *facilities;
 
 /* Section: not file related */
 void kvm_arch_hardware_enable(void *garbage)
@@ -288,6 +290,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->gmsor = vcpu->kvm->arch.guest_origin;
 	vcpu->arch.sie_block->ecb   = 2;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
+	vcpu->arch.sie_block->fac   = (int) (long) facilities;
 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 	tasklet_init(&vcpu->arch.tasklet, kvm_s390_tasklet,
 		     (unsigned long) vcpu);
@@ -739,11 +742,29 @@ gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
 
 static int __init kvm_s390_init(void)
 {
-	return kvm_init(NULL, sizeof(struct kvm_vcpu), THIS_MODULE);
+	int ret;
+	ret = kvm_init(NULL, sizeof(struct kvm_vcpu), THIS_MODULE);
+	if (ret)
+		return ret;
+
+	/*
+	 * guests can ask for up to 255+1 double words, we need a full page
+	 * to hold the maximum amount of facilites. On the other hand, we
+	 * only set facilities that are known to work in KVM.
+	 */
+	facilities = (unsigned long long *) get_zeroed_page(GFP_DMA);
+	if (!facilities) {
+		kvm_exit();
+		return -ENOMEM;
+	}
+	stfle(facilities, 1);
+	facilities[0] &= 0xff00fff3f0700000ULL;
+	return 0;
 }
 
 static void __exit kvm_s390_exit(void)
 {
+	free_page((unsigned long) facilities);
 	kvm_exit();
 }
 

commit dab4079d5b5ac421208499d5e554a07f9beb16e4
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jun 12 10:26:32 2009 +0200

    [S390] uaccess: use might_fault() instead of might_sleep()
    
    Adds more checking in case lockdep is turned on.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 10bccd1f8aee..c18b21d6991c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -512,7 +512,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 		BUG();
 	}
 
-	might_sleep();
+	might_fault();
 
 	do {
 		__vcpu_run(vcpu);

commit 51e4d5ab28c75d819b3840da11559ad5c1429135
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue May 12 17:21:53 2009 +0200

    KVM: s390: Verify memory in kvm run
    
    This check verifies that the guest we're trying to run in KVM_RUN
    has some memory assigned to it. It enters an endless exception
    loop if this is not the case.
    
    Reported-by: Mijo Safradin <mijo@linux.vnet.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Ehrhardt <ehrhardt@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 628494a43425..10bccd1f8aee 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -487,6 +487,12 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	vcpu_load(vcpu);
 
+	/* verify, that memory has been registered */
+	if (!vcpu->kvm->arch.guest_memsize) {
+		vcpu_put(vcpu);
+		return -EINVAL;
+	}
+
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 

commit abf4a71ed95ff29d696bf04633958b2068ed2e0b
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue May 12 17:21:51 2009 +0200

    KVM: s390: Unlink vcpu on destroy - v2
    
    This patch makes sure we do unlink a vcpu's sie control block
    from the system control area in kvm_arch_vcpu_destroy. This
    prevents illegal accesses to the sie control block from other
    virtual cpus after free.
    
    Reported-by: Mijo Safradin <mijo@linux.vnet.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Ehrhardt <ehrhardt@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 36c654d2d64a..628494a43425 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -196,6 +196,10 @@ struct kvm *kvm_arch_create_vm(void)
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
+	if (vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda ==
+		(__u64) vcpu->arch.sie_block)
+		vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda = 0;
+	smp_mb();
 	free_page((unsigned long)(vcpu->arch.sie_block));
 	kvm_vcpu_uninit(vcpu);
 	kfree(vcpu);
@@ -310,8 +314,10 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 
 	vcpu->arch.sie_block->icpua = id;
 	BUG_ON(!kvm->arch.sca);
-	BUG_ON(kvm->arch.sca->cpu[id].sda);
-	kvm->arch.sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+	if (!kvm->arch.sca->cpu[id].sda)
+		kvm->arch.sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+	else
+		BUG_ON(!kvm->vcpus[id]); /* vcpu does already exist */
 	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)kvm->arch.sca) >> 32);
 	vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
 

commit b037a4f34ec51b6c8ccb352a04056c04a4bfc269
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue May 12 17:21:50 2009 +0200

    KVM: s390: optimize float int lock: spin_lock_bh --> spin_lock
    
    The floating interrupt lock is only taken in process context. We can
    replace all spin_lock_bh with standard spin_lock calls.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Ehrhardt <ehrhardt@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index dc3d06811fd8..36c654d2d64a 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -318,11 +318,11 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	spin_lock_init(&vcpu->arch.local_int.lock);
 	INIT_LIST_HEAD(&vcpu->arch.local_int.list);
 	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
-	spin_lock_bh(&kvm->arch.float_int.lock);
+	spin_lock(&kvm->arch.float_int.lock);
 	kvm->arch.float_int.local_int[id] = &vcpu->arch.local_int;
 	init_waitqueue_head(&vcpu->arch.local_int.wq);
 	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
-	spin_unlock_bh(&kvm->arch.float_int.lock);
+	spin_unlock(&kvm->arch.float_int.lock);
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)

commit ca8723023f25c9a70d76cbd6101f8fb4ffec2fa0
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue May 12 17:21:49 2009 +0200

    KVM: s390: use hrtimer for clock wakeup from idle - v2
    
    This patch reworks the s390 clock comparator wakeup to hrtimer. The clock
    comparator is a per-cpu value that is compared against the TOD clock. If
    ckc <= TOD an external interrupt 1004 is triggered. Since the clock comparator
    and the TOD clock have a much higher resolution than jiffies we should use
    hrtimers to trigger the wakeup. This speeds up guest nanosleep for small
    values.
    
    Since hrtimers callbacks run in hard-irq context, I added a tasklet to do
    the actual work with enabled interrupts.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Ehrhardt <ehrhardt@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 86567e174fd7..dc3d06811fd8 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -15,6 +15,7 @@
 #include <linux/compiler.h>
 #include <linux/err.h>
 #include <linux/fs.h>
+#include <linux/hrtimer.h>
 #include <linux/init.h>
 #include <linux/kvm.h>
 #include <linux/kvm_host.h>
@@ -283,8 +284,10 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->gmsor = vcpu->kvm->arch.guest_origin;
 	vcpu->arch.sie_block->ecb   = 2;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
-	setup_timer(&vcpu->arch.ckc_timer, kvm_s390_idle_wakeup,
-		 (unsigned long) vcpu);
+	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
+	tasklet_init(&vcpu->arch.tasklet, kvm_s390_tasklet,
+		     (unsigned long) vcpu);
+	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 	get_cpu_id(&vcpu->arch.cpu_id);
 	vcpu->arch.cpu_id.version = 0xff;
 	return 0;

commit 2668dab794272f0898491acaf1e77e9a005abc0f
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue May 12 17:21:48 2009 +0200

    KVM: s390: Fix memory slot versus run - v3
    
    This patch fixes an incorrectness in the kvm backend for s390.
    In case virtual cpus are being created before the corresponding
    memory slot is being registered, we need to update the sie
    control blocks for the virtual cpus.
    
    *updates in v3*
    In consideration of the s390 memslot constraints locking was changed
    to trylock. These locks should never be held, as vcpu's can't run without
    the single memslot we just assign when running this code. To ensure this
    never deadlocks in case other code changes the code uses trylocks and bail
    out if it can't get all locks.
    
    Additionally most of the discussed special conditions for s390 like
    only one memslot and no user_alloc are now checked for validity in
    kvm_arch_set_memory_region.
    
    Reported-by: Mijo Safradin <mijo@linux.vnet.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Ehrhardt <ehrhardt@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index f4d56e9939c9..86567e174fd7 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -657,6 +657,8 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot old,
 				int user_alloc)
 {
+	int i;
+
 	/* A few sanity checks. We can have exactly one memory slot which has
 	   to start at guest virtual zero and which has to be located at a
 	   page boundary in userland and which has to end at a page boundary.
@@ -664,7 +666,7 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 	   vmas. It is okay to mmap() and munmap() stuff in this slot after
 	   doing this call at any time */
 
-	if (mem->slot)
+	if (mem->slot || kvm->arch.guest_memsize)
 		return -EINVAL;
 
 	if (mem->guest_phys_addr)
@@ -676,15 +678,39 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 	if (mem->memory_size & (PAGE_SIZE - 1))
 		return -EINVAL;
 
+	if (!user_alloc)
+		return -EINVAL;
+
+	/* lock all vcpus */
+	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
+		if (!kvm->vcpus[i])
+			continue;
+		if (!mutex_trylock(&kvm->vcpus[i]->mutex))
+			goto fail_out;
+	}
+
 	kvm->arch.guest_origin = mem->userspace_addr;
 	kvm->arch.guest_memsize = mem->memory_size;
 
-	/* FIXME: we do want to interrupt running CPUs and update their memory
-	   configuration now to avoid race conditions. But hey, changing the
-	   memory layout while virtual CPUs are running is usually bad
-	   programming practice. */
+	/* update sie control blocks, and unlock all vcpus */
+	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
+		if (kvm->vcpus[i]) {
+			kvm->vcpus[i]->arch.sie_block->gmsor =
+				kvm->arch.guest_origin;
+			kvm->vcpus[i]->arch.sie_block->gmslm =
+				kvm->arch.guest_memsize +
+				kvm->arch.guest_origin +
+				VIRTIODESCSPACE - 1ul;
+			mutex_unlock(&kvm->vcpus[i]->mutex);
+		}
+	}
 
 	return 0;
+
+fail_out:
+	for (; i >= 0; i--)
+		mutex_unlock(&kvm->vcpus[i]->mutex);
+	return -EINVAL;
 }
 
 void kvm_arch_flush_shadow(struct kvm *kvm)

commit 21cdbc1378e8aa96e1ed4a606dce1a8e7daf7fdf
Merge: 86d9c070175d ef3500b2b295
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 26 16:04:22 2009 -0700

    Merge branch 'for-linus' of git://git390.marist.edu/pub/scm/linux-2.6
    
    * 'for-linus' of git://git390.marist.edu/pub/scm/linux-2.6: (81 commits)
      [S390] remove duplicated #includes
      [S390] cpumask: use mm_cpumask() wrapper
      [S390] cpumask: Use accessors code.
      [S390] cpumask: prepare for iterators to only go to nr_cpu_ids/nr_cpumask_bits.
      [S390] cpumask: remove cpu_coregroup_map
      [S390] fix clock comparator save area usage
      [S390] Add hwcap flag for the etf3 enhancement facility
      [S390] Ensure that ipl panic notifier is called late.
      [S390] fix dfp elf hwcap/facility bit detection
      [S390] smp: perform initial cpu reset before starting a cpu
      [S390] smp: fix memory leak on __cpu_up
      [S390] ipl: Improve checking logic and remove switch defaults.
      [S390] s390dbf: Remove needless check for NULL pointer.
      [S390] s390dbf: Remove redundant initilizations.
      [S390] use kzfree()
      [S390] BUG to BUG_ON changes
      [S390] zfcpdump: Prevent zcore from beeing built as a kernel module.
      [S390] Use csum_partial in checksum.h
      [S390] cleanup lowcore.h
      [S390] eliminate ipl_device from lowcore
      ...

commit f5daba1d4116d964435ddd99f32b6c80448a496b
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Mar 26 15:24:01 2009 +0100

    [S390] split/move machine check handler code
    
    Split machine check handler code and move it to cio and kernel code
    where it belongs to. No functional change.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c55a4b9ffd88..caa4d2877016 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -23,7 +23,7 @@
 #include <linux/timer.h>
 #include <asm/lowcore.h>
 #include <asm/pgtable.h>
-
+#include <asm/nmi.h>
 #include "kvm-s390.h"
 #include "gaccess.h"
 
@@ -440,8 +440,6 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
-extern void s390_handle_mcck(void);
-
 static void __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->arch.guest_gprs[14], 16);

commit 92e6ecf392fac3082653ac9d84b1bdf53d0ea160
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Mar 26 15:23:58 2009 +0100

    [S390] Fix hypervisor detection for KVM
    
    Currently we use the cpuid (via STIDP instruction) to recognize LPAR,
    z/VM and KVM.
    The architecture states, that bit 0-7 of STIDP returns all zero, and
    if STIDP is executed in a virtual machine, the VM operating system
    will replace bits 0-7 with FF.
    
    KVM should not use FE to distinguish z/VM from KVM for interested
    guests. The proper way to detect the hypervisor is the STSI (Store
    System Information) instruction, which return information about the
    hypervisors via function code 3, selector1=2, selector2=2.
    
    This patch changes the detection routine of Linux to use STSI instead
    of STIDP. This detection is earlier than bootmem, we have to use a
    static buffer. Since STSI expects a 4kb block (4kb aligned) this
    patch also changes the init.data alignment for s390. As this section
    will be freed during boot, this should be no problem.
    
    Patch is tested with LPAR, z/VM, KVM on LPAR, and KVM under z/VM.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0d33893e1e89..c55a4b9ffd88 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -286,7 +286,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	setup_timer(&vcpu->arch.ckc_timer, kvm_s390_idle_wakeup,
 		 (unsigned long) vcpu);
 	get_cpu_id(&vcpu->arch.cpu_id);
-	vcpu->arch.cpu_id.version = 0xfe;
+	vcpu->arch.cpu_id.version = 0xff;
 	return 0;
 }
 

commit d0bfb940ecabf0b44fb1fd80d8d60594e569e5ec
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Dec 15 13:52:10 2008 +0100

    KVM: New guest debug interface
    
    This rips out the support for KVM_DEBUG_GUEST and introduces a new IOCTL
    instead: KVM_SET_GUEST_DEBUG. The IOCTL payload consists of a generic
    part, controlling the "main switch" and the single-step feature. The
    arch specific part adds an x86 interface for intercepting both types of
    debug exceptions separately and re-injecting them when the host was not
    interested. Moveover, the foundation for guest debugging via debug
    registers is layed.
    
    To signal breakpoint events properly back to userland, an arch-specific
    data block is now returned along KVM_EXIT_DEBUG. For x86, the arch block
    contains the PC, the debug exception, and relevant debug registers to
    tell debug events properly apart.
    
    The availability of this new interface is signaled by
    KVM_CAP_SET_GUEST_DEBUG. Empty stubs for not yet supported archs are
    provided.
    
    Note that both SVM and VTX are supported, but only the latter was tested
    yet. Based on the experience with all those VTX corner case, I would be
    fairly surprised if SVM will work out of the box.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0d33893e1e89..cbfe91e10120 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -422,8 +422,8 @@ int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
-int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
-				    struct kvm_debug_guest *dbg)
+int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
+					struct kvm_guest_debug *dbg)
 {
 	return -EINVAL; /* not implemented yet */
 }

commit ad8ba2cd44d4d39fb3fe55d5dcc565b19fc3a7fb
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Jan 6 10:03:02 2009 +0800

    KVM: Add kvm_arch_sync_events to sync with asynchronize events
    
    kvm_arch_sync_events is introduced to quiet down all other events may happen
    contemporary with VM destroy process, like IRQ handler and work struct for
    assigned device.
    
    For kvm_arch_sync_events is called at the very beginning of kvm_destroy_vm(), so
    the state of KVM here is legal and can provide a environment to quiet down other
    events.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index be8497186b96..0d33893e1e89 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -212,6 +212,10 @@ static void kvm_free_vcpus(struct kvm *kvm)
 	}
 }
 
+void kvm_arch_sync_events(struct kvm *kvm)
+{
+}
+
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvm_free_vcpus(kvm);

commit ca9edaee1aea34ebd9adb48910aba0b3d64b1b22
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Dec 8 18:29:29 2008 +0200

    KVM: Consolidate userspace memory capability reporting into common code
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 76f05ddaef10..be8497186b96 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -113,8 +113,6 @@ long kvm_arch_dev_ioctl(struct file *filp,
 int kvm_dev_ioctl_check_extension(long ext)
 {
 	switch (ext) {
-	case KVM_CAP_USER_MEMORY:
-		return 1;
 	default:
 		return 0;
 	}

commit 6692cef30b7caf7525ae99670cddbaf28f1f9d40
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Nov 26 14:51:08 2008 +0100

    KVM: s390: Fix memory leak of vcpu->run
    
    The s390 backend of kvm never calls kvm_vcpu_uninit. This causes
    a memory leak of vcpu->run pages.
    Lets call kvm_vcpu_uninit in kvm_arch_vcpu_destroy to free
    the vcpu->run.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 3db9e5d45a67..76f05ddaef10 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -198,6 +198,7 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
 	free_page((unsigned long)(vcpu->arch.sie_block));
+	kvm_vcpu_uninit(vcpu);
 	kfree(vcpu);
 }
 
@@ -230,8 +231,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 {
-	/* kvm common code refers to this, but does'nt call it */
-	BUG();
+	/* Nothing todo */
 }
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)

commit d329c035e754156ffabcb64ff75d05bb8e2ddbf5
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Nov 26 14:50:27 2008 +0100

    KVM: s390: Fix refcounting and allow module unload
    
    Currently it is impossible to unload the kvm module on s390.
    This patch fixes kvm_arch_destroy_vm to release all cpus.
    This make it possible to unload the module.
    
    In addition we stop messing with the module refcount in arch code.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 8b00eb2ddf57..3db9e5d45a67 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -185,8 +185,6 @@ struct kvm *kvm_arch_create_vm(void)
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 	VM_EVENT(kvm, 3, "%s", "vm created");
 
-	try_module_get(THIS_MODULE);
-
 	return kvm;
 out_nodbf:
 	free_page((unsigned long)(kvm->arch.sca));
@@ -196,13 +194,32 @@ struct kvm *kvm_arch_create_vm(void)
 	return ERR_PTR(rc);
 }
 
+void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
+{
+	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
+	free_page((unsigned long)(vcpu->arch.sie_block));
+	kfree(vcpu);
+}
+
+static void kvm_free_vcpus(struct kvm *kvm)
+{
+	unsigned int i;
+
+	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
+		if (kvm->vcpus[i]) {
+			kvm_arch_vcpu_destroy(kvm->vcpus[i]);
+			kvm->vcpus[i] = NULL;
+		}
+	}
+}
+
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
-	debug_unregister(kvm->arch.dbf);
+	kvm_free_vcpus(kvm);
 	kvm_free_physmem(kvm);
 	free_page((unsigned long)(kvm->arch.sca));
+	debug_unregister(kvm->arch.dbf);
 	kfree(kvm);
-	module_put(THIS_MODULE);
 }
 
 /* Section: vcpu related */
@@ -308,8 +325,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	VM_EVENT(kvm, 3, "create cpu %d at %p, sie block at %p", id, vcpu,
 		 vcpu->arch.sie_block);
 
-	try_module_get(THIS_MODULE);
-
 	return vcpu;
 out_free_cpu:
 	kfree(vcpu);
@@ -317,14 +332,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	return ERR_PTR(rc);
 }
 
-void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
-{
-	VCPU_EVENT(vcpu, 3, "%s", "destroy cpu");
-	free_page((unsigned long)(vcpu->arch.sie_block));
-	kfree(vcpu);
-	module_put(THIS_MODULE);
-}
-
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 {
 	/* kvm common code refers to this, but never calls it */

commit f5e10b09a5f8fc40666c95fe0cd6bcc2b8f11437
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jul 25 15:52:44 2008 +0200

    KVM: s390: Fix instruction naming for lctlg
    
    Lets fix the name for the lctlg instruction...
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index fcd41795b550..8b00eb2ddf57 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -39,7 +39,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_instruction", VCPU_STAT(exit_instruction) },
 	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
 	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
-	{ "instruction_lctg", VCPU_STAT(instruction_lctg) },
+	{ "instruction_lctlg", VCPU_STAT(instruction_lctlg) },
 	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
 	{ "deliver_emergency_signal", VCPU_STAT(deliver_emergency_signal) },
 	{ "deliver_service_signal", VCPU_STAT(deliver_service_signal) },

commit 2bd0ac4eb469ef58c3b1746fccd15da871fc55c4
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Fri Jul 25 15:49:13 2008 +0200

    KVM: s390: Advertise KVM_CAP_USER_MEMORY
    
    KVM_CAP_USER_MEMORY is used by s390, therefore, we should advertise it.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1782cbcd2829..fcd41795b550 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -112,7 +112,12 @@ long kvm_arch_dev_ioctl(struct file *filp,
 
 int kvm_dev_ioctl_check_extension(long ext)
 {
-	return 0;
+	switch (ext) {
+	case KVM_CAP_USER_MEMORY:
+		return 1;
+	default:
+		return 0;
+	}
 }
 
 /* Section: vm related */

commit 34d4cb8fca1f2a31be152b74797e6cd160ec9de6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jul 10 20:49:31 2008 -0300

    KVM: MMU: nuke shadowed pgtable pages and ptes on memslot destruction
    
    Flush the shadow mmu before removing regions to avoid stale entries.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 399acf3f64dd..1782cbcd2829 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -675,6 +675,10 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 	return 0;
 }
 
+void kvm_arch_flush_shadow(struct kvm *kvm)
+{
+}
+
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
 {
 	return gfn;

commit 180c12fb22bd17c7187ae1bce023d24a42b2980c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jun 27 15:05:40 2008 +0200

    KVM: s390: rename private structures
    
    While doing some tests with our lcrash implementation I have seen a
    naming conflict with prefix_info in kvm_host.h vs. addrconf.h
    
    To avoid future conflicts lets rename private definitions in
    asm/kvm_host.h by adding the kvm_s390 prefix.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index cdab57c5bc70..399acf3f64dd 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -275,7 +275,8 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	if (!vcpu)
 		goto out_nomem;
 
-	vcpu->arch.sie_block = (struct sie_block *) get_zeroed_page(GFP_KERNEL);
+	vcpu->arch.sie_block = (struct kvm_s390_sie_block *)
+					get_zeroed_page(GFP_KERNEL);
 
 	if (!vcpu->arch.sie_block)
 		goto out_free_cpu;

commit 4da29e909ea8087de09e27476f91f51a070cabe8
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jun 27 15:05:38 2008 +0200

    KVM: s390: Set guest storage limit and offset to sane values
    
    Some machines do not accept 16EB as guest storage limit. Lets change the
    default for the guest storage limit to a sane value. We also should set
    the guest_origin to what userspace thinks it is. This allows guests
    starting at an address != 0.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index b802ce6f675f..cdab57c5bc70 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -247,11 +247,16 @@ static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->gbea = 1;
 }
 
+/* The current code can have up to 256 pages for virtio */
+#define VIRTIODESCSPACE (256ul * 4096ul)
+
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH);
-	vcpu->arch.sie_block->gmslm = 0xffffffffffUL;
-	vcpu->arch.sie_block->gmsor = 0x000000000000;
+	vcpu->arch.sie_block->gmslm = vcpu->kvm->arch.guest_memsize +
+				      vcpu->kvm->arch.guest_origin +
+				      VIRTIODESCSPACE - 1ul;
+	vcpu->arch.sie_block->gmsor = vcpu->kvm->arch.guest_origin;
 	vcpu->arch.sie_block->ecb   = 2;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	setup_timer(&vcpu->arch.ckc_timer, kvm_s390_idle_wakeup,

commit dfdded7c41e5b68c79a9f8a942d41f56bc265ba4
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Fri Jun 27 15:05:34 2008 +0200

    KVM: Fix memory leak on guest exit
    
    This patch fixes a memory leak, we want to free the physmem when destroying
    the vm.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 4585c8ac2b0c..b802ce6f675f 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -194,6 +194,7 @@ struct kvm *kvm_arch_create_vm(void)
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	debug_unregister(kvm->arch.dbf);
+	kvm_free_physmem(kvm);
 	free_page((unsigned long)(kvm->arch.sca));
 	kfree(kvm);
 	module_put(THIS_MODULE);

commit 7cc8883074b040aa8c1ebd3a17463b0ea3a9ef16
Author: Avi Kivity <avi@qumranet.com>
Date:   Tue May 13 16:29:20 2008 +0300

    KVM: Remove decache_vcpus_on_cpu() and related callbacks
    
    Obsoleted by the vmx-specific per-cpu list.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6558b09ff579..4585c8ac2b0c 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -79,10 +79,6 @@ void kvm_arch_hardware_disable(void *garbage)
 {
 }
 
-void decache_vcpus_on_cpu(int cpu)
-{
-}
-
 int kvm_arch_hardware_setup(void)
 {
 	return 0;

commit 1f0d0f094df9a570dfc26d5eb825986b7e165e1d
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed May 21 13:37:40 2008 +0200

    KVM: s390: Send program check on access error
    
    If the guest accesses non-existing memory, the sie64a function returns
    -EFAULT. We must check the return value and send a program check to the
    guest if the sie instruction faulted, otherwise the guest will loop at
    the faulting code.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index ded27c7777cc..6558b09ff579 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -443,7 +443,10 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
 		   atomic_read(&vcpu->arch.sie_block->cpuflags));
-	sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs);
+	if (sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs)) {
+		VCPU_EVENT(vcpu, 3, "%s", "fault in sie instruction");
+		kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);
+	}
 	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
 		   vcpu->arch.sie_block->icptcode);
 	local_irq_disable();

commit 0ff318674503ce3787ef62d84f4d948db204b268
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed May 21 13:37:37 2008 +0200

    KVM: s390: fix interrupt delivery
    
    The current code delivers pending interrupts before it checks for
    need_resched. On a busy host, this can lead to a longer interrupt
    latency if the interrupt is injected while the process is scheduled
    away. This patch moves delivering the interrupt _after_ schedule(),
    which makes more sense.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 40e4f2de7320..ded27c7777cc 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -435,6 +435,8 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	if (test_thread_flag(TIF_MCCK_PENDING))
 		s390_handle_mcck();
 
+	kvm_s390_deliver_pending_interrupts(vcpu);
+
 	vcpu->arch.sie_block->icptcode = 0;
 	local_irq_disable();
 	kvm_guest_enter();
@@ -480,7 +482,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	might_sleep();
 
 	do {
-		kvm_s390_deliver_pending_interrupts(vcpu);
 		__vcpu_run(vcpu);
 		rc = kvm_handle_sie_intercept(vcpu);
 	} while (!signal_pending(current) && !rc);

commit 71cde5879f10b639506bc0b9f29a89f58b42a17e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed May 21 13:37:34 2008 +0200

    KVM: s390: handle machine checks when guest is running
    
    The low-level interrupt handler on s390 checks for _TIF_WORK_INT and
    exits the guest context, if work is pending.
    TIF_WORK_INT is defined as_TIF_SIGPENDING | _TIF_NEED_RESCHED |
     _TIF_MCCK_PENDING. Currently the sie loop checks for signals and
    reschedule, but it does not check for machine checks. That means that
    we exit the guest context if a machine check is pending, but we do not
    handle the machine check.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    CC: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 0ac36a649eba..40e4f2de7320 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -423,6 +423,8 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
+extern void s390_handle_mcck(void);
+
 static void __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->arch.guest_gprs[14], 16);
@@ -430,6 +432,9 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 	if (need_resched())
 		schedule();
 
+	if (test_thread_flag(TIF_MCCK_PENDING))
+		s390_handle_mcck();
+
 	vcpu->arch.sie_block->icptcode = 0;
 	local_irq_disable();
 	kvm_guest_enter();

commit 0eaeafa10f3b2bd027e95859a6785d4c7fcc174c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed May 7 09:22:53 2008 +0200

    [S390] s390-kvm: leave sie context on work. Removes preemption requirement
    
    From: Martin Schwidefsky <schwidefsky@de.ibm.com>
    
    This patch fixes a bug with cpu bound guest on kvm-s390. Sometimes it
    was impossible to deliver a signal to a spinning guest. We used
    preemption as a circumvention. The preemption notifiers called
    vcpu_load, which checked for pending signals and triggered a host
    intercept. But even with preemption, a sigkill was not delivered
    immediately.
    
    This patch changes the low level host interrupt handler to check for the
    SIE  instruction, if TIF_WORK is set. In that case we change the
    instruction pointer of the return PSW to rerun the vcpu_run loop. The kvm
    code sees an intercept reason 0 if that happens. This patch adds accounting
    for these types of intercept as well.
    
    The advantages:
    - works with and without preemption
    - signals are delivered immediately
    - much better host latencies without preemption
    
    Acked-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 98d1e73e01f1..0ac36a649eba 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -31,6 +31,7 @@
 
 struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "userspace_handled", VCPU_STAT(exit_userspace) },
+	{ "exit_null", VCPU_STAT(exit_null) },
 	{ "exit_validity", VCPU_STAT(exit_validity) },
 	{ "exit_stop_request", VCPU_STAT(exit_stop_request) },
 	{ "exit_external_request", VCPU_STAT(exit_external_request) },
@@ -221,10 +222,6 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	vcpu->arch.guest_fpregs.fpc &= FPC_VALID_MASK;
 	restore_fp_regs(&vcpu->arch.guest_fpregs);
 	restore_access_regs(vcpu->arch.guest_acrs);
-
-	if (signal_pending(current))
-		atomic_set_mask(CPUSTAT_STOP_INT,
-			&vcpu->arch.sie_block->cpuflags);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)

commit 62d9f0dbc92d7e398fde53fc6021338393522e68
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Apr 11 13:24:45 2008 -0300

    KVM: add ioctls to save/store mpstate
    
    So userspace can save/restore the mpstate during migration.
    
    [avi: export the #define constants describing the value]
    [christian: add s390 stubs]
    [avi: ditto for ia64]
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d96613765973..98d1e73e01f1 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -414,6 +414,18 @@ int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
 	return -EINVAL; /* not implemented yet */
 }
 
+int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
+				    struct kvm_mp_state *mp_state)
+{
+	return -EINVAL; /* not implemented yet */
+}
+
+int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
+				    struct kvm_mp_state *mp_state)
+{
+	return -EINVAL; /* not implemented yet */
+}
+
 static void __vcpu_run(struct kvm_vcpu *vcpu)
 {
 	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->arch.guest_gprs[14], 16);

commit 7e8e6ab48d78147f69c1ba2d6a362f8d33254468
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Apr 4 15:12:35 2008 +0200

    KVM: s390: Fix incorrect return value
    
    kvm_arch_vcpu_ioctl_run currently incorrectly always returns 0.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index d3b1de83678b..d96613765973 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -497,7 +497,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	vcpu_put(vcpu);
 
 	vcpu->stat.exit_userspace++;
-	return 0;
+	return rc;
 }
 
 static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, const void *from,

commit e28acfea5dd9dbc67c2594cbefc140129dbd0e3f
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 25 18:47:34 2008 +0100

    KVM: s390: intercepts for diagnose instructions
    
    This patch introduces interpretation of some diagnose instruction intercepts.
    Diagnose is our classic architected way of doing a hypercall. This patch
    features the following diagnose codes:
    - vm storage size, that tells the guest about its memory layout
    - time slice end, which is used by the guest to indicate that it waits
      for a lock and thus cannot use up its time slice in a useful way
    - ipl functions, which a guest can use to reset and reboot itself
    
    In order to implement ipl functions, we also introduce an exit reason that
    causes userspace to perform various resets on the virtual machine. All resets
    are described in the principles of operation book, except KVM_S390_RESET_IPL
    which causes a reboot of the machine.
    
    Acked-by: Martin Schwidefsky <martin.schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index c632180739ee..d3b1de83678b 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -63,6 +63,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_sigp_set_arch", VCPU_STAT(instruction_sigp_arch) },
 	{ "instruction_sigp_set_prefix", VCPU_STAT(instruction_sigp_prefix) },
 	{ "instruction_sigp_restart", VCPU_STAT(instruction_sigp_restart) },
+	{ "diagnose_44", VCPU_STAT(diagnose_44) },
 	{ NULL }
 };
 

commit 5288fbf0ef041ba0e8b4dcb2df4536b5e3a48b32
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 25 18:47:31 2008 +0100

    KVM: s390: interprocessor communication via sigp
    
    This patch introduces in-kernel handling of _some_ sigp interprocessor
    signals (similar to ipi).
    kvm_s390_handle_sigp() decodes the sigp instruction and calls individual
    handlers depending on the operation requested:
    - sigp sense tries to retrieve information such as existence or running state
      of the remote cpu
    - sigp emergency sends an external interrupt to the remove cpu
    - sigp stop stops a remove cpu
    - sigp stop store status stops a remote cpu, and stores its entire internal
      state to the cpus lowcore
    - sigp set arch sets the architecture mode of the remote cpu. setting to
      ESAME (s390x 64bit) is accepted, setting to ESA/S390 (s390, 31 or 24 bit) is
      denied, all others are passed to userland
    - sigp set prefix sets the prefix register of a remote cpu
    
    For implementation of this, the stop intercept indication starts to get reused
    on purpose: a set of action bits defines what to do once a cpu gets stopped:
    ACTION_STOP_ON_STOP  really stops the cpu when a stop intercept is recognized
    ACTION_STORE_ON_STOP stores the cpu status to lowcore when a stop intercept is
                         recognized
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5a17176fb641..c632180739ee 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -57,6 +57,12 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "instruction_chsc", VCPU_STAT(instruction_chsc) },
 	{ "instruction_stsi", VCPU_STAT(instruction_stsi) },
 	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
+	{ "instruction_sigp_sense", VCPU_STAT(instruction_sigp_sense) },
+	{ "instruction_sigp_emergency", VCPU_STAT(instruction_sigp_emergency) },
+	{ "instruction_sigp_stop", VCPU_STAT(instruction_sigp_stop) },
+	{ "instruction_sigp_set_arch", VCPU_STAT(instruction_sigp_arch) },
+	{ "instruction_sigp_set_prefix", VCPU_STAT(instruction_sigp_prefix) },
+	{ "instruction_sigp_restart", VCPU_STAT(instruction_sigp_restart) },
 	{ NULL }
 };
 
@@ -287,6 +293,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	spin_lock_bh(&kvm->arch.float_int.lock);
 	kvm->arch.float_int.local_int[id] = &vcpu->arch.local_int;
 	init_waitqueue_head(&vcpu->arch.local_int.wq);
+	vcpu->arch.local_int.cpuflags = &vcpu->arch.sie_block->cpuflags;
 	spin_unlock_bh(&kvm->arch.float_int.lock);
 
 	rc = kvm_vcpu_init(vcpu, kvm, id);

commit 453423dce2785b8e22077e3b3eeecb4f60fe3470
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 25 18:47:29 2008 +0100

    KVM: s390: intercepts for privileged instructions
    
    This patch introduces in-kernel handling of some intercepts for privileged
    instructions:
    
    handle_set_prefix()        sets the prefix register of the local cpu
    handle_store_prefix()      stores the content of the prefix register to memory
    handle_store_cpu_address() stores the cpu number of the current cpu to memory
    handle_skey()              just decrements the instruction address and retries
    handle_stsch()             delivers condition code 3 "operation not supported"
    handle_chsc()              same here
    handle_stfl()              stores the facility list which contains the
                               capabilities of the cpu
    handle_stidp()             stores cpu type/model/revision and such
    handle_stsi()              stores information about the system topology
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 5e3473c9a639..5a17176fb641 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -48,6 +48,15 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
 	{ "deliver_program_interruption", VCPU_STAT(deliver_program_int) },
 	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
+	{ "instruction_stidp", VCPU_STAT(instruction_stidp) },
+	{ "instruction_spx", VCPU_STAT(instruction_spx) },
+	{ "instruction_stpx", VCPU_STAT(instruction_stpx) },
+	{ "instruction_stap", VCPU_STAT(instruction_stap) },
+	{ "instruction_storage_key", VCPU_STAT(instruction_storage_key) },
+	{ "instruction_stsch", VCPU_STAT(instruction_stsch) },
+	{ "instruction_chsc", VCPU_STAT(instruction_chsc) },
+	{ "instruction_stsi", VCPU_STAT(instruction_stsi) },
+	{ "instruction_stfl", VCPU_STAT(instruction_stfl) },
 	{ NULL }
 };
 
@@ -246,6 +255,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
 	setup_timer(&vcpu->arch.ckc_timer, kvm_s390_idle_wakeup,
 		 (unsigned long) vcpu);
+	get_cpu_id(&vcpu->arch.cpu_id);
+	vcpu->arch.cpu_id.version = 0xfe;
 	return 0;
 }
 

commit ba5c1e9b6ceebdc39343cc03eb39f077abd3c571
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue Mar 25 18:47:26 2008 +0100

    KVM: s390: interrupt subsystem, cpu timer, waitpsw
    
    This patch contains the s390 interrupt subsystem (similar to in kernel apic)
    including timer interrupts (similar to in-kernel-pit) and enabled wait
    (similar to in kernel hlt).
    
    In order to achieve that, this patch also introduces intercept handling
    for instruction intercepts, and it implements load control instructions.
    
    This patch introduces an ioctl KVM_S390_INTERRUPT which is valid for both
    the vm file descriptors and the vcpu file descriptors. In case this ioctl is
    issued against a vm file descriptor, the interrupt is considered floating.
    Floating interrupts may be delivered to any virtual cpu in the configuration.
    
    The following interrupts are supported:
    SIGP STOP       - interprocessor signal that stops a remote cpu
    SIGP SET PREFIX - interprocessor signal that sets the prefix register of a
                      (stopped) remote cpu
    INT EMERGENCY   - interprocessor interrupt, usually used to signal need_reshed
                      and for smp_call_function() in the guest.
    PROGRAM INT     - exception during program execution such as page fault, illegal
                      instruction and friends
    RESTART         - interprocessor signal that starts a stopped cpu
    INT VIRTIO      - floating interrupt for virtio signalisation
    INT SERVICE     - floating interrupt for signalisations from the system
                      service processor
    
    struct kvm_s390_interrupt, which is submitted as ioctl parameter when injecting
    an interrupt, also carrys parameter data for interrupts along with the interrupt
    type. Interrupts on s390 usually have a state that represents the current
    operation, or identifies which device has caused the interruption on s390.
    
    kvm_s390_handle_wait() does handle waitpsw in two flavors: in case of a
    disabled wait (that is, disabled for interrupts), we exit to userspace. In case
    of an enabled wait we set up a timer that equals the cpu clock comparator value
    and sleep on a wait queue.
    
    [christian: change virtio interrupt to 0x2603]
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index a906499214bb..5e3473c9a639 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -20,6 +20,7 @@
 #include <linux/kvm_host.h>
 #include <linux/module.h>
 #include <linux/slab.h>
+#include <linux/timer.h>
 #include <asm/lowcore.h>
 #include <asm/pgtable.h>
 
@@ -34,6 +35,19 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "exit_stop_request", VCPU_STAT(exit_stop_request) },
 	{ "exit_external_request", VCPU_STAT(exit_external_request) },
 	{ "exit_external_interrupt", VCPU_STAT(exit_external_interrupt) },
+	{ "exit_instruction", VCPU_STAT(exit_instruction) },
+	{ "exit_program_interruption", VCPU_STAT(exit_program_interruption) },
+	{ "exit_instr_and_program_int", VCPU_STAT(exit_instr_and_program) },
+	{ "instruction_lctg", VCPU_STAT(instruction_lctg) },
+	{ "instruction_lctl", VCPU_STAT(instruction_lctl) },
+	{ "deliver_emergency_signal", VCPU_STAT(deliver_emergency_signal) },
+	{ "deliver_service_signal", VCPU_STAT(deliver_service_signal) },
+	{ "deliver_virtio_interrupt", VCPU_STAT(deliver_virtio_interrupt) },
+	{ "deliver_stop_signal", VCPU_STAT(deliver_stop_signal) },
+	{ "deliver_prefix_signal", VCPU_STAT(deliver_prefix_signal) },
+	{ "deliver_restart_signal", VCPU_STAT(deliver_restart_signal) },
+	{ "deliver_program_interruption", VCPU_STAT(deliver_program_int) },
+	{ "exit_wait_state", VCPU_STAT(exit_wait_state) },
 	{ NULL }
 };
 
@@ -106,6 +120,15 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	int r;
 
 	switch (ioctl) {
+	case KVM_S390_INTERRUPT: {
+		struct kvm_s390_interrupt s390int;
+
+		r = -EFAULT;
+		if (copy_from_user(&s390int, argp, sizeof(s390int)))
+			break;
+		r = kvm_s390_inject_vm(kvm, &s390int);
+		break;
+	}
 	default:
 		r = -EINVAL;
 	}
@@ -138,6 +161,9 @@ struct kvm *kvm_arch_create_vm(void)
 	if (!kvm->arch.dbf)
 		goto out_nodbf;
 
+	spin_lock_init(&kvm->arch.float_int.lock);
+	INIT_LIST_HEAD(&kvm->arch.float_int.list);
+
 	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 	VM_EVENT(kvm, 3, "%s", "vm created");
 
@@ -218,7 +244,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 	vcpu->arch.sie_block->gmsor = 0x000000000000;
 	vcpu->arch.sie_block->ecb   = 2;
 	vcpu->arch.sie_block->eca   = 0xC1002001U;
-
+	setup_timer(&vcpu->arch.ckc_timer, kvm_s390_idle_wakeup,
+		 (unsigned long) vcpu);
 	return 0;
 }
 
@@ -243,6 +270,14 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)kvm->arch.sca) >> 32);
 	vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
 
+	spin_lock_init(&vcpu->arch.local_int.lock);
+	INIT_LIST_HEAD(&vcpu->arch.local_int.list);
+	vcpu->arch.local_int.float_int = &kvm->arch.float_int;
+	spin_lock_bh(&kvm->arch.float_int.lock);
+	kvm->arch.float_int.local_int[id] = &vcpu->arch.local_int;
+	init_waitqueue_head(&vcpu->arch.local_int.wq);
+	spin_unlock_bh(&kvm->arch.float_int.lock);
+
 	rc = kvm_vcpu_init(vcpu, kvm, id);
 	if (rc)
 		goto out_free_cpu;
@@ -395,6 +430,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 
+	BUG_ON(vcpu->kvm->arch.float_int.local_int[vcpu->vcpu_id] == NULL);
+
 	switch (kvm_run->exit_reason) {
 	case KVM_EXIT_S390_SIEIC:
 		vcpu->arch.sie_block->gpsw.mask = kvm_run->s390_sieic.mask;
@@ -410,8 +447,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	might_sleep();
 
 	do {
+		kvm_s390_deliver_pending_interrupts(vcpu);
 		__vcpu_run(vcpu);
-
 		rc = kvm_handle_sie_intercept(vcpu);
 	} while (!signal_pending(current) && !rc);
 
@@ -538,6 +575,13 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 
 	switch (ioctl) {
+	case KVM_S390_INTERRUPT: {
+		struct kvm_s390_interrupt s390int;
+
+		if (copy_from_user(&s390int, argp, sizeof(s390int)))
+			return -EFAULT;
+		return kvm_s390_inject_vcpu(vcpu, &s390int);
+	}
 	case KVM_S390_STORE_STATUS:
 		return kvm_s390_vcpu_store_status(vcpu, arg);
 	case KVM_S390_SET_INITIAL_PSW: {

commit 8f2abe6a1e525e878bdf58f68ccd146d543fde84
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 25 18:47:23 2008 +0100

    KVM: s390: sie intercept handling
    
    This path introduces handling of sie intercepts in three flavors: Intercepts
    are either handled completely in-kernel by kvm_handle_sie_intercept(),
    or passed to userspace with corresponding data in struct kvm_run in case
    kvm_handle_sie_intercept() returns -ENOTSUPP.
    In case of partial execution in kernel with the need of userspace support,
    kvm_handle_sie_intercept() may choose to set up struct kvm_run and return
    -EREMOTE.
    
    The trivial intercept reasons are handled in this patch:
    handle_noop() just does nothing for intercepts that don't require our support
      at all
    handle_stop() is called when a cpu enters stopped state, and it drops out to
      userland after updating our vcpu state
    handle_validity() faults in the cpu lowcore if needed, or passes the request
      to userland
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 6e1e1d39ae15..a906499214bb 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -23,12 +23,17 @@
 #include <asm/lowcore.h>
 #include <asm/pgtable.h>
 
+#include "kvm-s390.h"
 #include "gaccess.h"
 
 #define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
 
 struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "userspace_handled", VCPU_STAT(exit_userspace) },
+	{ "exit_validity", VCPU_STAT(exit_validity) },
+	{ "exit_stop_request", VCPU_STAT(exit_stop_request) },
+	{ "exit_external_request", VCPU_STAT(exit_external_request) },
+	{ "exit_external_interrupt", VCPU_STAT(exit_external_interrupt) },
 	{ NULL }
 };
 
@@ -380,6 +385,7 @@ static void __vcpu_run(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
+	int rc;
 	sigset_t sigsaved;
 
 	vcpu_load(vcpu);
@@ -389,7 +395,45 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 
 	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
 
-	__vcpu_run(vcpu);
+	switch (kvm_run->exit_reason) {
+	case KVM_EXIT_S390_SIEIC:
+		vcpu->arch.sie_block->gpsw.mask = kvm_run->s390_sieic.mask;
+		vcpu->arch.sie_block->gpsw.addr = kvm_run->s390_sieic.addr;
+		break;
+	case KVM_EXIT_UNKNOWN:
+	case KVM_EXIT_S390_RESET:
+		break;
+	default:
+		BUG();
+	}
+
+	might_sleep();
+
+	do {
+		__vcpu_run(vcpu);
+
+		rc = kvm_handle_sie_intercept(vcpu);
+	} while (!signal_pending(current) && !rc);
+
+	if (signal_pending(current) && !rc)
+		rc = -EINTR;
+
+	if (rc == -ENOTSUPP) {
+		/* intercept cannot be handled in-kernel, prepare kvm-run */
+		kvm_run->exit_reason         = KVM_EXIT_S390_SIEIC;
+		kvm_run->s390_sieic.icptcode = vcpu->arch.sie_block->icptcode;
+		kvm_run->s390_sieic.mask     = vcpu->arch.sie_block->gpsw.mask;
+		kvm_run->s390_sieic.addr     = vcpu->arch.sie_block->gpsw.addr;
+		kvm_run->s390_sieic.ipa      = vcpu->arch.sie_block->ipa;
+		kvm_run->s390_sieic.ipb      = vcpu->arch.sie_block->ipb;
+		rc = 0;
+	}
+
+	if (rc == -EREMOTE) {
+		/* intercept was handled, but userspace support is needed
+		 * kvm_run has been prepared by the handler */
+		rc = 0;
+	}
 
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);

commit b0c632db637d68ad39d9f97f452ce176253f5f4e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Mar 25 18:47:20 2008 +0100

    KVM: s390: arch backend for the kvm kernel module
    
    This patch contains the port of Qumranet's kvm kernel module to IBM zSeries
     (aka s390x, mainframe) architecture. It uses the mainframe's virtualization
    instruction SIE to run virtual machines with up to 64 virtual CPUs each.
    This port is only usable on 64bit host kernels, and can only run 64bit guest
    kernels. However, running 31bit applications in guest userspace is possible.
    
    The following source files are introduced by this patch
    arch/s390/kvm/kvm-s390.c    similar to arch/x86/kvm/x86.c, this implements all
                                arch callbacks for kvm. __vcpu_run calls back into
                                sie64a to enter the guest machine context
    arch/s390/kvm/sie64a.S      assembler function sie64a, which enters guest
                                context via SIE, and switches world before and after                            that
    include/asm-s390/kvm_host.h contains all vital data structures needed to run
                                virtual machines on the mainframe
    include/asm-s390/kvm.h      defines kvm_regs and friends for user access to
                                guest register content
    arch/s390/kvm/gaccess.h     functions similar to uaccess to access guest memory
    arch/s390/kvm/kvm-s390.h    header file for kvm-s390 internals, extended by
                                later patches
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
new file mode 100644
index 000000000000..6e1e1d39ae15
--- /dev/null
+++ b/arch/s390/kvm/kvm-s390.c
@@ -0,0 +1,566 @@
+/*
+ * s390host.c --  hosting zSeries kernel virtual machines
+ *
+ * Copyright IBM Corp. 2008
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License (version 2 only)
+ * as published by the Free Software Foundation.
+ *
+ *    Author(s): Carsten Otte <cotte@de.ibm.com>
+ *               Christian Borntraeger <borntraeger@de.ibm.com>
+ *               Heiko Carstens <heiko.carstens@de.ibm.com>
+ */
+
+#include <linux/compiler.h>
+#include <linux/err.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <asm/lowcore.h>
+#include <asm/pgtable.h>
+
+#include "gaccess.h"
+
+#define VCPU_STAT(x) offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU
+
+struct kvm_stats_debugfs_item debugfs_entries[] = {
+	{ "userspace_handled", VCPU_STAT(exit_userspace) },
+	{ NULL }
+};
+
+
+/* Section: not file related */
+void kvm_arch_hardware_enable(void *garbage)
+{
+	/* every s390 is virtualization enabled ;-) */
+}
+
+void kvm_arch_hardware_disable(void *garbage)
+{
+}
+
+void decache_vcpus_on_cpu(int cpu)
+{
+}
+
+int kvm_arch_hardware_setup(void)
+{
+	return 0;
+}
+
+void kvm_arch_hardware_unsetup(void)
+{
+}
+
+void kvm_arch_check_processor_compat(void *rtn)
+{
+}
+
+int kvm_arch_init(void *opaque)
+{
+	return 0;
+}
+
+void kvm_arch_exit(void)
+{
+}
+
+/* Section: device related */
+long kvm_arch_dev_ioctl(struct file *filp,
+			unsigned int ioctl, unsigned long arg)
+{
+	if (ioctl == KVM_S390_ENABLE_SIE)
+		return s390_enable_sie();
+	return -EINVAL;
+}
+
+int kvm_dev_ioctl_check_extension(long ext)
+{
+	return 0;
+}
+
+/* Section: vm related */
+/*
+ * Get (and clear) the dirty memory log for a memory slot.
+ */
+int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
+			       struct kvm_dirty_log *log)
+{
+	return 0;
+}
+
+long kvm_arch_vm_ioctl(struct file *filp,
+		       unsigned int ioctl, unsigned long arg)
+{
+	struct kvm *kvm = filp->private_data;
+	void __user *argp = (void __user *)arg;
+	int r;
+
+	switch (ioctl) {
+	default:
+		r = -EINVAL;
+	}
+
+	return r;
+}
+
+struct kvm *kvm_arch_create_vm(void)
+{
+	struct kvm *kvm;
+	int rc;
+	char debug_name[16];
+
+	rc = s390_enable_sie();
+	if (rc)
+		goto out_nokvm;
+
+	rc = -ENOMEM;
+	kvm = kzalloc(sizeof(struct kvm), GFP_KERNEL);
+	if (!kvm)
+		goto out_nokvm;
+
+	kvm->arch.sca = (struct sca_block *) get_zeroed_page(GFP_KERNEL);
+	if (!kvm->arch.sca)
+		goto out_nosca;
+
+	sprintf(debug_name, "kvm-%u", current->pid);
+
+	kvm->arch.dbf = debug_register(debug_name, 8, 2, 8 * sizeof(long));
+	if (!kvm->arch.dbf)
+		goto out_nodbf;
+
+	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
+	VM_EVENT(kvm, 3, "%s", "vm created");
+
+	try_module_get(THIS_MODULE);
+
+	return kvm;
+out_nodbf:
+	free_page((unsigned long)(kvm->arch.sca));
+out_nosca:
+	kfree(kvm);
+out_nokvm:
+	return ERR_PTR(rc);
+}
+
+void kvm_arch_destroy_vm(struct kvm *kvm)
+{
+	debug_unregister(kvm->arch.dbf);
+	free_page((unsigned long)(kvm->arch.sca));
+	kfree(kvm);
+	module_put(THIS_MODULE);
+}
+
+/* Section: vcpu related */
+int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
+{
+	/* kvm common code refers to this, but does'nt call it */
+	BUG();
+}
+
+void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
+{
+	save_fp_regs(&vcpu->arch.host_fpregs);
+	save_access_regs(vcpu->arch.host_acrs);
+	vcpu->arch.guest_fpregs.fpc &= FPC_VALID_MASK;
+	restore_fp_regs(&vcpu->arch.guest_fpregs);
+	restore_access_regs(vcpu->arch.guest_acrs);
+
+	if (signal_pending(current))
+		atomic_set_mask(CPUSTAT_STOP_INT,
+			&vcpu->arch.sie_block->cpuflags);
+}
+
+void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
+{
+	save_fp_regs(&vcpu->arch.guest_fpregs);
+	save_access_regs(vcpu->arch.guest_acrs);
+	restore_fp_regs(&vcpu->arch.host_fpregs);
+	restore_access_regs(vcpu->arch.host_acrs);
+}
+
+static void kvm_s390_vcpu_initial_reset(struct kvm_vcpu *vcpu)
+{
+	/* this equals initial cpu reset in pop, but we don't switch to ESA */
+	vcpu->arch.sie_block->gpsw.mask = 0UL;
+	vcpu->arch.sie_block->gpsw.addr = 0UL;
+	vcpu->arch.sie_block->prefix    = 0UL;
+	vcpu->arch.sie_block->ihcpu     = 0xffff;
+	vcpu->arch.sie_block->cputm     = 0UL;
+	vcpu->arch.sie_block->ckc       = 0UL;
+	vcpu->arch.sie_block->todpr     = 0;
+	memset(vcpu->arch.sie_block->gcr, 0, 16 * sizeof(__u64));
+	vcpu->arch.sie_block->gcr[0]  = 0xE0UL;
+	vcpu->arch.sie_block->gcr[14] = 0xC2000000UL;
+	vcpu->arch.guest_fpregs.fpc = 0;
+	asm volatile("lfpc %0" : : "Q" (vcpu->arch.guest_fpregs.fpc));
+	vcpu->arch.sie_block->gbea = 1;
+}
+
+int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
+{
+	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH);
+	vcpu->arch.sie_block->gmslm = 0xffffffffffUL;
+	vcpu->arch.sie_block->gmsor = 0x000000000000;
+	vcpu->arch.sie_block->ecb   = 2;
+	vcpu->arch.sie_block->eca   = 0xC1002001U;
+
+	return 0;
+}
+
+struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
+				      unsigned int id)
+{
+	struct kvm_vcpu *vcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);
+	int rc = -ENOMEM;
+
+	if (!vcpu)
+		goto out_nomem;
+
+	vcpu->arch.sie_block = (struct sie_block *) get_zeroed_page(GFP_KERNEL);
+
+	if (!vcpu->arch.sie_block)
+		goto out_free_cpu;
+
+	vcpu->arch.sie_block->icpua = id;
+	BUG_ON(!kvm->arch.sca);
+	BUG_ON(kvm->arch.sca->cpu[id].sda);
+	kvm->arch.sca->cpu[id].sda = (__u64) vcpu->arch.sie_block;
+	vcpu->arch.sie_block->scaoh = (__u32)(((__u64)kvm->arch.sca) >> 32);
+	vcpu->arch.sie_block->scaol = (__u32)(__u64)kvm->arch.sca;
+
+	rc = kvm_vcpu_init(vcpu, kvm, id);
+	if (rc)
+		goto out_free_cpu;
+	VM_EVENT(kvm, 3, "create cpu %d at %p, sie block at %p", id, vcpu,
+		 vcpu->arch.sie_block);
+
+	try_module_get(THIS_MODULE);
+
+	return vcpu;
+out_free_cpu:
+	kfree(vcpu);
+out_nomem:
+	return ERR_PTR(rc);
+}
+
+void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
+{
+	VCPU_EVENT(vcpu, 3, "%s", "destroy cpu");
+	free_page((unsigned long)(vcpu->arch.sie_block));
+	kfree(vcpu);
+	module_put(THIS_MODULE);
+}
+
+int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
+{
+	/* kvm common code refers to this, but never calls it */
+	BUG();
+	return 0;
+}
+
+static int kvm_arch_vcpu_ioctl_initial_reset(struct kvm_vcpu *vcpu)
+{
+	vcpu_load(vcpu);
+	kvm_s390_vcpu_initial_reset(vcpu);
+	vcpu_put(vcpu);
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
+{
+	vcpu_load(vcpu);
+	memcpy(&vcpu->arch.guest_gprs, &regs->gprs, sizeof(regs->gprs));
+	vcpu_put(vcpu);
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
+{
+	vcpu_load(vcpu);
+	memcpy(&regs->gprs, &vcpu->arch.guest_gprs, sizeof(regs->gprs));
+	vcpu_put(vcpu);
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
+				  struct kvm_sregs *sregs)
+{
+	vcpu_load(vcpu);
+	memcpy(&vcpu->arch.guest_acrs, &sregs->acrs, sizeof(sregs->acrs));
+	memcpy(&vcpu->arch.sie_block->gcr, &sregs->crs, sizeof(sregs->crs));
+	vcpu_put(vcpu);
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
+				  struct kvm_sregs *sregs)
+{
+	vcpu_load(vcpu);
+	memcpy(&sregs->acrs, &vcpu->arch.guest_acrs, sizeof(sregs->acrs));
+	memcpy(&sregs->crs, &vcpu->arch.sie_block->gcr, sizeof(sregs->crs));
+	vcpu_put(vcpu);
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
+{
+	vcpu_load(vcpu);
+	memcpy(&vcpu->arch.guest_fpregs.fprs, &fpu->fprs, sizeof(fpu->fprs));
+	vcpu->arch.guest_fpregs.fpc = fpu->fpc;
+	vcpu_put(vcpu);
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
+{
+	vcpu_load(vcpu);
+	memcpy(&fpu->fprs, &vcpu->arch.guest_fpregs.fprs, sizeof(fpu->fprs));
+	fpu->fpc = vcpu->arch.guest_fpregs.fpc;
+	vcpu_put(vcpu);
+	return 0;
+}
+
+static int kvm_arch_vcpu_ioctl_set_initial_psw(struct kvm_vcpu *vcpu, psw_t psw)
+{
+	int rc = 0;
+
+	vcpu_load(vcpu);
+	if (atomic_read(&vcpu->arch.sie_block->cpuflags) & CPUSTAT_RUNNING)
+		rc = -EBUSY;
+	else
+		vcpu->arch.sie_block->gpsw = psw;
+	vcpu_put(vcpu);
+	return rc;
+}
+
+int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,
+				  struct kvm_translation *tr)
+{
+	return -EINVAL; /* not implemented yet */
+}
+
+int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
+				    struct kvm_debug_guest *dbg)
+{
+	return -EINVAL; /* not implemented yet */
+}
+
+static void __vcpu_run(struct kvm_vcpu *vcpu)
+{
+	memcpy(&vcpu->arch.sie_block->gg14, &vcpu->arch.guest_gprs[14], 16);
+
+	if (need_resched())
+		schedule();
+
+	vcpu->arch.sie_block->icptcode = 0;
+	local_irq_disable();
+	kvm_guest_enter();
+	local_irq_enable();
+	VCPU_EVENT(vcpu, 6, "entering sie flags %x",
+		   atomic_read(&vcpu->arch.sie_block->cpuflags));
+	sie64a(vcpu->arch.sie_block, vcpu->arch.guest_gprs);
+	VCPU_EVENT(vcpu, 6, "exit sie icptcode %d",
+		   vcpu->arch.sie_block->icptcode);
+	local_irq_disable();
+	kvm_guest_exit();
+	local_irq_enable();
+
+	memcpy(&vcpu->arch.guest_gprs[14], &vcpu->arch.sie_block->gg14, 16);
+}
+
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
+{
+	sigset_t sigsaved;
+
+	vcpu_load(vcpu);
+
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
+
+	atomic_set_mask(CPUSTAT_RUNNING, &vcpu->arch.sie_block->cpuflags);
+
+	__vcpu_run(vcpu);
+
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
+
+	vcpu_put(vcpu);
+
+	vcpu->stat.exit_userspace++;
+	return 0;
+}
+
+static int __guestcopy(struct kvm_vcpu *vcpu, u64 guestdest, const void *from,
+		       unsigned long n, int prefix)
+{
+	if (prefix)
+		return copy_to_guest(vcpu, guestdest, from, n);
+	else
+		return copy_to_guest_absolute(vcpu, guestdest, from, n);
+}
+
+/*
+ * store status at address
+ * we use have two special cases:
+ * KVM_S390_STORE_STATUS_NOADDR: -> 0x1200 on 64 bit
+ * KVM_S390_STORE_STATUS_PREFIXED: -> prefix
+ */
+int __kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
+{
+	const unsigned char archmode = 1;
+	int prefix;
+
+	if (addr == KVM_S390_STORE_STATUS_NOADDR) {
+		if (copy_to_guest_absolute(vcpu, 163ul, &archmode, 1))
+			return -EFAULT;
+		addr = SAVE_AREA_BASE;
+		prefix = 0;
+	} else if (addr == KVM_S390_STORE_STATUS_PREFIXED) {
+		if (copy_to_guest(vcpu, 163ul, &archmode, 1))
+			return -EFAULT;
+		addr = SAVE_AREA_BASE;
+		prefix = 1;
+	} else
+		prefix = 0;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, fp_regs),
+			vcpu->arch.guest_fpregs.fprs, 128, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, gp_regs),
+			vcpu->arch.guest_gprs, 128, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, psw),
+			&vcpu->arch.sie_block->gpsw, 16, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, pref_reg),
+			&vcpu->arch.sie_block->prefix, 4, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu,
+			addr + offsetof(struct save_area_s390x, fp_ctrl_reg),
+			&vcpu->arch.guest_fpregs.fpc, 4, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, tod_reg),
+			&vcpu->arch.sie_block->todpr, 4, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, timer),
+			&vcpu->arch.sie_block->cputm, 8, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, clk_cmp),
+			&vcpu->arch.sie_block->ckc, 8, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu, addr + offsetof(struct save_area_s390x, acc_regs),
+			&vcpu->arch.guest_acrs, 64, prefix))
+		return -EFAULT;
+
+	if (__guestcopy(vcpu,
+			addr + offsetof(struct save_area_s390x, ctrl_regs),
+			&vcpu->arch.sie_block->gcr, 128, prefix))
+		return -EFAULT;
+	return 0;
+}
+
+static int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu, unsigned long addr)
+{
+	int rc;
+
+	vcpu_load(vcpu);
+	rc = __kvm_s390_vcpu_store_status(vcpu, addr);
+	vcpu_put(vcpu);
+	return rc;
+}
+
+long kvm_arch_vcpu_ioctl(struct file *filp,
+			 unsigned int ioctl, unsigned long arg)
+{
+	struct kvm_vcpu *vcpu = filp->private_data;
+	void __user *argp = (void __user *)arg;
+
+	switch (ioctl) {
+	case KVM_S390_STORE_STATUS:
+		return kvm_s390_vcpu_store_status(vcpu, arg);
+	case KVM_S390_SET_INITIAL_PSW: {
+		psw_t psw;
+
+		if (copy_from_user(&psw, argp, sizeof(psw)))
+			return -EFAULT;
+		return kvm_arch_vcpu_ioctl_set_initial_psw(vcpu, psw);
+	}
+	case KVM_S390_INITIAL_RESET:
+		return kvm_arch_vcpu_ioctl_initial_reset(vcpu);
+	default:
+		;
+	}
+	return -EINVAL;
+}
+
+/* Section: memory related */
+int kvm_arch_set_memory_region(struct kvm *kvm,
+				struct kvm_userspace_memory_region *mem,
+				struct kvm_memory_slot old,
+				int user_alloc)
+{
+	/* A few sanity checks. We can have exactly one memory slot which has
+	   to start at guest virtual zero and which has to be located at a
+	   page boundary in userland and which has to end at a page boundary.
+	   The memory in userland is ok to be fragmented into various different
+	   vmas. It is okay to mmap() and munmap() stuff in this slot after
+	   doing this call at any time */
+
+	if (mem->slot)
+		return -EINVAL;
+
+	if (mem->guest_phys_addr)
+		return -EINVAL;
+
+	if (mem->userspace_addr & (PAGE_SIZE - 1))
+		return -EINVAL;
+
+	if (mem->memory_size & (PAGE_SIZE - 1))
+		return -EINVAL;
+
+	kvm->arch.guest_origin = mem->userspace_addr;
+	kvm->arch.guest_memsize = mem->memory_size;
+
+	/* FIXME: we do want to interrupt running CPUs and update their memory
+	   configuration now to avoid race conditions. But hey, changing the
+	   memory layout while virtual CPUs are running is usually bad
+	   programming practice. */
+
+	return 0;
+}
+
+gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
+{
+	return gfn;
+}
+
+static int __init kvm_s390_init(void)
+{
+	return kvm_init(NULL, sizeof(struct kvm_vcpu), THIS_MODULE);
+}
+
+static void __exit kvm_s390_exit(void)
+{
+	kvm_exit();
+}
+
+module_init(kvm_s390_init);
+module_exit(kvm_s390_exit);
