commit 6137fed0823247e32306bde2b48cac627c24f894
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 4 17:04:07 2018 +0200

    arch/tlb: Clean up simple architectures
    
    For the architectures that do not implement their own tlb_flush() but
    do already use the generic mmu_gather, there are two options:
    
     1) the platform has an efficient flush_tlb_range() and
        asm-generic/tlb.h doesn't need any overrides at all.
    
     2) the platform lacks an efficient flush_tlb_range() and
        we select MMU_GATHER_NO_RANGE to minimize full invalidates.
    
    Convert all 'simple' architectures to one of these two forms.
    
    alpha:      has no range invalidate -> 2
    arc:        already used flush_tlb_range() -> 1
    c6x:        has no range invalidate -> 2
    hexagon:    has an efficient flush_tlb_range() -> 1
                (flush_tlb_mm() is in fact a full range invalidate,
                 so no need to shoot down everything)
    m68k:       has inefficient flush_tlb_range() -> 2
    microblaze: has no flush_tlb_range() -> 2
    mips:       has efficient flush_tlb_range() -> 1
                (even though it currently seems to use flush_tlb_mm())
    nds32:      already uses flush_tlb_range() -> 1
    nios2:      has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    openrisc:   has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    parisc:     already uses flush_tlb_range() -> 1
    sparc32:    already uses flush_tlb_range() -> 1
    unicore32:  has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    xtensa:     has efficient flush_tlb_range() -> 1
    
    Note this also fixes a bug in the existing code for a number
    platforms. Those platforms that did:
    
      tlb_end_vma() -> if (!full_mm) flush_tlb_*()
      tlb_flush -> if (full_mm) flush_tlb_mm()
    
    missed the case of shift_arg_pages(), which doesn't have @fullmm set,
    nor calls into tlb_*vma(), but still frees page-tables and thus needs
    an invalidate. The new code handles this by detecting a non-empty
    range, and either issuing the matching range invalidate or a full
    invalidate, depending on the capabilities.
    
    No change in behavior intended.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Piggin <npiggin@gmail.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/xtensa/include/asm/tlb.h b/arch/xtensa/include/asm/tlb.h
index 1a93e350382e..50889935138a 100644
--- a/arch/xtensa/include/asm/tlb.h
+++ b/arch/xtensa/include/asm/tlb.h
@@ -14,23 +14,6 @@
 #include <asm/cache.h>
 #include <asm/page.h>
 
-#if (DCACHE_WAY_SIZE <= PAGE_SIZE)
-
-# define tlb_end_vma(tlb,vma)			do { } while (0)
-
-#else
-
-# define tlb_end_vma(tlb, vma)						      \
-	do {								      \
-		if (!tlb->fullmm)					      \
-			flush_tlb_range(vma, vma->vm_start, vma->vm_end);     \
-	} while(0)
-
-#endif
-
-#define __tlb_remove_tlb_entry(tlb,pte,addr)	do { } while (0)
-#define tlb_flush(tlb)				flush_tlb_mm((tlb)->mm)
-
 #include <asm-generic/tlb.h>
 
 #define __pte_free_tlb(tlb, pte, address)	pte_free((tlb)->mm, pte)

commit e7fd28a706bfaf9cd65dccf18140187f7ad04839
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 27 13:00:17 2018 +0200

    asm-generic/tlb, arch: Provide generic VIPT cache flush
    
    The one obvious thing SH and ARM want is a sensible default for
    tlb_start_vma(). (also: https://lkml.org/lkml/2004/1/15/6 )
    
    Avoid all VIPT architectures providing their own tlb_start_vma()
    implementation and rely on architectures to provide a no-op
    flush_cache_range() when it is not relevant.
    
    This patch makes tlb_start_vma() default to flush_cache_range(), which
    should be right and sufficient. The only exceptions that I found where
    (oddly):
    
      - m68k-mmu
      - sparc64
      - unicore
    
    Those architectures appear to have flush_cache_range(), but their
    current tlb_start_vma() does not call it.
    
    No change in behavior intended.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nick Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/xtensa/include/asm/tlb.h b/arch/xtensa/include/asm/tlb.h
index 0d766f9c1083..1a93e350382e 100644
--- a/arch/xtensa/include/asm/tlb.h
+++ b/arch/xtensa/include/asm/tlb.h
@@ -16,19 +16,10 @@
 
 #if (DCACHE_WAY_SIZE <= PAGE_SIZE)
 
-/* Note, read http://lkml.org/lkml/2004/1/15/6 */
-
-# define tlb_start_vma(tlb,vma)			do { } while (0)
 # define tlb_end_vma(tlb,vma)			do { } while (0)
 
 #else
 
-# define tlb_start_vma(tlb, vma)					      \
-	do {								      \
-		if (!tlb->fullmm)					      \
-			flush_cache_range(vma, vma->vm_start, vma->vm_end);   \
-	} while(0)
-
 # define tlb_end_vma(tlb, vma)						      \
 	do {								      \
 		if (!tlb->fullmm)					      \

commit 9e1b32caa525cb236e80e9c671e179bcecccc657
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jul 22 15:44:28 2009 +1000

    mm: Pass virtual address to [__]p{te,ud,md}_free_tlb()
    
    mm: Pass virtual address to [__]p{te,ud,md}_free_tlb()
    
    Upcoming paches to support the new 64-bit "BookE" powerpc architecture
    will need to have the virtual address corresponding to PTE page when
    freeing it, due to the way the HW table walker works.
    
    Basically, the TLB can be loaded with "large" pages that cover the whole
    virtual space (well, sort-of, half of it actually) represented by a PTE
    page, and which contain an "indirect" bit indicating that this TLB entry
    RPN points to an array of PTEs from which the TLB can then create direct
    entries. Thus, in order to invalidate those when PTE pages are deleted,
    we need the virtual address to pass to tlbilx or tlbivax instructions.
    
    The old trick of sticking it somewhere in the PTE page struct page sucks
    too much, the address is almost readily available in all call sites and
    almost everybody implemets these as macros, so we may as well add the
    argument everywhere. I added it to the pmd and pud variants for consistency.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: David Howells <dhowells@redhat.com> [MN10300 & FRV]
    Acked-by: Nick Piggin <npiggin@suse.de>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com> [s390]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/xtensa/include/asm/tlb.h b/arch/xtensa/include/asm/tlb.h
index 31c220faca02..0d766f9c1083 100644
--- a/arch/xtensa/include/asm/tlb.h
+++ b/arch/xtensa/include/asm/tlb.h
@@ -42,6 +42,6 @@
 
 #include <asm-generic/tlb.h>
 
-#define __pte_free_tlb(tlb, pte)		pte_free((tlb)->mm, pte)
+#define __pte_free_tlb(tlb, pte, address)	pte_free((tlb)->mm, pte)
 
 #endif	/* _XTENSA_TLB_H */

commit 367b8112fe2ea5c39a7bb4d263dcdd9b612fae18
Author: Chris Zankel <chris@zankel.net>
Date:   Thu Nov 6 06:40:46 2008 -0800

    xtensa: move headers files to arch/xtensa/include
    
    Move all header files for xtensa to arch/xtensa/include and platform and
    variant header files to the appropriate arch/xtensa/platforms/ and
    arch/xtensa/variants/ directories.
    
    Moving the files gets also rid of all uses of symlinks in the Makefile.
    
    This has been completed already for the majority of the architectures
    and xtensa is one out of six missing.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Chris Zankel <chris@zankel.net>

diff --git a/arch/xtensa/include/asm/tlb.h b/arch/xtensa/include/asm/tlb.h
new file mode 100644
index 000000000000..31c220faca02
--- /dev/null
+++ b/arch/xtensa/include/asm/tlb.h
@@ -0,0 +1,47 @@
+/*
+ * include/asm-xtensa/tlb.h
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2001 - 2005 Tensilica Inc.
+ */
+
+#ifndef _XTENSA_TLB_H
+#define _XTENSA_TLB_H
+
+#include <asm/cache.h>
+#include <asm/page.h>
+
+#if (DCACHE_WAY_SIZE <= PAGE_SIZE)
+
+/* Note, read http://lkml.org/lkml/2004/1/15/6 */
+
+# define tlb_start_vma(tlb,vma)			do { } while (0)
+# define tlb_end_vma(tlb,vma)			do { } while (0)
+
+#else
+
+# define tlb_start_vma(tlb, vma)					      \
+	do {								      \
+		if (!tlb->fullmm)					      \
+			flush_cache_range(vma, vma->vm_start, vma->vm_end);   \
+	} while(0)
+
+# define tlb_end_vma(tlb, vma)						      \
+	do {								      \
+		if (!tlb->fullmm)					      \
+			flush_tlb_range(vma, vma->vm_start, vma->vm_end);     \
+	} while(0)
+
+#endif
+
+#define __tlb_remove_tlb_entry(tlb,pte,addr)	do { } while (0)
+#define tlb_flush(tlb)				flush_tlb_mm((tlb)->mm)
+
+#include <asm-generic/tlb.h>
+
+#define __pte_free_tlb(tlb, pte)		pte_free((tlb)->mm, pte)
+
+#endif	/* _XTENSA_TLB_H */
