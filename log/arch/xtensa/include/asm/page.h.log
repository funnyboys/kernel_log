commit c62da0c35d58518ddb26ff641d2485596567fd96
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:05 2020 -0700

    mm/vma: define a default value for VM_DATA_DEFAULT_FLAGS
    
    There are many platforms with exact same value for VM_DATA_DEFAULT_FLAGS
    This creates a default value for VM_DATA_DEFAULT_FLAGS in line with the
    existing VM_STACK_DEFAULT_FLAGS.  While here, also define some more
    macros with standard VMA access flag combinations that are used
    frequently across many platforms.  Apart from simplification, this
    reduces code duplication as well.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Chris Zankel <chris@zankel.net>
    Link: http://lkml.kernel.org/r/1583391014-8170-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index f4771c29c7e9..37ce25ef92d6 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -203,8 +203,5 @@ static inline unsigned long ___pa(unsigned long va)
 
 #endif /* __ASSEMBLY__ */
 
-#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
-
 #include <asm-generic/memory_model.h>
 #endif /* _XTENSA_PAGE_H */

commit 7af710d988775aadf440222ecbe0c10eecf3eb54
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Tue Jan 3 17:57:51 2017 -0800

    xtensa: add XIP kernel support
    
    XIP (eXecute In Place) kernel image is the image that can be run
    directly from ROM, using RAM only for writable data.
    
    XIP xtensa kernel differs from regular xtensa kernel in the following
    ways:
    - it has exception/IRQ vectors merged into text section. No vectors
      relocation takes place at kernel startup.
    - .data/.bss location must be specified in the kernel configuration,
      its content is copied there in the _startup function.
    - .init.text is merged with the rest of text and is executed from ROM.
    - when MMU is used the virtual address where the kernel will be mapped
      must be specified in the kernel configuration. It may be in the KSEG
      or in the KIO, __pa macro is adjusted to be able to handle both.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 09c56cba442e..f4771c29c7e9 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -169,7 +169,18 @@ static inline unsigned long ___pa(unsigned long va)
 	if (off >= XCHAL_KSEG_SIZE)
 		off -= XCHAL_KSEG_SIZE;
 
+#ifndef CONFIG_XIP_KERNEL
 	return off + PHYS_OFFSET;
+#else
+	if (off < XCHAL_KSEG_SIZE)
+		return off + PHYS_OFFSET;
+
+	off -= XCHAL_KSEG_SIZE;
+	if (off >= XCHAL_KIO_SIZE)
+		off -= XCHAL_KIO_SIZE;
+
+	return off + XCHAL_KIO_PADDR;
+#endif
 }
 #define __pa(x)	___pa((unsigned long)(x))
 #else

commit aea731c81f998af5e45654459bac24a1c808fb22
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Mon Aug 13 16:45:54 2018 -0700

    xtensa: rework {CONFIG,PLATFORM}_DEFAULT_MEM_START
    
    Drop PLATFORM_DEFAULT_MEM_START from the platform/hardware.h headers.
    Provide definition of CONFIG_DEFAULT_MEM_START always, allow changing it
    only in noMMU configurations when PLATFORM_WANT_DEFAULT_MEM is selected.
    Change prompt and description so that it's clear that it controls
    PAGE_OFFSET and PHYS_OFFSET.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 5d69c11c01b8..09c56cba442e 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -14,7 +14,6 @@
 #include <asm/processor.h>
 #include <asm/types.h>
 #include <asm/cache.h>
-#include <platform/hardware.h>
 #include <asm/kmem_layout.h>
 
 /*
@@ -31,8 +30,8 @@
 #define MAX_LOW_PFN	(PHYS_PFN(XCHAL_KSEG_PADDR) + \
 			 PHYS_PFN(XCHAL_KSEG_SIZE))
 #else
-#define PAGE_OFFSET	PLATFORM_DEFAULT_MEM_START
-#define PHYS_OFFSET	PLATFORM_DEFAULT_MEM_START
+#define PAGE_OFFSET	_AC(CONFIG_DEFAULT_MEM_START, UL)
+#define PHYS_OFFSET	_AC(CONFIG_DEFAULT_MEM_START, UL)
 #define MAX_LOW_PFN	PHYS_PFN(0xfffffffful)
 #endif
 

commit 1af1e8a39dc0fab5e50f10462c636da8c1e0cfbb
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Sun Dec 3 19:09:41 2017 -0800

    xtensa: move fixmap and kmap just above the KSEG
    
    The virtual address space between the page table and the VMALLOC region
    is big enough to host KASAN shadow map and there's enough space between
    the VMALLOC area and KSEG for the fixmap and kmap.
    Move fixmap and kmap to the gap between VMALLOC area and KSEG, just
    above the KSEG. Reorder entries in the kernel memory layout printing
    code. Drop duplicate PGTABLE_START definition, use
    XCHAL_PAGE_TABLE_VADDR instead.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 4ddbfd57a7c8..5d69c11c01b8 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -36,8 +36,6 @@
 #define MAX_LOW_PFN	PHYS_PFN(0xfffffffful)
 #endif
 
-#define PGTABLE_START	0x80000000
-
 /*
  * Cache aliasing:
  *

commit 2b83878dd74a7c73bedcb6600663c1c46836e8af
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Wed Mar 29 15:44:47 2017 -0700

    xtensa: make __pa work with uncached KSEG addresses
    
    When __pa is applied to virtual address in uncached KSEG region the
    result is incorrect. Fix it by checking if the original address is in
    the uncached KSEG and adjusting the result. It looks better than masking
    off bits because pfn_valid would correctly work with new __pa results
    and it may be made working in noMMU case, once we get definition for
    uncached memory view.
    
    This is required for the dma_common_mmap and DMA debug code to work
    correctly: they both indirectly use __pa with coherent DMA addresses.
    In case of DMA debug the visible effect is false reports that an address
    mapped for DMA is accessed by CPU.
    
    Cc: stable@vger.kernel.org
    Tested-by: Boris Brezillon <boris.brezillon@free-electrons.com>
    Reviewed-by: Boris Brezillon <boris.brezillon@free-electrons.com>
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 976b1d70edbc..4ddbfd57a7c8 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -164,8 +164,21 @@ void copy_user_highpage(struct page *to, struct page *from,
 
 #define ARCH_PFN_OFFSET		(PHYS_OFFSET >> PAGE_SHIFT)
 
+#ifdef CONFIG_MMU
+static inline unsigned long ___pa(unsigned long va)
+{
+	unsigned long off = va - PAGE_OFFSET;
+
+	if (off >= XCHAL_KSEG_SIZE)
+		off -= XCHAL_KSEG_SIZE;
+
+	return off + PHYS_OFFSET;
+}
+#define __pa(x)	___pa((unsigned long)(x))
+#else
 #define __pa(x)	\
 	((unsigned long) (x) - PAGE_OFFSET + PHYS_OFFSET)
+#endif
 #define __va(x)	\
 	((void *)((unsigned long) (x) - PHYS_OFFSET + PAGE_OFFSET))
 #define pfn_valid(pfn) \

commit 3de00482b006daa110151ac6775adc52538a3d6a
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Sat Jul 23 02:47:58 2016 +0300

    xtensa: minimize use of PLATFORM_DEFAULT_MEM_{ADDR,SIZE}
    
    Now that the kernel load address and KSEG physical base address have
    their own Kconfig symbols PLATFORM_DEFAULT_MEM seems redundant. It makes
    little sense to use it in MMU configurations instead of KSEG_PADDR.
    In noMMU configurations there's no explicit KSEG, so it's still useful
    for the early cache initialization and definition of ARCH_PFN_OFFSET,
    which affects mem_map size.
    
    - limit it to noMMU; MMU variants have XCHAL_KSEG_PADDR and
      XCHAL_KSEG_SIZE;
    - don't use it to define TASK_SIZE or MAX_LOW_PFN: first doesn't make
      any difference in noMMU, second is meaningless as there's no high
      memory;
    - don't add default physical memory region: memory layout should come
      from the DT, bootloader tags, or memmap= command line parameter.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 3b5a49dbf8b2..976b1d70edbc 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -31,10 +31,9 @@
 #define MAX_LOW_PFN	(PHYS_PFN(XCHAL_KSEG_PADDR) + \
 			 PHYS_PFN(XCHAL_KSEG_SIZE))
 #else
-#define PAGE_OFFSET	__XTENSA_UL_CONST(0)
-#define PHYS_OFFSET	__XTENSA_UL_CONST(0)
-#define MAX_LOW_PFN	(PHYS_PFN(PLATFORM_DEFAULT_MEM_START) + \
-			 PHYS_PFN(PLATFORM_DEFAULT_MEM_SIZE))
+#define PAGE_OFFSET	PLATFORM_DEFAULT_MEM_START
+#define PHYS_OFFSET	PLATFORM_DEFAULT_MEM_START
+#define MAX_LOW_PFN	PHYS_PFN(0xfffffffful)
 #endif
 
 #define PGTABLE_START	0x80000000
@@ -163,7 +162,7 @@ void copy_user_highpage(struct page *to, struct page *from,
  * addresses.
  */
 
-#define ARCH_PFN_OFFSET		(PLATFORM_DEFAULT_MEM_START >> PAGE_SHIFT)
+#define ARCH_PFN_OFFSET		(PHYS_OFFSET >> PAGE_SHIFT)
 
 #define __pa(x)	\
 	((unsigned long) (x) - PAGE_OFFSET + PHYS_OFFSET)

commit a9f2fc628e3a26a829fd79aff74eb49839d1e74b
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Wed Apr 13 05:20:02 2016 +0300

    xtensa: cleanup MMU setup and kernel layout macros
    
    Make kernel load address explicit, independent of the selected MMU
    configuration and configurable from Kconfig. Do not restrict it to the
    first 512MB of the physical address space.
    
    Cleanup kernel memory layout macros:
    
    - rename VECBASE_RESET_VADDR to VECBASE_VADDR, XC_VADDR to VECTOR_VADDR;
    - drop VIRTUAL_MEMORY_ADDRESS and LOAD_MEMORY_ADDRESS;
    - introduce PHYS_OFFSET and use it in __va and __pa definitions;
    - synchronize MMU/noMMU vectors, drop unused NMI vector;
    - replace hardcoded vectors offset of 0x3000 with Kconfig symbol.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 8a02438232c4..3b5a49dbf8b2 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -27,10 +27,12 @@
 
 #ifdef CONFIG_MMU
 #define PAGE_OFFSET	XCHAL_KSEG_CACHED_VADDR
+#define PHYS_OFFSET	XCHAL_KSEG_PADDR
 #define MAX_LOW_PFN	(PHYS_PFN(XCHAL_KSEG_PADDR) + \
 			 PHYS_PFN(XCHAL_KSEG_SIZE))
 #else
 #define PAGE_OFFSET	__XTENSA_UL_CONST(0)
+#define PHYS_OFFSET	__XTENSA_UL_CONST(0)
 #define MAX_LOW_PFN	(PHYS_PFN(PLATFORM_DEFAULT_MEM_START) + \
 			 PHYS_PFN(PLATFORM_DEFAULT_MEM_SIZE))
 #endif
@@ -163,8 +165,10 @@ void copy_user_highpage(struct page *to, struct page *from,
 
 #define ARCH_PFN_OFFSET		(PLATFORM_DEFAULT_MEM_START >> PAGE_SHIFT)
 
-#define __pa(x)			((unsigned long) (x) - PAGE_OFFSET)
-#define __va(x)			((void *)((unsigned long) (x) + PAGE_OFFSET))
+#define __pa(x)	\
+	((unsigned long) (x) - PAGE_OFFSET + PHYS_OFFSET)
+#define __va(x)	\
+	((void *)((unsigned long) (x) - PHYS_OFFSET + PAGE_OFFSET))
 #define pfn_valid(pfn) \
 	((pfn) >= ARCH_PFN_OFFSET && ((pfn) - ARCH_PFN_OFFSET) < max_mapnr)
 

commit d39af90265feb40ec198c4ca8268724645b4b50e
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Mon Apr 11 21:14:17 2016 +0300

    xtensa: add alternative kernel memory layouts
    
    MMUv3 is able to support low memory bigger than 128MB.
    Implement 256MB and 512MB KSEG layouts:
    
    - add Kconfig selector for KSEG layout;
    - add KSEG base address, size and alignment definitions to
      arch/xtensa/include/asm/kmem_layout.h;
    - use new definitions in TLB initialization;
    - add build time memory map consistency checks.
    
    See Documentation/xtensa/mmu.txt for the details of new memory layouts.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index fd12a1977ba8..8a02438232c4 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -27,10 +27,12 @@
 
 #ifdef CONFIG_MMU
 #define PAGE_OFFSET	XCHAL_KSEG_CACHED_VADDR
-#define MAX_MEM_PFN	XCHAL_KSEG_SIZE
+#define MAX_LOW_PFN	(PHYS_PFN(XCHAL_KSEG_PADDR) + \
+			 PHYS_PFN(XCHAL_KSEG_SIZE))
 #else
 #define PAGE_OFFSET	__XTENSA_UL_CONST(0)
-#define MAX_MEM_PFN	(PLATFORM_DEFAULT_MEM_START + PLATFORM_DEFAULT_MEM_SIZE)
+#define MAX_LOW_PFN	(PHYS_PFN(PLATFORM_DEFAULT_MEM_START) + \
+			 PHYS_PFN(PLATFORM_DEFAULT_MEM_SIZE))
 #endif
 
 #define PGTABLE_START	0x80000000

commit f1883aa7d63e3be92ad18da7a1bfc6c9b15c4f9a
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Mon Apr 11 21:07:30 2016 +0300

    xtensa: move kernel mapping addresses into kmem_layout.h
    
    Create a header dedicated to memory layout definitions. Include it from
    places where these definitions are needed.
    Express vmalloc area address, VIRTUAL_MEMORY_ADDRESS and KERNELOFFSET
    through KSEG address.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index ad38500471fa..fd12a1977ba8 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -15,15 +15,7 @@
 #include <asm/types.h>
 #include <asm/cache.h>
 #include <platform/hardware.h>
-
-/*
- * Fixed TLB translations in the processor.
- */
-
-#define XCHAL_KSEG_CACHED_VADDR __XTENSA_UL_CONST(0xd0000000)
-#define XCHAL_KSEG_BYPASS_VADDR __XTENSA_UL_CONST(0xd8000000)
-#define XCHAL_KSEG_PADDR        __XTENSA_UL_CONST(0x00000000)
-#define XCHAL_KSEG_SIZE         __XTENSA_UL_CONST(0x08000000)
+#include <asm/kmem_layout.h>
 
 /*
  * PAGE_SHIFT determines the page size

commit 5a0b1d78bfc5ca4079ea03abb0ecc0d61d676e41
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Sat Oct 4 03:50:33 2014 +0400

    xtensa: nommu: clean up memory map dump
    
    noMMU configuration doesn't use special area for vmalloc allocations,
    don't print it in the memory map.
    PAGE_OFFSET is fixed to 0 in noMMU, use min_low_pfn and max_low_pfn for
    lowmem range display.
    Make all XCHAL_KSEG_* constants unsigned long for consistency with other
    addresses.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 619a51bb0163..ad38500471fa 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -20,10 +20,10 @@
  * Fixed TLB translations in the processor.
  */
 
-#define XCHAL_KSEG_CACHED_VADDR 0xd0000000
-#define XCHAL_KSEG_BYPASS_VADDR 0xd8000000
-#define XCHAL_KSEG_PADDR        0x00000000
-#define XCHAL_KSEG_SIZE         0x08000000
+#define XCHAL_KSEG_CACHED_VADDR __XTENSA_UL_CONST(0xd0000000)
+#define XCHAL_KSEG_BYPASS_VADDR __XTENSA_UL_CONST(0xd8000000)
+#define XCHAL_KSEG_PADDR        __XTENSA_UL_CONST(0x00000000)
+#define XCHAL_KSEG_SIZE         __XTENSA_UL_CONST(0x08000000)
 
 /*
  * PAGE_SHIFT determines the page size
@@ -37,7 +37,7 @@
 #define PAGE_OFFSET	XCHAL_KSEG_CACHED_VADDR
 #define MAX_MEM_PFN	XCHAL_KSEG_SIZE
 #else
-#define PAGE_OFFSET	0
+#define PAGE_OFFSET	__XTENSA_UL_CONST(0)
 #define MAX_MEM_PFN	(PLATFORM_DEFAULT_MEM_START + PLATFORM_DEFAULT_MEM_SIZE)
 #endif
 

commit b6cee17b7d5999ae5f9ea51643dc6ea6c3e4efd4
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Mon Sep 22 09:54:42 2014 +0400

    xtensa: nommu: don't build most of the cache flushing code
    
    Most cache flushing code is only relevant for MMU. Don't build it for
    nommu configuration.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index abe24c6f8b2f..619a51bb0163 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -145,7 +145,7 @@ extern void copy_page(void *to, void *from);
  * some extra work
  */
 
-#if DCACHE_WAY_SIZE > PAGE_SIZE
+#if defined(CONFIG_MMU) && DCACHE_WAY_SIZE > PAGE_SIZE
 extern void clear_page_alias(void *vaddr, unsigned long paddr);
 extern void copy_page_alias(void *to, void *from,
 			    unsigned long to_paddr, unsigned long from_paddr);

commit 32544d9c10c42bac3be8b87d2fc95b0aef008795
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Tue Jul 15 02:51:49 2014 +0400

    xtensa: support aliasing cache in k[un]map_atomic
    
    Map high memory pages at virtual addresses with color that match color
    of their physical address. Existing cache alias management mechanisms
    may be used with such pages.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 11721ccd7f23..abe24c6f8b2f 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -78,7 +78,9 @@
 # define DCACHE_ALIAS_EQ(a,b)	((((a) ^ (b)) & DCACHE_ALIAS_MASK) == 0)
 #else
 # define DCACHE_ALIAS_ORDER	0
+# define DCACHE_ALIAS(a)	((void)(a), 0)
 #endif
+#define DCACHE_N_COLORS		(1 << DCACHE_ALIAS_ORDER)
 
 #if ICACHE_WAY_SIZE > PAGE_SIZE
 # define ICACHE_ALIAS_ORDER	(ICACHE_WAY_SHIFT - PAGE_SHIFT)

commit a91902db2990909ea5e6b110811b448f2e8f1571
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Mon Jul 21 18:54:11 2014 +0400

    xtensa: implement clear_user_highpage and copy_user_highpage
    
    Existing clear_user_page and copy_user_page cannot be used with highmem
    because they calculate physical page address from its virtual address
    and do it incorrectly in case of high memory page mapped with
    kmap_atomic. Also kmap is not needed, as most likely userspace mapping
    color would be different from the kmapped color.
    
    Provide clear_user_highpage and copy_user_highpage functions that
    determine if temporary mapping is needed for the pages. Move most of the
    logic of the former clear_user_page and copy_user_page to
    xtensa/mm/cache.c only leaving temporary mapping setup, invalidation and
    clearing/copying in the xtensa/mm/misc.S. Rename these functions to
    clear_page_alias and copy_page_alias.
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 47f582333f6b..11721ccd7f23 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -134,6 +134,7 @@ static inline __attribute_const__ int get_order(unsigned long size)
 #endif
 
 struct page;
+struct vm_area_struct;
 extern void clear_page(void *page);
 extern void copy_page(void *to, void *from);
 
@@ -143,8 +144,15 @@ extern void copy_page(void *to, void *from);
  */
 
 #if DCACHE_WAY_SIZE > PAGE_SIZE
-extern void clear_user_page(void*, unsigned long, struct page*);
-extern void copy_user_page(void*, void*, unsigned long, struct page*);
+extern void clear_page_alias(void *vaddr, unsigned long paddr);
+extern void copy_page_alias(void *to, void *from,
+			    unsigned long to_paddr, unsigned long from_paddr);
+
+#define clear_user_highpage clear_user_highpage
+void clear_user_highpage(struct page *page, unsigned long vaddr);
+#define __HAVE_ARCH_COPY_USER_HIGHPAGE
+void copy_user_highpage(struct page *to, struct page *from,
+			unsigned long vaddr, struct vm_area_struct *vma);
 #else
 # define clear_user_page(page, vaddr, pg)	clear_page(page)
 # define copy_user_page(to, from, vaddr, pg)	copy_page(to, from)

commit c4c4594b005d89b56964071bbbdeb07daac5bc76
Author: Chris Zankel <chris@zankel.net>
Date:   Wed Nov 28 16:53:51 2012 -0800

    xtensa: clean up files to make them code-style compliant
    
    Remove heading and trailing spaces, trim trailing lines, and wrap lines
    that are longer than 80 characters.
    
    Signed-off-by: Chris Zankel <chris@zankel.net>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 7a5591a71f85..47f582333f6b 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -29,19 +29,19 @@
  * PAGE_SHIFT determines the page size
  */
 
-#define PAGE_SHIFT		12
-#define PAGE_SIZE		(__XTENSA_UL_CONST(1) << PAGE_SHIFT)
-#define PAGE_MASK		(~(PAGE_SIZE-1))
+#define PAGE_SHIFT	12
+#define PAGE_SIZE	(__XTENSA_UL_CONST(1) << PAGE_SHIFT)
+#define PAGE_MASK	(~(PAGE_SIZE-1))
 
 #ifdef CONFIG_MMU
-#define PAGE_OFFSET		XCHAL_KSEG_CACHED_VADDR
-#define MAX_MEM_PFN		XCHAL_KSEG_SIZE
+#define PAGE_OFFSET	XCHAL_KSEG_CACHED_VADDR
+#define MAX_MEM_PFN	XCHAL_KSEG_SIZE
 #else
-#define PAGE_OFFSET		0
-#define MAX_MEM_PFN		(PLATFORM_DEFAULT_MEM_START + PLATFORM_DEFAULT_MEM_SIZE)
+#define PAGE_OFFSET	0
+#define MAX_MEM_PFN	(PLATFORM_DEFAULT_MEM_START + PLATFORM_DEFAULT_MEM_SIZE)
 #endif
 
-#define PGTABLE_START		0x80000000
+#define PGTABLE_START	0x80000000
 
 /*
  * Cache aliasing:
@@ -161,7 +161,9 @@ extern void copy_user_page(void*, void*, unsigned long, struct page*);
 
 #define __pa(x)			((unsigned long) (x) - PAGE_OFFSET)
 #define __va(x)			((void *)((unsigned long) (x) + PAGE_OFFSET))
-#define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && ((pfn) - ARCH_PFN_OFFSET) < max_mapnr)
+#define pfn_valid(pfn) \
+	((pfn) >= ARCH_PFN_OFFSET && ((pfn) - ARCH_PFN_OFFSET) < max_mapnr)
+
 #ifdef CONFIG_DISCONTIGMEM
 # error CONFIG_DISCONTIGMEM not supported
 #endif

commit 851cc856d73d1185243c149ed0c0839df8a1b2fe
Author: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
Date:   Tue May 24 17:11:15 2011 -0700

    xtensa/mm: remove WANT_PAGE_VIRTUAL
    
    This is not useful: it provides page->virtual and is used with highmem.
    xtensa has no support for highmem and those HIGHMEM bits which are found
    by grep are partly implemented.  The interesting functions like kmap() are
    missing.  If someone actually implements the complete HIGHMEM support he
    could use HASHED_PAGE_VIRTUAL like most others do.
    
    Signed-off-by: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 161bb89e98c8..7a5591a71f85 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -171,10 +171,6 @@ extern void copy_user_page(void*, void*, unsigned long, struct page*);
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
 
-#ifdef CONFIG_MMU
-#define WANT_PAGE_VIRTUAL
-#endif
-
 #endif /* __ASSEMBLY__ */
 
 #define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \

commit 5b17e1cd8928ae65932758ce6478ac6d3e9a86b2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed May 13 22:56:30 2009 +0000

    asm-generic: rename page.h and uaccess.h
    
    The current asm-generic/page.h only contains the get_order
    function, and asm-generic/uaccess.h only implements
    unaligned accesses. This renames the file to getorder.h
    and uaccess-unaligned.h to make room for new page.h
    and uaccess.h file that will be usable by all simple
    (e.g. nommu) architectures.
    
    Signed-off-by: Remis Lima Baima <remis.developer@googlemail.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 17e0c5383b10..161bb89e98c8 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -129,7 +129,7 @@ static inline __attribute_const__ int get_order(unsigned long size)
 
 #else
 
-# include <asm-generic/page.h>
+# include <asm-generic/getorder.h>
 
 #endif
 

commit e5083a63b6a8546c5fe1e571fe529e3939787ec2
Author: Johannes Weiner <jw@emlix.com>
Date:   Wed Mar 4 16:21:31 2009 +0100

    xtensa: nommu support
    
    Add support for !CONFIG_MMU setups.
    
    Signed-off-by: Johannes Weiner <jw@emlix.com>
    Signed-off-by: Chris Zankel <chris@zankel.net>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index a5a5d33c15d0..17e0c5383b10 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -33,8 +33,14 @@
 #define PAGE_SIZE		(__XTENSA_UL_CONST(1) << PAGE_SHIFT)
 #define PAGE_MASK		(~(PAGE_SIZE-1))
 
+#ifdef CONFIG_MMU
 #define PAGE_OFFSET		XCHAL_KSEG_CACHED_VADDR
 #define MAX_MEM_PFN		XCHAL_KSEG_SIZE
+#else
+#define PAGE_OFFSET		0
+#define MAX_MEM_PFN		(PLATFORM_DEFAULT_MEM_START + PLATFORM_DEFAULT_MEM_SIZE)
+#endif
+
 #define PGTABLE_START		0x80000000
 
 /*
@@ -165,8 +171,9 @@ extern void copy_user_page(void*, void*, unsigned long, struct page*);
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
 
+#ifdef CONFIG_MMU
 #define WANT_PAGE_VIRTUAL
-
+#endif
 
 #endif /* __ASSEMBLY__ */
 

commit c947a585ab13f310c9223284dfd502790abd05f9
Author: Johannes Weiner <jw@emlix.com>
Date:   Wed Mar 4 16:21:30 2009 +0100

    xtensa: cope with ram beginning at higher addresses
    
    The current assumption of the memory code is that the first RAM PFN in
    the system is 0.
    
    Adjust the relevant code to play well with setups where memory starts
    at higher addresses, indicated by PLATFORM_DEFAULT_MEM_START.
    
    The new memory model looks like this:
    
    +----------+--+----------------------+----------------+
    |          |  |                      |                |
    |          |  |         RAM          |                |
    |          |  |                      |                |
    +----------+--+----------------------+----------------+
    |          |  |                      |                |
    +- PFN 0   |  +- min_low_pfn         +- max_low_pfn   +- max_pfn
               |
               +- ARCH_PFN_OFFSET
               +- PLATFORM_DEFAULT_MEM_START >> PAGE_SIZE
    
    The memory map contains pages starting from pfn ARCH_PFN_OFFSET up to
    max_low_pfn.  The only zone used right now will span exactly the same
    region.
    
    Usually, ARCH_PFN_OFFSET and min_low_pfn are the same value.  Handle
    them separately for robustness.  Gapping pages will be in the memory
    map but marked as reserved and won't be touched.
    
    Signed-off-by: Johannes Weiner <jw@emlix.com>
    Signed-off-by: Chris Zankel <chris@zankel.net>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
index 11f7dc2dbec7..a5a5d33c15d0 100644
--- a/arch/xtensa/include/asm/page.h
+++ b/arch/xtensa/include/asm/page.h
@@ -14,6 +14,7 @@
 #include <asm/processor.h>
 #include <asm/types.h>
 #include <asm/cache.h>
+#include <platform/hardware.h>
 
 /*
  * Fixed TLB translations in the processor.
@@ -150,9 +151,11 @@ extern void copy_user_page(void*, void*, unsigned long, struct page*);
  * addresses.
  */
 
+#define ARCH_PFN_OFFSET		(PLATFORM_DEFAULT_MEM_START >> PAGE_SHIFT)
+
 #define __pa(x)			((unsigned long) (x) - PAGE_OFFSET)
 #define __va(x)			((void *)((unsigned long) (x) + PAGE_OFFSET))
-#define pfn_valid(pfn)		((unsigned long)pfn < max_mapnr)
+#define pfn_valid(pfn)		((pfn) >= ARCH_PFN_OFFSET && ((pfn) - ARCH_PFN_OFFSET) < max_mapnr)
 #ifdef CONFIG_DISCONTIGMEM
 # error CONFIG_DISCONTIGMEM not supported
 #endif

commit 367b8112fe2ea5c39a7bb4d263dcdd9b612fae18
Author: Chris Zankel <chris@zankel.net>
Date:   Thu Nov 6 06:40:46 2008 -0800

    xtensa: move headers files to arch/xtensa/include
    
    Move all header files for xtensa to arch/xtensa/include and platform and
    variant header files to the appropriate arch/xtensa/platforms/ and
    arch/xtensa/variants/ directories.
    
    Moving the files gets also rid of all uses of symlinks in the Makefile.
    
    This has been completed already for the majority of the architectures
    and xtensa is one out of six missing.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Chris Zankel <chris@zankel.net>

diff --git a/arch/xtensa/include/asm/page.h b/arch/xtensa/include/asm/page.h
new file mode 100644
index 000000000000..11f7dc2dbec7
--- /dev/null
+++ b/arch/xtensa/include/asm/page.h
@@ -0,0 +1,174 @@
+/*
+ * include/asm-xtensa/page.h
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version2 as
+ * published by the Free Software Foundation.
+ *
+ * Copyright (C) 2001 - 2007 Tensilica Inc.
+ */
+
+#ifndef _XTENSA_PAGE_H
+#define _XTENSA_PAGE_H
+
+#include <asm/processor.h>
+#include <asm/types.h>
+#include <asm/cache.h>
+
+/*
+ * Fixed TLB translations in the processor.
+ */
+
+#define XCHAL_KSEG_CACHED_VADDR 0xd0000000
+#define XCHAL_KSEG_BYPASS_VADDR 0xd8000000
+#define XCHAL_KSEG_PADDR        0x00000000
+#define XCHAL_KSEG_SIZE         0x08000000
+
+/*
+ * PAGE_SHIFT determines the page size
+ */
+
+#define PAGE_SHIFT		12
+#define PAGE_SIZE		(__XTENSA_UL_CONST(1) << PAGE_SHIFT)
+#define PAGE_MASK		(~(PAGE_SIZE-1))
+
+#define PAGE_OFFSET		XCHAL_KSEG_CACHED_VADDR
+#define MAX_MEM_PFN		XCHAL_KSEG_SIZE
+#define PGTABLE_START		0x80000000
+
+/*
+ * Cache aliasing:
+ *
+ * If the cache size for one way is greater than the page size, we have to
+ * deal with cache aliasing. The cache index is wider than the page size:
+ *
+ * |    |cache| cache index
+ * | pfn  |off|	virtual address
+ * |xxxx:X|zzz|
+ * |    : |   |
+ * | \  / |   |
+ * |trans.|   |
+ * | /  \ |   |
+ * |yyyy:Y|zzz|	physical address
+ *
+ * When the page number is translated to the physical page address, the lowest
+ * bit(s) (X) that are part of the cache index are also translated (Y).
+ * If this translation changes bit(s) (X), the cache index is also afected,
+ * thus resulting in a different cache line than before.
+ * The kernel does not provide a mechanism to ensure that the page color
+ * (represented by this bit) remains the same when allocated or when pages
+ * are remapped. When user pages are mapped into kernel space, the color of
+ * the page might also change.
+ *
+ * We use the address space VMALLOC_END ... VMALLOC_END + DCACHE_WAY_SIZE * 2
+ * to temporarily map a patch so we can match the color.
+ */
+
+#if DCACHE_WAY_SIZE > PAGE_SIZE
+# define DCACHE_ALIAS_ORDER	(DCACHE_WAY_SHIFT - PAGE_SHIFT)
+# define DCACHE_ALIAS_MASK	(PAGE_MASK & (DCACHE_WAY_SIZE - 1))
+# define DCACHE_ALIAS(a)	(((a) & DCACHE_ALIAS_MASK) >> PAGE_SHIFT)
+# define DCACHE_ALIAS_EQ(a,b)	((((a) ^ (b)) & DCACHE_ALIAS_MASK) == 0)
+#else
+# define DCACHE_ALIAS_ORDER	0
+#endif
+
+#if ICACHE_WAY_SIZE > PAGE_SIZE
+# define ICACHE_ALIAS_ORDER	(ICACHE_WAY_SHIFT - PAGE_SHIFT)
+# define ICACHE_ALIAS_MASK	(PAGE_MASK & (ICACHE_WAY_SIZE - 1))
+# define ICACHE_ALIAS(a)	(((a) & ICACHE_ALIAS_MASK) >> PAGE_SHIFT)
+# define ICACHE_ALIAS_EQ(a,b)	((((a) ^ (b)) & ICACHE_ALIAS_MASK) == 0)
+#else
+# define ICACHE_ALIAS_ORDER	0
+#endif
+
+
+#ifdef __ASSEMBLY__
+
+#define __pgprot(x)	(x)
+
+#else
+
+/*
+ * These are used to make use of C type-checking..
+ */
+
+typedef struct { unsigned long pte; } pte_t;		/* page table entry */
+typedef struct { unsigned long pgd; } pgd_t;		/* PGD table entry */
+typedef struct { unsigned long pgprot; } pgprot_t;
+typedef struct page *pgtable_t;
+
+#define pte_val(x)	((x).pte)
+#define pgd_val(x)	((x).pgd)
+#define pgprot_val(x)	((x).pgprot)
+
+#define __pte(x)	((pte_t) { (x) } )
+#define __pgd(x)	((pgd_t) { (x) } )
+#define __pgprot(x)	((pgprot_t) { (x) } )
+
+/*
+ * Pure 2^n version of get_order
+ * Use 'nsau' instructions if supported by the processor or the generic version.
+ */
+
+#if XCHAL_HAVE_NSA
+
+static inline __attribute_const__ int get_order(unsigned long size)
+{
+	int lz;
+	asm ("nsau %0, %1" : "=r" (lz) : "r" ((size - 1) >> PAGE_SHIFT));
+	return 32 - lz;
+}
+
+#else
+
+# include <asm-generic/page.h>
+
+#endif
+
+struct page;
+extern void clear_page(void *page);
+extern void copy_page(void *to, void *from);
+
+/*
+ * If we have cache aliasing and writeback caches, we might have to do
+ * some extra work
+ */
+
+#if DCACHE_WAY_SIZE > PAGE_SIZE
+extern void clear_user_page(void*, unsigned long, struct page*);
+extern void copy_user_page(void*, void*, unsigned long, struct page*);
+#else
+# define clear_user_page(page, vaddr, pg)	clear_page(page)
+# define copy_user_page(to, from, vaddr, pg)	copy_page(to, from)
+#endif
+
+/*
+ * This handles the memory map.  We handle pages at
+ * XCHAL_KSEG_CACHED_VADDR for kernels with 32 bit address space.
+ * These macros are for conversion of kernel address, not user
+ * addresses.
+ */
+
+#define __pa(x)			((unsigned long) (x) - PAGE_OFFSET)
+#define __va(x)			((void *)((unsigned long) (x) + PAGE_OFFSET))
+#define pfn_valid(pfn)		((unsigned long)pfn < max_mapnr)
+#ifdef CONFIG_DISCONTIGMEM
+# error CONFIG_DISCONTIGMEM not supported
+#endif
+
+#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define page_to_virt(page)	__va(page_to_pfn(page) << PAGE_SHIFT)
+#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
+
+#define WANT_PAGE_VIRTUAL
+
+
+#endif /* __ASSEMBLY__ */
+
+#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+#include <asm-generic/memory_model.h>
+#endif /* _XTENSA_PAGE_H */
