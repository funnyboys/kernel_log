commit 7778d8417b74aded842eeb372961cfc460417fa0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 6 23:51:17 2020 +0200

    x86/vdso: Unbreak paravirt VDSO clocks
    
    The conversion of x86 VDSO to the generic clock mode storage broke the
    paravirt and hyperv clocksource logic. These clock sources have their own
    internal sequence counter to validate the clocksource at the point of
    reading it. This is necessary because the hypervisor can invalidate the
    clocksource asynchronously so a check during the VDSO data update is not
    sufficient. If the internal check during read invalidates the clocksource
    the read return U64_MAX. The original code checked this efficiently by
    testing whether the result (casted to signed) is negative, i.e. bit 63 is
    set. This was done that way because an extra indicator for the validity had
    more overhead.
    
    The conversion broke this check because the check was replaced by a check
    for a valid VDSO clock mode.
    
    The wreckage manifests itself when the paravirt clock is installed as a
    valid VDSO clock and during runtime invalidated by the hypervisor,
    e.g. after a host suspend/resume cycle. After the invalidation the read
    function returns U64_MAX which is used as cycles and makes the clock jump
    by ~2200 seconds, and become stale until the 2200 seconds have elapsed
    where it starts to jump again. The period of this effect depends on the
    shift/mult pair of the clocksource and the jumps and staleness are an
    artifact of undefined but reproducible behaviour of math overflow.
    
    Implement an x86 version of the new vdso_cycles_ok() inline which adds this
    check back and a variant of vdso_clocksource_ok() which lets the compiler
    optimize it out to avoid the extra conditional. That's suboptimal when the
    system does not have a VDSO capable clocksource, but that's not the case
    which is optimized for.
    
    Fixes: 5d51bee725cc ("clocksource: Add common vdso clock mode storage")
    Reported-by: Miklos Szeredi <miklos@szeredi.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Miklos Szeredi <mszeredi@redhat.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20200606221532.080560273@linutronix.de

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index 9a6dc9b4ec99..fb81fea99093 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -271,6 +271,24 @@ static __always_inline const struct vdso_data *__arch_get_vdso_data(void)
 	return __vdso_data;
 }
 
+static inline bool arch_vdso_clocksource_ok(const struct vdso_data *vd)
+{
+	return true;
+}
+#define vdso_clocksource_ok arch_vdso_clocksource_ok
+
+/*
+ * Clocksource read value validation to handle PV and HyperV clocksources
+ * which can be invalidated asynchronously and indicate invalidation by
+ * returning U64_MAX, which can be effectively tested by checking for a
+ * negative value after casting it to s64.
+ */
+static inline bool arch_vdso_cycles_ok(u64 cycles)
+{
+	return (s64)cycles >= 0;
+}
+#define vdso_cycles_ok arch_vdso_cycles_ok
+
 /*
  * x86 specific delta calculation.
  *

commit b95a8a27c300d1a39a4e36f63a518ef36e4b966c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 13:38:56 2020 +0100

    x86/vdso: Use generic VDSO clock mode storage
    
    Switch to the generic VDSO clock mode storage.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com> (VDSO parts)
    Acked-by: Juergen Gross <jgross@suse.com> (Xen parts)
    Acked-by: Paolo Bonzini <pbonzini@redhat.com> (KVM parts)
    Link: https://lkml.kernel.org/r/20200207124403.152039903@linutronix.de

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index 264d4fd3ff2c..9a6dc9b4ec99 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -243,7 +243,7 @@ static u64 vread_hvclock(void)
 
 static inline u64 __arch_get_hw_counter(s32 clock_mode)
 {
-	if (likely(clock_mode == VCLOCK_TSC))
+	if (likely(clock_mode == VDSO_CLOCKMODE_TSC))
 		return (u64)rdtsc_ordered();
 	/*
 	 * For any memory-mapped vclock type, we need to make sure that gcc
@@ -252,13 +252,13 @@ static inline u64 __arch_get_hw_counter(s32 clock_mode)
 	 * question isn't enabled, which will segfault.  Hence the barriers.
 	 */
 #ifdef CONFIG_PARAVIRT_CLOCK
-	if (clock_mode == VCLOCK_PVCLOCK) {
+	if (clock_mode == VDSO_CLOCKMODE_PVCLOCK) {
 		barrier();
 		return vread_pvclock();
 	}
 #endif
 #ifdef CONFIG_HYPERV_TIMER
-	if (clock_mode == VCLOCK_HVCLOCK) {
+	if (clock_mode == VDSO_CLOCKMODE_HVCLOCK) {
 		barrier();
 		return vread_hvclock();
 	}

commit 50e818715821b89c7abac90a97721f106e893d83
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 13:38:48 2020 +0100

    x86/vdso: Mark the TSC clocksource path likely
    
    Jumping out of line for the TSC clcoksource read is creating awful
    code. TSC is likely to be the clocksource at least on bare metal and the PV
    interfaces are sufficiently more work that the jump over the TSC read is
    just in the noise.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Link: https://lkml.kernel.org/r/20200207124402.328922847@linutronix.de

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index 6ee1f7dba34b..264d4fd3ff2c 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -243,7 +243,7 @@ static u64 vread_hvclock(void)
 
 static inline u64 __arch_get_hw_counter(s32 clock_mode)
 {
-	if (clock_mode == VCLOCK_TSC)
+	if (likely(clock_mode == VCLOCK_TSC))
 		return (u64)rdtsc_ordered();
 	/*
 	 * For any memory-mapped vclock type, we need to make sure that gcc

commit 550a77a74c87ecfdadc2214fef4b25ff125f65ab
Author: Dmitry Safonov <dima@arista.com>
Date:   Tue Nov 12 01:27:11 2019 +0000

    x86/vdso: Add time napespace page
    
    To support time namespaces in the VDSO with a minimal impact on regular non
    time namespace affected tasks, the namespace handling needs to be hidden in
    a slow path.
    
    The most obvious place is vdso_seq_begin(). If a task belongs to a time
    namespace then the VVAR page which contains the system wide VDSO data is
    replaced with a namespace specific page which has the same layout as the
    VVAR page. That page has vdso_data->seq set to 1 to enforce the slow path
    and vdso_data->clock_mode set to VCLOCK_TIMENS to enforce the time
    namespace handling path.
    
    The extra check in the case that vdso_data->seq is odd, e.g. a concurrent
    update of the VDSO data is in progress, is not really affecting regular
    tasks which are not part of a time namespace as the task is spin waiting
    for the update to finish and vdso_data->seq to become even again.
    
    If a time namespace task hits that code path, it invokes the corresponding
    time getter function which retrieves the real VVAR page, reads host time
    and then adds the offset for the requested clock which is stored in the
    special VVAR page.
    
    Allocate the time namespace page among VVAR pages and place vdso_data on
    it.  Provide __arch_get_timens_vdso_data() helper for VDSO code to get the
    code-relative position of VVARs on that special page.
    
    Co-developed-by: Andrei Vagin <avagin@openvz.org>
    Signed-off-by: Andrei Vagin <avagin@openvz.org>
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20191112012724.250792-23-dima@arista.com

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index 52c3bcd672cf..6ee1f7dba34b 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -21,6 +21,7 @@
 #include <clocksource/hyperv_timer.h>
 
 #define __vdso_data (VVAR(_vdso_data))
+#define __timens_vdso_data (TIMENS(_vdso_data))
 
 #define VDSO_HAS_TIME 1
 
@@ -56,6 +57,13 @@ extern struct ms_hyperv_tsc_page hvclock_page
 	__attribute__((visibility("hidden")));
 #endif
 
+#ifdef CONFIG_TIME_NS
+static __always_inline const struct vdso_data *__arch_get_timens_vdso_data(void)
+{
+	return __timens_vdso_data;
+}
+#endif
+
 #ifndef BUILD_VDSO32
 
 static __always_inline

commit 0b5c12332db5b71c6db0c102b3e6acc7c7c6a54d
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Aug 30 14:59:02 2019 +0100

    x86/vdso: Remove unused VDSO_HAS_32BIT_FALLBACK
    
    VDSO_HAS_32BIT_FALLBACK has been removed from the core since
    the architectures that support the generic vDSO library have
    been converted to support the 32 bit fallbacks.
    
    Remove unused VDSO_HAS_32BIT_FALLBACK from x86 vdso.
    
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20190830135902.20861-9-vincenzo.frascino@arm.com

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index e9ee139cf29e..52c3bcd672cf 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -96,8 +96,6 @@ long clock_getres_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
 
 #else
 
-#define VDSO_HAS_32BIT_FALLBACK	1
-
 static __always_inline
 long clock_gettime_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
 {

commit 7f2444d38f6bbfa12bc15e2533d8f9daa85ca02b
Merge: c5f12fdb8bd8 77b4b5420422
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 12:35:15 2019 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core timer updates from Thomas Gleixner:
     "Timers and timekeeping updates:
    
       - A large overhaul of the posix CPU timer code which is a preparation
         for moving the CPU timer expiry out into task work so it can be
         properly accounted on the task/process.
    
         An update to the bogus permission checks will come later during the
         merge window as feedback was not complete before heading of for
         travel.
    
       - Switch the timerqueue code to use cached rbtrees and get rid of the
         homebrewn caching of the leftmost node.
    
       - Consolidate hrtimer_init() + hrtimer_init_sleeper() calls into a
         single function
    
       - Implement the separation of hrtimers to be forced to expire in hard
         interrupt context even when PREEMPT_RT is enabled and mark the
         affected timers accordingly.
    
       - Implement a mechanism for hrtimers and the timer wheel to protect
         RT against priority inversion and live lock issues when a (hr)timer
         which should be canceled is currently executing the callback.
         Instead of infinitely spinning, the task which tries to cancel the
         timer blocks on a per cpu base expiry lock which is held and
         released by the (hr)timer expiry code.
    
       - Enable the Hyper-V TSC page based sched_clock for Hyper-V guests
         resulting in faster access to timekeeping functions.
    
       - Updates to various clocksource/clockevent drivers and their device
         tree bindings.
    
       - The usual small improvements all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      posix-cpu-timers: Fix permission check regression
      posix-cpu-timers: Always clear head pointer on dequeue
      hrtimer: Add a missing bracket and hide `migration_base' on !SMP
      posix-cpu-timers: Make expiry_active check actually work correctly
      posix-timers: Unbreak CONFIG_POSIX_TIMERS=n build
      tick: Mark sched_timer to expire in hard interrupt context
      hrtimer: Add kernel doc annotation for HRTIMER_MODE_HARD
      x86/hyperv: Hide pv_ops access for CONFIG_PARAVIRT=n
      posix-cpu-timers: Utilize timerqueue for storage
      posix-cpu-timers: Move state tracking to struct posix_cputimers
      posix-cpu-timers: Deduplicate rlimit handling
      posix-cpu-timers: Remove pointless comparisons
      posix-cpu-timers: Get rid of 64bit divisions
      posix-cpu-timers: Consolidate timer expiry further
      posix-cpu-timers: Get rid of zero checks
      rlimit: Rewrite non-sensical RLIMIT_CPU comment
      posix-cpu-timers: Respect INFINITY for hard RTTIME limit
      posix-cpu-timers: Switch thread group sampling to array
      posix-cpu-timers: Restructure expiry array
      posix-cpu-timers: Remove cputime_expires
      ...

commit 3e2d94535adb2df15f3907e4b4c7cd8a5a4c2b5a
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Aug 22 10:36:30 2019 +0200

    clocksource/drivers/hyperv: Enable TSC page clocksource on 32bit
    
    There is no particular reason to not enable TSC page clocksource on
    32-bit. mul_u64_u64_shr() is available and despite the increased
    computational complexity (compared to 64bit) TSC page is still a huge win
    compared to MSR-based clocksource.
    
    In-kernel reads:
      MSR based clocksource: 3361 cycles
      TSC page clocksource: 49 cycles
    
    Reads from userspace (utilizing vDSO in case of TSC page):
      MSR based clocksource: 5664 cycles
      TSC page clocksource: 131 cycles
    
    Enabling TSC page on 32bits allows to get rid of CONFIG_HYPERV_TSCPAGE as
    it is now not any different from CONFIG_HYPERV_TIMER.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lkml.kernel.org/r/20190822083630.17059-1-vkuznets@redhat.com

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index ae91429129a6..bcbf901befbe 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -51,7 +51,7 @@ extern struct pvclock_vsyscall_time_info pvclock_page
 	__attribute__((visibility("hidden")));
 #endif
 
-#ifdef CONFIG_HYPERV_TSCPAGE
+#ifdef CONFIG_HYPERV_TIMER
 extern struct ms_hyperv_tsc_page hvclock_page
 	__attribute__((visibility("hidden")));
 #endif
@@ -192,7 +192,7 @@ static u64 vread_pvclock(void)
 }
 #endif
 
-#ifdef CONFIG_HYPERV_TSCPAGE
+#ifdef CONFIG_HYPERV_TIMER
 static u64 vread_hvclock(void)
 {
 	return hv_read_tsc_page(&hvclock_page);
@@ -215,7 +215,7 @@ static inline u64 __arch_get_hw_counter(s32 clock_mode)
 		return vread_pvclock();
 	}
 #endif
-#ifdef CONFIG_HYPERV_TSCPAGE
+#ifdef CONFIG_HYPERV_TIMER
 	if (clock_mode == VCLOCK_HVCLOCK) {
 		barrier();
 		return vread_hvclock();

commit d2f5d3fa26196183adb44a413c44caa9872275b4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 28 15:12:55 2019 +0200

    x86/vdso/32: Use 32bit syscall fallback
    
    The generic VDSO implementation uses the Y2038 safe clock_gettime64() and
    clock_getres_time64() syscalls as fallback for 32bit VDSO. This breaks
    seccomp setups because these syscalls might be not (yet) allowed.
    
    Implement the 32bit variants which use the legacy syscalls and select the
    variant in the core library.
    
    The 64bit time variants are not removed because they are required for the
    time64 based vdso accessors.
    
    Fixes: 7ac870747988 ("x86/vdso: Switch to generic vDSO implementation")
    Reported-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reported-by: Paul Bolle <pebolle@tiscali.nl>
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20190728131648.879156507@linutronix.de

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index ae91429129a6..ba71a63cdac4 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -96,6 +96,8 @@ long clock_getres_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
 
 #else
 
+#define VDSO_HAS_32BIT_FALLBACK	1
+
 static __always_inline
 long clock_gettime_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
 {
@@ -113,6 +115,23 @@ long clock_gettime_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
 	return ret;
 }
 
+static __always_inline
+long clock_gettime32_fallback(clockid_t _clkid, struct old_timespec32 *_ts)
+{
+	long ret;
+
+	asm (
+		"mov %%ebx, %%edx \n"
+		"mov %[clock], %%ebx \n"
+		"call __kernel_vsyscall \n"
+		"mov %%edx, %%ebx \n"
+		: "=a" (ret), "=m" (*_ts)
+		: "0" (__NR_clock_gettime), [clock] "g" (_clkid), "c" (_ts)
+		: "edx");
+
+	return ret;
+}
+
 static __always_inline
 long gettimeofday_fallback(struct __kernel_old_timeval *_tv,
 			   struct timezone *_tz)
@@ -148,6 +167,23 @@ clock_getres_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
 	return ret;
 }
 
+static __always_inline
+long clock_getres32_fallback(clockid_t _clkid, struct old_timespec32 *_ts)
+{
+	long ret;
+
+	asm (
+		"mov %%ebx, %%edx \n"
+		"mov %[clock], %%ebx \n"
+		"call __kernel_vsyscall \n"
+		"mov %%edx, %%ebx \n"
+		: "=a" (ret), "=m" (*_ts)
+		: "0" (__NR_clock_getres), [clock] "g" (_clkid), "c" (_ts)
+		: "edx");
+
+	return ret;
+}
+
 #endif
 
 #ifdef CONFIG_PARAVIRT_CLOCK

commit dd2cb348613b44f9d948b068775e159aad298599
Author: Michael Kelley <mikelley@microsoft.com>
Date:   Mon Jul 1 04:26:06 2019 +0000

    clocksource/drivers: Continue making Hyper-V clocksource ISA agnostic
    
    Continue consolidating Hyper-V clock and timer code into an ISA
    independent Hyper-V clocksource driver.
    
    Move the existing clocksource code under drivers/hv and arch/x86 to the new
    clocksource driver while separating out the ISA dependencies. Update
    Hyper-V initialization to call initialization and cleanup routines since
    the Hyper-V synthetic clock is not independently enumerated in ACPI.
    
    Update Hyper-V clocksource users in KVM and VDSO to get definitions from
    the new include file.
    
    No behavior is changed and no new functionality is added.
    
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Michael Kelley <mikelley@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: "bp@alien8.de" <bp@alien8.de>
    Cc: "will.deacon@arm.com" <will.deacon@arm.com>
    Cc: "catalin.marinas@arm.com" <catalin.marinas@arm.com>
    Cc: "mark.rutland@arm.com" <mark.rutland@arm.com>
    Cc: "linux-arm-kernel@lists.infradead.org" <linux-arm-kernel@lists.infradead.org>
    Cc: "gregkh@linuxfoundation.org" <gregkh@linuxfoundation.org>
    Cc: "linux-hyperv@vger.kernel.org" <linux-hyperv@vger.kernel.org>
    Cc: "olaf@aepfle.de" <olaf@aepfle.de>
    Cc: "apw@canonical.com" <apw@canonical.com>
    Cc: "jasowang@redhat.com" <jasowang@redhat.com>
    Cc: "marcelo.cerri@canonical.com" <marcelo.cerri@canonical.com>
    Cc: Sunil Muthuswamy <sunilmut@microsoft.com>
    Cc: KY Srinivasan <kys@microsoft.com>
    Cc: "sashal@kernel.org" <sashal@kernel.org>
    Cc: "vincenzo.frascino@arm.com" <vincenzo.frascino@arm.com>
    Cc: "linux-arch@vger.kernel.org" <linux-arch@vger.kernel.org>
    Cc: "linux-mips@vger.kernel.org" <linux-mips@vger.kernel.org>
    Cc: "linux-kselftest@vger.kernel.org" <linux-kselftest@vger.kernel.org>
    Cc: "arnd@arndb.de" <arnd@arndb.de>
    Cc: "linux@armlinux.org.uk" <linux@armlinux.org.uk>
    Cc: "ralf@linux-mips.org" <ralf@linux-mips.org>
    Cc: "paul.burton@mips.com" <paul.burton@mips.com>
    Cc: "daniel.lezcano@linaro.org" <daniel.lezcano@linaro.org>
    Cc: "salyzyn@android.com" <salyzyn@android.com>
    Cc: "pcc@google.com" <pcc@google.com>
    Cc: "shuah@kernel.org" <shuah@kernel.org>
    Cc: "0x7f454c46@gmail.com" <0x7f454c46@gmail.com>
    Cc: "linux@rasmusvillemoes.dk" <linux@rasmusvillemoes.dk>
    Cc: "huw@codeweavers.com" <huw@codeweavers.com>
    Cc: "sfr@canb.auug.org.au" <sfr@canb.auug.org.au>
    Cc: "pbonzini@redhat.com" <pbonzini@redhat.com>
    Cc: "rkrcmar@redhat.com" <rkrcmar@redhat.com>
    Cc: "kvm@vger.kernel.org" <kvm@vger.kernel.org>
    Link: https://lkml.kernel.org/r/1561955054-1838-3-git-send-email-mikelley@microsoft.com

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index a14039a59abd..ae91429129a6 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -18,7 +18,7 @@
 #include <asm/unistd.h>
 #include <asm/msr.h>
 #include <asm/pvclock.h>
-#include <asm/mshyperv.h>
+#include <clocksource/hyperv_timer.h>
 
 #define __vdso_data (VVAR(_vdso_data))
 

commit 9d90b93bf325e015bbae31b83f16da5e4e17effa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 26 12:02:00 2019 +0200

    lib/vdso: Make delta calculation work correctly
    
    The x86 vdso implementation on which the generic vdso library is based on
    has subtle (unfortunately undocumented) twists:
    
     1) The code assumes that the clocksource mask is U64_MAX which means that
        no bits are masked. Which is true for any valid x86 VDSO clocksource.
        Stupidly it still did the mask operation for no reason and at the wrong
        place right after reading the clocksource.
    
     2) It contains a sanity check to catch the case where slightly
        unsynchronized TSC values can be observed which would cause the delta
        calculation to make a huge jump. It therefore checks whether the
        current TSC value is larger than the value on which the current
        conversion is based on. If it's not larger the base value is used to
        prevent time jumps.
    
    #1 Is not only stupid for the X86 case because it does the masking for no
    reason it is also completely wrong for clocksources with a smaller mask
    which can legitimately wrap around during a conversion period. The core
    timekeeping code does it correct by applying the mask after the delta
    calculation:
    
            (now - base) & mask
    
    #2 is equally broken for clocksources which have smaller masks and can wrap
    around during a conversion period because there the now > base check is
    just wrong and causes stale time stamps and time going backwards issues.
    
    Unbreak it by:
    
      1) Removing the mask operation from the clocksource read which makes the
         fallback detection work for all clocksources
    
      2) Replacing the conditional delta calculation with a overrideable inline
         function.
    
    #2 could reuse clocksource_delta() from the timekeeping code but that
    results in a significant performance hit for the x86 VSDO. The timekeeping
    core code must have the non optimized version as it has to operate
    correctly with clocksources which have smaller masks as well to handle the
    case where TSC is discarded as timekeeper clocksource and replaced by HPET
    or pmtimer. For the VDSO there is no replacement clocksource. If TSC is
    unusable the syscall is enforced which does the right thing.
    
    To accommodate to the needs of various architectures provide an
    override-able inline function which defaults to the regular delta
    calculation with masking:
    
            (now - base) & mask
    
    Override it for x86 with the non-masking and checking version.
    
    This unbreaks the ARM64 syscall fallback operation, allows to use
    clocksources with arbitrary width and preserves the performance
    optimization for x86.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: LAK <linux-arm-kernel@lists.infradead.org>
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kselftest@vger.kernel.org
    Cc: catalin.marinas@arm.com
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linux@armlinux.org.uk
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: paul.burton@mips.com
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: salyzyn@android.com
    Cc: pcc@google.com
    Cc: shuah@kernel.org
    Cc: 0x7f454c46@gmail.com
    Cc: linux@rasmusvillemoes.dk
    Cc: huw@codeweavers.com
    Cc: sthotton@marvell.com
    Cc: andre.przywara@arm.com
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1906261159230.32342@nanos.tec.linutronix.de

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index 5b63f1f78a1f..a14039a59abd 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -229,6 +229,33 @@ static __always_inline const struct vdso_data *__arch_get_vdso_data(void)
 	return __vdso_data;
 }
 
+/*
+ * x86 specific delta calculation.
+ *
+ * The regular implementation assumes that clocksource reads are globally
+ * monotonic. The TSC can be slightly off across sockets which can cause
+ * the regular delta calculation (@cycles - @last) to return a huge time
+ * jump.
+ *
+ * Therefore it needs to be verified that @cycles are greater than
+ * @last. If not then use @last, which is the base time of the current
+ * conversion period.
+ *
+ * This variant also removes the masking of the subtraction because the
+ * clocksource mask of all VDSO capable clocksources on x86 is U64_MAX
+ * which would result in a pointless operation. The compiler cannot
+ * optimize it away as the mask comes from the vdso data and is not compile
+ * time constant.
+ */
+static __always_inline
+u64 vdso_calc_delta(u64 cycles, u64 last, u64 mask, u32 mult)
+{
+	if (cycles > last)
+		return (cycles - last) * mult;
+	return 0;
+}
+#define vdso_calc_delta vdso_calc_delta
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_VDSO_GETTIMEOFDAY_H */

commit ecf9db3d1f1a8fd2c335148891c3b044e9ce0628
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sat Jun 22 15:08:18 2019 -0700

    x86/vdso: Give the [ph]vclock_page declarations real types
    
    Clean up the vDSO code a bit by giving pvclock_page and hvclock_page
    their actual types instead of u8[PAGE_SIZE].  This shouldn't
    materially affect the generated code.
    
    Heavily based on a patch from Linus.
    
    [ tglx: Adapted to the unified VDSO code ]
    
    Co-developed-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/6920c5188f8658001af1fc56fd35b815706d300c.1561241273.git.luto@kernel.org

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index f92752d6cbcf..5b63f1f78a1f 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -26,13 +26,33 @@
 
 #define VDSO_HAS_CLOCK_GETRES 1
 
+/*
+ * Declare the memory-mapped vclock data pages.  These come from hypervisors.
+ * If we ever reintroduce something like direct access to an MMIO clock like
+ * the HPET again, it will go here as well.
+ *
+ * A load from any of these pages will segfault if the clock in question is
+ * disabled, so appropriate compiler barriers and checks need to be used
+ * to prevent stray loads.
+ *
+ * These declarations MUST NOT be const.  The compiler will assume that
+ * an extern const variable has genuinely constant contents, and the
+ * resulting code won't work, since the whole point is that these pages
+ * change over time, possibly while we're accessing them.
+ */
+
 #ifdef CONFIG_PARAVIRT_CLOCK
-extern u8 pvclock_page[PAGE_SIZE]
+/*
+ * This is the vCPU 0 pvclock page.  We only use pvclock from the vDSO
+ * if the hypervisor tells us that all vCPUs can get valid data from the
+ * vCPU 0 page.
+ */
+extern struct pvclock_vsyscall_time_info pvclock_page
 	__attribute__((visibility("hidden")));
 #endif
 
 #ifdef CONFIG_HYPERV_TSCPAGE
-extern u8 hvclock_page[PAGE_SIZE]
+extern struct ms_hyperv_tsc_page hvclock_page
 	__attribute__((visibility("hidden")));
 #endif
 
@@ -131,14 +151,9 @@ clock_getres_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
 #endif
 
 #ifdef CONFIG_PARAVIRT_CLOCK
-static const struct pvclock_vsyscall_time_info *get_pvti0(void)
-{
-	return (const struct pvclock_vsyscall_time_info *)&pvclock_page;
-}
-
 static u64 vread_pvclock(void)
 {
-	const struct pvclock_vcpu_time_info *pvti = &get_pvti0()->pvti;
+	const struct pvclock_vcpu_time_info *pvti = &pvclock_page.pvti;
 	u32 version;
 	u64 ret;
 
@@ -180,10 +195,7 @@ static u64 vread_pvclock(void)
 #ifdef CONFIG_HYPERV_TSCPAGE
 static u64 vread_hvclock(void)
 {
-	const struct ms_hyperv_tsc_page *tsc_pg =
-		(const struct ms_hyperv_tsc_page *)&hvclock_page;
-
-	return hv_read_tsc_page(tsc_pg);
+	return hv_read_tsc_page(&hvclock_page);
 }
 #endif
 

commit f66501dc53e72079045a6a17e023b41316ede220
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Jun 21 10:52:50 2019 +0100

    x86/vdso: Add clock_getres() entry point
    
    The generic vDSO library provides an implementation of clock_getres()
    that can be leveraged by each architecture.
    
    Add the clock_getres() VDSO entry point on x86.
    
    [ tglx: Massaged changelog and cleaned up the function signature formatting ]
    
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kselftest@vger.kernel.org
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Mark Salyzyn <salyzyn@android.com>
    Cc: Peter Collingbourne <pcc@google.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Dmitry Safonov <0x7f454c46@gmail.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Huw Davies <huw@codeweavers.com>
    Cc: Shijith Thotton <sthotton@marvell.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Link: https://lkml.kernel.org/r/20190621095252.32307-24-vincenzo.frascino@arm.com

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index 0e2650fc191b..f92752d6cbcf 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -24,6 +24,8 @@
 
 #define VDSO_HAS_TIME 1
 
+#define VDSO_HAS_CLOCK_GETRES 1
+
 #ifdef CONFIG_PARAVIRT_CLOCK
 extern u8 pvclock_page[PAGE_SIZE]
 	__attribute__((visibility("hidden")));
@@ -60,6 +62,18 @@ long gettimeofday_fallback(struct __kernel_old_timeval *_tv,
 	return ret;
 }
 
+static __always_inline
+long clock_getres_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
+{
+	long ret;
+
+	asm ("syscall" : "=a" (ret), "=m" (*_ts) :
+	     "0" (__NR_clock_getres), "D" (_clkid), "S" (_ts) :
+	     "rcx", "r11");
+
+	return ret;
+}
+
 #else
 
 static __always_inline
@@ -97,6 +111,23 @@ long gettimeofday_fallback(struct __kernel_old_timeval *_tv,
 	return ret;
 }
 
+static __always_inline long
+clock_getres_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
+{
+	long ret;
+
+	asm (
+		"mov %%ebx, %%edx \n"
+		"mov %[clock], %%ebx \n"
+		"call __kernel_vsyscall \n"
+		"mov %%edx, %%ebx \n"
+		: "=a" (ret), "=m" (*_ts)
+		: "0" (__NR_clock_getres_time64), [clock] "g" (_clkid), "c" (_ts)
+		: "edx");
+
+	return ret;
+}
+
 #endif
 
 #ifdef CONFIG_PARAVIRT_CLOCK

commit 7ac8707479886c75f353bfb6a8273f423cfccb23
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Jun 21 10:52:49 2019 +0100

    x86/vdso: Switch to generic vDSO implementation
    
    The x86 vDSO library requires some adaptations to take advantage of the
    newly introduced generic vDSO library.
    
    Introduce the following changes:
     - Modification of vdso.c to be compliant with the common vdso datapage
     - Use of lib/vdso for gettimeofday
    
    [ tglx: Massaged changelog and cleaned up the function signature formatting ]
    
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kselftest@vger.kernel.org
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Mark Salyzyn <salyzyn@android.com>
    Cc: Peter Collingbourne <pcc@google.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Dmitry Safonov <0x7f454c46@gmail.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Huw Davies <huw@codeweavers.com>
    Cc: Shijith Thotton <sthotton@marvell.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Link: https://lkml.kernel.org/r/20190621095252.32307-23-vincenzo.frascino@arm.com

diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
new file mode 100644
index 000000000000..0e2650fc191b
--- /dev/null
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -0,0 +1,191 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Fast user context implementation of clock_gettime, gettimeofday, and time.
+ *
+ * Copyright (C) 2019 ARM Limited.
+ * Copyright 2006 Andi Kleen, SUSE Labs.
+ * 32 Bit compat layer by Stefani Seibold <stefani@seibold.net>
+ *  sponsored by Rohde & Schwarz GmbH & Co. KG Munich/Germany
+ */
+#ifndef __ASM_VDSO_GETTIMEOFDAY_H
+#define __ASM_VDSO_GETTIMEOFDAY_H
+
+#ifndef __ASSEMBLY__
+
+#include <uapi/linux/time.h>
+#include <asm/vgtod.h>
+#include <asm/vvar.h>
+#include <asm/unistd.h>
+#include <asm/msr.h>
+#include <asm/pvclock.h>
+#include <asm/mshyperv.h>
+
+#define __vdso_data (VVAR(_vdso_data))
+
+#define VDSO_HAS_TIME 1
+
+#ifdef CONFIG_PARAVIRT_CLOCK
+extern u8 pvclock_page[PAGE_SIZE]
+	__attribute__((visibility("hidden")));
+#endif
+
+#ifdef CONFIG_HYPERV_TSCPAGE
+extern u8 hvclock_page[PAGE_SIZE]
+	__attribute__((visibility("hidden")));
+#endif
+
+#ifndef BUILD_VDSO32
+
+static __always_inline
+long clock_gettime_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
+{
+	long ret;
+
+	asm ("syscall" : "=a" (ret), "=m" (*_ts) :
+	     "0" (__NR_clock_gettime), "D" (_clkid), "S" (_ts) :
+	     "rcx", "r11");
+
+	return ret;
+}
+
+static __always_inline
+long gettimeofday_fallback(struct __kernel_old_timeval *_tv,
+			   struct timezone *_tz)
+{
+	long ret;
+
+	asm("syscall" : "=a" (ret) :
+	    "0" (__NR_gettimeofday), "D" (_tv), "S" (_tz) : "memory");
+
+	return ret;
+}
+
+#else
+
+static __always_inline
+long clock_gettime_fallback(clockid_t _clkid, struct __kernel_timespec *_ts)
+{
+	long ret;
+
+	asm (
+		"mov %%ebx, %%edx \n"
+		"mov %[clock], %%ebx \n"
+		"call __kernel_vsyscall \n"
+		"mov %%edx, %%ebx \n"
+		: "=a" (ret), "=m" (*_ts)
+		: "0" (__NR_clock_gettime64), [clock] "g" (_clkid), "c" (_ts)
+		: "edx");
+
+	return ret;
+}
+
+static __always_inline
+long gettimeofday_fallback(struct __kernel_old_timeval *_tv,
+			   struct timezone *_tz)
+{
+	long ret;
+
+	asm(
+		"mov %%ebx, %%edx \n"
+		"mov %2, %%ebx \n"
+		"call __kernel_vsyscall \n"
+		"mov %%edx, %%ebx \n"
+		: "=a" (ret)
+		: "0" (__NR_gettimeofday), "g" (_tv), "c" (_tz)
+		: "memory", "edx");
+
+	return ret;
+}
+
+#endif
+
+#ifdef CONFIG_PARAVIRT_CLOCK
+static const struct pvclock_vsyscall_time_info *get_pvti0(void)
+{
+	return (const struct pvclock_vsyscall_time_info *)&pvclock_page;
+}
+
+static u64 vread_pvclock(void)
+{
+	const struct pvclock_vcpu_time_info *pvti = &get_pvti0()->pvti;
+	u32 version;
+	u64 ret;
+
+	/*
+	 * Note: The kernel and hypervisor must guarantee that cpu ID
+	 * number maps 1:1 to per-CPU pvclock time info.
+	 *
+	 * Because the hypervisor is entirely unaware of guest userspace
+	 * preemption, it cannot guarantee that per-CPU pvclock time
+	 * info is updated if the underlying CPU changes or that that
+	 * version is increased whenever underlying CPU changes.
+	 *
+	 * On KVM, we are guaranteed that pvti updates for any vCPU are
+	 * atomic as seen by *all* vCPUs.  This is an even stronger
+	 * guarantee than we get with a normal seqlock.
+	 *
+	 * On Xen, we don't appear to have that guarantee, but Xen still
+	 * supplies a valid seqlock using the version field.
+	 *
+	 * We only do pvclock vdso timing at all if
+	 * PVCLOCK_TSC_STABLE_BIT is set, and we interpret that bit to
+	 * mean that all vCPUs have matching pvti and that the TSC is
+	 * synced, so we can just look at vCPU 0's pvti.
+	 */
+
+	do {
+		version = pvclock_read_begin(pvti);
+
+		if (unlikely(!(pvti->flags & PVCLOCK_TSC_STABLE_BIT)))
+			return U64_MAX;
+
+		ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+	} while (pvclock_read_retry(pvti, version));
+
+	return ret;
+}
+#endif
+
+#ifdef CONFIG_HYPERV_TSCPAGE
+static u64 vread_hvclock(void)
+{
+	const struct ms_hyperv_tsc_page *tsc_pg =
+		(const struct ms_hyperv_tsc_page *)&hvclock_page;
+
+	return hv_read_tsc_page(tsc_pg);
+}
+#endif
+
+static inline u64 __arch_get_hw_counter(s32 clock_mode)
+{
+	if (clock_mode == VCLOCK_TSC)
+		return (u64)rdtsc_ordered();
+	/*
+	 * For any memory-mapped vclock type, we need to make sure that gcc
+	 * doesn't cleverly hoist a load before the mode check.  Otherwise we
+	 * might end up touching the memory-mapped page even if the vclock in
+	 * question isn't enabled, which will segfault.  Hence the barriers.
+	 */
+#ifdef CONFIG_PARAVIRT_CLOCK
+	if (clock_mode == VCLOCK_PVCLOCK) {
+		barrier();
+		return vread_pvclock();
+	}
+#endif
+#ifdef CONFIG_HYPERV_TSCPAGE
+	if (clock_mode == VCLOCK_HVCLOCK) {
+		barrier();
+		return vread_hvclock();
+	}
+#endif
+	return U64_MAX;
+}
+
+static __always_inline const struct vdso_data *__arch_get_vdso_data(void)
+{
+	return __vdso_data;
+}
+
+#endif /* !__ASSEMBLY__ */
+
+#endif /* __ASM_VDSO_GETTIMEOFDAY_H */
