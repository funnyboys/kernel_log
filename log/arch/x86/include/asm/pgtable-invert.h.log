commit f19f5c49bbc3ffcc9126cc245fc1b24cc29f4a37
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Aug 17 10:27:36 2018 -0700

    x86/speculation/l1tf: Exempt zeroed PTEs from inversion
    
    It turns out that we should *not* invert all not-present mappings,
    because the all zeroes case is obviously special.
    
    clear_page() does not undergo the XOR logic to invert the address bits,
    i.e. PTE, PMD and PUD entries that have not been individually written
    will have val=0 and so will trigger __pte_needs_invert(). As a result,
    {pte,pmd,pud}_pfn() will return the wrong PFN value, i.e. all ones
    (adjusted by the max PFN mask) instead of zero. A zeroed entry is ok
    because the page at physical address 0 is reserved early in boot
    specifically to mitigate L1TF, so explicitly exempt them from the
    inversion when reading the PFN.
    
    Manifested as an unexpected mprotect(..., PROT_NONE) failure when called
    on a VMA that has VM_PFNMAP and was mmap'd to as something other than
    PROT_NONE but never used. mprotect() sends the PROT_NONE request down
    prot_none_walk(), which walks the PTEs to check the PFNs.
    prot_none_pte_entry() gets the bogus PFN from pte_pfn() and returns
    -EACCES because it thinks mprotect() is trying to adjust a high MMIO
    address.
    
    [ This is a very modified version of Sean's original patch, but all
      credit goes to Sean for doing this and also pointing out that
      sometimes the __pte_needs_invert() function only gets the protection
      bits, not the full eventual pte.  But zero remains special even in
      just protection bits, so that's ok.   - Linus ]
    
    Fixes: f22cc87f6c1f ("x86/speculation/l1tf: Invert all not present mappings")
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/pgtable-invert.h b/arch/x86/include/asm/pgtable-invert.h
index 44b1203ece12..a0c1525f1b6f 100644
--- a/arch/x86/include/asm/pgtable-invert.h
+++ b/arch/x86/include/asm/pgtable-invert.h
@@ -4,9 +4,18 @@
 
 #ifndef __ASSEMBLY__
 
+/*
+ * A clear pte value is special, and doesn't get inverted.
+ *
+ * Note that even users that only pass a pgprot_t (rather
+ * than a full pte) won't trigger the special zero case,
+ * because even PAGE_NONE has _PAGE_PROTNONE | _PAGE_ACCESSED
+ * set. So the all zero case really is limited to just the
+ * cleared page table entry case.
+ */
 static inline bool __pte_needs_invert(u64 val)
 {
-	return !(val & _PAGE_PRESENT);
+	return val && !(val & _PAGE_PRESENT);
 }
 
 /* Get a mask to xor with the page table entry to get the correct pfn. */

commit f22cc87f6c1f771b57c407555cfefd811cdd9507
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Aug 7 15:09:36 2018 -0700

    x86/speculation/l1tf: Invert all not present mappings
    
    For kernel mappings PAGE_PROTNONE is not necessarily set for a non present
    mapping, but the inversion logic explicitely checks for !PRESENT and
    PROT_NONE.
    
    Remove the PROT_NONE check and make the inversion unconditional for all not
    present mappings.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/pgtable-invert.h b/arch/x86/include/asm/pgtable-invert.h
index 177564187fc0..44b1203ece12 100644
--- a/arch/x86/include/asm/pgtable-invert.h
+++ b/arch/x86/include/asm/pgtable-invert.h
@@ -6,7 +6,7 @@
 
 static inline bool __pte_needs_invert(u64 val)
 {
-	return (val & (_PAGE_PRESENT|_PAGE_PROTNONE)) == _PAGE_PROTNONE;
+	return !(val & _PAGE_PRESENT);
 }
 
 /* Get a mask to xor with the page table entry to get the correct pfn. */

commit 6b28baca9b1f0d4a42b865da7a05b1c81424bd5c
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Jun 13 15:48:24 2018 -0700

    x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation
    
    When PTEs are set to PROT_NONE the kernel just clears the Present bit and
    preserves the PFN, which creates attack surface for L1TF speculation
    speculation attacks.
    
    This is important inside guests, because L1TF speculation bypasses physical
    page remapping. While the host has its own migitations preventing leaking
    data from other VMs into the guest, this would still risk leaking the wrong
    page inside the current guest.
    
    This uses the same technique as Linus' swap entry patch: while an entry is
    is in PROTNONE state invert the complete PFN part part of it. This ensures
    that the the highest bit will point to non existing memory.
    
    The invert is done by pte/pmd_modify and pfn/pmd/pud_pte for PROTNONE and
    pte/pmd/pud_pfn undo it.
    
    This assume that no code path touches the PFN part of a PTE directly
    without using these primitives.
    
    This doesn't handle the case that MMIO is on the top of the CPU physical
    memory. If such an MMIO region was exposed by an unpriviledged driver for
    mmap it would be possible to attack some real memory.  However this
    situation is all rather unlikely.
    
    For 32bit non PAE the inversion is not done because there are really not
    enough bits to protect anything.
    
    Q: Why does the guest need to be protected when the HyperVisor already has
       L1TF mitigations?
    
    A: Here's an example:
    
       Physical pages 1 2 get mapped into a guest as
       GPA 1 -> PA 2
       GPA 2 -> PA 1
       through EPT.
    
       The L1TF speculation ignores the EPT remapping.
    
       Now the guest kernel maps GPA 1 to process A and GPA 2 to process B, and
       they belong to different users and should be isolated.
    
       A sets the GPA 1 PA 2 PTE to PROT_NONE to bypass the EPT remapping and
       gets read access to the underlying physical page. Which in this case
       points to PA 2, so it can read process B's data, if it happened to be in
       L1, so isolation inside the guest is broken.
    
       There's nothing the hypervisor can do about this. This mitigation has to
       be done in the guest itself.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Dave Hansen <dave.hansen@intel.com>

diff --git a/arch/x86/include/asm/pgtable-invert.h b/arch/x86/include/asm/pgtable-invert.h
new file mode 100644
index 000000000000..177564187fc0
--- /dev/null
+++ b/arch/x86/include/asm/pgtable-invert.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_PGTABLE_INVERT_H
+#define _ASM_PGTABLE_INVERT_H 1
+
+#ifndef __ASSEMBLY__
+
+static inline bool __pte_needs_invert(u64 val)
+{
+	return (val & (_PAGE_PRESENT|_PAGE_PROTNONE)) == _PAGE_PROTNONE;
+}
+
+/* Get a mask to xor with the page table entry to get the correct pfn. */
+static inline u64 protnone_mask(u64 val)
+{
+	return __pte_needs_invert(val) ?  ~0ull : 0;
+}
+
+static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask)
+{
+	/*
+	 * When a PTE transitions from NONE to !NONE or vice-versa
+	 * invert the PFN part to stop speculation.
+	 * pte_pfn undoes this when needed.
+	 */
+	if (__pte_needs_invert(oldval) != __pte_needs_invert(val))
+		val = (val & ~mask) | (~val & mask);
+	return val;
+}
+
+#endif /* __ASSEMBLY__ */
+
+#endif
