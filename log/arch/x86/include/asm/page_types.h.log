commit c164fbb40c43f8041f4d05ec9996d8ee343c92b1
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Apr 10 14:33:24 2020 -0700

    x86/mm: thread pgprot_t through init_memory_mapping()
    
    In preparation to support a pgprot_t argument for arch_add_memory().
    
    It's required to move the prototype of init_memory_mapping() seeing the
    original location came before the definition of pgprot_t.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Eric Badger <ebadger@gigaio.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200306170846.9333-4-logang@deltatee.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index e27aa6be6320..a506a411474d 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -71,9 +71,6 @@ static inline phys_addr_t get_max_mapped(void)
 
 bool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn);
 
-extern unsigned long init_memory_mapping(unsigned long start,
-					 unsigned long end);
-
 extern void initmem_init(void);
 
 #endif	/* !__ASSEMBLY__ */

commit c62da0c35d58518ddb26ff641d2485596567fd96
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:05 2020 -0700

    mm/vma: define a default value for VM_DATA_DEFAULT_FLAGS
    
    There are many platforms with exact same value for VM_DATA_DEFAULT_FLAGS
    This creates a default value for VM_DATA_DEFAULT_FLAGS in line with the
    existing VM_STACK_DEFAULT_FLAGS.  While here, also define some more
    macros with standard VMA access flag combinations that are used
    frequently across many platforms.  Apart from simplification, this
    reduces code duplication as well.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Chris Zankel <chris@zankel.net>
    Link: http://lkml.kernel.org/r/1583391014-8170-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index c85e15010f48..e27aa6be6320 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -35,9 +35,7 @@
 
 #define PAGE_OFFSET		((unsigned long)__PAGE_OFFSET)
 
-#define VM_DATA_DEFAULT_FLAGS \
-	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0 ) | \
-	 VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define VM_DATA_DEFAULT_FLAGS	VM_DATA_FLAGS_TSK_EXEC
 
 #define __PHYSICAL_START	ALIGN(CONFIG_PHYSICAL_START, \
 				      CONFIG_PHYSICAL_ALIGN)

commit 94d49eb30e854c84d1319095b5dd0405a7da9362
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri May 18 14:30:28 2018 +0300

    x86/mm: Decouple dynamic __PHYSICAL_MASK from AMD SME
    
    AMD SME claims one bit from physical address to indicate whether the
    page is encrypted or not. To achieve that we clear out the bit from
    __PHYSICAL_MASK.
    
    The capability to adjust __PHYSICAL_MASK is required beyond AMD SME.
    For instance for upcoming Intel Multi-Key Total Memory Encryption.
    
    Factor it out into a separate feature with own Kconfig handle.
    
    It also helps with overhead of AMD SME. It saves more than 3k in .text
    on defconfig + AMD_MEM_ENCRYPT:
    
            add/remove: 3/2 grow/shrink: 5/110 up/down: 189/-3753 (-3564)
    
    We would need to return to this once we have infrastructure to patch
    constants in code. That's good candidate for it.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: linux-mm@kvack.org
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/20180518113028.79825-1-kirill.shutemov@linux.intel.com

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 1e53560a84bb..c85e15010f48 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -17,7 +17,6 @@
 #define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
 #define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))
 
-#define __PHYSICAL_MASK		((phys_addr_t)(__sme_clr((1ULL << __PHYSICAL_MASK_SHIFT) - 1)))
 #define __VIRTUAL_MASK		((1UL << __VIRTUAL_MASK_SHIFT) - 1)
 
 /* Cast *PAGE_MASK to a signed type so that it is sign-extended if
@@ -55,6 +54,13 @@
 
 #ifndef __ASSEMBLY__
 
+#ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
+extern phys_addr_t physical_mask;
+#define __PHYSICAL_MASK		physical_mask
+#else
+#define __PHYSICAL_MASK		((phys_addr_t)((1ULL << __PHYSICAL_MASK_SHIFT) - 1))
+#endif
+
 extern int devmem_is_allowed(unsigned long pagenr);
 
 extern unsigned long max_low_pfn_mapped;

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index b98ed9d14630..1e53560a84bb 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_PAGE_DEFS_H
 #define _ASM_X86_PAGE_DEFS_H
 

commit 21729f81ce8ae76a6995681d40e16f7ce8075db4
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:07 2017 -0500

    x86/mm: Provide general kernel support for memory encryption
    
    Changes to the existing page table macros will allow the SME support to
    be enabled in a simple fashion with minimal changes to files that use these
    macros.  Since the memory encryption mask will now be part of the regular
    pagetable macros, we introduce two new macros (_PAGE_TABLE_NOENC and
    _KERNPG_TABLE_NOENC) to allow for early pagetable creation/initialization
    without the encryption mask before SME becomes active.  Two new pgprot()
    macros are defined to allow setting or clearing the page encryption mask.
    
    The FIXMAP_PAGE_NOCACHE define is introduced for use with MMIO.  SME does
    not support encryption for MMIO areas so this define removes the encryption
    mask from the page attribute.
    
    Two new macros are introduced (__sme_pa() / __sme_pa_nodebug()) to allow
    creating a physical address with the encryption mask.  These are used when
    working with the cr3 register so that the PGD can be encrypted. The current
    __va() macro is updated so that the virtual address is generated based off
    of the physical address without the encryption mask thus allowing the same
    virtual address to be generated regardless of whether encryption is enabled
    for that physical location or not.
    
    Also, an early initialization function is added for SME.  If SME is active,
    this function:
    
     - Updates the early_pmd_flags so that early page faults create mappings
       with the encryption mask.
    
     - Updates the __supported_pte_mask to include the encryption mask.
    
     - Updates the protection_map entries to include the encryption mask so
       that user-space allocations will automatically have the encryption mask
       applied.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/b36e952c4c39767ae7f0a41cf5345adf27438480.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 7bd0099384ca..b98ed9d14630 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -3,6 +3,7 @@
 
 #include <linux/const.h>
 #include <linux/types.h>
+#include <linux/mem_encrypt.h>
 
 /* PAGE_SHIFT determines the page size */
 #define PAGE_SHIFT		12
@@ -15,7 +16,7 @@
 #define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
 #define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))
 
-#define __PHYSICAL_MASK		((phys_addr_t)((1ULL << __PHYSICAL_MASK_SHIFT) - 1))
+#define __PHYSICAL_MASK		((phys_addr_t)(__sme_clr((1ULL << __PHYSICAL_MASK_SHIFT) - 1)))
 #define __VIRTUAL_MASK		((1UL << __VIRTUAL_MASK_SHIFT) - 1)
 
 /* Cast *PAGE_MASK to a signed type so that it is sign-extended if

commit 4baf7fe40790c8ffdab54edc8e5b7051cfce3968
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Dec 7 10:24:28 2015 +0100

    x86/mm: Align macro defines
    
    Bring PAGE_{SHIFT,SIZE,MASK} to the same indentation level as the rest
    of the header.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1449480268-26583-1-git-send-email-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index cc071c6f7d4d..7bd0099384ca 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -5,9 +5,9 @@
 #include <linux/types.h>
 
 /* PAGE_SHIFT determines the page size */
-#define PAGE_SHIFT	12
-#define PAGE_SIZE	(_AC(1,UL) << PAGE_SHIFT)
-#define PAGE_MASK	(~(PAGE_SIZE-1))
+#define PAGE_SHIFT		12
+#define PAGE_SIZE		(_AC(1,UL) << PAGE_SHIFT)
+#define PAGE_MASK		(~(PAGE_SIZE-1))
 
 #define PMD_PAGE_SIZE		(_AC(1, UL) << PMD_SHIFT)
 #define PMD_PAGE_MASK		(~(PMD_PAGE_SIZE-1))

commit 70f1528747651b20c7769d3516ade369f9963237
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Nov 30 11:10:33 2015 +0100

    x86/mm: Fix regression with huge pages on PAE
    
    Recent PAT patchset has caused issue on 32-bit PAE machines:
    
      page:eea45000 count:0 mapcount:-128 mapping:  (null) index:0x0 flags: 0x40000000()
      page dumped because: VM_BUG_ON_PAGE(page_mapcount(page) < 0)
      ------------[ cut here ]------------
      kernel BUG at /home/build/linux-boris/mm/huge_memory.c:1485!
      invalid opcode: 0000 [#1] SMP
      [...]
      Call Trace:
       unmap_single_vma
       ? __wake_up
       unmap_vmas
       unmap_region
       do_munmap
       vm_munmap
       SyS_munmap
       do_fast_syscall_32
       ? __do_page_fault
       sysenter_past_esp
      Code: ...
      EIP: [<c11bde80>] zap_huge_pmd+0x240/0x260 SS:ESP 0068:f6459d98
    
    The problem is in pmd_pfn_mask() and pmd_flags_mask(). These
    helpers use PMD_PAGE_MASK to calculate resulting mask.
    PMD_PAGE_MASK is 'unsigned long', not 'unsigned long long' as
    phys_addr_t is on 32-bit PAE (ARCH_PHYS_ADDR_T_64BIT). As a
    result, the upper bits of resulting mask get truncated.
    
    pud_pfn_mask() and pud_flags_mask() aren't problematic since we
    don't have PUD page table level on 32-bit systems, but it's
    reasonable to keep them consistent with PMD counterpart.
    
    Introduce PHYSICAL_PMD_PAGE_MASK and PHYSICAL_PUD_PAGE_MASK in
    addition to existing PHYSICAL_PAGE_MASK and reworks helpers to
    use them.
    
    Reported-and-Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    [ Fix -Woverflow warnings from the realmode code. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hpe.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jürgen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: elliott@hpe.com
    Cc: konrad.wilk@oracle.com
    Cc: linux-mm <linux-mm@kvack.org>
    Fixes: f70abb0fc3da ("x86/asm: Fix pud/pmd interfaces to handle large PAT bit")
    Link: http://lkml.kernel.org/r/1448878233-11390-2-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index c5b7fb2774d0..cc071c6f7d4d 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -9,19 +9,21 @@
 #define PAGE_SIZE	(_AC(1,UL) << PAGE_SHIFT)
 #define PAGE_MASK	(~(PAGE_SIZE-1))
 
+#define PMD_PAGE_SIZE		(_AC(1, UL) << PMD_SHIFT)
+#define PMD_PAGE_MASK		(~(PMD_PAGE_SIZE-1))
+
+#define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
+#define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))
+
 #define __PHYSICAL_MASK		((phys_addr_t)((1ULL << __PHYSICAL_MASK_SHIFT) - 1))
 #define __VIRTUAL_MASK		((1UL << __VIRTUAL_MASK_SHIFT) - 1)
 
-/* Cast PAGE_MASK to a signed type so that it is sign-extended if
+/* Cast *PAGE_MASK to a signed type so that it is sign-extended if
    virtual addresses are 32-bits but physical addresses are larger
    (ie, 32-bit PAE). */
 #define PHYSICAL_PAGE_MASK	(((signed long)PAGE_MASK) & __PHYSICAL_MASK)
-
-#define PMD_PAGE_SIZE		(_AC(1, UL) << PMD_SHIFT)
-#define PMD_PAGE_MASK		(~(PMD_PAGE_SIZE-1))
-
-#define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
-#define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))
+#define PHYSICAL_PMD_PAGE_MASK	(((signed long)PMD_PAGE_MASK) & __PHYSICAL_MASK)
+#define PHYSICAL_PUD_PAGE_MASK	(((signed long)PUD_PAGE_MASK) & __PHYSICAL_MASK)
 
 #define HPAGE_SHIFT		PMD_SHIFT
 #define HPAGE_SIZE		(_AC(1,UL) << HPAGE_SHIFT)

commit 832102671855f73962e7a04fdafd48b9385ea5c6
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Thu Sep 17 12:24:15 2015 -0600

    x86/asm: Move PUD_PAGE macros to page_types.h
    
    PUD_SHIFT is defined according to a given kernel configuration, which
    allows it be commonly used by any x86 kernels.  However, PUD_PAGE_SIZE
    and PUD_PAGE_MASK, which are set from PUD_SHIFT, are defined in
    page_64_types.h, which can be used by 64-bit kernel only.
    
    Move PUD_PAGE_SIZE and PUD_PAGE_MASK to page_types.h so that they can
    be used by any x86 kernels as well.
    
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Konrad Wilk <konrad.wilk@oracle.com>
    Cc: Robert Elliot <elliott@hpe.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/1442514264-12475-3-git-send-email-toshi.kani@hpe.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index c7c712f2648b..c5b7fb2774d0 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -20,6 +20,9 @@
 #define PMD_PAGE_SIZE		(_AC(1, UL) << PMD_SHIFT)
 #define PMD_PAGE_MASK		(~(PMD_PAGE_SIZE-1))
 
+#define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
+#define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))
+
 #define HPAGE_SHIFT		PMD_SHIFT
 #define HPAGE_SIZE		(_AC(1,UL) << HPAGE_SHIFT)
 #define HPAGE_MASK		(~(HPAGE_SIZE - 1))

commit 5d72b4fba40ef4b3f7a1a11d6aacc85d9af81561
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Tue Apr 14 15:47:29 2015 -0700

    x86, mm: support huge I/O mapping capability I/F
    
    Implement huge I/O mapping capability interfaces for ioremap() on x86.
    
    IOREMAP_MAX_ORDER is defined to PUD_SHIFT on x86/64 and PMD_SHIFT on
    x86/32, which overrides the default value defined in <linux/vmalloc.h>.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Robert Elliott <Elliott@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index f97fbe3abb67..c7c712f2648b 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -40,8 +40,10 @@
 
 #ifdef CONFIG_X86_64
 #include <asm/page_64_types.h>
+#define IOREMAP_MAX_ORDER       (PUD_SHIFT)
 #else
 #include <asm/page_32_types.h>
+#define IOREMAP_MAX_ORDER       (PMD_SHIFT)
 #endif	/* CONFIG_X86_64 */
 
 #ifndef __ASSEMBLY__

commit 69797dafe35541bfff1989c0b37c66ed785faf0e
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Mar 16 11:06:28 2015 +0100

    Revert "x86/mm/ASLR: Propagate base load address calculation"
    
    This reverts commit:
    
      f47233c2d34f ("x86/mm/ASLR: Propagate base load address calculation")
    
    The main reason for the revert is that the new boot flag does not work
    at all currently, and in order to make this work, we need non-trivial
    changes to the x86 boot code which we didn't manage to get done in
    time for merging.
    
    And even if we did, they would've been too risky so instead of
    rushing things and break booting 4.1 on boxes left and right, we
    will be very strict and conservative and will take our time with
    this to fix and test it properly.
    
    Reported-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: H. Peter Anvin <hpa@linux.intel.com
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Junjie Mao <eternal.n08@gmail.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt.fleming@intel.com>
    Link: http://lkml.kernel.org/r/20150316100628.GD22995@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 95e11f79f123..f97fbe3abb67 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -51,8 +51,6 @@ extern int devmem_is_allowed(unsigned long pagenr);
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
 
-extern bool kaslr_enabled;
-
 static inline phys_addr_t get_max_mapped(void)
 {
 	return (phys_addr_t)max_pfn_mapped << PAGE_SHIFT;

commit 570e1aa84c376ff39809442f09c7606ddf62cfd1
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Feb 20 10:18:59 2015 +0100

    x86/mm/ASLR: Avoid PAGE_SIZE redefinition for UML subarch
    
    Commit f47233c2d34 ("x86/mm/ASLR: Propagate base load address
    calculation") causes PAGE_SIZE redefinition warnings for UML
    subarch  builds. This is caused by added includes that were
    leftovers from previous  patch versions are are not actually
    needed (especially page_types.h  inlcude in module.c). Drop
    those stray includes.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/alpine.LNX.2.00.1502201017240.28769@pobox.suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 3d43ce36eaba..95e11f79f123 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -3,7 +3,6 @@
 
 #include <linux/const.h>
 #include <linux/types.h>
-#include <asm/bootparam.h>
 
 /* PAGE_SHIFT determines the page size */
 #define PAGE_SHIFT	12

commit f47233c2d34f243ecdaac179c3408a39ff9216a7
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Feb 13 16:04:55 2015 +0100

    x86/mm/ASLR: Propagate base load address calculation
    
    Commit:
    
      e2b32e678513 ("x86, kaslr: randomize module base load address")
    
    makes the base address for module to be unconditionally randomized in
    case when CONFIG_RANDOMIZE_BASE is defined and "nokaslr" option isn't
    present on the commandline.
    
    This is not consistent with how choose_kernel_location() decides whether
    it will randomize kernel load base.
    
    Namely, CONFIG_HIBERNATION disables kASLR (unless "kaslr" option is
    explicitly specified on kernel commandline), which makes the state space
    larger than what module loader is looking at. IOW CONFIG_HIBERNATION &&
    CONFIG_RANDOMIZE_BASE is a valid config option, kASLR wouldn't be applied
    by default in that case, but module loader is not aware of that.
    
    Instead of fixing the logic in module.c, this patch takes more generic
    aproach. It introduces a new bootparam setup data_type SETUP_KASLR and
    uses that to pass the information whether kaslr has been applied during
    kernel decompression, and sets a global 'kaslr_enabled' variable
    accordingly, so that any kernel code (module loading, livepatching, ...)
    can make decisions based on its value.
    
    x86 module loader is converted to make use of this flag.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: "H. Peter Anvin" <hpa@linux.intel.com>
    Link: https://lkml.kernel.org/r/alpine.LNX.2.00.1502101411280.10719@pobox.suse.cz
    [ Always dump correct kaslr status when panicking ]
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index f97fbe3abb67..3d43ce36eaba 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -3,6 +3,7 @@
 
 #include <linux/const.h>
 #include <linux/types.h>
+#include <asm/bootparam.h>
 
 /* PAGE_SHIFT determines the page size */
 #define PAGE_SHIFT	12
@@ -51,6 +52,8 @@ extern int devmem_is_allowed(unsigned long pagenr);
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
 
+extern bool kaslr_enabled;
+
 static inline phys_addr_t get_max_mapped(void)
 {
 	return (phys_addr_t)max_pfn_mapped << PAGE_SHIFT;

commit 4ce7a8697cb795fda6bdf082c14743b4bcd551c3
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Jan 27 17:06:50 2014 -0800

    x86: revert wrong memblock current limit setting
    
    Dave reported big numa system booting is broken.
    
    It turns out that commit 5b6e529521d3 ("x86: memblock: set current limit
    to max low memory address") sets the limit to low wrongly.
    
    max_low_pfn_mapped is different from max_pfn_mapped.
    max_low_pfn_mapped is always under 4G.
    
    That will memblock_alloc_nid all go under 4G.
    
    Revert 5b6e529521d3 to fix a no-boot regression which was triggered by
    457ff1de2d24 ("lib/swiotlb.c: use memblock apis for early memory
    allocations").
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Reported-by: Dave Hansen <dave.hansen@intel.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 2f59cce3b38a..f97fbe3abb67 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -51,9 +51,9 @@ extern int devmem_is_allowed(unsigned long pagenr);
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
 
-static inline phys_addr_t get_max_low_mapped(void)
+static inline phys_addr_t get_max_mapped(void)
 {
-	return (phys_addr_t)max_low_pfn_mapped << PAGE_SHIFT;
+	return (phys_addr_t)max_pfn_mapped << PAGE_SHIFT;
 }
 
 bool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn);

commit 5b6e529521d35e1bcaa0fe43456d1bbb335cae5d
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Tue Jan 21 15:50:03 2014 -0800

    x86: memblock: set current limit to max low memory address
    
    The memblock current limit value is used to limit early boot memory
    allocations below max low memory address by default, as the kernel can
    access only to the low memory.
    
    Hence, set memblock current limit value to the max mapped low memory
    address instead of max mapped memory address.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Grygorii Strashko <grygorii.strashko@ti.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Paul Walmsley <paul@pwsan.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index f97fbe3abb67..2f59cce3b38a 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -51,9 +51,9 @@ extern int devmem_is_allowed(unsigned long pagenr);
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
 
-static inline phys_addr_t get_max_mapped(void)
+static inline phys_addr_t get_max_low_mapped(void)
 {
-	return (phys_addr_t)max_pfn_mapped << PAGE_SHIFT;
+	return (phys_addr_t)max_low_pfn_mapped << PAGE_SHIFT;
 }
 
 bool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn);

commit a02150610776f66b40257624822a879311592bb2
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Jul 8 09:15:17 2013 -0700

    x86, relocs: Move ELF relocation handling to C
    
    Moves the relocation handling into C, after decompression. This requires
    that the decompressed size is passed to the decompression routine as
    well so that relocations can be found. Only kernels that need relocation
    support will use the code (currently just x86_32), but this is laying
    the ground work for 64-bit using it in support of KASLR.
    
    Based on work by Neill Clift and Michael Davidson.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/20130708161517.GA4832@www.outflux.net
    Acked-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 54c97879195e..f97fbe3abb67 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -33,6 +33,11 @@
 	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0 ) | \
 	 VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
+#define __PHYSICAL_START	ALIGN(CONFIG_PHYSICAL_START, \
+				      CONFIG_PHYSICAL_ALIGN)
+
+#define __START_KERNEL		(__START_KERNEL_map + __PHYSICAL_START)
+
 #ifdef CONFIG_X86_64
 #include <asm/page_64_types.h>
 #else

commit 9985b4c6fa7d660f685918a58282275e9e35d8e0
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Nov 16 19:39:02 2012 -0800

    x86, mm: Move min_pfn_mapped back to mm/init.c
    
    Also change it to static.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1353123563-3103-26-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 9f6f3e66e84d..54c97879195e 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -45,7 +45,6 @@ extern int devmem_is_allowed(unsigned long pagenr);
 
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
-extern unsigned long min_pfn_mapped;
 
 static inline phys_addr_t get_max_mapped(void)
 {

commit 8d57470d8f859635deffe3919d7d4867b488b85a
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Nov 16 19:38:58 2012 -0800

    x86, mm: setup page table in top-down
    
    Get pgt_buf early from BRK, and use it to map PMD_SIZE from top at first.
    Then use mapped pages to map more ranges below, and keep looping until
    all pages get mapped.
    
    alloc_low_page will use page from BRK at first, after that buffer is used
    up, will use memblock to find and reserve pages for page table usage.
    
    Introduce min_pfn_mapped to make sure find new pages from mapped ranges,
    that will be updated when lower pages get mapped.
    
    Also add step_size to make sure that don't try to map too big range with
    limited mapped pages initially, and increase the step_size when we have
    more mapped pages on hand.
    
    We don't need to call pagetable_reserve anymore, reserve work is done
    in alloc_low_page() directly.
    
    At last we can get rid of calculation and find early pgt related code.
    
    -v2: update to after fix_xen change,
         also use MACRO for initial pgt_buf size and add comments with it.
    -v3: skip big reserved range in memblock.reserved near end.
    -v4: don't need fix_xen change now.
    -v5: add changelog about moving about reserving pagetable to alloc_low_page.
    
    Suggested-by: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1353123563-3103-22-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 54c97879195e..9f6f3e66e84d 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -45,6 +45,7 @@ extern int devmem_is_allowed(unsigned long pagenr);
 
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
+extern unsigned long min_pfn_mapped;
 
 static inline phys_addr_t get_max_mapped(void)
 {

commit 66520ebc2df3fe52eb4792f8101fac573b766baf
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Fri Nov 16 19:38:52 2012 -0800

    x86, mm: Only direct map addresses that are marked as E820_RAM
    
    Currently direct mappings are created for [ 0 to max_low_pfn<<PAGE_SHIFT )
    and [ 4GB to max_pfn<<PAGE_SHIFT ), which may include regions that are not
    backed by actual DRAM. This is fine for holes under 4GB which are covered
    by fixed and variable range MTRRs to be UC. However, we run into trouble
    on higher memory addresses which cannot be covered by MTRRs.
    
    Our system with 1TB of RAM has an e820 that looks like this:
    
     BIOS-e820: [mem 0x0000000000000000-0x00000000000983ff] usable
     BIOS-e820: [mem 0x0000000000098400-0x000000000009ffff] reserved
     BIOS-e820: [mem 0x00000000000d0000-0x00000000000fffff] reserved
     BIOS-e820: [mem 0x0000000000100000-0x00000000c7ebffff] usable
     BIOS-e820: [mem 0x00000000c7ec0000-0x00000000c7ed7fff] ACPI data
     BIOS-e820: [mem 0x00000000c7ed8000-0x00000000c7ed9fff] ACPI NVS
     BIOS-e820: [mem 0x00000000c7eda000-0x00000000c7ffffff] reserved
     BIOS-e820: [mem 0x00000000fec00000-0x00000000fec0ffff] reserved
     BIOS-e820: [mem 0x00000000fee00000-0x00000000fee00fff] reserved
     BIOS-e820: [mem 0x00000000fff00000-0x00000000ffffffff] reserved
     BIOS-e820: [mem 0x0000000100000000-0x000000e037ffffff] usable
     BIOS-e820: [mem 0x000000e038000000-0x000000fcffffffff] reserved
     BIOS-e820: [mem 0x0000010000000000-0x0000011ffeffffff] usable
    
    and so direct mappings are created for huge memory hole between
    0x000000e038000000 to 0x0000010000000000. Even though the kernel never
    generates memory accesses in that region, since the page tables mark
    them incorrectly as being WB, our (AMD) processor ends up causing a MCE
    while doing some memory bookkeeping/optimizations around that area.
    
    This patch iterates through e820 and only direct maps ranges that are
    marked as E820_RAM, and keeps track of those pfn ranges. Depending on
    the alignment of E820 ranges, this may possibly result in using smaller
    size (i.e. 4K instead of 2M or 1G) page tables.
    
    -v2: move changes from setup.c to mm/init.c, also use for_each_mem_pfn_range
            instead.  - Yinghai Lu
    -v3: add calculate_all_table_space_size() to get correct needed page table
            size. - Yinghai Lu
    -v4: fix add_pfn_range_mapped() to get correct max_low_pfn_mapped when
         mem map does have hole under 4g that is found by Konard on xen
         domU with 8g ram. - Yinghai
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Link: http://lkml.kernel.org/r/1353123563-3103-16-git-send-email-yinghai@kernel.org
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 45aae6e5f649..54c97879195e 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -51,13 +51,7 @@ static inline phys_addr_t get_max_mapped(void)
 	return (phys_addr_t)max_pfn_mapped << PAGE_SHIFT;
 }
 
-static inline bool pfn_range_is_mapped(unsigned long start_pfn,
-					unsigned long end_pfn)
-{
-	return end_pfn <= max_low_pfn_mapped ||
-	       (end_pfn > (1UL << (32 - PAGE_SHIFT)) &&
-		end_pfn <= max_pfn_mapped);
-}
+bool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn);
 
 extern unsigned long init_memory_mapping(unsigned long start,
 					 unsigned long end);

commit dda56e134059b840631fdfd034784056b627c2a6
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Fri Nov 16 19:38:48 2012 -0800

    x86, mm: Fixup code testing if a pfn is direct mapped
    
    Update code that previously assumed pfns [ 0 - max_low_pfn_mapped ) and
    [ 4GB - max_pfn_mapped ) were always direct mapped, to now look up
    pfn_mapped ranges instead.
    
    -v2: change applying sequence to keep git bisecting working.
         so add dummy pfn_range_is_mapped(). - Yinghai Lu
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Link: http://lkml.kernel.org/r/1353123563-3103-12-git-send-email-yinghai@kernel.org
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index e21fdd10479f..45aae6e5f649 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -51,6 +51,14 @@ static inline phys_addr_t get_max_mapped(void)
 	return (phys_addr_t)max_pfn_mapped << PAGE_SHIFT;
 }
 
+static inline bool pfn_range_is_mapped(unsigned long start_pfn,
+					unsigned long end_pfn)
+{
+	return end_pfn <= max_low_pfn_mapped ||
+	       (end_pfn > (1UL << (32 - PAGE_SHIFT)) &&
+		end_pfn <= max_pfn_mapped);
+}
+
 extern unsigned long init_memory_mapping(unsigned long start,
 					 unsigned long end);
 

commit 49a7f04a4b9d45cd794741ce3d5d66524b37bdd0
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Move all declarations of free_initmem() to linux/mm.h
    
    Move all declarations of free_initmem() to linux/mm.h so that there's only one
    and it's used by everything.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-c6x-dev@linux-c6x.org
    cc: microblaze-uclinux@itee.uq.edu.au
    cc: linux-sh@vger.kernel.org
    cc: sparclinux@vger.kernel.org
    cc: x86@kernel.org
    cc: linux-mm@kvack.org

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index bce688d54c12..e21fdd10479f 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -55,7 +55,6 @@ extern unsigned long init_memory_mapping(unsigned long start,
 					 unsigned long end);
 
 extern void initmem_init(void);
-extern void free_initmem(void);
 
 #endif	/* !__ASSEMBLY__ */
 

commit f89112502805c1f6a6955f90ad158e538edb319d
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Mar 4 10:26:36 2011 +0100

    x86-64, NUMA: Revert NUMA affine page table allocation
    
    This patch reverts NUMA affine page table allocation added by commit
    1411e0ec31 (x86-64, numa: Put pgtable to local node memory).
    
    The commit made an undocumented change where the kernel linear mapping
    strictly follows intersection of e820 memory map and NUMA
    configuration.  If the physical memory configuration has holes or NUMA
    nodes are not properly aligned, this leads to using unnecessarily
    smaller mapping size which leads to increased TLB pressure.  For
    details,
    
      http://thread.gmane.org/gmane.linux.kernel/1104672
    
    Patches to fix the problem have been proposed but the underlying code
    needs more cleanup and the approach itself seems a bit heavy handed
    and it has been determined to revert the feature for now and come back
    to it in the next developement cycle.
    
      http://thread.gmane.org/gmane.linux.kernel/1105959
    
    As init_memory_mapping_high() callsites have been consolidated since
    the commit, reverting is done manually.  Also, the RED-PEN comment in
    arch/x86/mm/init.c is not restored as the problem no longer exists
    with memblock based top-down early memory allocation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 97e6007e4edd..bce688d54c12 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -54,8 +54,6 @@ static inline phys_addr_t get_max_mapped(void)
 extern unsigned long init_memory_mapping(unsigned long start,
 					 unsigned long end);
 
-void init_memory_mapping_high(void);
-
 extern void initmem_init(void);
 extern void free_initmem(void);
 

commit d8fc3afc49bb226c20e37f48a4ddd493cd092837
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 16 12:13:06 2011 +0100

    x86, NUMA: Move *_numa_init() invocations into initmem_init()
    
    There's no reason for these to live in setup_arch().  Move them inside
    initmem_init().
    
    - v2: x86-32 initmem_init() weren't updated breaking 32bit builds.
      Fixed.  Found by Ankita.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ankita Garg <ankita@in.ibm.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Shaohui Zheng <shaohui.zheng@intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index eb9ed00355a8..97e6007e4edd 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -56,7 +56,7 @@ extern unsigned long init_memory_mapping(unsigned long start,
 
 void init_memory_mapping_high(void);
 
-extern void initmem_init(int acpi, int k8);
+extern void initmem_init(void);
 extern void free_initmem(void);
 
 #endif	/* !__ASSEMBLY__ */

commit 86ef4dbf1f736bb1a4d567e043e3dd81b8b7860c
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 16 12:13:06 2011 +0100

    x86, NUMA: Drop @start/last_pfn from initmem_init()
    
    initmem_init() extensively accesses and modifies global data
    structures and the parameters aren't even followed depending on which
    path is being used.  Drop @start/last_pfn and let it deal with
    @max_pfn directly.  This is in preparation for further NUMA init
    cleanups.
    
    - v2: x86-32 initmem_init() weren't updated breaking 32bit builds.
      Fixed.  Found by Yinghai.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Shaohui Zheng <shaohui.zheng@intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 731d211a1b20..eb9ed00355a8 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -56,8 +56,7 @@ extern unsigned long init_memory_mapping(unsigned long start,
 
 void init_memory_mapping_high(void);
 
-extern void initmem_init(unsigned long start_pfn, unsigned long end_pfn,
-				int acpi, int k8);
+extern void initmem_init(int acpi, int k8);
 extern void free_initmem(void);
 
 #endif	/* !__ASSEMBLY__ */

commit 1411e0ec3123ae4c4ead6bfc9fe3ee5a3ae5c327
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Dec 27 16:48:17 2010 -0800

    x86-64, numa: Put pgtable to local node memory
    
    Introduce init_memory_mapping_high(), and use it with 64bit.
    
    It will go with every memory segment above 4g to create page table to the
    memory range itself.
    
    before this patch all page tables was on one node.
    
    with this patch, one RED-PEN is killed
    
    debug out for 8 sockets system after patch
    [    0.000000] initial memory mapped : 0 - 20000000
    [    0.000000] init_memory_mapping: [0x00000000000000-0x0000007f74ffff]
    [    0.000000]  0000000000 - 007f600000 page 2M
    [    0.000000]  007f600000 - 007f750000 page 4k
    [    0.000000] kernel direct mapping tables up to 7f750000 @ [0x7f74c000-0x7f74ffff]
    [    0.000000] RAMDISK: 7bc84000 - 7f745000
    ....
    [    0.000000] Adding active range (0, 0x10, 0x95) 0 entries of 3200 used
    [    0.000000] Adding active range (0, 0x100, 0x7f750) 1 entries of 3200 used
    [    0.000000] Adding active range (0, 0x100000, 0x1080000) 2 entries of 3200 used
    [    0.000000] Adding active range (1, 0x1080000, 0x2080000) 3 entries of 3200 used
    [    0.000000] Adding active range (2, 0x2080000, 0x3080000) 4 entries of 3200 used
    [    0.000000] Adding active range (3, 0x3080000, 0x4080000) 5 entries of 3200 used
    [    0.000000] Adding active range (4, 0x4080000, 0x5080000) 6 entries of 3200 used
    [    0.000000] Adding active range (5, 0x5080000, 0x6080000) 7 entries of 3200 used
    [    0.000000] Adding active range (6, 0x6080000, 0x7080000) 8 entries of 3200 used
    [    0.000000] Adding active range (7, 0x7080000, 0x8080000) 9 entries of 3200 used
    [    0.000000] init_memory_mapping: [0x00000100000000-0x0000107fffffff]
    [    0.000000]  0100000000 - 1080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 1080000000 @ [0x107ffbd000-0x107fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x107ffc2000-0x107fffffff]          PGTABLE
    [    0.000000] init_memory_mapping: [0x00001080000000-0x0000207fffffff]
    [    0.000000]  1080000000 - 2080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 2080000000 @ [0x207ff7d000-0x207fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x207ffc0000-0x207fffffff]          PGTABLE
    [    0.000000] init_memory_mapping: [0x00002080000000-0x0000307fffffff]
    [    0.000000]  2080000000 - 3080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 3080000000 @ [0x307ff3d000-0x307fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x307ffc0000-0x307fffffff]          PGTABLE
    [    0.000000] init_memory_mapping: [0x00003080000000-0x0000407fffffff]
    [    0.000000]  3080000000 - 4080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 4080000000 @ [0x407fefd000-0x407fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x407ffc0000-0x407fffffff]          PGTABLE
    [    0.000000] init_memory_mapping: [0x00004080000000-0x0000507fffffff]
    [    0.000000]  4080000000 - 5080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 5080000000 @ [0x507febd000-0x507fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x507ffc0000-0x507fffffff]          PGTABLE
    [    0.000000] init_memory_mapping: [0x00005080000000-0x0000607fffffff]
    [    0.000000]  5080000000 - 6080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 6080000000 @ [0x607fe7d000-0x607fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x607ffc0000-0x607fffffff]          PGTABLE
    [    0.000000] init_memory_mapping: [0x00006080000000-0x0000707fffffff]
    [    0.000000]  6080000000 - 7080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 7080000000 @ [0x707fe3d000-0x707fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x707ffc0000-0x707fffffff]          PGTABLE
    [    0.000000] init_memory_mapping: [0x00007080000000-0x0000807fffffff]
    [    0.000000]  7080000000 - 8080000000 page 2M
    [    0.000000] kernel direct mapping tables up to 8080000000 @ [0x807fdfc000-0x807fffffff]
    [    0.000000]     memblock_x86_reserve_range: [0x807ffbf000-0x807fffffff]          PGTABLE
    [    0.000000] Initmem setup node 0 [0000000000000000-000000107fffffff]
    [    0.000000]   NODE_DATA [0x0000107ffbd000-0x0000107ffc1fff]
    [    0.000000] Initmem setup node 1 [0000001080000000-000000207fffffff]
    [    0.000000]   NODE_DATA [0x0000207ffbb000-0x0000207ffbffff]
    [    0.000000] Initmem setup node 2 [0000002080000000-000000307fffffff]
    [    0.000000]   NODE_DATA [0x0000307ffbb000-0x0000307ffbffff]
    [    0.000000] Initmem setup node 3 [0000003080000000-000000407fffffff]
    [    0.000000]   NODE_DATA [0x0000407ffbb000-0x0000407ffbffff]
    [    0.000000] Initmem setup node 4 [0000004080000000-000000507fffffff]
    [    0.000000]   NODE_DATA [0x0000507ffbb000-0x0000507ffbffff]
    [    0.000000] Initmem setup node 5 [0000005080000000-000000607fffffff]
    [    0.000000]   NODE_DATA [0x0000607ffbb000-0x0000607ffbffff]
    [    0.000000] Initmem setup node 6 [0000006080000000-000000707fffffff]
    [    0.000000]   NODE_DATA [0x0000707ffbb000-0x0000707ffbffff]
    [    0.000000] Initmem setup node 7 [0000007080000000-000000807fffffff]
    [    0.000000]   NODE_DATA [0x0000807ffba000-0x0000807ffbefff]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <4D1933D1.9020609@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 93626e699679..731d211a1b20 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -54,6 +54,8 @@ static inline phys_addr_t get_max_mapped(void)
 extern unsigned long init_memory_mapping(unsigned long start,
 					 unsigned long end);
 
+void init_memory_mapping_high(void);
+
 extern void initmem_init(unsigned long start_pfn, unsigned long end_pfn,
 				int acpi, int k8);
 extern void free_initmem(void);

commit 45635ab5e41bcde94a82f9a05d660ef77fe38c1b
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Dec 27 16:47:54 2010 -0800

    x86: Change get_max_mapped() to inline
    
    Move it into head file. to prepare use it in other files.
    
    [ hpa: added missing <linux/types.h> and changed type to phys_addr_t. ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <4D1933BA.8000508@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 1df66211fd1b..93626e699679 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -2,6 +2,7 @@
 #define _ASM_X86_PAGE_DEFS_H
 
 #include <linux/const.h>
+#include <linux/types.h>
 
 /* PAGE_SHIFT determines the page size */
 #define PAGE_SHIFT	12
@@ -45,6 +46,11 @@ extern int devmem_is_allowed(unsigned long pagenr);
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
 
+static inline phys_addr_t get_max_mapped(void)
+{
+	return (phys_addr_t)max_pfn_mapped << PAGE_SHIFT;
+}
+
 extern unsigned long init_memory_mapping(unsigned long start,
 					 unsigned long end);
 

commit a416e9e1dde0fbcf20cda59df284cc0dcf2aadc4
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Wed Sep 29 23:29:48 2010 +0900

    x86-32: Fix sparse warning for the __PHYSICAL_MASK calculation
    
    On 32-bit non-PAE system, cast to 'phys_addr_t' truncates value
    before subtraction. Subtracting before cast produce same result
    but remove following warnings from sparse:
    
     arch/x86/include/asm/pgtable_types.h:255:38: warning: cast truncates bits from constant value (100000000 becomes 0)
     arch/x86/include/asm/pgtable_types.h:270:38: warning: cast truncates bits from constant value (100000000 becomes 0)
     arch/x86/include/asm/pgtable.h:127:32: warning: cast truncates bits from constant value (100000000 becomes 0)
     arch/x86/include/asm/pgtable.h:132:32: warning: cast truncates bits from constant value (100000000 becomes 0)
     arch/x86/include/asm/pgtable.h:344:31: warning: cast truncates bits from constant value (100000000 becomes 0)
    
    64-bit or PAE machines will not be affected by this change.
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    LKML-Reference: <1285770588-14065-1-git-send-email-namhyung@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index a667f24c7254..1df66211fd1b 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -8,7 +8,7 @@
 #define PAGE_SIZE	(_AC(1,UL) << PAGE_SHIFT)
 #define PAGE_MASK	(~(PAGE_SIZE-1))
 
-#define __PHYSICAL_MASK		((phys_addr_t)(1ULL << __PHYSICAL_MASK_SHIFT) - 1)
+#define __PHYSICAL_MASK		((phys_addr_t)((1ULL << __PHYSICAL_MASK_SHIFT) - 1))
 #define __VIRTUAL_MASK		((1UL << __VIRTUAL_MASK_SHIFT) - 1)
 
 /* Cast PAGE_MASK to a signed type so that it is sign-extended if

commit 13ca0fcaa33f6b1984c4111b6ec5df42689fea6f
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Fri Jan 22 11:21:05 2010 +0800

    x86: Use the generic page_is_ram()
    
    The generic resource based page_is_ram() works better with memory
    hotplug/hotremove. So switch the x86 e820map based code to it.
    
    CC: Andi Kleen <andi@firstfloor.org>
    CC: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    CC: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    LKML-Reference: <20100122033004.470767217@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 642fe34b36a2..a667f24c7254 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -40,7 +40,6 @@
 
 #ifndef __ASSEMBLY__
 
-extern int page_is_ram(unsigned long pagenr);
 extern int devmem_is_allowed(unsigned long pagenr);
 
 extern unsigned long max_low_pfn_mapped;

commit 8ee2debce32412118cf8c239e0026ace56ea1425
Author: David Rientjes <rientjes@google.com>
Date:   Fri Sep 25 15:20:00 2009 -0700

    x86: Export k8 physical topology
    
    To eventually interleave emulated nodes over physical nodes, we
    need to know the physical topology of the machine without actually
    registering it.  This does the k8 node setup in two parts:
    detection and registration.  NUMA emulation can then used the
    physical topology detected to setup the address ranges of emulated
    nodes accordingly.  If emulation isn't used, the k8 nodes are
    registered as normal.
    
    Two formals are added to the x86 NUMA setup functions: `acpi' and
    `k8'. These represent whether ACPI or K8 NUMA has been detected;
    both cannot be true at the same time.  This specifies to the NUMA
    emulation code whether an underlying physical NUMA topology exists
    and which interface to use.
    
    This patch deals solely with separating the k8 setup path into
    Northbridge detection and registration steps and leaves the ACPI
    changes for a subsequent patch.  The `acpi' formal is added here,
    however, to avoid touching all the header files again in the next
    patch.
    
    This approach also ensures emulated nodes will not span physical
    nodes so the true memory latency is not misrepresented.
    
    k8_get_nodes() may now be used to export the k8 physical topology
    of the machine for NUMA emulation.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Ankita Garg <ankita@in.ibm.com>
    Cc: Len Brown <len.brown@intel.com>
    LKML-Reference: <alpine.DEB.1.00.0909251518400.14754@chino.kir.corp.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 6473f5ccff85..642fe34b36a2 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -49,7 +49,8 @@ extern unsigned long max_pfn_mapped;
 extern unsigned long init_memory_mapping(unsigned long start,
 					 unsigned long end);
 
-extern void initmem_init(unsigned long start_pfn, unsigned long end_pfn);
+extern void initmem_init(unsigned long start_pfn, unsigned long end_pfn,
+				int acpi, int k8);
 extern void free_initmem(void);
 
 #endif	/* !__ASSEMBLY__ */

commit 66aa230e437d89ca56224135f617e2d8e391a3ef
Author: Jaswinder Singh Rajput <jaswinder@kernel.org>
Date:   Tue Apr 14 12:54:29 2009 +0530

    x86: page_types.h unification of declarations
    
    Impact: unification of declarations, cleanup
    
    Unification of declarations:
     moved init_memory_mapping, initmem_init and free_initmem from
    page_XX_types.h to page_types.h
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <1239693869.3033.31.camel@ht.satnam>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 826ad37006ab..6473f5ccff85 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -46,6 +46,12 @@ extern int devmem_is_allowed(unsigned long pagenr);
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;
 
+extern unsigned long init_memory_mapping(unsigned long start,
+					 unsigned long end);
+
+extern void initmem_init(unsigned long start_pfn, unsigned long end_pfn);
+extern void free_initmem(void);
+
 #endif	/* !__ASSEMBLY__ */
 
 #endif	/* _ASM_X86_PAGE_DEFS_H */

commit a964e33c5d7c0ea46376d20c2f02edf01c9db251
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 17:14:30 2009 -0800

    x86: clean up old gcc warnings
    
    gcc 3.2.2 reports:
    
    In file included from /usr/src/all/linux-next/arch/x86/include/asm/page.h:8,
                     from /usr/src/all/linux-next/arch/x86/include/asm/processor.h:18,
                     from /usr/src/all/linux-next/arch/x86/include/asm/atomic_32.h:6,
                     from /usr/src/all/linux-next/arch/x86/include/asm/atomic.h:2,
                     from include/linux/crypto.h:20,
                     from arch/x86/kernel/asm-offsets_32.c:7,
                     from arch/x86/kernel/asm-offsets.c:2:
    /usr/src/all/linux-next/arch/x86/include/asm/page_types.h:54: warning: parameter has incomplete type
    /usr/src/all/linux-next/arch/x86/include/asm/page_types.h:56: warning: parameter has incomplete type
    In file included from /usr/src/all/linux-next/arch/x86/include/asm/page.h:8,
                     from /usr/src/all/linux-next/arch/x86/include/asm/processor.h:18,
                     from include/linux/prefetch.h:14,
                     from include/linux/list.h:6,
                     from include/linux/module.h:9,
                     from init/main.c:13:
    /usr/src/all/linux-next/arch/x86/include/asm/page_types.h:54: warning: parameter has incomplete type
    /usr/src/all/linux-next/arch/x86/include/asm/page_types.h:56: warning: parameter has incomplete type
    
    This is a bogus warning, but moving the pat-related functions
    into asm/pat.h and including asm/pgtable_types.h should fix it.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 2d625da6603c..826ad37006ab 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -40,14 +40,8 @@
 
 #ifndef __ASSEMBLY__
 
-struct pgprot;
-
 extern int page_is_ram(unsigned long pagenr);
 extern int devmem_is_allowed(unsigned long pagenr);
-extern void map_devmem(unsigned long pfn, unsigned long size,
-		       struct pgprot vma_prot);
-extern void unmap_devmem(unsigned long pfn, unsigned long size,
-			 struct pgprot vma_prot);
 
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;

commit 9b3651cbc26cfcea8276ecaff66718ea087f2e91
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Feb 13 11:01:54 2009 -0800

    x86: move more pagetable-related definitions into pgtable*.h
    
    PAGETABLE_LEVELS and the PTE masks should be in pgtable*.h
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 2c52ff767584..2d625da6603c 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -16,12 +16,6 @@
    (ie, 32-bit PAE). */
 #define PHYSICAL_PAGE_MASK	(((signed long)PAGE_MASK) & __PHYSICAL_MASK)
 
-/* PTE_PFN_MASK extracts the PFN from a (pte|pmd|pud|pgd)val_t */
-#define PTE_PFN_MASK		((pteval_t)PHYSICAL_PAGE_MASK)
-
-/* PTE_FLAGS_MASK extracts the flags from a (pte|pmd|pud|pgd)val_t */
-#define PTE_FLAGS_MASK		(~PTE_PFN_MASK)
-
 #define PMD_PAGE_SIZE		(_AC(1, UL) << PMD_SHIFT)
 #define PMD_PAGE_MASK		(~(PMD_PAGE_SIZE-1))
 

commit b233969eaa98c7b339d955fe25a58bf6bf25739a
Merge: d040c1614c24 54321d947ae9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 13:09:00 2009 +0100

    Merge branch 'x86/untangle2' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen into x86/headers
    
    Conflicts:
            arch/x86/include/asm/page.h
            arch/x86/include/asm/pgtable.h
            arch/x86/mach-voyager/voyager_smp.c
            arch/x86/mm/fault.c

commit 54321d947ae9d6a051b81e3eccaf2d8658aeecc6
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Feb 11 10:20:05 2009 -0800

    x86: move pte types into pgtable*.h
    
    pgtable*.h is intended for definitions relating to actual pagetables
    and their entries, so move all the definitions for
    (pte|pmd|pud|pgd)(val)?_t to the appropriate pgtable*.h headers.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 0733853e7c87..9f0c95963358 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -46,106 +46,15 @@
 
 #ifndef __ASSEMBLY__
 
-#include <linux/types.h>
-
-typedef struct { pgdval_t pgd; } pgd_t;
-typedef struct { pgprotval_t pgprot; } pgprot_t;
-
-static inline pgd_t native_make_pgd(pgdval_t val)
-{
-	return (pgd_t) { val };
-}
-
-static inline pgdval_t native_pgd_val(pgd_t pgd)
-{
-	return pgd.pgd;
-}
-
-static inline pgdval_t pgd_flags(pgd_t pgd)
-{
-	return native_pgd_val(pgd) & PTE_FLAGS_MASK;
-}
-
-#if PAGETABLE_LEVELS > 3
-typedef struct { pudval_t pud; } pud_t;
-
-static inline pud_t native_make_pud(pmdval_t val)
-{
-	return (pud_t) { val };
-}
-
-static inline pudval_t native_pud_val(pud_t pud)
-{
-	return pud.pud;
-}
-#else
-#include <asm-generic/pgtable-nopud.h>
-
-static inline pudval_t native_pud_val(pud_t pud)
-{
-	return native_pgd_val(pud.pgd);
-}
-#endif
-
-#if PAGETABLE_LEVELS > 2
-typedef struct { pmdval_t pmd; } pmd_t;
-
-static inline pmd_t native_make_pmd(pmdval_t val)
-{
-	return (pmd_t) { val };
-}
-
-static inline pmdval_t native_pmd_val(pmd_t pmd)
-{
-	return pmd.pmd;
-}
-#else
-#include <asm-generic/pgtable-nopmd.h>
-
-static inline pmdval_t native_pmd_val(pmd_t pmd)
-{
-	return native_pgd_val(pmd.pud.pgd);
-}
-#endif
-
-static inline pudval_t pud_flags(pud_t pud)
-{
-	return native_pud_val(pud) & PTE_FLAGS_MASK;
-}
-
-static inline pmdval_t pmd_flags(pmd_t pmd)
-{
-	return native_pmd_val(pmd) & PTE_FLAGS_MASK;
-}
-
-static inline pte_t native_make_pte(pteval_t val)
-{
-	return (pte_t) { .pte = val };
-}
-
-static inline pteval_t native_pte_val(pte_t pte)
-{
-	return pte.pte;
-}
-
-static inline pteval_t pte_flags(pte_t pte)
-{
-	return native_pte_val(pte) & PTE_FLAGS_MASK;
-}
-
-#define pgprot_val(x)	((x).pgprot)
-#define __pgprot(x)	((pgprot_t) { (x) } )
-
-
-typedef struct page *pgtable_t;
+struct pgprot;
 
 extern int page_is_ram(unsigned long pagenr);
 extern int pagerange_is_ram(unsigned long start, unsigned long end);
 extern int devmem_is_allowed(unsigned long pagenr);
 extern void map_devmem(unsigned long pfn, unsigned long size,
-		       pgprot_t vma_prot);
+		       struct pgprot vma_prot);
 extern void unmap_devmem(unsigned long pfn, unsigned long size,
-			 pgprot_t vma_prot);
+			 struct pgprot vma_prot);
 
 extern unsigned long max_low_pfn_mapped;
 extern unsigned long max_pfn_mapped;

commit e2f5bda94152fa567f6b48126741014123f982b8
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Feb 9 00:09:52 2009 -0800

    x86: define pud_flags and pud_large properly to allow non-PAE builds

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index c41e3e8f2271..0733853e7c87 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -90,11 +90,6 @@ static inline pudval_t native_pud_val(pud_t pud)
 #if PAGETABLE_LEVELS > 2
 typedef struct { pmdval_t pmd; } pmd_t;
 
-static inline pudval_t pud_flags(pud_t pud)
-{
-	return native_pud_val(pud) & PTE_FLAGS_MASK;
-}
-
 static inline pmd_t native_make_pmd(pmdval_t val)
 {
 	return (pmd_t) { val };
@@ -113,6 +108,11 @@ static inline pmdval_t native_pmd_val(pmd_t pmd)
 }
 #endif
 
+static inline pudval_t pud_flags(pud_t pud)
+{
+	return native_pud_val(pud) & PTE_FLAGS_MASK;
+}
+
 static inline pmdval_t pmd_flags(pmd_t pmd)
 {
 	return native_pmd_val(pmd) & PTE_FLAGS_MASK;

commit e42778de31d78ae262a3b901264eabefb9c3b51b
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sun Feb 8 23:42:01 2009 -0800

    x86: move defs around to allow paravirt.h to just include page_types.h
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 92dfd251a659..c41e3e8f2271 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -51,18 +51,92 @@
 typedef struct { pgdval_t pgd; } pgd_t;
 typedef struct { pgprotval_t pgprot; } pgprot_t;
 
+static inline pgd_t native_make_pgd(pgdval_t val)
+{
+	return (pgd_t) { val };
+}
+
+static inline pgdval_t native_pgd_val(pgd_t pgd)
+{
+	return pgd.pgd;
+}
+
+static inline pgdval_t pgd_flags(pgd_t pgd)
+{
+	return native_pgd_val(pgd) & PTE_FLAGS_MASK;
+}
+
 #if PAGETABLE_LEVELS > 3
 typedef struct { pudval_t pud; } pud_t;
+
+static inline pud_t native_make_pud(pmdval_t val)
+{
+	return (pud_t) { val };
+}
+
+static inline pudval_t native_pud_val(pud_t pud)
+{
+	return pud.pud;
+}
 #else
 #include <asm-generic/pgtable-nopud.h>
+
+static inline pudval_t native_pud_val(pud_t pud)
+{
+	return native_pgd_val(pud.pgd);
+}
 #endif
 
 #if PAGETABLE_LEVELS > 2
 typedef struct { pmdval_t pmd; } pmd_t;
+
+static inline pudval_t pud_flags(pud_t pud)
+{
+	return native_pud_val(pud) & PTE_FLAGS_MASK;
+}
+
+static inline pmd_t native_make_pmd(pmdval_t val)
+{
+	return (pmd_t) { val };
+}
+
+static inline pmdval_t native_pmd_val(pmd_t pmd)
+{
+	return pmd.pmd;
+}
 #else
 #include <asm-generic/pgtable-nopmd.h>
+
+static inline pmdval_t native_pmd_val(pmd_t pmd)
+{
+	return native_pgd_val(pmd.pud.pgd);
+}
 #endif
 
+static inline pmdval_t pmd_flags(pmd_t pmd)
+{
+	return native_pmd_val(pmd) & PTE_FLAGS_MASK;
+}
+
+static inline pte_t native_make_pte(pteval_t val)
+{
+	return (pte_t) { .pte = val };
+}
+
+static inline pteval_t native_pte_val(pte_t pte)
+{
+	return pte.pte;
+}
+
+static inline pteval_t pte_flags(pte_t pte)
+{
+	return native_pte_val(pte) & PTE_FLAGS_MASK;
+}
+
+#define pgprot_val(x)	((x).pgprot)
+#define __pgprot(x)	((pgprot_t) { (x) } )
+
+
 typedef struct page *pgtable_t;
 
 extern int page_is_ram(unsigned long pagenr);

commit 1dfc07aad5479f1fb832ff6f61a5a9ce822d9e1f
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sun Feb 8 23:24:26 2009 -0800

    x86: move 2 and 3 level asm-generic defs into page-defs
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
index 65787ad4c59f..92dfd251a659 100644
--- a/arch/x86/include/asm/page_types.h
+++ b/arch/x86/include/asm/page_types.h
@@ -53,10 +53,14 @@ typedef struct { pgprotval_t pgprot; } pgprot_t;
 
 #if PAGETABLE_LEVELS > 3
 typedef struct { pudval_t pud; } pud_t;
+#else
+#include <asm-generic/pgtable-nopud.h>
 #endif
 
 #if PAGETABLE_LEVELS > 2
 typedef struct { pmdval_t pmd; } pmd_t;
+#else
+#include <asm-generic/pgtable-nopmd.h>
 #endif
 
 typedef struct page *pgtable_t;

commit 51c78eb3f0eb033f9fb4f2316851df1d9b07b953
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sun Feb 8 22:52:14 2009 -0800

    x86: create _types.h counterparts for page*.h
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>

diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
new file mode 100644
index 000000000000..65787ad4c59f
--- /dev/null
+++ b/arch/x86/include/asm/page_types.h
@@ -0,0 +1,77 @@
+#ifndef _ASM_X86_PAGE_DEFS_H
+#define _ASM_X86_PAGE_DEFS_H
+
+#include <linux/const.h>
+
+/* PAGE_SHIFT determines the page size */
+#define PAGE_SHIFT	12
+#define PAGE_SIZE	(_AC(1,UL) << PAGE_SHIFT)
+#define PAGE_MASK	(~(PAGE_SIZE-1))
+
+#define __PHYSICAL_MASK		((phys_addr_t)(1ULL << __PHYSICAL_MASK_SHIFT) - 1)
+#define __VIRTUAL_MASK		((1UL << __VIRTUAL_MASK_SHIFT) - 1)
+
+/* Cast PAGE_MASK to a signed type so that it is sign-extended if
+   virtual addresses are 32-bits but physical addresses are larger
+   (ie, 32-bit PAE). */
+#define PHYSICAL_PAGE_MASK	(((signed long)PAGE_MASK) & __PHYSICAL_MASK)
+
+/* PTE_PFN_MASK extracts the PFN from a (pte|pmd|pud|pgd)val_t */
+#define PTE_PFN_MASK		((pteval_t)PHYSICAL_PAGE_MASK)
+
+/* PTE_FLAGS_MASK extracts the flags from a (pte|pmd|pud|pgd)val_t */
+#define PTE_FLAGS_MASK		(~PTE_PFN_MASK)
+
+#define PMD_PAGE_SIZE		(_AC(1, UL) << PMD_SHIFT)
+#define PMD_PAGE_MASK		(~(PMD_PAGE_SIZE-1))
+
+#define HPAGE_SHIFT		PMD_SHIFT
+#define HPAGE_SIZE		(_AC(1,UL) << HPAGE_SHIFT)
+#define HPAGE_MASK		(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+
+#define HUGE_MAX_HSTATE 2
+
+#define PAGE_OFFSET		((unsigned long)__PAGE_OFFSET)
+
+#define VM_DATA_DEFAULT_FLAGS \
+	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0 ) | \
+	 VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+#ifdef CONFIG_X86_64
+#include <asm/page_64_types.h>
+#else
+#include <asm/page_32_types.h>
+#endif	/* CONFIG_X86_64 */
+
+#ifndef __ASSEMBLY__
+
+#include <linux/types.h>
+
+typedef struct { pgdval_t pgd; } pgd_t;
+typedef struct { pgprotval_t pgprot; } pgprot_t;
+
+#if PAGETABLE_LEVELS > 3
+typedef struct { pudval_t pud; } pud_t;
+#endif
+
+#if PAGETABLE_LEVELS > 2
+typedef struct { pmdval_t pmd; } pmd_t;
+#endif
+
+typedef struct page *pgtable_t;
+
+extern int page_is_ram(unsigned long pagenr);
+extern int pagerange_is_ram(unsigned long start, unsigned long end);
+extern int devmem_is_allowed(unsigned long pagenr);
+extern void map_devmem(unsigned long pfn, unsigned long size,
+		       pgprot_t vma_prot);
+extern void unmap_devmem(unsigned long pfn, unsigned long size,
+			 pgprot_t vma_prot);
+
+extern unsigned long max_low_pfn_mapped;
+extern unsigned long max_pfn_mapped;
+
+#endif	/* !__ASSEMBLY__ */
+
+#endif	/* _ASM_X86_PAGE_DEFS_H */
