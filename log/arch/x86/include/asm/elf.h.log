commit 9fccc5c0c99f238aa1b0460fccbdb30a887e7036
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Mar 26 23:48:17 2020 -0700

    x86/elf: Disable automatic READ_IMPLIES_EXEC on 64-bit
    
    With modern x86 64-bit environments, there should never be a need for
    automatic READ_IMPLIES_EXEC, as the architecture is intended to always
    be execute-bit aware (as in, the default memory protection should be NX
    unless a region explicitly requests to be executable).
    
    There were very old x86_64 systems that lacked the NX bit, but for those,
    the NX bit is, obviously, unenforceable, so these changes should have
    no impact on them.
    
    Suggested-by: Hector Marco-Gisbert <hecmargi@upv.es>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Link: https://lkml.kernel.org/r/20200327064820.12602-4-keescook@chromium.org

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 397a1c74433e..452beed7892b 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -287,7 +287,7 @@ extern u32 elf_hwcap2;
  *                 CPU: | lacks NX*  | has NX, ia32     | has NX, x86_64 |
  * ELF:                 |            |                  |                |
  * ---------------------|------------|------------------|----------------|
- * missing PT_GNU_STACK | exec-all   | exec-all         | exec-all       |
+ * missing PT_GNU_STACK | exec-all   | exec-all         | exec-none      |
  * PT_GNU_STACK == RWX  | exec-stack | exec-stack       | exec-stack     |
  * PT_GNU_STACK == RW   | exec-none  | exec-none        | exec-none      |
  *
@@ -303,7 +303,7 @@ extern u32 elf_hwcap2;
  *
  */
 #define elf_read_implies_exec(ex, executable_stack)	\
-	(executable_stack == EXSTACK_DEFAULT)
+	(mmap_is_ia32() && executable_stack == EXSTACK_DEFAULT)
 
 struct task_struct;
 

commit 122306117afe4ba202b5e57c61dfbeffc5c41387
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Mar 26 23:48:16 2020 -0700

    x86/elf: Split READ_IMPLIES_EXEC from executable PT_GNU_STACK
    
    The READ_IMPLIES_EXEC workaround was designed for old toolchains that
    lacked the ELF PT_GNU_STACK marking under the assumption that toolchains
    that couldn't specify executable permission flags for the stack may not
    know how to do it correctly for any memory region.
    
    This logic is sensible for having ancient binaries coexist in a system
    with possibly NX memory, but was implemented in a way that equated having
    a PT_GNU_STACK marked executable as being as "broken" as lacking the
    PT_GNU_STACK marking entirely. Things like unmarked assembly and stack
    trampolines may cause PT_GNU_STACK to need an executable bit, but they
    do not imply all mappings must be executable.
    
    This confusion has led to situations where modern programs with explicitly
    marked executable stacks are forced into the READ_IMPLIES_EXEC state when
    no such thing is needed. (And leads to unexpected failures when mmap()ing
    regions of device driver memory that wish to disallow VM_EXEC[1].)
    
    In looking for other reasons for the READ_IMPLIES_EXEC behavior, Jann
    Horn noted that glibc thread stacks have always been marked RWX (until
    2003 when they started tracking the PT_GNU_STACK flag instead[2]). And
    musl doesn't support executable stacks at all[3]. As such, no breakage
    for multithreaded applications is expected from this change.
    
    [1] https://lkml.kernel.org/r/20190418055759.GA3155@mellanox.com
    [2] https://sourceware.org/git/?p=glibc.git;a=commitdiff;h=54ee14b3882
    [3] https://lkml.kernel.org/r/20190423192534.GN23599@brightrain.aerifal.cx
    
    Suggested-by: Hector Marco-Gisbert <hecmargi@upv.es>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Link: https://lkml.kernel.org/r/20200327064820.12602-3-keescook@chromium.org

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index ee459d4c3b45..397a1c74433e 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -288,12 +288,13 @@ extern u32 elf_hwcap2;
  * ELF:                 |            |                  |                |
  * ---------------------|------------|------------------|----------------|
  * missing PT_GNU_STACK | exec-all   | exec-all         | exec-all       |
- * PT_GNU_STACK == RWX  | exec-all   | exec-all         | exec-all       |
+ * PT_GNU_STACK == RWX  | exec-stack | exec-stack       | exec-stack     |
  * PT_GNU_STACK == RW   | exec-none  | exec-none        | exec-none      |
  *
  *  exec-all  : all PROT_READ user mappings are executable, except when
  *              backed by files on a noexec-filesystem.
  *  exec-none : only PROT_EXEC user mappings are executable.
+ *  exec-stack: only the stack and PROT_EXEC user mappings are executable.
  *
  *  *this column has no architectural effect: NX markings are ignored by
  *   hardware, but may have behavioral effects when "wants X" collides with
@@ -302,7 +303,7 @@ extern u32 elf_hwcap2;
  *
  */
 #define elf_read_implies_exec(ex, executable_stack)	\
-	(executable_stack != EXSTACK_DISABLE_X)
+	(executable_stack == EXSTACK_DEFAULT)
 
 struct task_struct;
 

commit 9d9e435f3f2492bfd196acacb61cc9a9212d8170
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Mar 26 23:48:15 2020 -0700

    x86/elf: Add table to document READ_IMPLIES_EXEC
    
    Add a table to document the current behavior of READ_IMPLIES_EXEC in
    preparation for changing the behavior.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Link: https://lkml.kernel.org/r/20200327064820.12602-2-keescook@chromium.org

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 69c0f892e310..ee459d4c3b45 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -281,6 +281,25 @@ extern u32 elf_hwcap2;
 /*
  * An executable for which elf_read_implies_exec() returns TRUE will
  * have the READ_IMPLIES_EXEC personality flag set automatically.
+ *
+ * The decision process for determining the results are:
+ *
+ *                 CPU: | lacks NX*  | has NX, ia32     | has NX, x86_64 |
+ * ELF:                 |            |                  |                |
+ * ---------------------|------------|------------------|----------------|
+ * missing PT_GNU_STACK | exec-all   | exec-all         | exec-all       |
+ * PT_GNU_STACK == RWX  | exec-all   | exec-all         | exec-all       |
+ * PT_GNU_STACK == RW   | exec-none  | exec-none        | exec-none      |
+ *
+ *  exec-all  : all PROT_READ user mappings are executable, except when
+ *              backed by files on a noexec-filesystem.
+ *  exec-none : only PROT_EXEC user mappings are executable.
+ *
+ *  *this column has no architectural effect: NX markings are ignored by
+ *   hardware, but may have behavioral effects when "wants X" collides with
+ *   "cannot be X" constraints in memory permission flags, as in
+ *   https://lkml.kernel.org/r/20190418055759.GA3155@mellanox.com
+ *
  */
 #define elf_read_implies_exec(ex, executable_stack)	\
 	(executable_stack != EXSTACK_DISABLE_X)

commit e1d20beae70eb918cca7f07a77ce199fd148fdd2
Merge: cbbfb0ae2ca9 ec3a94188df7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 15:24:22 2018 +0100

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were the fsgsbase related preparatory
      patches from Chang S. Bae - but there's also an optimized
      memcpy_flushcache() and a cleanup for the __cmpxchg_double() assembly
      glue"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fsgsbase/64: Clean up various details
      x86/segments: Introduce the 'CPUNODE' naming to better document the segment limit CPU/node NR trick
      x86/vdso: Initialize the CPU/node NR segment descriptor earlier
      x86/vdso: Introduce helper functions for CPU and node number
      x86/segments/64: Rename the GDT PER_CPU entry to CPU_NUMBER
      x86/fsgsbase/64: Factor out FS/GS segment loading from __switch_to()
      x86/fsgsbase/64: Convert the ELF core dump code to the new FSGSBASE helpers
      x86/fsgsbase/64: Make ptrace use the new FS/GS base helpers
      x86/fsgsbase/64: Introduce FS/GS base helper functions
      x86/fsgsbase/64: Fix ptrace() to read the FS/GS base accurately
      x86/asm: Use CC_SET()/CC_OUT() in __cmpxchg_double()
      x86/asm: Optimize memcpy_flushcache()

commit 824eea38d239fb2a6027e65e18a5daef23019b00
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Tue Sep 18 16:08:55 2018 -0700

    x86/fsgsbase/64: Convert the ELF core dump code to the new FSGSBASE helpers
    
    Replace open-coded rdmsr()'s with their <asm/fsgsbase.h> API
    counterparts.
    
    No change in functionality intended.
    
    [ mingo: Wrote new changelog. ]
    
    Based-on-code-from: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Link: http://lkml.kernel.org/r/1537312139-5580-5-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 0d157d2a1e2a..1527ec351036 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -10,6 +10,7 @@
 #include <asm/ptrace.h>
 #include <asm/user.h>
 #include <asm/auxvec.h>
+#include <asm/fsgsbase.h>
 
 typedef unsigned long elf_greg_t;
 
@@ -205,7 +206,6 @@ void set_personality_ia32(bool);
 
 #define ELF_CORE_COPY_REGS(pr_reg, regs)			\
 do {								\
-	unsigned long base;					\
 	unsigned v;						\
 	(pr_reg)[0] = (regs)->r15;				\
 	(pr_reg)[1] = (regs)->r14;				\
@@ -228,8 +228,8 @@ do {								\
 	(pr_reg)[18] = (regs)->flags;				\
 	(pr_reg)[19] = (regs)->sp;				\
 	(pr_reg)[20] = (regs)->ss;				\
-	rdmsrl(MSR_FS_BASE, base); (pr_reg)[21] = base;		\
-	rdmsrl(MSR_KERNEL_GS_BASE, base); (pr_reg)[22] = base;	\
+	(pr_reg)[21] = x86_fsbase_read_cpu();			\
+	(pr_reg)[22] = x86_gsbase_read_cpu_inactive();		\
 	asm("movl %%ds,%0" : "=r" (v)); (pr_reg)[23] = v;	\
 	asm("movl %%es,%0" : "=r" (v)); (pr_reg)[24] = v;	\
 	asm("movl %%fs,%0" : "=r" (v)); (pr_reg)[25] = v;	\

commit b40a142b12b5c4312171140ff2bc92971b8a7a09
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 18 23:51:39 2018 -0700

    x86: Add support for 64-bit place relative relocations
    
    Add support for R_X86_64_PC64 relocations, which operate on 64-bit
    quantities holding a relative symbol reference. Also remove the
    definition of R_X86_64_NUM: given that it is currently unused, it
    is unclear what the new value should be.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-s390@vger.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Link: https://lkml.kernel.org/r/20180919065144.25010-5-ard.biesheuvel@linaro.org

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 0d157d2a1e2a..a357031d85b5 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -62,8 +62,7 @@ typedef struct user_fxsr_struct elf_fpxregset_t;
 #define R_X86_64_PC16		13	/* 16 bit sign extended pc relative */
 #define R_X86_64_8		14	/* Direct 8 bit sign extended  */
 #define R_X86_64_PC8		15	/* 8 bit sign extended pc relative */
-
-#define R_X86_64_NUM		16
+#define R_X86_64_PC64		24	/* Place relative 64-bit signed */
 
 /*
  * These are used to set parameters in the core dumps.

commit 1e0f25dbf2464df8445dd40881f4d9e732434947
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Nov 15 17:36:06 2017 +0300

    x86/mm: Prevent non-MAP_FIXED mapping across DEFAULT_MAP_WINDOW border
    
    In case of 5-level paging, the kernel does not place any mapping above
    47-bit, unless userspace explicitly asks for it.
    
    Userspace can request an allocation from the full address space by
    specifying the mmap address hint above 47-bit.
    
    Nicholas noticed that the current implementation violates this interface:
    
      If user space requests a mapping at the end of the 47-bit address space
      with a length which causes the mapping to cross the 47-bit border
      (DEFAULT_MAP_WINDOW), then the vma is partially in the address space
      below and above.
    
    Sanity check the mmap address hint so that start and end of the resulting
    vma are on the same side of the 47-bit border. If that's not the case fall
    back to the code path which ignores the address hint and allocate from the
    regular address space below 47-bit.
    
    To make the checks consistent, mask out the address hints lower bits
    (either PAGE_MASK or huge_page_mask()) instead of using ALIGN() which can
    push them up to the next boundary.
    
    [ tglx: Moved the address check to a function and massaged comment and
            changelog ]
    
    Reported-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: linux-mm@kvack.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: https://lkml.kernel.org/r/20171115143607.81541-1-kirill.shutemov@linux.intel.com

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 3a091cea36c5..0d157d2a1e2a 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -309,6 +309,7 @@ static inline int mmap_is_ia32(void)
 extern unsigned long task_size_32bit(void);
 extern unsigned long task_size_64bit(int full_addr_space);
 extern unsigned long get_mmap_base(int is_legacy);
+extern bool mmap_address_hint_valid(unsigned long addr, unsigned long len);
 
 #ifdef CONFIG_X86_32
 

commit be739f4b5ddece74ef25e2304b17a7fd24575e9b
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Nov 7 13:38:04 2017 +0300

    x86/mm: Fix ELF_ET_DYN_BASE for 5-level paging
    
    On machines with 5-level paging we don't want to allocate mapping above
    47-bit unless user explicitly asked for it. See b569bab78d8d ("x86/mm:
    Prepare to expose larger address space to userspace") for details.
    
    c715b72c1ba4 ("mm: revert x86_64 and arm64 ELF_ET_DYN_BASE base
    changes") broke the behaviour. After the commit elf binary and heap got
    mapped above 47-bits.
    
    Use DEFAULT_MAP_WINDOW instead of TASK_SIZE to determine ELF_ET_DYN_BASE so
    it's forced to be below 47-bits unconditionally.
    
    Fixes: c715b72c1ba4 ("mm: revert x86_64 and arm64 ELF_ET_DYN_BASE base changes")
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: linux-mm@kvack.org
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: https://lkml.kernel.org/r/20171107103804.47341-1-kirill.shutemov@linux.intel.com

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index c1a125e47ff3..3a091cea36c5 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -253,7 +253,7 @@ extern int force_personality32;
  * space open for things that want to use the area for 32-bit pointers.
  */
 #define ELF_ET_DYN_BASE		(mmap_is_ia32() ? 0x000400000UL : \
-						  (TASK_SIZE / 3 * 2))
+						  (DEFAULT_MAP_WINDOW / 3 * 2))
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 04330c8d9af9..c1a125e47ff3 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_ELF_H
 #define _ASM_X86_ELF_H
 

commit b1b6f83ac938d176742c85757960dec2cf10e468
Merge: 5f82e71a001d 9e52fc2b50de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 12:21:28 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "PCID support, 5-level paging support, Secure Memory Encryption support
    
      The main changes in this cycle are support for three new, complex
      hardware features of x86 CPUs:
    
       - Add 5-level paging support, which is a new hardware feature on
         upcoming Intel CPUs allowing up to 128 PB of virtual address space
         and 4 PB of physical RAM space - a 512-fold increase over the old
         limits. (Supercomputers of the future forecasting hurricanes on an
         ever warming planet can certainly make good use of more RAM.)
    
         Many of the necessary changes went upstream in previous cycles,
         v4.14 is the first kernel that can enable 5-level paging.
    
         This feature is activated via CONFIG_X86_5LEVEL=y - disabled by
         default.
    
         (By Kirill A. Shutemov)
    
       - Add 'encrypted memory' support, which is a new hardware feature on
         upcoming AMD CPUs ('Secure Memory Encryption', SME) allowing system
         RAM to be encrypted and decrypted (mostly) transparently by the
         CPU, with a little help from the kernel to transition to/from
         encrypted RAM. Such RAM should be more secure against various
         attacks like RAM access via the memory bus and should make the
         radio signature of memory bus traffic harder to intercept (and
         decrypt) as well.
    
         This feature is activated via CONFIG_AMD_MEM_ENCRYPT=y - disabled
         by default.
    
         (By Tom Lendacky)
    
       - Enable PCID optimized TLB flushing on newer Intel CPUs: PCID is a
         hardware feature that attaches an address space tag to TLB entries
         and thus allows to skip TLB flushing in many cases, even if we
         switch mm's.
    
         (By Andy Lutomirski)
    
      All three of these features were in the works for a long time, and
      it's coincidence of the three independent development paths that they
      are all enabled in v4.14 at once"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (65 commits)
      x86/mm: Enable RCU based page table freeing (CONFIG_HAVE_RCU_TABLE_FREE=y)
      x86/mm: Use pr_cont() in dump_pagetable()
      x86/mm: Fix SME encryption stack ptr handling
      kvm/x86: Avoid clearing the C-bit in rsvd_bits()
      x86/CPU: Align CR3 defines
      x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages
      acpi, x86/mm: Remove encryption mask from ACPI page protection type
      x86/mm, kexec: Fix memory corruption with SME on successive kexecs
      x86/mm/pkeys: Fix typo in Documentation/x86/protection-keys.txt
      x86/mm/dump_pagetables: Speed up page tables dump for CONFIG_KASAN=y
      x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID
      x86: Enable 5-level paging support via CONFIG_X86_5LEVEL=y
      x86/mm: Allow userspace have mappings above 47-bit
      x86/mm: Prepare to expose larger address space to userspace
      x86/mpx: Do not allow MPX if we have mappings above 47-bit
      x86/mm: Rename tasksize_32bit/64bit to task_size_32bit/64bit()
      x86/xen: Redefine XEN_ELFNOTE_INIT_P2M using PUD_SIZE * PTRS_PER_PUD
      x86/mm/dump_pagetables: Fix printout of p4d level
      x86/mm/dump_pagetables: Generalize address normalization
      x86/boot: Fix memremap() related build failure
      ...

commit b0c79f49c343cda8954b3322984c32f258ca4ccb
Merge: f213a6c84c1b dd88a0a0c861
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 09:52:57 2017 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
    
     - Introduce the ORC unwinder, which can be enabled via
       CONFIG_ORC_UNWINDER=y.
    
       The ORC unwinder is a lightweight, Linux kernel specific debuginfo
       implementation, which aims to be DWARF done right for unwinding.
       Objtool is used to generate the ORC unwinder tables during build, so
       the data format is flexible and kernel internal: there's no
       dependency on debuginfo created by an external toolchain.
    
       The ORC unwinder is almost two orders of magnitude faster than the
       (out of tree) DWARF unwinder - which is important for perf call graph
       profiling. It is also significantly simpler and is coded defensively:
       there has not been a single ORC related kernel crash so far, even
       with early versions. (knock on wood!)
    
       But the main advantage is that enabling the ORC unwinder allows
       CONFIG_FRAME_POINTERS to be turned off - which speeds up the kernel
       measurably:
    
       With frame pointers disabled, GCC does not have to add frame pointer
       instrumentation code to every function in the kernel. The kernel's
       .text size decreases by about 3.2%, resulting in better cache
       utilization and fewer instructions executed, resulting in a broad
       kernel-wide speedup. Average speedup of system calls should be
       roughly in the 1-3% range - measurements by Mel Gorman [1] have shown
       a speedup of 5-10% for some function execution intense workloads.
    
       The main cost of the unwinder is that the unwinder data has to be
       stored in RAM: the memory cost is 2-4MB of RAM, depending on kernel
       config - which is a modest cost on modern x86 systems.
    
       Given how young the ORC unwinder code is it's not enabled by default
       - but given the performance advantages the plan is to eventually make
       it the default unwinder on x86.
    
       See Documentation/x86/orc-unwinder.txt for more details.
    
     - Remove lguest support: its intended role was that of a temporary
       proof of concept for virtualization, plus its removal will enable the
       reduction (removal) of the paravirt API as well, so Rusty agreed to
       its removal. (Juergen Gross)
    
     - Clean up and fix FSGS related functionality (Andy Lutomirski)
    
     - Clean up IO access APIs (Andy Shevchenko)
    
     - Enhance the symbol namespace (Jiri Slaby)
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (47 commits)
      objtool: Handle GCC stack pointer adjustment bug
      x86/entry/64: Use ENTRY() instead of ALIGN+GLOBAL for stub32_clone()
      x86/fpu/math-emu: Add ENDPROC to functions
      x86/boot/64: Extract efi_pe_entry() from startup_64()
      x86/boot/32: Extract efi_pe_entry() from startup_32()
      x86/lguest: Remove lguest support
      x86/paravirt/xen: Remove xen_patch()
      objtool: Fix objtool fallthrough detection with function padding
      x86/xen/64: Fix the reported SS and CS in SYSCALL
      objtool: Track DRAP separately from callee-saved registers
      objtool: Fix validate_branch() return codes
      x86: Clarify/fix no-op barriers for text_poke_bp()
      x86/switch_to/64: Rewrite FS/GS switching yet again to fix AMD CPUs
      selftests/x86/fsgsbase: Test selectors 1, 2, and 3
      x86/fsgsbase/64: Report FSBASE and GSBASE correctly in core dumps
      x86/fsgsbase/64: Fully initialize FS and GS state in start_thread_common
      x86/asm: Fix UNWIND_HINT_REGS macro for older binutils
      x86/asm/32: Fix regs_get_register() on segment registers
      x86/xen/64: Rearrange the SYSCALL entries
      x86/asm/32: Remove a bunch of '& 0xffff' from pt_regs segment reads
      ...

commit 413d63d71b222108d19703f3fd5cf9108652a730
Merge: d6c8103b0265 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Aug 26 09:19:13 2017 +0200

    Merge branch 'linus' into x86/mm to pick up fixes and to fix conflicts
    
    Conflicts:
            arch/x86/kernel/head64.c
            arch/x86/mm/mmap.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c715b72c1ba406f133217b509044c38d8e714a37
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Aug 18 15:16:31 2017 -0700

    mm: revert x86_64 and arm64 ELF_ET_DYN_BASE base changes
    
    Moving the x86_64 and arm64 PIE base from 0x555555554000 to 0x000100000000
    broke AddressSanitizer.  This is a partial revert of:
    
      eab09532d400 ("binfmt_elf: use ELF_ET_DYN_BASE only for PIE")
      02445990a96e ("arm64: move ELF_ET_DYN_BASE to 4GB / 4MB")
    
    The AddressSanitizer tool has hard-coded expectations about where
    executable mappings are loaded.
    
    The motivation for changing the PIE base in the above commits was to
    avoid the Stack-Clash CVEs that allowed executable mappings to get too
    close to heap and stack.  This was mainly a problem on 32-bit, but the
    64-bit bases were moved too, in an effort to proactively protect those
    systems (proofs of concept do exist that show 64-bit collisions, but
    other recent changes to fix stack accounting and setuid behaviors will
    minimize the impact).
    
    The new 32-bit PIE base is fine for ASan (since it matches the ET_EXEC
    base), so only the 64-bit PIE base needs to be reverted to let x86 and
    arm64 ASan binaries run again.  Future changes to the 64-bit PIE base on
    these architectures can be made optional once a more dynamic method for
    dealing with AddressSanitizer is found.  (e.g.  always loading PIE into
    the mmap region for marked binaries.)
    
    Link: http://lkml.kernel.org/r/20170807201542.GA21271@beast
    Fixes: eab09532d400 ("binfmt_elf: use ELF_ET_DYN_BASE only for PIE")
    Fixes: 02445990a96e ("arm64: move ELF_ET_DYN_BASE to 4GB / 4MB")
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reported-by: Kostya Serebryany <kcc@google.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 1c18d83d3f09..9aeb91935ce0 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -247,11 +247,11 @@ extern int force_personality32;
 
 /*
  * This is the base location for PIE (ET_DYN with INTERP) loads. On
- * 64-bit, this is raised to 4GB to leave the entire 32-bit address
+ * 64-bit, this is above 4GB to leave the entire 32-bit address
  * space open for things that want to use the area for 32-bit pointers.
  */
 #define ELF_ET_DYN_BASE		(mmap_is_ia32() ? 0x000400000UL : \
-						  0x100000000UL)
+						  (TASK_SIZE / 3 * 2))
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,

commit 9584d98bed7a7a904d0702ad06bbcc94703cb5b4
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Aug 1 07:11:35 2017 -0700

    x86/fsgsbase/64: Report FSBASE and GSBASE correctly in core dumps
    
    In ELF_COPY_CORE_REGS, we're copying from the current task, so
    accessing thread.fsbase and thread.gsbase makes no sense.  Just read
    the values from the CPU registers.
    
    In practice, the old code would have been correct most of the time
    simply because thread.fsbase and thread.gsbase usually matched the
    CPU registers.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chang Seok <chang.seok.bae@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index a62a12c47233..da3250ea864a 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -204,6 +204,7 @@ void set_personality_ia32(bool);
 
 #define ELF_CORE_COPY_REGS(pr_reg, regs)			\
 do {								\
+	unsigned long base;					\
 	unsigned v;						\
 	(pr_reg)[0] = (regs)->r15;				\
 	(pr_reg)[1] = (regs)->r14;				\
@@ -226,8 +227,8 @@ do {								\
 	(pr_reg)[18] = (regs)->flags;				\
 	(pr_reg)[19] = (regs)->sp;				\
 	(pr_reg)[20] = (regs)->ss;				\
-	(pr_reg)[21] = current->thread.fsbase;			\
-	(pr_reg)[22] = current->thread.gsbase;			\
+	rdmsrl(MSR_FS_BASE, base); (pr_reg)[21] = base;		\
+	rdmsrl(MSR_KERNEL_GS_BASE, base); (pr_reg)[22] = base;	\
 	asm("movl %%ds,%0" : "=r" (v)); (pr_reg)[23] = v;	\
 	asm("movl %%es,%0" : "=r" (v)); (pr_reg)[24] = v;	\
 	asm("movl %%fs,%0" : "=r" (v)); (pr_reg)[25] = v;	\

commit 99504819fc643160afd6813921b1d42b18e52a49
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 28 06:00:32 2017 -0700

    x86/asm/32: Remove a bunch of '& 0xffff' from pt_regs segment reads
    
    Now that pt_regs properly defines segment fields as 16-bit on 32-bit
    CPUs, there's no need to mask off the high word.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 1c18d83d3f09..a62a12c47233 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -126,15 +126,15 @@ do {						\
 	pr_reg[4] = regs->di;			\
 	pr_reg[5] = regs->bp;			\
 	pr_reg[6] = regs->ax;			\
-	pr_reg[7] = regs->ds & 0xffff;		\
-	pr_reg[8] = regs->es & 0xffff;		\
-	pr_reg[9] = regs->fs & 0xffff;		\
+	pr_reg[7] = regs->ds;			\
+	pr_reg[8] = regs->es;			\
+	pr_reg[9] = regs->fs;			\
 	pr_reg[11] = regs->orig_ax;		\
 	pr_reg[12] = regs->ip;			\
-	pr_reg[13] = regs->cs & 0xffff;		\
+	pr_reg[13] = regs->cs;			\
 	pr_reg[14] = regs->flags;		\
 	pr_reg[15] = regs->sp;			\
-	pr_reg[16] = regs->ss & 0xffff;		\
+	pr_reg[16] = regs->ss;			\
 } while (0);
 
 #define ELF_CORE_COPY_REGS(pr_reg, regs)	\

commit b569bab78d8df157a6f91070af827753e4d1787c
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Jul 17 01:59:52 2017 +0300

    x86/mm: Prepare to expose larger address space to userspace
    
    On x86, 5-level paging enables 56-bit userspace virtual address space.
    Not all user space is ready to handle wide addresses. It's known that
    at least some JIT compilers use higher bits in pointers to encode their
    information. It collides with valid pointers with 5-level paging and
    leads to crashes.
    
    To mitigate this, we are not going to allocate virtual address space
    above 47-bit by default.
    
    But userspace can ask for allocation from full address space by
    specifying hint address (with or without MAP_FIXED) above 47-bits.
    
    If hint address set above 47-bit, but MAP_FIXED is not specified, we try
    to look for unmapped area by specified address. If it's already
    occupied, we look for unmapped area in *full* address space, rather than
    from 47-bit window.
    
    A high hint address would only affect the allocation in question, but not
    any future mmap()s.
    
    Specifying high hint address on older kernel or on machine without 5-level
    paging support is safe. The hint will be ignored and kernel will fall back
    to allocation from 47-bit address space.
    
    This approach helps to easily make application's memory allocator aware
    about large address space without manually tracking allocated virtual
    address space.
    
    The patch puts all machinery in place, but not yet allows userspace to have
    mappings above 47-bit -- TASK_SIZE_MAX has to be raised to get the effect.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170716225954.74185-7-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index c7090ef1388e..f87f4d73c700 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -305,7 +305,7 @@ static inline int mmap_is_ia32(void)
 }
 
 extern unsigned long task_size_32bit(void);
-extern unsigned long task_size_64bit(void);
+extern unsigned long task_size_64bit(int full_addr_space);
 extern unsigned long get_mmap_base(int is_legacy);
 
 #ifdef CONFIG_X86_32

commit e8f01a8dad7cdf9ddaa792451733a45ab5d385af
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Jul 17 01:59:50 2017 +0300

    x86/mm: Rename tasksize_32bit/64bit to task_size_32bit/64bit()
    
    Rename these helpers to be consistent with spelling of TASK_SIZE and
    related constants.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170716225954.74185-5-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 1c18d83d3f09..c7090ef1388e 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -304,8 +304,8 @@ static inline int mmap_is_ia32(void)
 		test_thread_flag(TIF_ADDR32));
 }
 
-extern unsigned long tasksize_32bit(void);
-extern unsigned long tasksize_64bit(void);
+extern unsigned long task_size_32bit(void);
+extern unsigned long task_size_64bit(void);
 extern unsigned long get_mmap_base(int is_legacy);
 
 #ifdef CONFIG_X86_32

commit eab09532d40090698b05a07c1c87f39fdbc5fab5
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Jul 10 15:52:37 2017 -0700

    binfmt_elf: use ELF_ET_DYN_BASE only for PIE
    
    The ELF_ET_DYN_BASE position was originally intended to keep loaders
    away from ET_EXEC binaries.  (For example, running "/lib/ld-linux.so.2
    /bin/cat" might cause the subsequent load of /bin/cat into where the
    loader had been loaded.)
    
    With the advent of PIE (ET_DYN binaries with an INTERP Program Header),
    ELF_ET_DYN_BASE continued to be used since the kernel was only looking
    at ET_DYN.  However, since ELF_ET_DYN_BASE is traditionally set at the
    top 1/3rd of the TASK_SIZE, a substantial portion of the address space
    is unused.
    
    For 32-bit tasks when RLIMIT_STACK is set to RLIM_INFINITY, programs are
    loaded above the mmap region.  This means they can be made to collide
    (CVE-2017-1000370) or nearly collide (CVE-2017-1000371) with
    pathological stack regions.
    
    Lowering ELF_ET_DYN_BASE solves both by moving programs below the mmap
    region in all cases, and will now additionally avoid programs falling
    back to the mmap region by enforcing MAP_FIXED for program loads (i.e.
    if it would have collided with the stack, now it will fail to load
    instead of falling back to the mmap region).
    
    To allow for a lower ELF_ET_DYN_BASE, loaders (ET_DYN without INTERP)
    are loaded into the mmap region, leaving space available for either an
    ET_EXEC binary with a fixed location or PIE being loaded into mmap by
    the loader.  Only PIE programs are loaded offset from ELF_ET_DYN_BASE,
    which means architectures can now safely lower their values without risk
    of loaders colliding with their subsequently loaded programs.
    
    For 64-bit, ELF_ET_DYN_BASE is best set to 4GB to allow runtimes to use
    the entire 32-bit address space for 32-bit pointers.
    
    Thanks to PaX Team, Daniel Micay, and Rik van Riel for inspiration and
    suggestions on how to implement this solution.
    
    Fixes: d1fd836dcf00 ("mm: split ET_DYN ASLR from mmap ASLR")
    Link: http://lkml.kernel.org/r/20170621173201.GA114489@beast
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Daniel Micay <danielmicay@gmail.com>
    Cc: Qualys Security Advisory <qsa@qualys.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Pratyush Anand <panand@redhat.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index e8ab9a46bc68..1c18d83d3f09 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -245,12 +245,13 @@ extern int force_personality32;
 #define CORE_DUMP_USE_REGSET
 #define ELF_EXEC_PAGESIZE	4096
 
-/* This is the location that an ET_DYN program is loaded if exec'ed.  Typical
-   use of this is to invoke "./ld.so someprog" to test out a new version of
-   the loader.  We need to make sure that it is out of the way of the program
-   that it will "exec", and that there is sufficient room for the brk.  */
-
-#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)
+/*
+ * This is the base location for PIE (ET_DYN with INTERP) loads. On
+ * 64-bit, this is raised to 4GB to leave the entire 32-bit address
+ * space open for things that want to use the area for 32-bit pointers.
+ */
+#define ELF_ET_DYN_BASE		(mmap_is_ia32() ? 0x000400000UL : \
+						  0x100000000UL)
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,

commit d3b5d35290d729a2518af00feca867385a1b08fa
Merge: aa2a4b6569d5 71389703839e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 23:54:56 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "The main x86 MM changes in this cycle were:
    
       - continued native kernel PCID support preparation patches to the TLB
         flushing code (Andy Lutomirski)
    
       - various fixes related to 32-bit compat syscall returning address
         over 4Gb in applications, launched from 64-bit binaries - motivated
         by C/R frameworks such as Virtuozzo. (Dmitry Safonov)
    
       - continued Intel 5-level paging enablement: in particular the
         conversion of x86 GUP to the generic GUP code. (Kirill A. Shutemov)
    
       - x86/mpx ABI corner case fixes/enhancements (Joerg Roedel)
    
       - ... plus misc updates, fixes and cleanups"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (62 commits)
      mm, zone_device: Replace {get, put}_zone_device_page() with a single reference to fix pmem crash
      x86/mm: Fix flush_tlb_page() on Xen
      x86/mm: Make flush_tlb_mm_range() more predictable
      x86/mm: Remove flush_tlb() and flush_tlb_current_task()
      x86/vm86/32: Switch to flush_tlb_mm_range() in mark_screen_rdonly()
      x86/mm/64: Fix crash in remove_pagetable()
      Revert "x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation"
      x86/boot/e820: Remove a redundant self assignment
      x86/mm: Fix dump pagetables for 4 levels of page tables
      x86/mpx, selftests: Only check bounds-vs-shadow when we keep shadow
      x86/mpx: Correctly report do_mpx_bt_fault() failures to user-space
      Revert "x86/mm/numa: Remove numa_nodemask_from_meminfo()"
      x86/espfix: Add support for 5-level paging
      x86/kasan: Extend KASAN to support 5-level paging
      x86/mm: Add basic defines/helpers for CONFIG_X86_5LEVEL=y
      x86/paravirt: Add 5-level support to the paravirt code
      x86/mm: Define virtual memory map for 5-level paging
      x86/asm: Remove __VIRTUAL_MASK_SHIFT==47 assert
      x86/boot: Detect 5-level paging support
      x86/mm/numa: Remove numa_nodemask_from_meminfo()
      ...

commit 6fdc6dd90272ce7e75d744f71535cfbd8d77da81
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Apr 10 17:14:28 2017 +0200

    x86/vdso: Plug race between mapping and ELF header setup
    
    The vsyscall32 sysctl can racy against a concurrent fork when it switches
    from disabled to enabled:
    
        arch_setup_additional_pages()
            if (vdso32_enabled)
               --> No mapping
                                            sysctl.vsysscall32()
                                              --> vdso32_enabled = true
        create_elf_tables()
          ARCH_DLINFO_IA32
            if (vdso32_enabled) {
               --> Add VDSO entry with NULL pointer
    
    Make ARCH_DLINFO_IA32 check whether the VDSO mapping has been set up for
    the newly forked process or not.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20170410151723.602367196@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 9d49c18b5ea9..3762536619f8 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -287,7 +287,7 @@ struct task_struct;
 
 #define	ARCH_DLINFO_IA32						\
 do {									\
-	if (vdso32_enabled) {						\
+	if (VDSO_CURRENT_BASE) {					\
 		NEW_AUX_ENT(AT_SYSINFO,	VDSO_ENTRY);			\
 		NEW_AUX_ENT(AT_SYSINFO_EHDR, VDSO_CURRENT_BASE);	\
 	}								\

commit e13b73dd9c8003993b171173ba803363faf74238
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Tue Mar 14 14:41:26 2017 +0300

    x86/hugetlb: Adjust to the new native/compat mmap bases
    
    Commit 1b028f784e8c introduced two mmap() bases for 32-bit syscalls and for
    64-bit syscalls. The mmap() code in x86 was modified to handle the
    separation, but the patch series missed to update the hugetlb code.
    
    As a consequence a 32bit application mapping a file on hugetlbfs uses the
    64-bit mmap base for address space allocation, which fails.
    
    Adjust the hugetlb mapping code to use the proper bases depending on the
    syscall invocation mode (64-bit or compat).
    
    [ tglx: Massaged changelog and switched from asm/compat.h to linux/compat.h ]
    
    Fixes: commit 1b028f784e8c ("x86/mm: Introduce mmap_compat_base() for 32-bit mmap()")
    Reported-by: kernel test robot <xiaolong.ye@intel.com>
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: 0x7f454c46@gmail.com
    Cc: linux-mm@kvack.org
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/20170314114126.9280-1-dsafonov@virtuozzo.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index ac5be5ba8527..d4d3ed456cb7 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -305,6 +305,7 @@ static inline int mmap_is_ia32(void)
 
 extern unsigned long tasksize_32bit(void);
 extern unsigned long tasksize_64bit(void);
+extern unsigned long get_mmap_base(int is_legacy);
 
 #ifdef CONFIG_X86_32
 

commit 1b028f784e8c341e762c264f70dc0ca1418c8b7a
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Mon Mar 6 17:17:19 2017 +0300

    x86/mm: Introduce mmap_compat_base() for 32-bit mmap()
    
    mmap() uses a base address, from which it starts to look for a free space
    for allocation.
    
    The base address is stored in mm->mmap_base, which is calculated during
    exec(). The address depends on task's size, set rlimit for stack, ASLR
    randomization. The base depends on the task size and the number of random
    bits which are different for 64-bit and 32bit applications.
    
    Due to the fact, that the base address is fixed, its mmap() from a compat
    (32bit) syscall issued by a 64bit task will return a address which is based
    on the 64bit base address and does not fit into the 32bit address space
    (4GB). The returned pointer is truncated to 32bit, which results in an
    invalid address.
    
    To solve store a seperate compat address base plus a compat legacy address
    base in mm_struct. These bases are calculated at exec() time and can be
    used later to address the 32bit compat mmap() issued by 64 bit
    applications.
    
    As a consequence of this change 32-bit applications issuing a 64-bit
    syscall (after doing a long jump) will get a 64-bit mapping now. Before
    this change 32-bit applications always got a 32bit mapping.
    
    [ tglx: Massaged changelog and added a comment ]
    
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: 0x7f454c46@gmail.com
    Cc: linux-mm@kvack.org
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/20170306141721.9188-4-dsafonov@virtuozzo.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index b908141cf0c4..ac5be5ba8527 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -303,6 +303,9 @@ static inline int mmap_is_ia32(void)
 		test_thread_flag(TIF_ADDR32));
 }
 
+extern unsigned long tasksize_32bit(void);
+extern unsigned long tasksize_64bit(void);
+
 #ifdef CONFIG_X86_32
 
 #define __STACK_RND_MASK(is32bit) (0x7ff)

commit 8f3e474f3cea7b2470218a6ed6da47ff02147dce
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Mon Mar 6 17:17:18 2017 +0300

    x86/mm: Add task_size parameter to mmap_base()
    
    To correctly handle 32-bit and 64-bit mmap() syscalls in 64bit applications
    its required to have separate address bases to place a mapping.
    
    The tasksize can be used as an indicator to select the proper parameters
    for mmap_base().
    
    This requires the following changes:
    
     - Add task_size argument to mmap_base() and make the calculation based on it.
     - Provide mmap_legacy_base() as a seperate function
     - Use the new functions in arch_pick_mmap_layout()
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: 0x7f454c46@gmail.com
    Cc: linux-mm@kvack.org
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/20170306141721.9188-3-dsafonov@virtuozzo.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 9d49c18b5ea9..b908141cf0c4 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -293,8 +293,19 @@ do {									\
 	}								\
 } while (0)
 
+/*
+ * True on X86_32 or when emulating IA32 on X86_64
+ */
+static inline int mmap_is_ia32(void)
+{
+	return IS_ENABLED(CONFIG_X86_32) ||
+	       (IS_ENABLED(CONFIG_COMPAT) &&
+		test_thread_flag(TIF_ADDR32));
+}
+
 #ifdef CONFIG_X86_32
 
+#define __STACK_RND_MASK(is32bit) (0x7ff)
 #define STACK_RND_MASK (0x7ff)
 
 #define ARCH_DLINFO		ARCH_DLINFO_IA32
@@ -304,7 +315,8 @@ do {									\
 #else /* CONFIG_X86_32 */
 
 /* 1GB for 64bit, 8MB for 32bit */
-#define STACK_RND_MASK (test_thread_flag(TIF_ADDR32) ? 0x7ff : 0x3fffff)
+#define __STACK_RND_MASK(is32bit) ((is32bit) ? 0x7ff : 0x3fffff)
+#define STACK_RND_MASK __STACK_RND_MASK(mmap_is_ia32())
 
 #define ARCH_DLINFO							\
 do {									\
@@ -348,16 +360,6 @@ extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
 					      int uses_interp);
 #define compat_arch_setup_additional_pages compat_arch_setup_additional_pages
 
-/*
- * True on X86_32 or when emulating IA32 on X86_64
- */
-static inline int mmap_is_ia32(void)
-{
-	return IS_ENABLED(CONFIG_X86_32) ||
-	       (IS_ENABLED(CONFIG_COMPAT) &&
-		test_thread_flag(TIF_ADDR32));
-}
-
 /* Do not change the values. See get_align_mask() */
 enum align_flags {
 	ALIGN_VA_32	= BIT(0),

commit 0274f9551eff55dbd63b5f5f3efe30fe5d4c801c
Author: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
Date:   Fri Jan 20 14:22:34 2017 +0100

    x86/elf: Add HWCAP2 to expose ring 3 MONITOR/MWAIT
    
    Introduce ELF_HWCAP2 variable for x86 and reserve its bit 0 to expose the
    ring 3 MONITOR/MWAIT.
    
    HWCAP variables contain bitmasks which can be used by userspace
    applications to detect which instruction sets are supported by CPU.  On x86
    architecture information about CPU capabilities can be checked via CPUID
    instructions, unfortunately presence of ring 3 MONITOR/MWAIT feature cannot
    be checked this way. ELF_HWCAP cannot be used as well, because on x86 it is
    set to CPUID[1].EDX which means that all bits are reserved there.
    
    HWCAP2 approach was chosen because it reuses existing solution present
    in other architectures, so only minor modifications are required to the
    kernel and userspace applications. When ELF_HWCAP2 is defined
    kernel maps it to AT_HWCAP2 during the start of the application.
    This way the ring 3 MONITOR/MWAIT feature can be detected using getauxval()
    API in a simple and fast manner. ELF_HWCAP2 type is u32 to be consistent
    with x86 ELF_HWCAP type.
    
    Signed-off-by: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: Piotr.Luc@intel.com
    Cc: dave.hansen@linux.intel.com
    Link: http://lkml.kernel.org/r/1484918557-15481-3-git-send-email-grzegorz.andrejczuk@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index e7f155c3045e..9d49c18b5ea9 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -258,6 +258,15 @@ extern int force_personality32;
 
 #define ELF_HWCAP		(boot_cpu_data.x86_capability[CPUID_1_EDX])
 
+extern u32 elf_hwcap2;
+
+/*
+ * HWCAP2 supplies mask with kernel enabled CPU features, so that
+ * the application can discover that it can safely use them.
+ * The bits are defined in uapi/asm/hwcap2.h.
+ */
+#define ELF_HWCAP2		(elf_hwcap2)
+
 /* This yields a string that ld.so will use to load implementation
    specific libraries for optimization.  This is more specific in
    intent than poking at uname or /proc/cpuinfo.

commit 97f2645f358b411ba2afb22e5966753f0ad92916
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Wed Aug 3 13:45:50 2016 -0700

    tree-wide: replace config_enabled() with IS_ENABLED()
    
    The use of config_enabled() against config options is ambiguous.  In
    practical terms, config_enabled() is equivalent to IS_BUILTIN(), but the
    author might have used it for the meaning of IS_ENABLED().  Using
    IS_ENABLED(), IS_BUILTIN(), IS_MODULE() etc.  makes the intention
    clearer.
    
    This commit replaces config_enabled() with IS_ENABLED() where possible.
    This commit is only touching bool config options.
    
    I noticed two cases where config_enabled() is used against a tristate
    option:
    
     - config_enabled(CONFIG_HWMON)
      [ drivers/net/wireless/ath/ath10k/thermal.c ]
    
     - config_enabled(CONFIG_BACKLIGHT_CLASS_DEVICE)
      [ drivers/gpu/drm/gma500/opregion.c ]
    
    I did not touch them because they should be converted to IS_BUILTIN()
    in order to keep the logic, but I was not sure it was the authors'
    intention.
    
    Link: http://lkml.kernel.org/r/1465215656-20569-1-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Stas Sergeev <stsp@list.ru>
    Cc: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Joshua Kinard <kumba@gentoo.org>
    Cc: Jiri Slaby <jslaby@suse.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Markos Chandras <markos.chandras@imgtec.com>
    Cc: "Dmitry V. Levin" <ldv@altlinux.org>
    Cc: yu-cheng yu <yu-cheng.yu@intel.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Nikolay Martynov <mar.kolya@gmail.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Leonid Yegoshin <Leonid.Yegoshin@imgtec.com>
    Cc: Rafal Milecki <zajec5@gmail.com>
    Cc: James Cowgill <James.Cowgill@imgtec.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Alex Smith <alex.smith@imgtec.com>
    Cc: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Mikko Rapeli <mikko.rapeli@iki.fi>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Brian Norris <computersforpeace@gmail.com>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: "Luis R. Rodriguez" <mcgrof@do-not-panic.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Kalle Valo <kvalo@qca.qualcomm.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Tony Wu <tung7970@gmail.com>
    Cc: Huaitong Han <huaitong.han@intel.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrea Gelmini <andrea.gelmini@gelma.net>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Rabin Vincent <rabin@rab.in>
    Cc: "Maciej W. Rozycki" <macro@imgtec.com>
    Cc: David Daney <david.daney@cavium.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index fea7724141a0..e7f155c3045e 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -344,8 +344,8 @@ extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
  */
 static inline int mmap_is_ia32(void)
 {
-	return config_enabled(CONFIG_X86_32) ||
-	       (config_enabled(CONFIG_COMPAT) &&
+	return IS_ENABLED(CONFIG_X86_32) ||
+	       (IS_ENABLED(CONFIG_COMPAT) &&
 		test_thread_flag(TIF_ADDR32));
 }
 

commit 296f781a4b7801ad9c1c0219f9e87b6c25e196fe
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Apr 26 12:23:29 2016 -0700

    x86/asm/64: Rename thread_struct's fs and gs to fsbase and gsbase
    
    Unlike ds and es, these are base addresses, not selectors.  Rename
    them so their meaning is more obvious.
    
    On x86_32, the field is still called fs.  Fixing that could make sense
    as a future cleanup.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/69a18a51c4cba0ce29a241e570fc618ad721d908.1461698311.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 15340e36ddcb..fea7724141a0 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -176,7 +176,7 @@ static inline void elf_common_init(struct thread_struct *t,
 	regs->si = regs->di = regs->bp = 0;
 	regs->r8 = regs->r9 = regs->r10 = regs->r11 = 0;
 	regs->r12 = regs->r13 = regs->r14 = regs->r15 = 0;
-	t->fs = t->gs = 0;
+	t->fsbase = t->gsbase = 0;
 	t->fsindex = t->gsindex = 0;
 	t->ds = t->es = ds;
 }
@@ -226,8 +226,8 @@ do {								\
 	(pr_reg)[18] = (regs)->flags;				\
 	(pr_reg)[19] = (regs)->sp;				\
 	(pr_reg)[20] = (regs)->ss;				\
-	(pr_reg)[21] = current->thread.fs;			\
-	(pr_reg)[22] = current->thread.gs;			\
+	(pr_reg)[21] = current->thread.fsbase;			\
+	(pr_reg)[22] = current->thread.gsbase;			\
 	asm("movl %%ds,%0" : "=r" (v)); (pr_reg)[23] = v;	\
 	asm("movl %%es,%0" : "=r" (v)); (pr_reg)[24] = v;	\
 	asm("movl %%fs,%0" : "=r" (v)); (pr_reg)[25] = v;	\

commit 16aaa53756501914a863ae7a15fcb070dc27c3d7
Author: Huaitong Han <huaitong.han@intel.com>
Date:   Mon Jan 25 20:41:47 2016 +0100

    x86/cpufeature: Use enum cpuid_leafs instead of magic numbers
    
    Most of the magic numbers in x86_capability[] have been converted to
    'enum cpuid_leafs', and this patch updates the remaining part.
    
    Signed-off-by: Huaitong Han <huaitong.han@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Kuleshov <kuleshovmail@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hector Marco-Gisbert <hecmargi@upv.es>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: lguest@lists.ozlabs.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1453750913-4781-3-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 1514753fd435..15340e36ddcb 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -256,7 +256,7 @@ extern int force_personality32;
    instruction set this CPU supports.  This could be done in user space,
    but it's not easy, and we've already done it here.  */
 
-#define ELF_HWCAP		(boot_cpu_data.x86_capability[0])
+#define ELF_HWCAP		(boot_cpu_data.x86_capability[CPUID_1_EDX])
 
 /* This yields a string that ld.so will use to load implementation
    specific libraries for optimization.  This is more specific in

commit 7bcdea4d050cbe4912854a68b93494203eec8b24
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Oct 5 17:48:00 2015 -0700

    x86/elf/64: Clear more registers in elf_common_init()
    
    Before we start calling execve in contexts that honor the full
    pt_regs, we need to teach it to initialize all registers.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/65a38a9edee61a1158cfd230800c61dbd963dac5.1444091584.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 2ee05c4f5f37..1514753fd435 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -171,11 +171,11 @@ do {						\
 static inline void elf_common_init(struct thread_struct *t,
 				   struct pt_regs *regs, const u16 ds)
 {
-	/* Commented-out registers are cleared in stub_execve */
-	/*regs->ax = regs->bx =*/ regs->cx = regs->dx = 0;
-	regs->si = regs->di /*= regs->bp*/ = 0;
+	/* ax gets execve's return value. */
+	/*regs->ax = */ regs->bx = regs->cx = regs->dx = 0;
+	regs->si = regs->di = regs->bp = 0;
 	regs->r8 = regs->r9 = regs->r10 = regs->r11 = 0;
-	/*regs->r12 = regs->r13 = regs->r14 = regs->r15 = 0;*/
+	regs->r12 = regs->r13 = regs->r14 = regs->r15 = 0;
 	t->fs = t->gs = 0;
 	t->fsindex = t->gsindex = 0;
 	t->ds = t->es = ds;

commit 0a6d1fa0d2b48fbae444e46e7f37a4832b2f8bdf
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Oct 5 17:47:56 2015 -0700

    x86/vdso: Remove runtime 32-bit vDSO selection
    
    32-bit userspace will now always see the same vDSO, which is
    exactly what used to be the int80 vDSO.  Subsequent patches will
    clean it up and make it support SYSENTER and SYSCALL using
    alternatives.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/e7e6b3526fa442502e6125fe69486aab50813c32.1444091584.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 141c561f4664..2ee05c4f5f37 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -328,7 +328,7 @@ else									\
 
 #define VDSO_ENTRY							\
 	((unsigned long)current->mm->context.vdso +			\
-	 selected_vdso32->sym___kernel_vsyscall)
+	 vdso_image_32.sym___kernel_vsyscall)
 
 struct linux_binprm;
 

commit ab8b82ee6dad7c9c257f450d14719a0e3f327244
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jun 22 07:55:15 2015 -0400

    x86/compat: Don't build the 32-bit VDSO if not needed
    
    Build the 32-bit vdso only for native 32-bit or 32-bit compat is
    enabled.  x32 should not force it to build.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1434974121-32575-7-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 2bf67c0e9339..141c561f4664 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -78,7 +78,7 @@ typedef struct user_fxsr_struct elf_fpxregset_t;
 #ifdef CONFIG_X86_64
 extern unsigned int vdso64_enabled;
 #endif
-#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)
 extern unsigned int vdso32_enabled;
 #endif
 

commit 7da770785f9740af1cb24b8fd63075543bd00711
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jun 22 07:55:13 2015 -0400

    x86/compat: Rename 'start_thread_ia32' to 'compat_start_thread'
    
    This function is shared between the 32-bit compat and x32 ABIs.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1434974121-32575-5-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 180b6fe8aed3..2bf67c0e9339 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -187,8 +187,8 @@ static inline void elf_common_init(struct thread_struct *t,
 #define	COMPAT_ELF_PLAT_INIT(regs, load_addr)		\
 	elf_common_init(&current->thread, regs, __USER_DS)
 
-void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp);
-#define compat_start_thread start_thread_ia32
+void compat_start_thread(struct pt_regs *regs, u32 new_ip, u32 new_sp);
+#define compat_start_thread compat_start_thread
 
 void set_personality_ia32(bool);
 #define COMPAT_SET_PERSONALITY(ex)			\

commit b2e02b820d5b42479195b89d3d73f31bcedb264e
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jun 22 07:55:11 2015 -0400

    x86/compat: Make mmap_is_ia32() common compat
    
    TIF_ADDR32 is set for both ia32 and x32 tasks, so change from
    CONFIG_IA32_EMULATION to CONFIG_COMPAT.  Use config_enabled()
    to make the function more readable.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1434974121-32575-3-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index f161c189c27b..180b6fe8aed3 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -344,14 +344,9 @@ extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
  */
 static inline int mmap_is_ia32(void)
 {
-#ifdef CONFIG_X86_32
-	return 1;
-#endif
-#ifdef CONFIG_IA32_EMULATION
-	if (test_thread_flag(TIF_ADDR32))
-		return 1;
-#endif
-	return 0;
+	return config_enabled(CONFIG_X86_32) ||
+	       (config_enabled(CONFIG_COMPAT) &&
+		test_thread_flag(TIF_ADDR32));
 }
 
 /* Do not change the values. See get_align_mask() */

commit 204db6ed17743000691d930368a5abd6ea541c58
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Apr 14 15:48:12 2015 -0700

    mm: fold arch_randomize_brk into ARCH_HAS_ELF_RANDOMIZE
    
    The arch_randomize_brk() function is used on several architectures,
    even those that don't support ET_DYN ASLR. To avoid bulky extern/#define
    tricks, consolidate the support under CONFIG_ARCH_HAS_ELF_RANDOMIZE for
    the architectures that support it, while still handling CONFIG_COMPAT_BRK.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Hector Marco-Gisbert <hecmargi@upv.es>
    Cc: Russell King <linux@arm.linux.org.uk>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: "David A. Long" <dave.long@linaro.org>
    Cc: Andrey Ryabinin <a.ryabinin@samsung.com>
    Cc: Arun Chandran <achandran@mvista.com>
    Cc: Yann Droneaud <ydroneaud@opteya.com>
    Cc: Min-Hua Chen <orca.chen@gmail.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Alex Smith <alex@alex-smith.me.uk>
    Cc: Markos Chandras <markos.chandras@imgtec.com>
    Cc: Vineeth Vijayan <vvijayan@mvista.com>
    Cc: Jeff Bailey <jeffbailey@google.com>
    Cc: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: Behan Webster <behanw@converseincode.com>
    Cc: Ismael Ripoll <iripoll@upv.es>
    Cc: Jan-Simon Mller <dl9pf@gmx.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 935588d95c82..f161c189c27b 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -339,9 +339,6 @@ extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
 					      int uses_interp);
 #define compat_arch_setup_additional_pages compat_arch_setup_additional_pages
 
-extern unsigned long arch_randomize_brk(struct mm_struct *mm);
-#define arch_randomize_brk arch_randomize_brk
-
 /*
  * True on X86_32 or when emulating IA32 on X86_64
  */

commit 6cf78d4b3766bcd25348d72377796f9566ac8e1a
Merge: 0ad5c6b3c2d1 4e26d11f5268
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 12:31:32 2015 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - reduce the x86/32 PAE per task PGD allocation overhead from 4K to
         0.032k (Fenghua Yu)
    
       - early_ioremap/memunmap() usage cleanups (Juergen Gross)
    
       - gbpages support cleanups (Luis R Rodriguez)
    
       - improve AMD Bulldozer (family 0x15) ASLR I$ aliasing workaround to
         increase randomization by 3 bits (per bootup) (Hector
         Marco-Gisbert)
    
       - misc fixlets"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Improve AMD Bulldozer ASLR workaround
      x86/mm/pat: Initialize __cachemode2pte_tbl[] and __pte2cachemode_tbl[] in a bit more readable fashion
      init.h: Clean up the __setup()/early_param() macros
      x86/mm: Simplify probe_page_size_mask()
      x86/mm: Further simplify 1 GB kernel linear mappings handling
      x86/mm: Use early_param_on_off() for direct_gbpages
      init.h: Add early_param_on_off()
      x86/mm: Simplify enabling direct_gbpages
      x86/mm: Use IS_ENABLED() for direct_gbpages
      x86/mm: Unexport set_memory_ro() and set_memory_rw()
      x86/mm, efi: Use early_ioremap() in arch/x86/platform/efi/efi-bgrt.c
      x86/mm: Use early_memunmap() instead of early_iounmap()
      x86/mm/pat: Ensure different messages in STRICT_DEVMEM and PAT cases
      x86/mm: Reduce PAE-mode per task pgd allocation overhead from 4K to 32 bytes

commit fc3e958a2b552fe6210e6de3bebb4348156a0b5f
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Sat Apr 4 20:55:19 2015 +0200

    x86/asm/entry: Clear EXTRA_REGS for all executable formats
    
    On failure, sys_execve() does not clobber EXTRA_REGS, so we can
    just return to userpsace without saving/restoring them.
    
    On success, ELF_PLAT_INIT() in sys_execve() clears all these
    registers.
    
    On other executable formats:
    
      - binfmt_flat.c has similar FLAT_PLAT_INIT, but x86 (and everyone
        else except sh) doesn't define it.
    
      - binfmt_elf_fdpic.c has ELF_FDPIC_PLAT_INIT, but x86 (and most
        others) doesn't define it.
    
      - There are no such hooks in binfmt_aout.c et al. We inherit
        EXTRA_REGS from the prior executable.
    
    This inconsistency was not intended.
    
    This change removes SAVE/RESTORE_EXTRA_REGS in stub_execve,
    removes register clearing in ELF_PLAT_INIT(),
    and instead simply clears them on success in stub_execve.
    
    Run-tested.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1428173719-7637-1-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index ca3347a9dab5..3563107b5060 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -171,10 +171,11 @@ do {						\
 static inline void elf_common_init(struct thread_struct *t,
 				   struct pt_regs *regs, const u16 ds)
 {
-	regs->ax = regs->bx = regs->cx = regs->dx = 0;
-	regs->si = regs->di = regs->bp = 0;
+	/* Commented-out registers are cleared in stub_execve */
+	/*regs->ax = regs->bx =*/ regs->cx = regs->dx = 0;
+	regs->si = regs->di /*= regs->bp*/ = 0;
 	regs->r8 = regs->r9 = regs->r10 = regs->r11 = 0;
-	regs->r12 = regs->r13 = regs->r14 = regs->r15 = 0;
+	/*regs->r12 = regs->r13 = regs->r14 = regs->r15 = 0;*/
 	t->fs = t->gs = 0;
 	t->fsindex = t->gsindex = 0;
 	t->ds = t->es = ds;

commit 4e26d11f52684dc8b1632a8cfe450cb5197a8464
Author: Hector Marco-Gisbert <hecmargi@upv.es>
Date:   Fri Mar 27 12:38:21 2015 +0100

    x86/mm: Improve AMD Bulldozer ASLR workaround
    
    The ASLR implementation needs to special-case AMD F15h processors by
    clearing out bits [14:12] of the virtual address in order to avoid I$
    cross invalidations and thus performance penalty for certain workloads.
    For details, see:
    
      dfb09f9b7ab0 ("x86, amd: Avoid cache aliasing penalties on AMD family 15h")
    
    This special case reduces the mmapped file's entropy by 3 bits.
    
    The following output is the run on an AMD Opteron 62xx class CPU
    processor under x86_64 Linux 4.0.0:
    
      $ for i in `seq 1 10`; do cat /proc/self/maps | grep "r-xp.*libc" ; done
      b7588000-b7736000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b7570000-b771e000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b75d0000-b777e000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b75b0000-b775e000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b7578000-b7726000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      ...
    
    Bits [12:14] are always 0, i.e. the address always ends in 0x8000 or
    0x0000.
    
    32-bit systems, as in the example above, are especially sensitive
    to this issue because 32-bit randomness for VA space is 8 bits (see
    mmap_rnd()). With the Bulldozer special case, this diminishes to only 32
    different slots of mmap virtual addresses.
    
    This patch randomizes per boot the three affected bits rather than
    setting them to zero. Since all the shared pages have the same value
    at bits [12..14], there is no cache aliasing problems. This value gets
    generated during system boot and it is thus not known to a potential
    remote attacker. Therefore, the impact from the Bulldozer workaround
    gets diminished and ASLR randomness increased.
    
    More details at:
    
      http://hmarco.org/bugs/AMD-Bulldozer-linux-ASLR-weakness-reducing-mmaped-files-by-eight.html
    
    Original white paper by AMD dealing with the issue:
    
      http://developer.amd.com/wordpress/media/2012/10/SharedL1InstructionCacheonAMD15hCPU.pdf
    
    Mentored-by: Ismael Ripoll <iripoll@disca.upv.es>
    Signed-off-by: Hector Marco-Gisbert <hecmargi@upv.es>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan-Simon <dl9pf@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-fsdevel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1427456301-3764-1-git-send-email-hecmargi@upv.es
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index ca3347a9dab5..bd292ce9be0a 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -365,6 +365,7 @@ enum align_flags {
 struct va_alignment {
 	int flags;
 	unsigned long mask;
+	unsigned long bits;
 } ____cacheline_aligned;
 
 extern struct va_alignment va_align;

commit 0e6d3112a4e95d55cf6dca88f298d5f4b8f29bd1
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Sun Sep 7 21:05:05 2014 +0100

    x86: Reject x32 executables if x32 ABI not supported
    
    It is currently possible to execve() an x32 executable on an x86_64
    kernel that has only ia32 compat enabled.  However all its syscalls
    will fail, even _exit().  This usually causes it to segfault.
    
    Change the ELF compat architecture check so that x32 executables are
    rejected if we don't support the x32 ABI.
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Link: http://lkml.kernel.org/r/1410120305.6822.9.camel@decadent.org.uk
    Cc: stable@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 1a055c81d864..ca3347a9dab5 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -160,8 +160,9 @@ do {						\
 #define elf_check_arch(x)			\
 	((x)->e_machine == EM_X86_64)
 
-#define compat_elf_check_arch(x)		\
-	(elf_check_arch_ia32(x) || (x)->e_machine == EM_X86_64)
+#define compat_elf_check_arch(x)					\
+	(elf_check_arch_ia32(x) ||					\
+	 (IS_ENABLED(CONFIG_X86_X32_ABI) && (x)->e_machine == EM_X86_64))
 
 #if __USER32_DS != __USER_DS
 # error "The following code assumes __USER32_DS == __USER_DS"

commit 18d0a6fd227177fd243993179c90e454d0638b06
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon May 5 12:19:35 2014 -0700

    x86, vdso: Move the 32-bit vdso special pages after the text
    
    This unifies the vdso mapping code and teaches it how to map special
    pages at addresses corresponding to symbols in the vdso image.  The
    new code is used for all vdso variants, but so far only the 32-bit
    variants use the new vvar page position.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/b6d7858ad7b5ac3fd3c29cab6d6d769bc45d195e.1399317206.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 65b21bcbe9f7..1a055c81d864 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -333,11 +333,9 @@ struct linux_binprm;
 #define ARCH_HAS_SETUP_ADDITIONAL_PAGES 1
 extern int arch_setup_additional_pages(struct linux_binprm *bprm,
 				       int uses_interp);
-extern int x32_setup_additional_pages(struct linux_binprm *bprm,
-				      int uses_interp);
-
-extern int syscall32_setup_pages(struct linux_binprm *, int exstack);
-#define compat_arch_setup_additional_pages	syscall32_setup_pages
+extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
+					      int uses_interp);
+#define compat_arch_setup_additional_pages compat_arch_setup_additional_pages
 
 extern unsigned long arch_randomize_brk(struct mm_struct *mm);
 #define arch_randomize_brk arch_randomize_brk

commit 6f121e548f83674ab4920a4e60afb58d4f61b829
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon May 5 12:19:34 2014 -0700

    x86, vdso: Reimplement vdso.so preparation in build-time C
    
    Currently, vdso.so files are prepared and analyzed by a combination
    of objcopy, nm, some linker script tricks, and some simple ELF
    parsers in the kernel.  Replace all of that with plain C code that
    runs at build time.
    
    All five vdso images now generate .c files that are compiled and
    linked in to the kernel image.
    
    This should cause only one userspace-visible change: the loaded vDSO
    images are stripped more heavily than they used to be.  Everything
    outside the loadable segment is dropped.  In particular, this causes
    the section table and section name strings to be missing.  This
    should be fine: real dynamic loaders don't load or inspect these
    tables anyway.  The result is roughly equivalent to eu-strip's
    --strip-sections option.
    
    The purpose of this change is to enable the vvar and hpet mappings
    to be moved to the page following the vDSO load segment.  Currently,
    it is possible for the section table to extend into the page after
    the load segment, so, if we map it, it risks overlapping the vvar or
    hpet page.  This happens whenever the load segment is just under a
    multiple of PAGE_SIZE.
    
    The only real subtlety here is that the old code had a C file with
    inline assembler that did 'call VDSO32_vsyscall' and a linker script
    that defined 'VDSO32_vsyscall = __kernel_vsyscall'.  This most
    likely worked by accident: the linker script entry defines a symbol
    associated with an address as opposed to an alias for the real
    dynamic symbol __kernel_vsyscall.  That caused ld to relocate the
    reference at link time instead of leaving an interposable dynamic
    relocation.  Since the VDSO32_vsyscall hack is no longer needed, I
    now use 'call __kernel_vsyscall', and I added -Bsymbolic to make it
    work.  vdso2c will generate an error and abort the build if the
    resulting image contains any dynamic relocations, so we won't
    silently generate bad vdso images.
    
    (Dynamic relocations are a problem because nothing will even attempt
    to relocate the vdso.)
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/2c4fcf45524162a34d87fdda1eb046b2a5cecee7.1399317206.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index e96df2c0dd69..65b21bcbe9f7 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -299,7 +299,7 @@ do {									\
 do {									\
 	if (vdso64_enabled)						\
 		NEW_AUX_ENT(AT_SYSINFO_EHDR,				\
-			    (unsigned long)current->mm->context.vdso);	\
+			    (unsigned long __force)current->mm->context.vdso); \
 } while (0)
 
 /* As a historical oddity, the x32 and x86_64 vDSOs are controlled together. */
@@ -307,7 +307,7 @@ do {									\
 do {									\
 	if (vdso64_enabled)						\
 		NEW_AUX_ENT(AT_SYSINFO_EHDR,				\
-			    (unsigned long)current->mm->context.vdso);	\
+			    (unsigned long __force)current->mm->context.vdso); \
 } while (0)
 
 #define AT_SYSINFO		32
@@ -325,7 +325,8 @@ else									\
 #define VDSO_CURRENT_BASE	((unsigned long)current->mm->context.vdso)
 
 #define VDSO_ENTRY							\
-	((unsigned long)VDSO32_SYMBOL(VDSO_CURRENT_BASE, vsyscall))
+	((unsigned long)current->mm->context.vdso +			\
+	 selected_vdso32->sym___kernel_vsyscall)
 
 struct linux_binprm;
 

commit 3d7ee969bffcc984c8aeaffc6ac6816fd929ace1
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon May 5 12:19:32 2014 -0700

    x86, vdso: Clean up 32-bit vs 64-bit vdso params
    
    Rather than using 'vdso_enabled' and an awful #define, just call the
    parameters vdso32_enabled and vdso64_enabled.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/87913de56bdcbae3d93917938302fc369b05caee.1399317206.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 2c71182d30ef..e96df2c0dd69 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -75,7 +75,12 @@ typedef struct user_fxsr_struct elf_fpxregset_t;
 
 #include <asm/vdso.h>
 
-extern unsigned int vdso_enabled;
+#ifdef CONFIG_X86_64
+extern unsigned int vdso64_enabled;
+#endif
+#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+extern unsigned int vdso32_enabled;
+#endif
 
 /*
  * This is used to ensure we don't load something for the wrong architecture.
@@ -269,9 +274,9 @@ extern int force_personality32;
 
 struct task_struct;
 
-#define	ARCH_DLINFO_IA32(vdso_enabled)					\
+#define	ARCH_DLINFO_IA32						\
 do {									\
-	if (vdso_enabled) {						\
+	if (vdso32_enabled) {						\
 		NEW_AUX_ENT(AT_SYSINFO,	VDSO_ENTRY);			\
 		NEW_AUX_ENT(AT_SYSINFO_EHDR, VDSO_CURRENT_BASE);	\
 	}								\
@@ -281,7 +286,7 @@ do {									\
 
 #define STACK_RND_MASK (0x7ff)
 
-#define ARCH_DLINFO		ARCH_DLINFO_IA32(vdso_enabled)
+#define ARCH_DLINFO		ARCH_DLINFO_IA32
 
 /* update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT entries changes */
 
@@ -292,14 +297,15 @@ do {									\
 
 #define ARCH_DLINFO							\
 do {									\
-	if (vdso_enabled)						\
+	if (vdso64_enabled)						\
 		NEW_AUX_ENT(AT_SYSINFO_EHDR,				\
 			    (unsigned long)current->mm->context.vdso);	\
 } while (0)
 
+/* As a historical oddity, the x32 and x86_64 vDSOs are controlled together. */
 #define ARCH_DLINFO_X32							\
 do {									\
-	if (vdso_enabled)						\
+	if (vdso64_enabled)						\
 		NEW_AUX_ENT(AT_SYSINFO_EHDR,				\
 			    (unsigned long)current->mm->context.vdso);	\
 } while (0)
@@ -310,7 +316,7 @@ do {									\
 if (test_thread_flag(TIF_X32))						\
 	ARCH_DLINFO_X32;						\
 else									\
-	ARCH_DLINFO_IA32(sysctl_vsyscall32)
+	ARCH_DLINFO_IA32
 
 #define COMPAT_ELF_ET_DYN_BASE	(TASK_UNMAPPED_BASE + 0x1000000)
 

commit b0b49f2673f011cad7deeabf7a683b388c351278
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 13 16:01:26 2014 -0700

    x86, vdso: Remove compat vdso support
    
    The compat vDSO is a complicated hack that's needed to maintain
    compatibility with a small range of glibc versions.
    
    This removes it and replaces it with a much simpler hack: a config
    option to disable the 32-bit vDSO by default.
    
    This also changes the default value of CONFIG_COMPAT_VDSO to n --
    users configuring kernels from scratch almost certainly want that
    choice.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/4bb4690899106eb11430b1186d5cc66ca9d1660c.1394751608.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 9c999c1674fa..2c71182d30ef 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -281,16 +281,12 @@ do {									\
 
 #define STACK_RND_MASK (0x7ff)
 
-#define VDSO_HIGH_BASE		(__fix_to_virt(FIX_VDSO))
-
 #define ARCH_DLINFO		ARCH_DLINFO_IA32(vdso_enabled)
 
 /* update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT entries changes */
 
 #else /* CONFIG_X86_32 */
 
-#define VDSO_HIGH_BASE		0xffffe000U /* CONFIG_COMPAT_VDSO address */
-
 /* 1GB for 64bit, 8MB for 32bit */
 #define STACK_RND_MASK (test_thread_flag(TIF_ADDR32) ? 0x7ff : 0x3fffff)
 

commit f99024729e689f5de4534fde5400e3b035f068de
Author: Michel Lespinasse <walken@google.com>
Date:   Tue Dec 11 16:01:52 2012 -0800

    mm: use vm_unmapped_area() on x86_64 architecture
    
    Update the x86_64 arch_get_unmapped_area[_topdown] functions to make use
    of vm_unmapped_area() instead of implementing a brute force search.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 5939f44fe0c0..9c999c1674fa 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -354,12 +354,10 @@ static inline int mmap_is_ia32(void)
 	return 0;
 }
 
-/* The first two values are special, do not change. See align_addr() */
+/* Do not change the values. See get_align_mask() */
 enum align_flags {
 	ALIGN_VA_32	= BIT(0),
 	ALIGN_VA_64	= BIT(1),
-	ALIGN_VDSO	= BIT(2),
-	ALIGN_TOPDOWN	= BIT(3),
 };
 
 struct va_alignment {
@@ -368,5 +366,5 @@ struct va_alignment {
 } ____cacheline_aligned;
 
 extern struct va_alignment va_align;
-extern unsigned long align_addr(unsigned long, struct file *, enum align_flags);
+extern unsigned long align_vdso_addr(unsigned long);
 #endif /* _ASM_X86_ELF_H */

commit a591afc01d9e48affbacb365558a31e53c85af45
Merge: 820d41cf0cd0 31796ac4e8f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:12:23 2012 -0700

    Merge branch 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x32 support for x86-64 from Ingo Molnar:
     "This tree introduces the X32 binary format and execution mode for x86:
      32-bit data space binaries using 64-bit instructions and 64-bit kernel
      syscalls.
    
      This allows applications whose working set fits into a 32 bits address
      space to make use of 64-bit instructions while using a 32-bit address
      space with shorter pointers, more compressed data structures, etc."
    
    Fix up trivial context conflicts in arch/x86/{Kconfig,vdso/vma.c}
    
    * 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (71 commits)
      x32: Fix alignment fail in struct compat_siginfo
      x32: Fix stupid ia32/x32 inversion in the siginfo format
      x32: Add ptrace for x32
      x32: Switch to a 64-bit clock_t
      x32: Provide separate is_ia32_task() and is_x32_task() predicates
      x86, mtrr: Use explicit sizing and padding for the 64-bit ioctls
      x86/x32: Fix the binutils auto-detect
      x32: Warn and disable rather than error if binutils too old
      x32: Only clear TIF_X32 flag once
      x32: Make sure TS_COMPAT is cleared for x32 tasks
      fs: Remove missed ->fds_bits from cessation use of fd_set structs internally
      fs: Fix close_on_exec pointer in alloc_fdtable
      x32: Drop non-__vdso weak symbols from the x32 VDSO
      x32: Fix coding style violations in the x32 VDSO code
      x32: Add x32 VDSO support
      x32: Allow x32 to be configured
      x32: If configured, add x32 system calls to system call tables
      x32: Handle process creation
      x32: Signal-related system calls
      x86: Add #ifdef CONFIG_COMPAT to <asm/sys_ia32.h>
      ...

commit f05e798ad4c09255f590f5b2c00a7ca6c172f983
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:11:12 2012 +0100

    Disintegrate asm/system.h for X86
    
    Disintegrate asm/system.h for X86.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    cc: x86@kernel.org

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 5f962df30d0f..f27f79abe021 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -84,7 +84,6 @@ extern unsigned int vdso_enabled;
 	(((x)->e_machine == EM_386) || ((x)->e_machine == EM_486))
 
 #include <asm/processor.h>
-#include <asm/system.h>
 
 #ifdef CONFIG_X86_32
 #include <asm/desc.h>

commit 22e842d4d90ffec9677cc114487a5cefd39b5643
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Feb 21 14:32:19 2012 -0800

    x32: Fix coding style violations in the x32 VDSO code
    
    Move the prototype for x32_setup_additional_pages() to a header file,
    and adjust the coding style to match standard.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: H. J. Lu <hjl.tools@gmail.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 83aabea95dd7..1e40634591a4 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -331,6 +331,8 @@ struct linux_binprm;
 #define ARCH_HAS_SETUP_ADDITIONAL_PAGES 1
 extern int arch_setup_additional_pages(struct linux_binprm *bprm,
 				       int uses_interp);
+extern int x32_setup_additional_pages(struct linux_binprm *bprm,
+				      int uses_interp);
 
 extern int syscall32_setup_pages(struct linux_binprm *, int exstack);
 #define compat_arch_setup_additional_pages	syscall32_setup_pages

commit d1a797f388d6d30fa502915d1b9937ed758b7137
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sun Feb 19 10:06:34 2012 -0800

    x32: Handle process creation
    
    Allow an x32 process to be started.
    
    Originally-by: H. J. Lu <hjl.tools@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 410fa6a219f6..83aabea95dd7 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -156,7 +156,12 @@ do {						\
 #define elf_check_arch(x)			\
 	((x)->e_machine == EM_X86_64)
 
-#define compat_elf_check_arch(x)	elf_check_arch_ia32(x)
+#define compat_elf_check_arch(x)		\
+	(elf_check_arch_ia32(x) || (x)->e_machine == EM_X86_64)
+
+#if __USER32_DS != __USER_DS
+# error "The following code assumes __USER32_DS == __USER_DS"
+#endif
 
 static inline void elf_common_init(struct thread_struct *t,
 				   struct pt_regs *regs, const u16 ds)
@@ -179,8 +184,9 @@ static inline void elf_common_init(struct thread_struct *t,
 void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp);
 #define compat_start_thread start_thread_ia32
 
-void set_personality_ia32(void);
-#define COMPAT_SET_PERSONALITY(ex) set_personality_ia32()
+void set_personality_ia32(bool);
+#define COMPAT_SET_PERSONALITY(ex)			\
+	set_personality_ia32((ex).e_machine == EM_X86_64)
 
 #define COMPAT_ELF_PLATFORM			("i686")
 
@@ -296,9 +302,20 @@ do {									\
 			    (unsigned long)current->mm->context.vdso);	\
 } while (0)
 
+#define ARCH_DLINFO_X32							\
+do {									\
+	if (vdso_enabled)						\
+		NEW_AUX_ENT(AT_SYSINFO_EHDR,				\
+			    (unsigned long)current->mm->context.vdso);	\
+} while (0)
+
 #define AT_SYSINFO		32
 
-#define COMPAT_ARCH_DLINFO	ARCH_DLINFO_IA32(sysctl_vsyscall32)
+#define COMPAT_ARCH_DLINFO						\
+if (test_thread_flag(TIF_X32))						\
+	ARCH_DLINFO_X32;						\
+else									\
+	ARCH_DLINFO_IA32(sysctl_vsyscall32)
 
 #define COMPAT_ELF_ET_DYN_BASE	(TASK_UNMAPPED_BASE + 0x1000000)
 

commit 6bd330083e0e97b7ddc053459190bf3d5768ca83
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Feb 6 13:03:09 2012 -0800

    x86: Factor out TIF_IA32 from 32-bit address space
    
    Factor out IA32 (compatibility instruction set) from 32-bit address
    space in the thread_info flags; this is a precondition patch for x32
    support.
    
    Originally-by: H. J. Lu <hjl.tools@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/n/tip-4pr1xnnksprt7t0h3w5fw4rv@git.kernel.org

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 5f962df30d0f..410fa6a219f6 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -287,7 +287,7 @@ do {									\
 #define VDSO_HIGH_BASE		0xffffe000U /* CONFIG_COMPAT_VDSO address */
 
 /* 1GB for 64bit, 8MB for 32bit */
-#define STACK_RND_MASK (test_thread_flag(TIF_IA32) ? 0x7ff : 0x3fffff)
+#define STACK_RND_MASK (test_thread_flag(TIF_ADDR32) ? 0x7ff : 0x3fffff)
 
 #define ARCH_DLINFO							\
 do {									\
@@ -330,7 +330,7 @@ static inline int mmap_is_ia32(void)
 	return 1;
 #endif
 #ifdef CONFIG_IA32_EMULATION
-	if (test_thread_flag(TIF_IA32))
+	if (test_thread_flag(TIF_ADDR32))
 		return 1;
 #endif
 	return 0;

commit dfb09f9b7ab03fd367740e541a5caf830ed56726
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Fri Aug 5 15:15:08 2011 +0200

    x86, amd: Avoid cache aliasing penalties on AMD family 15h
    
    This patch provides performance tuning for the "Bulldozer" CPU. With its
    shared instruction cache there is a chance of generating an excessive
    number of cache cross-invalidates when running specific workloads on the
    cores of a compute module.
    
    This excessive amount of cross-invalidations can be observed if cache
    lines backed by shared physical memory alias in bits [14:12] of their
    virtual addresses, as those bits are used for the index generation.
    
    This patch addresses the issue by clearing all the bits in the [14:12]
    slice of the file mapping's virtual address at generation time, thus
    forcing those bits the same for all mappings of a single shared library
    across processes and, in doing so, avoids instruction cache aliases.
    
    It also adds the command line option "align_va_addr=(32|64|on|off)" with
    which virtual address alignment can be enabled for 32-bit or 64-bit x86
    individually, or both, or be completely disabled.
    
    This change leaves virtual region address allocation on other families
    and/or vendors unaffected.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1312550110-24160-2-git-send-email-bp@amd64.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index f2ad2163109d..5f962df30d0f 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -4,6 +4,7 @@
 /*
  * ELF register definitions..
  */
+#include <linux/thread_info.h>
 
 #include <asm/ptrace.h>
 #include <asm/user.h>
@@ -320,4 +321,34 @@ extern int syscall32_setup_pages(struct linux_binprm *, int exstack);
 extern unsigned long arch_randomize_brk(struct mm_struct *mm);
 #define arch_randomize_brk arch_randomize_brk
 
+/*
+ * True on X86_32 or when emulating IA32 on X86_64
+ */
+static inline int mmap_is_ia32(void)
+{
+#ifdef CONFIG_X86_32
+	return 1;
+#endif
+#ifdef CONFIG_IA32_EMULATION
+	if (test_thread_flag(TIF_IA32))
+		return 1;
+#endif
+	return 0;
+}
+
+/* The first two values are special, do not change. See align_addr() */
+enum align_flags {
+	ALIGN_VA_32	= BIT(0),
+	ALIGN_VA_64	= BIT(1),
+	ALIGN_VDSO	= BIT(2),
+	ALIGN_TOPDOWN	= BIT(3),
+};
+
+struct va_alignment {
+	int flags;
+	unsigned long mask;
+} ____cacheline_aligned;
+
+extern struct va_alignment va_align;
+extern unsigned long align_addr(unsigned long, struct file *, enum align_flags);
 #endif /* _ASM_X86_ELF_H */

commit 11557b24fdec13cb1c3d5f681688401a651ed54e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Feb 16 15:24:01 2010 +0100

    x86: ELF_PLAT_INIT() shouldn't worry about TIF_IA32
    
    The 64-bit version of ELF_PLAT_INIT() clears TIF_IA32, but at this point
    it has already been cleared by SET_PERSONALITY == set_personality_64bit.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 1994d3f58443..f2ad2163109d 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -170,10 +170,7 @@ static inline void elf_common_init(struct thread_struct *t,
 }
 
 #define ELF_PLAT_INIT(_r, load_addr)			\
-do {							\
-	elf_common_init(&current->thread, _r, 0);	\
-	clear_thread_flag(TIF_IA32);			\
-} while (0)
+	elf_common_init(&current->thread, _r, 0)
 
 #define	COMPAT_ELF_PLAT_INIT(regs, load_addr)		\
 	elf_common_init(&current->thread, regs, __USER_DS)

commit 05d43ed8a89c159ff641d472f970e3f1baa66318
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jan 28 22:14:43 2010 -0800

    x86: get rid of the insane TIF_ABI_PENDING bit
    
    Now that the previous commit made it possible to do the personality
    setting at the point of no return, we do just that for ELF binaries.
    And suddenly all the reasons for that insane TIF_ABI_PENDING bit go
    away, and we can just make SET_PERSONALITY() just do the obvious thing
    for a 32-bit compat process.
    
    Everything becomes much more straightforward this way.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index b4501ee223ad..1994d3f58443 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -181,14 +181,8 @@ do {							\
 void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp);
 #define compat_start_thread start_thread_ia32
 
-#define COMPAT_SET_PERSONALITY(ex)			\
-do {							\
-	if (test_thread_flag(TIF_IA32))			\
-		clear_thread_flag(TIF_ABI_PENDING);	\
-	else						\
-		set_thread_flag(TIF_ABI_PENDING);	\
-	current->personality |= force_personality32;	\
-} while (0)
+void set_personality_ia32(void);
+#define COMPAT_SET_PERSONALITY(ex) set_personality_ia32()
 
 #define COMPAT_ELF_PLATFORM			("i686")
 

commit 698ba7b5a3a7be772922340fade365c675b8243f
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 15 16:47:37 2009 -0800

    elf: kill USE_ELF_CORE_DUMP
    
    Currently all architectures but microblaze unconditionally define
    USE_ELF_CORE_DUMP.  The microblaze omission seems like an error to me, so
    let's kill this ifdef and make sure we are the same everywhere.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: <linux-arch@vger.kernel.org>
    Cc: Michal Simek <michal.simek@petalogix.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 8a024babe5e6..b4501ee223ad 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -239,7 +239,6 @@ extern int force_personality32;
 #endif /* !CONFIG_X86_32 */
 
 #define CORE_DUMP_USE_REGSET
-#define USE_ELF_CORE_DUMP
 #define ELF_EXEC_PAGESIZE	4096
 
 /* This is the location that an ET_DYN program is loaded if exec'ed.  Typical

commit a6f05a6a0a1713d5b019f096799d49226807d3df
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Oct 8 18:02:54 2009 -0700

    x86-64: make compat_start_thread() match start_thread()
    
    For no real good reason, compat_start_thread() was embedded inline in
    <asm/elf.h> whereas the native start_thread() lives in process_*.c.
    Move compat_start_thread() to process_64.c, remove gratuitious
    differences, and fix a few items which mostly look like bit rot.
    
    In particular, compat_start_thread() didn't do free_thread_xstate(),
    which means it was hanging on to the xstate store area even when it
    was not needed.  It was also not setting old_rsp, but it looks like
    that generally shouldn't matter for a 32-bit process.
    
    Note: compat_start_thread *has* to be a macro, since it is tested with
    start_thread_ia32() as the out of line function name.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 456a304b8172..8a024babe5e6 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -157,19 +157,6 @@ do {						\
 
 #define compat_elf_check_arch(x)	elf_check_arch_ia32(x)
 
-static inline void start_ia32_thread(struct pt_regs *regs, u32 ip, u32 sp)
-{
-	loadsegment(fs, 0);
-	loadsegment(ds, __USER32_DS);
-	loadsegment(es, __USER32_DS);
-	load_gs_index(0);
-	regs->ip = ip;
-	regs->sp = sp;
-	regs->flags = X86_EFLAGS_IF;
-	regs->cs = __USER32_CS;
-	regs->ss = __USER32_DS;
-}
-
 static inline void elf_common_init(struct thread_struct *t,
 				   struct pt_regs *regs, const u16 ds)
 {
@@ -191,11 +178,8 @@ do {							\
 #define	COMPAT_ELF_PLAT_INIT(regs, load_addr)		\
 	elf_common_init(&current->thread, regs, __USER_DS)
 
-#define	compat_start_thread(regs, ip, sp)		\
-do {							\
-	start_ia32_thread(regs, ip, sp);		\
-	set_fs(USER_DS);				\
-} while (0)
+void start_thread_ia32(struct pt_regs *regs, u32 new_ip, u32 new_sp);
+#define compat_start_thread start_thread_ia32
 
 #define COMPAT_SET_PERSONALITY(ex)			\
 do {							\

commit 80938332d8cf652f6b16e0788cf0ca136befe0b5
Author: Michal Hocko <mhocko@suse.cz>
Date:   Tue Sep 8 11:01:55 2009 +0200

    x86: Increase MIN_GAP to include randomized stack
    
    Currently we are not including randomized stack size when calculating
    mmap_base address in arch_pick_mmap_layout for topdown case. This might
    cause that mmap_base starts in the stack reserved area because stack is
    randomized by 1GB for 64b (8MB for 32b) and the minimum gap is 128MB.
    
    If the stack really grows down to mmap_base then we can get silent mmap
    region overwrite by the stack values.
    
    Let's include maximum stack randomization size into MIN_GAP which is
    used as the low bound for the gap in mmap.
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    LKML-Reference: <1252400515-6866-1-git-send-email-mhocko@suse.cz>
    Acked-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Stable Team <stable@kernel.org>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 83c1bc8d2e8a..456a304b8172 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -299,6 +299,8 @@ do {									\
 
 #ifdef CONFIG_X86_32
 
+#define STACK_RND_MASK (0x7ff)
+
 #define VDSO_HIGH_BASE		(__fix_to_virt(FIX_VDSO))
 
 #define ARCH_DLINFO		ARCH_DLINFO_IA32(vdso_enabled)

commit ccbeed3a05908d201b47b6c3dd1a373138bba566
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 9 22:17:40 2009 +0900

    x86: make lazy %gs optional on x86_32
    
    Impact: pt_regs changed, lazy gs handling made optional, add slight
            overhead to SAVE_ALL, simplifies error_code path a bit
    
    On x86_32, %gs hasn't been used by kernel and handled lazily.  pt_regs
    doesn't have place for it and gs is saved/loaded only when necessary.
    In preparation for stack protector support, this patch makes lazy %gs
    handling optional by doing the followings.
    
    * Add CONFIG_X86_32_LAZY_GS and place for gs in pt_regs.
    
    * Save and restore %gs along with other registers in entry_32.S unless
      LAZY_GS.  Note that this unfortunately adds "pushl $0" on SAVE_ALL
      even when LAZY_GS.  However, it adds no overhead to common exit path
      and simplifies entry path with error code.
    
    * Define different user_gs accessors depending on LAZY_GS and add
      lazy_save_gs() and lazy_load_gs() which are noop if !LAZY_GS.  The
      lazy_*_gs() ops are used to save, load and clear %gs lazily.
    
    * Define ELF_CORE_COPY_KERNEL_REGS() which always read %gs directly.
    
    xen and lguest changes need to be verified.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 39b0aac1675c..83c1bc8d2e8a 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -112,7 +112,7 @@ extern unsigned int vdso_enabled;
  * now struct_user_regs, they are different)
  */
 
-#define ELF_CORE_COPY_REGS(pr_reg, regs)	\
+#define ELF_CORE_COPY_REGS_COMMON(pr_reg, regs)	\
 do {						\
 	pr_reg[0] = regs->bx;			\
 	pr_reg[1] = regs->cx;			\
@@ -124,7 +124,6 @@ do {						\
 	pr_reg[7] = regs->ds & 0xffff;		\
 	pr_reg[8] = regs->es & 0xffff;		\
 	pr_reg[9] = regs->fs & 0xffff;		\
-	pr_reg[10] = get_user_gs(regs);		\
 	pr_reg[11] = regs->orig_ax;		\
 	pr_reg[12] = regs->ip;			\
 	pr_reg[13] = regs->cs & 0xffff;		\
@@ -133,6 +132,18 @@ do {						\
 	pr_reg[16] = regs->ss & 0xffff;		\
 } while (0);
 
+#define ELF_CORE_COPY_REGS(pr_reg, regs)	\
+do {						\
+	ELF_CORE_COPY_REGS_COMMON(pr_reg, regs);\
+	pr_reg[10] = get_user_gs(regs);		\
+} while (0);
+
+#define ELF_CORE_COPY_KERNEL_REGS(pr_reg, regs)	\
+do {						\
+	ELF_CORE_COPY_REGS_COMMON(pr_reg, regs);\
+	savesegment(gs, pr_reg[10]);		\
+} while (0);
+
 #define ELF_PLATFORM	(utsname()->machine)
 #define set_personality_64bit()	do { } while (0)
 

commit d9a89a26e02ef9ed03f74a755a8b4d8f3a066622
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 9 22:17:40 2009 +0900

    x86: add %gs accessors for x86_32
    
    Impact: cleanup
    
    On x86_32, %gs is handled lazily.  It's not saved and restored on
    kernel entry/exit but only when necessary which usually is during task
    switch but there are few other places.  Currently, it's done by
    calling savesegment() and loadsegment() explicitly.  Define
    get_user_gs(), set_user_gs() and task_user_gs() and use them instead.
    
    While at it, clean up register access macros in signal.c.
    
    This cleans up code a bit and will help future changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index f51a3ddde01a..39b0aac1675c 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -124,7 +124,7 @@ do {						\
 	pr_reg[7] = regs->ds & 0xffff;		\
 	pr_reg[8] = regs->es & 0xffff;		\
 	pr_reg[9] = regs->fs & 0xffff;		\
-	savesegment(gs, pr_reg[10]);		\
+	pr_reg[10] = get_user_gs(regs);		\
 	pr_reg[11] = regs->orig_ax;		\
 	pr_reg[12] = regs->ip;			\
 	pr_reg[13] = regs->cs & 0xffff;		\

commit fc5243d98ac2575ad14a974b3c097e9ba874c03d
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Dec 25 13:38:35 2008 +0100

    [S390] arch_setup_additional_pages arguments
    
    arch_setup_additional_pages currently gets two arguments, the binary
    format descripton and an indication if the process uses an executable
    stack or not. The second argument is not used by anybody, it could
    be removed without replacement.
    
    What actually does make sense is to pass an indication if the process
    uses the elf interpreter or not. The glibc code will not use anything
    from the vdso if the process does not use the dynamic linker, so for
    statically linked binaries the architecture backend can choose not
    to map the vdso.
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 40ca1bea7916..f51a3ddde01a 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -325,7 +325,7 @@ struct linux_binprm;
 
 #define ARCH_HAS_SETUP_ADDITIONAL_PAGES 1
 extern int arch_setup_additional_pages(struct linux_binprm *bprm,
-				       int executable_stack);
+				       int uses_interp);
 
 extern int syscall32_setup_pages(struct linux_binprm *, int exstack);
 #define compat_arch_setup_additional_pages	syscall32_setup_pages

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
index 26bc15f01e78..40ca1bea7916 100644
--- a/arch/x86/include/asm/elf.h
+++ b/arch/x86/include/asm/elf.h
@@ -1,5 +1,5 @@
-#ifndef ASM_X86__ELF_H
-#define ASM_X86__ELF_H
+#ifndef _ASM_X86_ELF_H
+#define _ASM_X86_ELF_H
 
 /*
  * ELF register definitions..
@@ -333,4 +333,4 @@ extern int syscall32_setup_pages(struct linux_binprm *, int exstack);
 extern unsigned long arch_randomize_brk(struct mm_struct *mm);
 #define arch_randomize_brk arch_randomize_brk
 
-#endif /* ASM_X86__ELF_H */
+#endif /* _ASM_X86_ELF_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
new file mode 100644
index 000000000000..26bc15f01e78
--- /dev/null
+++ b/arch/x86/include/asm/elf.h
@@ -0,0 +1,336 @@
+#ifndef ASM_X86__ELF_H
+#define ASM_X86__ELF_H
+
+/*
+ * ELF register definitions..
+ */
+
+#include <asm/ptrace.h>
+#include <asm/user.h>
+#include <asm/auxvec.h>
+
+typedef unsigned long elf_greg_t;
+
+#define ELF_NGREG (sizeof(struct user_regs_struct) / sizeof(elf_greg_t))
+typedef elf_greg_t elf_gregset_t[ELF_NGREG];
+
+typedef struct user_i387_struct elf_fpregset_t;
+
+#ifdef __i386__
+
+typedef struct user_fxsr_struct elf_fpxregset_t;
+
+#define R_386_NONE	0
+#define R_386_32	1
+#define R_386_PC32	2
+#define R_386_GOT32	3
+#define R_386_PLT32	4
+#define R_386_COPY	5
+#define R_386_GLOB_DAT	6
+#define R_386_JMP_SLOT	7
+#define R_386_RELATIVE	8
+#define R_386_GOTOFF	9
+#define R_386_GOTPC	10
+#define R_386_NUM	11
+
+/*
+ * These are used to set parameters in the core dumps.
+ */
+#define ELF_CLASS	ELFCLASS32
+#define ELF_DATA	ELFDATA2LSB
+#define ELF_ARCH	EM_386
+
+#else
+
+/* x86-64 relocation types */
+#define R_X86_64_NONE		0	/* No reloc */
+#define R_X86_64_64		1	/* Direct 64 bit  */
+#define R_X86_64_PC32		2	/* PC relative 32 bit signed */
+#define R_X86_64_GOT32		3	/* 32 bit GOT entry */
+#define R_X86_64_PLT32		4	/* 32 bit PLT address */
+#define R_X86_64_COPY		5	/* Copy symbol at runtime */
+#define R_X86_64_GLOB_DAT	6	/* Create GOT entry */
+#define R_X86_64_JUMP_SLOT	7	/* Create PLT entry */
+#define R_X86_64_RELATIVE	8	/* Adjust by program base */
+#define R_X86_64_GOTPCREL	9	/* 32 bit signed pc relative
+					   offset to GOT */
+#define R_X86_64_32		10	/* Direct 32 bit zero extended */
+#define R_X86_64_32S		11	/* Direct 32 bit sign extended */
+#define R_X86_64_16		12	/* Direct 16 bit zero extended */
+#define R_X86_64_PC16		13	/* 16 bit sign extended pc relative */
+#define R_X86_64_8		14	/* Direct 8 bit sign extended  */
+#define R_X86_64_PC8		15	/* 8 bit sign extended pc relative */
+
+#define R_X86_64_NUM		16
+
+/*
+ * These are used to set parameters in the core dumps.
+ */
+#define ELF_CLASS	ELFCLASS64
+#define ELF_DATA	ELFDATA2LSB
+#define ELF_ARCH	EM_X86_64
+
+#endif
+
+#include <asm/vdso.h>
+
+extern unsigned int vdso_enabled;
+
+/*
+ * This is used to ensure we don't load something for the wrong architecture.
+ */
+#define elf_check_arch_ia32(x) \
+	(((x)->e_machine == EM_386) || ((x)->e_machine == EM_486))
+
+#include <asm/processor.h>
+#include <asm/system.h>
+
+#ifdef CONFIG_X86_32
+#include <asm/desc.h>
+
+#define elf_check_arch(x)	elf_check_arch_ia32(x)
+
+/* SVR4/i386 ABI (pages 3-31, 3-32) says that when the program starts %edx
+   contains a pointer to a function which might be registered using `atexit'.
+   This provides a mean for the dynamic linker to call DT_FINI functions for
+   shared libraries that have been loaded before the code runs.
+
+   A value of 0 tells we have no such handler.
+
+   We might as well make sure everything else is cleared too (except for %esp),
+   just to make things more deterministic.
+ */
+#define ELF_PLAT_INIT(_r, load_addr)		\
+	do {					\
+	_r->bx = 0; _r->cx = 0; _r->dx = 0;	\
+	_r->si = 0; _r->di = 0; _r->bp = 0;	\
+	_r->ax = 0;				\
+} while (0)
+
+/*
+ * regs is struct pt_regs, pr_reg is elf_gregset_t (which is
+ * now struct_user_regs, they are different)
+ */
+
+#define ELF_CORE_COPY_REGS(pr_reg, regs)	\
+do {						\
+	pr_reg[0] = regs->bx;			\
+	pr_reg[1] = regs->cx;			\
+	pr_reg[2] = regs->dx;			\
+	pr_reg[3] = regs->si;			\
+	pr_reg[4] = regs->di;			\
+	pr_reg[5] = regs->bp;			\
+	pr_reg[6] = regs->ax;			\
+	pr_reg[7] = regs->ds & 0xffff;		\
+	pr_reg[8] = regs->es & 0xffff;		\
+	pr_reg[9] = regs->fs & 0xffff;		\
+	savesegment(gs, pr_reg[10]);		\
+	pr_reg[11] = regs->orig_ax;		\
+	pr_reg[12] = regs->ip;			\
+	pr_reg[13] = regs->cs & 0xffff;		\
+	pr_reg[14] = regs->flags;		\
+	pr_reg[15] = regs->sp;			\
+	pr_reg[16] = regs->ss & 0xffff;		\
+} while (0);
+
+#define ELF_PLATFORM	(utsname()->machine)
+#define set_personality_64bit()	do { } while (0)
+
+#else /* CONFIG_X86_32 */
+
+/*
+ * This is used to ensure we don't load something for the wrong architecture.
+ */
+#define elf_check_arch(x)			\
+	((x)->e_machine == EM_X86_64)
+
+#define compat_elf_check_arch(x)	elf_check_arch_ia32(x)
+
+static inline void start_ia32_thread(struct pt_regs *regs, u32 ip, u32 sp)
+{
+	loadsegment(fs, 0);
+	loadsegment(ds, __USER32_DS);
+	loadsegment(es, __USER32_DS);
+	load_gs_index(0);
+	regs->ip = ip;
+	regs->sp = sp;
+	regs->flags = X86_EFLAGS_IF;
+	regs->cs = __USER32_CS;
+	regs->ss = __USER32_DS;
+}
+
+static inline void elf_common_init(struct thread_struct *t,
+				   struct pt_regs *regs, const u16 ds)
+{
+	regs->ax = regs->bx = regs->cx = regs->dx = 0;
+	regs->si = regs->di = regs->bp = 0;
+	regs->r8 = regs->r9 = regs->r10 = regs->r11 = 0;
+	regs->r12 = regs->r13 = regs->r14 = regs->r15 = 0;
+	t->fs = t->gs = 0;
+	t->fsindex = t->gsindex = 0;
+	t->ds = t->es = ds;
+}
+
+#define ELF_PLAT_INIT(_r, load_addr)			\
+do {							\
+	elf_common_init(&current->thread, _r, 0);	\
+	clear_thread_flag(TIF_IA32);			\
+} while (0)
+
+#define	COMPAT_ELF_PLAT_INIT(regs, load_addr)		\
+	elf_common_init(&current->thread, regs, __USER_DS)
+
+#define	compat_start_thread(regs, ip, sp)		\
+do {							\
+	start_ia32_thread(regs, ip, sp);		\
+	set_fs(USER_DS);				\
+} while (0)
+
+#define COMPAT_SET_PERSONALITY(ex)			\
+do {							\
+	if (test_thread_flag(TIF_IA32))			\
+		clear_thread_flag(TIF_ABI_PENDING);	\
+	else						\
+		set_thread_flag(TIF_ABI_PENDING);	\
+	current->personality |= force_personality32;	\
+} while (0)
+
+#define COMPAT_ELF_PLATFORM			("i686")
+
+/*
+ * regs is struct pt_regs, pr_reg is elf_gregset_t (which is
+ * now struct_user_regs, they are different). Assumes current is the process
+ * getting dumped.
+ */
+
+#define ELF_CORE_COPY_REGS(pr_reg, regs)			\
+do {								\
+	unsigned v;						\
+	(pr_reg)[0] = (regs)->r15;				\
+	(pr_reg)[1] = (regs)->r14;				\
+	(pr_reg)[2] = (regs)->r13;				\
+	(pr_reg)[3] = (regs)->r12;				\
+	(pr_reg)[4] = (regs)->bp;				\
+	(pr_reg)[5] = (regs)->bx;				\
+	(pr_reg)[6] = (regs)->r11;				\
+	(pr_reg)[7] = (regs)->r10;				\
+	(pr_reg)[8] = (regs)->r9;				\
+	(pr_reg)[9] = (regs)->r8;				\
+	(pr_reg)[10] = (regs)->ax;				\
+	(pr_reg)[11] = (regs)->cx;				\
+	(pr_reg)[12] = (regs)->dx;				\
+	(pr_reg)[13] = (regs)->si;				\
+	(pr_reg)[14] = (regs)->di;				\
+	(pr_reg)[15] = (regs)->orig_ax;				\
+	(pr_reg)[16] = (regs)->ip;				\
+	(pr_reg)[17] = (regs)->cs;				\
+	(pr_reg)[18] = (regs)->flags;				\
+	(pr_reg)[19] = (regs)->sp;				\
+	(pr_reg)[20] = (regs)->ss;				\
+	(pr_reg)[21] = current->thread.fs;			\
+	(pr_reg)[22] = current->thread.gs;			\
+	asm("movl %%ds,%0" : "=r" (v)); (pr_reg)[23] = v;	\
+	asm("movl %%es,%0" : "=r" (v)); (pr_reg)[24] = v;	\
+	asm("movl %%fs,%0" : "=r" (v)); (pr_reg)[25] = v;	\
+	asm("movl %%gs,%0" : "=r" (v)); (pr_reg)[26] = v;	\
+} while (0);
+
+/* I'm not sure if we can use '-' here */
+#define ELF_PLATFORM       ("x86_64")
+extern void set_personality_64bit(void);
+extern unsigned int sysctl_vsyscall32;
+extern int force_personality32;
+
+#endif /* !CONFIG_X86_32 */
+
+#define CORE_DUMP_USE_REGSET
+#define USE_ELF_CORE_DUMP
+#define ELF_EXEC_PAGESIZE	4096
+
+/* This is the location that an ET_DYN program is loaded if exec'ed.  Typical
+   use of this is to invoke "./ld.so someprog" to test out a new version of
+   the loader.  We need to make sure that it is out of the way of the program
+   that it will "exec", and that there is sufficient room for the brk.  */
+
+#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)
+
+/* This yields a mask that user programs can use to figure out what
+   instruction set this CPU supports.  This could be done in user space,
+   but it's not easy, and we've already done it here.  */
+
+#define ELF_HWCAP		(boot_cpu_data.x86_capability[0])
+
+/* This yields a string that ld.so will use to load implementation
+   specific libraries for optimization.  This is more specific in
+   intent than poking at uname or /proc/cpuinfo.
+
+   For the moment, we have only optimizations for the Intel generations,
+   but that could change... */
+
+#define SET_PERSONALITY(ex) set_personality_64bit()
+
+/*
+ * An executable for which elf_read_implies_exec() returns TRUE will
+ * have the READ_IMPLIES_EXEC personality flag set automatically.
+ */
+#define elf_read_implies_exec(ex, executable_stack)	\
+	(executable_stack != EXSTACK_DISABLE_X)
+
+struct task_struct;
+
+#define	ARCH_DLINFO_IA32(vdso_enabled)					\
+do {									\
+	if (vdso_enabled) {						\
+		NEW_AUX_ENT(AT_SYSINFO,	VDSO_ENTRY);			\
+		NEW_AUX_ENT(AT_SYSINFO_EHDR, VDSO_CURRENT_BASE);	\
+	}								\
+} while (0)
+
+#ifdef CONFIG_X86_32
+
+#define VDSO_HIGH_BASE		(__fix_to_virt(FIX_VDSO))
+
+#define ARCH_DLINFO		ARCH_DLINFO_IA32(vdso_enabled)
+
+/* update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT entries changes */
+
+#else /* CONFIG_X86_32 */
+
+#define VDSO_HIGH_BASE		0xffffe000U /* CONFIG_COMPAT_VDSO address */
+
+/* 1GB for 64bit, 8MB for 32bit */
+#define STACK_RND_MASK (test_thread_flag(TIF_IA32) ? 0x7ff : 0x3fffff)
+
+#define ARCH_DLINFO							\
+do {									\
+	if (vdso_enabled)						\
+		NEW_AUX_ENT(AT_SYSINFO_EHDR,				\
+			    (unsigned long)current->mm->context.vdso);	\
+} while (0)
+
+#define AT_SYSINFO		32
+
+#define COMPAT_ARCH_DLINFO	ARCH_DLINFO_IA32(sysctl_vsyscall32)
+
+#define COMPAT_ELF_ET_DYN_BASE	(TASK_UNMAPPED_BASE + 0x1000000)
+
+#endif /* !CONFIG_X86_32 */
+
+#define VDSO_CURRENT_BASE	((unsigned long)current->mm->context.vdso)
+
+#define VDSO_ENTRY							\
+	((unsigned long)VDSO32_SYMBOL(VDSO_CURRENT_BASE, vsyscall))
+
+struct linux_binprm;
+
+#define ARCH_HAS_SETUP_ADDITIONAL_PAGES 1
+extern int arch_setup_additional_pages(struct linux_binprm *bprm,
+				       int executable_stack);
+
+extern int syscall32_setup_pages(struct linux_binprm *, int exstack);
+#define compat_arch_setup_additional_pages	syscall32_setup_pages
+
+extern unsigned long arch_randomize_brk(struct mm_struct *mm);
+#define arch_randomize_brk arch_randomize_brk
+
+#endif /* ASM_X86__ELF_H */
