commit 2823e83a3dc0f54d23db67ca07d74b9c8bb1fdda
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 3 13:40:22 2020 +0200

    x86/entry: __always_inline CR2 for noinstr
    
    vmlinux.o: warning: objtool: exc_page_fault()+0x9: call to read_cr2() leaves .noinstr.text section
    vmlinux.o: warning: objtool: exc_page_fault()+0x24: call to prefetchw() leaves .noinstr.text section
    vmlinux.o: warning: objtool: exc_page_fault()+0x21: call to kvm_handle_async_pf.isra.0() leaves .noinstr.text section
    vmlinux.o: warning: objtool: exc_nmi()+0x1cc: call to write_cr2() leaves .noinstr.text section
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200603114052.243227806@infradead.org

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 57fd1966c4ea..49d3a9edb06f 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -141,7 +141,7 @@ static inline void kvm_disable_steal_time(void)
 	return;
 }
 
-static inline bool kvm_handle_async_pf(struct pt_regs *regs, u32 token)
+static __always_inline bool kvm_handle_async_pf(struct pt_regs *regs, u32 token)
 {
 	return false;
 }

commit 68fd66f100d196d35ab3008d4c69af3a0d7e7200
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon May 25 16:41:17 2020 +0200

    KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
    
    Currently, APF mechanism relies on the #PF abuse where the token is being
    passed through CR2. If we switch to using interrupts to deliver page-ready
    notifications we need a different way to pass the data. Extent the existing
    'struct kvm_vcpu_pv_apf_data' with token information for page-ready
    notifications.
    
    While on it, rename 'reason' to 'flags'. This doesn't change the semantics
    as we only have reasons '1' and '2' and these can be treated as bit flags
    but KVM_PV_REASON_PAGE_READY is going away with interrupt based delivery
    making 'reason' name misleading.
    
    The newly introduced apf_put_user_ready() temporary puts both flags and
    token information, this will be changed to put token only when we switch
    to interrupt based notifications.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200525144125.143875-3-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 118e5c2379f9..57fd1966c4ea 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -90,7 +90,7 @@ unsigned int kvm_arch_para_features(void);
 unsigned int kvm_arch_para_hints(void);
 void kvm_async_pf_task_wait_schedule(u32 token);
 void kvm_async_pf_task_wake(u32 token);
-u32 kvm_read_and_reset_pf_reason(void);
+u32 kvm_read_and_reset_apf_flags(void);
 void kvm_disable_steal_time(void);
 bool __kvm_handle_async_pf(struct pt_regs *regs, u32 token);
 
@@ -131,7 +131,7 @@ static inline unsigned int kvm_arch_para_hints(void)
 	return 0;
 }
 
-static inline u32 kvm_read_and_reset_pf_reason(void)
+static inline u32 kvm_read_and_reset_apf_flags(void)
 {
 	return 0;
 }

commit 6bca69ada4bc20fa27eb44a5e09da3363d1752af
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 7 00:42:06 2020 +0100

    x86/kvm: Sanitize kvm_async_pf_task_wait()
    
    While working on the entry consolidation I stumbled over the KVM async page
    fault handler and kvm_async_pf_task_wait() in particular. It took me a
    while to realize that the randomly sprinkled around rcu_irq_enter()/exit()
    invocations are just cargo cult programming. Several patches "fixed" RCU
    splats by curing the symptoms without noticing that the code is flawed
    from a design perspective.
    
    The main problem is that this async injection is not based on a proper
    handshake mechanism and only respects the minimal requirement, i.e. the
    guest is not in a state where it has interrupts disabled.
    
    Aside of that the actual code is a convoluted one fits it all swiss army
    knife. It is invoked from different places with different RCU constraints:
    
      1) Host side:
    
         vcpu_enter_guest()
           kvm_x86_ops->handle_exit()
             kvm_handle_page_fault()
               kvm_async_pf_task_wait()
    
         The invocation happens from fully preemptible context.
    
      2) Guest side:
    
         The async page fault interrupted:
    
             a) user space
    
             b) preemptible kernel code which is not in a RCU read side
                critical section
    
             c) non-preemtible kernel code or a RCU read side critical section
                or kernel code with CONFIG_PREEMPTION=n which allows not to
                differentiate between #2b and #2c.
    
    RCU is watching for:
    
      #1  The vCPU exited and current is definitely not the idle task
    
      #2a The #PF entry code on the guest went through enter_from_user_mode()
          which reactivates RCU
    
      #2b There is no preemptible, interrupts enabled code in the kernel
          which can run with RCU looking away. (The idle task is always
          non preemptible).
    
    I.e. all schedulable states (#1, #2a, #2b) do not need any of this RCU
    voodoo at all.
    
    In #2c RCU is eventually not watching, but as that state cannot schedule
    anyway there is no point to worry about it so it has to invoke
    rcu_irq_enter() before running that code. This can be optimized, but this
    will be done as an extra step in course of the entry code consolidation
    work.
    
    So the proper solution for this is to:
    
      - Split kvm_async_pf_task_wait() into schedule and halt based waiting
        interfaces which share the enqueueing code.
    
      - Add comments (condensed form of this changelog) to spare others the
        time waste and pain of reverse engineering all of this with the help of
        uncomprehensible changelogs and code history.
    
      - Invoke kvm_async_pf_task_wait_schedule() from kvm_handle_page_fault(),
        user mode and schedulable kernel side async page faults (#1, #2a, #2b)
    
      - Invoke kvm_async_pf_task_wait_halt() for the non schedulable kernel
        case (#2c).
    
        For this case also remove the rcu_irq_exit()/enter() pair around the
        halt as it is just a pointless exercise:
    
           - vCPUs can VMEXIT at any random point and can be scheduled out for
             an arbitrary amount of time by the host and this is not any
             different except that it voluntary triggers the exit via halt.
    
           - The interrupted context could have RCU watching already. So the
             rcu_irq_exit() before the halt is not gaining anything aside of
             confusing the reader. Claiming that this might prevent RCU stalls
             is just an illusion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134059.262701431@linutronix.de

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 5261363adda3..118e5c2379f9 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -88,7 +88,7 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 bool kvm_para_available(void);
 unsigned int kvm_arch_para_features(void);
 unsigned int kvm_arch_para_hints(void);
-void kvm_async_pf_task_wait(u32 token, int interrupt_kernel);
+void kvm_async_pf_task_wait_schedule(u32 token);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
 void kvm_disable_steal_time(void);
@@ -113,7 +113,7 @@ static inline void kvm_spinlock_init(void)
 #endif /* CONFIG_PARAVIRT_SPINLOCKS */
 
 #else /* CONFIG_KVM_GUEST */
-#define kvm_async_pf_task_wait(T, I) do {} while(0)
+#define kvm_async_pf_task_wait_schedule(T) do {} while(0)
 #define kvm_async_pf_task_wake(T) do {} while(0)
 
 static inline bool kvm_para_available(void)

commit ef68017eb5704eb2b0577c3aa6619e13caf2b59f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Feb 28 10:42:48 2020 -0800

    x86/kvm: Handle async page faults directly through do_page_fault()
    
    KVM overloads #PF to indicate two types of not-actually-page-fault
    events.  Right now, the KVM guest code intercepts them by modifying
    the IDT and hooking the #PF vector.  This makes the already fragile
    fault code even harder to understand, and it also pollutes call
    traces with async_page_fault and do_async_page_fault for normal page
    faults.
    
    Clean it up by moving the logic into do_page_fault() using a static
    branch.  This gets rid of the platform trap_init override mechanism
    completely.
    
    [ tglx: Fixed up 32bit, removed error code from the async functions and
            massaged coding style ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134059.169270470@linutronix.de

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 9b4df6eaa11a..5261363adda3 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -91,8 +91,18 @@ unsigned int kvm_arch_para_hints(void);
 void kvm_async_pf_task_wait(u32 token, int interrupt_kernel);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
-extern void kvm_disable_steal_time(void);
-void do_async_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address);
+void kvm_disable_steal_time(void);
+bool __kvm_handle_async_pf(struct pt_regs *regs, u32 token);
+
+DECLARE_STATIC_KEY_FALSE(kvm_async_pf_enabled);
+
+static __always_inline bool kvm_handle_async_pf(struct pt_regs *regs, u32 token)
+{
+	if (static_branch_unlikely(&kvm_async_pf_enabled))
+		return __kvm_handle_async_pf(regs, token);
+	else
+		return false;
+}
 
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
 void __init kvm_spinlock_init(void);
@@ -130,6 +140,11 @@ static inline void kvm_disable_steal_time(void)
 {
 	return;
 }
+
+static inline bool kvm_handle_async_pf(struct pt_regs *regs, u32 token)
+{
+	return false;
+}
 #endif
 
 #endif /* _ASM_X86_KVM_PARA_H */

commit a0d14b8909de55139b8702fe0c7e80b69763dcfb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 11 13:40:59 2019 +0200

    x86/mm, tracing: Fix CR2 corruption
    
    Despite the current efforts to read CR2 before tracing happens there still
    exist a number of possible holes:
    
      idtentry page_fault             do_page_fault           has_error_code=1
        call error_entry
          TRACE_IRQS_OFF
            call trace_hardirqs_off*
              #PF // modifies CR2
    
          CALL_enter_from_user_mode
            __context_tracking_exit()
              trace_user_exit(0)
                #PF // modifies CR2
    
        call do_page_fault
          address = read_cr2(); /* whoopsie */
    
    And similar for i386.
    
    Fix it by pulling the CR2 read into the entry code, before any of that
    stuff gets a chance to run and ruin things.
    
    Reported-by: He Zhe <zhe.he@windriver.com>
    Reported-by: Eiichi Tsukata <devel@etsukata.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: bp@alien8.de
    Cc: rostedt@goodmis.org
    Cc: torvalds@linux-foundation.org
    Cc: hpa@zytor.com
    Cc: dave.hansen@linux.intel.com
    Cc: jgross@suse.com
    Cc: joel@joelfernandes.org
    Link: https://lkml.kernel.org/r/20190711114336.116812491@infradead.org
    
    Debugged-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 5ed3cf1c3934..9b4df6eaa11a 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -92,7 +92,7 @@ void kvm_async_pf_task_wait(u32 token, int interrupt_kernel);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
 extern void kvm_disable_steal_time(void);
-void do_async_page_fault(struct pt_regs *regs, unsigned long error_code);
+void do_async_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address);
 
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
 void __init kvm_spinlock_init(void);

commit 89f579ce99f7e028e81885d3965f973c0f787611
Author: Yi Wang <wang.yi59@zte.com.cn>
Date:   Thu Nov 22 10:04:09 2018 +0800

    x86/headers: Fix -Wmissing-prototypes warning
    
    When building the kernel with W=1 we get a lot of -Wmissing-prototypes
    warnings, which are trivial in nature and easy to fix - and which may
    mask some real future bugs if the prototypes get out of sync with
    the function definition.
    
    This patch fixes most of -Wmissing-prototypes warnings which
    are in the root directory of arch/x86/kernel, not including
    the subdirectories.
    
    These are the warnings fixed in this patch:
    
      arch/x86/kernel/signal.c:865:17: warning: no previous prototype for ‘sys32_x32_rt_sigreturn’ [-Wmissing-prototypes]
      arch/x86/kernel/signal_compat.c:164:6: warning: no previous prototype for ‘sigaction_compat_abi’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:625:46: warning: no previous prototype for ‘sync_regs’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:640:24: warning: no previous prototype for ‘fixup_bad_iret’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:929:13: warning: no previous prototype for ‘trap_init’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:270:28: warning: no previous prototype for ‘smp_x86_platform_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:301:16: warning: no previous prototype for ‘smp_kvm_posted_intr_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:314:16: warning: no previous prototype for ‘smp_kvm_posted_intr_wakeup_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:328:16: warning: no previous prototype for ‘smp_kvm_posted_intr_nested_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq_work.c:16:28: warning: no previous prototype for ‘smp_irq_work_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/irqinit.c:79:13: warning: no previous prototype for ‘init_IRQ’ [-Wmissing-prototypes]
      arch/x86/kernel/quirks.c:672:13: warning: no previous prototype for ‘early_platform_quirks’ [-Wmissing-prototypes]
      arch/x86/kernel/tsc.c:1499:15: warning: no previous prototype for ‘calibrate_delay_is_known’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:653:13: warning: no previous prototype for ‘arch_post_acpi_subsys_init’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:717:15: warning: no previous prototype for ‘arch_randomize_brk’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:784:6: warning: no previous prototype for ‘do_arch_prctl_common’ [-Wmissing-prototypes]
      arch/x86/kernel/reboot.c:869:6: warning: no previous prototype for ‘nmi_panic_self_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:176:27: warning: no previous prototype for ‘smp_reboot_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:260:28: warning: no previous prototype for ‘smp_reschedule_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:281:28: warning: no previous prototype for ‘smp_call_function_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:291:28: warning: no previous prototype for ‘smp_call_function_single_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:840:6: warning: no previous prototype for ‘arch_ftrace_update_trampoline’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:934:7: warning: no previous prototype for ‘arch_ftrace_trampoline_func’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:946:6: warning: no previous prototype for ‘arch_ftrace_trampoline_free’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:114:6: warning: no previous prototype for ‘crash_smp_send_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:351:5: warning: no previous prototype for ‘crash_setup_memmap_entries’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:424:5: warning: no previous prototype for ‘crash_load_segments’ [-Wmissing-prototypes]
      arch/x86/kernel/machine_kexec_64.c:372:7: warning: no previous prototype for ‘arch_kexec_kernel_image_load’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:12:16: warning: no previous prototype for ‘__native_queued_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:18:6: warning: no previous prototype for ‘pv_is_native_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:24:16: warning: no previous prototype for ‘__native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:30:6: warning: no previous prototype for ‘pv_is_native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/kvm.c:258:1: warning: no previous prototype for ‘do_async_page_fault’ [-Wmissing-prototypes]
      arch/x86/kernel/jailhouse.c:200:6: warning: no previous prototype for ‘jailhouse_paravirt’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:91:13: warning: no previous prototype for ‘setup_bios_corruption_check’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:139:6: warning: no previous prototype for ‘check_for_bios_corruption’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:32:13: warning: no previous prototype for ‘early_init_dt_scan_chosen_arch’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:42:13: warning: no previous prototype for ‘add_dtb’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:108:6: warning: no previous prototype for ‘x86_of_pci_init’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:314:13: warning: no previous prototype for ‘x86_dtb_init’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:16:5: warning: no previous prototype for ‘trace_pagefault_reg’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:22:6: warning: no previous prototype for ‘trace_pagefault_unreg’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:113:22: warning: no previous prototype for ‘__startup_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:262:15: warning: no previous prototype for ‘__startup_secondary_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:350:12: warning: no previous prototype for ‘early_make_pgtable’ [-Wmissing-prototypes]
    
    [ mingo: rewrote the changelog, fixed build errors. ]
    
    Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: akpm@linux-foundation.org
    Cc: andy.shevchenko@gmail.com
    Cc: anton@enomsg.org
    Cc: ard.biesheuvel@linaro.org
    Cc: bhe@redhat.com
    Cc: bhelgaas@google.com
    Cc: bp@alien8.de
    Cc: ccross@android.com
    Cc: devicetree@vger.kernel.org
    Cc: douly.fnst@cn.fujitsu.com
    Cc: dwmw@amazon.co.uk
    Cc: dyoung@redhat.com
    Cc: ebiederm@xmission.com
    Cc: frank.rowand@sony.com
    Cc: frowand.list@gmail.com
    Cc: ivan.gorinov@intel.com
    Cc: jailhouse-dev@googlegroups.com
    Cc: jan.kiszka@siemens.com
    Cc: jgross@suse.com
    Cc: jroedel@suse.de
    Cc: keescook@chromium.org
    Cc: kexec@lists.infradead.org
    Cc: konrad.wilk@oracle.com
    Cc: kvm@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-pci@vger.kernel.org
    Cc: luto@kernel.org
    Cc: m.mizuma@jp.fujitsu.com
    Cc: namit@vmware.com
    Cc: oleg@redhat.com
    Cc: pasha.tatashin@oracle.com
    Cc: pbonzini@redhat.com
    Cc: prarit@redhat.com
    Cc: pravin.shedge4linux@gmail.com
    Cc: rajvi.jingar@intel.com
    Cc: rkrcmar@redhat.com
    Cc: robh+dt@kernel.org
    Cc: robh@kernel.org
    Cc: rostedt@goodmis.org
    Cc: takahiro.akashi@linaro.org
    Cc: thomas.lendacky@amd.com
    Cc: tony.luck@intel.com
    Cc: up2wing@gmail.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: zhe.he@windriver.com
    Cc: zhong.weidong@zte.com.cn
    Link: http://lkml.kernel.org/r/1542852249-19820-1-git-send-email-wang.yi59@zte.com.cn
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 4c723632c036..5ed3cf1c3934 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -92,6 +92,7 @@ void kvm_async_pf_task_wait(u32 token, int interrupt_kernel);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
 extern void kvm_disable_steal_time(void);
+void do_async_page_fault(struct pt_regs *regs, unsigned long error_code);
 
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
 void __init kvm_spinlock_init(void);

commit 7a5ddc8fe0ea9518cd7fb6a929cac7d864c6f300
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:22 2018 -0400

    x86/kvmclock: Decrapify kvm_register_clock()
    
    The return value is pointless because the wrmsr cannot fail if
    KVM_FEATURE_CLOCKSOURCE or KVM_FEATURE_CLOCKSOURCE2 are set.
    
    kvm_register_clock() is only called locally so wants to be static.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-4-pasha.tatashin@oracle.com

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 3aea2658323a..4c723632c036 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -7,7 +7,6 @@
 #include <uapi/asm/kvm_para.h>
 
 extern void kvmclock_init(void);
-extern int kvm_register_clock(char *txt);
 
 #ifdef CONFIG_KVM_GUEST
 bool kvm_check_and_clear_guest_paused(void);

commit a4429e53c9b3082b05e51224c3d58dbdd39306c5
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue Feb 13 09:05:40 2018 +0800

    KVM: Introduce paravirtualization hints and KVM_HINTS_DEDICATED
    
    This patch introduces kvm_para_has_hint() to query for hints about
    the configuration of the guests.  The first hint KVM_HINTS_DEDICATED,
    is set if the guest has dedicated physical CPUs for each vCPU (i.e.
    pinning and no over-commitment).  This allows optimizing spinlocks
    and tells the guest to avoid PV TLB flush.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 7b407dda2bd7..3aea2658323a 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -88,6 +88,7 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 #ifdef CONFIG_KVM_GUEST
 bool kvm_para_available(void);
 unsigned int kvm_arch_para_features(void);
+unsigned int kvm_arch_para_hints(void);
 void kvm_async_pf_task_wait(u32 token, int interrupt_kernel);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
@@ -115,6 +116,11 @@ static inline unsigned int kvm_arch_para_features(void)
 	return 0;
 }
 
+static inline unsigned int kvm_arch_para_hints(void)
+{
+	return 0;
+}
+
 static inline u32 kvm_read_and_reset_pf_reason(void)
 {
 	return 0;

commit f3614646005a1b59f836ff54351c9bd2224b6005
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Nov 9 14:27:38 2017 +0100

    x86/virt, x86/platform: Add ->guest_late_init() callback to hypervisor_x86 structure
    
    Add a new guest_late_init callback to the hypervisor_x86 structure. It
    will replace the current kvm_guest_init() call which is changed to
    make use of the new callback.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kvm@vger.kernel.org
    Cc: rkrcmar@redhat.com
    Link: http://lkml.kernel.org/r/20171109132739.23465-5-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index c373e44049b1..7b407dda2bd7 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -88,7 +88,6 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 #ifdef CONFIG_KVM_GUEST
 bool kvm_para_available(void);
 unsigned int kvm_arch_para_features(void);
-void __init kvm_guest_init(void);
 void kvm_async_pf_task_wait(u32 token, int interrupt_kernel);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
@@ -103,7 +102,6 @@ static inline void kvm_spinlock_init(void)
 #endif /* CONFIG_PARAVIRT_SPINLOCKS */
 
 #else /* CONFIG_KVM_GUEST */
-#define kvm_guest_init() do {} while (0)
 #define kvm_async_pf_task_wait(T, I) do {} while(0)
 #define kvm_async_pf_task_wake(T) do {} while(0)
 

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 59ad3d132353..c373e44049b1 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_KVM_PARA_H
 #define _ASM_X86_KVM_PARA_H
 

commit a2b7861bb33b2538420bb5d8554153484d3f961f
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Tue Oct 3 21:36:51 2017 +0800

    kvm/x86: Avoid async PF preempting the kernel incorrectly
    
    Currently, in PREEMPT_COUNT=n kernel, kvm_async_pf_task_wait() could call
    schedule() to reschedule in some cases.  This could result in
    accidentally ending the current RCU read-side critical section early,
    causing random memory corruption in the guest, or otherwise preempting
    the currently running task inside between preempt_disable and
    preempt_enable.
    
    The difficulty to handle this well is because we don't know whether an
    async PF delivered in a preemptible section or RCU read-side critical section
    for PREEMPT_COUNT=n, since preempt_disable()/enable() and rcu_read_lock/unlock()
    are both no-ops in that case.
    
    To cure this, we treat any async PF interrupting a kernel context as one
    that cannot be preempted, preventing kvm_async_pf_task_wait() from choosing
    the schedule() path in that case.
    
    To do so, a second parameter for kvm_async_pf_task_wait() is introduced,
    so that we know whether it's called from a context interrupting the
    kernel, and the parameter is set properly in all the callsites.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index bc62e7cbf1b1..59ad3d132353 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -88,7 +88,7 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 bool kvm_para_available(void);
 unsigned int kvm_arch_para_features(void);
 void __init kvm_guest_init(void);
-void kvm_async_pf_task_wait(u32 token);
+void kvm_async_pf_task_wait(u32 token, int interrupt_kernel);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
 extern void kvm_disable_steal_time(void);
@@ -103,7 +103,7 @@ static inline void kvm_spinlock_init(void)
 
 #else /* CONFIG_KVM_GUEST */
 #define kvm_guest_init() do {} while (0)
-#define kvm_async_pf_task_wait(T) do {} while(0)
+#define kvm_async_pf_task_wait(T, I) do {} while(0)
 #define kvm_async_pf_task_wake(T) do {} while(0)
 
 static inline bool kvm_para_available(void)

commit 9ccaf77cf05915f51231d158abfd5448aedde758
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Feb 17 14:41:14 2016 -0800

    x86/mm: Always enable CONFIG_DEBUG_RODATA and remove the Kconfig option
    
    This removes the CONFIG_DEBUG_RODATA option and makes it always enabled.
    
    This simplifies the code and also makes it clearer that read-only mapped
    memory is just as fundamental a security feature in kernel-space as it is
    in user-space.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Brown <david.brown@linaro.org>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Emese Revfy <re.emese@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-hardening@lists.openwall.com
    Cc: linux-arch <linux-arch@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1455748879-21872-4-git-send-email-keescook@chromium.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index c1adf33fdd0d..bc62e7cbf1b1 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -17,15 +17,8 @@ static inline bool kvm_check_and_clear_guest_paused(void)
 }
 #endif /* CONFIG_KVM_GUEST */
 
-#ifdef CONFIG_DEBUG_RODATA
 #define KVM_HYPERCALL \
         ALTERNATIVE(".byte 0x0f,0x01,0xc1", ".byte 0x0f,0x01,0xd9", X86_FEATURE_VMMCALL)
-#else
-/* On AMD processors, vmcall will generate a trap that we will
- * then rewrite to the appropriate instruction.
- */
-#define KVM_HYPERCALL ".byte 0x0f,0x01,0xc1"
-#endif
 
 /* For KVM hypercalls, a three-byte sequence of either the vmcall or the vmmcall
  * instruction.  The hypervisor may replace it with something else but only the

commit 1d804d079a92138d011900785193b6b00b44bc00
Author: Joe Perches <joe@perches.com>
Date:   Mon Mar 30 16:46:09 2015 -0700

    x86: Use bool function return values of true/false not 1/0
    
    Use the normal return values for bool functions
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Message-Id: <9f593eb2f43b456851cd73f7ed09654ca58fb570.1427759009.git.joe@perches.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index e62cf897f781..c1adf33fdd0d 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -115,7 +115,7 @@ static inline void kvm_spinlock_init(void)
 
 static inline bool kvm_para_available(void)
 {
-	return 0;
+	return false;
 }
 
 static inline unsigned int kvm_arch_para_features(void)

commit c1118b3602c2329671ad5ec8bdf8e374323d6343
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Sep 22 13:17:48 2014 +0200

    x86: kvm: use alternatives for VMCALL vs. VMMCALL if kernel text is read-only
    
    On x86_64, kernel text mappings are mapped read-only with CONFIG_DEBUG_RODATA.
    In that case, KVM will fail to patch VMCALL instructions to VMMCALL
    as required on AMD processors.
    
    The failure mode is currently a divide-by-zero exception, which obviously
    is a KVM bug that has to be fixed.  However, picking the right instruction
    between VMCALL and VMMCALL will be faster and will help if you cannot upgrade
    the hypervisor.
    
    Reported-by: Chris Webb <chris@arachsys.com>
    Tested-by: Chris Webb <chris@arachsys.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index c7678e43465b..e62cf897f781 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -2,6 +2,7 @@
 #define _ASM_X86_KVM_PARA_H
 
 #include <asm/processor.h>
+#include <asm/alternative.h>
 #include <uapi/asm/kvm_para.h>
 
 extern void kvmclock_init(void);
@@ -16,10 +17,15 @@ static inline bool kvm_check_and_clear_guest_paused(void)
 }
 #endif /* CONFIG_KVM_GUEST */
 
-/* This instruction is vmcall.  On non-VT architectures, it will generate a
- * trap that we will then rewrite to the appropriate instruction.
+#ifdef CONFIG_DEBUG_RODATA
+#define KVM_HYPERCALL \
+        ALTERNATIVE(".byte 0x0f,0x01,0xc1", ".byte 0x0f,0x01,0xd9", X86_FEATURE_VMMCALL)
+#else
+/* On AMD processors, vmcall will generate a trap that we will
+ * then rewrite to the appropriate instruction.
  */
 #define KVM_HYPERCALL ".byte 0x0f,0x01,0xc1"
+#endif
 
 /* For KVM hypercalls, a three-byte sequence of either the vmcall or the vmmcall
  * instruction.  The hypervisor may replace it with something else but only the

commit 77f01bdfa5e55dc19d3eb747181d2730a9bb3ca8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jan 27 14:51:44 2014 +0100

    x86, kvm: correctly access the KVM_CPUID_FEATURES leaf at 0x40000101
    
    When Hyper-V hypervisor leaves are present, KVM must relocate
    its own leaves at 0x40000100, because Windows does not look for
    Hyper-V leaves at indices other than 0x40000000.  In this case,
    the KVM features are at 0x40000101, but the old code would always
    look at 0x40000001.
    
    Fix by using kvm_cpuid_base().  This also requires making the
    function non-inline, since kvm_cpuid_base() is static.
    
    Fixes: 1085ba7f552d84aa8ac0ae903fa8d0cc2ff9f79d
    Cc: stable@vger.kernel.org
    Cc: mtosatti@redhat.com
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 1679cc799b26..c7678e43465b 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -85,13 +85,9 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 	return ret;
 }
 
-static inline unsigned int kvm_arch_para_features(void)
-{
-	return cpuid_eax(KVM_CPUID_FEATURES);
-}
-
 #ifdef CONFIG_KVM_GUEST
 bool kvm_para_available(void);
+unsigned int kvm_arch_para_features(void);
 void __init kvm_guest_init(void);
 void kvm_async_pf_task_wait(u32 token);
 void kvm_async_pf_task_wake(u32 token);
@@ -116,6 +112,11 @@ static inline bool kvm_para_available(void)
 	return 0;
 }
 
+static inline unsigned int kvm_arch_para_features(void)
+{
+	return 0;
+}
+
 static inline u32 kvm_read_and_reset_pf_reason(void)
 {
 	return 0;

commit 1c300a40772dae829b91dad634999a6a522c0829
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jan 27 14:49:40 2014 +0100

    x86, kvm: cache the base of the KVM cpuid leaves
    
    It is unnecessary to go through hypervisor_cpuid_base every time
    a leaf is found (which will be every time a feature is requested
    after the next patch).
    
    Fixes: 1085ba7f552d84aa8ac0ae903fa8d0cc2ff9f79d
    Cc: stable@vger.kernel.org
    Cc: mtosatti@redhat.com
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 1df115909758..1679cc799b26 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -85,28 +85,13 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 	return ret;
 }
 
-static inline uint32_t kvm_cpuid_base(void)
-{
-	if (boot_cpu_data.cpuid_level < 0)
-		return 0;	/* So we don't blow up on old processors */
-
-	if (cpu_has_hypervisor)
-		return hypervisor_cpuid_base("KVMKVMKVM\0\0\0", 0);
-
-	return 0;
-}
-
-static inline bool kvm_para_available(void)
-{
-	return kvm_cpuid_base() != 0;
-}
-
 static inline unsigned int kvm_arch_para_features(void)
 {
 	return cpuid_eax(KVM_CPUID_FEATURES);
 }
 
 #ifdef CONFIG_KVM_GUEST
+bool kvm_para_available(void);
 void __init kvm_guest_init(void);
 void kvm_async_pf_task_wait(u32 token);
 void kvm_async_pf_task_wake(u32 token);
@@ -126,6 +111,11 @@ static inline void kvm_spinlock_init(void)
 #define kvm_async_pf_task_wait(T) do {} while(0)
 #define kvm_async_pf_task_wake(T) do {} while(0)
 
+static inline bool kvm_para_available(void)
+{
+	return 0;
+}
+
 static inline u32 kvm_read_and_reset_pf_reason(void)
 {
 	return 0;

commit 816434ec4a674fcdb3c2221a6dffdc8f34020550
Merge: f357a82048ff 36bd621337c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 11:55:10 2013 -0700

    Merge branch 'x86-spinlocks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 spinlock changes from Ingo Molnar:
     "The biggest change here are paravirtualized ticket spinlocks (PV
      spinlocks), which bring a nice speedup on various benchmarks.
    
      The KVM host side will come to you via the KVM tree"
    
    * 'x86-spinlocks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/kvm/guest: Fix sparse warning: "symbol 'klock_waiting' was not declared as static"
      kvm: Paravirtual ticketlocks support for linux guests running on KVM hypervisor
      kvm guest: Add configuration support to enable debug information for KVM Guests
      kvm uapi: Add KICK_CPU and PV_UNHALT definition to uapi
      xen, pvticketlock: Allow interrupts to be enabled while blocking
      x86, ticketlock: Add slowpath logic
      jump_label: Split jumplabel ratelimit
      x86, pvticketlock: When paravirtualizing ticket locks, increment by 2
      x86, pvticketlock: Use callee-save for lock_spinning
      xen, pvticketlocks: Add xen_nopvspin parameter to disable xen pv ticketlocks
      xen, pvticketlock: Xen implementation for PV ticket locks
      xen: Defer spinlock setup until boot CPU setup
      x86, ticketlock: Collapse a layer of functions
      x86, ticketlock: Don't inline _spin_unlock when using paravirt spinlocks
      x86, spinlock: Replace pv spinlocks with pv ticketlocks

commit 92b75202e5e8790905f9441ccaea2456cc4621a5
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Tue Aug 6 14:55:41 2013 +0530

    kvm: Paravirtual ticketlocks support for linux guests running on KVM hypervisor
    
    During smp_boot_cpus  paravirtualied KVM guest detects if the hypervisor has
    required feature (KVM_FEATURE_PV_UNHALT) to support pv-ticketlocks. If so,
    support for pv-ticketlocks is registered via pv_lock_ops.
    
    Use KVM_HC_KICK_CPU hypercall to wakeup waiting/halted vcpu.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20130810193849.GA25260@linux.vnet.ibm.com
    Signed-off-by: Suzuki Poulose <suzuki@in.ibm.com>
    [Raghu: check_zero race fix, enum for kvm_contention_stat, jumplabel related changes,
    addition of safe_halt for irq enabled case, bailout spinning in nmi case(Gleb)]
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Acked-by: Gleb Natapov <gleb@redhat.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 695399f2d5eb..427afcbf3d55 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -118,10 +118,20 @@ void kvm_async_pf_task_wait(u32 token);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
 extern void kvm_disable_steal_time(void);
-#else
-#define kvm_guest_init() do { } while (0)
+
+#ifdef CONFIG_PARAVIRT_SPINLOCKS
+void __init kvm_spinlock_init(void);
+#else /* !CONFIG_PARAVIRT_SPINLOCKS */
+static inline void kvm_spinlock_init(void)
+{
+}
+#endif /* CONFIG_PARAVIRT_SPINLOCKS */
+
+#else /* CONFIG_KVM_GUEST */
+#define kvm_guest_init() do {} while (0)
 #define kvm_async_pf_task_wait(T) do {} while(0)
 #define kvm_async_pf_task_wake(T) do {} while(0)
+
 static inline u32 kvm_read_and_reset_pf_reason(void)
 {
 	return 0;

commit 1085ba7f552d84aa8ac0ae903fa8d0cc2ff9f79d
Author: Jason Wang <jasowang@redhat.com>
Date:   Thu Jul 25 16:54:34 2013 +0800

    x86, kvm: Switch to use hypervisor_cpuid_base()
    
    Switch to use hypervisor_cpuid_base() to detect KVM.
    
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Jason Wang <jasowang@redhat.com>
    Link: http://lkml.kernel.org/r/1374742475-2485-3-git-send-email-jasowang@redhat.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 695399f2d5eb..0644129a5333 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -85,26 +85,20 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 	return ret;
 }
 
-static inline bool kvm_para_available(void)
+static inline uint32_t kvm_cpuid_base(void)
 {
-	unsigned int eax, ebx, ecx, edx;
-	char signature[13];
-
 	if (boot_cpu_data.cpuid_level < 0)
-		return false;	/* So we don't blow up on old processors */
+		return 0;	/* So we don't blow up on old processors */
 
-	if (cpu_has_hypervisor) {
-		cpuid(KVM_CPUID_SIGNATURE, &eax, &ebx, &ecx, &edx);
-		memcpy(signature + 0, &ebx, 4);
-		memcpy(signature + 4, &ecx, 4);
-		memcpy(signature + 8, &edx, 4);
-		signature[12] = 0;
+	if (cpu_has_hypervisor)
+		return hypervisor_cpuid_base("KVMKVMKVM\0\0\0", 0);
 
-		if (strcmp(signature, "KVMKVMKVM") == 0)
-			return true;
-	}
+	return 0;
+}
 
-	return false;
+static inline bool kvm_para_available(void)
+{
+	return kvm_cpuid_base() != 0;
 }
 
 static inline unsigned int kvm_arch_para_features(void)

commit 89f883372fa60f604d136924baf3e89ff1870e9e
Merge: 9e2d59ad580d 6b73a96065e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 24 13:07:18 2013 -0800

    Merge tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "KVM updates for the 3.9 merge window, including x86 real mode
      emulation fixes, stronger memory slot interface restrictions, mmu_lock
      spinlock hold time reduction, improved handling of large page faults
      on shadow, initial APICv HW acceleration support, s390 channel IO
      based virtio, amongst others"
    
    * tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (143 commits)
      Revert "KVM: MMU: lazily drop large spte"
      x86: pvclock kvm: align allocation size to page size
      KVM: nVMX: Remove redundant get_vmcs12 from nested_vmx_exit_handled_msr
      x86 emulator: fix parity calculation for AAD instruction
      KVM: PPC: BookE: Handle alignment interrupts
      booke: Added DBCR4 SPR number
      KVM: PPC: booke: Allow multiple exception types
      KVM: PPC: booke: use vcpu reference from thread_struct
      KVM: Remove user_alloc from struct kvm_memory_slot
      KVM: VMX: disable apicv by default
      KVM: s390: Fix handling of iscs.
      KVM: MMU: cleanup __direct_map
      KVM: MMU: remove pt_access in mmu_set_spte
      KVM: MMU: cleanup mapping-level
      KVM: MMU: lazily drop large spte
      KVM: VMX: cleanup vmx_set_cr0().
      KVM: VMX: add missing exit names to VMX_EXIT_REASONS array
      KVM: VMX: disable SMEP feature when guest is in non-paging mode
      KVM: Remove duplicate text in api.txt
      Revert "KVM: MMU: split kvm_mmu_free_page"
      ...

commit 3b4a505821615b6c055536a0c23ea37c349bb6a9
Author: Alok N Kataria <akataria@vmware.com>
Date:   Mon Jan 28 18:59:12 2013 -0800

    x86, kvm: Fix intialization warnings in kvm.c
    
    With commit:
    
      4cca6ea04d31 ("x86/apic: Allow x2apic without IR on VMware platform")
    
    we started seeing "incompatible initialization" warning messages,
    since x2apic_available() expects a bool return type while
    kvm_para_available() returns an int.
    
    Reported by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 5ed1f16187be..65231e173baf 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -85,13 +85,13 @@ static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
 	return ret;
 }
 
-static inline int kvm_para_available(void)
+static inline bool kvm_para_available(void)
 {
 	unsigned int eax, ebx, ecx, edx;
 	char signature[13];
 
 	if (boot_cpu_data.cpuid_level < 0)
-		return 0;	/* So we don't blow up on old processors */
+		return false;	/* So we don't blow up on old processors */
 
 	if (cpu_has_hypervisor) {
 		cpuid(KVM_CPUID_SIGNATURE, &eax, &ebx, &ecx, &edx);
@@ -101,10 +101,10 @@ static inline int kvm_para_available(void)
 		signature[12] = 0;
 
 		if (strcmp(signature, "KVMKVMKVM") == 0)
-			return 1;
+			return true;
 	}
 
-	return 0;
+	return false;
 }
 
 static inline unsigned int kvm_arch_para_features(void)

commit 11393a077dcfa7fb827d957f0305fc369d402a5e
Author: Jesse Larrew <jlarrew@linux.vnet.ibm.com>
Date:   Mon Dec 10 15:31:51 2012 -0600

    x86: kvm_para: fix typo in hypercall comments
    
    Correct a typo in the comment explaining hypercalls.
    
    Signed-off-by: Jesse Larrew <jlarrew@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index eb3e9d85e1f1..f49c16d47581 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -122,7 +122,7 @@ static inline bool kvm_check_and_clear_guest_paused(void)
  *
  * Up to four arguments may be passed in rbx, rcx, rdx, and rsi respectively.
  * The hypercall number should be placed in rax and the return value will be
- * placed in rax.  No other registers will be clobbered unless explicited
+ * placed in rax.  No other registers will be clobbered unless explicitly
  * noted by the particular hypercall.
  */
 

commit af170c5061dd78512c469e6e2d211980cdb2c193
Author: David Howells <dhowells@redhat.com>
Date:   Fri Dec 14 22:37:13 2012 +0000

    UAPI: (Scripted) Disintegrate arch/x86/include/asm
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index eb3e9d85e1f1..5ed1f16187be 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -1,103 +1,8 @@
 #ifndef _ASM_X86_KVM_PARA_H
 #define _ASM_X86_KVM_PARA_H
 
-#include <linux/types.h>
-#include <asm/hyperv.h>
-
-/* This CPUID returns the signature 'KVMKVMKVM' in ebx, ecx, and edx.  It
- * should be used to determine that a VM is running under KVM.
- */
-#define KVM_CPUID_SIGNATURE	0x40000000
-
-/* This CPUID returns a feature bitmap in eax.  Before enabling a particular
- * paravirtualization, the appropriate feature bit should be checked.
- */
-#define KVM_CPUID_FEATURES	0x40000001
-#define KVM_FEATURE_CLOCKSOURCE		0
-#define KVM_FEATURE_NOP_IO_DELAY	1
-#define KVM_FEATURE_MMU_OP		2
-/* This indicates that the new set of kvmclock msrs
- * are available. The use of 0x11 and 0x12 is deprecated
- */
-#define KVM_FEATURE_CLOCKSOURCE2        3
-#define KVM_FEATURE_ASYNC_PF		4
-#define KVM_FEATURE_STEAL_TIME		5
-#define KVM_FEATURE_PV_EOI		6
-
-/* The last 8 bits are used to indicate how to interpret the flags field
- * in pvclock structure. If no bits are set, all flags are ignored.
- */
-#define KVM_FEATURE_CLOCKSOURCE_STABLE_BIT	24
-
-#define MSR_KVM_WALL_CLOCK  0x11
-#define MSR_KVM_SYSTEM_TIME 0x12
-
-#define KVM_MSR_ENABLED 1
-/* Custom MSRs falls in the range 0x4b564d00-0x4b564dff */
-#define MSR_KVM_WALL_CLOCK_NEW  0x4b564d00
-#define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
-#define MSR_KVM_ASYNC_PF_EN 0x4b564d02
-#define MSR_KVM_STEAL_TIME  0x4b564d03
-#define MSR_KVM_PV_EOI_EN      0x4b564d04
-
-struct kvm_steal_time {
-	__u64 steal;
-	__u32 version;
-	__u32 flags;
-	__u32 pad[12];
-};
-
-#define KVM_STEAL_ALIGNMENT_BITS 5
-#define KVM_STEAL_VALID_BITS ((-1ULL << (KVM_STEAL_ALIGNMENT_BITS + 1)))
-#define KVM_STEAL_RESERVED_MASK (((1 << KVM_STEAL_ALIGNMENT_BITS) - 1 ) << 1)
-
-#define KVM_MAX_MMU_OP_BATCH           32
-
-#define KVM_ASYNC_PF_ENABLED			(1 << 0)
-#define KVM_ASYNC_PF_SEND_ALWAYS		(1 << 1)
-
-/* Operations for KVM_HC_MMU_OP */
-#define KVM_MMU_OP_WRITE_PTE            1
-#define KVM_MMU_OP_FLUSH_TLB	        2
-#define KVM_MMU_OP_RELEASE_PT	        3
-
-/* Payload for KVM_HC_MMU_OP */
-struct kvm_mmu_op_header {
-	__u32 op;
-	__u32 pad;
-};
-
-struct kvm_mmu_op_write_pte {
-	struct kvm_mmu_op_header header;
-	__u64 pte_phys;
-	__u64 pte_val;
-};
-
-struct kvm_mmu_op_flush_tlb {
-	struct kvm_mmu_op_header header;
-};
-
-struct kvm_mmu_op_release_pt {
-	struct kvm_mmu_op_header header;
-	__u64 pt_phys;
-};
-
-#define KVM_PV_REASON_PAGE_NOT_PRESENT 1
-#define KVM_PV_REASON_PAGE_READY 2
-
-struct kvm_vcpu_pv_apf_data {
-	__u32 reason;
-	__u8 pad[60];
-	__u32 enabled;
-};
-
-#define KVM_PV_EOI_BIT 0
-#define KVM_PV_EOI_MASK (0x1 << KVM_PV_EOI_BIT)
-#define KVM_PV_EOI_ENABLED KVM_PV_EOI_MASK
-#define KVM_PV_EOI_DISABLED 0x0
-
-#ifdef __KERNEL__
 #include <asm/processor.h>
+#include <uapi/asm/kvm_para.h>
 
 extern void kvmclock_init(void);
 extern int kvm_register_clock(char *txt);
@@ -228,6 +133,4 @@ static inline void kvm_disable_steal_time(void)
 }
 #endif
 
-#endif /* __KERNEL__ */
-
 #endif /* _ASM_X86_KVM_PARA_H */

commit 90993cdd1800dc6ef9587431a0c625b978584e81
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Aug 16 17:00:19 2012 -0300

    x86: KVM guest: merge CONFIG_KVM_CLOCK into CONFIG_KVM_GUEST
    
    The distinction between CONFIG_KVM_CLOCK and CONFIG_KVM_GUEST is
    not so clear anymore, as demonstrated by recent bugs caused by poor
    handling of on/off combinations of these options.
    
    Merge CONFIG_KVM_CLOCK into CONFIG_KVM_GUEST.
    
    Reported-By: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 20f5697888bd..eb3e9d85e1f1 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -102,14 +102,14 @@ struct kvm_vcpu_pv_apf_data {
 extern void kvmclock_init(void);
 extern int kvm_register_clock(char *txt);
 
-#ifdef CONFIG_KVM_CLOCK
+#ifdef CONFIG_KVM_GUEST
 bool kvm_check_and_clear_guest_paused(void);
 #else
 static inline bool kvm_check_and_clear_guest_paused(void)
 {
 	return false;
 }
-#endif /* CONFIG_KVMCLOCK */
+#endif /* CONFIG_KVM_GUEST */
 
 /* This instruction is vmcall.  On non-VT architectures, it will generate a
  * trap that we will then rewrite to the appropriate instruction.

commit e423ca155d3f5f16b46e30de9c818875b1fd617d
Author: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
Date:   Tue Aug 7 13:10:13 2012 +0530

    KVM: Correct vmrun to vmcall typo
    
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 2f7712e08b1e..20f5697888bd 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -116,7 +116,7 @@ static inline bool kvm_check_and_clear_guest_paused(void)
  */
 #define KVM_HYPERCALL ".byte 0x0f,0x01,0xc1"
 
-/* For KVM hypercalls, a three-byte sequence of either the vmrun or the vmmrun
+/* For KVM hypercalls, a three-byte sequence of either the vmcall or the vmmcall
  * instruction.  The hypervisor may replace it with something else but only the
  * instructions are guaranteed to be supported.
  *

commit ab9cf4996bb989983e73da894b8dd0239aa2c3c2
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Jun 24 19:24:34 2012 +0300

    KVM guest: guest side for eoi avoidance
    
    The idea is simple: there's a bit, per APIC, in guest memory,
    that tells the guest that it does not need EOI.
    Guest tests it using a single est and clear operation - this is
    necessary so that host can detect interrupt nesting - and if set, it can
    skip the EOI MSR.
    
    I run a simple microbenchmark to show exit reduction
    (note: for testing, need to apply follow-up patch
    'kvm: host side for eoi optimization' + a qemu patch
     I posted separately, on host):
    
    Before:
    
    Performance counter stats for 'sleep 1s':
    
                47,357 kvm:kvm_entry                                                [99.98%]
                     0 kvm:kvm_hypercall                                            [99.98%]
                     0 kvm:kvm_hv_hypercall                                         [99.98%]
                 5,001 kvm:kvm_pio                                                  [99.98%]
                     0 kvm:kvm_cpuid                                                [99.98%]
                22,124 kvm:kvm_apic                                                 [99.98%]
                49,849 kvm:kvm_exit                                                 [99.98%]
                21,115 kvm:kvm_inj_virq                                             [99.98%]
                     0 kvm:kvm_inj_exception                                        [99.98%]
                     0 kvm:kvm_page_fault                                           [99.98%]
                22,937 kvm:kvm_msr                                                  [99.98%]
                     0 kvm:kvm_cr                                                   [99.98%]
                     0 kvm:kvm_pic_set_irq                                          [99.98%]
                     0 kvm:kvm_apic_ipi                                             [99.98%]
                22,207 kvm:kvm_apic_accept_irq                                      [99.98%]
                22,421 kvm:kvm_eoi                                                  [99.98%]
                     0 kvm:kvm_pv_eoi                                               [99.99%]
                     0 kvm:kvm_nested_vmrun                                         [99.99%]
                     0 kvm:kvm_nested_intercepts                                    [99.99%]
                     0 kvm:kvm_nested_vmexit                                        [99.99%]
                     0 kvm:kvm_nested_vmexit_inject                                    [99.99%]
                     0 kvm:kvm_nested_intr_vmexit                                    [99.99%]
                     0 kvm:kvm_invlpga                                              [99.99%]
                     0 kvm:kvm_skinit                                               [99.99%]
                    57 kvm:kvm_emulate_insn                                         [99.99%]
                     0 kvm:vcpu_match_mmio                                          [99.99%]
                     0 kvm:kvm_userspace_exit                                       [99.99%]
                     2 kvm:kvm_set_irq                                              [99.99%]
                     2 kvm:kvm_ioapic_set_irq                                       [99.99%]
                23,609 kvm:kvm_msi_set_irq                                          [99.99%]
                     1 kvm:kvm_ack_irq                                              [99.99%]
                   131 kvm:kvm_mmio                                                 [99.99%]
                   226 kvm:kvm_fpu                                                  [100.00%]
                     0 kvm:kvm_age_page                                             [100.00%]
                     0 kvm:kvm_try_async_get_page                                    [100.00%]
                     0 kvm:kvm_async_pf_doublefault                                    [100.00%]
                     0 kvm:kvm_async_pf_not_present                                    [100.00%]
                     0 kvm:kvm_async_pf_ready                                       [100.00%]
                     0 kvm:kvm_async_pf_completed
    
           1.002100578 seconds time elapsed
    
    After:
    
     Performance counter stats for 'sleep 1s':
    
                28,354 kvm:kvm_entry                                                [99.98%]
                     0 kvm:kvm_hypercall                                            [99.98%]
                     0 kvm:kvm_hv_hypercall                                         [99.98%]
                 1,347 kvm:kvm_pio                                                  [99.98%]
                     0 kvm:kvm_cpuid                                                [99.98%]
                 1,931 kvm:kvm_apic                                                 [99.98%]
                29,595 kvm:kvm_exit                                                 [99.98%]
                24,884 kvm:kvm_inj_virq                                             [99.98%]
                     0 kvm:kvm_inj_exception                                        [99.98%]
                     0 kvm:kvm_page_fault                                           [99.98%]
                 1,986 kvm:kvm_msr                                                  [99.98%]
                     0 kvm:kvm_cr                                                   [99.98%]
                     0 kvm:kvm_pic_set_irq                                          [99.98%]
                     0 kvm:kvm_apic_ipi                                             [99.99%]
                25,953 kvm:kvm_apic_accept_irq                                      [99.99%]
                26,132 kvm:kvm_eoi                                                  [99.99%]
                26,593 kvm:kvm_pv_eoi                                               [99.99%]
                     0 kvm:kvm_nested_vmrun                                         [99.99%]
                     0 kvm:kvm_nested_intercepts                                    [99.99%]
                     0 kvm:kvm_nested_vmexit                                        [99.99%]
                     0 kvm:kvm_nested_vmexit_inject                                    [99.99%]
                     0 kvm:kvm_nested_intr_vmexit                                    [99.99%]
                     0 kvm:kvm_invlpga                                              [99.99%]
                     0 kvm:kvm_skinit                                               [99.99%]
                   284 kvm:kvm_emulate_insn                                         [99.99%]
                    68 kvm:vcpu_match_mmio                                          [99.99%]
                    68 kvm:kvm_userspace_exit                                       [99.99%]
                     2 kvm:kvm_set_irq                                              [99.99%]
                     2 kvm:kvm_ioapic_set_irq                                       [99.99%]
                28,288 kvm:kvm_msi_set_irq                                          [99.99%]
                     1 kvm:kvm_ack_irq                                              [99.99%]
                   131 kvm:kvm_mmio                                                 [100.00%]
                   588 kvm:kvm_fpu                                                  [100.00%]
                     0 kvm:kvm_age_page                                             [100.00%]
                     0 kvm:kvm_try_async_get_page                                    [100.00%]
                     0 kvm:kvm_async_pf_doublefault                                    [100.00%]
                     0 kvm:kvm_async_pf_not_present                                    [100.00%]
                     0 kvm:kvm_async_pf_ready                                       [100.00%]
                     0 kvm:kvm_async_pf_completed
    
           1.002039622 seconds time elapsed
    
    We see that # of exits is almost halved.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 63ab1661d00e..2f7712e08b1e 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -22,6 +22,7 @@
 #define KVM_FEATURE_CLOCKSOURCE2        3
 #define KVM_FEATURE_ASYNC_PF		4
 #define KVM_FEATURE_STEAL_TIME		5
+#define KVM_FEATURE_PV_EOI		6
 
 /* The last 8 bits are used to indicate how to interpret the flags field
  * in pvclock structure. If no bits are set, all flags are ignored.
@@ -37,6 +38,7 @@
 #define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
 #define MSR_KVM_ASYNC_PF_EN 0x4b564d02
 #define MSR_KVM_STEAL_TIME  0x4b564d03
+#define MSR_KVM_PV_EOI_EN      0x4b564d04
 
 struct kvm_steal_time {
 	__u64 steal;
@@ -89,6 +91,11 @@ struct kvm_vcpu_pv_apf_data {
 	__u32 enabled;
 };
 
+#define KVM_PV_EOI_BIT 0
+#define KVM_PV_EOI_MASK (0x1 << KVM_PV_EOI_BIT)
+#define KVM_PV_EOI_ENABLED KVM_PV_EOI_MASK
+#define KVM_PV_EOI_DISABLED 0x0
+
 #ifdef __KERNEL__
 #include <asm/processor.h>
 

commit 07acfc2a9349a8ce45b236c2624dad452001966b
Merge: b5f4035adfff 322728e55aa7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 16:17:30 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Avi Kivity:
     "Changes include additional instruction emulation, page-crossing MMIO,
      faster dirty logging, preventing the watchdog from killing a stopped
      guest, module autoload, a new MSI ABI, and some minor optimizations
      and fixes.  Outside x86 we have a small s390 and a very large ppc
      update.
    
      Regarding the new (for kvm) rebaseless workflow, some of the patches
      that were merged before we switch trees had to be rebased, while
      others are true pulls.  In either case the signoffs should be correct
      now."
    
    Fix up trivial conflicts in Documentation/feature-removal-schedule.txt
    arch/powerpc/kvm/book3s_segment.S and arch/x86/include/asm/kvm_para.h.
    
    I suspect the kvm_para.h resolution ends up doing the "do I have cpuid"
    check effectively twice (it was done differently in two different
    commits), but better safe than sorry ;)
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (125 commits)
      KVM: make asm-generic/kvm_para.h have an ifdef __KERNEL__ block
      KVM: s390: onereg for timer related registers
      KVM: s390: epoch difference and TOD programmable field
      KVM: s390: KVM_GET/SET_ONEREG for s390
      KVM: s390: add capability indicating COW support
      KVM: Fix mmu_reload() clash with nested vmx event injection
      KVM: MMU: Don't use RCU for lockless shadow walking
      KVM: VMX: Optimize %ds, %es reload
      KVM: VMX: Fix %ds/%es clobber
      KVM: x86 emulator: convert bsf/bsr instructions to emulate_2op_SrcV_nobyte()
      KVM: VMX: unlike vmcs on fail path
      KVM: PPC: Emulator: clean up SPR reads and writes
      KVM: PPC: Emulator: clean up instruction parsing
      kvm/powerpc: Add new ioctl to retreive server MMU infos
      kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
      KVM: PPC: bookehv: Fix r8/r13 storing in level exception handler
      KVM: PPC: Book3S: Enable IRQs during exit handling
      KVM: PPC: Fix PR KVM on POWER7 bare metal
      KVM: PPC: Fix stbux emulation
      KVM: PPC: bookehv: Use lwz/stw instead of PPC_LL/PPC_STL for 32-bit fields
      ...

commit c3709e6734daa4d9b37fe31592ebb0eb57bae1bb
Author: Alan Cox <alan@linux.intel.com>
Date:   Mon May 14 18:41:09 2012 +0100

    x86, kvm: KVM paravirt kernels don't check for CPUID being unavailable
    
    We set cpuid_level to -1 if there is no CPUID instruction (only
    possible on i386).
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    Link: http://lkml.kernel.org/r/20120514174059.30236.1064.stgit@bluebook
    Resolves-bug: https://bugzilla.kernel.org/show_bug.cgi?id=12122
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 734c3767cfac..183922e13de1 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -170,6 +170,9 @@ static inline int kvm_para_available(void)
 	unsigned int eax, ebx, ecx, edx;
 	char signature[13];
 
+	if (boot_cpu_data.cpuid_level < 0)
+		return 0;	/* So we don't blow up on old processors */
+
 	cpuid(KVM_CPUID_SIGNATURE, &eax, &ebx, &ecx, &edx);
 	memcpy(signature + 0, &ebx, 4);
 	memcpy(signature + 4, &ecx, 4);

commit 9b72d3b07dd99ac8ab2b84de5004a295af460536
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Apr 30 14:45:49 2012 +0300

    KVM guest: make kvm_para_available() check hypervisor bit reading cpuid leaf
    
    This cpuid range does not exist on real HW and Intel spec says that
    "Information returned for highest basic information leaf" will be
    returned. Not very well defined.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 99c4bbe0cca2..a7a7a94b94ce 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -178,14 +178,16 @@ static inline int kvm_para_available(void)
 	unsigned int eax, ebx, ecx, edx;
 	char signature[13];
 
-	cpuid(KVM_CPUID_SIGNATURE, &eax, &ebx, &ecx, &edx);
-	memcpy(signature + 0, &ebx, 4);
-	memcpy(signature + 4, &ecx, 4);
-	memcpy(signature + 8, &edx, 4);
-	signature[12] = 0;
-
-	if (strcmp(signature, "KVMKVMKVM") == 0)
-		return 1;
+	if (cpu_has_hypervisor) {
+		cpuid(KVM_CPUID_SIGNATURE, &eax, &ebx, &ecx, &edx);
+		memcpy(signature + 0, &ebx, 4);
+		memcpy(signature + 4, &ecx, 4);
+		memcpy(signature + 8, &edx, 4);
+		signature[12] = 0;
+
+		if (strcmp(signature, "KVMKVMKVM") == 0)
+			return 1;
+	}
 
 	return 0;
 }

commit 3b5d56b9317fa7b5407dff1aa7b115bf6cdbd494
Author: Eric B Munson <emunson@mgebm.net>
Date:   Sat Mar 10 14:37:26 2012 -0500

    kvmclock: Add functions to check if the host has stopped the vm
    
    When a host stops or suspends a VM it will set a flag to show this.  The
    watchdog will use these functions to determine if a softlockup is real, or the
    result of a suspended VM.
    
    Signed-off-by: Eric B Munson <emunson@mgebm.net>
    asm-generic changes Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 734c3767cfac..99c4bbe0cca2 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -95,6 +95,14 @@ struct kvm_vcpu_pv_apf_data {
 extern void kvmclock_init(void);
 extern int kvm_register_clock(char *txt);
 
+#ifdef CONFIG_KVM_CLOCK
+bool kvm_check_and_clear_guest_paused(void);
+#else
+static inline bool kvm_check_and_clear_guest_paused(void)
+{
+	return false;
+}
+#endif /* CONFIG_KVMCLOCK */
 
 /* This instruction is vmcall.  On non-VT architectures, it will generate a
  * trap that we will then rewrite to the appropriate instruction.

commit d910f5c1064d7ff09c31b0191564f9f99e210f91
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Jul 11 15:28:19 2011 -0400

    KVM guest: KVM Steal time registration
    
    This patch implements the kvm bits of the steal time infrastructure.
    The most important part of it, is the steal time clock. It is an
    continuous clock that shows the accumulated amount of steal time
    since vcpu creation. It is supposed to survive cpu offlining/onlining.
    
    [marcelo: fix build with CONFIG_KVM_GUEST=n]
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Tested-by: Eric B Munson <emunson@mgebm.net>
    CC: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Avi Kivity <avi@redhat.com>
    CC: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index c484ba8e05ea..734c3767cfac 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -192,6 +192,7 @@ void __init kvm_guest_init(void);
 void kvm_async_pf_task_wait(u32 token);
 void kvm_async_pf_task_wake(u32 token);
 u32 kvm_read_and_reset_pf_reason(void);
+extern void kvm_disable_steal_time(void);
 #else
 #define kvm_guest_init() do { } while (0)
 #define kvm_async_pf_task_wait(T) do {} while(0)
@@ -200,6 +201,11 @@ static inline u32 kvm_read_and_reset_pf_reason(void)
 {
 	return 0;
 }
+
+static inline void kvm_disable_steal_time(void)
+{
+	return;
+}
 #endif
 
 #endif /* __KERNEL__ */

commit c9aaa8957f203bd6df83b002fb40b98390bed078
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Jul 11 15:28:14 2011 -0400

    KVM: Steal time implementation
    
    To implement steal time, we need the hypervisor to pass the guest
    information about how much time was spent running other processes
    outside the VM, while the vcpu had meaningful work to do - halt
    time does not count.
    
    This information is acquired through the run_delay field of
    delayacct/schedstats infrastructure, that counts time spent in a
    runqueue but not running.
    
    Steal time is a per-cpu information, so the traditional MSR-based
    infrastructure is used. A new msr, KVM_MSR_STEAL_TIME, holds the
    memory area address containing information about steal time
    
    This patch contains the hypervisor part of the steal time infrasructure,
    and can be backported independently of the guest portion.
    
    [avi, yongjie: export delayacct_on, to avoid build failures in some configs]
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Tested-by: Eric B Munson <emunson@mgebm.net>
    CC: Rik van Riel <riel@redhat.com>
    CC: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Yongjie Ren <yongjie.ren@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 65f8bb9279e0..c484ba8e05ea 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -45,6 +45,10 @@ struct kvm_steal_time {
 	__u32 pad[12];
 };
 
+#define KVM_STEAL_ALIGNMENT_BITS 5
+#define KVM_STEAL_VALID_BITS ((-1ULL << (KVM_STEAL_ALIGNMENT_BITS + 1)))
+#define KVM_STEAL_RESERVED_MASK (((1 << KVM_STEAL_ALIGNMENT_BITS) - 1 ) << 1)
+
 #define KVM_MAX_MMU_OP_BATCH           32
 
 #define KVM_ASYNC_PF_ENABLED			(1 << 0)

commit 9ddabbe72e41ca6794cb4947c70929c9410e6752
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Jul 11 15:28:13 2011 -0400

    KVM: KVM Steal time guest/host interface
    
    To implement steal time, we need the hypervisor to pass the guest information
    about how much time was spent running other processes outside the VM.
    This is per-vcpu, and using the kvmclock structure for that is an abuse
    we decided not to make.
    
    In this patchset, I am introducing a new msr, KVM_MSR_STEAL_TIME, that
    holds the memory area address containing information about steal time
    
    This patch contains the headers for it. I am keeping it separate to facilitate
    backports to people who wants to backport the kernel part but not the
    hypervisor, or the other way around.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Tested-by: Eric B Munson <emunson@mgebm.net>
    CC: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index d6cd79b34aab..65f8bb9279e0 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -21,6 +21,7 @@
  */
 #define KVM_FEATURE_CLOCKSOURCE2        3
 #define KVM_FEATURE_ASYNC_PF		4
+#define KVM_FEATURE_STEAL_TIME		5
 
 /* The last 8 bits are used to indicate how to interpret the flags field
  * in pvclock structure. If no bits are set, all flags are ignored.
@@ -35,6 +36,14 @@
 #define MSR_KVM_WALL_CLOCK_NEW  0x4b564d00
 #define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
 #define MSR_KVM_ASYNC_PF_EN 0x4b564d02
+#define MSR_KVM_STEAL_TIME  0x4b564d03
+
+struct kvm_steal_time {
+	__u64 steal;
+	__u32 version;
+	__u32 flags;
+	__u32 pad[12];
+};
 
 #define KVM_MAX_MMU_OP_BATCH           32
 

commit 4b6b35f55ca81d3bfdec63b0adb61798702ceb2e
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Jul 11 15:28:12 2011 -0400

    KVM: Add constant to represent KVM MSRs enabled bit in guest/host interface
    
    This patch is simple, put in a different commit so it can be more easily
    shared between guest and hypervisor. It just defines a named constant
    to indicate the enable bit for KVM-specific MSRs.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Tested-by: Eric B Munson <emunson@mgebm.net>
    CC: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index a427bf77a93d..d6cd79b34aab 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -30,6 +30,7 @@
 #define MSR_KVM_WALL_CLOCK  0x11
 #define MSR_KVM_SYSTEM_TIME 0x12
 
+#define KVM_MSR_ENABLED 1
 /* Custom MSRs falls in the range 0x4b564d00-0x4b564dff */
 #define MSR_KVM_WALL_CLOCK_NEW  0x4b564d00
 #define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01

commit d4c90b0043bdb40a6f340d34b2ac1861040de88c
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Wed Oct 20 18:34:54 2010 +0200

    KVM: x86: Add missing inline tag to kvm_read_and_reset_pf_reason
    
    May otherwise generates build warnings about unused
    kvm_read_and_reset_pf_reason if included without CONFIG_KVM_GUEST
    enabled.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index d3a1a4805ab8..a427bf77a93d 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -182,7 +182,7 @@ u32 kvm_read_and_reset_pf_reason(void);
 #define kvm_guest_init() do { } while (0)
 #define kvm_async_pf_task_wait(T) do {} while(0)
 #define kvm_async_pf_task_wake(T) do {} while(0)
-static u32 kvm_read_and_reset_pf_reason(void)
+static inline u32 kvm_read_and_reset_pf_reason(void)
 {
 	return 0;
 }

commit 6adba527420651b6cacaf392541c09fb108711a2
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:55 2010 +0200

    KVM: Let host know whether the guest can handle async PF in non-userspace context.
    
    If guest can detect that it runs in non-preemptable context it can
    handle async PFs at any time, so let host know that it can send async
    PF even if guest cpu is not in userspace.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index fbfd3679bc18..d3a1a4805ab8 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -38,6 +38,7 @@
 #define KVM_MAX_MMU_OP_BATCH           32
 
 #define KVM_ASYNC_PF_ENABLED			(1 << 0)
+#define KVM_ASYNC_PF_SEND_ALWAYS		(1 << 1)
 
 /* Operations for KVM_HC_MMU_OP */
 #define KVM_MMU_OP_WRITE_PTE            1

commit 631bc4878220932fe67fc46fc7cf7cccdb1ec597
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:52 2010 +0200

    KVM: Handle async PF in a guest.
    
    When async PF capability is detected hook up special page fault handler
    that will handle async page fault events and bypass other page faults to
    regular page fault handler. Also add async PF handling to nested SVM
    emulation. Async PF always generates exit to L1 where vcpu thread will
    be scheduled out until page is available.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 2315398230d1..fbfd3679bc18 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -65,6 +65,9 @@ struct kvm_mmu_op_release_pt {
 	__u64 pt_phys;
 };
 
+#define KVM_PV_REASON_PAGE_NOT_PRESENT 1
+#define KVM_PV_REASON_PAGE_READY 2
+
 struct kvm_vcpu_pv_apf_data {
 	__u32 reason;
 	__u8 pad[60];
@@ -171,8 +174,17 @@ static inline unsigned int kvm_arch_para_features(void)
 
 #ifdef CONFIG_KVM_GUEST
 void __init kvm_guest_init(void);
+void kvm_async_pf_task_wait(u32 token);
+void kvm_async_pf_task_wake(u32 token);
+u32 kvm_read_and_reset_pf_reason(void);
 #else
 #define kvm_guest_init() do { } while (0)
+#define kvm_async_pf_task_wait(T) do {} while(0)
+#define kvm_async_pf_task_wake(T) do {} while(0)
+static u32 kvm_read_and_reset_pf_reason(void)
+{
+	return 0;
+}
 #endif
 
 #endif /* __KERNEL__ */

commit fd10cde9294f73eeccbc16f3fec1ae6cde7b800c
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:51 2010 +0200

    KVM paravirt: Add async PF initialization to PV guest.
    
    Enable async PF in a guest if async PF capability is discovered.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 8662ae0a035c..2315398230d1 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -65,6 +65,12 @@ struct kvm_mmu_op_release_pt {
 	__u64 pt_phys;
 };
 
+struct kvm_vcpu_pv_apf_data {
+	__u32 reason;
+	__u8 pad[60];
+	__u32 enabled;
+};
+
 #ifdef __KERNEL__
 #include <asm/processor.h>
 

commit 344d9588a9df06182684168be4f1408b55c7da3e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:50 2010 +0200

    KVM: Add PV MSR to enable asynchronous page faults delivery.
    
    Guest enables async PF vcpu functionality using this MSR.
    
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index e3faaaf4301e..8662ae0a035c 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -20,6 +20,7 @@
  * are available. The use of 0x11 and 0x12 is deprecated
  */
 #define KVM_FEATURE_CLOCKSOURCE2        3
+#define KVM_FEATURE_ASYNC_PF		4
 
 /* The last 8 bits are used to indicate how to interpret the flags field
  * in pvclock structure. If no bits are set, all flags are ignored.
@@ -32,9 +33,12 @@
 /* Custom MSRs falls in the range 0x4b564d00-0x4b564dff */
 #define MSR_KVM_WALL_CLOCK_NEW  0x4b564d00
 #define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
+#define MSR_KVM_ASYNC_PF_EN 0x4b564d02
 
 #define KVM_MAX_MMU_OP_BATCH           32
 
+#define KVM_ASYNC_PF_ENABLED			(1 << 0)
+
 /* Operations for KVM_HC_MMU_OP */
 #define KVM_MMU_OP_WRITE_PTE            1
 #define KVM_MMU_OP_FLUSH_TLB	        2

commit ca3f10172eea9b95bbb66487656f3c3e93855702
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:49 2010 +0200

    KVM paravirt: Move kvm_smp_prepare_boot_cpu() from kvmclock.c to kvm.c.
    
    Async PF also needs to hook into smp_prepare_boot_cpu so move the hook
    into generic code.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 7b562b6184bc..e3faaaf4301e 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -65,6 +65,7 @@ struct kvm_mmu_op_release_pt {
 #include <asm/processor.h>
 
 extern void kvmclock_init(void);
+extern int kvm_register_clock(char *txt);
 
 
 /* This instruction is vmcall.  On non-VT architectures, it will generate a

commit ba492962363a02c45836be205f339be48093e1be
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:56 2010 +0200

    KVM: Move kvm_guest_init out of generic code
    
    Currently x86 is the only architecture that uses kvm_guest_init(). With
    PowerPC we're getting a second user, but the signature is different there
    and we don't need to export it, as it uses the normal kernel init framework.
    
    So let's move the x86 specific definition of that function over to the x86
    specfic header file.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 05eba5e9a8e8..7b562b6184bc 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -158,6 +158,12 @@ static inline unsigned int kvm_arch_para_features(void)
 	return cpuid_eax(KVM_CPUID_FEATURES);
 }
 
+#ifdef CONFIG_KVM_GUEST
+void __init kvm_guest_init(void);
+#else
+#define kvm_guest_init() do { } while (0)
 #endif
 
+#endif /* __KERNEL__ */
+
 #endif /* _ASM_X86_KVM_PARA_H */

commit 3a0d7256a6fb8c13f9fac6cd63250f97a8f0d8de
Author: Glauber Costa <glommer@redhat.com>
Date:   Tue May 11 12:17:45 2010 -0400

    x86, paravirt: don't compute pvclock adjustments if we trust the tsc
    
    If the HV told us we can fully trust the TSC, skip any
    correction
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index f019f8cb182e..05eba5e9a8e8 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -21,6 +21,11 @@
  */
 #define KVM_FEATURE_CLOCKSOURCE2        3
 
+/* The last 8 bits are used to indicate how to interpret the flags field
+ * in pvclock structure. If no bits are set, all flags are ignored.
+ */
+#define KVM_FEATURE_CLOCKSOURCE_STABLE_BIT	24
+
 #define MSR_KVM_WALL_CLOCK  0x11
 #define MSR_KVM_SYSTEM_TIME 0x12
 

commit 0e6ac58acbcddbc9d1687214f0d43d8657cc036c
Author: Glauber Costa <glommer@redhat.com>
Date:   Tue May 11 12:17:42 2010 -0400

    KVM: x86: add new KVMCLOCK cpuid feature
    
    This cpuid, KVM_CPUID_CLOCKSOURCE2, will indicate to the guest
    that kvmclock is available through a new set of MSRs. The old ones
    are deprecated.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 97348082782a..f019f8cb182e 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -16,6 +16,10 @@
 #define KVM_FEATURE_CLOCKSOURCE		0
 #define KVM_FEATURE_NOP_IO_DELAY	1
 #define KVM_FEATURE_MMU_OP		2
+/* This indicates that the new set of kvmclock msrs
+ * are available. The use of 0x11 and 0x12 is deprecated
+ */
+#define KVM_FEATURE_CLOCKSOURCE2        3
 
 #define MSR_KVM_WALL_CLOCK  0x11
 #define MSR_KVM_SYSTEM_TIME 0x12

commit 11c6bffa42b85e703c21a1d2372dce7262daca8e
Author: Glauber Costa <glommer@redhat.com>
Date:   Tue May 11 12:17:41 2010 -0400

    KVM: x86: change msr numbers for kvmclock
    
    Avi pointed out a while ago that those MSRs falls into the pentium
    PMU range. So the idea here is to add new ones, and after a while,
    deprecate the old ones.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index ffae1420e7d7..97348082782a 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -20,6 +20,10 @@
 #define MSR_KVM_WALL_CLOCK  0x11
 #define MSR_KVM_SYSTEM_TIME 0x12
 
+/* Custom MSRs falls in the range 0x4b564d00-0x4b564dff */
+#define MSR_KVM_WALL_CLOCK_NEW  0x4b564d00
+#define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
+
 #define KVM_MAX_MMU_OP_BATCH           32
 
 /* Operations for KVM_HC_MMU_OP */

commit 55cd8e5a4edb8e235163ffe8264b9aaa8d7c050f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Jan 17 15:51:22 2010 +0200

    KVM: Implement bare minimum of HYPER-V MSRs
    
    Minimum HYPER-V implementation should have GUEST_OS_ID, HYPERCALL and
    VP_INDEX MSRs.
    
    [avi: fix build on i386]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Vadim Rozenfeld <vrozenfe@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index c584076a47f4..ffae1420e7d7 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -2,6 +2,7 @@
 #define _ASM_X86_KVM_PARA_H
 
 #include <linux/types.h>
+#include <asm/hyperv.h>
 
 /* This CPUID returns the signature 'KVMKVMKVM' in ebx, ecx, and edx.  It
  * should be used to determine that a VM is running under KVM.

commit fa6870c6b64214aced218d930ec7221e2a9767b8
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Aug 16 15:31:33 2009 +0300

    KVM: Add missing #include
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index b8a3305ae093..c584076a47f4 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -1,6 +1,8 @@
 #ifndef _ASM_X86_KVM_PARA_H
 #define _ASM_X86_KVM_PARA_H
 
+#include <linux/types.h>
+
 /* This CPUID returns the signature 'KVMKVMKVM' in ebx, ecx, and edx.  It
  * should be used to determine that a VM is running under KVM.
  */

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 30054fded4fb..b8a3305ae093 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -1,5 +1,5 @@
-#ifndef ASM_X86__KVM_PARA_H
-#define ASM_X86__KVM_PARA_H
+#ifndef _ASM_X86_KVM_PARA_H
+#define _ASM_X86_KVM_PARA_H
 
 /* This CPUID returns the signature 'KVMKVMKVM' in ebx, ecx, and edx.  It
  * should be used to determine that a VM is running under KVM.
@@ -144,4 +144,4 @@ static inline unsigned int kvm_arch_para_features(void)
 
 #endif
 
-#endif /* ASM_X86__KVM_PARA_H */
+#endif /* _ASM_X86_KVM_PARA_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
new file mode 100644
index 000000000000..30054fded4fb
--- /dev/null
+++ b/arch/x86/include/asm/kvm_para.h
@@ -0,0 +1,147 @@
+#ifndef ASM_X86__KVM_PARA_H
+#define ASM_X86__KVM_PARA_H
+
+/* This CPUID returns the signature 'KVMKVMKVM' in ebx, ecx, and edx.  It
+ * should be used to determine that a VM is running under KVM.
+ */
+#define KVM_CPUID_SIGNATURE	0x40000000
+
+/* This CPUID returns a feature bitmap in eax.  Before enabling a particular
+ * paravirtualization, the appropriate feature bit should be checked.
+ */
+#define KVM_CPUID_FEATURES	0x40000001
+#define KVM_FEATURE_CLOCKSOURCE		0
+#define KVM_FEATURE_NOP_IO_DELAY	1
+#define KVM_FEATURE_MMU_OP		2
+
+#define MSR_KVM_WALL_CLOCK  0x11
+#define MSR_KVM_SYSTEM_TIME 0x12
+
+#define KVM_MAX_MMU_OP_BATCH           32
+
+/* Operations for KVM_HC_MMU_OP */
+#define KVM_MMU_OP_WRITE_PTE            1
+#define KVM_MMU_OP_FLUSH_TLB	        2
+#define KVM_MMU_OP_RELEASE_PT	        3
+
+/* Payload for KVM_HC_MMU_OP */
+struct kvm_mmu_op_header {
+	__u32 op;
+	__u32 pad;
+};
+
+struct kvm_mmu_op_write_pte {
+	struct kvm_mmu_op_header header;
+	__u64 pte_phys;
+	__u64 pte_val;
+};
+
+struct kvm_mmu_op_flush_tlb {
+	struct kvm_mmu_op_header header;
+};
+
+struct kvm_mmu_op_release_pt {
+	struct kvm_mmu_op_header header;
+	__u64 pt_phys;
+};
+
+#ifdef __KERNEL__
+#include <asm/processor.h>
+
+extern void kvmclock_init(void);
+
+
+/* This instruction is vmcall.  On non-VT architectures, it will generate a
+ * trap that we will then rewrite to the appropriate instruction.
+ */
+#define KVM_HYPERCALL ".byte 0x0f,0x01,0xc1"
+
+/* For KVM hypercalls, a three-byte sequence of either the vmrun or the vmmrun
+ * instruction.  The hypervisor may replace it with something else but only the
+ * instructions are guaranteed to be supported.
+ *
+ * Up to four arguments may be passed in rbx, rcx, rdx, and rsi respectively.
+ * The hypercall number should be placed in rax and the return value will be
+ * placed in rax.  No other registers will be clobbered unless explicited
+ * noted by the particular hypercall.
+ */
+
+static inline long kvm_hypercall0(unsigned int nr)
+{
+	long ret;
+	asm volatile(KVM_HYPERCALL
+		     : "=a"(ret)
+		     : "a"(nr)
+		     : "memory");
+	return ret;
+}
+
+static inline long kvm_hypercall1(unsigned int nr, unsigned long p1)
+{
+	long ret;
+	asm volatile(KVM_HYPERCALL
+		     : "=a"(ret)
+		     : "a"(nr), "b"(p1)
+		     : "memory");
+	return ret;
+}
+
+static inline long kvm_hypercall2(unsigned int nr, unsigned long p1,
+				  unsigned long p2)
+{
+	long ret;
+	asm volatile(KVM_HYPERCALL
+		     : "=a"(ret)
+		     : "a"(nr), "b"(p1), "c"(p2)
+		     : "memory");
+	return ret;
+}
+
+static inline long kvm_hypercall3(unsigned int nr, unsigned long p1,
+				  unsigned long p2, unsigned long p3)
+{
+	long ret;
+	asm volatile(KVM_HYPERCALL
+		     : "=a"(ret)
+		     : "a"(nr), "b"(p1), "c"(p2), "d"(p3)
+		     : "memory");
+	return ret;
+}
+
+static inline long kvm_hypercall4(unsigned int nr, unsigned long p1,
+				  unsigned long p2, unsigned long p3,
+				  unsigned long p4)
+{
+	long ret;
+	asm volatile(KVM_HYPERCALL
+		     : "=a"(ret)
+		     : "a"(nr), "b"(p1), "c"(p2), "d"(p3), "S"(p4)
+		     : "memory");
+	return ret;
+}
+
+static inline int kvm_para_available(void)
+{
+	unsigned int eax, ebx, ecx, edx;
+	char signature[13];
+
+	cpuid(KVM_CPUID_SIGNATURE, &eax, &ebx, &ecx, &edx);
+	memcpy(signature + 0, &ebx, 4);
+	memcpy(signature + 4, &ecx, 4);
+	memcpy(signature + 8, &edx, 4);
+	signature[12] = 0;
+
+	if (strcmp(signature, "KVMKVMKVM") == 0)
+		return 1;
+
+	return 0;
+}
+
+static inline unsigned int kvm_arch_para_features(void)
+{
+	return cpuid_eax(KVM_CPUID_FEATURES);
+}
+
+#endif
+
+#endif /* ASM_X86__KVM_PARA_H */
