commit c7aadc09321d8f9a1d3bd1e6d8a47222ecddf6c5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 17 18:25:57 2020 +0200

    x86/entry: Increase entry_stack size to a full page
    
    Marco crashed in bad_iret with a Clang11/KCSAN build due to
    overflowing the stack. Now that we run C code on it, expand it to a
    full page.
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Reported-by: Marco Elver <elver@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Lai Jiangshan <jiangshanlai@gmail.com>
    Tested-by: Marco Elver <elver@google.com>
    Link: https://lkml.kernel.org/r/20200618144801.819246178@infradead.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 42cd333616c4..03b7c4ca425a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -370,7 +370,7 @@ struct x86_hw_tss {
 #define IO_BITMAP_OFFSET_INVALID	(__KERNEL_TSS_LIMIT + 1)
 
 struct entry_stack {
-	unsigned long		words[64];
+	char	stack[PAGE_SIZE];
 };
 
 struct entry_stack_page {

commit 2823e83a3dc0f54d23db67ca07d74b9c8bb1fdda
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 3 13:40:22 2020 +0200

    x86/entry: __always_inline CR2 for noinstr
    
    vmlinux.o: warning: objtool: exc_page_fault()+0x9: call to read_cr2() leaves .noinstr.text section
    vmlinux.o: warning: objtool: exc_page_fault()+0x24: call to prefetchw() leaves .noinstr.text section
    vmlinux.o: warning: objtool: exc_page_fault()+0x21: call to kvm_handle_async_pf.isra.0() leaves .noinstr.text section
    vmlinux.o: warning: objtool: exc_nmi()+0x1cc: call to write_cr2() leaves .noinstr.text section
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200603114052.243227806@infradead.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 29ee0c088009..42cd333616c4 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -823,7 +823,7 @@ static inline void prefetch(const void *x)
  * Useful for spinlocks to avoid one state transition in the
  * cache coherency protocol:
  */
-static inline void prefetchw(const void *x)
+static __always_inline void prefetchw(const void *x)
 {
 	alternative_input(BASE_PREFETCH, "prefetchw %P1",
 			  X86_FEATURE_3DNOWPREFETCH,

commit 69fc06f70f4569c9969f99fe25bdc9a6bb537b43
Merge: 60056060bede 0decf1f8de91
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 13:13:00 2020 -0700

    Merge tag 'objtool-core-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull objtool updates from Ingo Molnar:
     "There are a lot of objtool changes in this cycle, all across the map:
    
       - Speed up objtool significantly, especially when there are large
         number of sections
    
       - Improve objtool's understanding of special instructions such as
         IRET, to reduce the number of annotations required
    
       - Implement 'noinstr' validation
    
       - Do baby steps for non-x86 objtool use
    
       - Simplify/fix retpoline decoding
    
       - Add vmlinux validation
    
       - Improve documentation
    
       - Fix various bugs and apply smaller cleanups"
    
    * tag 'objtool-core-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (54 commits)
      objtool: Enable compilation of objtool for all architectures
      objtool: Move struct objtool_file into arch-independent header
      objtool: Exit successfully when requesting help
      objtool: Add check_kcov_mode() to the uaccess safelist
      samples/ftrace: Fix asm function ELF annotations
      objtool: optimize add_dead_ends for split sections
      objtool: use gelf_getsymshndx to handle >64k sections
      objtool: Allow no-op CFI ops in alternatives
      x86/retpoline: Fix retpoline unwind
      x86: Change {JMP,CALL}_NOSPEC argument
      x86: Simplify retpoline declaration
      x86/speculation: Change FILL_RETURN_BUFFER to work with objtool
      objtool: Add support for intra-function calls
      objtool: Move the IRET hack into the arch decoder
      objtool: Remove INSN_STACK
      objtool: Make handle_insn_ops() unconditional
      objtool: Rework allocating stack_ops on decode
      objtool: UNWIND_HINT_RET_OFFSET should not check registers
      objtool: is_fentry_call() crashes if call has no destination
      x86,smap: Fix smap_{save,restore}() alternatives
      ...

commit f3d44f18b0662327c42128b9d3604489bdb6e36f
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Tue May 5 15:36:17 2020 -0700

    x86/resctrl: Support CPUID enumeration of MBM counter width
    
    The original Memory Bandwidth Monitoring (MBM) architectural
    definition defines counters of up to 62 bits in the
    IA32_QM_CTR MSR while the first-generation MBM implementation
    uses statically defined 24 bit counters.
    
    Expand the MBM CPUID enumeration properties to include the MBM
    counter width. The previously undefined EAX output register contains,
    in bits [7:0], the MBM counter width encoded as an offset from
    24 bits. Enumerating this property is only specified for Intel
    CPUs.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/afa3af2f753f6bc301fb743bc8944e749cb24afa.1588715690.git.reinette.chatre@intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3bcf27caf6c9..c4e8fd709cf6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -113,9 +113,10 @@ struct cpuinfo_x86 {
 	/* in KB - valid for CPUS which support this call: */
 	unsigned int		x86_cache_size;
 	int			x86_cache_alignment;	/* In bytes */
-	/* Cache QoS architectural values: */
+	/* Cache QoS architectural values, valid only on the BSP: */
 	int			x86_cache_max_rmid;	/* max index */
 	int			x86_cache_occ_scale;	/* scale to bytes */
+	int			x86_cache_mbm_width_offset;
 	int			x86_power;
 	unsigned long		loops_per_jiffy;
 	/* cpuid returned max cores value: */

commit b746046238bb99b8f703c79f6d95357428fb6476
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Apr 2 10:15:51 2020 +0200

    objtool: Better handle IRET
    
    Teach objtool a little more about IRET so that we can avoid using the
    SAVE/RESTORE annotation. In particular, make the weird corner case in
    insn->restore go away.
    
    The purpose of that corner case is to deal with the fact that
    UNWIND_HINT_RESTORE lands on the instruction after IRET, but that
    instruction can end up being outside the basic block, consider:
    
            if (cond)
                    sync_core()
            foo();
    
    Then the hint will land on foo(), and we'll encounter the restore
    hint without ever having seen the save hint.
    
    By teaching objtool about the arch specific exception frame size, and
    assuming that any IRET in an STT_FUNC symbol is an exception frame
    sized POP, we can remove the use of save/restore hints for this code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Miroslav Benes <mbenes@suse.cz>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Link: https://lkml.kernel.org/r/20200416115118.631224674@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3bcf27caf6c9..3eeaaeb75638 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -727,7 +727,6 @@ static inline void sync_core(void)
 	unsigned int tmp;
 
 	asm volatile (
-		UNWIND_HINT_SAVE
 		"mov %%ss, %0\n\t"
 		"pushq %q0\n\t"
 		"pushq %%rsp\n\t"
@@ -737,7 +736,6 @@ static inline void sync_core(void)
 		"pushq %q0\n\t"
 		"pushq $1f\n\t"
 		"iretq\n\t"
-		UNWIND_HINT_RESTORE
 		"1:"
 		: "=&r" (tmp), ASM_CALL_CONSTRAINT : : "cc", "memory");
 #endif

commit fdf5563a720004199324371c08071b8ea27bd994
Merge: 97cddfc34549 a2150327250e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 11:04:05 2020 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "This topic tree contains more commits than usual:
    
       - most of it are uaccess cleanups/reorganization by Al
    
       - there's a bunch of prototype declaration (--Wmissing-prototypes)
         cleanups
    
       - misc other cleanups all around the map"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      x86/mm/set_memory: Fix -Wmissing-prototypes warnings
      x86/efi: Add a prototype for efi_arch_mem_reserve()
      x86/mm: Mark setup_emu2phys_nid() static
      x86/jump_label: Move 'inline' keyword placement
      x86/platform/uv: Add a missing prototype for uv_bau_message_interrupt()
      kill uaccess_try()
      x86: unsafe_put-style macro for sigmask
      x86: x32_setup_rt_frame(): consolidate uaccess areas
      x86: __setup_rt_frame(): consolidate uaccess areas
      x86: __setup_frame(): consolidate uaccess areas
      x86: setup_sigcontext(): list user_access_{begin,end}() into callers
      x86: get rid of put_user_try in __setup_rt_frame() (both 32bit and 64bit)
      x86: ia32_setup_rt_frame(): consolidate uaccess areas
      x86: ia32_setup_frame(): consolidate uaccess areas
      x86: ia32_setup_sigcontext(): lift user_access_{begin,end}() into the callers
      x86/alternatives: Mark text_poke_loc_init() static
      x86/cpu: Fix a -Wmissing-prototypes warning for init_ia32_feat_ctl()
      x86/mm: Drop pud_mknotpresent()
      x86: Replace setup_irq() by request_irq()
      x86/configs: Slightly reduce defconfigs
      ...

commit cf122cfba5b1d9daf64009d143f51dfec4b1705a
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Feb 15 21:10:25 2020 -0500

    kill uaccess_try()
    
    finally
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 09705ccc393c..19718fcfdd6a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -541,7 +541,6 @@ struct thread_struct {
 	mm_segment_t		addr_limit;
 
 	unsigned int		sig_on_uaccess_err:1;
-	unsigned int		uaccess_err:1;	/* uaccess failed */
 
 	/* Floating point and extended processor state */
 	struct fpu		fpu;

commit abc22418db02b986fc5623c035507b6357e191ed
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Mar 20 14:53:48 2020 +0000

    x86/vdso: Enable x86 to use common headers
    
    Enable x86 to use only the common headers in the implementation
    of the vDSO library.
    
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200320145351.32292-24-vincenzo.frascino@arm.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 09705ccc393c..94789db550df 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -26,6 +26,7 @@ struct vm86;
 #include <asm/fpu/types.h>
 #include <asm/unwind_hints.h>
 #include <asm/vmxfeatures.h>
+#include <asm/vdso/processor.h>
 
 #include <linux/personality.h>
 #include <linux/cache.h>
@@ -677,17 +678,6 @@ static inline unsigned int cpuid_edx(unsigned int op)
 	return edx;
 }
 
-/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
-static __always_inline void rep_nop(void)
-{
-	asm volatile("rep; nop" ::: "memory");
-}
-
-static __always_inline void cpu_relax(void)
-{
-	rep_nop();
-}
-
 /*
  * This function forces the icache and prefetched instruction stream to
  * catch up with reality in two very specific cases:

commit ccaaaf6fe5a5e1fffca5cca0f3fc4ec84d7ae752
Merge: 35c222fd3236 45fc24e89b7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 30 16:11:50 2020 -0800

    Merge tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx
    
    Pull x86 MPX removal from Dave Hansen:
     "MPX requires recompiling applications, which requires compiler
      support. Unfortunately, GCC 9.1 is expected to be be released without
      support for MPX. This means that there was only a relatively small
      window where folks could have ever used MPX. It failed to gain wide
      adoption in the industry, and Linux was the only mainstream OS to ever
      support it widely.
    
      Support for the feature may also disappear on future processors.
    
      This set completes the process that we started during the 5.4 merge
      window when the MPX prctl()s were removed. XSAVE support is left in
      place, which allows MPX-using KVM guests to continue to function"
    
    * tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx:
      x86/mpx: remove MPX from arch/x86
      mm: remove arch_bprm_mm_init() hook
      x86/mpx: remove bounds exception code
      x86/mpx: remove build infrastructure
      x86/alternatives: add missing insn.h include

commit 45fc24e89b7cc2e227b2f03d99dda0a2204bf383
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jan 23 10:41:20 2020 -0800

    x86/mpx: remove MPX from arch/x86
    
    From: Dave Hansen <dave.hansen@linux.intel.com>
    
    MPX is being removed from the kernel due to a lack of support
    in the toolchain going forward (gcc).
    
    This removes all the remaining (dead at this point) MPX handling
    code remaining in the tree.  The only remaining code is the XSAVE
    support for MPX state which is currently needd for KVM to handle
    VMs which might use MPX.
    
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: x86@kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 54f5d54280f6..4482f14dc48d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -915,24 +915,6 @@ extern int set_tsc_mode(unsigned int val);
 
 DECLARE_PER_CPU(u64, msr_misc_features_shadow);
 
-/* Register/unregister a process' MPX related resource */
-#define MPX_ENABLE_MANAGEMENT()	mpx_enable_management()
-#define MPX_DISABLE_MANAGEMENT()	mpx_disable_management()
-
-#ifdef CONFIG_X86_INTEL_MPX
-extern int mpx_enable_management(void);
-extern int mpx_disable_management(void);
-#else
-static inline int mpx_enable_management(void)
-{
-	return -EINVAL;
-}
-static inline int mpx_disable_management(void)
-{
-	return -EINVAL;
-}
-#endif /* CONFIG_X86_INTEL_MPX */
-
 #ifdef CONFIG_CPU_SUP_AMD
 extern u16 amd_get_nb_id(int cpu);
 extern u32 amd_get_nodes_per_socket(void);

commit b47ce1fed42eeb9ac8c07fcda6c795884826723d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 20 20:45:04 2019 -0800

    x86/cpu: Detect VMX features on Intel, Centaur and Zhaoxin CPUs
    
    Add an entry in struct cpuinfo_x86 to track VMX capabilities and fill
    the capabilities during IA32_FEAT_CTL MSR initialization.
    
    Make the VMX capabilities dependent on IA32_FEAT_CTL and
    X86_FEATURE_NAMES so as to avoid unnecessary overhead on CPUs that can't
    possibly support VMX, or when /proc/cpuinfo is not available.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20191221044513.21680-11-sean.j.christopherson@intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b49b88bae92f..6fb4870ed759 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -85,6 +85,9 @@ struct cpuinfo_x86 {
 #ifdef CONFIG_X86_64
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
 	int			x86_tlbsize;
+#endif
+#ifdef CONFIG_X86_VMX_FEATURE_NAMES
+	__u32			vmx_capability[NVMXINTS];
 #endif
 	__u8			x86_virt_bits;
 	__u8			x86_phys_bits;

commit 159348784ff0654291f4c7607fc55e73da8e87e8
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 20 20:45:03 2019 -0800

    x86/vmx: Introduce VMX_FEATURES_*
    
    Add a VMX-specific variant of X86_FEATURE_* flags, which will eventually
    supplant the synthetic VMX flags defined in cpufeatures word 8.  Use the
    Intel-defined layouts for the major VMX execution controls so that their
    word entries can be directly populated from their respective MSRs, and
    so that the VMX_FEATURE_* flags can be used to define the existing bit
    definitions in asm/vmx.h, i.e. force developers to define a VMX_FEATURE
    flag when adding support for a new hardware feature.
    
    The majority of Intel's (and compatible CPU's) VMX capabilities are
    enumerated via MSRs and not CPUID, i.e. querying /proc/cpuinfo doesn't
    naturally provide any insight into the virtualization capabilities of
    VMX enabled CPUs.  Commit
    
      e38e05a85828d ("x86: extended "flags" to show virtualization HW feature
                     in /proc/cpuinfo")
    
    attempted to address the issue by synthesizing select VMX features into
    a Linux-defined word in cpufeatures.
    
    Lack of reporting of VMX capabilities via /proc/cpuinfo is problematic
    because there is no sane way for a user to query the capabilities of
    their platform, e.g. when trying to find a platform to test a feature or
    debug an issue that has a hardware dependency.  Lack of reporting is
    especially problematic when the user isn't familiar with VMX, e.g. the
    format of the MSRs is non-standard, existence of some MSRs is reported
    by bits in other MSRs, several "features" from KVM's point of view are
    enumerated as 3+ distinct features by hardware, etc...
    
    The synthetic cpufeatures approach has several flaws:
    
      - The set of synthesized VMX flags has become extremely stale with
        respect to the full set of VMX features, e.g. only one new flag
        (EPT A/D) has been added in the the decade since the introduction of
        the synthetic VMX features.  Failure to keep the VMX flags up to
        date is likely due to the lack of a mechanism that forces developers
        to consider whether or not a new feature is worth reporting.
    
      - The synthetic flags may incorrectly be misinterpreted as affecting
        kernel behavior, i.e. KVM, the kernel's sole consumer of VMX,
        completely ignores the synthetic flags.
    
      - New CPU vendors that support VMX have duplicated the hideous code
        that propagates VMX features from MSRs to cpufeatures.  Bringing the
        synthetic VMX flags up to date would exacerbate the copy+paste
        trainwreck.
    
    Define separate VMX_FEATURE flags to set the stage for enumerating VMX
    capabilities outside of the cpu_has() framework, and for adding
    functional usage of VMX_FEATURE_* to help ensure the features reported
    via /proc/cpuinfo is up to date with respect to kernel recognition of
    VMX capabilities.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20191221044513.21680-10-sean.j.christopherson@intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7c071f86a058..b49b88bae92f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -25,6 +25,7 @@ struct vm86;
 #include <asm/special_insns.h>
 #include <asm/fpu/types.h>
 #include <asm/unwind_hints.h>
+#include <asm/vmxfeatures.h>
 
 #include <linux/personality.h>
 #include <linux/cache.h>

commit 72c2ce9867d9e8535f8c29eb6d842d1caad281af
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Nov 12 21:58:57 2019 +0100

    x86/bugs: Move enum taa_mitigations to bugs.c
    
    ... because it is used only there.
    
    No functional changes.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: x86@kernel.org
    Link: https://lkml.kernel.org/r/20191112221823.19677-1-bp@alien8.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0340aad3f2fc..7c071f86a058 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1015,11 +1015,4 @@ enum mds_mitigations {
 	MDS_MITIGATION_VMWERV,
 };
 
-enum taa_mitigations {
-	TAA_MITIGATION_OFF,
-	TAA_MITIGATION_UCODE_NEEDED,
-	TAA_MITIGATION_VERW,
-	TAA_MITIGATION_TSX_DISABLED,
-};
-
 #endif /* _ASM_X86_PROCESSOR_H */

commit dc4e0021b00b5a4ecba56fae509217776592b0aa
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Nov 26 18:27:16 2019 +0100

    x86/doublefault/32: Move #DF stack and TSS to cpu_entry_area
    
    There are three problems with the current layout of the doublefault
    stack and TSS.  First, the TSS is only cacheline-aligned, which is
    not enough -- if the hardware portion of the TSS (struct x86_hw_tss)
    crosses a page boundary, horrible things happen [0].  Second, the
    stack and TSS are global, so simultaneous double faults on different
    CPUs will cause massive corruption.  Third, the whole mechanism
    won't work if user CR3 is loaded, resulting in a triple fault [1].
    
    Let the doublefault stack and TSS share a page (which prevents the
    TSS from spanning a page boundary), make it percpu, and move it into
    cpu_entry_area.  Teach the stack dump code about the doublefault
    stack.
    
    [0] Real hardware will read past the end of the page onto the next
        *physical* page if a task switch happens.  Virtual machines may
        have any number of bugs, and I would consider it reasonable for
        a VM to summarily kill the guest if it tries to task-switch to
        a page-spanning TSS.
    
    [1] Real hardware triple faults.  At least some VMs seem to hang.
        I'm not sure what's going on.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f6c630097d9f..0340aad3f2fc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -166,7 +166,6 @@ enum cpuid_regs_idx {
 extern struct cpuinfo_x86	boot_cpu_data;
 extern struct cpuinfo_x86	new_cpu_data;
 
-extern struct x86_hw_tss	doublefault_tss;
 extern __u32			cpu_caps_cleared[NCAPINTS + NBUGINTS];
 extern __u32			cpu_caps_set[NCAPINTS + NBUGINTS];
 

commit 93efbde2c331004d8053f04b4bf0ca3e630b474a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Nov 20 22:12:38 2019 -0800

    x86/traps: Disentangle the 32-bit and 64-bit doublefault code
    
    The 64-bit doublefault handler is much nicer than the 32-bit one.
    As a first step toward unifying them, make the 64-bit handler
    self-contained.  This should have no effect no functional effect
    except in the odd case of x86_64 with CONFIG_DOUBLEFAULT=n in which
    case it will change the logging a bit.
    
    This also gets rid of CONFIG_DOUBLEFAULT configurability on 64-bit
    kernels.  It didn't do anything useful -- CONFIG_DOUBLEFAULT=n
    didn't actually disable doublefault handling on x86_64.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index e51afbb0cbfb..f6c630097d9f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -997,7 +997,6 @@ bool xen_set_default_idle(void);
 #endif
 
 void stop_this_cpu(void *dummy);
-void df_debug(struct pt_regs *regs, long error_code);
 void microcode_check(void);
 
 enum l1tf_mitigations {

commit 0bcd7762727dd8ba9b9b6f828e5a4cbd5da4f725
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 26 21:49:04 2019 +0100

    x86/iopl: Make 'struct tss_struct' constant size again
    
    After the following commit:
    
      05b042a19443: ("x86/pti/32: Calculate the various PTI cpu_entry_area sizes correctly, make the CPU_ENTRY_AREA_PAGES assert precise")
    
    'struct cpu_entry_area' has to be Kconfig invariant, so that we always
    have a matching CPU_ENTRY_AREA_PAGES size.
    
    This commit added a CONFIG_X86_IOPL_IOPERM dependency to tss_struct:
    
      111e7b15cf10: ("x86/ioperm: Extend IOPL config to control ioperm() as well")
    
    Which, if CONFIG_X86_IOPL_IOPERM is turned off, reduces the size of
    cpu_entry_area by two pages, triggering the assert:
    
      ./include/linux/compiler.h:391:38: error: call to ‘__compiletime_assert_202’ declared with attribute error: BUILD_BUG_ON failed: (CPU_ENTRY_AREA_PAGES+1)*PAGE_SIZE != CPU_ENTRY_AREA_MAP_SIZE
    
    Simplify the Kconfig dependencies and make cpu_entry_area constant
    size on 32-bit kernels again.
    
    Fixes: 05b042a19443: ("x86/pti/32: Calculate the various PTI cpu_entry_area sizes correctly, make the CPU_ENTRY_AREA_PAGES assert precise")
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b4e29d8b9e5a..e51afbb0cbfb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -411,9 +411,7 @@ struct tss_struct {
 	 */
 	struct x86_hw_tss	x86_tss;
 
-#ifdef CONFIG_X86_IOPL_IOPERM
 	struct x86_io_bitmap	io_bitmap;
-#endif
 } __aligned(PAGE_SIZE);
 
 DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);

commit ab851d49f6bfc781edd8bd44c72ec1e49211670b
Merge: 1d87200446f1 e3cb0c7102f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 11:12:02 2019 -0800

    Merge branch 'x86-iopl-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 iopl updates from Ingo Molnar:
     "This implements a nice simplification of the iopl and ioperm code that
      Thomas Gleixner discovered: we can implement the IO privilege features
      of the iopl system call by using the IO permission bitmap in
      permissive mode, while trapping CLI/STI/POPF/PUSHF uses in user-space
      if they change the interrupt flag.
    
      This implements that feature, with testing facilities and related
      cleanups"
    
    [ "Simplification" may be an over-statement. The main goal is to avoid
      the cli/sti of iopl by effectively implementing the IO port access
      parts of iopl in terms of ioperm.
    
      This may end up not workign well in case people actually depend on
      cli/sti being available, or if there are mixed uses of iopl and
      ioperm. We will see..       - Linus ]
    
    * 'x86-iopl-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      x86/ioperm: Fix use of deprecated config option
      x86/entry/32: Clarify register saving in __switch_to_asm()
      selftests/x86/iopl: Extend test to cover IOPL emulation
      x86/ioperm: Extend IOPL config to control ioperm() as well
      x86/iopl: Remove legacy IOPL option
      x86/iopl: Restrict iopl() permission scope
      x86/iopl: Fixup misleading comment
      selftests/x86/ioperm: Extend testing so the shared bitmap is exercised
      x86/ioperm: Share I/O bitmap if identical
      x86/ioperm: Remove bitmap if all permissions dropped
      x86/ioperm: Move TSS bitmap update to exit to user work
      x86/ioperm: Add bitmap sequence number
      x86/ioperm: Move iobitmap data into a struct
      x86/tss: Move I/O bitmap data into a seperate struct
      x86/io: Speedup schedule out of I/O bitmap user
      x86/ioperm: Avoid bitmap allocation if no permissions are set
      x86/ioperm: Simplify first ioperm() invocation logic
      x86/iopl: Cleanup include maze
      x86/tss: Fix and move VMX BUILD_BUG_ON()
      x86/cpu: Unify cpu_init()
      ...

commit 1d87200446f1d10dfe9672ca8edb027a82612f8c
Merge: 5c4a1c090d86 f01ec4fca820
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 10:42:40 2019 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Cross-arch changes to move the linker sections for NOTES and
         EXCEPTION_TABLE into the RO_DATA area, where they belong on most
         architectures. (Kees Cook)
    
       - Switch the x86 linker fill byte from x90 (NOP) to 0xcc (INT3), to
         trap jumps into the middle of those padding areas instead of
         sliding execution. (Kees Cook)
    
       - A thorough cleanup of symbol definitions within x86 assembler code.
         The rather randomly named macros got streamlined around a
         (hopefully) straightforward naming scheme:
    
            SYM_START(name, linkage, align...)
            SYM_END(name, sym_type)
    
            SYM_FUNC_START(name)
            SYM_FUNC_END(name)
    
            SYM_CODE_START(name)
            SYM_CODE_END(name)
    
            SYM_DATA_START(name)
            SYM_DATA_END(name)
    
         etc - with about three times of these basic primitives with some
         label, local symbol or attribute variant, expressed via postfixes.
    
         No change in functionality intended. (Jiri Slaby)
    
       - Misc other changes, cleanups and smaller fixes"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (67 commits)
      x86/entry/64: Remove pointless jump in paranoid_exit
      x86/entry/32: Remove unused resume_userspace label
      x86/build/vdso: Remove meaningless CFLAGS_REMOVE_*.o
      m68k: Convert missed RODATA to RO_DATA
      x86/vmlinux: Use INT3 instead of NOP for linker fill bytes
      x86/mm: Report actual image regions in /proc/iomem
      x86/mm: Report which part of kernel image is freed
      x86/mm: Remove redundant address-of operators on addresses
      xtensa: Move EXCEPTION_TABLE to RO_DATA segment
      powerpc: Move EXCEPTION_TABLE to RO_DATA segment
      parisc: Move EXCEPTION_TABLE to RO_DATA segment
      microblaze: Move EXCEPTION_TABLE to RO_DATA segment
      ia64: Move EXCEPTION_TABLE to RO_DATA segment
      h8300: Move EXCEPTION_TABLE to RO_DATA segment
      c6x: Move EXCEPTION_TABLE to RO_DATA segment
      arm64: Move EXCEPTION_TABLE to RO_DATA segment
      alpha: Move EXCEPTION_TABLE to RO_DATA segment
      x86/vmlinux: Move EXCEPTION_TABLE to RO_DATA segment
      x86/vmlinux: Actually use _etext for the end of the text segment
      vmlinux.lds.h: Allow EXCEPTION_TABLE to live in RO_DATA
      ...

commit a25bbc2644f01a9e680af4f760b54bd4834fdfec
Merge: 85fbf15bc9ac db8c33f8b5be 446e693ca30b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 08:58:08 2019 -0800

    Merge branches 'x86-cpu-for-linus' and 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu and fpu updates from Ingo Molnar:
    
     - math-emu fixes
    
     - CPUID updates
    
     - sanity-check RDRAND output to see whether the CPU at least pretends
       to produce random data
    
     - various unaligned-access across cachelines fixes in preparation of
       hardware level split-lock detection
    
     - fix MAXSMP constraints to not allow !CPUMASK_OFFSTACK kernels with
       larger than 512 NR_CPUS
    
     - misc FPU related cleanups
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpu: Align the x86_capability array to size of unsigned long
      x86/cpu: Align cpu_caps_cleared and cpu_caps_set to unsigned long
      x86/umip: Make the comments vendor-agnostic
      x86/Kconfig: Rename UMIP config parameter
      x86/Kconfig: Enforce limit of 512 CPUs with MAXSMP and no CPUMASK_OFFSTACK
      x86/cpufeatures: Add feature bit RDPRU on AMD
      x86/math-emu: Limit MATH_EMULATION to 486SX compatibles
      x86/math-emu: Check __copy_from_user() result
      x86/rdrand: Sanity-check RDRAND output
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Use XFEATURE_FP/SSE enum values instead of hardcoded numbers
      x86/fpu: Shrink space allocated for xstate_comp_offsets
      x86/fpu: Update stale variable name in comment

commit 111e7b15cf10f6e973ccf537c70c66a5de539060
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 12 21:40:33 2019 +0100

    x86/ioperm: Extend IOPL config to control ioperm() as well
    
    If iopl() is disabled, then providing ioperm() does not make much sense.
    
    Rename the config option and disable/enable both syscalls with it. Guard
    the code with #ifdefs where appropriate.
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1387d31c5e07..45f416a2c1f1 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -340,13 +340,18 @@ struct x86_hw_tss {
 	(offsetof(struct tss_struct, io_bitmap.mapall) -	\
 	 offsetof(struct tss_struct, x86_tss))
 
+#ifdef CONFIG_X86_IOPL_IOPERM
 /*
  * sizeof(unsigned long) coming from an extra "long" at the end of the
  * iobitmap. The limit is inclusive, i.e. the last valid byte.
  */
-#define __KERNEL_TSS_LIMIT	\
+# define __KERNEL_TSS_LIMIT	\
 	(IO_BITMAP_OFFSET_VALID_ALL + IO_BITMAP_BYTES + \
 	 sizeof(unsigned long) - 1)
+#else
+# define __KERNEL_TSS_LIMIT	\
+	(offsetof(struct tss_struct, x86_tss) + sizeof(struct x86_hw_tss) - 1)
+#endif
 
 /* Base offset outside of TSS_LIMIT so unpriviledged IO causes #GP */
 #define IO_BITMAP_OFFSET_INVALID	(__KERNEL_TSS_LIMIT + 1)
@@ -398,7 +403,9 @@ struct tss_struct {
 	 */
 	struct x86_hw_tss	x86_tss;
 
+#ifdef CONFIG_X86_IOPL_IOPERM
 	struct x86_io_bitmap	io_bitmap;
+#endif
 } __aligned(PAGE_SIZE);
 
 DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);

commit a24ca9976843156eabbc5f2d798954b5674d1b61
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:29 2019 +0100

    x86/iopl: Remove legacy IOPL option
    
    The IOPL emulation via the I/O bitmap is sufficient. Remove the legacy
    cruft dealing with the (e)flags based IOPL mechanism.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Juergen Gross <jgross@suse.com> (Paravirt and Xen parts)
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b0e02aa3f46a..1387d31c5e07 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -516,10 +516,10 @@ struct thread_struct {
 	struct io_bitmap	*io_bitmap;
 
 	/*
-	 * IOPL. Priviledge level dependent I/O permission which includes
-	 * user space CLI/STI when granted.
+	 * IOPL. Priviledge level dependent I/O permission which is
+	 * emulated via the I/O bitmap to prevent user space from disabling
+	 * interrupts.
 	 */
-	unsigned long		iopl;
 	unsigned long		iopl_emul;
 
 	mm_segment_t		addr_limit;
@@ -552,25 +552,6 @@ static inline void arch_thread_struct_whitelist(unsigned long *offset,
  */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
 
-/*
- * Set IOPL bits in EFLAGS from given mask
- */
-static inline void native_set_iopl_mask(unsigned mask)
-{
-#ifdef CONFIG_X86_32
-	unsigned int reg;
-
-	asm volatile ("pushfl;"
-		      "popl %0;"
-		      "andl %1, %0;"
-		      "orl %2, %0;"
-		      "pushl %0;"
-		      "popfl"
-		      : "=&r" (reg)
-		      : "i" (~X86_EFLAGS_IOPL), "r" (mask));
-#endif
-}
-
 static inline void
 native_load_sp0(unsigned long sp0)
 {
@@ -610,7 +591,6 @@ static inline void load_sp0(unsigned long sp0)
 	native_load_sp0(sp0);
 }
 
-#define set_iopl_mask native_set_iopl_mask
 #endif /* CONFIG_PARAVIRT_XXL */
 
 /* Free all resources held by a thread. */

commit c8137ace56383688af911fea5934c71ad158135e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:28 2019 +0100

    x86/iopl: Restrict iopl() permission scope
    
    The access to the full I/O port range can be also provided by the TSS I/O
    bitmap, but that would require to copy 8k of data on scheduling in the
    task. As shown with the sched out optimization TSS.io_bitmap_base can be
    used to switch the incoming task to a preallocated I/O bitmap which has all
    bits zero, i.e. allows access to all I/O ports.
    
    Implementing this allows to provide an iopl() emulation mode which restricts
    the IOPL level 3 permissions to I/O port access but removes the STI/CLI
    permission which is coming with the hardware IOPL mechansim.
    
    Provide a config option to switch IOPL to emulation mode, make it the
    default and while at it also provide an option to disable IOPL completely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 40bb0f7bca3f..b0e02aa3f46a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -332,19 +332,21 @@ struct x86_hw_tss {
 #define IO_BITMAP_BYTES			(IO_BITMAP_BITS / BITS_PER_BYTE)
 #define IO_BITMAP_LONGS			(IO_BITMAP_BYTES / sizeof(long))
 
-#define IO_BITMAP_OFFSET_VALID					\
+#define IO_BITMAP_OFFSET_VALID_MAP				\
 	(offsetof(struct tss_struct, io_bitmap.bitmap) -	\
 	 offsetof(struct tss_struct, x86_tss))
 
+#define IO_BITMAP_OFFSET_VALID_ALL				\
+	(offsetof(struct tss_struct, io_bitmap.mapall) -	\
+	 offsetof(struct tss_struct, x86_tss))
+
 /*
- * sizeof(unsigned long) coming from an extra "long" at the end
- * of the iobitmap.
- *
- * -1? seg base+limit should be pointing to the address of the
- * last valid byte
+ * sizeof(unsigned long) coming from an extra "long" at the end of the
+ * iobitmap. The limit is inclusive, i.e. the last valid byte.
  */
 #define __KERNEL_TSS_LIMIT	\
-	(IO_BITMAP_OFFSET_VALID + IO_BITMAP_BYTES + sizeof(unsigned long) - 1)
+	(IO_BITMAP_OFFSET_VALID_ALL + IO_BITMAP_BYTES + \
+	 sizeof(unsigned long) - 1)
 
 /* Base offset outside of TSS_LIMIT so unpriviledged IO causes #GP */
 #define IO_BITMAP_OFFSET_INVALID	(__KERNEL_TSS_LIMIT + 1)
@@ -380,6 +382,12 @@ struct x86_io_bitmap {
 	 * be within the limit.
 	 */
 	unsigned long		bitmap[IO_BITMAP_LONGS + 1];
+
+	/*
+	 * Special I/O bitmap to emulate IOPL(3). All bytes zero,
+	 * except the additional byte at the end.
+	 */
+	unsigned long		mapall[IO_BITMAP_LONGS + 1];
 };
 
 struct tss_struct {
@@ -506,7 +514,13 @@ struct thread_struct {
 #endif
 	/* IO permissions: */
 	struct io_bitmap	*io_bitmap;
+
+	/*
+	 * IOPL. Priviledge level dependent I/O permission which includes
+	 * user space CLI/STI when granted.
+	 */
 	unsigned long		iopl;
+	unsigned long		iopl_emul;
 
 	mm_segment_t		addr_limit;
 

commit 060aa16fdb7c5078a4159a76e5dc87d6a493af9b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:22 2019 +0100

    x86/ioperm: Add bitmap sequence number
    
    Add a globally unique sequence number which is incremented when ioperm() is
    changing the I/O bitmap of a task. Store the new sequence number in the
    io_bitmap structure and compare it with the sequence number of the I/O
    bitmap which was last loaded on a CPU. Only update the bitmap if the
    sequence is different.
    
    That should further reduce the overhead of I/O bitmap scheduling when there
    are only a few I/O bitmap users on the system.
    
    The 64bit sequence counter is sufficient. A wraparound of the sequence
    counter assuming an ioperm() call every nanosecond would require about 584
    years of uptime.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c949e0e5cbe6..40bb0f7bca3f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -361,6 +361,9 @@ struct entry_stack_page {
  * All IO bitmap related data stored in the TSS:
  */
 struct x86_io_bitmap {
+	/* The sequence number of the last active bitmap. */
+	u64			prev_sequence;
+
 	/*
 	 * Store the dirty size of the last io bitmap offender. The next
 	 * one will have to do the cleanup as the switch out to a non io

commit 577d5cd7e5851d3832066cd0422475fa7db2ee17
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:21 2019 +0100

    x86/ioperm: Move iobitmap data into a struct
    
    No point in having all the data in thread_struct, especially as upcoming
    changes add more.
    
    Make the bitmap in the new struct accessible as array of longs and as array
    of characters via a union, so both the bitmap functions and the update
    logic can avoid type casts.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cd7cd7d10b81..c949e0e5cbe6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -7,6 +7,7 @@
 /* Forward declaration, a strange C thing */
 struct task_struct;
 struct mm_struct;
+struct io_bitmap;
 struct vm86;
 
 #include <asm/math_emu.h>
@@ -501,10 +502,8 @@ struct thread_struct {
 	struct vm86		*vm86;
 #endif
 	/* IO permissions: */
-	unsigned long		*io_bitmap_ptr;
+	struct io_bitmap	*io_bitmap;
 	unsigned long		iopl;
-	/* Max allowed port in the bitmap, in bytes: */
-	unsigned		io_bitmap_max;
 
 	mm_segment_t		addr_limit;
 
@@ -862,7 +861,6 @@ static inline void spin_lock_prefetch(const void *x)
 #define INIT_THREAD  {							  \
 	.sp0			= TOP_OF_INIT_STACK,			  \
 	.sysenter_cs		= __KERNEL_CS,				  \
-	.io_bitmap_ptr		= NULL,					  \
 	.addr_limit		= KERNEL_DS,				  \
 }
 

commit f5848e5fd2f813c3a8009a642dfbcf635287c199
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 12 18:45:29 2019 +0100

    x86/tss: Move I/O bitmap data into a seperate struct
    
    Move the non hardware portion of I/O bitmap data into a seperate struct for
    readability sake.
    
    Originally-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6d0059c21969..cd7cd7d10b81 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -328,11 +328,11 @@ struct x86_hw_tss {
  * IO-bitmap sizes:
  */
 #define IO_BITMAP_BITS			65536
-#define IO_BITMAP_BYTES			(IO_BITMAP_BITS/8)
-#define IO_BITMAP_LONGS			(IO_BITMAP_BYTES/sizeof(long))
+#define IO_BITMAP_BYTES			(IO_BITMAP_BITS / BITS_PER_BYTE)
+#define IO_BITMAP_LONGS			(IO_BITMAP_BYTES / sizeof(long))
 
-#define IO_BITMAP_OFFSET_VALID				\
-	(offsetof(struct tss_struct, io_bitmap) -	\
+#define IO_BITMAP_OFFSET_VALID					\
+	(offsetof(struct tss_struct, io_bitmap.bitmap) -	\
 	 offsetof(struct tss_struct, x86_tss))
 
 /*
@@ -356,14 +356,10 @@ struct entry_stack_page {
 	struct entry_stack stack;
 } __aligned(PAGE_SIZE);
 
-struct tss_struct {
-	/*
-	 * The fixed hardware portion.  This must not cross a page boundary
-	 * at risk of violating the SDM's advice and potentially triggering
-	 * errata.
-	 */
-	struct x86_hw_tss	x86_tss;
-
+/*
+ * All IO bitmap related data stored in the TSS:
+ */
+struct x86_io_bitmap {
 	/*
 	 * Store the dirty size of the last io bitmap offender. The next
 	 * one will have to do the cleanup as the switch out to a non io
@@ -371,7 +367,7 @@ struct tss_struct {
 	 * outside of the TSS limit. So for sane tasks there is no need to
 	 * actually touch the io_bitmap at all.
 	 */
-	unsigned int		io_bitmap_prev_max;
+	unsigned int		prev_max;
 
 	/*
 	 * The extra 1 is there because the CPU will access an
@@ -379,7 +375,18 @@ struct tss_struct {
 	 * bitmap. The extra byte must be all 1 bits, and must
 	 * be within the limit.
 	 */
-	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
+	unsigned long		bitmap[IO_BITMAP_LONGS + 1];
+};
+
+struct tss_struct {
+	/*
+	 * The fixed hardware portion.  This must not cross a page boundary
+	 * at risk of violating the SDM's advice and potentially triggering
+	 * errata.
+	 */
+	struct x86_hw_tss	x86_tss;
+
+	struct x86_io_bitmap	io_bitmap;
 } __aligned(PAGE_SIZE);
 
 DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);

commit ecc7e37d4dadd16f6be125ca496feccd05454da4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:20 2019 +0100

    x86/io: Speedup schedule out of I/O bitmap user
    
    There is no requirement to update the TSS I/O bitmap when a thread using it is
    scheduled out and the incoming thread does not use it.
    
    For the permission check based on the TSS I/O bitmap the CPU calculates the memory
    location of the I/O bitmap by the address of the TSS and the io_bitmap_base member
    of the tss_struct. The easiest way to invalidate the I/O bitmap is to switch the
    offset to an address outside of the TSS limit.
    
    If an I/O instruction is issued from user space the TSS limit causes #GP to be
    raised in the same was as valid I/O bitmap with all bits set to 1 would do.
    
    This removes the extra work when an I/O bitmap using task is scheduled out
    and puts the burden on the rare I/O bitmap users when they are scheduled
    in.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6e0a3b43d027..6d0059c21969 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -330,8 +330,23 @@ struct x86_hw_tss {
 #define IO_BITMAP_BITS			65536
 #define IO_BITMAP_BYTES			(IO_BITMAP_BITS/8)
 #define IO_BITMAP_LONGS			(IO_BITMAP_BYTES/sizeof(long))
-#define IO_BITMAP_OFFSET		(offsetof(struct tss_struct, io_bitmap) - offsetof(struct tss_struct, x86_tss))
-#define INVALID_IO_BITMAP_OFFSET	0x8000
+
+#define IO_BITMAP_OFFSET_VALID				\
+	(offsetof(struct tss_struct, io_bitmap) -	\
+	 offsetof(struct tss_struct, x86_tss))
+
+/*
+ * sizeof(unsigned long) coming from an extra "long" at the end
+ * of the iobitmap.
+ *
+ * -1? seg base+limit should be pointing to the address of the
+ * last valid byte
+ */
+#define __KERNEL_TSS_LIMIT	\
+	(IO_BITMAP_OFFSET_VALID + IO_BITMAP_BYTES + sizeof(unsigned long) - 1)
+
+/* Base offset outside of TSS_LIMIT so unpriviledged IO causes #GP */
+#define IO_BITMAP_OFFSET_INVALID	(__KERNEL_TSS_LIMIT + 1)
 
 struct entry_stack {
 	unsigned long		words[64];
@@ -349,6 +364,15 @@ struct tss_struct {
 	 */
 	struct x86_hw_tss	x86_tss;
 
+	/*
+	 * Store the dirty size of the last io bitmap offender. The next
+	 * one will have to do the cleanup as the switch out to a non io
+	 * bitmap user will just set x86_tss.io_bitmap_base to a value
+	 * outside of the TSS limit. So for sane tasks there is no need to
+	 * actually touch the io_bitmap at all.
+	 */
+	unsigned int		io_bitmap_prev_max;
+
 	/*
 	 * The extra 1 is there because the CPU will access an
 	 * additional byte beyond the end of the IO permission
@@ -360,16 +384,6 @@ struct tss_struct {
 
 DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);
 
-/*
- * sizeof(unsigned long) coming from an extra "long" at the end
- * of the iobitmap.
- *
- * -1? seg base+limit should be pointing to the address of the
- * last valid byte
- */
-#define __KERNEL_TSS_LIMIT	\
-	(IO_BITMAP_OFFSET + IO_BITMAP_BYTES + sizeof(unsigned long) - 1)
-
 /* Per CPU interrupt stacks */
 struct irq_stack {
 	char		stack[IRQ_STACK_SIZE];

commit db8c33f8b5bea59d00ca12dcd6b65d01b1ea98ef
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Mon Sep 16 15:39:58 2019 -0700

    x86/cpu: Align the x86_capability array to size of unsigned long
    
    The x86_capability array in cpuinfo_x86 is of type u32 and thus is
    naturally aligned to 4 bytes. But, set_bit() and clear_bit() require the
    array to be aligned to size of unsigned long (i.e. 8 bytes on 64-bit
    systems).
    
    The array pointer is handed into atomic bit operations. If the access is
    not aligned to unsigned long then the atomic bit operations can end up
    crossing a cache line boundary, which causes the CPU to do a full bus lock
    as it can't lock both cache lines at once. The bus lock operation is heavy
    weight and can cause severe performance degradation.
    
    The upcoming #AC split lock detection mechanism will issue warnings for
    this kind of access.
    
    Force the alignment of the array to unsigned long. This avoids the massive
    code changes which would be required when converting the array data type to
    unsigned long.
    
    [ tglx: Rewrote changelog so it contains information WHY this is required ]
    
    Suggested-by: David Laight <David.Laight@aculab.com>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190916223958.27048-4-tony.luck@intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6e0a3b43d027..c073534ca485 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -93,7 +93,15 @@ struct cpuinfo_x86 {
 	__u32			extended_cpuid_level;
 	/* Maximum supported CPUID level, -1=no CPUID: */
 	int			cpuid_level;
-	__u32			x86_capability[NCAPINTS + NBUGINTS];
+	/*
+	 * Align to size of unsigned long because the x86_capability array
+	 * is passed to bitops which require the alignment. Use unnamed
+	 * union to enforce the array is aligned to size of unsigned long.
+	 */
+	union {
+		__u32		x86_capability[NCAPINTS + NBUGINTS];
+		unsigned long	x86_capability_alignment;
+	};
 	char			x86_vendor_id[16];
 	char			x86_model_id[64];
 	/* in KB - valid for CPUS which support this call: */

commit 5494c3a6a0b965906ffdcb620d94079ea4cb69ea
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:49 2019 -0700

    x86/mm: Report which part of kernel image is freed
    
    The memory freeing report wasn't very useful for figuring out which
    parts of the kernel image were being freed. Add the details for clearer
    reporting in dmesg.
    
    Before:
    
      Freeing unused kernel image memory: 1348K
      Write protecting the kernel read-only data: 20480k
      Freeing unused kernel image memory: 2040K
      Freeing unused kernel image memory: 172K
    
    After:
    
      Freeing unused kernel image (initmem) memory: 1348K
      Write protecting the kernel read-only data: 20480k
      Freeing unused kernel image (text/rodata gap) memory: 2040K
      Freeing unused kernel image (rodata/data gap) memory: 172K
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-28-keescook@chromium.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6e0a3b43d027..790f250d39a8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -958,7 +958,7 @@ static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
 
 extern unsigned long arch_align_stack(unsigned long sp);
 void free_init_pages(const char *what, unsigned long begin, unsigned long end);
-extern void free_kernel_image_pages(void *begin, void *end);
+extern void free_kernel_image_pages(const char *what, void *begin, void *end);
 
 void default_idle(void);
 #ifdef	CONFIG_XEN

commit 1b42f017415b46c317e71d41c34ec088417a1883
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 11:30:45 2019 +0200

    x86/speculation/taa: Add mitigation for TSX Async Abort
    
    TSX Async Abort (TAA) is a side channel vulnerability to the internal
    buffers in some Intel processors similar to Microachitectural Data
    Sampling (MDS). In this case, certain loads may speculatively pass
    invalid data to dependent operations when an asynchronous abort
    condition is pending in a TSX transaction.
    
    This includes loads with no fault or assist condition. Such loads may
    speculatively expose stale data from the uarch data structures as in
    MDS. Scope of exposure is within the same-thread and cross-thread. This
    issue affects all current processors that support TSX, but do not have
    ARCH_CAP_TAA_NO (bit 8) set in MSR_IA32_ARCH_CAPABILITIES.
    
    On CPUs which have their IA32_ARCH_CAPABILITIES MSR bit MDS_NO=0,
    CPUID.MD_CLEAR=1 and the MDS mitigation is clearing the CPU buffers
    using VERW or L1D_FLUSH, there is no additional mitigation needed for
    TAA. On affected CPUs with MDS_NO=1 this issue can be mitigated by
    disabling the Transactional Synchronization Extensions (TSX) feature.
    
    A new MSR IA32_TSX_CTRL in future and current processors after a
    microcode update can be used to control the TSX feature. There are two
    bits in that MSR:
    
    * TSX_CTRL_RTM_DISABLE disables the TSX sub-feature Restricted
    Transactional Memory (RTM).
    
    * TSX_CTRL_CPUID_CLEAR clears the RTM enumeration in CPUID. The other
    TSX sub-feature, Hardware Lock Elision (HLE), is unconditionally
    disabled with updated microcode but still enumerated as present by
    CPUID(EAX=7).EBX{bit4}.
    
    The second mitigation approach is similar to MDS which is clearing the
    affected CPU buffers on return to user space and when entering a guest.
    Relevant microcode update is required for the mitigation to work.  More
    details on this approach can be found here:
    
      https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html
    
    The TSX feature can be controlled by the "tsx" command line parameter.
    If it is force-enabled then "Clear CPU buffers" (MDS mitigation) is
    deployed. The effective mitigation state can be read from sysfs.
    
     [ bp:
       - massage + comments cleanup
       - s/TAA_MITIGATION_TSX_DISABLE/TAA_MITIGATION_TSX_DISABLED/g - Josh.
       - remove partial TAA mitigation in update_mds_branch_idle() - Josh.
       - s/tsx_async_abort_cmdline/tsx_async_abort_parse_cmdline/g
     ]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6e0a3b43d027..54f5d54280f6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -988,4 +988,11 @@ enum mds_mitigations {
 	MDS_MITIGATION_VMWERV,
 };
 
+enum taa_mitigations {
+	TAA_MITIGATION_OFF,
+	TAA_MITIGATION_UCODE_NEEDED,
+	TAA_MITIGATION_VERW,
+	TAA_MITIGATION_TSX_DISABLED,
+};
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit 7652ac92018536eb807b6c2130100c85f1ba7e3b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 10 21:42:46 2019 +0200

    x86/asm: Move native_write_cr0/4() out of line
    
    The pinning of sensitive CR0 and CR4 bits caused a boot crash when loading
    the kvm_intel module on a kernel compiled with CONFIG_PARAVIRT=n.
    
    The reason is that the static key which controls the pinning is marked RO
    after init. The kvm_intel module contains a CR4 write which requires to
    update the static key entry list. That obviously does not work when the key
    is in a RO section.
    
    With CONFIG_PARAVIRT enabled this does not happen because the CR4 write
    uses the paravirt indirection and the actual write function is built in.
    
    As the key is intended to be immutable after init, move
    native_write_cr0/4() out of line.
    
    While at it consolidate the update of the cr4 shadow variable and store the
    value right away when the pinning is initialized on a booting CPU. No point
    in reading it back 20 instructions later. This allows to confine the static
    key and the pinning variable to cpu/common and allows to mark them static.
    
    Fixes: 8dbec27a242c ("x86/asm: Pin sensitive CR0 bits")
    Fixes: 873d50d58f67 ("x86/asm: Pin sensitive CR4 bits")
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Xi Ruoyao <xry111@mengyan1223.wang>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Xi Ruoyao <xry111@mengyan1223.wang>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1907102140340.1758@nanos.tec.linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3eab6ece52b4..6e0a3b43d027 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -741,6 +741,7 @@ extern void load_direct_gdt(int);
 extern void load_fixmap_gdt(int);
 extern void load_percpu_segment(int);
 extern void cpu_init(void);
+extern void cr4_init(void);
 
 static inline unsigned long get_debugctlmsr(void)
 {

commit 222a21d29521d144f3dd7a0bc4d4020e448f0126
Merge: 8faef7125d02 eb876fbc248e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 18:28:44 2019 -0700

    Merge branch 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 topology updates from Ingo Molnar:
     "Implement multi-die topology support on Intel CPUs and expose the die
      topology to user-space tooling, by Len Brown, Kan Liang and Zhang Rui.
    
      These changes should have no effect on the kernel's existing
      understanding of topologies, i.e. there should be no behavioral impact
      on cache, NUMA, scheduler, perf and other topologies and overall
      system performance"
    
    * 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86/intel/rapl: Cosmetic rename internal variables in response to multi-die/pkg support
      perf/x86/intel/uncore: Cosmetic renames in response to multi-die/pkg support
      hwmon/coretemp: Cosmetic: Rename internal variables to zones from packages
      thermal/x86_pkg_temp_thermal: Cosmetic: Rename internal variables to zones from packages
      perf/x86/intel/cstate: Support multi-die/package
      perf/x86/intel/rapl: Support multi-die/package
      perf/x86/intel/uncore: Support multi-die/package
      topology: Create core_cpus and die_cpus sysfs attributes
      topology: Create package_cpus sysfs attribute
      hwmon/coretemp: Support multi-die/package
      powercap/intel_rapl: Update RAPL domain name and debug messages
      thermal/x86_pkg_temp_thermal: Support multi-die/package
      powercap/intel_rapl: Support multi-die/package
      powercap/intel_rapl: Simplify rapl_find_package()
      x86/topology: Define topology_logical_die_id()
      x86/topology: Define topology_die_id()
      cpu/topology: Export die_id
      x86/topology: Create topology_max_die_per_package()
      x86/topology: Add CPUID.1F multi-die/package support

commit 761fdd5e3327db6c646a09bab5ad48cd42680cd2
Author: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
Date:   Tue Jun 18 08:37:05 2019 +0000

    x86/cpu: Create Zhaoxin processors architecture support file
    
    Add x86 architecture support for new Zhaoxin processors.
    Carve out initialization code needed by Zhaoxin processors into
    a separate compilation unit.
    
    To identify Zhaoxin CPU, add a new vendor type X86_VENDOR_ZHAOXIN
    for system recognition.
    
    Signed-off-by: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "hpa@zytor.com" <hpa@zytor.com>
    Cc: "gregkh@linuxfoundation.org" <gregkh@linuxfoundation.org>
    Cc: "rjw@rjwysocki.net" <rjw@rjwysocki.net>
    Cc: "lenb@kernel.org" <lenb@kernel.org>
    Cc: David Wang <DavidWang@zhaoxin.com>
    Cc: "Cooper Yan(BJ-RD)" <CooperYan@zhaoxin.com>
    Cc: "Qiyuan Wang(BJ-RD)" <QiyuanWang@zhaoxin.com>
    Cc: "Herry Yang(BJ-RD)" <HerryYang@zhaoxin.com>
    Link: https://lkml.kernel.org/r/01042674b2f741b2aed1f797359bdffb@zhaoxin.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c34a35c78618..e57d2ca2ed87 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -144,7 +144,8 @@ enum cpuid_regs_idx {
 #define X86_VENDOR_TRANSMETA	7
 #define X86_VENDOR_NSC		8
 #define X86_VENDOR_HYGON	9
-#define X86_VENDOR_NUM		10
+#define X86_VENDOR_ZHAOXIN	10
+#define X86_VENDOR_NUM		11
 
 #define X86_VENDOR_UNKNOWN	0xff
 

commit 212bf4fdb7f9eeeb99afd97ebad677d43e7b55ac
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:49 2019 -0400

    x86/topology: Define topology_logical_die_id()
    
    Define topology_logical_die_id() ala existing topology_logical_package_id()
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Zhang Rui <rui.zhang@intel.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/2f3526e25ae14fbeff26fb26e877d159df8946d9.1557769318.git.len.brown@intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7c17343946dd..6aba36bde57f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -118,6 +118,7 @@ struct cpuinfo_x86 {
 	/* Core id: */
 	u16			cpu_core_id;
 	u16			cpu_die_id;
+	u16			logical_die_id;
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 	u32			microcode;

commit 14d96d6c06b5d8116b8d52c9c5530f5528ef1e61
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:46 2019 -0400

    x86/topology: Create topology_max_die_per_package()
    
    topology_max_packages() is available to size resources to cover all
    packages in the system.
    
    But now multi-die/package systems are coming up, and some resources are
    per-die.
    
    Create topology_max_die_per_package(), for detecting multi-die/package
    systems, and sizing any per-die resources.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/e6eaf384571ae52ac7d0ca41510b7fb7d2fda0e4.1557769318.git.len.brown@intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ef0a44fccaec..7c17343946dd 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -106,7 +106,6 @@ struct cpuinfo_x86 {
 	unsigned long		loops_per_jiffy;
 	/* cpuid returned max cores value: */
 	u16			x86_max_cores;
-	u16			x86_max_dies;
 	u16			apicid;
 	u16			initial_apicid;
 	u16			x86_clflush_size;

commit 7745f03eb39587dd15a1fb26e6223678b8e906d2
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:45 2019 -0400

    x86/topology: Add CPUID.1F multi-die/package support
    
    Some new systems have multiple software-visible die within each package.
    
    Update Linux parsing of the Intel CPUID "Extended Topology Leaf" to handle
    either CPUID.B, or the new CPUID.1F.
    
    Add cpuinfo_x86.die_id and cpuinfo_x86.max_dies to store the result.
    
    die_id will be non-zero only for multi-die/package systems.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-doc@vger.kernel.org
    Link: https://lkml.kernel.org/r/7b23d2d26d717b8e14ba137c94b70943f1ae4b5c.1557769318.git.len.brown@intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c34a35c78618..ef0a44fccaec 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -105,7 +105,8 @@ struct cpuinfo_x86 {
 	int			x86_power;
 	unsigned long		loops_per_jiffy;
 	/* cpuid returned max cores value: */
-	u16			 x86_max_cores;
+	u16			x86_max_cores;
+	u16			x86_max_dies;
 	u16			apicid;
 	u16			initial_apicid;
 	u16			x86_clflush_size;
@@ -117,6 +118,7 @@ struct cpuinfo_x86 {
 	u16			logical_proc_id;
 	/* Core id: */
 	u16			cpu_core_id;
+	u16			cpu_die_id;
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 	u32			microcode;

commit fa4bff165070dc40a3de35b78e4f8da8e8d85ec5
Merge: 63863ee8e2f6 95310e348a32
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 14 07:57:29 2019 -0700

    Merge branch 'x86-mds-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 MDS mitigations from Thomas Gleixner:
     "Microarchitectural Data Sampling (MDS) is a hardware vulnerability
      which allows unprivileged speculative access to data which is
      available in various CPU internal buffers. This new set of misfeatures
      has the following CVEs assigned:
    
         CVE-2018-12126  MSBDS  Microarchitectural Store Buffer Data Sampling
         CVE-2018-12130  MFBDS  Microarchitectural Fill Buffer Data Sampling
         CVE-2018-12127  MLPDS  Microarchitectural Load Port Data Sampling
         CVE-2019-11091  MDSUM  Microarchitectural Data Sampling Uncacheable Memory
    
      MDS attacks target microarchitectural buffers which speculatively
      forward data under certain conditions. Disclosure gadgets can expose
      this data via cache side channels.
    
      Contrary to other speculation based vulnerabilities the MDS
      vulnerability does not allow the attacker to control the memory target
      address. As a consequence the attacks are purely sampling based, but
      as demonstrated with the TLBleed attack samples can be postprocessed
      successfully.
    
      The mitigation is to flush the microarchitectural buffers on return to
      user space and before entering a VM. It's bolted on the VERW
      instruction and requires a microcode update. As some of the attacks
      exploit data structures shared between hyperthreads, full protection
      requires to disable hyperthreading. The kernel does not do that by
      default to avoid breaking unattended updates.
    
      The mitigation set comes with documentation for administrators and a
      deeper technical view"
    
    * 'x86-mds-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/speculation/mds: Fix documentation typo
      Documentation: Correct the possible MDS sysfs values
      x86/mds: Add MDSUM variant to the MDS documentation
      x86/speculation/mds: Add 'mitigations=' support for MDS
      x86/speculation/mds: Print SMT vulnerable on MSBDS with mitigations off
      x86/speculation/mds: Fix comment
      x86/speculation/mds: Add SMT warning message
      x86/speculation: Move arch_smt_update() call to after mitigation decisions
      x86/speculation/mds: Add mds=full,nosmt cmdline option
      Documentation: Add MDS vulnerability documentation
      Documentation: Move L1TF to separate directory
      x86/speculation/mds: Add mitigation mode VMWERV
      x86/speculation/mds: Add sysfs reporting for MDS
      x86/speculation/mds: Add mitigation control for MDS
      x86/speculation/mds: Conditionally clear CPU buffers on idle entry
      x86/kvm/vmx: Add MDS protection when L1D Flush is not active
      x86/speculation/mds: Clear CPU buffers on exit to user
      x86/speculation/mds: Add mds_clear_cpu_buffers()
      x86/kvm: Expose X86_FEATURE_MD_CLEAR to guests
      x86/speculation/mds: Add BUG_MSBDS_ONLY
      ...

commit e6401c13093173aad709a5c6de00cf8d692ee786
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Apr 14 18:00:06 2019 +0200

    x86/irq/64: Split the IRQ stack into its own pages
    
    Currently, the IRQ stack is hardcoded as the first page of the percpu
    area, and the stack canary lives on the IRQ stack. The former gets in
    the way of adding an IRQ stack guard page, and the latter is a potential
    weakness in the stack canary mechanism.
    
    Split the IRQ stack into its own private percpu pages.
    
    [ tglx: Make 64 and 32 bit share struct irq_stack ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: Feng Tang <feng.tang@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jordan Borgner <mail@jordan-borgner.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Maran Wilson <maran.wilson@oracle.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: "Rafael Ávila de Espíndola" <rafael@espindo.la>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Cc: xen-devel@lists.xenproject.org
    Link: https://lkml.kernel.org/r/20190414160146.267376656@linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 5e3dd4e2136d..7e99ef67bff0 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -367,6 +367,13 @@ DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);
 #define __KERNEL_TSS_LIMIT	\
 	(IO_BITMAP_OFFSET + IO_BITMAP_BYTES + sizeof(unsigned long) - 1)
 
+/* Per CPU interrupt stacks */
+struct irq_stack {
+	char		stack[IRQ_STACK_SIZE];
+} __aligned(IRQ_STACK_SIZE);
+
+DECLARE_PER_CPU(struct irq_stack *, hardirq_stack_ptr);
+
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
 #else
@@ -375,28 +382,24 @@ DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
 #endif
 
 #ifdef CONFIG_X86_64
-union irq_stack_union {
-	char irq_stack[IRQ_STACK_SIZE];
+struct fixed_percpu_data {
 	/*
 	 * GCC hardcodes the stack canary as %gs:40.  Since the
 	 * irq_stack is the object at %gs:0, we reserve the bottom
 	 * 48 bytes of the irq stack for the canary.
 	 */
-	struct {
-		char gs_base[40];
-		unsigned long stack_canary;
-	};
+	char		gs_base[40];
+	unsigned long	stack_canary;
 };
 
-DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union) __visible;
-DECLARE_INIT_PER_CPU(irq_stack_union);
+DECLARE_PER_CPU_FIRST(struct fixed_percpu_data, fixed_percpu_data) __visible;
+DECLARE_INIT_PER_CPU(fixed_percpu_data);
 
 static inline unsigned long cpu_kernelmode_gs_base(int cpu)
 {
-	return (unsigned long)per_cpu(irq_stack_union.gs_base, cpu);
+	return (unsigned long)per_cpu(fixed_percpu_data.gs_base, cpu);
 }
 
-DECLARE_PER_CPU(char *, hardirq_stack_ptr);
 DECLARE_PER_CPU(unsigned int, irq_count);
 extern asmlinkage void ignore_sysret(void);
 
@@ -418,14 +421,7 @@ struct stack_canary {
 };
 DECLARE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif
-/*
- * per-CPU IRQ handling stacks
- */
-struct irq_stack {
-	char			stack[IRQ_STACK_SIZE];
-} __aligned(IRQ_STACK_SIZE);
-
-DECLARE_PER_CPU(struct irq_stack *, hardirq_stack_ptr);
+/* Per CPU softirq stack pointer */
 DECLARE_PER_CPU(struct irq_stack *, softirq_stack_ptr);
 #endif	/* X86_64 */
 

commit 758a2e312228410f2f5092ade558109e93dc3ee8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:02 2019 +0200

    x86/irq/64: Rename irq_stack_ptr to hardirq_stack_ptr
    
    Preparatory patch to share code with 32bit.
    
    No functional changes.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.912584074@linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0b7d866a1ad5..5e3dd4e2136d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -396,7 +396,7 @@ static inline unsigned long cpu_kernelmode_gs_base(int cpu)
 	return (unsigned long)per_cpu(irq_stack_union.gs_base, cpu);
 }
 
-DECLARE_PER_CPU(char *, irq_stack_ptr);
+DECLARE_PER_CPU(char *, hardirq_stack_ptr);
 DECLARE_PER_CPU(unsigned int, irq_count);
 extern asmlinkage void ignore_sysret(void);
 

commit a754fe2b76d1d6bce7069657bba975034f3ad961
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:01 2019 +0200

    x86/irq/32: Rename hard/softirq_stack to hard/softirq_stack_ptr
    
    The percpu storage holds a pointer to the stack not the stack
    itself. Rename it before sharing struct irq_stack with 64-bit.
    
    No functional changes.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.824805922@linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ff3f469ab2d4..0b7d866a1ad5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -425,8 +425,8 @@ struct irq_stack {
 	char			stack[IRQ_STACK_SIZE];
 } __aligned(IRQ_STACK_SIZE);
 
-DECLARE_PER_CPU(struct irq_stack *, hardirq_stack);
-DECLARE_PER_CPU(struct irq_stack *, softirq_stack);
+DECLARE_PER_CPU(struct irq_stack *, hardirq_stack_ptr);
+DECLARE_PER_CPU(struct irq_stack *, softirq_stack_ptr);
 #endif	/* X86_64 */
 
 extern unsigned int fpu_kernel_xstate_size;

commit 231c4846b106d526fa212b02b37447d3f2fcc99d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:00 2019 +0200

    x86/irq/32: Make irq stack a character array
    
    There is no reason to have an u32 array in struct irq_stack. The only
    purpose of the array is to size the struct properly.
    
    Preparatory change for sharing struct irq_stack with 64-bit.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.736241969@linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 8b4bf732e1b5..ff3f469ab2d4 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -422,7 +422,7 @@ DECLARE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
  * per-CPU IRQ handling stacks
  */
 struct irq_stack {
-	u32			stack[IRQ_STACK_SIZE / sizeof(u32)];
+	char			stack[IRQ_STACK_SIZE];
 } __aligned(IRQ_STACK_SIZE);
 
 DECLARE_PER_CPU(struct irq_stack *, hardirq_stack);

commit aa641c287b2f7676f6f0064a8351daf08eca6b0a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:59 2019 +0200

    x86/irq/32: Define IRQ_STACK_SIZE
    
    On 32-bit IRQ_STACK_SIZE is the same as THREAD_SIZE.
    
    To allow sharing struct irq_stack with 32-bit, define IRQ_STACK_SIZE for
    32-bit and use it for struct irq_stack.
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.632513987@linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 8fcfcd1a8375..8b4bf732e1b5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -422,8 +422,8 @@ DECLARE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
  * per-CPU IRQ handling stacks
  */
 struct irq_stack {
-	u32                     stack[THREAD_SIZE/sizeof(u32)];
-} __aligned(THREAD_SIZE);
+	u32			stack[IRQ_STACK_SIZE / sizeof(u32)];
+} __aligned(IRQ_STACK_SIZE);
 
 DECLARE_PER_CPU(struct irq_stack *, hardirq_stack);
 DECLARE_PER_CPU(struct irq_stack *, softirq_stack);

commit 4d68c3d0ecd5fcba8876e8a58ac41ffb360de43e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:54 2019 +0200

    x86/cpu: Remove orig_ist array
    
    All users gone.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.151435667@linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 2bb3a648fc12..8fcfcd1a8375 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -374,16 +374,7 @@ DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
 #define cpu_current_top_of_stack cpu_tss_rw.x86_tss.sp1
 #endif
 
-/*
- * Save the original ist values for checking stack pointers during debugging
- */
-struct orig_ist {
-	unsigned long		ist[7];
-};
-
 #ifdef CONFIG_X86_64
-DECLARE_PER_CPU(struct orig_ist, orig_ist);
-
 union irq_stack_union {
 	char irq_stack[IRQ_STACK_SIZE];
 	/*

commit 22dd8365088b6403630b82423cf906491859b65e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 20 09:40:40 2019 +0100

    x86/speculation/mds: Add mitigation mode VMWERV
    
    In virtualized environments it can happen that the host has the microcode
    update which utilizes the VERW instruction to clear CPU buffers, but the
    hypervisor is not yet updated to expose the X86_FEATURE_MD_CLEAR CPUID bit
    to guests.
    
    Introduce an internal mitigation mode VMWERV which enables the invocation
    of the CPU buffer clearing even if X86_FEATURE_MD_CLEAR is not set. If the
    system has no updated microcode this results in a pointless execution of
    the VERW instruction wasting a few CPU cycles. If the microcode is updated,
    but not exposed to a guest then the CPU buffers will be cleared.
    
    That said: Virtual Machines Will Eventually Receive Vaccine
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jon Masters <jcm@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1f0295783325..aca1ef8cc79f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -995,6 +995,7 @@ extern enum l1tf_mitigations l1tf_mitigation;
 enum mds_mitigations {
 	MDS_MITIGATION_OFF,
 	MDS_MITIGATION_FULL,
+	MDS_MITIGATION_VMWERV,
 };
 
 #endif /* _ASM_X86_PROCESSOR_H */

commit bc1241700acd82ec69fde98c5763ce51086269f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 18 22:04:08 2019 +0100

    x86/speculation/mds: Add mitigation control for MDS
    
    Now that the mitigations are in place, add a command line parameter to
    control the mitigation, a mitigation selector function and a SMT update
    mechanism.
    
    This is the minimal straight forward initial implementation which just
    provides an always on/off mode. The command line parameter is:
    
      mds=[full|off]
    
    This is consistent with the existing mitigations for other speculative
    hardware vulnerabilities.
    
    The idle invocation is dynamically updated according to the SMT state of
    the system similar to the dynamic update of the STIBP mitigation. The idle
    mitigation is limited to CPUs which are only affected by MSBDS and not any
    other variant, because the other variants cannot be mitigated on SMT
    enabled systems.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jon Masters <jcm@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 33051436c864..1f0295783325 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -992,4 +992,9 @@ enum l1tf_mitigations {
 
 extern enum l1tf_mitigations l1tf_mitigation;
 
+enum mds_mitigations {
+	MDS_MITIGATION_OFF,
+	MDS_MITIGATION_FULL,
+};
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit 439fbdf6a2021ab1cca94b30837674b2b7527ae8
Author: Pingfan Liu <kernelfans@gmail.com>
Date:   Fri Jan 4 16:46:19 2019 +0800

    x86/trap: Remove useless declaration
    
    There is no early_trap_pf_init() implementation, hence remove this useless
    declaration.
    
    Signed-off-by: Pingfan Liu <kernelfans@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/1546591579-23502-1-git-send-email-kernelfans@gmail.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 33051436c864..2bb3a648fc12 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -742,7 +742,6 @@ enum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 
-void early_trap_pf_init(void);
 
 /* Defined in head.S */
 extern struct desc_ptr		early_gdt_descr;

commit e5cb113f2dbc8125f31005faebab161a2a84ebe6
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Dec 28 00:36:03 2018 -0800

    mm: make free_reserved_area() return "const char *"
    
    and propagate through down the call stack.
    
    Link: http://lkml.kernel.org/r/20181124091411.GC10969@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 071b2a6fff85..33051436c864 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -967,7 +967,7 @@ static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
 }
 
 extern unsigned long arch_align_stack(unsigned long sp);
-extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
+void free_init_pages(const char *what, unsigned long begin, unsigned long end);
 extern void free_kernel_image_pages(void *begin, void *end);
 
 void default_idle(void);

commit de0d22e50cd3d57277f073ccf65d57aa519d6888
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Tue Oct 30 15:04:47 2018 -0700

    treewide: remove current_text_addr
    
    Prefer _THIS_IP_ defined in linux/kernel.h.
    
    Most definitions of current_text_addr were the same as _THIS_IP_, but
    a few archs had inline assembly instead.
    
    This patch removes the final call site of current_text_addr, making all
    of the definitions dead code.
    
    [akpm@linux-foundation.org: fix arch/csky/include/asm/processor.h]
    Link: http://lkml.kernel.org/r/20180911182413.180715-1-ndesaulniers@google.com
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 617805981cce..071b2a6fff85 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -42,18 +42,6 @@ struct vm86;
 #define NET_IP_ALIGN	0
 
 #define HBP_NUM 4
-/*
- * Default implementation of macro that returns current
- * instruction pointer ("program counter").
- */
-static inline void *current_text_addr(void)
-{
-	void *pc;
-
-	asm volatile("mov $1f, %0; 1:":"=r" (pc));
-
-	return pc;
-}
 
 /*
  * These alignment constraints are for performance in the vSMP case,

commit d82924c3b8d0607094b94fab290a33c5ad7d586c
Merge: d7197a5ad852 bb4b3b776273
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 18:43:04 2018 +0100

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 pti updates from Ingo Molnar:
     "The main changes:
    
       - Make the IBPB barrier more strict and add STIBP support (Jiri
         Kosina)
    
       - Micro-optimize and clean up the entry code (Andy Lutomirski)
    
       - ... plus misc other fixes"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/speculation: Propagate information about RSB filling mitigation to sysfs
      x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
      x86/speculation: Apply IBPB more strictly to avoid cross-process data leak
      x86/speculation: Add RETPOLINE_AMD support to the inline asm CALL_NOSPEC variant
      x86/CPU: Fix unused variable warning when !CONFIG_IA32_EMULATION
      x86/pti/64: Remove the SYSCALL64 entry trampoline
      x86/entry/64: Use the TSS sp2 slot for SYSCALL/SYSRET scratch space
      x86/entry/64: Document idtentry

commit f682a7920baf7b721d01dd317f3b532265357cbb
Merge: 99792e0cea1e 3a025de64bf8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:54:58 2018 +0100

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt updates from Ingo Molnar:
     "Two main changes:
    
       - Remove no longer used parts of the paravirt infrastructure and put
         large quantities of paravirt ops under a new config option
         PARAVIRT_XXL=y, which is selected by XEN_PV only. (Joergen Gross)
    
       - Enable PV spinlocks on Hyperv (Yi Sun)"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/hyperv: Enable PV qspinlock for Hyper-V
      x86/hyperv: Add GUEST_IDLE_MSR support
      x86/paravirt: Clean up native_patch()
      x86/paravirt: Prevent redefinition of SAVE_FLAGS macro
      x86/xen: Make xen_reservation_lock static
      x86/paravirt: Remove unneeded mmu related paravirt ops bits
      x86/paravirt: Move the Xen-only pv_mmu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the pv_irq_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move items in pv_info under PARAVIRT_XXL umbrella
      x86/paravirt: Introduce new config option PARAVIRT_XXL
      x86/paravirt: Remove unused paravirt bits
      x86/paravirt: Use a single ops structure
      x86/paravirt: Remove clobbers from struct paravirt_patch_site
      x86/paravirt: Remove clobbers parameter from paravirt patch functions
      x86/paravirt: Make paravirt_patch_call() and paravirt_patch_jmp() static
      x86/xen: Add SPDX identifier in arch/x86/xen files
      x86/xen: Link platform-pci-unplug.o only if CONFIG_XEN_PVHVM
      x86/xen: Move pv specific parts of arch/x86/xen/mmu.c to mmu_pv.c
      x86/xen: Move pv irq related functions under CONFIG_XEN_PV umbrella

commit c9661c1e80b609cd038db7c908e061f0535804ef
Author: Pu Wen <puwen@hygon.cn>
Date:   Sun Sep 23 17:33:12 2018 +0800

    x86/cpu: Create Hygon Dhyana architecture support file
    
    Add x86 architecture support for a new processor: Hygon Dhyana Family
    18h. Carve out initialization code needed by Dhyana into a separate
    compilation unit.
    
    To identify Hygon Dhyana CPU, add a new vendor type X86_VENDOR_HYGON.
    
    Since Dhyana uses AMD functionality to a large degree, select
    CPU_SUP_AMD which provides that functionality.
    
     [ bp: drop explicit license statement as it has an SPDX tag already. ]
    
    Signed-off-by: Pu Wen <puwen@hygon.cn>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: tglx@linutronix.de
    Cc: mingo@redhat.com
    Cc: hpa@zytor.com
    Cc: x86@kernel.org
    Cc: thomas.lendacky@amd.com
    Link: https://lkml.kernel.org/r/1a882065223bacbde5726f3beaa70cebd8dcd814.1537533369.git.puwen@hygon.cn

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d53c54b842da..d4dfd02b740e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -155,7 +155,8 @@ enum cpuid_regs_idx {
 #define X86_VENDOR_CENTAUR	5
 #define X86_VENDOR_TRANSMETA	7
 #define X86_VENDOR_NSC		8
-#define X86_VENDOR_NUM		9
+#define X86_VENDOR_HYGON	9
+#define X86_VENDOR_NUM		10
 
 #define X86_VENDOR_UNKNOWN	0xff
 

commit 98f05b5138f0a9b56022295cc1387e635b25635d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Sep 3 15:59:43 2018 -0700

    x86/entry/64: Use the TSS sp2 slot for SYSCALL/SYSRET scratch space
    
    In the non-trampoline SYSCALL64 path, a percpu variable is used to
    temporarily store the user RSP value.
    
    Instead of a separate variable, use the otherwise unused sp2 slot in the
    TSS.  This will improve cache locality, as the sp1 slot is already used in
    the same code to find the kernel stack.  It will also simplify a future
    change to make the non-trampoline path work in PTI mode.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/08e769a0023dbad4bac6f34f3631dbaf8ad59f4f.1536015544.git.luto@kernel.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d53c54b842da..b2bb1d691efc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -315,7 +315,13 @@ struct x86_hw_tss {
 	 */
 	u64			sp1;
 
+	/*
+	 * Since Linux does not use ring 2, the 'sp2' slot is unused by
+	 * hardware.  entry_SYSCALL_64 uses it as scratch space to stash
+	 * the user RSP value.
+	 */
 	u64			sp2;
+
 	u64			reserved2;
 	u64			ist[7];
 	u32			reserved3;

commit 9bad5658ea710f45e4ee68b88a01cfe1839d8b00
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:23 2018 +0200

    x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
    
    Most of the paravirt ops defined in pv_cpu_ops are for Xen PV guests
    only. Define them only if CONFIG_PARAVIRT_XXL is set.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-13-jgross@suse.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d53c54b842da..a47d3c84631d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -578,7 +578,7 @@ static inline bool on_thread_stack(void)
 			       current_stack_pointer) < THREAD_SIZE;
 }
 
-#ifdef CONFIG_PARAVIRT
+#ifdef CONFIG_PARAVIRT_XXL
 #include <asm/paravirt.h>
 #else
 #define __cpuid			native_cpuid
@@ -589,7 +589,7 @@ static inline void load_sp0(unsigned long sp0)
 }
 
 #define set_iopl_mask native_set_iopl_mask
-#endif /* CONFIG_PARAVIRT */
+#endif /* CONFIG_PARAVIRT_XXL */
 
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);

commit cc51e5428ea54f575d49cfcede1d4cb3a72b4ec4
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Aug 24 10:03:50 2018 -0700

    x86/speculation/l1tf: Increase l1tf memory limit for Nehalem+
    
    On Nehalem and newer core CPUs the CPU cache internally uses 44 bits
    physical address space. The L1TF workaround is limited by this internal
    cache address width, and needs to have one bit free there for the
    mitigation to work.
    
    Older client systems report only 36bit physical address space so the range
    check decides that L1TF is not mitigated for a 36bit phys/32GB system with
    some memory holes.
    
    But since these actually have the larger internal cache width this warning
    is bogus because it would only really be needed if the system had more than
    43bits of memory.
    
    Add a new internal x86_cache_bits field. Normally it is the same as the
    physical bits field reported by CPUID, but for Nehalem and newerforce it to
    be at least 44bits.
    
    Change the L1TF memory size warning to use the new cache_bits field to
    avoid bogus warnings and remove the bogus comment about memory size.
    
    Fixes: 17dbca119312 ("x86/speculation/l1tf: Add sysfs reporting for l1tf")
    Reported-by: George Anchev <studio@anchev.net>
    Reported-by: Christopher Snowhill <kode54@gmail.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Michael Hocko <mhocko@suse.com>
    Cc: vbabka@suse.cz
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180824170351.34874-1-andi@firstfloor.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c24297268ebc..d53c54b842da 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -132,6 +132,8 @@ struct cpuinfo_x86 {
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 	u32			microcode;
+	/* Address space bits used by the cache internally */
+	u8			x86_cache_bits;
 	unsigned		initialized : 1;
 } __randomize_layout;
 
@@ -183,7 +185,7 @@ extern void cpu_detect(struct cpuinfo_x86 *c);
 
 static inline unsigned long long l1tf_pfn_limit(void)
 {
-	return BIT_ULL(boot_cpu_data.x86_phys_bits - 1 - PAGE_SHIFT);
+	return BIT_ULL(boot_cpu_data.x86_cache_bits - 1 - PAGE_SHIFT);
 }
 
 extern void early_cpu_init(void);

commit b0a182f875689647b014bc01d36b340217792852
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Aug 23 15:44:18 2018 +0200

    x86/speculation/l1tf: Fix off-by-one error when warning that system has too much RAM
    
    Two users have reported [1] that they have an "extremely unlikely" system
    with more than MAX_PA/2 memory and L1TF mitigation is not effective. In
    fact it's a CPU with 36bits phys limit (64GB) and 32GB memory, but due to
    holes in the e820 map, the main region is almost 500MB over the 32GB limit:
    
    [    0.000000] BIOS-e820: [mem 0x0000000100000000-0x000000081effffff] usable
    
    Suggestions to use 'mem=32G' to enable the L1TF mitigation while losing the
    500MB revealed, that there's an off-by-one error in the check in
    l1tf_select_mitigation().
    
    l1tf_pfn_limit() returns the last usable pfn (inclusive) and the range
    check in the mitigation path does not take this into account.
    
    Instead of amending the range check, make l1tf_pfn_limit() return the first
    PFN which is over the limit which is less error prone. Adjust the other
    users accordingly.
    
    [1] https://bugzilla.suse.com/show_bug.cgi?id=1105536
    
    Fixes: 17dbca119312 ("x86/speculation/l1tf: Add sysfs reporting for l1tf")
    Reported-by: George Anchev <studio@anchev.net>
    Reported-by: Christopher Snowhill <kode54@gmail.com>
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180823134418.17008-1-vbabka@suse.cz

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index a0a52274cb4a..c24297268ebc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -183,7 +183,7 @@ extern void cpu_detect(struct cpuinfo_x86 *c);
 
 static inline unsigned long long l1tf_pfn_limit(void)
 {
-	return BIT_ULL(boot_cpu_data.x86_phys_bits - 1 - PAGE_SHIFT) - 1;
+	return BIT_ULL(boot_cpu_data.x86_phys_bits - 1 - PAGE_SHIFT);
 }
 
 extern void early_cpu_init(void);

commit 9df9516940a61d29aedf4d91b483ca6597e7d480
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Mon Aug 20 11:58:35 2018 +0200

    x86/speculation/l1tf: Fix overflow in l1tf_pfn_limit() on 32bit
    
    On 32bit PAE kernels on 64bit hardware with enough physical bits,
    l1tf_pfn_limit() will overflow unsigned long. This in turn affects
    max_swapfile_size() and can lead to swapon returning -EINVAL. This has been
    observed in a 32bit guest with 42 bits physical address size, where
    max_swapfile_size() overflows exactly to 1 << 32, thus zero, and produces
    the following warning to dmesg:
    
    [    6.396845] Truncating oversized swap area, only using 0k out of 2047996k
    
    Fix this by using unsigned long long instead.
    
    Fixes: 17dbca119312 ("x86/speculation/l1tf: Add sysfs reporting for l1tf")
    Fixes: 377eeaa8e11f ("x86/speculation/l1tf: Limit swap file size to MAX_PA/2")
    Reported-by: Dominique Leuenberger <dimstar@suse.de>
    Reported-by: Adrian Schroeter <adrian@suse.de>
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180820095835.5298-1-vbabka@suse.cz

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 682286aca881..a0a52274cb4a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -181,9 +181,9 @@ extern const struct seq_operations cpuinfo_op;
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
 
-static inline unsigned long l1tf_pfn_limit(void)
+static inline unsigned long long l1tf_pfn_limit(void)
 {
-	return BIT(boot_cpu_data.x86_phys_bits - 1 - PAGE_SHIFT) - 1;
+	return BIT_ULL(boot_cpu_data.x86_phys_bits - 1 - PAGE_SHIFT) - 1;
 }
 
 extern void early_cpu_init(void);

commit 958f338e96f874a0d29442396d6adf9c1e17aa2d
Merge: 781fca5b1046 07d981ad4cf1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 09:46:06 2018 -0700

    Merge branch 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Merge L1 Terminal Fault fixes from Thomas Gleixner:
     "L1TF, aka L1 Terminal Fault, is yet another speculative hardware
      engineering trainwreck. It's a hardware vulnerability which allows
      unprivileged speculative access to data which is available in the
      Level 1 Data Cache when the page table entry controlling the virtual
      address, which is used for the access, has the Present bit cleared or
      other reserved bits set.
    
      If an instruction accesses a virtual address for which the relevant
      page table entry (PTE) has the Present bit cleared or other reserved
      bits set, then speculative execution ignores the invalid PTE and loads
      the referenced data if it is present in the Level 1 Data Cache, as if
      the page referenced by the address bits in the PTE was still present
      and accessible.
    
      While this is a purely speculative mechanism and the instruction will
      raise a page fault when it is retired eventually, the pure act of
      loading the data and making it available to other speculative
      instructions opens up the opportunity for side channel attacks to
      unprivileged malicious code, similar to the Meltdown attack.
    
      While Meltdown breaks the user space to kernel space protection, L1TF
      allows to attack any physical memory address in the system and the
      attack works across all protection domains. It allows an attack of SGX
      and also works from inside virtual machines because the speculation
      bypasses the extended page table (EPT) protection mechanism.
    
      The assoicated CVEs are: CVE-2018-3615, CVE-2018-3620, CVE-2018-3646
    
      The mitigations provided by this pull request include:
    
       - Host side protection by inverting the upper address bits of a non
         present page table entry so the entry points to uncacheable memory.
    
       - Hypervisor protection by flushing L1 Data Cache on VMENTER.
    
       - SMT (HyperThreading) control knobs, which allow to 'turn off' SMT
         by offlining the sibling CPU threads. The knobs are available on
         the kernel command line and at runtime via sysfs
    
       - Control knobs for the hypervisor mitigation, related to L1D flush
         and SMT control. The knobs are available on the kernel command line
         and at runtime via sysfs
    
       - Extensive documentation about L1TF including various degrees of
         mitigations.
    
      Thanks to all people who have contributed to this in various ways -
      patches, review, testing, backporting - and the fruitful, sometimes
      heated, but at the end constructive discussions.
    
      There is work in progress to provide other forms of mitigations, which
      might be less horrible performance wise for a particular kind of
      workloads, but this is not yet ready for consumption due to their
      complexity and limitations"
    
    * 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (75 commits)
      x86/microcode: Allow late microcode loading with SMT disabled
      tools headers: Synchronise x86 cpufeatures.h for L1TF additions
      x86/mm/kmmio: Make the tracer robust against L1TF
      x86/mm/pat: Make set_memory_np() L1TF safe
      x86/speculation/l1tf: Make pmd/pud_mknotpresent() invert
      x86/speculation/l1tf: Invert all not present mappings
      cpu/hotplug: Fix SMT supported evaluation
      KVM: VMX: Tell the nested hypervisor to skip L1D flush on vmentry
      x86/speculation: Use ARCH_CAPABILITIES to skip L1D flush on vmentry
      x86/speculation: Simplify sysfs report of VMX L1TF vulnerability
      Documentation/l1tf: Remove Yonah processors from not vulnerable list
      x86/KVM/VMX: Don't set l1tf_flush_l1d from vmx_handle_external_intr()
      x86/irq: Let interrupt handlers set kvm_cpu_l1tf_flush_l1d
      x86: Don't include linux/irq.h from asm/hardirq.h
      x86/KVM/VMX: Introduce per-host-cpu analogue of l1tf_flush_l1d
      x86/irq: Demote irq_cpustat_t::__softirq_pending to u16
      x86/KVM/VMX: Move the l1tf_flush_l1d test to vmx_l1d_flush()
      x86/KVM/VMX: Replace 'vmx_l1d_flush_always' with 'vmx_l1d_flush_cond'
      x86/KVM/VMX: Don't set l1tf_flush_l1d to true from vmx_l1d_flush()
      cpu/hotplug: detect SMT disabled by BIOS
      ...

commit 6ea2738e0ca0e626c75202fb051c1e88d7a950fa
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Aug 2 15:58:29 2018 -0700

    x86/mm/init: Add helper for freeing kernel image pages
    
    When chunks of the kernel image are freed, free_init_pages() is used
    directly.  Consolidate the three sites that do this.  Also update the
    string to give an incrementally better description of that memory versus
    what was there before.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: keescook@google.com
    Cc: aarcange@redhat.com
    Cc: jgross@suse.com
    Cc: jpoimboe@redhat.com
    Cc: gregkh@linuxfoundation.org
    Cc: peterz@infradead.org
    Cc: hughd@google.com
    Cc: torvalds@linux-foundation.org
    Cc: bp@alien8.de
    Cc: luto@kernel.org
    Cc: ak@linux.intel.com
    Cc: Kees Cook <keescook@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Link: https://lkml.kernel.org/r/20180802225829.FE0E32EA@viggo.jf.intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cfd29ee8c3da..59663c08c949 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -966,6 +966,7 @@ static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
 
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
+extern void free_kernel_image_pages(void *begin, void *end);
 
 void default_idle(void);
 #ifdef	CONFIG_XEN

commit d90a7a0ec83fb86622cd7dae23255d3c50a99ec8
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Jul 13 16:23:25 2018 +0200

    x86/bugs, kvm: Introduce boot-time control of L1TF mitigations
    
    Introduce the 'l1tf=' kernel command line option to allow for boot-time
    switching of mitigation that is used on processors affected by L1TF.
    
    The possible values are:
    
      full
            Provides all available mitigations for the L1TF vulnerability. Disables
            SMT and enables all mitigations in the hypervisors. SMT control via
            /sys/devices/system/cpu/smt/control is still possible after boot.
            Hypervisors will issue a warning when the first VM is started in
            a potentially insecure configuration, i.e. SMT enabled or L1D flush
            disabled.
    
      full,force
            Same as 'full', but disables SMT control. Implies the 'nosmt=force'
            command line option. sysfs control of SMT and the hypervisor flush
            control is disabled.
    
      flush
            Leaves SMT enabled and enables the conditional hypervisor mitigation.
            Hypervisors will issue a warning when the first VM is started in a
            potentially insecure configuration, i.e. SMT enabled or L1D flush
            disabled.
    
      flush,nosmt
            Disables SMT and enables the conditional hypervisor mitigation. SMT
            control via /sys/devices/system/cpu/smt/control is still possible
            after boot. If SMT is reenabled or flushing disabled at runtime
            hypervisors will issue a warning.
    
      flush,nowarn
            Same as 'flush', but hypervisors will not warn when
            a VM is started in a potentially insecure configuration.
    
      off
            Disables hypervisor mitigations and doesn't emit any warnings.
    
    Default is 'flush'.
    
    Let KVM adhere to these semantics, which means:
    
      - 'lt1f=full,force'   : Performe L1D flushes. No runtime control
                              possible.
    
      - 'l1tf=full'
      - 'l1tf-flush'
      - 'l1tf=flush,nosmt'  : Perform L1D flushes and warn on VM start if
                              SMT has been runtime enabled or L1D flushing
                              has been run-time enabled
    
      - 'l1tf=flush,nowarn' : Perform L1D flushes and no warnings are emitted.
    
      - 'l1tf=off'          : L1D flushes are not performed and no warnings
                              are emitted.
    
    KVM can always override the L1D flushing behavior using its 'vmentry_l1d_flush'
    module parameter except when lt1f=full,force is set.
    
    This makes KVM's private 'nosmt' option redundant, and as it is a bit
    non-systematic anyway (this is something to control globally, not on
    hypervisor level), remove that option.
    
    Add the missing Documentation entry for the l1tf vulnerability sysfs file
    while at it.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jiri Kosina <jkosina@suse.cz>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Link: https://lkml.kernel.org/r/20180713142323.202758176@linutronix.de

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7e3ac5eedcd6..79e409974ccc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -982,4 +982,16 @@ bool xen_set_default_idle(void);
 void stop_this_cpu(void *dummy);
 void df_debug(struct pt_regs *regs, long error_code);
 void microcode_check(void);
+
+enum l1tf_mitigations {
+	L1TF_MITIGATION_OFF,
+	L1TF_MITIGATION_FLUSH_NOWARN,
+	L1TF_MITIGATION_FLUSH,
+	L1TF_MITIGATION_FLUSH_NOSMT,
+	L1TF_MITIGATION_FULL,
+	L1TF_MITIGATION_FULL_FORCE
+};
+
+extern enum l1tf_mitigations l1tf_mitigation;
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit 17dbca119312b4e8173d4e25ff64262119fcef38
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Jun 13 15:48:26 2018 -0700

    x86/speculation/l1tf: Add sysfs reporting for l1tf
    
    L1TF core kernel workarounds are cheap and normally always enabled, However
    they still should be reported in sysfs if the system is vulnerable or
    mitigated. Add the necessary CPU feature/bug bits.
    
    - Extend the existing checks for Meltdowns to determine if the system is
      vulnerable. All CPUs which are not vulnerable to Meltdown are also not
      vulnerable to L1TF
    
    - Check for 32bit non PAE and emit a warning as there is no practical way
      for mitigation due to the limited physical address bits
    
    - If the system has more than MAX_PA/2 physical memory the invert page
      workarounds don't protect the system against the L1TF attack anymore,
      because an inverted physical address will also point to valid
      memory. Print a warning in this case and report that the system is
      vulnerable.
    
    Add a function which returns the PFN limit for the L1TF mitigation, which
    will be used in follow up patches for sanity and range checks.
    
    [ tglx: Renamed the CPU feature bit to L1TF_PTEINV ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Dave Hansen <dave.hansen@intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cfd29ee8c3da..7e3ac5eedcd6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -181,6 +181,11 @@ extern const struct seq_operations cpuinfo_op;
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
 
+static inline unsigned long l1tf_pfn_limit(void)
+{
+	return BIT(boot_cpu_data.x86_phys_bits - 1 - PAGE_SHIFT) - 1;
+}
+
 extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);
 extern void identify_secondary_cpu(struct cpuinfo_x86 *);

commit 050e9baa9dc9fbd9ce2b27f0056990fc9e0a08a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 14 12:21:18 2018 +0900

    Kbuild: rename CC_STACKPROTECTOR[_STRONG] config variables
    
    The changes to automatically test for working stack protector compiler
    support in the Kconfig files removed the special STACKPROTECTOR_AUTO
    option that picked the strongest stack protector that the compiler
    supported.
    
    That was all a nice cleanup - it makes no sense to have the AUTO case
    now that the Kconfig phase can just determine the compiler support
    directly.
    
    HOWEVER.
    
    It also meant that doing "make oldconfig" would now _disable_ the strong
    stackprotector if you had AUTO enabled, because in a legacy config file,
    the sane stack protector configuration would look like
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_NONE is not set
      # CONFIG_CC_STACKPROTECTOR_REGULAR is not set
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_STACKPROTECTOR_AUTO=y
    
    and when you ran this through "make oldconfig" with the Kbuild changes,
    it would ask you about the regular CONFIG_CC_STACKPROTECTOR (that had
    been renamed from CONFIG_CC_STACKPROTECTOR_REGULAR to just
    CONFIG_CC_STACKPROTECTOR), but it would think that the STRONG version
    used to be disabled (because it was really enabled by AUTO), and would
    disable it in the new config, resulting in:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    That's dangerously subtle - people could suddenly find themselves with
    the weaker stack protector setup without even realizing.
    
    The solution here is to just rename not just the old RECULAR stack
    protector option, but also the strong one.  This does that by just
    removing the CC_ prefix entirely for the user choices, because it really
    is not about the compiler support (the compiler support now instead
    automatially impacts _visibility_ of the options to users).
    
    This results in "make oldconfig" actually asking the user for their
    choice, so that we don't have any silent subtle security model changes.
    The end result would generally look like this:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_STACKPROTECTOR=y
      CONFIG_STACKPROTECTOR_STRONG=y
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    where the "CC_" versions really are about internal compiler
    infrastructure, not the user selections.
    
    Acked-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index e28add6b791f..cfd29ee8c3da 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -412,7 +412,7 @@ extern asmlinkage void ignore_sysret(void);
 void save_fsgs_for_kvm(void);
 #endif
 #else	/* X86_64 */
-#ifdef CONFIG_CC_STACKPROTECTOR
+#ifdef CONFIG_STACKPROTECTOR
 /*
  * Make sure stack canary segment base is cached-aligned:
  *   "For Intel Atom processors, avoid non zero segment base address

commit 177bfd725bd1b67c7254248cf19f0465d493e631
Merge: 9305bd6ca7b4 fed71f7d9879 7dec80ccbe31
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat May 19 08:18:56 2018 +0200

    Merge branches 'x86/urgent' and 'core/urgent' into x86/boot, to pick up fixes and avoid conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b5cf8707e6c9d85819b4bee3218ec560953149f7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 13 11:29:07 2018 +0200

    x86/CPU: Move cpu local function declarations to local header
    
    No point in exposing all these functions globaly as they are strict local
    to the cpu management code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7aa2caa82b2d..d68554613ecd 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -186,15 +186,6 @@ extern void identify_boot_cpu(void);
 extern void identify_secondary_cpu(struct cpuinfo_x86 *);
 extern void print_cpu_info(struct cpuinfo_x86 *);
 void print_cpu_msr(struct cpuinfo_x86 *);
-extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
-extern u32 get_scattered_cpuid_leaf(unsigned int level,
-				    unsigned int sub_leaf,
-				    enum cpuid_regs_idx reg);
-extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
-extern void init_amd_cacheinfo(struct cpuinfo_x86 *c);
-
-extern int detect_extended_topology(struct cpuinfo_x86 *c);
-extern void detect_ht(struct cpuinfo_x86 *c);
 
 #ifdef CONFIG_X86_32
 extern int have_cpuid_p(void);

commit 6c4f5abaf3566dbf5b26e7b14f4392be400f12e3
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Fri Apr 27 16:48:00 2018 -0500

    x86/CPU: Modify detect_extended_topology() to return result
    
    Current implementation does not communicate whether it can successfully
    detect CPUID function 0xB information. Therefore, modify the function to
    return success or error codes. This will be used by subsequent patches.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1524865681-112110-2-git-send-email-suravee.suthikulpanit@amd.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 4fa4206029e3..7aa2caa82b2d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -193,7 +193,7 @@ extern u32 get_scattered_cpuid_leaf(unsigned int level,
 extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
 extern void init_amd_cacheinfo(struct cpuinfo_x86 *c);
 
-extern void detect_extended_topology(struct cpuinfo_x86 *c);
+extern int detect_extended_topology(struct cpuinfo_x86 *c);
 extern void detect_ht(struct cpuinfo_x86 *c);
 
 #ifdef CONFIG_X86_32

commit 451cf3ca7d4615631443014ee769c25e267c25ff
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Wed Apr 4 14:45:27 2018 +0800

    x86/processor: Remove two unused function declarations
    
    early_trap_init() and cpu_set_gdt() have been removed, so remove the stale
    declarations as well.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: keescook@chromium.org
    Cc: luto@kernel.org
    Cc: hpa@zytor.com
    Cc: bp@suse.de
    Cc: kirill.shutemov@linux.intel.com
    Link: https://lkml.kernel.org/r/20180404064527.10562-1-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 4fa4206029e3..21a114914ba4 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -749,13 +749,11 @@ enum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 
-extern void early_trap_init(void);
 void early_trap_pf_init(void);
 
 /* Defined in head.S */
 extern struct desc_ptr		early_gdt_descr;
 
-extern void cpu_set_gdt(int);
 extern void switch_to_new_gdt(int);
 extern void load_direct_gdt(int);
 extern void load_fixmap_gdt(int);

commit 35060ed6a1ffc92106904f5e4b5823ca4facfc73
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 13 18:48:05 2018 +0100

    x86/kvm/vmx: avoid expensive rdmsr for MSR_GS_BASE
    
    vmx_save_host_state() is only called from kvm_arch_vcpu_ioctl_run() so
    the context is pretty well defined and as we're past 'swapgs' MSR_GS_BASE
    should contain kernel's GS base which we point to irq_stack_union.
    
    Add new kernelmode_gs_base() API, irq_stack_union needs to be exported
    as KVM can be build as module.
    
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1d816a3325a1..4fa4206029e3 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -407,6 +407,11 @@ union irq_stack_union {
 DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union) __visible;
 DECLARE_INIT_PER_CPU(irq_stack_union);
 
+static inline unsigned long cpu_kernelmode_gs_base(int cpu)
+{
+	return (unsigned long)per_cpu(irq_stack_union.gs_base, cpu);
+}
+
 DECLARE_PER_CPU(char *, irq_stack_ptr);
 DECLARE_PER_CPU(unsigned int, irq_count);
 extern asmlinkage void ignore_sysret(void);

commit 42b933b59721f288e3ce23ca79a17a973808dab9
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 13 18:48:04 2018 +0100

    x86/kvm/vmx: read MSR_{FS,KERNEL_GS}_BASE from current->thread
    
    vmx_save_host_state() is only called from kvm_arch_vcpu_ioctl_run() so
    the context is pretty well defined. Read MSR_{FS,KERNEL_GS}_BASE from
    current->thread after calling save_fsgs() which takes care of
    X86_BUG_NULL_SEG case now and will do RD[FG,GS]BASE when FSGSBASE
    extensions are exposed to userspace (currently they are not).
    
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b0ccd4847a58..1d816a3325a1 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -410,6 +410,11 @@ DECLARE_INIT_PER_CPU(irq_stack_union);
 DECLARE_PER_CPU(char *, irq_stack_ptr);
 DECLARE_PER_CPU(unsigned int, irq_count);
 extern asmlinkage void ignore_sysret(void);
+
+#if IS_ENABLED(CONFIG_KVM)
+/* Save actual FS/GS selectors and bases to current->thread */
+void save_fsgs_for_kvm(void);
+#endif
 #else	/* X86_64 */
 #ifdef CONFIG_CC_STACKPROTECTOR
 /*

commit 1008c52c09dcb23d93f8e0ea83a6246265d2cce0
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Feb 16 12:26:39 2018 +0100

    x86/CPU: Add a microcode loader callback
    
    Add a callback function which the microcode loader calls when microcode
    has been updated to a newer revision. Do the callback only when no error
    was encountered during loading.
    
    Tested-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ashok Raj <ashok.raj@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180216112640.11554-3-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1bd9ed87606f..b0ccd4847a58 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -977,4 +977,5 @@ bool xen_set_default_idle(void);
 
 void stop_this_cpu(void *dummy);
 void df_debug(struct pt_regs *regs, long error_code);
+void microcode_check(void);
 #endif /* _ASM_X86_PROCESSOR_H */

commit d4667ca142610961c89ae7c41a823b3358fcdd0e
Merge: 6556677a8040 e48657573481
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 14 17:02:15 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 PTI and Spectre related fixes and updates from Ingo Molnar:
     "Here's the latest set of Spectre and PTI related fixes and updates:
    
      Spectre:
       - Add entry code register clearing to reduce the Spectre attack
         surface
       - Update the Spectre microcode blacklist
       - Inline the KVM Spectre helpers to get close to v4.14 performance
         again.
       - Fix indirect_branch_prediction_barrier()
       - Fix/improve Spectre related kernel messages
       - Fix array_index_nospec_mask() asm constraint
       - KVM: fix two MSR handling bugs
    
      PTI:
       - Fix a paranoid entry PTI CR3 handling bug
       - Fix comments
    
      objtool:
       - Fix paranoid_entry() frame pointer warning
       - Annotate WARN()-related UD2 as reachable
       - Various fixes
       - Add Add Peter Zijlstra as objtool co-maintainer
    
      Misc:
       - Various x86 entry code self-test fixes
       - Improve/simplify entry code stack frame generation and handling
         after recent heavy-handed PTI and Spectre changes. (There's two
         more WIP improvements expected here.)
       - Type fix for cache entries
    
      There's also some low risk non-fix changes I've included in this
      branch to reduce backporting conflicts:
    
       - rename a confusing x86_cpu field name
       - de-obfuscate the naming of single-TLB flushing primitives"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (41 commits)
      x86/entry/64: Fix CR3 restore in paranoid_exit()
      x86/cpu: Change type of x86_cache_size variable to unsigned int
      x86/spectre: Fix an error message
      x86/cpu: Rename cpu_data.x86_mask to cpu_data.x86_stepping
      selftests/x86/mpx: Fix incorrect bounds with old _sigfault
      x86/mm: Rename flush_tlb_single() and flush_tlb_one() to __flush_tlb_one_[user|kernel]()
      x86/speculation: Add <asm/msr-index.h> dependency
      nospec: Move array_index_nospec() parameter checking into separate macro
      x86/speculation: Fix up array_index_nospec_mask() asm constraint
      x86/debug: Use UD2 for WARN()
      x86/debug, objtool: Annotate WARN()-related UD2 as reachable
      objtool: Fix segfault in ignore_unreachable_insn()
      selftests/x86: Disable tests requiring 32-bit support on pure 64-bit systems
      selftests/x86: Do not rely on "int $0x80" in single_step_syscall.c
      selftests/x86: Do not rely on "int $0x80" in test_mremap_vdso.c
      selftests/x86: Fix build bug caused by the 5lvl test which has been moved to the VM directory
      selftests/x86/pkeys: Remove unused functions
      selftests/x86: Clean up and document sscanf() usage
      selftests/x86: Fix vDSO selftest segfault for vsyscall=none
      x86/entry/64: Remove the unused 'icebp' macro
      ...

commit 24dbc6000f4b9b0ef5a9daecb161f1907733765a
Author: Gustavo A. R. Silva <garsilva@embeddedor.com>
Date:   Tue Feb 13 13:22:08 2018 -0600

    x86/cpu: Change type of x86_cache_size variable to unsigned int
    
    Currently, x86_cache_size is of type int, which makes no sense as we
    will never have a valid cache size equal or less than 0. So instead of
    initializing this variable to -1, it can perfectly be initialized to 0
    and use it as an unsigned variable instead.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Gustavo A. R. Silva <garsilva@embeddedor.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Addresses-Coverity-ID: 1464429
    Link: http://lkml.kernel.org/r/20180213192208.GA26414@embeddedor.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b7c8583328c7..44c2c4ec6d60 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -109,7 +109,7 @@ struct cpuinfo_x86 {
 	char			x86_vendor_id[16];
 	char			x86_model_id[64];
 	/* in KB - valid for CPUS which support this call: */
-	int			x86_cache_size;
+	unsigned int		x86_cache_size;
 	int			x86_cache_alignment;	/* In bytes */
 	/* Cache QoS architectural values: */
 	int			x86_cache_max_rmid;	/* max index */

commit b399151cb48db30ad1e0e93dd40d68c6d007b637
Author: Jia Zhang <qianyue.zj@alibaba-inc.com>
Date:   Mon Jan 1 09:52:10 2018 +0800

    x86/cpu: Rename cpu_data.x86_mask to cpu_data.x86_stepping
    
    x86_mask is a confusing name which is hard to associate with the
    processor's stepping.
    
    Additionally, correct an indent issue in lib/cpu.c.
    
    Signed-off-by: Jia Zhang <qianyue.zj@alibaba-inc.com>
    [ Updated it to more recent kernels. ]
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: tony.luck@intel.com
    Link: http://lkml.kernel.org/r/1514771530-70829-1-git-send-email-qianyue.zj@alibaba-inc.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 99799fbd0f7e..b7c8583328c7 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -91,7 +91,7 @@ struct cpuinfo_x86 {
 	__u8			x86;		/* CPU family */
 	__u8			x86_vendor;	/* CPU vendor */
 	__u8			x86_model;
-	__u8			x86_mask;
+	__u8			x86_stepping;
 #ifdef CONFIG_X86_64
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
 	int			x86_tlbsize;

commit f208820a321f9b23d77d7eed89945d862d62a3ed
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Sat Feb 10 23:39:23 2018 +0000

    Revert "x86/speculation: Simplify indirect_branch_prediction_barrier()"
    
    This reverts commit 64e16720ea0879f8ab4547e3b9758936d483909b.
    
    We cannot call C functions like that, without marking all the
    call-clobbered registers as, well, clobbered. We might have got away
    with it for now because the __ibp_barrier() function was *fairly*
    unlikely to actually use any other registers. But no. Just no.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan.van.de.ven@intel.com
    Cc: dave.hansen@intel.com
    Cc: jmattson@google.com
    Cc: karahmed@amazon.de
    Cc: kvm@vger.kernel.org
    Cc: pbonzini@redhat.com
    Cc: rkrcmar@redhat.com
    Cc: sironi@amazon.de
    Link: http://lkml.kernel.org/r/1518305967-31356-3-git-send-email-dwmw@amazon.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 513f9604c192..99799fbd0f7e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -969,7 +969,4 @@ bool xen_set_default_idle(void);
 
 void stop_this_cpu(void *dummy);
 void df_debug(struct pt_regs *regs, long error_code);
-
-void __ibp_barrier(void);
-
 #endif /* _ASM_X86_PROCESSOR_H */

commit 35277995e17919ab838beae765f440674e8576eb
Merge: 0a646e9c992e b2ac58f90540
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 4 11:45:55 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull spectre/meltdown updates from Thomas Gleixner:
     "The next round of updates related to melted spectrum:
    
       - The initial set of spectre V1 mitigations:
    
           - Array index speculation blocker and its usage for syscall,
             fdtable and the n180211 driver.
    
           - Speculation barrier and its usage in user access functions
    
       - Make indirect calls in KVM speculation safe
    
       - Blacklisting of known to be broken microcodes so IPBP/IBSR are not
         touched.
    
       - The initial IBPB support and its usage in context switch
    
       - The exposure of the new speculation MSRs to KVM guests.
    
       - A fix for a regression in x86/32 related to the cpu entry area
    
       - Proper whitelisting for known to be safe CPUs from the mitigations.
    
       - objtool fixes to deal proper with retpolines and alternatives
    
       - Exclude __init functions from retpolines which speeds up the boot
         process.
    
       - Removal of the syscall64 fast path and related cleanups and
         simplifications
    
       - Removal of the unpatched paravirt mode which is yet another source
         of indirect unproteced calls.
    
       - A new and undisputed version of the module mismatch warning
    
       - A couple of cleanup and correctness fixes all over the place
    
      Yet another step towards full mitigation. There are a few things still
      missing like the RBS underflow mitigation for Skylake and other small
      details, but that's being worked on.
    
      That said, I'm taking a belated christmas vacation for a week and hope
      that everything is magically solved when I'm back on Feb 12th"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      KVM/SVM: Allow direct access to MSR_IA32_SPEC_CTRL
      KVM/VMX: Allow direct access to MSR_IA32_SPEC_CTRL
      KVM/VMX: Emulate MSR_IA32_ARCH_CAPABILITIES
      KVM/x86: Add IBPB support
      KVM/x86: Update the reverse_cpuid list to include CPUID_7_EDX
      x86/speculation: Fix typo IBRS_ATT, which should be IBRS_ALL
      x86/pti: Mark constant arrays as __initconst
      x86/spectre: Simplify spectre_v2 command line parsing
      x86/retpoline: Avoid retpolines for built-in __init functions
      x86/kvm: Update spectre-v1 mitigation
      KVM: VMX: make MSR bitmaps per-VCPU
      x86/paravirt: Remove 'noreplace-paravirt' cmdline option
      x86/speculation: Use Indirect Branch Prediction Barrier in context switch
      x86/cpuid: Fix up "virtual" IBRS/IBPB/STIBP feature bits on Intel
      x86/spectre: Fix spelling mistake: "vunerable"-> "vulnerable"
      x86/spectre: Report get_user mitigation for spectre_v1
      nl80211: Sanitize array index in parse_txq_params
      vfs, fdtable: Prevent bounds-check bypass via speculative execution
      x86/syscall: Sanitize syscall table de-references under speculation
      x86/get_user: Use pointer masking to limit speculation
      ...

commit 617aebe6a97efa539cc4b8a52adccd89596e6be0
Merge: 0771ad44a20b e47e311843de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 3 16:25:42 2018 -0800

    Merge tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull hardened usercopy whitelisting from Kees Cook:
     "Currently, hardened usercopy performs dynamic bounds checking on slab
      cache objects. This is good, but still leaves a lot of kernel memory
      available to be copied to/from userspace in the face of bugs.
    
      To further restrict what memory is available for copying, this creates
      a way to whitelist specific areas of a given slab cache object for
      copying to/from userspace, allowing much finer granularity of access
      control.
    
      Slab caches that are never exposed to userspace can declare no
      whitelist for their objects, thereby keeping them unavailable to
      userspace via dynamic copy operations. (Note, an implicit form of
      whitelisting is the use of constant sizes in usercopy operations and
      get_user()/put_user(); these bypass all hardened usercopy checks since
      these sizes cannot change at runtime.)
    
      This new check is WARN-by-default, so any mistakes can be found over
      the next several releases without breaking anyone's system.
    
      The series has roughly the following sections:
       - remove %p and improve reporting with offset
       - prepare infrastructure and whitelist kmalloc
       - update VFS subsystem with whitelists
       - update SCSI subsystem with whitelists
       - update network subsystem with whitelists
       - update process memory with whitelists
       - update per-architecture thread_struct with whitelists
       - update KVM with whitelists and fix ioctl bug
       - mark all other allocations as not whitelisted
       - update lkdtm for more sensible test overage"
    
    * tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux: (38 commits)
      lkdtm: Update usercopy tests for whitelisting
      usercopy: Restrict non-usercopy caches to size 0
      kvm: x86: fix KVM_XEN_HVM_CONFIG ioctl
      kvm: whitelist struct kvm_vcpu_arch
      arm: Implement thread_struct whitelist for hardened usercopy
      arm64: Implement thread_struct whitelist for hardened usercopy
      x86: Implement thread_struct whitelist for hardened usercopy
      fork: Provide usercopy whitelisting for task_struct
      fork: Define usercopy region in thread_stack slab caches
      fork: Define usercopy region in mm_struct slab caches
      net: Restrict unwhitelisted proto caches to size 0
      sctp: Copy struct sctp_sock.autoclose to userspace using put_user()
      sctp: Define usercopy region in SCTP proto slab cache
      caif: Define usercopy region in caif proto slab cache
      ip: Define usercopy region in IP proto slab cache
      net: Define usercopy region in struct proto slab cache
      scsi: Define usercopy region in scsi_sense_cache slab cache
      cifs: Define usercopy region in cifs_request slab cache
      vxfs: Define usercopy region in vxfs_inode slab cache
      ufs: Define usercopy region in ufs_inode_cache slab cache
      ...

commit 37a8f7c38339b22b69876d6f5a0ab851565284e3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Jan 28 10:38:50 2018 -0800

    x86/asm: Move 'status' from thread_struct to thread_info
    
    The TS_COMPAT bit is very hot and is accessed from code paths that mostly
    also touch thread_info::flags.  Move it into struct thread_info to improve
    cache locality.
    
    The only reason it was in thread_struct is that there was a brief period
    during which arch-specific fields were not allowed in struct thread_info.
    
    Linus suggested further changing:
    
      ti->status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
    
    to:
    
      if (unlikely(ti->status & (TS_COMPAT|TS_I386_REGS_POKED)))
              ti->status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
    
    on the theory that frequently dirtying the cacheline even in pure 64-bit
    code that never needs to modify status hurts performance.  That could be a
    reasonable followup patch, but I suspect it matters less on top of this
    patch.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Kernel Hardening <kernel-hardening@lists.openwall.com>
    Link: https://lkml.kernel.org/r/03148bcc1b217100e6e8ecf6a5468c45cf4304b6.1517164461.git.luto@kernel.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index efbde088a718..513f9604c192 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -460,8 +460,6 @@ struct thread_struct {
 	unsigned short		gsindex;
 #endif
 
-	u32			status;		/* thread synchronous flags */
-
 #ifdef CONFIG_X86_64
 	unsigned long		fsbase;
 	unsigned long		gsbase;

commit 7e86548e2cc8d308cb75439480f428137151b0de
Merge: 64e16720ea08 d8a5b80568a9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jan 30 15:08:27 2018 +0100

    Merge tag 'v4.15' into x86/pti, to be able to merge dependent changes
    
    Time has come to switch PTI development over to a v4.15 base - we'll still
    try to make sure that all PTI fixes backport cleanly to v4.14 and earlier.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6304672b7f0a5c010002e63a075160856dc4f88d
Merge: 942633523cde 64e16720ea08
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 29 19:08:02 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/pti updates from Thomas Gleixner:
     "Another set of melted spectrum related changes:
    
       - Code simplifications and cleanups for RSB and retpolines.
    
       - Make the indirect calls in KVM speculation safe.
    
       - Whitelist CPUs which are known not to speculate from Meltdown and
         prepare for the new CPUID flag which tells the kernel that a CPU is
         not affected.
    
       - A less rigorous variant of the module retpoline check which merily
         warns when a non-retpoline protected module is loaded and reflects
         that fact in the sysfs file.
    
       - Prepare for Indirect Branch Prediction Barrier support.
    
       - Prepare for exposure of the Speculation Control MSRs to guests, so
         guest OSes which depend on those "features" can use them. Includes
         a blacklist of the broken microcodes. The actual exposure of the
         MSRs through KVM is still being worked on"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/speculation: Simplify indirect_branch_prediction_barrier()
      x86/retpoline: Simplify vmexit_fill_RSB()
      x86/cpufeatures: Clean up Spectre v2 related CPUID flags
      x86/cpu/bugs: Make retpoline module warning conditional
      x86/bugs: Drop one "mitigation" from dmesg
      x86/nospec: Fix header guards names
      x86/alternative: Print unadorned pointers
      x86/speculation: Add basic IBPB (Indirect Branch Prediction Barrier) support
      x86/cpufeature: Blacklist SPEC_CTRL/PRED_CMD on early Spectre v2 microcodes
      x86/pti: Do not enable PTI on CPUs which are not vulnerable to Meltdown
      x86/msr: Add definitions for new speculation control MSRs
      x86/cpufeatures: Add AMD feature bits for Speculation Control
      x86/cpufeatures: Add Intel feature bits for Speculation Control
      x86/cpufeatures: Add CPUID_7_EDX CPUID leaf
      module/retpoline: Warn about missing retpoline in module
      KVM: VMX: Make indirect call speculation safe
      KVM: x86: Make indirect calls in emulator speculation safe

commit 64e16720ea0879f8ab4547e3b9758936d483909b
Author: Borislav Petkov <bp@suse.de>
Date:   Sat Jan 27 16:24:34 2018 +0000

    x86/speculation: Simplify indirect_branch_prediction_barrier()
    
    Make it all a function which does the WRMSR instead of having a hairy
    inline asm.
    
    [dwmw2: export it, fix CONFIG_RETPOLINE issues]
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ak@linux.intel.com
    Cc: dave.hansen@intel.com
    Cc: karahmed@amazon.de
    Cc: arjan@linux.intel.com
    Cc: torvalds@linux-foundation.org
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Cc: pbonzini@redhat.com
    Cc: tim.c.chen@linux.intel.com
    Cc: gregkh@linux-foundation.org
    Link: https://lkml.kernel.org/r/1517070274-12128-4-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 9c18da64daa9..881ca3b1d6d4 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -970,4 +970,7 @@ bool xen_set_default_idle(void);
 
 void stop_this_cpu(void *dummy);
 void df_debug(struct pt_regs *regs, long error_code);
+
+void __ibp_barrier(void);
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit f7d83c1cf3c77ae45876792aee5285ae970413ac
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Aug 16 13:26:03 2017 -0700

    x86: Implement thread_struct whitelist for hardened usercopy
    
    This whitelists the FPU register state portion of the thread_struct for
    copying to userspace, instead of the default entire struct. This is needed
    because FPU register state is dynamically sized, so it doesn't bypass the
    hardened usercopy checks.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cc16fa882e3e..2b037b7fe0eb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -492,6 +492,14 @@ struct thread_struct {
 	 */
 };
 
+/* Whitelist the FPU state from the task_struct for hardened usercopy. */
+static inline void arch_thread_struct_whitelist(unsigned long *offset,
+						unsigned long *size)
+{
+	*offset = offsetof(struct thread_struct, fpu.state);
+	*size = fpu_kernel_xstate_size;
+}
+
 /*
  * Thread-synchronous status.
  *

commit 5aa90a84589282b87666f92b6c3c917c8080a9bf
Merge: 61233580f1f3 9f5cb6b32d9e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 29 17:02:49 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 page table isolation updates from Thomas Gleixner:
     "This is the final set of enabling page table isolation on x86:
    
       - Infrastructure patches for handling the extra page tables.
    
       - Patches which map the various bits and pieces which are required to
         get in and out of user space into the user space visible page
         tables.
    
       - The required changes to have CR3 switching in the entry/exit code.
    
       - Optimizations for the CR3 switching along with documentation how
         the ASID/PCID mechanism works.
    
       - Updates to dump pagetables to cover the user space page tables for
         W+X scans and extra debugfs files to analyze both the kernel and
         the user space visible page tables
    
      The whole functionality is compile time controlled via a config switch
      and can be turned on/off on the command line as well"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (32 commits)
      x86/ldt: Make the LDT mapping RO
      x86/mm/dump_pagetables: Allow dumping current pagetables
      x86/mm/dump_pagetables: Check user space page table for WX pages
      x86/mm/dump_pagetables: Add page table directory to the debugfs VFS hierarchy
      x86/mm/pti: Add Kconfig
      x86/dumpstack: Indicate in Oops whether PTI is configured and enabled
      x86/mm: Clarify the whole ASID/kernel PCID/user PCID naming
      x86/mm: Use INVPCID for __native_flush_tlb_single()
      x86/mm: Optimize RESTORE_CR3
      x86/mm: Use/Fix PCID to optimize user/kernel switches
      x86/mm: Abstract switching CR3
      x86/mm: Allow flushing for future ASID switches
      x86/pti: Map the vsyscall page if needed
      x86/pti: Put the LDT in its own PGD if PTI is on
      x86/mm/64: Make a full PGD-entry size hole in the memory map
      x86/events/intel/ds: Map debug buffers in cpu_entry_area
      x86/cpu_entry_area: Add debugstore entries to cpu_entry_area
      x86/mm/pti: Map ESPFIX into user space
      x86/mm/pti: Share entry text PMD
      x86/entry: Align entry text section to PMD boundary
      ...

commit f55f0501cbf65ec41cca5058513031b711730b1d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Dec 12 07:56:45 2017 -0800

    x86/pti: Put the LDT in its own PGD if PTI is on
    
    With PTI enabled, the LDT must be mapped in the usermode tables somewhere.
    The LDT is per process, i.e. per mm.
    
    An earlier approach mapped the LDT on context switch into a fixmap area,
    but that's a big overhead and exhausted the fixmap space when NR_CPUS got
    big.
    
    Take advantage of the fact that there is an address space hole which
    provides a completely unused pgd. Use this pgd to manage per-mm LDT
    mappings.
    
    This has a down side: the LDT isn't (currently) randomized, and an attack
    that can write the LDT is instant root due to call gates (thanks, AMD, for
    leaving call gates in AMD64 but designing them wrong so they're only useful
    for exploits).  This can be mitigated by making the LDT read-only or
    randomizing the mapping, either of which is strightforward on top of this
    patch.
    
    This will significantly slow down LDT users, but that shouldn't matter for
    important workloads -- the LDT is only used by DOSEMU(2), Wine, and very
    old libc implementations.
    
    [ tglx: Cleaned it up. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 9e482d8b0b97..9c18da64daa9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -851,13 +851,22 @@ static inline void spin_lock_prefetch(const void *x)
 
 #else
 /*
- * User space process size. 47bits minus one guard page.  The guard
- * page is necessary on Intel CPUs: if a SYSCALL instruction is at
- * the highest possible canonical userspace address, then that
- * syscall will enter the kernel with a non-canonical return
- * address, and SYSRET will explode dangerously.  We avoid this
- * particular problem by preventing anything from being mapped
- * at the maximum canonical address.
+ * User space process size.  This is the first address outside the user range.
+ * There are a few constraints that determine this:
+ *
+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical
+ * address, then that syscall will enter the kernel with a
+ * non-canonical return address, and SYSRET will explode dangerously.
+ * We avoid this particular problem by preventing anything executable
+ * from being mapped at the maximum canonical address.
+ *
+ * On AMD CPUs in the Ryzen family, there's a nasty bug in which the
+ * CPUs malfunction if they execute code from the highest canonical page.
+ * They'll speculate right off the end of the canonical space, and
+ * bad things happen.  This is worked around in the same way as the
+ * Intel problem.
+ *
+ * With page table isolation enabled, we map the LDT in ... [stay tuned]
  */
 #define TASK_SIZE_MAX	((1UL << __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 

commit caf9a82657b313106aae8f4a35936c116a152299
Merge: 9c294ec08408 f6c4fd506cb6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 23 11:53:04 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 PTI preparatory patches from Thomas Gleixner:
     "Todays Advent calendar window contains twentyfour easy to digest
      patches. The original plan was to have twenty three matching the date,
      but a late fixup made that moot.
    
       - Move the cpu_entry_area mapping out of the fixmap into a separate
         address space. That's necessary because the fixmap becomes too big
         with NRCPUS=8192 and this caused already subtle and hard to
         diagnose failures.
    
         The top most patch is fresh from today and cures a brain slip of
         that tall grumpy german greybeard, who ignored the intricacies of
         32bit wraparounds.
    
       - Limit the number of CPUs on 32bit to 64. That's insane big already,
         but at least it's small enough to prevent address space issues with
         the cpu_entry_area map, which have been observed and debugged with
         the fixmap code
    
       - A few TLB flush fixes in various places plus documentation which of
         the TLB functions should be used for what.
    
       - Rename the SYSENTER stack to CPU_ENTRY_AREA stack as it is used for
         more than sysenter now and keeping the name makes backtraces
         confusing.
    
       - Prevent LDT inheritance on exec() by moving it to arch_dup_mmap(),
         which is only invoked on fork().
    
       - Make vysycall more robust.
    
       - A few fixes and cleanups of the debug_pagetables code. Check
         PAGE_PRESENT instead of checking the PTE for 0 and a cleanup of the
         C89 initialization of the address hint array which already was out
         of sync with the index enums.
    
       - Move the ESPFIX init to a different place to prepare for PTI.
    
       - Several code moves with no functional change to make PTI
         integration simpler and header files less convoluted.
    
       - Documentation fixes and clarifications"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/cpu_entry_area: Prevent wraparound in setup_cpu_entry_area_ptes() on 32bit
      init: Invoke init_espfix_bsp() from mm_init()
      x86/cpu_entry_area: Move it out of the fixmap
      x86/cpu_entry_area: Move it to a separate unit
      x86/mm: Create asm/invpcid.h
      x86/mm: Put MMU to hardware ASID translation in one place
      x86/mm: Remove hard-coded ASID limit checks
      x86/mm: Move the CR3 construction functions to tlbflush.h
      x86/mm: Add comments to clarify which TLB-flush functions are supposed to flush what
      x86/mm: Remove superfluous barriers
      x86/mm: Use __flush_tlb_one() for kernel memory
      x86/microcode: Dont abuse the TLB-flush interface
      x86/uv: Use the right TLB-flush API
      x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
      x86/doc: Remove obvious weirdnesses from the x86 MM layout documentation
      x86/mm/64: Improve the memory map documentation
      x86/ldt: Prevent LDT inheritance on exec
      x86/ldt: Rework locking
      arch, mm: Allow arch_dup_mmap() to fail
      x86/vsyscall/64: Warn and fail vsyscall emulation in NATIVE mode
      ...

commit 4fe2d8b11a370af286287a2661de9d4e6c9a145a
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Mon Dec 4 17:25:07 2017 -0800

    x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
    
    If the kernel oopses while on the trampoline stack, it will print
    "<SYSENTER>" even if SYSENTER is not involved.  That is rather confusing.
    
    The "SYSENTER" stack is used for a lot more than SYSENTER now.  Give it a
    better string to display in stack dumps, and rename the kernel code to
    match.
    
    Also move the 32-bit code over to the new naming even though it still uses
    the entry stack only for SYSENTER.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index da943411d3d8..9e482d8b0b97 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -336,12 +336,12 @@ struct x86_hw_tss {
 #define IO_BITMAP_OFFSET		(offsetof(struct tss_struct, io_bitmap) - offsetof(struct tss_struct, x86_tss))
 #define INVALID_IO_BITMAP_OFFSET	0x8000
 
-struct SYSENTER_stack {
+struct entry_stack {
 	unsigned long		words[64];
 };
 
-struct SYSENTER_stack_page {
-	struct SYSENTER_stack stack;
+struct entry_stack_page {
+	struct entry_stack stack;
 } __aligned(PAGE_SIZE);
 
 struct tss_struct {

commit 64a48099b3b31568ac45716b7fafcb74a0c2fcfe
Merge: 1291a0d5049d 6cbd2171e89b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 18 08:59:15 2017 -0800

    Merge branch 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 syscall entry code changes for PTI from Ingo Molnar:
     "The main changes here are Andy Lutomirski's changes to switch the
      x86-64 entry code to use the 'per CPU entry trampoline stack'. This,
      besides helping fix KASLR leaks (the pending Page Table Isolation
      (PTI) work), also robustifies the x86 entry code"
    
    * 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      x86/cpufeatures: Make CPU bugs sticky
      x86/paravirt: Provide a way to check for hypervisors
      x86/paravirt: Dont patch flush_tlb_single
      x86/entry/64: Make cpu_entry_area.tss read-only
      x86/entry: Clean up the SYSENTER_stack code
      x86/entry/64: Remove the SYSENTER stack canary
      x86/entry/64: Move the IST stacks into struct cpu_entry_area
      x86/entry/64: Create a per-CPU SYSCALL entry trampoline
      x86/entry/64: Return to userspace from the trampoline stack
      x86/entry/64: Use a per-CPU trampoline stack for IDT entries
      x86/espfix/64: Stop assuming that pt_regs is on the entry stack
      x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
      x86/entry: Remap the TSS into the CPU entry area
      x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct
      x86/dumpstack: Handle stack overflow on all stacks
      x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
      x86/kasan/64: Teach KASAN about the cpu_entry_area
      x86/mm/fixmap: Generalize the GDT fixmap mechanism, introduce struct cpu_entry_area
      x86/entry/gdt: Put per-CPU GDT remaps in ascending order
      x86/dumpstack: Add get_stack_info() support for the SYSENTER stack
      ...

commit 6cbd2171e89b13377261d15e64384df60ecb530e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 4 15:07:32 2017 +0100

    x86/cpufeatures: Make CPU bugs sticky
    
    There is currently no way to force CPU bug bits like CPU feature bits. That
    makes it impossible to set a bug bit once at boot and have it stick for all
    upcoming CPUs.
    
    Extend the force set/clear arrays to handle bug bits as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.992156574@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index e8991d7f7034..da943411d3d8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -163,8 +163,8 @@ extern struct cpuinfo_x86	boot_cpu_data;
 extern struct cpuinfo_x86	new_cpu_data;
 
 extern struct x86_hw_tss	doublefault_tss;
-extern __u32			cpu_caps_cleared[NCAPINTS];
-extern __u32			cpu_caps_set[NCAPINTS];
+extern __u32			cpu_caps_cleared[NCAPINTS + NBUGINTS];
+extern __u32			cpu_caps_set[NCAPINTS + NBUGINTS];
 
 #ifdef CONFIG_SMP
 DECLARE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);

commit c482feefe1aeb150156248ba0fd3e029bc886605
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:29 2017 +0100

    x86/entry/64: Make cpu_entry_area.tss read-only
    
    The TSS is a fairly juicy target for exploits, and, now that the TSS
    is in the cpu_entry_area, it's no longer protected by kASLR.  Make it
    read-only on x86_64.
    
    On x86_32, it can't be RO because it's written by the CPU during task
    switches, and we use a task gate for double faults.  I'd also be
    nervous about errata if we tried to make it RO even on configurations
    without double fault handling.
    
    [ tglx: AMD confirmed that there is no problem on 64-bit with TSS RO.  So
            it's probably safe to assume that it's a non issue, though Intel
            might have been creative in that area. Still waiting for
            confirmation. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.733700132@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f933869470b8..e8991d7f7034 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -340,13 +340,11 @@ struct SYSENTER_stack {
 	unsigned long		words[64];
 };
 
-struct tss_struct {
-	/*
-	 * Space for the temporary SYSENTER stack, used for SYSENTER
-	 * and the entry trampoline as well.
-	 */
-	struct SYSENTER_stack	SYSENTER_stack;
+struct SYSENTER_stack_page {
+	struct SYSENTER_stack stack;
+} __aligned(PAGE_SIZE);
 
+struct tss_struct {
 	/*
 	 * The fixed hardware portion.  This must not cross a page boundary
 	 * at risk of violating the SDM's advice and potentially triggering
@@ -363,7 +361,7 @@ struct tss_struct {
 	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
 } __aligned(PAGE_SIZE);
 
-DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss);
+DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);
 
 /*
  * sizeof(unsigned long) coming from an extra "long" at the end
@@ -378,7 +376,8 @@ DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss);
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
 #else
-#define cpu_current_top_of_stack cpu_tss.x86_tss.sp1
+/* The RO copy can't be accessed with this_cpu_xyz(), so use the RW copy. */
+#define cpu_current_top_of_stack cpu_tss_rw.x86_tss.sp1
 #endif
 
 /*
@@ -538,7 +537,7 @@ static inline void native_set_iopl_mask(unsigned mask)
 static inline void
 native_load_sp0(unsigned long sp0)
 {
-	this_cpu_write(cpu_tss.x86_tss.sp0, sp0);
+	this_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);
 }
 
 static inline void native_swapgs(void)

commit 0f9a48100fba3f189724ae88a450c2261bf91c80
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:28 2017 +0100

    x86/entry: Clean up the SYSENTER_stack code
    
    The existing code was a mess, mainly because C arrays are nasty.  Turn
    SYSENTER_stack into a struct, add a helper to find it, and do all the
    obvious cleanups this enables.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.653244723@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d34ac13c5866..f933869470b8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -336,12 +336,16 @@ struct x86_hw_tss {
 #define IO_BITMAP_OFFSET		(offsetof(struct tss_struct, io_bitmap) - offsetof(struct tss_struct, x86_tss))
 #define INVALID_IO_BITMAP_OFFSET	0x8000
 
+struct SYSENTER_stack {
+	unsigned long		words[64];
+};
+
 struct tss_struct {
 	/*
 	 * Space for the temporary SYSENTER stack, used for SYSENTER
 	 * and the entry trampoline as well.
 	 */
-	unsigned long		SYSENTER_stack[64];
+	struct SYSENTER_stack	SYSENTER_stack;
 
 	/*
 	 * The fixed hardware portion.  This must not cross a page boundary

commit 7fbbd5cbebf118a9e09f5453f686656a167c3d1c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:27 2017 +0100

    x86/entry/64: Remove the SYSENTER stack canary
    
    Now that the SYSENTER stack has a guard page, there's no need for a canary
    to detect overflow after the fact.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.572577316@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b0cf0612a454..d34ac13c5866 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -341,7 +341,6 @@ struct tss_struct {
 	 * Space for the temporary SYSENTER stack, used for SYSENTER
 	 * and the entry trampoline as well.
 	 */
-	unsigned long		SYSENTER_stack_canary;
 	unsigned long		SYSENTER_stack[64];
 
 	/*

commit 9aaefe7b59ae00605256a7d6bd1c1456432495fc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:21 2017 +0100

    x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
    
    On 64-bit kernels, we used to assume that TSS.sp0 was the current
    top of stack.  With the addition of an entry trampoline, this will
    no longer be the case.  Store the current top of stack in TSS.sp1,
    which is otherwise unused but shares the same cacheline.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.050864668@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 759051251664..b0cf0612a454 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -309,7 +309,13 @@ struct x86_hw_tss {
 struct x86_hw_tss {
 	u32			reserved1;
 	u64			sp0;
+
+	/*
+	 * We store cpu_current_top_of_stack in sp1 so it's always accessible.
+	 * Linux does not use ring 1, so sp1 is not otherwise needed.
+	 */
 	u64			sp1;
+
 	u64			sp2;
 	u64			reserved2;
 	u64			ist[7];
@@ -368,6 +374,8 @@ DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss);
 
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
+#else
+#define cpu_current_top_of_stack cpu_tss.x86_tss.sp1
 #endif
 
 /*
@@ -539,12 +547,12 @@ static inline void native_swapgs(void)
 
 static inline unsigned long current_top_of_stack(void)
 {
-#ifdef CONFIG_X86_64
-	return this_cpu_read_stable(cpu_tss.x86_tss.sp0);
-#else
-	/* sp0 on x86_32 is special in and around vm86 mode. */
+	/*
+	 *  We can't read directly from tss.sp0: sp0 on x86_32 is special in
+	 *  and around vm86 mode and sp0 on x86_64 is special because of the
+	 *  entry trampoline.
+	 */
 	return this_cpu_read_stable(cpu_current_top_of_stack);
-#endif
 }
 
 static inline bool on_thread_stack(void)

commit 1a935bc3d4ea61556461a9e92a68ca3556232efd
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:19 2017 +0100

    x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct
    
    SYSENTER_stack should have reliable overflow detection, which
    means that it needs to be at the bottom of a page, not the top.
    Move it to the beginning of struct tss_struct and page-align it.
    
    Also add an assertion to make sure that the fixed hardware TSS
    doesn't cross a page boundary.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.881827433@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 555c9478f3df..759051251664 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -332,7 +332,16 @@ struct x86_hw_tss {
 
 struct tss_struct {
 	/*
-	 * The hardware state:
+	 * Space for the temporary SYSENTER stack, used for SYSENTER
+	 * and the entry trampoline as well.
+	 */
+	unsigned long		SYSENTER_stack_canary;
+	unsigned long		SYSENTER_stack[64];
+
+	/*
+	 * The fixed hardware portion.  This must not cross a page boundary
+	 * at risk of violating the SDM's advice and potentially triggering
+	 * errata.
 	 */
 	struct x86_hw_tss	x86_tss;
 
@@ -343,15 +352,9 @@ struct tss_struct {
 	 * be within the limit.
 	 */
 	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
+} __aligned(PAGE_SIZE);
 
-	/*
-	 * Space for the temporary SYSENTER stack.
-	 */
-	unsigned long		SYSENTER_stack_canary;
-	unsigned long		SYSENTER_stack[64];
-} ____cacheline_aligned;
-
-DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);
+DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss);
 
 /*
  * sizeof(unsigned long) coming from an extra "long" at the end

commit 7fb983b4dd569e08564134a850dfd4eb1c63d9b8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:17 2017 +0100

    x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
    
    A future patch will move SYSENTER_stack to the beginning of cpu_tss
    to help detect overflow.  Before this can happen, fix several code
    paths that hardcode assumptions about the old layout.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.722425540@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 789dad5da20f..555c9478f3df 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -162,7 +162,7 @@ enum cpuid_regs_idx {
 extern struct cpuinfo_x86	boot_cpu_data;
 extern struct cpuinfo_x86	new_cpu_data;
 
-extern struct tss_struct	doublefault_tss;
+extern struct x86_hw_tss	doublefault_tss;
 extern __u32			cpu_caps_cleared[NCAPINTS];
 extern __u32			cpu_caps_set[NCAPINTS];
 
@@ -252,6 +252,11 @@ static inline void load_cr3(pgd_t *pgdir)
 	write_cr3(__sme_pa(pgdir));
 }
 
+/*
+ * Note that while the legacy 'TSS' name comes from 'Task State Segment',
+ * on modern x86 CPUs the TSS also holds information important to 64-bit mode,
+ * unrelated to the task-switch mechanism:
+ */
 #ifdef CONFIG_X86_32
 /* This is the TSS defined by the hardware. */
 struct x86_hw_tss {
@@ -322,7 +327,7 @@ struct x86_hw_tss {
 #define IO_BITMAP_BITS			65536
 #define IO_BITMAP_BYTES			(IO_BITMAP_BITS/8)
 #define IO_BITMAP_LONGS			(IO_BITMAP_BYTES/sizeof(long))
-#define IO_BITMAP_OFFSET		offsetof(struct tss_struct, io_bitmap)
+#define IO_BITMAP_OFFSET		(offsetof(struct tss_struct, io_bitmap) - offsetof(struct tss_struct, x86_tss))
 #define INVALID_IO_BITMAP_OFFSET	0x8000
 
 struct tss_struct {

commit 1a79797b58cddfa948420a7553241c79c013e3ca
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:12 2017 +0100

    x86/entry/64: Allocate and enable the SYSENTER stack
    
    This will simplify future changes that want scratch variables early in
    the SYSENTER handler -- they'll be able to spill registers to the
    stack.  It also lets us get rid of a SWAPGS_UNSAFE_STACK user.
    
    This does not depend on CONFIG_IA32_EMULATION=y because we'll want the
    stack space even without IA32 emulation.
    
    As far as I can tell, the reason that this wasn't done from day 1 is
    that we use IST for #DB and #BP, which is IMO rather nasty and causes
    a lot more problems than it solves.  But, since #DB uses IST, we don't
    actually need a real stack for SYSENTER (because SYSENTER with TF set
    will invoke #DB on the IST stack rather than the SYSENTER stack).
    
    I want to remove IST usage from these vectors some day, and this patch
    is a prerequisite for that as well.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.312726423@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 2db7cf720b04..789dad5da20f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -339,14 +339,11 @@ struct tss_struct {
 	 */
 	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
 
-#ifdef CONFIG_X86_32
 	/*
 	 * Space for the temporary SYSENTER stack.
 	 */
 	unsigned long		SYSENTER_stack_canary;
 	unsigned long		SYSENTER_stack[64];
-#endif
-
 } ____cacheline_aligned;
 
 DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);

commit 0fd2e9c53d82704a3ba87ea1980ec515188c5316
Merge: 1784f9144b14 1e4c4f610f77
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Dec 1 10:32:48 2017 +0100

    Merge commit 'upstream-x86-entry' into WIP.x86/mm
    
    Pull in a minimal set of v4.15 entry code changes, for a base for the MM isolation patches.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 30bb9811856f667042e746d8033883b1091a46ce
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Nov 14 07:42:56 2017 -0500

    x86/topology: Avoid wasting 128k for package id array
    
    Analyzing large early boot allocations unveiled the logical package id
    storage as a prominent memory waste. Since commit 1f12e32f4cd5
    ("x86/topology: Create logical package id") every 64-bit system allocates a
    128k array to convert logical package ids.
    
    This happens because the array is sized for MAX_LOCAL_APIC which is always
    32k on 64bit systems, and it needs 4 bytes for each entry.
    
    This is fairly wasteful, especially for the common case of having only one
    socket, which uses exactly 4 byte out of 128K.
    
    There is no user of the package id map which is performance critical, so
    the lookup is not required to be O(1). Store the logical processor id in
    cpu_data and use a loop based lookup.
    
    To keep the mapping stable accross cpu hotplug operations, add a flag to
    cpu_data which is set when the CPU is brought up the first time. When the
    flag is set, then cpu_data is not reinitialized by copying boot_cpu_data on
    subsequent bringups.
    
    [ tglx: Rename the flag to 'initialized', use proper pointers instead of
            repeated cpu_data(x) evaluation and massage changelog. ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: He Chen <he.chen@linux.intel.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Piotr Luc <piotr.luc@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arvind Yadav <arvind.yadav.cs@gmail.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: https://lkml.kernel.org/r/20171114124257.22013-3-prarit@redhat.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 2db7cf720b04..cc16fa882e3e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -132,6 +132,7 @@ struct cpuinfo_x86 {
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 	u32			microcode;
+	unsigned		initialized : 1;
 } __randomize_layout;
 
 struct cpuid_regs {

commit b3d9a136815ca9284ade2a897a3b7d2b0084c33c
Merge: c7da092a1f24 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:53:06 2017 +0100

    Merge branch 'linus' into x86/asm, to pick up fixes and resolve conflicts
    
    Conflicts:
            arch/x86/kernel/cpu/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b390ff76e58f..bdac19ab2488 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_PROCESSOR_H
 #define _ASM_X86_PROCESSOR_H
 

commit 3383642c2f9d4f5b4fa37436db4a109a1a10018c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:17 2017 -0700

    x86/traps: Use a new on_thread_stack() helper to clean up an assertion
    
    Let's keep the stack-related logic together rather than open-coding
    a comparison in an assertion in the traps code.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/856b15bee1f55017b8f79d3758b0d51c48a08cf8.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ae2ae6d80674..f10dae14f951 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -541,6 +541,12 @@ static inline unsigned long current_top_of_stack(void)
 #endif
 }
 
+static inline bool on_thread_stack(void)
+{
+	return (unsigned long)(current_top_of_stack() -
+			       current_stack_pointer) < THREAD_SIZE;
+}
+
 #ifdef CONFIG_PARAVIRT
 #include <asm/paravirt.h>
 #else

commit d375cf1530595e33961a8844192cddab913650e3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:16 2017 -0700

    x86/entry/64: Remove thread_struct::sp0
    
    On x86_64, we can easily calculate sp0 when needed instead of
    storing it in thread_struct.
    
    On x86_32, a similar cleanup would be possible, but it would require
    cleaning up the vm86 code first, and that can wait for a later
    cleanup series.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/719cd9c66c548c4350d98a90f050aee8b17f8919.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ad59cec14239..ae2ae6d80674 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -430,7 +430,9 @@ typedef struct {
 struct thread_struct {
 	/* Cached TLS descriptors: */
 	struct desc_struct	tls_array[GDT_ENTRY_TLS_ENTRIES];
+#ifdef CONFIG_X86_32
 	unsigned long		sp0;
+#endif
 	unsigned long		sp;
 #ifdef CONFIG_X86_32
 	unsigned long		sysenter_cs;
@@ -797,6 +799,13 @@ static inline void spin_lock_prefetch(const void *x)
 
 #define task_top_of_stack(task) ((unsigned long)(task_pt_regs(task) + 1))
 
+#define task_pt_regs(task) \
+({									\
+	unsigned long __ptr = (unsigned long)task_stack_page(task);	\
+	__ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;		\
+	((struct pt_regs *)__ptr) - 1;					\
+})
+
 #ifdef CONFIG_X86_32
 /*
  * User space process size: 3GB (default).
@@ -816,23 +825,6 @@ static inline void spin_lock_prefetch(const void *x)
 	.addr_limit		= KERNEL_DS,				  \
 }
 
-/*
- * TOP_OF_KERNEL_STACK_PADDING reserves 8 bytes on top of the ring0 stack.
- * This is necessary to guarantee that the entire "struct pt_regs"
- * is accessible even if the CPU haven't stored the SS/ESP registers
- * on the stack (interrupt gate does not save these registers
- * when switching to the same priv ring).
- * Therefore beware: accessing the ss/esp fields of the
- * "struct pt_regs" is possible, but they may contain the
- * completely wrong values.
- */
-#define task_pt_regs(task) \
-({									\
-	unsigned long __ptr = (unsigned long)task_stack_page(task);	\
-	__ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;		\
-	((struct pt_regs *)__ptr) - 1;					\
-})
-
 #define KSTK_ESP(task)		(task_pt_regs(task)->sp)
 
 #else
@@ -866,11 +858,9 @@ static inline void spin_lock_prefetch(const void *x)
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
-	.sp0			= TOP_OF_INIT_STACK,		\
 	.addr_limit		= KERNEL_DS,			\
 }
 
-#define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.sp0 - 1)
 extern unsigned long KSTK_ESP(struct task_struct *task);
 
 #endif /* CONFIG_X86_64 */

commit 3500130b84a3cdc5b6796eba1daf178944935efe
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:11 2017 -0700

    x86/entry: Add task_top_of_stack() to find the top of a task's stack
    
    This will let us get rid of a few places that hardcode accesses to
    thread.sp0.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/b49b3f95a8ff858c40c9b0f5b32be0355324327d.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 064b84722166..ad59cec14239 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -795,6 +795,8 @@ static inline void spin_lock_prefetch(const void *x)
 #define TOP_OF_INIT_STACK ((unsigned long)&init_stack + sizeof(init_stack) - \
 			   TOP_OF_KERNEL_STACK_PADDING)
 
+#define task_top_of_stack(task) ((unsigned long)(task_pt_regs(task) + 1))
+
 #ifdef CONFIG_X86_32
 /*
  * User space process size: 3GB (default).

commit da51da189a24bb9b7e2d5a123be096e51a4695a5
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:10 2017 -0700

    x86/entry/64: Pass SP0 directly to load_sp0()
    
    load_sp0() had an odd signature:
    
      void load_sp0(struct tss_struct *tss, struct thread_struct *thread);
    
    Simplify it to:
    
      void load_sp0(unsigned long sp0);
    
    Also simplify a few get_cpu()/put_cpu() sequences to
    preempt_disable()/preempt_enable().
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2655d8b42ed940aa384fe18ee1129bbbcf730a08.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0167e3e35a57..064b84722166 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -517,9 +517,9 @@ static inline void native_set_iopl_mask(unsigned mask)
 }
 
 static inline void
-native_load_sp0(struct tss_struct *tss, struct thread_struct *thread)
+native_load_sp0(unsigned long sp0)
 {
-	tss->x86_tss.sp0 = thread->sp0;
+	this_cpu_write(cpu_tss.x86_tss.sp0, sp0);
 }
 
 static inline void native_swapgs(void)
@@ -544,10 +544,9 @@ static inline unsigned long current_top_of_stack(void)
 #else
 #define __cpuid			native_cpuid
 
-static inline void load_sp0(struct tss_struct *tss,
-			    struct thread_struct *thread)
+static inline void load_sp0(unsigned long sp0)
 {
-	native_load_sp0(tss, thread);
+	native_load_sp0(sp0);
 }
 
 #define set_iopl_mask native_set_iopl_mask

commit bd7dc5a6afac719d8ce4092391eef2c7e83c2a75
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:09 2017 -0700

    x86/entry/32: Pull the MSR_IA32_SYSENTER_CS update code out of native_load_sp0()
    
    This causes the MSR_IA32_SYSENTER_CS write to move out of the
    paravirt callback.  This shouldn't affect Xen PV: Xen already ignores
    MSR_IA32_SYSENTER_ESP writes.  In any event, Xen doesn't support
    vm86() in a useful way.
    
    Note to any potential backporters: This patch won't break lguest, as
    lguest didn't have any SYSENTER support at all.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/75cf09fe03ae778532d0ca6c65aa58e66bc2f90c.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b390ff76e58f..0167e3e35a57 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -520,13 +520,6 @@ static inline void
 native_load_sp0(struct tss_struct *tss, struct thread_struct *thread)
 {
 	tss->x86_tss.sp0 = thread->sp0;
-#ifdef CONFIG_X86_32
-	/* Only happens when SEP is enabled, no need to test "SEP"arately: */
-	if (unlikely(tss->x86_tss.ss1 != thread->sysenter_cs)) {
-		tss->x86_tss.ss1 = thread->sysenter_cs;
-		wrmsr(MSR_IA32_SYSENTER_CS, thread->sysenter_cs, 0);
-	}
-#endif
 }
 
 static inline void native_swapgs(void)

commit f5caf621ee357279e759c0911daf6d55c7d36f03
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Sep 20 16:24:33 2017 -0500

    x86/asm: Fix inline asm call constraints for Clang
    
    For inline asm statements which have a CALL instruction, we list the
    stack pointer as a constraint to convince GCC to ensure the frame
    pointer is set up first:
    
      static inline void foo()
      {
            register void *__sp asm(_ASM_SP);
            asm("call bar" : "+r" (__sp))
      }
    
    Unfortunately, that pattern causes Clang to corrupt the stack pointer.
    
    The fix is easy: convert the stack pointer register variable to a global
    variable.
    
    It should be noted that the end result is different based on the GCC
    version.  With GCC 6.4, this patch has exactly the same result as
    before:
    
            defconfig       defconfig-nofp  distro          distro-nofp
     before 9820389         9491555         8816046         8516940
     after  9820389         9491555         8816046         8516940
    
    With GCC 7.2, however, GCC's behavior has changed.  It now changes its
    behavior based on the conversion of the register variable to a global.
    That somehow convinces it to *always* set up the frame pointer before
    inserting *any* inline asm.  (Therefore, listing the variable as an
    output constraint is a no-op and is no longer necessary.)  It's a bit
    overkill, but the performance impact should be negligible.  And in fact,
    there's a nice improvement with frame pointers disabled:
    
            defconfig       defconfig-nofp  distro          distro-nofp
     before 9796316         9468236         9076191         8790305
     after  9796957         9464267         9076381         8785949
    
    So in summary, while listing the stack pointer as an output constraint
    is no longer necessary for newer versions of GCC, it's still needed for
    older versions.
    
    Suggested-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reported-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dmitriy Vyukov <dvyukov@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Miguel Bernal Marin <miguel.bernal.marin@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/3db862e970c432ae823cf515c52b54fec8270e0e.1505942196.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3fa26a61eabc..b390ff76e58f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -677,8 +677,6 @@ static inline void sync_core(void)
 	 * Like all of Linux's memory ordering operations, this is a
 	 * compiler barrier as well.
 	 */
-	register void *__sp asm(_ASM_SP);
-
 #ifdef CONFIG_X86_32
 	asm volatile (
 		"pushfl\n\t"
@@ -686,7 +684,7 @@ static inline void sync_core(void)
 		"pushl $1f\n\t"
 		"iret\n\t"
 		"1:"
-		: "+r" (__sp) : : "memory");
+		: ASM_CALL_CONSTRAINT : : "memory");
 #else
 	unsigned int tmp;
 
@@ -703,7 +701,7 @@ static inline void sync_core(void)
 		"iretq\n\t"
 		UNWIND_HINT_RESTORE
 		"1:"
-		: "=&r" (tmp), "+r" (__sp) : : "cc", "memory");
+		: "=&r" (tmp), ASM_CALL_CONSTRAINT : : "cc", "memory");
 #endif
 }
 

commit b1b6f83ac938d176742c85757960dec2cf10e468
Merge: 5f82e71a001d 9e52fc2b50de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 12:21:28 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "PCID support, 5-level paging support, Secure Memory Encryption support
    
      The main changes in this cycle are support for three new, complex
      hardware features of x86 CPUs:
    
       - Add 5-level paging support, which is a new hardware feature on
         upcoming Intel CPUs allowing up to 128 PB of virtual address space
         and 4 PB of physical RAM space - a 512-fold increase over the old
         limits. (Supercomputers of the future forecasting hurricanes on an
         ever warming planet can certainly make good use of more RAM.)
    
         Many of the necessary changes went upstream in previous cycles,
         v4.14 is the first kernel that can enable 5-level paging.
    
         This feature is activated via CONFIG_X86_5LEVEL=y - disabled by
         default.
    
         (By Kirill A. Shutemov)
    
       - Add 'encrypted memory' support, which is a new hardware feature on
         upcoming AMD CPUs ('Secure Memory Encryption', SME) allowing system
         RAM to be encrypted and decrypted (mostly) transparently by the
         CPU, with a little help from the kernel to transition to/from
         encrypted RAM. Such RAM should be more secure against various
         attacks like RAM access via the memory bus and should make the
         radio signature of memory bus traffic harder to intercept (and
         decrypt) as well.
    
         This feature is activated via CONFIG_AMD_MEM_ENCRYPT=y - disabled
         by default.
    
         (By Tom Lendacky)
    
       - Enable PCID optimized TLB flushing on newer Intel CPUs: PCID is a
         hardware feature that attaches an address space tag to TLB entries
         and thus allows to skip TLB flushing in many cases, even if we
         switch mm's.
    
         (By Andy Lutomirski)
    
      All three of these features were in the works for a long time, and
      it's coincidence of the three independent development paths that they
      are all enabled in v4.14 at once"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (65 commits)
      x86/mm: Enable RCU based page table freeing (CONFIG_HAVE_RCU_TABLE_FREE=y)
      x86/mm: Use pr_cont() in dump_pagetable()
      x86/mm: Fix SME encryption stack ptr handling
      kvm/x86: Avoid clearing the C-bit in rsvd_bits()
      x86/CPU: Align CR3 defines
      x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages
      acpi, x86/mm: Remove encryption mask from ACPI page protection type
      x86/mm, kexec: Fix memory corruption with SME on successive kexecs
      x86/mm/pkeys: Fix typo in Documentation/x86/protection-keys.txt
      x86/mm/dump_pagetables: Speed up page tables dump for CONFIG_KASAN=y
      x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID
      x86: Enable 5-level paging support via CONFIG_X86_5LEVEL=y
      x86/mm: Allow userspace have mappings above 47-bit
      x86/mm: Prepare to expose larger address space to userspace
      x86/mpx: Do not allow MPX if we have mappings above 47-bit
      x86/mm: Rename tasksize_32bit/64bit to task_size_32bit/64bit()
      x86/xen: Redefine XEN_ELFNOTE_INIT_P2M using PUD_SIZE * PTRS_PER_PUD
      x86/mm/dump_pagetables: Fix printout of p4d level
      x86/mm/dump_pagetables: Generalize address normalization
      x86/boot: Fix memremap() related build failure
      ...

commit 413d63d71b222108d19703f3fd5cf9108652a730
Merge: d6c8103b0265 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Aug 26 09:19:13 2017 +0200

    Merge branch 'linus' into x86/mm to pick up fixes and to fix conflicts
    
    Conflicts:
            arch/x86/kernel/head64.c
            arch/x86/mm/mmap.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ecda85e70277ef24e44a1f6bc00243cebd19f985
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Aug 16 19:31:57 2017 +0200

    x86/lguest: Remove lguest support
    
    Lguest seems to be rather unused these days. It has seen only patches
    ensuring it still builds the last two years and its official state is
    "Odd Fixes".
    
    Remove it in order to be able to clean up the paravirt code.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: boris.ostrovsky@oracle.com
    Cc: lguest@lists.ozlabs.org
    Cc: rusty@rustcorp.com.au
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20170816173157.8633-3-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0b03d655db7c..abc99b9c7ffd 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -662,7 +662,7 @@ static inline void sync_core(void)
 	 * In case NMI unmasking or performance ever becomes a problem,
 	 * the next best option appears to be MOV-to-CR2 and an
 	 * unconditional jump.  That sequence also works on all CPUs,
-	 * but it will fault at CPL3 (i.e. Xen PV and lguest).
+	 * but it will fault at CPL3 (i.e. Xen PV).
 	 *
 	 * CPUID is the conventional way, but it's nasty: it doesn't
 	 * exist on some 486-like CPUs, and it usually exits to a

commit 1d0f49e14007a5426eb7e9e5808168cdb77b3e7f
Merge: 99504819fc64 e93c17301ac5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Aug 10 13:14:15 2017 +0200

    Merge branch 'x86/urgent' into x86/asm, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ee00f4a32a76ef631394f31d5b6028d50462b357
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Jul 17 01:59:53 2017 +0300

    x86/mm: Allow userspace have mappings above 47-bit
    
    All bits and pieces are now in place and we can allow userspace to have VMAs
    above 47 bits.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170716225954.74185-8-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 58fd40a3bf52..ed26e14cfc2f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -850,9 +850,9 @@ static inline void spin_lock_prefetch(const void *x)
  * particular problem by preventing anything from being mapped
  * at the maximum canonical address.
  */
-#define TASK_SIZE_MAX	((1UL << 47) - PAGE_SIZE)
+#define TASK_SIZE_MAX	((1UL << __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
-#define DEFAULT_MAP_WINDOW	TASK_SIZE_MAX
+#define DEFAULT_MAP_WINDOW	((1UL << 47) - PAGE_SIZE)
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap's.

commit b569bab78d8df157a6f91070af827753e4d1787c
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Jul 17 01:59:52 2017 +0300

    x86/mm: Prepare to expose larger address space to userspace
    
    On x86, 5-level paging enables 56-bit userspace virtual address space.
    Not all user space is ready to handle wide addresses. It's known that
    at least some JIT compilers use higher bits in pointers to encode their
    information. It collides with valid pointers with 5-level paging and
    leads to crashes.
    
    To mitigate this, we are not going to allocate virtual address space
    above 47-bit by default.
    
    But userspace can ask for allocation from full address space by
    specifying hint address (with or without MAP_FIXED) above 47-bits.
    
    If hint address set above 47-bit, but MAP_FIXED is not specified, we try
    to look for unmapped area by specified address. If it's already
    occupied, we look for unmapped area in *full* address space, rather than
    from 47-bit window.
    
    A high hint address would only affect the allocation in question, but not
    any future mmap()s.
    
    Specifying high hint address on older kernel or on machine without 5-level
    paging support is safe. The hint will be ignored and kernel will fall back
    to allocation from 47-bit address space.
    
    This approach helps to easily make application's memory allocator aware
    about large address space without manually tracking allocated virtual
    address space.
    
    The patch puts all machinery in place, but not yet allows userspace to have
    mappings above 47-bit -- TASK_SIZE_MAX has to be raised to get the effect.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170716225954.74185-7-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c8d59fcc5665..58fd40a3bf52 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -808,6 +808,7 @@ static inline void spin_lock_prefetch(const void *x)
  */
 #define IA32_PAGE_OFFSET	PAGE_OFFSET
 #define TASK_SIZE		PAGE_OFFSET
+#define TASK_SIZE_LOW		TASK_SIZE
 #define TASK_SIZE_MAX		TASK_SIZE
 #define DEFAULT_MAP_WINDOW	TASK_SIZE
 #define STACK_TOP		TASK_SIZE
@@ -859,12 +860,14 @@ static inline void spin_lock_prefetch(const void *x)
 #define IA32_PAGE_OFFSET	((current->personality & ADDR_LIMIT_3GB) ? \
 					0xc0000000 : 0xFFFFe000)
 
+#define TASK_SIZE_LOW		(test_thread_flag(TIF_ADDR32) ? \
+					IA32_PAGE_OFFSET : DEFAULT_MAP_WINDOW)
 #define TASK_SIZE		(test_thread_flag(TIF_ADDR32) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
-#define STACK_TOP		TASK_SIZE
+#define STACK_TOP		TASK_SIZE_LOW
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
@@ -885,7 +888,7 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
  * space during mmap's.
  */
 #define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))
-#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)
+#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE_LOW)
 
 #define KSTK_EIP(task)		(task_pt_regs(task)->ip)
 

commit 44b04912fa72489d403738f39e1c782614b7ae7c
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Jul 17 01:59:51 2017 +0300

    x86/mpx: Do not allow MPX if we have mappings above 47-bit
    
    MPX (without MAWA extension) cannot handle addresses above 47 bits, so we
    need to make sure that MPX cannot be enabled if we already have a VMA above
    the boundary and forbid creating such VMAs once MPX is enabled.
    
    The patch implements mpx_unmapped_area_check() which is called from all
    variants of get_unmapped_area() to check if the requested address fits
    mpx.
    
    On enabling MPX, we check if we already have any vma above 47-bit
    boundary and forbit the enabling if we do.
    
    As long as DEFAULT_MAP_WINDOW is equal to TASK_SIZE_MAX, the change is
    nop. It will change when we allow userspace to have mappings above
    47-bits.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170716225954.74185-6-kirill.shutemov@linux.intel.com
    [ Readability edits. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 973709d2938f..c8d59fcc5665 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -809,6 +809,7 @@ static inline void spin_lock_prefetch(const void *x)
 #define IA32_PAGE_OFFSET	PAGE_OFFSET
 #define TASK_SIZE		PAGE_OFFSET
 #define TASK_SIZE_MAX		TASK_SIZE
+#define DEFAULT_MAP_WINDOW	TASK_SIZE
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		STACK_TOP
 
@@ -850,6 +851,8 @@ static inline void spin_lock_prefetch(const void *x)
  */
 #define TASK_SIZE_MAX	((1UL << 47) - PAGE_SIZE)
 
+#define DEFAULT_MAP_WINDOW	TASK_SIZE_MAX
+
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap's.
  */

commit e06fdaf40a5c021dd4a2ec797e8b724f07360070
Merge: a90c6ac2b565 8acdf5055974
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 19 08:55:18 2017 -0700

    Merge tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull structure randomization updates from Kees Cook:
     "Now that IPC and other changes have landed, enable manual markings for
      randstruct plugin, including the task_struct.
    
      This is the rest of what was staged in -next for the gcc-plugins, and
      comes in three patches, largest first:
    
       - mark "easy" structs with __randomize_layout
    
       - mark task_struct with an optional anonymous struct to isolate the
         __randomize_layout section
    
       - mark structs to opt _out_ of automated marking (which will come
         later)
    
      And, FWIW, this continues to pass allmodconfig (normal and patched to
      enable gcc-plugins) builds of x86_64, i386, arm64, arm, powerpc, and
      s390 for me"
    
    * tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      randstruct: opt-out externally exposed function pointer structs
      task_struct: Allow randomized layout
      randstruct: Mark various structs for randomization

commit eef9c4abe77f55b1600f59d8ac5f1d953e2f5384
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:08 2017 -0500

    x86/mm: Add SME support for read_cr3_pa()
    
    The CR3 register entry can contain the SME encryption mask that indicates
    the PGD is encrypted.  The encryption mask should not be used when
    creating a virtual address from the CR3 register, so remove the SME
    encryption mask in the read_cr3_pa() function.
    
    During early boot SME will need to use a native version of read_cr3_pa(),
    so create native_read_cr3_pa().
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/767b085c384a46f67f451f8589903a462c7ff68a.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index a68f70c3debc..973709d2938f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -240,6 +240,11 @@ static inline unsigned long read_cr3_pa(void)
 	return __read_cr3() & CR3_ADDR_MASK;
 }
 
+static inline unsigned long native_read_cr3_pa(void)
+{
+	return __native_read_cr3() & CR3_ADDR_MASK;
+}
+
 static inline void load_cr3(pgd_t *pgdir)
 {
 	write_cr3(__sme_pa(pgdir));

commit 21729f81ce8ae76a6995681d40e16f7ce8075db4
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:07 2017 -0500

    x86/mm: Provide general kernel support for memory encryption
    
    Changes to the existing page table macros will allow the SME support to
    be enabled in a simple fashion with minimal changes to files that use these
    macros.  Since the memory encryption mask will now be part of the regular
    pagetable macros, we introduce two new macros (_PAGE_TABLE_NOENC and
    _KERNPG_TABLE_NOENC) to allow for early pagetable creation/initialization
    without the encryption mask before SME becomes active.  Two new pgprot()
    macros are defined to allow setting or clearing the page encryption mask.
    
    The FIXMAP_PAGE_NOCACHE define is introduced for use with MMIO.  SME does
    not support encryption for MMIO areas so this define removes the encryption
    mask from the page attribute.
    
    Two new macros are introduced (__sme_pa() / __sme_pa_nodebug()) to allow
    creating a physical address with the encryption mask.  These are used when
    working with the cr3 register so that the PGD can be encrypted. The current
    __va() macro is updated so that the virtual address is generated based off
    of the physical address without the encryption mask thus allowing the same
    virtual address to be generated regardless of whether encryption is enabled
    for that physical location or not.
    
    Also, an early initialization function is added for SME.  If SME is active,
    this function:
    
     - Updates the early_pmd_flags so that early page faults create mappings
       with the encryption mask.
    
     - Updates the __supported_pte_mask to include the encryption mask.
    
     - Updates the protection_map entries to include the encryption mask so
       that user-space allocations will automatically have the encryption mask
       applied.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/b36e952c4c39767ae7f0a41cf5345adf27438480.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6a79547e8ee0..a68f70c3debc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -29,6 +29,7 @@ struct vm86;
 #include <linux/math64.h>
 #include <linux/err.h>
 #include <linux/irqflags.h>
+#include <linux/mem_encrypt.h>
 
 /*
  * We handle most unaligned accesses in hardware.  On the other hand
@@ -241,7 +242,7 @@ static inline unsigned long read_cr3_pa(void)
 
 static inline void load_cr3(pgd_t *pgdir)
 {
-	write_cr3(__pa(pgdir));
+	write_cr3(__sme_pa(pgdir));
 }
 
 #ifdef CONFIG_X86_32

commit 76846bf3cb09e98881cb4908385a0e899716b01f
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Tue Jul 11 10:33:45 2017 -0500

    x86/asm: Add unwind hint annotations to sync_core()
    
    This enables objtool to grok the iret in the middle of a C function.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: live-patching@vger.kernel.org
    Link: http://lkml.kernel.org/r/b057be26193c11d2ed3337b2107bc7adcba42c99.1499786555.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6a79547e8ee0..b27dc9b4a148 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -22,6 +22,7 @@ struct vm86;
 #include <asm/nops.h>
 #include <asm/special_insns.h>
 #include <asm/fpu/types.h>
+#include <asm/unwind_hints.h>
 
 #include <linux/personality.h>
 #include <linux/cache.h>
@@ -684,6 +685,7 @@ static inline void sync_core(void)
 	unsigned int tmp;
 
 	asm volatile (
+		UNWIND_HINT_SAVE
 		"mov %%ss, %0\n\t"
 		"pushq %q0\n\t"
 		"pushq %%rsp\n\t"
@@ -693,6 +695,7 @@ static inline void sync_core(void)
 		"pushq %q0\n\t"
 		"pushq $1f\n\t"
 		"iretq\n\t"
+		UNWIND_HINT_RESTORE
 		"1:"
 		: "=&r" (tmp), "+r" (__sp) : : "cc", "memory");
 #endif

commit 4422d80ed7d4bdb2d6e9fb890c66c3d9250ba694
Merge: 9a9594efe543 32288daf3f24
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 18:33:03 2017 -0700

    Merge branch 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RAS updates from Thomas Gleixner:
     "The RAS updates for the 4.13 merge window:
    
       - Cleanup of the MCE injection facility (Borsilav Petkov)
    
       - Rework of the AMD/SMCA handling (Yazen Ghannam)
    
       - Enhancements for ACPI/APEI to handle new notitication types (Shiju
         Jose)
    
       - atomic_t to refcount_t conversion (Elena Reshetova)
    
       - A few fixes and enhancements all over the place"
    
    * 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      RAS/CEC: Check the correct variable in the debugfs error handling
      x86/mce: Always save severity in machine_check_poll()
      x86/MCE, xen/mcelog: Make /dev/mcelog registration messages more precise
      x86/mce: Update bootlog description to reflect behavior on AMD
      x86/mce: Don't disable MCA banks when offlining a CPU on AMD
      x86/mce/mce-inject: Preset the MCE injection struct
      x86/mce: Clean up include files
      x86/mce: Get rid of register_mce_write_callback()
      x86/mce: Merge mce_amd_inj into mce-inject
      x86/mce/AMD: Use saved threshold block info in interrupt handler
      x86/mce/AMD: Use msr_stat when clearing MCA_STATUS
      x86/mce/AMD: Carve out SMCA bank configuration
      x86/mce/AMD: Redo error logging from APIC LVT interrupt handlers
      x86/mce: Convert threshold_bank.cpus from atomic_t to refcount_t
      RAS: Make local function parse_ras_param() static
      ACPI/APEI: Handle GSIV and GPIO notification types

commit 7a69f9c60b49699579f5bfb71f928cceba0afe1a
Merge: 9bc088ab66be 8781fb7e9749
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 14:45:09 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Continued work to add support for 5-level paging provided by future
         Intel CPUs. In particular we switch the x86 GUP code to the generic
         implementation. (Kirill A. Shutemov)
    
       - Continued work to add PCID CPU support to native kernels as well.
         In this round most of the focus is on reworking/refreshing the TLB
         flush infrastructure for the upcoming PCID changes. (Andy
         Lutomirski)"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (34 commits)
      x86/mm: Delete a big outdated comment about TLB flushing
      x86/mm: Don't reenter flush_tlb_func_common()
      x86/KASLR: Fix detection 32/64 bit bootloaders for 5-level paging
      x86/ftrace: Exclude functions in head64.c from function-tracing
      x86/mmap, ASLR: Do not treat unlimited-stack tasks as legacy mmap
      x86/mm: Remove reset_lazy_tlbstate()
      x86/ldt: Simplify the LDT switching logic
      x86/boot/64: Put __startup_64() into .head.text
      x86/mm: Add support for 5-level paging for KASLR
      x86/mm: Make kernel_physical_mapping_init() support 5-level paging
      x86/mm: Add sync_global_pgds() for configuration with 5-level paging
      x86/boot/64: Add support of additional page table level during early boot
      x86/boot/64: Rename init_level4_pgt and early_level4_pgt
      x86/boot/64: Rewrite startup_64() in C
      x86/boot/compressed: Enable 5-level paging during decompression stage
      x86/boot/efi: Define __KERNEL32_CS GDT on 64-bit configurations
      x86/boot/efi: Fix __KERNEL_CS definition of GDT entry on 64-bit configurations
      x86/boot/efi: Cleanup initialization of GDT entries
      x86/asm: Fix comment in return_from_SYSCALL_64()
      x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation
      ...

commit 3859a271a003aba01e45b85c9d8b355eb7bf25f9
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Oct 28 01:22:25 2016 -0700

    randstruct: Mark various structs for randomization
    
    This marks many critical kernel structures for randomization. These are
    structures that have been targeted in the past in security exploits, or
    contain functions pointers, pointers to function pointer tables, lists,
    workqueues, ref-counters, credentials, permissions, or are otherwise
    sensitive. This initial list was extracted from Brad Spengler/PaX Team's
    code in the last public patch of grsecurity/PaX based on my understanding
    of the code. Changes or omissions from the original code are mine and
    don't reflect the original grsecurity/PaX code.
    
    Left out of this list is task_struct, which requires special handling
    and will be covered in a subsequent patch.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3cada998a402..e2335edb9fc5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -129,7 +129,7 @@ struct cpuinfo_x86 {
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 	u32			microcode;
-};
+} __randomize_layout;
 
 struct cpuid_regs {
 	u32 eax, ebx, ecx, edx;

commit 6474924e2b5ddb0030c355558966adcbe3b49022
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Wed Jun 28 15:30:02 2017 +0200

    arch: remove unused macro/function thread_saved_pc()
    
    The only user of thread_saved_pc() in non-arch-specific code was removed
    in commit 8243d5597793 ("sched/core: Remove pointless printout in
    sched_show_task()").  Remove the implementations as well.
    
    Some architectures use thread_saved_pc() in their arch-specific code.
    Leave their thread_saved_pc() intact.
    
    Signed-off-by: Tobias Klauser <tklauser@distanz.ch>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3cada998a402..a28b671f1549 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -860,8 +860,6 @@ extern unsigned long KSTK_ESP(struct task_struct *task);
 
 #endif /* CONFIG_X86_64 */
 
-extern unsigned long thread_saved_pc(struct task_struct *tsk);
-
 extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 					       unsigned long new_sp);
 

commit bc8e80d56c1ecb35e65df392d7601d1427d14efe
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jun 13 18:28:30 2017 +0200

    x86/mce: Merge mce_amd_inj into mce-inject
    
    Reuse mce_amd_inj's debugfs interface so that mce-inject can
    benefit from it too. The old functionality is still preserved under
    CONFIG_X86_MCELOG_LEGACY.
    
    Tested-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/20170613162835.30750-4-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3cada998a402..71f6fba95aa6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -901,8 +901,13 @@ static inline int mpx_disable_management(void)
 }
 #endif /* CONFIG_X86_INTEL_MPX */
 
+#ifdef CONFIG_CPU_SUP_AMD
 extern u16 amd_get_nb_id(int cpu);
 extern u32 amd_get_nodes_per_socket(void);
+#else
+static inline u16 amd_get_nb_id(int cpu)		{ return 0; }
+static inline u32 amd_get_nodes_per_socket(void)	{ return 0; }
+#endif
 
 static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
 {

commit 6c690ee1039b251e583fc65b28da30e97d6a7385
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Jun 12 10:26:14 2017 -0700

    x86/mm: Split read_cr3() into read_cr3_pa() and __read_cr3()
    
    The kernel has several code paths that read CR3.  Most of them assume that
    CR3 contains the PGD's physical address, whereas some of them awkwardly
    use PHYSICAL_PAGE_MASK to mask off low bits.
    
    Add explicit mask macros for CR3 and convert all of the CR3 readers.
    This will keep them from breaking when PCID is enabled.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/883f8fb121f4616c1c1427ad87350bb2f5ffeca1.1497288170.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3cada998a402..9de02c985aa4 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -231,6 +231,14 @@ native_cpuid_reg(ebx)
 native_cpuid_reg(ecx)
 native_cpuid_reg(edx)
 
+/*
+ * Friendlier CR3 helpers.
+ */
+static inline unsigned long read_cr3_pa(void)
+{
+	return __read_cr3() & CR3_ADDR_MASK;
+}
+
 static inline void load_cr3(pgd_t *pgdir)
 {
 	write_cr3(__pa(pgdir));

commit 73fa1362a7f337d149f76d26b6c6845cb38c1af9
Merge: fdd3d8ce0ea6 6415813bae75
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Mar 30 09:07:54 2017 +0200

    Merge branch 'x86/cpu' into x86/mm, before applying dependent patch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1f9ca184049724209cddbddac593d13eb87e48c0
Merge: b23adb7d3f7d d582799fe5de
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Mar 23 08:28:19 2017 +0100

    Merge branch 'x86/process' into x86/mm, to create new base for further patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e9ea1e7f53b852147cbd568b0568c7ad97ec21a3
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:26 2017 -0700

    x86/arch_prctl: Add ARCH_[GET|SET]_CPUID
    
    Intel supports faulting on the CPUID instruction beginning with Ivy Bridge.
    When enabled, the processor will fault on attempts to execute the CPUID
    instruction with CPL>0. Exposing this feature to userspace will allow a
    ptracer to trap and emulate the CPUID instruction.
    
    When supported, this feature is controlled by toggling bit 0 of
    MSR_MISC_FEATURES_ENABLES. It is documented in detail in Section 2.3.2 of
    https://bugzilla.kernel.org/attachment.cgi?id=243991
    
    Implement a new pair of arch_prctls, available on both x86-32 and x86-64.
    
    ARCH_GET_CPUID: Returns the current CPUID state, either 0 if CPUID faulting
        is enabled (and thus the CPUID instruction is not available) or 1 if
        CPUID faulting is not enabled.
    
    ARCH_SET_CPUID: Set the CPUID state to the second argument. If
        cpuid_enabled is 0 CPUID faulting will be activated, otherwise it will
        be deactivated. Returns ENODEV if CPUID faulting is not supported on
        this system.
    
    The state of the CPUID faulting flag is propagated across forks, but reset
    upon exec.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-9-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f385eca5407a..a80c1b3997ed 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -884,6 +884,8 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
+DECLARE_PER_CPU(u64, msr_misc_features_shadow);
+
 /* Register/unregister a process' MPX related resource */
 #define MPX_ENABLE_MANAGEMENT()	mpx_enable_management()
 #define MPX_DISABLE_MANAGEMENT()	mpx_disable_management()

commit 45fc8757d1d2128e342b4e7ef39adedf7752faac
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Mar 14 10:05:08 2017 -0700

    x86: Make the GDT remapping read-only on 64-bit
    
    This patch makes the GDT remapped pages read-only, to prevent accidental
    (or intentional) corruption of this key data structure.
    
    This change is done only on 64-bit, because 32-bit needs it to be writable
    for TSS switches.
    
    The native_load_tr_desc function was adapted to correctly handle a
    read-only GDT. The LTR instruction always writes to the GDT TSS entry.
    This generates a page fault if the GDT is read-only. This change checks
    if the current GDT is a remap and swap GDTs as needed. This function was
    tested by booting multiple machines and checking hibernation works
    properly.
    
    KVM SVM and VMX were adapted to use the writeable GDT. On VMX, the
    per-cpu variable was removed for functions to fetch the original GDT.
    Instead of reloading the previous GDT, VMX will reload the fixmap GDT as
    expected. For testing, VMs were started and restored on multiple
    configurations.
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lorenzo Stoakes <lstoakes@gmail.com>
    Cc: Luis R . Rodriguez <mcgrof@kernel.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kvm@vger.kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-pm@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Cc: zijun_hu <zijun_hu@htc.com>
    Link: http://lkml.kernel.org/r/20170314170508.100882-3-thgarnie@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1150e1b21b0d..edf42c4ac8c8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -716,6 +716,7 @@ extern struct desc_ptr		early_gdt_descr;
 
 extern void cpu_set_gdt(int);
 extern void switch_to_new_gdt(int);
+extern void load_direct_gdt(int);
 extern void load_fixmap_gdt(int);
 extern void load_percpu_segment(int);
 extern void cpu_init(void);

commit 69218e47994da614e7af600bf06887750ab6657a
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Mar 14 10:05:07 2017 -0700

    x86: Remap GDT tables in the fixmap section
    
    Each processor holds a GDT in its per-cpu structure. The sgdt
    instruction gives the base address of the current GDT. This address can
    be used to bypass KASLR memory randomization. With another bug, an
    attacker could target other per-cpu structures or deduce the base of
    the main memory section (PAGE_OFFSET).
    
    This patch relocates the GDT table for each processor inside the
    fixmap section. The space is reserved based on number of supported
    processors.
    
    For consistency, the remapping is done by default on 32 and 64-bit.
    
    Each processor switches to its remapped GDT at the end of
    initialization. For hibernation, the main processor returns with the
    original GDT and switches back to the remapping at completion.
    
    This patch was tested on both architectures. Hibernation and KVM were
    both tested specially for their usage of the GDT.
    
    Thanks to Boris Ostrovsky <boris.ostrovsky@oracle.com> for testing and
    recommending changes for Xen support.
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lorenzo Stoakes <lstoakes@gmail.com>
    Cc: Luis R . Rodriguez <mcgrof@kernel.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kvm@vger.kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-pm@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Cc: zijun_hu <zijun_hu@htc.com>
    Link: http://lkml.kernel.org/r/20170314170508.100882-2-thgarnie@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7caa2ac50ea2..1150e1b21b0d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -716,6 +716,7 @@ extern struct desc_ptr		early_gdt_descr;
 
 extern void cpu_set_gdt(int);
 extern void switch_to_new_gdt(int);
+extern void load_fixmap_gdt(int);
 extern void load_percpu_segment(int);
 extern void cpu_init(void);
 

commit 8f3e474f3cea7b2470218a6ed6da47ff02147dce
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Mon Mar 6 17:17:18 2017 +0300

    x86/mm: Add task_size parameter to mmap_base()
    
    To correctly handle 32-bit and 64-bit mmap() syscalls in 64bit applications
    its required to have separate address bases to place a mapping.
    
    The tasksize can be used as an indicator to select the proper parameters
    for mmap_base().
    
    This requires the following changes:
    
     - Add task_size argument to mmap_base() and make the calculation based on it.
     - Provide mmap_legacy_base() as a seperate function
     - Use the new functions in arch_pick_mmap_layout()
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: 0x7f454c46@gmail.com
    Cc: linux-mm@kvack.org
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/20170306141721.9188-3-dsafonov@virtuozzo.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f385eca5407a..7caa2ac50ea2 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -797,6 +797,7 @@ static inline void spin_lock_prefetch(const void *x)
 /*
  * User space process size: 3GB (default).
  */
+#define IA32_PAGE_OFFSET	PAGE_OFFSET
 #define TASK_SIZE		PAGE_OFFSET
 #define TASK_SIZE_MAX		TASK_SIZE
 #define STACK_TOP		TASK_SIZE
@@ -873,7 +874,8 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
  * This decides where the kernel will search for a free chunk of vm
  * space during mmap's.
  */
-#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 3))
+#define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))
+#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)
 
 #define KSTK_EIP(task)		(task_pt_regs(task)->ip)
 

commit 6415813bae75feba10b8ca3ed6634a72c2a4d313
Author: Mathias Krause <minipli@googlemail.com>
Date:   Sun Feb 12 22:12:08 2017 +0100

    x86/cpu: Drop wp_works_ok member of struct cpuinfo_x86
    
    Remove the wp_works_ok member of struct cpuinfo_x86. It's an
    optimization back from Linux v0.99 times where we had no fixup support
    yet and did the CR0.WP test via special code in the page fault handler.
    The < 0 test was an optimization to not do the special casing for each
    NULL ptr access violation but just for the first one doing the WP test.
    Today it serves no real purpose as the test no longer needs special code
    in the page fault handler and the only call side -- mem_init() -- calls
    it just once, anyway. However, Xen pre-initializes it to 1, to skip the
    test.
    
    Doing the test again for Xen should be no issue at all, as even the
    commit introducing skipping the test (commit d560bc61575e ("x86, xen:
    Suppress WP test on Xen")) mentioned it being ban aid only. And, in
    fact, testing the patch on Xen showed nothing breaks.
    
    The pre-fixup times are long gone and with the removal of the fallback
    handling code in commit a5c2a893dbd4 ("x86, 386 removal: Remove
    CONFIG_X86_WP_WORKS_OK") the kernel requires a working CR0.WP anyway.
    So just get rid of the "optimization" and do the test unconditionally.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Arnd Hannemann <hannemann@nets.rwth-aachen.de>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Link: http://lkml.kernel.org/r/1486933932-585-3-git-send-email-minipli@googlemail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 893f80e30bfb..4aa93b560a2b 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -89,9 +89,7 @@ struct cpuinfo_x86 {
 	__u8			x86_vendor;	/* CPU vendor */
 	__u8			x86_model;
 	__u8			x86_mask;
-#ifdef CONFIG_X86_32
-	char			wp_works_ok;	/* It doesn't on 386's */
-#else
+#ifdef CONFIG_X86_64
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
 	int			x86_tlbsize;
 #endif

commit 04402116846f36adea9503d7cd5104a7ed27a1a6
Author: Mathias Krause <minipli@googlemail.com>
Date:   Sun Feb 12 22:12:07 2017 +0100

    x86/cpu: Drop unneded members of struct cpuinfo_x86
    
    Those member serve no purpose -- not even fill padding for alignment or
    such. So just get rid of them.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Link: http://lkml.kernel.org/r/1486933932-585-2-git-send-email-minipli@googlemail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f385eca5407a..893f80e30bfb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -80,7 +80,7 @@ extern u16 __read_mostly tlb_lld_1g[NR_INFO];
 
 /*
  *  CPU type and hardware bug flags. Kept separately for each CPU.
- *  Members of this structure are referenced in head.S, so think twice
+ *  Members of this structure are referenced in head_32.S, so think twice
  *  before touching them. [mj]
  */
 
@@ -91,11 +91,6 @@ struct cpuinfo_x86 {
 	__u8			x86_mask;
 #ifdef CONFIG_X86_32
 	char			wp_works_ok;	/* It doesn't on 386's */
-
-	/* Problems on some 486Dx4's and old 386's: */
-	char			rfu;
-	char			pad0;
-	char			pad1;
 #else
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
 	int			x86_tlbsize;

commit fd7e9a88348472521d999434ee02f25735c7dadf
Merge: 5066e4a34081 dd0fd8bca185
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 18:22:53 2017 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "4.11 is going to be a relatively large release for KVM, with a little
      over 200 commits and noteworthy changes for most architectures.
    
      ARM:
       - GICv3 save/restore
       - cache flushing fixes
       - working MSI injection for GICv3 ITS
       - physical timer emulation
    
      MIPS:
       - various improvements under the hood
       - support for SMP guests
       - a large rewrite of MMU emulation. KVM MIPS can now use MMU
         notifiers to support copy-on-write, KSM, idle page tracking,
         swapping, ballooning and everything else. KVM_CAP_READONLY_MEM is
         also supported, so that writes to some memory regions can be
         treated as MMIO. The new MMU also paves the way for hardware
         virtualization support.
    
      PPC:
       - support for POWER9 using the radix-tree MMU for host and guest
       - resizable hashed page table
       - bugfixes.
    
      s390:
       - expose more features to the guest
       - more SIMD extensions
       - instruction execution protection
       - ESOP2
    
      x86:
       - improved hashing in the MMU
       - faster PageLRU tracking for Intel CPUs without EPT A/D bits
       - some refactoring of nested VMX entry/exit code, preparing for live
         migration support of nested hypervisors
       - expose yet another AVX512 CPUID bit
       - host-to-guest PTP support
       - refactoring of interrupt injection, with some optimizations thrown
         in and some duct tape removed.
       - remove lazy FPU handling
       - optimizations of user-mode exits
       - optimizations of vcpu_is_preempted() for KVM guests
    
      generic:
       - alternative signaling mechanism that doesn't pound on
         tsk->sighand->siglock"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (195 commits)
      x86/kvm: Provide optimized version of vcpu_is_preempted() for x86-64
      x86/paravirt: Change vcp_is_preempted() arg type to long
      KVM: VMX: use correct vmcs_read/write for guest segment selector/base
      x86/kvm/vmx: Defer TR reload after VM exit
      x86/asm/64: Drop __cacheline_aligned from struct x86_hw_tss
      x86/kvm/vmx: Simplify segment_base()
      x86/kvm/vmx: Get rid of segment_base() on 64-bit kernels
      x86/kvm/vmx: Don't fetch the TSS base from the GDT
      x86/asm: Define the kernel TSS limit in a macro
      kvm: fix page struct leak in handle_vmon
      KVM: PPC: Book3S HV: Disable HPT resizing on POWER9 for now
      KVM: Return an error code only as a constant in kvm_get_dirty_log()
      KVM: Return an error code only as a constant in kvm_get_dirty_log_protect()
      KVM: Return directly after a failed copy_from_user() in kvm_vm_compat_ioctl()
      KVM: x86: remove code for lazy FPU handling
      KVM: race-free exit from KVM_RUN without POSIX signals
      KVM: PPC: Book3S HV: Turn "KVM guest htab" message into a debug message
      KVM: PPC: Book3S PR: Ratelimit copy data failure error messages
      KVM: Support vCPU-based gfn->hva cache
      KVM: use separate generations for each address space
      ...

commit d3273deac9c0cdae32eb46f928487433eaa37f87
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Feb 20 08:56:13 2017 -0800

    x86/asm/64: Drop __cacheline_aligned from struct x86_hw_tss
    
    Historically, the entire TSS + io bitmap structure was cacheline
    aligned, but commit ca241c75037b ("x86: unify tss_struct") changed it
    (presumably inadvertently) so that the fixed-layout hardware part is
    cacheline-aligned and the io bitmap is after the padding.  This wastes
    24 bytes (the hardware part should be 104 bytes, but this pads it to
    128 bytes) and, serves no purpose, and causes sizeof(struct
    x86_hw_tss) to have a confusing value.
    
    Drop the pointless alignment.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f8f1b7537abe..1879cdf2b6ae 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -303,7 +303,7 @@ struct x86_hw_tss {
 	u16			reserved5;
 	u16			io_bitmap_base;
 
-} __attribute__((packed)) ____cacheline_aligned;
+} __attribute__((packed));
 #endif
 
 /*

commit 4f53ab14285802b298261f8b52af322039d1dfd0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Feb 20 08:56:09 2017 -0800

    x86/asm: Define the kernel TSS limit in a macro
    
    Rather than open-coding the kernel TSS limit in set_tss_desc(), make
    it a real macro near the TSS layout definition.
    
    This is purely a cleanup.
    
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Jim Mattson <jmattson@google.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1be64da0384e..f8f1b7537abe 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -341,6 +341,16 @@ struct tss_struct {
 
 DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);
 
+/*
+ * sizeof(unsigned long) coming from an extra "long" at the end
+ * of the iobitmap.
+ *
+ * -1? seg base+limit should be pointing to the address of the
+ * last valid byte
+ */
+#define __KERNEL_TSS_LIMIT	\
+	(IO_BITMAP_OFFSET + IO_BITMAP_BYTES + sizeof(unsigned long) - 1)
+
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
 #endif

commit 79a8b9aa388b0620cc1d525d7c0f0d9a8a85e08e
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Feb 5 11:50:21 2017 +0100

    x86/CPU/AMD: Bring back Compute Unit ID
    
    Commit:
    
      a33d331761bc ("x86/CPU/AMD: Fix Bulldozer topology")
    
    restored the initial approach we had with the Fam15h topology of
    enumerating CU (Compute Unit) threads as cores. And this is still
    correct - they're beefier than HT threads but still have some
    shared functionality.
    
    Our current approach has a problem with the Mad Max Steam game, for
    example. Yves Dionne reported a certain "choppiness" while playing on
    v4.9.5.
    
    That problem stems most likely from the fact that the CU threads share
    resources within one CU and when we schedule to a thread of a different
    compute unit, this incurs latency due to migrating the working set to a
    different CU through the caches.
    
    When the thread siblings mask mirrors that aspect of the CUs and
    threads, the scheduler pays attention to it and tries to schedule within
    one CU first. Which takes care of the latency, of course.
    
    Reported-by: Yves Dionne <yves.dionne@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # 4.9
    Cc: Brice Goglin <Brice.Goglin@inria.fr>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20170205105022.8705-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1be64da0384e..e6cfe7ba2d65 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -104,6 +104,7 @@ struct cpuinfo_x86 {
 	__u8			x86_phys_bits;
 	/* CPUID returned core id bits: */
 	__u8			x86_coreid_bits;
+	__u8			cu_id;
 	/* Max extended CPUID function supported: */
 	__u32			extended_cpuid_level;
 	/* Maximum supported CPUID level, -1=no CPUID: */

commit 5dedade6dfa243c130b85d1e4daba6f027805033
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Jan 9 12:41:43 2017 +0100

    x86/CPU: Add native CPUID variants returning a single datum
    
    ... similarly to the cpuid_<reg>() variants.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/20170109114147.5082-2-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index eaf100508c36..1be64da0384e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -219,6 +219,24 @@ static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
 	    : "memory");
 }
 
+#define native_cpuid_reg(reg)					\
+static inline unsigned int native_cpuid_##reg(unsigned int op)	\
+{								\
+	unsigned int eax = op, ebx, ecx = 0, edx;		\
+								\
+	native_cpuid(&eax, &ebx, &ecx, &edx);			\
+								\
+	return reg;						\
+}
+
+/*
+ * Native CPUID functions returning a single datum.
+ */
+native_cpuid_reg(eax)
+native_cpuid_reg(ebx)
+native_cpuid_reg(ecx)
+native_cpuid_reg(edx)
+
 static inline void load_cr3(pgd_t *pgdir)
 {
 	write_cr3(__pa(pgdir));

commit c198b121b1a1d7a7171770c634cd49191bac4477
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Dec 9 10:24:08 2016 -0800

    x86/asm: Rewrite sync_core() to use IRET-to-self
    
    Aside from being excessively slow, CPUID is problematic: Linux runs
    on a handful of CPUs that don't have CPUID.  Use IRET-to-self
    instead.  IRET-to-self works everywhere, so it makes testing easy.
    
    For reference, On my laptop, IRET-to-self is ~110ns,
    CPUID(eax=1, ecx=0) is ~83ns on native and very very slow under KVM,
    and MOV-to-CR2 is ~42ns.
    
    While we're at it: sync_core() serves a very specific purpose.
    Document it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Matthew Whitehead <tedheadster@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Henrique de Moraes Holschuh <hmh@hmh.eng.br>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: xen-devel <Xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/5c79f0225f68bc8c40335612bf624511abb78941.1481307769.git.luto@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b934871ae226..eaf100508c36 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -602,33 +602,69 @@ static __always_inline void cpu_relax(void)
 	rep_nop();
 }
 
-/* Stop speculative execution and prefetching of modified code. */
+/*
+ * This function forces the icache and prefetched instruction stream to
+ * catch up with reality in two very specific cases:
+ *
+ *  a) Text was modified using one virtual address and is about to be executed
+ *     from the same physical page at a different virtual address.
+ *
+ *  b) Text was modified on a different CPU, may subsequently be
+ *     executed on this CPU, and you want to make sure the new version
+ *     gets executed.  This generally means you're calling this in a IPI.
+ *
+ * If you're calling this for a different reason, you're probably doing
+ * it wrong.
+ */
 static inline void sync_core(void)
 {
-	int tmp;
-
-#ifdef CONFIG_X86_32
 	/*
-	 * Do a CPUID if available, otherwise do a jump.  The jump
-	 * can conveniently enough be the jump around CPUID.
+	 * There are quite a few ways to do this.  IRET-to-self is nice
+	 * because it works on every CPU, at any CPL (so it's compatible
+	 * with paravirtualization), and it never exits to a hypervisor.
+	 * The only down sides are that it's a bit slow (it seems to be
+	 * a bit more than 2x slower than the fastest options) and that
+	 * it unmasks NMIs.  The "push %cs" is needed because, in
+	 * paravirtual environments, __KERNEL_CS may not be a valid CS
+	 * value when we do IRET directly.
+	 *
+	 * In case NMI unmasking or performance ever becomes a problem,
+	 * the next best option appears to be MOV-to-CR2 and an
+	 * unconditional jump.  That sequence also works on all CPUs,
+	 * but it will fault at CPL3 (i.e. Xen PV and lguest).
+	 *
+	 * CPUID is the conventional way, but it's nasty: it doesn't
+	 * exist on some 486-like CPUs, and it usually exits to a
+	 * hypervisor.
+	 *
+	 * Like all of Linux's memory ordering operations, this is a
+	 * compiler barrier as well.
 	 */
-	asm volatile("cmpl %2,%1\n\t"
-		     "jl 1f\n\t"
-		     "cpuid\n"
-		     "1:"
-		     : "=a" (tmp)
-		     : "rm" (boot_cpu_data.cpuid_level), "ri" (0), "0" (1)
-		     : "ebx", "ecx", "edx", "memory");
+	register void *__sp asm(_ASM_SP);
+
+#ifdef CONFIG_X86_32
+	asm volatile (
+		"pushfl\n\t"
+		"pushl %%cs\n\t"
+		"pushl $1f\n\t"
+		"iret\n\t"
+		"1:"
+		: "+r" (__sp) : : "memory");
 #else
-	/*
-	 * CPUID is a barrier to speculative execution.
-	 * Prefetched instructions are automatically
-	 * invalidated when modified.
-	 */
-	asm volatile("cpuid"
-		     : "=a" (tmp)
-		     : "0" (1)
-		     : "ebx", "ecx", "edx", "memory");
+	unsigned int tmp;
+
+	asm volatile (
+		"mov %%ss, %0\n\t"
+		"pushq %q0\n\t"
+		"pushq %%rsp\n\t"
+		"addq $8, (%%rsp)\n\t"
+		"pushfq\n\t"
+		"mov %%cs, %0\n\t"
+		"pushq %q0\n\t"
+		"pushq $1f\n\t"
+		"iretq\n\t"
+		"1:"
+		: "=&r" (tmp), "+r" (__sp) : : "cc", "memory");
 #endif
 }
 

commit 1c52d859cb2d417e7216d3e56bb7fea88444cec9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Dec 9 10:24:05 2016 -0800

    x86/asm/32: Make sync_core() handle missing CPUID on all 32-bit kernels
    
    We support various non-Intel CPUs that don't have the CPUID
    instruction, so the M486 test was wrong.  For now, fix it with a big
    hammer: handle missing CPUID on all 32-bit CPUs.
    
    Reported-by: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Matthew Whitehead <tedheadster@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Henrique de Moraes Holschuh <hmh@hmh.eng.br>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: xen-devel <Xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/685bd083a7c036f7769510b6846315b17d6ba71f.1481307769.git.luto@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6aa741fbe1df..b934871ae226 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -607,7 +607,7 @@ static inline void sync_core(void)
 {
 	int tmp;
 
-#ifdef CONFIG_M486
+#ifdef CONFIG_X86_32
 	/*
 	 * Do a CPUID if available, otherwise do a jump.  The jump
 	 * can conveniently enough be the jump around CPUID.

commit 212f30008a284a9312d95dad6cc237ff81173d73
Merge: 6f3be0f04354 34bc3560c657
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:55:04 2016 -0800

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 idle updates from Ingo Molnar:
     "There were two bigger changes in this development cycle:
    
       - remove idle notifiers:
    
           32 files changed, 74 insertions(+), 803 deletions(-)
    
         These notifiers were of questionable value and the main usecase,
         the i7300 driver, was essentially unmaintained and can be removed,
         plus modern power management concepts don't need the callback - so
         use this golden opportunity and get rid of this opaque and fragile
         callback from a latency sensitive code path.
    
         (Len Brown, Thomas Gleixner)
    
       - improve the AMD Erratum 400 workaround that used high overhead MSR
         polling in the idle loop (Borisla Petkov, Thomas Gleixner)"
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove empty idle.h header
      x86/amd: Simplify AMD E400 aware idle routine
      x86/amd: Check for the C1E bug post ACPI subsystem init
      x86/bugs: Separate AMD E400 erratum and C1E bug
      x86/cpufeature: Provide helper to set bugs bits
      x86/idle: Remove enter_idle(), exit_idle()
      x86: Remove x86_test_and_clear_bit_percpu()
      x86/idle: Remove is_idle flag
      x86/idle: Remove idle_notifier
      i7300_idle: Remove this driver

commit 5645688f9d0d5a32f030f9c5429e1a58bedca23b
Merge: 4ade5b2268b9 53938ee427bf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 13:49:57 2016 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this development cycle were:
    
       - a large number of call stack dumping/printing improvements: higher
         robustness, better cross-context dumping, improved output, etc.
         (Josh Poimboeuf)
    
       - vDSO getcpu() performance improvement for future Intel CPUs with
         the RDPID instruction (Andy Lutomirski)
    
       - add two new Intel AVX512 features and the CPUID support
         infrastructure for it: AVX512IFMA and AVX512VBMI. (Gayatri Kammela,
         He Chen)
    
       - more copy-user unification (Borislav Petkov)
    
       - entry code assembly macro simplifications (Alexander Kuleshov)
    
       - vDSO C/R support improvements (Dmitry Safonov)
    
       - misc fixes and cleanups (Borislav Petkov, Paul Bolle)"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (40 commits)
      scripts/decode_stacktrace.sh: Fix address line detection on x86
      x86/boot/64: Use defines for page size
      x86/dumpstack: Make stack name tags more comprehensible
      selftests/x86: Add test_vdso to test getcpu()
      x86/vdso: Use RDPID in preference to LSL when available
      x86/dumpstack: Handle NULL stack pointer in show_trace_log_lvl()
      x86/cpufeatures: Enable new AVX512 cpu features
      x86/cpuid: Provide get_scattered_cpuid_leaf()
      x86/cpuid: Cleanup cpuid_regs definitions
      x86/copy_user: Unify the code by removing the 64-bit asm _copy_*_user() variants
      x86/unwind: Ensure stack grows down
      x86/vdso: Set vDSO pointer only after success
      x86/prctl/uapi: Remove #ifdef for CHECKPOINT_RESTORE
      x86/unwind: Detect bad stack return address
      x86/dumpstack: Warn on stack recursion
      x86/unwind: Warn on bad frame pointer
      x86/decoder: Use stderr if insn sanity test fails
      x86/decoder: Use stdout if insn decoder test is successful
      mm/page_alloc: Remove kernel address exposure in free_reserved_area()
      x86/dumpstack: Remove raw stack dump
      ...

commit 07c94a38125376d70d156bd8bff98ddfe4c8ea95
Author: Borislav Petkov <bp@alien8.de>
Date:   Fri Dec 9 19:29:11 2016 +0100

    x86/amd: Simplify AMD E400 aware idle routine
    
    Reorganize the E400 detection now that we have everything in place:
    switch the CPUs to broadcast mode after the LAPIC has been initialized
    and remove the facilities that were used previously on the idle path.
    
    Unfortunately static_cpu_has_bug() cannpt be used in the E400 idle routine
    because alternatives have been applied when the actual detection happens,
    so the static switching does not take effect and the test will stay
    false. Use boot_cpu_has_bug() instead which is definitely an improvement
    over the RDMSR and the cpumask handling.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-5-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 984a7bf17f6a..fa609c6f6ba9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -621,10 +621,9 @@ static inline void sync_core(void)
 }
 
 extern void select_idle_routine(const struct cpuinfo_x86 *c);
-extern void init_amd_e400_c1e_mask(void);
+extern void amd_e400_c1e_apic_setup(void);
 
 extern unsigned long		boot_option_idle_override;
-extern bool			amd_e400_c1e_detected;
 
 enum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,
 			 IDLE_POLL};

commit 6d0d287891a022ebba572327cbd70b5de69a63a2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Nov 16 13:23:05 2016 +0100

    locking/core: Provide common cpu_relax_yield() definition
    
    No need to duplicate the same define everywhere. Since
    the only user is stop-machine and the only provider is
    s390, we can use a default implementation of cpu_relax_yield()
    in sched.h.
    
    Suggested-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-s390 <linux-s390@vger.kernel.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: sparclinux@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1479298985-191589-1-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7513c996f673..c84605bb2a15 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -588,8 +588,6 @@ static __always_inline void cpu_relax(void)
 	rep_nop();
 }
 
-#define cpu_relax_yield() cpu_relax()
-
 /* Stop speculative execution and prefetching of modified code. */
 static inline void sync_core(void)
 {

commit 47bdf3378d62a627cfb8a54e1180c08d67078b61
Author: He Chen <he.chen@linux.intel.com>
Date:   Fri Nov 11 17:25:35 2016 +0800

    x86/cpuid: Provide get_scattered_cpuid_leaf()
    
    Sparse populated CPUID leafs are collected in a software provided leaf to
    avoid bloat of the x86_capability array, but there is no way to rebuild the
    real leafs (e.g. for KVM CPUID enumeration) other than rereading the CPUID
    leaf from the CPU. While this is possible it is problematic as it does not
    take software disabled features into account. If a feature is disabled on
    the host it should not be exposed to a guest either.
    
    Add get_scattered_cpuid_leaf() which rebuilds the leaf from the scattered
    cpuid table information and the active CPU features.
    
    [ tglx: Rewrote changelog ]
    
    Signed-off-by: He Chen <he.chen@linux.intel.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Luwei Kang <luwei.kang@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Piotr Luc <Piotr.Luc@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/1478856336-9388-3-git-send-email-he.chen@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 8f6ac5be84b7..e7f8c62701d4 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -189,6 +189,9 @@ extern void identify_secondary_cpu(struct cpuinfo_x86 *);
 extern void print_cpu_info(struct cpuinfo_x86 *);
 void print_cpu_msr(struct cpuinfo_x86 *);
 extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
+extern u32 get_scattered_cpuid_leaf(unsigned int level,
+				    unsigned int sub_leaf,
+				    enum cpuid_regs_idx reg);
 extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
 extern void init_amd_cacheinfo(struct cpuinfo_x86 *c);
 

commit 47f10a36003eaf493125a5e6687dd1ff775bfd8c
Author: He Chen <he.chen@linux.intel.com>
Date:   Fri Nov 11 17:25:34 2016 +0800

    x86/cpuid: Cleanup cpuid_regs definitions
    
    cpuid_regs is defined multiple times as structure and enum. Rename the enum
    and move all of it to processor.h so we don't end up with more instances.
    
    Rename the misnomed register enumeration from CR_* to the obvious CPUID_*.
    
    [ tglx: Rewrote changelog ]
    
    Signed-off-by: He Chen <he.chen@linux.intel.com>
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Luwei Kang <luwei.kang@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Piotr Luc <Piotr.Luc@intel.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/1478856336-9388-2-git-send-email-he.chen@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 984a7bf17f6a..8f6ac5be84b7 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -137,6 +137,17 @@ struct cpuinfo_x86 {
 	u32			microcode;
 };
 
+struct cpuid_regs {
+	u32 eax, ebx, ecx, edx;
+};
+
+enum cpuid_regs_idx {
+	CPUID_EAX = 0,
+	CPUID_EBX,
+	CPUID_ECX,
+	CPUID_EDX,
+};
+
 #define X86_VENDOR_INTEL	0
 #define X86_VENDOR_CYRIX	1
 #define X86_VENDOR_AMD		2

commit 5bd0b85ba8bb9de6f61f33f3752fc85f4c87fc22
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 25 11:03:15 2016 +0200

    locking/core, arch: Remove cpu_relax_lowlatency()
    
    As there are no users left, we can remove cpu_relax_lowlatency()
    implementations from every architecture.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Cc: <linux-arch@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1477386195-32736-6-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 44adadab44d6..7513c996f673 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -589,7 +589,6 @@ static __always_inline void cpu_relax(void)
 }
 
 #define cpu_relax_yield() cpu_relax()
-#define cpu_relax_lowlatency() cpu_relax()
 
 /* Stop speculative execution and prefetching of modified code. */
 static inline void sync_core(void)

commit 79ab11cdb90d8536817ab7357ecb6b1ff76be26c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 25 11:03:11 2016 +0200

    locking/core: Introduce cpu_relax_yield()
    
    For spinning loops people do often use barrier() or cpu_relax().
    For most architectures cpu_relax and barrier are the same, but on
    some architectures cpu_relax can add some latency.
    For example on power,sparc64 and arc, cpu_relax can shift the CPU
    towards other hardware threads in an SMT environment.
    On s390 cpu_relax does even more, it uses an hypercall to the
    hypervisor to give up the timeslice.
    In contrast to the SMT yielding this can result in larger latencies.
    In some places this latency is unwanted, so another variant
    "cpu_relax_lowlatency" was introduced. Before this is used in more
    and more places, lets revert the logic and provide a cpu_relax_yield
    that can be called in places where yielding is more important than
    latency. By default this is the same as cpu_relax on all architectures.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1477386195-32736-2-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 984a7bf17f6a..44adadab44d6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -588,6 +588,7 @@ static __always_inline void cpu_relax(void)
 	rep_nop();
 }
 
+#define cpu_relax_yield() cpu_relax()
 #define cpu_relax_lowlatency() cpu_relax()
 
 /* Stop speculative execution and prefetching of modified code. */

commit b9d989c7218ac922185d82ad46f3e58b27a4bea9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:21 2016 -0700

    x86/asm: Move the thread_info::status field to thread_struct
    
    Because sched.h and thread_info.h are a tangled mess, I turned
    in_compat_syscall() into a macro.  If we had current_thread_struct()
    or similar and we could use it from thread_info.h, then this would
    be a bit cleaner.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/ccc8a1b2f41f9c264a41f771bb4a6539a642ad72.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b22fb5a4ff3c..984a7bf17f6a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -389,6 +389,9 @@ struct thread_struct {
 	unsigned short		fsindex;
 	unsigned short		gsindex;
 #endif
+
+	u32			status;		/* thread synchronous flags */
+
 #ifdef CONFIG_X86_64
 	unsigned long		fsbase;
 	unsigned long		gsbase;
@@ -434,6 +437,15 @@ struct thread_struct {
 	 */
 };
 
+/*
+ * Thread-synchronous status.
+ *
+ * This is different from the flags in that nobody else
+ * ever touches our thread-synchronous status, so we don't
+ * have to worry about atomic accesses.
+ */
+#define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
+
 /*
  * Set IOPL bits in EFLAGS from given mask
  */

commit ffcb043ba524d3fbd979a9dac2c9ce8ad352000d
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:21 2016 -0400

    sched/x86: Fix thread_saved_pc()
    
    thread_saved_pc() was using a completely bogus method to get the return
    address.  Since switch_to() was previously inlined, there was no sane way
    to know where on the stack the return address was stored.  Now with the
    frame of a sleeping thread well defined, this can be implemented correctly.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-7-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6fee8635340b..b22fb5a4ff3c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -721,8 +721,6 @@ static inline void spin_lock_prefetch(const void *x)
 	.addr_limit		= KERNEL_DS,				  \
 }
 
-extern unsigned long thread_saved_pc(struct task_struct *tsk);
-
 /*
  * TOP_OF_KERNEL_STACK_PADDING reserves 8 bytes on top of the ring0 stack.
  * This is necessary to guarantee that the entire "struct pt_regs"
@@ -773,17 +771,13 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 	.addr_limit		= KERNEL_DS,			\
 }
 
-/*
- * Return saved PC of a blocked thread.
- * What is this good for? it will be always the scheduler or ret_from_fork.
- */
-#define thread_saved_pc(t)	READ_ONCE_NOCHECK(*(unsigned long *)((t)->thread.sp - 8))
-
 #define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.sp0 - 1)
 extern unsigned long KSTK_ESP(struct task_struct *task);
 
 #endif /* CONFIG_X86_64 */
 
+extern unsigned long thread_saved_pc(struct task_struct *tsk);
+
 extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 					       unsigned long new_sp);
 

commit 0100301bfdf56a2a370c7157b5ab0fbf9313e1cd
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:19 2016 -0400

    sched/x86: Rewrite the switch_to() code
    
    Move the low-level context switch code to an out-of-line asm stub instead of
    using complex inline asm.  This allows constructing a new stack frame for the
    child process to make it seamlessly flow to ret_from_fork without an extra
    test and branch in __switch_to().  It also improves code generation for
    __schedule() by using the C calling convention instead of clobbering all
    registers.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-5-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 63def9537a2d..6fee8635340b 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -389,9 +389,6 @@ struct thread_struct {
 	unsigned short		fsindex;
 	unsigned short		gsindex;
 #endif
-#ifdef CONFIG_X86_32
-	unsigned long		ip;
-#endif
 #ifdef CONFIG_X86_64
 	unsigned long		fsbase;
 	unsigned long		gsbase;

commit 2d724ffddd958f21e2711b7400c63bdfee287d75
Merge: 36e635cb21d9 ec3ed4a2104b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 18:48:27 2016 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fpu updates from Ingo Molnar:
     "The main x86 FPU changes in this cycle were:
    
       - a large series of cleanups, fixes and enhancements to re-enable the
         XSAVES instruction on Intel CPUs - which is the most advanced
         instruction to do FPU context switches (Yu-cheng Yu, Fenghua Yu)
    
       - Add FPU tracepoints for the FPU state machine (Dave Hansen)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Do not BUG_ON() in early FPU code
      x86/fpu/xstate: Re-enable XSAVES
      x86/fpu/xstate: Fix fpstate_init() for XRSTORS
      x86/fpu/xstate: Return NULL for disabled xstate component address
      x86/fpu/xstate: Fix __fpu_restore_sig() for XSAVES
      x86/fpu/xstate: Fix xstate_offsets, xstate_sizes for non-extended xstates
      x86/fpu/xstate: Fix XSTATE component offset print out
      x86/fpu/xstate: Fix PTRACE frames for XSAVES
      x86/fpu/xstate: Fix supervisor xstate component offset
      x86/fpu/xstate: Align xstate components according to CPUID
      x86/fpu/xstate: Copy xstate registers directly to the signal frame when compacted format is in use
      x86/fpu/xstate: Keep init_fpstate.xsave.header.xfeatures as zero for init optimization
      x86/fpu/xstate: Rename 'xstate_size' to 'fpu_kernel_xstate_size', to distinguish it from 'fpu_user_xstate_size'
      x86/fpu/xstate: Define and use 'fpu_user_xstate_size'
      x86/fpu: Add tracepoints to dump FPU state at key points

commit 13d4ea097d18b419ad2a2b696063d44bf59acec0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 14 13:22:57 2016 -0700

    x86/uaccess: Move thread_info::addr_limit to thread_struct
    
    struct thread_info is a legacy mess.  To prepare for its partial removal,
    move thread_info::addr_limit out.
    
    As an added benefit, this way is simpler.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/15bee834d09402b47ac86f2feccdf6529f9bc5b0.1468527351.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cbdfe5f76347..89314ed74fee 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -371,6 +371,10 @@ extern unsigned int xstate_size;
 
 struct perf_event;
 
+typedef struct {
+	unsigned long		seg;
+} mm_segment_t;
+
 struct thread_struct {
 	/* Cached TLS descriptors: */
 	struct desc_struct	tls_array[GDT_ENTRY_TLS_ENTRIES];
@@ -419,6 +423,8 @@ struct thread_struct {
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
 
+	mm_segment_t		addr_limit;
+
 	unsigned int		sig_on_uaccess_err:1;
 	unsigned int		uaccess_err:1;	/* uaccess failed */
 
@@ -493,11 +499,6 @@ static inline void load_sp0(struct tss_struct *tss,
 #define set_iopl_mask native_set_iopl_mask
 #endif /* CONFIG_PARAVIRT */
 
-typedef struct {
-	unsigned long		seg;
-} mm_segment_t;
-
-
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);
 
@@ -719,6 +720,7 @@ static inline void spin_lock_prefetch(const void *x)
 	.sp0			= TOP_OF_INIT_STACK,			  \
 	.sysenter_cs		= __KERNEL_CS,				  \
 	.io_bitmap_ptr		= NULL,					  \
+	.addr_limit		= KERNEL_DS,				  \
 }
 
 extern unsigned long thread_saved_pc(struct task_struct *tsk);
@@ -768,8 +770,9 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
-#define INIT_THREAD  { \
-	.sp0 = TOP_OF_INIT_STACK \
+#define INIT_THREAD  {						\
+	.sp0			= TOP_OF_INIT_STACK,		\
+	.addr_limit		= KERNEL_DS,			\
 }
 
 /*

commit 2a53ccbc0de1b1950aeedd24680f7eca65c86ff5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 15 10:21:11 2016 +0200

    x86/dumpstack: Rename thread_struct::sig_on_uaccess_error to sig_on_uaccess_err
    
    Rename it to match the thread_struct::uaccess_err pattern and also
    because it was too long.
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f53ae57bd985..cbdfe5f76347 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -419,7 +419,7 @@ struct thread_struct {
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
 
-	unsigned int		sig_on_uaccess_error:1;
+	unsigned int		sig_on_uaccess_err:1;
 	unsigned int		uaccess_err:1;	/* uaccess failed */
 
 	/* Floating point and extended processor state */

commit dfa9a942fd7951c8f333cf3f377dde51ebd21685
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 14 13:22:56 2016 -0700

    x86/uaccess: Move thread_info::uaccess_err and thread_info::sig_on_uaccess_err to thread_struct
    
    struct thread_info is a legacy mess.  To prepare for its partial removal,
    move the uaccess control fields out -- they're straightforward.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/d0ac4d01c8e4d4d756264604e47445d5acc7900e.1468527351.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 62c6cc3cc5d3..f53ae57bd985 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -419,6 +419,9 @@ struct thread_struct {
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
 
+	unsigned int		sig_on_uaccess_error:1;
+	unsigned int		uaccess_err:1;	/* uaccess failed */
+
 	/* Floating point and extended processor state */
 	struct fpu		fpu;
 	/*

commit bf15a8cf8d14879b785c548728415d36ccb6a33b
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Fri May 20 10:47:06 2016 -0700

    x86/fpu/xstate: Rename 'xstate_size' to 'fpu_kernel_xstate_size', to distinguish it from 'fpu_user_xstate_size'
    
    User space uses standard format xsave area. fpstate in signal frame
    should have standard format size.
    
    To explicitly distinguish between xstate size in kernel space and the
    one in user space, we rename 'xstate_size' to 'fpu_kernel_xstate_size'.
    
    Cleanup only, no change in functionality.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    [ Rebased the patch and cleaned up the naming. ]
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Ravi V. Shankar <ravi.v.shankar@intel.com>
    Cc: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2ecbae347a5152d94be52adf7d0f3b7305d90d99.1463760376.git.yu-cheng.yu@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0a16a16284f5..965c5d212c31 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -367,7 +367,7 @@ DECLARE_PER_CPU(struct irq_stack *, hardirq_stack);
 DECLARE_PER_CPU(struct irq_stack *, softirq_stack);
 #endif	/* X86_64 */
 
-extern unsigned int xstate_size;
+extern unsigned int fpu_kernel_xstate_size;
 extern unsigned int fpu_user_xstate_size;
 
 struct perf_event;

commit a1141e0b5ca6ee3e5e35d5f1a310a5ecb9c96ce5
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Fri May 20 10:47:05 2016 -0700

    x86/fpu/xstate: Define and use 'fpu_user_xstate_size'
    
    The kernel xstate area can be in standard or compacted format;
    it is always in standard format for user mode. When XSAVES is
    enabled, the kernel uses the compacted format and it is necessary
    to use a separate fpu_user_xstate_size for signal/ptrace frames.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    [ Rebased the patch and cleaned up the naming. ]
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Ravi V. Shankar <ravi.v.shankar@intel.com>
    Cc: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8756ec34dabddfc727cda5743195eb81e8caf91c.1463760376.git.yu-cheng.yu@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 62c6cc3cc5d3..0a16a16284f5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -368,6 +368,7 @@ DECLARE_PER_CPU(struct irq_stack *, softirq_stack);
 #endif	/* X86_64 */
 
 extern unsigned int xstate_size;
+extern unsigned int fpu_user_xstate_size;
 
 struct perf_event;
 

commit 9a45f036af363aec1efec08827c825d69c115a9a
Merge: 168f1a7163b3 d2d3462f9f08
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 15:54:01 2016 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - prepare for more KASLR related changes, by restructuring, cleaning
         up and fixing the existing boot code.  (Kees Cook, Baoquan He,
         Yinghai Lu)
    
       - simplifly/concentrate subarch handling code, eliminate
         paravirt_enabled() usage.  (Luis R Rodriguez)"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (50 commits)
      x86/KASLR: Clarify purpose of each get_random_long()
      x86/KASLR: Add virtual address choosing function
      x86/KASLR: Return earliest overlap when avoiding regions
      x86/KASLR: Add 'struct slot_area' to manage random_addr slots
      x86/boot: Add missing file header comments
      x86/KASLR: Initialize mapping_info every time
      x86/boot: Comment what finalize_identity_maps() does
      x86/KASLR: Build identity mappings on demand
      x86/boot: Split out kernel_ident_mapping_init()
      x86/boot: Clean up indenting for asm/boot.h
      x86/KASLR: Improve comments around the mem_avoid[] logic
      x86/boot: Simplify pointer casting in choose_random_location()
      x86/KASLR: Consolidate mem_avoid[] entries
      x86/boot: Clean up pointer casting
      x86/boot: Warn on future overlapping memcpy() use
      x86/boot: Extract error reporting functions
      x86/boot: Correctly bounds-check relocations
      x86/KASLR: Clean up unused code from old 'run_size' and rename it to 'kernel_total_size'
      x86/boot: Fix "run_size" calculation
      x86/boot: Calculate decompression size during boot not build
      ...

commit 296f781a4b7801ad9c1c0219f9e87b6c25e196fe
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Apr 26 12:23:29 2016 -0700

    x86/asm/64: Rename thread_struct's fs and gs to fsbase and gsbase
    
    Unlike ds and es, these are base addresses, not selectors.  Rename
    them so their meaning is more obvious.
    
    On x86_32, the field is still called fs.  Fixing that could make sense
    as a future cleanup.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/69a18a51c4cba0ce29a241e570fc618ad721d908.1461698311.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 9264476f3d57..9251aa962721 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -388,9 +388,16 @@ struct thread_struct {
 	unsigned long		ip;
 #endif
 #ifdef CONFIG_X86_64
-	unsigned long		fs;
+	unsigned long		fsbase;
+	unsigned long		gsbase;
+#else
+	/*
+	 * XXX: this could presumably be unsigned short.  Alternatively,
+	 * 32-bit kernels could be taught to use fsindex instead.
+	 */
+	unsigned long fs;
+	unsigned long gs;
 #endif
-	unsigned long		gs;
 
 	/* Save middle states of ptrace breakpoints */
 	struct perf_event	*ptrace_bps[HBP_NUM];

commit 867fe800b4c423bce46e66ccb2ce91bebbd5afc6
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Wed Apr 13 17:04:44 2016 -0700

    x86/paravirt: Remove paravirt_enabled()
    
    Now that all previous paravirt_enabled() uses were replaced with proper
    x86 semantics by the previous patches we can remove the unused
    paravirt_enabled() mechanism.
    
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrew.cooper3@citrix.com
    Cc: andriy.shevchenko@linux.intel.com
    Cc: bigeasy@linutronix.de
    Cc: boris.ostrovsky@oracle.com
    Cc: david.vrabel@citrix.com
    Cc: ffainelli@freebox.fr
    Cc: george.dunlap@citrix.com
    Cc: glin@suse.com
    Cc: jlee@suse.com
    Cc: josh@joshtriplett.org
    Cc: julien.grall@linaro.org
    Cc: konrad.wilk@oracle.com
    Cc: kozerkov@parallels.com
    Cc: lenb@kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-acpi@vger.kernel.org
    Cc: lv.zheng@intel.com
    Cc: matt@codeblueprint.co.uk
    Cc: mbizon@freebox.fr
    Cc: rjw@rjwysocki.net
    Cc: robert.moore@intel.com
    Cc: rusty@rustcorp.com.au
    Cc: tiwai@suse.de
    Cc: toshi.kani@hp.com
    Cc: xen-devel@lists.xensource.com
    Link: http://lkml.kernel.org/r/1460592286-300-15-git-send-email-mcgrof@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0c70c7daa6b8..8d326e822cb8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -473,7 +473,6 @@ static inline unsigned long current_top_of_stack(void)
 #include <asm/paravirt.h>
 #else
 #define __cpuid			native_cpuid
-#define paravirt_enabled()	0
 
 static inline void load_sp0(struct tss_struct *tss,
 			    struct thread_struct *thread)

commit 8d152e7a5c7537b18b4e9e0eb96f549b016636dc
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Wed Apr 13 17:04:34 2016 -0700

    x86/rtc: Replace paravirt rtc check with platform legacy quirk
    
    We have 4 types of x86 platforms that disable RTC:
    
      * Intel MID
      * Lguest - uses paravirt
      * Xen dom-U - uses paravirt
      * x86 on legacy systems annotated with an ACPI legacy flag
    
    We can consolidate all of these into a platform specific legacy
    quirk set early in boot through i386_start_kernel() and through
    x86_64_start_reservations(). This deals with the RTC quirks which
    we can rely on through the hardware subarch, the ACPI check can
    be dealt with separately.
    
    For Xen things are bit more complex given that the @X86_SUBARCH_XEN
    x86_hardware_subarch is shared on for Xen which uses the PV path for
    both domU and dom0. Since the semantics for differentiating between
    the two are Xen specific we provide a platform helper to help override
    default legacy features -- x86_platform.set_legacy_features(). Use
    of this helper is highly discouraged, its only purpose should be
    to account for the lack of semantics available within your given
    x86_hardware_subarch.
    
    As per 0-day, this bumps the vmlinux size using i386-tinyconfig as
    follows:
    
    TOTAL   TEXT   init.text    x86_early_init_platform_quirks()
    +70     +62    +62          +43
    
    Only 8 bytes overhead total, as the main increase in size is
    all removed via __init.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrew.cooper3@citrix.com
    Cc: andriy.shevchenko@linux.intel.com
    Cc: bigeasy@linutronix.de
    Cc: boris.ostrovsky@oracle.com
    Cc: david.vrabel@citrix.com
    Cc: ffainelli@freebox.fr
    Cc: george.dunlap@citrix.com
    Cc: glin@suse.com
    Cc: jlee@suse.com
    Cc: josh@joshtriplett.org
    Cc: julien.grall@linaro.org
    Cc: konrad.wilk@oracle.com
    Cc: kozerkov@parallels.com
    Cc: lenb@kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-acpi@vger.kernel.org
    Cc: lv.zheng@intel.com
    Cc: matt@codeblueprint.co.uk
    Cc: mbizon@freebox.fr
    Cc: rjw@rjwysocki.net
    Cc: robert.moore@intel.com
    Cc: rusty@rustcorp.com.au
    Cc: tiwai@suse.de
    Cc: toshi.kani@hp.com
    Cc: xen-devel@lists.xensource.com
    Link: http://lkml.kernel.org/r/1460592286-300-5-git-send-email-mcgrof@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 9264476f3d57..0c70c7daa6b8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -474,7 +474,6 @@ static inline unsigned long current_top_of_stack(void)
 #else
 #define __cpuid			native_cpuid
 #define paravirt_enabled()	0
-#define paravirt_has(x) 	0
 
 static inline void load_sp0(struct tss_struct *tss,
 			    struct thread_struct *thread)

commit 8196dab4fc159943df6baaac04973bb1accb7100
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 25 15:52:36 2016 +0100

    x86/cpu: Get rid of compute_unit_id
    
    It is cpu_core_id anyway.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1458917557-8757-3-git-send-email-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 983738ac014c..9264476f3d57 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -132,8 +132,6 @@ struct cpuinfo_x86 {
 	u16			logical_proc_id;
 	/* Core id: */
 	u16			cpu_core_id;
-	/* Compute unit id */
-	u8			compute_unit_id;
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 	u32			microcode;

commit ba33ea811e1ff6726abb7f8f96df38c2d7b50304
Merge: e23604edac2a d05004944206
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 09:32:27 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "This is another big update. Main changes are:
    
       - lots of x86 system call (and other traps/exceptions) entry code
         enhancements.  In particular the complex parts of the 64-bit entry
         code have been migrated to C code as well, and a number of dusty
         corners have been refreshed.  (Andy Lutomirski)
    
       - vDSO special mapping robustification and general cleanups (Andy
         Lutomirski)
    
       - cpufeature refactoring, cleanups and speedups (Borislav Petkov)
    
       - lots of other changes ..."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (64 commits)
      x86/cpufeature: Enable new AVX-512 features
      x86/entry/traps: Show unhandled signal for i386 in do_trap()
      x86/entry: Call enter_from_user_mode() with IRQs off
      x86/entry/32: Change INT80 to be an interrupt gate
      x86/entry: Improve system call entry comments
      x86/entry: Remove TIF_SINGLESTEP entry work
      x86/entry/32: Add and check a stack canary for the SYSENTER stack
      x86/entry/32: Simplify and fix up the SYSENTER stack #DB/NMI fixup
      x86/entry: Only allocate space for tss_struct::SYSENTER_stack if needed
      x86/entry: Vastly simplify SYSENTER TF (single-step) handling
      x86/entry/traps: Clear DR6 early in do_debug() and improve the comment
      x86/entry/traps: Clear TIF_BLOCKSTEP on all debug exceptions
      x86/entry/32: Restore FLAGS on SYSEXIT
      x86/entry/32: Filter NT and speed up AC filtering in SYSENTER
      x86/entry/compat: In SYSENTER, sink AC clearing below the existing FLAGS test
      selftests/x86: In syscall_nt, test NT|TF as well
      x86/asm-offsets: Remove PARAVIRT_enabled
      x86/entry/32: Introduce and use X86_BUG_ESPFIX instead of paravirt_enabled
      uprobes: __create_xol_area() must nullify xol_mapping.fault
      x86/cpufeature: Create a new synthetic cpu capability for machine check recovery
      ...

commit 2a41aa4feb25af3ead60b740c43df80c576efea2
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:33 2016 -0800

    x86/entry/32: Add and check a stack canary for the SYSENTER stack
    
    The first instruction of the SYSENTER entry runs on its own tiny
    stack.  That stack can be used if a #DB or NMI is delivered before
    the SYSENTER prologue switches to a real stack.
    
    We have code in place to prevent us from overflowing the tiny stack.
    For added paranoia, add a canary to the stack and check it in
    do_debug() -- that way, if something goes wrong with the #DB logic,
    we'll eventually notice.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/6ff9a806f39098b166dc2c41c1db744df5272f29.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7cd01b71b5bd..50a6dc871cc0 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -299,8 +299,9 @@ struct tss_struct {
 
 #ifdef CONFIG_X86_32
 	/*
-	 * Space for the temporary SYSENTER stack:
+	 * Space for the temporary SYSENTER stack.
 	 */
+	unsigned long		SYSENTER_stack_canary;
 	unsigned long		SYSENTER_stack[64];
 #endif
 

commit 6dcc94149d605908a7c0c4cf2085340637aac86d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:31 2016 -0800

    x86/entry: Only allocate space for tss_struct::SYSENTER_stack if needed
    
    The SYSENTER stack is only used on 32-bit kernels.  Remove it on 64-bit kernels.
    
    ( We may end up using it down the road on 64-bit kernels. If so,
      we'll re-enable it for CONFIG_IA32_EMULATION. )
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/9dbd18429f9ff61a76b6eda97a9ea20510b9f6ba.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ecb410310e70..7cd01b71b5bd 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -297,10 +297,12 @@ struct tss_struct {
 	 */
 	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
 
+#ifdef CONFIG_X86_32
 	/*
 	 * Space for the temporary SYSENTER stack:
 	 */
 	unsigned long		SYSENTER_stack[64];
+#endif
 
 } ____cacheline_aligned;
 

commit 1f12e32f4cd5243ae46d8b933181be0d022c6793
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 22 22:19:15 2016 +0000

    x86/topology: Create logical package id
    
    For per package oriented services we must be able to rely on the number of CPU
    packages to be within bounds. Create a tracking facility, which
    
    - calculates the number of possible packages depending on nr_cpu_ids after boot
    
    - makes sure that the package id is within the number of possible packages. If
      the apic id is outside we map it to a logical package id if there is enough
      space available.
    
    Provide interfaces for drivers to query the mapping and do translations from
    physcial to logical ids.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Harish Chegondi <harish.chegondi@intel.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160222221011.541071755@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 20c11d1aa4cc..813384ef811a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -129,6 +129,8 @@ struct cpuinfo_x86 {
 	u16			booted_cores;
 	/* Physical processor id: */
 	u16			phys_proc_id;
+	/* Logical processor id: */
+	u16			logical_proc_id;
 	/* Core id: */
 	u16			cpu_core_id;
 	/* Compute unit id */

commit 3a2f2ac9b96f9a9f5538396a212d3b9fb543bfc5
Merge: 4e79e182b419 f4eafd8bcd52
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 18 09:28:03 2016 +0100

    Merge branch 'x86/urgent' into x86/asm, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 75edb54a1dea5ea1c8d3d82e27dc9ee3070f5935
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Thu Feb 4 16:27:50 2016 +0100

    x86: Fix KASAN false positives in thread_saved_pc()
    
    thread_saved_pc() reads stack of a potentially running task.
    This can cause false KASAN stack-out-of-bounds reports,
    because the running task concurrently poisons and unpoisons
    own stack.
    
    The same happens in get_wchan(), and get get_wchan() was fixed
    by using READ_ONCE_NOCHECK(). Do the same here.
    
    Example KASAN report triggered by sysrq-t:
    
      BUG: KASAN: out-of-bounds in sched_show_task+0x306/0x3b0 at addr ffff880043c97c18
      Read of size 8 by task syz-executor/23839
      [...]
      page dumped because: kasan: bad access detected
      [...]
      Call Trace:
       [<ffffffff8175ea0e>] __asan_report_load8_noabort+0x3e/0x40
       [<ffffffff813e7a26>] sched_show_task+0x306/0x3b0
       [<ffffffff813e7bf4>] show_state_filter+0x124/0x1a0
       [<ffffffff82d2ca00>] fn_show_state+0x10/0x20
       [<ffffffff82d2cf98>] k_spec+0xa8/0xe0
       [<ffffffff82d3354f>] kbd_event+0xb9f/0x4000
       [<ffffffff843ca8a7>] input_to_handler+0x3a7/0x4b0
       [<ffffffff843d1954>] input_pass_values.part.5+0x554/0x6b0
       [<ffffffff843d29bc>] input_handle_event+0x2ac/0x1070
       [<ffffffff843d3a47>] input_inject_event+0x237/0x280
       [<ffffffff843e8c28>] evdev_write+0x478/0x680
       [<ffffffff817ac653>] __vfs_write+0x113/0x480
       [<ffffffff817ae0e7>] vfs_write+0x167/0x4a0
       [<ffffffff817b13d1>] SyS_write+0x111/0x220
    
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: glider@google.com
    Cc: kasan-dev@googlegroups.com
    Cc: kcc@google.com
    Cc: linux-kernel@vger.kernel.org
    Cc: ryabinin.a.a@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 2d5a50cb61a2..20c11d1aa4cc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -766,7 +766,7 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
  * Return saved PC of a blocked thread.
  * What is this good for? it will be always the scheduler or ret_from_fork.
  */
-#define thread_saved_pc(t)	(*(unsigned long *)((t)->thread.sp - 8))
+#define thread_saved_pc(t)	READ_ONCE_NOCHECK(*(unsigned long *)((t)->thread.sp - 8))
 
 #define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.sp0 - 1)
 extern unsigned long KSTK_ESP(struct task_struct *task);

commit cd4d09ec6f6c12a2cc3db5b7d8876a325a53545b
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 26 22:12:04 2016 +0100

    x86/cpufeature: Carve out X86_FEATURE_*
    
    Move them to a separate header and have the following
    dependency:
    
      x86/cpufeatures.h <- x86/processor.h <- x86/cpufeature.h
    
    This makes it easier to use the header in asm code and not
    include the whole cpufeature.h and add guards for asm.
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453842730-28463-5-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 2d5a50cb61a2..491a3d9dbb15 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -13,7 +13,7 @@ struct vm86;
 #include <asm/types.h>
 #include <uapi/asm/sigcontext.h>
 #include <asm/current.h>
-#include <asm/cpufeature.h>
+#include <asm/cpufeatures.h>
 #include <asm/page.h>
 #include <asm/pgtable_types.h>
 #include <asm/percpu.h>
@@ -24,7 +24,6 @@ struct vm86;
 #include <asm/fpu/types.h>
 
 #include <linux/personality.h>
-#include <linux/cpumask.h>
 #include <linux/cache.h>
 #include <linux/threads.h>
 #include <linux/math64.h>

commit d8c98a1d1488747625ad6044d423406e17e99b7a
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Dec 11 09:07:53 2015 -0500

    x86/paravirt: Prevent rtc_cmos platform device init on PV guests
    
    Adding the rtc platform device in non-privileged Xen PV guests causes
    an IRQ conflict because these guests do not have legacy PIC and may
    allocate irqs in the legacy range.
    
    In a single VCPU Xen PV guest we should have:
    
    /proc/interrupts:
               CPU0
      0:       4934  xen-percpu-virq      timer0
      1:          0  xen-percpu-ipi       spinlock0
      2:          0  xen-percpu-ipi       resched0
      3:          0  xen-percpu-ipi       callfunc0
      4:          0  xen-percpu-virq      debug0
      5:          0  xen-percpu-ipi       callfuncsingle0
      6:          0  xen-percpu-ipi       irqwork0
      7:        321   xen-dyn-event     xenbus
      8:         90   xen-dyn-event     hvc_console
      ...
    
    But hvc_console cannot get its interrupt because it is already in use
    by rtc0 and the console does not work.
    
      genirq: Flags mismatch irq 8. 00000000 (hvc_console) vs. 00000000 (rtc0)
    
    We can avoid this problem by realizing that unprivileged PV guests (both
    Xen and lguests) are not supposed to have rtc_cmos device and so
    adding it is not necessary.
    
    Privileged guests (i.e. Xen's dom0) do use it but they should not have
    irq conflicts since they allocate irqs above legacy range (above
    gsi_top, in fact).
    
    Instead of explicitly testing whether the guest is privileged we can
    extend pv_info structure to include information about guest's RTC
    support.
    
    Reported-and-tested-by: Sander Eikelenboom <linux@eikelenboom.it>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: vkuznets@redhat.com
    Cc: xen-devel@lists.xenproject.org
    Cc: konrad.wilk@oracle.com
    Cc: stable@vger.kernel.org # 4.2+
    Link: http://lkml.kernel.org/r/1449842873-2613-1-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 67522256c7ff..2d5a50cb61a2 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -472,6 +472,7 @@ static inline unsigned long current_top_of_stack(void)
 #else
 #define __cpuid			native_cpuid
 #define paravirt_enabled()	0
+#define paravirt_has(x) 	0
 
 static inline void load_sp0(struct tss_struct *tss,
 			    struct thread_struct *thread)

commit 4302d506d5f3419109abdd0d6e400ed6e8148209
Merge: ce4d72fac16a 0e2815de552a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 21:05:40 2015 -0800

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 sigcontext header cleanups from Ingo Molnar:
     "This series reorganizes and cleans up various aspects of the main
      sigcontext UAPI headers, such as unifying the data structures and
      updating/adding lots of comments to explain all the ABI details and
      quirks.  The headers can now also be built in user-space standalone"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/headers: Clean up too long lines
      x86/headers: Remove <asm/sigcontext.h> references on the kernel side
      x86/headers: Remove direct sigcontext32.h uses
      x86/headers: Convert sigcontext_ia32 uses to sigcontext_32
      x86/headers: Unify 'struct sigcontext_ia32' and 'struct sigcontext_32'
      x86/headers: Make sigcontext pointers bit independent
      x86/headers: Move the 'struct sigcontext' definitions into the UAPI header
      x86/headers: Clean up the kernel's struct sigcontext types to be ABI-clean
      x86/headers: Convert uses of _fpstate_ia32 to _fpstate_32
      x86/headers: Unify 'struct _fpstate_ia32' and i386 struct _fpstate
      x86/headers: Unify register type definitions between 32-bit compat and i386
      x86/headers: Use ABI types consistently in sigcontext*.h
      x86/headers: Separate out legacy user-space structure definitions
      x86/headers: Clean up and better document uapi/asm/sigcontext.h
      x86/headers: Clean up uapi/asm/sigcontext32.h
      x86/headers: Fix (old) header file dependency bug in uapi/asm/sigcontext32.h

commit 0b101e62afe626ecae60173f92f1e0ec72151653
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Thu Sep 24 14:02:29 2015 +0200

    x86/asm: Force inlining of cpu_relax()
    
    On x86, cpu_relax() simply calls rep_nop(), which generates one
    instruction, PAUSE (aka REP NOP).
    
    With this config:
    
      http://busybox.net/~vda/kernel_config_OPTIMIZE_INLINING_and_Os
    
    gcc-4.7.2 does not always inline rep_nop(): it generates several
    copies of this:
    
      <rep_nop> (16 copies, 194 calls):
           55                      push   %rbp
           48 89 e5                mov    %rsp,%rbp
           f3 90                   pause
           5d                      pop    %rbp
           c3                      retq
    
    See: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=66122
    
    This patch fixes this via s/inline/__always_inline/
    on rep_nop() and cpu_relax().
    
    ( Forcing inlining only on rep_nop() causes GCC to
      deinline cpu_relax(), with almost no change in generated code).
    
          text     data      bss       dec     hex filename
      88118971 19905208 36421632 144445811 89c1173 vmlinux.before
      88118139 19905208 36421632 144444979 89c0e33 vmlinux
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1443096149-27291-1-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 19577dd325fa..b55f30960554 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -556,12 +556,12 @@ static inline unsigned int cpuid_edx(unsigned int op)
 }
 
 /* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
-static inline void rep_nop(void)
+static __always_inline void rep_nop(void)
 {
 	asm volatile("rep; nop" ::: "memory");
 }
 
-static inline void cpu_relax(void)
+static __always_inline void cpu_relax(void)
 {
 	rep_nop();
 }

commit decb4c41159e1511197f2964da758fa7f2eeb741
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 5 09:32:43 2015 +0200

    x86/headers: Remove <asm/sigcontext.h> references on the kernel side
    
    Now that all type definitions are in the UAPI header, include it
    directly, instead of through <asm/sigcontext.h>.
    
    [ We still keep asm/sigcontext.h, so that uapi/asm/sigcontext32.h
      can include <asm/sigcontext.h>. ]
    
    Acked-by: Mikko Rapeli <mikko.rapeli@iki.fi>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1441438363-9999-16-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 19577dd325fa..bb911e718330 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -11,7 +11,7 @@ struct vm86;
 #include <asm/math_emu.h>
 #include <asm/segment.h>
 #include <asm/types.h>
-#include <asm/sigcontext.h>
+#include <uapi/asm/sigcontext.h>
 #include <asm/current.h>
 #include <asm/cpufeature.h>
 #include <asm/page.h>

commit 11e612ddb4bc667d0b830bfaab3acdf8b86da845
Merge: 5778077d03cb 656bba306827
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 09:04:31 2015 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
     "The main x86 bootup related changes in this cycle were:
    
       - more boot time optimizations.  (Len Brown)
    
       - implement hex output to allow the debugging of early bootup
         parameters.  (Kees Cook)
    
       - remove obsolete MCA leftovers.  (Paolo Pisati)"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/smpboot: Remove APIC.wait_for_init_deassert and atomic init_deasserted
      x86/smpboot: Remove SIPI delays from cpu_up()
      x86/smpboot: Remove udelay(100) when polling cpu_callin_map
      x86/smpboot: Remove udelay(100) when polling cpu_initialized_map
      x86/boot: Obsolete the MCA sys_desc_table
      x86/boot: Add hex output for debugging

commit 5461bd81bf7f86503f393b0b1d57e69b1f1bd44e
Merge: 949163015ce6 2c6625cd545b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Aug 17 10:41:59 2015 +0200

    Merge tag 'v4.2-rc7' into x86/boot, to refresh the branch before merging new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9fda6a0681e070b496235b132bc70ceb80300211
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jul 29 01:41:16 2015 -0400

    x86/vm86: Move vm86 fields out of 'thread_struct'
    
    Allocate a separate structure for the vm86 fields.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1438148483-11932-2-git-send-email-brgerst@gmail.com
    [ Build fixes. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index befc1341f110..9615a4e2645e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -6,8 +6,8 @@
 /* Forward declaration, a strange C thing */
 struct task_struct;
 struct mm_struct;
+struct vm86;
 
-#include <asm/vm86.h>
 #include <asm/math_emu.h>
 #include <asm/segment.h>
 #include <asm/types.h>
@@ -400,13 +400,9 @@ struct thread_struct {
 	unsigned long		cr2;
 	unsigned long		trap_nr;
 	unsigned long		error_code;
-#ifdef CONFIG_X86_32
+#ifdef CONFIG_VM86
 	/* Virtual 86 mode info */
-	struct vm86plus_struct __user *vm86_info;
-	unsigned long		screen_bitmap;
-	unsigned long		v86flags;
-	unsigned long		v86mask;
-	unsigned long		saved_sp0;
+	struct vm86		*vm86;
 #endif
 	/* IO permissions: */
 	unsigned long		*io_bitmap_ptr;
@@ -718,7 +714,6 @@ static inline void spin_lock_prefetch(const void *x)
 
 #define INIT_THREAD  {							  \
 	.sp0			= TOP_OF_INIT_STACK,			  \
-	.vm86_info		= NULL,					  \
 	.sysenter_cs		= __KERNEL_CS,				  \
 	.io_bitmap_ptr		= NULL,					  \
 }

commit 5b929bd11df23922daf1be5d52731cc3900c1d79
Merge: b2c51106c758 37868fe113ff
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 31 10:23:35 2015 +0200

    Merge branch 'x86/urgent' into x86/asm, before applying dependent patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 949163015ce6fdb76a5e846a3582d3c40c23c001
Author: Paolo Pisati <p.pisati@gmail.com>
Date:   Mon Jul 20 18:23:50 2015 +0200

    x86/boot: Obsolete the MCA sys_desc_table
    
    The kernel does not support the MCA bus anymroe, so mark sys_desc_table
    as obsolete: remove any reference from the code together with the remaining
    of MCA logic.
    
    bloat-o-meter output:
    
      add/remove: 0/0 grow/shrink: 0/2 up/down: 0/-55 (-55)
      function                                     old     new   delta
      i386_start_kernel                            128     119      -9
      setup_arch                                  1421    1375     -46
    
    Signed-off-by: Paolo Pisati <p.pisati@gmail.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437409430-8491-1-git-send-email-p.pisati@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 43e6519df0d5..3e15e1358f21 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -647,14 +647,6 @@ static inline void update_debugctlmsr(unsigned long debugctlmsr)
 
 extern void set_task_blockstep(struct task_struct *task, bool on);
 
-/*
- * from system description table in BIOS. Mostly for MCA use, but
- * others may find it useful:
- */
-extern unsigned int		machine_id;
-extern unsigned int		machine_submodel_id;
-extern unsigned int		BIOS_revision;
-
 /* Boot loader type from the setup header: */
 extern int			bootloader_type;
 extern int			bootloader_version;

commit ed0b2edb61ba4e557de759093d965654186f28b2
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Jul 19 21:09:06 2015 -0400

    x86/entry/vm86: Move userspace accesses to do_sys_vm86()
    
    Move the userspace accesses down into the common function in
    preparation for the next set of patches.  Also change to copying
    the fields explicitly instead of assuming a fixed order in
    pt_regs and the kernel data structures.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437354550-25858-4-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f4e4e3ff753c..35ad5547a417 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -405,7 +405,7 @@ struct thread_struct {
 	unsigned long		error_code;
 #ifdef CONFIG_X86_32
 	/* Virtual 86 mode info */
-	struct vm86_struct __user *vm86_info;
+	struct vm86plus_struct __user *vm86_info;
 	unsigned long		screen_bitmap;
 	unsigned long		v86flags;
 	unsigned long		v86mask;

commit 0233606ce5cf12c1a0e27cb197066ea5bc2bb488
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Jul 19 21:09:04 2015 -0400

    x86/entry/vm86: Clean up saved_fs/gs
    
    There is no need to save FS and non-lazy GS outside the 32-bit
    regs.  Lazy GS still needs to be saved because it wasn't saved
    on syscall entry.  Save it in the gs slot of regs32, which is
    present but unused.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437354550-25858-2-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 43e6519df0d5..f4e4e3ff753c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -410,8 +410,6 @@ struct thread_struct {
 	unsigned long		v86flags;
 	unsigned long		v86mask;
 	unsigned long		saved_sp0;
-	unsigned int		saved_fs;
-	unsigned int		saved_gs;
 #endif
 	/* IO permissions: */
 	unsigned long		*io_bitmap_ptr;

commit 0c8c0f03e3a292e031596484275c14cf39c0ab7a
Author: Dave Hansen <dave@sr71.net>
Date:   Fri Jul 17 12:28:11 2015 +0200

    x86/fpu, sched: Dynamically allocate 'struct fpu'
    
    The FPU rewrite removed the dynamic allocations of 'struct fpu'.
    But, this potentially wastes massive amounts of memory (2k per
    task on systems that do not have AVX-512 for instance).
    
    Instead of having a separate slab, this patch just appends the
    space that we need to the 'task_struct' which we dynamically
    allocate already.  This saves from doing an extra slab
    allocation at fork().
    
    The only real downside here is that we have to stick everything
    and the end of the task_struct.  But, I think the
    BUILD_BUG_ON()s I stuck in there should keep that from being too
    fragile.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437128892-9831-2-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 43e6519df0d5..944f1785ed0d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -390,9 +390,6 @@ struct thread_struct {
 #endif
 	unsigned long		gs;
 
-	/* Floating point and extended processor state */
-	struct fpu		fpu;
-
 	/* Save middle states of ptrace breakpoints */
 	struct perf_event	*ptrace_bps[HBP_NUM];
 	/* Debug status used for traps, single steps, etc... */
@@ -418,6 +415,13 @@ struct thread_struct {
 	unsigned long		iopl;
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
+
+	/* Floating point and extended processor state */
+	struct fpu		fpu;
+	/*
+	 * WARNING: 'fpu' is dynamically-sized.  It *MUST* be at
+	 * the end.
+	 */
 };
 
 /*

commit e75c73ad64478c12b3a44b86a3e7f62a4f65b93e
Merge: cfe3eceb7a2e a8424003679e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:16:11 2015 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "This tree contains two main changes:
    
       - The big FPU code rewrite: wide reaching cleanups and reorganization
         that pulls all the FPU code together into a clean base in
         arch/x86/fpu/.
    
         The resulting code is leaner and faster, and much easier to
         understand.  This enables future work to further simplify the FPU
         code (such as removing lazy FPU restores).
    
         By its nature these changes have a substantial regression risk: FPU
         code related bugs are long lived, because races are often subtle
         and bugs mask as user-space failures that are difficult to track
         back to kernel side backs.  I'm aware of no unfixed (or even
         suspected) FPU related regression so far.
    
       - MPX support rework/fixes.  As this is still not a released CPU
         feature, there were some buglets in the code - should be much more
         robust now (Dave Hansen)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (250 commits)
      x86/fpu: Fix double-increment in setup_xstate_features()
      x86/mpx: Allow 32-bit binaries on 64-bit kernels again
      x86/mpx: Do not count MPX VMAs as neighbors when unmapping
      x86/mpx: Rewrite the unmap code
      x86/mpx: Support 32-bit binaries on 64-bit kernels
      x86/mpx: Use 32-bit-only cmpxchg() for 32-bit apps
      x86/mpx: Introduce new 'directory entry' to 'addr' helper function
      x86/mpx: Add temporary variable to reduce masking
      x86: Make is_64bit_mm() widely available
      x86/mpx: Trace allocation of new bounds tables
      x86/mpx: Trace the attempts to find bounds tables
      x86/mpx: Trace entry to bounds exception paths
      x86/mpx: Trace #BR exceptions
      x86/mpx: Introduce a boot-time disable flag
      x86/mpx: Restrict the mmap() size check to bounds tables
      x86/mpx: Remove redundant MPX_BNDCFG_ADDR_MASK
      x86/mpx: Clean up the code by not passing a task pointer around when unnecessary
      x86/mpx: Use the new get_xsave_field_ptr()API
      x86/fpu/xstate: Wrap get_xsave_addr() to make it safer
      x86/fpu/xstate: Fix up bad get_xsave_addr() assumptions
      ...

commit cc2749e4095cbbcb35518fb2db5e926b85c3f25f
Author: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
Date:   Mon Jun 15 10:28:15 2015 +0200

    x86/cpu/amd: Give access to the number of nodes in a physical package
    
    Stash the number of nodes in a physical processor package
    locally and add an accessor to be called by interested parties.
    The first user is the MCE injection module which uses it to find
    the node base core in a package for injecting a certain type of
    errors.
    
    Signed-off-by: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    [ Rewrote the commit message, merged it with the accessor patch and unified naming. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-edac <linux-edac@vger.kernel.org>
    Cc: mchehab@osg.samsung.com
    Link: http://lkml.kernel.org/r/1433868317-18417-2-git-send-email-Aravind.Gopalakrishnan@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 23ba6765b718..9aa52fd13a78 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -946,6 +946,7 @@ static inline int mpx_disable_management(struct task_struct *tsk)
 #endif /* CONFIG_X86_INTEL_MPX */
 
 extern u16 amd_get_nb_id(int cpu);
+extern u32 amd_get_nodes_per_socket(void);
 
 static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
 {

commit 46a6e0cf1c6665a8e867d8f7798d7a3538633f03
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Sun Jun 7 11:37:02 2015 -0700

    x86/mpx: Clean up the code by not passing a task pointer around when unnecessary
    
    The MPX code can only work on the current task.  You can not,
    for instance, enable MPX management in another process or
    thread. You can also not handle a fault for another process or
    thread.
    
    Despite this, we pass a task_struct around prolifically.  This
    patch removes all of the task struct passing for code paths
    where the code can not deal with another task (which turns out
    to be all of them).
    
    This has no functional changes.  It's just a cleanup.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: bp@alien8.de
    Link: http://lkml.kernel.org/r/20150607183702.6A81DA2C@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 8e04f51d6bea..53dbd2b4f1d8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -802,18 +802,18 @@ extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
 /* Register/unregister a process' MPX related resource */
-#define MPX_ENABLE_MANAGEMENT(tsk)	mpx_enable_management((tsk))
-#define MPX_DISABLE_MANAGEMENT(tsk)	mpx_disable_management((tsk))
+#define MPX_ENABLE_MANAGEMENT()	mpx_enable_management()
+#define MPX_DISABLE_MANAGEMENT()	mpx_disable_management()
 
 #ifdef CONFIG_X86_INTEL_MPX
-extern int mpx_enable_management(struct task_struct *tsk);
-extern int mpx_disable_management(struct task_struct *tsk);
+extern int mpx_enable_management(void);
+extern int mpx_disable_management(void);
 #else
-static inline int mpx_enable_management(struct task_struct *tsk)
+static inline int mpx_enable_management(void)
 {
 	return -EINVAL;
 }
-static inline int mpx_disable_management(struct task_struct *tsk)
+static inline int mpx_disable_management(void)
 {
 	return -EINVAL;
 }

commit b8c1b8ea7b219a7ba6d58d97bfdf1403b741f8d5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun May 24 09:58:12 2015 +0200

    x86/fpu: Fix FPU state save area alignment bug
    
    On most configs task-struct is cache line aligned, which makes
    the XSAVE area's 64-byte required alignment work out fine.
    
    But on some .config's task_struct is aligned only to 16 bytes
    (enforced by ARCH_MIN_TASKALIGN), which makes things like
    fpu__copy() (that XSAVEOPT uses) not work so well.
    
    I broke this in:
    
      7366ed771f6e ("x86/fpu: Simplify FPU handling by embedding the fpstate in task_struct (again)")
    
    which embedded the fpstate in the task_struct.
    
    The alignment requirements of the FPU code were originally present
    in ARCH_MIN_TASKALIGN, which still has a value of 16, which was the
    alignment requirement of the FPU state area prior XSAVE. But this
    link was not documented (and not required) and the link got lost
    when the FPU state area was made dynamic years ago.
    
    With XSAVEOPT the minimum alignment requirment went up to 64 bytes,
    and the embedding of the FPU state area in task_struct exposed it
    again - and '16' was not increased to '64'.
    
    So fix this bug, but also try to address the underlying lost link
    of information that made it easier to happen:
    
      - document ARCH_MIN_TASKALIGN a bit better
    
      - use alignof() to recover the current alignment requirements.
        This would work in the future as well, should the alignment
        requirements go up to 128 bytes with things like AVX512.
    
    ( We should probably also use the vSMP alignment rules for all
      of x86, but that's for another patch. )
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b9e487499ae2..8e04f51d6bea 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -53,11 +53,16 @@ static inline void *current_text_addr(void)
 	return pc;
 }
 
+/*
+ * These alignment constraints are for performance in the vSMP case,
+ * but in the task_struct case we must also meet hardware imposed
+ * alignment requirements of the FPU state:
+ */
 #ifdef CONFIG_X86_VSMP
 # define ARCH_MIN_TASKALIGN		(1 << INTERNODE_CACHE_SHIFT)
 # define ARCH_MIN_MMSTRUCT_ALIGN	(1 << INTERNODE_CACHE_SHIFT)
 #else
-# define ARCH_MIN_TASKALIGN		16
+# define ARCH_MIN_TASKALIGN		__alignof__(union fpregs_state)
 # define ARCH_MIN_MMSTRUCT_ALIGN	0
 #endif
 

commit c66e3f28237199629358e9e5a76973c400a54041
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 15:12:44 2015 +0200

    x86/fpu: Remove the extra fpu__detect() layer
    
    Now that fpu__detect() has become an empty layer around
    fpu__init_system(), eliminate it and make fpu__init_system()
    the main system initialization routine.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0f4add462697..b9e487499ae2 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -167,7 +167,6 @@ extern const struct seq_operations cpuinfo_op;
 #define cache_line_size()	(boot_cpu_data.x86_cache_alignment)
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
-extern void fpu__detect(struct cpuinfo_x86 *c);
 
 extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);

commit c5bedc6847c3be6efe0e671a6155c9a25fd468bf
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 23 12:49:20 2015 +0200

    x86/fpu: Get rid of PF_USED_MATH usage, convert it to fpu->fpstate_active
    
    Introduce a simple fpu->fpstate_active flag in the fpu context data structure
    and use that instead of PF_USED_MATH in task->flags.
    
    Testing for this flag byte should be slightly more efficient than
    testing a bit in a bitmask, but the main advantage is that most
    FPU functions can now be performed on a 'struct fpu' alone, they
    don't need access to 'struct task_struct' anymore.
    
    There's a slight linecount increase, mostly due to the 'fpu' local
    variables and due to extra comments. The local variables will go away
    once we move most of the FPU methods to pure 'struct fpu' parameters.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d50cc7f61559..0f4add462697 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -385,6 +385,10 @@ struct thread_struct {
 	unsigned long		fs;
 #endif
 	unsigned long		gs;
+
+	/* Floating point and extended processor state */
+	struct fpu		fpu;
+
 	/* Save middle states of ptrace breakpoints */
 	struct perf_event	*ptrace_bps[HBP_NUM];
 	/* Debug status used for traps, single steps, etc... */
@@ -395,8 +399,6 @@ struct thread_struct {
 	unsigned long		cr2;
 	unsigned long		trap_nr;
 	unsigned long		error_code;
-	/* floating point and extended processor state */
-	struct fpu		fpu;
 #ifdef CONFIG_X86_32
 	/* Virtual 86 mode info */
 	struct vm86_struct __user *vm86_info;

commit f55f88e25e9b5232054a82d47de7aaf67179b78b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 16:02:33 2015 +0200

    x86/fpu: Make task_xstate_cachep static
    
    It's now local to fpu/core.c, make it static.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index fef8db024ece..d50cc7f61559 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -362,7 +362,6 @@ DECLARE_PER_CPU(struct irq_stack *, softirq_stack);
 #endif	/* X86_64 */
 
 extern unsigned int xstate_size;
-extern struct kmem_cache *task_xstate_cachep;
 
 struct perf_event;
 

commit 11ad19277e025f914518bc2943a240cdd37cf844
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 11:44:46 2015 +0200

    x86/fpu: Remove the free_thread_xstate() complication
    
    Use fpstate_free() directly to manage FPU state.
    
    Only process.c was using this method, so this is a speedup as well,
    as it removes the extra function call and related clobbers.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6b75c4b927ec..fef8db024ece 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -362,7 +362,6 @@ DECLARE_PER_CPU(struct irq_stack *, softirq_stack);
 #endif	/* X86_64 */
 
 extern unsigned int xstate_size;
-extern void free_thread_xstate(struct task_struct *);
 extern struct kmem_cache *task_xstate_cachep;
 
 struct perf_event;

commit 14b9675ae9c83c764c0c1fdf4b33f0e9156a4e4f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 09:57:24 2015 +0200

    x86/fpu: Move FPU data structures to asm/fpu_types.h
    
    Move the FPU details to asm/fpu_types.h, to further factor out the
    FPU code.
    
    ( As an added bonus, the 'struct orig_ist' definition now moves
      next to its other data types - the FPU definitions were
      slapped in the middle of them for some mysterious reason. )
    
    No code changed.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 28df85561730..6b75c4b927ec 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -21,6 +21,7 @@ struct mm_struct;
 #include <asm/desc_defs.h>
 #include <asm/nops.h>
 #include <asm/special_insns.h>
+#include <asm/fpu/types.h>
 
 #include <linux/personality.h>
 #include <linux/cpumask.h>
@@ -313,137 +314,6 @@ struct orig_ist {
 	unsigned long		ist[7];
 };
 
-#define	MXCSR_DEFAULT		0x1f80
-
-struct i387_fsave_struct {
-	u32			cwd;	/* FPU Control Word		*/
-	u32			swd;	/* FPU Status Word		*/
-	u32			twd;	/* FPU Tag Word			*/
-	u32			fip;	/* FPU IP Offset		*/
-	u32			fcs;	/* FPU IP Selector		*/
-	u32			foo;	/* FPU Operand Pointer Offset	*/
-	u32			fos;	/* FPU Operand Pointer Selector	*/
-
-	/* 8*10 bytes for each FP-reg = 80 bytes:			*/
-	u32			st_space[20];
-
-	/* Software status information [not touched by FSAVE ]:		*/
-	u32			status;
-};
-
-struct i387_fxsave_struct {
-	u16			cwd; /* Control Word			*/
-	u16			swd; /* Status Word			*/
-	u16			twd; /* Tag Word			*/
-	u16			fop; /* Last Instruction Opcode		*/
-	union {
-		struct {
-			u64	rip; /* Instruction Pointer		*/
-			u64	rdp; /* Data Pointer			*/
-		};
-		struct {
-			u32	fip; /* FPU IP Offset			*/
-			u32	fcs; /* FPU IP Selector			*/
-			u32	foo; /* FPU Operand Offset		*/
-			u32	fos; /* FPU Operand Selector		*/
-		};
-	};
-	u32			mxcsr;		/* MXCSR Register State */
-	u32			mxcsr_mask;	/* MXCSR Mask		*/
-
-	/* 8*16 bytes for each FP-reg = 128 bytes:			*/
-	u32			st_space[32];
-
-	/* 16*16 bytes for each XMM-reg = 256 bytes:			*/
-	u32			xmm_space[64];
-
-	u32			padding[12];
-
-	union {
-		u32		padding1[12];
-		u32		sw_reserved[12];
-	};
-
-} __attribute__((aligned(16)));
-
-struct i387_soft_struct {
-	u32			cwd;
-	u32			swd;
-	u32			twd;
-	u32			fip;
-	u32			fcs;
-	u32			foo;
-	u32			fos;
-	/* 8*10 bytes for each FP-reg = 80 bytes: */
-	u32			st_space[20];
-	u8			ftop;
-	u8			changed;
-	u8			lookahead;
-	u8			no_update;
-	u8			rm;
-	u8			alimit;
-	struct math_emu_info	*info;
-	u32			entry_eip;
-};
-
-struct ymmh_struct {
-	/* 16 * 16 bytes for each YMMH-reg = 256 bytes */
-	u32 ymmh_space[64];
-};
-
-/* We don't support LWP yet: */
-struct lwp_struct {
-	u8 reserved[128];
-};
-
-struct bndreg {
-	u64 lower_bound;
-	u64 upper_bound;
-} __packed;
-
-struct bndcsr {
-	u64 bndcfgu;
-	u64 bndstatus;
-} __packed;
-
-struct xsave_hdr_struct {
-	u64 xstate_bv;
-	u64 xcomp_bv;
-	u64 reserved[6];
-} __attribute__((packed));
-
-struct xsave_struct {
-	struct i387_fxsave_struct i387;
-	struct xsave_hdr_struct xsave_hdr;
-	struct ymmh_struct ymmh;
-	struct lwp_struct lwp;
-	struct bndreg bndreg[4];
-	struct bndcsr bndcsr;
-	/* new processor state extensions will go here */
-} __attribute__ ((packed, aligned (64)));
-
-union thread_xstate {
-	struct i387_fsave_struct	fsave;
-	struct i387_fxsave_struct	fxsave;
-	struct i387_soft_struct		soft;
-	struct xsave_struct		xsave;
-};
-
-struct fpu {
-	unsigned int last_cpu;
-	unsigned int has_fpu;
-	union thread_xstate *state;
-	/*
-	 * This counter contains the number of consecutive context switches
-	 * during which the FPU stays used. If this is over a threshold, the
-	 * lazy fpu saving logic becomes unlazy, to save the trap overhead.
-	 * This is an unsigned char so that after 256 iterations the counter
-	 * wraps and the context switch behavior turns lazy again; this is to
-	 * deal with bursty apps that only use the FPU for a short time:
-	 */
-	unsigned char counter;
-};
-
 #ifdef CONFIG_X86_64
 DECLARE_PER_CPU(struct orig_ist, orig_ist);
 

commit 126009993faa7a750835e67f3ccb90cee124ffa7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 09:57:24 2015 +0200

    x86/fpu: Improve the comment for the fpu::counter field
    
    This was pretty hard to read, improve it.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 64d6b5d97ce9..28df85561730 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -435,11 +435,11 @@ struct fpu {
 	union thread_xstate *state;
 	/*
 	 * This counter contains the number of consecutive context switches
-	 * that the FPU is used. If this is over a threshold, the lazy fpu
-	 * saving becomes unlazy to save the trap. This is an unsigned char
-	 * so that after 256 times the counter wraps and the behavior turns
-	 * lazy again; this to deal with bursty apps that only use FPU for
-	 * a short time
+	 * during which the FPU stays used. If this is over a threshold, the
+	 * lazy fpu saving logic becomes unlazy, to save the trap overhead.
+	 * This is an unsigned char so that after 256 iterations the counter
+	 * wraps and the context switch behavior turns lazy again; this is to
+	 * deal with bursty apps that only use the FPU for a short time:
 	 */
 	unsigned char counter;
 };

commit c0c2803dee21bef08ef5aacdf96fe2f1759ccc62
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 09:52:56 2015 +0200

    x86/fpu: Move thread_info::fpu_counter into thread_info::fpu.counter
    
    This field is kept separate from the main FPU state structure for
    no good reason.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 2dc08c231a9a..64d6b5d97ce9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -433,6 +433,15 @@ struct fpu {
 	unsigned int last_cpu;
 	unsigned int has_fpu;
 	union thread_xstate *state;
+	/*
+	 * This counter contains the number of consecutive context switches
+	 * that the FPU is used. If this is over a threshold, the lazy fpu
+	 * saving becomes unlazy to save the trap. This is an unsigned char
+	 * so that after 256 times the counter wraps and the behavior turns
+	 * lazy again; this to deal with bursty apps that only use FPU for
+	 * a short time
+	 */
+	unsigned char counter;
 };
 
 #ifdef CONFIG_X86_64
@@ -535,15 +544,6 @@ struct thread_struct {
 	unsigned long		iopl;
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
-	/*
-	 * fpu_counter contains the number of consecutive context switches
-	 * that the FPU is used. If this is over a threshold, the lazy fpu
-	 * saving becomes unlazy to save the trap. This is an unsigned char
-	 * so that after 256 times the counter wraps and the behavior turns
-	 * lazy again; this to deal with bursty apps that only use FPU for
-	 * a short time
-	 */
-	unsigned char fpu_counter;
 };
 
 /*

commit 1a7dc0db7181ebc8b9ec17e5a15ad4c766c7d3d4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 11:33:52 2015 +0200

    x86/fpu: Rename fpu_detect() to fpu__detect()
    
    Use the fpu__*() namespace to organize FPU ops better.
    
    Also document fpu__detect() a bit.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 23ba6765b718..2dc08c231a9a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -166,7 +166,7 @@ extern const struct seq_operations cpuinfo_op;
 #define cache_line_size()	(boot_cpu_data.x86_cache_alignment)
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
-extern void fpu_detect(struct cpuinfo_x86 *c);
+extern void fpu__detect(struct cpuinfo_x86 *c);
 
 extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);

commit 6c8a53c9e6a151fffb07f8b4c34bd1e33dddd467
Merge: e95e7f627062 066450be419f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 14:37:47 2015 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf changes from Ingo Molnar:
     "Core kernel changes:
    
       - One of the more interesting features in this cycle is the ability
         to attach eBPF programs (user-defined, sandboxed bytecode executed
         by the kernel) to kprobes.
    
         This allows user-defined instrumentation on a live kernel image
         that can never crash, hang or interfere with the kernel negatively.
         (Right now it's limited to root-only, but in the future we might
         allow unprivileged use as well.)
    
         (Alexei Starovoitov)
    
       - Another non-trivial feature is per event clockid support: this
         allows, amongst other things, the selection of different clock
         sources for event timestamps traced via perf.
    
         This feature is sought by people who'd like to merge perf generated
         events with external events that were measured with different
         clocks:
    
           - cluster wide profiling
    
           - for system wide tracing with user-space events,
    
           - JIT profiling events
    
         etc.  Matching perf tooling support is added as well, available via
         the -k, --clockid <clockid> parameter to perf record et al.
    
         (Peter Zijlstra)
    
      Hardware enablement kernel changes:
    
       - x86 Intel Processor Trace (PT) support: which is a hardware tracer
         on steroids, available on Broadwell CPUs.
    
         The hardware trace stream is directly output into the user-space
         ring-buffer, using the 'AUX' data format extension that was added
         to the perf core to support hardware constraints such as the
         necessity to have the tracing buffer physically contiguous.
    
         This patch-set was developed for two years and this is the result.
         A simple way to make use of this is to use BTS tracing, the PT
         driver emulates BTS output - available via the 'intel_bts' PMU.
         More explicit PT specific tooling support is in the works as well -
         will probably be ready by 4.2.
    
         (Alexander Shishkin, Peter Zijlstra)
    
       - x86 Intel Cache QoS Monitoring (CQM) support: this is a hardware
         feature of Intel Xeon CPUs that allows the measurement and
         allocation/partitioning of caches to individual workloads.
    
         These kernel changes expose the measurement side as a new PMU
         driver, which exposes various QoS related PMU events.  (The
         partitioning change is work in progress and is planned to be merged
         as a cgroup extension.)
    
         (Matt Fleming, Peter Zijlstra; CPU feature detection by Peter P
         Waskiewicz Jr)
    
       - x86 Intel Haswell LBR call stack support: this is a new Haswell
         feature that allows the hardware recording of call chains, plus
         tooling support.  To activate this feature you have to enable it
         via the new 'lbr' call-graph recording option:
    
            perf record --call-graph lbr
            perf report
    
         or:
    
            perf top --call-graph lbr
    
         This hardware feature is a lot faster than stack walk or dwarf
         based unwinding, but has some limitations:
    
           - It reuses the current LBR facility, so LBR call stack and
             branch record can not be enabled at the same time.
    
           - It is only available for user-space callchains.
    
         (Yan, Zheng)
    
       - x86 Intel Broadwell CPU support and various event constraints and
         event table fixes for earlier models.
    
         (Andi Kleen)
    
       - x86 Intel HT CPUs event scheduling workarounds.  This is a complex
         CPU bug affecting the SNB,IVB,HSW families that results in counter
         value corruption.  The mitigation code is automatically enabled and
         is transparent.
    
         (Maria Dimakopoulou, Stephane Eranian)
    
      The perf tooling side had a ton of changes in this cycle as well, so
      I'm only able to list the user visible changes here, in addition to
      the tooling changes outlined above:
    
      User visible changes affecting all tools:
    
          - Improve support of compressed kernel modules (Jiri Olsa)
          - Save DSO loading errno to better report errors (Arnaldo Carvalho de Melo)
          - Bash completion for subcommands (Yunlong Song)
          - Add 'I' event modifier for perf_event_attr.exclude_idle bit (Jiri Olsa)
          - Support missing -f to override perf.data file ownership. (Yunlong Song)
          - Show the first event with an invalid filter (David Ahern, Arnaldo Carvalho de Melo)
    
      User visible changes in individual tools:
    
        'perf data':
    
            New tool for converting perf.data to other formats, initially
            for the CTF (Common Trace Format) from LTTng (Jiri Olsa,
            Sebastian Siewior)
    
        'perf diff':
    
            Add --kallsyms option (David Ahern)
    
        'perf list':
    
            Allow listing events with 'tracepoint' prefix (Yunlong Song)
    
            Sort the output of the command (Yunlong Song)
    
        'perf kmem':
    
            Respect -i option (Jiri Olsa)
    
            Print big numbers using thousands' group (Namhyung Kim)
    
            Allow -v option (Namhyung Kim)
    
            Fix alignment of slab result table (Namhyung Kim)
    
        'perf probe':
    
            Support multiple probes on different binaries on the same command line (Masami Hiramatsu)
    
            Support unnamed union/structure members data collection. (Masami Hiramatsu)
    
            Check kprobes blacklist when adding new events. (Masami Hiramatsu)
    
        'perf record':
    
            Teach 'perf record' about perf_event_attr.clockid (Peter Zijlstra)
    
            Support recording running/enabled time (Andi Kleen)
    
        'perf sched':
    
            Improve the performance of 'perf sched replay' on high CPU core count machines (Yunlong Song)
    
        'perf report' and 'perf top':
    
            Allow annotating entries in callchains in the hists browser (Arnaldo Carvalho de Melo)
    
            Indicate which callchain entries are annotated in the
            TUI hists browser (Arnaldo Carvalho de Melo)
    
            Add pid/tid filtering to 'report' and 'script' commands (David Ahern)
    
            Consider PERF_RECORD_ events with cpumode == 0 in 'perf top', removing one
            cause of long term memory usage buildup, i.e. not processing PERF_RECORD_EXIT
            events (Arnaldo Carvalho de Melo)
    
        'perf stat':
    
            Report unsupported events properly (Suzuki K. Poulose)
    
            Output running time and run/enabled ratio in CSV mode (Andi Kleen)
    
        'perf trace':
    
            Handle legacy syscalls tracepoints (David Ahern, Arnaldo Carvalho de Melo)
    
            Only insert blank duration bracket when tracing syscalls (Arnaldo Carvalho de Melo)
    
            Filter out the trace pid when no threads are specified (Arnaldo Carvalho de Melo)
    
            Dump stack on segfaults (Arnaldo Carvalho de Melo)
    
            No need to explicitely enable evsels for workload started from perf, let it
            be enabled via perf_event_attr.enable_on_exec, removing some events that take
            place in the 'perf trace' before a workload is really started by it.
            (Arnaldo Carvalho de Melo)
    
            Allow mixing with tracepoints and suppressing plain syscalls. (Arnaldo Carvalho de Melo)
    
      There's also been a ton of infrastructure work done, such as the
      split-out of perf's build system into tools/build/ and other changes -
      see the shortlog and changelog for details"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (358 commits)
      perf/x86/intel/pt: Clean up the control flow in pt_pmu_hw_init()
      perf evlist: Fix type for references to data_head/tail
      perf probe: Check the orphaned -x option
      perf probe: Support multiple probes on different binaries
      perf buildid-list: Fix segfault when show DSOs with hits
      perf tools: Fix cross-endian analysis
      perf tools: Fix error path to do closedir() when synthesizing threads
      perf tools: Fix synthesizing fork_event.ppid for non-main thread
      perf tools: Add 'I' event modifier for exclude_idle bit
      perf report: Don't call map__kmap if map is NULL.
      perf tests: Fix attr tests
      perf probe: Fix ARM 32 building error
      perf tools: Merge all perf_event_attr print functions
      perf record: Add clockid parameter
      perf sched replay: Use replay_repeat to calculate the runavg of cpu usage instead of the default value 10
      perf sched replay: Support using -f to override perf.data file ownership
      perf sched replay: Fix the EMFILE error caused by the limitation of the maximum open files
      perf sched replay: Handle the dead halt of sem_wait when create_tasks() fails for any task
      perf sched replay: Fix the segmentation fault problem caused by pr_err in threads
      perf sched replay: Realloc the memory of pid_to_task stepwise to adapt to the different pid_max configurations
      ...

commit cf9328cc9989e028fdc64d8c0a7b1b043dc96735
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Apr 2 12:41:45 2015 -0700

    x86/asm/entry/32: Stop caching MSR_IA32_SYSENTER_ESP in tss.sp1
    
    We write a stack pointer to MSR_IA32_SYSENTER_ESP exactly once,
    and we unnecessarily cache the value in tss.sp1.  We never
    read the cached value.
    
    Remove all of the caching.  It serves no purpose.
    
    Suggested-by: Denys Vlasenko <dvlasenk@redhat.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/05a0163eb33ef5208363f0015496855da7cebadd.1428002830.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 572099710ba2..d2203b5d9538 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -209,21 +209,21 @@ struct x86_hw_tss {
 	unsigned short		back_link, __blh;
 	unsigned long		sp0;
 	unsigned short		ss0, __ss0h;
+	unsigned long		sp1;
 
 	/*
-	 * We don't use ring 1, so sp1 and ss1 are convenient scratch
-	 * spaces in the same cacheline as sp0.  We use them to cache
-	 * some MSR values to avoid unnecessary wrmsr instructions.
+	 * We don't use ring 1, so ss1 is a convenient scratch space in
+	 * the same cacheline as sp0.  We use ss1 to cache the value in
+	 * MSR_IA32_SYSENTER_CS.  When we context switch
+	 * MSR_IA32_SYSENTER_CS, we first check if the new value being
+	 * written matches ss1, and, if it's not, then we wrmsr the new
+	 * value and update ss1.
 	 *
-	 * We use SYSENTER_ESP to find sp0 and for the NMI emergency
-	 * stack, but we need to context switch it because we do
-	 * horrible things to the kernel stack in vm86 mode.
-	 *
-	 * We use SYSENTER_CS to disable sysenter in vm86 mode to avoid
-	 * corrupting the stack if we went through the sysenter path
-	 * from vm86 mode.
+	 * The only reason we context switch MSR_IA32_SYSENTER_CS is
+	 * that we set it to zero in vm86 tasks to avoid corrupting the
+	 * stack if we were to go through the sysenter path from vm86
+	 * mode.
 	 */
-	unsigned long		sp1;	/* MSR_IA32_SYSENTER_ESP */
 	unsigned short		ss1;	/* MSR_IA32_SYSENTER_CS */
 
 	unsigned short		__ss1h;

commit ac9af4983e77765a642b5a21086bc1fdc55418c4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 17 14:42:59 2015 +0100

    x86/asm/entry/64: Remove thread_struct::usersp
    
    Nothing uses thread_struct::usersp anymore, so remove it.
    
    Originally-from: Denys Vlasenko <dvlasenk@redhat.com>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3ac5092ec113..572099710ba2 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -493,7 +493,6 @@ struct thread_struct {
 #ifdef CONFIG_X86_32
 	unsigned long		sysenter_cs;
 #else
-	unsigned long		usersp;	/* Copy from PDA */
 	unsigned short		es;
 	unsigned short		ds;
 	unsigned short		fsindex;

commit 9854dd74c3f6af8d9d527de86c6074b7ed0495f1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 17 14:42:59 2015 +0100

    x86/asm/entry/64: Simplify 'old_rsp' usage
    
    Remove all manipulations of PER_CPU(old_rsp) in C code:
    
     - it is not used on SYSRET return anymore, and system entries
       are atomic, so updating it from the fork and context switch
       paths is pointless.
    
     - Tweak a few related comments as well: we no longer have a
       "partial stack frame" on entry, ever.
    
    Based on (split out of) patch from Denys Vlasenko.
    
    Originally-from: Denys Vlasenko <dvlasenk@redhat.com>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426599779-8010-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 5abd9a535a24..3ac5092ec113 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -905,11 +905,6 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 #define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.sp0 - 1)
 extern unsigned long KSTK_ESP(struct task_struct *task);
 
-/*
- * User space RSP while inside the SYSCALL fast path
- */
-DECLARE_PER_CPU(unsigned long, old_rsp);
-
 #endif /* CONFIG_X86_64 */
 
 extern void start_thread(struct pt_regs *regs, unsigned long new_ip,

commit d828c71fba8922b116b4ec56c3e5bca8c822d5ae
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Mon Mar 9 15:52:18 2015 +0100

    x86/asm/entry/32: Document the 32-bit SYSENTER "emergency stack" better
    
    Before the patch, the 'tss_struct::stack' field was not referenced anywhere.
    
    It was used only to set SYSENTER's stack to point after the last byte
    of tss_struct, thus the trailing field, stack[64], was used.
    
    But grep would not know it. You can comment it out, compile,
    and kernel will even run until an unlucky NMI corrupts
    io_bitmap[] (which is also not easily detectable).
    
    This patch changes code so that the purpose and usage of this
    field is not mysterious anymore, and can be easily grepped for.
    
    This does change generated code, for a subtle reason:
    since tss_struct is ____cacheline_aligned, there happens to be
    5 longs of padding at the end. Old code was using the padding
    too; new code will strictly use it only for SYSENTER_stack[].
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1425912738-559-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6a5c0ec5ee0e..5abd9a535a24 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -291,9 +291,9 @@ struct tss_struct {
 	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
 
 	/*
-	 * .. and then another 0x100 bytes for the emergency kernel stack:
+	 * Space for the temporary SYSENTER stack:
 	 */
-	unsigned long		stack[64];
+	unsigned long		SYSENTER_stack[64];
 
 } ____cacheline_aligned;
 

commit 5c39403e004bec75ce0c549541be5479595d6ad0
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Mar 13 15:09:03 2015 +0100

    x86/asm/entry: Simplify task_pt_regs() macro definition
    
    Before this change, task_pt_regs() was using KSTK_TOP(),
    and it was the only use of that macro. In turn, KSTK_TOP used
    THREAD_SIZE_LONGS, and it was the only use of that macro too.
    
    Fold these macros into task_pt_regs(). Tweak comment
    about "- 8" - we now use a symbolic constant, not literal 8.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426255743-5394-1-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b26208998b7c..6a5c0ec5ee0e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -847,15 +847,8 @@ static inline void spin_lock_prefetch(const void *x)
 
 extern unsigned long thread_saved_pc(struct task_struct *tsk);
 
-#define THREAD_SIZE_LONGS      (THREAD_SIZE/sizeof(unsigned long))
-#define KSTK_TOP(info)                                                 \
-({                                                                     \
-       unsigned long *__ptr = (unsigned long *)(info);                 \
-       (unsigned long)(&__ptr[THREAD_SIZE_LONGS]);                     \
-})
-
 /*
- * The below -8 is to reserve 8 bytes on top of the ring0 stack.
+ * TOP_OF_KERNEL_STACK_PADDING reserves 8 bytes on top of the ring0 stack.
  * This is necessary to guarantee that the entire "struct pt_regs"
  * is accessible even if the CPU haven't stored the SS/ESP registers
  * on the stack (interrupt gate does not save these registers
@@ -864,12 +857,11 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
  * "struct pt_regs" is possible, but they may contain the
  * completely wrong values.
  */
-#define task_pt_regs(task)                                             \
-({                                                                     \
-       struct pt_regs *__regs__;                                       \
-       __regs__ = (struct pt_regs *)(KSTK_TOP(task_stack_page(task)) - \
-				     TOP_OF_KERNEL_STACK_PADDING);     \
-       __regs__ - 1;                                                   \
+#define task_pt_regs(task) \
+({									\
+	unsigned long __ptr = (unsigned long)task_stack_page(task);	\
+	__ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;		\
+	((struct pt_regs *)__ptr) - 1;					\
 })
 
 #define KSTK_ESP(task)		(task_pt_regs(task)->sp)

commit 76e4c4908a4904a61aa67ae5eb0b2a7588c4a546
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Mar 10 11:06:00 2015 -0700

    x86/asm/entry/32: Document our abuse of x86_hw_tss::ss1 and x86_hw_tss::sp1
    
    This has confused me for a while.  Now that I figured it out, document it.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/b7efc1b7364039824776f68e9ddee9ec1500e894.1426009661.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index fc6d8d0d8d53..b26208998b7c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -209,9 +209,24 @@ struct x86_hw_tss {
 	unsigned short		back_link, __blh;
 	unsigned long		sp0;
 	unsigned short		ss0, __ss0h;
-	unsigned long		sp1;
-	/* ss1 caches MSR_IA32_SYSENTER_CS: */
-	unsigned short		ss1, __ss1h;
+
+	/*
+	 * We don't use ring 1, so sp1 and ss1 are convenient scratch
+	 * spaces in the same cacheline as sp0.  We use them to cache
+	 * some MSR values to avoid unnecessary wrmsr instructions.
+	 *
+	 * We use SYSENTER_ESP to find sp0 and for the NMI emergency
+	 * stack, but we need to context switch it because we do
+	 * horrible things to the kernel stack in vm86 mode.
+	 *
+	 * We use SYSENTER_CS to disable sysenter in vm86 mode to avoid
+	 * corrupting the stack if we went through the sysenter path
+	 * from vm86 mode.
+	 */
+	unsigned long		sp1;	/* MSR_IA32_SYSENTER_ESP */
+	unsigned short		ss1;	/* MSR_IA32_SYSENTER_CS */
+
+	unsigned short		__ss1h;
 	unsigned long		sp2;
 	unsigned short		ss2, __ss2h;
 	unsigned long		__cr3;

commit d9e05cc5a53246e074dc2b84956252e4bbe392cd
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Mar 10 11:05:59 2015 -0700

    x86/asm/entry: Unify and fix initial thread_struct::sp0 values
    
    x86_32 and x86_64 need slightly different thread_struct::sp0 values, and
    x86_32's was incorrect for init.
    
    This never mattered -- the init thread never runs user code, so we never
    used thread_struct::sp0 for anything.
    
    Fix it and mostly unify them.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1b810c1d2e797e27bb4a7708c426101161edd1f6.1426009661.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 88d9aa745898..fc6d8d0d8d53 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -811,6 +811,9 @@ static inline void spin_lock_prefetch(const void *x)
 	prefetchw(x);
 }
 
+#define TOP_OF_INIT_STACK ((unsigned long)&init_stack + sizeof(init_stack) - \
+			   TOP_OF_KERNEL_STACK_PADDING)
+
 #ifdef CONFIG_X86_32
 /*
  * User space process size: 3GB (default).
@@ -821,7 +824,7 @@ static inline void spin_lock_prefetch(const void *x)
 #define STACK_TOP_MAX		STACK_TOP
 
 #define INIT_THREAD  {							  \
-	.sp0			= sizeof(init_stack) + (long)&init_stack, \
+	.sp0			= TOP_OF_INIT_STACK,			  \
 	.vm86_info		= NULL,					  \
 	.sysenter_cs		= __KERNEL_CS,				  \
 	.io_bitmap_ptr		= NULL,					  \
@@ -883,7 +886,7 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  { \
-	.sp0 = (unsigned long)&init_stack + sizeof(init_stack) \
+	.sp0 = TOP_OF_INIT_STACK \
 }
 
 /*

commit 3ee4298f440c81638cbb5ec06f2497fb7a9a9eb4
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Mar 10 11:05:58 2015 -0700

    x86/asm/entry: Create and use a 'TOP_OF_KERNEL_STACK_PADDING' macro
    
    x86_32, unlike x86_64, pads the top of the kernel stack, because the
    hardware stack frame formats are variable in size.
    
    Document this padding and give it a name.
    
    This should make no change whatsoever to the compiled kernel
    image. It also doesn't fix any of the current bugs in this area.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/02bf2f54b8dcb76a62a142b6dfe07d4ef7fc582e.1426009661.git.luto@amacapital.net
    [ Fixed small details, such as a missed magic constant in entry_32.S pointed out by Denys Vlasenko. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 48a61c1c626e..88d9aa745898 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -849,7 +849,8 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 #define task_pt_regs(task)                                             \
 ({                                                                     \
        struct pt_regs *__regs__;                                       \
-       __regs__ = (struct pt_regs *)(KSTK_TOP(task_stack_page(task))-8); \
+       __regs__ = (struct pt_regs *)(KSTK_TOP(task_stack_page(task)) - \
+				     TOP_OF_KERNEL_STACK_PADDING);     \
        __regs__ - 1;                                                   \
 })
 

commit a7fcf28d431ef70afaa91496e64e16dc51dccec4
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Mar 6 17:50:19 2015 -0800

    x86/asm/entry: Replace this_cpu_sp0() with current_top_of_stack() and fix it on x86_32
    
    I broke 32-bit kernels.  The implementation of sp0 was correct
    as far as I can tell, but sp0 was much weirder on x86_32 than I
    realized.  It has the following issues:
    
     - Init's sp0 is inconsistent with everything else's: non-init tasks
       are offset by 8 bytes.  (I have no idea why, and the comment is unhelpful.)
    
     - vm86 does crazy things to sp0.
    
    Fix it up by replacing this_cpu_sp0() with
    current_top_of_stack() and using a new percpu variable to track
    the top of the stack on x86_32.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 75182b1632a8 ("x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()")
    Link: http://lkml.kernel.org/r/d09dbe270883433776e0cbee3c7079433349e96d.1425692936.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f5e3ec63767d..48a61c1c626e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -284,6 +284,10 @@ struct tss_struct {
 
 DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);
 
+#ifdef CONFIG_X86_32
+DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
+#endif
+
 /*
  * Save the original ist values for checking stack pointers during debugging
  */
@@ -564,9 +568,14 @@ static inline void native_swapgs(void)
 #endif
 }
 
-static inline unsigned long this_cpu_sp0(void)
+static inline unsigned long current_top_of_stack(void)
 {
+#ifdef CONFIG_X86_64
 	return this_cpu_read_stable(cpu_tss.x86_tss.sp0);
+#else
+	/* sp0 on x86_32 is special in and around vm86 mode. */
+	return this_cpu_read_stable(cpu_current_top_of_stack);
+#endif
 }
 
 #ifdef CONFIG_PARAVIRT

commit d0a0de21f82bbc1737ea3c831f018d0c2bc6b9c2
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:06 2015 -0800

    x86/asm/entry: Remove INIT_TSS and fold the definitions into 'cpu_tss'
    
    The INIT_TSS is unnecessary.  Just define the initial TSS where
    'cpu_tss' is defined.
    
    While we're at it, merge the 32-bit and 64-bit definitions.  The
    only syntactic change is that 32-bit kernels were computing sp0
    as long, but now they compute it as unsigned long.
    
    Verified by objdump: the contents and relocations of
    .data..percpu..shared_aligned are unchanged on 32-bit and 64-bit
    kernels.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8fc39fa3f6c5d635e93afbdd1a0fe0678a6d7913.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 117ee65473e2..f5e3ec63767d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -818,22 +818,6 @@ static inline void spin_lock_prefetch(const void *x)
 	.io_bitmap_ptr		= NULL,					  \
 }
 
-/*
- * Note that the .io_bitmap member must be extra-big. This is because
- * the CPU will access an additional byte beyond the end of the IO
- * permission bitmap. The extra byte must be all 1 bits, and must
- * be within the limit.
- */
-#define INIT_TSS  {							  \
-	.x86_tss = {							  \
-		.sp0		= sizeof(init_stack) + (long)&init_stack, \
-		.ss0		= __KERNEL_DS,				  \
-		.ss1		= __KERNEL_CS,				  \
-		.io_bitmap_base	= INVALID_IO_BITMAP_OFFSET,		  \
-	 },								  \
-	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },	  \
-}
-
 extern unsigned long thread_saved_pc(struct task_struct *tsk);
 
 #define THREAD_SIZE_LONGS      (THREAD_SIZE/sizeof(unsigned long))
@@ -892,10 +876,6 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 	.sp0 = (unsigned long)&init_stack + sizeof(init_stack) \
 }
 
-#define INIT_TSS  { \
-	.x86_tss.sp0 = (unsigned long)&init_stack + sizeof(init_stack) \
-}
-
 /*
  * Return saved PC of a blocked thread.
  * What is this good for? it will be always the scheduler or ret_from_fork.

commit 24933b82c0d9a711475a5ef7904eb733f561e637
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:05 2015 -0800

    x86/asm/entry: Rename 'init_tss' to 'cpu_tss'
    
    It has nothing to do with init -- there's only one TSS per cpu.
    
    Other names considered include:
    
     - current_tss: Confusing because we never switch the tss.
     - singleton_tss: Too long.
    
    This patch was generated with 's/init_tss/cpu_tss/g'.  Followup
    patches will fix INIT_TSS and INIT_TSS_IST by hand.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/da29fb2a793e4f649d93ce2d1ed320ebe8516262.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 71c3a826a690..117ee65473e2 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -282,7 +282,7 @@ struct tss_struct {
 
 } ____cacheline_aligned;
 
-DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss);
+DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);
 
 /*
  * Save the original ist values for checking stack pointers during debugging
@@ -566,7 +566,7 @@ static inline void native_swapgs(void)
 
 static inline unsigned long this_cpu_sp0(void)
 {
-	return this_cpu_read_stable(init_tss.x86_tss.sp0);
+	return this_cpu_read_stable(cpu_tss.x86_tss.sp0);
 }
 
 #ifdef CONFIG_PARAVIRT

commit 8ef46a672a7d852709561d10672b6eaa8a4acd82
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:02 2015 -0800

    x86/asm/entry: Add this_cpu_sp0() to read sp0 for the current cpu
    
    We currently store references to the top of the kernel stack in
    multiple places: kernel_stack (with an offset) and
    init_tss.x86_tss.sp0 (no offset).  The latter is defined by
    hardware and is a clean canonical way to find the top of the
    stack.  Add an accessor so we can start using it.
    
    This needs minor paravirt tweaks.  On native, sp0 defines the
    top of the kernel stack and is therefore always correct.  On Xen
    and lguest, the hypervisor tracks the top of the stack, but we
    want to start reading sp0 in the kernel.  Fixing this is simple:
    just update our local copy of sp0 as well as the hypervisor's
    copy on task switches.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8d675581859712bee09a055ed8f785d80dac1eca.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7be2c9a6caba..71c3a826a690 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -564,6 +564,11 @@ static inline void native_swapgs(void)
 #endif
 }
 
+static inline unsigned long this_cpu_sp0(void)
+{
+	return this_cpu_read_stable(init_tss.x86_tss.sp0);
+}
+
 #ifdef CONFIG_PARAVIRT
 #include <asm/paravirt.h>
 #else

commit cbc82b17263877ea5d21e84c58ce03f0292458a1
Author: Peter P Waskiewicz Jr <peter.p.waskiewicz.jr@intel.com>
Date:   Fri Jan 23 18:45:43 2015 +0000

    x86: Add support for Intel Cache QoS Monitoring (CQM) detection
    
    This patch adds support for the new Cache QoS Monitoring (CQM)
    feature found in future Intel Xeon processors.  It includes the
    new values to track CQM resources to the cpuinfo_x86 structure,
    plus the CPUID detection routines for CQM.
    
    CQM allows a process, or set of processes, to be tracked by the CPU
    to determine the cache usage of that task group.  Using this data
    from the CPU, software can be written to extract this data and
    report cache usage and occupancy for a particular process, or
    group of processes.
    
    More information about Cache QoS Monitoring can be found in the
    Intel (R) x86 Architecture Software Developer Manual, section 17.14.
    
    Signed-off-by: Peter P Waskiewicz Jr <peter.p.waskiewicz.jr@intel.com>
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Webb <chris@arachsys.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kanaka Juvva <kanaka.d.juvva@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Steven Honeyman <stevenhoneyman@gmail.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1422038748-21397-5-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ec1c93588cef..a12d50e04d7a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -109,6 +109,9 @@ struct cpuinfo_x86 {
 	/* in KB - valid for CPUS which support this call: */
 	int			x86_cache_size;
 	int			x86_cache_alignment;	/* In bytes */
+	/* Cache QoS architectural values: */
+	int			x86_cache_max_rmid;	/* max index */
+	int			x86_cache_occ_scale;	/* scale to bytes */
 	int			x86_power;
 	unsigned long		loops_per_jiffy;
 	/* cpuid returned max cores value: */

commit a930dc4543a2b213deb9fde12682716edff8a4a6
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Jan 18 17:48:18 2015 +0100

    x86/asm: Cleanup prefetch primitives
    
    This is based on a patch originally by hpa.
    
    With the current improvements to the alternatives, we can simply use %P1
    as a mem8 operand constraint and rely on the toolchain to generate the
    proper instruction sizes. For example, on 32-bit, where we use an empty
    old instruction we get:
    
      apply_alternatives: feat: 6*32+8, old: (c104648b, len: 4), repl: (c195566c, len: 4)
      c104648b: alt_insn: 90 90 90 90
      c195566c: rpl_insn: 0f 0d 4b 5c
    
      ...
    
      apply_alternatives: feat: 6*32+8, old: (c18e09b4, len: 3), repl: (c1955948, len: 3)
      c18e09b4: alt_insn: 90 90 90
      c1955948: rpl_insn: 0f 0d 08
    
      ...
    
      apply_alternatives: feat: 6*32+8, old: (c1190cf9, len: 7), repl: (c1955a79, len: 7)
      c1190cf9: alt_insn: 90 90 90 90 90 90 90
      c1955a79: rpl_insn: 0f 0d 0d a0 d4 85 c1
    
    all with the proper padding done depending on the size of the
    replacement instruction the compiler generates.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ec1c93588cef..7be2c9a6caba 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -761,10 +761,10 @@ extern char			ignore_fpu_irq;
 #define ARCH_HAS_SPINLOCK_PREFETCH
 
 #ifdef CONFIG_X86_32
-# define BASE_PREFETCH		ASM_NOP4
+# define BASE_PREFETCH		""
 # define ARCH_HAS_PREFETCH
 #else
-# define BASE_PREFETCH		"prefetcht0 (%1)"
+# define BASE_PREFETCH		"prefetcht0 %P1"
 #endif
 
 /*
@@ -775,10 +775,9 @@ extern char			ignore_fpu_irq;
  */
 static inline void prefetch(const void *x)
 {
-	alternative_input(BASE_PREFETCH,
-			  "prefetchnta (%1)",
+	alternative_input(BASE_PREFETCH, "prefetchnta %P1",
 			  X86_FEATURE_XMM,
-			  "r" (x));
+			  "m" (*(const char *)x));
 }
 
 /*
@@ -788,10 +787,9 @@ static inline void prefetch(const void *x)
  */
 static inline void prefetchw(const void *x)
 {
-	alternative_input(BASE_PREFETCH,
-			  "prefetchw (%1)",
-			  X86_FEATURE_3DNOW,
-			  "r" (x));
+	alternative_input(BASE_PREFETCH, "prefetchw %P1",
+			  X86_FEATURE_3DNOWPREFETCH,
+			  "m" (*(const char *)x));
 }
 
 static inline void spin_lock_prefetch(const void *x)

commit 375074cc736ab1d89a708c0a8d7baa4a70d5d476
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:07 2014 -0700

    x86: Clean up cr4 manipulation
    
    CR4 manipulation was split, seemingly at random, between direct
    (write_cr4) and using a helper (set/clear_in_cr4).  Unfortunately,
    the set_in_cr4 and clear_in_cr4 helpers also poke at the boot code,
    which only a small subset of users actually wanted.
    
    This patch replaces all cr4 access in functions that don't leave cr4
    exactly the way they found it with new helpers cr4_set_bits,
    cr4_clear_bits, and cr4_set_bits_and_update_boot.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/495a10bdc9e67016b8fd3945700d46cfd5c12c2f.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index a092a0cce0b7..ec1c93588cef 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -579,39 +579,6 @@ static inline void load_sp0(struct tss_struct *tss,
 #define set_iopl_mask native_set_iopl_mask
 #endif /* CONFIG_PARAVIRT */
 
-/*
- * Save the cr4 feature set we're using (ie
- * Pentium 4MB enable and PPro Global page
- * enable), so that any CPU's that boot up
- * after us can get the correct flags.
- */
-extern unsigned long mmu_cr4_features;
-extern u32 *trampoline_cr4_features;
-
-static inline void set_in_cr4(unsigned long mask)
-{
-	unsigned long cr4;
-
-	mmu_cr4_features |= mask;
-	if (trampoline_cr4_features)
-		*trampoline_cr4_features = mmu_cr4_features;
-	cr4 = read_cr4();
-	cr4 |= mask;
-	write_cr4(cr4);
-}
-
-static inline void clear_in_cr4(unsigned long mask)
-{
-	unsigned long cr4;
-
-	mmu_cr4_features &= ~mask;
-	if (trampoline_cr4_features)
-		*trampoline_cr4_features = mmu_cr4_features;
-	cr4 = read_cr4();
-	cr4 &= ~mask;
-	write_cr4(cr4);
-}
-
 typedef struct {
 	unsigned long		seg;
 } mm_segment_t;

commit 3100e448e7d74489a96cb7b45d88fe6962774eaa
Merge: c9f861c77269 26893107aa71
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 14:24:20 2014 -0800

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 vdso updates from Ingo Molnar:
     "Various vDSO updates from Andy Lutomirski, mostly cleanups and
      reorganization to improve maintainability, but also some
      micro-optimizations and robustization changes"
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86_64/vsyscall: Restore orig_ax after vsyscall seccomp
      x86_64: Add a comment explaining the TASK_SIZE_MAX guard page
      x86_64,vsyscall: Make vsyscall emulation configurable
      x86_64, vsyscall: Rewrite comment and clean up headers in vsyscall code
      x86_64, vsyscall: Turn vsyscalls all the way off when vsyscall==none
      x86,vdso: Use LSL unconditionally for vgetcpu
      x86: vdso: Fix build with older gcc
      x86_64/vdso: Clean up vgetcpu init and merge the vdso initcalls
      x86_64/vdso: Remove jiffies from the vvar page
      x86/vdso: Make the PER_CPU segment 32 bits
      x86/vdso: Make the PER_CPU segment start out accessed
      x86/vdso: Change the PER_CPU segment to use struct desc_struct
      x86_64/vdso: Move getcpu code from vsyscall_64.c to vdso/vma.c
      x86_64/vsyscall: Move all of the gate_area code to vsyscall_64.c

commit b6444bd0a18eb47343e16749ce80a6ebd521f124
Merge: 9d0cf6f56454 97b67ae55994
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 12:10:24 2014 -0800

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot and percpu updates from Ingo Molnar:
     "This tree contains a bootable images documentation update plus three
      slightly misplaced x86/asm percpu changes/optimizations"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86-64: Use RIP-relative addressing for most per-CPU accesses
      x86-64: Handle PC-relative relocations on per-CPU data
      x86: Convert a few more per-CPU items to read-mostly ones
      x86, boot: Document intermediates more clearly

commit fe3d197f84319d3bce379a9c0dc17b1f48ad358c
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Nov 14 07:18:29 2014 -0800

    x86, mpx: On-demand kernel allocation of bounds tables
    
    This is really the meat of the MPX patch set.  If there is one patch to
    review in the entire series, this is the one.  There is a new ABI here
    and this kernel code also interacts with userspace memory in a
    relatively unusual manner.  (small FAQ below).
    
    Long Description:
    
    This patch adds two prctl() commands to provide enable or disable the
    management of bounds tables in kernel, including on-demand kernel
    allocation (See the patch "on-demand kernel allocation of bounds tables")
    and cleanup (See the patch "cleanup unused bound tables"). Applications
    do not strictly need the kernel to manage bounds tables and we expect
    some applications to use MPX without taking advantage of this kernel
    support. This means the kernel can not simply infer whether an application
    needs bounds table management from the MPX registers.  The prctl() is an
    explicit signal from userspace.
    
    PR_MPX_ENABLE_MANAGEMENT is meant to be a signal from userspace to
    require kernel's help in managing bounds tables.
    
    PR_MPX_DISABLE_MANAGEMENT is the opposite, meaning that userspace don't
    want kernel's help any more. With PR_MPX_DISABLE_MANAGEMENT, the kernel
    won't allocate and free bounds tables even if the CPU supports MPX.
    
    PR_MPX_ENABLE_MANAGEMENT will fetch the base address of the bounds
    directory out of a userspace register (bndcfgu) and then cache it into
    a new field (->bd_addr) in  the 'mm_struct'.  PR_MPX_DISABLE_MANAGEMENT
    will set "bd_addr" to an invalid address.  Using this scheme, we can
    use "bd_addr" to determine whether the management of bounds tables in
    kernel is enabled.
    
    Also, the only way to access that bndcfgu register is via an xsaves,
    which can be expensive.  Caching "bd_addr" like this also helps reduce
    the cost of those xsaves when doing table cleanup at munmap() time.
    Unfortunately, we can not apply this optimization to #BR fault time
    because we need an xsave to get the value of BNDSTATUS.
    
    ==== Why does the hardware even have these Bounds Tables? ====
    
    MPX only has 4 hardware registers for storing bounds information.
    If MPX-enabled code needs more than these 4 registers, it needs to
    spill them somewhere. It has two special instructions for this
    which allow the bounds to be moved between the bounds registers
    and some new "bounds tables".
    
    They are similar conceptually to a page fault and will be raised by
    the MPX hardware during both bounds violations or when the tables
    are not present. This patch handles those #BR exceptions for
    not-present tables by carving the space out of the normal processes
    address space (essentially calling the new mmap() interface indroduced
    earlier in this patch set.) and then pointing the bounds-directory
    over to it.
    
    The tables *need* to be accessed and controlled by userspace because
    the instructions for moving bounds in and out of them are extremely
    frequent. They potentially happen every time a register pointing to
    memory is dereferenced. Any direct kernel involvement (like a syscall)
    to access the tables would obviously destroy performance.
    
    ==== Why not do this in userspace? ====
    
    This patch is obviously doing this allocation in the kernel.
    However, MPX does not strictly *require* anything in the kernel.
    It can theoretically be done completely from userspace. Here are
    a few ways this *could* be done. I don't think any of them are
    practical in the real-world, but here they are.
    
    Q: Can virtual space simply be reserved for the bounds tables so
       that we never have to allocate them?
    A: As noted earlier, these tables are *HUGE*. An X-GB virtual
       area needs 4*X GB of virtual space, plus 2GB for the bounds
       directory. If we were to preallocate them for the 128TB of
       user virtual address space, we would need to reserve 512TB+2GB,
       which is larger than the entire virtual address space today.
       This means they can not be reserved ahead of time. Also, a
       single process's pre-popualated bounds directory consumes 2GB
       of virtual *AND* physical memory. IOW, it's completely
       infeasible to prepopulate bounds directories.
    
    Q: Can we preallocate bounds table space at the same time memory
       is allocated which might contain pointers that might eventually
       need bounds tables?
    A: This would work if we could hook the site of each and every
       memory allocation syscall. This can be done for small,
       constrained applications. But, it isn't practical at a larger
       scale since a given app has no way of controlling how all the
       parts of the app might allocate memory (think libraries). The
       kernel is really the only place to intercept these calls.
    
    Q: Could a bounds fault be handed to userspace and the tables
       allocated there in a signal handler instead of in the kernel?
    A: (thanks to tglx) mmap() is not on the list of safe async
       handler functions and even if mmap() would work it still
       requires locking or nasty tricks to keep track of the
       allocation state there.
    
    Having ruled out all of the userspace-only approaches for managing
    bounds tables that we could think of, we create them on demand in
    the kernel.
    
    Based-on-patch-by: Qiaowei Ren <qiaowei.ren@intel.com>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-mm@kvack.org
    Cc: linux-mips@linux-mips.org
    Cc: Dave Hansen <dave@sr71.net>
    Link: http://lkml.kernel.org/r/20141114151829.AD4310DE@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6571aaabacb9..9617a1716813 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -954,6 +954,24 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
+/* Register/unregister a process' MPX related resource */
+#define MPX_ENABLE_MANAGEMENT(tsk)	mpx_enable_management((tsk))
+#define MPX_DISABLE_MANAGEMENT(tsk)	mpx_disable_management((tsk))
+
+#ifdef CONFIG_X86_INTEL_MPX
+extern int mpx_enable_management(struct task_struct *tsk);
+extern int mpx_disable_management(struct task_struct *tsk);
+#else
+static inline int mpx_enable_management(struct task_struct *tsk)
+{
+	return -EINVAL;
+}
+static inline int mpx_disable_management(struct task_struct *tsk)
+{
+	return -EINVAL;
+}
+#endif /* CONFIG_X86_INTEL_MPX */
+
 extern u16 amd_get_nb_id(int cpu);
 
 static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)

commit 62e7759b1bdaf9b753c3724a56fcbe3235ebd5f3
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Nov 14 07:18:17 2014 -0800

    x86, mpx: Rename cfg_reg_u and status_reg
    
    According to Intel SDM extension, MPX configuration and status registers
    should be BNDCFGU and BNDSTATUS. This patch renames cfg_reg_u and
    status_reg to bndcfgu and bndstatus.
    
    [ tglx: Renamed 'struct bndscr_struct' to 'struct bndscr' ]
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-mm@kvack.org
    Cc: linux-mips@linux-mips.org
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Qiaowei Ren <qiaowei.ren@intel.com>
    Link: http://lkml.kernel.org/r/20141114151817.031762AC@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0f2263a8ad31..6571aaabacb9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -379,9 +379,9 @@ struct bndreg {
 	u64 upper_bound;
 } __packed;
 
-struct bndcsr_struct {
-	u64 cfg_reg_u;
-	u64 status_reg;
+struct bndcsr {
+	u64 bndcfgu;
+	u64 bndstatus;
 } __packed;
 
 struct xsave_hdr_struct {
@@ -396,7 +396,7 @@ struct xsave_struct {
 	struct ymmh_struct ymmh;
 	struct lwp_struct lwp;
 	struct bndreg bndreg[4];
-	struct bndcsr_struct bndcsr;
+	struct bndcsr bndcsr;
 	/* new processor state extensions will go here */
 } __attribute__ ((packed, aligned (64)));
 

commit c04e051cccd2446d9ca373628d14b7e732462f5d
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Oct 31 14:58:20 2014 -0700

    x86: mpx: Give bndX registers actual names
    
    Consider the bndX MPX registers.  There 4 registers each
    containing a 64-bit lower and a 64-bit upper bound.  That's 8*64
    bits and we declare it thusly:
    
            struct bndregs_struct {
                    u64 bndregs[8];
            }
    
    Let's say you want to read the upper bound from the MPX register
    bnd2 out of the xsave buf.  You do:
    
            bndregno = 2;
            upper_bound = xsave_buf->bndregs.bndregs[2*bndregno+1];
    
    That kinda sucks.  Every time you access it, you need to know:
    1. Each bndX register is two entries wide in "bndregs"
    2. The lower comes first followed by upper.  We do the +1 to get
       upper vs. lower.
    
    This replaces the old definition.  You can now access them
    indexed by the register number directly, and with a meaningful
    name for the lower and upper bound:
    
            bndregno = 2;
            xsave_buf->bndreg[bndregno].upper_bound;
    
    It's now *VERY* clear that there are 4 registers.  The programmer
    now doesn't have to care what order the lower and upper bounds
    are in, and it's harder to get it wrong.
    
    [ tglx: Changed ub/lb to upper_bound/lower_bound and renamed struct
    bndreg_struct to struct bndreg ]
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: x86@kernel.org
    Cc: "H. Peter Anvin" <hpa@linux.intel.com>
    Cc: Qiaowei Ren <qiaowei.ren@intel.com>
    Cc: "Yu, Fenghua" <fenghua.yu@intel.com>
    Cc: Dave Hansen <dave@sr71.net>
    Link: http://lkml.kernel.org/r/20141031215820.5EA5E0EC@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index eb71ec794732..0f2263a8ad31 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -374,8 +374,9 @@ struct lwp_struct {
 	u8 reserved[128];
 };
 
-struct bndregs_struct {
-	u64 bndregs[8];
+struct bndreg {
+	u64 lower_bound;
+	u64 upper_bound;
 } __packed;
 
 struct bndcsr_struct {
@@ -394,7 +395,7 @@ struct xsave_struct {
 	struct xsave_hdr_struct xsave_hdr;
 	struct ymmh_struct ymmh;
 	struct lwp_struct lwp;
-	struct bndregs_struct bndregs;
+	struct bndreg bndreg[4];
 	struct bndcsr_struct bndcsr;
 	/* new processor state extensions will go here */
 } __attribute__ ((packed, aligned (64)));

commit 07114f0f1cda8b2ef6e884d0c7b268a32cce7903
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Nov 4 15:46:21 2014 -0800

    x86_64: Add a comment explaining the TASK_SIZE_MAX guard page
    
    That guard page is absolutely necessary; explain why for
    posterity.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/23320cb5017c2da8475ec20fcde8089d82aa2699.1415144745.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index eb71ec794732..82d93ea13c0c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -893,7 +893,13 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 
 #else
 /*
- * User space process size. 47bits minus one guard page.
+ * User space process size. 47bits minus one guard page.  The guard
+ * page is necessary on Intel CPUs: if a SYSCALL instruction is at
+ * the highest possible canonical userspace address, then that
+ * syscall will enter the kernel with a non-canonical return
+ * address, and SYSRET will explode dangerously.  We avoid this
+ * particular problem by preventing anything from being mapped
+ * at the maximum canonical address.
  */
 #define TASK_SIZE_MAX	((1UL << 47) - PAGE_SIZE)
 

commit 2c773dd31fbacbbb6425f8a9d3f97e0010272368
Author: Jan Beulich <JBeulich@suse.com>
Date:   Tue Nov 4 08:26:42 2014 +0000

    x86: Convert a few more per-CPU items to read-mostly ones
    
    Both this_cpu_off and cpu_info aren't getting modified post boot, yet
    are being accessed on enough code paths that grouping them with other
    frequently read items seems desirable. For cpu_info this at the same
    time implies removing the cache line alignment (which afaict became
    pointless when it got converted to per-CPU data years ago).
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Link: http://lkml.kernel.org/r/54589BD20200007800044A84@mail.emea.novell.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index eb71ec794732..3aeb31cc0511 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -127,7 +127,7 @@ struct cpuinfo_x86 {
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 	u32			microcode;
-} __attribute__((__aligned__(SMP_CACHE_BYTES)));
+};
 
 #define X86_VENDOR_INTEL	0
 #define X86_VENDOR_CYRIX	1
@@ -151,7 +151,7 @@ extern __u32			cpu_caps_cleared[NCAPINTS];
 extern __u32			cpu_caps_set[NCAPINTS];
 
 #ifdef CONFIG_SMP
-DECLARE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
+DECLARE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);
 #define cpu_data(cpu)		per_cpu(cpu_info, cpu)
 #else
 #define cpu_info		boot_cpu_data

commit 7453f33b2e07fc2835e24cda0893de83c78d8d76
Merge: fd1cf9058028 d0f2dd186133
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 13 18:20:04 2014 -0600

    Merge branch 'x86-xsave-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/xsave changes from Peter Anvin:
     "This is a patchset to support the XSAVES instruction required to
      support context switch of supervisor-only features in upcoming
      silicon.
    
      This patchset missed the 3.16 merge window, which is why it is based
      on 3.15-rc7"
    
    * 'x86-xsave-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, xsave: Add forgotten inline annotation
      x86/xsaves: Clean up code in xstate offsets computation in xsave area
      x86/xsave: Make it clear that the XSAVE macros use (%edi)/(%rdi)
      Define kernel API to get address of each state in xsave area
      x86/xsaves: Enable xsaves/xrstors
      x86/xsaves: Call booting time xsaves and xrstors in setup_init_fpu_buf
      x86/xsaves: Save xstate to task's xsave area in __save_fpu during booting time
      x86/xsaves: Add xsaves and xrstors support for booting time
      x86/xsaves: Clear reserved bits in xsave header
      x86/xsaves: Use xsave/xrstor for saving and restoring user space context
      x86/xsaves: Use xsaves/xrstors for context switch
      x86/xsaves: Use xsaves/xrstors to save and restore xsave area
      x86/xsaves: Define a macro for handling xsave/xrstor instruction fault
      x86/xsaves: Define macros for xsave instructions
      x86/xsaves: Change compacted format xsave area header
      x86/alternative: Add alternative_input_2 to support alternative with two features and input
      x86/xsaves: Add a kernel parameter noxsaves to disable xsaves/xrstors

commit ce4747963252a30613ebf1c1df3d83b9526a342e
Merge: 76f09aa464a1 a5102476a24b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 17:15:45 2014 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "The main change in this cycle is the rework of the TLB range flushing
      code, to simplify, fix and consolidate the code.  By Dave Hansen"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Set TLB flush tunable to sane value (33)
      x86/mm: New tunable for single vs full TLB flush
      x86/mm: Add tracepoints for TLB flushes
      x86/mm: Unify remote INVLPG code
      x86/mm: Fix missed global TLB flush stat
      x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
      x86/mm: Clean up the TLB flushing code
      x86/smep: Be more informative when signalling an SMEP fault

commit e9f4e0a9fe2723078b7a1a1169828dd46a7b2f9e
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jul 31 08:40:55 2014 -0700

    x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
    
    I think the flush_tlb_mm_range() code that tries to tune the
    flush sizes based on the CPU needs to get ripped out for
    several reasons:
    
    1. It is obviously buggy.  It uses mm->total_vm to judge the
       task's footprint in the TLB.  It should certainly be using
       some measure of RSS, *NOT* ->total_vm since only resident
       memory can populate the TLB.
    2. Haswell, and several other CPUs are missing from the
       intel_tlb_flushall_shift_set() function.  Thus, it has been
       demonstrated to bitrot quickly in practice.
    3. It is plain wrong in my vm:
            [    0.037444] Last level iTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] Last level dTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] tlb_flushall_shift: 6
       Which leads to it to never use invlpg.
    4. The assumptions about TLB refill costs are wrong:
            http://lkml.kernel.org/r/1337782555-8088-3-git-send-email-alex.shi@intel.com
        (more on this in later patches)
    5. I can not reproduce the original data: https://lkml.org/lkml/2012/5/17/59
       I believe the sample times were too short.  Running the
       benchmark in a loop yields times that vary quite a bit.
    
    Note that this leaves us with a static ceiling of 1 page.  This
    is a conservative, dumb setting, and will be revised in a later
    patch.
    
    This also removes the code which attempts to predict whether we
    are flushing data or instructions.  We expect instruction flushes
    to be relatively rare and not worth tuning for explicitly.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20140731154055.ABC88E89@viggo.jf.intel.com
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index a4ea02351f4d..43d61daea966 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -72,7 +72,6 @@ extern u16 __read_mostly tlb_lld_4k[NR_INFO];
 extern u16 __read_mostly tlb_lld_2m[NR_INFO];
 extern u16 __read_mostly tlb_lld_4m[NR_INFO];
 extern u16 __read_mostly tlb_lld_1g[NR_INFO];
-extern s8  __read_mostly tlb_flushall_shift;
 
 /*
  *  CPU type and hardware bug flags. Kept separately for each CPU.

commit 3a6bfbc91df04b081a44d419e0260bad54abddf7
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sun Jun 29 15:09:33 2014 -0700

    arch, locking: Ciao arch_mutex_cpu_relax()
    
    The arch_mutex_cpu_relax() function, introduced by 34b133f, is
    hacky and ugly. It was added a few years ago to address the fact
    that common cpu_relax() calls include yielding on s390, and thus
    impact the optimistic spinning functionality of mutexes. Nowadays
    we use this function well beyond mutexes: rwsem, qrwlock, mcs and
    lockref. Since the macro that defines the call is in the mutex header,
    any users must include mutex.h and the naming is misleading as well.
    
    This patch (i) renames the call to cpu_relax_lowlatency  ("relax, but
    only if you can do it with very low latency") and (ii) defines it in
    each arch's asm/processor.h local header, just like for regular cpu_relax
    functions. On all archs, except s390, cpu_relax_lowlatency is simply cpu_relax,
    and thus we can take it out of mutex.h. While this can seem redundant,
    I believe it is a good choice as it allows us to move out arch specific
    logic from generic locking primitives and enables future(?) archs to
    transparently define it, similarly to System Z.
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Bharat Bhushan <r65777@freescale.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Joseph Myers <joseph@codesourcery.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Qiaowei Ren <qiaowei.ren@intel.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Stratos Karafotis <stratosk@semaphore.gr>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vasily Kulikov <segoon@openwall.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Vineet Gupta <Vineet.Gupta1@synopsys.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wolfram Sang <wsa@the-dreams.de>
    Cc: adi-buildroot-devel@lists.sourceforge.net
    Cc: linux390@de.ibm.com
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-am33-list@redhat.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-cris-kernel@axis.com
    Cc: linux-hexagon@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux@lists.openrisc.net
    Cc: linux-m32r-ja@ml.linux-m32r.org
    Cc: linux-m32r@ml.linux-m32r.org
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linux-metag@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1404079773.2619.4.camel@buesod1.americas.hpqcorp.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index a4ea02351f4d..32cc237f8e20 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -696,6 +696,8 @@ static inline void cpu_relax(void)
 	rep_nop();
 }
 
+#define cpu_relax_lowlatency() cpu_relax()
+
 /* Stop speculative execution and prefetching of modified code. */
 static inline void sync_core(void)
 {

commit 0b29643a58439dc9a8b0c0cacad0e7cb608c8199
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu May 29 11:12:33 2014 -0700

    x86/xsaves: Change compacted format xsave area header
    
    The XSAVE area header is changed to support both compacted format and
    standard format of xsave area.
    
    The XSAVE header of an xsave area comprises the 64 bytes starting at offset
    512 from the area base address:
    
    - Bytes 7:0 of the xsave header is a state-component bitmap called
      xstate_bv. It identifies the state components in the xsave area.
    
    - Bytes 15:8 of the xsave header is a state-component bitmap called
      xcomp_bv. It is used as follows:
      - xcomp_bv[63] indicates the format of the extended region of
        the xsave area. If it is clear, the standard format is used.
        If it is set, the compacted format is used.
      - xcomp_bv[62:0] indicate which features (starting at feature 2)
        have space allocated for them in the compacted format.
    
    - Bytes 63:16 of the xsave header are reserved.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1401387164-43416-6-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index a4ea02351f4d..2c8d3b8e7fcb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -386,8 +386,8 @@ struct bndcsr_struct {
 
 struct xsave_hdr_struct {
 	u64 xstate_bv;
-	u64 reserved1[2];
-	u64 reserved2[5];
+	u64 xcomp_bv;
+	u64 reserved[6];
 } __attribute__((packed));
 
 struct xsave_struct {

commit 198d208df4371734ac4728f69cb585c284d20a15
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 6 09:41:31 2014 -0500

    x86: Keep thread_info on thread stack in x86_32
    
    x86_64 uses a per_cpu variable kernel_stack to always point to
    the thread stack of current. This is where the thread_info is stored
    and is accessed from this location even when the irq or exception stack
    is in use. This removes the complexity of having to maintain the
    thread info on the stack when interrupts are running and having to
    copy the preempt_count and other fields to the interrupt stack.
    
    x86_32 uses the old method of copying the thread_info from the thread
    stack to the exception stack just before executing the exception.
    
    Having the two different requires #ifdefs and also the x86_32 way
    is a bit of a pain to maintain. By converting x86_32 to the same
    method of x86_64, we can remove #ifdefs, clean up the x86_32 code
    a little, and remove the overhead of the copy.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20110806012354.263834829@goodmis.org
    Link: http://lkml.kernel.org/r/20140206144321.852942014@goodmis.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index fdedd38fd0fc..a4ea02351f4d 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -449,6 +449,15 @@ struct stack_canary {
 };
 DECLARE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif
+/*
+ * per-CPU IRQ handling stacks
+ */
+struct irq_stack {
+	u32                     stack[THREAD_SIZE/sizeof(u32)];
+} __aligned(THREAD_SIZE);
+
+DECLARE_PER_CPU(struct irq_stack *, hardirq_stack);
+DECLARE_PER_CPU(struct irq_stack *, softirq_stack);
 #endif	/* X86_64 */
 
 extern unsigned int xstate_size;

commit c9cdd9a6ae49704f12a289706551536ec842693f
Merge: f4bcd8ccddb0 741e3902cd89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 14:46:32 2014 -0800

    Merge branch 'x86/mpx' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpufeature and mpx updates from Peter Anvin:
     "This includes the basic infrastructure for MPX (Memory Protection
      Extensions) support, but does not include MPX support itself.  It is,
      however, a prerequisite for KVM support for MPX, which I believe will
      be pushed later this merge window by the KVM team.
    
      This includes moving the functionality in
      futex_atomic_cmpxchg_inatomic() into a new function in uaccess.h so it
      can be reused - this will be used by the final MPX patches.
    
      The actual MPX functionality (map management and so on) will be pushed
      in a future merge window, when ready"
    
    * 'x86/mpx' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/intel/mpx: Remove unused LWP structure
      x86, mpx: Add MPX related opcodes to the x86 opcode map
      x86: replace futex_atomic_cmpxchg_inatomic() with user_atomic_cmpxchg_inatomic
      x86: add user_atomic_cmpxchg_inatomic at uaccess.h
      x86, xsave: Support eager-only xsave features, add MPX support
      x86, cpufeature: Define the Intel MPX feature flag

commit 5d4863e4cc4dc12d1d5e42da3cb5d38c535e4ad6
Merge: 2a0fede97fd5 dd360393f4d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:04:45 2014 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 TLB detection update from Ingo Molnar:
     "A single change that extends our TLB cache size detection+reporting
      code"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpu: Detect more TLB configuration

commit 2a0fede97fd52a5c9789d1d54ebd3b46878151c3
Merge: 06bc0f4a2e7f d139336700a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:03:57 2014 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpu, amd: Fix a shadowed variable situation
      um, x86: Fix vDSO build
      x86: Delete non-required instances of include <linux/init.h>
      x86, realmode: Pointer walk cleanups, pull out invariant use of __pa()
      x86/traps: Clean up error exception handler definitions

commit 741e3902cd89a7fbc04ae53f29a7ca0da452aa8e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jan 20 19:51:05 2014 +0100

    x86/intel/mpx: Remove unused LWP structure
    
    We don't support LWP yet, don't give the impression that we do:
    represent the LWP state as opaque 128 bytes, the way Linux sees it
    currently.
    
    Cc: Qiaowei Ren <qiaowei.ren@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-ecarmjtfKpanpAapfck6dj6g@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b7845a126792..865c39910cc8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -370,15 +370,9 @@ struct ymmh_struct {
 	u32 ymmh_space[64];
 };
 
+/* We don't support LWP yet: */
 struct lwp_struct {
-	u64 lwpcb_addr;
-	u32 flags;
-	u32 buf_head_offset;
-	u64 buf_base;
-	u32 buf_size;
-	u32 filters;
-	u64 saved_event_record[4];
-	u32 event_counter[16];
+	u8 reserved[128];
 };
 
 struct bndregs_struct {

commit 663b55b9b39fa9c848cca273ca4e12bf29b32c71
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Jan 6 19:20:26 2014 -0500

    x86: Delete non-required instances of include <linux/init.h>
    
    None of these files are actually using any __init type directives
    and hence don't need to include <linux/init.h>.  Most are just a
    left over from __devinit and __cpuinit removal, or simply due to
    code getting copied from one driver to the next.
    
    [ hpa: undid incorrect removal from arch/x86/kernel/head_32.S ]
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Link: http://lkml.kernel.org/r/1389054026-12947-1-git-send-email-paul.gortmaker@windriver.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7b034a4057f9..8ade61721ffb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -27,7 +27,6 @@ struct mm_struct;
 #include <linux/cache.h>
 #include <linux/threads.h>
 #include <linux/math64.h>
-#include <linux/init.h>
 #include <linux/err.h>
 #include <linux/irqflags.h>
 

commit dd360393f4d948eb518372316e52101cf3b44212
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Dec 23 14:16:58 2013 +0200

    x86, cpu: Detect more TLB configuration
    
    The Intel Software Developer’s Manual covers few more TLB
    configurations exposed as CPUID 2 descriptors:
    
    61H Instruction TLB: 4 KByte pages, fully associative, 48 entries
    63H Data TLB: 1 GByte pages, 4-way set associative, 4 entries
    76H Instruction TLB: 2M/4M pages, fully associative, 8 entries
    B5H Instruction TLB: 4KByte pages, 8-way set associative, 64 entries
    B6H Instruction TLB: 4KByte pages, 8-way set associative, 128 entries
    C1H Shared 2nd-Level TLB: 4 KByte/2MByte pages, 8-way associative, 1024 entries
    C2H DTLB DTLB: 2 MByte/$MByte pages, 4-way associative, 16 entries
    
    Let's detect them as well.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/1387801018-14499-1-git-send-email-kirill.shutemov@linux.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7b034a4057f9..1dd6260ed940 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -72,6 +72,7 @@ extern u16 __read_mostly tlb_lli_4m[NR_INFO];
 extern u16 __read_mostly tlb_lld_4k[NR_INFO];
 extern u16 __read_mostly tlb_lld_2m[NR_INFO];
 extern u16 __read_mostly tlb_lld_4m[NR_INFO];
+extern u16 __read_mostly tlb_lld_1g[NR_INFO];
 extern s8  __read_mostly tlb_flushall_shift;
 
 /*

commit 16824255394f55adf31b9a96a9965d8c15bdac4c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 12 15:08:36 2013 +0100

    x86, acpi, idle: Restructure the mwait idle routines
    
    People seem to delight in writing wrong and broken mwait idle routines;
    collapse the lot.
    
    This leaves mwait_play_dead() the sole remaining user of __mwait() and
    new __mwait() users are probably doing it wrong.
    
    Also remove __sti_mwait() as its unused.
    
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jacob Jun Pan <jacob.jun.pan@linux.intel.com>
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Acked-by: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20131212141654.616820819@infradead.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7b034a4057f9..24821f5768bc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -700,29 +700,6 @@ static inline void sync_core(void)
 #endif
 }
 
-static inline void __monitor(const void *eax, unsigned long ecx,
-			     unsigned long edx)
-{
-	/* "monitor %eax, %ecx, %edx;" */
-	asm volatile(".byte 0x0f, 0x01, 0xc8;"
-		     :: "a" (eax), "c" (ecx), "d"(edx));
-}
-
-static inline void __mwait(unsigned long eax, unsigned long ecx)
-{
-	/* "mwait %eax, %ecx;" */
-	asm volatile(".byte 0x0f, 0x01, 0xc9;"
-		     :: "a" (eax), "c" (ecx));
-}
-
-static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
-{
-	trace_hardirqs_on();
-	/* "mwait %eax, %ecx;" */
-	asm volatile("sti; .byte 0x0f, 0x01, 0xc9;"
-		     :: "a" (eax), "c" (ecx));
-}
-
 extern void select_idle_routine(const struct cpuinfo_x86 *c);
 extern void init_amd_e400_c1e_mask(void);
 

commit e7d820a5e549b3eb6c3f9467507566565646a669
Author: Qiaowei Ren <qiaowei.ren@intel.com>
Date:   Thu Dec 5 17:15:34 2013 +0800

    x86, xsave: Support eager-only xsave features, add MPX support
    
    Some features, like Intel MPX, work only if the kernel uses eagerfpu
    model.  So we should force eagerfpu on unless the user has explicitly
    disabled it.
    
    Add definitions for Intel MPX and add it to the supported list.
    
    [ hpa: renamed XSTATE_FLEXIBLE to XSTATE_LAZY and added comments ]
    
    Signed-off-by: Qiaowei Ren <qiaowei.ren@intel.com>
    Link: http://lkml.kernel.org/r/9E0BE1322F2F2246BD820DA9FC397ADE014A6115@SHSMSX102.ccr.corp.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7b034a4057f9..b7845a126792 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -370,6 +370,26 @@ struct ymmh_struct {
 	u32 ymmh_space[64];
 };
 
+struct lwp_struct {
+	u64 lwpcb_addr;
+	u32 flags;
+	u32 buf_head_offset;
+	u64 buf_base;
+	u32 buf_size;
+	u32 filters;
+	u64 saved_event_record[4];
+	u32 event_counter[16];
+};
+
+struct bndregs_struct {
+	u64 bndregs[8];
+} __packed;
+
+struct bndcsr_struct {
+	u64 cfg_reg_u;
+	u64 status_reg;
+} __packed;
+
 struct xsave_hdr_struct {
 	u64 xstate_bv;
 	u64 reserved1[2];
@@ -380,6 +400,9 @@ struct xsave_struct {
 	struct i387_fxsave_struct i387;
 	struct xsave_hdr_struct xsave_hdr;
 	struct ymmh_struct ymmh;
+	struct lwp_struct lwp;
+	struct bndregs_struct bndregs;
+	struct bndcsr_struct bndcsr;
 	/* new processor state extensions will go here */
 } __attribute__ ((packed, aligned (64)));
 

commit c375f15a434db1867cb004bafba92aba739e4e39
Author: Vineet Gupta <Vineet.Gupta1@synopsys.com>
Date:   Tue Nov 12 15:08:46 2013 -0800

    x86: move fpu_counter into ARCH specific thread_struct
    
    Only a couple of arches (sh/x86) use fpu_counter in task_struct so it can
    be moved out into ARCH specific thread_struct, reducing the size of
    task_struct for other arches.
    
    Compile tested i386_defconfig + gcc 4.7.3
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Paul Mundt <paul.mundt@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 987c75ecc334..7b034a4057f9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -488,6 +488,15 @@ struct thread_struct {
 	unsigned long		iopl;
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
+	/*
+	 * fpu_counter contains the number of consecutive context switches
+	 * that the FPU is used. If this is over a threshold, the lazy fpu
+	 * saving becomes unlazy to save the trap. This is an unsigned char
+	 * so that after 256 times the counter wraps and the behavior turns
+	 * lazy again; this to deal with bursty apps that only use FPU for
+	 * a short time
+	 */
+	unsigned char fpu_counter;
 };
 
 /*

commit 05eebfb26bac9215410b70e0f043861aecff896c
Merge: cb3e4330e697 9df56f19a500
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 11:05:13 2013 -0700

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt changes from Ingo Molnar:
     "Hypervisor signature detection cleanup and fixes - the goal is to make
      KVM guests run better on MS/Hyperv and to generalize and factor out
      the code a bit"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Correctly detect hypervisor
      x86, kvm: Switch to use hypervisor_cpuid_base()
      xen: Switch to use hypervisor_cpuid_base()
      x86: Introduce hypervisor_cpuid_base()

commit 2a475501b81f06f64c474cfad66f8807294b4534
Merge: 3d7e5fc37f91 eb86b5fd505c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 08:42:44 2013 -0700

    Merge branch 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/asmlinkage changes from Ingo Molnar:
     "As a preparation for Andi Kleen's LTO patchset (link time
      optimizations using GCC's -flto which build time optimization has
      steadily increased in quality over the past few years and might
      eventually be usable for the kernel too) this tree includes a handful
      of preparatory patches that make function calling convention
      annotations consistent again:
    
       - Mark every function without arguments (or 64bit only) that is used
         by assembly code with asmlinkage()
    
       - Mark every function with parameters or variables that is used by
         assembly code as __visible.
    
      For the vanilla kernel this has documentation, consistency and
      debuggability advantages, for the time being"
    
    * 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/asmlinkage: Fix warning in xen asmlinkage change
      x86, asmlinkage, vdso: Mark vdso variables __visible
      x86, asmlinkage, power: Make various symbols used by the suspend asm code visible
      x86, asmlinkage: Make dump_stack visible
      x86, asmlinkage: Make 64bit checksum functions visible
      x86, asmlinkage, paravirt: Add __visible/asmlinkage to xen paravirt ops
      x86, asmlinkage, apm: Make APM data structure used from assembler visible
      x86, asmlinkage: Make syscall tables visible
      x86, asmlinkage: Make several variables used from assembler/linker script visible
      x86, asmlinkage: Make kprobes code visible and fix assembler code
      x86, asmlinkage: Make various syscalls asmlinkage
      x86, asmlinkage: Make 32bit/64bit __switch_to visible
      x86, asmlinkage: Make _*_start_kernel visible
      x86, asmlinkage: Make all interrupt handlers asmlinkage / __visible
      x86, asmlinkage: Change dotraplinkage into __visible on 32bit
      x86: Fix sys_call_table type in asm/syscall.h

commit 277d5b40b7bf495d2d4193746181b17dd98441b2
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:43 2013 -0700

    x86, asmlinkage: Make several variables used from assembler/linker script visible
    
    Plus one function, load_gs_index().
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-10-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 24cf5aefb704..573c1ad4994e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -412,7 +412,7 @@ union irq_stack_union {
 	};
 };
 
-DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union);
+DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union) __visible;
 DECLARE_INIT_PER_CPU(irq_stack_union);
 
 DECLARE_PER_CPU(char *, irq_stack_ptr);

commit 96e39ac0e9d18020c8c5a41b64b8fe86c26575ab
Author: Jason Wang <jasowang@redhat.com>
Date:   Thu Jul 25 16:54:32 2013 +0800

    x86: Introduce hypervisor_cpuid_base()
    
    This patch introduce hypervisor_cpuid_base() which loop test the hypervisor
    existence function until the signature match and check the number of leaves if
    required. This could be used by Xen/KVM guest to detect the existence of
    hypervisor.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Jason Wang <jasowang@redhat.com>
    Link: http://lkml.kernel.org/r/1374742475-2485-1-git-send-email-jasowang@redhat.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 24cf5aefb704..7763307b4137 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -971,6 +971,21 @@ unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
 	return ratio;
 }
 
+static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
+{
+	uint32_t base, eax, signature[3];
+
+	for (base = 0x40000000; base < 0x40010000; base += 0x100) {
+		cpuid(base, &eax, &signature[0], &signature[1], &signature[2]);
+
+		if (!memcmp(sig, signature, 12) &&
+		    (leaves == 0 || ((eax - base) >= leaves)))
+			return base;
+	}
+
+	return 0;
+}
+
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 

commit 61c63e5ed3b9c472899d7152e961f2ffaafcf5a0
Author: Stratos Karafotis <stratosk@semaphore.gr>
Date:   Wed Jun 5 19:01:42 2013 +0300

    cpufreq: Remove unused APERF/MPERF support
    
    The target frequency calculation method in the ondemand governor has
    changed and it is now independent of the measured average frequency.
    Consequently, the APERF/MPERF support in cpufreq is not used any
    more, so drop it.
    
    [rjw: Changelog]
    Signed-off-by: Stratos Karafotis <stratosk@semaphore.gr>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 24cf5aefb704..4f4a3d98c170 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -942,35 +942,6 @@ extern int set_tsc_mode(unsigned int val);
 
 extern u16 amd_get_nb_id(int cpu);
 
-struct aperfmperf {
-	u64 aperf, mperf;
-};
-
-static inline void get_aperfmperf(struct aperfmperf *am)
-{
-	WARN_ON_ONCE(!boot_cpu_has(X86_FEATURE_APERFMPERF));
-
-	rdmsrl(MSR_IA32_APERF, am->aperf);
-	rdmsrl(MSR_IA32_MPERF, am->mperf);
-}
-
-#define APERFMPERF_SHIFT 10
-
-static inline
-unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
-				    struct aperfmperf *new)
-{
-	u64 aperf = new->aperf - old->aperf;
-	u64 mperf = new->mperf - old->mperf;
-	unsigned long ratio = aperf;
-
-	mperf >>= APERFMPERF_SHIFT;
-	if (mperf)
-		ratio = div64_u64(aperf, mperf);
-
-	return ratio;
-}
-
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 29937c4f6ff8..24cf5aefb704 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -164,7 +164,7 @@ extern const struct seq_operations cpuinfo_op;
 #define cache_line_size()	(boot_cpu_data.x86_cache_alignment)
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
-extern void __cpuinit fpu_detect(struct cpuinfo_x86 *c);
+extern void fpu_detect(struct cpuinfo_x86 *c);
 
 extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);

commit d652df0b2f0b9c2e655be0d6f90989fae0a511b4
Merge: 4d6f843a38fa 5f8c42181488
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 16:26:44 2013 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU changes from Ingo Molnar:
     "There are two bigger changes in this tree:
    
       - Add an [early-use-]safe static_cpu_has() variant and other
         robustness improvements, including the new X86_DEBUG_STATIC_CPU_HAS
         configurable debugging facility, motivated by recent obscure FPU
         code bugs, by Borislav Petkov
    
       - Reimplement FPU detection code in C and drop the old asm code, by
         Peter Anvin."
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: Use static_cpu_has_safe before alternatives
      x86: Add a static_cpu_has_safe variant
      x86: Sanity-check static_cpu_has usage
      x86, cpu: Add a synthetic, always true, cpu feature
      x86: Get rid of ->hard_math and all the FPU asm fu

commit 60e019eb37a8d989031ad47ae9810453536f3127
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Apr 29 16:04:20 2013 +0200

    x86: Get rid of ->hard_math and all the FPU asm fu
    
    Reimplement FPU detection code in C and drop old, not-so-recommended
    detection method in asm. Move all the relevant stuff into i387.c where
    it conceptually belongs. Finally drop cpuinfo_x86.hard_math.
    
    [ hpa: huge thanks to Borislav for taking my original concept patch
      and productizing it ]
    
    [ Boris, note to self: do not use static_cpu_has before alternatives! ]
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1367244262-29511-2-git-send-email-bp@alien8.de
    Link: http://lkml.kernel.org/r/1365436666-9837-2-git-send-email-bp@alien8.de
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 22224b3b43bb..578f8b1d6910 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -89,9 +89,9 @@ struct cpuinfo_x86 {
 	char			wp_works_ok;	/* It doesn't on 386's */
 
 	/* Problems on some 486Dx4's and old 386's: */
-	char			hard_math;
 	char			rfu;
 	char			pad0;
+	char			pad1;
 #else
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
 	int			x86_tlbsize;
@@ -164,6 +164,7 @@ extern const struct seq_operations cpuinfo_op;
 #define cache_line_size()	(boot_cpu_data.x86_cache_alignment)
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
+extern void __cpuinit fpu_detect(struct cpuinfo_x86 *c);
 
 extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);

commit 4d067d8e056d76a3327f0517c7722db55e7888fc
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 9 12:02:29 2013 +0200

    x86: Extend #DF debugging aid to 64-bit
    
    It is sometimes very helpful to be able to pinpoint the location which
    causes a double fault before it turns into a triple fault and the
    machine reboots. We have this for 32-bit already so extend it to 64-bit.
    On 64-bit we get the register snapshot at #DF time and not from the
    first exception which actually causes the #DF. It should be close
    enough, though.
    
    [ hpa: and definitely better than nothing, which is what we have now. ]
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1368093749-31296-1-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 22224b3b43bb..5b87d52eed0b 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -981,5 +981,5 @@ bool xen_set_default_idle(void);
 #endif
 
 void stop_this_cpu(void *dummy);
-
+void df_debug(struct pt_regs *regs, long error_code);
 #endif /* _ASM_X86_PROCESSOR_H */

commit 7d7dc116e56c8a1ba4beb36d06a77a48fe5f750b
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:28 2013 +0100

    x86, cpu: Convert AMD Erratum 400
    
    Convert AMD erratum 400 to the bug infrastructure. Then, retract all
    exports for modules since they're not needed now and make the AMD
    erratum checking machinery local to amd.c. Use forward declarations to
    avoid shuffling too much code around needlessly.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-7-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index e044ef35f91f..22224b3b43bb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -970,25 +970,6 @@ unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
 	return ratio;
 }
 
-/*
- * AMD errata checking
- */
-#ifdef CONFIG_CPU_SUP_AMD
-extern const int amd_erratum_400[];
-extern bool cpu_has_amd_erratum(const int *);
-
-#define AMD_LEGACY_ERRATUM(...)		{ -1, __VA_ARGS__, 0 }
-#define AMD_OSVW_ERRATUM(osvw_id, ...)	{ osvw_id, __VA_ARGS__, 0 }
-#define AMD_MODEL_RANGE(f, m_start, s_start, m_end, s_end) \
-	((f << 24) | (m_start << 16) | (s_start << 12) | (m_end << 4) | (s_end))
-#define AMD_MODEL_RANGE_FAMILY(range)	(((range) >> 24) & 0xff)
-#define AMD_MODEL_RANGE_START(range)	(((range) >> 12) & 0xfff)
-#define AMD_MODEL_RANGE_END(range)	((range) & 0xfff)
-
-#else
-#define cpu_has_amd_erratum(x)	(false)
-#endif /* CONFIG_CPU_SUP_AMD */
-
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 

commit e6ee94d58dfd06ec64c55f91581f00d4f98bf1f6
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:27 2013 +0100

    x86, cpu: Convert AMD Erratum 383
    
    Convert the AMD erratum 383 testing code to the bug infrastructure. This
    allows keeping the AMD-specific erratum testing machinery private to
    amd.c and not export symbols to modules needlessly.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-6-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 4e2fa2859e39..e044ef35f91f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -974,7 +974,6 @@ unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
  * AMD errata checking
  */
 #ifdef CONFIG_CPU_SUP_AMD
-extern const int amd_erratum_383[];
 extern const int amd_erratum_400[];
 extern bool cpu_has_amd_erratum(const int *);
 

commit c5b41a67505cc3c9744d8f105c63a3bf3c443a01
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:26 2013 +0100

    x86, cpu: Convert Cyrix coma bug detection
    
    ... to the new facility.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-5-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ea22dfaf6c5e..4e2fa2859e39 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -91,7 +91,6 @@ struct cpuinfo_x86 {
 	/* Problems on some 486Dx4's and old 386's: */
 	char			hard_math;
 	char			rfu;
-	char			coma_bug;
 	char			pad0;
 #else
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */

commit 93a829e8e2c292f1d30155f64803101ca1cb7d3d
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:25 2013 +0100

    x86, cpu: Convert FDIV bug detection
    
    ... to the new facility. Add a reference to the wikipedia article
    explaining the FDIV test we're doing here.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-4-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1e55e2d543b5..ea22dfaf6c5e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -91,7 +91,6 @@ struct cpuinfo_x86 {
 	/* Problems on some 486Dx4's and old 386's: */
 	char			hard_math;
 	char			rfu;
-	char			fdiv_bug;
 	char			coma_bug;
 	char			pad0;
 #else

commit e2604b49e8a8827e8bc5b61bc0c5a889e028fdad
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:24 2013 +0100

    x86, cpu: Convert F00F bug detection
    
    ... to using the new facility and drop the cpuinfo_x86 member.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-3-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 23c8081d3870..1e55e2d543b5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -92,7 +92,6 @@ struct cpuinfo_x86 {
 	char			hard_math;
 	char			rfu;
 	char			fdiv_bug;
-	char			f00f_bug;
 	char			coma_bug;
 	char			pad0;
 #else

commit 65fc985b37dc241c4db7cd32adcbc989193fe3c8
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:23 2013 +0100

    x86, cpu: Expand cpufeature facility to include cpu bugs
    
    We add another 32-bit vector at the end of the ->x86_capability
    bitvector which collects bugs present in CPUs. After all, a CPU bug is a
    kind of a capability, albeit a strange one.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-2-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3270116b1488..23c8081d3870 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -107,7 +107,7 @@ struct cpuinfo_x86 {
 	__u32			extended_cpuid_level;
 	/* Maximum supported CPUID level, -1=no CPUID: */
 	int			cpuid_level;
-	__u32			x86_capability[NCAPINTS];
+	__u32			x86_capability[NCAPINTS + NBUGINTS];
 	char			x86_vendor_id[16];
 	char			x86_model_id[64];
 	/* in KB - valid for CPUS which support this call: */

commit c47f39e3b75e1138823984ad5079547c7a41b726
Merge: 0cc9129d75ef da76f64e7eb2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 22 19:22:52 2013 -0800

    Merge branch 'x86/microcode' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 microcode loading update from Peter Anvin:
     "This patchset lets us update the CPU microcode very, very early in
      initialization if the BIOS fails to do so (never happens, right?)
    
      This is handy for dealing with things like the Atom erratum where we
      have to run without PSE because microcode loading happens too late.
    
      As I mentioned in the x86/mm push request it depends on that
      infrastructure but it is otherwise a standalone feature."
    
    * 'x86/microcode' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/Kconfig: Make early microcode loading a configuration feature
      x86/mm/init.c: Copy ucode from initrd image to kernel memory
      x86/head64.c: Early update ucode in 64-bit
      x86/head_32.S: Early update ucode in 32-bit
      x86/microcode_intel_early.c: Early update ucode on Intel's CPU
      x86/tlbflush.h: Define __native_flush_tlb_global_irq_disabled()
      x86/microcode_intel_lib.c: Early update ucode on Intel's CPU
      x86/microcode_core_early.c: Define interfaces for early loading ucode
      x86/common.c: load ucode in 64 bit or show loading ucode info in 32 bit on AP
      x86/common.c: Make have_cpuid_p() a global function
      x86/microcode_intel.h: Define functions and macros for early loading ucode
      x86, doc: Documentation for early microcode loading

commit 2ef14f465b9e096531343f5b734cffc5f759f4a6
Merge: cb715a836642 0da3e7f526fd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 21 18:06:55 2013 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Peter Anvin:
     "This is a huge set of several partly interrelated (and concurrently
      developed) changes, which is why the branch history is messier than
      one would like.
    
      The *really* big items are two humonguous patchsets mostly developed
      by Yinghai Lu at my request, which completely revamps the way we
      create initial page tables.  In particular, rather than estimating how
      much memory we will need for page tables and then build them into that
      memory -- a calculation that has shown to be incredibly fragile -- we
      now build them (on 64 bits) with the aid of a "pseudo-linear mode" --
      a #PF handler which creates temporary page tables on demand.
    
      This has several advantages:
    
      1. It makes it much easier to support things that need access to data
         very early (a followon patchset uses this to load microcode way
         early in the kernel startup).
    
      2. It allows the kernel and all the kernel data objects to be invoked
         from above the 4 GB limit.  This allows kdump to work on very large
         systems.
    
      3. It greatly reduces the difference between Xen and native (Xen's
         equivalent of the #PF handler are the temporary page tables created
         by the domain builder), eliminating a bunch of fragile hooks.
    
      The patch series also gets us a bit closer to W^X.
    
      Additional work in this pull is the 64-bit get_user() work which you
      were also involved with, and a bunch of cleanups/speedups to
      __phys_addr()/__pa()."
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (105 commits)
      x86, mm: Move reserving low memory later in initialization
      x86, doc: Clarify the use of asm("%edx") in uaccess.h
      x86, mm: Redesign get_user with a __builtin_choose_expr hack
      x86: Be consistent with data size in getuser.S
      x86, mm: Use a bitfield to mask nuisance get_user() warnings
      x86/kvm: Fix compile warning in kvm_register_steal_time()
      x86-32: Add support for 64bit get_user()
      x86-32, mm: Remove reference to alloc_remap()
      x86-32, mm: Remove reference to resume_map_numa_kva()
      x86-32, mm: Rip out x86_32 NUMA remapping code
      x86/numa: Use __pa_nodebug() instead
      x86: Don't panic if can not alloc buffer for swiotlb
      mm: Add alloc_bootmem_low_pages_nopanic()
      x86, 64bit, mm: hibernate use generic mapping_init
      x86, 64bit, mm: Mark data/bss/brk to nx
      x86: Merge early kernel reserve for 32bit and 64bit
      x86: Add Crash kernel low reservation
      x86, kdump: Remove crashkernel range find limit for 64bit
      memblock: Add memblock_mem_size()
      x86, boot: Not need to check setup_header version for setup_data
      ...

commit 8793422fd9ac5037f5047f80473007301df3689f
Merge: b3cdda2b4f54 10baf04e95fb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 20 11:26:56 2013 -0800

    Merge tag 'pm+acpi-3.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI and power management updates from Rafael Wysocki:
    
     - Rework of the ACPI namespace scanning code from Rafael J.  Wysocki
       with contributions from Bjorn Helgaas, Jiang Liu, Mika Westerberg,
       Toshi Kani, and Yinghai Lu.
    
     - ACPI power resources handling and ACPI device PM update from Rafael
       J Wysocki.
    
     - ACPICA update to version 20130117 from Bob Moore and Lv Zheng with
       contributions from Aaron Lu, Chao Guan, Jesper Juhl, and Tim Gardner.
    
     - Support for Intel Lynxpoint LPSS from Mika Westerberg.
    
     - cpuidle update from Len Brown including Intel Haswell support, C1
       state for intel_idle, removal of global pm_idle.
    
     - cpuidle fixes and cleanups from Daniel Lezcano.
    
     - cpufreq fixes and cleanups from Viresh Kumar and Fabio Baltieri with
       contributions from Stratos Karafotis and Rickard Andersson.
    
     - Intel P-states driver for Sandy Bridge processors from Dirk
       Brandewie.
    
     - cpufreq driver for Marvell Kirkwood SoCs from Andrew Lunn.
    
     - cpufreq fixes related to ordering issues between acpi-cpufreq and
       powernow-k8 from Borislav Petkov and Matthew Garrett.
    
     - cpufreq support for Calxeda Highbank processors from Mark Langsdorf
       and Rob Herring.
    
     - cpufreq driver for the Freescale i.MX6Q SoC and cpufreq-cpu0 update
       from Shawn Guo.
    
     - cpufreq Exynos fixes and cleanups from Jonghwan Choi, Sachin Kamat,
       and Inderpal Singh.
    
     - Support for "lightweight suspend" from Zhang Rui.
    
     - Removal of the deprecated power trace API from Paul Gortmaker.
    
     - Assorted updates from Andreas Fleig, Colin Ian King, Davidlohr Bueso,
       Joseph Salisbury, Kees Cook, Li Fei, Nishanth Menon, ShuoX Liu,
       Srinivas Pandruvada, Tejun Heo, Thomas Renninger, and Yasuaki
       Ishimatsu.
    
    * tag 'pm+acpi-3.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (267 commits)
      PM idle: remove global declaration of pm_idle
      unicore32 idle: delete stray pm_idle comment
      openrisc idle: delete pm_idle
      mn10300 idle: delete pm_idle
      microblaze idle: delete pm_idle
      m32r idle: delete pm_idle, and other dead idle code
      ia64 idle: delete pm_idle
      cris idle: delete idle and pm_idle
      ARM64 idle: delete pm_idle
      ARM idle: delete pm_idle
      blackfin idle: delete pm_idle
      sparc idle: rename pm_idle to sparc_idle
      sh idle: rename global pm_idle to static sh_idle
      x86 idle: rename global pm_idle to static x86_idle
      APM idle: register apm_cpu_idle via cpuidle
      cpufreq / intel_pstate: Add kernel command line option disable intel_pstate.
      cpufreq / intel_pstate: Change to disallow module build
      tools/power turbostat: display SMI count by default
      intel_idle: export both C1 and C1E
      ACPI / hotplug: Fix concurrency issues and memory leaks
      ...

commit 27be457000211a6903968dfce06d5f73f051a217
Author: Len Brown <len.brown@intel.com>
Date:   Sun Feb 10 02:28:46 2013 -0500

    x86 idle: remove 32-bit-only "no-hlt" parameter, hlt_works_ok flag
    
    Remove 32-bit x86 a cmdline param "no-hlt",
    and the cpuinfo_x86.hlt_works_ok that it sets.
    
    If a user wants to avoid HLT, then "idle=poll"
    is much more useful, as it avoids invocation of HLT
    in idle, while "no-hlt" failed to do so.
    
    Indeed, hlt_works_ok was consulted in only 3 places.
    
    First, in /proc/cpuinfo where "hlt_bug yes"
    would be printed if and only if the user booted
    the system with "no-hlt" -- as there was no other code
    to set that flag.
    
    Second, check_hlt() would not invoke halt() if "no-hlt"
    were on the cmdline.
    
    Third, it was consulted in stop_this_cpu(), which is invoked
    by native_machine_halt()/reboot_interrupt()/smp_stop_nmi_callback() --
    all cases where the machine is being shutdown/reset.
    The flag was not consulted in the more frequently invoked
    play_dead()/hlt_play_dead() used in processor offline and suspend.
    
    Since Linux-3.0 there has been a run-time notice upon "no-hlt" invocations
    indicating that it would be removed in 2012.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 8a28feae1c92..b9e7d279f8ef 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -89,7 +89,6 @@ struct cpuinfo_x86 {
 	char			wp_works_ok;	/* It doesn't on 386's */
 
 	/* Problems on some 486Dx4's and old 386's: */
-	char			hlt_works_ok;
 	char			hard_math;
 	char			rfu;
 	char			fdiv_bug;
@@ -165,15 +164,6 @@ DECLARE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 
 extern const struct seq_operations cpuinfo_op;
 
-static inline int hlt_works(int cpu)
-{
-#ifdef CONFIG_X86_32
-	return cpu_data(cpu).hlt_works_ok;
-#else
-	return 1;
-#endif
-}
-
 #define cache_line_size()	(boot_cpu_data.x86_cache_alignment)
 
 extern void cpu_detect(struct cpuinfo_x86 *c);

commit 69fb3676df3329a7142803bb3502fa59dc0db2e3
Author: Len Brown <len.brown@intel.com>
Date:   Sun Feb 10 01:38:39 2013 -0500

    x86 idle: remove mwait_idle() and "idle=mwait" cmdline param
    
    mwait_idle() is a C1-only idle loop intended to be more efficient
    than HLT, starting on Pentium-4 HT-enabled processors.
    
    But mwait_idle() has been replaced by the more general
    mwait_idle_with_hints(), which handles both C1 and deeper C-states.
    ACPI processor_idle and intel_idle use only mwait_idle_with_hints(),
    and no longer use mwait_idle().
    
    Here we simplify the x86 native idle code by removing mwait_idle(),
    and the "idle=mwait" bootparam used to invoke it.
    
    Since Linux 3.0 there has been a boot-time warning when "idle=mwait"
    was invoked saying it would be removed in 2012.  This removal
    was also noted in the (now removed:-) feature-removal-schedule.txt.
    
    After this change, kernels configured with
    (CONFIG_ACPI=n && CONFIG_INTEL_IDLE=n) when run on hardware
    that supports MWAIT will simply use HLT.  If MWAIT is desired
    on those systems, cpuidle and the cpuidle drivers above
    can be enabled.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c2f7f472275e..8a28feae1c92 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -725,7 +725,7 @@ extern unsigned long		boot_option_idle_override;
 extern bool			amd_e400_c1e_detected;
 
 enum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,
-			 IDLE_POLL, IDLE_FORCE_MWAIT};
+			 IDLE_POLL};
 
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);

commit 6a377ddc4e4ede2eeb9cd46ada23bbe417704fc9
Author: Len Brown <len.brown@intel.com>
Date:   Sat Feb 9 23:08:07 2013 -0500

    xen idle: make xen-specific macro xen-specific
    
    This macro is only invoked by Xen,
    so make its definition specific to Xen.
    
    > set_pm_idle_to_default()
    < xen_set_default_idle()
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: xen-devel@lists.xensource.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 888184b2fc85..c2f7f472275e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -998,7 +998,11 @@ extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 
 void default_idle(void);
-bool set_pm_idle_to_default(void);
+#ifdef	CONFIG_XEN
+bool xen_set_default_idle(void);
+#else
+#define xen_set_default_idle 0
+#endif
 
 void stop_this_cpu(void *dummy);
 

commit d288e1cf8e62f3e4034f1f021f047009c4ac0b3c
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu Dec 20 23:44:23 2012 -0800

    x86/common.c: Make have_cpuid_p() a global function
    
    Remove static declaration in have_cpuid_p() to make it a global function. The
    function will be called in early loading microcode.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1356075872-3054-4-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index bdee8bd318ea..3cdf4aa4d23f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -190,6 +190,14 @@ extern void init_amd_cacheinfo(struct cpuinfo_x86 *c);
 extern void detect_extended_topology(struct cpuinfo_x86 *c);
 extern void detect_ht(struct cpuinfo_x86 *c);
 
+#ifdef CONFIG_X86_32
+extern int have_cpuid_p(void);
+#else
+static inline int have_cpuid_p(void)
+{
+	return 1;
+}
+#endif
 static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
 				unsigned int *ecx, unsigned int *edx)
 {

commit 8170e6bed465b4b0c7687f93e9948aca4358a33b
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jan 24 12:19:52 2013 -0800

    x86, 64bit: Use a #PF handler to materialize early mappings on demand
    
    Linear mode (CR0.PG = 0) is mutually exclusive with 64-bit mode; all
    64-bit code has to use page tables.  This makes it awkward before we
    have first set up properly all-covering page tables to access objects
    that are outside the static kernel range.
    
    So far we have dealt with that simply by mapping a fixed amount of
    low memory, but that fails in at least two upcoming use cases:
    
    1. We will support load and run kernel, struct boot_params, ramdisk,
       command line, etc. above the 4 GiB mark.
    2. need to access ramdisk early to get microcode to update that as
       early possible.
    
    We could use early_iomap to access them too, but it will make code to
    messy and hard to be unified with 32 bit.
    
    Hence, set up a #PF table and use a fixed number of buffers to set up
    page tables on demand.  If the buffers fill up then we simply flush
    them and start over.  These buffers are all in __initdata, so it does
    not increase RAM usage at runtime.
    
    Thus, with the help of the #PF handler, we can set the final kernel
    mapping from blank, and switch to init_level4_pgt later.
    
    During the switchover in head_64.S, before #PF handler is available,
    we use three pages to handle kernel crossing 1G, 512G boundaries with
    sharing page by playing games with page aliasing: the same page is
    mapped twice in the higher-level tables with appropriate wraparound.
    The kernel region itself will be properly mapped; other mappings may
    be spurious.
    
    early_make_pgtable is using kernel high mapping address to access pages
    to set page table.
    
    -v4: Add phys_base offset to make kexec happy, and add
            init_mapping_kernel()   - Yinghai
    -v5: fix compiling with xen, and add back ident level3 and level2 for xen
         also move back init_level4_pgt from BSS to DATA again.
         because we have to clear it anyway.  - Yinghai
    -v6: switch to init_level4_pgt in init_mem_mapping. - Yinghai
    -v7: remove not needed clear_page for init_level4_page
         it is with fill 512,8,0 already in head_64.S  - Yinghai
    -v8: we need to keep that handler alive until init_mem_mapping and don't
         let early_trap_init to trash that early #PF handler.
         So split early_trap_pf_init out and move it down. - Yinghai
    -v9: switchover only cover kernel space instead of 1G so could avoid
         touch possible mem holes. - Yinghai
    -v11: change far jmp back to far return to initial_code, that is needed
         to fix failure that is reported by Konrad on AMD systems.  - Yinghai
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1359058816-7615-12-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 888184b2fc85..bdee8bd318ea 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -731,6 +731,7 @@ extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 
 extern void early_trap_init(void);
+void early_trap_pf_init(void);
 
 /* Defined in head.S */
 extern struct desc_ptr		early_gdt_descr;

commit 8b84c8df38d5796da2e8cd051666d203ddabcb62
Author: Daniel J Blueman <daniel@numascale-asia.com>
Date:   Tue Nov 27 14:32:10 2012 +0800

    x86, AMD, NB: Use u16 for northbridge IDs in amd_get_nb_id
    
    Change amd_get_nb_id to return u16 to support >255 memory controllers,
    and related consistency fixes.
    
    Signed-off-by: Daniel J Blueman <daniel@numascale-asia.com>
    Link: http://lkml.kernel.org/r/1353997932-8475-2-git-send-email-daniel@numascale-asia.com
    Signed-off-by: Borislav Petkov <bp@alien8.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 888184b2fc85..cf500543f6ff 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -943,7 +943,7 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
-extern int amd_get_nb_id(int cpu);
+extern u16 amd_get_nb_id(int cpu);
 
 struct aperfmperf {
 	u64 aperf, mperf;

commit 9977d9b379cb77e0f67bd6f4563618106e58e11d
Merge: cf4af0122157 541880d9a2c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 12:22:13 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull big execve/kernel_thread/fork unification series from Al Viro:
     "All architectures are converted to new model.  Quite a bit of that
      stuff is actually shared with architecture trees; in such cases it's
      literally shared branch pulled by both, not a cherry-pick.
    
      A lot of ugliness and black magic is gone (-3KLoC total in this one):
    
       - kernel_thread()/kernel_execve()/sys_execve() redesign.
    
         We don't do syscalls from kernel anymore for either kernel_thread()
         or kernel_execve():
    
         kernel_thread() is essentially clone(2) with callback run before we
         return to userland, the callbacks either never return or do
         successful do_execve() before returning.
    
         kernel_execve() is a wrapper for do_execve() - it doesn't need to
         do transition to user mode anymore.
    
         As a result kernel_thread() and kernel_execve() are
         arch-independent now - they live in kernel/fork.c and fs/exec.c
         resp.  sys_execve() is also in fs/exec.c and it's completely
         architecture-independent.
    
       - daemonize() is gone, along with its parts in fs/*.c
    
       - struct pt_regs * is no longer passed to do_fork/copy_process/
         copy_thread/do_execve/search_binary_handler/->load_binary/do_coredump.
    
       - sys_fork()/sys_vfork()/sys_clone() unified; some architectures
         still need wrappers (ones with callee-saved registers not saved in
         pt_regs on syscall entry), but the main part of those suckers is in
         kernel/fork.c now."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (113 commits)
      do_coredump(): get rid of pt_regs argument
      print_fatal_signal(): get rid of pt_regs argument
      ptrace_signal(): get rid of unused arguments
      get rid of ptrace_signal_deliver() arguments
      new helper: signal_pt_regs()
      unify default ptrace_signal_deliver
      flagday: kill pt_regs argument of do_fork()
      death to idle_regs()
      don't pass regs to copy_process()
      flagday: don't pass regs to copy_thread()
      bfin: switch to generic vfork, get rid of pointless wrappers
      xtensa: switch to generic clone()
      openrisc: switch to use of generic fork and clone
      unicore32: switch to generic clone(2)
      score: switch to generic fork/vfork/clone
      c6x: sanitize copy_thread(), get rid of clone(2) wrapper, switch to generic clone()
      take sys_fork/sys_vfork/sys_clone prototypes to linux/syscalls.h
      mn10300: switch to generic fork/vfork/clone
      h8300: switch to generic fork/vfork/clone
      tile: switch to generic clone()
      ...
    
    Conflicts:
            arch/microblaze/include/asm/Kbuild

commit 743aa456c1834f76982af44e8b71d1a0b2a82e21
Merge: a05a4e24dcd7 11af32b69ef7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 19:59:32 2012 -0800

    Merge branch 'x86-nuke386-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull "Nuke 386-DX/SX support" from Ingo Molnar:
     "This tree removes ancient-386-CPUs support and thus zaps quite a bit
      of complexity:
    
        24 files changed, 56 insertions(+), 425 deletions(-)
    
      ... which complexity has plagued us with extra work whenever we wanted
      to change SMP primitives, for years.
    
      Unfortunately there's a nostalgic cost: your old original 386 DX33
      system from early 1991 won't be able to boot modern Linux kernels
      anymore.  Sniff."
    
    I'm not sentimental.  Good riddance.
    
    * 'x86-nuke386-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, 386 removal: Document Nx586 as a 386 and thus unsupported
      x86, cleanups: Simplify sync_core() in the case of no CPUID
      x86, 386 removal: Remove CONFIG_X86_POPAD_OK
      x86, 386 removal: Remove CONFIG_X86_WP_WORKS_OK
      x86, 386 removal: Remove CONFIG_INVLPG
      x86, 386 removal: Remove CONFIG_BSWAP
      x86, 386 removal: Remove CONFIG_XADD
      x86, 386 removal: Remove CONFIG_CMPXCHG
      x86, 386 removal: Remove CONFIG_M386 from Kconfig

commit 45c39fb0cc20d24da08d5bb159f57d191098104d
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Nov 28 11:50:30 2012 -0800

    x86, cleanups: Simplify sync_core() in the case of no CPUID
    
    Simplify the implementation of sync_core() for the case where we may
    not have the CPUID instruction available.
    
    [ v2: stylistic cleanup of the #else clause per suggestion by Borislav
      Petkov. ]
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1354132230-21854-9-git-send-email-hpa@linux.intel.com
    Cc: Borislav Petkov <bp@alien8.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 9a4ee46959f3..b0d3e7362205 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -673,17 +673,28 @@ static inline void sync_core(void)
 	int tmp;
 
 #ifdef CONFIG_M486
-	if (boot_cpu_data.x86 < 5)
-		/* There is no speculative execution.
-		 * jmp is a barrier to prefetching. */
-		asm volatile("jmp 1f\n1:\n" ::: "memory");
-	else
+	/*
+	 * Do a CPUID if available, otherwise do a jump.  The jump
+	 * can conveniently enough be the jump around CPUID.
+	 */
+	asm volatile("cmpl %2,%1\n\t"
+		     "jl 1f\n\t"
+		     "cpuid\n"
+		     "1:"
+		     : "=a" (tmp)
+		     : "rm" (boot_cpu_data.cpuid_level), "ri" (0), "0" (1)
+		     : "ebx", "ecx", "edx", "memory");
+#else
+	/*
+	 * CPUID is a barrier to speculative execution.
+	 * Prefetched instructions are automatically
+	 * invalidated when modified.
+	 */
+	asm volatile("cpuid"
+		     : "=a" (tmp)
+		     : "0" (1)
+		     : "ebx", "ecx", "edx", "memory");
 #endif
-		/* cpuid is a barrier to speculative execution.
-		 * Prefetched instructions are automatically
-		 * invalidated when modified. */
-		asm volatile("cpuid" : "=a" (tmp) : "0" (1)
-			     : "ebx", "ecx", "edx", "memory");
 }
 
 static inline void __monitor(const void *eax, unsigned long ecx,

commit eb068e781020cf491333c773fb41820b57bfada4
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Nov 28 11:50:23 2012 -0800

    x86, 386 removal: Remove CONFIG_M386 from Kconfig
    
    Remove the CONFIG_M386 symbol from Kconfig so that it cannot be
    selected.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1354132230-21854-2-git-send-email-hpa@linux.intel.com

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ad1fc8511674..9a4ee46959f3 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -672,7 +672,7 @@ static inline void sync_core(void)
 {
 	int tmp;
 
-#if defined(CONFIG_M386) || defined(CONFIG_M486)
+#ifdef CONFIG_M486
 	if (boot_cpu_data.x86 < 5)
 		/* There is no speculative execution.
 		 * jmp is a barrier to prefetching. */

commit 18c26c27ae0abe82253cb2e2363df465dbbb657e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:53:20 2012 -0400

    death to idle_regs()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ad1fc8511674..92f48a5a6b2e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -178,8 +178,6 @@ static inline int hlt_works(int cpu)
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
 
-extern struct pt_regs *idle_regs(struct pt_regs *);
-
 extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);
 extern void identify_secondary_cpu(struct cpuinfo_x86 *);

commit 04a1541828ea223169eb44a336bfad8ec0dfb46a
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Fri Oct 19 10:59:33 2012 +0200

    x86, cacheinfo: Determine number of cache leafs using CPUID 0x8000001d on AMD
    
    CPUID 0x8000001d works quite similar to Intels' CPUID function 4.
    Use it to determine number of cache leafs.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/20121019085933.GE26718@alberich
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ad1fc8511674..db0d8c32090c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -187,7 +187,7 @@ extern void print_cpu_info(struct cpuinfo_x86 *);
 void print_cpu_msr(struct cpuinfo_x86 *);
 extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
 extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
-extern unsigned short num_cache_leaves;
+extern void init_amd_cacheinfo(struct cpuinfo_x86 *c);
 
 extern void detect_extended_topology(struct cpuinfo_x86 *c);
 extern void detect_ht(struct cpuinfo_x86 *c);

commit 42859eea96ba6beabfb0369a1eeffa3c7d2bd9cb
Merge: f59b51fe3d30 f322220d6159
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 10 12:02:25 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull generic execve() changes from Al Viro:
     "This introduces the generic kernel_thread() and kernel_execve()
      functions, and switches x86, arm, alpha, um and s390 over to them."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (26 commits)
      s390: convert to generic kernel_execve()
      s390: switch to generic kernel_thread()
      s390: fold kernel_thread_helper() into ret_from_fork()
      s390: fold execve_tail() into start_thread(), convert to generic sys_execve()
      um: switch to generic kernel_thread()
      x86, um/x86: switch to generic sys_execve and kernel_execve
      x86: split ret_from_fork
      alpha: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      alpha: switch to generic kernel_thread()
      alpha: switch to generic sys_execve()
      arm: get rid of execve wrapper, switch to generic execve() implementation
      arm: optimized current_pt_regs()
      arm: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      arm: split ret_from_fork, simplify kernel_thread() [based on patch by rmk]
      generic sys_execve()
      generic kernel_execve()
      new helper: current_pt_regs()
      preparation for generic kernel_thread()
      um: kill thread->forking
      um: let signal_delivered() do SIGTRAP on singlestepping into handler
      ...

commit da8347969f324db5f572581397d9b3a8e108cda4
Merge: 80749df4a149 c416ddf5b909
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:46:27 2012 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/asm changes from Ingo Molnar:
     "The one change that stands out is the alternatives patching change
      that prevents us from ever patching back instructions from SMP to UP:
      this simplifies things and speeds up CPU hotplug.
    
      Other than that it's smaller fixes, cleanups and improvements."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Unspaghettize do_trap()
      x86_64: Work around old GAS bug
      x86: Use REP BSF unconditionally
      x86: Prefer TZCNT over BFS
      x86/64: Adjust types of temporaries used by ffs()/fls()/fls64()
      x86: Drop unnecessary kernel_eflags variable on 64-bit
      x86/smp: Don't ever patch back to UP if we unplug cpus

commit 7076aada1040de4ed79a5977dbabdb5e5ea5e249
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Sep 10 16:44:54 2012 -0400

    x86: split ret_from_fork
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d048cad9bcad..078f3fdedf95 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -589,11 +589,6 @@ typedef struct {
 } mm_segment_t;
 
 
-/*
- * create a kernel thread without removing it from tasklists
- */
-extern int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
-
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);
 

commit 9bd1190a11c9d2c59d35cb999b8d170ad52aab5f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Sep 3 15:24:17 2012 +0200

    uprobes/x86: Do not (ab)use TIF_SINGLESTEP/user_*_single_step() for single-stepping
    
    user_enable/disable_single_step() was designed for ptrace, it assumes
    a single user and does unnecessary and wrong things for uprobes. For
    example:
    
            - arch_uprobe_enable_step() can't trust TIF_SINGLESTEP, an
              application itself can set X86_EFLAGS_TF which must be
              preserved after arch_uprobe_disable_step().
    
            - we do not want to set TIF_SINGLESTEP/TIF_FORCED_TF in
              arch_uprobe_enable_step(), this only makes sense for ptrace.
    
            - otoh we leak TIF_SINGLESTEP if arch_uprobe_disable_step()
              doesn't do user_disable_single_step(), the application will
              be killed after the next syscall.
    
            - arch_uprobe_enable_step() does access_process_vm() we do
              not need/want.
    
    Change arch_uprobe_enable/disable_step() to set/clear X86_EFLAGS_TF
    directly, this is much simpler and more correct. However, we need to
    clear TIF_BLOCKSTEP/DEBUGCTLMSR_BTF before executing the probed insn,
    add set_task_blockstep(false).
    
    Note: with or without this patch, there is another (hopefully minor)
    problem. A probed "pushf" insn can see the wrong X86_EFLAGS_TF set by
    uprobes. Perhaps we should change _disable to update the stack, or
    teach arch_uprobe_skip_sstep() to emulate this insn.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d048cad9bcad..433d2e5c98a7 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -759,6 +759,8 @@ static inline void update_debugctlmsr(unsigned long debugctlmsr)
 	wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);
 }
 
+extern void set_task_blockstep(struct task_struct *task, bool on);
+
 /*
  * from system description table in BIOS. Mostly for MCA use, but
  * others may find it useful:

commit 6eebdda35e6b18d0dddb2a44e34211bd94f0cad6
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Fri Aug 24 23:58:47 2012 +0400

    x86: Drop unnecessary kernel_eflags variable on 64-bit
    
    On 64 bit x86 we save the current eflags in cpu_init for use in
    ret_from_fork. Strictly speaking reserved bits in EFLAGS should
    be read as written but in practise it is unlikely that EFLAGS
    could ever be extended in this way and the kernel alread clears
    any undefined flags early on.
    
    The equivalent 32 bit code simply hard codes 0x0202 as the new
    EFLAGS.
    
    This change makes 64 bit use the same mechanism to setup the
    initial EFLAGS on fork. Note that 64 bit resets EFLAGS before
    calling schedule_tail() as opposed to 32 bit which calls
    schedule_tail() first. Therefore the correct value for EFLAGS
    has opposite IF bit.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/20120824195847.GA31628@moon
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d048cad9bcad..9738b39e4eb9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -423,7 +423,6 @@ DECLARE_INIT_PER_CPU(irq_stack_union);
 
 DECLARE_PER_CPU(char *, irq_stack_ptr);
 DECLARE_PER_CPU(unsigned int, irq_count);
-extern unsigned long kernel_eflags;
 extern asmlinkage void ignore_sysret(void);
 #else	/* X86_64 */
 #ifdef CONFIG_CC_STACKPROTECTOR

commit c4211f42d3e66875298a5e26a75109878c80f15b
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 28 09:02:19 2012 +0800

    x86/tlb: add tlb_flushall_shift for specific CPU
    
    Testing show different CPU type(micro architectures and NUMA mode) has
    different balance points between the TLB flush all and multiple invlpg.
    And there also has cases the tlb flush change has no any help.
    
    This patch give a interface to let x86 vendor developers have a chance
    to set different shift for different CPU type.
    
    like some machine in my hands, balance points is 16 entries on
    Romely-EP; while it is at 8 entries on Bloomfield NHM-EP; and is 256 on
    IVB mobile CPU. but on model 15 core2 Xeon using invlpg has nothing
    help.
    
    For untested machine, do a conservative optimization, same as NHM CPU.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Link: http://lkml.kernel.org/r/1340845344-27557-5-git-send-email-alex.shi@intel.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 39b2bd48dfbc..d048cad9bcad 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -72,6 +72,8 @@ extern u16 __read_mostly tlb_lli_4m[NR_INFO];
 extern u16 __read_mostly tlb_lld_4k[NR_INFO];
 extern u16 __read_mostly tlb_lld_2m[NR_INFO];
 extern u16 __read_mostly tlb_lld_4m[NR_INFO];
+extern s8  __read_mostly tlb_flushall_shift;
+
 /*
  *  CPU type and hardware bug flags. Kept separately for each CPU.
  *  Members of this structure are referenced in head.S, so think twice

commit e0ba94f14f747c2661c4d21f8c44e5b0b8cd8e48
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 28 09:02:16 2012 +0800

    x86/tlb_info: get last level TLB entry number of CPU
    
    For 4KB pages, x86 CPU has 2 or 1 level TLB, first level is data TLB and
    instruction TLB, second level is shared TLB for both data and instructions.
    
    For hupe page TLB, usually there is just one level and seperated by 2MB/4MB
    and 1GB.
    
    Although each levels TLB size is important for performance tuning, but for
    genernal and rude optimizing, last level TLB entry number is suitable. And
    in fact, last level TLB always has the biggest entry number.
    
    This patch will get the biggest TLB entry number and use it in furture TLB
    optimizing.
    
    Accroding Borislav's suggestion, except tlb_ll[i/d]_* array, other
    function and data will be released after system boot up.
    
    For all kinds of x86 vendor friendly, vendor specific code was moved to its
    specific files.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Link: http://lkml.kernel.org/r/1340845344-27557-2-git-send-email-alex.shi@intel.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 39bc5777211a..39b2bd48dfbc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -61,6 +61,17 @@ static inline void *current_text_addr(void)
 # define ARCH_MIN_MMSTRUCT_ALIGN	0
 #endif
 
+enum tlb_infos {
+	ENTRIES,
+	NR_INFO
+};
+
+extern u16 __read_mostly tlb_lli_4k[NR_INFO];
+extern u16 __read_mostly tlb_lli_2m[NR_INFO];
+extern u16 __read_mostly tlb_lli_4m[NR_INFO];
+extern u16 __read_mostly tlb_lld_4k[NR_INFO];
+extern u16 __read_mostly tlb_lld_2m[NR_INFO];
+extern u16 __read_mostly tlb_lld_4m[NR_INFO];
 /*
  *  CPU type and hardware bug flags. Kept separately for each CPU.
  *  Members of this structure are referenced in head.S, so think twice

commit 731a7378b81c2f5fa88ca1ae20b83d548d5613dc
Merge: 87a5af24e548 61f544616904
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 29 20:14:53 2012 -0700

    Merge branch 'x86-trampoline-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 trampoline rework from H. Peter Anvin:
     "This code reworks all the "trampoline"/"realmode" code (various bits
      that need to live in the first megabyte of memory, most but not all of
      which runs in real mode at some point) in the kernel into a single
      object.  The main reason for doing this is that it eliminates the last
      place in the kernel where we needed pages to be mapped RWX.  This code
      separates all that code into proper R/RW/RX pages."
    
    Fix up conflicts in arch/x86/kernel/Makefile (mca removed next to reboot
    code), and arch/x86/kernel/reboot.c (reboot code moved around in one
    branch, modified in this one), and arch/x86/tools/relocs.c (mostly same
    code came in earlier due to working around the ld bugs just before the
    3.4 release).
    
    Also remove stale x86-relocs entry from scripts/.gitignore as per Peter
    Anvin.
    
    * commit '61f5446169046c217a5479517edac3a890c3bee7': (36 commits)
      x86, realmode: Move end signature into header.S
      x86, relocs: When printing an error, say relative or absolute
      x86, relocs: More relocations which may end up as absolute
      x86, relocs: Workaround for binutils 2.22.52.0.1 section bug
      xen-acpi-processor: Add missing #include <xen/xen.h>
      acpi, bgrd: Add missing <linux/io.h> to drivers/acpi/bgrt.c
      x86, realmode: Change EFER to a single u64 field
      x86, realmode: Move kernel/realmode.c to realmode/init.c
      x86, realmode: Move not-common bits out of trampoline_common.S
      x86, realmode: Mask out EFER.LMA when saving trampoline EFER
      x86, realmode: Fix no cache bits test in reboot_32.S
      x86, realmode: Make sure all generated files are listed in targets
      x86, realmode: build fix: remove duplicate build
      x86, realmode: read cr4 and EFER from kernel for 64-bit trampoline
      x86, realmode: fixes compilation issue in tboot.c
      x86, realmode: move relocs from scripts/ to arch/x86/tools
      x86, realmode: header for trampoline code
      x86, realmode: flattened rm hierachy
      x86, realmode: don't copy real_mode_header
      x86, realmode: fix 64-bit wakeup sequence
      ...

commit ec0d7f18ab7b5097d7c0c8f3d909ca1031b9d5cd
Merge: 269af9a1a08d 1dcc8d7ba235
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 10:59:07 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull fpu state cleanups from Ingo Molnar:
     "This tree streamlines further aspects of FPU handling by eliminating
      the prepare_to_copy() complication and moving that logic to
      arch_dup_task_struct().
    
      It also fixes the FPU dumps in threaded core dumps, removes and old
      (and now invalid) assumption plus micro-optimizes the exit path by
      avoiding an FPU save for dead tasks."
    
    Fixed up trivial add-add conflict in arch/sh/kernel/process.c that came
    in because we now do the FPU handling in arch_dup_task_struct() rather
    than the legacy (and now gone) prepare_to_copy().
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: drop the fpu state during thread exit
      x86, xsave: remove thread_has_fpu() bug check in __sanitize_i387_state()
      coredump: ensure the fpu state is flushed for proper multi-threaded core dump
      fork: move the real prepare_to_copy() users to arch_dup_task_struct()

commit 55ccf3fe3f9a3441731aa79cf42a628fc4ecace9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed May 16 15:03:51 2012 -0700

    fork: move the real prepare_to_copy() users to arch_dup_task_struct()
    
    Historical prepare_to_copy() is mostly a no-op, duplicated for majority of
    the architectures and the rest following the x86 model of flushing the extended
    register state like fpu there.
    
    Remove it and use the arch_dup_task_struct() instead.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1336692811-30576-1-git-send-email-suresh.b.siddha@intel.com
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 4fa7dcceb6c0..97fe04318e95 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -579,9 +579,6 @@ extern int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);
 
-/* Prepare to copy thread state - unlazy all lazy state */
-extern void prepare_to_copy(struct task_struct *tsk);
-
 unsigned long get_wchan(struct task_struct *p);
 
 /*

commit cda846f101fb1396b6924f1d9b68ac3d42de5403
Author: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
Date:   Tue May 8 21:22:46 2012 +0300

    x86, realmode: read cr4 and EFER from kernel for 64-bit trampoline
    
    This patch changes 64-bit trampoline so that CR4 and
    EFER are provided by the kernel instead of using fixed
    values.
    
    Signed-off-by: Jarkko Sakkinen <jarkko.sakkinen@intel.com>
    Link: http://lkml.kernel.org/r/1336501366-28617-24-git-send-email-jarkko.sakkinen@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 4fa7dcceb6c0..404583ccf0cf 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -544,13 +544,16 @@ static inline void load_sp0(struct tss_struct *tss,
  * enable), so that any CPU's that boot up
  * after us can get the correct flags.
  */
-extern unsigned long		mmu_cr4_features;
+extern unsigned long mmu_cr4_features;
+extern u32 *trampoline_cr4_features;
 
 static inline void set_in_cr4(unsigned long mask)
 {
 	unsigned long cr4;
 
 	mmu_cr4_features |= mask;
+	if (trampoline_cr4_features)
+		*trampoline_cr4_features = mmu_cr4_features;
 	cr4 = read_cr4();
 	cr4 |= mask;
 	write_cr4(cr4);
@@ -561,6 +564,8 @@ static inline void clear_in_cr4(unsigned long mask)
 	unsigned long cr4;
 
 	mmu_cr4_features &= ~mask;
+	if (trampoline_cr4_features)
+		*trampoline_cr4_features = mmu_cr4_features;
 	cr4 = read_cr4();
 	cr4 &= ~mask;
 	write_cr4(cr4);

commit 85f7f656274fa0ba109dd8774db3887d42de5c6b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 7 17:59:49 2012 +0000

    x86: Use kick_all_cpus_sync()
    
    Use kick_all_cpus_sync() and remove cpu_idle_wait().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120507175652.190382227@linutronix.de
    Cc: x86@kernel.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 4fa7dcceb6c0..ccbb1ea99ccb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -974,8 +974,6 @@ extern bool cpu_has_amd_erratum(const int *);
 #define cpu_has_amd_erratum(x)	(false)
 #endif /* CONFIG_CPU_SUP_AMD */
 
-void cpu_idle_wait(void);
-
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 

commit f6365201d8a21fb347260f89d6e9b3e718d63c70
Author: Len Brown <len.brown@intel.com>
Date:   Thu Mar 29 14:49:17 2012 -0700

    x86: Remove the ancient and deprecated disable_hlt() and enable_hlt() facility
    
    The X86_32-only disable_hlt/enable_hlt mechanism was used by the
    32-bit floppy driver. Its effect was to replace the use of the
    HLT instruction inside default_idle() with cpu_relax() - essentially
    it turned off the use of HLT.
    
    This workaround was commented in the code as:
    
     "disable hlt during certain critical i/o operations"
    
     "This halt magic was a workaround for ancient floppy DMA
      wreckage. It should be safe to remove."
    
    H. Peter Anvin additionally adds:
    
     "To the best of my knowledge, no-hlt only existed because of
      flaky power distributions on 386/486 systems which were sold to
      run DOS.  Since DOS did no power management of any kind,
      including HLT, the power draw was fairly uniform; when exposed
      to the much hhigher noise levels you got when Linux used HLT
      caused some of these systems to fail.
    
      They were by far in the minority even back then."
    
    Alan Cox further says:
    
     "Also for the Cyrix 5510 which tended to go castors up if a HLT
      occurred during a DMA cycle and on a few other boxes HLT during
      DMA tended to go astray.
    
      Do we care ? I doubt it. The 5510 was pretty obscure, the 5520
      fixed it, the 5530 is probably the oldest still in any kind of
      use."
    
    So, let's finally drop this.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Josh Boyer <jwboyer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Stephen Hemminger <shemminger@vyatta.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: <stable@kernel.org>
    Link: http://lkml.kernel.org/n/tip-3rhk9bzf0x9rljkv488tloib@git.kernel.org
    [ If anyone cares then alternative instruction patching could be
      used to replace HLT with a one-byte NOP instruction. Much simpler. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7284c9a6a0b5..4fa7dcceb6c0 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -974,16 +974,6 @@ extern bool cpu_has_amd_erratum(const int *);
 #define cpu_has_amd_erratum(x)	(false)
 #endif /* CONFIG_CPU_SUP_AMD */
 
-#ifdef CONFIG_X86_32
-/*
- * disable hlt during certain critical i/o operations
- */
-#define HAVE_DISABLE_HLT
-#endif
-
-void disable_hlt(void);
-void enable_hlt(void);
-
 void cpu_idle_wait(void);
 
 extern unsigned long arch_align_stack(unsigned long sp);

commit eb05df9e7e793f3134dbb574c7ccc05f7932bc59
Merge: a591afc01d9e ef334a20d84f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:21:35 2012 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Peter Anvin:
     "The biggest textual change is the cleanup to use symbolic constants
      for x86 trap values.
    
      The only *functional* change and the reason for the x86/x32 dependency
      is the move of is_ia32_task() into <asm/thread_info.h> so that it can
      be used in other code that needs to understand if a system call comes
      from the compat entry point (and therefore uses i386 system call
      numbers) or not.  One intended user for that is the BPF system call
      filter.  Moving it out of <asm/compat.h> means we can define it
      unconditionally, returning always true on i386."
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Move is_ia32_task to asm/thread_info.h from asm/compat.h
      x86: Rename trap_no to trap_nr in thread_struct
      x86: Use enum instead of literals for trap values

commit a591afc01d9e48affbacb365558a31e53c85af45
Merge: 820d41cf0cd0 31796ac4e8f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:12:23 2012 -0700

    Merge branch 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x32 support for x86-64 from Ingo Molnar:
     "This tree introduces the X32 binary format and execution mode for x86:
      32-bit data space binaries using 64-bit instructions and 64-bit kernel
      syscalls.
    
      This allows applications whose working set fits into a 32 bits address
      space to make use of 64-bit instructions while using a 32-bit address
      space with shorter pointers, more compressed data structures, etc."
    
    Fix up trivial context conflicts in arch/x86/{Kconfig,vdso/vma.c}
    
    * 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (71 commits)
      x32: Fix alignment fail in struct compat_siginfo
      x32: Fix stupid ia32/x32 inversion in the siginfo format
      x32: Add ptrace for x32
      x32: Switch to a 64-bit clock_t
      x32: Provide separate is_ia32_task() and is_x32_task() predicates
      x86, mtrr: Use explicit sizing and padding for the 64-bit ioctls
      x86/x32: Fix the binutils auto-detect
      x32: Warn and disable rather than error if binutils too old
      x32: Only clear TIF_X32 flag once
      x32: Make sure TS_COMPAT is cleared for x32 tasks
      fs: Remove missed ->fds_bits from cessation use of fd_set structs internally
      fs: Fix close_on_exec pointer in alloc_fdtable
      x32: Drop non-__vdso weak symbols from the x32 VDSO
      x32: Fix coding style violations in the x32 VDSO code
      x32: Add x32 VDSO support
      x32: Allow x32 to be configured
      x32: If configured, add x32 system calls to system call tables
      x32: Handle process creation
      x32: Signal-related system calls
      x86: Add #ifdef CONFIG_COMPAT to <asm/sys_ia32.h>
      ...

commit 0195c00244dc2e9f522475868fa278c473ba7339
Merge: f21ce8f8447c 141124c02059
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 15:58:21 2012 -0700

    Merge tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system
    
    Pull "Disintegrate and delete asm/system.h" from David Howells:
     "Here are a bunch of patches to disintegrate asm/system.h into a set of
      separate bits to relieve the problem of circular inclusion
      dependencies.
    
      I've built all the working defconfigs from all the arches that I can
      and made sure that they don't break.
    
      The reason for these patches is that I recently encountered a circular
      dependency problem that came about when I produced some patches to
      optimise get_order() by rewriting it to use ilog2().
    
      This uses bitops - and on the SH arch asm/bitops.h drags in
      asm-generic/get_order.h by a circuituous route involving asm/system.h.
    
      The main difficulty seems to be asm/system.h.  It holds a number of
      low level bits with no/few dependencies that are commonly used (eg.
      memory barriers) and a number of bits with more dependencies that
      aren't used in many places (eg.  switch_to()).
    
      These patches break asm/system.h up into the following core pieces:
    
        (1) asm/barrier.h
    
            Move memory barriers here.  This already done for MIPS and Alpha.
    
        (2) asm/switch_to.h
    
            Move switch_to() and related stuff here.
    
        (3) asm/exec.h
    
            Move arch_align_stack() here.  Other process execution related bits
            could perhaps go here from asm/processor.h.
    
        (4) asm/cmpxchg.h
    
            Move xchg() and cmpxchg() here as they're full word atomic ops and
            frequently used by atomic_xchg() and atomic_cmpxchg().
    
        (5) asm/bug.h
    
            Move die() and related bits.
    
        (6) asm/auxvec.h
    
            Move AT_VECTOR_SIZE_ARCH here.
    
      Other arch headers are created as needed on a per-arch basis."
    
    Fixed up some conflicts from other header file cleanups and moving code
    around that has happened in the meantime, so David's testing is somewhat
    weakened by that.  We'll find out anything that got broken and fix it..
    
    * tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system: (38 commits)
      Delete all instances of asm/system.h
      Remove all #inclusions of asm/system.h
      Add #includes needed to permit the removal of asm/system.h
      Move all declarations of free_initmem() to linux/mm.h
      Disintegrate asm/system.h for OpenRISC
      Split arch_align_stack() out from asm-generic/system.h
      Split the switch_to() wrapper out of asm-generic/system.h
      Move the asm-generic/system.h xchg() implementation to asm-generic/cmpxchg.h
      Create asm-generic/barrier.h
      Make asm-generic/cmpxchg.h #include asm-generic/cmpxchg-local.h
      Disintegrate asm/system.h for Xtensa
      Disintegrate asm/system.h for Unicore32 [based on ver #3, changed by gxt]
      Disintegrate asm/system.h for Tile
      Disintegrate asm/system.h for Sparc
      Disintegrate asm/system.h for SH
      Disintegrate asm/system.h for Score
      Disintegrate asm/system.h for S390
      Disintegrate asm/system.h for PowerPC
      Disintegrate asm/system.h for PA-RISC
      Disintegrate asm/system.h for MN10300
      ...

commit f05e798ad4c09255f590f5b2c00a7ca6c172f983
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:11:12 2012 +0100

    Disintegrate asm/system.h for X86
    
    Disintegrate asm/system.h for X86.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    cc: x86@kernel.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 95da14f7ee85..78e30ea492b2 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -14,13 +14,13 @@ struct mm_struct;
 #include <asm/sigcontext.h>
 #include <asm/current.h>
 #include <asm/cpufeature.h>
-#include <asm/system.h>
 #include <asm/page.h>
 #include <asm/pgtable_types.h>
 #include <asm/percpu.h>
 #include <asm/msr.h>
 #include <asm/desc_defs.h>
 #include <asm/nops.h>
+#include <asm/special_insns.h>
 
 #include <linux/personality.h>
 #include <linux/cpumask.h>
@@ -29,6 +29,15 @@ struct mm_struct;
 #include <linux/math64.h>
 #include <linux/init.h>
 #include <linux/err.h>
+#include <linux/irqflags.h>
+
+/*
+ * We handle most unaligned accesses in hardware.  On the other hand
+ * unaligned DMA can be quite expensive on some Nehalem processors.
+ *
+ * Based on this we disable the IP header alignment in network drivers.
+ */
+#define NET_IP_ALIGN	0
 
 #define HBP_NUM 4
 /*
@@ -1022,4 +1031,24 @@ extern bool cpu_has_amd_erratum(const int *);
 #define cpu_has_amd_erratum(x)	(false)
 #endif /* CONFIG_CPU_SUP_AMD */
 
+#ifdef CONFIG_X86_32
+/*
+ * disable hlt during certain critical i/o operations
+ */
+#define HAVE_DISABLE_HLT
+#endif
+
+void disable_hlt(void);
+void enable_hlt(void);
+
+void cpu_idle_wait(void);
+
+extern unsigned long arch_align_stack(unsigned long sp);
+extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
+
+void default_idle(void);
+bool set_pm_idle_to_default(void);
+
+void stop_this_cpu(void *dummy);
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit ed2d265d1266736bd294332d7f649003943ae36e
Merge: f1d38e423a69 6c03438edeb5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 24 10:08:39 2012 -0700

    Merge tag 'bug-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    Pull <linux/bug.h> cleanup from Paul Gortmaker:
     "The changes shown here are to unify linux's BUG support under the one
      <linux/bug.h> file.  Due to historical reasons, we have some BUG code
      in bug.h and some in kernel.h -- i.e.  the support for BUILD_BUG in
      linux/kernel.h predates the addition of linux/bug.h, but old code in
      kernel.h wasn't moved to bug.h at that time.  As a band-aid, kernel.h
      was including <asm/bug.h> to pseudo link them.
    
      This has caused confusion[1] and general yuck/WTF[2] reactions.  Here
      is an example that violates the principle of least surprise:
    
          CC      lib/string.o
          lib/string.c: In function 'strlcat':
          lib/string.c:225:2: error: implicit declaration of function 'BUILD_BUG_ON'
          make[2]: *** [lib/string.o] Error 1
          $
          $ grep linux/bug.h lib/string.c
          #include <linux/bug.h>
          $
    
      We've included <linux/bug.h> for the BUG infrastructure and yet we
      still get a compile fail! [We've not kernel.h for BUILD_BUG_ON.] Ugh -
      very confusing for someone who is new to kernel development.
    
      With the above in mind, the goals of this changeset are:
    
      1) find and fix any include/*.h files that were relying on the
         implicit presence of BUG code.
      2) find and fix any C files that were consuming kernel.h and hence
         relying on implicitly getting some/all BUG code.
      3) Move the BUG related code living in kernel.h to <linux/bug.h>
      4) remove the asm/bug.h from kernel.h to finally break the chain.
    
      During development, the order was more like 3-4, build-test, 1-2.  But
      to ensure that git history for bisect doesn't get needless build
      failures introduced, the commits have been reorderd to fix the problem
      areas in advance.
    
            [1]  https://lkml.org/lkml/2012/1/3/90
            [2]  https://lkml.org/lkml/2012/1/17/414"
    
    Fix up conflicts (new radeon file, reiserfs header cleanups) as per Paul
    and linux-next.
    
    * tag 'bug-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux:
      kernel.h: doesn't explicitly use bug.h, so don't include it.
      bug: consolidate BUILD_BUG_ON with other bug code
      BUG: headers with BUG/BUG_ON etc. need linux/bug.h
      bug.h: add include of it to various implicit C users
      lib: fix implicit users of kernel.h for TAINT_WARN
      spinlock: macroize assert_spin_locked to avoid bug.h dependency
      x86: relocate get/set debugreg fcns to include/asm/debugreg.

commit 4c64616bb51b399886ded8f4f69bad4da2da1817
Merge: c5c7fb8fbd7c 943bc7e110f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:30:39 2012 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/debug changes from Ingo Molnar.
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Fix section warnings
      x86-64: Fix CFI data for common_interrupt()
      x86: Properly _init-annotate NMI selftest code
      x86/debug: Fix/improve the show_msr=<cpus> debug print out

commit 51e7dc7011c99e1e5294658c7b551b92ca069985
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Mon Mar 12 14:55:55 2012 +0530

    x86: Rename trap_no to trap_nr in thread_struct
    
    There are precedences of trap number being referred to as
    trap_nr. However thread struct refers trap number as trap_no.
    Change it to trap_nr.
    
    Also use enum instead of left-over literals for trap values.
    
    This is pure cleanup, no functional change intended.
    
    Suggested-by: Ingo Molnar <mingo@eltu.hu>
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120312092555.5379.942.sendpatchset@srdronam.in.ibm.com
    [ Fixed the math-emu build ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 02ce0b379647..f6d0d2eb0832 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -453,7 +453,7 @@ struct thread_struct {
 	unsigned long           ptrace_dr7;
 	/* Fault info: */
 	unsigned long		cr2;
-	unsigned long		trap_no;
+	unsigned long		trap_nr;
 	unsigned long		error_code;
 	/* floating point and extended processor state */
 	struct fpu		fpu;

commit e898c6706869fdcbd68b1e7fb0ac7461d98710fe
Merge: c94082656dac 55283e253771
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 13 05:54:39 2012 +0100

    Merge branch 'x86/x32' into x86/cleanups
    
    Merge reason: We are going to merge a dependent patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f649e9388cd46ad1634164e56f96ae092ca59e4a
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jan 20 16:24:09 2012 -0500

    x86: relocate get/set debugreg fcns to include/asm/debugreg.
    
    Since we already have a debugreg.h header file, move the
    assoc. get/set functions to it.  In addition to it being the
    logical home for them, it has a secondary advantage.  The
    functions that are moved use BUG().  So we really need to
    have linux/bug.h in scope.  But asm/processor.h is used about
    600 times, vs. only about 15 for debugreg.h -- so adding bug.h
    to the latter reduces the amount of time we'll be processing
    it during a compile.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 58545c97d071..30aa6e95f814 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -474,61 +474,6 @@ struct thread_struct {
 	unsigned		io_bitmap_max;
 };
 
-static inline unsigned long native_get_debugreg(int regno)
-{
-	unsigned long val = 0;	/* Damn you, gcc! */
-
-	switch (regno) {
-	case 0:
-		asm("mov %%db0, %0" :"=r" (val));
-		break;
-	case 1:
-		asm("mov %%db1, %0" :"=r" (val));
-		break;
-	case 2:
-		asm("mov %%db2, %0" :"=r" (val));
-		break;
-	case 3:
-		asm("mov %%db3, %0" :"=r" (val));
-		break;
-	case 6:
-		asm("mov %%db6, %0" :"=r" (val));
-		break;
-	case 7:
-		asm("mov %%db7, %0" :"=r" (val));
-		break;
-	default:
-		BUG();
-	}
-	return val;
-}
-
-static inline void native_set_debugreg(int regno, unsigned long value)
-{
-	switch (regno) {
-	case 0:
-		asm("mov %0, %%db0"	::"r" (value));
-		break;
-	case 1:
-		asm("mov %0, %%db1"	::"r" (value));
-		break;
-	case 2:
-		asm("mov %0, %%db2"	::"r" (value));
-		break;
-	case 3:
-		asm("mov %0, %%db3"	::"r" (value));
-		break;
-	case 6:
-		asm("mov %0, %%db6"	::"r" (value));
-		break;
-	case 7:
-		asm("mov %0, %%db7"	::"r" (value));
-		break;
-	default:
-		BUG();
-	}
-}
-
 /*
  * Set IOPL bits in EFLAGS from given mask
  */
@@ -574,14 +519,6 @@ static inline void native_swapgs(void)
 #define __cpuid			native_cpuid
 #define paravirt_enabled()	0
 
-/*
- * These special macros can be used to get or set a debugging register
- */
-#define get_debugreg(var, register)				\
-	(var) = native_get_debugreg(register)
-#define set_debugreg(value, register)				\
-	native_set_debugreg(register, value)
-
 static inline void load_sp0(struct tss_struct *tss,
 			    struct thread_struct *thread)
 {

commit d046ff8b30319d9aa38d877a0ba4206771e54346
Author: H. J. Lu <hjl.tools@gmail.com>
Date:   Tue Feb 14 13:49:48 2012 -0800

    x86-64: Add prototype for old_rsp to a header file
    
    So far this has only been used in process_64.c, but the x32 code will
    need it in additional code.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 9f748b5fb701..e34f95129f16 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -948,6 +948,12 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 
 #define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.sp0 - 1)
 extern unsigned long KSTK_ESP(struct task_struct *task);
+
+/*
+ * User space RSP while inside the SYSCALL fast path
+ */
+DECLARE_PER_CPU(unsigned long, old_rsp);
+
 #endif /* CONFIG_X86_64 */
 
 extern void start_thread(struct pt_regs *regs, unsigned long new_ip,

commit 6bd330083e0e97b7ddc053459190bf3d5768ca83
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Feb 6 13:03:09 2012 -0800

    x86: Factor out TIF_IA32 from 32-bit address space
    
    Factor out IA32 (compatibility instruction set) from 32-bit address
    space in the thread_info flags; this is a precondition patch for x32
    support.
    
    Originally-by: H. J. Lu <hjl.tools@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/n/tip-4pr1xnnksprt7t0h3w5fw4rv@git.kernel.org

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index aa9088c26931..9f748b5fb701 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -924,9 +924,9 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 #define IA32_PAGE_OFFSET	((current->personality & ADDR_LIMIT_3GB) ? \
 					0xc0000000 : 0xFFFFe000)
 
-#define TASK_SIZE		(test_thread_flag(TIF_IA32) ? \
+#define TASK_SIZE		(test_thread_flag(TIF_ADDR32) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
-#define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_IA32)) ? \
+#define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
 #define STACK_TOP		TASK_SIZE

commit 7e16838d94b566a17b65231073d179bc04d590c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 19 13:27:00 2012 -0800

    i387: support lazy restore of FPU state
    
    This makes us recognize when we try to restore FPU state that matches
    what we already have in the FPU on this CPU, and avoids the restore
    entirely if so.
    
    To do this, we add two new data fields:
    
     - a percpu 'fpu_owner_task' variable that gets written any time we
       update the "has_fpu" field, and thus acts as a kind of back-pointer
       to the task that owns the CPU.  The exception is when we save the FPU
       state as part of a context switch - if the save can keep the FPU
       state around, we leave the 'fpu_owner_task' variable pointing at the
       task whose FP state still remains on the CPU.
    
     - a per-thread 'last_cpu' field, that indicates which CPU that thread
       used its FPU on last.  We update this on every context switch
       (writing an invalid CPU number if the last context switch didn't
       leave the FPU in a lazily usable state), so we know that *that*
       thread has done nothing else with the FPU since.
    
    These two fields together can be used when next switching back to the
    task to see if the CPU still matches: if 'fpu_owner_task' matches the
    task we are switching to, we know that no other task (or kernel FPU
    usage) touched the FPU on this CPU in the meantime, and if the current
    CPU number matches the 'last_cpu' field, we know that this thread did no
    other FP work on any other CPU, so the FPU state on the CPU must match
    what was saved on last context switch.
    
    In that case, we can avoid the 'f[x]rstor' entirely, and just clear the
    CR0.TS bit.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f7c89e231c6c..58545c97d071 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -374,6 +374,8 @@ union thread_xstate {
 };
 
 struct fpu {
+	unsigned int last_cpu;
+	unsigned int has_fpu;
 	union thread_xstate *state;
 };
 
@@ -454,7 +456,6 @@ struct thread_struct {
 	unsigned long		trap_no;
 	unsigned long		error_code;
 	/* floating point and extended processor state */
-	unsigned long		has_fpu;
 	struct fpu		fpu;
 #ifdef CONFIG_X86_32
 	/* Virtual 86 mode info */

commit f94edacf998516ac9d849f7bc6949a703977a7f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 17 21:48:54 2012 -0800

    i387: move TS_USEDFPU flag from thread_info to task_struct
    
    This moves the bit that indicates whether a thread has ownership of the
    FPU from the TS_USEDFPU bit in thread_info->status to a word of its own
    (called 'has_fpu') in task_struct->thread.has_fpu.
    
    This fixes two independent bugs at the same time:
    
     - changing 'thread_info->status' from the scheduler causes nasty
       problems for the other users of that variable, since it is defined to
       be thread-synchronous (that's what the "TS_" part of the naming was
       supposed to indicate).
    
       So perfectly valid code could (and did) do
    
            ti->status |= TS_RESTORE_SIGMASK;
    
       and the compiler was free to do that as separate load, or and store
       instructions.  Which can cause problems with preemption, since a task
       switch could happen in between, and change the TS_USEDFPU bit. The
       change to TS_USEDFPU would be overwritten by the final store.
    
       In practice, this seldom happened, though, because the 'status' field
       was seldom used more than once, so gcc would generally tend to
       generate code that used a read-modify-write instruction and thus
       happened to avoid this problem - RMW instructions are naturally low
       fat and preemption-safe.
    
     - On x86-32, the current_thread_info() pointer would, during interrupts
       and softirqs, point to a *copy* of the real thread_info, because
       x86-32 uses %esp to calculate the thread_info address, and thus the
       separate irq (and softirq) stacks would cause these kinds of odd
       thread_info copy aliases.
    
       This is normally not a problem, since interrupts aren't supposed to
       look at thread information anyway (what thread is running at
       interrupt time really isn't very well-defined), but it confused the
       heck out of irq_fpu_usable() and the code that tried to squirrel
       away the FPU state.
    
       (It also caused untold confusion for us poor kernel developers).
    
    It also turns out that using 'task_struct' is actually much more natural
    for most of the call sites that care about the FPU state, since they
    tend to work with the task struct for other reasons anyway (ie
    scheduling).  And the FPU data that we are going to save/restore is
    found there too.
    
    Thanks to Arjan Van De Ven <arjan@linux.intel.com> for pointing us to
    the %esp issue.
    
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Reported-and-tested-by: Raphael Prevost <raphael@buro.asia>
    Acked-and-tested-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Tested-by: Peter Anvin <hpa@zytor.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index aa9088c26931..f7c89e231c6c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -454,6 +454,7 @@ struct thread_struct {
 	unsigned long		trap_no;
 	unsigned long		error_code;
 	/* floating point and extended processor state */
+	unsigned long		has_fpu;
 	struct fpu		fpu;
 #ifdef CONFIG_X86_32
 	/* Virtual 86 mode info */

commit 21c3fcf3e39353d4f21d50e257cc74f3204b1988
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sun Feb 12 09:53:57 2012 -0800

    x86/debug: Fix/improve the show_msr=<cpus> debug print out
    
    Found out that show_msr=<cpus> is broken, when I asked a
    user to use it to capture debug info about broken MTRR's
    whose MTRR settings are probably different between CPUs.
    
    Only the first CPUs MSRs are printed, but that is not
    enough to track down the suspected bug.
    
    For years we called print_cpu_msr from print_cpu_info(),
    but this commit:
    
    | commit 2eaad1fddd7450a48ad464229775f97fbfe8af36
    | Author: Mike Travis <travis@sgi.com>
    | Date:   Thu Dec 10 17:19:36 2009 -0800
    |
    |    x86: Limit the number of processor bootup messages
    
    removed the print_cpu_info() call from all APs.
    
    Put it back - it will only print MSRs when the user
    specifically requests them via show_msr=<cpus>.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Mike Travis <travis@sgi.com>
    Link: http://lkml.kernel.org/r/1329069237-11483-1-git-send-email-yinghai@kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index aa9088c26931..8bb062bbcbec 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -162,6 +162,7 @@ extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);
 extern void identify_secondary_cpu(struct cpuinfo_x86 *);
 extern void print_cpu_info(struct cpuinfo_x86 *);
+void print_cpu_msr(struct cpuinfo_x86 *);
 extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
 extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
 extern unsigned short num_cache_leaves;

commit 141168c36cdee3ff23d9c7700b0edc47cb65479f
Author: Kevin Winchester <kjwinchester@gmail.com>
Date:   Tue Dec 20 20:52:22 2011 -0400

    x86: Simplify code by removing a !SMP #ifdefs from 'struct cpuinfo_x86'
    
    Several fields in struct cpuinfo_x86 were not defined for the
    !SMP case, likely to save space.  However, those fields still
    have some meaning for UP, and keeping them allows some #ifdef
    removal from other files.  The additional size of the UP kernel
    from this change is not significant enough to worry about
    keeping up the distinction:
    
               text    data     bss     dec     hex filename
            4737168  506459  972040 6215667  5ed7f3 vmlinux.o.before
            4737444  506459  972040 6215943  5ed907 vmlinux.o.after
    
    for a difference of 276 bytes for an example UP config.
    
    If someone wants those 276 bytes back badly then it should
    be implemented in a cleaner way.
    
    Signed-off-by: Kevin Winchester <kjwinchester@gmail.com>
    Cc: Steffen Persvold <sp@numascale.com>
    Link: http://lkml.kernel.org/r/1324428742-12498-1-git-send-email-kjwinchester@gmail.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b650435ffb53..aa9088c26931 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -99,7 +99,6 @@ struct cpuinfo_x86 {
 	u16			apicid;
 	u16			initial_apicid;
 	u16			x86_clflush_size;
-#ifdef CONFIG_SMP
 	/* number of cores as seen by the OS: */
 	u16			booted_cores;
 	/* Physical processor id: */
@@ -110,7 +109,6 @@ struct cpuinfo_x86 {
 	u8			compute_unit_id;
 	/* Index into per_cpu list: */
 	u16			cpu_index;
-#endif
 	u32			microcode;
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 

commit 506ed6b53e00ba303ad778122f08e1fca7cf5efb
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Oct 12 17:46:33 2011 -0700

    x86, intel: Output microcode revision in /proc/cpuinfo
    
    I got a request to make it easier to determine the microcode
    update level on Intel CPUs. This patch adds a new "microcode"
    field to /proc/cpuinfo.
    
    The microcode level is also outputed on fatal machine checks
    together with the other CPUID model information.
    
    I removed the respective code from the microcode update driver,
    it just reads the field from cpu_data. Also when the microcode
    is updated it fills in the new values too.
    
    I had to add a memory barrier to native_cpuid to prevent it
    being optimized away when the result is not used.
    
    This turns out to clean up further code which already got this
    information manually. This is done in followon patches.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1318466795-7393-1-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0d1171c97729..b650435ffb53 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -111,6 +111,7 @@ struct cpuinfo_x86 {
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 #endif
+	u32			microcode;
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 
 #define X86_VENDOR_INTEL	0
@@ -179,7 +180,8 @@ static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
 	      "=b" (*ebx),
 	      "=c" (*ecx),
 	      "=d" (*edx)
-	    : "0" (*eax), "2" (*ecx));
+	    : "0" (*eax), "2" (*ecx)
+	    : "memory");
 }
 
 static inline void load_cr3(pgd_t *pgdir)

commit 4bfc8288bc4a64529c5547d17349a2a1f4675507
Author: Len Brown <len.brown@intel.com>
Date:   Wed Mar 30 23:52:29 2011 -0400

    x86 idle: move mwait_idle_with_hints() to where it is used
    
    ...and make it static
    
    no functional change
    
    cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 219371546afd..0d1171c97729 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -751,8 +751,6 @@ static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
 		     :: "a" (eax), "c" (ecx));
 }
 
-extern void mwait_idle_with_hints(unsigned long eax, unsigned long ecx);
-
 extern void select_idle_routine(const struct cpuinfo_x86 *c);
 extern void init_amd_e400_c1e_mask(void);
 

commit f310642123e0d32d919c60ca3fab5acd130c4ba3
Merge: ef1d57599dc9 5d4c47e0195b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 29 11:18:09 2011 -0700

    Merge branch 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6
    
    * 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6:
      x86 idle: deprecate mwait_idle() and "idle=mwait" cmdline param
      x86 idle: deprecate "no-hlt" cmdline param
      x86 idle APM: deprecate CONFIG_APM_CPU_IDLE
      x86 idle floppy: deprecate disable_hlt()
      x86 idle: EXPORT_SYMBOL(default_idle, pm_idle) only when APM demands it
      x86 idle: clarify AMD erratum 400 workaround
      idle governor: Avoid lock acquisition to read pm_qos before entering idle
      cpuidle: menu: fixed wrapping timers at 4.294 seconds

commit 02c68a02018669d1817c43c42de800975cbec467
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 16:59:53 2011 -0400

    x86 idle: clarify AMD erratum 400 workaround
    
    The workaround for AMD erratum 400 uses the term "c1e" falsely suggesting:
    1. Intel C1E is somehow involved
    2. All AMD processors with C1E are involved
    
    Use the string "amd_c1e" instead of simply "c1e" to clarify that
    this workaround is specific to AMD's version of C1E.
    Use the string "e400" to clarify that the workaround is specific
    to AMD processors with Erratum 400.
    
    This patch is text-substitution only, with no functional change.
    
    cc: x86@kernel.org
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 45636cefa186..b9c03fb3369a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -758,10 +758,10 @@ static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
 extern void mwait_idle_with_hints(unsigned long eax, unsigned long ecx);
 
 extern void select_idle_routine(const struct cpuinfo_x86 *c);
-extern void init_c1e_mask(void);
+extern void init_amd_e400_c1e_mask(void);
 
 extern unsigned long		boot_option_idle_override;
-extern bool			c1e_detected;
+extern bool			amd_e400_c1e_detected;
 
 enum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,
 			 IDLE_POLL, IDLE_FORCE_MWAIT};

commit b3d7336db553d318e7ec042eb50a70d307013339
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Jan 21 15:29:44 2011 -0800

    x86: Move llc_shared_map out of cpu_info
    
    cpu_info is already with per_cpu, We can take llc_shared_map out
    of cpu_info, and declare it as per_cpu variable directly.
    
    So later referencing could be simple and directly instead of
    diving to find cpu_info at first.
    
    Also could make smp_store_cpu_info() much simple to avoid to do
    save and restore trick.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Hans Rosenfeld <hans.rosenfeld@amd.com>
    Cc: Alok N Kataria <akataria@vmware.com>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Hans J. Koch <hjk@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <4D3A16E8.5020608@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 45636cefa186..4c25ab48257b 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -94,10 +94,6 @@ struct cpuinfo_x86 {
 	int			x86_cache_alignment;	/* In bytes */
 	int			x86_power;
 	unsigned long		loops_per_jiffy;
-#ifdef CONFIG_SMP
-	/* cpus sharing the last level cache: */
-	cpumask_var_t		llc_shared_map;
-#endif
 	/* cpuid returned max cores value: */
 	u16			 x86_max_cores;
 	u16			apicid;

commit dc8e7e3ec60bd5ef7868aa88755e9d4c948dc5cc
Merge: 2c79c69adc62 43952886f0b8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 13 20:15:18 2011 -0800

    Merge branch 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6
    
    * 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6:
      cpuidle/x86/perf: fix power:cpu_idle double end events and throw cpu_idle events from the cpuidle layer
      intel_idle: open broadcast clock event
      cpuidle: CPUIDLE_FLAG_CHECK_BM is omap3_idle specific
      cpuidle: CPUIDLE_FLAG_TLB_FLUSHED is specific to intel_idle
      cpuidle: delete unused CPUIDLE_FLAG_SHALLOW, BALANCED, DEEP definitions
      SH, cpuidle: delete use of NOP CPUIDLE_FLAGS_SHALLOW
      cpuidle: delete NOP CPUIDLE_FLAG_POLL
      ACPI: processor_idle: delete use of NOP CPUIDLE_FLAGs
      cpuidle: Rename X86 specific idle poll state[0] from C0 to POLL
      ACPI, intel_idle: Cleanup idle= internal variables
      cpuidle: Make cpuidle_enable_device() call poll_idle_init()
      intel_idle: update Sandy Bridge core C-state residency targets

commit 008d23e4852d78bb2618f2035f8b2110b6a6b968
Merge: 8f685fbda43d bfc672dcf323
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 13 10:05:56 2011 -0800

    Merge branch 'for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (43 commits)
      Documentation/trace/events.txt: Remove obsolete sched_signal_send.
      writeback: fix global_dirty_limits comment runtime -> real-time
      ppc: fix comment typo singal -> signal
      drivers: fix comment typo diable -> disable.
      m68k: fix comment typo diable -> disable.
      wireless: comment typo fix diable -> disable.
      media: comment typo fix diable -> disable.
      remove doc for obsolete dynamic-printk kernel-parameter
      remove extraneous 'is' from Documentation/iostats.txt
      Fix spelling milisec -> ms in snd_ps3 module parameter description
      Fix spelling mistakes in comments
      Revert conflicting V4L changes
      i7core_edac: fix typos in comments
      mm/rmap.c: fix comment
      sound, ca0106: Fix assignment to 'channel'.
      hrtimer: fix a typo in comment
      init/Kconfig: fix typo
      anon_inodes: fix wrong function name in comment
      fix comment typos concerning "consistent"
      poll: fix a typo in comment
      ...
    
    Fix up trivial conflicts in:
     - drivers/net/wireless/iwlwifi/iwl-core.c (moved to iwl-legacy.c)
     - fs/ext4/ext4.h
    
    Also fix missed 'diabled' typo in drivers/net/bnx2x/bnx2x.h while at it.

commit 56dbed129df3fdd4caf9018b6e7599ee258a5420
Merge: 2a2d31c8dc6f f878133bf022
Author: Len Brown <len.brown@intel.com>
Date:   Wed Jan 12 18:06:06 2011 -0500

    Merge branch 'linus' into idle-test

commit d18960494f65ca4fa0d67c865aaca99452070d15
Author: Thomas Renninger <trenn@suse.de>
Date:   Wed Nov 3 17:06:14 2010 +0100

    ACPI, intel_idle: Cleanup idle= internal variables
    
    Having four variables for the same thing:
      idle_halt, idle_nomwait, force_mwait and boot_option_idle_overrides
    is rather confusing and unnecessary complex.
    
    if idle= boot param is passed, only set up one variable:
    boot_option_idle_overrides
    
    Introduces following functional changes/fixes:
      - intel_idle driver does not register if any idle=xy
        boot param is passed.
      - processor_idle.c will also not register a cpuidle driver
        and get active if idle=halt is passed.
        Before a cpuidle driver with one (C1, halt) state got registered
        Now the default_idle function will be used which finally uses
        the same idle call to enter sleep state (safe_halt()), but
        without registering a whole cpuidle driver.
    
    That means idle= param will always avoid cpuidle drivers to register
    with one exception (same behavior as before):
    idle=nomwait
    may still register acpi_idle cpuidle driver, but C1 will not use
    mwait, but hlt. This can be a workaround for IO based deeper sleep
    states where C1 mwait causes problems.
    
    Signed-off-by: Thomas Renninger <trenn@suse.de>
    cc: x86@kernel.org
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cae9c3cb95cf..b79bd980461c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -762,10 +762,11 @@ extern void select_idle_routine(const struct cpuinfo_x86 *c);
 extern void init_c1e_mask(void);
 
 extern unsigned long		boot_option_idle_override;
-extern unsigned long		idle_halt;
-extern unsigned long		idle_nomwait;
 extern bool			c1e_detected;
 
+enum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,
+			 IDLE_POLL, IDLE_FORCE_MWAIT};
+
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 

commit 7b543a5334ff4ea2e3ad3b777fc23cdb8072a988
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Dec 18 16:30:05 2010 +0100

    x86: Replace uses of current_cpu_data with this_cpu ops
    
    Replace all uses of current_cpu_data with this_cpu operations on the
    per cpu structure cpu_info.  The scala accesses are replaced with the
    matching this_cpu ops which results in smaller and more efficient
    code.
    
    In the long run, it might be a good idea to remove cpu_data() macro
    too and use per_cpu macro directly.
    
    tj: updated description
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cae9c3cb95cf..c6efecf85a6a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -141,10 +141,9 @@ extern __u32			cpu_caps_set[NCAPINTS];
 #ifdef CONFIG_SMP
 DECLARE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 #define cpu_data(cpu)		per_cpu(cpu_info, cpu)
-#define current_cpu_data	__get_cpu_var(cpu_info)
 #else
+#define cpu_info		boot_cpu_data
 #define cpu_data(cpu)		boot_cpu_data
-#define current_cpu_data	boot_cpu_data
 #endif
 
 extern const struct seq_operations cpuinfo_op;

commit b595076a180a56d1bb170e6eceda6eb9d76f4cd3
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Mon Nov 1 15:38:34 2010 -0400

    tree-wide: fix comment/printk typos
    
    "gadget", "through", "command", "maintain", "maintain", "controller", "address",
    "between", "initiali[zs]e", "instead", "function", "select", "already",
    "equal", "access", "management", "hierarchy", "registration", "interest",
    "relative", "memory", "offset", "already",
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index cae9c3cb95cf..c3e6d9a3de65 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -902,7 +902,7 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 /*
  * The below -8 is to reserve 8 bytes on top of the ring0 stack.
  * This is necessary to guarantee that the entire "struct pt_regs"
- * is accessable even if the CPU haven't stored the SS/ESP registers
+ * is accessible even if the CPU haven't stored the SS/ESP registers
  * on the stack (interrupt gate does not save these registers
  * when switching to the same priv ring).
  * Therefore beware: accessing the ss/esp fields of the

commit 2a8b67fb72c4c4bc15fe8095e3ed613789c8b82f
Merge: b6f7e38dbb31 ce5f68246bf2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:45:38 2010 -0700

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hotplug: In the MWAIT case of play_dead, CLFLUSH the cache line
      x86, hotplug: Move WBINVD back outside the play_dead loop
      x86, hotplug: Use mwait to offline a processor, fix the legacy case
      x86, mwait: Move mwait constants to a common header file

commit b6f7e38dbb310557fe890b04b1a376c93f638c3b
Merge: 214515b5787a b2b57fe053c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:34:32 2010 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, fpu: Merge fpu_save_init()
      x86-32, fpu: Rewrite fpu_save_init()
      x86, fpu: Remove PSHUFB_XMM5_* macros
      x86, fpu: Remove unnecessary ifdefs from i387 code.
      x86-32, fpu: Remove math_emulate stub
      x86-64, fpu: Simplify constraints for fxsave/fxtstor
      x86-64, fpu: Fix %cs value in convert_from_fxsr()
      x86-64, fpu: Disable preemption when using TS_USEDFPU
      x86, fpu: Merge __save_init_fpu()
      x86, fpu: Merge tolerant_fwait()
      x86, fpu: Merge fpu_init()
      x86: Use correct type for %cr4
      x86, xsave: Disable xsave in i387 emulation mode
    
    Fixed up fxsaveq-induced conflict in arch/x86/include/asm/i387.h

commit 6057b4d331f19a3ea51aec463ea7839c128b3227
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 30 14:38:57 2010 +0200

    x86, amd: Extract compute unit information for AMD CPUs
    
    Get compute unit information from CPUID Fn8000_001E_EBX.
    (See AMD CPUID Specification - publication # 25481, revision 2.34,
    September 2010.)
    
    Note that each core on a compute unit still has a core_id of its own.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20100930123857.GE20545@loge.amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 325b7bdbebaa..69e80c2ec6c2 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -110,6 +110,8 @@ struct cpuinfo_x86 {
 	u16			phys_proc_id;
 	/* Core id: */
 	u16			cpu_core_id;
+	/* Compute unit id */
+	u8			compute_unit_id;
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 #endif

commit ea53069231f9317062910d6e772cca4ce93de8c8
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 17 15:39:11 2010 -0700

    x86, hotplug: Use mwait to offline a processor, fix the legacy case
    
    The code in native_play_dead() has a number of problems:
    
    1. We should use MWAIT when available, to put ourselves into a deeper
       sleep state.
    2. We use the existence of CLFLUSH to determine if WBINVD is safe, but
       that is totally bogus -- WBINVD is 486+, whereas CLFLUSH is a much
       later addition.
    3. We should do WBINVD inside the loop, just in case of something like
       setting an A bit on page tables.  Pointed out by Arjan van de Ven.
    
    This code is based in part of a previous patch by Venki Pallipadi, but
    unlike that patch this one keeps all the detection code local instead
    of pre-caching a bunch of information.  We're shutting down the CPU;
    there is absolutely no hurry.
    
    This patch moves all the code to C and deletes the global
    wbinvd_halt() which is broken anyway.
    
    Originally-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.hl>
    LKML-Reference: <20090522232230.162239000@intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 325b7bdbebaa..f358241e4121 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -764,29 +764,6 @@ extern unsigned long		idle_halt;
 extern unsigned long		idle_nomwait;
 extern bool			c1e_detected;
 
-/*
- * on systems with caches, caches must be flashed as the absolute
- * last instruction before going into a suspended halt.  Otherwise,
- * dirty data can linger in the cache and become stale on resume,
- * leading to strange errors.
- *
- * perform a variety of operations to guarantee that the compiler
- * will not reorder instructions.  wbinvd itself is serializing
- * so the processor will not reorder.
- *
- * Systems without cache can just go into halt.
- */
-static inline void wbinvd_halt(void)
-{
-	mb();
-	/* check for clflush to determine if wbinvd is legal */
-	if (cpu_has_clflush)
-		asm volatile("cli; wbinvd; 1: hlt; jmp 1b" : : : "memory");
-	else
-		while (1)
-			halt();
-}
-
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 

commit 2df7a6e9e8e67c19e5fe2eac3f2d2223b7bb4a7b
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Sep 3 21:17:08 2010 -0400

    x86: Use correct type for %cr4
    
    %cr4 is 64-bit in 64-bit mode (although the upper 32-bits are currently reserved).
    Use unsigned long for the temporary variable to get the right size.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1283563039-3466-2-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 325b7bdbebaa..396b80f2d871 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -602,7 +602,7 @@ extern unsigned long		mmu_cr4_features;
 
 static inline void set_in_cr4(unsigned long mask)
 {
-	unsigned cr4;
+	unsigned long cr4;
 
 	mmu_cr4_features |= mask;
 	cr4 = read_cr4();
@@ -612,7 +612,7 @@ static inline void set_in_cr4(unsigned long mask)
 
 static inline void clear_in_cr4(unsigned long mask)
 {
-	unsigned cr4;
+	unsigned long cr4;
 
 	mmu_cr4_features &= ~mask;
 	cr4 = read_cr4();

commit e8c534ec068af1a0845aceda373a9bfd2de62030
Author: Michal Schmidt <mschmidt@redhat.com>
Date:   Tue Jul 27 18:53:35 2010 +0200

    x86: Fix keeping track of AMD C1E
    
    Accomodate the original C1E-aware idle routine to the different times
    during boot when the BIOS enables C1E. While at it, remove the synthetic
    CPUID flag in favor of a single global setting which denotes C1E status
    on the system.
    
    [ hpa: changed c1e_enabled to be a bool; clarified cpu bit 3:21 comment ]
    
    Signed-off-by: Michal Schmidt <mschmidt@redhat.com>
    LKML-Reference: <20100727165335.GA11630@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d85637bb9505..325b7bdbebaa 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -762,6 +762,7 @@ extern void init_c1e_mask(void);
 extern unsigned long		boot_option_idle_override;
 extern unsigned long		idle_halt;
 extern unsigned long		idle_nomwait;
+extern bool			c1e_detected;
 
 /*
  * on systems with caches, caches must be flashed as the absolute

commit 1be85a6d93f4207d8c2c6238c4a96895e28cefba
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Jul 28 19:09:32 2010 +0200

    x86, cpu: Use AMD errata checking framework for erratum 383
    
    Use the AMD errata checking framework instead of open-coding the test.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-3-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index eebdc1fde3d1..d85637bb9505 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1029,6 +1029,7 @@ unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
  * AMD errata checking
  */
 #ifdef CONFIG_CPU_SUP_AMD
+extern const int amd_erratum_383[];
 extern const int amd_erratum_400[];
 extern bool cpu_has_amd_erratum(const int *);
 

commit 9d8888c2a214aece2494a49e699a097c2ba9498b
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Jul 28 19:09:31 2010 +0200

    x86, cpu: Clean up AMD erratum 400 workaround
    
    Remove check_c1e_idle() and use the new AMD errata checking framework
    instead.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-2-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 5084c2f5ac20..eebdc1fde3d1 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1029,6 +1029,7 @@ unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
  * AMD errata checking
  */
 #ifdef CONFIG_CPU_SUP_AMD
+extern const int amd_erratum_400[];
 extern bool cpu_has_amd_erratum(const int *);
 
 #define AMD_LEGACY_ERRATUM(...)		{ -1, __VA_ARGS__, 0 }

commit d78d671db478eb8b14c78501c0cee1cc7baf6967
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Jul 28 19:09:30 2010 +0200

    x86, cpu: AMD errata checking framework
    
    Errata are defined using the AMD_LEGACY_ERRATUM() or AMD_OSVW_ERRATUM()
    macros. The latter is intended for newer errata that have an OSVW id
    assigned, which it takes as first argument. Both take a variable number
    of family-specific model-stepping ranges created by AMD_MODEL_RANGE().
    
    Iff an erratum has an OSVW id, OSVW is available on the CPU, and the
    OSVW id is known to the hardware, it is used to determine whether an
    erratum is present. Otherwise, the model-stepping ranges are matched
    against the current CPU to find out whether the erratum applies.
    
    For certain special errata, the code using this framework might have to
    conduct further checks to make sure an erratum is really (not) present.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-1-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7e5c6a60b8ee..5084c2f5ac20 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1025,4 +1025,22 @@ unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
 	return ratio;
 }
 
+/*
+ * AMD errata checking
+ */
+#ifdef CONFIG_CPU_SUP_AMD
+extern bool cpu_has_amd_erratum(const int *);
+
+#define AMD_LEGACY_ERRATUM(...)		{ -1, __VA_ARGS__, 0 }
+#define AMD_OSVW_ERRATUM(osvw_id, ...)	{ osvw_id, __VA_ARGS__, 0 }
+#define AMD_MODEL_RANGE(f, m_start, s_start, m_end, s_end) \
+	((f << 24) | (m_start << 16) | (s_start << 12) | (m_end << 4) | (s_end))
+#define AMD_MODEL_RANGE_FAMILY(range)	(((range) >> 24) & 0xff)
+#define AMD_MODEL_RANGE_START(range)	(((range) >> 12) & 0xfff)
+#define AMD_MODEL_RANGE_END(range)	((range) & 0xfff)
+
+#else
+#define cpu_has_amd_erratum(x)	(false)
+#endif /* CONFIG_CPU_SUP_AMD */
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit 29c843912a0baa7fa63033fe28e1ca7e796686a5
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Thu May 20 21:04:29 2010 -0500

    x86, kgdb: early trap init for early debug
    
    Allow the x86 arch to have early exception processing for the purpose
    of debugging via the kgdb.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@web.de>
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 5a51379dcbe4..7e5c6a60b8ee 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -789,6 +789,8 @@ static inline void wbinvd_halt(void)
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 
+extern void early_trap_init(void);
+
 /* Defined in head.S */
 extern struct desc_ptr		early_gdt_descr;
 

commit 41d59102e146a4423a490b8eca68a5860af4fe1c
Merge: 3e1dd193edef c9775b4cc522
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:58:16 2010 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, fpu: Use static_cpu_has() to implement use_xsave()
      x86: Add new static_cpu_has() function using alternatives
      x86, fpu: Use the proper asm constraint in use_xsave()
      x86, fpu: Unbreak FPU emulation
      x86: Introduce 'struct fpu' and related API
      x86: Eliminate TS_XSAVE
      x86-32: Don't set ignore_fpu_irq in simd exception
      x86: Merge kernel_math_error() into math_error()
      x86: Merge simd_math_error() into math_error()
      x86-32: Rework cache flush denied handler
    
    Fix trivial conflict in arch/x86/kernel/process.c

commit 07d77759c95d899b84f8e473a01cff001019dd5f
Merge: b7723f9d21d8 3998d095354d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:49:13 2010 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hypervisor: add missing <linux/module.h>
      Modify the VMware balloon driver for the new x86_hyper API
      x86, hypervisor: Export the x86_hyper* symbols
      x86: Clean up the hypervisor layer
      x86, HyperV: fix up the license to mshyperv.c
      x86: Detect running on a Microsoft HyperV system
      x86, cpu: Make APERF/MPERF a normal table-driven flag
      x86, k8: Fix build error when K8_NB is disabled
      x86, cacheinfo: Disable index in all four subcaches
      x86, cacheinfo: Make L3 cache info per node
      x86, cacheinfo: Reorganize AMD L3 cache structure
      x86, cacheinfo: Turn off L3 cache index disable feature in virtualized environments
      x86, cacheinfo: Unify AMD L3 cache index disable checking
      cpufreq: Unify sysfs attribute definition macros
      powernow-k8: Fix frequency reporting
      x86, cpufreq: Add APERF/MPERF support for AMD processors
      x86: Unify APERF/MPERF support
      powernow-k8: Add core performance boost support
      x86, cpu: Add AMD core boosting feature flag to /proc/cpuinfo
    
    Fix up trivial conflicts in arch/x86/kernel/cpu/intel_cacheinfo.c and
    drivers/cpufreq/cpufreq_ondemand.c

commit 86603283326c9e95e5ad4e9fdddeec93cac5d9ad
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 6 11:45:46 2010 +0300

    x86: Introduce 'struct fpu' and related API
    
    Currently all fpu state access is through tsk->thread.xstate.  Since we wish
    to generalize fpu access to non-task contexts, wrap the state in a new
    'struct fpu' and convert existing access to use an fpu API.
    
    Signal frame handlers are not converted to the API since they will remain
    task context only things.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1273135546-29690-3-git-send-email-avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b753ea59703a..b684f587647c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -380,6 +380,10 @@ union thread_xstate {
 	struct xsave_struct		xsave;
 };
 
+struct fpu {
+	union thread_xstate *state;
+};
+
 #ifdef CONFIG_X86_64
 DECLARE_PER_CPU(struct orig_ist, orig_ist);
 
@@ -457,7 +461,7 @@ struct thread_struct {
 	unsigned long		trap_no;
 	unsigned long		error_code;
 	/* floating point and extended processor state */
-	union thread_xstate	*xstate;
+	struct fpu		fpu;
 #ifdef CONFIG_X86_32
 	/* Virtual 86 mode info */
 	struct vm86_struct __user *vm86_info;

commit e08cae4181af9483b04ecfac48f01c8e5a5f27bf
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri May 7 16:57:28 2010 -0700

    x86: Clean up the hypervisor layer
    
    Clean up the hypervisor layer and the hypervisor drivers, using an ops
    structure instead of an enumeration with if statements.
    
    The identity of the hypervisor, if needed, can be tested by testing
    the pointer value in x86_hyper.
    
    The MS-HyperV private state is moved into a normal global variable
    (it's per-system state, not per-CPU state).  Being a normal bss
    variable, it will be left at all zero on non-HyperV platforms, and so
    can generally be tested for HyperV-specific features without
    additional qualification.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Greg KH <greg@kroah.com>
    Cc: Hank Janssen <hjanssen@microsoft.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Ky Srinivasan <ksrinivasan@novell.com>
    LKML-Reference: <4BE49778.6060800@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 597c041bd124..e4f1dfb2d05b 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -113,9 +113,6 @@ struct cpuinfo_x86 {
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 #endif
-	unsigned int		x86_hyper_vendor;
-	/* The layout of this field is hypervisor specific */
-	unsigned int		x86_hyper_features;
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 
 #define X86_VENDOR_INTEL	0
@@ -129,10 +126,6 @@ struct cpuinfo_x86 {
 
 #define X86_VENDOR_UNKNOWN	0xff
 
-#define X86_HYPER_VENDOR_NONE  0
-#define X86_HYPER_VENDOR_VMWARE 1
-#define X86_HYPER_VENDOR_MSFT	2
-
 /*
  * capabilities of CPUs
  */

commit a2a47c6c3d1a7c01da4464b3b7be93b924f874c1
Author: Ky Srinivasan <ksrinivasan@novell.com>
Date:   Thu May 6 12:08:41 2010 -0700

    x86: Detect running on a Microsoft HyperV system
    
    This patch integrates HyperV detection within the framework currently
    used by VmWare. With this patch, we can avoid having to replicate the
    HyperV detection code in each of the Microsoft HyperV drivers.
    
    Reworked and tweaked by Greg K-H to build properly.
    
    Signed-off-by: K. Y. Srinivasan <ksrinivasan@novell.com>
    LKML-Reference: <20100506190841.GA1605@kroah.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Vadim Rozenfeld <vrozenfe@redhat.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "K.Prasad" <prasad@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Hank Janssen <hjanssen@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b753ea59703a..597c041bd124 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -114,6 +114,8 @@ struct cpuinfo_x86 {
 	u16			cpu_index;
 #endif
 	unsigned int		x86_hyper_vendor;
+	/* The layout of this field is hypervisor specific */
+	unsigned int		x86_hyper_features;
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 
 #define X86_VENDOR_INTEL	0
@@ -129,6 +131,7 @@ struct cpuinfo_x86 {
 
 #define X86_HYPER_VENDOR_NONE  0
 #define X86_HYPER_VENDOR_VMWARE 1
+#define X86_HYPER_VENDOR_MSFT	2
 
 /*
  * capabilities of CPUs

commit ea8e61b7bbc4a2faef77db34eb2db2a2c2372ff6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:51 2010 +0100

    x86, ptrace: Fix block-step
    
    Implement ptrace-block-step using TIF_BLOCKSTEP which will set
    DEBUGCTLMSR_BTF when set for a task while preserving any other
    DEBUGCTLMSR bits.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100325135414.017536066@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 5bec21a66dc5..32428b410b55 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -799,7 +799,7 @@ extern void cpu_init(void);
 
 static inline unsigned long get_debugctlmsr(void)
 {
-    unsigned long debugctlmsr = 0;
+	unsigned long debugctlmsr = 0;
 
 #ifndef CONFIG_X86_DEBUGCTLMSR
 	if (boot_cpu_data.x86 < 6)
@@ -807,7 +807,7 @@ static inline unsigned long get_debugctlmsr(void)
 #endif
 	rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);
 
-    return debugctlmsr;
+	return debugctlmsr;
 }
 
 static inline void update_debugctlmsr(unsigned long debugctlmsr)

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index b753ea59703a..5bec21a66dc5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -21,7 +21,6 @@ struct mm_struct;
 #include <asm/msr.h>
 #include <asm/desc_defs.h>
 #include <asm/nops.h>
-#include <asm/ds.h>
 
 #include <linux/personality.h>
 #include <linux/cpumask.h>
@@ -29,6 +28,7 @@ struct mm_struct;
 #include <linux/threads.h>
 #include <linux/math64.h>
 #include <linux/init.h>
+#include <linux/err.h>
 
 #define HBP_NUM 4
 /*
@@ -473,10 +473,6 @@ struct thread_struct {
 	unsigned long		iopl;
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
-/* MSR_IA32_DEBUGCTLMSR value to switch in if TIF_DEBUGCTLMSR is set.  */
-	unsigned long	debugctlmsr;
-	/* Debug Store context; see asm/ds.h */
-	struct ds_context	*ds_ctx;
 };
 
 static inline unsigned long native_get_debugreg(int regno)
@@ -814,21 +810,6 @@ static inline unsigned long get_debugctlmsr(void)
     return debugctlmsr;
 }
 
-static inline unsigned long get_debugctlmsr_on_cpu(int cpu)
-{
-	u64 debugctlmsr = 0;
-	u32 val1, val2;
-
-#ifndef CONFIG_X86_DEBUGCTLMSR
-	if (boot_cpu_data.x86 < 6)
-		return 0;
-#endif
-	rdmsr_on_cpu(cpu, MSR_IA32_DEBUGCTLMSR, &val1, &val2);
-	debugctlmsr = val1 | ((u64)val2 << 32);
-
-	return debugctlmsr;
-}
-
 static inline void update_debugctlmsr(unsigned long debugctlmsr)
 {
 #ifndef CONFIG_X86_DEBUGCTLMSR
@@ -838,18 +819,6 @@ static inline void update_debugctlmsr(unsigned long debugctlmsr)
 	wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);
 }
 
-static inline void update_debugctlmsr_on_cpu(int cpu,
-					     unsigned long debugctlmsr)
-{
-#ifndef CONFIG_X86_DEBUGCTLMSR
-	if (boot_cpu_data.x86 < 6)
-		return;
-#endif
-	wrmsr_on_cpu(cpu, MSR_IA32_DEBUGCTLMSR,
-		     (u32)((u64)debugctlmsr),
-		     (u32)((u64)debugctlmsr >> 32));
-}
-
 /*
  * from system description table in BIOS. Mostly for MCA use, but
  * others may find it useful:

commit 326264a02448b0ac51f78f178b78e830aa077a0b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Feb 18 18:24:18 2010 +0100

    hw-breakpoint: Keep track of dr7 local enable bits
    
    When the user enables breakpoints through dr7, he can choose
    between "local" or "global" enable bits but given how linux is
    implemented, both have the same effect.
    
    That said we don't keep track how the user enabled the breakpoints
    so when the user requests the dr7 value, we only translate the
    "enabled" status using the global enabled bits. It means that if
    the user enabled a breakpoint using the local enabled bit, reading
    back dr7 will set the global bit and clear the local one.
    
    Apps like Wine expect a full dr7 POKEUSER/PEEKUSER match for emulated
    softwares that implement old reverse engineering protection schemes.
    
    We fix that by keeping track of the whole dr7 value given by the user
    in the thread structure to drop this bug. We'll think about
    something more proper later.
    
    This fixes a 2.6.32 - 2.6.33-x ptrace regression.
    
    Reported-and-tested-by: Michael Stefaniuc <mstefani@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Maneesh Soni <maneesh@linux.vnet.ibm.com>
    Cc: Alexandre Julliard <julliard@winehq.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Maciej Rutecki <maciej.rutecki@gmail.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index fc801bab1b3b..b753ea59703a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -450,6 +450,8 @@ struct thread_struct {
 	struct perf_event	*ptrace_bps[HBP_NUM];
 	/* Debug status used for traps, single steps, etc... */
 	unsigned long           debugreg6;
+	/* Keep track of the exact dr7 value set by the user */
+	unsigned long           ptrace_dr7;
 	/* Fault info: */
 	unsigned long		cr2;
 	unsigned long		trap_no;

commit 45a94d7cd45ed991914011919e7d40eb6d2546d1
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed Dec 16 16:25:42 2009 -0800

    x86, cpuid: Add "volatile" to asm in native_cpuid()
    
    xsave_cntxt_init() does something like:
    
            cpuid(0xd, ..); // find out what features FP/SSE/.. etc are supported
    
            xsetbv();       // enable the features known to OS
    
            cpuid(0xd, ..); // find out the size of the context for features enabled
    
    Depending on what features get enabled in xsetbv(), value of the
    cpuid.eax=0xd.ecx=0.ebx changes correspondingly (representing the
    size of the context that is enabled).
    
    As we don't have volatile keyword for native_cpuid(), gcc 4.1.2
    optimizes away the second cpuid and the kernel continues to use
    the cpuid information obtained before xsetbv(), ultimately leading to kernel
    crash on processors supporting more state than the legacy FP/SSE.
    
    Add "volatile" for native_cpuid().
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1261009542.2745.55.camel@sbs-t61.sc.intel.com>
    Cc: stable@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6f8ec1c37e0a..fc801bab1b3b 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -181,7 +181,7 @@ static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
 				unsigned int *ecx, unsigned int *edx)
 {
 	/* ecx is often an input as well as an output. */
-	asm("cpuid"
+	asm volatile("cpuid"
 	    : "=a" (*eax),
 	      "=b" (*ebx),
 	      "=c" (*ecx),

commit 96200591a34f8ecb98481c626125df43a2463b55
Merge: 7031281e02bf 68efa37df779
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 21 14:07:23 2009 +0100

    Merge branch 'tracing/hw-breakpoints' into perf/core
    
    Conflicts:
            arch/x86/kernel/kprobes.c
            kernel/trace/Makefile
    
    Merge reason: hw-breakpoints perf integration is looking
                  good in testing and in reviews, plus conflicts
                  are mounting up - so merge & resolve.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 24f1e32c60c45c89a997c73395b69c8af6f0a84e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 9 19:22:48 2009 +0200

    hw-breakpoints: Rewrite the hw-breakpoints layer on top of perf events
    
    This patch rebase the implementation of the breakpoints API on top of
    perf events instances.
    
    Each breakpoints are now perf events that handle the
    register scheduling, thread/cpu attachment, etc..
    
    The new layering is now made as follows:
    
           ptrace       kgdb      ftrace   perf syscall
              \          |          /         /
               \         |         /         /
                                            /
                Core breakpoint API        /
                                          /
                         |               /
                         |              /
    
                  Breakpoints perf events
    
                         |
                         |
    
                   Breakpoints PMU ---- Debug Register constraints handling
                                        (Part of core breakpoint API)
                         |
                         |
    
                 Hardware debug registers
    
    Reasons of this rewrite:
    
    - Use the centralized/optimized pmu registers scheduling,
      implying an easier arch integration
    - More powerful register handling: perf attributes (pinned/flexible
      events, exclusive/non-exclusive, tunable period, etc...)
    
    Impact:
    
    - New perf ABI: the hardware breakpoints counters
    - Ptrace breakpoints setting remains tricky and still needs some per
      thread breakpoints references.
    
    Todo (in the order):
    
    - Support breakpoints perf counter events for perf tools (ie: implement
      perf_bpcounter_event())
    - Support from perf tools
    
    Changes in v2:
    
    - Follow the perf "event " rename
    - The ptrace regression have been fixed (ptrace breakpoint perf events
      weren't released when a task ended)
    - Drop the struct hw_breakpoint and store generic fields in
      perf_event_attr.
    - Separate core and arch specific headers, drop
      asm-generic/hw_breakpoint.h and create linux/hw_breakpoint.h
    - Use new generic len/type for breakpoint
    - Handle off case: when breakpoints api is not supported by an arch
    
    Changes in v3:
    
    - Fix broken CONFIG_KVM, we need to propagate the breakpoint api
      changes to kvm when we exit the guest and restore the bp registers
      to the host.
    
    Changes in v4:
    
    - Drop the hw_breakpoint_restore() stub as it is only used by KVM
    - EXPORT_SYMBOL_GPL hw_breakpoint_restore() as KVM can be built as a
      module
    - Restore the breakpoints unconditionally on kvm guest exit:
      TIF_DEBUG_THREAD doesn't anymore cover every cases of running
      breakpoints and vcpu->arch.switch_db_regs might not always be
      set when the guest used debug registers.
      (Waiting for a reliable optimization)
    
    Changes in v5:
    
    - Split-up the asm-generic/hw-breakpoint.h moving to
      linux/hw_breakpoint.h into a separate patch
    - Optimize the breakpoints restoring while switching from kvm guest
      to host. We only want to restore the state if we have active
      breakpoints to the host, otherwise we don't care about messed-up
      address registers.
    - Add asm/hw_breakpoint.h to Kbuild
    - Fix bad breakpoint type in trace_selftest.c
    
    Changes in v6:
    
    - Fix wrong header inclusion in trace.h (triggered a build
      error with CONFIG_FTRACE_SELFTEST
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 61aafb71c7ef..820f3000f736 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -423,6 +423,8 @@ extern unsigned int xstate_size;
 extern void free_thread_xstate(struct task_struct *);
 extern struct kmem_cache *task_xstate_cachep;
 
+struct perf_event;
+
 struct thread_struct {
 	/* Cached TLS descriptors: */
 	struct desc_struct	tls_array[GDT_ENTRY_TLS_ENTRIES];
@@ -444,12 +446,10 @@ struct thread_struct {
 	unsigned long		fs;
 #endif
 	unsigned long		gs;
-	/* Hardware debugging registers: */
-	unsigned long		debugreg[HBP_NUM];
-	unsigned long		debugreg6;
-	unsigned long		debugreg7;
-	/* Hardware breakpoint info */
-	struct hw_breakpoint	*hbp[HBP_NUM];
+	/* Save middle states of ptrace breakpoints */
+	struct perf_event	*ptrace_bps[HBP_NUM];
+	/* Debug status used for traps, single steps, etc... */
+	unsigned long           debugreg6;
 	/* Fault info: */
 	unsigned long		cr2;
 	unsigned long		trap_no;

commit 89240ba059ca468ae7a8346edf7f95082458c2fc
Author: Stefani Seibold <stefani@seibold.net>
Date:   Tue Nov 3 10:22:40 2009 +0100

    x86, fs: Fix x86 procfs stack information for threads on 64-bit
    
    This patch fixes two issues in the procfs stack information on
    x86-64 linux.
    
    The 32 bit loader compat_do_execve did not store stack
    start. (this was figured out by Alexey Dobriyan).
    
    The stack information on a x64_64 kernel always shows 0 kbyte
    stack usage, because of a missing implementation of the KSTK_ESP
    macro which always returned -1.
    
    The new implementation now returns the right value.
    
    Signed-off-by: Stefani Seibold <stefani@seibold.net>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <1257240160.4889.24.camel@wall-e>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c3429e8b2424..c9786480f0fe 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1000,7 +1000,7 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 #define thread_saved_pc(t)	(*(unsigned long *)((t)->thread.sp - 8))
 
 #define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.sp0 - 1)
-#define KSTK_ESP(tsk)		-1 /* sorry. doesn't work for syscall. */
+extern unsigned long KSTK_ESP(struct task_struct *task);
 #endif /* CONFIG_X86_64 */
 
 extern void start_thread(struct pt_regs *regs, unsigned long new_ip,

commit 0f8f86c7bdd1c954fbe153af437a0d91a6c5721a
Merge: dca2d6ac09d9 f39cdf25bf77
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Oct 18 01:09:09 2009 +0200

    Merge commit 'perf/core' into perf/hw-breakpoint
    
    Conflicts:
            kernel/Makefile
            kernel/trace/Makefile
            kernel/trace/trace.h
            samples/Makefile
    
    Merge reason: We need to be uptodate with the perf events development
    branch because we plan to rewrite the breakpoints API on top of
    perf events.

commit dcbf77b9e86e1726f5fbd01bb98820dac06d456e
Merge: ca043a66ae48 29cd8bae3965
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 17 21:00:02 2009 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (37 commits)
      sched: Fix SD_POWERSAVING_BALANCE|SD_PREFER_LOCAL vs SD_WAKE_AFFINE
      sched: Stop buddies from hogging the system
      sched: Add new wakeup preemption mode: WAKEUP_RUNNING
      sched: Fix TASK_WAKING & loadaverage breakage
      sched: Disable wakeup balancing
      sched: Rename flags to wake_flags
      sched: Clean up the load_idx selection in select_task_rq_fair
      sched: Optimize cgroup vs wakeup a bit
      sched: x86: Name old_perf in a unique way
      sched: Implement a gentler fair-sleepers feature
      sched: Add SD_PREFER_LOCAL
      sched: Add a few SYNC hint knobs to play with
      sched: Fix sync wakeups again
      sched: Add WF_FORK
      sched: Rename sync arguments
      sched: Rename select_task_rq() argument
      sched: Feature to disable APERF/MPERF cpu_power
      x86: sched: Provide arch implementations using aperf/mperf
      x86: Add generic aperf/mperf code
      x86: Move APERF/MPERF into a X86_FEATURE
      ...
    
    Fix up trivial conflict in arch/x86/include/asm/processor.h due to
    nearby addition of amd_get_nb_id() declaration from the EDAC merge.

commit 6a8126911a5ab167783fce18ae9cc70ec9b84fe2
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Wed Sep 16 11:33:40 2009 +0200

    x86, EDAC: Provide function to return NodeId of a CPU
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index e08ea043e085..42a3f936dadc 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1020,4 +1020,6 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
+extern int amd_get_nb_id(int cpu);
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit 5cbc19a983141729d716be17197028434127b376
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 2 11:49:52 2009 +0200

    x86: Add generic aperf/mperf code
    
    Move some of the aperf/mperf code out from the cpufreq driver
    thingy so that other people can enjoy it too.
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: cpufreq@vger.kernel.org
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index e08ea043e085..4ae2ccfed638 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -27,6 +27,7 @@ struct mm_struct;
 #include <linux/cpumask.h>
 #include <linux/cache.h>
 #include <linux/threads.h>
+#include <linux/math64.h>
 #include <linux/init.h>
 
 /*
@@ -1020,4 +1021,33 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
+struct aperfmperf {
+	u64 aperf, mperf;
+};
+
+static inline void get_aperfmperf(struct aperfmperf *am)
+{
+	WARN_ON_ONCE(!boot_cpu_has(X86_FEATURE_APERFMPERF));
+
+	rdmsrl(MSR_IA32_APERF, am->aperf);
+	rdmsrl(MSR_IA32_MPERF, am->mperf);
+}
+
+#define APERFMPERF_SHIFT 10
+
+static inline
+unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
+				    struct aperfmperf *new)
+{
+	u64 aperf = new->aperf - old->aperf;
+	u64 mperf = new->mperf - old->mperf;
+	unsigned long ratio = aperf;
+
+	mperf >>= APERFMPERF_SHIFT;
+	if (mperf)
+		ratio = div64_u64(aperf, mperf);
+
+	return ratio;
+}
+
 #endif /* _ASM_X86_PROCESSOR_H */

commit dca2d6ac09d9ef59ff46820d4f0c94b08a671202
Merge: d6a65dffb30d 18240904960a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 15 12:18:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/kernel/process_64.c
    
    Semantic conflict fixed in:
            arch/x86/kvm/x86.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c7208de304ac335d5d58db346bb773a471fc636b
Merge: 15b0404272e1 5367b6887e7d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 07:57:32 2009 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (22 commits)
      x86: Fix code patching for paravirt-alternatives on 486
      x86, msr: change msr-reg.o to obj-y, and export its symbols
      x86: Use hard_smp_processor_id() to get apic id for AMD K8 cpus
      x86, sched: Workaround broken sched domain creation for AMD Magny-Cours
      x86, mcheck: Use correct cpumask for shared bank4
      x86, cacheinfo: Fixup L3 cache information for AMD multi-node processors
      x86: Fix CPU llc_shared_map information for AMD Magny-Cours
      x86, msr: Fix msr-reg.S compilation with gas 2.16.1, on 32-bit too
      x86: Move kernel_fpu_using to irq_fpu_usable in asm/i387.h
      x86, msr: fix msr-reg.S compilation with gas 2.16.1
      x86, msr: Export the register-setting MSR functions via /dev/*/msr
      x86, msr: Create _on_cpu helpers for {rw,wr}msr_safe_regs()
      x86, msr: Have the _safe MSR functions return -EIO, not -EFAULT
      x86, msr: CFI annotations, cleanups for msr-reg.S
      x86, asm: Make _ASM_EXTABLE() usable from assembly code
      x86, asm: Add 32-bit versions of the combined CFI macros
      x86, AMD: Disable wrongly set X86_FEATURE_LAHF_LM CPUID bit
      x86, msr: Rewrite AMD rd/wrmsr variants
      x86, msr: Add rd/wrmsr interfaces with preset registers
      x86: add specific support for Intel Atom architecture
      ...

commit 5367b6887e7d8c870a5da7d9b8c6e9c207684e43
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Thu Sep 10 02:53:50 2009 +0100

    x86: Fix code patching for paravirt-alternatives on 486
    
    As reported in <http://bugs.debian.org/511703> and
    <http://bugs.debian.org/515982>, kernels with paravirt-alternatives
    enabled crash in text_poke_early() on at least some 486-class
    processors.
    
    The problem is that text_poke_early() itself uses inline functions
    affected by paravirt-alternatives and so will modify instructions that
    have already been prefetched.  Pentium and later processors will
    invalidate the prefetched instructions in this case, but 486-class
    processors do not.
    
    Change sync_core() to limit prefetching on 486-class (and 386-class)
    processors, and move the call to sync_core() above the call to the
    modifiable local_irq_restore().
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    LKML-Reference: <1252547631.3423.134.camel@localhost>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c7768269b1cf..2db56c57a281 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -703,13 +703,23 @@ static inline void cpu_relax(void)
 	rep_nop();
 }
 
-/* Stop speculative execution: */
+/* Stop speculative execution and prefetching of modified code. */
 static inline void sync_core(void)
 {
 	int tmp;
 
-	asm volatile("cpuid" : "=a" (tmp) : "0" (1)
-		     : "ebx", "ecx", "edx", "memory");
+#if defined(CONFIG_M386) || defined(CONFIG_M486)
+	if (boot_cpu_data.x86 < 5)
+		/* There is no speculative execution.
+		 * jmp is a barrier to prefetching. */
+		asm volatile("jmp 1f\n1:\n" ::: "memory");
+	else
+#endif
+		/* cpuid is a barrier to speculative execution.
+		 * Prefetched instructions are automatically
+		 * invalidated when modified. */
+		asm volatile("cpuid" : "=a" (tmp) : "0" (1)
+			     : "ebx", "ecx", "edx", "memory");
 }
 
 static inline void __monitor(const void *eax, unsigned long ecx,

commit 53f824520b6d84ca5b4a8fd71addc91dbf64357e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Sep 3 14:31:44 2009 -0700

    x86/i386: Put aligned stack-canary in percpu shared_aligned section
    
    Pack aligned things together into a special section to minimize
    padding holes.
    
    Suggested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Tejun Heo <tj@kernel.org>
    LKML-Reference: <4AA035C0.9070202@goop.org>
    [ queued up in tip:x86/asm because it depends on this commit:
      x86/i386: Make sure stack-protector segment base is cache aligned ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index e597ecc8753c..ac7e79654f3a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -413,7 +413,7 @@ struct stack_canary {
 	char __pad[20];		/* canary at %gs:20 */
 	unsigned long canary;
 };
-DECLARE_PER_CPU(struct stack_canary, stack_canary) ____cacheline_aligned;
+DECLARE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif
 #endif	/* X86_64 */
 

commit 1ea0d14e480c245683927eecc03a70faf06e80c8
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Sep 3 12:27:15 2009 -0700

    x86/i386: Make sure stack-protector segment base is cache aligned
    
    The Intel Optimization Reference Guide says:
    
            In Intel Atom microarchitecture, the address generation unit
            assumes that the segment base will be 0 by default. Non-zero
            segment base will cause load and store operations to experience
            a delay.
                    - If the segment base isn't aligned to a cache line
                      boundary, the max throughput of memory operations is
                      reduced to one [e]very 9 cycles.
            [...]
            Assembly/Compiler Coding Rule 15. (H impact, ML generality)
            For Intel Atom processors, use segments with base set to 0
            whenever possible; avoid non-zero segment base address that is
            not aligned to cache line boundary at all cost.
    
    We can't avoid having a non-zero base for the stack-protector
    segment, but we can make it cache-aligned.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: <stable@kernel.org>
    LKML-Reference: <4AA01893.6000507@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c7768269b1cf..e597ecc8753c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -403,7 +403,17 @@ extern unsigned long kernel_eflags;
 extern asmlinkage void ignore_sysret(void);
 #else	/* X86_64 */
 #ifdef CONFIG_CC_STACKPROTECTOR
-DECLARE_PER_CPU(unsigned long, stack_canary);
+/*
+ * Make sure stack canary segment base is cached-aligned:
+ *   "For Intel Atom processors, avoid non zero segment base address
+ *    that is not aligned to cache line boundary at all cost."
+ * (Optim Ref Manual Assembly/Compiler Coding Rule 15.)
+ */
+struct stack_canary {
+	char __pad[20];		/* canary at %gs:20 */
+	unsigned long canary;
+};
+DECLARE_PER_CPU(struct stack_canary, stack_canary) ____cacheline_aligned;
 #endif
 #endif	/* X86_64 */
 

commit eadb8a091b27a840de7450f84ecff5ef13476424
Merge: 73874005cd88 65795efbd380
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 17 12:52:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/Kconfig
            arch/x86/kernel/traps.c
            arch/x86/power/cpu.c
            arch/x86/power/cpu_32.c
            kernel/Makefile
    
    Semantic conflict:
            arch/x86/kernel/hw_breakpoint.c
    
    Merge reason: Resolve the conflicts, move from put_cpu_no_sched() to
                  put_cpu() in arch/x86/kernel/hw_breakpoint.c.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 862366118026a358882eefc70238dbcc3db37aac
Merge: 57eee9ae7bbc 511b01bdf64a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:53:40 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (244 commits)
      Revert "x86, bts: reenable ptrace branch trace support"
      tracing: do not translate event helper macros in print format
      ftrace/documentation: fix typo in function grapher name
      tracing/events: convert block trace points to TRACE_EVENT(), fix !CONFIG_BLOCK
      tracing: add protection around module events unload
      tracing: add trace_seq_vprint interface
      tracing: fix the block trace points print size
      tracing/events: convert block trace points to TRACE_EVENT()
      ring-buffer: fix ret in rb_add_time_stamp
      ring-buffer: pass in lockdep class key for reader_lock
      tracing: add annotation to what type of stack trace is recorded
      tracing: fix multiple use of __print_flags and __print_symbolic
      tracing/events: fix output format of user stack
      tracing/events: fix output format of kernel stack
      tracing/trace_stack: fix the number of entries in the header
      ring-buffer: discard timestamps that are at the start of the buffer
      ring-buffer: try to discard unneeded timestamps
      ring-buffer: fix bug in ring_buffer_discard_commit
      ftrace: do not profile functions when disabled
      tracing: make trace pipe recognize latency format flag
      ...

commit 7dc3ca39cb1e22eedbf1207ff9ac7bf682fc0f6d
Merge: aa98936e4f42 a4046f8d299e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 15:49:36 2009 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, nmi: Use predefined numbers instead of hardcoded one
      x86: asm/processor.h: remove double declaration
      x86, mtrr: replace MTRRdefType_MSR with msr-index's MSR_MTRRdefType
      x86, mtrr: replace MTRRfix4K_C0000_MSR with msr-index's MSR_MTRRfix4K_C0000
      x86, mtrr: remove mtrr MSRs double declaration
      x86, mtrr: replace MTRRfix16K_80000_MSR with msr-index's MSR_MTRRfix16K_80000
      x86, mtrr: replace MTRRfix64K_00000_MSR with msr-index's MSR_MTRRfix64K_00000
      x86, mtrr: replace MTRRcap_MSR with msr-index's MSR_MTRRcap
      x86: mce: remove duplicated #include
      x86: msr-index.h remove duplicate MSR C001_0015 declaration
      x86: clean up arch/x86/kernel/tsc_sync.c a bit
      x86: use symbolic name for VM86_SIGNAL when used as vm86 default return
      x86: added 'ifndef _ASM_X86_IOMAP_H' to iomap.h
      x86: avoid multiple declaration of kstack_depth_to_print
      x86: vdso/vma.c declare vdso_enabled and arch_setup_additional_pages before they get used
      x86: clean up declarations and variables
      x86: apic/x2apic_cluster.c x86_cpu_to_logical_apicid should be static
      x86 early quirks: eliminate unused function

commit aa98936e4f423dc2706771368598b04870059d14
Merge: 082b63ae45e7 0c23590f00f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 15:49:10 2009 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, 64-bit: ifdef out struct thread_struct::ip
      x86, 32-bit: ifdef out struct thread_struct::fs
      x86: clean up alternative.h

commit 82782ca77d1bfb32b0334cce40a25b91bd8ec016
Merge: f0d5e12bd42b 6799687a53a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 15:30:41 2009 -0700

    Merge branch 'x86-kbuild-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-kbuild-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (46 commits)
      x86, boot: add new generated files to the appropriate .gitignore files
      x86, boot: correct the calculation of ZO_INIT_SIZE
      x86-64: align __PHYSICAL_START, remove __KERNEL_ALIGN
      x86, boot: correct sanity checks in boot/compressed/misc.c
      x86: add extension fields for bootloader type and version
      x86, defconfig: update kernel position parameters
      x86, defconfig: update to current, no material changes
      x86: make CONFIG_RELOCATABLE the default
      x86: default CONFIG_PHYSICAL_START and CONFIG_PHYSICAL_ALIGN to 16 MB
      x86: document new bzImage fields
      x86, boot: make kernel_alignment adjustable; new bzImage fields
      x86, boot: remove dead code from boot/compressed/head_*.S
      x86, boot: use LOAD_PHYSICAL_ADDR on 64 bits
      x86, boot: make symbols from the main vmlinux available
      x86, boot: determine compressed code offset at compile time
      x86, boot: use appropriate rep string for move and clear
      x86, boot: zero EFLAGS on 32 bits
      x86, boot: set up the decompression stack as early as possible
      x86, boot: straighten out ranges to copy/zero in compressed/head*.S
      x86, boot: stylistic cleanups for boot/compressed/head_64.S
      ...
    
    Fixed trivial conflict in arch/x86/configs/x86_64_defconfig manually

commit b332828c39326b1dca617f387dd15d12e81cd5f0
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:43:10 2009 +0530

    hw-breakpoints: prepare the code for Hardware Breakpoint interfaces
    
    The generic hardware breakpoint interface provides an abstraction of
    hardware breakpoints in front of specific arch implementations for both kernel
    and user side breakpoints.
    This includes execution breakpoints and read/write breakpoints, also known as
    "watchpoints".
    
    This patch introduces header files containing constants, structure definitions
    and declaration of functions used by the hardware breakpoint core and x86
    specific code.
    It also introduces an array based storage for the debug-register values in
    'struct thread_struct', while modifying all users of debugreg<n> member in the
    structure.
    
    [ Impact: add headers for new hardware breakpoint interface ]
    
    Original-patch-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Reviewed-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0b2fab0051e0..448b34a8e393 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -29,6 +29,7 @@ struct mm_struct;
 #include <linux/threads.h>
 #include <linux/init.h>
 
+#define HBP_NUM 4
 /*
  * Default implementation of macro that returns current
  * instruction pointer ("program counter").
@@ -431,12 +432,11 @@ struct thread_struct {
 	unsigned long		fs;
 	unsigned long		gs;
 	/* Hardware debugging registers: */
-	unsigned long		debugreg0;
-	unsigned long		debugreg1;
-	unsigned long		debugreg2;
-	unsigned long		debugreg3;
+	unsigned long		debugreg[HBP_NUM];
 	unsigned long		debugreg6;
 	unsigned long		debugreg7;
+	/* Hardware breakpoint info */
+	struct hw_breakpoint	*hbp[HBP_NUM];
 	/* Fault info: */
 	unsigned long		cr2;
 	unsigned long		trap_no;

commit 4aee2ad461889132bfb5a1518a9580d00e17008c
Author: Jaswinder Singh Rajput <jaswinder@kernel.org>
Date:   Tue May 19 17:07:01 2009 +0530

    x86: asm/processor.h: remove double declaration
    
    Remove double declaration of:
    
     extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
     extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
     extern unsigned short num_cache_leaves;
    
    they are already defined in the same file.
    
    [ Impact: cleanup ]
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    LKML-Reference: <1242733021.3377.1.camel@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 34c52370f2fe..85628ea9d9b5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -403,9 +403,6 @@ DECLARE_PER_CPU(unsigned long, stack_canary);
 extern unsigned int xstate_size;
 extern void free_thread_xstate(struct task_struct *);
 extern struct kmem_cache *task_xstate_cachep;
-extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
-extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
-extern unsigned short num_cache_leaves;
 
 struct thread_struct {
 	/* Cached TLS descriptors: */

commit 5031296c57024a78ddad4edfc993367dbf4abb98
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu May 7 16:54:11 2009 -0700

    x86: add extension fields for bootloader type and version
    
    A long ago, in days of yore, it all began with a god named Thor.
    There were vikings and boats and some plans for a Linux kernel
    header.  Unfortunately, a single 8-bit field was used for bootloader
    type and version.  This has generally worked without *too* much pain,
    but we're getting close to flat running out of ID fields.
    
    Add extension fields for both type and version.  The type will be
    extended if it the old field is 0xE; the version is a simple MSB
    extension.
    
    Keep /proc/sys/kernel/bootloader_type containing
    (type << 4) + (ver & 0xf) for backwards compatiblity, but also add
    /proc/sys/kernel/bootloader_version which contains the full version
    number.
    
    [ Impact: new feature to support more bootloaders ]
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index fcf4d92e7e04..6384d25121ca 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -814,6 +814,7 @@ extern unsigned int		BIOS_revision;
 
 /* Boot loader type from the setup header: */
 extern int			bootloader_type;
+extern int			bootloader_version;
 
 extern char			ignore_fpu_irq;
 

commit 0c23590f00f85467b318ad0c20c36796a5bd4c60
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 4 03:30:15 2009 +0400

    x86, 64-bit: ifdef out struct thread_struct::ip
    
    struct thread_struct::ip isn't used on x86_64, struct pt_regs::ip is used
    instead.
    
    kgdb should be reading 0 always, but I can't check it.
    
    [ Impact: (potentially) reduce thread_struct size on 64-bit ]
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: containers@lists.linux-foundation.org
    LKML-Reference: <20090503233015.GJ16631@x200.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index a6732ff7b016..a9ba7436821e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -427,7 +427,9 @@ struct thread_struct {
 	unsigned short		fsindex;
 	unsigned short		gsindex;
 #endif
+#ifdef CONFIG_X86_32
 	unsigned long		ip;
+#endif
 #ifdef CONFIG_X86_64
 	unsigned long		fs;
 #endif

commit d756f4adb9d8a86e347a2d5435bb5cc95744733e
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 4 03:29:52 2009 +0400

    x86, 32-bit: ifdef out struct thread_struct::fs
    
    After commit 464d1a78fbf8cf6c7fd970e7b3e2db50a320ce28 aka
    "[PATCH] i386: Convert i386 PDA code to use %fs"
    %fs saved during context switch moved from thread_struct to pt_regs
    and value on thread_struct became unused.
    
    [ Impact: reduce thread_struct size on 32-bit ]
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: containers@lists.linux-foundation.org
    LKML-Reference: <20090503232952.GI16631@x200.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c2cceae709c8..a6732ff7b016 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -428,7 +428,9 @@ struct thread_struct {
 	unsigned short		gsindex;
 #endif
 	unsigned long		ip;
+#ifdef CONFIG_X86_64
 	unsigned long		fs;
+#endif
 	unsigned long		gs;
 	/* Hardware debugging registers: */
 	unsigned long		debugreg0;
@@ -874,7 +876,6 @@ static inline void spin_lock_prefetch(const void *x)
 	.vm86_info		= NULL,					  \
 	.sysenter_cs		= __KERNEL_CS,				  \
 	.io_bitmap_ptr		= NULL,					  \
-	.fs			= __KERNEL_PERCPU,			  \
 }
 
 /*

commit 3e0c373749d7eb5b354ac0b043f2b2cdf84eefef
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat May 9 23:47:42 2009 -0700

    x86: clean up and fix setup_clear/force_cpu_cap handling
    
    setup_force_cpu_cap() only have one user (Xen guest code),
    but it should not reuse cleared_cpu_cpus, otherwise it
    will have problems on SMP.
    
    Need to have a separate cpu_cpus_set array too, for forced-on
    flags, beyond the forced-off flags.
    
    Also need to setup handling before all cpus caps are combined.
    
    [ Impact: fix the forced-set CPU feature flag logic ]
    
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Yinghai Lu <yinghai.lu@kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c2cceae709c8..fed93fec9764 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -135,7 +135,8 @@ extern struct cpuinfo_x86	boot_cpu_data;
 extern struct cpuinfo_x86	new_cpu_data;
 
 extern struct tss_struct	doublefault_tss;
-extern __u32			cleared_cpu_caps[NCAPINTS];
+extern __u32			cpu_caps_cleared[NCAPINTS];
+extern __u32			cpu_caps_set[NCAPINTS];
 
 #ifdef CONFIG_SMP
 DECLARE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);

commit 416dfdcdb894432547ead4fcb9fa6a36b396059e
Merge: 56449f437add 091069740304
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 24 10:11:18 2009 +0200

    Merge commit 'v2.6.30-rc3' into tracing/hw-branch-tracing
    
    Conflicts:
            arch/x86/kernel/ptrace.c
    
    Merge reason: fix the conflict above, and also pick up the CONFIG_BROKEN
                  dependency change from upstream so that we can remove it
                  here.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9b8de7479d0dbab1ed98b5b015d44232c9d3d08e
Author: David Howells <dhowells@redhat.com>
Date:   Tue Apr 21 23:00:24 2009 +0100

    FRV: Fix the section attribute on UP DECLARE_PER_CPU()
    
    In non-SMP mode, the variable section attribute specified by DECLARE_PER_CPU()
    does not agree with that specified by DEFINE_PER_CPU().  This means that
    architectures that have a small data section references relative to a base
    register may throw up linkage errors due to too great a displacement between
    where the base register points and the per-CPU variable.
    
    On FRV, the .h declaration says that the variable is in the .sdata section, but
    the .c definition says it's actually in the .data section.  The linker throws
    up the following errors:
    
    kernel/built-in.o: In function `release_task':
    kernel/exit.c:78: relocation truncated to fit: R_FRV_GPREL12 against symbol `per_cpu__process_counts' defined in .data section in kernel/built-in.o
    kernel/exit.c:78: relocation truncated to fit: R_FRV_GPREL12 against symbol `per_cpu__process_counts' defined in .data section in kernel/built-in.o
    
    To fix this, DECLARE_PER_CPU() should simply apply the same section attribute
    as does DEFINE_PER_CPU().  However, this is made slightly more complex by
    virtue of the fact that there are several variants on DEFINE, so these need to
    be matched by variants on DECLARE.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index fcf4d92e7e04..c2cceae709c8 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -138,7 +138,7 @@ extern struct tss_struct	doublefault_tss;
 extern __u32			cleared_cpu_caps[NCAPINTS];
 
 #ifdef CONFIG_SMP
-DECLARE_PER_CPU(struct cpuinfo_x86, cpu_info);
+DECLARE_PER_CPU_SHARED_ALIGNED(struct cpuinfo_x86, cpu_info);
 #define cpu_data(cpu)		per_cpu(cpu_info, cpu)
 #define current_cpu_data	__get_cpu_var(cpu_info)
 #else
@@ -270,7 +270,7 @@ struct tss_struct {
 
 } ____cacheline_aligned;
 
-DECLARE_PER_CPU(struct tss_struct, init_tss);
+DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss);
 
 /*
  * Save the original ist values for checking stack pointers during debugging
@@ -393,7 +393,7 @@ union irq_stack_union {
 	};
 };
 
-DECLARE_PER_CPU(union irq_stack_union, irq_stack_union);
+DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union);
 DECLARE_INIT_PER_CPU(irq_stack_union);
 
 DECLARE_PER_CPU(char *, irq_stack_ptr);

commit a30469e7921a6dd2067e9e836d7787cfa0105627
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Apr 10 15:21:24 2009 -0700

    x86: add linux kernel support for YMM state
    
    Impact: save/restore Intel-AVX state properly between tasks
    
    Intel Advanced Vector Extensions (AVX) introduce 256-bit vector processing
    capability. More about AVX at http://software.intel.com/sites/avx
    
    Add OS support for YMM state management using xsave/xrstor infrastructure
    to support AVX.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1239402084.27006.8057.camel@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 34c52370f2fe..fcf4d92e7e04 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -352,6 +352,11 @@ struct i387_soft_struct {
 	u32			entry_eip;
 };
 
+struct ymmh_struct {
+	/* 16 * 16 bytes for each YMMH-reg = 256 bytes */
+	u32 ymmh_space[64];
+};
+
 struct xsave_hdr_struct {
 	u64 xstate_bv;
 	u64 reserved1[2];
@@ -361,6 +366,7 @@ struct xsave_hdr_struct {
 struct xsave_struct {
 	struct i387_fxsave_struct i387;
 	struct xsave_hdr_struct xsave_hdr;
+	struct ymmh_struct ymmh;
 	/* new processor state extensions will go here */
 } __attribute__ ((packed, aligned (64)));
 

commit 2311f0de21c17b2a8b960677a9cccfbfa52beb35
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:46 2009 +0200

    x86, ds: add leakage warning
    
    Add a warning in case a debug store context is not removed before
    the task it is attached to is freed.
    
    Remove the old warning at thread exit. It is too early.
    
    Declare the debug store context field in thread_struct unconditionally.
    
    Remove ds_copy_thread() and ds_exit_thread() and do the work directly
    in process*.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: oleg@redhat.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144601.254472000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1efeb497f1f9..7c39de7e709a 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -454,10 +454,8 @@ struct thread_struct {
 	unsigned		io_bitmap_max;
 /* MSR_IA32_DEBUGCTLMSR value to switch in if TIF_DEBUGCTLMSR is set.  */
 	unsigned long	debugctlmsr;
-#ifdef CONFIG_X86_DS
-/* Debug Store context; see include/asm-x86/ds.h; goes into MSR_IA32_DS_AREA */
+	/* Debug Store context; see asm/ds.h */
 	struct ds_context	*ds_ctx;
-#endif /* CONFIG_X86_DS */
 };
 
 static inline unsigned long native_get_debugreg(int regno)

commit 35bb7600c17762bb129588c1877d2717fe325289
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:39 2009 +0200

    x86, debugctlmsr: add _on_cpu variants to debugctlmsr functions
    
    Add functions to get and set the debugctlmsr on different cpus.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: oleg@redhat.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144554.738772000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 2483807e06e7..1efeb497f1f9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -785,6 +785,21 @@ static inline unsigned long get_debugctlmsr(void)
     return debugctlmsr;
 }
 
+static inline unsigned long get_debugctlmsr_on_cpu(int cpu)
+{
+	u64 debugctlmsr = 0;
+	u32 val1, val2;
+
+#ifndef CONFIG_X86_DEBUGCTLMSR
+	if (boot_cpu_data.x86 < 6)
+		return 0;
+#endif
+	rdmsr_on_cpu(cpu, MSR_IA32_DEBUGCTLMSR, &val1, &val2);
+	debugctlmsr = val1 | ((u64)val2 << 32);
+
+	return debugctlmsr;
+}
+
 static inline void update_debugctlmsr(unsigned long debugctlmsr)
 {
 #ifndef CONFIG_X86_DEBUGCTLMSR
@@ -794,6 +809,18 @@ static inline void update_debugctlmsr(unsigned long debugctlmsr)
 	wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);
 }
 
+static inline void update_debugctlmsr_on_cpu(int cpu,
+					     unsigned long debugctlmsr)
+{
+#ifndef CONFIG_X86_DEBUGCTLMSR
+	if (boot_cpu_data.x86 < 6)
+		return;
+#endif
+	wrmsr_on_cpu(cpu, MSR_IA32_DEBUGCTLMSR,
+		     (u32)((u64)debugctlmsr),
+		     (u32)((u64)debugctlmsr >> 32));
+}
+
 /*
  * from system description table in BIOS. Mostly for MCA use, but
  * others may find it useful:

commit e2b371f00a6f529f6362654239bdec8dcd510760
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:35 2009 +0200

    mm, x86, ptrace, bts: defer branch trace stopping
    
    When a ptraced task is unlinked, we need to stop branch tracing for
    that task.
    
    Since the unlink is called with interrupts disabled, and we need
    interrupts enabled to stop branch tracing, we defer the work.
    
    Collect all branch tracing related stuff in a branch tracing context.
    
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144550.712401000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 34c52370f2fe..2483807e06e7 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -458,10 +458,6 @@ struct thread_struct {
 /* Debug Store context; see include/asm-x86/ds.h; goes into MSR_IA32_DS_AREA */
 	struct ds_context	*ds_ctx;
 #endif /* CONFIG_X86_DS */
-#ifdef CONFIG_X86_PTRACE_BTS
-/* the signal to send on a bts buffer overflow */
-	unsigned int	bts_ovfl_signal;
-#endif /* CONFIG_X86_PTRACE_BTS */
 };
 
 static inline unsigned long native_get_debugreg(int regno)

commit 65fb0d23fcddd8697c871047b700c78817bdaa43
Merge: 8c083f081d00 dfbbe89e197a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 30 23:53:32 2009 +0200

    Merge branch 'linus' into cpumask-for-linus
    
    Conflicts:
            arch/x86/kernel/cpu/common.c

commit 30e1e6d1af2b67558bccf322af2b3e0676b209ae
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Mar 17 14:50:34 2009 +1030

    cpumask: fix CONFIG_CPUMASK_OFFSTACK=y cpu hotunplug crash
    
    Impact: Fix cpu offline when CONFIG_MAXSMP=y
    
    Changeset bc9b83dd1f66402b870301c3c7117b9c1484abb4 "cpumask: convert
    c1e_mask in arch/x86/kernel/process.c to cpumask_var_t" contained a
    bug: c1e_mask is manipulated even if C1E isn't detected (and hence
    not allocated).
    
    This is simply fixed by checking for NULL (which gcc optimizes out
    anyway of CONFIG_CPUMASK_OFFSTACK=n, since it knows ce1_mask can never
    be NULL).
    
    In addition, fix a leak where select_idle_routine re-allocates
    (and re-clears) c1e_mask on every cpu init.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Mike Travis <travis@sgi.com>
    LKML-Reference: <200903171450.34549.rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index d794d9483c56..9874dd98a29f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -733,6 +733,7 @@ static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
 extern void mwait_idle_with_hints(unsigned long eax, unsigned long ecx);
 
 extern void select_idle_routine(const struct cpuinfo_x86 *c);
+extern void init_c1e_mask(void);
 
 extern unsigned long		boot_option_idle_override;
 extern unsigned long		idle_halt;

commit 0ca0f16fd17c5d880dd0abbe03595b0c7c5b3c95
Merge: c550033ced48 7a81d9a7da03 88200bc28da3 0f3fa48a7eaf 91219bcbdccc 063402356280 773e673de272 5a8ac9d28dae 16a6791934a1 895791dac694
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Mar 14 16:25:40 2009 +0100

    Merge branches 'x86/apic', 'x86/asm', 'x86/cleanups', 'x86/debug', 'x86/kconfig', 'x86/mm', 'x86/ptrace', 'x86/setup' and 'x86/urgent'; commit 'v2.6.29-rc8' into x86/core

commit 9766cdbcb260389669e9679b2aa87c11832f479f
Author: Jaswinder Singh Rajput <jaswinder@kernel.org>
Date:   Sat Mar 14 11:19:49 2009 +0530

    x86: cpu/common.c cleanups
    
    - fix various style problems
     - declare varibles before they get used
     - introduced clear_all_debug_regs
     - fix header files issues
    
    LKML-Reference: <1237009789.4387.2.camel@localhost.localdomain>
    Signed-off-by: Jaswinder Singh Rajput <jaswinder@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 76139506c3e4..dccef5be0fc1 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -391,6 +391,9 @@ DECLARE_PER_CPU(union irq_stack_union, irq_stack_union);
 DECLARE_INIT_PER_CPU(irq_stack_union);
 
 DECLARE_PER_CPU(char *, irq_stack_ptr);
+DECLARE_PER_CPU(unsigned int, irq_count);
+extern unsigned long kernel_eflags;
+extern asmlinkage void ignore_sysret(void);
 #else	/* X86_64 */
 #ifdef CONFIG_CC_STACKPROTECTOR
 DECLARE_PER_CPU(unsigned long, stack_canary);

commit 155dd720d06a219ddf5a56b473cb3325441fc879
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:53 2009 +1030

    cpumask: convert struct cpuinfo_x86's llc_shared_map to cpumask_var_t
    
    Impact: reduce kernel memory usage when CONFIG_CPUMASK_OFFSTACK=y
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 76139506c3e4..d794d9483c56 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -94,7 +94,7 @@ struct cpuinfo_x86 {
 	unsigned long		loops_per_jiffy;
 #ifdef CONFIG_SMP
 	/* cpus sharing the last level cache: */
-	cpumask_t		llc_shared_map;
+	cpumask_var_t		llc_shared_map;
 #endif
 	/* cpuid returned max cores value: */
 	u16			 x86_max_cores;

commit 13c6c53282d99c82e79b02477efd2c1e30a991ef
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Mar 12 12:37:34 2009 +0000

    x86, 32-bit: also use cpuinfo_x86's x86_{phys,virt}_bits members
    
    Impact: 32/64-bit consolidation
    
    In a first step, this allows fixing phys_addr_valid() for PAE (which
    until now reported all addresses to be valid). Subsequently, this will
    also allow simplifying some MTRR handling code.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <49B9101E.76E4.0078.0@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 76139506c3e4..bd3406db1d68 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -75,9 +75,9 @@ struct cpuinfo_x86 {
 #else
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
 	int			x86_tlbsize;
+#endif
 	__u8			x86_virt_bits;
 	__u8			x86_phys_bits;
-#endif
 	/* CPUID returned core id bits: */
 	__u8			x86_coreid_bits;
 	/* Max extended CPUID function supported: */

commit db949bba3c7cf2e664ac12e237c6d4c914f0c69d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 27 13:25:21 2009 -0800

    x86-32: use non-lazy io bitmap context switching
    
    Impact: remove 32-bit optimization to prepare unification
    
    x86-32 and -64 differ in the way they context-switch tasks
    with io permission bitmaps.  x86-64 simply copies the next
    tasks io bitmap into place (if any) on context switch.  x86-32
    invalidates the bitmap on context switch, so that the next
    IO instruction will fault; at that point it installs the
    appropriate IO bitmap.
    
    This makes context switching IO-bitmap-using tasks a bit more
    less expensive, at the cost of making the next IO instruction
    slower due to the extra fault.  This tradeoff only makes sense
    if IO-bitmap-using processes are relatively common, but they
    don't actually use IO instructions very often.
    
    However, in a typical desktop system, the only process likely
    to be using IO bitmaps is the X server, and nothing at all on
    a server.  Therefore the lazy context switch doesn't really win
    all that much, and its just a gratuitious difference from
    64-bit code.
    
    This patch removes the lazy context switch, with a view to
    unifying this code in a later change.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index c7a98f738210..76139506c3e4 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -248,7 +248,6 @@ struct x86_hw_tss {
 #define IO_BITMAP_LONGS			(IO_BITMAP_BYTES/sizeof(long))
 #define IO_BITMAP_OFFSET		offsetof(struct tss_struct, io_bitmap)
 #define INVALID_IO_BITMAP_OFFSET	0x8000
-#define INVALID_IO_BITMAP_OFFSET_LAZY	0x9000
 
 struct tss_struct {
 	/*
@@ -263,11 +262,6 @@ struct tss_struct {
 	 * be within the limit.
 	 */
 	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
-	/*
-	 * Cache the current maximum and the last task that used the bitmap:
-	 */
-	unsigned long		io_bitmap_max;
-	struct thread_struct	*io_bitmap_owner;
 
 	/*
 	 * .. and then another 0x100 bytes for the emergency kernel stack:

commit d951734654f76a370a89b4e531af9b765bd13541
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 20 23:32:28 2009 +0100

    x86, mm: rename TASK_SIZE64 => TASK_SIZE_MAX
    
    Impact: cleanup
    
    Rename TASK_SIZE64 to TASK_SIZE_MAX, and provide the
    define on 32-bit too. (mapped to TASK_SIZE)
    
    This allows 32-bit code to make use of the (former-) TASK_SIZE64
    symbol as well, in a clean way.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 72914d0315e9..c7a98f738210 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -861,6 +861,7 @@ static inline void spin_lock_prefetch(const void *x)
  * User space process size: 3GB (default).
  */
 #define TASK_SIZE		PAGE_OFFSET
+#define TASK_SIZE_MAX		TASK_SIZE
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		STACK_TOP
 
@@ -920,7 +921,7 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 /*
  * User space process size. 47bits minus one guard page.
  */
-#define TASK_SIZE64	((1UL << 47) - PAGE_SIZE)
+#define TASK_SIZE_MAX	((1UL << 47) - PAGE_SIZE)
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap's.
@@ -929,12 +930,12 @@ extern unsigned long thread_saved_pc(struct task_struct *tsk);
 					0xc0000000 : 0xFFFFe000)
 
 #define TASK_SIZE		(test_thread_flag(TIF_IA32) ? \
-					IA32_PAGE_OFFSET : TASK_SIZE64)
+					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_IA32)) ? \
-					IA32_PAGE_OFFSET : TASK_SIZE64)
+					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
 #define STACK_TOP		TASK_SIZE
-#define STACK_TOP_MAX		TASK_SIZE64
+#define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  { \
 	.sp0 = (unsigned long)&init_stack + sizeof(init_stack) \

commit de5483029b8f18e23395d8fd4f4ef6ae15beb809
Author: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
Date:   Thu Feb 19 12:20:17 2009 +0530

    x86: include/asm/processor.h remove double declaration of print_cpu_info
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index dabab1a19ddd..72914d0315e9 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -403,7 +403,6 @@ DECLARE_PER_CPU(unsigned long, stack_canary);
 #endif
 #endif	/* X86_64 */
 
-extern void print_cpu_info(struct cpuinfo_x86 *);
 extern unsigned int xstate_size;
 extern void free_thread_xstate(struct task_struct *);
 extern struct kmem_cache *task_xstate_cachep;

commit b233969eaa98c7b339d955fe25a58bf6bf25739a
Merge: d040c1614c24 54321d947ae9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 13:09:00 2009 +0100

    Merge branch 'x86/untangle2' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen into x86/headers
    
    Conflicts:
            arch/x86/include/asm/page.h
            arch/x86/include/asm/pgtable.h
            arch/x86/mach-voyager/voyager_smp.c
            arch/x86/mm/fault.c

commit ab639f3593f0b5e4439d549831442c18c3baf989
Merge: f8a6b2b9cee2 58105ef18571
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:45:09 2009 +0100

    Merge branch 'core/percpu' into x86/core

commit f8a6b2b9cee298a9663cbe38ce1eb5240987cb62
Merge: ba1511bf7fbd 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:44:22 2009 +0100

    Merge branch 'linus' into x86/apic
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c
            arch/x86/mm/fault.c

commit 54321d947ae9d6a051b81e3eccaf2d8658aeecc6
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Feb 11 10:20:05 2009 -0800

    x86: move pte types into pgtable*.h
    
    pgtable*.h is intended for definitions relating to actual pagetables
    and their entries, so move all the definitions for
    (pte|pmd|pud|pgd)(val)?_t to the appropriate pgtable*.h headers.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 656d02ea509b..963028807744 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -16,6 +16,7 @@ struct mm_struct;
 #include <asm/cpufeature.h>
 #include <asm/system.h>
 #include <asm/page.h>
+#include <asm/pgtable_types.h>
 #include <asm/percpu.h>
 #include <asm/msr.h>
 #include <asm/desc_defs.h>

commit 60a5317ff0f42dd313094b88f809f63041568b08
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 9 22:17:40 2009 +0900

    x86: implement x86_32 stack protector
    
    Impact: stack protector for x86_32
    
    Implement stack protector for x86_32.  GDT entry 28 is used for it.
    It's set to point to stack_canary-20 and have the length of 24 bytes.
    CONFIG_CC_STACKPROTECTOR turns off CONFIG_X86_32_LAZY_GS and sets %gs
    to the stack canary segment on entry.  As %gs is otherwise unused by
    the kernel, the canary can be anywhere.  It's defined as a percpu
    variable.
    
    x86_32 exception handlers take register frame on stack directly as
    struct pt_regs.  With -fstack-protector turned on, gcc copies the
    whole structure after the stack canary and (of course) doesn't copy
    back on return thus losing all changed.  For now, -fno-stack-protector
    is added to all files which contain those functions.  We definitely
    need something better.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 9763eb700138..5a9472104253 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -396,7 +396,11 @@ DECLARE_PER_CPU(union irq_stack_union, irq_stack_union);
 DECLARE_INIT_PER_CPU(irq_stack_union);
 
 DECLARE_PER_CPU(char *, irq_stack_ptr);
+#else	/* X86_64 */
+#ifdef CONFIG_CC_STACKPROTECTOR
+DECLARE_PER_CPU(unsigned long, stack_canary);
 #endif
+#endif	/* X86_64 */
 
 extern void print_cpu_info(struct cpuinfo_x86 *);
 extern unsigned int xstate_size;

commit 92e2d508464b6293ad274fb606f766a458894142
Merge: 5d96218b4a5e d315760ffa26
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 10 00:41:02 2009 +0100

    Merge branch 'x86/urgent' into core/percpu
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit ae6af41f5a4841f06eb92bc86ad020ad44ae2a30
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 9 22:17:39 2009 +0900

    x86: math_emu info cleanup
    
    Impact: cleanup
    
    * Come on, struct info?  s/struct info/struct math_emu_info/
    
    * Use struct pt_regs and kernel_vm86_regs instead of defining its own
      register frame structure.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 091cd8855f2e..3bfd5235a9eb 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -353,7 +353,7 @@ struct i387_soft_struct {
 	u8			no_update;
 	u8			rm;
 	u8			alimit;
-	struct info		*info;
+	struct math_emu_info	*info;
 	u32			entry_eip;
 };
 

commit eca217b36e5d7d4377493d5cedd89105e66a5a72
Merge: 54a353a0f845 e4d0407185cd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 9 12:16:59 2009 +0100

    Merge branch 'x86/paravirt' into x86/apic
    
    Conflicts:
            arch/x86/mach-voyager/voyager_smp.c

commit 2add8e235cbe0dcd672c33fc322754e15500238c
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Feb 8 09:58:39 2009 -0500

    x86: use linker to offset symbols by __per_cpu_load
    
    Impact: cleanup and bug fix
    
    Use the linker to create symbols for certain per-cpu variables
    that are offset by __per_cpu_load.  This allows the removal of
    the runtime fixup of the GDT pointer, which fixes a bug with
    resume reported by Jiri Slaby.
    
    Reported-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 656d02ea509b..373d3f628d24 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -393,6 +393,8 @@ union irq_stack_union {
 };
 
 DECLARE_PER_CPU(union irq_stack_union, irq_stack_union);
+DECLARE_INIT_PER_CPU(irq_stack_union);
+
 DECLARE_PER_CPU(char *, irq_stack_ptr);
 #endif
 

commit 11e3a840cd5b731cdd8f6f956dfae78a8046d09c
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Jan 30 17:47:54 2009 +0900

    x86: split loading percpu segments from loading gdt
    
    Impact: split out a function, no functional change
    
    Xen needs to be able to access percpu data from very early on.  For
    various reasons, it cannot also load the gdt at that time.   It does,
    however, have a pefectly functional gdt at that point, so there's no
    pressing need to reload the gdt.
    
    Split the function to load the segment registers off, so Xen can call
    it directly.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 1c25eb69ea86..656d02ea509b 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -769,6 +769,7 @@ extern struct desc_ptr		early_gdt_descr;
 
 extern void cpu_set_gdt(int);
 extern void switch_to_new_gdt(int);
+extern void load_percpu_segment(int);
 extern void cpu_init(void);
 
 static inline unsigned long get_debugctlmsr(void)

commit 552be871e67ff577ed36beb2f53d078b42304739
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Jan 30 17:47:53 2009 +0900

    x86: pass in cpu number to switch_to_new_gdt()
    
    Impact: cleanup, prepare for xen boot fix.
    
    Xen needs to call this function very early to setup the GDT and
    per-cpu segments.  Remove the call to smp_processor_id() and just
    pass in the cpu number.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index befa20b4a68c..1c25eb69ea86 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -768,7 +768,7 @@ extern int sysenter_setup(void);
 extern struct desc_ptr		early_gdt_descr;
 
 extern void cpu_set_gdt(int);
-extern void switch_to_new_gdt(void);
+extern void switch_to_new_gdt(int);
 extern void cpu_init(void);
 
 static inline unsigned long get_debugctlmsr(void)

commit 74b6eb6b937df07d0757e8642b7538b07da4290f
Merge: 6a385db5ce7f 2d4d57db692e 8f6d86dc4178 b38b06659055 d5e397cb49b5 e56d0cfe7790 dbca1df48e89 fb746d0e1365 6522869c3466 d639bab8da86 042cbaf88ab4 5662a2f8e731 3b4b75700a24 30a0fb947a68
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 23:13:53 2009 +0100

    Merge branches 'x86/asm', 'x86/cleanups', 'x86/cpudetect', 'x86/debug', 'x86/doc', 'x86/header-fixes', 'x86/mm', 'x86/paravirt', 'x86/pat', 'x86/setup-v2', 'x86/subarch', 'x86/uaccess' and 'x86/urgent' into x86/core

commit 2697fbd5faf19c84c17441b1752bdcbdcfd1248c
Author: Brian Gerst <brgerst@gmail.com>
Date:   Tue Jan 27 12:56:48 2009 +0900

    x86: load new GDT after setting up boot cpu per-cpu area
    
    Impact: sync 32 and 64-bit code
    
    Merge load_gs_base() into switch_to_new_gdt().  Load the GDT and
    per-cpu state for the boot cpu when its new area is set up.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 794234eba317..befa20b4a68c 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -394,11 +394,6 @@ union irq_stack_union {
 
 DECLARE_PER_CPU(union irq_stack_union, irq_stack_union);
 DECLARE_PER_CPU(char *, irq_stack_ptr);
-
-static inline void load_gs_base(int cpu)
-{
-	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
-}
 #endif
 
 extern void print_cpu_info(struct cpuinfo_x86 *);

commit 1825b8edc2034c012ae48f797d74efd1bd9d4f72
Author: Brian Gerst <brgerst@gmail.com>
Date:   Tue Jan 27 12:56:48 2009 +0900

    x86: remove extra barriers from load_gs_base()
    
    Impact: optimization
    
    mb() generates an mfence instruction, which is not needed here.  Only
    a compiler barrier is needed, and that is handled by the memory clobber
    in the wrmsrl function.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 32c30b02b51f..794234eba317 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -397,10 +397,7 @@ DECLARE_PER_CPU(char *, irq_stack_ptr);
 
 static inline void load_gs_base(int cpu)
 {
-	/* Memory clobbers used to order pda/percpu accesses */
-	mb();
 	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
-	mb();
 }
 #endif
 

commit b2d2f4312b117a6cc647c8521e2643a88771f757
Author: Brian Gerst <brgerst@gmail.com>
Date:   Tue Jan 27 12:56:48 2009 +0900

    x86: initialize per-cpu GDT segment in per-cpu setup
    
    Impact: cleanup
    
    Rename init_gdt() to setup_percpu_segment(), and move it to
    setup_percpu.c.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 48676b943b92..32c30b02b51f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -778,7 +778,6 @@ extern struct desc_ptr		early_gdt_descr;
 extern void cpu_set_gdt(int);
 extern void switch_to_new_gdt(void);
 extern void cpu_init(void);
-extern void init_gdt(int cpu);
 
 static inline unsigned long get_debugctlmsr(void)
 {

commit b1882e68d17a93b523dce09c3a181319aace2f0e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Jan 23 17:18:52 2009 -0800

    x86: clean up stray space in <asm/processor.h>
    
    Impact: Whitespace cleanup only
    
    Clean up a stray space character in arch/x86/include/asm/processor.h.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 091cd8855f2e..ac8fab3b868f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -73,7 +73,7 @@ struct cpuinfo_x86 {
 	char			pad0;
 #else
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
-	int			 x86_tlbsize;
+	int			x86_tlbsize;
 	__u8			x86_virt_bits;
 	__u8			x86_phys_bits;
 #endif

commit 947e76cdc34c782fc947313d4331380686eebbad
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 12:21:28 2009 +0900

    x86: move stack_canary into irq_stack
    
    Impact: x86_64 percpu area layout change, irq_stack now at the beginning
    
    Now that the PDA is empty except for the stack canary, it can be removed.
    The irqstack is moved to the start of the per-cpu section.  If the stack
    protector is enabled, the canary overlaps the bottom 48 bytes of the irqstack.
    
    tj: * updated subject
        * dropped asm relocation of irq_stack_ptr
        * updated comments a bit
        * rebased on top of stack canary changes
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index f511246fa6cd..48676b943b92 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -379,8 +379,29 @@ union thread_xstate {
 #ifdef CONFIG_X86_64
 DECLARE_PER_CPU(struct orig_ist, orig_ist);
 
-DECLARE_PER_CPU(char[IRQ_STACK_SIZE], irq_stack);
+union irq_stack_union {
+	char irq_stack[IRQ_STACK_SIZE];
+	/*
+	 * GCC hardcodes the stack canary as %gs:40.  Since the
+	 * irq_stack is the object at %gs:0, we reserve the bottom
+	 * 48 bytes of the irq stack for the canary.
+	 */
+	struct {
+		char gs_base[40];
+		unsigned long stack_canary;
+	};
+};
+
+DECLARE_PER_CPU(union irq_stack_union, irq_stack_union);
 DECLARE_PER_CPU(char *, irq_stack_ptr);
+
+static inline void load_gs_base(int cpu)
+{
+	/* Memory clobbers used to order pda/percpu accesses */
+	mb();
+	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
+	mb();
+}
 #endif
 
 extern void print_cpu_info(struct cpuinfo_x86 *);

commit 26f80bd6a9ab17bc8a60b6092e7c0d05c5927ce5
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Convert irqstacks to per-cpu
    
    Move the irqstackptr variable from the PDA to per-cpu.  Make the
    stacks themselves per-cpu, removing some specific allocation code.
    Add a seperate flag (is_boot_cpu) to simplify the per-cpu boot
    adjustments.
    
    tj: * sprinkle some underbars around.
    
        * irq_stack_ptr is not used till traps_init(), no reason to
          initialize it early.  On SMP, just leaving it NULL till proper
          initialization in setup_per_cpu_areas() works.  Dropped
          is_boot_cpu and early irq_stack_ptr initialization.
    
        * do DECLARE/DEFINE_PER_CPU(char[IRQ_STACK_SIZE], irq_stack)
          instead of (char, irq_stack[IRQ_STACK_SIZE]).
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 091cd8855f2e..f511246fa6cd 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -378,6 +378,9 @@ union thread_xstate {
 
 #ifdef CONFIG_X86_64
 DECLARE_PER_CPU(struct orig_ist, orig_ist);
+
+DECLARE_PER_CPU(char[IRQ_STACK_SIZE], irq_stack);
+DECLARE_PER_CPU(char *, irq_stack_ptr);
 #endif
 
 extern void print_cpu_info(struct cpuinfo_x86 *);

commit b0f4b285d7ed174804658539129a834270f4829a
Merge: be9c5ae4eeec 5250d329e38c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:21:10 2008 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (241 commits)
      sched, trace: update trace_sched_wakeup()
      tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
      Revert "x86: disable X86_PTRACE_BTS"
      ring-buffer: prevent false positive warning
      ring-buffer: fix dangling commit race
      ftrace: enable format arguments checking
      x86, bts: memory accounting
      x86, bts: add fork and exit handling
      ftrace: introduce tracing_reset_online_cpus() helper
      tracing: fix warnings in kernel/trace/trace_sched_switch.c
      tracing: fix warning in kernel/trace/trace.c
      tracing/ring-buffer: remove unused ring_buffer size
      trace: fix task state printout
      ftrace: add not to regex on filtering functions
      trace: better use of stack_trace_enabled for boot up code
      trace: add a way to enable or disable the stack tracer
      x86: entry_64 - introduce FTRACE_ frame macro v2
      tracing/ftrace: add the printk-msg-only option
      tracing/ftrace: use preempt_enable_no_resched_notrace in ring_buffer_time_stamp()
      x86, bts: correctly report invalid bts records
      ...
    
    Fixed up trivial conflict in scripts/recordmcount.pl due to SH bits
    being already partly merged by the SH merge.

commit c2724775ce57c98b8af9694857b941dc61056516
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Thu Dec 11 13:49:59 2008 +0100

    x86, bts: provide in-kernel branch-trace interface
    
    Impact: cleanup
    
    Move the BTS bits from ptrace.c into ds.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 5ca01e383269..aa5914f8e501 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -752,6 +752,19 @@ extern void switch_to_new_gdt(void);
 extern void cpu_init(void);
 extern void init_gdt(int cpu);
 
+static inline unsigned long get_debugctlmsr(void)
+{
+    unsigned long debugctlmsr = 0;
+
+#ifndef CONFIG_X86_DEBUGCTLMSR
+	if (boot_cpu_data.x86 < 6)
+		return 0;
+#endif
+	rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);
+
+    return debugctlmsr;
+}
+
 static inline void update_debugctlmsr(unsigned long debugctlmsr)
 {
 #ifndef CONFIG_X86_DEBUGCTLMSR

commit 88b094fb8d4fe43b7025ea8d487059e8813e02cd
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Oct 27 10:41:46 2008 -0700

    x86: Hypervisor detection and get tsc_freq from hypervisor
    
    Impact: Changes timebase calibration on Vmware.
    
    v3->v2 : Abstract the hypervisor detection and feature (tsc_freq) request
             behind a hypervisor.c file
    v2->v1 : Add a x86_hyper_vendor field to the cpuinfo_x86 structure.
             This avoids multiple calls to the hypervisor detection function.
    
    This patch adds function to detect if we are running under VMware.
    The current way to check if we are on VMware is following,
    #  check if "hypervisor present bit" is set, if so read the 0x40000000
       cpuid leaf and check for "VMwareVMware" signature.
    #  if the above fails, check the DMI vendors name for "VMware" string
       if we find one we query the VMware hypervisor port to check if we are
       under VMware.
    
    The DMI + "VMware hypervisor port check" is needed for older VMware products,
    which don't implement the hypervisor signature cpuid leaf.
    Also note that since we are checking for the DMI signature the hypervisor
    port should never be accessed on native hardware.
    
    This patch also adds a hypervisor_get_tsc_freq function, instead of
    calibrating the frequency which can be error prone in virtualized
    environment, we ask the hypervisor for it. We get the frequency from
    the hypervisor by accessing the hypervisor port if we are running on VMware.
    Other hypervisors too can add code to the generic routine to get frequency on
    their platform.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 5ca01e383269..a570eafa4755 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -110,6 +110,7 @@ struct cpuinfo_x86 {
 	/* Index into per_cpu list: */
 	u16			cpu_index;
 #endif
+	unsigned int		x86_hyper_vendor;
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 
 #define X86_VENDOR_INTEL	0
@@ -123,6 +124,9 @@ struct cpuinfo_x86 {
 
 #define X86_VENDOR_UNKNOWN	0xff
 
+#define X86_HYPER_VENDOR_NONE  0
+#define X86_HYPER_VENDOR_VMWARE 1
+
 /*
  * capabilities of CPUs
  */

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ee7cbb30773a..5ca01e383269 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -1,5 +1,5 @@
-#ifndef ASM_X86__PROCESSOR_H
-#define ASM_X86__PROCESSOR_H
+#ifndef _ASM_X86_PROCESSOR_H
+#define _ASM_X86_PROCESSOR_H
 
 #include <asm/processor-flags.h>
 
@@ -933,4 +933,4 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
-#endif /* ASM_X86__PROCESSOR_H */
+#endif /* _ASM_X86_PROCESSOR_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
new file mode 100644
index 000000000000..ee7cbb30773a
--- /dev/null
+++ b/arch/x86/include/asm/processor.h
@@ -0,0 +1,936 @@
+#ifndef ASM_X86__PROCESSOR_H
+#define ASM_X86__PROCESSOR_H
+
+#include <asm/processor-flags.h>
+
+/* Forward declaration, a strange C thing */
+struct task_struct;
+struct mm_struct;
+
+#include <asm/vm86.h>
+#include <asm/math_emu.h>
+#include <asm/segment.h>
+#include <asm/types.h>
+#include <asm/sigcontext.h>
+#include <asm/current.h>
+#include <asm/cpufeature.h>
+#include <asm/system.h>
+#include <asm/page.h>
+#include <asm/percpu.h>
+#include <asm/msr.h>
+#include <asm/desc_defs.h>
+#include <asm/nops.h>
+#include <asm/ds.h>
+
+#include <linux/personality.h>
+#include <linux/cpumask.h>
+#include <linux/cache.h>
+#include <linux/threads.h>
+#include <linux/init.h>
+
+/*
+ * Default implementation of macro that returns current
+ * instruction pointer ("program counter").
+ */
+static inline void *current_text_addr(void)
+{
+	void *pc;
+
+	asm volatile("mov $1f, %0; 1:":"=r" (pc));
+
+	return pc;
+}
+
+#ifdef CONFIG_X86_VSMP
+# define ARCH_MIN_TASKALIGN		(1 << INTERNODE_CACHE_SHIFT)
+# define ARCH_MIN_MMSTRUCT_ALIGN	(1 << INTERNODE_CACHE_SHIFT)
+#else
+# define ARCH_MIN_TASKALIGN		16
+# define ARCH_MIN_MMSTRUCT_ALIGN	0
+#endif
+
+/*
+ *  CPU type and hardware bug flags. Kept separately for each CPU.
+ *  Members of this structure are referenced in head.S, so think twice
+ *  before touching them. [mj]
+ */
+
+struct cpuinfo_x86 {
+	__u8			x86;		/* CPU family */
+	__u8			x86_vendor;	/* CPU vendor */
+	__u8			x86_model;
+	__u8			x86_mask;
+#ifdef CONFIG_X86_32
+	char			wp_works_ok;	/* It doesn't on 386's */
+
+	/* Problems on some 486Dx4's and old 386's: */
+	char			hlt_works_ok;
+	char			hard_math;
+	char			rfu;
+	char			fdiv_bug;
+	char			f00f_bug;
+	char			coma_bug;
+	char			pad0;
+#else
+	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
+	int			 x86_tlbsize;
+	__u8			x86_virt_bits;
+	__u8			x86_phys_bits;
+#endif
+	/* CPUID returned core id bits: */
+	__u8			x86_coreid_bits;
+	/* Max extended CPUID function supported: */
+	__u32			extended_cpuid_level;
+	/* Maximum supported CPUID level, -1=no CPUID: */
+	int			cpuid_level;
+	__u32			x86_capability[NCAPINTS];
+	char			x86_vendor_id[16];
+	char			x86_model_id[64];
+	/* in KB - valid for CPUS which support this call: */
+	int			x86_cache_size;
+	int			x86_cache_alignment;	/* In bytes */
+	int			x86_power;
+	unsigned long		loops_per_jiffy;
+#ifdef CONFIG_SMP
+	/* cpus sharing the last level cache: */
+	cpumask_t		llc_shared_map;
+#endif
+	/* cpuid returned max cores value: */
+	u16			 x86_max_cores;
+	u16			apicid;
+	u16			initial_apicid;
+	u16			x86_clflush_size;
+#ifdef CONFIG_SMP
+	/* number of cores as seen by the OS: */
+	u16			booted_cores;
+	/* Physical processor id: */
+	u16			phys_proc_id;
+	/* Core id: */
+	u16			cpu_core_id;
+	/* Index into per_cpu list: */
+	u16			cpu_index;
+#endif
+} __attribute__((__aligned__(SMP_CACHE_BYTES)));
+
+#define X86_VENDOR_INTEL	0
+#define X86_VENDOR_CYRIX	1
+#define X86_VENDOR_AMD		2
+#define X86_VENDOR_UMC		3
+#define X86_VENDOR_CENTAUR	5
+#define X86_VENDOR_TRANSMETA	7
+#define X86_VENDOR_NSC		8
+#define X86_VENDOR_NUM		9
+
+#define X86_VENDOR_UNKNOWN	0xff
+
+/*
+ * capabilities of CPUs
+ */
+extern struct cpuinfo_x86	boot_cpu_data;
+extern struct cpuinfo_x86	new_cpu_data;
+
+extern struct tss_struct	doublefault_tss;
+extern __u32			cleared_cpu_caps[NCAPINTS];
+
+#ifdef CONFIG_SMP
+DECLARE_PER_CPU(struct cpuinfo_x86, cpu_info);
+#define cpu_data(cpu)		per_cpu(cpu_info, cpu)
+#define current_cpu_data	__get_cpu_var(cpu_info)
+#else
+#define cpu_data(cpu)		boot_cpu_data
+#define current_cpu_data	boot_cpu_data
+#endif
+
+extern const struct seq_operations cpuinfo_op;
+
+static inline int hlt_works(int cpu)
+{
+#ifdef CONFIG_X86_32
+	return cpu_data(cpu).hlt_works_ok;
+#else
+	return 1;
+#endif
+}
+
+#define cache_line_size()	(boot_cpu_data.x86_cache_alignment)
+
+extern void cpu_detect(struct cpuinfo_x86 *c);
+
+extern struct pt_regs *idle_regs(struct pt_regs *);
+
+extern void early_cpu_init(void);
+extern void identify_boot_cpu(void);
+extern void identify_secondary_cpu(struct cpuinfo_x86 *);
+extern void print_cpu_info(struct cpuinfo_x86 *);
+extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
+extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
+extern unsigned short num_cache_leaves;
+
+extern void detect_extended_topology(struct cpuinfo_x86 *c);
+extern void detect_ht(struct cpuinfo_x86 *c);
+
+static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
+				unsigned int *ecx, unsigned int *edx)
+{
+	/* ecx is often an input as well as an output. */
+	asm("cpuid"
+	    : "=a" (*eax),
+	      "=b" (*ebx),
+	      "=c" (*ecx),
+	      "=d" (*edx)
+	    : "0" (*eax), "2" (*ecx));
+}
+
+static inline void load_cr3(pgd_t *pgdir)
+{
+	write_cr3(__pa(pgdir));
+}
+
+#ifdef CONFIG_X86_32
+/* This is the TSS defined by the hardware. */
+struct x86_hw_tss {
+	unsigned short		back_link, __blh;
+	unsigned long		sp0;
+	unsigned short		ss0, __ss0h;
+	unsigned long		sp1;
+	/* ss1 caches MSR_IA32_SYSENTER_CS: */
+	unsigned short		ss1, __ss1h;
+	unsigned long		sp2;
+	unsigned short		ss2, __ss2h;
+	unsigned long		__cr3;
+	unsigned long		ip;
+	unsigned long		flags;
+	unsigned long		ax;
+	unsigned long		cx;
+	unsigned long		dx;
+	unsigned long		bx;
+	unsigned long		sp;
+	unsigned long		bp;
+	unsigned long		si;
+	unsigned long		di;
+	unsigned short		es, __esh;
+	unsigned short		cs, __csh;
+	unsigned short		ss, __ssh;
+	unsigned short		ds, __dsh;
+	unsigned short		fs, __fsh;
+	unsigned short		gs, __gsh;
+	unsigned short		ldt, __ldth;
+	unsigned short		trace;
+	unsigned short		io_bitmap_base;
+
+} __attribute__((packed));
+#else
+struct x86_hw_tss {
+	u32			reserved1;
+	u64			sp0;
+	u64			sp1;
+	u64			sp2;
+	u64			reserved2;
+	u64			ist[7];
+	u32			reserved3;
+	u32			reserved4;
+	u16			reserved5;
+	u16			io_bitmap_base;
+
+} __attribute__((packed)) ____cacheline_aligned;
+#endif
+
+/*
+ * IO-bitmap sizes:
+ */
+#define IO_BITMAP_BITS			65536
+#define IO_BITMAP_BYTES			(IO_BITMAP_BITS/8)
+#define IO_BITMAP_LONGS			(IO_BITMAP_BYTES/sizeof(long))
+#define IO_BITMAP_OFFSET		offsetof(struct tss_struct, io_bitmap)
+#define INVALID_IO_BITMAP_OFFSET	0x8000
+#define INVALID_IO_BITMAP_OFFSET_LAZY	0x9000
+
+struct tss_struct {
+	/*
+	 * The hardware state:
+	 */
+	struct x86_hw_tss	x86_tss;
+
+	/*
+	 * The extra 1 is there because the CPU will access an
+	 * additional byte beyond the end of the IO permission
+	 * bitmap. The extra byte must be all 1 bits, and must
+	 * be within the limit.
+	 */
+	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
+	/*
+	 * Cache the current maximum and the last task that used the bitmap:
+	 */
+	unsigned long		io_bitmap_max;
+	struct thread_struct	*io_bitmap_owner;
+
+	/*
+	 * .. and then another 0x100 bytes for the emergency kernel stack:
+	 */
+	unsigned long		stack[64];
+
+} ____cacheline_aligned;
+
+DECLARE_PER_CPU(struct tss_struct, init_tss);
+
+/*
+ * Save the original ist values for checking stack pointers during debugging
+ */
+struct orig_ist {
+	unsigned long		ist[7];
+};
+
+#define	MXCSR_DEFAULT		0x1f80
+
+struct i387_fsave_struct {
+	u32			cwd;	/* FPU Control Word		*/
+	u32			swd;	/* FPU Status Word		*/
+	u32			twd;	/* FPU Tag Word			*/
+	u32			fip;	/* FPU IP Offset		*/
+	u32			fcs;	/* FPU IP Selector		*/
+	u32			foo;	/* FPU Operand Pointer Offset	*/
+	u32			fos;	/* FPU Operand Pointer Selector	*/
+
+	/* 8*10 bytes for each FP-reg = 80 bytes:			*/
+	u32			st_space[20];
+
+	/* Software status information [not touched by FSAVE ]:		*/
+	u32			status;
+};
+
+struct i387_fxsave_struct {
+	u16			cwd; /* Control Word			*/
+	u16			swd; /* Status Word			*/
+	u16			twd; /* Tag Word			*/
+	u16			fop; /* Last Instruction Opcode		*/
+	union {
+		struct {
+			u64	rip; /* Instruction Pointer		*/
+			u64	rdp; /* Data Pointer			*/
+		};
+		struct {
+			u32	fip; /* FPU IP Offset			*/
+			u32	fcs; /* FPU IP Selector			*/
+			u32	foo; /* FPU Operand Offset		*/
+			u32	fos; /* FPU Operand Selector		*/
+		};
+	};
+	u32			mxcsr;		/* MXCSR Register State */
+	u32			mxcsr_mask;	/* MXCSR Mask		*/
+
+	/* 8*16 bytes for each FP-reg = 128 bytes:			*/
+	u32			st_space[32];
+
+	/* 16*16 bytes for each XMM-reg = 256 bytes:			*/
+	u32			xmm_space[64];
+
+	u32			padding[12];
+
+	union {
+		u32		padding1[12];
+		u32		sw_reserved[12];
+	};
+
+} __attribute__((aligned(16)));
+
+struct i387_soft_struct {
+	u32			cwd;
+	u32			swd;
+	u32			twd;
+	u32			fip;
+	u32			fcs;
+	u32			foo;
+	u32			fos;
+	/* 8*10 bytes for each FP-reg = 80 bytes: */
+	u32			st_space[20];
+	u8			ftop;
+	u8			changed;
+	u8			lookahead;
+	u8			no_update;
+	u8			rm;
+	u8			alimit;
+	struct info		*info;
+	u32			entry_eip;
+};
+
+struct xsave_hdr_struct {
+	u64 xstate_bv;
+	u64 reserved1[2];
+	u64 reserved2[5];
+} __attribute__((packed));
+
+struct xsave_struct {
+	struct i387_fxsave_struct i387;
+	struct xsave_hdr_struct xsave_hdr;
+	/* new processor state extensions will go here */
+} __attribute__ ((packed, aligned (64)));
+
+union thread_xstate {
+	struct i387_fsave_struct	fsave;
+	struct i387_fxsave_struct	fxsave;
+	struct i387_soft_struct		soft;
+	struct xsave_struct		xsave;
+};
+
+#ifdef CONFIG_X86_64
+DECLARE_PER_CPU(struct orig_ist, orig_ist);
+#endif
+
+extern void print_cpu_info(struct cpuinfo_x86 *);
+extern unsigned int xstate_size;
+extern void free_thread_xstate(struct task_struct *);
+extern struct kmem_cache *task_xstate_cachep;
+extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
+extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
+extern unsigned short num_cache_leaves;
+
+struct thread_struct {
+	/* Cached TLS descriptors: */
+	struct desc_struct	tls_array[GDT_ENTRY_TLS_ENTRIES];
+	unsigned long		sp0;
+	unsigned long		sp;
+#ifdef CONFIG_X86_32
+	unsigned long		sysenter_cs;
+#else
+	unsigned long		usersp;	/* Copy from PDA */
+	unsigned short		es;
+	unsigned short		ds;
+	unsigned short		fsindex;
+	unsigned short		gsindex;
+#endif
+	unsigned long		ip;
+	unsigned long		fs;
+	unsigned long		gs;
+	/* Hardware debugging registers: */
+	unsigned long		debugreg0;
+	unsigned long		debugreg1;
+	unsigned long		debugreg2;
+	unsigned long		debugreg3;
+	unsigned long		debugreg6;
+	unsigned long		debugreg7;
+	/* Fault info: */
+	unsigned long		cr2;
+	unsigned long		trap_no;
+	unsigned long		error_code;
+	/* floating point and extended processor state */
+	union thread_xstate	*xstate;
+#ifdef CONFIG_X86_32
+	/* Virtual 86 mode info */
+	struct vm86_struct __user *vm86_info;
+	unsigned long		screen_bitmap;
+	unsigned long		v86flags;
+	unsigned long		v86mask;
+	unsigned long		saved_sp0;
+	unsigned int		saved_fs;
+	unsigned int		saved_gs;
+#endif
+	/* IO permissions: */
+	unsigned long		*io_bitmap_ptr;
+	unsigned long		iopl;
+	/* Max allowed port in the bitmap, in bytes: */
+	unsigned		io_bitmap_max;
+/* MSR_IA32_DEBUGCTLMSR value to switch in if TIF_DEBUGCTLMSR is set.  */
+	unsigned long	debugctlmsr;
+#ifdef CONFIG_X86_DS
+/* Debug Store context; see include/asm-x86/ds.h; goes into MSR_IA32_DS_AREA */
+	struct ds_context	*ds_ctx;
+#endif /* CONFIG_X86_DS */
+#ifdef CONFIG_X86_PTRACE_BTS
+/* the signal to send on a bts buffer overflow */
+	unsigned int	bts_ovfl_signal;
+#endif /* CONFIG_X86_PTRACE_BTS */
+};
+
+static inline unsigned long native_get_debugreg(int regno)
+{
+	unsigned long val = 0;	/* Damn you, gcc! */
+
+	switch (regno) {
+	case 0:
+		asm("mov %%db0, %0" :"=r" (val));
+		break;
+	case 1:
+		asm("mov %%db1, %0" :"=r" (val));
+		break;
+	case 2:
+		asm("mov %%db2, %0" :"=r" (val));
+		break;
+	case 3:
+		asm("mov %%db3, %0" :"=r" (val));
+		break;
+	case 6:
+		asm("mov %%db6, %0" :"=r" (val));
+		break;
+	case 7:
+		asm("mov %%db7, %0" :"=r" (val));
+		break;
+	default:
+		BUG();
+	}
+	return val;
+}
+
+static inline void native_set_debugreg(int regno, unsigned long value)
+{
+	switch (regno) {
+	case 0:
+		asm("mov %0, %%db0"	::"r" (value));
+		break;
+	case 1:
+		asm("mov %0, %%db1"	::"r" (value));
+		break;
+	case 2:
+		asm("mov %0, %%db2"	::"r" (value));
+		break;
+	case 3:
+		asm("mov %0, %%db3"	::"r" (value));
+		break;
+	case 6:
+		asm("mov %0, %%db6"	::"r" (value));
+		break;
+	case 7:
+		asm("mov %0, %%db7"	::"r" (value));
+		break;
+	default:
+		BUG();
+	}
+}
+
+/*
+ * Set IOPL bits in EFLAGS from given mask
+ */
+static inline void native_set_iopl_mask(unsigned mask)
+{
+#ifdef CONFIG_X86_32
+	unsigned int reg;
+
+	asm volatile ("pushfl;"
+		      "popl %0;"
+		      "andl %1, %0;"
+		      "orl %2, %0;"
+		      "pushl %0;"
+		      "popfl"
+		      : "=&r" (reg)
+		      : "i" (~X86_EFLAGS_IOPL), "r" (mask));
+#endif
+}
+
+static inline void
+native_load_sp0(struct tss_struct *tss, struct thread_struct *thread)
+{
+	tss->x86_tss.sp0 = thread->sp0;
+#ifdef CONFIG_X86_32
+	/* Only happens when SEP is enabled, no need to test "SEP"arately: */
+	if (unlikely(tss->x86_tss.ss1 != thread->sysenter_cs)) {
+		tss->x86_tss.ss1 = thread->sysenter_cs;
+		wrmsr(MSR_IA32_SYSENTER_CS, thread->sysenter_cs, 0);
+	}
+#endif
+}
+
+static inline void native_swapgs(void)
+{
+#ifdef CONFIG_X86_64
+	asm volatile("swapgs" ::: "memory");
+#endif
+}
+
+#ifdef CONFIG_PARAVIRT
+#include <asm/paravirt.h>
+#else
+#define __cpuid			native_cpuid
+#define paravirt_enabled()	0
+
+/*
+ * These special macros can be used to get or set a debugging register
+ */
+#define get_debugreg(var, register)				\
+	(var) = native_get_debugreg(register)
+#define set_debugreg(value, register)				\
+	native_set_debugreg(register, value)
+
+static inline void load_sp0(struct tss_struct *tss,
+			    struct thread_struct *thread)
+{
+	native_load_sp0(tss, thread);
+}
+
+#define set_iopl_mask native_set_iopl_mask
+#endif /* CONFIG_PARAVIRT */
+
+/*
+ * Save the cr4 feature set we're using (ie
+ * Pentium 4MB enable and PPro Global page
+ * enable), so that any CPU's that boot up
+ * after us can get the correct flags.
+ */
+extern unsigned long		mmu_cr4_features;
+
+static inline void set_in_cr4(unsigned long mask)
+{
+	unsigned cr4;
+
+	mmu_cr4_features |= mask;
+	cr4 = read_cr4();
+	cr4 |= mask;
+	write_cr4(cr4);
+}
+
+static inline void clear_in_cr4(unsigned long mask)
+{
+	unsigned cr4;
+
+	mmu_cr4_features &= ~mask;
+	cr4 = read_cr4();
+	cr4 &= ~mask;
+	write_cr4(cr4);
+}
+
+typedef struct {
+	unsigned long		seg;
+} mm_segment_t;
+
+
+/*
+ * create a kernel thread without removing it from tasklists
+ */
+extern int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
+
+/* Free all resources held by a thread. */
+extern void release_thread(struct task_struct *);
+
+/* Prepare to copy thread state - unlazy all lazy state */
+extern void prepare_to_copy(struct task_struct *tsk);
+
+unsigned long get_wchan(struct task_struct *p);
+
+/*
+ * Generic CPUID function
+ * clear %ecx since some cpus (Cyrix MII) do not set or clear %ecx
+ * resulting in stale register contents being returned.
+ */
+static inline void cpuid(unsigned int op,
+			 unsigned int *eax, unsigned int *ebx,
+			 unsigned int *ecx, unsigned int *edx)
+{
+	*eax = op;
+	*ecx = 0;
+	__cpuid(eax, ebx, ecx, edx);
+}
+
+/* Some CPUID calls want 'count' to be placed in ecx */
+static inline void cpuid_count(unsigned int op, int count,
+			       unsigned int *eax, unsigned int *ebx,
+			       unsigned int *ecx, unsigned int *edx)
+{
+	*eax = op;
+	*ecx = count;
+	__cpuid(eax, ebx, ecx, edx);
+}
+
+/*
+ * CPUID functions returning a single datum
+ */
+static inline unsigned int cpuid_eax(unsigned int op)
+{
+	unsigned int eax, ebx, ecx, edx;
+
+	cpuid(op, &eax, &ebx, &ecx, &edx);
+
+	return eax;
+}
+
+static inline unsigned int cpuid_ebx(unsigned int op)
+{
+	unsigned int eax, ebx, ecx, edx;
+
+	cpuid(op, &eax, &ebx, &ecx, &edx);
+
+	return ebx;
+}
+
+static inline unsigned int cpuid_ecx(unsigned int op)
+{
+	unsigned int eax, ebx, ecx, edx;
+
+	cpuid(op, &eax, &ebx, &ecx, &edx);
+
+	return ecx;
+}
+
+static inline unsigned int cpuid_edx(unsigned int op)
+{
+	unsigned int eax, ebx, ecx, edx;
+
+	cpuid(op, &eax, &ebx, &ecx, &edx);
+
+	return edx;
+}
+
+/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
+static inline void rep_nop(void)
+{
+	asm volatile("rep; nop" ::: "memory");
+}
+
+static inline void cpu_relax(void)
+{
+	rep_nop();
+}
+
+/* Stop speculative execution: */
+static inline void sync_core(void)
+{
+	int tmp;
+
+	asm volatile("cpuid" : "=a" (tmp) : "0" (1)
+		     : "ebx", "ecx", "edx", "memory");
+}
+
+static inline void __monitor(const void *eax, unsigned long ecx,
+			     unsigned long edx)
+{
+	/* "monitor %eax, %ecx, %edx;" */
+	asm volatile(".byte 0x0f, 0x01, 0xc8;"
+		     :: "a" (eax), "c" (ecx), "d"(edx));
+}
+
+static inline void __mwait(unsigned long eax, unsigned long ecx)
+{
+	/* "mwait %eax, %ecx;" */
+	asm volatile(".byte 0x0f, 0x01, 0xc9;"
+		     :: "a" (eax), "c" (ecx));
+}
+
+static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
+{
+	trace_hardirqs_on();
+	/* "mwait %eax, %ecx;" */
+	asm volatile("sti; .byte 0x0f, 0x01, 0xc9;"
+		     :: "a" (eax), "c" (ecx));
+}
+
+extern void mwait_idle_with_hints(unsigned long eax, unsigned long ecx);
+
+extern void select_idle_routine(const struct cpuinfo_x86 *c);
+
+extern unsigned long		boot_option_idle_override;
+extern unsigned long		idle_halt;
+extern unsigned long		idle_nomwait;
+
+/*
+ * on systems with caches, caches must be flashed as the absolute
+ * last instruction before going into a suspended halt.  Otherwise,
+ * dirty data can linger in the cache and become stale on resume,
+ * leading to strange errors.
+ *
+ * perform a variety of operations to guarantee that the compiler
+ * will not reorder instructions.  wbinvd itself is serializing
+ * so the processor will not reorder.
+ *
+ * Systems without cache can just go into halt.
+ */
+static inline void wbinvd_halt(void)
+{
+	mb();
+	/* check for clflush to determine if wbinvd is legal */
+	if (cpu_has_clflush)
+		asm volatile("cli; wbinvd; 1: hlt; jmp 1b" : : : "memory");
+	else
+		while (1)
+			halt();
+}
+
+extern void enable_sep_cpu(void);
+extern int sysenter_setup(void);
+
+/* Defined in head.S */
+extern struct desc_ptr		early_gdt_descr;
+
+extern void cpu_set_gdt(int);
+extern void switch_to_new_gdt(void);
+extern void cpu_init(void);
+extern void init_gdt(int cpu);
+
+static inline void update_debugctlmsr(unsigned long debugctlmsr)
+{
+#ifndef CONFIG_X86_DEBUGCTLMSR
+	if (boot_cpu_data.x86 < 6)
+		return;
+#endif
+	wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);
+}
+
+/*
+ * from system description table in BIOS. Mostly for MCA use, but
+ * others may find it useful:
+ */
+extern unsigned int		machine_id;
+extern unsigned int		machine_submodel_id;
+extern unsigned int		BIOS_revision;
+
+/* Boot loader type from the setup header: */
+extern int			bootloader_type;
+
+extern char			ignore_fpu_irq;
+
+#define HAVE_ARCH_PICK_MMAP_LAYOUT 1
+#define ARCH_HAS_PREFETCHW
+#define ARCH_HAS_SPINLOCK_PREFETCH
+
+#ifdef CONFIG_X86_32
+# define BASE_PREFETCH		ASM_NOP4
+# define ARCH_HAS_PREFETCH
+#else
+# define BASE_PREFETCH		"prefetcht0 (%1)"
+#endif
+
+/*
+ * Prefetch instructions for Pentium III (+) and AMD Athlon (+)
+ *
+ * It's not worth to care about 3dnow prefetches for the K6
+ * because they are microcoded there and very slow.
+ */
+static inline void prefetch(const void *x)
+{
+	alternative_input(BASE_PREFETCH,
+			  "prefetchnta (%1)",
+			  X86_FEATURE_XMM,
+			  "r" (x));
+}
+
+/*
+ * 3dnow prefetch to get an exclusive cache line.
+ * Useful for spinlocks to avoid one state transition in the
+ * cache coherency protocol:
+ */
+static inline void prefetchw(const void *x)
+{
+	alternative_input(BASE_PREFETCH,
+			  "prefetchw (%1)",
+			  X86_FEATURE_3DNOW,
+			  "r" (x));
+}
+
+static inline void spin_lock_prefetch(const void *x)
+{
+	prefetchw(x);
+}
+
+#ifdef CONFIG_X86_32
+/*
+ * User space process size: 3GB (default).
+ */
+#define TASK_SIZE		PAGE_OFFSET
+#define STACK_TOP		TASK_SIZE
+#define STACK_TOP_MAX		STACK_TOP
+
+#define INIT_THREAD  {							  \
+	.sp0			= sizeof(init_stack) + (long)&init_stack, \
+	.vm86_info		= NULL,					  \
+	.sysenter_cs		= __KERNEL_CS,				  \
+	.io_bitmap_ptr		= NULL,					  \
+	.fs			= __KERNEL_PERCPU,			  \
+}
+
+/*
+ * Note that the .io_bitmap member must be extra-big. This is because
+ * the CPU will access an additional byte beyond the end of the IO
+ * permission bitmap. The extra byte must be all 1 bits, and must
+ * be within the limit.
+ */
+#define INIT_TSS  {							  \
+	.x86_tss = {							  \
+		.sp0		= sizeof(init_stack) + (long)&init_stack, \
+		.ss0		= __KERNEL_DS,				  \
+		.ss1		= __KERNEL_CS,				  \
+		.io_bitmap_base	= INVALID_IO_BITMAP_OFFSET,		  \
+	 },								  \
+	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },	  \
+}
+
+extern unsigned long thread_saved_pc(struct task_struct *tsk);
+
+#define THREAD_SIZE_LONGS      (THREAD_SIZE/sizeof(unsigned long))
+#define KSTK_TOP(info)                                                 \
+({                                                                     \
+       unsigned long *__ptr = (unsigned long *)(info);                 \
+       (unsigned long)(&__ptr[THREAD_SIZE_LONGS]);                     \
+})
+
+/*
+ * The below -8 is to reserve 8 bytes on top of the ring0 stack.
+ * This is necessary to guarantee that the entire "struct pt_regs"
+ * is accessable even if the CPU haven't stored the SS/ESP registers
+ * on the stack (interrupt gate does not save these registers
+ * when switching to the same priv ring).
+ * Therefore beware: accessing the ss/esp fields of the
+ * "struct pt_regs" is possible, but they may contain the
+ * completely wrong values.
+ */
+#define task_pt_regs(task)                                             \
+({                                                                     \
+       struct pt_regs *__regs__;                                       \
+       __regs__ = (struct pt_regs *)(KSTK_TOP(task_stack_page(task))-8); \
+       __regs__ - 1;                                                   \
+})
+
+#define KSTK_ESP(task)		(task_pt_regs(task)->sp)
+
+#else
+/*
+ * User space process size. 47bits minus one guard page.
+ */
+#define TASK_SIZE64	((1UL << 47) - PAGE_SIZE)
+
+/* This decides where the kernel will search for a free chunk of vm
+ * space during mmap's.
+ */
+#define IA32_PAGE_OFFSET	((current->personality & ADDR_LIMIT_3GB) ? \
+					0xc0000000 : 0xFFFFe000)
+
+#define TASK_SIZE		(test_thread_flag(TIF_IA32) ? \
+					IA32_PAGE_OFFSET : TASK_SIZE64)
+#define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_IA32)) ? \
+					IA32_PAGE_OFFSET : TASK_SIZE64)
+
+#define STACK_TOP		TASK_SIZE
+#define STACK_TOP_MAX		TASK_SIZE64
+
+#define INIT_THREAD  { \
+	.sp0 = (unsigned long)&init_stack + sizeof(init_stack) \
+}
+
+#define INIT_TSS  { \
+	.x86_tss.sp0 = (unsigned long)&init_stack + sizeof(init_stack) \
+}
+
+/*
+ * Return saved PC of a blocked thread.
+ * What is this good for? it will be always the scheduler or ret_from_fork.
+ */
+#define thread_saved_pc(t)	(*(unsigned long *)((t)->thread.sp - 8))
+
+#define task_pt_regs(tsk)	((struct pt_regs *)(tsk)->thread.sp0 - 1)
+#define KSTK_ESP(tsk)		-1 /* sorry. doesn't work for syscall. */
+#endif /* CONFIG_X86_64 */
+
+extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
+					       unsigned long new_sp);
+
+/*
+ * This decides where the kernel will search for a free chunk of vm
+ * space during mmap's.
+ */
+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 3))
+
+#define KSTK_EIP(task)		(task_pt_regs(task)->ip)
+
+/* Get/set a process' ability to use the timestamp counter instruction */
+#define GET_TSC_CTL(adr)	get_tsc_mode((adr))
+#define SET_TSC_CTL(val)	set_tsc_mode((val))
+
+extern int get_tsc_mode(unsigned long adr);
+extern int set_tsc_mode(unsigned int val);
+
+#endif /* ASM_X86__PROCESSOR_H */
