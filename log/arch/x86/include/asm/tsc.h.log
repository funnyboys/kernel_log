commit 89f579ce99f7e028e81885d3965f973c0f787611
Author: Yi Wang <wang.yi59@zte.com.cn>
Date:   Thu Nov 22 10:04:09 2018 +0800

    x86/headers: Fix -Wmissing-prototypes warning
    
    When building the kernel with W=1 we get a lot of -Wmissing-prototypes
    warnings, which are trivial in nature and easy to fix - and which may
    mask some real future bugs if the prototypes get out of sync with
    the function definition.
    
    This patch fixes most of -Wmissing-prototypes warnings which
    are in the root directory of arch/x86/kernel, not including
    the subdirectories.
    
    These are the warnings fixed in this patch:
    
      arch/x86/kernel/signal.c:865:17: warning: no previous prototype for ‘sys32_x32_rt_sigreturn’ [-Wmissing-prototypes]
      arch/x86/kernel/signal_compat.c:164:6: warning: no previous prototype for ‘sigaction_compat_abi’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:625:46: warning: no previous prototype for ‘sync_regs’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:640:24: warning: no previous prototype for ‘fixup_bad_iret’ [-Wmissing-prototypes]
      arch/x86/kernel/traps.c:929:13: warning: no previous prototype for ‘trap_init’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:270:28: warning: no previous prototype for ‘smp_x86_platform_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:301:16: warning: no previous prototype for ‘smp_kvm_posted_intr_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:314:16: warning: no previous prototype for ‘smp_kvm_posted_intr_wakeup_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq.c:328:16: warning: no previous prototype for ‘smp_kvm_posted_intr_nested_ipi’ [-Wmissing-prototypes]
      arch/x86/kernel/irq_work.c:16:28: warning: no previous prototype for ‘smp_irq_work_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/irqinit.c:79:13: warning: no previous prototype for ‘init_IRQ’ [-Wmissing-prototypes]
      arch/x86/kernel/quirks.c:672:13: warning: no previous prototype for ‘early_platform_quirks’ [-Wmissing-prototypes]
      arch/x86/kernel/tsc.c:1499:15: warning: no previous prototype for ‘calibrate_delay_is_known’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:653:13: warning: no previous prototype for ‘arch_post_acpi_subsys_init’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:717:15: warning: no previous prototype for ‘arch_randomize_brk’ [-Wmissing-prototypes]
      arch/x86/kernel/process.c:784:6: warning: no previous prototype for ‘do_arch_prctl_common’ [-Wmissing-prototypes]
      arch/x86/kernel/reboot.c:869:6: warning: no previous prototype for ‘nmi_panic_self_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:176:27: warning: no previous prototype for ‘smp_reboot_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:260:28: warning: no previous prototype for ‘smp_reschedule_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:281:28: warning: no previous prototype for ‘smp_call_function_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/smp.c:291:28: warning: no previous prototype for ‘smp_call_function_single_interrupt’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:840:6: warning: no previous prototype for ‘arch_ftrace_update_trampoline’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:934:7: warning: no previous prototype for ‘arch_ftrace_trampoline_func’ [-Wmissing-prototypes]
      arch/x86/kernel/ftrace.c:946:6: warning: no previous prototype for ‘arch_ftrace_trampoline_free’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:114:6: warning: no previous prototype for ‘crash_smp_send_stop’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:351:5: warning: no previous prototype for ‘crash_setup_memmap_entries’ [-Wmissing-prototypes]
      arch/x86/kernel/crash.c:424:5: warning: no previous prototype for ‘crash_load_segments’ [-Wmissing-prototypes]
      arch/x86/kernel/machine_kexec_64.c:372:7: warning: no previous prototype for ‘arch_kexec_kernel_image_load’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:12:16: warning: no previous prototype for ‘__native_queued_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:18:6: warning: no previous prototype for ‘pv_is_native_spin_unlock’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:24:16: warning: no previous prototype for ‘__native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/paravirt-spinlocks.c:30:6: warning: no previous prototype for ‘pv_is_native_vcpu_is_preempted’ [-Wmissing-prototypes]
      arch/x86/kernel/kvm.c:258:1: warning: no previous prototype for ‘do_async_page_fault’ [-Wmissing-prototypes]
      arch/x86/kernel/jailhouse.c:200:6: warning: no previous prototype for ‘jailhouse_paravirt’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:91:13: warning: no previous prototype for ‘setup_bios_corruption_check’ [-Wmissing-prototypes]
      arch/x86/kernel/check.c:139:6: warning: no previous prototype for ‘check_for_bios_corruption’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:32:13: warning: no previous prototype for ‘early_init_dt_scan_chosen_arch’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:42:13: warning: no previous prototype for ‘add_dtb’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:108:6: warning: no previous prototype for ‘x86_of_pci_init’ [-Wmissing-prototypes]
      arch/x86/kernel/devicetree.c:314:13: warning: no previous prototype for ‘x86_dtb_init’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:16:5: warning: no previous prototype for ‘trace_pagefault_reg’ [-Wmissing-prototypes]
      arch/x86/kernel/tracepoint.c:22:6: warning: no previous prototype for ‘trace_pagefault_unreg’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:113:22: warning: no previous prototype for ‘__startup_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:262:15: warning: no previous prototype for ‘__startup_secondary_64’ [-Wmissing-prototypes]
      arch/x86/kernel/head64.c:350:12: warning: no previous prototype for ‘early_make_pgtable’ [-Wmissing-prototypes]
    
    [ mingo: rewrote the changelog, fixed build errors. ]
    
    Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: akpm@linux-foundation.org
    Cc: andy.shevchenko@gmail.com
    Cc: anton@enomsg.org
    Cc: ard.biesheuvel@linaro.org
    Cc: bhe@redhat.com
    Cc: bhelgaas@google.com
    Cc: bp@alien8.de
    Cc: ccross@android.com
    Cc: devicetree@vger.kernel.org
    Cc: douly.fnst@cn.fujitsu.com
    Cc: dwmw@amazon.co.uk
    Cc: dyoung@redhat.com
    Cc: ebiederm@xmission.com
    Cc: frank.rowand@sony.com
    Cc: frowand.list@gmail.com
    Cc: ivan.gorinov@intel.com
    Cc: jailhouse-dev@googlegroups.com
    Cc: jan.kiszka@siemens.com
    Cc: jgross@suse.com
    Cc: jroedel@suse.de
    Cc: keescook@chromium.org
    Cc: kexec@lists.infradead.org
    Cc: konrad.wilk@oracle.com
    Cc: kvm@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-pci@vger.kernel.org
    Cc: luto@kernel.org
    Cc: m.mizuma@jp.fujitsu.com
    Cc: namit@vmware.com
    Cc: oleg@redhat.com
    Cc: pasha.tatashin@oracle.com
    Cc: pbonzini@redhat.com
    Cc: prarit@redhat.com
    Cc: pravin.shedge4linux@gmail.com
    Cc: rajvi.jingar@intel.com
    Cc: rkrcmar@redhat.com
    Cc: robh+dt@kernel.org
    Cc: robh@kernel.org
    Cc: rostedt@goodmis.org
    Cc: takahiro.akashi@linaro.org
    Cc: thomas.lendacky@amd.com
    Cc: tony.luck@intel.com
    Cc: up2wing@gmail.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: zhe.he@windriver.com
    Cc: zhong.weidong@zte.com.cn
    Link: http://lkml.kernel.org/r/1542852249-19820-1-git-send-email-wang.yi59@zte.com.cn
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index eb5bbfeccb66..8a0c25c6bf09 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -35,6 +35,7 @@ extern struct system_counterval_t convert_art_ns_to_tsc(u64 art_ns);
 
 extern void tsc_early_init(void);
 extern void tsc_init(void);
+extern unsigned long calibrate_delay_is_known(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);

commit 8dbe438589f373544a1af8b4a859e4da853c0f90
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:45 2018 -0400

    x86/tsc: Make use of tsc_calibrate_cpu_early()
    
    During early boot enable tsc_calibrate_cpu_early() and switch to
    tsc_calibrate_cpu() only later. Do this unconditionally, because it is
    unknown what methods other cpus will use to calibrate once they are
    onlined.
    
    If by the time tsc_init() is called tsc frequency is still unknown do only
    pit_hpet_ptimer_calibrate_cpu() to calibrate, as this function contains the
    only methods wich have not been called and tried earlier.
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-27-pasha.tatashin@oracle.com

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 88140e4f2292..eb5bbfeccb66 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -39,7 +39,6 @@ extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
 extern void mark_tsc_async_resets(char *reason);
-extern unsigned long native_calibrate_cpu(void);
 extern unsigned long native_calibrate_cpu_early(void);
 extern unsigned long native_calibrate_tsc(void);
 extern unsigned long long native_sched_clock_from_tsc(u64 tsc);

commit 03821f451d2d2d7599061244734245be139014ea
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:44 2018 -0400

    x86/tsc: Split native_calibrate_cpu() into early and late parts
    
    During early boot TSC and CPU frequency can be calibrated using MSR, CPUID,
    and quick PIT calibration methods. The other methods PIT/HPET/PMTIMER are
    available only after ACPI is initialized.
    
    Split native_calibrate_cpu() into early and late parts so they can be
    called separately during early and late tsc calibration.
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-26-pasha.tatashin@oracle.com

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index c4368ff73652..88140e4f2292 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -40,6 +40,7 @@ extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
 extern void mark_tsc_async_resets(char *reason);
 extern unsigned long native_calibrate_cpu(void);
+extern unsigned long native_calibrate_cpu_early(void);
 extern unsigned long native_calibrate_tsc(void);
 extern unsigned long long native_sched_clock_from_tsc(u64 tsc);
 

commit cf7a63ef4e0203f6f33284c69e8188d91422de83
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:38 2018 -0400

    x86/tsc: Calibrate tsc only once
    
    During boot tsc is calibrated twice: once in tsc_early_delay_calibrate(),
    and the second time in tsc_init().
    
    Rename tsc_early_delay_calibrate() to tsc_early_init(), and rework it so
    the calibration is done only early, and make tsc_init() to use the values
    already determined in tsc_early_init().
    
    Sometimes it is not possible to determine tsc early, as the subsystem that
    is required is not yet initialized, in such case try again later in
    tsc_init().
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-20-pasha.tatashin@oracle.com

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 2701d221583a..c4368ff73652 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -33,7 +33,7 @@ static inline cycles_t get_cycles(void)
 extern struct system_counterval_t convert_art_to_tsc(u64 art);
 extern struct system_counterval_t convert_art_ns_to_tsc(u64 art_ns);
 
-extern void tsc_early_delay_calibrate(void);
+extern void tsc_early_init(void);
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);

commit fc804f65d46236c211f530174904c1ed70db5888
Author: Rajvi Jingar <rajvi.jingar@intel.com>
Date:   Thu Mar 8 09:28:36 2018 -0800

    x86/tsc: Convert ART in nanoseconds to TSC
    
    Device drivers use get_device_system_crosststamp() to produce precise
    system/device cross-timestamps. The PHC clock and ALSA interfaces, for
    example, make the cross-timestamps available to user applications.  On
    Intel platforms, get_device_system_crosststamp() requires a TSC value
    derived from ART (Always Running Timer) to compute the monotonic raw and
    realtime system timestamps.
    
    Starting with Intel Goldmont platforms, the PCIe root complex supports the
    PTM time sync protocol. PTM requires all timestamps to be in units of
    nanoseconds. The Intel root complex hardware propagates system time derived
    from ART in units of nanoseconds performing the conversion as follows:
    
         ART_NS = ART * 1e9 / <crystal frequency>
    
    When user software requests a cross-timestamp, the system timestamps
    (generally read from device registers) must be converted to TSC by the
    driver software as follows:
    
        TSC = ART_NS * TSC_KHZ / 1e6
    
    This is valid when CPU feature flag X86_FEATURE_TSC_KNOWN_FREQ is set
    indicating that tsc_khz is derived from CPUID[15H]. Drivers should check
    whether this flag is set before conversion to TSC is attempted.
    
    Suggested-by: Christopher S. Hall <christopher.s.hall@intel.com>
    Signed-off-by: Rajvi Jingar <rajvi.jingar@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: peterz@infradead.org
    Link: https://lkml.kernel.org/r/1520530116-4925-1-git-send-email-rajvi.jingar@intel.com

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index cf5d53c3f9ea..2701d221583a 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -31,6 +31,7 @@ static inline cycles_t get_cycles(void)
 }
 
 extern struct system_counterval_t convert_art_to_tsc(u64 art);
+extern struct system_counterval_t convert_art_ns_to_tsc(u64 art_ns);
 
 extern void tsc_early_delay_calibrate(void);
 extern void tsc_init(void);

commit 99306dfc067e6098365d395168b6fd5db3095292
Merge: 3643b7e05b16 120fc3fbb778
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 19:07:38 2017 -0800

    Merge branch 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 timer updates from Thomas Gleixner:
     "These updates are related to TSC handling:
    
       - Support platforms which have synchronized TSCs but the boot CPU has
         a non zero TSC_ADJUST value, which is considered a firmware bug on
         normal systems.
    
         This applies to HPE/SGI UV platforms where the platform firmware
         uses TSC_ADJUST to ensure TSC synchronization across a huge number
         of sockets, but due to power on timings the boot CPU cannot be
         guaranteed to have a zero TSC_ADJUST register value.
    
       - Fix the ordering of udelay calibration and kvmclock_init()
    
       - Cleanup the udelay and calibration code"
    
    * 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tsc: Mark cyc2ns_init() and detect_art() __init
      x86/platform/UV: Mark tsc_check_sync as an init function
      x86/tsc: Make CONFIG_X86_TSC=n build work again
      x86/platform/UV: Add check of TSC state set by UV BIOS
      x86/tsc: Provide a means to disable TSC ART
      x86/tsc: Drastically reduce the number of firmware bug warnings
      x86/tsc: Skip TSC test and error messages if already unstable
      x86/tsc: Add option that TSC on Socket 0 being non-zero is valid
      x86/timers: Move simple_udelay_calibration() past kvmclock_init()
      x86/timers: Make recalibrate_cpu_khz() void
      x86/timers: Move the simple udelay calibration to tsc.h

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index f5e6f1c417df..8da0efb13544 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * x86 TSC related functions
  */

commit c0fc9b1350a317da22b310d68117b0d01cb9065e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Oct 17 08:49:37 2017 +0200

    x86/tsc: Make CONFIG_X86_TSC=n build work again
    
    tsc_async_resets is only available when CONFIG_X86_TSC=y. So a build with
    CONFIG_X86_TSC=n breaks:
    
    arch/x86/kernel/tsc.o: In function `tsc_init':
    (.init.text+0x87b): undefined reference to `tsc_async_resets'
    
    Add a stub define for the TSC=n case.
    
    Side note: This config switch should simply be removed.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Fixes: 341102c3ef29 ("x86/tsc: Add option that TSC on Socket 0 being non-zero is valid")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mike Travis <mike.travis@hpe.com>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 79125f3609c4..c016167720b1 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -42,7 +42,11 @@ extern unsigned long native_calibrate_tsc(void);
 extern unsigned long long native_sched_clock_from_tsc(u64 tsc);
 
 extern int tsc_clocksource_reliable;
+#ifdef CONFIG_X86_TSC
 extern bool tsc_async_resets;
+#else
+# define tsc_async_resets	false
+#endif
 
 /*
  * Boot-time check whether the TSCs are synchronized across

commit 341102c3ef29c33611586072363cf9982a8bdb77
Author: mike.travis@hpe.com <mike.travis@hpe.com>
Date:   Thu Oct 12 11:32:02 2017 -0500

    x86/tsc: Add option that TSC on Socket 0 being non-zero is valid
    
    Add a flag to indicate and process that TSC counters are on chassis
    that reset at different times during system startup.  Therefore which
    TSC ADJUST values should be zero is not predictable.
    
    Signed-off-by: Mike Travis <mike.travis@hpe.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Dimitri Sivanich <dimitri.sivanich@hpe.com>
    Reviewed-by: Russ Anderson <russ.anderson@hpe.com>
    Reviewed-by: Andrew Banman <andrew.abanman@hpe.com>
    Reviewed-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Bin Gao <bin.gao@linux.intel.com>
    Link: https://lkml.kernel.org/r/20171012163201.944370012@stormcage.americas.sgi.com

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index d0509c75e150..79125f3609c4 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -36,11 +36,13 @@ extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
+extern void mark_tsc_async_resets(char *reason);
 extern unsigned long native_calibrate_cpu(void);
 extern unsigned long native_calibrate_tsc(void);
 extern unsigned long long native_sched_clock_from_tsc(u64 tsc);
 
 extern int tsc_clocksource_reliable;
+extern bool tsc_async_resets;
 
 /*
  * Boot-time check whether the TSCs are synchronized across

commit eb496063c9904ce682253ee445b9acb9b6257581
Author: Dou Liyang <douly.fnst@cn.fujitsu.com>
Date:   Fri Jul 14 11:34:06 2017 +0800

    x86/timers: Move the simple udelay calibration to tsc.h
    
    Commit dd759d93f4dd ("x86/timers: Add simple udelay calibration") adds
    an static function in x86 boot-time initializations.
    
    But, this function is actually related to TSC, so it should be maintained
    in tsc.c, not in setup.c.
    
    Move simple_udelay_calibration() from setup.c to tsc.c and rename it to
    tsc_early_delay_calibrate for more readability.
    
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1500003247-17368-1-git-send-email-douly.fnst@cn.fujitsu.com

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index f5e6f1c417df..d0509c75e150 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -31,6 +31,7 @@ static inline cycles_t get_cycles(void)
 
 extern struct system_counterval_t convert_art_to_tsc(u64 art);
 
+extern void tsc_early_delay_calibrate(void);
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index abb1fdcc545a..f5e6f1c417df 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -29,7 +29,7 @@ static inline cycles_t get_cycles(void)
 	return rdtsc();
 }
 
-extern struct system_counterval_t convert_art_to_tsc(cycle_t art);
+extern struct system_counterval_t convert_art_to_tsc(u64 art);
 
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);

commit 5bae156241e05d25171b18ee43e49f103c3f8097
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 13:14:17 2016 +0000

    x86/tsc: Force TSC_ADJUST register to value >= zero
    
    Roland reported that his DELL T5810 sports a value add BIOS which
    completely wreckages the TSC. The squirmware [(TM) Ingo Molnar] boots with
    random negative TSC_ADJUST values, different on all CPUs. That renders the
    TSC useless because the sycnchronization check fails.
    
    Roland tested the new TSC_ADJUST mechanism. While it manages to readjust
    the TSCs he needs to disable the TSC deadline timer, otherwise the machine
    just stops booting.
    
    Deeper investigation unearthed that the TSC deadline timer is sensitive to
    the TSC_ADJUST value. Writing TSC_ADJUST to a negative value results in an
    interrupt storm caused by the TSC deadline timer.
    
    This does not make any sense and it's hard to imagine what kind of hardware
    wreckage is behind that misfeature, but it's reliably reproducible on other
    systems which have TSC_ADJUST and TSC deadline timer.
    
    While it would be understandable that a big enough negative value which
    moves the resulting TSC readout into the negative space could have the
    described effect, this happens even with a adjust value of -1, which keeps
    the TSC readout definitely in the positive space. The compare register for
    the TSC deadline timer is set to a positive value larger than the TSC, but
    despite not having reached the deadline the interrupt is raised
    immediately. If this happens on the boot CPU, then the machine dies
    silently because this setup happens before the NMI watchdog is armed.
    
    Further experiments showed that any other adjustment of TSC_ADJUST works as
    expected as long as it stays in the positive range. The direction of the
    adjustment has no influence either. See the lkml link for further analysis.
    
    Yet another proof for the theory that timers are designed by janitors and
    the underlying (obviously undocumented) mechanisms which allow BIOSes to
    wreckage them are considered a feature. Well done Intel - NOT!
    
    To address this wreckage add the following sanity measures:
    
    - If the TSC_ADJUST value on the boot cpu is not 0, set it to 0
    
    - If the TSC_ADJUST value on any cpu is negative, set it to 0
    
    - Prevent the cross package synchronization mechanism from setting negative
      TSC_ADJUST values.
    
    Reported-and-tested-by: Roland Scheidegger <rscheidegger_lists@hispeed.ch>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bruce Schlobohm <bruce.schlobohm@intel.com>
    Cc: Kevin Stanton <kevin.b.stanton@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Allen Hung <allen_hung@dell.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161213131211.397588033@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 372ad0cd1357..abb1fdcc545a 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -46,12 +46,12 @@ extern int tsc_clocksource_reliable;
  * all CPUs/cores:
  */
 #ifdef CONFIG_X86_TSC
-extern bool tsc_store_and_check_tsc_adjust(void);
+extern bool tsc_store_and_check_tsc_adjust(bool bootcpu);
 extern void tsc_verify_tsc_adjust(bool resume);
 extern void check_tsc_sync_source(int cpu);
 extern void check_tsc_sync_target(void);
 #else
-static inline bool tsc_store_and_check_tsc_adjust(void) { return false; }
+static inline bool tsc_store_and_check_tsc_adjust(bool bootcpu) { return false; }
 static inline void tsc_verify_tsc_adjust(bool resume) { }
 static inline void check_tsc_sync_source(int cpu) { }
 static inline void check_tsc_sync_target(void) { }

commit 6a369583178d0b89c2c3919c4456ee22fee0f249
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 13:14:17 2016 +0000

    x86/tsc: Validate TSC_ADJUST after resume
    
    Some 'feature' BIOSes fiddle with the TSC_ADJUST register during
    suspend/resume which renders the TSC unusable.
    
    Add sanity checks into the resume path and restore the
    original value if it was adjusted.
    
    Reported-and-tested-by: Roland Scheidegger <rscheidegger_lists@hispeed.ch>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bruce Schlobohm <bruce.schlobohm@intel.com>
    Cc: Kevin Stanton <kevin.b.stanton@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Allen Hung <allen_hung@dell.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161213131211.317654500@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index c054eaa7dc94..372ad0cd1357 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -47,12 +47,12 @@ extern int tsc_clocksource_reliable;
  */
 #ifdef CONFIG_X86_TSC
 extern bool tsc_store_and_check_tsc_adjust(void);
-extern void tsc_verify_tsc_adjust(void);
+extern void tsc_verify_tsc_adjust(bool resume);
 extern void check_tsc_sync_source(int cpu);
 extern void check_tsc_sync_target(void);
 #else
 static inline bool tsc_store_and_check_tsc_adjust(void) { return false; }
-static inline void tsc_verify_tsc_adjust(void) { }
+static inline void tsc_verify_tsc_adjust(bool resume) { }
 static inline void check_tsc_sync_source(int cpu) { }
 static inline void check_tsc_sync_target(void) { }
 #endif

commit b836554386cc77f31ab43a8492a2587e0c51d51e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 29 20:28:31 2016 +0100

    x86/tsc: Fix broken CONFIG_X86_TSC=n build
    
    Add the missing return statement to the inline stub
    tsc_store_and_check_tsc_adjust() and add the other stubs to make a
    SMP=y,TSC=n build happy.
    
    While at it, remove the unused variable from the UP variant of
    tsc_store_and_check_tsc_adjust().
    
    Fixes: commit ba75fb646931 ("x86/tsc: Sync test only for the first cpu in a package")
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 04721d54d8d9..c054eaa7dc94 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -45,15 +45,16 @@ extern int tsc_clocksource_reliable;
  * Boot-time check whether the TSCs are synchronized across
  * all CPUs/cores:
  */
-extern void check_tsc_sync_source(int cpu);
-extern void check_tsc_sync_target(void);
-
 #ifdef CONFIG_X86_TSC
 extern bool tsc_store_and_check_tsc_adjust(void);
 extern void tsc_verify_tsc_adjust(void);
+extern void check_tsc_sync_source(int cpu);
+extern void check_tsc_sync_target(void);
 #else
-static inline bool tsc_store_and_check_tsc_adjust(void) { }
+static inline bool tsc_store_and_check_tsc_adjust(void) { return false; }
 static inline void tsc_verify_tsc_adjust(void) { }
+static inline void check_tsc_sync_source(int cpu) { }
+static inline void check_tsc_sync_target(void) { }
 #endif
 
 extern int notsc_setup(char *);

commit a36f5136814b6a87601220927cb9ad9ecc731e92
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 19 13:47:39 2016 +0000

    x86/tsc: Sync test only for the first cpu in a package
    
    If the TSC_ADJUST MSR is available all CPUs in a package are forced to the
    same value. So TSCs cannot be out of sync when the first CPU in the package
    was in sync.
    
    That allows to skip the sync test for all CPUs except the first starting
    CPU in a package.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161119134017.809901363@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index b896e9ee65bc..04721d54d8d9 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -49,10 +49,10 @@ extern void check_tsc_sync_source(int cpu);
 extern void check_tsc_sync_target(void);
 
 #ifdef CONFIG_X86_TSC
-extern void tsc_store_and_check_tsc_adjust(void);
+extern bool tsc_store_and_check_tsc_adjust(void);
 extern void tsc_verify_tsc_adjust(void);
 #else
-static inline void tsc_store_and_check_tsc_adjust(void) { }
+static inline bool tsc_store_and_check_tsc_adjust(void) { }
 static inline void tsc_verify_tsc_adjust(void) { }
 #endif
 

commit 1d0095feea591bbd94f35d8a98aed746319783e1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 19 13:47:37 2016 +0000

    x86/tsc: Verify TSC_ADJUST from idle
    
    When entering idle, it's a good oportunity to verify that the TSC_ADJUST
    MSR has not been tampered with (BIOS hiding SMM cycles). If tampering is
    detected, emit a warning and restore it to the previous value.
    
    This is especially important for machines, which mark the TSC reliable
    because there is no watchdog clocksource available (SoCs).
    
    This is not sufficient for HPC (NOHZ_FULL) situations where a CPU never
    goes idle, but adding a timer to do the check periodically is not an option
    either. On a machine, which has this issue, the check triggeres right
    during boot, so there is a decent chance that the sysadmin will notice.
    
    Rate limit the check to once per second and warn only once per cpu.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161119134017.732180441@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 1ec0595867d9..b896e9ee65bc 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -50,8 +50,10 @@ extern void check_tsc_sync_target(void);
 
 #ifdef CONFIG_X86_TSC
 extern void tsc_store_and_check_tsc_adjust(void);
+extern void tsc_verify_tsc_adjust(void);
 #else
 static inline void tsc_store_and_check_tsc_adjust(void) { }
+static inline void tsc_verify_tsc_adjust(void) { }
 #endif
 
 extern int notsc_setup(char *);

commit 8b223bc7abe0e30e8d297a24ee6c6c07ef8d0bb9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 19 13:47:36 2016 +0000

    x86/tsc: Store and check TSC ADJUST MSR
    
    The TSC_ADJUST MSR shows whether the TSC has been modified. This is helpful
    in a two aspects:
    
    1) It allows to detect BIOS wreckage, where SMM code tries to 'hide' the
       cycles spent by storing the TSC value at SMM entry and restoring it at
       SMM exit. On affected machines the TSCs run slowly out of sync up to the
       point where the clocksource watchdog (if available) detects it.
    
       The TSC_ADJUST MSR allows to detect the TSC modification before that and
       eventually restore it. This is also important for SoCs which have no
       watchdog clocksource and therefore TSC wreckage cannot be detected and
       acted upon.
    
    2) All threads in a package are required to have the same TSC_ADJUST
       value. Broken BIOSes break that and as a result the TSC synchronization
       check fails.
    
       The TSC_ADJUST MSR allows to detect the deviation when a CPU comes
       online. If detected set it to the value of an already online CPU in the
       same package. This also allows to reduce the number of sync tests
       because with that in place the test is only required for the first CPU
       in a package.
    
       In principle all CPUs in a system should have the same TSC_ADJUST value
       even across packages, but with physical CPU hotplug this assumption is
       not true because the TSC starts with power on, so physical hotplug has
       to do some trickery to bring the TSC into sync with already running
       packages, which requires to use an TSC_ADJUST value different from CPUs
       which got powered earlier.
    
       A final enhancement is the opportunity to compensate for unsynced TSCs
       accross nodes at boot time and make the TSC usable that way. It won't
       help for TSCs which run apart due to frequency skew between packages,
       but this gets detected by the clocksource watchdog later.
    
    The first step toward this is to store the TSC_ADJUST value of a starting
    CPU and compare it with the value of an already online CPU in the same
    package. If they differ, emit a warning and adjust it to the reference
    value. The !SMP version just stores the boot value for later verification.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161119134017.655323776@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 33b6365c22fe..1ec0595867d9 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -48,6 +48,12 @@ extern int tsc_clocksource_reliable;
 extern void check_tsc_sync_source(int cpu);
 extern void check_tsc_sync_target(void);
 
+#ifdef CONFIG_X86_TSC
+extern void tsc_store_and_check_tsc_adjust(void);
+#else
+static inline void tsc_store_and_check_tsc_adjust(void) { }
+#endif
+
 extern int notsc_setup(char *);
 extern void tsc_save_sched_clock_state(void);
 extern void tsc_restore_sched_clock_state(void);

commit c48ec42d6eae08f55685ab660f0743ed33b9f22a
Author: Wei Jiangang <weijg.fnst@cn.fujitsu.com>
Date:   Fri Jul 15 16:12:10 2016 +0800

    x86/tsc: Remove the unused check_tsc_disabled()
    
    check_tsc_disabled() was introduced by commit:
    
      c73deb6aecda ("perf/x86: Add ability to calculate TSC from perf sample timestamps")
    
    The only caller was arch_perf_update_userpage(), which had been refactored
    by commit:
    
      d8b11a0cbd1c ("perf/x86: Clean up cap_user_time* setting")
    
    ... so no need keep and export it any more.
    
    Signed-off-by: Wei Jiangang <weijg.fnst@cn.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: a.p.zijlstra@chello.nl
    Cc: adrian.hunter@intel.com
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/1468570330-25810-1-git-send-email-weijg.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index a30591e1567c..33b6365c22fe 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -35,7 +35,6 @@ extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
-extern int check_tsc_disabled(void);
 extern unsigned long native_calibrate_cpu(void);
 extern unsigned long native_calibrate_tsc(void);
 extern unsigned long long native_sched_clock_from_tsc(u64 tsc);

commit aa297292d708e89773b3b2cdcaf33f01bfa095d8
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jun 17 01:22:51 2016 -0400

    x86/tsc: Enumerate SKL cpu_khz and tsc_khz via CPUID
    
    Skylake CPU base-frequency and TSC frequency may differ
    by up to 2%.
    
    Enumerate CPU and TSC frequencies separately, allowing
    cpu_khz and tsc_khz to differ.
    
    The existing CPU frequency calibration mechanism is unchanged.
    However, CPUID extensions are preferred, when available.
    
    CPUID.0x16 is preferred over MSR and timer calibration
    for CPU frequency discovery.
    
    CPUID.0x15 takes precedence over CPU-frequency
    for TSC frequency discovery.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/b27ec289fd005833b27d694d9c2dbb716c5cdff7.1466138954.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index db1f779a3766..a30591e1567c 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -36,6 +36,7 @@ extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
 extern int check_tsc_disabled(void);
+extern unsigned long native_calibrate_cpu(void);
 extern unsigned long native_calibrate_tsc(void);
 extern unsigned long long native_sched_clock_from_tsc(u64 tsc);
 

commit 02c0cd2dcf7fdc47d054b855b148ea8b82dbb7eb
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jun 17 01:22:50 2016 -0400

    x86/tsc_msr: Remove irqoff around MSR-based TSC enumeration
    
    Remove the irqoff/irqon around MSR-based TSC enumeration,
    as it is not necessary.
    
    Also rename: try_msr_calibrate_tsc() to cpu_khz_from_msr(),
    as that better describes what the routine does.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/a6b5c3ecd3b068175d2309599ab28163fc34215e.1466138954.git.len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 7428697c5b8d..db1f779a3766 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -52,7 +52,6 @@ extern int notsc_setup(char *);
 extern void tsc_save_sched_clock_state(void);
 extern void tsc_restore_sched_clock_state(void);
 
-/* MSR based TSC calibration for Intel Atom SoC platforms */
-unsigned long try_msr_calibrate_tsc(void);
+unsigned long cpu_khz_from_msr(void);
 
 #endif /* _ASM_X86_TSC_H */

commit 59e21e3d00e6bc23186763c3e0bf11baf8924124
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 4 22:24:59 2016 +0200

    x86/cpufeature: Replace cpu_has_tsc with boot_cpu_has() usage
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Sailer <t.sailer@alumni.ethz.ch>
    Link: http://lkml.kernel.org/r/1459801503-15600-7-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 174c4212780a..7428697c5b8d 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -22,7 +22,7 @@ extern void disable_TSC(void);
 static inline cycles_t get_cycles(void)
 {
 #ifndef CONFIG_X86_TSC
-	if (!cpu_has_tsc)
+	if (!boot_cpu_has(X86_FEATURE_TSC))
 		return 0;
 #endif
 

commit f9677e0f83080bb4186865868c359e72e1fac1ea
Author: Christopher S. Hall <christopher.s.hall@intel.com>
Date:   Mon Feb 29 06:33:47 2016 -0800

    x86/tsc: Always Running Timer (ART) correlated clocksource
    
    On modern Intel systems TSC is derived from the new Always Running Timer
    (ART). ART can be captured simultaneous to the capture of
    audio and network device clocks, allowing a correlation between timebases
    to be constructed. Upon capture, the driver converts the captured ART
    value to the appropriate system clock using the correlated clocksource
    mechanism.
    
    On systems that support ART a new CPUID leaf (0x15) returns parameters
    “m” and “n” such that:
    
    TSC_value = (ART_value * m) / n + k [n >= 1]
    
    [k is an offset that can adjusted by a privileged agent. The
    IA32_TSC_ADJUST MSR is an example of an interface to adjust k.
    See 17.14.4 of the Intel SDM for more details]
    
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: kevin.b.stanton@intel.com
    Cc: kevin.j.clarke@intel.com
    Cc: hpa@zytor.com
    Cc: jeffrey.t.kirsher@intel.com
    Cc: netdev@vger.kernel.org
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christopher S. Hall <christopher.s.hall@intel.com>
    [jstultz: Tweaked to fix build issue, also reworked math for
    64bit division on 32bit systems, as well as !CONFIG_CPU_FREQ build
    fixes]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 6d7c5479bcea..174c4212780a 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -29,6 +29,8 @@ static inline cycles_t get_cycles(void)
 	return rdtsc();
 }
 
+extern struct system_counterval_t convert_art_to_tsc(cycle_t art);
+
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);

commit 5778077d03cb25aac9b6a428e18970642fc019e3
Merge: 65a99597f044 7e01ebffffed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 08:40:25 2015 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm changes from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - Revamp, simplify (and in some cases fix) Time Stamp Counter (TSC)
         primitives.  (Andy Lutomirski)
    
       - Add new, comprehensible entry and exit handlers written in C.
         (Andy Lutomirski)
    
       - vm86 mode cleanups and fixes.  (Brian Gerst)
    
       - 32-bit compat code cleanups.  (Brian Gerst)
    
      The amount of simplification in low level assembly code is already
      palpable:
    
         arch/x86/entry/entry_32.S                          | 130 +----
         arch/x86/entry/entry_64.S                          | 197 ++-----
    
      but more simplifications are planned.
    
      There's also the usual laudry mix of low level changes - see the
      changelog for details"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (83 commits)
      x86/asm: Drop repeated macro of X86_EFLAGS_AC definition
      x86/asm/msr: Make wrmsrl() a function
      x86/asm/delay: Introduce an MWAITX-based delay with a configurable timer
      x86/asm: Add MONITORX/MWAITX instruction support
      x86/traps: Weaken context tracking entry assertions
      x86/asm/tsc: Add rdtscll() merge helper
      selftests/x86: Add syscall_nt selftest
      selftests/x86: Disable sigreturn_64
      x86/vdso: Emit a GNU hash
      x86/entry: Remove do_notify_resume(), syscall_trace_leave(), and their TIF masks
      x86/entry/32: Migrate to C exit path
      x86/entry/32: Remove 32-bit syscall audit optimizations
      x86/vm86: Rename vm86->v86flags and v86mask
      x86/vm86: Rename vm86->vm86_info to user_vm86
      x86/vm86: Clean up vm86.h includes
      x86/vm86: Move the vm86 IRQ definitions to vm86.h
      x86/vm86: Use the normal pt_regs area for vm86
      x86/vm86: Eliminate 'struct kernel_vm86_struct'
      x86/vm86: Move fields from 'struct kernel_vm86_struct' to 'struct vm86'
      x86/vm86: Move vm86 fields out of 'thread_struct'
      ...

commit a94cab2376cb35f236be14e2833cef63a8762a31
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sun May 10 12:22:39 2015 -0700

    perf/x86: Add a native_perf_sched_clock_from_tsc()
    
    PEBSv3 has a raw TSC time stamp in its memory buffer that
    later needs to to be converted to perf_clock.
    
    Add a native_sched_clock_from_tsc() that works the same
    as native_sched_clock(), but starts with an already given
    TSC value.
    
    Paravirt is ignored, it will just get the native clock.
    But there isn't a para virtualized PEBS anyway.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/1431285767-27027-2-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 94605c0e9cee..aad56eb3bbe2 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -51,6 +51,7 @@ extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
 extern int check_tsc_disabled(void);
 extern unsigned long native_calibrate_tsc(void);
+extern unsigned long long native_sched_clock_from_tsc(u64 tsc);
 
 extern int tsc_clocksource_reliable;
 

commit 4ea1636b04dbd66536fa387bae2eea463efc705b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:44:07 2015 +0200

    x86/asm/tsc: Rename native_read_tsc() to rdtsc()
    
    Now that there is no paravirt TSC, the "native" is
    inappropriate. The function does RDTSC, so give it the obvious
    name: rdtsc().
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/fd43e16281991f096c1e4d21574d9e1402c62d39.1434501121.git.luto@kernel.org
    [ Ported it to v4.2-rc1. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index b4883902948b..3df7675debcf 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -26,7 +26,7 @@ static inline cycles_t get_cycles(void)
 		return 0;
 #endif
 
-	return native_read_tsc();
+	return rdtsc();
 }
 
 extern void tsc_init(void);

commit 87be28aaf1458445d5f648688c2eec0f13b8f3b9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:58 2015 +0200

    x86/asm/tsc: Replace rdtscll() with native_read_tsc()
    
    Now that the ->read_tsc() paravirt hook is gone, rdtscll() is
    just a wrapper around native_read_tsc(). Unwrap it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/d2449ae62c1b1fb90195bcfb19ef4a35883a04dc.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 3da1cc1218ac..b4883902948b 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -21,15 +21,12 @@ extern void disable_TSC(void);
 
 static inline cycles_t get_cycles(void)
 {
-	unsigned long long ret = 0;
-
 #ifndef CONFIG_X86_TSC
 	if (!cpu_has_tsc)
 		return 0;
 #endif
-	rdtscll(ret);
 
-	return ret;
+	return native_read_tsc();
 }
 
 extern void tsc_init(void);

commit 881d7bf843d7139c6dfbffdec4903b3354423c49
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:56 2015 +0200

    x86/asm/tsc, kvm: Remove vget_cycles()
    
    The only caller was KVM's read_tsc(). The only difference
    between vget_cycles() and native_read_tsc() was that
    vget_cycles() returned zero instead of crashing on TSC-less
    systems. KVM already checks vclock_mode() before calling that
    function, so the extra check is unnecessary. Also, KVM
    (host-side) requires the TSC to exist.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/20615df14ae2eb713ea7a5f5123c1dc4c7ca993d.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index fd11128faf25..3da1cc1218ac 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -32,19 +32,6 @@ static inline cycles_t get_cycles(void)
 	return ret;
 }
 
-static __always_inline cycles_t vget_cycles(void)
-{
-	/*
-	 * We only do VDSOs on TSC capable CPUs, so this shouldn't
-	 * access boot_cpu_data (which is not VDSO-safe):
-	 */
-#ifndef CONFIG_X86_TSC
-	if (!cpu_has_tsc)
-		return 0;
-#endif
-	return (cycles_t)native_read_tsc();
-}
-
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);

commit c6e5ca35c4685cd920b1d5279dbc9f4483d7dfd4
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:55 2015 +0200

    x86/asm/tsc: Inline native_read_tsc() and remove __native_read_tsc()
    
    In the following commit:
    
      cdc7957d1954 ("x86: move native_read_tsc() offline")
    
    ... native_read_tsc() was moved out of line, presumably for some
    now-obsolete vDSO-related reason. Undo it.
    
    The entire rdtsc, shl, or sequence is only 11 bytes, and calls
    via rdtscl() and similar helpers were already inlined.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/d05ffe2aaf8468ca475ebc00efad7b2fa174af19.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 94605c0e9cee..fd11128faf25 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -42,7 +42,7 @@ static __always_inline cycles_t vget_cycles(void)
 	if (!cpu_has_tsc)
 		return 0;
 #endif
-	return (cycles_t)__native_read_tsc();
+	return (cycles_t)native_read_tsc();
 }
 
 extern void tsc_init(void);

commit 5f0e030930d715920be4de638084aaf8653867e8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 19 13:52:29 2014 +0200

    x86, tsc: Fallback to normal calibration if fast MSR calibration fails
    
    If we cannot calibrate TSC via MSR based calibration
    try_msr_calibrate_tsc() stores zero to fast_calibrate and returns that
    to the caller. This value gets then propagated further to clockevents
    code resulting division by zero oops like the one below:
    
     divide error: 0000 [#1] PREEMPT SMP
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Tainted: G        W    3.13.0+ #47
     task: ffff880075508000 ti: ffff880075506000 task.ti: ffff880075506000
     RIP: 0010:[<ffffffff810aec14>]  [<ffffffff810aec14>] clockevents_config.part.3+0x24/0xa0
     RSP: 0000:ffff880075507e58  EFLAGS: 00010246
     RAX: ffffffffffffffff RBX: ffff880079c0cd80 RCX: 0000000000000000
     RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffffffffffffffff
     RBP: ffff880075507e70 R08: 0000000000000001 R09: 00000000000000be
     R10: 00000000000000bd R11: 0000000000000003 R12: 000000000000b008
     R13: 0000000000000008 R14: 000000000000b010 R15: 0000000000000000
     FS:  0000000000000000(0000) GS:ffff880079c00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: ffff880079fff000 CR3: 0000000001c0b000 CR4: 00000000001006f0
     Stack:
      ffff880079c0cd80 000000000000b008 0000000000000008 ffff880075507e88
      ffffffff810aecb0 ffff880079c0cd80 ffff880075507e98 ffffffff81030168
      ffff880075507ed8 ffffffff81d1104f 00000000000000c3 0000000000000000
     Call Trace:
      [<ffffffff810aecb0>] clockevents_config_and_register+0x20/0x30
      [<ffffffff81030168>] setup_APIC_timer+0xc8/0xd0
      [<ffffffff81d1104f>] setup_boot_APIC_clock+0x4cc/0x4d8
      [<ffffffff81d0f5de>] native_smp_prepare_cpus+0x3dd/0x3f0
      [<ffffffff81d02ee9>] kernel_init_freeable+0xc3/0x205
      [<ffffffff8177c910>] ? rest_init+0x90/0x90
      [<ffffffff8177c91e>] kernel_init+0xe/0x120
      [<ffffffff8178deec>] ret_from_fork+0x7c/0xb0
      [<ffffffff8177c910>] ? rest_init+0x90/0x90
    
    Prevent this from happening by:
     1) Modifying try_msr_calibrate_tsc() to return calibration value or zero
        if it fails.
     2) Check this return value in native_calibrate_tsc() and in case of zero
        fallback to use normal non-MSR based calibration.
    
    [mw: Added subject and changelog]
    
    Reported-and-tested-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bin Gao <bin.gao@linux.intel.com>
    Cc: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1392810750-18660-1-git-send-email-mika.westerberg@linux.intel.com
    Signed-off-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 57ae63cd6ee2..94605c0e9cee 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -66,6 +66,6 @@ extern void tsc_save_sched_clock_state(void);
 extern void tsc_restore_sched_clock_state(void);
 
 /* MSR based TSC calibration for Intel Atom SoC platforms */
-int try_msr_calibrate_tsc(unsigned long *fast_calibrate);
+unsigned long try_msr_calibrate_tsc(void);
 
 #endif /* _ASM_X86_TSC_H */

commit 7da7c1561366ba8adb7275464ab44e84e1faa7e0
Author: Bin Gao <bin.gao@intel.com>
Date:   Mon Oct 21 09:16:33 2013 -0700

    x86, tsc: Add static (MSR) TSC calibration on Intel Atom SoCs
    
    On SoCs that have the calibration MSRs available, either there is no
    PIT, HPET or PMTIMER to calibrate against, or the PIT/HPET/PMTIMER is
    driven from the same clock as the TSC, so calibration is redundant and
    just slows down the boot.
    
    TSC rate is caculated by this formula:
    <maximum core-clock to bus-clock ratio> * <maximum resolved frequency>
    The ratio and the resolved frequency ID can be obtained from MSR.
    See Intel 64 and IA-32 System Programming Guid section 16.12 and 30.11.5
    for details.
    
    Signed-off-by: Bin Gao <bin.gao@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/n/tip-rgm7xmg7k6qnjlw3ynkcjsmh@git.kernel.org

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 235be70d5bb4..57ae63cd6ee2 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -65,4 +65,7 @@ extern int notsc_setup(char *);
 extern void tsc_save_sched_clock_state(void);
 extern void tsc_restore_sched_clock_state(void);
 
+/* MSR based TSC calibration for Intel Atom SoC platforms */
+int try_msr_calibrate_tsc(unsigned long *fast_calibrate);
+
 #endif /* _ASM_X86_TSC_H */

commit c73deb6aecda2955716f31572516f09d930ef450
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Fri Jun 28 16:22:18 2013 +0300

    perf/x86: Add ability to calculate TSC from perf sample timestamps
    
    For modern CPUs, perf clock is directly related to TSC.  TSC
    can be calculated from perf clock and vice versa using a simple
    calculation.  Two of the three componenets of that calculation
    are already exported in struct perf_event_mmap_page.  This patch
    exports the third.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1372425741-1676-3-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index c91e8b9d588b..235be70d5bb4 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -49,6 +49,7 @@ extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
+extern int check_tsc_disabled(void);
 extern unsigned long native_calibrate_tsc(void);
 
 extern int tsc_clocksource_reliable;

commit b74f05d61b73af584d0c39121980171389ecfaaa
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Feb 13 11:07:27 2012 -0200

    x86: kvmclock: abstract save/restore sched_clock_state
    
    Upon resume from hibernation, CPU 0's hvclock area contains the old
    values for system_time and tsc_timestamp. It is necessary for the
    hypervisor to update these values with uptodate ones before the CPU uses
    them.
    
    Abstract TSC's save/restore sched_clock_state functions and use
    restore_state to write to KVM_SYSTEM_TIME MSR, forcing an update.
    
    Also move restore_sched_clock_state before __restore_processor_state,
    since the later calls CONFIG_LOCK_STAT's lockstat_clock (also for TSC).
    Thanks to Igor Mammedov for tracking it down.
    
    Fixes suspend-to-disk with kvmclock.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 15d99153a96d..c91e8b9d588b 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -61,7 +61,7 @@ extern void check_tsc_sync_source(int cpu);
 extern void check_tsc_sync_target(void);
 
 extern int notsc_setup(char *);
-extern void save_sched_clock_state(void);
-extern void restore_sched_clock_state(void);
+extern void tsc_save_sched_clock_state(void);
+extern void tsc_restore_sched_clock_state(void);
 
 #endif /* _ASM_X86_TSC_H */

commit 28a00184be261e3dc152ba0d664a067bbe235b6a
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Nov 4 15:42:17 2011 -0700

    x86, tsc: Skip TSC synchronization checks for tsc=reliable
    
    tsc=reliable boot parameter is supposed to skip all the TSC
    stablility checks during boot time.
    
    On a 8-socket system where we want to run an experiment with the
    "tsc=reliable" boot option, TSC synchronization checks are not
    getting skipped and marking the TSC as not stable.
    
    Check for tsc_clocksource_reliable (which is set via
    tsc=reliable or for platforms supporting synthetic TSC_RELIABLE
    feature bit etc) and when set, skip the TSC synchronization
    tests during boot.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Tested-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1320446537.15071.14.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 83e2efd181e2..15d99153a96d 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -51,6 +51,8 @@ extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
 extern unsigned long native_calibrate_tsc(void);
 
+extern int tsc_clocksource_reliable;
+
 /*
  * Boot-time check whether the TSCs are synchronized across
  * all CPUs/cores:

commit 98d0ac38ca7b1b7a552c9a2359174ff84decb600
Author: Andy Lutomirski <luto@mit.edu>
Date:   Thu Jul 14 06:47:22 2011 -0400

    x86-64: Move vread_tsc and vread_hpet into the vDSO
    
    The vsyscall page now consists entirely of trap instructions.
    
    Cc: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Link: http://lkml.kernel.org/r/637648f303f2ef93af93bae25186e9a1bea093f5.1310639973.git.luto@mit.edu
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 9db5583b6d38..83e2efd181e2 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -51,10 +51,6 @@ extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
 extern unsigned long native_calibrate_tsc(void);
 
-#ifdef CONFIG_X86_64
-extern cycles_t vread_tsc(void);
-#endif
-
 /*
  * Boot-time check whether the TSCs are synchronized across
  * all CPUs/cores:

commit 44259b1abfaa8bb819d25d41d71e8e33e25dd36a
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Mon May 23 09:31:28 2011 -0400

    x86-64: Move vread_tsc into a new file with sensible options
    
    vread_tsc is short and hot, and it's userspace code so the usual
    reasons to enable -pg and turn off sibling calls don't apply.
    
    (OK, turning off sibling calls has no effect.  But it might
    someday...)
    
    As an added benefit, tsc.c is profilable now.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@amd64.org>
    Link: http://lkml.kernel.org/r/%3C99c6d7f5efa3ccb65b4ac6eb443e1ab7bad47d7b.1306156808.git.luto%40mit.edu%3E
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 83e2efd181e2..9db5583b6d38 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -51,6 +51,10 @@ extern int unsynchronized_tsc(void);
 extern int check_tsc_unstable(void);
 extern unsigned long native_calibrate_tsc(void);
 
+#ifdef CONFIG_X86_64
+extern cycles_t vread_tsc(void);
+#endif
+
 /*
  * Boot-time check whether the TSCs are synchronized across
  * all CPUs/cores:

commit 0d2eb44f631d9d0a826efa3156f157477fdaecf4
Author: Lucas De Marchi <lucas.de.marchi@gmail.com>
Date:   Thu Mar 17 16:24:16 2011 -0300

    x86: Fix common misspellings
    
    They were generated by 'codespell' and then manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>
    Cc: trivial@kernel.org
    LKML-Reference: <1300389856-1099-3-git-send-email-lucas.demarchi@profusion.mobi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 1ca132fc0d03..83e2efd181e2 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -35,7 +35,7 @@ static inline cycles_t get_cycles(void)
 static __always_inline cycles_t vget_cycles(void)
 {
 	/*
-	 * We only do VDSOs on TSC capable CPUs, so this shouldnt
+	 * We only do VDSOs on TSC capable CPUs, so this shouldn't
 	 * access boot_cpu_data (which is not VDSO-safe):
 	 */
 #ifndef CONFIG_X86_TSC

commit cd7240c0b900eb6d690ccee088a6c9b46dae815a
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Aug 19 17:03:38 2010 -0700

    x86, tsc, sched: Recompute cyc2ns_offset's during resume from sleep states
    
    TSC's get reset after suspend/resume (even on cpu's with invariant TSC
    which runs at a constant rate across ACPI P-, C- and T-states). And in
    some systems BIOS seem to reinit TSC to arbitrary large value (still
    sync'd across cpu's) during resume.
    
    This leads to a scenario of scheduler rq->clock (sched_clock_cpu()) less
    than rq->age_stamp (introduced in 2.6.32). This leads to a big value
    returned by scale_rt_power() and the resulting big group power set by the
    update_group_power() is causing improper load balancing between busy and
    idle cpu's after suspend/resume.
    
    This resulted in multi-threaded workloads (like kernel-compilation) go
    slower after suspend/resume cycle on core i5 laptops.
    
    Fix this by recomputing cyc2ns_offset's during resume, so that
    sched_clock() continues from the point where it was left off during
    suspend.
    
    Reported-by: Florian Pritz <flo@xssn.at>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: <stable@kernel.org> # [v2.6.32+]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1282262618.2675.24.camel@sbsiddha-MOBL3.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index c0427295e8f5..1ca132fc0d03 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -59,5 +59,7 @@ extern void check_tsc_sync_source(int cpu);
 extern void check_tsc_sync_target(void);
 
 extern int notsc_setup(char *);
+extern void save_sched_clock_state(void);
+extern void restore_sched_clock_state(void);
 
 #endif /* _ASM_X86_TSC_H */

commit 2d826404f0bdcac2a4dd7e3c446b70d6a3b63b78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 17:06:25 2009 +0200

    x86: Move tsc_calibration to x86_init_ops
    
    TSC calibration is modified by the vmware hypervisor and paravirt by
    separate means. Moorestown wants to add its own calibration routine as
    well. So make calibrate_tsc a proper x86_init_ops function and
    override it by paravirt or by the early setup of the vmware
    hypervisor.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 38ae163cc91b..c0427295e8f5 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -48,7 +48,8 @@ static __always_inline cycles_t vget_cycles(void)
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
-int check_tsc_unstable(void);
+extern int check_tsc_unstable(void);
+extern unsigned long native_calibrate_tsc(void);
 
 /*
  * Boot-time check whether the TSCs are synchronized across

commit 4fcc50abdffb517cee36cec9cb22138d84fb62d0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 9 08:10:03 2008 +0100

    x86: clean up vget_cycles()
    
    Impact: remove unused variable
    
    I forgot to remove the now unused "cycles_t cycles" parameter from
    vget_cycles() - which triggers build warnings as tsc.h is included
    in a number of files.
    
    Remove it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 700aeb8d2098..38ae163cc91b 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -34,8 +34,6 @@ static inline cycles_t get_cycles(void)
 
 static __always_inline cycles_t vget_cycles(void)
 {
-	cycles_t cycles;
-
 	/*
 	 * We only do VDSOs on TSC capable CPUs, so this shouldnt
 	 * access boot_cpu_data (which is not VDSO-safe):

commit cb9e35dce94a1b9c59d46224e8a94377d673e204
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 8 20:27:00 2008 +0100

    x86: clean up rdtsc_barrier() use
    
    Impact: cleanup
    
    Move rdtsc_barrier() use to vsyscall_64.c where it's relied on,
    and point out its role in the context of its use.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 9cd83a8e40d5..700aeb8d2098 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -44,11 +44,7 @@ static __always_inline cycles_t vget_cycles(void)
 	if (!cpu_has_tsc)
 		return 0;
 #endif
-	rdtsc_barrier();
-	cycles = (cycles_t)__native_read_tsc();
-	rdtsc_barrier();
-
-	return cycles;
+	return (cycles_t)__native_read_tsc();
 }
 
 extern void tsc_init(void);

commit 0d12cdd5f883f508d33b85c1bae98fa28987c8c7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 8 16:19:55 2008 +0100

    sched: improve sched_clock() performance
    
    in scheduler-intense workloads native_read_tsc() overhead accounts for
    20% of the system overhead:
    
     659567 system_call                              41222.9375
     686796 schedule                                 435.7843
     718382 __switch_to                              665.1685
     823875 switch_mm                                4526.7857
     1883122 native_read_tsc                          55385.9412
     9761990 total                                      2.8468
    
    this is large part due to the rdtsc_barrier() that is done before
    and after reading the TSC.
    
    But sched_clock() is not a precise clock in the GTOD sense, using such
    barriers is completely pointless. So remove the barriers and only use
    them in vget_cycles().
    
    This improves lat_ctx performance by about 5%.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 38ae163cc91b..9cd83a8e40d5 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -34,6 +34,8 @@ static inline cycles_t get_cycles(void)
 
 static __always_inline cycles_t vget_cycles(void)
 {
+	cycles_t cycles;
+
 	/*
 	 * We only do VDSOs on TSC capable CPUs, so this shouldnt
 	 * access boot_cpu_data (which is not VDSO-safe):
@@ -42,7 +44,11 @@ static __always_inline cycles_t vget_cycles(void)
 	if (!cpu_has_tsc)
 		return 0;
 #endif
-	return (cycles_t)__native_read_tsc();
+	rdtsc_barrier();
+	cycles = (cycles_t)__native_read_tsc();
+	rdtsc_barrier();
+
+	return cycles;
 }
 
 extern void tsc_init(void);

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index ad0f5c41e78c..38ae163cc91b 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -1,8 +1,8 @@
 /*
  * x86 TSC related functions
  */
-#ifndef ASM_X86__TSC_H
-#define ASM_X86__TSC_H
+#ifndef _ASM_X86_TSC_H
+#define _ASM_X86_TSC_H
 
 #include <asm/processor.h>
 
@@ -59,4 +59,4 @@ extern void check_tsc_sync_target(void);
 
 extern int notsc_setup(char *);
 
-#endif /* ASM_X86__TSC_H */
+#endif /* _ASM_X86_TSC_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
new file mode 100644
index 000000000000..ad0f5c41e78c
--- /dev/null
+++ b/arch/x86/include/asm/tsc.h
@@ -0,0 +1,62 @@
+/*
+ * x86 TSC related functions
+ */
+#ifndef ASM_X86__TSC_H
+#define ASM_X86__TSC_H
+
+#include <asm/processor.h>
+
+#define NS_SCALE	10 /* 2^10, carefully chosen */
+#define US_SCALE	32 /* 2^32, arbitralrily chosen */
+
+/*
+ * Standard way to access the cycle counter.
+ */
+typedef unsigned long long cycles_t;
+
+extern unsigned int cpu_khz;
+extern unsigned int tsc_khz;
+
+extern void disable_TSC(void);
+
+static inline cycles_t get_cycles(void)
+{
+	unsigned long long ret = 0;
+
+#ifndef CONFIG_X86_TSC
+	if (!cpu_has_tsc)
+		return 0;
+#endif
+	rdtscll(ret);
+
+	return ret;
+}
+
+static __always_inline cycles_t vget_cycles(void)
+{
+	/*
+	 * We only do VDSOs on TSC capable CPUs, so this shouldnt
+	 * access boot_cpu_data (which is not VDSO-safe):
+	 */
+#ifndef CONFIG_X86_TSC
+	if (!cpu_has_tsc)
+		return 0;
+#endif
+	return (cycles_t)__native_read_tsc();
+}
+
+extern void tsc_init(void);
+extern void mark_tsc_unstable(char *reason);
+extern int unsynchronized_tsc(void);
+int check_tsc_unstable(void);
+
+/*
+ * Boot-time check whether the TSCs are synchronized across
+ * all CPUs/cores:
+ */
+extern void check_tsc_sync_source(int cpu);
+extern void check_tsc_sync_target(void);
+
+extern int notsc_setup(char *);
+
+#endif /* ASM_X86__TSC_H */
