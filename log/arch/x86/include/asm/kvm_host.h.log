commit 2dbebf7ae1ed9a420d954305e2c9d5ed39ec57c3
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Jun 22 14:58:29 2020 -0700

    KVM: nVMX: Plumb L2 GPA through to PML emulation
    
    Explicitly pass the L2 GPA to kvm_arch_write_log_dirty(), which for all
    intents and purposes is vmx_write_pml_buffer(), instead of having the
    latter pull the GPA from vmcs.GUEST_PHYSICAL_ADDRESS.  If the dirty bit
    update is the result of KVM emulation (rare for L2), then the GPA in the
    VMCS may be stale and/or hold a completely unrelated GPA.
    
    Fixes: c5f983f6e8455 ("nVMX: Implement emulated Page Modification Logging")
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200622215832.22090-2-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f852ee350beb..be5363b21540 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1220,7 +1220,7 @@ struct kvm_x86_ops {
 	void (*enable_log_dirty_pt_masked)(struct kvm *kvm,
 					   struct kvm_memory_slot *slot,
 					   gfn_t offset, unsigned long mask);
-	int (*write_log_dirty)(struct kvm_vcpu *vcpu);
+	int (*write_log_dirty)(struct kvm_vcpu *vcpu, gpa_t l2_gpa);
 
 	/* pmu operations of sub-arch */
 	const struct kvm_pmu_ops *pmu_ops;

commit 44d527170731c75587e95052f3eea72b8c651daf
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jun 22 16:37:42 2020 +0200

    KVM: LAPIC: ensure APIC map is up to date on concurrent update requests
    
    The following race can cause lost map update events:
    
             cpu1                            cpu2
    
                                    apic_map_dirty = true
      ------------------------------------------------------------
                                    kvm_recalculate_apic_map:
                                         pass check
                                             mutex_lock(&kvm->arch.apic_map_lock);
                                             if (!kvm->arch.apic_map_dirty)
                                         and in process of updating map
      -------------------------------------------------------------
        other calls to
           apic_map_dirty = true         might be too late for affected cpu
      -------------------------------------------------------------
                                         apic_map_dirty = false
      -------------------------------------------------------------
        kvm_recalculate_apic_map:
        bail out on
          if (!kvm->arch.apic_map_dirty)
    
    To fix it, record the beginning of an update of the APIC map in
    apic_map_dirty.  If another APIC map change switches apic_map_dirty
    back to DIRTY during the update, kvm_recalculate_apic_map should not
    make it CLEAN, and the other caller will go through the slow path.
    
    Reported-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f8998e97457f..f852ee350beb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -943,7 +943,7 @@ struct kvm_arch {
 	atomic_t vapics_in_nmi_mode;
 	struct mutex apic_map_lock;
 	struct kvm_apic_map *apic_map;
-	bool apic_map_dirty;
+	atomic_t apic_map_dirty;
 
 	bool apic_access_page_done;
 	unsigned long apicv_inhibit_reasons;

commit 2a18b7e7cd8882f626316c340c6f2fca49b5fa12
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Jun 10 19:55:32 2020 +0200

    KVM: async_pf: Inject 'page ready' event only if 'page not present' was previously injected
    
    'Page not present' event may or may not get injected depending on
    guest's state. If the event wasn't injected, there is no need to
    inject the corresponding 'page ready' event as the guest may get
    confused. E.g. Linux thinks that the corresponding 'page not present'
    event wasn't delivered *yet* and allocates a 'dummy entry' for it.
    This entry is never freed.
    
    Note, 'wakeup all' events have no corresponding 'page not present'
    event and always get injected.
    
    s390 seems to always be able to inject 'page not present', the
    change is effectively a nop.
    
    Suggested-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200610175532.779793-2-vkuznets@redhat.com>
    Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=208081
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7030f2221259..f8998e97457f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1670,7 +1670,7 @@ void kvm_make_scan_ioapic_request(struct kvm *kvm);
 void kvm_make_scan_ioapic_request_mask(struct kvm *kvm,
 				       unsigned long *vcpu_bitmap);
 
-void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
+bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work);
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);

commit 80fbd280beea1bcc298660f9024cbc4bf392c7f7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Jun 8 11:02:18 2020 -0700

    KVM: x86: Unexport x86_fpu_cache and make it static
    
    Make x86_fpu_cache static now that FPU allocation and destruction is
    handled entirely by common x86 code.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200608180218.20946-1-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1da5858501ca..7030f2221259 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1306,7 +1306,6 @@ struct kvm_arch_async_pf {
 extern u64 __read_mostly host_efer;
 
 extern struct kvm_x86_ops kvm_x86_ops;
-extern struct kmem_cache *x86_fpu_cache;
 
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit 6b2591c21273ebf65c13dae5d260ce88f0f197dd
Merge: f1e455352b6f afaa33da08ab
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:00:05 2020 -0700

    Merge tag 'hyperv-next-signed' of git://git.kernel.org/pub/scm/linux/kernel/git/hyperv/linux
    
    Pull hyper-v updates from Wei Liu:
    
     - a series from Andrea to support channel reassignment
    
     - a series from Vitaly to clean up Vmbus message handling
    
     - a series from Michael to clean up and augment hyperv-tlfs.h
    
     - patches from Andy to clean up GUID usage in Hyper-V code
    
     - a few other misc patches
    
    * tag 'hyperv-next-signed' of git://git.kernel.org/pub/scm/linux/kernel/git/hyperv/linux: (29 commits)
      Drivers: hv: vmbus: Resolve more races involving init_vp_index()
      Drivers: hv: vmbus: Resolve race between init_vp_index() and CPU hotplug
      vmbus: Replace zero-length array with flexible-array
      Driver: hv: vmbus: drop a no long applicable comment
      hyper-v: Switch to use UUID types directly
      hyper-v: Replace open-coded variant of %*phN specifier
      hyper-v: Supply GUID pointer to printf() like functions
      hyper-v: Use UUID API for exporting the GUID (part 2)
      asm-generic/hyperv: Add definitions for Get/SetVpRegister hypercalls
      x86/hyperv: Split hyperv-tlfs.h into arch dependent and independent files
      x86/hyperv: Remove HV_PROCESSOR_POWER_STATE #defines
      KVM: x86: hyperv: Remove duplicate definitions of Reference TSC Page
      drivers: hv: remove redundant assignment to pointer primary_channel
      scsi: storvsc: Re-init stor_chns when a channel interrupt is re-assigned
      Drivers: hv: vmbus: Introduce the CHANNELMSG_MODIFYCHANNEL message type
      Drivers: hv: vmbus: Synchronize init_vp_index() vs. CPU hotplug
      Drivers: hv: vmbus: Remove the unused HV_LOCALIZED channel affinity logic
      PCI: hv: Prepare hv_compose_msi_msg() for the VMBus-channel-interrupt-to-vCPU reassignment functionality
      Drivers: hv: vmbus: Use a spin lock for synchronizing channel scheduling vs. channel removal
      hv_utils: Always execute the fcopy and vss callbacks in a tasklet
      ...

commit 88dca4ca5a93d2c09e5bbc6a62fbfc3af83c4fca
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:40 2020 -0700

    mm: remove the pgprot argument to __vmalloc
    
    The pgprot argument to __vmalloc is always PAGE_KERNEL now, so remove it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com> [hyperv]
    Acked-by: Gao Xiang <xiang@kernel.org> [erofs]
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Wei Liu <wei.liu@kernel.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-22-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0a6b35353fc7..e94b3de564d6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1279,8 +1279,7 @@ extern struct kmem_cache *x86_fpu_cache;
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
-	return __vmalloc(kvm_x86_ops.vm_size,
-			 GFP_KERNEL_ACCOUNT | __GFP_ZERO, PAGE_KERNEL);
+	return __vmalloc(kvm_x86_ops.vm_size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
 }
 void kvm_arch_free_vm(struct kvm *kvm);
 

commit f97f5a56f5977311f3833056a73cdbb0ee56cb1e
Author: Jon Doron <arilou@gmail.com>
Date:   Fri May 29 16:45:40 2020 +0300

    x86/kvm/hyper-v: Add support for synthetic debugger interface
    
    Add support for Hyper-V synthetic debugger (syndbg) interface.
    The syndbg interface is using MSRs to emulate a way to send/recv packets
    data.
    
    The debug transport dll (kdvm/kdnet) will identify if Hyper-V is enabled
    and if it supports the synthetic debugger interface it will attempt to
    use it, instead of trying to initialize a network adapter.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Jon Doron <arilou@gmail.com>
    Message-Id: <20200529134543.1127440-4-arilou@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b878fcd164ce..58337a25396a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -863,6 +863,18 @@ struct kvm_apic_map {
 	struct kvm_lapic *phys_map[];
 };
 
+/* Hyper-V synthetic debugger (SynDbg)*/
+struct kvm_hv_syndbg {
+	struct {
+		u64 control;
+		u64 status;
+		u64 send_page;
+		u64 recv_page;
+		u64 pending_page;
+	} control;
+	u64 options;
+};
+
 /* Hyper-V emulation context */
 struct kvm_hv {
 	struct mutex hv_lock;
@@ -886,6 +898,7 @@ struct kvm_hv {
 	atomic_t num_mismatched_vp_indexes;
 
 	struct hv_partition_assist_pg *hv_pa_pg;
+	struct kvm_hv_syndbg hv_syndbg;
 };
 
 enum kvm_irqchip_mode {

commit 27461da31089ace6966e4f1695cba7eb87ffbe4f
Author: Like Xu <like.xu@linux.intel.com>
Date:   Fri May 29 15:43:45 2020 +0800

    KVM: x86/pmu: Support full width counting
    
    Intel CPUs have a new alternative MSR range (starting from MSR_IA32_PMC0)
    for GP counters that allows writing the full counter width. Enable this
    range from a new capability bit (IA32_PERF_CAPABILITIES.FW_WRITE[bit 13]).
    
    The guest would query CPUID to get the counter width, and sign extends
    the counter values as needed. The traditional MSRs always limit to 32bit,
    even though the counter internally is larger (48 or 57 bits).
    
    When the new capability is set, use the alternative range which do not
    have these restrictions. This lowers the overhead of perf stat slightly
    because it has to do less interrupts to accumulate the counter value.
    
    Signed-off-by: Like Xu <like.xu@linux.intel.com>
    Message-Id: <20200529074347.124619-3-like.xu@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f345bcf38cfb..b878fcd164ce 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -601,6 +601,7 @@ struct kvm_vcpu_arch {
 	u64 ia32_xss;
 	u64 microcode_version;
 	u64 arch_capabilities;
+	u64 perf_capabilities;
 
 	/*
 	 * Paging state of the vcpu

commit 557a961abbe06ed9dfd3b55ef7bd6e68295cda3d
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon May 25 16:41:21 2020 +0200

    KVM: x86: acknowledgment mechanism for async pf page ready notifications
    
    If two page ready notifications happen back to back the second one is not
    delivered and the only mechanism we currently have is
    kvm_check_async_pf_completion() check in vcpu_run() loop. The check will
    only be performed with the next vmexit when it happens and in some cases
    it may take a while. With interrupt based page ready notification delivery
    the situation is even worse: unlike exceptions, interrupts are not handled
    immediately so we must check if the slot is empty. This is slow and
    unnecessary. Introduce dedicated MSR_KVM_ASYNC_PF_ACK MSR to communicate
    the fact that the slot is free and host should check its notification
    queue. Mandate using it for interrupt based 'page ready' APF event
    delivery.
    
    As kvm_check_async_pf_completion() is going away from vcpu_run() we need
    a way to communicate the fact that vcpu->async_pf.done queue has
    transitioned from empty to non-empty state. Introduce
    kvm_arch_async_page_present_queued() and KVM_REQ_APF_READY to do the job.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200525144125.143875-7-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2d39571451a0..f345bcf38cfb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -86,6 +86,7 @@
 #define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
 #define KVM_REQ_HV_TLB_FLUSH \
 	KVM_ARCH_REQ_FLAGS(27, KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_APF_READY		KVM_ARCH_REQ(28)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
@@ -775,6 +776,7 @@ struct kvm_vcpu_arch {
 		u32 host_apf_flags;
 		unsigned long nested_apf_token;
 		bool delivery_as_pf_vmexit;
+		bool pageready_pending;
 	} apf;
 
 	/* OSVW MSRs (AMD only) */
@@ -1662,6 +1664,7 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 			       struct kvm_async_pf *work);
+void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu);
 bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 

commit 2635b5c4a0e407b84f68e188c719f28ba0e9ae1b
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon May 25 16:41:20 2020 +0200

    KVM: x86: interrupt based APF 'page ready' event delivery
    
    Concerns were expressed around APF delivery via synthetic #PF exception as
    in some cases such delivery may collide with real page fault. For 'page
    ready' notifications we can easily switch to using an interrupt instead.
    Introduce new MSR_KVM_ASYNC_PF_INT mechanism and deprecate the legacy one.
    
    One notable difference between the two mechanisms is that interrupt may not
    get handled immediately so whenever we would like to deliver next event
    (regardless of its type) we must be sure the guest had read and cleared
    previous event in the slot.
    
    While on it, get rid on 'type 1/type 2' names for APF events in the
    documentation as they are causing confusion. Use 'page not present'
    and 'page ready' everywhere instead.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200525144125.143875-6-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f3897e417b69..2d39571451a0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -767,7 +767,9 @@ struct kvm_vcpu_arch {
 		bool halted;
 		gfn_t gfns[ASYNC_PF_PER_VCPU];
 		struct gfn_to_hva_cache data;
-		u64 msr_val;
+		u64 msr_en_val; /* MSR_KVM_ASYNC_PF_EN */
+		u64 msr_int_val; /* MSR_KVM_ASYNC_PF_INT */
+		u16 vec;
 		u32 id;
 		bool send_user_only;
 		u32 host_apf_flags;

commit 7c0ade6c9023b2b90b757e2927b306bec1cc4ca6
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon May 25 16:41:18 2020 +0200

    KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
    
    An innocent reader of the following x86 KVM code:
    
    bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu)
    {
            if (!(vcpu->arch.apf.msr_val & KVM_ASYNC_PF_ENABLED))
                    return true;
    ...
    
    may get very confused: if APF mechanism is not enabled, why do we report
    that we 'can inject async page present'? In reality, upon injection
    kvm_arch_async_page_present() will check the same condition again and,
    in case APF is disabled, will just drop the item. This is fine as the
    guest which deliberately disabled APF doesn't expect to get any APF
    notifications.
    
    Rename kvm_arch_can_inject_async_page_present() to
    kvm_arch_can_dequeue_async_page_present() to make it clear what we are
    checking: if the item can be dequeued (meaning either injected or just
    dropped).
    
    On s390 kvm_arch_can_inject_async_page_present() always returns 'true' so
    the rename doesn't matter much.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200525144125.143875-4-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 033f6173a857..f3897e417b69 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1660,7 +1660,7 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 			       struct kvm_async_pf *work);
-bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu);
+bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 int kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu);

commit 68fd66f100d196d35ab3008d4c69af3a0d7e7200
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon May 25 16:41:17 2020 +0200

    KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
    
    Currently, APF mechanism relies on the #PF abuse where the token is being
    passed through CR2. If we switch to using interrupts to deliver page-ready
    notifications we need a different way to pass the data. Extent the existing
    'struct kvm_vcpu_pv_apf_data' with token information for page-ready
    notifications.
    
    While on it, rename 'reason' to 'flags'. This doesn't change the semantics
    as we only have reasons '1' and '2' and these can be treated as bit flags
    but KVM_PV_REASON_PAGE_READY is going away with interrupt based delivery
    making 'reason' name misleading.
    
    The newly introduced apf_put_user_ready() temporary puts both flags and
    token information, this will be changed to put token only when we switch
    to interrupt based notifications.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200525144125.143875-3-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3485f8454088..033f6173a857 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -770,7 +770,7 @@ struct kvm_vcpu_arch {
 		u64 msr_val;
 		u32 id;
 		bool send_user_only;
-		u32 host_apf_reason;
+		u32 host_apf_flags;
 		unsigned long nested_apf_token;
 		bool delivery_as_pf_vmexit;
 	} apf;

commit 08245e6d2e589f2b6e9e275ddb343e2ec9ce94ec
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 19 09:21:04 2020 -0400

    KVM: nSVM: remove HF_HIF_MASK
    
    The L1 flags can be found in the save area of svm->nested.hsave, fish
    it from there so that there is one fewer thing to migrate.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0dfc522f96cc..3485f8454088 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1595,7 +1595,6 @@ enum {
 };
 
 #define HF_GIF_MASK		(1 << 0)
-#define HF_HIF_MASK		(1 << 1)
 #define HF_NMI_MASK		(1 << 3)
 #define HF_IRET_MASK		(1 << 4)
 #define HF_GUEST_MASK		(1 << 5) /* VCPU is in guest-mode */

commit e9fd761a46b8655aa5ff84b4adc0c8cf43952145
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 13 13:28:23 2020 -0400

    KVM: nSVM: remove HF_VINTR_MASK
    
    Now that the int_ctl field is stored in svm->nested.ctl.int_ctl, we can
    use it instead of vcpu->arch.hflags to check whether L2 is running
    in V_INTR_MASKING mode.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e6f2e1a2dab6..0dfc522f96cc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1596,7 +1596,6 @@ enum {
 
 #define HF_GIF_MASK		(1 << 0)
 #define HF_HIF_MASK		(1 << 1)
-#define HF_VINTR_MASK		(1 << 2)
 #define HF_NMI_MASK		(1 << 3)
 #define HF_IRET_MASK		(1 << 4)
 #define HF_GUEST_MASK		(1 << 5) /* VCPU is in guest-mode */

commit 7c86663b68bab393633d8312a0d25a3d004de182
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat May 16 08:42:28 2020 -0400

    KVM: nSVM: inject exceptions via svm_check_nested_events
    
    This allows exceptions injected by the emulator to be properly delivered
    as vmexits.  The code also becomes simpler, because we can just let all
    L0-intercepted exceptions go through the usual path.  In particular, our
    emulation of the VMX #DB exit qualification is very much simplified,
    because the vmexit injection path can use kvm_deliver_exception_payload
    to update DR6.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7707bd4b0593..e6f2e1a2dab6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1495,6 +1495,8 @@ void kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);
 
 void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
+void kvm_update_dr7(struct kvm_vcpu *vcpu);
+
 int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn);
 int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva);
 void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);

commit c9d40913ac5a21eb2b976bb221a4677540e84eba
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri May 22 11:21:49 2020 -0400

    KVM: x86: enable event window in inject_pending_event
    
    In case an interrupt arrives after nested.check_events but before the
    call to kvm_cpu_has_injectable_intr, we could end up enabling the interrupt
    window even if the interrupt is actually going to be a vmexit.  This is
    useless rather than harmful, but it really complicates reasoning about
    SVM's handling of the VINTR intercept.  We'd like to never bother with
    the VINTR intercept if V_INTR_MASKING=1 && INTERCEPT_INTR=1, because in
    that case there is no interrupt window and we can just exit the nested
    guest whenever we want.
    
    This patch moves the opening of the interrupt window inside
    inject_pending_event.  This consolidates the check for pending
    interrupt/NMI/SMI in one place, and makes KVM's usage of immediate
    exits more consistent, extending it beyond just nested virtualization.
    
    There are two functional changes here.  They only affect corner cases,
    but overall they simplify the inject_pending_event.
    
    - re-injection of still-pending events will also use req_immediate_exit
    instead of using interrupt-window intercepts.  This should have no impact
    on performance on Intel since it simply replaces an interrupt-window
    or NMI-window exit for a preemption-timer exit.  On AMD, which has no
    equivalent of the preemption time, it may incur some overhead but an
    actual effect on performance should only be visible in pathological cases.
    
    - kvm_arch_interrupt_allowed and kvm_vcpu_has_events will return true
    if an interrupt, NMI or SMI is blocked by nested_run_pending.  This
    makes sense because entering the VM will allow it to make progress
    and deliver the event.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index db261da578f3..7707bd4b0593 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1136,8 +1136,8 @@ struct kvm_x86_ops {
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu);
 	void (*cancel_injection)(struct kvm_vcpu *vcpu);
-	bool (*interrupt_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
-	bool (*nmi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
+	int (*interrupt_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
+	int (*nmi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
 	void (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
@@ -1234,10 +1234,10 @@ struct kvm_x86_ops {
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
 
-	bool (*smi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
+	int (*smi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, const char *smstate);
-	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
+	void (*enable_smi_window)(struct kvm_vcpu *vcpu);
 
 	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
 	int (*mem_enc_reg_region)(struct kvm *kvm, struct kvm_enc_region *argp);

commit cb97c2d680dd9cefd2b36cafd2426f52d85b27e7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:40:11 2020 -0800

    KVM: x86: Take an unsigned 32-bit int for has_emulated_msr()'s index
    
    Take a u32 for the index in has_emulated_msr() to match hardware, which
    treats MSR indices as unsigned 32-bit values.  Functionally, taking a
    signed int doesn't cause problems with the current code base, but could
    theoretically cause problems with 32-bit KVM, e.g. if the index were
    checked via a less-than statement, which would evaluate incorrectly for
    MSR indices with bit 31 set.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200218234012.7110-3-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fd78bd44b2d6..db261da578f3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1064,7 +1064,7 @@ struct kvm_x86_ops {
 	void (*hardware_disable)(void);
 	void (*hardware_unsetup)(void);
 	bool (*cpu_has_accelerated_tpr)(void);
-	bool (*has_emulated_msr)(int index);
+	bool (*has_emulated_msr)(u32 index);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
 	unsigned int vm_size;

commit 7357b1df744c2a3bcbe00cea0eef1509d004f488
Author: Michael Kelley <mikelley@microsoft.com>
Date:   Wed Apr 22 12:57:34 2020 -0700

    KVM: x86: hyperv: Remove duplicate definitions of Reference TSC Page
    
    The Hyper-V Reference TSC Page structure is defined twice. struct
    ms_hyperv_tsc_page has padding out to a full 4 Kbyte page size. But
    the padding is not needed because the declaration includes a union
    with HV_HYP_PAGE_SIZE.  KVM uses the second definition, which is
    struct _HV_REFERENCE_TSC_PAGE, because it does not have the padding.
    
    Fix the duplication by removing the padding from ms_hyperv_tsc_page.
    Fix up the KVM code to use it. Remove the no longer used struct
    _HV_REFERENCE_TSC_PAGE.
    
    There is no functional change.
    
    Signed-off-by: Michael Kelley <mikelley@microsoft.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Link: https://lore.kernel.org/r/20200422195737.10223-2-mikelley@microsoft.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 42a2d0d3984a..4698343b9a05 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -865,7 +865,7 @@ struct kvm_hv {
 	u64 hv_crash_param[HV_X64_MSR_CRASH_PARAMS];
 	u64 hv_crash_ctl;
 
-	HV_REFERENCE_TSC_PAGE tsc_ref;
+	struct ms_hyperv_tsc_page tsc_ref;
 
 	struct idr conn_to_evt;
 

commit cb953129bfe5c0f2da835a0469930873fb7e71df
Author: David Matlack <dmatlack@google.com>
Date:   Fri May 8 11:22:40 2020 -0700

    kvm: add halt-polling cpu usage stats
    
    Two new stats for exposing halt-polling cpu usage:
    halt_poll_success_ns
    halt_poll_fail_ns
    
    Thus sum of these 2 stats is the total cpu time spent polling. "success"
    means the VCPU polled until a virtual interrupt was delivered. "fail"
    means the VCPU had to schedule out (either because the maximum poll time
    was reached or it needed to yield the CPU).
    
    To avoid touching every arch's kvm_vcpu_stat struct, only update and
    export halt-polling cpu usage stats if we're on x86.
    
    Exporting cpu usage as a u64 and in nanoseconds means we will overflow at
    ~500 years, which seems reasonably large.
    
    Signed-off-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Jon Cargille <jcargill@google.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    
    Message-Id: <20200508182240.68440-1-jcargill@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a3ce391c9e7a..fd78bd44b2d6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1031,6 +1031,8 @@ struct kvm_vcpu_stat {
 	u64 irq_injections;
 	u64 nmi_injections;
 	u64 req_event;
+	u64 halt_poll_success_ns;
+	u64 halt_poll_fail_ns;
 };
 
 struct x86_instruction_info;

commit 93dff2fed2fb4a513196b7df05742c6fcdfd5178
Author: Jim Mattson <jmattson@google.com>
Date:   Fri May 8 13:36:43 2020 -0700

    KVM: nVMX: Migrate the VMX-preemption timer
    
    The hrtimer used to emulate the VMX-preemption timer must be pinned to
    the same logical processor as the vCPU thread to be interrupted if we
    want to have any hope of adhering to the architectural specification
    of the VMX-preemption timer. Even with this change, the emulated
    VMX-preemption timer VM-exit occasionally arrives too late.
    
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Reviewed-by: Peter Shier <pshier@google.com>
    Reviewed-by: Oliver Upton <oupton@google.com>
    Message-Id: <20200508203643.85477-4-jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c3906fb2b93f..a3ce391c9e7a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1247,6 +1247,8 @@ struct kvm_x86_ops {
 
 	bool (*apic_init_signal_blocked)(struct kvm_vcpu *vcpu);
 	int (*enable_direct_tlbflush)(struct kvm_vcpu *vcpu);
+
+	void (*migrate_timers)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_x86_nested_ops {

commit 404d5d7bff0d419fe11c7eaebca9ec8f25258f95
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue Apr 28 14:23:25 2020 +0800

    KVM: X86: Introduce more exit_fastpath_completion enum values
    
    Adds a fastpath_t typedef since enum lines are a bit long, and replace
    EXIT_FASTPATH_SKIP_EMUL_INS with two new exit_fastpath_completion enum values.
    
    - EXIT_FASTPATH_EXIT_HANDLED  kvm will still go through it's full run loop,
                                  but it would skip invoking the exit handler.
    
    - EXIT_FASTPATH_REENTER_GUEST complete fastpath, guest can be re-entered
                                  without invoking the exit handler or going
                                  back to vcpu_run
    
    Tested-by: Haiwei Li <lihaiwei@tencent.com>
    Cc: Haiwei Li <lihaiwei@tencent.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Message-Id: <1588055009-12677-4-git-send-email-wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 04f44961858b..c3906fb2b93f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -182,8 +182,10 @@ enum {
 
 enum exit_fastpath_completion {
 	EXIT_FASTPATH_NONE,
-	EXIT_FASTPATH_SKIP_EMUL_INS,
+	EXIT_FASTPATH_REENTER_GUEST,
+	EXIT_FASTPATH_EXIT_HANDLED,
 };
+typedef enum exit_fastpath_completion fastpath_t;
 
 struct x86_emulate_ctxt;
 struct x86_exception;

commit dd03bcaad0b1a62c8ea6297e6f2a5993c1c5cd30
Author: Peter Xu <peterx@redhat.com>
Date:   Thu Apr 16 11:58:59 2020 -0400

    KVM: X86: Force ASYNC_PF_PER_VCPU to be power of two
    
    Forcing the ASYNC_PF_PER_VCPU to be power of two is much easier to be
    used rather than calling roundup_pow_of_two() from time to time.  Do
    this by adding a BUILD_BUG_ON() inside the hash function.
    
    Another point is that generally async pf does not allow concurrency
    over ASYNC_PF_PER_VCPU after all (see kvm_setup_async_pf()), so it
    does not make much sense either to have it not a power of two or some
    of the entries will definitely be wasted.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Message-Id: <20200416155859.267366-1-peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b8d035694b87..04f44961858b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -763,7 +763,7 @@ struct kvm_vcpu_arch {
 
 	struct {
 		bool halted;
-		gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
+		gfn_t gfns[ASYNC_PF_PER_VCPU];
 		struct gfn_to_hva_cache data;
 		u64 msr_val;
 		u32 id;

commit 3bae0459bcd559506a2ca5807040ff722de5b136
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Apr 27 17:54:22 2020 -0700

    KVM: x86/mmu: Drop KVM's hugepage enums in favor of the kernel's enums
    
    Replace KVM's PT_PAGE_TABLE_LEVEL, PT_DIRECTORY_LEVEL and PT_PDPE_LEVEL
    with the kernel's PG_LEVEL_4K, PG_LEVEL_2M and PG_LEVEL_1G.  KVM's
    enums are borderline impossible to remember and result in code that is
    visually difficult to audit, e.g.
    
            if (!enable_ept)
                    ept_lpage_level = 0;
            else if (cpu_has_vmx_ept_1g_page())
                    ept_lpage_level = PT_PDPE_LEVEL;
            else if (cpu_has_vmx_ept_2m_page())
                    ept_lpage_level = PT_DIRECTORY_LEVEL;
            else
                    ept_lpage_level = PT_PAGE_TABLE_LEVEL;
    
    versus
    
            if (!enable_ept)
                    ept_lpage_level = 0;
            else if (cpu_has_vmx_ept_1g_page())
                    ept_lpage_level = PG_LEVEL_1G;
            else if (cpu_has_vmx_ept_2m_page())
                    ept_lpage_level = PG_LEVEL_2M;
            else
                    ept_lpage_level = PG_LEVEL_4K;
    
    No functional change intended.
    
    Suggested-by: Barret Rhoden <brho@google.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200428005422.4235-4-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 77a97a72bb3e..b8d035694b87 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -110,14 +110,8 @@
 #define UNMAPPED_GVA (~(gpa_t)0)
 
 /* KVM Hugepage definitions for x86 */
-enum {
-	PT_PAGE_TABLE_LEVEL   = 1,
-	PT_DIRECTORY_LEVEL    = 2,
-	PT_PDPE_LEVEL         = 3,
-};
-#define KVM_MAX_HUGEPAGE_LEVEL	PT_PDPE_LEVEL
-#define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - \
-				 PT_PAGE_TABLE_LEVEL + 1)
+#define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
+#define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
@@ -126,7 +120,7 @@ enum {
 
 static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 {
-	/* KVM_HPAGE_GFN_SHIFT(PT_PAGE_TABLE_LEVEL) must be 0. */
+	/* KVM_HPAGE_GFN_SHIFT(PG_LEVEL_4K) must be 0. */
 	return (gfn >> KVM_HPAGE_GFN_SHIFT(level)) -
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }

commit e662ec3e0705cfee5b36aecd62adbc36df85eab3
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Apr 27 17:54:21 2020 -0700

    KVM: x86/mmu: Move max hugepage level to a separate #define
    
    Rename PT_MAX_HUGEPAGE_LEVEL to KVM_MAX_HUGEPAGE_LEVEL and make it a
    separate define in anticipation of dropping KVM's PT_*_LEVEL enums in
    favor of the kernel's PG_LEVEL_* enums.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200428005422.4235-3-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 86295a01f5ca..77a97a72bb3e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -114,10 +114,9 @@ enum {
 	PT_PAGE_TABLE_LEVEL   = 1,
 	PT_DIRECTORY_LEVEL    = 2,
 	PT_PDPE_LEVEL         = 3,
-	/* set max level to the biggest one */
-	PT_MAX_HUGEPAGE_LEVEL = PT_PDPE_LEVEL,
 };
-#define KVM_NR_PAGE_SIZES	(PT_MAX_HUGEPAGE_LEVEL - \
+#define KVM_MAX_HUGEPAGE_LEVEL	PT_PDPE_LEVEL
+#define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - \
 				 PT_PAGE_TABLE_LEVEL + 1)
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))

commit a71936ab46f1da1539d97a98dfb2f94ee383d687
Author: Xiaoyao Li <xiaoyao.li@intel.com>
Date:   Wed Apr 29 23:43:12 2020 +0800

    kvm: x86: Cleanup vcpu->arch.guest_xstate_size
    
    vcpu->arch.guest_xstate_size lost its only user since commit df1daba7d1cb
    ("KVM: x86: support XSAVES usage in the host"), so clean it up.
    
    Signed-off-by: Xiaoyao Li <xiaoyao.li@intel.com>
    Message-Id: <20200429154312.1411-1-xiaoyao.li@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 505cf19ace80..86295a01f5ca 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -657,7 +657,6 @@ struct kvm_vcpu_arch {
 
 	u64 xcr0;
 	u64 guest_supported_xcr0;
-	u32 guest_xstate_size;
 
 	struct kvm_pio_request pio;
 	void *pio_data;

commit e93fd3b3e89e9664039281fe7e56e6f764f2a909
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri May 1 21:32:34 2020 -0700

    KVM: x86/mmu: Capture TDP level when updating CPUID
    
    Snapshot the TDP level now that it's invariant (SVM) or dependent only
    on host capabilities and guest CPUID (VMX).  This avoids having to call
    kvm_x86_ops.get_tdp_level() when initializing a TDP MMU and/or
    calculating the page role, and thus avoids the associated retpoline.
    
    Drop the WARN in vmx_get_tdp_level() as updating CPUID while L2 is
    active is legal, if dodgy.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200502043234.12481-11-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ad882127079f..505cf19ace80 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -687,6 +687,7 @@ struct kvm_vcpu_arch {
 	struct kvm_cpuid_entry2 cpuid_entries[KVM_MAX_CPUID_ENTRIES];
 
 	int maxphyaddr;
+	int tdp_level;
 
 	/* emulate context */
 

commit bd31fe495d0d1a67fe6f44f06dfef637f202241d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri May 1 21:32:31 2020 -0700

    KVM: VMX: Add proper cache tracking for CR0
    
    Move CR0 caching into the standard register caching mechanism in order
    to take advantage of the availability checks provided by regs_avail.
    This avoids multiple VMREADs in the (uncommon) case where kvm_read_cr0()
    is called multiple times in a single VM-Exit, and more importantly
    eliminates a kvm_x86_ops hook, saves a retpoline on SVM when reading
    CR0, and squashes the confusing naming discrepancy of "cache_reg" vs.
    "decache_cr0_guest_bits".
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200502043234.12481-8-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3e2f582160fe..ad882127079f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -167,6 +167,7 @@ enum kvm_reg {
 	NR_VCPU_REGS,
 
 	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
+	VCPU_EXREG_CR0,
 	VCPU_EXREG_CR3,
 	VCPU_EXREG_CR4,
 	VCPU_EXREG_RFLAGS,
@@ -1092,7 +1093,6 @@ struct kvm_x86_ops {
 	void (*set_segment)(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg);
 	void (*get_cs_db_l_bits)(struct kvm_vcpu *vcpu, int *db, int *l);
-	void (*decache_cr0_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
 	int (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
 	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);

commit f98c1e77127de7d9ff558570c25d02ef077df50f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri May 1 21:32:30 2020 -0700

    KVM: VMX: Add proper cache tracking for CR4
    
    Move CR4 caching into the standard register caching mechanism in order
    to take advantage of the availability checks provided by regs_avail.
    This avoids multiple VMREADs and retpolines (when configured) during
    nested VMX transitions as kvm_read_cr4_bits() is invoked multiple times
    on each transition, e.g. when stuffing CR0 and CR3.
    
    As an added bonus, this eliminates a kvm_x86_ops hook, saves a retpoline
    on SVM when reading CR4, and squashes the confusing naming discrepancy
    of "cache_reg" vs. "decache_cr4_guest_bits".
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200502043234.12481-7-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3ff671e35d76..3e2f582160fe 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -168,6 +168,7 @@ enum kvm_reg {
 
 	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
 	VCPU_EXREG_CR3,
+	VCPU_EXREG_CR4,
 	VCPU_EXREG_RFLAGS,
 	VCPU_EXREG_SEGMENTS,
 	VCPU_EXREG_EXIT_INFO_1,
@@ -1092,7 +1093,6 @@ struct kvm_x86_ops {
 			    struct kvm_segment *var, int seg);
 	void (*get_cs_db_l_bits)(struct kvm_vcpu *vcpu, int *db, int *l);
 	void (*decache_cr0_guest_bits)(struct kvm_vcpu *vcpu);
-	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
 	int (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
 	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);

commit 56ba77a459a72a7d95be74355a40a91e1f6dd7f7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri May 1 21:32:25 2020 -0700

    KVM: x86: Save L1 TSC offset in 'struct kvm_vcpu_arch'
    
    Save L1's TSC offset in 'struct kvm_vcpu_arch' and drop the kvm_x86_ops
    hook read_l1_tsc_offset().  This avoids a retpoline (when configured)
    when reading L1's effective TSC, which is done at least once on every
    VM-Exit.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200502043234.12481-2-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index efe6199c596c..3ff671e35d76 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -708,6 +708,7 @@ struct kvm_vcpu_arch {
 		struct gfn_to_pfn_cache cache;
 	} st;
 
+	u64 l1_tsc_offset;
 	u64 tsc_offset;
 	u64 last_guest_tsc;
 	u64 last_host_tsc;
@@ -1165,7 +1166,6 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
-	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);
 	/* Returns actual tsc_offset set in active VMCS */
 	u64 (*write_l1_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 

commit c300ab9f08df9e4b9f39d53a0691e234330df124
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 23 14:08:58 2020 -0400

    KVM: x86: Replace late check_nested_events() hack with more precise fix
    
    Add an argument to interrupt_allowed and nmi_allowed, to checking if
    interrupt injection is blocked.  Use the hook to handle the case where
    an interrupt arrives between check_nested_events() and the injection
    logic.  Drop the retry of check_nested_events() that hack-a-fixed the
    same condition.
    
    Blocking injection is also a bit of a hack, e.g. KVM should do exiting
    and non-exiting interrupt processing in a single pass, but it's a more
    precise hack.  The old comment is also misleading, e.g. KVM_REQ_EVENT is
    purely an optimization, setting it on every run loop (which KVM doesn't
    do) should not affect functionality, only performance.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200423022550.15113-13-sean.j.christopherson@intel.com>
    [Extend to SVM, add SMI and NMI.  Even though NMI and SMI cannot come
     asynchronously right now, making the fix generic is easy and removes a
     special case. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 44268642b3c6..efe6199c596c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1138,8 +1138,8 @@ struct kvm_x86_ops {
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu);
 	void (*cancel_injection)(struct kvm_vcpu *vcpu);
-	bool (*interrupt_allowed)(struct kvm_vcpu *vcpu);
-	bool (*nmi_allowed)(struct kvm_vcpu *vcpu);
+	bool (*interrupt_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
+	bool (*nmi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
 	void (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
@@ -1237,7 +1237,7 @@ struct kvm_x86_ops {
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
 
-	bool (*smi_allowed)(struct kvm_vcpu *vcpu);
+	bool (*smi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, const char *smstate);
 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);

commit 88c604b66eb6a74841cd40f5bdf639112ad69115
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 22 19:25:41 2020 -0700

    KVM: x86: Make return for {interrupt_nmi,smi}_allowed() a bool instead of int
    
    Return an actual bool for kvm_x86_ops' {interrupt_nmi}_allowed() hook to
    better reflect the return semantics, and to avoid creating an even
    bigger mess when the related VMX code is refactored in upcoming patches.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200423022550.15113-5-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e6671c61fd65..44268642b3c6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1138,8 +1138,8 @@ struct kvm_x86_ops {
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu);
 	void (*cancel_injection)(struct kvm_vcpu *vcpu);
-	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
-	int (*nmi_allowed)(struct kvm_vcpu *vcpu);
+	bool (*interrupt_allowed)(struct kvm_vcpu *vcpu);
+	bool (*nmi_allowed)(struct kvm_vcpu *vcpu);
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
 	void (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
@@ -1237,7 +1237,7 @@ struct kvm_x86_ops {
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
 
-	int (*smi_allowed)(struct kvm_vcpu *vcpu);
+	bool (*smi_allowed)(struct kvm_vcpu *vcpu);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, const char *smstate);
 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);

commit d2060bd42e4482b15c35f961a294ee57f369027d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 22 19:25:39 2020 -0700

    KVM: nVMX: Open a window for pending nested VMX preemption timer
    
    Add a kvm_x86_ops hook to detect a nested pending "hypervisor timer" and
    use it to effectively open a window for servicing the expired timer.
    Like pending SMIs on VMX, opening a window simply means requesting an
    immediate exit.
    
    This fixes a bug where an expired VMX preemption timer (for L2) will be
    delayed and/or lost if a pending exception is injected into L2.  The
    pending exception is rightly prioritized by vmx_check_nested_events()
    and injected into L2, with the preemption timer left pending.  Because
    no window opened, L2 is free to run uninterrupted.
    
    Fixes: f4124500c2c13 ("KVM: nVMX: Fully emulate preemption timer")
    Reported-by: Jim Mattson <jmattson@google.com>
    Cc: Oliver Upton <oupton@google.com>
    Cc: Peter Shier <pshier@google.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200423022550.15113-3-sean.j.christopherson@intel.com>
    [Check it in kvm_vcpu_has_events too, to ensure that the preemption
     timer is serviced promptly even if the vCPU is halted and L1 is not
     intercepting HLT. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b3a5da27c2a5..e6671c61fd65 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1256,6 +1256,7 @@ struct kvm_x86_ops {
 
 struct kvm_x86_nested_ops {
 	int (*check_events)(struct kvm_vcpu *vcpu);
+	bool (*hv_timer_pending)(struct kvm_vcpu *vcpu);
 	int (*get_state)(struct kvm_vcpu *vcpu,
 			 struct kvm_nested_state __user *user_kvm_nested_state,
 			 unsigned user_data_size);

commit 4aef2ec9022b217f74d0f4c9b84081f07cc223d9
Merge: 7c67f54661fc 37486135d3a7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 13 12:14:05 2020 -0400

    Merge branch 'kvm-amd-fixes' into HEAD

commit 37486135d3a7b03acc7755b63627a130437f066a
Author: Babu Moger <babu.moger@amd.com>
Date:   Tue May 12 18:59:06 2020 -0500

    KVM: x86: Fix pkru save/restore when guest CR4.PKE=0, move it to x86.c
    
    Though rdpkru and wrpkru are contingent upon CR4.PKE, the PKRU
    resource isn't. It can be read with XSAVE and written with XRSTOR.
    So, if we don't set the guest PKRU value here(kvm_load_guest_xsave_state),
    the guest can read the host value.
    
    In case of kvm_load_host_xsave_state, guest with CR4.PKE clear could
    potentially use XRSTOR to change the host PKRU value.
    
    While at it, move pkru state save/restore to common code and the
    host_pkru field to kvm_vcpu_arch.  This will let SVM support protection keys.
    
    Cc: stable@vger.kernel.org
    Reported-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Babu Moger <babu.moger@amd.com>
    Message-Id: <158932794619.44260.14508381096663848853.stgit@naples-babu.amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9e8263b1e6fe..0a6b35353fc7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -578,6 +578,7 @@ struct kvm_vcpu_arch {
 	unsigned long cr4;
 	unsigned long cr4_guest_owned_bits;
 	unsigned long cr8;
+	u32 host_pkru;
 	u32 pkru;
 	u32 hflags;
 	u64 efer;

commit d67668e9dd76d98136048935723947156737932b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 6 06:40:04 2020 -0400

    KVM: x86, SVM: isolate vcpu->arch.dr6 from vmcb->save.dr6
    
    There are two issues with KVM_EXIT_DEBUG on AMD, whose root cause is the
    different handling of DR6 on intercepted #DB exceptions on Intel and AMD.
    
    On Intel, #DB exceptions transmit the DR6 value via the exit qualification
    field of the VMCS, and the exit qualification only contains the description
    of the precise event that caused a vmexit.
    
    On AMD, instead the DR6 field of the VMCB is filled in as if the #DB exception
    was to be injected into the guest.  This has two effects when guest debugging
    is in use:
    
    * the guest DR6 is clobbered
    
    * the kvm_run->debug.arch.dr6 field can accumulate more debug events, rather
    than just the last one that happened (the testcase in the next patch covers
    this issue).
    
    This patch fixes both issues by emulating, so to speak, the Intel behavior
    on AMD processors.  The important observation is that (after the previous
    patches) the VMCB value of DR6 is only ever observable from the guest is
    KVM_DEBUGREG_WONT_EXIT is set.  Therefore we can actually set vmcb->save.dr6
    to any value we want as long as KVM_DEBUGREG_WONT_EXIT is clear, which it
    will be if guest debugging is enabled.
    
    Therefore it is possible to enter the guest with an all-zero DR6,
    reconstruct the #DB payload from the DR6 we get at exit time, and let
    kvm_deliver_exception_payload move the newly set bits into vcpu->arch.dr6.
    Some extra bits may be included in the payload if KVM_DEBUGREG_WONT_EXIT
    is set, but this is harmless.
    
    This may not be the most optimized way to deal with this, but it is
    simple and, being confined within SVM code, it gets rid of the set_dr6
    callback and kvm_update_dr6.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index de0c28814348..9e8263b1e6fe 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1093,7 +1093,6 @@ struct kvm_x86_ops {
 	void (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*get_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*set_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
-	void (*set_dr6)(struct kvm_vcpu *vcpu, unsigned long value);
 	void (*sync_dirty_debug_regs)(struct kvm_vcpu *vcpu);
 	void (*set_dr7)(struct kvm_vcpu *vcpu, unsigned long value);
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
@@ -1623,7 +1622,6 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
-void kvm_update_dr6(struct kvm_vcpu *vcpu);
 
 u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc);
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc);

commit 5679b803e44ed8947e8c2a7f44cdef1d93ea24d5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 4 11:28:25 2020 -0400

    KVM: SVM: keep DR6 synchronized with vcpu->arch.dr6
    
    kvm_x86_ops.set_dr6 is only ever called with vcpu->arch.dr6 as the
    second argument.  Ensure that the VMCB value is synchronized to
    vcpu->arch.dr6 on #DB (both "normal" and nested) and nested vmentry, so
    that the current value of DR6 is always available in vcpu->arch.dr6.
    The get_dr6 callback can just access vcpu->arch.dr6 and becomes redundant.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8c247bcb037e..de0c28814348 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1093,7 +1093,6 @@ struct kvm_x86_ops {
 	void (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*get_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*set_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
-	u64 (*get_dr6)(struct kvm_vcpu *vcpu);
 	void (*set_dr6)(struct kvm_vcpu *vcpu, unsigned long value);
 	void (*sync_dirty_debug_regs)(struct kvm_vcpu *vcpu);
 	void (*set_dr7)(struct kvm_vcpu *vcpu, unsigned long value);
@@ -1624,6 +1623,7 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
+void kvm_update_dr6(struct kvm_vcpu *vcpu);
 
 u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc);
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc);

commit 4d5523cfd5d298c58743eb31c003886cfc856709
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 5 07:33:20 2020 -0400

    KVM: x86: fix DR6 delivery for various cases of #DB injection
    
    Go through kvm_queue_exception_p so that the payload is correctly delivered
    through the exit qualification, and add a kvm_update_dr6 call to
    kvm_deliver_exception_payload that is needed on AMD.
    
    Reported-by: Peter Xu <peterx@redhat.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0dea9f122bb9..8c247bcb037e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1449,6 +1449,7 @@ bool kvm_rdpmc(struct kvm_vcpu *vcpu);
 
 void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
+void kvm_queue_exception_p(struct kvm_vcpu *vcpu, unsigned nr, unsigned long payload);
 void kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);

commit 637543a8d61c6afe4e9be64bfb43c78701a83375
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Tue Apr 7 01:13:09 2020 -0500

    KVM: x86: Fixes posted interrupt check for IRQs delivery modes
    
    Current logic incorrectly uses the enum ioapic_irq_destination_types
    to check the posted interrupt destination types. However, the value was
    set using APIC_DM_XXX macros, which are left-shifted by 8 bits.
    
    Fixes by using the APIC_DM_FIXED and APIC_DM_LOWEST instead.
    
    Fixes: (fdcf75621375 'KVM: x86: Disable posted interrupts for non-standard IRQs delivery modes')
    Cc: Alexander Graf <graf@amazon.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Message-Id: <1586239989-58305-1-git-send-email-suravee.suthikulpanit@amd.com>
    Reviewed-by: Maxim Levitsky <mlevitsk@redhat.com>
    Tested-by: Maxim Levitsky <mlevitsk@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 42a2d0d3984a..0dea9f122bb9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1663,8 +1663,8 @@ void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 static inline bool kvm_irq_is_postable(struct kvm_lapic_irq *irq)
 {
 	/* We can only post Fixed and LowPrio IRQs */
-	return (irq->delivery_mode == dest_Fixed ||
-		irq->delivery_mode == dest_LowestPrio);
+	return (irq->delivery_mode == APIC_DM_FIXED ||
+		irq->delivery_mode == APIC_DM_LOWEST);
 }
 
 static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)

commit 33b22172452f05c351fd2fa24c28d2e76c7b0692
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Apr 17 10:24:18 2020 -0400

    KVM: x86: move nested-related kvm_x86_ops to a separate struct
    
    Clean up some of the patching of kvm_x86_ops, by moving kvm_x86_ops related to
    nested virtualization into a separate struct.
    
    As a result, these ops will always be non-NULL on VMX.  This is not a problem:
    
    * check_nested_events is only called if is_guest_mode(vcpu) returns true
    
    * get_nested_state treats VMXOFF state the same as nested being disabled
    
    * set_nested_state fails if you attempt to set nested state while
      nesting is disabled
    
    * nested_enable_evmcs could already be called on a CPU without VMX enabled
      in CPUID.
    
    * nested_get_evmcs_version was fixed in the previous patch
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f26df2cb0591..a239a297be33 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1178,7 +1178,6 @@ struct kvm_x86_ops {
 			       struct x86_exception *exception);
 	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu);
 
-	int (*check_nested_events)(struct kvm_vcpu *vcpu);
 	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);
 
 	void (*sched_in)(struct kvm_vcpu *kvm, int cpu);
@@ -1211,6 +1210,7 @@ struct kvm_x86_ops {
 
 	/* pmu operations of sub-arch */
 	const struct kvm_pmu_ops *pmu_ops;
+	const struct kvm_x86_nested_ops *nested_ops;
 
 	/*
 	 * Architecture specific hooks for vCPU blocking due to
@@ -1238,14 +1238,6 @@ struct kvm_x86_ops {
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
 
-	int (*get_nested_state)(struct kvm_vcpu *vcpu,
-				struct kvm_nested_state __user *user_kvm_nested_state,
-				unsigned user_data_size);
-	int (*set_nested_state)(struct kvm_vcpu *vcpu,
-				struct kvm_nested_state __user *user_kvm_nested_state,
-				struct kvm_nested_state *kvm_state);
-	bool (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
-
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, const char *smstate);
@@ -1257,16 +1249,27 @@ struct kvm_x86_ops {
 
 	int (*get_msr_feature)(struct kvm_msr_entry *entry);
 
-	int (*nested_enable_evmcs)(struct kvm_vcpu *vcpu,
-				   uint16_t *vmcs_version);
-	uint16_t (*nested_get_evmcs_version)(struct kvm_vcpu *vcpu);
-
 	bool (*need_emulation_on_page_fault)(struct kvm_vcpu *vcpu);
 
 	bool (*apic_init_signal_blocked)(struct kvm_vcpu *vcpu);
 	int (*enable_direct_tlbflush)(struct kvm_vcpu *vcpu);
 };
 
+struct kvm_x86_nested_ops {
+	int (*check_events)(struct kvm_vcpu *vcpu);
+	int (*get_state)(struct kvm_vcpu *vcpu,
+			 struct kvm_nested_state __user *user_kvm_nested_state,
+			 unsigned user_data_size);
+	int (*set_state)(struct kvm_vcpu *vcpu,
+			 struct kvm_nested_state __user *user_kvm_nested_state,
+			 struct kvm_nested_state *kvm_state);
+	bool (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
+
+	int (*enable_evmcs)(struct kvm_vcpu *vcpu,
+			    uint16_t *vmcs_version);
+	uint16_t (*get_evmcs_version)(struct kvm_vcpu *vcpu);
+};
+
 struct kvm_x86_init_ops {
 	int (*cpu_has_kvm_support)(void);
 	int (*disabled_by_bios)(void);

commit a9ab13ff6e844ad5b3ed39339e6db9a76bb539ad
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Fri Apr 10 10:47:03 2020 -0700

    KVM: X86: Improve latency for single target IPI fastpath
    
    IPI and Timer cause the main MSRs write vmexits in cloud environment
    observation, let's optimize virtual IPI latency more aggressively to
    inject target IPI as soon as possible.
    
    Running kvm-unit-tests/vmexit.flat IPI testing on SKX server, disable
    adaptive advance lapic timer and adaptive halt-polling to avoid the
    interference, this patch can give another 7% improvement.
    
    w/o fastpath   -> x86.c fastpath      4238 -> 3543  16.4%
    x86.c fastpath -> vmx.c fastpath      3543 -> 3293     7%
    w/o fastpath   -> vmx.c fastpath      4238 -> 3293  22.3%
    
    Cc: Haiwei Li <lihaiwei@tencent.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200410174703.1138-3-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ceba205d32a2..f26df2cb0591 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1126,7 +1126,7 @@ struct kvm_x86_ops {
 	 */
 	void (*tlb_flush_guest)(struct kvm_vcpu *vcpu);
 
-	void (*run)(struct kvm_vcpu *vcpu);
+	enum exit_fastpath_completion (*run)(struct kvm_vcpu *vcpu);
 	int (*handle_exit)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion exit_fastpath);
 	int (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
@@ -1176,8 +1176,7 @@ struct kvm_x86_ops {
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage,
 			       struct x86_exception *exception);
-	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu,
-		enum exit_fastpath_completion *exit_fastpath);
+	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu);
 	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);

commit 8791585837f659943936b8e1cce9d039436ad1ca
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 15 13:34:54 2020 -0700

    KVM: VMX: Cache vmcs.EXIT_INTR_INFO using arch avail_reg flags
    
    Introduce a new "extended register" type, EXIT_INFO_2 (to pair with the
    nomenclature in .get_exit_info()), and use it to cache VMX's
    vmcs.EXIT_INTR_INFO.  Drop a comment in vmx_recover_nmi_blocking() that
    is obsoleted by the generic caching mechanism.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200415203454.8296-6-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9673ec700cd0..ceba205d32a2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -171,6 +171,7 @@ enum kvm_reg {
 	VCPU_EXREG_RFLAGS,
 	VCPU_EXREG_SEGMENTS,
 	VCPU_EXREG_EXIT_INFO_1,
+	VCPU_EXREG_EXIT_INFO_2,
 };
 
 enum {

commit 5addc235199f15ae964e7ac6b20cf43f4a661573
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 15 13:34:53 2020 -0700

    KVM: VMX: Cache vmcs.EXIT_QUALIFICATION using arch avail_reg flags
    
    Introduce a new "extended register" type, EXIT_INFO_1 (to pair with the
    nomenclature in .get_exit_info()), and use it to cache VMX's
    vmcs.EXIT_QUALIFICATION.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200415203454.8296-5-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d099749168f0..9673ec700cd0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -170,6 +170,7 @@ enum kvm_reg {
 	VCPU_EXREG_CR3,
 	VCPU_EXREG_RFLAGS,
 	VCPU_EXREG_SEGMENTS,
+	VCPU_EXREG_EXIT_INFO_1,
 };
 
 enum {

commit be01e8e2c632c41c69bb30e7196661ec6e8fdc10
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:32 2020 -0700

    KVM: x86: Replace "cr3" with "pgd" in "new cr3/pgd" related code
    
    Rename functions and variables in kvm_mmu_new_cr3() and related code to
    replace "cr3" with "pgd", i.e. continue the work started by commit
    727a7e27cf88a ("KVM: x86: rename set_cr3 callback and related flags to
    load_mmu_pgd").  kvm_mmu_new_cr3() and company are not always loading a
    new CR3, e.g. when nested EPT is enabled "cr3" is actually an EPTP.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-37-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ee5886a5006a..d099749168f0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -375,12 +375,12 @@ struct rsvd_bits_validate {
 };
 
 struct kvm_mmu_root_info {
-	gpa_t cr3;
+	gpa_t pgd;
 	hpa_t hpa;
 };
 
 #define KVM_MMU_ROOT_INFO_INVALID \
-	((struct kvm_mmu_root_info) { .cr3 = INVALID_PAGE, .hpa = INVALID_PAGE })
+	((struct kvm_mmu_root_info) { .pgd = INVALID_PAGE, .hpa = INVALID_PAGE })
 
 #define KVM_MMU_NUM_PREV_ROOTS 3
 
@@ -406,7 +406,7 @@ struct kvm_mmu {
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
 	hpa_t root_hpa;
-	gpa_t root_cr3;
+	gpa_t root_pgd;
 	union kvm_mmu_role mmu_role;
 	u8 root_level;
 	u8 shadow_root_level;
@@ -1524,7 +1524,7 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_invalidate_gva(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gva_t gva, hpa_t root_hpa);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
-void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush,
+void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd, bool skip_tlb_flush,
 		     bool skip_mmu_sync);
 
 void kvm_configure_mmu(bool enable_tdp, int tdp_page_level);

commit 4a632ac6ca66fb29b94a16495624c58f4d313f2f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:27 2020 -0700

    KVM: x86/mmu: Add separate override for MMU sync during fast CR3 switch
    
    Add a separate "skip" override for MMU sync, a future change to avoid
    TLB flushes on nested VMX transitions may need to sync the MMU even if
    the TLB flush is unnecessary.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-32-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 541e2df8fc6e..ee5886a5006a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1524,7 +1524,8 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_invalidate_gva(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gva_t gva, hpa_t root_hpa);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
-void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush);
+void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush,
+		     bool skip_mmu_sync);
 
 void kvm_configure_mmu(bool enable_tdp, int tdp_page_level);
 

commit a4148b7ca2a5afe1295a41b5e30048cabcb74f8d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:24 2020 -0700

    KVM: VMX: Retrieve APIC access page HPA only when necessary
    
    Move the retrieval of the HPA associated with L1's APIC access page into
    VMX code to avoid unnecessarily calling gfn_to_page(), e.g. when the
    vCPU is in guest mode (L2).  Alternatively, the optimization logic in
    VMX could be mirrored into the common x86 code, but that will get ugly
    fast when further optimizations are introduced.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-29-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 72e9c4492f47..541e2df8fc6e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1152,7 +1152,7 @@ struct kvm_x86_ops {
 	bool (*guest_apic_has_interrupt)(struct kvm_vcpu *vcpu);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_apic_mode)(struct kvm_vcpu *vcpu);
-	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
+	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu);
 	int (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	int (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);

commit eeeb4f67a6cd437da1f5d1a20596cdc2d7b50551
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:20 2020 -0700

    KVM: x86: Introduce KVM_REQ_TLB_FLUSH_CURRENT to flush current ASID
    
    Add KVM_REQ_TLB_FLUSH_CURRENT to allow optimized TLB flushing of VMX's
    EPTP/VPID contexts[*] from the KVM MMU and/or in a deferred manner, e.g.
    to flush L2's context during nested VM-Enter.
    
    Convert KVM_REQ_TLB_FLUSH to KVM_REQ_TLB_FLUSH_CURRENT in flows where
    the flush is directly associated with vCPU-scoped instruction emulation,
    i.e. MOV CR3 and INVPCID.
    
    Add a comment in vmx_vcpu_load_vmcs() above its KVM_REQ_TLB_FLUSH to
    make it clear that it deliberately requests a flush of all contexts.
    
    Service any pending flush request on nested VM-Exit as it's possible a
    nested VM-Exit could occur after requesting a flush for L2.  Add the
    same logic for nested VM-Enter even though it's _extremely_ unlikely
    for flush to be pending on nested VM-Enter, but theoretically possible
    (in the future) due to RSM (SMM) emulation.
    
    [*] Intel also has an Address Space Identifier (ASID) concept, e.g.
        EPTP+VPID+PCID == ASID, it's just not documented in the SDM because
        the rules of invalidation are different based on which piece of the
        ASID is being changed, i.e. whether the EPTP, VPID, or PCID context
        must be invalidated.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-25-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e6305d5e28fa..72e9c4492f47 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -83,8 +83,9 @@
 #define KVM_REQ_GET_VMCS12_PAGES	KVM_ARCH_REQ(24)
 #define KVM_REQ_APICV_UPDATE \
 	KVM_ARCH_REQ_FLAGS(25, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
 #define KVM_REQ_HV_TLB_FLUSH \
-	KVM_ARCH_REQ_FLAGS(26, KVM_REQUEST_NO_WAKEUP)
+	KVM_ARCH_REQ_FLAGS(27, KVM_REQUEST_NO_WAKEUP)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
@@ -1104,6 +1105,7 @@ struct kvm_x86_ops {
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
 
 	void (*tlb_flush_all)(struct kvm_vcpu *vcpu);
+	void (*tlb_flush_current)(struct kvm_vcpu *vcpu);
 	int  (*tlb_remote_flush)(struct kvm *kvm);
 	int  (*tlb_remote_flush_with_range)(struct kvm *kvm,
 			struct kvm_tlb_range *range);

commit 7780938cc70b848650722762fa4c7496fa68f9ec
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:18 2020 -0700

    KVM: x86: Rename ->tlb_flush() to ->tlb_flush_all()
    
    Rename ->tlb_flush() to ->tlb_flush_all() in preparation for adding a
    new hook to flush only the current ASID/context.
    
    Opportunstically replace the comment in vmx_flush_tlb() that explains
    why it flushes all EPTP/VPID contexts with a comment explaining why it
    unconditionally uses INVEPT when EPT is enabled.  I.e. rely on the "all"
    part of the name to clarify why it does global INVEPT/INVVPID.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-23-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7156c749677a..e6305d5e28fa 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1103,7 +1103,7 @@ struct kvm_x86_ops {
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
 
-	void (*tlb_flush)(struct kvm_vcpu *vcpu);
+	void (*tlb_flush_all)(struct kvm_vcpu *vcpu);
 	int  (*tlb_remote_flush)(struct kvm *kvm);
 	int  (*tlb_remote_flush_with_range)(struct kvm *kvm,
 			struct kvm_tlb_range *range);

commit f55ac304ca47039368a5971fa61ebc8160c90659
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:12 2020 -0700

    KVM: x86: Drop @invalidate_gpa param from kvm_x86_ops' tlb_flush()
    
    Drop @invalidate_gpa from ->tlb_flush() and kvm_vcpu_flush_tlb() now
    that all callers pass %true for said param, or ignore the param (SVM has
    an internal call to svm_flush_tlb() in svm_flush_tlb_guest that somewhat
    arbitrarily passes %false).
    
    Remove __vmx_flush_tlb() as it is no longer used.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-17-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9951b01df57c..7156c749677a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1103,7 +1103,7 @@ struct kvm_x86_ops {
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
 
-	void (*tlb_flush)(struct kvm_vcpu *vcpu, bool invalidate_gpa);
+	void (*tlb_flush)(struct kvm_vcpu *vcpu);
 	int  (*tlb_remote_flush)(struct kvm *kvm);
 	int  (*tlb_remote_flush_with_range)(struct kvm *kvm,
 			struct kvm_tlb_range *range);

commit 0baedd792713063213f1e2060dc6a5d536638f0a
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Mar 25 12:28:24 2020 -0400

    KVM: x86: make Hyper-V PV TLB flush use tlb_flush_guest()
    
    Hyper-V PV TLB flush mechanism does TLB flush on behalf of the guest
    so doing tlb_flush_all() is an overkill, switch to using tlb_flush_guest()
    (just like KVM PV TLB flush mechanism) instead. Introduce
    KVM_REQ_HV_TLB_FLUSH to support the change.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a7c1ea136c8c..9951b01df57c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -83,6 +83,8 @@
 #define KVM_REQ_GET_VMCS12_PAGES	KVM_ARCH_REQ(24)
 #define KVM_REQ_APICV_UPDATE \
 	KVM_ARCH_REQ_FLAGS(25, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_HV_TLB_FLUSH \
+	KVM_ARCH_REQ_FLAGS(26, KVM_REQUEST_NO_WAKEUP)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \

commit e64419d991ea212af087d3c57fcabb4d27db03fc
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:10 2020 -0700

    KVM: x86: Move "flush guest's TLB" logic to separate kvm_x86_ops hook
    
    Add a dedicated hook to handle flushing TLB entries on behalf of the
    guest, i.e. for a paravirtualized TLB flush, and use it directly instead
    of bouncing through kvm_vcpu_flush_tlb().
    
    For VMX, change the effective implementation implementation to never do
    INVEPT and flush only the current context, i.e. to always flush via
    INVVPID(SINGLE_CONTEXT).  The INVEPT performed by __vmx_flush_tlb() when
    @invalidate_gpa=false and enable_vpid=0 is unnecessary, as it will only
    flush guest-physical mappings; linear and combined mappings are flushed
    by VM-Enter when VPID is disabled, and changes in the guest pages tables
    do not affect guest-physical mappings.
    
    When EPT and VPID are enabled, doing INVVPID is not required (by Intel's
    architecture) to invalidate guest-physical mappings, i.e. TLB entries
    that cache guest-physical mappings can live across INVVPID as the
    mappings are associated with an EPTP, not a VPID.  The intent of
    @invalidate_gpa is to inform vmx_flush_tlb() that it must "invalidate
    gpa mappings", i.e. do INVEPT and not simply INVVPID.  Other than nested
    VPID handling, which now calls vpid_sync_context() directly, the only
    scenario where KVM can safely do INVVPID instead of INVEPT (when EPT is
    enabled) is if KVM is flushing TLB entries from the guest's perspective,
    i.e. is only required to invalidate linear mappings.
    
    For SVM, flushing TLB entries from the guest's perspective can be done
    by flushing the current ASID, as changes to the guest's page tables are
    associated only with the current ASID.
    
    Adding a dedicated ->tlb_flush_guest() paves the way toward removing
    @invalidate_gpa, which is a potentially dangerous control flag as its
    meaning is not exactly crystal clear, even for those who are familiar
    with the subtleties of what mappings Intel CPUs are/aren't allowed to
    keep across various invalidation scenarios.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-15-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8c1e094a639f..a7c1ea136c8c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1114,6 +1114,12 @@ struct kvm_x86_ops {
 	 */
 	void (*tlb_flush_gva)(struct kvm_vcpu *vcpu, gva_t addr);
 
+	/*
+	 * Flush any TLB entries created by the guest.  Like tlb_flush_gva(),
+	 * does not need to flush GPA->HPA mappings.
+	 */
+	void (*tlb_flush_guest)(struct kvm_vcpu *vcpu);
+
 	void (*run)(struct kvm_vcpu *vcpu);
 	int (*handle_exit)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion exit_fastpath);

commit 5efac0741ce238e0844d3f7af00198f81e84926a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Mar 23 20:42:57 2020 -0400

    KVM: x86: introduce kvm_mmu_invalidate_gva
    
    Wrap the combination of mmu->invlpg and kvm_x86_ops->tlb_flush_gva
    into a new function.  This function also lets us specify the host PGD to
    invalidate and also the MMU, both of which will be useful in fixing and
    simplifying kvm_inject_emulated_page_fault.
    
    A nested guest's MMU however has g_context->invlpg == NULL.  Instead of
    setting it to nonpaging_invlpg, make kvm_mmu_invalidate_gva the only
    entry point to mmu->invlpg and make a NULL invlpg pointer equivalent
    to nonpaging_invlpg, saving a retpoline.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 34ad7297b06b..8c1e094a639f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1511,6 +1511,8 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
+void kvm_mmu_invalidate_gva(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
+			    gva_t gva, hpa_t root_hpa);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush);
 

commit 53b3d8e9d57753295b33065f80b1e2fb4fcb946d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:01 2020 -0700

    KVM: x86: Export kvm_propagate_fault() (as kvm_inject_emulated_page_fault)
    
    Export the page fault propagation helper so that VMX can use it to
    correctly emulate TLB invalidation on page faults in an upcoming patch.
    
    In the (hopefully) not-too-distant future, SGX virtualization will also
    want access to the helper for injecting page faults to the correct level
    (L1 vs. L2) when emulating ENCLS instructions.
    
    Rename the function to kvm_inject_emulated_page_fault() to clarify that
    it is (a) injecting a fault and (b) only for page faults.  WARN if it's
    invoked with an exception other than PF_VECTOR.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-6-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 42a2d0d3984a..34ad7297b06b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1452,6 +1452,8 @@ void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);
+bool kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
+				    struct x86_exception *fault);
 int kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gfn_t gfn, void *data, int offset, int len,
 			    u32 access);

commit 6e4fd06f3ee1b12ca42fc70522f371bf10977745
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 13:26:01 2020 -0700

    KVM: x86: Drop __exit from kvm_x86_ops' hardware_unsetup()
    
    Remove the __exit annotation from VMX hardware_unsetup(), the hook
    can be reached during kvm_init() by way of kvm_arch_hardware_unsetup()
    if failure occurs at various points during initialization.
    
    Removing the annotation also lets us annotate vmx_x86_ops and svm_x86_ops
    with __initdata; otherwise, objtool complains because it doesn't
    understand that the vendor specific __initdata is being copied by value
    to a non-__initdata instance.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321202603.19355-8-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 54f991244fae..42a2d0d3984a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1056,7 +1056,7 @@ static inline u16 kvm_lapic_irq_dest_mode(bool dest_mode_logical)
 struct kvm_x86_ops {
 	int (*hardware_enable)(void);
 	void (*hardware_disable)(void);
-	void (*hardware_unsetup)(void);            /* __exit */
+	void (*hardware_unsetup)(void);
 	bool (*cpu_has_accelerated_tpr)(void);
 	bool (*has_emulated_msr)(int index);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);

commit afaf0b2f9b801c6eb2278b52d49e6a7d7b659cf1
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 13:26:00 2020 -0700

    KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection
    
    Replace the kvm_x86_ops pointer in common x86 with an instance of the
    struct to save one pointer dereference when invoking functions.  Copy the
    struct by value to set the ops during kvm_init().
    
    Arbitrarily use kvm_x86_ops.hardware_enable to track whether or not the
    ops have been initialized, i.e. a vendor KVM module has been loaded.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321202603.19355-7-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f4c5b49299ff..54f991244fae 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1274,13 +1274,13 @@ struct kvm_arch_async_pf {
 
 extern u64 __read_mostly host_efer;
 
-extern struct kvm_x86_ops *kvm_x86_ops;
+extern struct kvm_x86_ops kvm_x86_ops;
 extern struct kmem_cache *x86_fpu_cache;
 
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
-	return __vmalloc(kvm_x86_ops->vm_size,
+	return __vmalloc(kvm_x86_ops.vm_size,
 			 GFP_KERNEL_ACCOUNT | __GFP_ZERO, PAGE_KERNEL);
 }
 void kvm_arch_free_vm(struct kvm *kvm);
@@ -1288,8 +1288,8 @@ void kvm_arch_free_vm(struct kvm *kvm);
 #define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLB
 static inline int kvm_arch_flush_remote_tlb(struct kvm *kvm)
 {
-	if (kvm_x86_ops->tlb_remote_flush &&
-	    !kvm_x86_ops->tlb_remote_flush(kvm))
+	if (kvm_x86_ops.tlb_remote_flush &&
+	    !kvm_x86_ops.tlb_remote_flush(kvm))
 		return 0;
 	else
 		return -ENOTSUPP;
@@ -1375,7 +1375,7 @@ extern u64 kvm_mce_cap_supported;
  *
  * EMULTYPE_SKIP - Set when emulating solely to skip an instruction, i.e. to
  *		   decode the instruction length.  For use *only* by
- *		   kvm_x86_ops->skip_emulated_instruction() implementations.
+ *		   kvm_x86_ops.skip_emulated_instruction() implementations.
  *
  * EMULTYPE_ALLOW_RETRY_PF - Set when the emulator should resume the guest to
  *			     retry native execution under certain conditions,
@@ -1669,14 +1669,14 @@ static inline bool kvm_irq_is_postable(struct kvm_lapic_irq *irq)
 
 static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)
 {
-	if (kvm_x86_ops->vcpu_blocking)
-		kvm_x86_ops->vcpu_blocking(vcpu);
+	if (kvm_x86_ops.vcpu_blocking)
+		kvm_x86_ops.vcpu_blocking(vcpu);
 }
 
 static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
 {
-	if (kvm_x86_ops->vcpu_unblocking)
-		kvm_x86_ops->vcpu_unblocking(vcpu);
+	if (kvm_x86_ops.vcpu_unblocking)
+		kvm_x86_ops.vcpu_unblocking(vcpu);
 }
 
 static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}

commit d008dfdb0e7012ddff5bd6c0d2abd3b8ec6e77f5
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 13:25:56 2020 -0700

    KVM: x86: Move init-only kvm_x86_ops to separate struct
    
    Move the kvm_x86_ops functions that are used only within the scope of
    kvm_init() into a separate struct, kvm_x86_init_ops.  In addition to
    identifying the init-only functions without restorting to code comments,
    this also sets the stage for waiting until after ->hardware_setup() to
    set kvm_x86_ops.  Setting kvm_x86_ops after ->hardware_setup() is
    desirable as many of the hooks are not usable until ->hardware_setup()
    completes.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321202603.19355-3-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9a183e9d4cb1..f4c5b49299ff 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1054,12 +1054,8 @@ static inline u16 kvm_lapic_irq_dest_mode(bool dest_mode_logical)
 }
 
 struct kvm_x86_ops {
-	int (*cpu_has_kvm_support)(void);          /* __init */
-	int (*disabled_by_bios)(void);             /* __init */
 	int (*hardware_enable)(void);
 	void (*hardware_disable)(void);
-	int (*check_processor_compatibility)(void);/* __init */
-	int (*hardware_setup)(void);               /* __init */
 	void (*hardware_unsetup)(void);            /* __exit */
 	bool (*cpu_has_accelerated_tpr)(void);
 	bool (*has_emulated_msr)(int index);
@@ -1260,6 +1256,15 @@ struct kvm_x86_ops {
 	int (*enable_direct_tlbflush)(struct kvm_vcpu *vcpu);
 };
 
+struct kvm_x86_init_ops {
+	int (*cpu_has_kvm_support)(void);
+	int (*disabled_by_bios)(void);
+	int (*check_processor_compatibility)(void);
+	int (*hardware_setup)(void);
+
+	struct kvm_x86_ops *runtime_ops;
+};
+
 struct kvm_arch_async_pf {
 	u32 token;
 	gfn_t gfn;

commit 727a7e27cf88a261c5a0f14f4f9ee4d767352766
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 5 03:52:50 2020 -0500

    KVM: x86: rename set_cr3 callback and related flags to load_mmu_pgd
    
    The set_cr3 callback is not setting the guest CR3, it is setting the
    root of the guest page tables, either shadow or two-dimensional.
    To make this clearer as well as to indicate that the MMU calls it
    via kvm_mmu_load_cr3, rename it to load_mmu_pgd.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c3e4e764a291..9a183e9d4cb1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -58,7 +58,7 @@
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
-#define KVM_REQ_LOAD_CR3		KVM_ARCH_REQ(5)
+#define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
@@ -1091,7 +1091,6 @@ struct kvm_x86_ops {
 	void (*decache_cr0_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
-	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 	int (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
 	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);
 	void (*get_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
@@ -1155,6 +1154,8 @@ struct kvm_x86_ops {
 	int (*get_tdp_level)(struct kvm_vcpu *vcpu);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 
+	void (*load_mmu_pgd)(struct kvm_vcpu *vcpu, unsigned long cr3);
+
 	bool (*has_wbinvd_exit)(void);
 
 	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);

commit 689f3bf2162895cf0b847f36584309064887c966
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Mar 3 10:11:10 2020 +0100

    KVM: x86: unify callbacks to load paging root
    
    Similar to what kvm-intel.ko is doing, provide a single callback that
    merges svm_set_cr3, set_tdp_cr3 and nested_svm_set_tdp_cr3.
    
    This lets us unify the set_cr3 and set_tdp_cr3 entries in kvm_x86_ops.
    I'm doing that in this same patch because splitting it adds quite a bit
    of churn due to the need for forward declarations.  For the same reason
    the assignment to vcpu->arch.mmu->set_cr3 is moved to kvm_init_shadow_mmu
    from init_kvm_softmmu and nested_svm_init_mmu_context.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 24c90ea5ddbd..c3e4e764a291 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -387,7 +387,6 @@ struct kvm_mmu_root_info {
  * current mmu mode.
  */
 struct kvm_mmu {
-	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_guest_pgd)(struct kvm_vcpu *vcpu);
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u32 err,
@@ -1156,8 +1155,6 @@ struct kvm_x86_ops {
 	int (*get_tdp_level)(struct kvm_vcpu *vcpu);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 
-	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
-
 	bool (*has_wbinvd_exit)(void);
 
 	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);

commit 257038745cae1fdaa3948013a22eba3b1d610174
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:57:09 2020 -0800

    KVM: x86: Move nSVM CPUID 0x8000000A handling into common x86 code
    
    Handle CPUID 0x8000000A in the main switch in __do_cpuid_func() and drop
    ->set_supported_cpuid() now that both VMX and SVM implementations are
    empty.  Like leaf 0x14 (Intel PT) and leaf 0x8000001F (SEV), leaf
    0x8000000A is is (obviously) vendor specific but can be queried in
    common code while respecting SVM's wishes by querying kvm_cpu_cap_has().
    
    Suggested-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 531b5a96df33..24c90ea5ddbd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1158,8 +1158,6 @@ struct kvm_x86_ops {
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 
-	void (*set_supported_cpuid)(struct kvm_cpuid_entry2 *entry);
-
 	bool (*has_wbinvd_exit)(void);
 
 	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);

commit 91661989d17ccec17bca199e7cb1f463ba4e5b78
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:57:06 2020 -0800

    KVM: x86: Move VMX's host_efer to common x86 code
    
    Move host_efer to common x86 code and use it for CPUID's is_efer_nx() to
    avoid constantly re-reading the MSR.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c817987c599e..531b5a96df33 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1271,6 +1271,8 @@ struct kvm_arch_async_pf {
 	bool direct_map;
 };
 
+extern u64 __read_mostly host_efer;
+
 extern struct kvm_x86_ops *kvm_x86_ops;
 extern struct kmem_cache *x86_fpu_cache;
 

commit 703c335d06934401763863cf24fee61a13de055b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:57:03 2020 -0800

    KVM: x86/mmu: Configure max page level during hardware setup
    
    Configure the max page level during hardware setup to avoid a retpoline
    in the page fault handler.  Drop ->get_lpage_level() as the page fault
    handler was the last user.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c9b721140f59..c817987c599e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1155,7 +1155,6 @@ struct kvm_x86_ops {
 	int (*set_identity_map_addr)(struct kvm *kvm, u64 ident_addr);
 	int (*get_tdp_level)(struct kvm_vcpu *vcpu);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
-	int (*get_lpage_level)(void);
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 
@@ -1510,7 +1509,7 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush);
 
-void kvm_configure_mmu(bool enable_tdp);
+void kvm_configure_mmu(bool enable_tdp, int tdp_page_level);
 
 static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 				  struct x86_exception *exception)

commit bde7723559586d6afd18fa1717fc143531d4c77d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:57:02 2020 -0800

    KVM: x86/mmu: Merge kvm_{enable,disable}_tdp() into a common function
    
    Combine kvm_enable_tdp() and kvm_disable_tdp() into a single function,
    kvm_configure_mmu(), in preparation for doing additional configuration
    during hardware setup.  And because having separate helpers is silly.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 143d0ce493d5..c9b721140f59 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1510,8 +1510,7 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush);
 
-void kvm_enable_tdp(void);
-void kvm_disable_tdp(void);
+void kvm_configure_mmu(bool enable_tdp);
 
 static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 				  struct x86_exception *exception)

commit a1bead2abaa162e5e67ad258a06c9d71dddad00d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:57:00 2020 -0800

    KVM: VMX: Directly query Intel PT mode when refreshing PMUs
    
    Use vmx_pt_mode_is_host_guest() in intel_pmu_refresh() instead of
    bouncing through kvm_x86_ops->pt_supported, and remove ->pt_supported()
    as the PMU code was the last remaining user.
    
    Opportunistically clean up the wording of a comment that referenced
    kvm_x86_ops->pt_supported().
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 00a1be55e90a..143d0ce493d5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1176,8 +1176,6 @@ struct kvm_x86_ops {
 	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion *exit_fastpath);
 
-	bool (*pt_supported)(void);
-
 	int (*check_nested_events)(struct kvm_vcpu *vcpu);
 	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);
 

commit 139085101f8500b09c681b1e52c3839df681a0d2
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:57 2020 -0800

    KVM: x86: Use KVM cpu caps to detect MSR_TSC_AUX virt support
    
    Check for MSR_TSC_AUX virtualization via kvm_cpu_cap_has() and drop
    ->rdtscp_supported().
    
    Note, vmx_rdtscp_supported() needs to hang around a tiny bit longer due
    other usage in VMX code.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c46373016574..00a1be55e90a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1156,7 +1156,6 @@ struct kvm_x86_ops {
 	int (*get_tdp_level)(struct kvm_vcpu *vcpu);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	int (*get_lpage_level)(void);
-	bool (*rdtscp_supported)(void);
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 

commit 90d2f60f41f73b90768554e5a30b1cfedd167731
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:47 2020 -0800

    KVM: x86: Use KVM cpu caps to track UMIP emulation
    
    Set UMIP in kvm_cpu_caps when it is emulated by VMX, even though the
    bit will effectively be dropped by do_host_cpuid().  This allows
    checking for UMIP emulation via kvm_cpu_caps instead of a dedicated
    kvm_x86_ops callback.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d05138058a07..c46373016574 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1177,7 +1177,6 @@ struct kvm_x86_ops {
 	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion *exit_fastpath);
 
-	bool (*umip_emulated)(void);
 	bool (*pt_supported)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu);

commit b3d895d5c4154156894fd1df2158d82f94fb5527
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:44 2020 -0800

    KVM: x86: Move XSAVES CPUID adjust to VMX's KVM cpu cap update
    
    Move the clearing of the XSAVES CPUID bit into VMX, which has a separate
    VMCS control to enable XSAVES in non-root, to eliminate the last ugly
    renmant of the undesirable "unsigned f_* = *_supported ? F(*) : 0"
    pattern in the common CPUID handling code.
    
    Drop ->xsaves_supported(), CPUID adjustment was the only user.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5b7848e55efb..d05138058a07 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1177,7 +1177,6 @@ struct kvm_x86_ops {
 	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion *exit_fastpath);
 
-	bool (*xsaves_supported)(void);
 	bool (*umip_emulated)(void);
 	bool (*pt_supported)(void);
 

commit d64d83d1e026f9fea9c8f18bf97b9529f7e4189c
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:36 2020 -0800

    KVM: x86: Handle PKU CPUID adjustment in VMX code
    
    Move the setting of the PKU CPUID bit into VMX to eliminate an instance
    of the undesirable "unsigned f_* = *_supported ? F(*) : 0" pattern in
    the common CPUID handling code.  Drop ->pku_supported(), CPUID
    adjustment was the only user.
    
    Note, some AMD CPUs now support PKU, but SVM doesn't yet support
    exposing it to a guest.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 52470ccde7f7..5b7848e55efb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1180,7 +1180,6 @@ struct kvm_x86_ops {
 	bool (*xsaves_supported)(void);
 	bool (*umip_emulated)(void);
 	bool (*pt_supported)(void);
-	bool (*pku_supported)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu);
 	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);

commit 5ffec6f910dc8998c9da9320550ffddebe2e7afc
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:34 2020 -0800

    KVM: x86: Handle INVPCID CPUID adjustment in VMX code
    
    Move the INVPCID CPUID adjustments into VMX to eliminate an instance of
    the undesirable "unsigned f_* = *_supported ? F(*) : 0" pattern in the
    common CPUID handling code.  Drop ->invpcid_supported(), CPUID
    adjustment was the only user.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 626cbe161c57..52470ccde7f7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1157,7 +1157,6 @@ struct kvm_x86_ops {
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
-	bool (*invpcid_supported)(void);
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 

commit 160b486f65ff89be7f90ff9297bb4bb0da446d91
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:27 2020 -0800

    KVM: x86: Drop explicit @func param from ->set_supported_cpuid()
    
    Drop the explicit @func param from ->set_supported_cpuid() and instead
    pull the CPUID function from the relevant entry.  This sets the stage
    for hardening guest CPUID updates in future patches, e.g. allows adding
    run-time assertions that the CPUID feature being changed is actually
    a bit in the referenced CPUID entry.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f817ddf876b5..626cbe161c57 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1161,7 +1161,7 @@ struct kvm_x86_ops {
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 
-	void (*set_supported_cpuid)(u32 func, struct kvm_cpuid_entry2 *entry);
+	void (*set_supported_cpuid)(struct kvm_cpuid_entry2 *entry);
 
 	bool (*has_wbinvd_exit)(void);
 

commit 7f5581f592984901620d34aa86a730092ae65092
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:24 2020 -0800

    KVM: x86: Use supported_xcr0 to detect MPX support
    
    Query supported_xcr0 when checking for MPX support instead of invoking
    ->mpx_supported() and drop ->mpx_supported() as kvm_mpx_supported() was
    its last user.  Rename vmx_mpx_supported() to cpu_has_vmx_mpx() to
    better align with VMX/VMCS nomenclature.
    
    Modify VMX's adjustment of xcr0 to call cpus_has_vmx_mpx() (renamed from
    vmx_mpx_supported()) directly to avoid reading supported_xcr0 before
    it's fully configured.
    
    No functional change intended.
    
    Reviewed-by: Xiaoyao Li <xiaoyao.li@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    [Test that *all* bits are set. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6ac67b6d6692..f817ddf876b5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1177,7 +1177,7 @@ struct kvm_x86_ops {
 			       struct x86_exception *exception);
 	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion *exit_fastpath);
-	bool (*mpx_supported)(void);
+
 	bool (*xsaves_supported)(void);
 	bool (*umip_emulated)(void);
 	bool (*pt_supported)(void);

commit 2f728d66e8a7d89d7cb141bf0acb30c61ae7ded5
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:29:49 2020 -0800

    KVM: x86: Move kvm_emulate.h into KVM's private directory
    
    Now that the emulation context is dynamically allocated and not embedded
    in struct kvm_vcpu, move its header, kvm_emulate.h, out of the public
    asm directory and into KVM's private x86 directory.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 03887ec21dd8..6ac67b6d6692 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -185,7 +185,10 @@ enum exit_fastpath_completion {
 	EXIT_FASTPATH_SKIP_EMUL_INS,
 };
 
-#include <asm/kvm_emulate.h>
+struct x86_emulate_ctxt;
+struct x86_exception;
+enum x86_intercept;
+enum x86_intercept_stage;
 
 #define KVM_NR_MEM_OBJS 40
 
@@ -1418,8 +1421,6 @@ int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data);
 int kvm_emulate_rdmsr(struct kvm_vcpu *vcpu);
 int kvm_emulate_wrmsr(struct kvm_vcpu *vcpu);
 
-struct x86_emulate_ctxt;
-
 int kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in);
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);

commit c9b8b07cded58c55ad2bf67e68b9bfae96092293
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:29:48 2020 -0800

    KVM: x86: Dynamically allocate per-vCPU emulation context
    
    Allocate the emulation context instead of embedding it in struct
    kvm_vcpu_arch.
    
    Dynamic allocation provides several benefits:
    
      - Shrinks the size x86 vcpus by ~2.5k bytes, dropping them back below
        the PAGE_ALLOC_COSTLY_ORDER threshold.
      - Allows for dropping the include of kvm_emulate.h from asm/kvm_host.h
        and moving kvm_emulate.h into KVM's private directory.
      - Allows a reducing KVM's attack surface by shrinking the amount of
        vCPU data that is exposed to usercopy.
      - Allows a future patch to disable the emulator entirely, which may or
        may not be a realistic endeavor.
    
    Mark the entire struct as valid for usercopy to maintain existing
    behavior with respect to hardened usercopy.  Future patches can shrink
    the usercopy range to cover only what is necessary.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index af4264498554..03887ec21dd8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -680,7 +680,7 @@ struct kvm_vcpu_arch {
 
 	/* emulate context */
 
-	struct x86_emulate_ctxt emulate_ctxt;
+	struct x86_emulate_ctxt *emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
 	int (*complete_userspace_io)(struct kvm_vcpu *vcpu);

commit 21f1b8f29ea5b2301af7f2cc41a20b7b87a22bec
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:29:42 2020 -0800

    KVM: x86: Explicitly pass an exception struct to check_intercept
    
    Explicitly pass an exception struct when checking for intercept from
    the emulator, which eliminates the last reference to arch.emulate_ctxt
    in vendor specific code.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 316ec6cf532b..af4264498554 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1170,7 +1170,8 @@ struct kvm_x86_ops {
 
 	int (*check_intercept)(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
-			       enum x86_intercept_stage stage);
+			       enum x86_intercept_stage stage,
+			       struct x86_exception *exception);
 	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion *exit_fastpath);
 	bool (*mpx_supported)(void);

commit d8dd54e06348c43b97e5c0d488e5ee4e004bfb6f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 18:02:39 2020 -0800

    KVM: x86/mmu: Rename kvm_mmu->get_cr3() to ->get_guest_pgd()
    
    Rename kvm_mmu->get_cr3() to call out that it is retrieving a guest
    value, as opposed to kvm_mmu->set_cr3(), which sets a host value, and to
    note that it will return something other than CR3 when nested EPT is in
    use.  Hopefully the new name will also make it more obvious that L1's
    nested_cr3 is returned in SVM's nested NPT case.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 327cfce91185..316ec6cf532b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -385,7 +385,7 @@ struct kvm_mmu_root_info {
  */
 struct kvm_mmu {
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
-	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
+	unsigned long (*get_guest_pgd)(struct kvm_vcpu *vcpu);
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u32 err,
 			  bool prefault);

commit 8053f924cad30bf9f9a24e02b6c8ddfabf5202ea
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 18:02:35 2020 -0800

    KVM: x86/mmu: Drop kvm_mmu_extended_role.cr4_la57 hack
    
    Drop kvm_mmu_extended_role.cr4_la57 now that mmu_role doesn't mask off
    level, which already incorporates the guest's CR4.LA57 for a shadow MMU
    by querying is_la57_mode().
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6093ff1f3bdb..327cfce91185 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -300,7 +300,6 @@ union kvm_mmu_extended_role {
 		unsigned int cr4_pke:1;
 		unsigned int cr4_smap:1;
 		unsigned int cr4_smep:1;
-		unsigned int cr4_la57:1;
 		unsigned int maxphyaddr:6;
 	};
 };

commit a1c77abb8d93381e25a8d2df3a917388244ba776
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 22:27:35 2020 -0800

    KVM: nVMX: Properly handle userspace interrupt window request
    
    Return true for vmx_interrupt_allowed() if the vCPU is in L2 and L1 has
    external interrupt exiting enabled.  IRQs are never blocked in hardware
    if the CPU is in the guest (L2 from L1's perspective) when IRQs trigger
    VM-Exit.
    
    The new check percolates up to kvm_vcpu_ready_for_interrupt_injection()
    and thus vcpu_run(), and so KVM will exit to userspace if userspace has
    requested an interrupt window (to inject an IRQ into L1).
    
    Remove the @external_intr param from vmx_check_nested_events(), which is
    actually an indicator that userspace wants an interrupt window, e.g.
    it's named @req_int_win further up the stack.  Injecting a VM-Exit into
    L1 to try and bounce out to L0 userspace is all kinds of broken and is
    no longer necessary.
    
    Remove the hack in nested_vmx_vmexit() that attempted to workaround the
    breakage in vmx_check_nested_events() by only filling interrupt info if
    there's an actual interrupt pending.  The hack actually made things
    worse because it caused KVM to _never_ fill interrupt info when the
    LAPIC resides in userspace (kvm_cpu_has_interrupt() queries
    interrupt.injected, which is always cleared by prepare_vmcs12() before
    reaching the hack in nested_vmx_vmexit()).
    
    Fixes: 6550c4df7e50 ("KVM: nVMX: Fix interrupt window request with "Acknowledge interrupt on exit"")
    Cc: stable@vger.kernel.org
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 642273712e6a..6093ff1f3bdb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1180,7 +1180,7 @@ struct kvm_x86_ops {
 	bool (*pt_supported)(void);
 	bool (*pku_supported)(void);
 
-	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
+	int (*check_nested_events)(struct kvm_vcpu *vcpu);
 	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);
 
 	void (*sched_in)(struct kvm_vcpu *kvm, int cpu);

commit 4abaffce4d25aa41392d2e81835592726d757857
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Wed Feb 26 10:41:02 2020 +0800

    KVM: LAPIC: Recalculate apic map in batch
    
    In the vCPU reset and set APIC_BASE MSR path, the apic map will be recalculated
    several times, each time it will consume 10+ us observed by ftrace in my
    non-overcommit environment since the expensive memory allocate/mutex/rcu etc
    operations. This patch optimizes it by recaluating apic map in batch, I hope
    this can benefit the serverless scenario which can frequently create/destroy
    VMs.
    
    Before patch:
    
    kvm_lapic_reset  ~27us
    
    After patch:
    
    kvm_lapic_reset  ~14us
    
    Observed by ftrace, improve ~48%.
    
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 681e23071847..642273712e6a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -920,6 +920,7 @@ struct kvm_arch {
 	atomic_t vapics_in_nmi_mode;
 	struct mutex apic_map_lock;
 	struct kvm_apic_map *apic_map;
+	bool apic_map_dirty;
 
 	bool apic_access_page_done;
 	unsigned long apicv_inhibit_reasons;

commit 3c9bd4006bfc2dccda1823db61b3f470ef91cfaa
Author: Jay Zhou <jianjay.zhou@huawei.com>
Date:   Thu Feb 27 09:32:27 2020 +0800

    KVM: x86: enable dirty log gradually in small chunks
    
    It could take kvm->mmu_lock for an extended period of time when
    enabling dirty log for the first time. The main cost is to clear
    all the D-bits of last level SPTEs. This situation can benefit from
    manual dirty log protect as well, which can reduce the mmu_lock
    time taken. The sequence is like this:
    
    1. Initialize all the bits of the dirty bitmap to 1 when enabling
       dirty log for the first time
    2. Only write protect the huge pages
    3. KVM_GET_DIRTY_LOG returns the dirty bitmap info
    4. KVM_CLEAR_DIRTY_LOG will clear D-bit for each of the leaf level
       SPTEs gradually in small chunks
    
    Under the Intel(R) Xeon(R) Gold 6152 CPU @ 2.10GHz environment,
    I did some tests with a 128G windows VM and counted the time taken
    of memory_global_dirty_log_start, here is the numbers:
    
    VM Size        Before    After optimization
    128G           460ms     10ms
    
    Signed-off-by: Jay Zhou <jianjay.zhou@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f58861e2ece5..681e23071847 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -49,6 +49,9 @@
 
 #define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS
 
+#define KVM_DIRTY_LOG_MANUAL_CAPS   (KVM_DIRTY_LOG_MANUAL_PROTECT_ENABLE | \
+					KVM_DIRTY_LOG_INITIALLY_SET)
+
 /* x86-specific vcpu->requests bit members */
 #define KVM_REQ_MIGRATE_TIMER		KVM_ARCH_REQ(0)
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
@@ -1306,7 +1309,8 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
-				      struct kvm_memory_slot *memslot);
+				      struct kvm_memory_slot *memslot,
+				      int start_level);
 void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,

commit cc7f5577adfc766de8613b71e9ae52c053fcca01
Author: Oliver Upton <oupton@google.com>
Date:   Fri Feb 28 00:59:04 2020 -0800

    KVM: SVM: Inhibit APIC virtualization for X2APIC guest
    
    The AVIC does not support guest use of the x2APIC interface. Currently,
    KVM simply chooses to squash the x2APIC feature in the guest's CPUID
    If the AVIC is enabled. Doing so prevents KVM from running a guest
    with greater than 255 vCPUs, as such a guest necessitates the use
    of the x2APIC interface.
    
    Instead, inhibit AVIC enablement on a per-VM basis whenever the x2APIC
    feature is set in the guest's CPUID.
    
    Signed-off-by: Oliver Upton <oupton@google.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5edf6425c747..f58861e2ece5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -886,6 +886,7 @@ enum kvm_irqchip_mode {
 #define APICV_INHIBIT_REASON_NESTED     2
 #define APICV_INHIBIT_REASON_IRQWIN     3
 #define APICV_INHIBIT_REASON_PIT_REINJ  4
+#define APICV_INHIBIT_REASON_X2APIC	5
 
 struct kvm_arch {
 	unsigned long n_used_mmu_pages;

commit 562b6b089d64724278de61114da658fb0a516250
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sun Jan 26 16:41:13 2020 -0800

    KVM: x86: Consolidate VM allocation and free for VMX and SVM
    
    Move the VM allocation and free code to common x86 as the logic is
    more or less identical across SVM and VMX.
    
    Note, although hyperv.hv_pa_pg is part of the common kvm->arch, it's
    (currently) only allocated by VMX VMs.  But, since kfree() plays nice
    when passed a NULL pointer, the superfluous call for SVM is harmless
    and avoids future churn if SVM gains support for HyperV's direct TLB
    flush.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    [Make vm_size a field instead of a function. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 06c21f14298b..5edf6425c747 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1059,8 +1059,7 @@ struct kvm_x86_ops {
 	bool (*has_emulated_msr)(int index);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
-	struct kvm *(*vm_alloc)(void);
-	void (*vm_free)(struct kvm *);
+	unsigned int vm_size;
 	int (*vm_init)(struct kvm *kvm);
 	void (*vm_destroy)(struct kvm *kvm);
 
@@ -1278,13 +1277,10 @@ extern struct kmem_cache *x86_fpu_cache;
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
-	return kvm_x86_ops->vm_alloc();
-}
-
-static inline void kvm_arch_free_vm(struct kvm *kvm)
-{
-	return kvm_x86_ops->vm_free(kvm);
+	return __vmalloc(kvm_x86_ops->vm_size,
+			 GFP_KERNEL_ACCOUNT | __GFP_ZERO, PAGE_KERNEL);
 }
+void kvm_arch_free_vm(struct kvm *kvm);
 
 #define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLB
 static inline int kvm_arch_flush_remote_tlb(struct kvm *kvm)

commit 744e699c7e9913a9b1f825f189ab547c4da0d182
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:03:09 2020 -0800

    KVM: x86: Move gpa_val and gpa_available into the emulator context
    
    Move the GPA tracking into the emulator context now that the context is
    guaranteed to be initialized via __init_emulate_ctxt() prior to
    dereferencing gpa_{available,val}, i.e. now that seeing a stale
    gpa_available will also trigger a WARN due to an invalid context.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ba4cf00fdb42..06c21f14298b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -808,10 +808,6 @@ struct kvm_vcpu_arch {
 	int pending_ioapic_eoi;
 	int pending_external_vector;
 
-	/* GPA available */
-	bool gpa_available;
-	gpa_t gpa_val;
-
 	/* be preempted when it's in kernel-mode(cpl=0) */
 	bool preempted_in_kernel;
 

commit 92daa48b34d784748b575ae424def4ea7f024b2f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:03:08 2020 -0800

    KVM: x86: Add EMULTYPE_PF when emulation is triggered by a page fault
    
    Add a new emulation type flag to explicitly mark emulation related to a
    page fault.  Move the propation of the GPA into the emulator from the
    page fault handler into x86_emulate_instruction, using EMULTYPE_PF as an
    indicator that cr2 is valid.  Similarly, don't propagate cr2 into the
    exception.address when it's *not* valid.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 98959e8cd448..ba4cf00fdb42 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1381,8 +1381,9 @@ extern u64 kvm_mce_cap_supported;
  *		   decode the instruction length.  For use *only* by
  *		   kvm_x86_ops->skip_emulated_instruction() implementations.
  *
- * EMULTYPE_ALLOW_RETRY - Set when the emulator should resume the guest to
- *			  retry native execution under certain conditions.
+ * EMULTYPE_ALLOW_RETRY_PF - Set when the emulator should resume the guest to
+ *			     retry native execution under certain conditions,
+ *			     Can only be set in conjunction with EMULTYPE_PF.
  *
  * EMULTYPE_TRAP_UD_FORCED - Set when emulating an intercepted #UD that was
  *			     triggered by KVM's magic "force emulation" prefix,
@@ -1395,13 +1396,18 @@ extern u64 kvm_mce_cap_supported;
  *			backdoor emulation, which is opt in via module param.
  *			VMware backoor emulation handles select instructions
  *			and reinjects the #GP for all other cases.
+ *
+ * EMULTYPE_PF - Set when emulating MMIO by way of an intercepted #PF, in which
+ *		 case the CR2/GPA value pass on the stack is valid.
  */
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
-#define EMULTYPE_ALLOW_RETRY	    (1 << 3)
+#define EMULTYPE_ALLOW_RETRY_PF	    (1 << 3)
 #define EMULTYPE_TRAP_UD_FORCED	    (1 << 4)
 #define EMULTYPE_VMWARE_GP	    (1 << 5)
+#define EMULTYPE_PF		    (1 << 6)
+
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type);
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 					void *insn, int insn_len);

commit 5ef8acbdd687c9d72582e2c05c0b9756efb37863
Author: Oliver Upton <oupton@google.com>
Date:   Fri Feb 7 02:36:07 2020 -0800

    KVM: nVMX: Emulate MTF when performing instruction emulation
    
    Since commit 5f3d45e7f282 ("kvm/x86: add support for
    MONITOR_TRAP_FLAG"), KVM has allowed an L1 guest to use the monitor trap
    flag processor-based execution control for its L2 guest. KVM simply
    forwards any MTF VM-exits to the L1 guest, which works for normal
    instruction execution.
    
    However, when KVM needs to emulate an instruction on the behalf of an L2
    guest, the monitor trap flag is not emulated. Add the necessary logic to
    kvm_skip_emulated_instruction() to synthesize an MTF VM-exit to L1 upon
    instruction emulation for L2.
    
    Fixes: 5f3d45e7f282 ("kvm/x86: add support for MONITOR_TRAP_FLAG")
    Signed-off-by: Oliver Upton <oupton@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a84e8c5acda8..98959e8cd448 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1122,6 +1122,7 @@ struct kvm_x86_ops {
 	int (*handle_exit)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion exit_fastpath);
 	int (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
+	void (*update_emulated_instruction)(struct kvm_vcpu *vcpu);
 	void (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
 	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu);
 	void (*patch_hypercall)(struct kvm_vcpu *vcpu,

commit 91a5f413af596ad01097e59bf487eb07cb3f1331
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Feb 20 18:22:05 2020 +0100

    KVM: nVMX: handle nested posted interrupts when apicv is disabled for L1
    
    Even when APICv is disabled for L1 it can (and, actually, is) still
    available for L2, this means we need to always call
    vmx_deliver_nested_posted_interrupt() when attempting an interrupt
    delivery.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 40a0c0fd95ca..a84e8c5acda8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1146,7 +1146,7 @@ struct kvm_x86_ops {
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_apic_mode)(struct kvm_vcpu *vcpu);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
-	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
+	int (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	int (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*set_identity_map_addr)(struct kvm *kvm, u64 ident_addr);

commit ffdbd50dca67b1f12d6f531a0eaf2028d793e54f
Author: Miaohe Lin <linmiaohe@huawei.com>
Date:   Fri Feb 7 23:22:45 2020 +0800

    KVM: nVMX: Fix some comment typos and coding style
    
    Fix some typos in the comments. Also fix coding style.
    [Sean Christopherson rewrites the comment of write_fault_to_shadow_pgtable
    field in struct kvm_vcpu_arch.]
    
    Signed-off-by: Miaohe Lin <linmiaohe@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4dffbc10d3f8..40a0c0fd95ca 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -781,9 +781,19 @@ struct kvm_vcpu_arch {
 	u64 msr_kvm_poll_control;
 
 	/*
-	 * Indicate whether the access faults on its page table in guest
-	 * which is set when fix page fault and used to detect unhandeable
-	 * instruction.
+	 * Indicates the guest is trying to write a gfn that contains one or
+	 * more of the PTEs used to translate the write itself, i.e. the access
+	 * is changing its own translation in the guest page tables.  KVM exits
+	 * to userspace if emulation of the faulting instruction fails and this
+	 * flag is set, as KVM cannot make forward progress.
+	 *
+	 * If emulation fails for a write to guest page tables, KVM unprotects
+	 * (zaps) the shadow page for the target gfn and resumes the guest to
+	 * retry the non-emulatable instruction (on hardware).  Unprotecting the
+	 * gfn doesn't allow forward progress for a self-changing access because
+	 * doing so also zaps the translation for the gfn, i.e. retrying the
+	 * instruction will hit a !PRESENT fault, which results in a new shadow
+	 * page and sends KVM back to square one.
 	 */
 	bool write_fault_to_shadow_pgtable;
 

commit e2ed4078a6ef3ddf4063329298852e24c36d46c8
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:16 2019 -0600

    kvm: i8254: Deactivate APICv when using in-kernel PIT re-injection mode.
    
    AMD SVM AVIC accelerates EOI write and does not trap. This causes
    in-kernel PIT re-injection mode to fail since it relies on irq-ack
    notifier mechanism. So, APICv is activated only when in-kernel PIT
    is in discard mode e.g. w/ qemu option:
    
      -global kvm-pit.lost_tick_policy=discard
    
    Also, introduce APICV_INHIBIT_REASON_PIT_REINJ bit to be used for this
    reason.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2bd7fd96d994..4dffbc10d3f8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -879,6 +879,7 @@ enum kvm_irqchip_mode {
 #define APICV_INHIBIT_REASON_HYPERV     1
 #define APICV_INHIBIT_REASON_NESTED     2
 #define APICV_INHIBIT_REASON_IRQWIN     3
+#define APICV_INHIBIT_REASON_PIT_REINJ  4
 
 struct kvm_arch {
 	unsigned long n_used_mmu_pages;

commit f3515dc3bef81e96bdb2ac93ef8fd20b1c2aaae5
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:15 2019 -0600

    svm: Temporarily deactivate AVIC during ExtINT handling
    
    AMD AVIC does not support ExtINT. Therefore, AVIC must be temporary
    deactivated and fall back to using legacy interrupt injection via vINTR
    and interrupt window.
    
    Also, introduce APICV_INHIBIT_REASON_IRQWIN to be used for this reason.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    [Rename svm_request_update_avic to svm_toggle_avic_for_extint. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ce19dea5f2dd..2bd7fd96d994 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -878,6 +878,7 @@ enum kvm_irqchip_mode {
 #define APICV_INHIBIT_REASON_DISABLE    0
 #define APICV_INHIBIT_REASON_HYPERV     1
 #define APICV_INHIBIT_REASON_NESTED     2
+#define APICV_INHIBIT_REASON_IRQWIN     3
 
 struct kvm_arch {
 	unsigned long n_used_mmu_pages;

commit 9a0bf05430699dc94b7ced940f6270c7cf1d77ef
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:14 2019 -0600

    svm: Deactivate AVIC when launching guest with nested SVM support
    
    Since AVIC does not currently work w/ nested virtualization,
    deactivate AVIC for the guest if setting CPUID Fn80000001_ECX[SVM]
    (i.e. indicate support for SVM, which is needed for nested virtualization).
    Also, introduce a new APICV_INHIBIT_REASON_NESTED bit to be used for
    this reason.
    
    Suggested-by: Alexander Graf <graf@amazon.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 90bfe8becc56..ce19dea5f2dd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -877,6 +877,7 @@ enum kvm_irqchip_mode {
 
 #define APICV_INHIBIT_REASON_DISABLE    0
 #define APICV_INHIBIT_REASON_HYPERV     1
+#define APICV_INHIBIT_REASON_NESTED     2
 
 struct kvm_arch {
 	unsigned long n_used_mmu_pages;

commit f4fdc0a2edf48f16f7b10cceaf4781fc56ab7fd9
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:13 2019 -0600

    kvm: x86: hyperv: Use APICv update request interface
    
    Since disabling APICv has to be done for all vcpus on AMD-based
    system, adopt the newly introduced kvm_request_apicv_update()
    interface, and introduce a new APICV_INHIBIT_REASON_HYPERV.
    
    Also, remove the kvm_vcpu_deactivate_apicv() since no longer used.
    
    Cc: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 19a7d0d3a5fa..90bfe8becc56 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -876,6 +876,7 @@ enum kvm_irqchip_mode {
 };
 
 #define APICV_INHIBIT_REASON_DISABLE    0
+#define APICV_INHIBIT_REASON_HYPERV     1
 
 struct kvm_arch {
 	unsigned long n_used_mmu_pages;
@@ -1483,7 +1484,6 @@ gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,
 gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
 				struct x86_exception *exception);
 
-void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu);
 bool kvm_apicv_activated(struct kvm *kvm);
 void kvm_apicv_init(struct kvm *kvm, bool enable);
 void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu);

commit 2de9d0ccd0fea32fc6a684f3f22496967ed608bc
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:11 2019 -0600

    kvm: x86: Introduce x86 ops hook for pre-update APICv
    
    AMD SVM AVIC needs to update APIC backing page mapping before changing
    APICv mode. Introduce struct kvm_x86_ops.pre_update_apicv_exec_ctrl
    function hook to be called prior KVM APICv update request to each vcpu.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 81c41bfb0a5f..19a7d0d3a5fa 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1124,6 +1124,7 @@ struct kvm_x86_ops {
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
 	bool (*check_apicv_inhibit_reasons)(ulong bit);
+	void (*pre_update_apicv_exec_ctrl)(struct kvm *kvm, bool activate);
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);

commit ef8efd7a15bb7147a4ffb09758a6bd25d744a14e
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:10 2019 -0600

    kvm: x86: Introduce APICv x86 ops for checking APIC inhibit reasons
    
    Inibit reason bits are used to determine if APICv deactivation is
    applicable for a particular hardware virtualization architecture.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0189687877a7..81c41bfb0a5f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1123,6 +1123,7 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
+	bool (*check_apicv_inhibit_reasons)(ulong bit);
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);

commit 8df14af42f00a434c492c9964a8095bf59831a45
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:06 2019 -0600

    kvm: x86: Add support for dynamic APICv activation
    
    Certain runtime conditions require APICv to be temporary deactivated
    during runtime.  The current implementation only support run-time
    deactivation of APICv when Hyper-V SynIC is enabled, which is not
    temporary.
    
    In addition, for AMD, when APICv is (de)activated at runtime,
    all vcpus in the VM have to operate in the same mode.  Thus the
    requesting vcpu must notify the others.
    
    So, introduce the following:
     * A new KVM_REQ_APICV_UPDATE request bit
     * Interfaces to request all vcpus to update APICv status
     * A new interface to update APICV-related parameters for each vcpu
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9945c7bebdf8..0189687877a7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -78,6 +78,8 @@
 #define KVM_REQ_HV_STIMER		KVM_ARCH_REQ(22)
 #define KVM_REQ_LOAD_EOI_EXITMAP	KVM_ARCH_REQ(23)
 #define KVM_REQ_GET_VMCS12_PAGES	KVM_ARCH_REQ(24)
+#define KVM_REQ_APICV_UPDATE \
+	KVM_ARCH_REQ_FLAGS(25, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
@@ -1482,6 +1484,9 @@ gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
 void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu);
 bool kvm_apicv_activated(struct kvm *kvm);
 void kvm_apicv_init(struct kvm *kvm, bool enable);
+void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu);
+void kvm_request_apicv_update(struct kvm *kvm, bool activate,
+			      unsigned long bit);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 

commit 7e3e67a98701cbcb4378b4f69b28a43351ca27c2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jan 22 16:54:37 2020 +0100

    KVM: x86: remove get_enable_apicv from kvm_x86_ops
    
    It is unused now.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4d57e4b74aae..9945c7bebdf8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1121,7 +1121,6 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
-	bool (*get_enable_apicv)(struct kvm *kvm);
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);

commit 4e19c36f2df8f84da22c7287de86729aaf3e352b
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Thu Nov 14 14:15:05 2019 -0600

    kvm: x86: Introduce APICv inhibit reason bits
    
    There are several reasons in which a VM needs to deactivate APICv
    e.g. disable APICv via parameter during module loading, or when
    enable Hyper-V SynIC support. Additional inhibit reasons will be
    introduced later on when dynamic APICv is supported,
    
    Introduce KVM APICv inhibit reason bits along with a new variable,
    apicv_inhibit_reasons, to help keep track of APICv state for each VM,
    
    Initially, the APICV_INHIBIT_REASON_DISABLE bit is used to indicate
    the case where APICv is disabled during KVM module load.
    (e.g. insmod kvm_amd avic=0 or insmod kvm_intel enable_apicv=0).
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    [Do not use get_enable_apicv; consider irqchip_split in svm.c. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 329d01c689b7..4d57e4b74aae 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -873,6 +873,8 @@ enum kvm_irqchip_mode {
 	KVM_IRQCHIP_SPLIT,        /* created with KVM_CAP_SPLIT_IRQCHIP */
 };
 
+#define APICV_INHIBIT_REASON_DISABLE    0
+
 struct kvm_arch {
 	unsigned long n_used_mmu_pages;
 	unsigned long n_requested_mmu_pages;
@@ -904,6 +906,7 @@ struct kvm_arch {
 	struct kvm_apic_map *apic_map;
 
 	bool apic_access_page_done;
+	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
 
@@ -1478,6 +1481,8 @@ gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
 				struct x86_exception *exception);
 
 void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu);
+bool kvm_apicv_activated(struct kvm *kvm);
+void kvm_apicv_init(struct kvm *kvm, bool enable);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 

commit 4cbc418a44d5067133271bb6eeac2382f2bf94f7
Merge: 1d5920c306f1 a6bd811f1209
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 30 18:47:38 2020 +0100

    Merge branch 'cve-2019-3016' into kvm-next-5.6
    
    From Boris Ostrovsky:
    
    The KVM hypervisor may provide a guest with ability to defer remote TLB
    flush when the remote VCPU is not running. When this feature is used,
    the TLB flush will happen only when the remote VPCU is scheduled to run
    again. This will avoid unnecessary (and expensive) IPIs.
    
    Under certain circumstances, when a guest initiates such deferred action,
    the hypervisor may miss the request. It is also possible that the guest
    may mistakenly assume that it has already marked remote VCPU as needing
    a flush when in fact that request had already been processed by the
    hypervisor. In both cases this will result in an invalid translation
    being present in a vCPU, potentially allowing accesses to memory locations
    in that guest's address space that should not be accessible.
    
    Note that only intra-guest memory is vulnerable.
    
    The five patches address both of these problems:
    1. The first patch makes sure the hypervisor doesn't accidentally clear
    a guest's remote flush request
    2. The rest of the patches prevent the race between hypervisor
    acknowledging a remote flush request and guest issuing a new one.
    
    Conflicts:
            arch/x86/kvm/x86.c [move from kvm_arch_vcpu_free to kvm_arch_vcpu_destroy]

commit a6bd811f1209fe1c64c9f6fd578101d6436c6b6e
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Fri Dec 6 15:36:12 2019 +0000

    x86/KVM: Clean up host's steal time structure
    
    Now that we are mapping kvm_steal_time from the guest directly we
    don't need keep a copy of it in kvm_vcpu_arch.st. The same is true
    for the stime field.
    
    This is part of CVE-2019-3016.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f48a306e1d66..4925bdbfb516 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -685,10 +685,9 @@ struct kvm_vcpu_arch {
 	bool pvclock_set_guest_stopped_request;
 
 	struct {
+		u8 preempted;
 		u64 msr_val;
 		u64 last_steal;
-		struct gfn_to_hva_cache stime;
-		struct kvm_steal_time steal;
 		struct gfn_to_pfn_cache cache;
 	} st;
 

commit 917248144db5d7320655dbb41d3af0b8a0f3d589
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Thu Dec 5 01:30:51 2019 +0000

    x86/kvm: Cache gfn to pfn translation
    
    __kvm_map_gfn()'s call to gfn_to_pfn_memslot() is
    * relatively expensive
    * in certain cases (such as when done from atomic context) cannot be called
    
    Stashing gfn-to-pfn mapping should help with both cases.
    
    This is part of CVE-2019-3016.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b79cd6aa4075..f48a306e1d66 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -689,6 +689,7 @@ struct kvm_vcpu_arch {
 		u64 last_steal;
 		struct gfn_to_hva_cache stime;
 		struct kvm_steal_time steal;
+		struct gfn_to_pfn_cache cache;
 	} st;
 
 	u64 tsc_offset;

commit 6a3c623ba8a842f895e80a7fa0feb94b7b4368f2
Author: Peter Xu <peterx@redhat.com>
Date:   Thu Jan 9 09:57:16 2020 -0500

    KVM: X86: Drop x86_set_memory_region()
    
    The helper x86_set_memory_region() is only used in vmx_set_tss_addr()
    and kvm_arch_destroy_vm().  Push the lock upper in both cases.  With
    that, drop x86_set_memory_region().
    
    This prepares to allow __x86_set_memory_region() to return a HVA
    mapped, because the HVA will need to be protected by the lock too even
    after __x86_set_memory_region() returns.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 49751cbd6e63..69e31dbdfdc2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1627,7 +1627,6 @@ void __kvm_request_immediate_exit(struct kvm_vcpu *vcpu);
 int kvm_is_in_guest(void);
 
 int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size);
-int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size);
 bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 

commit a47970ed74a535b1accb4bc73643fd5a93993c3e
Author: John Allen <john.allen@amd.com>
Date:   Thu Dec 19 14:17:59 2019 -0600

    kvm/svm: PKU not currently supported
    
    Current SVM implementation does not have support for handling PKU. Guests
    running on a host with future AMD cpus that support the feature will read
    garbage from the PKRU register and will hit segmentation faults on boot as
    memory is getting marked as protected that should not be. Ensure that cpuid
    from SVM does not advertise the feature.
    
    Signed-off-by: John Allen <john.allen@amd.com>
    Cc: stable@vger.kernel.org
    Fixes: 0556cbdc2fbc ("x86/pkeys: Don't check if PKRU is zero before writing it")
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fff9ed6956b5..49751cbd6e63 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1157,6 +1157,7 @@ struct kvm_x86_ops {
 	bool (*xsaves_supported)(void);
 	bool (*umip_emulated)(void);
 	bool (*pt_supported)(void);
+	bool (*pku_supported)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
 	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);

commit 987b2594ed5d128c95c5255a9c7755f7480bf407
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:54:55 2019 -0800

    KVM: x86: Move kvm_vcpu_init() invocation to common code
    
    Move the kvm_cpu_{un}init() calls to common x86 code as an intermediate
    step to removing kvm_cpu_{un}init() altogether.
    
    Note, VMX'x alloc_apic_access_page() and init_rmode_identity_map() are
    per-VM allocations and are intentionally kept if vCPU creation fails.
    They are freed by kvm_arch_destroy_vm().
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aa591a77072b..fff9ed6956b5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1050,7 +1050,7 @@ struct kvm_x86_ops {
 	void (*vm_destroy)(struct kvm *kvm);
 
 	/* Create, but do not attach this VCPU */
-	int (*vcpu_create)(struct kvm *kvm, struct kvm_vcpu *vcpu, unsigned id);
+	int (*vcpu_create)(struct kvm_vcpu *vcpu);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
 	void (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);
 

commit a9dd6f09d7e54d3f58be32d7d051196f7a00e69e
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:54:52 2019 -0800

    KVM: x86: Allocate vcpu struct in common x86 code
    
    Move allocation of VMX and SVM vcpus to common x86.  Although the struct
    being allocated is technically a VMX/SVM struct, it can be interpreted
    directly as a 'struct kvm_vcpu' because of the pre-existing requirement
    that 'struct kvm_vcpu' be located at offset zero of the arch/vendor vcpu
    struct.
    
    Remove the message from the build-time assertions regarding placement of
    the struct, as compatibility with the arch usercopy region is no longer
    the sole dependent on 'struct kvm_vcpu' being at offset zero.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0b5c280644e5..aa591a77072b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1050,7 +1050,7 @@ struct kvm_x86_ops {
 	void (*vm_destroy)(struct kvm *kvm);
 
 	/* Create, but do not attach this VCPU */
-	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);
+	int (*vcpu_create)(struct kvm *kvm, struct kvm_vcpu *vcpu, unsigned id);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
 	void (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);
 

commit 311497e0c5565e7d9cf7b0987d17626b228b8fec
Author: Miaohe Lin <linmiaohe@huawei.com>
Date:   Wed Dec 11 14:26:25 2019 +0800

    KVM: Fix some writing mistakes
    
    Fix some writing mistakes in the comments.
    
    Signed-off-by: Miaohe Lin <linmiaohe@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e2b793477243..0b5c280644e5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -611,7 +611,7 @@ struct kvm_vcpu_arch {
 	 * Paging state of an L2 guest (used for nested npt)
 	 *
 	 * This context will save all necessary information to walk page tables
-	 * of the an L2 guest. This context is only initialized for page table
+	 * of an L2 guest. This context is only initialized for page table
 	 * walking and not for faulting since we never handle l2 page faults on
 	 * the host.
 	 */

commit 1e9e2622a149e88bd636c9f8fb346a6e6aefeae0
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Thu Nov 21 11:17:11 2019 +0800

    KVM: VMX: FIXED+PHYSICAL mode single target IPI fastpath
    
    ICR and TSCDEADLINE MSRs write cause the main MSRs write vmexits in our
    product observation, multicast IPIs are not as common as unicast IPI like
    RESCHEDULE_VECTOR and CALL_FUNCTION_SINGLE_VECTOR etc.
    
    This patch introduce a mechanism to handle certain performance-critical
    WRMSRs in a very early stage of KVM VMExit handler.
    
    This mechanism is specifically used for accelerating writes to x2APIC ICR
    that attempt to send a virtual IPI with physical destination-mode, fixed
    delivery-mode and single target. Which was found as one of the main causes
    of VMExits for Linux workloads.
    
    The reason this mechanism significantly reduce the latency of such virtual
    IPIs is by sending the physical IPI to the target vCPU in a very early stage
    of KVM VMExit handler, before host interrupts are enabled and before expensive
    operations such as reacquiring KVM’s SRCU lock.
    Latency is reduced even more when KVM is able to use APICv posted-interrupt
    mechanism (which allows to deliver the virtual IPI directly to target vCPU
    without the need to kick it to host).
    
    Testing on Xeon Skylake server:
    
    The virtual IPI latency from sender send to receiver receive reduces
    more than 200+ cpu cycles.
    
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 159a28512e4c..e2b793477243 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -175,6 +175,11 @@ enum {
 	VCPU_SREG_LDTR,
 };
 
+enum exit_fastpath_completion {
+	EXIT_FASTPATH_NONE,
+	EXIT_FASTPATH_SKIP_EMUL_INS,
+};
+
 #include <asm/kvm_emulate.h>
 
 #define KVM_NR_MEM_OBJS 40
@@ -1095,7 +1100,8 @@ struct kvm_x86_ops {
 	void (*tlb_flush_gva)(struct kvm_vcpu *vcpu, gva_t addr);
 
 	void (*run)(struct kvm_vcpu *vcpu);
-	int (*handle_exit)(struct kvm_vcpu *vcpu);
+	int (*handle_exit)(struct kvm_vcpu *vcpu,
+		enum exit_fastpath_completion exit_fastpath);
 	int (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
 	void (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
 	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu);
@@ -1145,7 +1151,8 @@ struct kvm_x86_ops {
 	int (*check_intercept)(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage);
-	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu);
+	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu,
+		enum exit_fastpath_completion *exit_fastpath);
 	bool (*mpx_supported)(void);
 	bool (*xsaves_supported)(void);
 	bool (*umip_emulated)(void);

commit 736c291c9f36b07f8889c61764c28edce20e715d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 6 15:57:14 2019 -0800

    KVM: x86: Use gpa_t for cr2/gpa to fix TDP support on 32-bit KVM
    
    Convert a plethora of parameters and variables in the MMU and page fault
    flows from type gva_t to gpa_t to properly handle TDP on 32-bit KVM.
    
    Thanks to PSE and PAE paging, 32-bit kernels can access 64-bit physical
    addresses.  When TDP is enabled, the fault address is a guest physical
    address and thus can be a 64-bit value, even when both KVM and its guest
    are using 32-bit virtual addressing, e.g. VMX's VMCS.GUEST_PHYSICAL is a
    64-bit field, not a natural width field.
    
    Using a gva_t for the fault address means KVM will incorrectly drop the
    upper 32-bits of the GPA.  Ditto for gva_to_gpa() when it is used to
    translate L2 GPAs to L1 GPAs.
    
    Opportunistically rename variables and parameters to better reflect the
    dual address modes, e.g. use "cr2_or_gpa" for fault addresses and plain
    "addr" instead of "vaddr" when the address may be either a GVA or an L2
    GPA.  Similarly, use "gpa" in the nonpaging_page_fault() flows to avoid
    a confusing "gpa_t gva" declaration; this also sets the stage for a
    future patch to combing nonpaging_page_fault() and tdp_page_fault() with
    minimal churn.
    
    Sprinkle in a few comments to document flows where an address is known
    to be a GVA and thus can be safely truncated to a 32-bit value.  Add
    WARNs in kvm_handle_page_fault() and FNAME(gva_to_gpa_nested)() to help
    document such cases and detect bugs.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2893eae5df9f..159a28512e4c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -378,12 +378,12 @@ struct kvm_mmu {
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
-	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err,
+	int (*page_fault)(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u32 err,
 			  bool prefault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
-	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
-			    struct x86_exception *exception);
+	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gpa_t gva_or_gpa,
+			    u32 access, struct x86_exception *exception);
 	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			       struct x86_exception *exception);
 	int (*sync_page)(struct kvm_vcpu *vcpu,
@@ -1473,7 +1473,7 @@ void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 
-int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u64 error_code,
+int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);

commit c96001c5702e66b64e0ffe533aa19d6567ce15bc
Author: Peter Xu <peterx@redhat.com>
Date:   Wed Dec 4 20:07:18 2019 +0100

    KVM: X86: Use APIC_DEST_* macros properly in kvm_lapic_irq.dest_mode
    
    We were using either APIC_DEST_PHYSICAL|APIC_DEST_LOGICAL or 0|1 to
    fill in kvm_lapic_irq.dest_mode.  It's fine only because in most cases
    when we check against dest_mode it's against APIC_DEST_PHYSICAL (which
    equals to 0).  However, that's not consistent.  We'll have problem
    when we want to start checking against APIC_DEST_LOGICAL, which does
    not equals to 1.
    
    This patch firstly introduces kvm_lapic_irq_dest_mode() helper to take
    any boolean of destination mode and return the APIC_DEST_* macro.
    Then, it replaces the 0|1 settings of irq.dest_mode with the helper.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b79cd6aa4075..2893eae5df9f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1022,6 +1022,11 @@ struct kvm_lapic_irq {
 	bool msi_redir_hint;
 };
 
+static inline u16 kvm_lapic_irq_dest_mode(bool dest_mode_logical)
+{
+	return dest_mode_logical ? APIC_DEST_LOGICAL : APIC_DEST_PHYSICAL;
+}
+
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */

commit 46f4f0aabc61bfd365e1eb3c8a6d766d1a49cf32
Merge: 14edff88315a b07a5c53d42a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 21 10:01:51 2019 +0100

    Merge branch 'kvm-tsx-ctrl' into HEAD
    
    Conflicts:
            arch/x86/kvm/vmx/vmx.c

commit edef5c36b0c7f07ab4926f6c9e50731f3772c79d
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 18 12:23:00 2019 -0500

    KVM: x86: implement MSR_IA32_TSX_CTRL effect on CPUID
    
    Because KVM always emulates CPUID, the CPUID clear bit
    (bit 1) of MSR_IA32_TSX_CTRL must be emulated "manually"
    by the hypervisor when performing said emulation.
    
    Right now neither kvm-intel.ko nor kvm-amd.ko implement
    MSR_IA32_TSX_CTRL but this will change in the next patch.
    
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Tested-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4fc61483919a..663d09ac7778 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1357,6 +1357,7 @@ int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 
 void kvm_enable_efer_bits(u64);
 bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);
+int __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data, bool host_initiated);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data);
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data);
 int kvm_emulate_rdmsr(struct kvm_vcpu *vcpu);

commit 7ee30bc132c683d06a6d9e360e39e483e3990708
Author: Nitesh Narayan Lal <nitesh@redhat.com>
Date:   Thu Nov 7 07:53:43 2019 -0500

    KVM: x86: deliver KVM IOAPIC scan request to target vCPUs
    
    In IOAPIC fixed delivery mode instead of flushing the scan
    requests to all vCPUs, we should only send the requests to
    vCPUs specified within the destination field.
    
    This patch introduces kvm_get_dest_vcpus_mask() API which
    retrieves an array of target vCPUs by using
    kvm_apic_map_get_dest_lapic() and then based on the
    vcpus_idx, it sets the bit in a bitmap. However, if the above
    fails kvm_get_dest_vcpus_mask() finds the target vCPUs by
    traversing all available vCPUs. Followed by setting the
    bits in the bitmap.
    
    If we had different vCPUs in the previous request for the
    same redirection table entry then bits corresponding to
    these vCPUs are also set. This to done to keep
    ioapic_handled_vectors synchronized.
    
    This bitmap is then eventually passed on to
    kvm_make_vcpus_request_mask() to generate a masked request
    only for the target vCPUs.
    
    This would enable us to reduce the latency overhead on isolated
    vCPUs caused by the IPI to process due to KVM_REQ_IOAPIC_SCAN.
    
    Suggested-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Nitesh Narayan Lal <nitesh@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 20bb2fc0883a..898ab9eb4dc8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1588,6 +1588,8 @@ bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
 
 void kvm_make_mclock_inprogress_request(struct kvm *kvm);
 void kvm_make_scan_ioapic_request(struct kvm *kvm);
+void kvm_make_scan_ioapic_request_mask(struct kvm *kvm,
+				       unsigned long *vcpu_bitmap);
 
 void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work);

commit b35e5548b41131eb06de041af2f5fb0890d96f96
Author: Like Xu <like.xu@linux.intel.com>
Date:   Sun Oct 27 18:52:43 2019 +0800

    KVM: x86/vPMU: Add lazy mechanism to release perf_event per vPMC
    
    Currently, a host perf_event is created for a vPMC functionality emulation.
    It’s unpredictable to determine if a disabled perf_event will be reused.
    If they are disabled and are not reused for a considerable period of time,
    those obsolete perf_events would increase host context switch overhead that
    could have been avoided.
    
    If the guest doesn't WRMSR any of the vPMC's MSRs during an entire vcpu
    sched time slice, and its independent enable bit of the vPMC isn't set,
    we can predict that the guest has finished the use of this vPMC, and then
    do request KVM_REQ_PMU in kvm_arch_sched_in and release those perf_events
    in the first call of kvm_pmu_handle_event() after the vcpu is scheduled in.
    
    This lazy mechanism delays the event release time to the beginning of the
    next scheduled time slice if vPMC's MSRs aren't changed during this time
    slice. If guest comes back to use this vPMC in next time slice, a new perf
    event would be re-created via perf_event_create_kernel_counter() as usual.
    
    Suggested-by: Wei Wang <wei.w.wang@intel.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Like Xu <like.xu@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a87a6c98adee..20bb2fc0883a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -473,6 +473,20 @@ struct kvm_pmu {
 	struct kvm_pmc fixed_counters[INTEL_PMC_MAX_FIXED];
 	struct irq_work irq_work;
 	DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
+	DECLARE_BITMAP(pmc_in_use, X86_PMC_IDX_MAX);
+
+	/*
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
+	bool need_cleanup;
+
+	/*
+	 * The total number of programmed perf_events and it helps to avoid
+	 * redundant check before cleanup if guest don't use vPMU at all.
+	 */
+	u8 event_count;
 };
 
 struct kvm_pmu_ops;

commit a6da0d77e98e94fa66187a5ce3cf7e11fbf95503
Author: Like Xu <like.xu@linux.intel.com>
Date:   Sun Oct 27 18:52:42 2019 +0800

    KVM: x86/vPMU: Reuse perf_event to avoid unnecessary pmc_reprogram_counter
    
    The perf_event_create_kernel_counter() in the pmc_reprogram_counter() is
    a heavyweight and high-frequency operation, especially when host disables
    the watchdog (maximum 21000000 ns) which leads to an unacceptable latency
    of the guest NMI handler. It limits the use of vPMUs in the guest.
    
    When a vPMC is fully enabled, the legacy reprogram_*_counter() would stop
    and release its existing perf_event (if any) every time EVEN in most cases
    almost the same requested perf_event will be created and configured again.
    
    For each vPMC, if the reuqested config ('u64 eventsel' for gp and 'u8 ctrl'
    for fixed) is the same as its current config AND a new sample period based
    on pmc->counter is accepted by host perf interface, the current event could
    be reused safely as a new created one does. Otherwise, do release the
    undesirable perf_event and reprogram a new one as usual.
    
    It's light-weight to call pmc_pause_counter (disable, read and reset event)
    and pmc_resume_counter (recalibrate period and re-enable event) as guest
    expects instead of release-and-create again on any condition. Compared to
    use the filterable event->attr or hw.config, a new 'u64 current_config'
    field is added to save the last original programed config for each vPMC.
    
    Based on this implementation, the number of calls to pmc_reprogram_counter
    is reduced by ~82.5% for a gp sampling event and ~99.9% for a fixed event.
    In the usage of multiplexing perf sampling mode, the average latency of the
    guest NMI handler is reduced from 104923 ns to 48393 ns (~2.16x speed up).
    If host disables watchdog, the minimum latecy of guest NMI handler could be
    speed up at ~3413x (from 20407603 to 5979 ns) and at ~786x in the average.
    
    Suggested-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Like Xu <like.xu@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6f6b8886a8eb..a87a6c98adee 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -449,6 +449,11 @@ struct kvm_pmc {
 	u64 eventsel;
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * eventsel value for general purpose counters,
+	 * ctrl value for fixed counters.
+	 */
+	u64 current_config;
 };
 
 struct kvm_pmu {

commit 1aa9b9572b10529c2e64e2b8f44025d86e124308
Author: Junaid Shahid <junaids@google.com>
Date:   Mon Nov 4 20:26:00 2019 +0100

    kvm: x86: mmu: Recovery of shattered NX large pages
    
    The page table pages corresponding to broken down large pages are zapped in
    FIFO order, so that the large page can potentially be recovered, if it is
    not longer being used for execution.  This removes the performance penalty
    for walking deeper EPT page tables.
    
    By default, one large page will last about one hour once the guest
    reaches a steady state.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a37b03483b66..4fc61483919a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -312,6 +312,8 @@ struct kvm_rmap_head {
 struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
+	struct list_head lpage_disallowed_link;
+
 	bool unsync;
 	u8 mmu_valid_gen;
 	bool mmio_cached;
@@ -860,6 +862,7 @@ struct kvm_arch {
 	 */
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
+	struct list_head lpage_disallowed_mmu_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
 
@@ -934,6 +937,7 @@ struct kvm_arch {
 	bool exception_payload_enabled;
 
 	struct kvm_pmu_event_filter *pmu_event_filter;
+	struct task_struct *nx_lpage_recovery_thread;
 };
 
 struct kvm_vm_stat {

commit b8e8c8303ff28c61046a4d0f6ea99aea609a7dc0
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 4 12:22:02 2019 +0100

    kvm: mmu: ITLB_MULTIHIT mitigation
    
    With some Intel processors, putting the same virtual address in the TLB
    as both a 4 KiB and 2 MiB page can confuse the instruction fetch unit
    and cause the processor to issue a machine check resulting in a CPU lockup.
    
    Unfortunately when EPT page tables use huge pages, it is possible for a
    malicious guest to cause this situation.
    
    Add a knob to mark huge pages as non-executable. When the nx_huge_pages
    parameter is enabled (and we are using EPT), all huge pages are marked as
    NX. If the guest attempts to execute in one of those pages, the page is
    broken down into 4K pages, which are then marked executable.
    
    This is not an issue for shadow paging (except nested EPT), because then
    the host is in control of TLB flushes and the problematic situation cannot
    happen.  With nested EPT, again the nested guest can cause problems shadow
    and direct EPT is treated in the same way.
    
    [ tglx: Fixup default to auto and massage wording a bit ]
    
    Originally-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 24d6598dea29..a37b03483b66 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -315,6 +315,7 @@ struct kvm_mmu_page {
 	bool unsync;
 	u8 mmu_valid_gen;
 	bool mmio_cached;
+	bool lpage_disallowed; /* Can't be replaced by an equiv large page */
 
 	/*
 	 * The following two entries are used to key the shadow page in the
@@ -946,6 +947,7 @@ struct kvm_vm_stat {
 	ulong mmu_unsync;
 	ulong remote_tlb_flush;
 	ulong lpages;
+	ulong nx_lpage_splits;
 	ulong max_mmu_page_hash_collisions;
 };
 

commit 671ddc700fd08b94967b1e2a937020e30c838609
Author: Jim Mattson <jmattson@google.com>
Date:   Tue Oct 15 10:44:05 2019 -0700

    KVM: nVMX: Don't leak L1 MMIO regions to L2
    
    If the "virtualize APIC accesses" VM-execution control is set in the
    VMCS, the APIC virtualization hardware is triggered when a page walk
    in VMX non-root mode terminates at a PTE wherein the address of the 4k
    page frame matches the APIC-access address specified in the VMCS. On
    hardware, the APIC-access address may be any valid 4k-aligned physical
    address.
    
    KVM's nVMX implementation enforces the additional constraint that the
    APIC-access address specified in the vmcs12 must be backed by
    a "struct page" in L1. If not, L0 will simply clear the "virtualize
    APIC accesses" VM-execution control in the vmcs02.
    
    The problem with this approach is that the L1 guest has arranged the
    vmcs12 EPT tables--or shadow page tables, if the "enable EPT"
    VM-execution control is clear in the vmcs12--so that the L2 guest
    physical address(es)--or L2 guest linear address(es)--that reference
    the L2 APIC map to the APIC-access address specified in the
    vmcs12. Without the "virtualize APIC accesses" VM-execution control in
    the vmcs02, the APIC accesses in the L2 guest will directly access the
    APIC-access page in L1.
    
    When there is no mapping whatsoever for the APIC-access address in L1,
    the L2 VM just loses the intended APIC virtualization. However, when
    the APIC-access address is mapped to an MMIO region in L1, the L2
    guest gets direct access to the L1 MMIO device. For example, if the
    APIC-access address specified in the vmcs12 is 0xfee00000, then L2
    gets direct access to L1's APIC.
    
    Since this vmcs12 configuration is something that KVM cannot
    faithfully emulate, the appropriate response is to exit to userspace
    with KVM_INTERNAL_ERROR_EMULATION.
    
    Fixes: fe3ef05c7572 ("KVM: nVMX: Prepare vmcs02 from vmcs01 and vmcs12")
    Reported-by: Dan Cross <dcross@google.com>
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Reviewed-by: Peter Shier <pshier@google.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 50eb430b0ad8..24d6598dea29 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1189,7 +1189,7 @@ struct kvm_x86_ops {
 	int (*set_nested_state)(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				struct kvm_nested_state *kvm_state);
-	void (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
+	bool (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
 
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);

commit 7204160eb7809345d10c983d9d1dfbd98060a56d
Author: Aaron Lewis <aaronlewis@google.com>
Date:   Mon Oct 21 16:30:20 2019 -0700

    KVM: x86: Introduce vcpu->arch.xsaves_enabled
    
    Cache whether XSAVES is enabled in the guest by adding xsaves_enabled to
    vcpu->arch.
    
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Aaron Lewis <aaronlewis@google.com>
    Change-Id: If4638e0901c28a4494dad2e103e2c075e8ab5d68
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 62f32a61c250..6f6b8886a8eb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -560,6 +560,7 @@ struct kvm_vcpu_arch {
 	u64 smbase;
 	u64 smi_count;
 	bool tpr_access_reporting;
+	bool xsaves_enabled;
 	u64 ia32_xss;
 	u64 microcode_version;
 	u64 arch_capabilities;

commit 4be946728f65c10c9bb1a1580ec47a316f5ee6ac
Author: Like Xu <like.xu@linux.intel.com>
Date:   Mon Oct 21 18:55:04 2019 +0800

    KVM: x86/vPMU: Declare kvm_pmu->reprogram_pmi field using DECLARE_BITMAP
    
    Replace the explicit declaration of "u64 reprogram_pmi" with the generic
    macro DECLARE_BITMAP for all possible appropriate number of bits.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Like Xu <like.xu@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5d8056ff7390..62f32a61c250 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -467,7 +467,7 @@ struct kvm_pmu {
 	struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
 	struct kvm_pmc fixed_counters[INTEL_PMC_MAX_FIXED];
 	struct irq_work irq_work;
-	u64 reprogram_pmi;
+	DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
 };
 
 struct kvm_pmu_ops;

commit 2cf9af0b566823de418eb2ff357a2f8233c718e9
Author: Suthikulpanit, Suravee <Suravee.Suthikulpanit@amd.com>
Date:   Fri Sep 13 19:00:49 2019 +0000

    kvm: x86: Modify kvm_x86_ops.get_enable_apicv() to use struct kvm parameter
    
    Generally, APICv for all vcpus in the VM are enable/disable in the same
    manner. So, get_enable_apicv() should represent APICv status of the VM
    instead of each VCPU.
    
    Modify kvm_x86_ops.get_enable_apicv() to take struct kvm as parameter
    instead of struct kvm_vcpu.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cdde7488430d..5d8056ff7390 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1081,7 +1081,7 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
-	bool (*get_enable_apicv)(struct kvm_vcpu *vcpu);
+	bool (*get_enable_apicv)(struct kvm *kvm);
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);

commit 34059c2570102870df8d8a31bd42f8d9c19cce87
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Sep 27 14:45:23 2019 -0700

    KVM: x86: Fold decache_cr3() into cache_reg()
    
    Handle caching CR3 (from VMX's VMCS) into struct kvm_vcpu via the common
    cache_reg() callback and drop the dedicated decache_cr3().  The name
    decache_cr3() is somewhat confusing as the caching behavior of CR3
    follows that of GPRs, RFLAGS and PDPTRs, (handled via cache_reg()), and
    has nothing in common with the caching behavior of CR0/CR4 (whose
    decache_cr{0,4}_guest_bits() likely provided the 'decache' verbiage).
    
    This would effectivel adds a BUG() if KVM attempts to cache CR3 on SVM.
    Change it to a WARN_ON_ONCE() -- if the cache never requires filling,
    the value is already in the right place -- and opportunistically add one
    in VMX to provide an equivalent check.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c86c95a499af..cdde7488430d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1033,7 +1033,6 @@ struct kvm_x86_ops {
 			    struct kvm_segment *var, int seg);
 	void (*get_cs_db_l_bits)(struct kvm_vcpu *vcpu, int *db, int *l);
 	void (*decache_cr0_guest_bits)(struct kvm_vcpu *vcpu);
-	void (*decache_cr3)(struct kvm_vcpu *vcpu);
 	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);

commit f8845541e93c5b41618405de6735edd6f0cc8984
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Sep 27 14:45:21 2019 -0700

    KVM: x86: Fold 'enum kvm_ex_reg' definitions into 'enum kvm_reg'
    
    Now that indexing into arch.regs is either protected by WARN_ON_ONCE or
    done with hardcoded enums, combine all definitions for registers that
    are tracked by regs_avail and regs_dirty into 'enum kvm_reg'.  Having a
    single enum type will simplify additional cleanup related to regs_avail
    and regs_dirty.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 50eb430b0ad8..c86c95a499af 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -156,10 +156,8 @@ enum kvm_reg {
 	VCPU_REGS_R15 = __VCPU_REGS_R15,
 #endif
 	VCPU_REGS_RIP,
-	NR_VCPU_REGS
-};
+	NR_VCPU_REGS,
 
-enum kvm_reg_ex {
 	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
 	VCPU_EXREG_CR3,
 	VCPU_EXREG_RFLAGS,

commit 6eeb4ef049e7cd89783e8ebe1ea2f1dac276f82c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 24 12:43:08 2019 +0200

    KVM: x86: assign two bits to track SPTE kinds
    
    Currently, we are overloading SPTE_SPECIAL_MASK to mean both
    "A/D bits unavailable" and MMIO, where the difference between the
    two is determined by mio_mask and mmio_value.
    
    However, the next patch will need two bits to distinguish
    availability of A/D bits from write protection.  So, while at
    it give MMIO its own bit pattern, and move the two bits from
    bit 62 to bits 52..53 since Intel is allocating EPT page table
    bits from the top.
    
    Reviewed-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 23edf56cf577..50eb430b0ad8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -219,13 +219,6 @@ enum {
 				 PFERR_WRITE_MASK |		\
 				 PFERR_PRESENT_MASK)
 
-/*
- * The mask used to denote special SPTEs, which can be either MMIO SPTEs or
- * Access Tracking SPTEs. We use bit 62 instead of bit 63 to avoid conflicting
- * with the SVE bit in EPT PTEs.
- */
-#define SPTE_SPECIAL_MASK (1ULL << 62)
-
 /* apic attention bits */
 #define KVM_APIC_CHECK_VAPIC	0
 /*

commit f209a26dd5a5038c53097b5c38e313b8cd042adb
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jul 19 13:41:10 2019 -0700

    KVM: x86: Don't check kvm_rebooting in __kvm_handle_fault_on_reboot()
    
    Remove the kvm_rebooting check from VMX/SVM instruction exception fixup
    now that kvm_spurious_fault() conditions its BUG() on !kvm_rebooting.
    Because the 'cleanup_insn' functionally is also gone, deferring to
    kvm_spurious_fault() means __kvm_handle_fault_on_reboot() can eliminate
    its .fixup code entirely and have its exception table entry branch
    directly to the call to kvm_spurious_fault().
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 202f4d4a892a..23edf56cf577 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1549,13 +1549,7 @@ asmlinkage void kvm_spurious_fault(void);
 	"667: \n\t"							\
 	"call	kvm_spurious_fault \n\t"				\
 	"668: \n\t"							\
-	".pushsection .fixup, \"ax\" \n\t"				\
-	"700: \n\t"							\
-	"cmpb	$0, kvm_rebooting\n\t"					\
-	"je	667b \n\t"						\
-	"jmp	668b \n\t"						\
-	".popsection \n\t"						\
-	_ASM_EXTABLE(666b, 700b)
+	_ASM_EXTABLE(666b, 667b)
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end);

commit 98cd382d509061a9c6ae4cac38ff0721be05adef
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jul 19 13:41:09 2019 -0700

    KVM: x86: Drop ____kvm_handle_fault_on_reboot()
    
    Remove the variation of __kvm_handle_fault_on_reboot() that accepts a
    post-fault cleanup instruction now that its sole user (VMREAD) uses
    a different method for handling faults.
    
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index de0c9d8097c2..202f4d4a892a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1542,7 +1542,7 @@ asmlinkage void kvm_spurious_fault(void);
  * Usually after catching the fault we just panic; during reboot
  * instead the instruction is ignored.
  */
-#define ____kvm_handle_fault_on_reboot(insn, cleanup_insn)		\
+#define __kvm_handle_fault_on_reboot(insn)				\
 	"666: \n\t"							\
 	insn "\n\t"							\
 	"jmp	668f \n\t"						\
@@ -1551,16 +1551,12 @@ asmlinkage void kvm_spurious_fault(void);
 	"668: \n\t"							\
 	".pushsection .fixup, \"ax\" \n\t"				\
 	"700: \n\t"							\
-	cleanup_insn "\n\t"						\
 	"cmpb	$0, kvm_rebooting\n\t"					\
 	"je	667b \n\t"						\
 	"jmp	668b \n\t"						\
 	".popsection \n\t"						\
 	_ASM_EXTABLE(666b, 700b)
 
-#define __kvm_handle_fault_on_reboot(insn)		\
-	____kvm_handle_fault_on_reboot(insn, "")
-
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);

commit 4b526de50e39b38cd828396267379183c7c21354
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jul 19 13:41:06 2019 -0700

    KVM: x86: Check kvm_rebooting in kvm_spurious_fault()
    
    Explicitly check kvm_rebooting in kvm_spurious_fault() prior to invoking
    BUG(), as opposed to assuming the caller has already done so.  Letting
    kvm_spurious_fault() be called "directly" will allow VMX to better
    optimize its low level assembly flows.
    
    As a happy side effect, kvm_spurious_fault() no longer needs to be
    marked as a dead end since it doesn't unconditionally BUG().
    
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c5ed92482e2c..de0c9d8097c2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1534,7 +1534,7 @@ enum {
 #define kvm_arch_vcpu_memslots_id(vcpu) ((vcpu)->arch.hflags & HF_SMM_MASK ? 1 : 0)
 #define kvm_memslots_for_spte_role(kvm, role) __kvm_memslots(kvm, (role).smm)
 
-asmlinkage void __noreturn kvm_spurious_fault(void);
+asmlinkage void kvm_spurious_fault(void);
 
 /*
  * Hardware virtualization extension instructions may fault if a

commit ca333add6933ad9732ba2c6671f133d7367ad96c
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Sep 12 19:46:11 2019 -0700

    KVM: x86/mmu: Explicitly track only a single invalid mmu generation
    
    Toggle mmu_valid_gen between '0' and '1' instead of blindly incrementing
    the generation.  Because slots_lock is held for the entire duration of
    zapping obsolete pages, it's impossible for there to be multiple invalid
    generations associated with shadow pages at any given time.
    
    Toggling between the two generations (valid vs. invalid) allows changing
    mmu_valid_gen from an unsigned long to a u8, which reduces the size of
    struct kvm_mmu_page from 160 to 152 bytes on 64-bit KVM, i.e. reduces
    KVM's memory footprint by 8 bytes per shadow page.
    
    Set sp->mmu_valid_gen before it is added to active_mmu_pages.
    Functionally this has no effect as kvm_mmu_alloc_page() has a single
    caller that sets sp->mmu_valid_gen soon thereafter, but visually it is
    jarring to see a shadow page being added to the list without its
    mmu_valid_gen first being set.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 53e0b956ed3c..c5ed92482e2c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -320,6 +320,7 @@ struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
 	bool unsync;
+	u8 mmu_valid_gen;
 	bool mmio_cached;
 
 	/*
@@ -335,7 +336,6 @@ struct kvm_mmu_page {
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
-	unsigned long mmu_valid_gen;
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32
@@ -859,7 +859,7 @@ struct kvm_arch {
 	unsigned long n_requested_mmu_pages;
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
-	unsigned long mmu_valid_gen;
+	u8 mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.

commit 31741eb11a43066a3da92996bcfccfb42e248d44
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Sep 12 19:46:09 2019 -0700

    KVM: x86/mmu: Revert "Revert "KVM: MMU: reclaim the zapped-obsolete page first""
    
    Now that the fast invalidate mechanism has been reintroduced, restore
    the performance tweaks for fast invalidation that existed prior to its
    removal.
    
    Paraphrashing the original changelog:
    
      Introduce a per-VM list to track obsolete shadow pages, i.e. pages
      which have been deleted from the mmu cache but haven't yet been freed.
      When page reclaiming is needed, zap/free the deleted pages first.
    
    This reverts commit 52d5dedc79bdcbac2976159a172069618cf31be5.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 78f0a919173e..53e0b956ed3c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -865,6 +865,7 @@ struct kvm_arch {
 	 * Hash table of struct kvm_mmu_page.
 	 */
 	struct list_head active_mmu_pages;
+	struct list_head zapped_obsolete_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
 

commit 41577ab8bd72f8813cfd74cd01da3ae6a643878e
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Aug 27 14:40:40 2019 -0700

    KVM: x86: Add comments to document various emulation types
    
    Document the intended usage of each emulation type as each exists to
    handle an edge case of one kind or another and can be easily
    misinterpreted at first glance.
    
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 26f85a24b5e7..78f0a919173e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1315,6 +1315,36 @@ extern u64  kvm_default_tsc_scaling_ratio;
 
 extern u64 kvm_mce_cap_supported;
 
+/*
+ * EMULTYPE_NO_DECODE - Set when re-emulating an instruction (after completing
+ *			userspace I/O) to indicate that the emulation context
+ *			should be resued as is, i.e. skip initialization of
+ *			emulation context, instruction fetch and decode.
+ *
+ * EMULTYPE_TRAP_UD - Set when emulating an intercepted #UD from hardware.
+ *		      Indicates that only select instructions (tagged with
+ *		      EmulateOnUD) should be emulated (to minimize the emulator
+ *		      attack surface).  See also EMULTYPE_TRAP_UD_FORCED.
+ *
+ * EMULTYPE_SKIP - Set when emulating solely to skip an instruction, i.e. to
+ *		   decode the instruction length.  For use *only* by
+ *		   kvm_x86_ops->skip_emulated_instruction() implementations.
+ *
+ * EMULTYPE_ALLOW_RETRY - Set when the emulator should resume the guest to
+ *			  retry native execution under certain conditions.
+ *
+ * EMULTYPE_TRAP_UD_FORCED - Set when emulating an intercepted #UD that was
+ *			     triggered by KVM's magic "force emulation" prefix,
+ *			     which is opt in via module param (off by default).
+ *			     Bypasses EmulateOnUD restriction despite emulating
+ *			     due to an intercepted #UD (see EMULTYPE_TRAP_UD).
+ *			     Used to test the full emulator from userspace.
+ *
+ * EMULTYPE_VMWARE_GP - Set when emulating an intercepted #GP for VMware
+ *			backdoor emulation, which is opt in via module param.
+ *			VMware backoor emulation handles select instructions
+ *			and reinjects the #GP for all other cases.
+ */
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)

commit 60fc3d02d5b8829b91b7b443ef6c7e8f0bbae868
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Aug 27 14:40:38 2019 -0700

    KVM: x86: Remove emulation_result enums, EMULATE_{DONE,FAIL,USER_EXIT}
    
    Deferring emulation failure handling (in some cases) to the caller of
    x86_emulate_instruction() has proven fragile, e.g. multiple instances of
    KVM not setting run->exit_reason on EMULATE_FAIL, largely due to it
    being difficult to discern what emulation types can return what result,
    and which combination of types and results are handled where.
    
    Now that x86_emulate_instruction() always handles emulation failure,
    i.e. EMULATION_FAIL is only referenced in callers, remove the
    emulation_result enums entirely.  Per KVM's existing exit handling
    conventions, return '0' and '1' for "exit to userspace" and "resume
    guest" respectively.  Doing so cleans up many callers, e.g. they can
    return kvm_emulate_instruction() directly instead of having to interpret
    its result.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f3c623dbd5ab..26f85a24b5e7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1315,12 +1315,6 @@ extern u64  kvm_default_tsc_scaling_ratio;
 
 extern u64 kvm_mce_cap_supported;
 
-enum emulation_result {
-	EMULATE_DONE,         /* no further processing */
-	EMULATE_USER_EXIT,    /* kvm_run ready for userspace exit */
-	EMULATE_FAIL,         /* can't emulate this instruction */
-};
-
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)

commit b4000606205959e6cfe1fd3a71c490267ff23506
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Aug 27 14:40:32 2019 -0700

    KVM: x86: Add explicit flag for forced emulation on #UD
    
    Add an explicit emulation type for forced #UD emulation and use it to
    detect that KVM should unconditionally inject a #UD instead of falling
    into its standard emulation failure handling.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 80071d02b87d..f3c623dbd5ab 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1325,6 +1325,7 @@ enum emulation_result {
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_ALLOW_RETRY	    (1 << 3)
+#define EMULTYPE_TRAP_UD_FORCED	    (1 << 4)
 #define EMULTYPE_VMWARE_GP	    (1 << 5)
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type);
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,

commit 42cbf06872cc44fb8d2d6665fa6494b5925eaf1c
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Aug 27 14:40:31 2019 -0700

    KVM: x86: Move #GP injection for VMware into x86_emulate_instruction()
    
    Immediately inject a #GP when VMware emulation fails and return
    EMULATE_DONE instead of propagating EMULATE_FAIL up the stack.  This
    helps pave the way for removing EMULATE_FAIL altogether.
    
    Rename EMULTYPE_VMWARE to EMULTYPE_VMWARE_GP to document that the x86
    emulator is called to handle VMware #GP interception, e.g. why a #GP
    is injected on emulation failure for EMULTYPE_VMWARE_GP.
    
    Drop EMULTYPE_NO_UD_ON_FAIL as a standalone type.  The "no #UD on fail"
    is used only in the VMWare case and is obsoleted by having the emulator
    itself reinject #GP.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 13b35f94dec4..80071d02b87d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1325,8 +1325,7 @@ enum emulation_result {
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_ALLOW_RETRY	    (1 << 3)
-#define EMULTYPE_NO_UD_ON_FAIL	    (1 << 4)
-#define EMULTYPE_VMWARE		    (1 << 5)
+#define EMULTYPE_VMWARE_GP	    (1 << 5)
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type);
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 					void *insn, int insn_len);

commit 6f6a657c99980485c265363447b269083dd1dc3a
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Aug 22 22:30:21 2019 +0800

    KVM/Hyper-V/VMX: Add direct tlb flush support
    
    Hyper-V provides direct tlb flush function which helps
    L1 Hypervisor to handle Hyper-V tlb flush request from
    L2 guest. Add the function support for VMX.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4765ae0fc506..13b35f94dec4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -844,6 +844,8 @@ struct kvm_hv {
 
 	/* How many vCPUs have VP index != vCPU index */
 	atomic_t num_mismatched_vp_indexes;
+
+	struct hv_partition_assist_pg *hv_pa_pg;
 };
 
 enum kvm_irqchip_mode {

commit 344c6c804703841d2bff4d68d7390ba726053874
Author: Tianyu Lan <Tianyu.Lan@microsoft.com>
Date:   Thu Aug 22 22:30:20 2019 +0800

    KVM/Hyper-V: Add new KVM capability KVM_CAP_HYPERV_DIRECT_TLBFLUSH
    
    Hyper-V direct tlb flush function should be enabled for
    guest that only uses Hyper-V hypercall. User space
    hypervisor(e.g, Qemu) can disable KVM identification in
    CPUID and just exposes Hyper-V identification to make
    sure the precondition. Add new KVM capability KVM_CAP_
    HYPERV_DIRECT_TLBFLUSH for user space to enable Hyper-V
    direct tlb function and this function is default to be
    disabled in KVM.
    
    Signed-off-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a3a3ec73fa2f..4765ae0fc506 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1213,6 +1213,7 @@ struct kvm_x86_ops {
 	bool (*need_emulation_on_page_fault)(struct kvm_vcpu *vcpu);
 
 	bool (*apic_init_signal_blocked)(struct kvm_vcpu *vcpu);
+	int (*enable_direct_tlbflush)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit fe38bd6862074c0a2b9be7f31f043aaa70b2af5f
Merge: 404e634fdb96 fb3925d06c28
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 18 09:49:13 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "s390:
       - ioctl hardening
       - selftests
    
      ARM:
       - ITS translation cache
       - support for 512 vCPUs
       - various cleanups and bugfixes
    
      PPC:
       - various minor fixes and preparation
    
      x86:
       - bugfixes all over the place (posted interrupts, SVM, emulation
         corner cases, blocked INIT)
       - some IPI optimizations"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (75 commits)
      KVM: X86: Use IPI shorthands in kvm guest when support
      KVM: x86: Fix INIT signal handling in various CPU states
      KVM: VMX: Introduce exit reason for receiving INIT signal on guest-mode
      KVM: VMX: Stop the preemption timer during vCPU reset
      KVM: LAPIC: Micro optimize IPI latency
      kvm: Nested KVM MMUs need PAE root too
      KVM: x86: set ctxt->have_exception in x86_decode_insn()
      KVM: x86: always stop emulation on page fault
      KVM: nVMX: trace nested VM-Enter failures detected by H/W
      KVM: nVMX: add tracepoint for failed nested VM-Enter
      x86: KVM: svm: Fix a check in nested_svm_vmrun()
      KVM: x86: Return to userspace with internal error on unexpected exit reason
      KVM: x86: Add kvm_emulate_{rd,wr}msr() to consolidate VXM/SVM code
      KVM: x86: Refactor up kvm_{g,s}et_msr() to simplify callers
      doc: kvm: Fix return description of KVM_SET_MSRS
      KVM: X86: Tune PLE Window tracepoint
      KVM: VMX: Change ple_window type to unsigned int
      KVM: X86: Remove tailing newline for tracepoints
      KVM: X86: Trace vcpu_id for vmexit
      KVM: x86: Manually calculate reserved bits when loading PDPTRS
      ...

commit 002c5f73c508f7df5681bda339831c27f3c1aef4
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Sep 12 19:46:02 2019 -0700

    KVM: x86/mmu: Reintroduce fast invalidate/zap for flushing memslot
    
    James Harvey reported a livelock that was introduced by commit
    d012a06ab1d23 ("Revert "KVM: x86/mmu: Zap only the relevant pages when
    removing a memslot"").
    
    The livelock occurs because kvm_mmu_zap_all() as it exists today will
    voluntarily reschedule and drop KVM's mmu_lock, which allows other vCPUs
    to add shadow pages.  With enough vCPUs, kvm_mmu_zap_all() can get stuck
    in an infinite loop as it can never zap all pages before observing lock
    contention or the need to reschedule.  The equivalent of kvm_mmu_zap_all()
    that was in use at the time of the reverted commit (4e103134b8623, "KVM:
    x86/mmu: Zap only the relevant pages when removing a memslot") employed
    a fast invalidate mechanism and was not susceptible to the above livelock.
    
    There are three ways to fix the livelock:
    
    - Reverting the revert (commit d012a06ab1d23) is not a viable option as
      the revert is needed to fix a regression that occurs when the guest has
      one or more assigned devices.  It's unlikely we'll root cause the device
      assignment regression soon enough to fix the regression timely.
    
    - Remove the conditional reschedule from kvm_mmu_zap_all().  However, although
      removing the reschedule would be a smaller code change, it's less safe
      in the sense that the resulting kvm_mmu_zap_all() hasn't been used in
      the wild for flushing memslots since the fast invalidate mechanism was
      introduced by commit 6ca18b6950f8d ("KVM: x86: use the fast way to
      invalidate all pages"), back in 2013.
    
    - Reintroduce the fast invalidate mechanism and use it when zapping shadow
      pages in response to a memslot being deleted/moved, which is what this
      patch does.
    
    For all intents and purposes, this is a revert of commit ea145aacf4ae8
    ("Revert "KVM: MMU: fast invalidate all pages"") and a partial revert of
    commit 7390de1e99a70 ("Revert "KVM: x86: use the fast way to invalidate
    all pages""), i.e. restores the behavior of commit 5304b8d37c2a5 ("KVM:
    MMU: fast invalidate all pages") and commit 6ca18b6950f8d ("KVM: x86:
    use the fast way to invalidate all pages") respectively.
    
    Fixes: d012a06ab1d23 ("Revert "KVM: x86/mmu: Zap only the relevant pages when removing a memslot"")
    Reported-by: James Harvey <jamespharvey20@gmail.com>
    Cc: Alex Willamson <alex.williamson@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 74e88e5edd9c..bdc16b0aa7c6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -335,6 +335,7 @@ struct kvm_mmu_page {
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
+	unsigned long mmu_valid_gen;
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32
@@ -856,6 +857,7 @@ struct kvm_arch {
 	unsigned long n_requested_mmu_pages;
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
+	unsigned long mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.

commit 4b9852f4f38909a9ca74e71afb35aafba0871aa1
Author: Liran Alon <liran.alon@oracle.com>
Date:   Mon Aug 26 13:24:49 2019 +0300

    KVM: x86: Fix INIT signal handling in various CPU states
    
    Commit cd7764fe9f73 ("KVM: x86: latch INITs while in system management mode")
    changed code to latch INIT while vCPU is in SMM and process latched INIT
    when leaving SMM. It left a subtle remark in commit message that similar
    treatment should also be done while vCPU is in VMX non-root-mode.
    
    However, INIT signals should actually be latched in various vCPU states:
    (*) For both Intel and AMD, INIT signals should be latched while vCPU
    is in SMM.
    (*) For Intel, INIT should also be latched while vCPU is in VMX
    operation and later processed when vCPU leaves VMX operation by
    executing VMXOFF.
    (*) For AMD, INIT should also be latched while vCPU runs with GIF=0
    or in guest-mode with intercept defined on INIT signal.
    
    To fix this:
    1) Add kvm_x86_ops->apic_init_signal_blocked() such that each CPU vendor
    can define the various CPU states in which INIT signals should be
    blocked and modify kvm_apic_accept_events() to use it.
    2) Modify vmx_check_nested_events() to check for pending INIT signal
    while vCPU in guest-mode. If so, emualte vmexit on
    EXIT_REASON_INIT_SIGNAL. Note that nSVM should have similar behaviour
    but is currently left as a TODO comment to implement in the future
    because nSVM don't yet implement svm_check_nested_events().
    
    Note: Currently KVM nVMX implementation don't support VMX wait-for-SIPI
    activity state as specified in MSR_IA32_VMX_MISC bits 6:8 exposed to
    guest (See nested_vmx_setup_ctls_msrs()).
    If and when support for this activity state will be implemented,
    kvm_check_nested_events() would need to avoid emulating vmexit on
    INIT signal in case activity-state is wait-for-SIPI. In addition,
    kvm_apic_accept_events() would need to be modified to avoid discarding
    SIPI in case VMX activity-state is wait-for-SIPI but instead delay
    SIPI processing to vmx_check_nested_events() that would clear
    pending APIC events and emulate vmexit on SIPI.
    
    Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
    Co-developed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Signed-off-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c4f271a9b306..b523949a8df8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1209,6 +1209,8 @@ struct kvm_x86_ops {
 	uint16_t (*nested_get_evmcs_version)(struct kvm_vcpu *vcpu);
 
 	bool (*need_emulation_on_page_fault)(struct kvm_vcpu *vcpu);
+
+	bool (*apic_init_signal_blocked)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit 1edce0a9eb239b7a2768d10cf7d21dd83e269d5b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Sep 5 14:22:55 2019 -0700

    KVM: x86: Add kvm_emulate_{rd,wr}msr() to consolidate VXM/SVM code
    
    Move RDMSR and WRMSR emulation into common x86 code to consolidate
    nearly identical SVM and VMX code.
    
    Note, consolidating RDMSR introduces an extra indirect call, i.e.
    retpoline, due to reaching {svm,vmx}_get_msr() via kvm_x86_ops, but a
    guest kernel likely has bigger problems if increasing the latency of
    RDMSR VM-Exits by ~70 cycles has a measurable impact on overall VM
    performance.  E.g. the only recurring RDMSR VM-Exits (after booting) on
    my system running Linux 5.2 in the guest are for MSR_IA32_TSC_ADJUST via
    arch_cpu_idle_enter().
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 059c29cb0714..c4f271a9b306 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1328,6 +1328,8 @@ void kvm_enable_efer_bits(u64);
 bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data);
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data);
+int kvm_emulate_rdmsr(struct kvm_vcpu *vcpu);
+int kvm_emulate_wrmsr(struct kvm_vcpu *vcpu);
 
 struct x86_emulate_ctxt;
 

commit f20935d85a23c33f5361661747ffa975549f56d5
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Sep 5 14:22:54 2019 -0700

    KVM: x86: Refactor up kvm_{g,s}et_msr() to simplify callers
    
    Refactor the top-level MSR accessors to take/return the index and value
    directly instead of requiring the caller to dump them into a msr_data
    struct.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5b14aa1fbeeb..059c29cb0714 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1326,8 +1326,8 @@ int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 
 void kvm_enable_efer_bits(u64);
 bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);
-int kvm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
-int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
+int kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data);
+int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data);
 
 struct x86_emulate_ctxt;
 

commit fdcf756213756c23b533ca4974d1f48c6a4d4281
Author: Alexander Graf <graf@amazon.com>
Date:   Thu Sep 5 14:58:18 2019 +0200

    KVM: x86: Disable posted interrupts for non-standard IRQs delivery modes
    
    We can easily route hardware interrupts directly into VM context when
    they target the "Fixed" or "LowPriority" delivery modes.
    
    However, on modes such as "SMI" or "Init", we need to go via KVM code
    to actually put the vCPU into a different mode of operation, so we can
    not post the interrupt
    
    Add code in the VMX and SVM PI logic to explicitly refuse to establish
    posted mappings for advanced IRQ deliver modes. This reflects the logic
    in __apic_accept_irq() which also only ever passes Fixed and LowPriority
    interrupts as posted interrupts into the guest.
    
    This fixes a bug I have with code which configures real hardware to
    inject virtual SMIs into my guest.
    
    Signed-off-by: Alexander Graf <graf@amazon.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Wanpeng Li <wanpengli@tencent.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 44a5ce57a905..5b14aa1fbeeb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1581,6 +1581,13 @@ bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq);
 
+static inline bool kvm_irq_is_postable(struct kvm_lapic_irq *irq)
+{
+	/* We can only post Fixed and LowPrio IRQs */
+	return (irq->delivery_mode == dest_Fixed ||
+		irq->delivery_mode == dest_LowestPrio);
+}
+
 static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)
 {
 	if (kvm_x86_ops->vcpu_blocking)

commit 871bd0346018df53055141f09754cb5ffb334c7b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 1 13:35:21 2019 -0700

    KVM: x86: Rename access permissions cache member in struct kvm_vcpu_arch
    
    Rename "access" to "mmio_access" to match the other MMIO cache members
    and to make it more obvious that it's tracking the access permissions
    for the MMIO cache.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 707ae7ff8e1e..44a5ce57a905 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -717,7 +717,7 @@ struct kvm_vcpu_arch {
 
 	/* Cache MMIO info */
 	u64 mmio_gva;
-	unsigned access;
+	unsigned mmio_access;
 	gfn_t mmio_gfn;
 	u64 mmio_gen;
 

commit f8ea7c6049d5d0766b84c8107583ed0094773d06
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Aug 13 15:53:30 2019 +0200

    x86: kvm: svm: propagate errors from skip_emulated_instruction()
    
    On AMD, kvm_x86_ops->skip_emulated_instruction(vcpu) can, in theory,
    fail: in !nrips case we call kvm_emulate_instruction(EMULTYPE_SKIP).
    Currently, we only do printk(KERN_DEBUG) when this happens and this
    is not ideal. Propagate the error up the stack.
    
    On VMX, skip_emulated_instruction() doesn't fail, we have two call
    sites calling it explicitly: handle_exception_nmi() and
    handle_task_switch(), we can just ignore the result.
    
    On SVM, we also have two explicit call sites:
    svm_queue_exception() and it seems we don't need to do anything there as
    we check if RIP was advanced or not. In task_switch_interception(),
    however, we are better off not proceeding to kvm_task_switch() in case
    skip_emulated_instruction() failed.
    
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 74e88e5edd9c..707ae7ff8e1e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1070,7 +1070,7 @@ struct kvm_x86_ops {
 
 	void (*run)(struct kvm_vcpu *vcpu);
 	int (*handle_exit)(struct kvm_vcpu *vcpu);
-	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
+	int (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
 	void (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
 	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu);
 	void (*patch_hypercall)(struct kvm_vcpu *vcpu,

commit 0e1c438c44dd9cde56effb44c5f1cfeda72e108d
Merge: c096397c78f7 cdb2d3ee0436
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Aug 9 16:53:39 2019 +0200

    Merge tag 'kvmarm-fixes-for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm fixes for 5.3
    
    - A bunch of switch/case fall-through annotation, fixing one actual bug
    - Fix PMU reset bug
    - Add missing exception class debug strings

commit 741cbbae0768b828be2d48331eb371a4f08bbea8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Aug 3 08:14:25 2019 +0200

    KVM: remove kvm_arch_has_vcpu_debugfs()
    
    There is no need for this function as all arches have to implement
    kvm_arch_create_vcpu_debugfs() no matter what.  A #define symbol
    let us actually simplify the code.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fc046ca89d32..e92725b2a46f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -35,6 +35,8 @@
 #include <asm/kvm_vcpu_regs.h>
 #include <asm/hyperv-tlfs.h>
 
+#define __KVM_HAVE_ARCH_VCPU_DEBUGFS
+
 #define KVM_MAX_VCPUS 288
 #define KVM_SOFT_MAX_VCPUS 240
 #define KVM_MAX_VCPU_ID 1023

commit 17e433b54393a6269acbcb792da97791fe1592d8
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Aug 5 10:03:19 2019 +0800

    KVM: Fix leak vCPU's VMCS value into other pCPU
    
    After commit d73eb57b80b (KVM: Boost vCPUs that are delivering interrupts), a
    five years old bug is exposed. Running ebizzy benchmark in three 80 vCPUs VMs
    on one 80 pCPUs Skylake server, a lot of rcu_sched stall warning splatting
    in the VMs after stress testing:
    
     INFO: rcu_sched detected stalls on CPUs/tasks: { 4 41 57 62 77} (detected by 15, t=60004 jiffies, g=899, c=898, q=15073)
     Call Trace:
       flush_tlb_mm_range+0x68/0x140
       tlb_flush_mmu.part.75+0x37/0xe0
       tlb_finish_mmu+0x55/0x60
       zap_page_range+0x142/0x190
       SyS_madvise+0x3cd/0x9c0
       system_call_fastpath+0x1c/0x21
    
    swait_active() sustains to be true before finish_swait() is called in
    kvm_vcpu_block(), voluntarily preempted vCPUs are taken into account
    by kvm_vcpu_on_spin() loop greatly increases the probability condition
    kvm_arch_vcpu_runnable(vcpu) is checked and can be true, when APICv
    is enabled the yield-candidate vCPU's VMCS RVI field leaks(by
    vmx_sync_pir_to_irr()) into spinning-on-a-taken-lock vCPU's current
    VMCS.
    
    This patch fixes it by checking conservatively a subset of events.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Marc Zyngier <Marc.Zyngier@arm.com>
    Cc: stable@vger.kernel.org
    Fixes: 98f4a1467 (KVM: add kvm_arch_vcpu_runnable() test to kvm_vcpu_on_spin() loop)
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e74f0711eaaf..fc046ca89d32 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1175,6 +1175,7 @@ struct kvm_x86_ops {
 	int (*update_pi_irte)(struct kvm *kvm, unsigned int host_irq,
 			      uint32_t guest_irq, bool set);
 	void (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);
+	bool (*dy_apicv_has_pending_interrupt)(struct kvm_vcpu *vcpu);
 
 	int (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,
 			    bool *expired);

commit d9a710e5fc4941944d565b013414e9fdc66242b5
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Jul 22 12:26:21 2019 +0800

    KVM: X86: Dynamically allocate user_fpu
    
    After reverting commit 240c35a3783a (kvm: x86: Use task structs fpu field
    for user), struct kvm_vcpu is 19456 bytes on my server, PAGE_ALLOC_COSTLY_ORDER(3)
    is the order at which allocations are deemed costly to service. In serveless
    scenario, one host can service hundreds/thoudands firecracker/kata-container
    instances, howerver, new instance will fail to launch after memory is too
    fragmented to allocate kvm_vcpu struct on host, this was observed in some
    cloud provider product environments.
    
    This patch dynamically allocates user_fpu, kvm_vcpu is 15168 bytes now on my
    Skylake server.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b2f1ffb937af..e74f0711eaaf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -616,7 +616,7 @@ struct kvm_vcpu_arch {
 	 * "guest_fpu" state here contains the guest FPU context, with the
 	 * host PRKU bits.
 	 */
-	struct fpu user_fpu;
+	struct fpu *user_fpu;
 	struct fpu *guest_fpu;
 
 	u64 xcr0;

commit ec269475cba7bcdd1eb8fdf8e87f4c6c81a376fe
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jul 22 13:31:27 2019 +0200

    Revert "kvm: x86: Use task structs fpu field for user"
    
    This reverts commit 240c35a3783ab9b3a0afaba0dde7291295680a6b
    ("kvm: x86: Use task structs fpu field for user", 2018-11-06).
    The commit is broken and causes QEMU's FPU state to be destroyed
    when KVM_RUN is preempted.
    
    Fixes: 240c35a3783a ("kvm: x86: Use task structs fpu field for user")
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0cc5b611a113..b2f1ffb937af 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -607,15 +607,16 @@ struct kvm_vcpu_arch {
 
 	/*
 	 * QEMU userspace and the guest each have their own FPU state.
-	 * In vcpu_run, we switch between the user, maintained in the
-	 * task_struct struct, and guest FPU contexts. While running a VCPU,
-	 * the VCPU thread will have the guest FPU context.
+	 * In vcpu_run, we switch between the user and guest FPU contexts.
+	 * While running a VCPU, the VCPU thread will have the guest FPU
+	 * context.
 	 *
 	 * Note that while the PKRU state lives inside the fpu registers,
 	 * it is switched out separately at VMENTER and VMEXIT time. The
 	 * "guest_fpu" state here contains the guest FPU context, with the
 	 * host PRKU bits.
 	 */
+	struct fpu user_fpu;
 	struct fpu *guest_fpu;
 
 	u64 xcr0;

commit 3901336ed9887b075531bffaeef7742ba614058b
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Jul 17 20:36:39 2019 -0500

    x86/kvm: Don't call kvm_spurious_fault() from .fixup
    
    After making a change to improve objtool's sibling call detection, it
    started showing the following warning:
    
      arch/x86/kvm/vmx/nested.o: warning: objtool: .fixup+0x15: sibling call from callable instruction with modified stack frame
    
    The problem is the ____kvm_handle_fault_on_reboot() macro.  It does a
    fake call by pushing a fake RIP and doing a jump.  That tricks the
    unwinder into printing the function which triggered the exception,
    rather than the .fixup code.
    
    Instead of the hack to make it look like the original function made the
    call, just change the macro so that the original function actually does
    make the call.  This allows removal of the hack, and also makes objtool
    happy.
    
    I triggered a vmx instruction exception and verified that the stack
    trace is still sane:
    
      kernel BUG at arch/x86/kvm/x86.c:358!
      invalid opcode: 0000 [#1] SMP PTI
      CPU: 28 PID: 4096 Comm: qemu-kvm Not tainted 5.2.0+ #16
      Hardware name: Lenovo THINKSYSTEM SD530 -[7X2106Z000]-/-[7X2106Z000]-, BIOS -[TEE113Z-1.00]- 07/17/2017
      RIP: 0010:kvm_spurious_fault+0x5/0x10
      Code: 00 00 00 00 00 8b 44 24 10 89 d2 45 89 c9 48 89 44 24 10 8b 44 24 08 48 89 44 24 08 e9 d4 40 22 00 0f 1f 40 00 0f 1f 44 00 00 <0f> 0b 66 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 41 55 49 89 fd 41
      RSP: 0018:ffffbf91c683bd00 EFLAGS: 00010246
      RAX: 000061f040000000 RBX: ffff9e159c77bba0 RCX: ffff9e15a5c87000
      RDX: 0000000665c87000 RSI: ffff9e15a5c87000 RDI: ffff9e159c77bba0
      RBP: 0000000000000000 R08: 0000000000000000 R09: ffff9e15a5c87000
      R10: 0000000000000000 R11: fffff8f2d99721c0 R12: ffff9e159c77bba0
      R13: ffffbf91c671d960 R14: ffff9e159c778000 R15: 0000000000000000
      FS:  00007fa341cbe700(0000) GS:ffff9e15b7400000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: 00007fdd38356804 CR3: 00000006759de003 CR4: 00000000007606e0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      PKRU: 55555554
      Call Trace:
       loaded_vmcs_init+0x4f/0xe0
       alloc_loaded_vmcs+0x38/0xd0
       vmx_create_vcpu+0xf7/0x600
       kvm_vm_ioctl+0x5e9/0x980
       ? __switch_to_asm+0x40/0x70
       ? __switch_to_asm+0x34/0x70
       ? __switch_to_asm+0x40/0x70
       ? __switch_to_asm+0x34/0x70
       ? free_one_page+0x13f/0x4e0
       do_vfs_ioctl+0xa4/0x630
       ksys_ioctl+0x60/0x90
       __x64_sys_ioctl+0x16/0x20
       do_syscall_64+0x55/0x1c0
       entry_SYSCALL_64_after_hwframe+0x44/0xa9
      RIP: 0033:0x7fa349b1ee5b
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/64a9b64d127e87b6920a97afde8e96ea76f6524e.1563413318.git.jpoimboe@redhat.com

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0cc5b611a113..8282b8d41209 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1496,25 +1496,29 @@ enum {
 #define kvm_arch_vcpu_memslots_id(vcpu) ((vcpu)->arch.hflags & HF_SMM_MASK ? 1 : 0)
 #define kvm_memslots_for_spte_role(kvm, role) __kvm_memslots(kvm, (role).smm)
 
+asmlinkage void __noreturn kvm_spurious_fault(void);
+
 /*
  * Hardware virtualization extension instructions may fault if a
  * reboot turns off virtualization while processes are running.
- * Trap the fault and ignore the instruction if that happens.
+ * Usually after catching the fault we just panic; during reboot
+ * instead the instruction is ignored.
  */
-asmlinkage void kvm_spurious_fault(void);
-
-#define ____kvm_handle_fault_on_reboot(insn, cleanup_insn)	\
-	"666: " insn "\n\t" \
-	"668: \n\t"                           \
-	".pushsection .fixup, \"ax\" \n" \
-	"667: \n\t" \
-	cleanup_insn "\n\t"		      \
-	"cmpb $0, kvm_rebooting \n\t"	      \
-	"jne 668b \n\t"      		      \
-	__ASM_SIZE(push) " $666b \n\t"	      \
-	"jmp kvm_spurious_fault \n\t"	      \
-	".popsection \n\t" \
-	_ASM_EXTABLE(666b, 667b)
+#define ____kvm_handle_fault_on_reboot(insn, cleanup_insn)		\
+	"666: \n\t"							\
+	insn "\n\t"							\
+	"jmp	668f \n\t"						\
+	"667: \n\t"							\
+	"call	kvm_spurious_fault \n\t"				\
+	"668: \n\t"							\
+	".pushsection .fixup, \"ax\" \n\t"				\
+	"700: \n\t"							\
+	cleanup_insn "\n\t"						\
+	"cmpb	$0, kvm_rebooting\n\t"					\
+	"je	667b \n\t"						\
+	"jmp	668b \n\t"						\
+	".popsection \n\t"						\
+	_ASM_EXTABLE(666b, 700b)
 
 #define __kvm_handle_fault_on_reboot(insn)		\
 	____kvm_handle_fault_on_reboot(insn, "")

commit a45ff5994c9cde41af627c46abb9f32beae68943
Merge: 429bb83af8bc 1e0cf16cdad1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jul 11 15:14:16 2019 +0200

    Merge tag 'kvm-arm-for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for 5.3
    
    - Add support for chained PMU counters in guests
    - Improve SError handling
    - Handle Neoverse N1 erratum #1349291
    - Allow side-channel mitigation status to be migrated
    - Standardise most AArch64 system register accesses to msr_s/mrs_s
    - Fix host MPIDR corruption on 32bit

commit 66bb8a065f5aedd4551d8d3fbce582972f65c2e1
Author: Eric Hankland <ehankland@google.com>
Date:   Wed Jul 10 18:25:15 2019 -0700

    KVM: x86: PMU Event Filter
    
    Some events can provide a guest with information about other guests or the
    host (e.g. L3 cache stats); providing the capability to restrict access
    to a "safe" set of events would limit the potential for the PMU to be used
    in any side channel attacks. This change introduces a new VM ioctl that
    sets an event filter. If the guest attempts to program a counter for
    any blacklisted or non-whitelisted event, the kernel counter won't be
    created, so any RDPMC/RDMSR will show 0 instances of that event.
    
    Signed-off-by: Eric Hankland <ehankland@google.com>
    [Lots of changes. All remaining bugs are probably mine. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f46a12a5cf2e..34d017bd1d1b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -933,6 +933,8 @@ struct kvm_arch {
 
 	bool guest_can_read_msr_platform_info;
 	bool exception_payload_enabled;
+
+	struct kvm_pmu_event_filter *pmu_event_filter;
 };
 
 struct kvm_vm_stat {

commit 20c8ccb1975b8d5639789d1025ad6ada38bd6f48
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:32 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 499
    
    Based on 1 normalized pattern(s):
    
      this work is licensed under the terms of the gnu gpl version 2 see
      the copying file in the top level directory
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 35 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.797835076@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 450d69a1e6fa..26d1eb83f72a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1,11 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Kernel-based Virtual Machine driver for Linux
  *
  * This header defines architecture specific interfaces, x86 version
- *
- * This work is licensed under the terms of the GNU GPL, version 2.  See
- * the COPYING file in the top-level directory.
- *
  */
 
 #ifndef _ASM_X86_KVM_HOST_H

commit 95b5a48c4f2b7755702c2993f9986e4a45d85e45
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Apr 19 22:50:59 2019 -0700

    KVM: VMX: Handle NMIs, #MCs and async #PFs in common irqs-disabled fn
    
    Per commit 1b6269db3f833 ("KVM: VMX: Handle NMIs before enabling
    interrupts and preemption"), NMIs are handled directly in vmx_vcpu_run()
    to "make sure we handle NMI on the current cpu, and that we don't
    service maskable interrupts before non-maskable ones".  The other
    exceptions handled by complete_atomic_exit(), e.g. async #PF and #MC,
    have similar requirements, and are located there to avoid extra VMREADs
    since VMX bins hardware exceptions and NMIs into a single exit reason.
    
    Clean up the code and eliminate the vaguely named complete_atomic_exit()
    by moving the interrupts-disabled exception and NMI handling into the
    existing handle_external_intrs() callback, and rename the callback to
    a more appropriate name.  Rename VMexit handlers throughout so that the
    atomic and non-atomic counterparts have similar names.
    
    In addition to improving code readability, this also ensures the NMI
    handler is run with the host's debug registers loaded in the unlikely
    event that the user is debugging NMIs.  Accuracy of the last_guest_tsc
    field is also improved when handling NMIs (and #MCs) as the handler
    will run after updating said field.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    [Naming cleanups. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 35e7937cc9ac..f46a12a5cf2e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1117,7 +1117,7 @@ struct kvm_x86_ops {
 	int (*check_intercept)(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage);
-	void (*handle_external_intr)(struct kvm_vcpu *vcpu);
+	void (*handle_exit_irqoff)(struct kvm_vcpu *vcpu);
 	bool (*mpx_supported)(void);
 	bool (*xsaves_supported)(void);
 	bool (*umip_emulated)(void);

commit 73f624f47c495d7129abef4b7031ed371cc7abb6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 6 14:32:59 2019 +0200

    KVM: x86: move MSR_IA32_POWER_CTL handling to common code
    
    Make it available to AMD hosts as well, just in case someone is trying
    to use an Intel processor's CPUID setup.
    
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a86026969b19..35e7937cc9ac 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -689,6 +689,7 @@ struct kvm_vcpu_arch {
 	u32 virtual_tsc_mult;
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
+	u64 msr_ia32_power_ctl;
 	u64 tsc_scaling_ratio;
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */

commit 2d5ba19bdfef4dd06add144eb04287ee98409f75
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Jun 3 19:52:44 2019 -0300

    kvm: x86: add host poll control msrs
    
    Add an MSRs which allows the guest to disable
    host polling (specifically the cpuidle-haltpoll,
    when performing polling in the guest, disables
    host side polling).
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aeadbc770eb2..a86026969b19 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -755,6 +755,8 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache data;
 	} pv_eoi;
 
+	u64 msr_kvm_poll_control;
+
 	/*
 	 * Indicate whether the access faults on its page table in guest
 	 * which is set when fix page fault and used to detect unhandeable

commit b51700632e0e53254733ff706e5bdca22d19dbe5
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue May 21 14:06:53 2019 +0800

    KVM: X86: Provide a capability to disable cstate msr read intercepts
    
    Allow guest reads CORE cstate when exposing host CPU power management capabilities
    to the guest. PKG cstate is restricted to avoid a guest to get the whole package
    information in multi-tenant scenario.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 15e973d9b840..aeadbc770eb2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -882,6 +882,7 @@ struct kvm_arch {
 	bool mwait_in_guest;
 	bool hlt_in_guest;
 	bool pause_in_guest;
+	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
 	s64 kvmclock_offset;

commit 4d22c17c17d228b7f43e51293c7bb7dac87dea40
Author: Xiaoyao Li <xiaoyao.li@linux.intel.com>
Date:   Fri Apr 19 10:16:24 2019 +0800

    kvm: x86: refine kvm_get_arch_capabilities()
    
    1. Using X86_FEATURE_ARCH_CAPABILITIES to enumerate the existence of
    MSR_IA32_ARCH_CAPABILITIES to avoid using rdmsrl_safe().
    
    2. Since kvm_get_arch_capabilities() is only used in this file, making
    it static.
    
    Signed-off-by: Xiaoyao Li <xiaoyao.li@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d5457c7bb243..15e973d9b840 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1532,7 +1532,6 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 		    unsigned long ipi_bitmap_high, u32 min,
 		    unsigned long icr, int op_64_bit);
 
-u64 kvm_get_arch_capabilities(void);
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 

commit f257d6dcda0187693407e0c2e5dab69bdab3223f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Apr 19 22:18:17 2019 -0700

    KVM: Directly return result from kvm_arch_check_processor_compat()
    
    Add a wrapper to invoke kvm_arch_check_processor_compat() so that the
    boilerplate ugliness of checking virtualization support on all CPUs is
    hidden from the arch specific code.  x86's implementation in particular
    is quite heinous, as it unnecessarily propagates the out-param pattern
    into kvm_x86_ops.
    
    While the x86 specific issue could be resolved solely by changing
    kvm_x86_ops, make the change for all architectures as returning a value
    directly is prettier and technically more robust, e.g. s390 doesn't set
    the out param, which could lead to subtle breakage in the (highly
    unlikely) scenario where the out-param was not pre-initialized by the
    caller.
    
    Opportunistically annotate svm_check_processor_compat() with __init.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 450d69a1e6fa..d5457c7bb243 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -999,7 +999,7 @@ struct kvm_x86_ops {
 	int (*disabled_by_bios)(void);             /* __init */
 	int (*hardware_enable)(void);
 	void (*hardware_disable)(void);
-	void (*check_processor_compatibility)(void *rtn);
+	int (*check_processor_compatibility)(void);/* __init */
 	int (*hardware_setup)(void);               /* __init */
 	void (*hardware_unsetup)(void);            /* __exit */
 	bool (*cpu_has_accelerated_tpr)(void);

commit 0ef0fd351550130129bbdb77362488befd7b69d2
Merge: 4489da718309 c011d23ba046
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 17 10:33:30 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for SVE and Pointer Authentication in guests
       - PMU improvements
    
      POWER:
       - support for direct access to the POWER9 XIVE interrupt controller
       - memory and performance optimizations
    
      x86:
       - support for accessing memory not backed by struct page
       - fixes and refactoring
    
      Generic:
       - dirty page tracking improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (155 commits)
      kvm: fix compilation on aarch64
      Revert "KVM: nVMX: Expose RDPMC-exiting only when guest supports PMU"
      kvm: x86: Fix L1TF mitigation for shadow MMU
      KVM: nVMX: Disable intercept for FS/GS base MSRs in vmcs02 when possible
      KVM: PPC: Book3S: Remove useless checks in 'release' method of KVM device
      KVM: PPC: Book3S HV: XIVE: Fix spelling mistake "acessing" -> "accessing"
      KVM: PPC: Book3S HV: Make sure to load LPID for radix VCPUs
      kvm: nVMX: Set nested_run_pending in vmx_set_nested_state after checks complete
      tests: kvm: Add tests for KVM_SET_NESTED_STATE
      KVM: nVMX: KVM_SET_NESTED_STATE - Tear down old EVMCS state before setting new state
      tests: kvm: Add tests for KVM_CAP_MAX_VCPUS and KVM_CAP_MAX_CPU_ID
      tests: kvm: Add tests to .gitignore
      KVM: Introduce KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2
      KVM: Fix kvm_clear_dirty_log_protect off-by-(minus-)one
      KVM: Fix the bitmap range to copy during clear dirty
      KVM: arm64: Fix ptrauth ID register masking logic
      KVM: x86: use direct accessors for RIP and RSP
      KVM: VMX: Use accessors for GPRs outside of dedicated caching logic
      KVM: x86: Omit caching logic for always-available GPRs
      kvm, x86: Properly check whether a pfn is an MMIO or not
      ...

commit 191c8137a93989825f0e9f78a198367dde677216
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Apr 18 18:32:50 2019 +0200

    x86/kvm: Implement HWCR support
    
    The hardware configuration register has some useful bits which can be
    used by guests. Implement McStatusWrEn which can be used by guests when
    injecting MCEs with the in-kernel mce-inject module.
    
    For that, we need to set bit 18 - McStatusWrEn - first, before writing
    the MCi_STATUS registers (otherwise we #GP).
    
    Add the required machinery to do so.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jim Mattson <jmattson@google.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: KVM <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Yazen Ghannam <Yazen.Ghannam@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8d68ba0cba0c..2532bfb09d8b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -781,6 +781,9 @@ struct kvm_vcpu_arch {
 
 	/* Flush the L1 Data cache for L1TF mitigation on VMENTER */
 	bool l1tf_flush_l1d;
+
+	/* AMD MSRC001_0015 Hardware Configuration */
+	u64 msr_hwcr;
 };
 
 struct kvm_lpage_info {

commit f99279825ee30b829da9d3b7cf0b9d1b9b2596e6
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Apr 16 13:32:46 2019 -0700

    KVM: lapic: Refactor ->set_hv_timer to use an explicit expired param
    
    Refactor kvm_x86_ops->set_hv_timer to use an explicit parameter for
    stating that the timer has expired.  Overloading the return value is
    unnecessarily clever, e.g. can lead to confusion over the proper return
    value from start_hv_timer() when r==1.
    
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 275990e3415b..8d68ba0cba0c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1168,7 +1168,8 @@ struct kvm_x86_ops {
 			      uint32_t guest_irq, bool set);
 	void (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);
 
-	int (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc);
+	int (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,
+			    bool *expired);
 	void (*cancel_hv_timer)(struct kvm_vcpu *vcpu);
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);

commit c715eb9fe9027ed118630adb0d59acf36b848d4f
Author: Luwei Kang <luwei.kang@intel.com>
Date:   Mon Feb 18 19:26:08 2019 -0500

    KVM: x86: Add support of clear Trace_ToPA_PMI status
    
    Let guests clear the Intel PT ToPA PMI status (bit 55 of
    MSR_CORE_PERF_GLOBAL_OVF_CTRL).
    
    Signed-off-by: Luwei Kang <luwei.kang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a9d03af34030..275990e3415b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -469,6 +469,7 @@ struct kvm_pmu {
 	u64 global_ovf_ctrl;
 	u64 counter_bitmask[2];
 	u64 global_ctrl_mask;
+	u64 global_ovf_ctrl_mask;
 	u64 reserved_bits;
 	u8 version;
 	struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];

commit 0699c64a4be6e4a6137240379a1f82c752e663d8
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Apr 30 19:33:26 2019 +0200

    x86/kvm/mmu: reset MMU context when 32-bit guest switches PAE
    
    Commit 47c42e6b4192 ("KVM: x86: fix handling of role.cr4_pae and rename it
    to 'gpte_size'") introduced a regression: 32-bit PAE guests stopped
    working. The issue appears to be: when guest switches (enables) PAE we need
    to re-initialize MMU context (set context->root_level, do
    reset_rsvds_bits_mask(), ...) but init_kvm_tdp_mmu() doesn't do that
    because we threw away is_pae(vcpu) flag from mmu role. Restore it to
    kvm_mmu_extended_role (as we now don't need it in base role) to fix
    the issue.
    
    Fixes: 47c42e6b4192 ("KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size'")
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a9d03af34030..c79abe7ca093 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -295,6 +295,7 @@ union kvm_mmu_extended_role {
 		unsigned int valid:1;
 		unsigned int execonly:1;
 		unsigned int cr0_pg:1;
+		unsigned int cr4_pae:1;
 		unsigned int cr4_pse:1;
 		unsigned int cr4_pke:1;
 		unsigned int cr4_smap:1;

commit ed19321fb6571214f410b30322e4ad6e6b7c3915
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Apr 2 08:03:09 2019 -0700

    KVM: x86: Load SMRAM in a single shot when leaving SMM
    
    RSM emulation is currently broken on VMX when the interrupted guest has
    CR4.VMXE=1.  Rather than dance around the issue of HF_SMM_MASK being set
    when loading SMSTATE into architectural state, ideally RSM emulation
    itself would be reworked to clear HF_SMM_MASK prior to loading non-SMM
    architectural state.
    
    Ostensibly, the only motivation for having HF_SMM_MASK set throughout
    the loading of state from the SMRAM save state area is so that the
    memory accesses from GET_SMSTATE() are tagged with role.smm.  Load
    all of the SMRAM save state area from guest memory at the beginning of
    RSM emulation, and load state from the buffer instead of reading guest
    memory one-by-one.
    
    This paves the way for clearing HF_SMM_MASK prior to loading state,
    and also aligns RSM with the enter_smm() behavior, which fills a
    buffer and writes SMRAM save state in a single go.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9b7b731a0032..a9d03af34030 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1182,7 +1182,7 @@ struct kvm_x86_ops {
 
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
-	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
+	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, const char *smstate);
 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
 
 	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
@@ -1592,4 +1592,7 @@ static inline int kvm_cpu_get_apicid(int mps_cpu)
 #define put_smstate(type, buf, offset, val)                      \
 	*(type *)((buf) + (offset) - 0x7e00) = val
 
+#define GET_SMSTATE(type, buf, offset)		\
+	(*(type *)((buf) + (offset) - 0x7e00))
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit bc8a3d8925a8fa09fa550e0da115d95851ce33c6
Author: Ben Gardon <bgardon@google.com>
Date:   Mon Apr 8 11:07:30 2019 -0700

    kvm: mmu: Fix overflow on kvm mmu page limit calculation
    
    KVM bases its memory usage limits on the total number of guest pages
    across all memslots. However, those limits, and the calculations to
    produce them, use 32 bit unsigned integers. This can result in overflow
    if a VM has more guest pages that can be represented by a u32. As a
    result of this overflow, KVM can use a low limit on the number of MMU
    pages it will allocate. This makes KVM unable to map all of guest memory
    at once, prompting spurious faults.
    
    Tested: Ran all kvm-unit-tests on an Intel Haswell machine. This patch
            introduced no new failures.
    
    Signed-off-by: Ben Gardon <bgardon@google.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 159b5988292f..9b7b731a0032 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -126,7 +126,7 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 }
 
 #define KVM_PERMILLE_MMU_PAGES 20
-#define KVM_MIN_ALLOC_MMU_PAGES 64
+#define KVM_MIN_ALLOC_MMU_PAGES 64UL
 #define KVM_MMU_HASH_SHIFT 12
 #define KVM_NUM_MMU_PAGES (1 << KVM_MMU_HASH_SHIFT)
 #define KVM_MIN_FREE_MMU_PAGES 5
@@ -844,9 +844,9 @@ enum kvm_irqchip_mode {
 };
 
 struct kvm_arch {
-	unsigned int n_used_mmu_pages;
-	unsigned int n_requested_mmu_pages;
-	unsigned int n_max_mmu_pages;
+	unsigned long n_used_mmu_pages;
+	unsigned long n_requested_mmu_pages;
+	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
@@ -1256,8 +1256,8 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				   gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen);
-unsigned int kvm_mmu_calculate_default_mmu_pages(struct kvm *kvm);
-void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
+unsigned long kvm_mmu_calculate_default_mmu_pages(struct kvm *kvm);
+void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long kvm_nr_mmu_pages);
 
 int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3);
 bool pdptrs_changed(struct kvm_vcpu *vcpu);

commit 45def77ebf79e2e8942b89ed79294d97ce914fa0
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 11 20:01:05 2019 -0700

    KVM: x86: update %rip after emulating IO
    
    Most (all?) x86 platforms provide a port IO based reset mechanism, e.g.
    OUT 92h or CF9h.  Userspace may emulate said mechanism, i.e. reset a
    vCPU in response to KVM_EXIT_IO, without explicitly announcing to KVM
    that it is doing a reset, e.g. Qemu jams vCPU state and resumes running.
    
    To avoid corruping %rip after such a reset, commit 0967b7bf1c22 ("KVM:
    Skip pio instruction when it is emulated, not executed") changed the
    behavior of PIO handlers, i.e. today's "fast" PIO handling to skip the
    instruction prior to exiting to userspace.  Full emulation doesn't need
    such tricks becase re-emulating the instruction will naturally handle
    %rip being changed to point at the reset vector.
    
    Updating %rip prior to executing to userspace has several drawbacks:
    
      - Userspace sees the wrong %rip on the exit, e.g. if PIO emulation
        fails it will likely yell about the wrong address.
      - Single step exits to userspace for are effectively dropped as
        KVM_EXIT_DEBUG is overwritten with KVM_EXIT_IO.
      - Behavior of PIO emulation is different depending on whether it
        goes down the fast path or the slow path.
    
    Rather than skip the PIO instruction before exiting to userspace,
    snapshot the linear %rip and cancel PIO completion if the current
    value does not match the snapshot.  For a 64-bit vCPU, i.e. the most
    common scenario, the snapshot and comparison has negligible overhead
    as VMCS.GUEST_RIP will be cached regardless, i.e. there is no extra
    VMREAD in this case.
    
    All other alternatives to snapshotting the linear %rip that don't
    rely on an explicit reset announcenment suffer from one corner case
    or another.  For example, canceling PIO completion on any write to
    %rip fails if userspace does a save/restore of %rip, and attempting to
    avoid that issue by canceling PIO only if %rip changed then fails if PIO
    collides with the reset %rip.  Attempting to zero in on the exact reset
    vector won't work for APs, which means adding more hooks such as the
    vCPU's MP_STATE, and so on and so forth.
    
    Checking for a linear %rip match technically suffers from corner cases,
    e.g. userspace could theoretically rewrite the underlying code page and
    expect a different instruction to execute, or the guest hardcodes a PIO
    reset at 0xfffffff0, but those are far, far outside of what can be
    considered normal operation.
    
    Fixes: 432baf60eee3 ("KVM: VMX: use kvm_fast_pio_in for handling IN I/O")
    Cc: <stable@vger.kernel.org>
    Reported-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 264814f26ce0..159b5988292f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -350,6 +350,7 @@ struct kvm_mmu_page {
 };
 
 struct kvm_pio_request {
+	unsigned long linear_rip;
 	unsigned long count;
 	int in;
 	int port;

commit 0cf9135b773bf32fba9dd8e6699c1b331ee4b749
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Mar 7 15:43:02 2019 -0800

    KVM: x86: Emulate MSR_IA32_ARCH_CAPABILITIES on AMD hosts
    
    The CPUID flag ARCH_CAPABILITIES is unconditioinally exposed to host
    userspace for all x86 hosts, i.e. KVM advertises ARCH_CAPABILITIES
    regardless of hardware support under the pretense that KVM fully
    emulates MSR_IA32_ARCH_CAPABILITIES.  Unfortunately, only VMX hosts
    handle accesses to MSR_IA32_ARCH_CAPABILITIES (despite KVM_GET_MSRS
    also reporting MSR_IA32_ARCH_CAPABILITIES for all hosts).
    
    Move the MSR_IA32_ARCH_CAPABILITIES handling to common x86 code so
    that it's emulated on AMD hosts.
    
    Fixes: 1eaafe91a0df4 ("kvm: x86: IA32_ARCH_CAPABILITIES is always supported")
    Cc: stable@vger.kernel.org
    Reported-by: Xiaoyao Li <xiaoyao.li@linux.intel.com>
    Cc: Jim Mattson <jmattson@google.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 679168931c40..264814f26ce0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -568,6 +568,7 @@ struct kvm_vcpu_arch {
 	bool tpr_access_reporting;
 	u64 ia32_xss;
 	u64 microcode_version;
+	u64 arch_capabilities;
 
 	/*
 	 * Paging state of the vcpu

commit 4d66623cfba0949b2f0d669bd2ae732124c99ded
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Thu Sep 27 08:31:26 2018 +0800

    KVM: x86: remove check on nr_mmu_pages in kvm_arch_commit_memory_region()
    
    * nr_mmu_pages would be non-zero only if kvm->arch.n_requested_mmu_pages is
      non-zero.
    
    * nr_mmu_pages is always non-zero, since kvm_mmu_calculate_mmu_pages()
      never return zero.
    
    Based on these two reasons, we can merge the two *if* clause and use the
    return value from kvm_mmu_calculate_mmu_pages() directly. This simplify
    the code and also eliminate the possibility for reader to believe
    nr_mmu_pages would be zero.
    
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5b03006c00be..679168931c40 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1254,7 +1254,7 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				   gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen);
-unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
+unsigned int kvm_mmu_calculate_default_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 
 int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3);

commit 05d5a48635259e621ea26d01e8316c6feeb34190
Author: Singh, Brijesh <brijesh.singh@amd.com>
Date:   Fri Feb 15 17:24:12 2019 +0000

    KVM: SVM: Workaround errata#1096 (insn_len maybe zero on SMAP violation)
    
    Errata#1096:
    
    On a nested data page fault when CR.SMAP=1 and the guest data read
    generates a SMAP violation, GuestInstrBytes field of the VMCB on a
    VMEXIT will incorrectly return 0h instead the correct guest
    instruction bytes .
    
    Recommend Workaround:
    
    To determine what instruction the guest was executing the hypervisor
    will have to decode the instruction at the instruction pointer.
    
    The recommended workaround can not be implemented for the SEV
    guest because guest memory is encrypted with the guest specific key,
    and instruction decoder will not be able to decode the instruction
    bytes. If we hit this errata in the SEV guest then log the message
    and request a guest shutdown.
    
    Reported-by: Venkatesh Srinivas <venkateshs@google.com>
    Cc: Jim Mattson <jmattson@google.com>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 88f5192ce05e..5b03006c00be 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1192,6 +1192,8 @@ struct kvm_x86_ops {
 	int (*nested_enable_evmcs)(struct kvm_vcpu *vcpu,
 				   uint16_t *vmcs_version);
 	uint16_t (*nested_get_evmcs_version)(struct kvm_vcpu *vcpu);
+
+	bool (*need_emulation_on_page_fault)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit 47c42e6b4192a2ac8b6c9858ebcf400a9eff7a10
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Mar 7 15:27:44 2019 -0800

    KVM: x86: fix handling of role.cr4_pae and rename it to 'gpte_size'
    
    The cr4_pae flag is a bit of a misnomer, its purpose is really to track
    whether the guest PTE that is being shadowed is a 4-byte entry or an
    8-byte entry.  Prior to supporting nested EPT, the size of the gpte was
    reflected purely by CR4.PAE.  KVM fudged things a bit for direct sptes,
    but it was mostly harmless since the size of the gpte never mattered.
    Now that a spte may be tracking an indirect EPT entry, relying on
    CR4.PAE is wrong and ill-named.
    
    For direct shadow pages, force the gpte_size to '1' as they are always
    8-byte entries; EPT entries can only be 8-bytes and KVM always uses
    8-byte entries for NPT and its identity map (when running with EPT but
    not unrestricted guest).
    
    Likewise, nested EPT entries are always 8-bytes.  Nested EPT presents a
    unique scenario as the size of the entries are not dictated by CR4.PAE,
    but neither is the shadow page a direct map.  To handle this scenario,
    set cr0_wp=1 and smap_andnot_wp=1, an otherwise impossible combination,
    to denote a nested EPT shadow page.  Use the information to avoid
    incorrectly zapping an unsync'd indirect page in __kvm_sync_page().
    
    Providing a consistent and accurate gpte_size fixes a bug reported by
    Vitaly where fast_cr3_switch() always fails when switching from L2 to
    L1 as kvm_mmu_get_page() would force role.cr4_pae=0 for direct pages,
    whereas kvm_calc_mmu_role_common() would set it according to CR4.PAE.
    
    Fixes: 7dcd575520082 ("x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed")
    Reported-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Tested-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a5db4475e72d..88f5192ce05e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -253,14 +253,14 @@ struct kvm_mmu_memory_cache {
  * kvm_memory_slot.arch.gfn_track which is 16 bits, so the role bits used
  * by indirect shadow page can not be more than 15 bits.
  *
- * Currently, we used 14 bits that are @level, @cr4_pae, @quadrant, @access,
+ * Currently, we used 14 bits that are @level, @gpte_is_8_bytes, @quadrant, @access,
  * @nxe, @cr0_wp, @smep_andnot_wp and @smap_andnot_wp.
  */
 union kvm_mmu_page_role {
 	u32 word;
 	struct {
 		unsigned level:4;
-		unsigned cr4_pae:1;
+		unsigned gpte_is_8_bytes:1;
 		unsigned quadrant:2;
 		unsigned direct:1;
 		unsigned access:3;

commit 636deed6c0bc137a7c4f4a97ae1fcf0ad75323da
Merge: aa2e3ac64ace 4a605bc08e98
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 15 15:00:28 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - some cleanups
       - direct physical timer assignment
       - cache sanitization for 32-bit guests
    
      s390:
       - interrupt cleanup
       - introduction of the Guest Information Block
       - preparation for processor subfunctions in cpu models
    
      PPC:
       - bug fixes and improvements, especially related to machine checks
         and protection keys
    
      x86:
       - many, many cleanups, including removing a bunch of MMU code for
         unnecessary optimizations
       - AVIC fixes
    
      Generic:
       - memcg accounting"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (147 commits)
      kvm: vmx: fix formatting of a comment
      KVM: doc: Document the life cycle of a VM and its resources
      MAINTAINERS: Add KVM selftests to existing KVM entry
      Revert "KVM/MMU: Flush tlb directly in the kvm_zap_gfn_range()"
      KVM: PPC: Book3S: Add count cache flush parameters to kvmppc_get_cpu_char()
      KVM: PPC: Fix compilation when KVM is not enabled
      KVM: Minor cleanups for kvm_main.c
      KVM: s390: add debug logging for cpu model subfunctions
      KVM: s390: implement subfunction processor calls
      arm64: KVM: Fix architecturally invalid reset value for FPEXC32_EL2
      KVM: arm/arm64: Remove unused timer variable
      KVM: PPC: Book3S: Improve KVM reference counting
      KVM: PPC: Book3S HV: Fix build failure without IOMMU support
      Revert "KVM: Eliminate extra function calls in kvm_get_dirty_log_protect()"
      x86: kvmguest: use TSC clocksource if invariant TSC is exposed
      KVM: Never start grow vCPU halt_poll_ns from value below halt_poll_ns_grow_start
      KVM: Expose the initial start value in grow_halt_poll_ns() as a module parameter
      KVM: grow_halt_poll_ns() should never shrink vCPU halt_poll_ns
      KVM: x86/mmu: Consolidate kvm_mmu_zap_all() and kvm_mmu_zap_mmio_sptes()
      KVM: x86/mmu: WARN if zapping a MMIO spte results in zapping children
      ...

commit de3ccd26fafc707b09792d9b633c8b5b48865315
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Fri Feb 1 00:09:23 2019 +0800

    KVM: MMU: record maximum physical address width in kvm_mmu_extended_role
    
    Previously, commit 7dcd57552008 ("x86/kvm/mmu: check if tdp/shadow
    MMU reconfiguration is needed") offered some optimization to avoid
    the unnecessary reconfiguration. Yet one scenario is broken - when
    cpuid changes VM's maximum physical address width, reconfiguration
    is needed to reset the reserved bits.  Also, the TDP may need to
    reset its shadow_root_level when this value is changed.
    
    To fix this, a new field, maxphyaddr, is introduced in the extended
    role structure to keep track of the configured guest physical address
    width.
    
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 593e17b7797e..180373360e34 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -299,6 +299,7 @@ union kvm_mmu_extended_role {
 		unsigned int cr4_smap:1;
 		unsigned int cr4_smep:1;
 		unsigned int cr4_la57:1;
+		unsigned int maxphyaddr:6;
 	};
 };
 

commit ad7dc69aeb23138cc23c406cac25003b97e8ee17
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Fri Feb 22 17:45:01 2019 +0100

    x86/kvm/mmu: fix switch between root and guest MMUs
    
    Commit 14c07ad89f4d ("x86/kvm/mmu: introduce guest_mmu") brought one subtle
    change: previously, when switching back from L2 to L1, we were resetting
    MMU hooks (like mmu->get_cr3()) in kvm_init_mmu() called from
    nested_vmx_load_cr3() and now we do that in nested_ept_uninit_mmu_context()
    when we re-target vcpu->arch.mmu pointer.
    The change itself looks logical: if nested_ept_init_mmu_context() changes
    something than nested_ept_uninit_mmu_context() restores it back. There is,
    however, one thing: the following call chain:
    
     nested_vmx_load_cr3()
      kvm_mmu_new_cr3()
        __kvm_mmu_new_cr3()
          fast_cr3_switch()
            cached_root_available()
    
    now happens with MMU hooks pointing to the new MMU (root MMU in our case)
    while previously it was happening with the old one. cached_root_available()
    tries to stash current root but it is incorrect to read current CR3 with
    mmu->get_cr3(), we need to use old_mmu->get_cr3() which in case we're
    switching from L2 to L1 is guest_mmu. (BTW, in shadow page tables case this
    is a non-issue because we don't switch MMU).
    
    While we could've tried to guess that we're switching between MMUs and call
    the right ->get_cr3() from cached_root_available() this seems to be overly
    complicated. Instead, just stash the corresponding CR3 when setting
    root_hpa and make cached_root_available() use the stashed value.
    
    Fixes: 14c07ad89f4d ("x86/kvm/mmu: introduce guest_mmu")
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4660ce90de7f..593e17b7797e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -397,6 +397,7 @@ struct kvm_mmu {
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
 	hpa_t root_hpa;
+	gpa_t root_cr3;
 	union kvm_mmu_role mmu_role;
 	u8 root_level;
 	u8 shadow_root_level;

commit ea145aacf4ae8485cf179a4d0dc502e9f75044f4
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:34 2019 -0800

    Revert "KVM: MMU: fast invalidate all pages"
    
    Remove x86 KVM's fast invalidate mechanism, i.e. revert all patches
    from the original series[1], now that all users of the fast invalidate
    mechanism are gone.
    
    This reverts commit 5304b8d37c2a5ebca48330f5e7868d240eafbed1.
    
    [1] https://lkml.kernel.org/r/1369960590-14138-1-git-send-email-xiaoguangrong@linux.vnet.ibm.com
    
    Cc: Xiao Guangrong <guangrong.xiao@gmail.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fbe16a908076..9417febf8490 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -334,7 +334,6 @@ struct kvm_mmu_page {
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
-	unsigned long mmu_valid_gen;
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32
@@ -845,7 +844,6 @@ struct kvm_arch {
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
-	unsigned long mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.

commit 52d5dedc79bdcbac2976159a172069618cf31be5
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:26 2019 -0800

    Revert "KVM: MMU: reclaim the zapped-obsolete page first"
    
    Unwinding optimizations related to obsolete pages is a step towards
    removing x86 KVM's fast invalidate mechanism, i.e. this is one part of
    a revert all patches from the series that introduced the mechanism[1].
    
    This reverts commit 365c886860c4ba670d245e762b23987c912c129a.
    
    [1] https://lkml.kernel.org/r/1369960590-14138-1-git-send-email-xiaoguangrong@linux.vnet.ibm.com
    
    Cc: Xiao Guangrong <guangrong.xiao@gmail.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aeca3fb1cf63..fbe16a908076 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -851,7 +851,6 @@ struct kvm_arch {
 	 * Hash table of struct kvm_mmu_page.
 	 */
 	struct list_head active_mmu_pages;
-	struct list_head zapped_obsolete_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
 

commit 4771450c345dc5e3e3417d82aff62e0d88e7eee6
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:23 2019 -0800

    Revert "KVM: MMU: drop kvm_mmu_zap_mmio_sptes"
    
    Revert back to a dedicated (and slower) mechanism for handling the
    scenario where all MMIO shadow PTEs need to be zapped due to overflowing
    the MMIO generation number.  The MMIO generation scenario is almost
    literally a one-in-a-million occurrence, i.e. is not a performance
    sensitive scenario.
    
    Restoring kvm_mmu_zap_mmio_sptes() leaves VM teardown as the only user
    of kvm_mmu_invalidate_zap_all_pages() and paves the way for removing
    the fast invalidate mechanism altogether.
    
    This reverts commit a8eca9dcc656a405a28ffba43f3d86a1ff0eb331.
    
    Cc: Xiao Guangrong <guangrong.xiao@gmail.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 69daa57b08a7..aeca3fb1cf63 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -319,6 +319,7 @@ struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
 	bool unsync;
+	bool mmio_cached;
 
 	/*
 	 * The following two entries are used to key the shadow page in the

commit a592a3b8fc62af25a6e76aebde97a5d5f6f13e0f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:22 2019 -0800

    Revert "KVM: MMU: document fast invalidate all pages"
    
    Remove x86 KVM's fast invalidate mechanism, i.e. revert all patches
    from the original series[1].
    
    Though not explicitly stated, for all intents and purposes the fast
    invalidate mechanism was added to speed up the scenario where removing
    a memslot, e.g. as part of accessing reading PCI ROM, caused KVM to
    flush all shadow entries[1].  Now that the memslot case flushes only
    shadow entries belonging to the memslot, i.e. doesn't use the fast
    invalidate mechanism, the only remaining usage of the mechanism are
    when the VM is being destroyed and when the MMIO generation rolls
    over.
    
    When a VM is being destroyed, either there are no active vcpus, i.e.
    there's no lock contention, or the VM has ungracefully terminated, in
    which case we want to reclaim its pages as quickly as possible, i.e.
    not release the MMU lock if there are still CPUs executing in the VM.
    
    The MMIO generation scenario is almost literally a one-in-a-million
    occurrence, i.e. is not a performance sensitive scenario.
    
    Given that lock-breaking is not desirable (VM teardown) or irrelevant
    (MMIO generation overflow), remove the fast invalidate mechanism to
    simplify the code (a small amount) and to discourage future code from
    zapping all pages as using such a big hammer should be a last restort.
    
    This reverts commit f6f8adeef542a18b1cb26a0b772c9781a10bb477.
    
    [1] https://lkml.kernel.org/r/1369960590-14138-1-git-send-email-xiaoguangrong@linux.vnet.ibm.com
    
    Cc: Xiao Guangrong <guangrong.xiao@gmail.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c4758e1a8843..69daa57b08a7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -333,10 +333,7 @@ struct kvm_mmu_page {
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
-
-	/* The page is obsolete if mmu_valid_gen != kvm->arch.mmu_valid_gen.  */
 	unsigned long mmu_valid_gen;
-
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32

commit 152482580a1b0accb60676063a1ac57b2d12daf6
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 12:54:17 2019 -0800

    KVM: Call kvm_arch_memslots_updated() before updating memslots
    
    kvm_arch_memslots_updated() is at this point in time an x86-specific
    hook for handling MMIO generation wraparound.  x86 stashes 19 bits of
    the memslots generation number in its MMIO sptes in order to avoid
    full page fault walks for repeat faults on emulated MMIO addresses.
    Because only 19 bits are used, wrapping the MMIO generation number is
    possible, if unlikely.  kvm_arch_memslots_updated() alerts x86 that
    the generation has changed so that it can invalidate all MMIO sptes in
    case the effective MMIO generation has wrapped so as to avoid using a
    stale spte, e.g. a (very) old spte that was created with generation==0.
    
    Given that the purpose of kvm_arch_memslots_updated() is to prevent
    consuming stale entries, it needs to be called before the new generation
    is propagated to memslots.  Invalidating the MMIO sptes after updating
    memslots means that there is a window where a vCPU could dereference
    the new memslots generation, e.g. 0, and incorrectly reuse an old MMIO
    spte that was created with (pre-wrap) generation==0.
    
    Fixes: e59dbe09f8e6 ("KVM: Introduce kvm_arch_memslots_updated()")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0e2ef41efb9d..c4758e1a8843 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1254,7 +1254,7 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				   struct kvm_memory_slot *slot,
 				   gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
-void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, struct kvm_memslots *slots);
+void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 

commit 95c7b77d6e40a407ca5bd21d7ba2e1c28ad8e85a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jan 25 07:41:09 2019 -0800

    KVM: x86: Explicitly #define the VCPU_REGS_* indices
    
    Declaring the VCPU_REGS_* as enums allows for more robust C code, but it
    prevents using the values in assembly files.  Expliciting #define the
    indices in an asm-friendly file to prepare for VMX moving its transition
    code to a proper assembly file, but keep the enums for general usage.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4660ce90de7f..0e2ef41efb9d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -35,6 +35,7 @@
 #include <asm/msr-index.h>
 #include <asm/asm.h>
 #include <asm/kvm_page_track.h>
+#include <asm/kvm_vcpu_regs.h>
 #include <asm/hyperv-tlfs.h>
 
 #define KVM_MAX_VCPUS 288
@@ -137,23 +138,23 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 #define ASYNC_PF_PER_VCPU 64
 
 enum kvm_reg {
-	VCPU_REGS_RAX = 0,
-	VCPU_REGS_RCX = 1,
-	VCPU_REGS_RDX = 2,
-	VCPU_REGS_RBX = 3,
-	VCPU_REGS_RSP = 4,
-	VCPU_REGS_RBP = 5,
-	VCPU_REGS_RSI = 6,
-	VCPU_REGS_RDI = 7,
+	VCPU_REGS_RAX = __VCPU_REGS_RAX,
+	VCPU_REGS_RCX = __VCPU_REGS_RCX,
+	VCPU_REGS_RDX = __VCPU_REGS_RDX,
+	VCPU_REGS_RBX = __VCPU_REGS_RBX,
+	VCPU_REGS_RSP = __VCPU_REGS_RSP,
+	VCPU_REGS_RBP = __VCPU_REGS_RBP,
+	VCPU_REGS_RSI = __VCPU_REGS_RSI,
+	VCPU_REGS_RDI = __VCPU_REGS_RDI,
 #ifdef CONFIG_X86_64
-	VCPU_REGS_R8 = 8,
-	VCPU_REGS_R9 = 9,
-	VCPU_REGS_R10 = 10,
-	VCPU_REGS_R11 = 11,
-	VCPU_REGS_R12 = 12,
-	VCPU_REGS_R13 = 13,
-	VCPU_REGS_R14 = 14,
-	VCPU_REGS_R15 = 15,
+	VCPU_REGS_R8  = __VCPU_REGS_R8,
+	VCPU_REGS_R9  = __VCPU_REGS_R9,
+	VCPU_REGS_R10 = __VCPU_REGS_R10,
+	VCPU_REGS_R11 = __VCPU_REGS_R11,
+	VCPU_REGS_R12 = __VCPU_REGS_R12,
+	VCPU_REGS_R13 = __VCPU_REGS_R13,
+	VCPU_REGS_R14 = __VCPU_REGS_R14,
+	VCPU_REGS_R15 = __VCPU_REGS_R15,
 #endif
 	VCPU_REGS_RIP,
 	NR_VCPU_REGS

commit e81434995081fd7efb755fd75576b35dbb0850b1
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Dec 20 14:21:08 2018 -0800

    KVM: x86: Use jmp to invoke kvm_spurious_fault() from .fixup
    
    ____kvm_handle_fault_on_reboot() provides a generic exception fixup
    handler that is used to cleanly handle faults on VMX/SVM instructions
    during reboot (or at least try to).  If there isn't a reboot in
    progress, ____kvm_handle_fault_on_reboot() treats any exception as
    fatal to KVM and invokes kvm_spurious_fault(), which in turn generates
    a BUG() to get a stack trace and die.
    
    When it was originally added by commit 4ecac3fd6dc2 ("KVM: Handle
    virtualization instruction #UD faults during reboot"), the "call" to
    kvm_spurious_fault() was handcoded as PUSH+JMP, where the PUSH'd value
    is the RIP of the faulting instructing.
    
    The PUSH+JMP trickery is necessary because the exception fixup handler
    code lies outside of its associated function, e.g. right after the
    function.  An actual CALL from the .fixup code would show a slightly
    bogus stack trace, e.g. an extra "random" function would be inserted
    into the trace, as the return RIP on the stack would point to no known
    function (and the unwinder will likely try to guess who owns the RIP).
    
    Unfortunately, the JMP was replaced with a CALL when the macro was
    reworked to not spin indefinitely during reboot (commit b7c4145ba2eb
    "KVM: Don't spin on virt instruction faults during reboot").  This
    causes the aforementioned behavior where a bogus function is inserted
    into the stack trace, e.g. my builds like to blame free_kvm_area().
    
    Revert the CALL back to a JMP.  The changelog for commit b7c4145ba2eb
    ("KVM: Don't spin on virt instruction faults during reboot") contains
    nothing that indicates the switch to CALL was deliberate.  This is
    backed up by the fact that the PUSH <insn RIP> was left intact.
    
    Note that an alternative to the PUSH+JMP magic would be to JMP back
    to the "real" code and CALL from there, but that would require adding
    a JMP in the non-faulting path to avoid calling kvm_spurious_fault()
    and would add no value, i.e. the stack trace would be the same.
    
    Using CALL:
    
    ------------[ cut here ]------------
    kernel BUG at /home/sean/go/src/kernel.org/linux/arch/x86/kvm/x86.c:356!
    invalid opcode: 0000 [#1] SMP
    CPU: 4 PID: 1057 Comm: qemu-system-x86 Not tainted 4.20.0-rc6+ #75
    Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 0.0.0 02/06/2015
    RIP: 0010:kvm_spurious_fault+0x5/0x10 [kvm]
    Code: <0f> 0b 66 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 41 55 49 89 fd 41
    RSP: 0018:ffffc900004bbcc8 EFLAGS: 00010046
    RAX: 0000000000000000 RBX: 0000000000000000 RCX: ffffffffffffffff
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
    RBP: ffff888273fd8000 R08: 00000000000003e8 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000784 R12: ffffc90000371fb0
    R13: 0000000000000000 R14: 000000026d763cf4 R15: ffff888273fd8000
    FS:  00007f3d69691700(0000) GS:ffff888277800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 000055f89bc56fe0 CR3: 0000000271a5a001 CR4: 0000000000362ee0
    Call Trace:
     free_kvm_area+0x1044/0x43ea [kvm_intel]
     ? vmx_vcpu_run+0x156/0x630 [kvm_intel]
     ? kvm_arch_vcpu_ioctl_run+0x447/0x1a40 [kvm]
     ? kvm_vcpu_ioctl+0x368/0x5c0 [kvm]
     ? kvm_vcpu_ioctl+0x368/0x5c0 [kvm]
     ? __set_task_blocked+0x38/0x90
     ? __set_current_blocked+0x50/0x60
     ? __fpu__restore_sig+0x97/0x490
     ? do_vfs_ioctl+0xa1/0x620
     ? __x64_sys_futex+0x89/0x180
     ? ksys_ioctl+0x66/0x70
     ? __x64_sys_ioctl+0x16/0x20
     ? do_syscall_64+0x4f/0x100
     ? entry_SYSCALL_64_after_hwframe+0x44/0xa9
    Modules linked in: vhost_net vhost tap kvm_intel kvm irqbypass bridge stp llc
    ---[ end trace 9775b14b123b1713 ]---
    
    Using JMP:
    
    ------------[ cut here ]------------
    kernel BUG at /home/sean/go/src/kernel.org/linux/arch/x86/kvm/x86.c:356!
    invalid opcode: 0000 [#1] SMP
    CPU: 6 PID: 1067 Comm: qemu-system-x86 Not tainted 4.20.0-rc6+ #75
    Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 0.0.0 02/06/2015
    RIP: 0010:kvm_spurious_fault+0x5/0x10 [kvm]
    Code: <0f> 0b 66 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 41 55 49 89 fd 41
    RSP: 0018:ffffc90000497cd0 EFLAGS: 00010046
    RAX: 0000000000000000 RBX: 0000000000000000 RCX: ffffffffffffffff
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
    RBP: ffff88827058bd40 R08: 00000000000003e8 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000784 R12: ffffc90000369fb0
    R13: 0000000000000000 R14: 00000003c8fc6642 R15: ffff88827058bd40
    FS:  00007f3d7219e700(0000) GS:ffff888277900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3d64001000 CR3: 0000000271c6b004 CR4: 0000000000362ee0
    Call Trace:
     vmx_vcpu_run+0x156/0x630 [kvm_intel]
     ? kvm_arch_vcpu_ioctl_run+0x447/0x1a40 [kvm]
     ? kvm_vcpu_ioctl+0x368/0x5c0 [kvm]
     ? kvm_vcpu_ioctl+0x368/0x5c0 [kvm]
     ? __set_task_blocked+0x38/0x90
     ? __set_current_blocked+0x50/0x60
     ? __fpu__restore_sig+0x97/0x490
     ? do_vfs_ioctl+0xa1/0x620
     ? __x64_sys_futex+0x89/0x180
     ? ksys_ioctl+0x66/0x70
     ? __x64_sys_ioctl+0x16/0x20
     ? do_syscall_64+0x4f/0x100
     ? entry_SYSCALL_64_after_hwframe+0x44/0xa9
    Modules linked in: vhost_net vhost tap kvm_intel kvm irqbypass bridge stp llc
    ---[ end trace f9daedb85ab3ddba ]---
    
    Fixes: b7c4145ba2eb ("KVM: Don't spin on virt instruction faults during reboot")
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 35638ace7387..4660ce90de7f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1501,7 +1501,7 @@ asmlinkage void kvm_spurious_fault(void);
 	"cmpb $0, kvm_rebooting \n\t"	      \
 	"jne 668b \n\t"      		      \
 	__ASM_SIZE(push) " $666b \n\t"	      \
-	"call kvm_spurious_fault \n\t"	      \
+	"jmp kvm_spurious_fault \n\t"	      \
 	".popsection \n\t" \
 	_ASM_EXTABLE(666b, 667b)
 

commit 748c0e312fce983bd7854b369b192e24dce90878
Author: Lan Tianyu <Tianyu.Lan@microsoft.com>
Date:   Thu Dec 6 21:21:10 2018 +0800

    KVM: Make kvm_set_spte_hva() return int
    
    The patch is to make kvm_set_spte_hva() return int and caller can
    check return value to determine flush tlb or not.
    
    Signed-off-by: Lan Tianyu <Tianyu.Lan@microsoft.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3639077d285b..35638ace7387 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1512,7 +1512,7 @@ asmlinkage void kvm_spurious_fault(void);
 int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
-void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
+int kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);

commit a49b96352e68368c2e6784d13b0b4b7b8d830922
Author: Lan Tianyu <Tianyu.Lan@microsoft.com>
Date:   Thu Dec 6 21:21:04 2018 +0800

    KVM: Add tlb_remote_flush_with_range callback in kvm_x86_ops
    
    Add flush range call back in the kvm_x86_ops and platform can use it
    to register its associated function. The parameter "kvm_tlb_range"
    accepts a single range and flush list which contains a list of ranges.
    
    Signed-off-by: Lan Tianyu <Tianyu.Lan@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ffb8a853e0d4..3639077d285b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -439,6 +439,11 @@ struct kvm_mmu {
 	u64 pdptrs[4]; /* pae */
 };
 
+struct kvm_tlb_range {
+	u64 start_gfn;
+	u64 pages;
+};
+
 enum pmc_type {
 	KVM_PMC_GP = 0,
 	KVM_PMC_FIXED,
@@ -1041,6 +1046,8 @@ struct kvm_x86_ops {
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu, bool invalidate_gpa);
 	int  (*tlb_remote_flush)(struct kvm *kvm);
+	int  (*tlb_remote_flush_with_range)(struct kvm *kvm,
+			struct kvm_tlb_range *range);
 
 	/*
 	 * Flush any TLB entries associated with the given GVA.

commit 86f5201df0d3e3efc78d3eac7fc5a59b813287cd
Author: Chao Peng <chao.p.peng@linux.intel.com>
Date:   Wed Oct 24 16:05:11 2018 +0800

    KVM: x86: Add Intel Processor Trace cpuid emulation
    
    Expose Intel Processor Trace to guest only when
    the PT works in Host-Guest mode.
    
    Signed-off-by: Chao Peng <chao.p.peng@linux.intel.com>
    Signed-off-by: Luwei Kang <luwei.kang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bca77c25a19a..ffb8a853e0d4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1105,6 +1105,7 @@ struct kvm_x86_ops {
 	bool (*mpx_supported)(void);
 	bool (*xsaves_supported)(void);
 	bool (*umip_emulated)(void);
+	bool (*pt_supported)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
 	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);

commit b666a4b697397f8492dc11a2a1877557d3e0af56
Author: Marc Orr <marcorr@google.com>
Date:   Tue Nov 6 14:53:56 2018 -0800

    kvm: x86: Dynamically allocate guest_fpu
    
    Previously, the guest_fpu field was embedded in the kvm_vcpu_arch
    struct. Unfortunately, the field is quite large, (e.g., 4352 bytes on my
    current setup). This bloats the kvm_vcpu_arch struct for x86 into an
    order 3 memory allocation, which can become a problem on overcommitted
    machines. Thus, this patch moves the fpu state outside of the
    kvm_vcpu_arch struct.
    
    With this patch applied, the kvm_vcpu_arch struct is reduced to 15168
    bytes for vmx on my setup when building the kernel with kvmconfig.
    
    Suggested-by: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Marc Orr <marcorr@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c143dfe75869..bca77c25a19a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -610,7 +610,7 @@ struct kvm_vcpu_arch {
 	 * "guest_fpu" state here contains the guest FPU context, with the
 	 * host PRKU bits.
 	 */
-	struct fpu guest_fpu;
+	struct fpu *guest_fpu;
 
 	u64 xcr0;
 	u64 guest_supported_xcr0;
@@ -1196,6 +1196,7 @@ struct kvm_arch_async_pf {
 };
 
 extern struct kvm_x86_ops *kvm_x86_ops;
+extern struct kmem_cache *x86_fpu_cache;
 
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)

commit 240c35a3783ab9b3a0afaba0dde7291295680a6b
Author: Marc Orr <marcorr@google.com>
Date:   Tue Nov 6 14:53:55 2018 -0800

    kvm: x86: Use task structs fpu field for user
    
    Previously, x86's instantiation of 'struct kvm_vcpu_arch' added an fpu
    field to save/restore fpu-related architectural state, which will differ
    from kvm's fpu state. However, this is redundant to the 'struct fpu'
    field, called fpu, embedded in the task struct, via the thread field.
    Thus, this patch removes the user_fpu field from the kvm_vcpu_arch
    struct and replaces it with the task struct's fpu field.
    
    This change is significant because the fpu struct is actually quite
    large. For example, on the system used to develop this patch, this
    change reduces the size of the vcpu_vmx struct from 23680 bytes down to
    19520 bytes, when building the kernel with kvmconfig. This reduction in
    the size of the vcpu_vmx struct moves us closer to being able to
    allocate the struct at order 2, rather than order 3.
    
    Suggested-by: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Marc Orr <marcorr@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ddeddf2a992c..c143dfe75869 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -601,16 +601,15 @@ struct kvm_vcpu_arch {
 
 	/*
 	 * QEMU userspace and the guest each have their own FPU state.
-	 * In vcpu_run, we switch between the user and guest FPU contexts.
-	 * While running a VCPU, the VCPU thread will have the guest FPU
-	 * context.
+	 * In vcpu_run, we switch between the user, maintained in the
+	 * task_struct struct, and guest FPU contexts. While running a VCPU,
+	 * the VCPU thread will have the guest FPU context.
 	 *
 	 * Note that while the PKRU state lives inside the fpu registers,
 	 * it is switched out separately at VMENTER and VMEXIT time. The
 	 * "guest_fpu" state here contains the guest FPU context, with the
 	 * host PRKU bits.
 	 */
-	struct fpu user_fpu;
 	struct fpu guest_fpu;
 
 	u64 xcr0;

commit 6a058a1eadc3882eff6efaa757f2c71a31fe9906
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Nov 26 16:47:30 2018 +0100

    x86/kvm/hyper-v: use stimer config definition from hyperv-tlfs.h
    
    As a preparation to implementing Direct Mode for Hyper-V synthetic
    timers switch to using stimer config definition from hyperv-tlfs.h.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f5c2ce4f01e8..ddeddf2a992c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -497,7 +497,7 @@ struct kvm_mtrr {
 struct kvm_vcpu_hv_stimer {
 	struct hrtimer timer;
 	int index;
-	u64 config;
+	union hv_stimer_config config;
 	u64 count;
 	u64 exp_time;
 	struct hv_message msg;

commit e2e871ab2f02dc9ca5f06065234475393dcec38b
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Dec 10 18:21:55 2018 +0100

    x86/kvm/hyper-v: Introduce nested_get_evmcs_version() helper
    
    The upcoming KVM_GET_SUPPORTED_HV_CPUID ioctl will need to return
    Enlightened VMCS version in HYPERV_CPUID_NESTED_FEATURES.EAX when
    it was enabled.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fbda5a917c5b..f5c2ce4f01e8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1186,6 +1186,7 @@ struct kvm_x86_ops {
 
 	int (*nested_enable_evmcs)(struct kvm_vcpu *vcpu,
 				   uint16_t *vmcs_version);
+	uint16_t (*nested_get_evmcs_version)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit 326e742533bf0a23f0127d8ea62fb558ba665f08
Author: Leonid Shatz <leonid.shatz@oracle.com>
Date:   Tue Nov 6 12:14:25 2018 +0200

    KVM: nVMX/nSVM: Fix bug which sets vcpu->arch.tsc_offset to L1 tsc_offset
    
    Since commit e79f245ddec1 ("X86/KVM: Properly update 'tsc_offset' to
    represent the running guest"), vcpu->arch.tsc_offset meaning was
    changed to always reflect the tsc_offset value set on active VMCS.
    Regardless if vCPU is currently running L1 or L2.
    
    However, above mentioned commit failed to also change
    kvm_vcpu_write_tsc_offset() to set vcpu->arch.tsc_offset correctly.
    This is because vmx_write_tsc_offset() could set the tsc_offset value
    in active VMCS to given offset parameter *plus vmcs12->tsc_offset*.
    However, kvm_vcpu_write_tsc_offset() just sets vcpu->arch.tsc_offset
    to given offset parameter. Without taking into account the possible
    addition of vmcs12->tsc_offset. (Same is true for SVM case).
    
    Fix this issue by changing kvm_x86_ops->write_tsc_offset() to return
    actually set tsc_offset in active VMCS and modify
    kvm_vcpu_write_tsc_offset() to set returned value in
    vcpu->arch.tsc_offset.
    In addition, rename write_tsc_offset() callback to write_l1_tsc_offset()
    to make it clear that it is meant to set L1 TSC offset.
    
    Fixes: e79f245ddec1 ("X86/KVM: Properly update 'tsc_offset' to represent the running guest")
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Mihai Carabas <mihai.carabas@oracle.com>
    Reviewed-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Signed-off-by: Leonid Shatz <leonid.shatz@oracle.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 55e51ff7e421..fbda5a917c5b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1094,7 +1094,8 @@ struct kvm_x86_ops {
 	bool (*has_wbinvd_exit)(void);
 
 	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);
-	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
+	/* Returns actual tsc_offset set in active VMCS */
+	u64 (*write_l1_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
 

commit 59073aaf6de0d2dacc2603cee6d1d6cd5592ac08
Author: Jim Mattson <jmattson@google.com>
Date:   Tue Oct 16 14:29:20 2018 -0700

    kvm: x86: Add exception payload fields to kvm_vcpu_events
    
    The per-VM capability KVM_CAP_EXCEPTION_PAYLOAD (to be introduced in a
    later commit) adds the following fields to struct kvm_vcpu_events:
    exception_has_payload, exception_payload, and exception.pending.
    
    With this capability set, all of the details of vcpu->arch.exception,
    including the payload for a pending exception, are reported to
    userspace in response to KVM_GET_VCPU_EVENTS.
    
    With this capability clear, the original ABI is preserved, and the
    exception.injected field is set for either pending or injected
    exceptions.
    
    When userspace calls KVM_SET_VCPU_EVENTS with
    KVM_CAP_EXCEPTION_PAYLOAD clear, exception.injected is no longer
    translated to exception.pending. KVM_SET_VCPU_EVENTS can now only
    establish a pending exception when KVM_CAP_EXCEPTION_PAYLOAD is set.
    
    Reported-by: Jim Mattson <jmattson@google.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 20f7c994afeb..55e51ff7e421 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -919,6 +919,7 @@ struct kvm_arch {
 	bool x2apic_broadcast_quirk_disabled;
 
 	bool guest_can_read_msr_platform_info;
+	bool exception_payload_enabled;
 };
 
 struct kvm_vm_stat {

commit c851436a34cad09388f1303e11ccb6b9420e5692
Author: Jim Mattson <jmattson@google.com>
Date:   Tue Oct 16 14:29:19 2018 -0700

    kvm: x86: Add has_payload and payload to kvm_queued_exception
    
    The payload associated with a #PF exception is the linear address of
    the fault to be loaded into CR2 when the fault is delivered. The
    payload associated with a #DB exception is a mask of the DR6 bits to
    be set (or in the case of DR6.RTM, cleared) when the fault is
    delivered. Add fields has_payload and payload to kvm_queued_exception
    to track payloads for pending exceptions.
    
    The new fields are introduced here, but for now, they are just cleared.
    
    Reported-by: Jim Mattson <jmattson@google.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 258fc2c85301..20f7c994afeb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -628,6 +628,8 @@ struct kvm_vcpu_arch {
 		bool has_error_code;
 		u8 nr;
 		u32 error_code;
+		unsigned long payload;
+		bool has_payload;
 		u8 nested_apf;
 	} exception;
 

commit 57b119da3594f5145a64fdebe0ac9ee0cc65f371
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Oct 16 18:50:01 2018 +0200

    KVM: nVMX: add KVM_CAP_HYPERV_ENLIGHTENED_VMCS capability
    
    Enlightened VMCS is opt-in. The current version does not contain all
    fields supported by nested VMX so we must not advertise the
    corresponding VMX features if enlightened VMCS is enabled.
    
    Userspace is given the enlightened VMCS version supported by KVM as
    part of enabling KVM_CAP_HYPERV_ENLIGHTENED_VMCS. The version is to
    be advertised to the nested hypervisor, currently done via a cpuid
    leaf for Hyper-V.
    
    Suggested-by: Ladi Prosek <lprosek@redhat.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4b09d4aa9bf4..258fc2c85301 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1179,6 +1179,9 @@ struct kvm_x86_ops {
 	int (*mem_enc_unreg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
 
 	int (*get_msr_feature)(struct kvm_msr_entry *entry);
+
+	int (*nested_enable_evmcs)(struct kvm_vcpu *vcpu,
+				   uint16_t *vmcs_version);
 };
 
 struct kvm_arch_async_pf {

commit 7dcd575520082f186231777f4ac9f59ce14d6961
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Oct 8 21:28:12 2018 +0200

    x86/kvm/mmu: check if tdp/shadow MMU reconfiguration is needed
    
    MMU reconfiguration in init_kvm_tdp_mmu()/kvm_init_shadow_mmu() can be
    avoided if the source data used to configure it didn't change; enhance
    MMU extended role with the required fields and consolidate common code in
    kvm_calc_mmu_role_common().
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bf541af7442d..4b09d4aa9bf4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -293,10 +293,12 @@ union kvm_mmu_extended_role {
 	struct {
 		unsigned int valid:1;
 		unsigned int execonly:1;
+		unsigned int cr0_pg:1;
 		unsigned int cr4_pse:1;
 		unsigned int cr4_pke:1;
 		unsigned int cr4_smap:1;
 		unsigned int cr4_smep:1;
+		unsigned int cr4_la57:1;
 	};
 };
 

commit a336282d7753a92ced7b8e52ff959929f6e550ff
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Oct 8 21:28:11 2018 +0200

    x86/kvm/nVMX: introduce source data cache for kvm_init_shadow_ept_mmu()
    
    MMU re-initialization is expensive, in particular,
    update_permission_bitmask() and update_pkru_bitmask() are.
    
    Cache the data used to setup shadow EPT MMU and avoid full re-init when
    it is unchanged.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 51ce635494b0..bf541af7442d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -283,7 +283,21 @@ union kvm_mmu_page_role {
 };
 
 union kvm_mmu_extended_role {
+/*
+ * This structure complements kvm_mmu_page_role caching everything needed for
+ * MMU configuration. If nothing in both these structures changed, MMU
+ * re-configuration can be skipped. @valid bit is set on first usage so we don't
+ * treat all-zero structure as valid data.
+ */
 	u32 word;
+	struct {
+		unsigned int valid:1;
+		unsigned int execonly:1;
+		unsigned int cr4_pse:1;
+		unsigned int cr4_pke:1;
+		unsigned int cr4_smap:1;
+		unsigned int cr4_smep:1;
+	};
 };
 
 union kvm_mmu_role {

commit 36d9594dfbf22a59adb986d85e0543886ab898f2
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Oct 8 21:28:10 2018 +0200

    x86/kvm/mmu: make space for source data caching in struct kvm_mmu
    
    In preparation to MMU reconfiguration avoidance we need a space to
    cache source data. As this partially intersects with kvm_mmu_page_role,
    create 64bit sized union kvm_mmu_role holding both base and extended data.
    No functional change.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 992bc1058170..51ce635494b0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -256,7 +256,7 @@ struct kvm_mmu_memory_cache {
  * @nxe, @cr0_wp, @smep_andnot_wp and @smap_andnot_wp.
  */
 union kvm_mmu_page_role {
-	unsigned word;
+	u32 word;
 	struct {
 		unsigned level:4;
 		unsigned cr4_pae:1;
@@ -282,6 +282,18 @@ union kvm_mmu_page_role {
 	};
 };
 
+union kvm_mmu_extended_role {
+	u32 word;
+};
+
+union kvm_mmu_role {
+	u64 as_u64;
+	struct {
+		union kvm_mmu_page_role base;
+		union kvm_mmu_extended_role ext;
+	};
+};
+
 struct kvm_rmap_head {
 	unsigned long val;
 };
@@ -369,7 +381,7 @@ struct kvm_mmu {
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
 	hpa_t root_hpa;
-	union kvm_mmu_page_role base_role;
+	union kvm_mmu_role mmu_role;
 	u8 root_level;
 	u8 shadow_root_level;
 	u8 ept_ad;

commit e173299101affc677db085b2894a43be4a17a94b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Oct 8 21:28:09 2018 +0200

    x86/kvm/mmu: get rid of redundant kvm_mmu_setup()
    
    Just inline the contents into the sole caller, kvm_init_mmu is now
    public.
    
    Suggested-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7448d0b744c9..992bc1058170 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1188,7 +1188,6 @@ void kvm_mmu_module_exit(void);
 
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
 int kvm_mmu_create(struct kvm_vcpu *vcpu);
-void kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_init_vm(struct kvm *kvm);
 void kvm_mmu_uninit_vm(struct kvm *kvm);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,

commit 14c07ad89f4d728a468caaea6a769c018c2b8dd6
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Oct 8 21:28:08 2018 +0200

    x86/kvm/mmu: introduce guest_mmu
    
    When EPT is used for nested guest we need to re-init MMU as shadow
    EPT MMU (nested_ept_init_mmu_context() does that). When we return back
    from L2 to L1 kvm_mmu_reset_context() in nested_vmx_load_cr3() resets
    MMU back to normal TDP mode. Add a special 'guest_mmu' so we can use
    separate root caches; the improved hit rate is not very important for
    single vCPU performance, but it avoids contention on the mmu_lock for
    many vCPUs.
    
    On the nested CPUID benchmark, with 16 vCPUs, an L2->L1->L2 vmexit
    goes from 42k to 26k cycles.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 586ef144e564..7448d0b744c9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -548,6 +548,9 @@ struct kvm_vcpu_arch {
 	/* Non-nested MMU for L1 */
 	struct kvm_mmu root_mmu;
 
+	/* L1 MMU when running nested */
+	struct kvm_mmu guest_mmu;
+
 	/*
 	 * Paging state of an L2 guest (used for nested npt)
 	 *

commit 6a82cd1c7b1e38d3b940fcb35a81e902dd52fb35
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Oct 8 21:28:07 2018 +0200

    x86/kvm/mmu.c: add kvm_mmu parameter to kvm_mmu_free_roots()
    
    Add an option to specify which MMU root we want to free. This will
    be used when nested and non-nested MMUs for L1 are split.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5e8f1a08b9d2..586ef144e564 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1339,7 +1339,8 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
-void kvm_mmu_free_roots(struct kvm_vcpu *vcpu, ulong roots_to_free);
+void kvm_mmu_free_roots(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
+			ulong roots_to_free);
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			   struct x86_exception *exception);
 gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,

commit 44dd3ffa7bb31126e0fc4f6f30398546eb494388
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Oct 8 21:28:05 2018 +0200

    x86/kvm/mmu: make vcpu->mmu a pointer to the current MMU
    
    As a preparation to full MMU split between L1 and L2 make vcpu->arch.mmu
    a pointer to the currently used mmu. For now, this is always
    vcpu->arch.root_mmu. No functional change.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 901572b4f6f7..5e8f1a08b9d2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -543,7 +543,10 @@ struct kvm_vcpu_arch {
 	 * the paging mode of the l1 guest. This context is always used to
 	 * handle faults.
 	 */
-	struct kvm_mmu mmu;
+	struct kvm_mmu *mmu;
+
+	/* Non-nested MMU for L1 */
+	struct kvm_mmu root_mmu;
 
 	/*
 	 * Paging state of an L2 guest (used for nested npt)

commit e6b6c483ebe9b3f82710fc39eff5531b4d80b089
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Oct 8 19:19:04 2018 +0200

    KVM: x86: hyperv: fix 'tlb_lush' typo
    
    Regardless of whether your TLB is lush or not it still needs flushing.
    
    Reported-by: Roman Kagan <rkagan@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d81f536d024a..901572b4f6f7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -499,7 +499,7 @@ struct kvm_vcpu_hv {
 	struct kvm_hyperv_exit exit;
 	struct kvm_vcpu_hv_stimer stimer[HV_SYNIC_STIMER_COUNT];
 	DECLARE_BITMAP(stimer_pending_bitmap, HV_SYNIC_STIMER_COUNT);
-	cpumask_t tlb_lush;
+	cpumask_t tlb_flush;
 };
 
 struct kvm_vcpu_arch {

commit 87ee613d076351950b74383215437f841ebbeb75
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Sep 26 19:02:56 2018 +0200

    KVM: x86: hyperv: keep track of mismatched VP indexes
    
    In most common cases VP index of a vcpu matches its vcpu index. Userspace
    is, however, free to set any mapping it wishes and we need to account for
    that when we need to find a vCPU with a particular VP index. To keep search
    algorithms optimal in both cases introduce 'num_mismatched_vp_indexes'
    counter showing how many vCPUs with mismatching VP index we have. In case
    the counter is zero we can assume vp_index == vcpu_idx.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ed4d7848ebf2..d81f536d024a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -790,6 +790,9 @@ struct kvm_hv {
 	u64 hv_reenlightenment_control;
 	u64 hv_tsc_emulation_control;
 	u64 hv_tsc_emulation_status;
+
+	/* How many vCPUs have VP index != vCPU index */
+	atomic_t num_mismatched_vp_indexes;
 };
 
 enum kvm_irqchip_mode {

commit 4fef0f491347785dfd4cca206b80f75c4ec2dc9f
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Fri Sep 28 22:08:50 2018 +0800

    KVM: x86: move definition PT_MAX_HUGEPAGE_LEVEL and KVM_NR_PAGE_SIZES together
    
    Currently, there are two definitions related to huge page, but a little bit
    far from each other and seems loosely connected:
    
     * KVM_NR_PAGE_SIZES defines the number of different size a page could map
     * PT_MAX_HUGEPAGE_LEVEL means the maximum level of huge page
    
    The number of different size a page could map equals the maximum level
    of huge page, which is implied by current definition.
    
    While current implementation may not be kind to readers and further
    developers:
    
     * KVM_NR_PAGE_SIZES looks like a stand alone definition at first sight
     * in case we need to support more level, two places need to change
    
    This patch tries to make these two definition more close, so that reader
    and developer would feel more comfortable to manipulate.
    
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 576ff47a79c4..ed4d7848ebf2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -102,7 +102,15 @@
 #define UNMAPPED_GVA (~(gpa_t)0)
 
 /* KVM Hugepage definitions for x86 */
-#define KVM_NR_PAGE_SIZES	3
+enum {
+	PT_PAGE_TABLE_LEVEL   = 1,
+	PT_DIRECTORY_LEVEL    = 2,
+	PT_PDPE_LEVEL         = 3,
+	/* set max level to the biggest one */
+	PT_MAX_HUGEPAGE_LEVEL = PT_PDPE_LEVEL,
+};
+#define KVM_NR_PAGE_SIZES	(PT_MAX_HUGEPAGE_LEVEL - \
+				 PT_PAGE_TABLE_LEVEL + 1)
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))

commit 3ff519f29d98ecdc1961d825d105d68711093b6b
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Thu Sep 6 05:58:16 2018 +0800

    KVM: x86: adjust kvm_mmu_page member to save 8 bytes
    
    On a 64bits machine, struct is naturally aligned with 8 bytes. Since
    kvm_mmu_page member *unsync* and *role* are less then 4 bytes, we can
    rearrange the sequence to compace the struct.
    
    As the comment shows, *role* and *gfn* are used to key the shadow page. In
    order to keep the comment valid, this patch moves the *unsync* up and
    exchange the position of *role* and *gfn*.
    
    From /proc/slabinfo, it shows the size of kvm_mmu_page is 8 bytes less and
    with one more object per slap after applying this patch.
    
        # name            <active_objs> <num_objs> <objsize> <objperslab>
        kvm_mmu_page_header      0           0       168         24
    
        kvm_mmu_page_header      0           0       160         25
    
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1c09a0d1771f..576ff47a79c4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -281,18 +281,18 @@ struct kvm_rmap_head {
 struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
+	bool unsync;
 
 	/*
 	 * The following two entries are used to key the shadow page in the
 	 * hash table.
 	 */
-	gfn_t gfn;
 	union kvm_mmu_page_role role;
+	gfn_t gfn;
 
 	u64 *spt;
 	/* hold the gfn of each spte inside spt */
 	gfn_t *gfns;
-	bool unsync;
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */

commit cfb634fe3052aefc4e1360fa322018c9a0b49755
Author: Jim Mattson <jmattson@google.com>
Date:   Fri Sep 21 10:36:17 2018 -0700

    KVM: nVMX: Clear reserved bits of #DB exit qualification
    
    According to volume 3 of the SDM, bits 63:15 and 12:4 of the exit
    qualification field for debug exceptions are reserved (cleared to
    0). However, the SDM is incorrect about bit 16 (corresponding to
    DR6.RTM). This bit should be set if a debug exception (#DB) or a
    breakpoint exception (#BP) occurred inside an RTM region while
    advanced debugging of RTM transactional regions was enabled. Note that
    this is the opposite of DR6.RTM, which "indicates (when clear) that a
    debug exception (#DB) or breakpoint exception (#BP) occurred inside an
    RTM region while advanced debugging of RTM transactional regions was
    enabled."
    
    There is still an issue with stale DR6 bits potentially being
    misreported for the current debug exception.  DR6 should not have been
    modified before vectoring the #DB exception, and the "new DR6 bits"
    should be available somewhere, but it was and they aren't.
    
    Fixes: b96fb439774e1 ("KVM: nVMX: fixes to nested virt interrupt injection")
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 09b2e3e2cf1b..1c09a0d1771f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -177,6 +177,7 @@ enum {
 
 #define DR6_BD		(1 << 13)
 #define DR6_BS		(1 << 14)
+#define DR6_BT		(1 << 15)
 #define DR6_RTM		(1 << 16)
 #define DR6_FIXED_1	0xfffe0ff0
 #define DR6_INIT	0xffff0ff0

commit 6fbbde9a1969dfb476467ebf69a475095ef3fd4d
Author: Drew Schmitt <dasch@google.com>
Date:   Mon Aug 20 10:32:15 2018 -0700

    KVM: x86: Control guest reads of MSR_PLATFORM_INFO
    
    Add KVM_CAP_MSR_PLATFORM_INFO so that userspace can disable guest access
    to reads of MSR_PLATFORM_INFO.
    
    Disabling access to reads of this MSR gives userspace the control to "expose"
    this platform-dependent information to guests in a clear way. As it exists
    today, guests that read this MSR would get unpopulated information if userspace
    hadn't already set it (and prior to this patch series, only the CPUID faulting
    information could have been populated). This existing interface could be
    confusing if guests don't handle the potential for incorrect/incomplete
    information gracefully (e.g. zero reported for base frequency).
    
    Signed-off-by: Drew Schmitt <dasch@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index af63c2ca1616..09b2e3e2cf1b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -869,6 +869,8 @@ struct kvm_arch {
 
 	bool x2apic_format;
 	bool x2apic_broadcast_quirk_disabled;
+
+	bool guest_can_read_msr_platform_info;
 };
 
 struct kvm_vm_stat {

commit e6c67d8cf1173b229f0c4343d1cc7925eca11c11
Author: Liran Alon <liran.alon@oracle.com>
Date:   Tue Sep 4 10:56:52 2018 +0300

    KVM: nVMX: Wake blocked vCPU in guest-mode if pending interrupt in virtual APICv
    
    In case L1 do not intercept L2 HLT or enter L2 in HLT activity-state,
    it is possible for a vCPU to be blocked while it is in guest-mode.
    
    According to Intel SDM 26.6.5 Interrupt-Window Exiting and
    Virtual-Interrupt Delivery: "These events wake the logical processor
    if it just entered the HLT state because of a VM entry".
    Therefore, if L1 enters L2 in HLT activity-state and L2 has a pending
    deliverable interrupt in vmcs12->guest_intr_status.RVI, then the vCPU
    should be waken from the HLT state and injected with the interrupt.
    
    In addition, if while the vCPU is blocked (while it is in guest-mode),
    it receives a nested posted-interrupt, then the vCPU should also be
    waken and injected with the posted interrupt.
    
    To handle these cases, this patch enhances kvm_vcpu_has_events() to also
    check if there is a pending interrupt in L2 virtual APICv provided by
    L1. That is, it evaluates if there is a pending virtual interrupt for L2
    by checking RVI[7:4] > VPPR[7:4] as specified in Intel SDM 29.2.1
    Evaluation of Pending Interrupts.
    
    Note that this also handles the case of nested posted-interrupt by the
    fact RVI is updated in vmx_complete_nested_posted_interrupt() which is
    called from kvm_vcpu_check_block() -> kvm_arch_vcpu_runnable() ->
    kvm_vcpu_running() -> vmx_check_nested_events() ->
    vmx_complete_nested_posted_interrupt().
    
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Darren Kenny <darren.kenny@oracle.com>
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bffb25b50425..af63c2ca1616 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1022,6 +1022,7 @@ struct kvm_x86_ops {
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);
+	bool (*guest_apic_has_interrupt)(struct kvm_vcpu *vcpu);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_apic_mode)(struct kvm_vcpu *vcpu);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);

commit d264ee0c2ed20c6a426663590d4fc7a36cb6abd7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Aug 27 15:21:12 2018 -0700

    KVM: VMX: use preemption timer to force immediate VMExit
    
    A VMX preemption timer value of '0' is guaranteed to cause a VMExit
    prior to the CPU executing any instructions in the guest.  Use the
    preemption timer (if it's supported) to trigger immediate VMExit
    in place of the current method of sending a self-IPI.  This ensures
    that pending VMExit injection to L1 occurs prior to executing any
    instructions in the guest (regardless of nesting level).
    
    When deferring VMExit injection, KVM generates an immediate VMExit
    from the (possibly nested) guest by sending itself an IPI.  Because
    hardware interrupts are blocked prior to VMEnter and are unblocked
    (in hardware) after VMEnter, this results in taking a VMExit(INTR)
    before any guest instruction is executed.  But, as this approach
    relies on the IPI being received before VMEnter executes, it only
    works as intended when KVM is running as L0.  Because there are no
    architectural guarantees regarding when IPIs are delivered, when
    running nested the INTR may "arrive" long after L2 is running e.g.
    L0 KVM doesn't force an immediate switch to L1 to deliver an INTR.
    
    For the most part, this unintended delay is not an issue since the
    events being injected to L1 also do not have architectural guarantees
    regarding their timing.  The notable exception is the VMX preemption
    timer[1], which is architecturally guaranteed to cause a VMExit prior
    to executing any instructions in the guest if the timer value is '0'
    at VMEnter.  Specifically, the delay in injecting the VMExit causes
    the preemption timer KVM unit test to fail when run in a nested guest.
    
    Note: this approach is viable even on CPUs with a broken preemption
    timer, as broken in this context only means the timer counts at the
    wrong rate.  There are no known errata affecting timer value of '0'.
    
    [1] I/O SMIs also have guarantees on when they arrive, but I have
        no idea if/how those are emulated in KVM.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    [Use a hook for SVM instead of leaving the default in x86.c - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8e90488c3d56..bffb25b50425 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1055,6 +1055,7 @@ struct kvm_x86_ops {
 	bool (*umip_emulated)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
+	void (*request_immediate_exit)(struct kvm_vcpu *vcpu);
 
 	void (*sched_in)(struct kvm_vcpu *kvm, int cpu);
 
@@ -1482,6 +1483,7 @@ extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 int kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu);
 int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
+void __kvm_request_immediate_exit(struct kvm_vcpu *vcpu);
 
 int kvm_is_in_guest(void);
 

commit bdf7ffc89922a52a4f08a12f7421ea24bb7626a0
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Thu Aug 30 10:03:30 2018 +0800

    KVM: LAPIC: Fix pv ipis out-of-bounds access
    
    Dan Carpenter reported that the untrusted data returns from kvm_register_read()
    results in the following static checker warning:
      arch/x86/kvm/lapic.c:576 kvm_pv_send_ipi()
      error: buffer underflow 'map->phys_map' 's32min-s32max'
    
    KVM guest can easily trigger this by executing the following assembly sequence
    in Ring0:
    
    mov $10, %rax
    mov $0xFFFFFFFF, %rbx
    mov $0xFFFFFFFF, %rdx
    mov $0, %rsi
    vmcall
    
    As this will cause KVM to execute the following code-path:
    vmx_handle_exit() -> handle_vmcall() -> kvm_emulate_hypercall() -> kvm_pv_send_ipi()
    which will reach out-of-bounds access.
    
    This patch fixes it by adding a check to kvm_pv_send_ipi() against map->max_apic_id,
    ignoring destinations that are not present and delivering the rest. We also check
    whether or not map->phys_map[min + i] is NULL since the max_apic_id is set to the
    max apic id, some phys_map maybe NULL when apic id is sparse, especially kvm
    unconditionally set max_apic_id to 255 to reserve enough space for any xAPIC ID.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    [Add second "if (min > map->max_apic_id)" to complete the fix. -Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3ad10f634d4c..8e90488c3d56 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1455,7 +1455,7 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event);
 void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu);
 
 int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
-    		    unsigned long ipi_bitmap_high, int min,
+		    unsigned long ipi_bitmap_high, u32 min,
 		    unsigned long icr, int op_64_bit);
 
 u64 kvm_get_arch_capabilities(void);

commit 564ad0aa85b3202311c4c8744fd1fdab4568d529
Merge: ed2ef2910064 df3190e22016
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Fri Sep 7 18:38:25 2018 +0200

    Merge tag 'kvm-arm-fixes-for-v4.19-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm
    
    Fixes for KVM/ARM for Linux v4.19 v2:
    
     - Fix a VFP corruption in 32-bit guest
     - Add missing cache invalidation for CoW pages
     - Two small cleanups

commit a35381e10dc46dd75e65e4b3832d9a0005d48d44
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Aug 23 10:18:14 2018 +0100

    KVM: Remove obsolete kvm_unmap_hva notifier backend
    
    kvm_unmap_hva is long gone, and we only have kvm_unmap_hva_range to
    deal with. Drop the now obsolete code.
    
    Fixes: fb1522e099f0 ("KVM: update to new mmu_notifier semantic v2")
    Cc: James Hogan <jhogan@kernel.org>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 00ddb0c9e612..e6a33420b871 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1450,7 +1450,6 @@ asmlinkage void kvm_spurious_fault(void);
 	____kvm_handle_fault_on_reboot(insn, "")
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
-int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);

commit c60658d1d983641fcdbb16f86bc2f3806d88bab4
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 23 13:56:53 2018 -0700

    KVM: x86: Unexport x86_emulate_instruction()
    
    Allowing x86_emulate_instruction() to be called directly has led to
    subtle bugs being introduced, e.g. not setting EMULTYPE_NO_REEXECUTE
    in the emulation type.  While most of the blame lies on re-execute
    being opt-out, exporting x86_emulate_instruction() also exposes its
    cr2 parameter, which may have contributed to commit d391f1207067
    ("x86/kvm/vmx: do not use vm-exit instruction length for fast MMIO
    when running nested") using x86_emulate_instruction() instead of
    emulate_instruction() because "hey, I have a cr2!", which in turn
    introduced its EMULTYPE_NO_REEXECUTE bug.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8c9023661351..e12916e7c2fb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1240,20 +1240,9 @@ enum emulation_result {
 #define EMULTYPE_ALLOW_RETRY	    (1 << 3)
 #define EMULTYPE_NO_UD_ON_FAIL	    (1 << 4)
 #define EMULTYPE_VMWARE		    (1 << 5)
-int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
-			    int emulation_type, void *insn, int insn_len);
-
-static inline int kvm_emulate_instruction(struct kvm_vcpu *vcpu,
-			int emulation_type)
-{
-	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
-}
-
-static inline int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
-						      void *insn, int insn_len)
-{
-	return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
-}
+int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type);
+int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
+					void *insn, int insn_len);
 
 void kvm_enable_efer_bits(u64);
 bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);

commit 0ce97a2b627c5e26347aee298f571ddf925e5fe4
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 23 13:56:52 2018 -0700

    KVM: x86: Rename emulate_instruction() to kvm_emulate_instruction()
    
    Lack of the kvm_ prefix gives the impression that it's a VMX or SVM
    specific function, and there's no conflict that prevents adding the
    kvm_ prefix.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 35e03b13edcb..8c9023661351 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1243,7 +1243,7 @@ enum emulation_result {
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 			    int emulation_type, void *insn, int insn_len);
 
-static inline int emulate_instruction(struct kvm_vcpu *vcpu,
+static inline int kvm_emulate_instruction(struct kvm_vcpu *vcpu,
 			int emulation_type)
 {
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);

commit 384bf2218e96f57118270945b1841e4dbbe9e352
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 23 13:56:49 2018 -0700

    KVM: x86: Merge EMULTYPE_RETRY and EMULTYPE_ALLOW_REEXECUTE
    
    retry_instruction() and reexecute_instruction() are a package deal,
    i.e. there is no scenario where one is allowed and the other is not.
    Merge their controlling emulation type flags to enforce this in code.
    Name the combined flag EMULTYPE_ALLOW_RETRY to make it abundantly
    clear that we are allowing re{try,execute} to occur, as opposed to
    explicitly requesting retry of a previously failed instruction.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a69ea11f3bab..35e03b13edcb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1237,10 +1237,9 @@ enum emulation_result {
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
-#define EMULTYPE_RETRY		    (1 << 3)
-#define EMULTYPE_ALLOW_REEXECUTE    (1 << 4)
-#define EMULTYPE_NO_UD_ON_FAIL	    (1 << 5)
-#define EMULTYPE_VMWARE		    (1 << 6)
+#define EMULTYPE_ALLOW_RETRY	    (1 << 3)
+#define EMULTYPE_NO_UD_ON_FAIL	    (1 << 4)
+#define EMULTYPE_VMWARE		    (1 << 5)
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 			    int emulation_type, void *insn, int insn_len);
 

commit 8065dbd1ee0ef04321d80da7999b4f0086e0a407
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 23 13:56:48 2018 -0700

    KVM: x86: Invert emulation re-execute behavior to make it opt-in
    
    Re-execution of an instruction after emulation decode failure is
    intended to be used only when emulating shadow page accesses.  Invert
    the flag to make allowing re-execution opt-in since that behavior is
    by far in the minority.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3b220b86c8e4..a69ea11f3bab 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1238,7 +1238,7 @@ enum emulation_result {
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_RETRY		    (1 << 3)
-#define EMULTYPE_NO_REEXECUTE	    (1 << 4)
+#define EMULTYPE_ALLOW_REEXECUTE    (1 << 4)
 #define EMULTYPE_NO_UD_ON_FAIL	    (1 << 5)
 #define EMULTYPE_VMWARE		    (1 << 6)
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
@@ -1247,15 +1247,13 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 			int emulation_type)
 {
-	return x86_emulate_instruction(vcpu, 0,
-			emulation_type | EMULTYPE_NO_REEXECUTE, NULL, 0);
+	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 
 static inline int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 						      void *insn, int insn_len)
 {
-	return x86_emulate_instruction(vcpu, 0, EMULTYPE_NO_REEXECUTE,
-				       insn, insn_len);
+	return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
 }
 
 void kvm_enable_efer_bits(u64);

commit 35be0aded76b54a24dc8aa678a71bca22273e8d8
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 23 13:56:47 2018 -0700

    KVM: x86: SVM: Set EMULTYPE_NO_REEXECUTE for RSM emulation
    
    Re-execution after an emulation decode failure is only intended to
    handle a case where two or vCPUs race to write a shadowed page, i.e.
    we should never re-execute an instruction as part of RSM emulation.
    
    Add a new helper, kvm_emulate_instruction_from_buffer(), to support
    emulating from a pre-defined buffer.  This eliminates the last direct
    call to x86_emulate_instruction() outside of kvm_mmu_page_fault(),
    which means x86_emulate_instruction() can be unexported in a future
    patch.
    
    Fixes: 7607b7174405 ("KVM: SVM: install RSM intercept")
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 00ddb0c9e612..3b220b86c8e4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1251,6 +1251,13 @@ static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 			emulation_type | EMULTYPE_NO_REEXECUTE, NULL, 0);
 }
 
+static inline int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
+						      void *insn, int insn_len)
+{
+	return x86_emulate_instruction(vcpu, 0, EMULTYPE_NO_REEXECUTE,
+				       insn, insn_len);
+}
+
 void kvm_enable_efer_bits(u64);
 bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);
 int kvm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);

commit e61cf2e3a5b452cfefcb145021f5a8ea88735cc1
Merge: 1009aa1205c2 28a1f3ac1d0c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 19 10:38:36 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull first set of KVM updates from Paolo Bonzini:
     "PPC:
       - minor code cleanups
    
      x86:
       - PCID emulation and CR3 caching for shadow page tables
       - nested VMX live migration
       - nested VMCS shadowing
       - optimized IPI hypercall
       - some optimizations
    
      ARM will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (85 commits)
      kvm: x86: Set highest physical address bits in non-present/reserved SPTEs
      KVM/x86: Use CC_SET()/CC_OUT in arch/x86/kvm/vmx.c
      KVM: X86: Implement PV IPIs in linux guest
      KVM: X86: Add kvm hypervisor init time platform setup callback
      KVM: X86: Implement "send IPI" hypercall
      KVM/x86: Move X86_CR4_OSXSAVE check into kvm_valid_sregs()
      KVM: x86: Skip pae_root shadow allocation if tdp enabled
      KVM/MMU: Combine flushing remote tlb in mmu_set_spte()
      KVM: vmx: skip VMWRITE of HOST_{FS,GS}_BASE when possible
      KVM: vmx: skip VMWRITE of HOST_{FS,GS}_SEL when possible
      KVM: vmx: always initialize HOST_{FS,GS}_BASE to zero during setup
      KVM: vmx: move struct host_state usage to struct loaded_vmcs
      KVM: vmx: compute need to reload FS/GS/LDT on demand
      KVM: nVMX: remove a misleading comment regarding vmcs02 fields
      KVM: vmx: rename __vmx_load_host_state() and vmx_save_host_state()
      KVM: vmx: add dedicated utility to access guest's kernel_gs_base
      KVM: vmx: track host_state.loaded using a loaded_vmcs pointer
      KVM: vmx: refactor segmentation code in vmx_save_host_state()
      kvm: nVMX: Fix fault priority for VMX operations
      kvm: nVMX: Fix fault vector for VMX operation at CPL > 0
      ...

commit 4180bf1b655a791a0a6ef93a2ffffc762722c782
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Jul 23 14:39:54 2018 +0800

    KVM: X86: Implement "send IPI" hypercall
    
    Using hypercall to send IPIs by one vmexit instead of one by one for
    xAPIC/x2APIC physical mode and one vmexit per-cluster for x2APIC cluster
    mode. Intel guest can enter x2apic cluster mode when interrupt remmaping
    is enabled in qemu, however, latest AMD EPYC still just supports xapic
    mode which can get great improvement by Exit-less IPIs. This patchset
    lets a guest send multicast IPIs, with at most 128 destinations per
    hypercall in 64-bit mode and 64 vCPUs per hypercall in 32-bit mode.
    
    Hardware: Xeon Skylake 2.5GHz, 2 sockets, 40 cores, 80 threads, the VM
    is 80 vCPUs, IPI microbenchmark(https://lkml.org/lkml/2017/12/19/141):
    
    x2apic cluster mode, vanilla
    
     Dry-run:                         0,            2392199 ns
     Self-IPI:                  6907514,           15027589 ns
     Normal IPI:              223910476,          251301666 ns
     Broadcast IPI:                   0,         9282161150 ns
     Broadcast lock:                  0,         8812934104 ns
    
    x2apic cluster mode, pv-ipi
    
     Dry-run:                         0,            2449341 ns
     Self-IPI:                  6720360,           15028732 ns
     Normal IPI:              228643307,          255708477 ns
     Broadcast IPI:                   0,         7572293590 ns  => 22% performance boost
     Broadcast lock:                  0,         8316124651 ns
    
    x2apic physical mode, vanilla
    
     Dry-run:                         0,            3135933 ns
     Self-IPI:                  8572670,           17901757 ns
     Normal IPI:              226444334,          255421709 ns
     Broadcast IPI:                   0,        19845070887 ns
     Broadcast lock:                  0,        19827383656 ns
    
    x2apic physical mode, pv-ipi
    
     Dry-run:                         0,            2446381 ns
     Self-IPI:                  6788217,           15021056 ns
     Normal IPI:              219454441,          249583458 ns
     Broadcast IPI:                   0,         7806540019 ns  => 154% performance boost
     Broadcast lock:                  0,         9143618799 ns
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 150937e64f63..c18958ef17d2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1457,6 +1457,10 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event);
 void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu);
 
+int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
+    		    unsigned long ipi_bitmap_high, int min,
+		    unsigned long icr, int op_64_bit);
+
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 

commit b08660e59dbdb600c55953787ed2265a0b510f77
Author: Tianyu Lan <Tianyu.Lan@microsoft.com>
Date:   Thu Jul 19 08:40:17 2018 +0000

    KVM: x86: Add tlb remote flush callback in kvm_x86_ops.
    
    This patch is to provide a way for platforms to register hv tlb remote
    flush callback and this helps to optimize operation of tlb flush
    among vcpus for nested virtualization case.
    
    Signed-off-by: Lan Tianyu <Tianyu.Lan@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 75ab578c3f6a..150937e64f63 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -985,6 +985,7 @@ struct kvm_x86_ops {
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu, bool invalidate_gpa);
+	int  (*tlb_remote_flush)(struct kvm *kvm);
 
 	/*
 	 * Flush any TLB entries associated with the given GVA.
@@ -1145,6 +1146,16 @@ static inline void kvm_arch_free_vm(struct kvm *kvm)
 	return kvm_x86_ops->vm_free(kvm);
 }
 
+#define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLB
+static inline int kvm_arch_flush_remote_tlb(struct kvm *kvm)
+{
+	if (kvm_x86_ops->tlb_remote_flush &&
+	    !kvm_x86_ops->tlb_remote_flush(kvm))
+		return 0;
+	else
+		return -ENOTSUPP;
+}
+
 int kvm_mmu_module_init(void);
 void kvm_mmu_module_exit(void);
 

commit 208320ba103e01fd2f3a7b81e97c9c5bc85f0612
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:21 2018 -0700

    kvm: x86: Remove CR3_PCID_INVD flag
    
    It is a duplicate of X86_CR3_PCID_NOFLUSH. So just use that instead.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4f1983640bda..75ab578c3f6a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -83,7 +83,6 @@
 			  | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM \
 			  | X86_CR0_NW | X86_CR0_CD | X86_CR0_PG))
 
-#define CR3_PCID_INVD		 BIT_64(63)
 #define CR4_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \

commit b94742c958f0b97d304d4aecb4603a20ee9a2df3
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:20 2018 -0700

    kvm: x86: Add multi-entry LRU cache for previous CR3s
    
    Adds support for storing multiple previous CR3/root_hpa pairs maintained
    as an LRU cache, so that the lockless CR3 switch path can be used when
    switching back to any of them.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dcdc9150bb76..4f1983640bda 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -335,6 +335,8 @@ struct kvm_mmu_root_info {
 #define KVM_MMU_ROOT_INFO_INVALID \
 	((struct kvm_mmu_root_info) { .cr3 = INVALID_PAGE, .hpa = INVALID_PAGE })
 
+#define KVM_MMU_NUM_PREV_ROOTS 3
+
 /*
  * x86 supports 4 paging modes (5-level 64-bit, 4-level 64-bit, 3-level 32-bit,
  * and 2-level 32-bit).  The kvm_mmu structure abstracts the details of the
@@ -363,7 +365,7 @@ struct kvm_mmu {
 	u8 shadow_root_level;
 	u8 ept_ad;
 	bool direct_map;
-	struct kvm_mmu_root_info prev_root;
+	struct kvm_mmu_root_info prev_roots[KVM_MMU_NUM_PREV_ROOTS];
 
 	/*
 	 * Bitmap; bit set = permission fault
@@ -1295,9 +1297,9 @@ static inline int __kvm_irq_line_state(unsigned long *irq_state,
 	return !!(*irq_state);
 }
 
-#define KVM_MMU_ROOT_CURRENT	BIT(0)
-#define KVM_MMU_ROOT_PREVIOUS	BIT(1)
-#define KVM_MMU_ROOTS_ALL	(~0UL)
+#define KVM_MMU_ROOT_CURRENT		BIT(0)
+#define KVM_MMU_ROOT_PREVIOUS(i)	BIT(1+i)
+#define KVM_MMU_ROOTS_ALL		(~0UL)
 
 int kvm_pic_set_irq(struct kvm_pic *pic, int irq, int irq_source_id, int level);
 void kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);

commit faff87588d8bfd9e56e9203412f0bb80455da7b9
Author: Junaid Shahid <junaids@google.com>
Date:   Fri Jun 29 13:10:05 2018 -0700

    kvm: x86: Flush only affected TLB entries in kvm_mmu_invlpg*
    
    This needs a minor bug fix. The updated patch is as follows.
    
    Thanks,
    Junaid
    
    ------------------------------------------------------------------------------
    
    kvm_mmu_invlpg() and kvm_mmu_invpcid_gva() only need to flush the TLB
    entries for the specific guest virtual address, instead of flushing all
    TLB entries associated with the VM.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 262b0bc64dfc..dcdc9150bb76 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -985,6 +985,14 @@ struct kvm_x86_ops {
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu, bool invalidate_gpa);
 
+	/*
+	 * Flush any TLB entries associated with the given GVA.
+	 * Does not need to flush GPA->HPA mappings.
+	 * Can potentially get non-canonical addresses through INVLPGs, which
+	 * the implementation may choose to ignore if appropriate.
+	 */
+	void (*tlb_flush_gva)(struct kvm_vcpu *vcpu, gva_t addr);
+
 	void (*run)(struct kvm_vcpu *vcpu);
 	int (*handle_exit)(struct kvm_vcpu *vcpu);
 	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);

commit 08fb59d8a47d5e1f9de08659603a47f117fe60d5
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:17 2018 -0700

    kvm: x86: Support selectively freeing either current or previous MMU root
    
    kvm_mmu_free_roots() now takes a mask specifying which roots to free, so
    that either one of the roots (active/previous) can be individually freed
    when needed.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0b77c233e441..262b0bc64dfc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1287,6 +1287,10 @@ static inline int __kvm_irq_line_state(unsigned long *irq_state,
 	return !!(*irq_state);
 }
 
+#define KVM_MMU_ROOT_CURRENT	BIT(0)
+#define KVM_MMU_ROOT_PREVIOUS	BIT(1)
+#define KVM_MMU_ROOTS_ALL	(~0UL)
+
 int kvm_pic_set_irq(struct kvm_pic *pic, int irq, int irq_source_id, int level);
 void kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);
 
@@ -1298,7 +1302,7 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
-void kvm_mmu_free_roots(struct kvm_vcpu *vcpu, bool free_prev_root);
+void kvm_mmu_free_roots(struct kvm_vcpu *vcpu, ulong roots_to_free);
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			   struct x86_exception *exception);
 gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,

commit 7eb77e9f5fcf652a21b2d12bff1cd509b6b14f21
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:16 2018 -0700

    kvm: x86: Add a root_hpa parameter to kvm_mmu->invlpg()
    
    This allows invlpg() to be called using either the active root_hpa
    or the prev_root_hpa.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8b4aa5e7ff92..0b77c233e441 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -354,7 +354,7 @@ struct kvm_mmu {
 			       struct x86_exception *exception);
 	int (*sync_page)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp);
-	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
+	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva, hpa_t root_hpa);
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
 	hpa_t root_hpa;

commit ade61e2824443a208bb3aaafd8b345ce878298cd
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:15 2018 -0700

    kvm: x86: Skip TLB flush on fast CR3 switch when indicated by guest
    
    When PCIDs are enabled, the MSb of the source operand for a MOV-to-CR3
    instruction indicates that the TLB doesn't need to be flushed.
    
    This change enables this optimization for MOV-to-CR3s in the guest
    that have been intercepted by KVM for shadow paging and are handled
    within the fast CR3 switch path.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1c253f15f9cd..8b4aa5e7ff92 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1318,7 +1318,7 @@ int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u64 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
-void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3);
+void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush);
 
 void kvm_enable_tdp(void);
 void kvm_disable_tdp(void);

commit eb4b248e152d3ecf189b9d32c04961360dbd938a
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:14 2018 -0700

    kvm: vmx: Support INVPCID in shadow paging mode
    
    Implement support for INVPCID in shadow paging mode as well.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c2b4df8a03cd..1c253f15f9cd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1317,6 +1317,7 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u64 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
+void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3);
 
 void kvm_enable_tdp(void);

commit 6e42782f516f05c8030f63308f2457681b1c9919
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:08 2018 -0700

    kvm: x86: Introduce KVM_REQ_LOAD_CR3
    
    The KVM_REQ_LOAD_CR3 request loads the hardware CR3 using the
    current root_hpa.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 290b7d05790a..c2b4df8a03cd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -54,6 +54,7 @@
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
+#define KVM_REQ_LOAD_CR3		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)

commit 7c390d350f8b677df3236afef4ced80dba6c3201
Author: Junaid Shahid <junaids@google.com>
Date:   Wed Jun 27 14:59:06 2018 -0700

    kvm: x86: Add fast CR3 switch code path
    
    When using shadow paging, a CR3 switch in the guest results in a VM Exit.
    In the common case, that VM exit doesn't require much processing by KVM.
    However, it does acquire the MMU lock, which can start showing signs of
    contention under some workloads even on a 2 VCPU VM when the guest is
    using KPTI. Therefore, we add a fast path that avoids acquiring the MMU
    lock in the most common cases e.g. when switching back and forth between
    the kernel and user mode CR3s used by KPTI with no guest page table
    changes in between.
    
    For now, this fast path is implemented only for 64-bit guests and hosts
    to avoid the handling of PDPTEs, but it can be extended later to 32-bit
    guests and/or hosts as well.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bd287b348751..290b7d05790a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -326,6 +326,14 @@ struct rsvd_bits_validate {
 	u64 bad_mt_xwr;
 };
 
+struct kvm_mmu_root_info {
+	gpa_t cr3;
+	hpa_t hpa;
+};
+
+#define KVM_MMU_ROOT_INFO_INVALID \
+	((struct kvm_mmu_root_info) { .cr3 = INVALID_PAGE, .hpa = INVALID_PAGE })
+
 /*
  * x86 supports 4 paging modes (5-level 64-bit, 4-level 64-bit, 3-level 32-bit,
  * and 2-level 32-bit).  The kvm_mmu structure abstracts the details of the
@@ -354,6 +362,7 @@ struct kvm_mmu {
 	u8 shadow_root_level;
 	u8 ept_ad;
 	bool direct_map;
+	struct kvm_mmu_root_info prev_root;
 
 	/*
 	 * Bitmap; bit set = permission fault
@@ -1288,7 +1297,7 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
-void kvm_mmu_free_roots(struct kvm_vcpu *vcpu);
+void kvm_mmu_free_roots(struct kvm_vcpu *vcpu, bool free_prev_root);
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			   struct x86_exception *exception);
 gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,
@@ -1307,7 +1316,7 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u64 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
-void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu);
+void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3);
 
 void kvm_enable_tdp(void);
 void kvm_disable_tdp(void);

commit 8fcc4b5923af5de58b80b53a069453b135693304
Author: Jim Mattson <jmattson@google.com>
Date:   Tue Jul 10 11:27:20 2018 +0200

    kvm: nVMX: Introduce KVM_CAP_NESTED_STATE
    
    For nested virtualization L0 KVM is managing a bit of state for L2 guests,
    this state can not be captured through the currently available IOCTLs. In
    fact the state captured through all of these IOCTLs is usually a mix of L1
    and L2 state. It is also dependent on whether the L2 guest was running at
    the moment when the process was interrupted to save its state.
    
    With this capability, there are two new vcpu ioctls: KVM_GET_NESTED_STATE
    and KVM_SET_NESTED_STATE. These can be used for saving and restoring a VM
    that is in VMX operation.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Jim Mattson <jmattson@google.com>
    [karahmed@ - rename structs and functions and make them ready for AMD and
                 address previous comments.
               - handle nested.smm state.
               - rebase & a bit of refactoring.
               - Merge 7/8 and 8/8 into one patch. ]
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index da957725992d..bd287b348751 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1086,6 +1086,12 @@ struct kvm_x86_ops {
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
 
+	int (*get_nested_state)(struct kvm_vcpu *vcpu,
+				struct kvm_nested_state __user *user_kvm_nested_state,
+				unsigned user_data_size);
+	int (*set_nested_state)(struct kvm_vcpu *vcpu,
+				struct kvm_nested_state __user *user_kvm_nested_state,
+				struct kvm_nested_state *kvm_state);
 	void (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
 
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);

commit 7f7f1ba33cf2c21d001821313088c231db42ff40
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jul 18 18:49:01 2018 +0200

    KVM: x86: do not load vmcs12 pages while still in SMM
    
    If the vCPU enters system management mode while running a nested guest,
    RSM starts processing the vmentry while still in SMM.  In that case,
    however, the pages pointed to by the vmcs12 might be incorrectly
    loaded from SMRAM.  To avoid this, delay the handling of the pages
    until just before the next vmentry.  This is done with a new request
    and a new entry in kvm_x86_ops, which we will be able to reuse for
    nested VMX state migration.
    
    Extracted from a patch by Jim Mattson and KarimAllah Ahmed.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c13cd28d9d1b..da957725992d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -75,6 +75,7 @@
 #define KVM_REQ_HV_EXIT			KVM_ARCH_REQ(21)
 #define KVM_REQ_HV_STIMER		KVM_ARCH_REQ(22)
 #define KVM_REQ_LOAD_EOI_EXITMAP	KVM_ARCH_REQ(23)
+#define KVM_REQ_GET_VMCS12_PAGES	KVM_ARCH_REQ(24)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
@@ -1085,6 +1086,8 @@ struct kvm_x86_ops {
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
 
+	void (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
+
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);

commit 5b76a3cff011df2dcb6186c965a2e4d809a05ad4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun Aug 5 16:07:47 2018 +0200

    KVM: VMX: Tell the nested hypervisor to skip L1D flush on vmentry
    
    When nested virtualization is in use, VMENTER operations from the nested
    hypervisor into the nested guest will always be processed by the bare metal
    hypervisor, and KVM's "conditional cache flushes" mode in particular does a
    flush on nested vmentry.  Therefore, include the "skip L1D flush on
    vmentry" bit in KVM's suggested ARCH_CAPABILITIES setting.
    
    Add the relevant Documentation.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 37749429afd9..acebb808c4b5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1418,6 +1418,7 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event);
 void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu);
 
+u64 kvm_get_arch_capabilities(void);
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 

commit 447ae316670230d7d29430e2cbf1f5db4f49d14c
Author: Nicolai Stange <nstange@suse.de>
Date:   Sun Jul 29 12:15:33 2018 +0200

    x86: Don't include linux/irq.h from asm/hardirq.h
    
    The next patch in this series will have to make the definition of
    irq_cpustat_t available to entering_irq().
    
    Inclusion of asm/hardirq.h into asm/apic.h would cause circular header
    dependencies like
    
      asm/smp.h
        asm/apic.h
          asm/hardirq.h
            linux/irq.h
              linux/topology.h
                linux/smp.h
                  asm/smp.h
    
    or
    
      linux/gfp.h
        linux/mmzone.h
          asm/mmzone.h
            asm/mmzone_64.h
              asm/smp.h
                asm/apic.h
                  asm/hardirq.h
                    linux/irq.h
                      linux/irqdesc.h
                        linux/kobject.h
                          linux/sysfs.h
                            linux/kernfs.h
                              linux/idr.h
                                linux/gfp.h
    
    and others.
    
    This causes compilation errors because of the header guards becoming
    effective in the second inclusion: symbols/macros that had been defined
    before wouldn't be available to intermediate headers in the #include chain
    anymore.
    
    A possible workaround would be to move the definition of irq_cpustat_t
    into its own header and include that from both, asm/hardirq.h and
    asm/apic.h.
    
    However, this wouldn't solve the real problem, namely asm/harirq.h
    unnecessarily pulling in all the linux/irq.h cruft: nothing in
    asm/hardirq.h itself requires it. Also, note that there are some other
    archs, like e.g. arm64, which don't have that #include in their
    asm/hardirq.h.
    
    Remove the linux/irq.h #include from x86' asm/hardirq.h.
    
    Fix resulting compilation errors by adding appropriate #includes to *.c
    files as needed.
    
    Note that some of these *.c files could be cleaned up a bit wrt. to their
    set of #includes, but that should better be done from separate patches, if
    at all.
    
    Signed-off-by: Nicolai Stange <nstange@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 57d418061c55..37749429afd9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -17,6 +17,7 @@
 #include <linux/tracepoint.h>
 #include <linux/cpumask.h>
 #include <linux/irq_work.h>
+#include <linux/irq.h>
 
 #include <linux/kvm.h>
 #include <linux/kvm_para.h>

commit c595ceee45707f00f64f61c54fb64ef0cc0b4e85
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jul 2 13:07:14 2018 +0200

    x86/KVM/VMX: Add L1D flush logic
    
    Add the logic for flushing L1D on VMENTER. The flush depends on the static
    key being enabled and the new l1tf_flush_l1d flag being set.
    
    The flags is set:
     - Always, if the flush module parameter is 'always'
    
     - Conditionally at:
       - Entry to vcpu_run(), i.e. after executing user space
    
       - From the sched_in notifier, i.e. when switching to a vCPU thread.
    
       - From vmexit handlers which are considered unsafe, i.e. where
         sensitive data can be brought into L1D:
    
         - The emulator, which could be a good target for other speculative
           execution-based threats,
    
         - The MMU, which can bring host page tables in the L1 cache.
    
         - External interrupts
    
         - Nested operations that require the MMU (see above). That is
           vmptrld, vmptrst, vmclear,vmwrite,vmread.
    
         - When handling invept,invvpid
    
    [ tglx: Split out from combo patch and reduced to a single flag ]
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c13cd28d9d1b..57d418061c55 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -713,6 +713,9 @@ struct kvm_vcpu_arch {
 
 	/* be preempted when it's in kernel-mode(cpl=0) */
 	bool preempted_in_kernel;
+
+	/* Flush the L1 Data cache for L1TF mitigation on VMENTER */
+	bool l1tf_flush_l1d;
 };
 
 struct kvm_lpage_info {
@@ -881,6 +884,7 @@ struct kvm_vcpu_stat {
 	u64 signal_exits;
 	u64 irq_window_exits;
 	u64 nmi_window_exits;
+	u64 l1d_flush;
 	u64 halt_exits;
 	u64 halt_successful_poll;
 	u64 halt_attempted_poll;

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit e2f11f42824bf2d906468a94888718ae24bf0270
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed May 16 17:21:29 2018 +0200

    KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
    
    Implement HvFlushVirtualAddress{List,Space} hypercalls in a simplistic way:
    do full TLB flush with KVM_REQ_TLB_FLUSH and kick vCPUs which are currently
    IN_GUEST_MODE.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b27de80f5870..0ebe659f2802 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -477,6 +477,7 @@ struct kvm_vcpu_hv {
 	struct kvm_hyperv_exit exit;
 	struct kvm_vcpu_hv_stimer stimer[HV_SYNIC_STIMER_COUNT];
 	DECLARE_BITMAP(stimer_pending_bitmap, HV_SYNIC_STIMER_COUNT);
+	cpumask_t tlb_lush;
 };
 
 struct kvm_vcpu_arch {

commit bc226f07dcd3c9ef0b7f6236fe356ea4a9cb4769
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Thu May 10 22:06:39 2018 +0200

    KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD
    
    Expose the new virtualized architectural mechanism, VIRT_SSBD, for using
    speculative store bypass disable (SSBD) under SVM.  This will allow guests
    to use SSBD on hardware that uses non-architectural mechanisms for enabling
    SSBD.
    
    [ tglx: Folded the migration fixup from Paolo Bonzini ]
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c25775fad4ed..f4b2588865e9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -924,7 +924,7 @@ struct kvm_x86_ops {
 	int (*hardware_setup)(void);               /* __init */
 	void (*hardware_unsetup)(void);            /* __exit */
 	bool (*cpu_has_accelerated_tpr)(void);
-	bool (*cpu_has_high_real_mode_segbase)(void);
+	bool (*has_emulated_msr)(int index);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
 	struct kvm *(*vm_alloc)(void);

commit 1313cc2bd8f6568dd8801feef446afbe43e6d313
Author: Jim Mattson <jmattson@google.com>
Date:   Wed May 9 17:02:04 2018 -0400

    kvm: mmu: Add guest_mode to kvm_mmu_page_role
    
    L1 and L2 need to have disjoint mappings, so that L1's APIC access
    page (under VMX) can be omitted from L2's mappings.
    
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 187c8e09a019..b27de80f5870 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -258,7 +258,8 @@ union kvm_mmu_page_role {
 		unsigned smep_andnot_wp:1;
 		unsigned smap_andnot_wp:1;
 		unsigned ad_disabled:1;
-		unsigned :7;
+		unsigned guest_mode:1;
+		unsigned :6;
 
 		/*
 		 * This is left at the top of the word so that

commit 8d860bbeedef97fe981d28fa7b71d77f3b29563f
Author: Jim Mattson <jmattson@google.com>
Date:   Wed May 9 16:56:05 2018 -0400

    kvm: vmx: Basic APIC virtualization controls have three settings
    
    Previously, we toggled between SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE
    and SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES, depending on whether or
    not the EXTD bit was set in MSR_IA32_APICBASE. However, if the local
    APIC is disabled, we should not set either of these APIC
    virtualization control bits.
    
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8cb846162694..187c8e09a019 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -995,7 +995,7 @@ struct kvm_x86_ops {
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
-	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
+	void (*set_virtual_apic_mode)(struct kvm_vcpu *vcpu);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	int (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);

commit 74b566e6cf21f07df385d593a7fcf8bbfc5d3f0f
Author: Junaid Shahid <junaids@google.com>
Date:   Fri May 4 11:37:11 2018 -0700

    kvm: x86: Refactor mmu_free_roots()
    
    Extract the logic to free a root page in a separate function to avoid code
    duplication in mmu_free_roots(). Also, change it to an exported function
    i.e. kvm_mmu_free_roots().
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c25775fad4ed..8cb846162694 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1277,6 +1277,7 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
+void kvm_mmu_free_roots(struct kvm_vcpu *vcpu);
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			   struct x86_exception *exception);
 gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,

commit e79f245ddec17bbd89d73cd0169dba4be46c9b55
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Sat Apr 14 05:10:52 2018 +0200

    X86/KVM: Properly update 'tsc_offset' to represent the running guest
    
    Update 'tsc_offset' on vmentry/vmexit of L2 guests to ensure that it always
    captures the TSC_OFFSET of the running guest whether it is the L1 or L2
    guest.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    [AMD changes, fix update_ia32_tsc_adjust_msr. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 949c977bc4c9..c25775fad4ed 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1013,6 +1013,7 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
+	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);

commit 04140b4144cd888c080cddbb2be2ec603f00d081
Author: Liran Alon <liran.alon@oracle.com>
Date:   Fri Mar 23 03:01:31 2018 +0300

    KVM: x86: Rename interrupt.pending to interrupt.injected
    
    For exceptions & NMIs events, KVM code use the following
    coding convention:
    *) "pending" represents an event that should be injected to guest at
    some point but it's side-effects have not yet occurred.
    *) "injected" represents an event that it's side-effects have already
    occurred.
    
    However, interrupts don't conform to this coding convention.
    All current code flows mark interrupt.pending when it's side-effects
    have already taken place (For example, bit moved from LAPIC IRR to
    ISR). Therefore, it makes sense to just rename
    interrupt.pending to interrupt.injected.
    
    This change follows logic of previous commit 664f8e26b00c ("KVM: X86:
    Fix loss of exception which has not yet been injected") which changed
    exception to follow this coding convention as well.
    
    It is important to note that in case !lapic_in_kernel(vcpu),
    interrupt.pending usage was and still incorrect.
    In this case, interrrupt.pending can only be set using one of the
    following ioctls: KVM_INTERRUPT, KVM_SET_VCPU_EVENTS and
    KVM_SET_SREGS. Looking at how QEMU uses these ioctls, one can see that
    QEMU uses them either to re-set an "interrupt.pending" state it has
    received from KVM (via KVM_GET_VCPU_EVENTS interrupt.pending or
    via KVM_GET_SREGS interrupt_bitmap) or by dispatching a new interrupt
    from QEMU's emulated LAPIC which reset bit in IRR and set bit in ISR
    before sending ioctl to KVM. So it seems that indeed "interrupt.pending"
    in this case is also suppose to represent "interrupt.injected".
    However, kvm_cpu_has_interrupt() & kvm_cpu_has_injectable_intr()
    is misusing (now named) interrupt.injected in order to return if
    there is a pending interrupt.
    This leads to nVMX/nSVM not be able to distinguish if it should exit
    from L2 to L1 on EXTERNAL_INTERRUPT on pending interrupt or should
    re-inject an injected interrupt.
    Therefore, add a FIXME at these functions for handling this issue.
    
    This patch introduce no semantics change.
    
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 74b5b3e518df..949c977bc4c9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -574,7 +574,7 @@ struct kvm_vcpu_arch {
 	} exception;
 
 	struct kvm_queued_interrupt {
-		bool pending;
+		bool injected;
 		bool soft;
 		u8 nr;
 	} interrupt;

commit 5a485803221777013944cbd1a7cd5c62efba3ffa
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 20 15:02:05 2018 +0100

    x86/hyper-v: move hyperv.h out of uapi
    
    hyperv.h is not part of uapi, there are no (known) users outside of kernel.
    We are making changes to this file to match current Hyper-V Hypervisor
    Top-Level Functional Specification (TLFS, see:
    https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/reference/tlfs)
    and we don't want to maintain backwards compatibility.
    
    Move the file renaming to hyperv-tlfs.h to avoid confusing it with
    mshyperv.h. In future, all definitions from TLFS should go to it and
    all kernel objects should go to mshyperv.h or include/linux/hyperv.h.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 38b4080b29c2..74b5b3e518df 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -34,6 +34,7 @@
 #include <asm/msr-index.h>
 #include <asm/asm.h>
 #include <asm/kvm_page_track.h>
+#include <asm/hyperv-tlfs.h>
 
 #define KVM_MAX_VCPUS 288
 #define KVM_SOFT_MAX_VCPUS 240

commit 81811c162d4da1ececef14a1efc9602e86d29ef5
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Mar 20 12:17:21 2018 -0700

    KVM: SVM: add struct kvm_svm to hold SVM specific KVM vars
    
    Add struct kvm_svm, which is analagous to struct vcpu_svm, along with
    a helper to_kvm_svm() to retrieve kvm_svm from a struct kvm *.  Move
    the SVM specific variables and struct definitions out of kvm_arch
    and into kvm_svm.
    
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1e1a49c1f4fe..38b4080b29c2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -770,15 +770,6 @@ enum kvm_irqchip_mode {
 	KVM_IRQCHIP_SPLIT,        /* created with KVM_CAP_SPLIT_IRQCHIP */
 };
 
-struct kvm_sev_info {
-	bool active;		/* SEV enabled guest */
-	unsigned int asid;	/* ASID used for this guest */
-	unsigned int handle;	/* SEV firmware handle */
-	int fd;			/* SEV device fd */
-	unsigned long pages_locked; /* Number of pages locked */
-	struct list_head regions_list;  /* List of registered regions */
-};
-
 struct kvm_arch {
 	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;
@@ -857,17 +848,8 @@ struct kvm_arch {
 
 	bool disabled_lapic_found;
 
-	/* Struct members for AVIC */
-	u32 avic_vm_id;
-	u32 ldr_mode;
-	struct page *avic_logical_id_table_page;
-	struct page *avic_physical_id_table_page;
-	struct hlist_node hnode;
-
 	bool x2apic_format;
 	bool x2apic_broadcast_quirk_disabled;
-
-	struct kvm_sev_info sev_info;
 };
 
 struct kvm_vm_stat {

commit 40bbb9d03f05d08b10a7ec1a5c229e1a3f3fc3a9
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Mar 20 12:17:20 2018 -0700

    KVM: VMX: add struct kvm_vmx to hold VMX specific KVM vars
    
    Add struct kvm_vmx, which wraps struct kvm, and a helper to_kvm_vmx()
    that retrieves 'struct kvm_vmx *' from 'struct kvm *'.  Move the VMX
    specific variables out of kvm_arch and into kvm_vmx.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5f17a2386460..1e1a49c1f4fe 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -808,7 +808,6 @@ struct kvm_arch {
 	struct mutex apic_map_lock;
 	struct kvm_apic_map *apic_map;
 
-	unsigned int tss_addr;
 	bool apic_access_page_done;
 
 	gpa_t wall_clock;
@@ -817,9 +816,6 @@ struct kvm_arch {
 	bool hlt_in_guest;
 	bool pause_in_guest;
 
-	bool ept_identity_pagetable_done;
-	gpa_t ept_identity_map_addr;
-
 	unsigned long irq_sources_bitmap;
 	s64 kvmclock_offset;
 	raw_spinlock_t tsc_write_lock;

commit 2ac52ab861b920b56e349244e14f957bb349010e
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Mar 20 12:17:19 2018 -0700

    KVM: x86: move setting of ept_identity_map_addr to vmx.c
    
    Add kvm_x86_ops->set_identity_map_addr and set ept_identity_map_addr
    in VMX specific code so that ept_identity_map_addr can be moved out
    of 'struct kvm_arch' in a future patch.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 94dcc55d8af7..5f17a2386460 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1021,6 +1021,7 @@ struct kvm_x86_ops {
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	int (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
+	int (*set_identity_map_addr)(struct kvm *kvm, u64 ident_addr);
 	int (*get_tdp_level)(struct kvm_vcpu *vcpu);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	int (*get_lpage_level)(void);

commit 434a1e94469d3b603f1efabfb044182de4cf88ef
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Mar 20 12:17:18 2018 -0700

    KVM: x86: define SVM/VMX specific kvm_arch_[alloc|free]_vm
    
    Define kvm_arch_[alloc|free]_vm in x86 as pass through functions
    to new kvm_x86_ops vm_alloc and vm_free, and move the current
    allocation logic as-is to SVM and VMX.  Vendor specific alloc/free
    functions set the stage for SVM/VMX wrappers of 'struct kvm',
    which will allow us to move the growing number of SVM/VMX specific
    member variables out of 'struct kvm_arch'.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c72891dd7d78..94dcc55d8af7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -948,6 +948,8 @@ struct kvm_x86_ops {
 	bool (*cpu_has_high_real_mode_segbase)(void);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
+	struct kvm *(*vm_alloc)(void);
+	void (*vm_free)(struct kvm *);
 	int (*vm_init)(struct kvm *kvm);
 	void (*vm_destroy)(struct kvm *kvm);
 
@@ -1121,6 +1123,17 @@ struct kvm_arch_async_pf {
 
 extern struct kvm_x86_ops *kvm_x86_ops;
 
+#define __KVM_HAVE_ARCH_VM_ALLOC
+static inline struct kvm *kvm_arch_alloc_vm(void)
+{
+	return kvm_x86_ops->vm_alloc();
+}
+
+static inline void kvm_arch_free_vm(struct kvm *kvm)
+{
+	return kvm_x86_ops->vm_free(kvm);
+}
+
 int kvm_mmu_module_init(void);
 void kvm_mmu_module_exit(void);
 

commit e40ff1d6608dd9a5e07d7bc3079c64d9d676fe15
Author: Liran Alon <liran.alon@oracle.com>
Date:   Wed Mar 21 02:50:31 2018 +0200

    KVM: nVMX: Do not load EOI-exitmap while running L2
    
    When L1 IOAPIC redirection-table is written, a request of
    KVM_REQ_SCAN_IOAPIC is set on all vCPUs. This is done such that
    all vCPUs will now recalc their IOAPIC handled vectors and load
    it to their EOI-exitmap.
    
    However, it could be that one of the vCPUs is currently running
    L2. In this case, load_eoi_exitmap() will be called which would
    write to vmcs02->eoi_exit_bitmap, which is wrong because
    vmcs02->eoi_exit_bitmap should always be equal to
    vmcs12->eoi_exit_bitmap. Furthermore, at this point
    KVM_REQ_SCAN_IOAPIC was already consumed and therefore we will
    never update vmcs01->eoi_exit_bitmap. This could lead to remote_irr
    of some IOAPIC level-triggered entry to remain set forever.
    
    Fix this issue by delaying the load of EOI-exitmap to when vCPU
    is running L1.
    
    One may wonder why not just delay entire KVM_REQ_SCAN_IOAPIC
    processing to when vCPU is running L1. This is done in order to handle
    correctly the case where LAPIC & IO-APIC of L1 is pass-throughed into
    L2. In this case, vmcs12->virtual_interrupt_delivery should be 0. In
    current nVMX implementation, that results in
    vmcs02->virtual_interrupt_delivery to also be 0. Thus,
    vmcs02->eoi_exit_bitmap is not used. Therefore, every L2 EOI cause
    a #VMExit into L0 (either on MSR_WRITE to x2APIC MSR or
    APIC_ACCESS/APIC_WRITE/EPT_MISCONFIG to APIC MMIO page).
    In order for such L2 EOI to be broadcasted, if needed, from LAPIC
    to IO-APIC, vcpu->arch.ioapic_handled_vectors must be updated
    while L2 is running. Therefore, patch makes sure to delay only the
    loading of EOI-exitmap but not the update of
    vcpu->arch.ioapic_handled_vectors.
    
    Reviewed-by: Arbel Moshe <arbel.moshe@oracle.com>
    Reviewed-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8fb60287d987..c72891dd7d78 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -73,6 +73,7 @@
 #define KVM_REQ_HV_RESET		KVM_ARCH_REQ(20)
 #define KVM_REQ_HV_EXIT			KVM_ARCH_REQ(21)
 #define KVM_REQ_HV_STIMER		KVM_ARCH_REQ(22)
+#define KVM_REQ_LOAD_EOI_EXITMAP	KVM_ARCH_REQ(23)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
@@ -498,6 +499,7 @@ struct kvm_vcpu_arch {
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
 	bool apicv_active;
+	bool load_eoi_exitmap_pending;
 	DECLARE_BITMAP(ioapic_handled_vectors, 256);
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;

commit b31c114b82b2b55913d2cf744e6a665c2ca090ac
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Mar 12 04:53:04 2018 -0700

    KVM: X86: Provide a capability to disable PAUSE intercepts
    
    Allow to disable pause loop exit/pause filtering on a per VM basis.
    
    If some VMs have dedicated host CPUs, they won't be negatively affected
    due to needlessly intercepted PAUSE instructions.
    
    Thanks to Jan H. Schönherr's initial patch.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 31f418669827..8fb60287d987 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -813,6 +813,7 @@ struct kvm_arch {
 
 	bool mwait_in_guest;
 	bool hlt_in_guest;
+	bool pause_in_guest;
 
 	bool ept_identity_pagetable_done;
 	gpa_t ept_identity_map_addr;

commit caa057a2cad647fb368a12c8e6c410ac4c28e063
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Mar 12 04:53:03 2018 -0700

    KVM: X86: Provide a capability to disable HLT intercepts
    
    If host CPUs are dedicated to a VM, we can avoid VM exits on HLT.
    This patch adds the per-VM capability to disable them.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a85b640aee1e..31f418669827 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -812,6 +812,7 @@ struct kvm_arch {
 	gpa_t wall_clock;
 
 	bool mwait_in_guest;
+	bool hlt_in_guest;
 
 	bool ept_identity_pagetable_done;
 	gpa_t ept_identity_map_addr;

commit 4d5422cea3b61f158d58924cbb43feada456ba5c
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Mar 12 04:53:02 2018 -0700

    KVM: X86: Provide a capability to disable MWAIT intercepts
    
    Allowing a guest to execute MWAIT without interception enables a guest
    to put a (physical) CPU into a power saving state, where it takes
    longer to return from than what may be desired by the host.
    
    Don't give a guest that power over a host by default. (Especially,
    since nothing prevents a guest from using MWAIT even when it is not
    advertised via CPUID.)
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 480a75b22b69..a85b640aee1e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -811,6 +811,8 @@ struct kvm_arch {
 
 	gpa_t wall_clock;
 
+	bool mwait_in_guest;
+
 	bool ept_identity_pagetable_done;
 	gpa_t ept_identity_map_addr;
 

commit 04789b6664a60474aeb8b07a9a94d923a217690e
Author: Liran Alon <liran.alon@oracle.com>
Date:   Mon Mar 12 13:12:50 2018 +0200

    KVM: x86: Emulate only IN/OUT instructions when accessing VMware backdoor
    
    Access to VMware backdoor ports is done by one of the IN/OUT/INS/OUTS
    instructions. These ports must be allowed access even if TSS I/O
    permission bitmap don't allow it.
    
    To handle this, VMX/SVM will be changed in future commits
    to intercept #GP which was raised by such access and
    handle it by calling x86 emulator to emulate instruction.
    If it was one of these instructions, the x86 emulator already handles
    it correctly (Since commit "KVM: x86: Always allow access to VMware
    backdoor I/O ports") by not checking these ports against TSS I/O
    permission bitmap.
    
    One may wonder why checking for specific instructions is necessary
    as we can just forward all #GPs to the x86 emulator.
    There are multiple reasons for doing so:
    
    1. We don't want the x86 emulator to be reached easily
    by guest by just executing an instruction that raises #GP as that
    exposes the x86 emulator as a bigger attack surface.
    
    2. The x86 emulator is incomplete and therefore certain instructions
    that can cause #GP cannot be emulated. Such an example is "INT x"
    (opcode 0xcd) which reaches emulate_int() which can only emulate
    the instruction if vCPU is in real-mode.
    
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 41f32268a9a2..480a75b22b69 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1194,6 +1194,7 @@ enum emulation_result {
 #define EMULTYPE_RETRY		    (1 << 3)
 #define EMULTYPE_NO_REEXECUTE	    (1 << 4)
 #define EMULTYPE_NO_UD_ON_FAIL	    (1 << 5)
+#define EMULTYPE_VMWARE		    (1 << 6)
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 			    int emulation_type, void *insn, int insn_len);
 

commit e23661712005fd01ad9d2bca6eb4a122b79c8b0b
Author: Liran Alon <liran.alon@oracle.com>
Date:   Mon Mar 12 13:12:49 2018 +0200

    KVM: x86: Add emulation_type to not raise #UD on emulation failure
    
    Next commits are going introduce support for accessing VMware backdoor
    ports even though guest's TSS I/O permissions bitmap doesn't allow
    access. This mimic VMware hypervisor behavior.
    
    In order to support this, next commits will change VMX/SVM to
    intercept #GP which was raised by such access and handle it by calling
    the x86 emulator to emulate instruction. Since commit "KVM: x86:
    Always allow access to VMware backdoor I/O ports", the x86 emulator
    handles access to these I/O ports by not checking these ports against
    the TSS I/O permission bitmap.
    
    However, there could be cases that CPU rasies a #GP on instruction
    that fails to be disassembled by the x86 emulator (Because of
    incomplete implementation for example).
    
    In those cases, we would like the #GP intercept to just forward #GP
    as-is to guest as if there was no intercept to begin with.
    However, current emulator code always queues #UD exception in case
    emulator fails (including disassembly failures) which is not what is
    wanted in this flow.
    
    This commit addresses this issue by adding a new emulation_type flag
    that will allow the #GP intercept handler to specify that it wishes
    to be aware when instruction emulation fails and doesn't want #UD
    exception to be queued.
    
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dc0ae1bbec96..41f32268a9a2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1193,6 +1193,7 @@ enum emulation_result {
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_RETRY		    (1 << 3)
 #define EMULTYPE_NO_REEXECUTE	    (1 << 4)
+#define EMULTYPE_NO_UD_ON_FAIL	    (1 << 5)
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 			    int emulation_type, void *insn, int insn_len);
 

commit dca7f1284f49911b459da313ece5d9c6dc3291f6
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Mar 8 08:57:27 2018 -0800

    KVM: x86: add kvm_fast_pio() to consolidate fast PIO code
    
    Add kvm_fast_pio() to consolidate duplicate code in VMX and SVM.
    Unexport kvm_fast_pio_in() and kvm_fast_pio_out().
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0395c354a504..dc0ae1bbec96 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1210,8 +1210,7 @@ int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
 
 struct x86_emulate_ctxt;
 
-int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
-int kvm_fast_pio_in(struct kvm_vcpu *vcpu, int size, unsigned short port);
+int kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in);
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
 int kvm_vcpu_halt(struct kvm_vcpu *vcpu);

commit a2e164e7f45ab21742b2e32c0195b699ae2ebfc0
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Mar 1 15:15:12 2018 +0100

    x86/kvm/hyper-v: add reenlightenment MSRs support
    
    Nested Hyper-V/Windows guest running on top of KVM will use TSC page
    clocksource in two cases:
    - L0 exposes invariant TSC (CPUID.80000007H:EDX[8]).
    - L0 provides Hyper-V Reenlightenment support (CPUID.40000003H:EAX[13]).
    
    Exposing invariant TSC effectively blocks migration to hosts with different
    TSC frequencies, providing reenlightenment support will be needed when we
    start migrating nested workloads.
    
    Implement rudimentary support for reenlightenment MSRs. For now, these are
    just read/write MSRs with no effect.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index df6720fc57e6..0395c354a504 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -756,6 +756,10 @@ struct kvm_hv {
 	HV_REFERENCE_TSC_PAGE tsc_ref;
 
 	struct idr conn_to_evt;
+
+	u64 hv_reenlightenment_control;
+	u64 hv_tsc_emulation_control;
+	u64 hv_tsc_emulation_status;
 };
 
 enum kvm_irqchip_mode {

commit faeb7833eee0d6afe0ecb6bdfa6042556c2c352e
Author: Roman Kagan <rkagan@virtuozzo.com>
Date:   Thu Feb 1 16:48:32 2018 +0300

    kvm: x86: hyperv: guest->host event signaling via eventfd
    
    In Hyper-V, the fast guest->host notification mechanism is the
    SIGNAL_EVENT hypercall, with a single parameter of the connection ID to
    signal.
    
    Currently this hypercall incurs a user exit and requires the userspace
    to decode the parameters and trigger the notification of the potentially
    different I/O context.
    
    To avoid the costly user exit, process this hypercall and signal the
    corresponding eventfd in KVM, similar to ioeventfd.  The association
    between the connection id and the eventfd is established via the newly
    introduced KVM_HYPERV_EVENTFD ioctl, and maintained in an
    (srcu-protected) IDR.
    
    Signed-off-by: Roman Kagan <rkagan@virtuozzo.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [asm/hyperv.h changes approved by KY Srinivasan. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b605a5b6a30c..df6720fc57e6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -754,6 +754,8 @@ struct kvm_hv {
 	u64 hv_crash_ctl;
 
 	HV_REFERENCE_TSC_PAGE tsc_ref;
+
+	struct idr conn_to_evt;
 };
 
 enum kvm_irqchip_mode {

commit 518e7b94817abed94becfe6a44f1ece0d4745afe
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Wed Feb 28 14:03:31 2018 +0800

    KVM: X86: Allow userspace to define the microcode version
    
    Linux (among the others) has checks to make sure that certain features
    aren't enabled on a certain family/model/stepping if the microcode version
    isn't greater than or equal to a known good version.
    
    By exposing the real microcode version, we're preventing buggy guests that
    don't check that they are running virtualized (i.e., they should trust the
    hypervisor) from disabling features that are effectively not buggy.
    
    Suggested-by: Filippo Sironi <sironi@amazon.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bab0694b35c3..b605a5b6a30c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -507,6 +507,7 @@ struct kvm_vcpu_arch {
 	u64 smi_count;
 	bool tpr_access_reporting;
 	u64 ia32_xss;
+	u64 microcode_version;
 
 	/*
 	 * Paging state of the vcpu

commit 801e459a6f3a63af9d447e6249088c76ae16efc4
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Wed Feb 21 13:39:51 2018 -0600

    KVM: x86: Add a framework for supporting MSR-based features
    
    Provide a new KVM capability that allows bits within MSRs to be recognized
    as features.  Two new ioctls are added to the /dev/kvm ioctl routine to
    retrieve the list of these MSRs and then retrieve their values. A kvm_x86_ops
    callback is used to determine support for the listed MSR-based features.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [Tweaked documentation. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0a9e330b34f0..bab0694b35c3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1095,6 +1095,8 @@ struct kvm_x86_ops {
 	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
 	int (*mem_enc_reg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
 	int (*mem_enc_unreg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
+
+	int (*get_msr_feature)(struct kvm_msr_entry *entry);
 };
 
 struct kvm_arch_async_pf {

commit f75e4924f0152be747bf04c9d16bb23fd8baf5f9
Author: Sebastian Ott <sebott@linux.vnet.ibm.com>
Date:   Thu Feb 22 13:04:39 2018 +0100

    kvm: fix warning for non-x86 builds
    
    Fix the following sparse warning by moving the prototype
    of kvm_arch_mmu_notifier_invalidate_range() to linux/kvm_host.h .
    
      CHECK   arch/s390/kvm/../../../virt/kvm/kvm_main.c
    arch/s390/kvm/../../../virt/kvm/kvm_main.c:138:13: warning: symbol 'kvm_arch_mmu_notifier_invalidate_range' was not declared. Should it be static?
    
    Signed-off-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dd6f57a54a26..0a9e330b34f0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1464,7 +1464,4 @@ static inline int kvm_cpu_get_apicid(int mps_cpu)
 #define put_smstate(type, buf, offset, val)                      \
 	*(type *)((buf) + (offset) - 0x7e00) = val
 
-void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
-		unsigned long start, unsigned long end);
-
 #endif /* _ASM_X86_KVM_HOST_H */

commit 87cedc6be55954c6efd6eca2e694132513f65a2a
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Fri Jan 26 17:34:08 2018 +0800

    kvm: x86: remove efer_reload entry in kvm_vcpu_stat
    
    The efer_reload is never used since
    commit 26bb0981b3ff ("KVM: VMX: Use shared msr infrastructure"),
    so remove it.
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ea7e40e9c1f0..dd6f57a54a26 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -895,7 +895,6 @@ struct kvm_vcpu_stat {
 	u64 request_irq_exits;
 	u64 irq_exits;
 	u64 host_state_reload;
-	u64 efer_reload;
 	u64 fpu_reload;
 	u64 insn_emulation;
 	u64 insn_emulation_fail;

commit 65e38583c3bbbba78a081c808e2d58a8454a821e
Merge: 476b7adaa327 00b10fe1046c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jan 16 16:34:48 2018 +0100

    Merge branch 'sev-v9-p2' of https://github.com/codomania/kvm
    
    This part of Secure Encrypted Virtualization (SEV) patch series focuses on KVM
    changes required to create and manage SEV guests.
    
    SEV is an extension to the AMD-V architecture which supports running encrypted
    virtual machine (VMs) under the control of a hypervisor. Encrypted VMs have their
    pages (code and data) secured such that only the guest itself has access to
    unencrypted version. Each encrypted VM is associated with a unique encryption key;
    if its data is accessed to a different entity using a different key the encrypted
    guest's data will be incorrectly decrypted, leading to unintelligible data.
    This security model ensures that hypervisor will no longer able to inspect or
    alter any guest code or data.
    
    The key management of this feature is handled by a separate processor known as
    the AMD Secure Processor (AMD-SP) which is present on AMD SOCs. The SEV Key
    Management Specification (see below) provides a set of commands which can be
    used by hypervisor to load virtual machine keys through the AMD-SP driver.
    
    The patch series adds a new ioctl in KVM driver (KVM_MEMORY_ENCRYPT_OP). The
    ioctl will be used by qemu to issue SEV guest-specific commands defined in Key
    Management Specification.
    
    The following links provide additional details:
    
    AMD Memory Encryption white paper:
    http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2013/12/AMD_Memory_Encryption_Whitepaper_v7-Public.pdf
    
    AMD64 Architecture Programmer's Manual:
        http://support.amd.com/TechDocs/24593.pdf
        SME is section 7.10
        SEV is section 15.34
    
    SEV Key Management:
    http://support.amd.com/TechDocs/55766_SEV-KM API_Specification.pdf
    
    KVM Forum Presentation:
    http://www.linux-kvm.org/images/7/74/02x08A-Thomas_Lendacky-AMDs_Virtualizatoin_Memory_Encryption_Technology.pdf
    
    SEV Guest BIOS support:
      SEV support has been add to EDKII/OVMF BIOS
      https://github.com/tianocore/edk2
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit c2ba05ccfde2f069a66c0462e5b5ef8a517dcc9c
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Tue Dec 12 17:33:03 2017 -0800

    KVM: X86: introduce invalidate_gpa argument to tlb flush
    
    Introduce a new bool invalidate_gpa argument to kvm_x86_ops->tlb_flush,
    it will be used by later patches to just flush guest tlb.
    
    For VMX, this will use INVVPID instead of INVEPT, which will invalidate
    combined mappings while keeping guest-physical mappings.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 340e3604dcc7..44de261e9223 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -966,7 +966,7 @@ struct kvm_x86_ops {
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
 
-	void (*tlb_flush)(struct kvm_vcpu *vcpu);
+	void (*tlb_flush)(struct kvm_vcpu *vcpu, bool invalidate_gpa);
 
 	void (*run)(struct kvm_vcpu *vcpu);
 	int (*handle_exit)(struct kvm_vcpu *vcpu);

commit 52797bf9a875c4a30f846196386684e646e08a91
Author: Liran Alon <liran.alon@oracle.com>
Date:   Wed Nov 15 13:43:14 2017 +0200

    KVM: x86: Add emulation of MSR_SMI_COUNT
    
    This MSR returns the number of #SMIs that occurred on CPU since
    boot.
    
    It was seen to be used frequently by ESXi guest.
    
    Patch adds a new vcpu-arch specific var called smi_count to
    save the number of #SMIs which occurred on CPU since boot.
    It is exposed as a read-only MSR to guest (causing #GP
    on wrmsr) in RDMSR/WRMSR emulation code.
    MSR_SMI_COUNT is also added to emulated_msrs[] to make sure
    user-space can save/restore it for migration purposes.
    
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Bhavesh Davda <bhavesh.davda@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 515db75081d1..340e3604dcc7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -504,6 +504,7 @@ struct kvm_vcpu_arch {
 	int mp_state;
 	u64 ia32_misc_enable_msr;
 	u64 smbase;
+	u64 smi_count;
 	bool tpr_access_reporting;
 	u64 ia32_xss;
 

commit 66336cab3531d3325ebde36a04725dddd0c42cb5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jul 12 10:36:41 2016 +0200

    KVM: x86: add support for emulating UMIP
    
    The User-Mode Instruction Prevention feature present in recent Intel
    processor prevents a group of instructions (sgdt, sidt, sldt, smsw, and
    str) from being executed with CPL > 0. Otherwise, a general protection
    fault is issued.
    
    UMIP instructions in general are also able to trigger vmexits, so we can
    actually emulate UMIP on older processors.  This commit sets up the
    infrastructure so that kvm-intel.ko and kvm-amd.ko can set the UMIP
    feature bit for CPUID even if the feature is not actually available
    in hardware.
    
    Reviewed-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ff79134d1d71..515db75081d1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1017,6 +1017,7 @@ struct kvm_x86_ops {
 	void (*handle_external_intr)(struct kvm_vcpu *vcpu);
 	bool (*mpx_supported)(void);
 	bool (*xsaves_supported)(void);
+	bool (*umip_emulated)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
 

commit ae3e61e1c28338d077b704505570fa181df1e41f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jul 12 10:36:41 2016 +0200

    KVM: x86: add support for UMIP
    
    Add the CPUID bits, make the CR4.UMIP bit not reserved anymore, and
    add UMIP support for instructions that are already emulated by KVM.
    
    Reviewed-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 516798431328..ff79134d1d71 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -86,7 +86,7 @@
 			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR | X86_CR4_PCIDE \
 			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_FSGSBASE \
 			  | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_VMXE \
-			  | X86_CR4_SMAP | X86_CR4_PKE))
+			  | X86_CR4_SMAP | X86_CR4_PKE | X86_CR4_UMIP))
 
 #define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)
 

commit b1394e745b9453dcb5b0671c205b770e87dedb87
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Nov 30 19:05:45 2017 +0100

    KVM: x86: fix APIC page invalidation
    
    Implementation of the unpinned APIC page didn't update the VMCS address
    cache when invalidation was done through range mmu notifiers.
    This became a problem when the page notifier was removed.
    
    Re-introduce the arch-specific helper and call it from ...range_start.
    
    Reported-by: Fabian Grünbichler <f.gruenbichler@proxmox.com>
    Fixes: 38b9917350cb ("kvm: vmx: Implement set_apic_access_page_addr")
    Fixes: 369ea8242c0f ("mm/rmap: update to new mmu_notifier semantic v2")
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Andrea Arcangeli <aarcange@redhat.com>
    Tested-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Tested-by: Fabian Grünbichler <f.gruenbichler@proxmox.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 62527e053ee4..516798431328 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1448,4 +1448,7 @@ static inline int kvm_cpu_get_apicid(int mps_cpu)
 #define put_smstate(type, buf, offset, val)                      \
 	*(type *)((buf) + (offset) - 0x7e00) = val
 
+void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
+		unsigned long start, unsigned long end);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit f775b13eedee2f7f3c6fdd4e90fb79090ce5d339
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Nov 14 16:54:23 2017 -0500

    x86,kvm: move qemu/guest FPU switching out to vcpu_run
    
    Currently, every time a VCPU is scheduled out, the host kernel will
    first save the guest FPU/xstate context, then load the qemu userspace
    FPU context, only to then immediately save the qemu userspace FPU
    context back to memory. When scheduling in a VCPU, the same extraneous
    FPU loads and saves are done.
    
    This could be avoided by moving from a model where the guest FPU is
    loaded and stored with preemption disabled, to a model where the
    qemu userspace FPU is swapped out for the guest FPU context for
    the duration of the KVM_RUN ioctl.
    
    This is done under the VCPU mutex, which is also taken when other
    tasks inspect the VCPU FPU context, so the code should already be
    safe for this change. That should come as no surprise, given that
    s390 already has this optimization.
    
    This can fix a bug where KVM calls get_user_pages while owning the
    FPU, and the file system ends up requesting the FPU again:
    
        [258270.527947]  __warn+0xcb/0xf0
        [258270.527948]  warn_slowpath_null+0x1d/0x20
        [258270.527951]  kernel_fpu_disable+0x3f/0x50
        [258270.527953]  __kernel_fpu_begin+0x49/0x100
        [258270.527955]  kernel_fpu_begin+0xe/0x10
        [258270.527958]  crc32c_pcl_intel_update+0x84/0xb0
        [258270.527961]  crypto_shash_update+0x3f/0x110
        [258270.527968]  crc32c+0x63/0x8a [libcrc32c]
        [258270.527975]  dm_bm_checksum+0x1b/0x20 [dm_persistent_data]
        [258270.527978]  node_prepare_for_write+0x44/0x70 [dm_persistent_data]
        [258270.527985]  dm_block_manager_write_callback+0x41/0x50 [dm_persistent_data]
        [258270.527988]  submit_io+0x170/0x1b0 [dm_bufio]
        [258270.527992]  __write_dirty_buffer+0x89/0x90 [dm_bufio]
        [258270.527994]  __make_buffer_clean+0x4f/0x80 [dm_bufio]
        [258270.527996]  __try_evict_buffer+0x42/0x60 [dm_bufio]
        [258270.527998]  dm_bufio_shrink_scan+0xc0/0x130 [dm_bufio]
        [258270.528002]  shrink_slab.part.40+0x1f5/0x420
        [258270.528004]  shrink_node+0x22c/0x320
        [258270.528006]  do_try_to_free_pages+0xf5/0x330
        [258270.528008]  try_to_free_pages+0xe9/0x190
        [258270.528009]  __alloc_pages_slowpath+0x40f/0xba0
        [258270.528011]  __alloc_pages_nodemask+0x209/0x260
        [258270.528014]  alloc_pages_vma+0x1f1/0x250
        [258270.528017]  do_huge_pmd_anonymous_page+0x123/0x660
        [258270.528021]  handle_mm_fault+0xfd3/0x1330
        [258270.528025]  __get_user_pages+0x113/0x640
        [258270.528027]  get_user_pages+0x4f/0x60
        [258270.528063]  __gfn_to_pfn_memslot+0x120/0x3f0 [kvm]
        [258270.528108]  try_async_pf+0x66/0x230 [kvm]
        [258270.528135]  tdp_page_fault+0x130/0x280 [kvm]
        [258270.528149]  kvm_mmu_page_fault+0x60/0x120 [kvm]
        [258270.528158]  handle_ept_violation+0x91/0x170 [kvm_intel]
        [258270.528162]  vmx_handle_exit+0x1ca/0x1400 [kvm_intel]
    
    No performance changes were detected in quick ping-pong tests on
    my 4 socket system, which is expected since an FPU+xstate load is
    on the order of 0.1us, while ping-ponging between CPUs is on the
    order of 20us, and somewhat noisy.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Suggested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [Fixed a bug where reset_vcpu called put_fpu without preceding load_fpu,
     which happened inside from KVM_CREATE_VCPU ioctl. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 977de5fb968b..62527e053ee4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -536,7 +536,20 @@ struct kvm_vcpu_arch {
 	struct kvm_mmu_memory_cache mmu_page_cache;
 	struct kvm_mmu_memory_cache mmu_page_header_cache;
 
+	/*
+	 * QEMU userspace and the guest each have their own FPU state.
+	 * In vcpu_run, we switch between the user and guest FPU contexts.
+	 * While running a VCPU, the VCPU thread will have the guest FPU
+	 * context.
+	 *
+	 * Note that while the PKRU state lives inside the fpu registers,
+	 * it is switched out separately at VMENTER and VMEXIT time. The
+	 * "guest_fpu" state here contains the guest FPU context, with the
+	 * host PRKU bits.
+	 */
+	struct fpu user_fpu;
 	struct fpu guest_fpu;
+
 	u64 xcr0;
 	u64 guest_supported_xcr0;
 	u32 guest_xstate_size;

commit 1e80fdc09d121d8327cdf62eefbb5abadddca792
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Mon Dec 4 10:57:38 2017 -0600

    KVM: SVM: Pin guest memory when SEV is active
    
    The SEV memory encryption engine uses a tweak such that two identical
    plaintext pages at different location will have different ciphertext.
    So swapping or moving ciphertext of two pages will not result in
    plaintext being swapped. Relocating (or migrating) physical backing
    pages for a SEV guest will require some additional steps. The current SEV
    key management spec does not provide commands to swap or migrate (move)
    ciphertext pages. For now, we pin the guest memory registered through
    KVM_MEMORY_ENCRYPT_REG_REGION ioctl.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a0b021f1fd05..262950f9f2d9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -753,6 +753,7 @@ struct kvm_sev_info {
 	unsigned int handle;	/* SEV firmware handle */
 	int fd;			/* SEV device fd */
 	unsigned long pages_locked; /* Number of pages locked */
+	struct list_head regions_list;  /* List of registered regions */
 };
 
 struct kvm_arch {

commit 89c5058090528026f6542e8b12f4262e492bd3a2
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Mon Dec 4 10:57:35 2017 -0600

    KVM: SVM: Add support for KVM_SEV_LAUNCH_UPDATE_DATA command
    
    The command is used for encrypting the guest memory region using the VM
    encryption key (VEK) created during KVM_SEV_LAUNCH_START.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Improvements-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0ea890375532..a0b021f1fd05 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -752,6 +752,7 @@ struct kvm_sev_info {
 	unsigned int asid;	/* ASID used for this guest */
 	unsigned int handle;	/* SEV firmware handle */
 	int fd;			/* SEV device fd */
+	unsigned long pages_locked; /* Number of pages locked */
 };
 
 struct kvm_arch {

commit 59414c989220825f970f38dbcbf11f18e817d73c
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Mon Dec 4 10:57:35 2017 -0600

    KVM: SVM: Add support for KVM_SEV_LAUNCH_START command
    
    The KVM_SEV_LAUNCH_START command is used to create a memory encryption
    context within the SEV firmware. In order to do so, the guest owner
    should provide the guest's policy, its public Diffie-Hellman (PDH) key
    and session information. The command implements the LAUNCH_START flow
    defined in SEV spec Section 6.2.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Improvements-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 384dcfda43cc..0ea890375532 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -750,6 +750,8 @@ enum kvm_irqchip_mode {
 struct kvm_sev_info {
 	bool active;		/* SEV enabled guest */
 	unsigned int asid;	/* ASID used for this guest */
+	unsigned int handle;	/* SEV firmware handle */
+	int fd;			/* SEV device fd */
 };
 
 struct kvm_arch {

commit 1654efcbc431a369397a20bf85e45870d15c8689
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Mon Dec 4 10:57:34 2017 -0600

    KVM: SVM: Add KVM_SEV_INIT command
    
    The command initializes the SEV platform context and allocates a new ASID
    for this guest from the SEV ASID pool. The firmware must be initialized
    before we issue any guest launch commands to create a new memory encryption
    context.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 58b7cc30466b..384dcfda43cc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -747,6 +747,11 @@ enum kvm_irqchip_mode {
 	KVM_IRQCHIP_SPLIT,        /* created with KVM_CAP_SPLIT_IRQCHIP */
 };
 
+struct kvm_sev_info {
+	bool active;		/* SEV enabled guest */
+	unsigned int asid;	/* ASID used for this guest */
+};
+
 struct kvm_arch {
 	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;
@@ -834,6 +839,8 @@ struct kvm_arch {
 
 	bool x2apic_format;
 	bool x2apic_broadcast_quirk_disabled;
+
+	struct kvm_sev_info sev_info;
 };
 
 struct kvm_vm_stat {

commit 69eaedee411c1fc1cf123520897a96b7cf04d8a0
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Mon Dec 4 10:57:26 2017 -0600

    KVM: Introduce KVM_MEMORY_ENCRYPT_{UN,}REG_REGION ioctl
    
    If hardware supports memory encryption then KVM_MEMORY_ENCRYPT_REG_REGION
    and KVM_MEMORY_ENCRYPT_UNREG_REGION ioctl's can be used by userspace to
    register/unregister the guest memory regions which may contain the encrypted
    data (e.g guest RAM, PCI BAR, SMRAM etc).
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Improvements-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c87e214d55df..58b7cc30466b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1068,6 +1068,8 @@ struct kvm_x86_ops {
 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
 
 	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
+	int (*mem_enc_reg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
+	int (*mem_enc_unreg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
 };
 
 struct kvm_arch_async_pf {

commit 5acc5c063196b4a531a761a954023c1848ec832b
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Mon Dec 4 10:57:26 2017 -0600

    KVM: Introduce KVM_MEMORY_ENCRYPT_OP ioctl
    
    If the hardware supports memory encryption then the
    KVM_MEMORY_ENCRYPT_OP ioctl can be used by qemu to issue a platform
    specific memory encryption commands.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1bfb99770c34..c87e214d55df 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1066,6 +1066,8 @@ struct kvm_x86_ops {
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
+
+	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
 };
 
 struct kvm_arch_async_pf {

commit 9b8ae63798cb97e785a667ff27e43fa6220cb734
Author: Liran Alon <liran.alon@oracle.com>
Date:   Sun Nov 5 16:56:34 2017 +0200

    KVM: x86: Don't re-execute instruction when not passing CR2 value
    
    In case of instruction-decode failure or emulation failure,
    x86_emulate_instruction() will call reexecute_instruction() which will
    attempt to use the cr2 value passed to x86_emulate_instruction().
    However, when x86_emulate_instruction() is called from
    emulate_instruction(), cr2 is not passed (passed as 0) and therefore
    it doesn't make sense to execute reexecute_instruction() logic at all.
    
    Fixes: 51d8b66199e9 ("KVM: cleanup emulate_instruction")
    
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1bfb99770c34..977de5fb968b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1161,7 +1161,8 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 			int emulation_type)
 {
-	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	return x86_emulate_instruction(vcpu, 0,
+			emulation_type | EMULTYPE_NO_REEXECUTE, NULL, 0);
 }
 
 void kvm_enable_efer_bits(u64);

commit 974aa5630b318938273d7efe7a2cf031c7b927db
Merge: 441692aafc17 a6014f1ab708
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 13:00:24 2017 -0800

    Merge tag 'kvm-4.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "First batch of KVM changes for 4.15
    
      Common:
       - Python 3 support in kvm_stat
       - Accounting of slabs to kmemcg
    
      ARM:
       - Optimized arch timer handling for KVM/ARM
       - Improvements to the VGIC ITS code and introduction of an ITS reset
         ioctl
       - Unification of the 32-bit fault injection logic
       - More exact external abort matching logic
    
      PPC:
       - Support for running hashed page table (HPT) MMU mode on a host that
         is using the radix MMU mode; single threaded mode on POWER 9 is
         added as a pre-requisite
       - Resolution of merge conflicts with the last second 4.14 HPT fixes
       - Fixes and cleanups
    
      s390:
       - Some initial preparation patches for exitless interrupts and crypto
       - New capability for AIS migration
       - Fixes
    
      x86:
       - Improved emulation of LAPIC timer mode changes, MCi_STATUS MSRs,
         and after-reset state
       - Refined dependencies for VMX features
       - Fixes for nested SMI injection
       - A lot of cleanups"
    
    * tag 'kvm-4.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (89 commits)
      KVM: s390: provide a capability for AIS state migration
      KVM: s390: clear_io_irq() requests are not expected for adapter interrupts
      KVM: s390: abstract conversion between isc and enum irq_types
      KVM: s390: vsie: use common code functions for pinning
      KVM: s390: SIE considerations for AP Queue virtualization
      KVM: s390: document memory ordering for kvm_s390_vcpu_wakeup
      KVM: PPC: Book3S HV: Cosmetic post-merge cleanups
      KVM: arm/arm64: fix the incompatible matching for external abort
      KVM: arm/arm64: Unify 32bit fault injection
      KVM: arm/arm64: vgic-its: Implement KVM_DEV_ARM_ITS_CTRL_RESET
      KVM: arm/arm64: Document KVM_DEV_ARM_ITS_CTRL_RESET
      KVM: arm/arm64: vgic-its: Free caches when GITS_BASER Valid bit is cleared
      KVM: arm/arm64: vgic-its: New helper functions to free the caches
      KVM: arm/arm64: vgic-its: Remove kvm_its_unmap_device
      arm/arm64: KVM: Load the timer state when enabling the timer
      KVM: arm/arm64: Rework kvm_timer_should_fire
      KVM: arm/arm64: Get rid of kvm_timer_flush_hwstate
      KVM: arm/arm64: Avoid phys timer emulation in vcpu entry/exit
      KVM: arm/arm64: Move phys_timer_emulate function
      KVM: arm/arm64: Use kvm_arm_timer_set/get_reg for guest register traps
      ...

commit cc3d967f7e32ceeb9b78dc962126ebcf1a2b24b2
Author: Ladi Prosek <lprosek@redhat.com>
Date:   Tue Oct 17 16:02:39 2017 +0200

    KVM: SVM: detect opening of SMI window using STGI intercept
    
    Commit 05cade71cf3b ("KVM: nSVM: fix SMI injection in guest mode") made
    KVM mask SMI if GIF=0 but it didn't do anything to unmask it when GIF is
    enabled.
    
    The issue manifests for me as a significantly longer boot time of Windows
    guests when running with SMM-enabled OVMF.
    
    This commit fixes it by intercepting STGI instead of requesting immediate
    exit if the reason why SMM was masked is GIF.
    
    Fixes: 05cade71cf3b ("KVM: nSVM: fix SMI injection in guest mode")
    Signed-off-by: Ladi Prosek <lprosek@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8700b845f780..7233445a20bd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1065,6 +1065,7 @@ struct kvm_x86_ops {
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
+	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit 05cade71cf3b925042569c3e8dc1fa68a2b26995
Author: Ladi Prosek <lprosek@redhat.com>
Date:   Wed Oct 11 16:54:45 2017 +0200

    KVM: nSVM: fix SMI injection in guest mode
    
    Entering SMM while running in guest mode wasn't working very well because several
    pieces of the vcpu state were left set up for nested operation.
    
    Some of the issues observed:
    
    * L1 was getting unexpected VM exits (using L1 interception controls but running
      in SMM execution environment)
    * MMU was confused (walk_mmu was still set to nested_mmu)
    * INTERCEPT_SMI was not emulated for L1 (KVM never injected SVM_EXIT_SMI)
    
    Intel SDM actually prescribes the logical processor to "leave VMX operation" upon
    entering SMM in 34.14.1 Default Treatment of SMI Delivery. AMD doesn't seem to
    document this but they provide fields in the SMM state-save area to stash the
    current state of SVM. What we need to do is basically get out of guest mode for
    the duration of SMM. All this completely transparent to L1, i.e. L1 is not given
    control and no L1 observable state changes.
    
    To avoid code duplication this commit takes advantage of the existing nested
    vmexit and run functionality, perhaps at the cost of efficiency. To get out of
    guest mode, nested_svm_vmexit is called, unchanged. Re-entering is performed using
    enter_svm_guest_mode.
    
    This commit fixes running Windows Server 2016 with Hyper-V enabled in a VM with
    OVMF firmware (OVMF_CODE-need-smm.fd).
    
    Signed-off-by: Ladi Prosek <lprosek@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 411ddbbaeabf..8700b845f780 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1430,4 +1430,7 @@ static inline int kvm_cpu_get_apicid(int mps_cpu)
 #endif
 }
 
+#define put_smstate(type, buf, offset, val)                      \
+	*(type *)((buf) + (offset) - 0x7e00) = val
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit 72d7b374b14d67e973bce476e4a75552478cc42d
Author: Ladi Prosek <lprosek@redhat.com>
Date:   Wed Oct 11 16:54:41 2017 +0200

    KVM: x86: introduce ISA specific smi_allowed callback
    
    Similar to NMI, there may be ISA specific reasons why an SMI cannot be
    injected into the guest. This commit adds a new smi_allowed callback to
    be implemented in following commits.
    
    Signed-off-by: Ladi Prosek <lprosek@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 23a9a5339f3f..411ddbbaeabf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1062,6 +1062,7 @@ struct kvm_x86_ops {
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
 
+	int (*smi_allowed)(struct kvm_vcpu *vcpu);
 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
 };

commit 0234bf885236a41ef05376039f2a8ebe7028a388
Author: Ladi Prosek <lprosek@redhat.com>
Date:   Wed Oct 11 16:54:40 2017 +0200

    KVM: x86: introduce ISA specific SMM entry/exit callbacks
    
    Entering and exiting SMM may require ISA specific handling under certain
    circumstances. This commit adds two new callbacks with empty implementations.
    Actual functionality will be added in following commits.
    
    * pre_enter_smm() is to be called when injecting an SMM, before any
      SMM related vcpu state has been changed
    * pre_leave_smm() is to be called when emulating the RSM instruction,
      when the vcpu is in real mode and before any SMM related vcpu state
      has been restored
    
    Signed-off-by: Ladi Prosek <lprosek@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c73e493adf07..23a9a5339f3f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1061,6 +1061,9 @@ struct kvm_x86_ops {
 	void (*cancel_hv_timer)(struct kvm_vcpu *vcpu);
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
+
+	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
+	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
 };
 
 struct kvm_arch_async_pf {

commit 64063505835663c67cf18524c46e1eb70d30fb54
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:21 2017 +0200

    x86/apic: Sanitize 32/64bit APIC callbacks
    
    The 32bit and the 64bit implementation of default_cpu_present_to_apicid()
    and default_check_phys_apicid_present() are exactly the same, but
    implemented and located differently.
    
    Move them to common apic code and get rid of the pointless difference.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213153.757329991@linutronix.de

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c73e493adf07..9d7d856b2d89 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1419,7 +1419,7 @@ static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
 static inline int kvm_cpu_get_apicid(int mps_cpu)
 {
 #ifdef CONFIG_X86_LOCAL_APIC
-	return __default_cpu_present_to_apicid(mps_cpu);
+	return default_cpu_present_to_apicid(mps_cpu);
 #else
 	WARN_ON_ONCE(1);
 	return BAD_APICID;

commit b2a05feff274e007abd7cda0d2cdae7fdf5cbb0a
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Tue Sep 12 10:42:41 2017 -0500

    KVM: Add struct kvm_vcpu pointer parameter to get_enable_apicv()
    
    Modify struct kvm_x86_ops.arch.apicv_active() to take struct kvm_vcpu
    pointer as parameter in preparation to subsequent changes.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bbb802382a3e..c73e493adf07 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -972,7 +972,7 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
-	bool (*get_enable_apicv)(void);
+	bool (*get_enable_apicv)(struct kvm_vcpu *vcpu);
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);

commit 98152b83e065d5593c261b0cc58197666f94a34f
Author: Joerg Roedel <jroedel@suse.de>
Date:   Mon Aug 28 16:38:35 2017 +0200

    KVM: x86: Remove .get_pkru() from kvm_x86_ops
    
    The commit
    
            9dd21e104bc ('KVM: x86: simplify handling of PKRU')
    
    removed all users and providers of that call-back, but
    didn't remove it. Remove it now.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8844eee290b2..bbb802382a3e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -951,7 +951,6 @@ struct kvm_x86_ops {
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
-	u32 (*get_pkru)(struct kvm_vcpu *vcpu);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);
 

commit 0756b7fbb696d2cb18785da9cab13ec164017f64
Merge: 6d6218976df1 5f54c8b2d4fa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 8 15:18:36 2017 -0700

    Merge tag 'kvm-4.14-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "First batch of KVM changes for 4.14
    
      Common:
       - improve heuristic for boosting preempted spinlocks by ignoring
         VCPUs in user mode
    
      ARM:
       - fix for decoding external abort types from guests
    
       - added support for migrating the active priority of interrupts when
         running a GICv2 guest on a GICv3 host
    
       - minor cleanup
    
      PPC:
       - expose storage keys to userspace
    
       - merge kvm-ppc-fixes with a fix that missed 4.13 because of
         vacations
    
       - fixes
    
      s390:
       - merge of kvm/master to avoid conflicts with additional sthyi fixes
    
       - wire up the no-dat enhancements in KVM
    
       - multiple epoch facility (z14 feature)
    
       - Configuration z/Architecture Mode
    
       - more sthyi fixes
    
       - gdb server range checking fix
    
       - small code cleanups
    
      x86:
       - emulate Hyper-V TSC frequency MSRs
    
       - add nested INVPCID
    
       - emulate EPTP switching VMFUNC
    
       - support Virtual GIF
    
       - support 5 level page tables
    
       - speedup nested VM exits by packing byte operations
    
       - speedup MMIO by using hardware provided physical address
    
       - a lot of fixes and cleanups, especially nested"
    
    * tag 'kvm-4.14-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (67 commits)
      KVM: arm/arm64: Support uaccess of GICC_APRn
      KVM: arm/arm64: Extract GICv3 max APRn index calculation
      KVM: arm/arm64: vITS: Drop its_ite->lpi field
      KVM: arm/arm64: vgic: constify seq_operations and file_operations
      KVM: arm/arm64: Fix guest external abort matching
      KVM: PPC: Book3S HV: Fix memory leak in kvm_vm_ioctl_get_htab_fd
      KVM: s390: vsie: cleanup mcck reinjection
      KVM: s390: use WARN_ON_ONCE only for checking
      KVM: s390: guestdbg: fix range check
      KVM: PPC: Book3S HV: Report storage key support to userspace
      KVM: PPC: Book3S HV: Fix case where HDEC is treated as 32-bit on POWER9
      KVM: PPC: Book3S HV: Fix invalid use of register expression
      KVM: PPC: Book3S HV: Fix H_REGISTER_VPA VPA size validation
      KVM: PPC: Book3S HV: Fix setting of storage key in H_ENTER
      KVM: PPC: e500mc: Fix a NULL dereference
      KVM: PPC: e500: Fix some NULL dereferences on error
      KVM: PPC: Book3S HV: Protect updates to spapr_tce_tables list
      KVM: s390: we are always in czam mode
      KVM: s390: expose no-DAT to guest and migration support
      KVM: s390: sthyi: remove invalid guest write access
      ...

commit 5f54c8b2d4fad95d1f8ecbe023ebe6038e6d3760
Merge: 78809a68490d edd03602d972
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Fri Sep 8 14:40:43 2017 +0200

    Merge branch 'kvm-ppc-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    This fix was intended for 4.13, but didn't get in because both
    maintainers were on vacation.
    
    Paul Mackerras:
     "It adds mutual exclusion between list_add_rcu and list_del_rcu calls
      on the kvm->arch.spapr_tce_tables list.  Without this, userspace could
      potentially trigger corruption of the list and cause a host crash or
      worse."

commit b1b6f83ac938d176742c85757960dec2cf10e468
Merge: 5f82e71a001d 9e52fc2b50de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 12:21:28 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "PCID support, 5-level paging support, Secure Memory Encryption support
    
      The main changes in this cycle are support for three new, complex
      hardware features of x86 CPUs:
    
       - Add 5-level paging support, which is a new hardware feature on
         upcoming Intel CPUs allowing up to 128 PB of virtual address space
         and 4 PB of physical RAM space - a 512-fold increase over the old
         limits. (Supercomputers of the future forecasting hurricanes on an
         ever warming planet can certainly make good use of more RAM.)
    
         Many of the necessary changes went upstream in previous cycles,
         v4.14 is the first kernel that can enable 5-level paging.
    
         This feature is activated via CONFIG_X86_5LEVEL=y - disabled by
         default.
    
         (By Kirill A. Shutemov)
    
       - Add 'encrypted memory' support, which is a new hardware feature on
         upcoming AMD CPUs ('Secure Memory Encryption', SME) allowing system
         RAM to be encrypted and decrypted (mostly) transparently by the
         CPU, with a little help from the kernel to transition to/from
         encrypted RAM. Such RAM should be more secure against various
         attacks like RAM access via the memory bus and should make the
         radio signature of memory bus traffic harder to intercept (and
         decrypt) as well.
    
         This feature is activated via CONFIG_AMD_MEM_ENCRYPT=y - disabled
         by default.
    
         (By Tom Lendacky)
    
       - Enable PCID optimized TLB flushing on newer Intel CPUs: PCID is a
         hardware feature that attaches an address space tag to TLB entries
         and thus allows to skip TLB flushing in many cases, even if we
         switch mm's.
    
         (By Andy Lutomirski)
    
      All three of these features were in the works for a long time, and
      it's coincidence of the three independent development paths that they
      are all enabled in v4.14 at once"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (65 commits)
      x86/mm: Enable RCU based page table freeing (CONFIG_HAVE_RCU_TABLE_FREE=y)
      x86/mm: Use pr_cont() in dump_pagetable()
      x86/mm: Fix SME encryption stack ptr handling
      kvm/x86: Avoid clearing the C-bit in rsvd_bits()
      x86/CPU: Align CR3 defines
      x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages
      acpi, x86/mm: Remove encryption mask from ACPI page protection type
      x86/mm, kexec: Fix memory corruption with SME on successive kexecs
      x86/mm/pkeys: Fix typo in Documentation/x86/protection-keys.txt
      x86/mm/dump_pagetables: Speed up page tables dump for CONFIG_KASAN=y
      x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID
      x86: Enable 5-level paging support via CONFIG_X86_5LEVEL=y
      x86/mm: Allow userspace have mappings above 47-bit
      x86/mm: Prepare to expose larger address space to userspace
      x86/mpx: Do not allow MPX if we have mappings above 47-bit
      x86/mm: Rename tasksize_32bit/64bit to task_size_32bit/64bit()
      x86/xen: Redefine XEN_ELFNOTE_INIT_P2M using PUD_SIZE * PTRS_PER_PUD
      x86/mm/dump_pagetables: Fix printout of p4d level
      x86/mm/dump_pagetables: Generalize address normalization
      x86/boot: Fix memremap() related build failure
      ...

commit fb1522e099f0c69f36655af233a64e3f55941f5b
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Thu Aug 31 17:17:37 2017 -0400

    KVM: update to new mmu_notifier semantic v2
    
    Calls to mmu_notifier_invalidate_page() were replaced by calls to
    mmu_notifier_invalidate_range() and are now bracketed by calls to
    mmu_notifier_invalidate_range_start()/end()
    
    Remove now useless invalidate_page callback.
    
    Changed since v1 (Linus Torvalds)
        - remove now useless kvm_arch_mmu_notifier_invalidate_page()
    
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Tested-by: Mike Galbraith <efault@gmx.de>
    Tested-by: Adam Borowski <kilobyte@angband.pl>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f4d120a3e22e..92c9032502d8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1375,8 +1375,6 @@ int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event);
 void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu);
-void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
-					   unsigned long address);
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);

commit b9dd21e104bcd45e124acfe978a79df71259e59b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Aug 23 23:14:38 2017 +0200

    KVM: x86: simplify handling of PKRU
    
    Move it to struct kvm_arch_vcpu, replacing guest_pkru_valid with a
    simple comparison against the host value of the register.  The write of
    PKRU in addition can be skipped if the guest has not enabled the feature.
    Once we do this, we need not test OSPKE in the host anymore, because
    guest_CR4.PKE=1 implies host_CR4.PKE=1.
    
    The static PKU test is kept to elide the code on older CPUs.
    
    Suggested-by: Yang Zhang <zy107165@alibaba-inc.com>
    Fixes: 1be0e61c1f255faaeab04a390e00c8b9b9042870
    Cc: stable@vger.kernel.org
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 87ac4fba6d8e..f4d120a3e22e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -492,6 +492,7 @@ struct kvm_vcpu_arch {
 	unsigned long cr4;
 	unsigned long cr4_guest_owned_bits;
 	unsigned long cr8;
+	u32 pkru;
 	u32 hflags;
 	u64 efer;
 	u64 apic_base;

commit 664f8e26b00c7673a8303b0d40853a0c24ca93e1
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Aug 24 03:35:09 2017 -0700

    KVM: X86: Fix loss of exception which has not yet been injected
    
    vmx_complete_interrupts() assumes that the exception is always injected,
    so it can be dropped by kvm_clear_exception_queue().  However,
    an exception cannot be injected immediately if it is: 1) originally
    destined to a nested guest; 2) trapped to cause a vmexit; 3) happening
    right after VMLAUNCH/VMRESUME, i.e. when nested_run_pending is true.
    
    This patch applies to exceptions the same algorithm that is used for
    NMIs, replacing exception.reinject with "exception.injected" (equivalent
    to nmi_injected).
    
    exception.pending now represents an exception that is queued and whose
    side effects (e.g., update RFLAGS.RF or DR7) have not been applied yet.
    If exception.pending is true, the exception might result in a nested
    vmexit instead, too (in which case the side effects must not be applied).
    
    exception.injected instead represents an exception that is going to be
    injected into the guest at the next vmentry.
    
    Reported-by: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b4d4f5151489..afa70749903e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -547,8 +547,8 @@ struct kvm_vcpu_arch {
 
 	struct kvm_queued_exception {
 		bool pending;
+		bool injected;
 		bool has_error_code;
-		bool reinject;
 		u8 nr;
 		u32 error_code;
 		u8 nested_apf;

commit fd8cb433734eeb870156a67f5d56b6564cd2ea94
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Thu Aug 24 20:27:56 2017 +0800

    KVM: MMU: Expose the LA57 feature to VM.
    
    This patch exposes 5 level page table feature to the VM.
    At the same time, the canonical virtual address checking is
    extended to support both 48-bits and 57-bits address width.
    
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bdef5329d408..b4d4f5151489 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -85,8 +85,8 @@
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
 			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR | X86_CR4_PCIDE \
 			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_FSGSBASE \
-			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE | X86_CR4_SMAP \
-			  | X86_CR4_PKE))
+			  | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_VMXE \
+			  | X86_CR4_SMAP | X86_CR4_PKE))
 
 #define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)
 
@@ -1300,20 +1300,6 @@ static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 }
 
-static inline u64 get_canonical(u64 la)
-{
-	return ((int64_t)la << 16) >> 16;
-}
-
-static inline bool is_noncanonical_address(u64 la)
-{
-#ifdef CONFIG_X86_64
-	return get_canonical(la) != la;
-#else
-	return false;
-#endif
-}
-
 #define TSS_IOPB_BASE_OFFSET 0x66
 #define TSS_BASE_SIZE 0x68
 #define TSS_IOPB_SIZE (65536 / 8)

commit 855feb6736403f398dd43764254c5f0522bfc130
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Thu Aug 24 20:27:55 2017 +0800

    KVM: MMU: Add 5 level EPT & Shadow page table support.
    
    Extends the shadow paging code, so that 5 level shadow page
    table can be constructed if VM is running in 5 level paging
    mode.
    
    Also extends the ept code, so that 5 level ept table can be
    constructed if maxphysaddr of VM exceeds 48 bits. Unlike the
    shadow logic, KVM should still use 4 level ept table for a VM
    whose physical address width is less than 48 bits, even when
    the VM is running in 5 level paging mode.
    
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    [Unconditionally reset the MMU context in kvm_cpuid_update.
     Changing MAXPHYADDR invalidates the reserved bit bitmasks.
     - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5907d46d306d..bdef5329d408 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -315,7 +315,7 @@ struct kvm_pio_request {
 	int size;
 };
 
-#define PT64_ROOT_MAX_LEVEL 4
+#define PT64_ROOT_MAX_LEVEL 5
 
 struct rsvd_bits_validate {
 	u64 rsvd_bits_mask[2][PT64_ROOT_MAX_LEVEL];
@@ -323,9 +323,9 @@ struct rsvd_bits_validate {
 };
 
 /*
- * x86 supports 3 paging modes (4-level 64-bit, 3-level 64-bit, and 2-level
- * 32-bit).  The kvm_mmu structure abstracts the details of the current mmu
- * mode.
+ * x86 supports 4 paging modes (5-level 64-bit, 4-level 64-bit, 3-level 32-bit,
+ * and 2-level 32-bit).  The kvm_mmu structure abstracts the details of the
+ * current mmu mode.
  */
 struct kvm_mmu {
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
@@ -982,7 +982,7 @@ struct kvm_x86_ops {
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	int (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
-	int (*get_tdp_level)(void);
+	int (*get_tdp_level)(struct kvm_vcpu *vcpu);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);

commit 2a7266a8f9074f03c014dca641721d451881a42b
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Thu Aug 24 20:27:54 2017 +0800

    KVM: MMU: Rename PT64_ROOT_LEVEL to PT64_ROOT_4LEVEL.
    
    Now we have 4 level page table and 5 level page table in 64 bits
    long mode, let's rename the PT64_ROOT_LEVEL to PT64_ROOT_4LEVEL,
    then we can use PT64_ROOT_5LEVEL for 5 level page table, it's
    helpful to make the code more clear.
    
    Also PT64_ROOT_MAX_LEVEL is defined as 4, so that we can just
    redefine it to 5 whenever a replacement is needed for 5 level
    paging.
    
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e7162285b22e..5907d46d306d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -315,8 +315,10 @@ struct kvm_pio_request {
 	int size;
 };
 
+#define PT64_ROOT_MAX_LEVEL 4
+
 struct rsvd_bits_validate {
-	u64 rsvd_bits_mask[2][4];
+	u64 rsvd_bits_mask[2][PT64_ROOT_MAX_LEVEL];
 	u64 bad_mt_xwr;
 };
 

commit d1cd3ce9004412949163bfaa062a4df98fe75a98
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Thu Aug 24 20:27:53 2017 +0800

    KVM: MMU: check guest CR3 reserved bits based on its physical address width.
    
    Currently, KVM uses CR3_L_MODE_RESERVED_BITS to check the
    reserved bits in CR3. Yet the length of reserved bits in
    guest CR3 should be based on the physical address width
    exposed to the VM. This patch changes CR3 check logic to
    calculate the reserved bits at runtime.
    
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6db0ed9cf59e..e7162285b22e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -79,7 +79,6 @@
 			  | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM \
 			  | X86_CR0_NW | X86_CR0_CD | X86_CR0_PG))
 
-#define CR3_L_MODE_RESERVED_BITS 0xFFFFFF0000000000ULL
 #define CR3_PCID_INVD		 BIT_64(63)
 #define CR4_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\

commit 618232e2196a6db1ed66b5e1ec049e5c46480f49
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Thu Aug 17 18:36:57 2017 +0200

    KVM: x86: Avoid guest page table walk when gpa_available is set
    
    When a guest causes a page fault which requires emulation, the
    vcpu->arch.gpa_available flag is set to indicate that cr2 contains a
    valid GPA.
    
    Currently, emulator_read_write_onepage() makes use of gpa_available flag
    to avoid a guest page walk for a known MMIO regions. Lets not limit
    the gpa_available optimization to just MMIO region. The patch extends
    the check to avoid page walk whenever gpa_available flag is set.
    
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    [Fix EPT=0 according to Wanpeng Li's fix, plus ensure VMX also uses the
     new code. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [Moved "ret < 0" to the else brach, as per David's review. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9e4862e0e978..6db0ed9cf59e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -685,8 +685,9 @@ struct kvm_vcpu_arch {
 	int pending_ioapic_eoi;
 	int pending_external_vector;
 
-	/* GPA available (AMD only) */
+	/* GPA available */
 	bool gpa_available;
+	gpa_t gpa_val;
 
 	/* be preempted when it's in kernel-mode(cpl=0) */
 	bool preempted_in_kernel;

commit eebed2438923f8df465c27f8fa41303771fdb2e8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 28 14:39:58 2016 +0100

    kvm: nVMX: Add support for fast unprotection of nested guest page tables
    
    This is the same as commit 147277540bbc ("kvm: svm: Add support for
    additional SVM NPF error codes", 2016-11-23), but for Intel processors.
    In this case, the exit qualification field's bit 8 says whether the
    EPT violation occurred while translating the guest's final physical
    address or rather while translating the guest page tables.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1679aabcabe5..9e4862e0e978 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -204,7 +204,6 @@ enum {
 #define PFERR_GUEST_PAGE_MASK (1ULL << PFERR_GUEST_PAGE_BIT)
 
 #define PFERR_NESTED_GUEST_PAGE (PFERR_GUEST_PAGE_MASK |	\
-				 PFERR_USER_MASK |		\
 				 PFERR_WRITE_MASK |		\
 				 PFERR_PRESENT_MASK)
 

commit de63ad4cf4973462953c29c363f3cfa7117c2b2d
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Tue Aug 8 12:05:33 2017 +0800

    KVM: X86: implement the logic for spinlock optimization
    
    get_cpl requires vcpu_load, so we must cache the result (whether the
    vcpu was preempted when its cpl=0) in kvm_vcpu_arch.
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 87ac4fba6d8e..1679aabcabe5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -688,6 +688,9 @@ struct kvm_vcpu_arch {
 
 	/* GPA available (AMD only) */
 	bool gpa_available;
+
+	/* be preempted when it's in kernel-mode(cpl=0) */
+	bool preempted_in_kernel;
 };
 
 struct kvm_lpage_info {

commit d0ec49d4de90806755e17289bd48464a1a515823
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:27 2017 -0500

    kvm/x86/svm: Support Secure Memory Encryption within KVM
    
    Update the KVM support to work with SME. The VMCB has a number of fields
    where physical addresses are used and these addresses must contain the
    memory encryption mask in order to properly access the encrypted memory.
    Also, use the memory encryption mask when creating and using the nested
    page tables.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/89146eccfa50334409801ff20acd52a90fb5efcf.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 87ac4fba6d8e..7cbaab523f22 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1078,7 +1078,7 @@ void kvm_mmu_init_vm(struct kvm *kvm);
 void kvm_mmu_uninit_vm(struct kvm *kvm);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 p_mask,
-		u64 acc_track_mask);
+		u64 acc_track_mask, u64 me_mask);
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,

commit d3457c877b14aaee8c52923eedf05a3b78af0476
Author: Roman Kagan <rkagan@virtuozzo.com>
Date:   Fri Jul 14 17:13:20 2017 +0300

    kvm: x86: hyperv: make VP_INDEX managed by userspace
    
    Hyper-V identifies vCPUs by Virtual Processor Index, which can be
    queried via HV_X64_MSR_VP_INDEX msr.  It is defined by the spec as a
    sequential number which can't exceed the maximum number of vCPUs per VM.
    APIC ids can be sparse and thus aren't a valid replacement for VP
    indices.
    
    Current KVM uses its internal vcpu index as VP_INDEX.  However, to make
    it predictable and persistent across VM migrations, the userspace has to
    control the value of VP_INDEX.
    
    This patch achieves that, by storing vp_index explicitly on vcpu, and
    allowing HV_X64_MSR_VP_INDEX to be set from the host side.  For
    compatibility it's initialized to KVM vcpu index.  Also a few variables
    are renamed to make clear distinction betweed this Hyper-V vp_index and
    KVM vcpu_id (== APIC id).  Besides, a new capability,
    KVM_CAP_HYPERV_VP_INDEX, is added to allow the userspace to skip
    attempting msr writes where unsupported, to avoid spamming error logs.
    
    Signed-off-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index da3261e384d3..87ac4fba6d8e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -467,6 +467,7 @@ struct kvm_vcpu_hv_synic {
 
 /* Hyper-V per vcpu emulation context */
 struct kvm_vcpu_hv {
+	u32 vp_index;
 	u64 hv_vapic;
 	s64 runtime_offset;
 	struct kvm_vcpu_hv_synic synic;

commit 52a5c155cf79f1f059bffebf4d06d0249573e659
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Jul 13 18:30:42 2017 -0700

    KVM: async_pf: Let guest support delivery of async_pf from guest mode
    
    Adds another flag bit (bit 2) to MSR_KVM_ASYNC_PF_EN. If bit 2 is 1,
    async page faults are delivered to L1 as #PF vmexits; if bit 2 is 0,
    kvm_can_do_async_pf returns 0 if in guest mode.
    
    This is similar to what svm.c wanted to do all along, but it is only
    enabled for Linux as L1 hypervisor.  Foreign hypervisors must never
    receive async page faults as vmexits, because they'd probably be very
    confused about that.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5e9ac508f718..da3261e384d3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -653,6 +653,7 @@ struct kvm_vcpu_arch {
 		bool send_user_only;
 		u32 host_apf_reason;
 		unsigned long nested_apf_token;
+		bool delivery_as_pf_vmexit;
 	} apf;
 
 	/* OSVW MSRs (AMD only) */

commit adfe20fb48785dd73af3bf91407196eb5403c8cf
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Jul 13 18:30:41 2017 -0700

    KVM: async_pf: Force a nested vmexit if the injected #PF is async_pf
    
    Add an nested_apf field to vcpu->arch.exception to identify an async page
    fault, and constructs the expected vm-exit information fields. Force a
    nested VM exit from nested_vmx_check_exception() if the injected #PF is
    async page fault.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4f20ee6c79a1..5e9ac508f718 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -550,6 +550,7 @@ struct kvm_vcpu_arch {
 		bool reinject;
 		u8 nr;
 		u32 error_code;
+		u8 nested_apf;
 	} exception;
 
 	struct kvm_queued_interrupt {
@@ -651,6 +652,7 @@ struct kvm_vcpu_arch {
 		u32 id;
 		bool send_user_only;
 		u32 host_apf_reason;
+		unsigned long nested_apf_token;
 	} apf;
 
 	/* OSVW MSRs (AMD only) */

commit 1261bfa326f5e903166498628a1894edce0caabc
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Jul 13 18:30:40 2017 -0700

    KVM: async_pf: Add L1 guest async_pf #PF vmexit handler
    
    This patch adds the L1 guest async page fault #PF vmexit handler, such
    by L1 similar to ordinary async page fault.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    [Passed insn parameters to kvm_mmu_page_fault().]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8d11ddcb0dbf..4f20ee6c79a1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -650,6 +650,7 @@ struct kvm_vcpu_arch {
 		u64 msr_val;
 		u32 id;
 		bool send_user_only;
+		u32 host_apf_reason;
 	} apf;
 
 	/* OSVW MSRs (AMD only) */

commit cfcd20e5caad6ba552978c16ed8bed7edb0143cf
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Jul 13 18:30:39 2017 -0700

    KVM: x86: Simplify kvm_x86_ops->queue_exception parameter list
    
    This patch removes all arguments except the first in
    kvm_x86_ops->queue_exception since they can extract the arguments from
    vcpu->arch.exception themselves.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9d8de5dd7546..8d11ddcb0dbf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -954,9 +954,7 @@ struct kvm_x86_ops {
 				unsigned char *hypercall_addr);
 	void (*set_irq)(struct kvm_vcpu *vcpu);
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
-	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
-				bool has_error_code, u32 error_code,
-				bool reinject);
+	void (*queue_exception)(struct kvm_vcpu *vcpu);
 	void (*cancel_injection)(struct kvm_vcpu *vcpu);
 	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
 	int (*nmi_allowed)(struct kvm_vcpu *vcpu);

commit efc479e6900c22bad9a2b649d13405ed9cde2d53
Author: Roman Kagan <rkagan@virtuozzo.com>
Date:   Thu Jun 22 16:51:01 2017 +0300

    kvm: x86: hyperv: add KVM_CAP_HYPERV_SYNIC2
    
    There is a flaw in the Hyper-V SynIC implementation in KVM: when message
    page or event flags page is enabled by setting the corresponding msr,
    KVM zeroes it out.  This is problematic because on migration the
    corresponding MSRs are loaded on the destination, so the content of
    those pages is lost.
    
    This went unnoticed so far because the only user of those pages was
    in-KVM hyperv synic timers, which could continue working despite that
    zeroing.
    
    Newer QEMU uses those pages for Hyper-V VMBus implementation, and
    zeroing them breaks the migration.
    
    Besides, in newer QEMU the content of those pages is fully managed by
    QEMU, so zeroing them is undesirable even when writing the MSRs from the
    guest side.
    
    To support this new scheme, introduce a new capability,
    KVM_CAP_HYPERV_SYNIC2, which, when enabled, makes sure that the synic
    pages aren't zeroed out in KVM.
    
    Signed-off-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ef37d0dc61bd..9d8de5dd7546 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -462,6 +462,7 @@ struct kvm_vcpu_hv_synic {
 	DECLARE_BITMAP(auto_eoi_bitmap, 256);
 	DECLARE_BITMAP(vec_bitmap, 256);
 	bool active;
+	bool dont_zero_synic_pages;
 };
 
 /* Hyper-V per vcpu emulation context */

commit a826faf108e2d855929342268e68c43ba667379a
Author: Ladi Prosek <lprosek@redhat.com>
Date:   Mon Jun 26 09:56:43 2017 +0200

    KVM: x86: make backwards_tsc_observed a per-VM variable
    
    The backwards_tsc_observed global introduced in commit 16a9602 is never
    reset to false. If a VM happens to be running while the host is suspended
    (a common source of the TSC jumping backwards), master clock will never
    be enabled again for any VM. In contrast, if no VM is running while the
    host is suspended, master clock is unaffected. This is inconsistent and
    unnecessarily strict. Let's track the backwards_tsc_observed variable
    separately and let each VM start with a clean slate.
    
    Real world impact: My Windows VMs get slower after my laptop undergoes a
    suspend/resume cycle. The only way to get the perf back is unloading and
    reloading the kvm module.
    
    Signed-off-by: Ladi Prosek <lprosek@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1588e9e3dc01..ef37d0dc61bd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -803,6 +803,7 @@ struct kvm_arch {
 	int audit_point;
 	#endif
 
+	bool backwards_tsc_observed;
 	bool boot_vcpu_runs_old_kvmclock;
 	u32 bsp_vcpu_id;
 

commit ac8d57e5734389da18633d4e8cc030fe10843da7
Author: Peter Feiner <pfeiner@google.com>
Date:   Fri Jun 30 17:26:31 2017 -0700

    kvm: x86: mmu: allow A/D bits to be disabled in an mmu
    
    Adds the plumbing to disable A/D bits in the MMU based on a new role
    bit, ad_disabled. When A/D is disabled, the MMU operates as though A/D
    aren't available (i.e., using access tracking faults instead).
    
    To avoid SP -> kvm_mmu_page.role.ad_disabled lookups all over the
    place, A/D disablement is now stored in the SPTE. This state is stored
    in the SPTE by tweaking the use of SPTE_SPECIAL_MASK for access
    tracking. Rather than just setting SPTE_SPECIAL_MASK when an
    access-tracking SPTE is non-present, we now always set
    SPTE_SPECIAL_MASK for access-tracking SPTEs.
    
    Signed-off-by: Peter Feiner <pfeiner@google.com>
    [Use role.ad_disabled even for direct (non-shadow) EPT page tables.  Add
     documentation and a few MMU_WARN_ONs. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9be890893885..1588e9e3dc01 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -257,7 +257,8 @@ union kvm_mmu_page_role {
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
 		unsigned smap_andnot_wp:1;
-		unsigned :8;
+		unsigned ad_disabled:1;
+		unsigned :7;
 
 		/*
 		 * This is left at the top of the word so that

commit 04a7ea04d508b925e7f829305b358157d58b4f82
Merge: c853354429f7 d38338e396ee
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jun 30 12:38:26 2017 +0200

    Merge tag 'kvmarm-for-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM updates for 4.13
    
    - vcpu request overhaul
    - allow timer and PMU to have their interrupt number
      selected from userspace
    - workaround for Cavium erratum 30115
    - handling of memory poisonning
    - the usual crop of fixes and cleanups
    
    Conflicts:
            arch/s390/include/asm/kvm_host.h

commit 2387149eade25f32dcf1398811b3d0293181d005
Author: Andrew Jones <drjones@redhat.com>
Date:   Sun Jun 4 14:43:51 2017 +0200

    KVM: improve arch vcpu request defining
    
    Marc Zyngier suggested that we define the arch specific VCPU request
    base, rather than requiring each arch to remember to start from 8.
    That suggestion, along with Radim Krcmar's recent VCPU request flag
    addition, snowballed into defining something of an arch VCPU request
    defining API.
    
    No functional change.
    
    (Looks like x86 is running out of arch VCPU request bits.  Maybe
     someday we'll need to extend to 64.)
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9c761fea0c98..563979976fab 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -48,28 +48,31 @@
 #define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS
 
 /* x86-specific vcpu->requests bit members */
-#define KVM_REQ_MIGRATE_TIMER      8
-#define KVM_REQ_REPORT_TPR_ACCESS  9
-#define KVM_REQ_TRIPLE_FAULT      10
-#define KVM_REQ_MMU_SYNC          11
-#define KVM_REQ_CLOCK_UPDATE      12
-#define KVM_REQ_EVENT             14
-#define KVM_REQ_APF_HALT          15
-#define KVM_REQ_STEAL_UPDATE      16
-#define KVM_REQ_NMI               17
-#define KVM_REQ_PMU               18
-#define KVM_REQ_PMI               19
-#define KVM_REQ_SMI               20
-#define KVM_REQ_MASTERCLOCK_UPDATE 21
-#define KVM_REQ_MCLOCK_INPROGRESS (22 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
-#define KVM_REQ_SCAN_IOAPIC       (23 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
-#define KVM_REQ_GLOBAL_CLOCK_UPDATE 24
-#define KVM_REQ_APIC_PAGE_RELOAD  (25 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
-#define KVM_REQ_HV_CRASH          26
-#define KVM_REQ_IOAPIC_EOI_EXIT   27
-#define KVM_REQ_HV_RESET          28
-#define KVM_REQ_HV_EXIT           29
-#define KVM_REQ_HV_STIMER         30
+#define KVM_REQ_MIGRATE_TIMER		KVM_ARCH_REQ(0)
+#define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
+#define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
+#define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+#define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
+#define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
+#define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
+#define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
+#define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+#define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+#define KVM_REQ_PMI			KVM_ARCH_REQ(11)
+#define KVM_REQ_SMI			KVM_ARCH_REQ(12)
+#define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
+#define KVM_REQ_MCLOCK_INPROGRESS \
+	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_SCAN_IOAPIC \
+	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
+#define KVM_REQ_APIC_PAGE_RELOAD \
+	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_HV_CRASH		KVM_ARCH_REQ(18)
+#define KVM_REQ_IOAPIC_EOI_EXIT		KVM_ARCH_REQ(19)
+#define KVM_REQ_HV_RESET		KVM_ARCH_REQ(20)
+#define KVM_REQ_HV_EXIT			KVM_ARCH_REQ(21)
+#define KVM_REQ_HV_STIMER		KVM_ARCH_REQ(22)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \

commit b401ee0b85a53e89739ff68a5b1a0667d664afc9
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Apr 18 12:41:18 2017 +0200

    KVM: x86: lower default for halt_poll_ns
    
    In some fio benchmarks, halt_poll_ns=400000 caused CPU utilization to
    increase heavily even in cases where the performance improvement was
    small.  In particular, bandwidth divided by CPU usage was as much as
    60% lower.
    
    To some extent this is the expected effect of the patch, and the
    additional CPU utilization is only visible when running the
    benchmarks.  However, halving the threshold also halves the extra
    CPU utilization (from +30-130% to +20-70%) and has no negative
    effect on performance.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9c761fea0c98..695605eb1dfb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -43,7 +43,7 @@
 #define KVM_PRIVATE_MEM_SLOTS 3
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 
-#define KVM_HALT_POLL_NS_DEFAULT 400000
+#define KVM_HALT_POLL_NS_DEFAULT 200000
 
 #define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS
 

commit bab4165e2f031905bd0a2bb8b6ad65c5c8cfa870
Author: Bandan Das <bsd@redhat.com>
Date:   Fri May 5 15:25:13 2017 -0400

    kvm: x86: Add a hook for arch specific dirty logging emulation
    
    When KVM updates accessed/dirty bits, this hook can be used
    to invoke an arch specific function that implements/emulates
    dirty logging such as PML.
    
    Signed-off-by: Bandan Das <bsd@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f5bddf92faba..9c761fea0c98 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1020,6 +1020,8 @@ struct kvm_x86_ops {
 	void (*enable_log_dirty_pt_masked)(struct kvm *kvm,
 					   struct kvm_memory_slot *slot,
 					   gfn_t offset, unsigned long mask);
+	int (*write_log_dirty)(struct kvm_vcpu *vcpu);
+
 	/* pmu operations of sub-arch */
 	const struct kvm_pmu_ops *pmu_ops;
 

commit 5c0aea0e8d98e38858fbb3a09870ed8487a01da2
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Apr 28 17:06:20 2017 +0200

    KVM: x86: don't hold kvm->lock in KVM_SET_GSI_ROUTING
    
    We needed the lock to avoid racing with creation of the irqchip on x86. As
    kvm_set_irq_routing() calls srcu_synchronize_expedited(), this lock
    might be held for a longer time.
    
    Let's introduce an arch specific callback to check if we can actually
    add irq routes. For x86, all we have to do is check if we have an
    irqchip in the kernel. We don't need kvm->lock at that point as the
    irqchip is marked as inititalized only when actually fully created.
    
    Reported-by: Steve Rutherford <srutherford@google.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Fixes: 1df6ddede10a ("KVM: x86: race between KVM_SET_GSI_ROUTING and KVM_CREATE_IRQCHIP")
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 84c8489531bb..f5bddf92faba 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -728,7 +728,6 @@ struct kvm_hv {
 
 enum kvm_irqchip_mode {
 	KVM_IRQCHIP_NONE,
-	KVM_IRQCHIP_INIT_IN_PROGRESS, /* temporarily set during creation */
 	KVM_IRQCHIP_KERNEL,       /* created with KVM_CREATE_IRQCHIP */
 	KVM_IRQCHIP_SPLIT,        /* created with KVM_CAP_SPLIT_IRQCHIP */
 };

commit 7a97cec26b94c909f4cbad2dc3186af3e457a522
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 27 14:33:43 2017 +0200

    KVM: mark requests that need synchronization
    
    kvm_make_all_requests() provides a synchronization that waits until all
    kicked VCPUs have acknowledged the kick.  This is important for
    KVM_REQ_MMU_RELOAD as it prevents freeing while lockless paging is
    underway.
    
    This patch adds the synchronization property into all requests that are
    currently being used with kvm_make_all_requests() in order to preserve
    the current behavior and only introduce a new framework.  Removing it
    from requests where it is not necessary is left for future patches.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 19219826bed6..84c8489531bb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -61,10 +61,10 @@
 #define KVM_REQ_PMI               19
 #define KVM_REQ_SMI               20
 #define KVM_REQ_MASTERCLOCK_UPDATE 21
-#define KVM_REQ_MCLOCK_INPROGRESS (22 | KVM_REQUEST_NO_WAKEUP)
-#define KVM_REQ_SCAN_IOAPIC       (23 | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_MCLOCK_INPROGRESS (22 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_SCAN_IOAPIC       (23 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE 24
-#define KVM_REQ_APIC_PAGE_RELOAD  (25 | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_APIC_PAGE_RELOAD  (25 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_HV_CRASH          26
 #define KVM_REQ_IOAPIC_EOI_EXIT   27
 #define KVM_REQ_HV_RESET          28

commit 930f7fd6da77ed9476a538345513460fd304aaf5
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Apr 26 22:32:22 2017 +0200

    KVM: mark requests that do not need a wakeup
    
    Some operations must ensure that the guest is not running with stale
    data, but if the guest is halted, then the update can wait until another
    event happens.  kvm_make_all_requests() currently doesn't wake up, so we
    can mark all requests used with it.
    
    First 8 bits were arbitrarily reserved for request numbers.
    
    Most uses of requests have the request type as a constant, so a compiler
    will optimize the '&'.
    
    An alternative would be to have an inline function that would return
    whether the request needs a wake-up or not, but I like this one better
    even though it might produce worse assembly.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f5c942edbc86..19219826bed6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -61,10 +61,10 @@
 #define KVM_REQ_PMI               19
 #define KVM_REQ_SMI               20
 #define KVM_REQ_MASTERCLOCK_UPDATE 21
-#define KVM_REQ_MCLOCK_INPROGRESS 22
-#define KVM_REQ_SCAN_IOAPIC       23
+#define KVM_REQ_MCLOCK_INPROGRESS (22 | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_SCAN_IOAPIC       (23 | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE 24
-#define KVM_REQ_APIC_PAGE_RELOAD  25
+#define KVM_REQ_APIC_PAGE_RELOAD  (25 | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_HV_CRASH          26
 #define KVM_REQ_IOAPIC_EOI_EXIT   27
 #define KVM_REQ_HV_RESET          28

commit db2336a80489e7c3c7728cefd9be58fac5ecfb39
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:28 2017 -0700

    KVM: x86: virtualize cpuid faulting
    
    Hardware support for faulting on the cpuid instruction is not required to
    emulate it, because cpuid triggers a VM exit anyways. KVM handles the relevant
    MSRs (MSR_PLATFORM_INFO and MSR_MISC_FEATURES_ENABLE) and upon a
    cpuid-induced VM exit checks the cpuid faulting state and the CPL.
    kvm_require_cpl is even kind enough to inject the GP fault for us.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Reviewed-by: David Matlack <dmatlack@google.com>
    [Return "1" from kvm_emulate_cpuid, it's not void. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2cc5ec7cc6f5..f5c942edbc86 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -611,6 +611,8 @@ struct kvm_vcpu_arch {
 	unsigned long dr7;
 	unsigned long eff_db[KVM_NR_DB_REGS];
 	unsigned long guest_debug_dr7;
+	u64 msr_platform_info;
+	u64 msr_misc_features_enables;
 
 	u64 mcg_cap;
 	u64 mcg_status;

commit 637e3f86faf97a930f1ac4c3f79fd667addae9e8
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Apr 7 10:50:19 2017 +0200

    KVM: x86: new irqchip mode KVM_IRQCHIP_INIT_IN_PROGRESS
    
    Let's add a new mode and set it while we create the irqchip via
    KVM_CREATE_IRQCHIP and KVM_CAP_SPLIT_IRQCHIP.
    
    This mode will be used later to test if adding routes
    (in kvm_set_routing_entry()) is already allowed.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d962fa998a6f..2cc5ec7cc6f5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -726,6 +726,7 @@ struct kvm_hv {
 
 enum kvm_irqchip_mode {
 	KVM_IRQCHIP_NONE,
+	KVM_IRQCHIP_INIT_IN_PROGRESS, /* temporarily set during creation */
 	KVM_IRQCHIP_KERNEL,       /* created with KVM_CREATE_IRQCHIP */
 	KVM_IRQCHIP_SPLIT,        /* created with KVM_CAP_SPLIT_IRQCHIP */
 };

commit 4b4357e02523ec63ad853f927f5d93a25101a1d2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Mar 31 13:53:23 2017 +0200

    kvm: make KVM_COALESCED_MMIO_PAGE_OFFSET public
    
    Its value has never changed; we might as well make it part of the ABI instead
    of using the return value of KVM_CHECK_EXTENSION(KVM_CAP_COALESCED_MMIO).
    
    Because PPC does not always make MMIO available, the code has to be made
    dependent on CONFIG_KVM_MMIO rather than KVM_COALESCED_MMIO_PAGE_OFFSET.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7dbb8d622683..d962fa998a6f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -43,8 +43,6 @@
 #define KVM_PRIVATE_MEM_SLOTS 3
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 
-#define KVM_PIO_PAGE_OFFSET 1
-#define KVM_COALESCED_MMIO_PAGE_OFFSET 2
 #define KVM_HALT_POLL_NS_DEFAULT 400000
 
 #define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS

commit ae1e2d1082ae6969ff8c626ef80804d950bf256b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 30 11:55:30 2017 +0200

    kvm: nVMX: support EPT accessed/dirty bits
    
    Now use bit 6 of EPTP to optionally enable A/D bits for EPTP.  Another
    thing to change is that, when EPT accessed and dirty bits are not in use,
    VMX treats accesses to guest paging structures as data reads.  When they
    are in use (bit 6 of EPTP is set), they are treated as writes and the
    corresponding EPT dirty bit is set.  The MMU didn't know this detail,
    so this patch adds it.
    
    We also have to fix up the exit qualification.  It may be wrong because
    KVM sets bit 6 but the guest might not.
    
    L1 emulates EPT A/D bits using write permissions, so in principle it may
    be possible for EPT A/D bits to be used by L1 even though not available
    in hardware.  The problem is that guest page-table walks will be treated
    as reads rather than writes, so they would not cause an EPT violation.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [Fixed typo in walk_addr_generic() comment and changed bit clear +
     conditional-set pattern in handle_ept_violation() to conditional-clear]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 74ef58c8ff53..7dbb8d622683 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -343,9 +343,10 @@ struct kvm_mmu {
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
 	hpa_t root_hpa;
-	int root_level;
-	int shadow_root_level;
 	union kvm_mmu_page_role base_role;
+	u8 root_level;
+	u8 shadow_root_level;
+	u8 ept_ad;
 	bool direct_map;
 
 	/*

commit bd7e5b0899a429445cc6e3037c13f8b5ae3be903
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 3 21:18:52 2017 -0800

    KVM: x86: remove code for lazy FPU handling
    
    The FPU is always active now when running KVM.
    
    Reviewed-by: David Matlack <dmatlack@google.com>
    Reviewed-by: Bandan Das <bsd@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e4f13e714bcf..74ef58c8ff53 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -55,7 +55,6 @@
 #define KVM_REQ_TRIPLE_FAULT      10
 #define KVM_REQ_MMU_SYNC          11
 #define KVM_REQ_CLOCK_UPDATE      12
-#define KVM_REQ_DEACTIVATE_FPU    13
 #define KVM_REQ_EVENT             14
 #define KVM_REQ_APF_HALT          15
 #define KVM_REQ_STEAL_UPDATE      16
@@ -936,8 +935,6 @@ struct kvm_x86_ops {
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
 	u32 (*get_pkru)(struct kvm_vcpu *vcpu);
-	void (*fpu_activate)(struct kvm_vcpu *vcpu);
-	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);
 

commit 76dfafd536730ef9b9d99b1cf596916d52be76d1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Dec 19 17:17:11 2016 +0100

    KVM: x86: do not scan IRR twice on APICv vmentry
    
    Calls to apic_find_highest_irr are scanning IRR twice, once
    in vmx_sync_pir_from_irr and once in apic_search_irr.  Change
    sync_pir_from_irr to get the new maximum IRR from kvm_apic_update_irr;
    now that it does the computation, it can also do the RVI write.
    
    In order to avoid complications in svm.c, make the callback optional.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 417502cf42b6..e4f13e714bcf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -969,7 +969,7 @@ struct kvm_x86_ops {
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
-	void (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
+	int (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);

commit 0f1e261ead16ce09169bf2d223d4c8803576f85e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Dec 17 16:05:19 2016 +0100

    KVM: x86: add VCPU stat for KVM_REQ_EVENT processing
    
    This statistic can be useful to estimate the cost of an IRQ injection
    scenario, by comparing it with irq_injections.  For example the stat
    shows that sti;hlt triggers more KVM_REQ_EVENT than sti;nop.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0419e114f27b..417502cf42b6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -861,6 +861,7 @@ struct kvm_vcpu_stat {
 	u64 hypercalls;
 	u64 irq_injections;
 	u64 nmi_injections;
+	u64 req_event;
 };
 
 struct x86_instruction_info;

commit 0f89b207b04a1a399e19d35293658e3a571da3d7
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Wed Dec 14 14:59:23 2016 -0500

    kvm: svm: Use the hardware provided GPA instead of page walk
    
    When a guest causes a NPF which requires emulation, KVM sometimes walks
    the guest page tables to translate the GVA to a GPA. This is unnecessary
    most of the time on AMD hardware since the hardware provides the GPA in
    EXITINFO2.
    
    The only exception cases involve string operations involving rep or
    operations that use two memory locations. With rep, the GPA will only be
    the value of the initial NPF and with dual memory locations we won't know
    which memory address was translated into EXITINFO2.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 99a71d90b6ae..0419e114f27b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -675,6 +675,9 @@ struct kvm_vcpu_arch {
 
 	int pending_ioapic_eoi;
 	int pending_external_vector;
+
+	/* GPA available (AMD only) */
+	bool gpa_available;
 };
 
 struct kvm_lpage_info {

commit f160c7b7bb322bf079a5bb4dd34c58f17553f193
Author: Junaid Shahid <junaids@google.com>
Date:   Tue Dec 6 16:46:16 2016 -0800

    kvm: x86: mmu: Lockless access tracking for Intel CPUs without EPT A bits.
    
    This change implements lockless access tracking for Intel CPUs without EPT
    A bits. This is achieved by marking the PTEs as not-present (but not
    completely clearing them) when clear_flush_young() is called after marking
    the pages as accessed. When an EPT Violation is generated as a result of
    the VM accessing those pages, the PTEs are restored to their original values.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3272a5e4aaad..99a71d90b6ae 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1064,7 +1064,8 @@ void kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_init_vm(struct kvm *kvm);
 void kvm_mmu_uninit_vm(struct kvm *kvm);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
-		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 p_mask);
+		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 p_mask,
+		u64 acc_track_mask);
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,

commit 37f0e8fe6b10ee2ab52576caa721ee1282de74a6
Author: Junaid Shahid <junaids@google.com>
Date:   Tue Dec 6 16:46:15 2016 -0800

    kvm: x86: mmu: Do not use bit 63 for tracking special SPTEs
    
    MMIO SPTEs currently set both bits 62 and 63 to distinguish them as special
    PTEs. However, bit 63 is used as the SVE bit in Intel EPT PTEs. The SVE bit
    is ignored for misconfigured PTEs but not necessarily for not-Present PTEs.
    Since MMIO SPTEs use an EPT misconfiguration, so using bit 63 for them is
    acceptable. However, the upcoming fast access tracking feature adds another
    type of special tracking PTE, which uses not-Present PTEs and hence should
    not set bit 63.
    
    In order to use common bits to distinguish both type of special PTEs, we
    now use only bit 62 as the special bit.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7e594a325158..3272a5e4aaad 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -208,6 +208,13 @@ enum {
 				 PFERR_WRITE_MASK |		\
 				 PFERR_PRESENT_MASK)
 
+/*
+ * The mask used to denote special SPTEs, which can be either MMIO SPTEs or
+ * Access Tracking SPTEs. We use bit 62 instead of bit 63 to avoid conflicting
+ * with the SVE bit in EPT PTEs.
+ */
+#define SPTE_SPECIAL_MASK (1ULL << 62)
+
 /* apic attention bits */
 #define KVM_APIC_CHECK_VAPIC	0
 /*

commit 114df303a7eeae8b50ebf68229b7e647714a9bea
Author: David Matlack <dmatlack@google.com>
Date:   Mon Dec 19 13:58:25 2016 -0800

    kvm: x86: reduce collisions in mmu_page_hash
    
    When using two-dimensional paging, the mmu_page_hash (which provides
    lookups for existing kvm_mmu_page structs), becomes imbalanced; with
    too many collisions in buckets 0 and 512. This has been seen to cause
    mmu_lock to be held for multiple milliseconds in kvm_mmu_get_page on
    VMs with a large amount of RAM mapped with 4K pages.
    
    The current hash function uses the lower 10 bits of gfn to index into
    mmu_page_hash. When doing shadow paging, gfn is the address of the
    guest page table being shadow. These tables are 4K-aligned, which
    makes the low bits of gfn a good hash. However, with two-dimensional
    paging, no guest page tables are being shadowed, so gfn is the base
    address that is mapped by the table. Thus page tables (level=1) have
    a 2MB aligned gfn, page directories (level=2) have a 1GB aligned gfn,
    etc. This means hashes will only differ in their 10th bit.
    
    hash_64() provides a better hash. For example, on a VM with ~200G
    (99458 direct=1 kvm_mmu_page structs):
    
    hash            max_mmu_page_hash_collisions
    --------------------------------------------
    low 10 bits     49847
    hash_64         105
    perfect         97
    
    While we're changing the hash, increase the table size by 4x to better
    support large VMs (further reduces number of collisions in 200G VM to
    29).
    
    Note that hash_64() does not provide a good distribution prior to commit
    ef703f49a6c5 ("Eliminate bad hash multipliers from hash_32() and
    hash_64()").
    
    Signed-off-by: David Matlack <dmatlack@google.com>
    Change-Id: I5aa6b13c834722813c6cca46b8b1ed6f53368ade
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1bb1ffc0024c..7e594a325158 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -115,7 +115,7 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 
 #define KVM_PERMILLE_MMU_PAGES 20
 #define KVM_MIN_ALLOC_MMU_PAGES 64
-#define KVM_MMU_HASH_SHIFT 10
+#define KVM_MMU_HASH_SHIFT 12
 #define KVM_NUM_MMU_PAGES (1 << KVM_MMU_HASH_SHIFT)
 #define KVM_MIN_FREE_MMU_PAGES 5
 #define KVM_REFILL_PAGES 25

commit f3414bc77419463c0d81eaa2cea7ee4ccb447c7d
Author: David Matlack <dmatlack@google.com>
Date:   Tue Dec 20 15:25:57 2016 -0800

    kvm: x86: export maximum number of mmu_page_hash collisions
    
    Report the maximum number of mmu_page_hash collisions as a per-VM stat.
    This will make it easy to identify problems with the mmu_page_hash in
    the future.
    
    Signed-off-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fc03ab1f6110..1bb1ffc0024c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -821,6 +821,7 @@ struct kvm_vm_stat {
 	ulong mmu_unsync;
 	ulong remote_tlb_flush;
 	ulong lpages;
+	ulong max_mmu_page_hash_collisions;
 };
 
 struct kvm_vcpu_stat {

commit 49776faf93f8074bb4990beac04781a9507d3650
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Fri Dec 16 16:10:02 2016 +0100

    KVM: x86: decouple irqchip_in_kernel() and pic_irqchip()
    
    irqchip_in_kernel() tried to save a bit by reusing pic_irqchip(), but it
    just complicated the code.
    Add a separate state for the irqchip mode.
    
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [Used Paolo's version of condition in irqchip_in_kernel().]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a7066dc1a7e9..fc03ab1f6110 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -716,6 +716,12 @@ struct kvm_hv {
 	HV_REFERENCE_TSC_PAGE tsc_ref;
 };
 
+enum kvm_irqchip_mode {
+	KVM_IRQCHIP_NONE,
+	KVM_IRQCHIP_KERNEL,       /* created with KVM_CREATE_IRQCHIP */
+	KVM_IRQCHIP_SPLIT,        /* created with KVM_CAP_SPLIT_IRQCHIP */
+};
+
 struct kvm_arch {
 	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;
@@ -788,7 +794,7 @@ struct kvm_arch {
 
 	u64 disabled_quirks;
 
-	bool irqchip_split;
+	enum kvm_irqchip_mode irqchip_mode;
 	u8 nr_reserved_ioapic_pins;
 
 	bool disabled_lapic_found;

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2e25038dbd93..a7066dc1a7e9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -768,7 +768,7 @@ struct kvm_arch {
 	spinlock_t pvclock_gtod_sync_lock;
 	bool use_master_clock;
 	u64 master_kernel_ns;
-	cycle_t master_cycle_now;
+	u64 master_cycle_now;
 	struct delayed_work kvmclock_update_work;
 	struct delayed_work kvmclock_sync_work;
 

commit 3f5ad8be3713572f3946b69eb376206153d0ea2d
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Dec 12 10:12:53 2016 +0100

    KVM: hyperv: fix locking of struct kvm_hv fields
    
    Introduce a new mutex to avoid an AB-BA deadlock between kvm->lock and
    vcpu->mutex.  Protect accesses in kvm_hv_setup_tsc_page too, as suggested
    by Roman.
    
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7892530cbacf..2e25038dbd93 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -704,6 +704,7 @@ struct kvm_apic_map {
 
 /* Hyper-V emulation context */
 struct kvm_hv {
+	struct mutex hv_lock;
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
 	u64 hv_tsc_page;

commit 9ed38ffad47316dbdc16de0de275868c7771754d
Author: Ladi Prosek <lprosek@redhat.com>
Date:   Wed Nov 30 16:03:10 2016 +0100

    KVM: nVMX: introduce nested_vmx_load_cr3 and call it on vmentry
    
    Loading CR3 as part of emulating vmentry is different from regular CR3 loads,
    as implemented in kvm_set_cr3, in several ways.
    
    * different rules are followed to check CR3 and it is desirable for the caller
    to distinguish between the possible failures
    * PDPTRs are not loaded if PAE paging and nested EPT are both enabled
    * many MMU operations are not necessary
    
    This patch introduces nested_vmx_load_cr3 suitable for CR3 loads as part of
    nested vmentry and vmexit, and makes use of it on the nested vmentry path.
    
    Signed-off-by: Ladi Prosek <lprosek@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8d1587092851..7892530cbacf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1071,6 +1071,7 @@ unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 
 int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3);
+bool pdptrs_changed(struct kvm_vcpu *vcpu);
 
 int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
 			  const void *val, int bytes);

commit 6affcbedcac79b01c8d01948a693461040133e46
Author: Kyle Huey <me@kylehuey.com>
Date:   Tue Nov 29 12:40:40 2016 -0800

    KVM: x86: Add kvm_skip_emulated_instruction and use it.
    
    kvm_skip_emulated_instruction calls both
    kvm_x86_ops->skip_emulated_instruction and kvm_vcpu_check_singlestep,
    skipping the emulated instruction and generating a trap if necessary.
    
    Replacing skip_emulated_instruction calls with
    kvm_skip_emulated_instruction is straightforward, except for:
    
    - ICEBP, which is already inside a trap, so avoid triggering another trap.
    - Instructions that can trigger exits to userspace, such as the IO insns,
      MOVs to CR8, and HALT. If kvm_skip_emulated_instruction does trigger a
      KVM_GUESTDBG_SINGLESTEP exit, and the handling code for
      IN/OUT/MOV CR8/HALT also triggers an exit to userspace, the latter will
      take precedence. The singlestep will be triggered again on the next
      instruction, which is the current behavior.
    - Task switch instructions which would require additional handling (e.g.
      the task switch bit) and are instead left alone.
    - Cases where VMLAUNCH/VMRESUME do not proceed to the next instruction,
      which do not trigger singlestep traps as mentioned previously.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 80bad5c372bf..8d1587092851 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1368,7 +1368,8 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
-void kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
+int kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu);
+int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
 
 int kvm_is_in_guest(void);
 

commit 6a908b628cff81d3f1eb737327c8b726c8fdfd4e
Author: Kyle Huey <me@kylehuey.com>
Date:   Tue Nov 29 12:40:37 2016 -0800

    KVM: x86: Add a return value to kvm_emulate_cpuid
    
    Once skipping the emulated instruction can potentially trigger an exit to
    userspace (via KVM_GUESTDBG_SINGLESTEP) kvm_emulate_cpuid will need to
    propagate a return value.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 77cb3f93de2b..80bad5c372bf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1134,7 +1134,7 @@ struct x86_emulate_ctxt;
 
 int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
 int kvm_fast_pio_in(struct kvm_vcpu *vcpu, int size, unsigned short port);
-void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
+int kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
 int kvm_vcpu_halt(struct kvm_vcpu *vcpu);
 int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);

commit 8370c3d08bd98576d97514eca29970e03767a5d1
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Wed Nov 23 12:01:50 2016 -0500

    kvm: svm: Add kvm_fast_pio_in support
    
    Update the I/O interception support to add the kvm_fast_pio_in function
    to speed up the in instruction similar to the out instruction.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index da07e175dac8..77cb3f93de2b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1133,6 +1133,7 @@ int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
 struct x86_emulate_ctxt;
 
 int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
+int kvm_fast_pio_in(struct kvm_vcpu *vcpu, int size, unsigned short port);
 void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
 int kvm_vcpu_halt(struct kvm_vcpu *vcpu);

commit 147277540bbc54119172481c8ef6d930cc9fbfc2
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Wed Nov 23 12:01:38 2016 -0500

    kvm: svm: Add support for additional SVM NPF error codes
    
    AMD hardware adds two additional bits to aid in nested page fault handling.
    
    Bit 32 - NPF occurred while translating the guest's final physical address
    Bit 33 - NPF occurred while translating the guest page tables
    
    The guest page tables fault indicator can be used as an aid for nested
    virtualization. Using V0 for the host, V1 for the first level guest and
    V2 for the second level guest, when both V1 and V2 are using nested paging
    there are currently a number of unnecessary instruction emulations. When
    V2 is launched shadow paging is used in V1 for the nested tables of V2. As
    a result, KVM marks these pages as RO in the host nested page tables. When
    V2 exits and we resume V1, these pages are still marked RO.
    
    Every nested walk for a guest page table is treated as a user-level write
    access and this causes a lot of NPFs because the V1 page tables are marked
    RO in the V0 nested tables. While executing V1, when these NPFs occur KVM
    sees a write to a read-only page, emulates the V1 instruction and unprotects
    the page (marking it RW). This patch looks for cases where we get a NPF due
    to a guest page table walk where the page was marked RO. It immediately
    unprotects the page and resumes the guest, leading to far fewer instruction
    emulations when nested virtualization is used.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bdde80731f49..da07e175dac8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -191,6 +191,8 @@ enum {
 #define PFERR_RSVD_BIT 3
 #define PFERR_FETCH_BIT 4
 #define PFERR_PK_BIT 5
+#define PFERR_GUEST_FINAL_BIT 32
+#define PFERR_GUEST_PAGE_BIT 33
 
 #define PFERR_PRESENT_MASK (1U << PFERR_PRESENT_BIT)
 #define PFERR_WRITE_MASK (1U << PFERR_WRITE_BIT)
@@ -198,6 +200,13 @@ enum {
 #define PFERR_RSVD_MASK (1U << PFERR_RSVD_BIT)
 #define PFERR_FETCH_MASK (1U << PFERR_FETCH_BIT)
 #define PFERR_PK_MASK (1U << PFERR_PK_BIT)
+#define PFERR_GUEST_FINAL_MASK (1ULL << PFERR_GUEST_FINAL_BIT)
+#define PFERR_GUEST_PAGE_MASK (1ULL << PFERR_GUEST_PAGE_BIT)
+
+#define PFERR_NESTED_GUEST_PAGE (PFERR_GUEST_PAGE_MASK |	\
+				 PFERR_USER_MASK |		\
+				 PFERR_WRITE_MASK |		\
+				 PFERR_PRESENT_MASK)
 
 /* apic attention bits */
 #define KVM_APIC_CHECK_VAPIC	0
@@ -1203,7 +1212,7 @@ void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 
-int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u32 error_code,
+int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u64 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu);

commit ea26e4ec08d4727e3a9e48a6b74695861effcbd9
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Nov 1 00:39:48 2016 +0100

    KVM: x86: drop TSC offsetting kvm_x86_ops to fix KVM_GET/SET_CLOCK
    
    Since commit a545ab6a0085 ("kvm: x86: add tsc_offset field to struct
    kvm_vcpu_arch", 2016-09-07) the offset between host and L1 TSC is
    cached and need not be fished out of the VMCS or VMCB.  This means
    that we can implement adjust_tsc_offset_guest and read_l1_tsc
    entirely in generic code.  The simplification is particularly
    significant for VMX code, where vmx->nested.vmcs01_tsc_offset
    was duplicating what is now in vcpu->arch.tsc_offset.  Therefore
    the vmcs01_tsc_offset can be dropped completely.
    
    More importantly, this fixes KVM_GET_CLOCK/KVM_SET_CLOCK
    which, after commit 108b249c453d ("KVM: x86: introduce get_kvmclock_ns",
    2016-09-01) called read_l1_tsc while the VMCS was not loaded.
    It thus returned bogus values on Intel CPUs.
    
    Fixes: 108b249c453dd7132599ab6dc7e435a7036c193f
    Reported-by: Roman Kagan <rkagan@virtuozzo.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4b20f7304b9c..bdde80731f49 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -948,7 +948,6 @@ struct kvm_x86_ops {
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
 	bool (*invpcid_supported)(void);
-	void (*adjust_tsc_offset_guest)(struct kvm_vcpu *vcpu, s64 adjustment);
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 
@@ -958,8 +957,6 @@ struct kvm_x86_ops {
 
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
-	u64 (*read_l1_tsc)(struct kvm_vcpu *vcpu, u64 host_tsc);
-
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
 
 	int (*check_intercept)(struct kvm_vcpu *vcpu,

commit 095cf55df715d14d5dad75326faf5984e7fc0b3a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Feb 8 12:54:12 2016 +0100

    KVM: x86: Hyper-V tsc page setup
    
    Lately tsc page was implemented but filled with empty
    values. This patch setup tsc page scale and offset based
    on vcpu tsc, tsc_khz and  HV_X64_MSR_TIME_REF_COUNT value.
    
    The valid tsc page drops HV_X64_MSR_TIME_REF_COUNT msr
    reads count to zero which potentially improves performance.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Peter Hornyack <peterhornyack@google.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    [Computation of TSC page parameters rewritten to use the Linux timekeeper
     parameters. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 32a43a25d415..4b20f7304b9c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -702,6 +702,8 @@ struct kvm_hv {
 	/* Hyper-v based guest crash (NT kernel bugcheck) parameters */
 	u64 hv_crash_param[HV_X64_MSR_CRASH_PARAMS];
 	u64 hv_crash_ctl;
+
+	HV_REFERENCE_TSC_PAGE tsc_ref;
 };
 
 struct kvm_arch {

commit 3e3f50262eb441d0fd1de4dce06739e9c0fe7c61
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Wed Sep 7 14:47:20 2016 -0400

    kvm: x86: drop read_tsc_offset()
    
    The TSC offset can now be read directly from struct kvm_arch_vcpu.
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ddffcfbe155c..32a43a25d415 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -954,7 +954,6 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
-	u64 (*read_tsc_offset)(struct kvm_vcpu *vcpu);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	u64 (*read_l1_tsc)(struct kvm_vcpu *vcpu, u64 host_tsc);

commit a545ab6a0085e6df9c7b6e9734b40ba4d2aca8c9
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Wed Sep 7 14:47:19 2016 -0400

    kvm: x86: add tsc_offset field to struct kvm_vcpu_arch
    
    A future commit will want to easily read a vCPU's TSC offset,
    so we store it in struct kvm_arch_vcpu_arch for easy access.
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cd82bf74e7a5..ddffcfbe155c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -568,6 +568,7 @@ struct kvm_vcpu_arch {
 		struct kvm_steal_time steal;
 	} st;
 
+	u64 tsc_offset;
 	u64 last_guest_tsc;
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;

commit ad53e35ae529e65cbd8e75a1e66fdcd10275c8d9
Merge: 6f90f1d1d2d8 aad9e5ba2433
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 13 15:01:29 2016 +0200

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    Paul Mackerras writes:
    
        The highlights are:
    
        * Reduced latency for interrupts from PCI pass-through devices, from
          Suresh Warrier and me.
        * Halt-polling implementation from Suraj Jitindar Singh.
        * 64-bit VCPU statistics, also from Suraj.
        * Various other minor fixes and improvements.

commit 5881f73757cc3dbada878e67c119a801ed0f9a07
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Tue Aug 23 13:52:42 2016 -0500

    svm: Introduce AMD IOMMU avic_ga_log_notifier
    
    This patch introduces avic_ga_log_notifier, which will be called
    by IOMMU driver whenever it handles the Guest vAPIC (GA) log entry.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d36176bb1b6d..4c738c206be3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -785,6 +785,7 @@ struct kvm_arch {
 	u32 ldr_mode;
 	struct page *avic_logical_id_table_page;
 	struct page *avic_physical_id_table_page;
+	struct hlist_node hnode;
 
 	bool x2apic_format;
 	bool x2apic_broadcast_quirk_disabled;

commit 5ea11f2b31b83bd3c05357e6bec20f598e00705a
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Tue Aug 23 13:52:41 2016 -0500

    svm: Introduces AVIC per-VM ID
    
    Introduces per-VM AVIC ID and helper functions to manage the IDs.
    Currently, the ID will be used to implement 32-bit AVIC IOMMU GA tag.
    
    The ID is 24-bit one-based indexing value, and is managed via helper
    functions to get the next ID, or to free an ID once a VM is destroyed.
    There should be no ID conflict for any active VMs.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 33ae3a4d0159..d36176bb1b6d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -781,6 +781,7 @@ struct kvm_arch {
 	bool disabled_lapic_found;
 
 	/* Struct members for AVIC */
+	u32 avic_vm_id;
 	u32 ldr_mode;
 	struct page *avic_logical_id_table_page;
 	struct page *avic_physical_id_table_page;

commit 8a7e75d47b68193339f8727cf4503271d0a0b1d0
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 2 14:03:22 2016 +1000

    KVM: Add provisioning for ulong vm stats and u64 vcpu stats
    
    vms and vcpus have statistics associated with them which can be viewed
    within the debugfs. Currently it is assumed within the vcpu_stat_get() and
    vm_stat_get() functions that all of these statistics are represented as
    u32s, however the next patch adds some u64 vcpu statistics.
    
    Change all vcpu statistics to u64 and modify vcpu_stat_get() accordingly.
    Since vcpu statistics are per vcpu, they will only be updated by a single
    vcpu at a time so this shouldn't present a problem on 32-bit machines
    which can't atomically increment 64-bit numbers. However vm statistics
    could potentially be updated by multiple vcpus from that vm at a time.
    To avoid the overhead of atomics make all vm statistics ulong such that
    they are 64-bit on 64-bit systems where they can be atomically incremented
    and are 32-bit on 32-bit systems which may not be able to atomically
    increment 64-bit numbers. Modify vm_stat_get() to expect ulongs.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Reviewed-by: David Matlack <dmatlack@google.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 33ae3a4d0159..67c8f5268af5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -790,45 +790,45 @@ struct kvm_arch {
 };
 
 struct kvm_vm_stat {
-	u32 mmu_shadow_zapped;
-	u32 mmu_pte_write;
-	u32 mmu_pte_updated;
-	u32 mmu_pde_zapped;
-	u32 mmu_flooded;
-	u32 mmu_recycled;
-	u32 mmu_cache_miss;
-	u32 mmu_unsync;
-	u32 remote_tlb_flush;
-	u32 lpages;
+	ulong mmu_shadow_zapped;
+	ulong mmu_pte_write;
+	ulong mmu_pte_updated;
+	ulong mmu_pde_zapped;
+	ulong mmu_flooded;
+	ulong mmu_recycled;
+	ulong mmu_cache_miss;
+	ulong mmu_unsync;
+	ulong remote_tlb_flush;
+	ulong lpages;
 };
 
 struct kvm_vcpu_stat {
-	u32 pf_fixed;
-	u32 pf_guest;
-	u32 tlb_flush;
-	u32 invlpg;
-
-	u32 exits;
-	u32 io_exits;
-	u32 mmio_exits;
-	u32 signal_exits;
-	u32 irq_window_exits;
-	u32 nmi_window_exits;
-	u32 halt_exits;
-	u32 halt_successful_poll;
-	u32 halt_attempted_poll;
-	u32 halt_poll_invalid;
-	u32 halt_wakeup;
-	u32 request_irq_exits;
-	u32 irq_exits;
-	u32 host_state_reload;
-	u32 efer_reload;
-	u32 fpu_reload;
-	u32 insn_emulation;
-	u32 insn_emulation_fail;
-	u32 hypercalls;
-	u32 irq_injections;
-	u32 nmi_injections;
+	u64 pf_fixed;
+	u64 pf_guest;
+	u64 tlb_flush;
+	u64 invlpg;
+
+	u64 exits;
+	u64 io_exits;
+	u64 mmio_exits;
+	u64 signal_exits;
+	u64 irq_window_exits;
+	u64 nmi_window_exits;
+	u64 halt_exits;
+	u64 halt_successful_poll;
+	u64 halt_attempted_poll;
+	u64 halt_poll_invalid;
+	u64 halt_wakeup;
+	u64 request_irq_exits;
+	u64 irq_exits;
+	u64 host_state_reload;
+	u64 efer_reload;
+	u64 fpu_reload;
+	u64 insn_emulation;
+	u64 insn_emulation_fail;
+	u64 hypercalls;
+	u64 irq_injections;
+	u64 nmi_injections;
 };
 
 struct x86_instruction_info;

commit 221bb8a46e230b9824204ae86537183d9991ff2a
Merge: f7b32e4c021f 23528bb21ee2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 16:11:27 2016 -0400

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
    
     - ARM: GICv3 ITS emulation and various fixes.  Removal of the
       old VGIC implementation.
    
     - s390: support for trapping software breakpoints, nested
       virtualization (vSIE), the STHYI opcode, initial extensions
       for CPU model support.
    
     - MIPS: support for MIPS64 hosts (32-bit guests only) and lots
       of cleanups, preliminary to this and the upcoming support for
       hardware virtualization extensions.
    
     - x86: support for execute-only mappings in nested EPT; reduced
       vmexit latency for TSC deadline timer (by about 30%) on Intel
       hosts; support for more than 255 vCPUs.
    
     - PPC: bugfixes.
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (302 commits)
      KVM: PPC: Introduce KVM_CAP_PPC_HTM
      MIPS: Select HAVE_KVM for MIPS64_R{2,6}
      MIPS: KVM: Reset CP0_PageMask during host TLB flush
      MIPS: KVM: Fix ptr->int cast via KVM_GUEST_KSEGX()
      MIPS: KVM: Sign extend MFC0/RDHWR results
      MIPS: KVM: Fix 64-bit big endian dynamic translation
      MIPS: KVM: Fail if ebase doesn't fit in CP0_EBase
      MIPS: KVM: Use 64-bit CP0_EBase when appropriate
      MIPS: KVM: Set CP0_Status.KX on MIPS64
      MIPS: KVM: Make entry code MIPS64 friendly
      MIPS: KVM: Use kmap instead of CKSEG0ADDR()
      MIPS: KVM: Use virt_to_phys() to get commpage PFN
      MIPS: Fix definition of KSEGX() for 64-bit
      KVM: VMX: Add VMCS to CPU's loaded VMCSs before VMPTRLD
      kvm: x86: nVMX: maintain internal copy of current VMCS
      KVM: PPC: Book3S HV: Save/restore TM state in H_CEDE
      KVM: PPC: Book3S HV: Pull out TM state save/restore into separate procedures
      KVM: arm64: vgic-its: Simplify MAPI error handling
      KVM: arm64: vgic-its: Make vgic_its_cmd_handle_mapi similar to other handlers
      KVM: arm64: vgic-its: Turn device_id validation into generic ID validation
      ...

commit af1bae5497b98cb99d6b0492e6981f060420a00c
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:30 2016 +0200

    KVM: x86: bump KVM_MAX_VCPU_ID to 1023
    
    kzalloc was replaced with kvm_kvzalloc to allow non-contiguous areas and
    rcu had to be modified to cope with it.
    
    The practical limit for KVM_MAX_VCPU_ID right now is INT_MAX, but lower
    value was chosen in case there were bugs.  1023 is sufficient maximum
    APIC ID for 288 VCPUs.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 21a40dc7aad6..9fcb197aa5ce 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -36,6 +36,7 @@
 
 #define KVM_MAX_VCPUS 288
 #define KVM_SOFT_MAX_VCPUS 240
+#define KVM_MAX_VCPU_ID 1023
 #define KVM_USER_MEM_SLOTS 509
 /* memory slots that are not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 3

commit 682f732ecf7396e9d6fe24d44738966699fae6c0
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:29 2016 +0200

    KVM: x86: bump MAX_VCPUS to 288
    
    288 is in high demand because of Knights Landing CPU.
    We cannot set the limit to 640k, because that would be wasting space.
    
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 074b5c760327..21a40dc7aad6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -34,7 +34,7 @@
 #include <asm/asm.h>
 #include <asm/kvm_page_track.h>
 
-#define KVM_MAX_VCPUS 255
+#define KVM_MAX_VCPUS 288
 #define KVM_SOFT_MAX_VCPUS 240
 #define KVM_USER_MEM_SLOTS 509
 /* memory slots that are not exposed to userspace */

commit c519265f2aa348b2f1b9ecf8fbe20bb7c0fb102e
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:28 2016 +0200

    KVM: x86: add a flag to disable KVM x2apic broadcast quirk
    
    Add KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK as a feature flag to
    KVM_CAP_X2APIC_API.
    
    The quirk made KVM interpret 0xff as a broadcast even in x2APIC mode.
    The enableable capability is needed in order to support standard x2APIC and
    remain backward compatible.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    [Expand kvm_apic_mda comment. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7c00ba3242d7..074b5c760327 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -784,6 +784,7 @@ struct kvm_arch {
 	struct page *avic_physical_id_table_page;
 
 	bool x2apic_format;
+	bool x2apic_broadcast_quirk_disabled;
 };
 
 struct kvm_vm_stat {

commit 3713131345fbea291cbd859d248e06ed77815962
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:27 2016 +0200

    KVM: x86: add KVM_CAP_X2APIC_API
    
    KVM_CAP_X2APIC_API is a capability for features related to x2APIC
    enablement.  KVM_X2APIC_API_32BIT_FORMAT feature can be enabled to
    extend APIC ID in get/set ioctl and MSI addresses to 32 bits.
    Both are needed to support x2APIC.
    
    The feature has to be enableable and disabled by default, because
    get/set ioctl shifted and truncated APIC ID to 8 bits by using a
    non-standard protocol inspired by xAPIC and the change is not
    backward-compatible.
    
    Changes to MSI addresses follow the format used by interrupt remapping
    unit.  The upper address word, that used to be 0, contains upper 24 bits
    of the LAPIC address in its upper 24 bits.  Lower 8 bits are reserved as
    0.  Using the upper address word is not backward-compatible either as we
    didn't check that userspace zeroed the word.  Reserved bits are still
    not explicitly checked, but non-zero data will affect LAPIC addresses,
    which will cause a bug.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a2832cc3cb81..7c00ba3242d7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -782,6 +782,8 @@ struct kvm_arch {
 	u32 ldr_mode;
 	struct page *avic_logical_id_table_page;
 	struct page *avic_physical_id_table_page;
+
+	bool x2apic_format;
 };
 
 struct kvm_vm_stat {
@@ -1364,7 +1366,7 @@ bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			     struct kvm_vcpu **dest_vcpu);
 
-void kvm_set_msi_irq(struct kvm_kernel_irq_routing_entry *e,
+void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq);
 
 static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)

commit 0ca52e7b81a37260c7edb823c8ac6a49c6280b5e
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:20 2016 +0200

    KVM: x86: dynamic kvm_apic_map
    
    x2APIC supports up to 2^32-1 LAPICs, but most guest in coming years will
    probably has fewer VCPUs.  Dynamic size saves memory at the cost of
    turning one constant into a variable.
    
    apic_map mutex had to be moved before allocation to avoid races with cpu
    hotplug.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 623089c4e1a7..a2832cc3cb81 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -682,11 +682,12 @@ struct kvm_arch_memory_slot {
 struct kvm_apic_map {
 	struct rcu_head rcu;
 	u8 mode;
-	struct kvm_lapic *phys_map[256];
+	u32 max_apic_id;
 	union {
 		struct kvm_lapic *xapic_flat_map[8];
 		struct kvm_lapic *xapic_cluster_map[16][4];
 	};
+	struct kvm_lapic *phys_map[];
 };
 
 /* Hyper-V emulation context */

commit e45115b62f9abb143a03036dbde05faf5864aa01
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:19 2016 +0200

    KVM: x86: use physical LAPIC array for logical x2APIC
    
    Logical x2APIC IDs map injectively to physical x2APIC IDs, so we can
    reuse the physical array for them.  This allows us to save space by
    sizing the logical maps according to the needs of xAPIC.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5f90dce6fbd1..623089c4e1a7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -683,8 +683,10 @@ struct kvm_apic_map {
 	struct rcu_head rcu;
 	u8 mode;
 	struct kvm_lapic *phys_map[256];
-	/* first index is cluster id second is cpu id in a cluster */
-	struct kvm_lapic *logical_map[16][16];
+	union {
+		struct kvm_lapic *xapic_flat_map[8];
+		struct kvm_lapic *xapic_cluster_map[16][4];
+	};
 };
 
 /* Hyper-V emulation context */

commit 757883de41eca292765578ef87c4f49453529bb2
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:17 2016 +0200

    KVM: x86: bump KVM_SOFT_MAX_VCPUS to 240
    
    240 has been well tested by Red Hat.
    
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d0845b289adb..5f90dce6fbd1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -35,7 +35,7 @@
 #include <asm/kvm_page_track.h>
 
 #define KVM_MAX_VCPUS 255
-#define KVM_SOFT_MAX_VCPUS 160
+#define KVM_SOFT_MAX_VCPUS 240
 #define KVM_USER_MEM_SLOTS 509
 /* memory slots that are not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 3

commit ffb128c89b77b44da18ccf51844a8e750e2c427a
Author: Bandan Das <bsd@redhat.com>
Date:   Tue Jul 12 18:18:49 2016 -0400

    kvm: mmu: don't set the present bit unconditionally
    
    To support execute only mappings on behalf of L1
    hypervisors, we need to teach set_spte() to honor all three of
    L1's XWR bits.  As a start, add a new variable "shadow_present_mask"
    that will be set for non-EPT shadow paging and clear for EPT.
    
    Signed-off-by: Bandan Das <bsd@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7a628fb6a2c2..d0845b289adb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1031,7 +1031,7 @@ void kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_init_vm(struct kvm *kvm);
 void kvm_mmu_uninit_vm(struct kvm *kvm);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
-		u64 dirty_mask, u64 nx_mask, u64 x_mask);
+		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 p_mask);
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,

commit c45dcc71b794b5a346a43ad83bdcfac2138f0a2c
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Wed Jun 22 14:59:56 2016 +0800

    KVM: VMX: enable guest access to LMCE related MSRs
    
    On Intel platforms, this patch adds LMCE to KVM MCE supported
    capabilities and handles guest access to LMCE related MSRs.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    [Haozhong: macro KVM_MCE_CAP_SUPPORTED => variable kvm_mce_cap_supported
               Only enable LMCE on Intel platform
               Check MSR_IA32_FEATURE_CONTROL when handling guest
                 access to MSR_IA32_MCG_EXT_CTL]
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 360c5171ea1a..7a628fb6a2c2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -598,6 +598,7 @@ struct kvm_vcpu_arch {
 	u64 mcg_cap;
 	u64 mcg_status;
 	u64 mcg_ctl;
+	u64 mcg_ext_ctl;
 	u64 *mce_banks;
 
 	/* Cache MMIO info */
@@ -1008,6 +1009,8 @@ struct kvm_x86_ops {
 
 	int (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc);
 	void (*cancel_hv_timer)(struct kvm_vcpu *vcpu);
+
+	void (*setup_mce)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {
@@ -1082,6 +1085,8 @@ extern u64  kvm_max_tsc_scaling_ratio;
 /* 1ull << kvm_tsc_scaling_ratio_frac_bits */
 extern u64  kvm_default_tsc_scaling_ratio;
 
+extern u64 kvm_mce_cap_supported;
+
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */
 	EMULATE_USER_EXIT,    /* kvm_run ready for userspace exit */

commit 64672c95ea4c2f7096e519e826076867e8ef0938
Author: Yunhong Jiang <yunhong.jiang@intel.com>
Date:   Mon Jun 13 14:19:59 2016 -0700

    kvm: vmx: hook preemption timer support
    
    Hook the VMX preemption timer to the "hv timer" functionality added
    by the previous patch.  This includes: checking if the feature is
    supported, if the feature is broken on the CPU, the hooks to
    setup/clean the VMX preemption timer, arming the timer on vmentry
    and handling the vmexit.
    
    A module parameter states if the VMX preemption timer should be
    utilized.
    
    Signed-off-by: Yunhong Jiang <yunhong.jiang@intel.com>
    [Move hv_deadline_tsc to struct vcpu_vmx, use -1 as the "unset" value.
     Put all VMX bits here.  Enable it by default #yolo. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e055f3787dc9..360c5171ea1a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1079,6 +1079,8 @@ extern u32  kvm_max_guest_tsc_khz;
 extern u8   kvm_tsc_scaling_ratio_frac_bits;
 /* maximum allowed value of TSC scaling ratio */
 extern u64  kvm_max_tsc_scaling_ratio;
+/* 1ull << kvm_tsc_scaling_ratio_frac_bits */
+extern u64  kvm_default_tsc_scaling_ratio;
 
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */

commit ce7a058a2117f0bca2f42f2870a97bfa9aa8e099
Author: Yunhong Jiang <yunhong.jiang@gmail.com>
Date:   Mon Jun 13 14:20:01 2016 -0700

    KVM: x86: support using the vmx preemption timer for tsc deadline timer
    
    The VMX preemption timer can be used to virtualize the TSC deadline timer.
    The VMX preemption timer is armed when the vCPU is running, and a VMExit
    will happen if the virtual TSC deadline timer expires.
    
    When the vCPU thread is blocked because of HLT, KVM will switch to use
    an hrtimer, and then go back to the VMX preemption timer when the vCPU
    thread is unblocked.
    
    This solution avoids the complex OS's hrtimer system, and the host
    timer interrupt handling cost, replacing them with a little math
    (for guest->host TSC and host TSC->preemption timer conversion)
    and a cheaper VMexit.  This benefits latency for isolated pCPUs.
    
    [A word about performance... Yunhong reported a 30% reduction in average
     latency from cyclictest.  I made a similar test with tscdeadline_latency
     from kvm-unit-tests, and measured
    
     - ~20 clock cycles loss (out of ~3200, so less than 1% but still
       statistically significant) in the worst case where the test halts
       just after programming the TSC deadline timer
    
     - ~800 clock cycles gain (25% reduction in latency) in the best case
       where the test busy waits.
    
     I removed the VMX bits from Yunhong's patch, to concentrate them in the
     next patch - Paolo]
    
    Signed-off-by: Yunhong Jiang <yunhong.jiang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e0fbe7e70dc1..e055f3787dc9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1005,6 +1005,9 @@ struct kvm_x86_ops {
 	int (*update_pi_irte)(struct kvm *kvm, unsigned int host_irq,
 			      uint32_t guest_irq, bool set);
 	void (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);
+
+	int (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc);
+	void (*cancel_hv_timer)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit 7d669f50847481c52faf0656aea7b4be63113210
Author: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
Date:   Wed Jun 15 17:23:45 2016 -0500

    kvm: svm: Fix implicit declaration for __default_cpu_present_to_apicid()
    
    The commit 8221c1370056 ("svm: Manage vcpu load/unload when enable AVIC")
    introduces a build error due to implicit function declaration
    when #ifdef CONFIG_X86_32 and #ifndef CONFIG_X86_LOCAL_APIC
    (as reported by Kbuild test robot i386-randconfig-x0-06121009).
    
    So, this patch introduces kvm_cpu_get_apicid() wrapper
    around __default_cpu_present_to_apicid() with additional
    handling if CONFIG_X86_LOCAL_APIC is not defined.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Fixes: commit 8221c1370056 ("svm: Manage vcpu load/unload when enable AVIC")
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e0fbe7e70dc1..69e62862b622 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -27,6 +27,7 @@
 #include <linux/irqbypass.h>
 #include <linux/hyperv.h>
 
+#include <asm/apic.h>
 #include <asm/pvclock-abi.h>
 #include <asm/desc.h>
 #include <asm/mtrr.h>
@@ -1368,4 +1369,14 @@ static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
 
 static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
 
+static inline int kvm_cpu_get_apicid(int mps_cpu)
+{
+#ifdef CONFIG_X86_LOCAL_APIC
+	return __default_cpu_present_to_apicid(mps_cpu);
+#else
+	WARN_ON_ONCE(1);
+	return BAD_APICID;
+#endif
+}
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit 67c9dddc95ac16a09db996e8e4dcacfd94cf2306
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 10 17:01:23 2016 +0200

    KVM: x86: make hwapic_isr_update and hwapic_irr_update look the same
    
    Neither APICv nor AVIC actually need the first argument of
    hwapic_isr_update, but the vCPU makes more sense than passing the
    pointer to the whole virtual machine!  In fact in the APICv case it's
    just happening that the vCPU is used implicitly, through the loaded VMCS.
    
    The second argument instead is named differently, make it consistent.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 43e74384c606..e0fbe7e70dc1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -922,7 +922,7 @@ struct kvm_x86_ops {
 	bool (*get_enable_apicv)(void);
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
-	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
+	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);

commit be8ca170edf342cc45ae4a6431ef192e5cbe211e
Author: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
Date:   Wed May 4 14:09:49 2016 -0500

    KVM: x86: Introducing kvm_x86_ops.apicv_post_state_restore
    
    Adding kvm_x86_ops hooks to allow APICv to do post state restore.
    This is required to support VM save and restore feature.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7aaa10891e2c..43e74384c606 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1004,6 +1004,7 @@ struct kvm_x86_ops {
 
 	int (*update_pi_irte)(struct kvm *kvm, unsigned int host_irq,
 			      uint32_t guest_irq, bool set);
+	void (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit 18f40c53e10f8d1267dc47cce4487664eececd6d
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Wed May 4 14:09:48 2016 -0500

    svm: Add VMEXIT handlers for AVIC
    
    This patch introduces VMEXIT handlers, avic_incomplete_ipi_interception()
    and avic_unaccelerated_access_interception() along with two trace points
    (trace_kvm_avic_incomplete_ipi and trace_kvm_avic_unaccelerated_access).
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 337e13b0d01d..7aaa10891e2c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -775,6 +775,7 @@ struct kvm_arch {
 	bool disabled_lapic_found;
 
 	/* Struct members for AVIC */
+	u32 ldr_mode;
 	struct page *avic_logical_id_table_page;
 	struct page *avic_physical_id_table_page;
 };

commit 44a95dae1d229a4967bbfcc56fb2116a9b4fe942
Author: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
Date:   Wed May 4 14:09:46 2016 -0500

    KVM: x86: Detect and Initialize AVIC support
    
    This patch introduces AVIC-related data structure, and AVIC
    initialization code.
    
    There are three main data structures for AVIC:
        * Virtual APIC (vAPIC) backing page (per-VCPU)
        * Physical APIC ID table (per-VM)
        * Logical APIC ID table (per-VM)
    
    Currently, AVIC is disabled by default. Users can manually
    enable AVIC via kernel boot option kvm-amd.avic=1 or during
    kvm-amd module loading with parameter avic=1.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    [Avoid extra indentation (Boris). - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2df5db6fb58b..337e13b0d01d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -773,6 +773,10 @@ struct kvm_arch {
 	u8 nr_reserved_ioapic_pins;
 
 	bool disabled_lapic_found;
+
+	/* Struct members for AVIC */
+	struct page *avic_logical_id_table_page;
+	struct page *avic_physical_id_table_page;
 };
 
 struct kvm_vm_stat {

commit d1ed092f77ef562edcc1b45ae331ff1683f50295
Author: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
Date:   Wed May 4 14:09:43 2016 -0500

    KVM: x86: Introducing kvm_x86_ops VCPU blocking/unblocking hooks
    
    Adding new function pointer in struct kvm_x86_ops, and calling them
    from the kvm_arch_vcpu[blocking/unblocking].
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d644226737ee..2df5db6fb58b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -993,6 +993,10 @@ struct kvm_x86_ops {
 	 */
 	int (*pre_block)(struct kvm_vcpu *vcpu);
 	void (*post_block)(struct kvm_vcpu *vcpu);
+
+	void (*vcpu_blocking)(struct kvm_vcpu *vcpu);
+	void (*vcpu_unblocking)(struct kvm_vcpu *vcpu);
+
 	int (*update_pi_irte)(struct kvm *kvm, unsigned int host_irq,
 			      uint32_t guest_irq, bool set);
 };
@@ -1344,8 +1348,18 @@ bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 void kvm_set_msi_irq(struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq);
 
-static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu) {}
-static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu) {}
+static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)
+{
+	if (kvm_x86_ops->vcpu_blocking)
+		kvm_x86_ops->vcpu_blocking(vcpu);
+}
+
+static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
+{
+	if (kvm_x86_ops->vcpu_unblocking)
+		kvm_x86_ops->vcpu_unblocking(vcpu);
+}
+
 static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit 03543133cea646406307870823343912c1ef0a3a
Author: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
Date:   Wed May 4 14:09:42 2016 -0500

    KVM: x86: Introducing kvm_x86_ops VM init/destroy hooks
    
    Adding function pointers in struct kvm_x86_ops for processor-specific
    layer to provide hooks for when KVM initialize and destroy VM.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c99494b4bdf7..d644226737ee 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -848,6 +848,9 @@ struct kvm_x86_ops {
 	bool (*cpu_has_high_real_mode_segbase)(void);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
+	int (*vm_init)(struct kvm *kvm);
+	void (*vm_destroy)(struct kvm *kvm);
+
 	/* Create, but do not attach this VCPU */
 	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);

commit 3491caf2755e9f312666712510d80b00c81ff247
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 13 12:16:35 2016 +0200

    KVM: halt_polling: provide a way to qualify wakeups during poll
    
    Some wakeups should not be considered a sucessful poll. For example on
    s390 I/O interrupts are usually floating, which means that _ALL_ CPUs
    would be considered runnable - letting all vCPUs poll all the time for
    transactional like workload, even if one vCPU would be enough.
    This can result in huge CPU usage for large guests.
    This patch lets architectures provide a way to qualify wakeups if they
    should be considered a good/bad wakeups in regard to polls.
    
    For s390 the implementation will fence of halt polling for anything but
    known good, single vCPU events. The s390 implementation for floating
    interrupts does a wakeup for one vCPU, but the interrupt will be delivered
    by whatever CPU checks first for a pending interrupt. We prefer the
    woken up CPU by marking the poll of this CPU as "good" poll.
    This code will also mark several other wakeup reasons like IPI or
    expired timers as "good". This will of course also mark some events as
    not sucessful. As  KVM on z runs always as a 2nd level hypervisor,
    we prefer to not poll, unless we are really sure, though.
    
    This patch successfully limits the CPU usage for cases like uperf 1byte
    transactional ping pong workload or wakeup heavy workload like OLTP
    while still providing a proper speedup.
    
    This also introduced a new vcpu stat "halt_poll_no_tuning" that marks
    wakeups that are considered not good for polling.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Radim Krčmář <rkrcmar@redhat.com> (for an earlier version)
    Cc: David Matlack <dmatlack@google.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    [Rename config symbol. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c66e26280707..c99494b4bdf7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -803,6 +803,7 @@ struct kvm_vcpu_stat {
 	u32 halt_exits;
 	u32 halt_successful_poll;
 	u32 halt_attempted_poll;
+	u32 halt_poll_invalid;
 	u32 halt_wakeup;
 	u32 request_irq_exits;
 	u32 irq_exits;
@@ -1342,5 +1343,6 @@ void kvm_set_msi_irq(struct kvm_kernel_irq_routing_entry *e,
 
 static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu) {}
 static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu) {}
+static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit c54cdf141c40a5115774e91fc947c34e91df0259
Author: Liang Chen <liangchen.linux@gmail.com>
Date:   Wed Mar 16 19:33:16 2016 +0800

    KVM: x86: optimize steal time calculation
    
    Since accumulate_steal_time is now only called in record_steal_time, it
    doesn't quite make sense to put the delta calculation in a separate
    function. The function could be called thousands of times before guest
    enables the steal time MSR (though the compiler may optimize out this
    function call). And after it's enabled, the MSR enable bit is tested twice
    every time. Removing the accumulate_steal_time function also avoids the
    necessity of having the accum_steal field.
    
    Signed-off-by: Liang Chen <liangchen.linux@gmail.com>
    Signed-off-by: Gavin Guo <gavin.guo@canonical.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b7e394485a5f..c66e26280707 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -562,7 +562,6 @@ struct kvm_vcpu_arch {
 	struct {
 		u64 msr_val;
 		u64 last_steal;
-		u64 accum_steal;
 		struct gfn_to_hva_cache stime;
 		struct kvm_steal_time steal;
 	} st;

commit 14ebda3394fd3e5388747e742e510b0802a65d24
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Mar 29 17:56:57 2016 +0200

    KVM: x86: reduce default value of halt_poll_ns parameter
    
    Windows lets applications choose the frequency of the timer tick,
    and in Windows 10 the maximum rate was changed from 1024 Hz to
    2048 Hz.  Unfortunately, because of the way the Windows API
    works, most applications who need a higher rate than the default
    64 Hz will just do
    
       timeGetDevCaps(&tc, sizeof(tc));
       timeBeginPeriod(tc.wPeriodMin);
    
    and pick the maximum rate.  This causes very high CPU usage when
    playing media or games on Windows 10, even if the guest does not
    actually use the CPU very much, because the frequent timer tick
    causes halt_poll_ns to kick in.
    
    There is no really good solution, especially because Microsoft
    could sooner or later bump the limit to 4096 Hz, but for now
    the best we can do is lower a bit the upper limit for
    halt_poll_ns. :-(
    
    Reported-by: Jon Panozzo <jonp@lime-technology.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f62a9f37f79f..b7e394485a5f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -43,7 +43,7 @@
 
 #define KVM_PIO_PAGE_OFFSET 1
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 2
-#define KVM_HALT_POLL_NS_DEFAULT 500000
+#define KVM_HALT_POLL_NS_DEFAULT 400000
 
 #define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS
 

commit b9baba861489041b37b54fc7ee0b0006b5327151
Author: Huaitong Han <huaitong.han@intel.com>
Date:   Tue Mar 22 16:51:21 2016 +0800

    KVM, pkeys: expose CPUID/CR4 to guest
    
    X86_FEATURE_PKU is referred to as "PKU" in the hardware documentation:
    CPUID.7.0.ECX[3]:PKU. X86_FEATURE_OSPKE is software support for pkeys,
    enumerated with CPUID.7.0.ECX[4]:OSPKE, and it reflects the setting of
    CR4.PKE(bit 22).
    
    This patch disables CPUID:PKU without ept, because pkeys is not yet
    implemented for shadow paging.
    
    Signed-off-by: Huaitong Han <huaitong.han@intel.com>
    Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0d75ecdfa077..f62a9f37f79f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -84,7 +84,8 @@
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
 			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR | X86_CR4_PCIDE \
 			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_FSGSBASE \
-			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE | X86_CR4_SMAP))
+			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE | X86_CR4_SMAP \
+			  | X86_CR4_PKE))
 
 #define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)
 

commit be94f6b71067df47d623fc6c6983a8dee504fb4d
Author: Huaitong Han <huaitong.han@intel.com>
Date:   Tue Mar 22 16:51:20 2016 +0800

    KVM, pkeys: add pkeys support for permission_fault
    
    Protection keys define a new 4-bit protection key field (PKEY) in bits
    62:59 of leaf entries of the page tables, the PKEY is an index to PKRU
    register(16 domains), every domain has 2 bits(write disable bit, access
    disable bit).
    
    Static logic has been produced in update_pkru_bitmask, dynamic logic need
    read pkey from page table entries, get pkru value, and deduce the correct
    result.
    
    [ Huaitong: Xiao helps to modify many sections. ]
    
    Signed-off-by: Huaitong Han <huaitong.han@intel.com>
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8968165963e4..0d75ecdfa077 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -187,12 +187,14 @@ enum {
 #define PFERR_USER_BIT 2
 #define PFERR_RSVD_BIT 3
 #define PFERR_FETCH_BIT 4
+#define PFERR_PK_BIT 5
 
 #define PFERR_PRESENT_MASK (1U << PFERR_PRESENT_BIT)
 #define PFERR_WRITE_MASK (1U << PFERR_WRITE_BIT)
 #define PFERR_USER_MASK (1U << PFERR_USER_BIT)
 #define PFERR_RSVD_MASK (1U << PFERR_RSVD_BIT)
 #define PFERR_FETCH_MASK (1U << PFERR_FETCH_BIT)
+#define PFERR_PK_MASK (1U << PFERR_PK_BIT)
 
 /* apic attention bits */
 #define KVM_APIC_CHECK_VAPIC	0
@@ -882,6 +884,7 @@ struct kvm_x86_ops {
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
+	u32 (*get_pkru)(struct kvm_vcpu *vcpu);
 	void (*fpu_activate)(struct kvm_vcpu *vcpu);
 	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);
 

commit 2d344105f57ca77fc9c5d4377f65d1082f71ac4b
Author: Huaitong Han <huaitong.han@intel.com>
Date:   Tue Mar 22 16:51:19 2016 +0800

    KVM, pkeys: introduce pkru_mask to cache conditions
    
    PKEYS defines a new status bit in the PFEC. PFEC.PK (bit 5), if some
    conditions is true, the fault is considered as a PKU violation.
    pkru_mask indicates if we need to check PKRU.ADi and PKRU.WDi, and
    does cache some conditions for permission_fault.
    
    [ Huaitong: Xiao helps to modify many sections. ]
    
    Signed-off-by: Huaitong Han <huaitong.han@intel.com>
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 01c8b501cb6d..8968165963e4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -335,6 +335,14 @@ struct kvm_mmu {
 	 */
 	u8 permissions[16];
 
+	/*
+	* The pkru_mask indicates if protection key checks are needed.  It
+	* consists of 16 domains indexed by page fault error code bits [4:1],
+	* with PFEC.RSVD replaced by ACC_USER_MASK from the page tables.
+	* Each domain has 2 bits which are ANDed with AD and WD from PKRU.
+	*/
+	u32 pkru_mask;
+
 	u64 *pae_root;
 	u64 *lm_root;
 

commit 5a5fbdc0e3f1159a734f1890da60fce70e98271d
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Mar 8 10:00:11 2016 +0100

    KVM: x86: remove eager_fpu field of struct kvm_vcpu_arch
    
    It is now equal to use_eager_fpu(), which simply tests a cpufeature bit.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d110dc44d6c2..01c8b501cb6d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -503,7 +503,6 @@ struct kvm_vcpu_arch {
 	struct kvm_mmu_memory_cache mmu_page_header_cache;
 
 	struct fpu guest_fpu;
-	bool eager_fpu;
 	u64 xcr0;
 	u64 guest_supported_xcr0;
 	u32 guest_xstate_size;

commit 6bb69c9b69c315200ddc2bc79aee14c0184cf5b2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Feb 23 12:51:19 2016 +0100

    KVM: MMU: simplify last_pte_bitmap
    
    Branch-free code is fun and everybody knows how much Avi loves it,
    but last_pte_bitmap takes it a bit to the extreme.  Since the code
    is simply doing a range check, like
    
            (level == 1 ||
             ((gpte & PT_PAGE_SIZE_MASK) && level < N)
    
    we can make it branch-free without storing the entire truth table;
    it is enough to cache N.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1c3e390993a2..d110dc44d6c2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -347,12 +347,8 @@ struct kvm_mmu {
 
 	struct rsvd_bits_validate guest_rsvd_check;
 
-	/*
-	 * Bitmap: bit set = last pte in walk
-	 * index[0:1]: level (zero-based)
-	 * index[2]: pte.ps
-	 */
-	u8 last_pte_bitmap;
+	/* Can have large pages at levels 2..last_nonleaf_level-1. */
+	u8 last_nonleaf_level;
 
 	bool nx;
 

commit 13d268ca2c4c29d6da2ba79419f9a655e602afed
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Wed Feb 24 17:51:16 2016 +0800

    KVM: MMU: apply page track notifier
    
    Register the notifier to receive write track event so that we can update
    our shadow page table
    
    It makes kvm_mmu_pte_write() be the callback of the notifier, no function
    is changed
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index eb68e6aca0cf..1c3e390993a2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -704,6 +704,7 @@ struct kvm_arch {
 	 */
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
+	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
 
 	struct list_head assigned_dev_head;
@@ -1001,6 +1002,8 @@ void kvm_mmu_module_exit(void);
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
 int kvm_mmu_create(struct kvm_vcpu *vcpu);
 void kvm_mmu_setup(struct kvm_vcpu *vcpu);
+void kvm_mmu_init_vm(struct kvm *kvm);
+void kvm_mmu_uninit_vm(struct kvm *kvm);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask);
 
@@ -1140,8 +1143,6 @@ void kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);
 
 void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
-void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
-		       const u8 *new, int bytes);
 int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn);
 int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva);
 void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);

commit 0eb05bf290cfe8610d9680b49abef37febd1c38a
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Wed Feb 24 17:51:13 2016 +0800

    KVM: page track: add notifier support
    
    Notifier list is introduced so that any node wants to receive the track
    event can register to the list
    
    Two APIs are introduced here:
    - kvm_page_track_register_notifier(): register the notifier to receive
      track event
    
    - kvm_page_track_unregister_notifier(): stop receiving track event by
      unregister the notifier
    
    The callback, node->track_write() is called when a write access on the
    write tracked page happens
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e2fc5c0ec86a..eb68e6aca0cf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -704,6 +704,7 @@ struct kvm_arch {
 	 */
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
+	struct kvm_page_track_notifier_head track_notifier_head;
 
 	struct list_head assigned_dev_head;
 	struct iommu_domain *iommu_domain;

commit e5691a81e830c12d396b3f219ab999be87a1208f
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Wed Feb 24 17:51:12 2016 +0800

    KVM: MMU: clear write-flooding on the fast path of tracked page
    
    If the page fault is caused by write access on write tracked page, the
    real shadow page walking is skipped, we lost the chance to clear write
    flooding for the page structure current vcpu is using
    
    Fix it by locklessly waking shadow page table to clear write flooding
    on the shadow page structure out of mmu-lock. So that we change the
    count to atomic_t
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 71e43fe04bbc..e2fc5c0ec86a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -285,7 +285,7 @@ struct kvm_mmu_page {
 #endif
 
 	/* Number of writes since the last time traversal visited this page.  */
-	int write_flooding_count;
+	atomic_t write_flooding_count;
 };
 
 struct kvm_pio_request {

commit 21ebbedaddf25a35a70fedc001ba7e5f5b9129bc
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Wed Feb 24 17:51:09 2016 +0800

    KVM: page track: add the framework of guest page tracking
    
    The array, gfn_track[mode][gfn], is introduced in memory slot for every
    guest page, this is the tracking count for the gust page on different
    modes. If the page is tracked then the count is increased, the page is
    not tracked after the count reaches zero
    
    We use 'unsigned short' as the tracking count which should be enough as
    shadow page table only can use 2^14 (2^3 for level, 2^1 for cr4_pae, 2^2
    for quadrant, 2^3 for access, 2^1 for nxe, 2^1 for cr0_wp, 2^1 for
    smep_andnot_wp, 2^1 for smap_andnot_wp, and 2^1 for smm) at most, there
    is enough room for other trackers
    
    Two callbacks, kvm_page_track_create_memslot() and
    kvm_page_track_free_memslot() are implemented in this patch, they are
    internally used to initialize and reclaim the memory of the array
    
    Currently, only write track mode is supported
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1f7fed5f35fc..71e43fe04bbc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -32,6 +32,7 @@
 #include <asm/mtrr.h>
 #include <asm/msr-index.h>
 #include <asm/asm.h>
+#include <asm/kvm_page_track.h>
 
 #define KVM_MAX_VCPUS 255
 #define KVM_SOFT_MAX_VCPUS 160
@@ -214,6 +215,14 @@ struct kvm_mmu_memory_cache {
 	void *objects[KVM_NR_MEM_OBJS];
 };
 
+/*
+ * the pages used as guest page table on soft mmu are tracked by
+ * kvm_memory_slot.arch.gfn_track which is 16 bits, so the role bits used
+ * by indirect shadow page can not be more than 15 bits.
+ *
+ * Currently, we used 14 bits that are @level, @cr4_pae, @quadrant, @access,
+ * @nxe, @cr0_wp, @smep_andnot_wp and @smap_andnot_wp.
+ */
 union kvm_mmu_page_role {
 	unsigned word;
 	struct {
@@ -650,6 +659,7 @@ struct kvm_lpage_info {
 struct kvm_arch_memory_slot {
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
 };
 
 /*

commit 92f94f1e9e509caa564353c516c904278999e350
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Wed Feb 24 17:51:06 2016 +0800

    KVM: MMU: rename has_wrprotected_page to mmu_gfn_lpage_is_disallowed
    
    kvm_lpage_info->write_count is used to detect if the large page mapping
    for the gfn on the specified level is allowed, rename it to disallow_lpage
    to reflect its purpose, also we rename has_wrprotected_page() to
    mmu_gfn_lpage_is_disallowed() to make the code more clearer
    
    Later we will extend this mechanism for page tracking: if the gfn is
    tracked then large mapping for that gfn on any level is not allowed.
    The new name is more straightforward
    
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7b5459982433..1f7fed5f35fc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -644,7 +644,7 @@ struct kvm_vcpu_arch {
 };
 
 struct kvm_lpage_info {
-	int write_count;
+	int disallow_lpage;
 };
 
 struct kvm_arch_memory_slot {

commit 520040146a0af36f7875ec06b58f44b19a0edf53
Author: Feng Wu <feng.wu@intel.com>
Date:   Mon Jan 25 16:53:33 2016 +0800

    KVM: x86: Use vector-hashing to deliver lowest-priority interrupts
    
    Use vector-hashing to deliver lowest-priority interrupts, As an
    example, modern Intel CPUs in server platform use this method to
    handle lowest-priority interrupts.
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 44adbb819041..7b5459982433 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -754,6 +754,8 @@ struct kvm_arch {
 
 	bool irqchip_split;
 	u8 nr_reserved_ioapic_pins;
+
+	bool disabled_lapic_found;
 };
 
 struct kvm_vm_stat {

commit 2860c4b1678646c99f5f1d77d026cd12ffd8a3a9
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 7 15:05:10 2016 +0100

    KVM: move architecture-dependent requests to arch/
    
    Since the numbers now overlap, it makes sense to enumerate
    them in asm/kvm_host.h rather than linux/kvm_host.h.  Functions
    that refer to architecture-specific requests are also moved
    to arch/.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a7c89876698b..44adbb819041 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -46,6 +46,31 @@
 
 #define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS
 
+/* x86-specific vcpu->requests bit members */
+#define KVM_REQ_MIGRATE_TIMER      8
+#define KVM_REQ_REPORT_TPR_ACCESS  9
+#define KVM_REQ_TRIPLE_FAULT      10
+#define KVM_REQ_MMU_SYNC          11
+#define KVM_REQ_CLOCK_UPDATE      12
+#define KVM_REQ_DEACTIVATE_FPU    13
+#define KVM_REQ_EVENT             14
+#define KVM_REQ_APF_HALT          15
+#define KVM_REQ_STEAL_UPDATE      16
+#define KVM_REQ_NMI               17
+#define KVM_REQ_PMU               18
+#define KVM_REQ_PMI               19
+#define KVM_REQ_SMI               20
+#define KVM_REQ_MASTERCLOCK_UPDATE 21
+#define KVM_REQ_MCLOCK_INPROGRESS 22
+#define KVM_REQ_SCAN_IOAPIC       23
+#define KVM_REQ_GLOBAL_CLOCK_UPDATE 24
+#define KVM_REQ_APIC_PAGE_RELOAD  25
+#define KVM_REQ_HV_CRASH          26
+#define KVM_REQ_IOAPIC_EOI_EXIT   27
+#define KVM_REQ_HV_RESET          28
+#define KVM_REQ_HV_EXIT           29
+#define KVM_REQ_HV_STIMER         30
+
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
 			  | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM \
@@ -1268,6 +1293,9 @@ u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc);
 unsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu);
 bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
 
+void kvm_make_mclock_inprogress_request(struct kvm *kvm);
+void kvm_make_scan_ioapic_request(struct kvm *kvm);
+
 void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work);
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,

commit 1f4b34f825e8cef6f493d06b46605384785b3d16
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Mon Nov 30 19:22:21 2015 +0300

    kvm/x86: Hyper-V SynIC timers
    
    Per Hyper-V specification (and as required by Hyper-V-aware guests),
    SynIC provides 4 per-vCPU timers.  Each timer is programmed via a pair
    of MSRs, and signals expiration by delivering a special format message
    to the configured SynIC message slot and triggering the corresponding
    synthetic interrupt.
    
    Note: as implemented by this patch, all periodic timers are "lazy"
    (i.e. if the vCPU wasn't scheduled for more than the timer period the
    timer events are lost), regardless of the corresponding configuration
    MSR.  If deemed necessary, the "catch up" mode (the timer period is
    shortened until the timer catches up) will be implemented later.
    
    Changes v2:
    * Use remainder to calculate periodic timer expiration time
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: "K. Y. Srinivasan" <kys@microsoft.com>
    CC: Haiyang Zhang <haiyangz@microsoft.com>
    CC: Vitaly Kuznetsov <vkuznets@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 814007701f8b..a7c89876698b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -379,6 +379,17 @@ struct kvm_mtrr {
 	struct list_head head;
 };
 
+/* Hyper-V SynIC timer */
+struct kvm_vcpu_hv_stimer {
+	struct hrtimer timer;
+	int index;
+	u64 config;
+	u64 count;
+	u64 exp_time;
+	struct hv_message msg;
+	bool msg_pending;
+};
+
 /* Hyper-V synthetic interrupt controller (SynIC)*/
 struct kvm_vcpu_hv_synic {
 	u64 version;
@@ -398,6 +409,8 @@ struct kvm_vcpu_hv {
 	s64 runtime_offset;
 	struct kvm_vcpu_hv_synic synic;
 	struct kvm_hyperv_exit exit;
+	struct kvm_vcpu_hv_stimer stimer[HV_SYNIC_STIMER_COUNT];
+	DECLARE_BITMAP(stimer_pending_bitmap, HV_SYNIC_STIMER_COUNT);
 };
 
 struct kvm_vcpu_arch {

commit 018aabb56d6109c8f12397c24e59f67c58870ac1
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Fri Nov 20 17:41:28 2015 +0900

    KVM: x86: MMU: Encapsulate the type of rmap-chain head in a new struct
    
    New struct kvm_rmap_head makes the code type-safe to some extent.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f608e170ba3d..814007701f8b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -214,6 +214,10 @@ union kvm_mmu_page_role {
 	};
 };
 
+struct kvm_rmap_head {
+	unsigned long val;
+};
+
 struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
@@ -231,7 +235,7 @@ struct kvm_mmu_page {
 	bool unsync;
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
-	unsigned long parent_ptes;	/* Reverse mapping for parent_pte */
+	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
 
 	/* The page is obsolete if mmu_valid_gen != kvm->arch.mmu_valid_gen.  */
 	unsigned long mmu_valid_gen;
@@ -606,7 +610,7 @@ struct kvm_lpage_info {
 };
 
 struct kvm_arch_memory_slot {
-	unsigned long *rmap[KVM_NR_PAGE_SIZES];
+	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 };
 

commit db3975717ac5e2c2761bae7b90c4f2e0abb5ef22
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Tue Nov 10 15:36:35 2015 +0300

    kvm/x86: Hyper-V kvm exit
    
    A new vcpu exit is introduced to notify the userspace of the
    changes in Hyper-V SynIC configuration triggered by guest writing to the
    corresponding MSRs.
    
    Changes v4:
    * exit into userspace only if guest writes into SynIC MSR's
    
    Changes v3:
    * added KVM_EXIT_HYPERV types and structs notes into docs
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bab47b61d2b0..f608e170ba3d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -393,6 +393,7 @@ struct kvm_vcpu_hv {
 	u64 hv_vapic;
 	s64 runtime_offset;
 	struct kvm_vcpu_hv_synic synic;
+	struct kvm_hyperv_exit exit;
 };
 
 struct kvm_vcpu_arch {

commit 5c919412fe61c35947816fdbd5f7bd09fe0dd073
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Tue Nov 10 15:36:34 2015 +0300

    kvm/x86: Hyper-V synthetic interrupt controller
    
    SynIC (synthetic interrupt controller) is a lapic extension,
    which is controlled via MSRs and maintains for each vCPU
     - 16 synthetic interrupt "lines" (SINT's); each can be configured to
       trigger a specific interrupt vector optionally with auto-EOI
       semantics
     - a message page in the guest memory with 16 256-byte per-SINT message
       slots
     - an event flag page in the guest memory with 16 2048-bit per-SINT
       event flag areas
    
    The host triggers a SINT whenever it delivers a new message to the
    corresponding slot or flips an event flag bit in the corresponding area.
    The guest informs the host that it can try delivering a message by
    explicitly asserting EOI in lapic or writing to End-Of-Message (EOM)
    MSR.
    
    The userspace (qemu) triggers interrupts and receives EOM notifications
    via irqfd with resampler; for that, a GSI is allocated for each
    configured SINT, and irq_routing api is extended to support GSI-SINT
    mapping.
    
    Changes v4:
    * added activation of SynIC by vcpu KVM_ENABLE_CAP
    * added per SynIC active flag
    * added deactivation of APICv upon SynIC activation
    
    Changes v3:
    * added KVM_CAP_HYPERV_SYNIC and KVM_IRQ_ROUTING_HV_SINT notes into
    docs
    
    Changes v2:
    * do not use posted interrupts for Hyper-V SynIC AutoEOI vectors
    * add Hyper-V SynIC vectors into EOI exit bitmap
    * Hyper-V SyniIC SINT msr write logic simplified
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bac0d540f49c..bab47b61d2b0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -25,6 +25,7 @@
 #include <linux/pvclock_gtod.h>
 #include <linux/clocksource.h>
 #include <linux/irqbypass.h>
+#include <linux/hyperv.h>
 
 #include <asm/pvclock-abi.h>
 #include <asm/desc.h>
@@ -374,10 +375,24 @@ struct kvm_mtrr {
 	struct list_head head;
 };
 
+/* Hyper-V synthetic interrupt controller (SynIC)*/
+struct kvm_vcpu_hv_synic {
+	u64 version;
+	u64 control;
+	u64 msg_page;
+	u64 evt_page;
+	atomic64_t sint[HV_SYNIC_SINT_COUNT];
+	atomic_t sint_to_gsi[HV_SYNIC_SINT_COUNT];
+	DECLARE_BITMAP(auto_eoi_bitmap, 256);
+	DECLARE_BITMAP(vec_bitmap, 256);
+	bool active;
+};
+
 /* Hyper-V per vcpu emulation context */
 struct kvm_vcpu_hv {
 	u64 hv_vapic;
 	s64 runtime_offset;
+	struct kvm_vcpu_hv_synic synic;
 };
 
 struct kvm_vcpu_arch {

commit d62caabb41f33d96333f9ef15e09cd26e1c12760
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Tue Nov 10 15:36:33 2015 +0300

    kvm/x86: per-vcpu apicv deactivation support
    
    The decision on whether to use hardware APIC virtualization used to be
    taken globally, based on the availability of the feature in the CPU
    and the value of a module parameter.
    
    However, under certain circumstances we want to control it on per-vcpu
    basis.  In particular, when the userspace activates HyperV synthetic
    interrupt controller (SynIC), APICv has to be disabled as it's
    incompatible with SynIC auto-EOI behavior.
    
    To achieve that, introduce 'apicv_active' flag on struct
    kvm_vcpu_arch, and kvm_vcpu_deactivate_apicv() function to turn APICv
    off.  The flag is initialized based on the module parameter and CPU
    capability, and consulted whenever an APICv-specific action is
    performed.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f6d8894f25b4..bac0d540f49c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -400,6 +400,7 @@ struct kvm_vcpu_arch {
 	u64 efer;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
+	bool apicv_active;
 	DECLARE_BITMAP(ioapic_handled_vectors, 256);
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;
@@ -831,7 +832,8 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
-	int (*cpu_uses_apicv)(struct kvm_vcpu *vcpu);
+	bool (*get_enable_apicv)(void);
+	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
@@ -1086,6 +1088,8 @@ gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,
 gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
 				struct x86_exception *exception);
 
+void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu);
+
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u32 error_code,

commit 6308630bd3dbb6a8a883c4c571ce5e5a759a8a0e
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Tue Nov 10 15:36:32 2015 +0300

    kvm/x86: split ioapic-handled and EOI exit bitmaps
    
    The function to determine if the vector is handled by ioapic used to
    rely on the fact that only ioapic-handled vectors were set up to
    cause vmexits when virtual apic was in use.
    
    We're going to break this assumption when introducing Hyper-V
    synthetic interrupts: they may need to cause vmexits too.
    
    To achieve that, introduce a new bitmap dedicated specifically for
    ioapic-handled vectors, and populate EOI exit bitmap from it for now.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 30cfd64295a0..f6d8894f25b4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -400,7 +400,7 @@ struct kvm_vcpu_arch {
 	u64 efer;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
-	u64 eoi_exit_bitmap[4];
+	DECLARE_BITMAP(ioapic_handled_vectors, 256);
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;
 	int mp_state;
@@ -834,7 +834,7 @@ struct kvm_x86_ops {
 	int (*cpu_uses_apicv)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
-	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu);
+	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);

commit a96036b8ef7df9f10cd575c0d78359bd33188e8e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Nov 10 11:55:36 2015 +0100

    KVM: x86: rename update_db_bp_intercept to update_bp_intercept
    
    Because #DB is now intercepted unconditionally, this callback
    only operates on #BP for both VMX and SVM.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 456a3869a57e..30cfd64295a0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -778,7 +778,7 @@ struct kvm_x86_ops {
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
 
-	void (*update_db_bp_intercept)(struct kvm_vcpu *vcpu);
+	void (*update_bp_intercept)(struct kvm_vcpu *vcpu);
 	int (*get_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);
 	int (*set_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);
 	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);

commit 4ba76538dd52dd9b18b464e509cb8f3ed4ed993f
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:07 2015 +0800

    KVM: x86: Move TSC scaling logic out of call-back read_l1_tsc()
    
    Both VMX and SVM scales the host TSC in the same way in call-back
    read_l1_tsc(), so this patch moves the scaling logic from call-back
    read_l1_tsc() to a common function kvm_read_l1_tsc().
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8465944fe8da..456a3869a57e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1226,6 +1226,7 @@ void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
 u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc);
+u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc);
 
 unsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu);
 bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);

commit 58ea6767874e791a6c4f5c96c7d9155de4b1af28
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:06 2015 +0800

    KVM: x86: Move TSC scaling logic out of call-back adjust_tsc_offset()
    
    For both VMX and SVM, if the 2nd argument of call-back
    adjust_tsc_offset() is the host TSC, then adjust_tsc_offset() will scale
    it first. This patch moves this common TSC scaling logic to its caller
    adjust_tsc_offset_host() and rename the call-back adjust_tsc_offset() to
    adjust_tsc_offset_guest().
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 672f960e8144..8465944fe8da 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -845,7 +845,7 @@ struct kvm_x86_ops {
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
 	bool (*invpcid_supported)(void);
-	void (*adjust_tsc_offset)(struct kvm_vcpu *vcpu, s64 adjustment, bool host);
+	void (*adjust_tsc_offset_guest)(struct kvm_vcpu *vcpu, s64 adjustment);
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 
@@ -922,17 +922,6 @@ struct kvm_arch_async_pf {
 
 extern struct kvm_x86_ops *kvm_x86_ops;
 
-static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
-					   s64 adjustment)
-{
-	kvm_x86_ops->adjust_tsc_offset(vcpu, adjustment, false);
-}
-
-static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
-{
-	kvm_x86_ops->adjust_tsc_offset(vcpu, adjustment, true);
-}
-
 int kvm_mmu_module_init(void);
 void kvm_mmu_module_exit(void);
 

commit 07c1419a32bbba08cf1efb6d1ecaf24f174fa4c3
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:05 2015 +0800

    KVM: x86: Replace call-back compute_tsc_offset() with a common function
    
    Both VMX and SVM calculate the tsc-offset in the same way, so this
    patch removes the call-back compute_tsc_offset() and replaces it with a
    common function kvm_compute_tsc_offset().
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c5a3f3d66e90..672f960e8144 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -856,7 +856,6 @@ struct kvm_x86_ops {
 	u64 (*read_tsc_offset)(struct kvm_vcpu *vcpu);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
-	u64 (*compute_tsc_offset)(struct kvm_vcpu *vcpu, u64 target_tsc);
 	u64 (*read_l1_tsc)(struct kvm_vcpu *vcpu, u64 host_tsc);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);

commit 381d585c80e34988269bd7901ad910981e900be1
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:04 2015 +0800

    KVM: x86: Replace call-back set_tsc_khz() with a common function
    
    Both VMX and SVM propagate virtual_tsc_khz in the same way, so this
    patch removes the call-back set_tsc_khz() and replaces it with a common
    function.
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 52d1419968eb..c5a3f3d66e90 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -853,7 +853,6 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
-	void (*set_tsc_khz)(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale);
 	u64 (*read_tsc_offset)(struct kvm_vcpu *vcpu);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 

commit 35181e86df97e4223f4a28fb33e2bcf3b73de141
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:03 2015 +0800

    KVM: x86: Add a common TSC scaling function
    
    VMX and SVM calculate the TSC scaling ratio in a similar logic, so this
    patch generalizes it to a common TSC scaling function.
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    [Inline the multiplication and shift steps into mul_u64_u64_shr.  Remove
     BUG_ON.  - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f3354bd92364..52d1419968eb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1238,6 +1238,8 @@ void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
+u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc);
+
 unsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu);
 bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
 

commit ad721883e9c5f46cc5fa9496bc12c097c6238b4a
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:02 2015 +0800

    KVM: x86: Add a common TSC scaling ratio field in kvm_vcpu_arch
    
    This patch moves the field of TSC scaling ratio from the architecture
    struct vcpu_svm to the common struct kvm_vcpu_arch.
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5333767560c0..f3354bd92364 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -505,6 +505,7 @@ struct kvm_vcpu_arch {
 	u32 virtual_tsc_mult;
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
+	u64 tsc_scaling_ratio;
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
 	unsigned nmi_pending; /* NMI queued after currently running handler */

commit bc9b961b357ea8129d75613b7af4fdf57ced9b9f
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:01 2015 +0800

    KVM: x86: Collect information for setting TSC scaling ratio
    
    The number of bits of the fractional part of the 64-bit TSC scaling
    ratio in VMX and SVM is different. This patch makes the architecture
    code to collect the number of fractional bits and other related
    information into variables that can be accessed in the common code.
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b0322d8df03b..5333767560c0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -988,6 +988,10 @@ u64 vcpu_tsc_khz(struct kvm_vcpu *vcpu);
 extern bool kvm_has_tsc_control;
 /* maximum supported tsc_khz for guests */
 extern u32  kvm_max_guest_tsc_khz;
+/* number of bits of the fractional part of the TSC scaling ratio */
+extern u8   kvm_tsc_scaling_ratio_frac_bits;
+/* maximum allowed value of TSC scaling ratio */
+extern u64  kvm_max_tsc_scaling_ratio;
 
 enum emulation_result {
 	EMULATE_DONE,         /* no further processing */

commit 893590c73426585dfd9f33358b19f18d9395fb2f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Nov 6 11:46:24 2015 +0100

    KVM: x86: declare a few variables as __read_mostly
    
    These include module parameters and variables that are set by
    kvm_x86_ops->hardware_setup.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9265196e877f..b0322d8df03b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -986,8 +986,6 @@ u64 vcpu_tsc_khz(struct kvm_vcpu *vcpu);
 
 /* control of guest tsc rate supported? */
 extern bool kvm_has_tsc_control;
-/* minimum supported tsc_khz for guests */
-extern u32  kvm_min_guest_tsc_khz;
 /* maximum supported tsc_khz for guests */
 extern u32  kvm_max_guest_tsc_khz;
 

commit 197a4f4b063e4e7a603ff1de56b3cf0400fabc30
Merge: d6cf98e06ea4 26caea7693cb
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Nov 4 16:24:17 2015 +0100

    Merge tag 'kvm-arm-for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM Changes for v4.4-rc1
    
    Includes a number of fixes for the arch-timer, introducing proper
    level-triggered semantics for the arch-timers, a series of patches to
    synchronously halt a guest (prerequisite for IRQ forwarding), some tracepoint
    improvements, a tweak for the EL2 panic handlers, some more VGIC cleanups
    getting rid of redundant state, and finally a stylistic change that gets rid of
    some ctags warnings.
    
    Conflicts:
            arch/x86/include/asm/kvm_host.h

commit 3217f7c25bca66eed9b07f0b8bfd1937169b0736
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Aug 27 16:41:15 2015 +0200

    KVM: Add kvm_arch_vcpu_{un}blocking callbacks
    
    Some times it is useful for architecture implementations of KVM to know
    when the VCPU thread is about to block or when it comes back from
    blocking (arm/arm64 needs to know this to properly implement timers, for
    example).
    
    Therefore provide a generic architecture callback function in line with
    what we do elsewhere for KVM generic-arch interactions.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2beee0382088..b28f0f142ecb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1233,4 +1233,7 @@ int x86_set_memory_region(struct kvm *kvm,
 bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 
+static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu) {}
+static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu) {}
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit 58f800d5ace99c49e6418cb5757d868f2746acb4
Merge: 1330a0170a48 73917739334c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Oct 13 21:32:50 2015 +0200

    Merge branch 'kvm-master' into HEAD
    
    This merge brings in a couple important SMM fixes, which makes it
    easier to test latest KVM with unrestricted_guest=0 and to test
    the in-progress work on SMM support in the firmware.
    
    Conflicts:
            arch/x86/kvm/x86.c

commit 1d8007bdee074fdffcf3539492d8a151a1fb3436
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Oct 12 13:38:32 2015 +0200

    KVM: x86: build kvm_userspace_memory_region in x86_set_memory_region
    
    The next patch will make x86_set_memory_region fill the
    userspace_addr.  Since the struct is not used untouched
    anymore, it makes sense to build it in x86_set_memory_region
    directly; it also simplifies the callers.
    
    Reported-by: Alexandre DERUMIER <aderumier@odiso.com>
    Cc: stable@vger.kernel.org
    Fixes: 9da0e4d5ac969909f6b435ce28ea28135a9cbd69
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2beee0382088..3a36ee704c30 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1226,10 +1226,8 @@ void kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
 
 int kvm_is_in_guest(void);
 
-int __x86_set_memory_region(struct kvm *kvm,
-			    const struct kvm_userspace_memory_region *mem);
-int x86_set_memory_region(struct kvm *kvm,
-			  const struct kvm_userspace_memory_region *mem);
+int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size);
+int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size);
 bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 

commit bf9f6ac8d74969690df1485b33b7c238ca9f2269
Author: Feng Wu <feng.wu@intel.com>
Date:   Fri Sep 18 22:29:55 2015 +0800

    KVM: Update Posted-Interrupts Descriptor when vCPU is blocked
    
    This patch updates the Posted-Interrupts Descriptor when vCPU
    is blocked.
    
    pre-block:
    - Add the vCPU to the blocked per-CPU list
    - Set 'NV' to POSTED_INTR_WAKEUP_VECTOR
    
    post-block:
    - Remove the vCPU from the per-CPU list
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    [Concentrate invocation of pre/post-block hooks to vcpu_block. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 15664994b6f3..cdbdb559ecd2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -899,6 +899,17 @@ struct kvm_x86_ops {
 	/* pmu operations of sub-arch */
 	const struct kvm_pmu_ops *pmu_ops;
 
+	/*
+	 * Architecture specific hooks for vCPU blocking due to
+	 * HLT instruction.
+	 * Returns for .pre_block():
+	 *    - 0 means continue to block the vCPU.
+	 *    - 1 means we cannot block the vCPU since some event
+	 *        happens during this period, such as, 'ON' bit in
+	 *        posted-interrupts descriptor is set.
+	 */
+	int (*pre_block)(struct kvm_vcpu *vcpu);
+	void (*post_block)(struct kvm_vcpu *vcpu);
 	int (*update_pi_irte)(struct kvm *kvm, unsigned int host_irq,
 			      uint32_t guest_irq, bool set);
 };

commit 87276880065246ce49ec571130d3d1e4a22e5604
Author: Feng Wu <feng.wu@intel.com>
Date:   Fri Sep 18 22:29:40 2015 +0800

    KVM: x86: select IRQ_BYPASS_MANAGER
    
    Select IRQ_BYPASS_MANAGER for x86 when CONFIG_KVM is set
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a4067ecd4376..15664994b6f3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -24,6 +24,7 @@
 #include <linux/perf_event.h>
 #include <linux/pvclock_gtod.h>
 #include <linux/clocksource.h>
+#include <linux/irqbypass.h>
 
 #include <asm/pvclock-abi.h>
 #include <asm/desc.h>

commit efc644048ecde54f016011fe10110addd0de348f
Author: Feng Wu <feng.wu@intel.com>
Date:   Fri Sep 18 22:29:51 2015 +0800

    KVM: x86: Update IRTE for posted-interrupts
    
    This patch adds the routine to update IRTE for posted-interrupts
    when guest changes the interrupt configuration.
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    [Squashed in automatically generated patch from the build robot
     "KVM: x86: vcpu_to_pi_desc() can be static" - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 79fffbbe2348..a4067ecd4376 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -897,6 +897,9 @@ struct kvm_x86_ops {
 					   gfn_t offset, unsigned long mask);
 	/* pmu operations of sub-arch */
 	const struct kvm_pmu_ops *pmu_ops;
+
+	int (*update_pi_irte)(struct kvm *kvm, unsigned int host_irq,
+			      uint32_t guest_irq, bool set);
 };
 
 struct kvm_arch_async_pf {

commit d84f1e0755ba1e87d20d1f90c1e6eb0cbbc0af9d
Author: Feng Wu <feng.wu@intel.com>
Date:   Fri Sep 18 22:29:49 2015 +0800

    KVM: make kvm_set_msi_irq() public
    
    Make kvm_set_msi_irq() public, we can use this function outside.
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ba4e5673e604..79fffbbe2348 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -176,6 +176,8 @@ enum {
  */
 #define KVM_APIC_PV_EOI_PENDING	1
 
+struct kvm_kernel_irq_routing_entry;
+
 /*
  * We don't want allocation failures within the mmu code, so we preallocate
  * enough memory for a single page fault in a cache.
@@ -1244,4 +1246,6 @@ bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			     struct kvm_vcpu **dest_vcpu);
 
+void kvm_set_msi_irq(struct kvm_kernel_irq_routing_entry *e,
+		     struct kvm_lapic_irq *irq);
 #endif /* _ASM_X86_KVM_HOST_H */

commit 8feb4a04dc756002f78df0026e58118669de4851
Author: Feng Wu <feng.wu@intel.com>
Date:   Fri Sep 18 22:29:47 2015 +0800

    KVM: Define a new interface kvm_intr_is_single_vcpu()
    
    This patch defines a new interface kvm_intr_is_single_vcpu(),
    which can returns whether the interrupt is for single-CPU or not.
    
    It is used by VT-d PI, since now we only support single-CPU
    interrupts, For lowest-priority interrupts, if user configures
    it via /proc/irq or uses irqbalance to make it single-CPU, we
    can use PI to deliver the interrupts to it. Full functionality
    of lowest-priority support will be added later.
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d064cb2e19e8..ba4e5673e604 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1241,4 +1241,7 @@ int x86_set_memory_region(struct kvm *kvm,
 bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 
+bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
+			     struct kvm_vcpu **dest_vcpu);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit 9eec50b8bbe1535c440a1ee88c1958f78fc55957
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Wed Sep 16 12:29:50 2015 +0300

    kvm/x86: Hyper-V HV_X64_MSR_VP_RUNTIME support
    
    HV_X64_MSR_VP_RUNTIME msr used by guest to get
    "the time the virtual processor consumes running guest code,
    and the time the associated logical processor spends running
    hypervisor code on behalf of that guest."
    
    Calculation of this time is performed by task_cputime_adjusted()
    for vcpu task.
    
    Necessary to support loading of winhv.sys in guest, which in turn is
    required to support Windows VMBus.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 76a5b30979b3..d064cb2e19e8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -374,6 +374,7 @@ struct kvm_mtrr {
 /* Hyper-V per vcpu emulation context */
 struct kvm_vcpu_hv {
 	u64 hv_vapic;
+	s64 runtime_offset;
 };
 
 struct kvm_vcpu_arch {

commit 1c1a9ce973a7863dd46767226bce2a5f12d48bc6
Author: Steve Rutherford <srutherford@google.com>
Date:   Thu Jul 30 11:27:16 2015 +0200

    KVM: x86: Add support for local interrupt requests from userspace
    
    In order to enable userspace PIC support, the userspace PIC needs to
    be able to inject local interrupts even when the APICs are in the
    kernel.
    
    KVM_INTERRUPT now supports sending local interrupts to an APIC when
    APICs are in the kernel.
    
    The ready_for_interrupt_request flag is now only set when the CPU/APIC
    will immediately accept and inject an interrupt (i.e. APIC has not
    masked the PIC).
    
    When the PIC wishes to initiate an INTA cycle with, say, CPU0, it
    kicks CPU0 out of the guest, and renedezvous with CPU0 once it arrives
    in userspace.
    
    When the CPU/APIC unmasks the PIC, a KVM_EXIT_IRQ_WINDOW_OPEN is
    triggered, so that userspace has a chance to inject a PIC interrupt
    if it had been pending.
    
    Overall, this design can lead to a small number of spurious userspace
    renedezvous. In particular, whenever the PIC transistions from low to
    high while it is masked and whenever the PIC becomes unmasked while
    it is low.
    
    Note: this does not buffer more than one local interrupt in the
    kernel, so the VMM needs to enter the guest in order to complete
    interrupt injection before injecting an additional interrupt.
    
    Compiles for x86.
    
    Can pass the KVM Unit Tests.
    
    Signed-off-by: Steve Rutherford <srutherford@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7a5f9debbcd8..76a5b30979b3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -576,6 +576,7 @@ struct kvm_vcpu_arch {
 	} pv;
 
 	int pending_ioapic_eoi;
+	int pending_external_vector;
 };
 
 struct kvm_lpage_info {

commit b053b2aef25d00773fa6762dcd4b7f5c9c42d171
Author: Steve Rutherford <srutherford@google.com>
Date:   Wed Jul 29 23:32:35 2015 -0700

    KVM: x86: Add EOI exit bitmap inference
    
    In order to support a userspace IOAPIC interacting with an in kernel
    APIC, the EOI exit bitmaps need to be configurable.
    
    If the IOAPIC is in userspace (i.e. the irqchip has been split), the
    EOI exit bitmaps will be set whenever the GSI Routes are configured.
    In particular, for the low MSI routes are reservable for userspace
    IOAPICs. For these MSI routes, the EOI Exit bit corresponding to the
    destination vector of the route will be set for the destination VCPU.
    
    The intention is for the userspace IOAPICs to use the reservable MSI
    routes to inject interrupts into the guest.
    
    This is a slight abuse of the notion of an MSI Route, given that MSIs
    classically bypass the IOAPIC. It might be worthwhile to add an
    additional route type to improve clarity.
    
    Compile tested for Intel x86.
    
    Signed-off-by: Steve Rutherford <srutherford@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index af09fa1d1be7..7a5f9debbcd8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -688,6 +688,7 @@ struct kvm_arch {
 	u64 disabled_quirks;
 
 	bool irqchip_split;
+	u8 nr_reserved_ioapic_pins;
 };
 
 struct kvm_vm_stat {

commit 7543a635aa09eb138b2cbf60ac3ff19503ae6954
Author: Steve Rutherford <srutherford@google.com>
Date:   Wed Jul 29 23:21:41 2015 -0700

    KVM: x86: Add KVM exit for IOAPIC EOIs
    
    Adds KVM_EXIT_IOAPIC_EOI which allows the kernel to EOI
    level-triggered IOAPIC interrupts.
    
    Uses a per VCPU exit bitmap to decide whether or not the IOAPIC needs
    to be informed (which is identical to the EOI_EXIT_BITMAP field used
    by modern x86 processors, but can also be used to elide kvm IOAPIC EOI
    exits on older processors).
    
    [Note: A prototype using ResampleFDs found that decoupling the EOI
    from the VCPU's thread made it possible for the VCPU to not see a
    recent EOI after reentering the guest. This does not match real
    hardware.]
    
    Compile tested for Intel x86.
    
    Signed-off-by: Steve Rutherford <srutherford@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index befcf555bddc..af09fa1d1be7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -574,6 +574,8 @@ struct kvm_vcpu_arch {
 	struct {
 		bool pv_unhalted;
 	} pv;
+
+	int pending_ioapic_eoi;
 };
 
 struct kvm_lpage_info {

commit 49df6397edfc5a8ba8ca813b51fb9729d8e94b40
Author: Steve Rutherford <srutherford@google.com>
Date:   Wed Jul 29 23:21:40 2015 -0700

    KVM: x86: Split the APIC from the rest of IRQCHIP.
    
    First patch in a series which enables the relocation of the
    PIC/IOAPIC to userspace.
    
    Adds capability KVM_CAP_SPLIT_IRQCHIP;
    
    KVM_CAP_SPLIT_IRQCHIP enables the construction of LAPICs without the
    rest of the irqchip.
    
    Compile tested for x86.
    
    Signed-off-by: Steve Rutherford <srutherford@google.com>
    Suggested-by: Andrew Honig <ahonig@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a0ef289d5a86..befcf555bddc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -684,6 +684,8 @@ struct kvm_arch {
 	u32 bsp_vcpu_id;
 
 	u64 disabled_quirks;
+
+	bool irqchip_split;
 };
 
 struct kvm_vm_stat {

commit d50ab6c1a2b24e12d3012d7beb343eba5b94a6ca
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jul 29 11:49:59 2015 +0200

    KVM: x86: replace vm_has_apicv hook with cpu_uses_apicv
    
    This will avoid an unnecessary trip to ->kvm and from there to the VPIC.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 33609c2c743b..a0ef289d5a86 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -820,7 +820,7 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
-	int (*vm_has_apicv)(struct kvm *kvm);
+	int (*cpu_uses_apicv)(struct kvm_vcpu *vcpu);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu);

commit 3bb345f387dd26beb097cf776e342bc0d96d805a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jul 29 10:43:18 2015 +0200

    KVM: x86: store IOAPIC-handled vectors in each VCPU
    
    We can reuse the algorithm that computes the EOI exit bitmap to figure
    out which vectors are handled by the IOAPIC.  The only difference
    between the two is for edge-triggered interrupts other than IRQ8
    that have no notifiers active; however, the IOAPIC does not have to
    do anything special for these interrupts anyway.
    
    This again limits the interactions between the IOAPIC and the LAPIC,
    making it easier to move the former to userspace.
    
    Inspired by a patch from Steve Rutherford.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2beee0382088..33609c2c743b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -396,6 +396,7 @@ struct kvm_vcpu_arch {
 	u64 efer;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
+	u64 eoi_exit_bitmap[4];
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;
 	int mp_state;
@@ -822,7 +823,7 @@ struct kvm_x86_ops {
 	int (*vm_has_apicv)(struct kvm *kvm);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
 	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
-	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
+	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu);
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);

commit 920552b213e3dc832a874b4e7ba29ecddbab31bc
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Sep 18 12:34:53 2015 +0200

    KVM: disable halt_poll_ns as default for s390x
    
    We observed some performance degradation on s390x with dynamic
    halt polling. Until we can provide a proper fix, let's enable
    halt_poll_ns as default only for supported architectures.
    
    Architectures are now free to set their own halt_poll_ns
    default value.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 349f80a82b82..2beee0382088 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -40,6 +40,7 @@
 
 #define KVM_PIO_PAGE_OFFSET 1
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 2
+#define KVM_HALT_POLL_NS_DEFAULT 500000
 
 #define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS
 

commit 62bea5bff486644ecf363fe8a1a2f6f32c614a49
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 15 18:27:57 2015 +0200

    KVM: add halt_attempted_poll to VCPU stats
    
    This new statistic can help diagnosing VCPUs that, for any reason,
    trigger bad behavior of halt_poll_ns autotuning.
    
    For example, say halt_poll_ns = 480000, and wakeups are spaced exactly
    like 479us, 481us, 479us, 481us. Then KVM always fails polling and wastes
    10+20+40+80+160+320+480 = 1110 microseconds out of every
    479+481+479+481+479+481+479 = 3359 microseconds. The VCPU then
    is consuming about 30% more CPU than it would use without
    polling.  This would show as an abnormally high number of
    attempted polling compared to the successful polls.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com<
    Reviewed-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c12e845f59e6..349f80a82b82 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -711,6 +711,7 @@ struct kvm_vcpu_stat {
 	u32 nmi_window_exits;
 	u32 halt_exits;
 	u32 halt_successful_poll;
+	u32 halt_attempted_poll;
 	u32 halt_wakeup;
 	u32 request_irq_exits;
 	u32 irq_exits;

commit c258b62b264fdc469b6d3610a907708068145e3b
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Wed Aug 5 12:04:24 2015 +0800

    KVM: MMU: introduce the framework to check zero bits on sptes
    
    We have abstracted the data struct and functions which are used to check
    reserved bit on guest page tables, now we extend the logic to check
    zero bits on shadow page tables
    
    The zero bits on sptes include not only reserved bits on hardware but also
    the bits that SPTEs willnever use.  For example, shadow pages will never
    use GB pages unless the guest uses them too.
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 847b37cbf211..c12e845f59e6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -294,6 +294,14 @@ struct kvm_mmu {
 
 	u64 *pae_root;
 	u64 *lm_root;
+
+	/*
+	 * check zero bits on shadow page table entries, these
+	 * bits include not only hardware reserved bits but also
+	 * the bits spte never used.
+	 */
+	struct rsvd_bits_validate shadow_zero_check;
+
 	struct rsvd_bits_validate guest_rsvd_check;
 
 	/*

commit a0a64f50aac731d42125dd8581b9a31e4fdb0f75
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Wed Aug 5 12:04:21 2015 +0800

    KVM: MMU: introduce rsvd_bits_validate
    
    These two fields, rsvd_bits_mask and bad_mt_xwr, in "struct kvm_mmu" are
    used to check if reserved bits set on guest ptes, move them to a data
    struct so that the approach can be applied to check host shadow page
    table entries as well
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2f9e504f9f0c..847b37cbf211 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -252,6 +252,11 @@ struct kvm_pio_request {
 	int size;
 };
 
+struct rsvd_bits_validate {
+	u64 rsvd_bits_mask[2][4];
+	u64 bad_mt_xwr;
+};
+
 /*
  * x86 supports 3 paging modes (4-level 64-bit, 3-level 64-bit, and 2-level
  * 32-bit).  The kvm_mmu structure abstracts the details of the current mmu
@@ -289,8 +294,7 @@ struct kvm_mmu {
 
 	u64 *pae_root;
 	u64 *lm_root;
-	u64 rsvd_bits_mask[2][4];
-	u64 bad_mt_xwr;
+	struct rsvd_bits_validate guest_rsvd_check;
 
 	/*
 	 * Bitmap: bit set = last pte in walk

commit d71ba788345c2b5646101766e0c52714a9b5ed7f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jul 29 11:56:48 2015 +0200

    KVM: move code related to KVM_SET_BOOT_CPU_ID to x86
    
    This is another remnant of ia64 support.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fa32b5314dcd..2f9e504f9f0c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -667,6 +667,7 @@ struct kvm_arch {
 	#endif
 
 	bool boot_vcpu_runs_old_kvmclock;
+	u32 bsp_vcpu_id;
 
 	u64 disabled_quirks;
 };
@@ -1215,5 +1216,7 @@ int __x86_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem);
 int x86_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem);
+bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);
+bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit e7d9513b60e87f62e41090fa3a26eca796924346
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Fri Jul 3 15:01:37 2015 +0300

    kvm/x86: added hyper-v crash msrs into kvm hyperv context
    
    Added kvm Hyper-V context hv crash variables as storage
    of Hyper-V crash msrs.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Reviewed-by: Peter Hornyack <peterhornyack@google.com>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 24168822212b..fa32b5314dcd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -595,6 +595,10 @@ struct kvm_hv {
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
 	u64 hv_tsc_page;
+
+	/* Hyper-v based guest crash (NT kernel bugcheck) parameters */
+	u64 hv_crash_param[HV_X64_MSR_CRASH_PARAMS];
+	u64 hv_crash_ctl;
 };
 
 struct kvm_arch {

commit e83d58874ba1de74c13d3c6b05f95a023c860d25
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Fri Jul 3 15:01:34 2015 +0300

    kvm/x86: move Hyper-V MSR's/hypercall code into hyperv.c file
    
    This patch introduce Hyper-V related source code file - hyperv.c and
    per vm and per vcpu hyperv context structures.
    All Hyper-V MSR's and hypercall code moved into hyperv.c.
    All Hyper-V kvm/vcpu fields moved into appropriate hyperv context
    structures. Copyrights and authors information copied from x86.c
    to hyperv.c.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Reviewed-by: Peter Hornyack <peterhornyack@google.com>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 49ec9038ec14..24168822212b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -358,6 +358,11 @@ struct kvm_mtrr {
 	struct list_head head;
 };
 
+/* Hyper-V per vcpu emulation context */
+struct kvm_vcpu_hv {
+	u64 hv_vapic;
+};
+
 struct kvm_vcpu_arch {
 	/*
 	 * rip and regs accesses must go through
@@ -514,8 +519,7 @@ struct kvm_vcpu_arch {
 	/* used for guest single stepping over the given code position */
 	unsigned long singlestep_rip;
 
-	/* fields used by HYPER-V emulation */
-	u64 hv_vapic;
+	struct kvm_vcpu_hv hyperv;
 
 	cpumask_var_t wbinvd_dirty_mask;
 
@@ -586,6 +590,13 @@ struct kvm_apic_map {
 	struct kvm_lapic *logical_map[16][16];
 };
 
+/* Hyper-V emulation context */
+struct kvm_hv {
+	u64 hv_guest_os_id;
+	u64 hv_hypercall;
+	u64 hv_tsc_page;
+};
+
 struct kvm_arch {
 	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;
@@ -645,10 +656,7 @@ struct kvm_arch {
 	/* reads protected by irq_srcu, writes by irq_lock */
 	struct hlist_head mask_notifier_list;
 
-	/* fields used by HYPER-V emulation */
-	u64 hv_guest_os_id;
-	u64 hv_hypercall;
-	u64 hv_tsc_page;
+	struct kvm_hv hyperv;
 
 	#ifdef CONFIG_KVM_MMU_AUDIT
 	int audit_point;

commit 5544eb9b81940647b8fad1f251b37cbe2819ce44
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jul 7 15:41:58 2015 +0200

    KVM: count number of assigned devices
    
    If there are no assigned devices, the guest PAT are not providing
    any useful information and can be overridden to writeback; VMX
    always does this because it has the "IPAT" bit in its extended
    page table entries, but SVM does not have anything similar.
    Hook into VFIO and legacy device assignment so that they
    provide this information to KVM.
    
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Tested-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2a7f5d782c33..49ec9038ec14 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -604,6 +604,8 @@ struct kvm_arch {
 	bool iommu_noncoherent;
 #define __KVM_HAVE_ARCH_NONCOHERENT_DMA
 	atomic_t noncoherent_dma_count;
+#define __KVM_HAVE_ARCH_ASSIGNED_DEVICE
+	atomic_t assigned_device_count;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;

commit 42720138b06301cc8a7ee8a495a6d021c4b6a9bc
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Jul 1 15:31:49 2015 +0200

    KVM: x86: make vapics_in_nmi_mode atomic
    
    Writes were a bit racy, but hard to turn into a bug at the same time.
    (Particularly because modern Linux doesn't use this feature anymore.)
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    [Actually the next patch makes it much, much easier to trigger the race
     so I'm including this one for stable@ as well. - Paolo]
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c7fa57b529d2..2a7f5d782c33 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -607,7 +607,7 @@ struct kvm_arch {
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;
-	int vapics_in_nmi_mode;
+	atomic_t vapics_in_nmi_mode;
 	struct mutex apic_map_lock;
 	struct kvm_apic_map *apic_map;
 

commit 4e241557fc1cb560bd9e77ca1b4a9352732a5427
Merge: 08d183e3c1f6 f2ae45edbca7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 09:36:49 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull first batch of KVM updates from Paolo Bonzini:
     "The bulk of the changes here is for x86.  And for once it's not for
      silicon that no one owns: these are really new features for everyone.
    
      Details:
    
       - ARM:
            several features are in progress but missed the 4.2 deadline.
            So here is just a smattering of bug fixes, plus enabling the
            VFIO integration.
    
       - s390:
            Some fixes/refactorings/optimizations, plus support for 2GB
            pages.
    
       - x86:
            * host and guest support for marking kvmclock as a stable
              scheduler clock.
            * support for write combining.
            * support for system management mode, needed for secure boot in
              guests.
            * a bunch of cleanups required for the above
            * support for virtualized performance counters on AMD
            * legacy PCI device assignment is deprecated and defaults to "n"
              in Kconfig; VFIO replaces it
    
            On top of this there are also bug fixes and eager FPU context
            loading for FPU-heavy guests.
    
       - Common code:
            Support for multiple address spaces; for now it is used only for
            x86 SMM but the s390 folks also have plans"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (124 commits)
      KVM: s390: clear floating interrupt bitmap and parameters
      KVM: x86/vPMU: Enable PMU handling for AMD PERFCTRn and EVNTSELn MSRs
      KVM: x86/vPMU: Implement AMD vPMU code for KVM
      KVM: x86/vPMU: Define kvm_pmu_ops to support vPMU function dispatch
      KVM: x86/vPMU: introduce kvm_pmu_msr_idx_to_pmc
      KVM: x86/vPMU: reorder PMU functions
      KVM: x86/vPMU: whitespace and stylistic adjustments in PMU code
      KVM: x86/vPMU: use the new macros to go between PMC, PMU and VCPU
      KVM: x86/vPMU: introduce pmu.h header
      KVM: x86/vPMU: rename a few PMU functions
      KVM: MTRR: do not map huge page for non-consistent range
      KVM: MTRR: simplify kvm_mtrr_get_guest_memory_type
      KVM: MTRR: introduce mtrr_for_each_mem_type
      KVM: MTRR: introduce fixed_mtrr_addr_* functions
      KVM: MTRR: sort variable MTRRs
      KVM: MTRR: introduce var_mtrr_range
      KVM: MTRR: introduce fixed_mtrr_segment table
      KVM: MTRR: improve kvm_mtrr_get_guest_memory_type
      KVM: MTRR: do not split 64 bits MSR content
      KVM: MTRR: clean up mtrr default type
      ...

commit 25462f7f5295e2d3e9c2b31761ac95f0b3c8562f
Author: Wei Huang <wehuang@redhat.com>
Date:   Fri Jun 19 15:45:05 2015 +0200

    KVM: x86/vPMU: Define kvm_pmu_ops to support vPMU function dispatch
    
    This patch defines a new function pointer struct (kvm_pmu_ops) to
    support vPMU for both Intel and AMD. The functions pointers defined in
    this new struct will be linked with Intel and AMD functions later. In the
    meanwhile the struct that maps from event_sel bits to PERF_TYPE_HARDWARE
    events is renamed and moved from Intel specific code to kvm_host.h as a
    common struct.
    
    Reviewed-by: Joerg Roedel <jroedel@suse.de>
    Tested-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Wei Huang <wei@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 534dfa324e35..5a2b4508be44 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -336,6 +336,8 @@ struct kvm_pmu {
 	u64 reprogram_pmi;
 };
 
+struct kvm_pmu_ops;
+
 enum {
 	KVM_DEBUGREG_BP_ENABLED = 1,
 	KVM_DEBUGREG_WONT_EXIT = 2,
@@ -854,6 +856,8 @@ struct kvm_x86_ops {
 	void (*enable_log_dirty_pt_masked)(struct kvm *kvm,
 					   struct kvm_memory_slot *slot,
 					   gfn_t offset, unsigned long mask);
+	/* pmu operations of sub-arch */
+	const struct kvm_pmu_ops *pmu_ops;
 };
 
 struct kvm_arch_async_pf {

commit 474a5bb944d2ad308a1360dcae72b16b8eecd250
Author: Wei Huang <wehuang@redhat.com>
Date:   Fri Jun 19 13:54:23 2015 +0200

    KVM: x86/vPMU: introduce pmu.h header
    
    This will be used for private function used by AMD- and Intel-specific
    PMU implementations.
    
    Signed-off-by: Wei Huang <wei@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d92d7edc016b..534dfa324e35 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1195,18 +1195,6 @@ void kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
 
 int kvm_is_in_guest(void);
 
-void kvm_pmu_init(struct kvm_vcpu *vcpu);
-void kvm_pmu_destroy(struct kvm_vcpu *vcpu);
-void kvm_pmu_reset(struct kvm_vcpu *vcpu);
-void kvm_pmu_refresh(struct kvm_vcpu *vcpu);
-bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr);
-int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data);
-int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info);
-int kvm_pmu_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned pmc);
-int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
-void kvm_pmu_handle_event(struct kvm_vcpu *vcpu);
-void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu);
-
 int __x86_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem);
 int x86_set_memory_region(struct kvm *kvm,

commit c6702c9dcfe72b63a85e7ae35533c11e2b7c1040
Author: Wei Huang <wehuang@redhat.com>
Date:   Fri Jun 19 13:44:45 2015 +0200

    KVM: x86/vPMU: rename a few PMU functions
    
    Before introducing a pmu.h header for them, make the naming more
    consistent.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f2d60cce7595..d92d7edc016b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1198,14 +1198,14 @@ int kvm_is_in_guest(void);
 void kvm_pmu_init(struct kvm_vcpu *vcpu);
 void kvm_pmu_destroy(struct kvm_vcpu *vcpu);
 void kvm_pmu_reset(struct kvm_vcpu *vcpu);
-void kvm_pmu_cpuid_update(struct kvm_vcpu *vcpu);
-bool kvm_pmu_msr(struct kvm_vcpu *vcpu, u32 msr);
+void kvm_pmu_refresh(struct kvm_vcpu *vcpu);
+bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr);
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data);
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info);
-int kvm_pmu_check_pmc(struct kvm_vcpu *vcpu, unsigned pmc);
-int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
-void kvm_handle_pmu_event(struct kvm_vcpu *vcpu);
-void kvm_deliver_pmi(struct kvm_vcpu *vcpu);
+int kvm_pmu_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned pmc);
+int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
+void kvm_pmu_handle_event(struct kvm_vcpu *vcpu);
+void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu);
 
 int __x86_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem);

commit 19efffa244071ccd0385b240d03adb38feaab04e
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:31 2015 +0800

    KVM: MTRR: sort variable MTRRs
    
    Sort all valid variable MTRRs based on its base address, it will help us to
    check a range to see if it's fully contained in variable MTRRs
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    [Fix list insertion sort, simplify var_mtrr_range_is_valid to just
     test the V bit. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f73554874845..f2d60cce7595 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -345,12 +345,15 @@ enum {
 struct kvm_mtrr_range {
 	u64 base;
 	u64 mask;
+	struct list_head node;
 };
 
 struct kvm_mtrr {
 	struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
 	mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
 	u64 deftype;
+
+	struct list_head head;
 };
 
 struct kvm_vcpu_arch {

commit 86fd52701cfae760711fb02a03c1ab8c80ea72f3
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:27 2015 +0800

    KVM: MTRR: do not split 64 bits MSR content
    
    Variable MTRR MSRs are 64 bits which are directly accessed with full length,
    no reason to split them to two 32 bits
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8d43006a6df0..f73554874845 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -342,8 +342,13 @@ enum {
 	KVM_DEBUGREG_RELOAD = 4,
 };
 
+struct kvm_mtrr_range {
+	u64 base;
+	u64 mask;
+};
+
 struct kvm_mtrr {
-	struct mtrr_var_range var_ranges[KVM_NR_VAR_MTRR];
+	struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
 	mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
 	u64 deftype;
 };

commit 10fac2dc2b3b549d371d67f57193362b6bcc6dfd
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:26 2015 +0800

    KVM: MTRR: clean up mtrr default type
    
    Drop kvm_mtrr->enable, omit the decode/code workload and get rid of
    all the hard code
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fe9cbe49e272..8d43006a6df0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -345,8 +345,7 @@ enum {
 struct kvm_mtrr {
 	struct mtrr_var_range var_ranges[KVM_NR_VAR_MTRR];
 	mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
-	unsigned char enabled;
-	mtrr_type def_type;
+	u64 deftype;
 };
 
 struct kvm_vcpu_arch {

commit 910a6aae4e2e45855efc4a268e43eed2d8445575
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:25 2015 +0800

    KVM: MTRR: exactly define the size of variable MTRRs
    
    Only KVM_NR_VAR_MTRR variable MTRRs are available in KVM guest
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cbf9f076f57c..fe9cbe49e272 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -343,7 +343,7 @@ enum {
 };
 
 struct kvm_mtrr {
-	struct mtrr_var_range var_ranges[MTRR_MAX_VAR_RANGES];
+	struct mtrr_var_range var_ranges[KVM_NR_VAR_MTRR];
 	mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
 	unsigned char enabled;
 	mtrr_type def_type;

commit 70109e7d9d4ac7182786ddf7cd53bc651a157896
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:24 2015 +0800

    KVM: MTRR: remove mtrr_state.have_fixed
    
    vMTRR does not depend on any host MTRR feature and fixed MTRRs have always
    been implemented, so drop this field
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cf8d320dc7a5..cbf9f076f57c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -342,6 +342,13 @@ enum {
 	KVM_DEBUGREG_RELOAD = 4,
 };
 
+struct kvm_mtrr {
+	struct mtrr_var_range var_ranges[MTRR_MAX_VAR_RANGES];
+	mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	unsigned char enabled;
+	mtrr_type def_type;
+};
+
 struct kvm_vcpu_arch {
 	/*
 	 * rip and regs accesses must go through
@@ -472,7 +479,7 @@ struct kvm_vcpu_arch {
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
 	bool smi_pending;    /* SMI queued after currently running handler */
 
-	struct mtrr_state_type mtrr_state;
+	struct kvm_mtrr mtrr_state;
 	u64 pat;
 
 	unsigned switch_db_regs;

commit ff53604b40b439cbb235f89bda99839ca81d3b9d
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:22 2015 +0800

    KVM: x86: move MTRR related code to a separate file
    
    MTRR code locates in x86.c and mmu.c so that move them to a separate file to
    make the organization more clearer and it will be the place where we fully
    implement vMTRR
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8ca32cfbcbd8..cf8d320dc7a5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -894,7 +894,6 @@ int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3);
 
 int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
 			  const void *val, int bytes);
-u8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 struct kvm_irq_mask_notifier {
 	void (*func)(struct kvm_irq_mask_notifier *kimn, bool masked);

commit 6d396b55203969ca61cc8f838db2e68433e13f7b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Apr 1 14:25:33 2015 +0200

    KVM: x86: advertise KVM_CAP_X86_SMM
    
    ... and we're done. :)
    
    Because SMBASE is usually relocated above 1M on modern chipsets, and
    SMM handlers might indeed rely on 4G segment limits, we only expose it
    if KVM is able to run the guest in big real mode.  This includes any
    of VMX+emulate_invalid_guest_state, VMX+unrestricted_guest, or SVM.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 47006683f2fe..8ca32cfbcbd8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -709,6 +709,7 @@ struct kvm_x86_ops {
 	int (*hardware_setup)(void);               /* __init */
 	void (*hardware_unsetup)(void);            /* __exit */
 	bool (*cpu_has_accelerated_tpr)(void);
+	bool (*cpu_has_high_real_mode_segbase)(void);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
 	/* Create, but do not attach this VCPU */

commit 699023e239658e62da6f42f47d31b54788521ec1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 15:03:39 2015 +0200

    KVM: x86: add SMM to the MMU role, support SMRAM address space
    
    This is now very simple to do.  The only interesting part is a simple
    trick to find the right memslot in gfn_to_rmap, retrieving the address
    space from the spte role word.  The same trick is used in the auditing
    code.
    
    The comment on top of union kvm_mmu_page_role has been stale forever,
    so remove it.  Speaking of stale code, remove pad_for_nice_hex_output
    too: it was splitting the "access" bitfield across two bytes and thus
    had effectively turned into pad_for_ugly_hex_output.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5a5e13af6e03..47006683f2fe 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -184,23 +184,12 @@ struct kvm_mmu_memory_cache {
 	void *objects[KVM_NR_MEM_OBJS];
 };
 
-/*
- * kvm_mmu_page_role, below, is defined as:
- *
- *   bits 0:3 - total guest paging levels (2-4, or zero for real mode)
- *   bits 4:7 - page table level for this shadow (1-4)
- *   bits 8:9 - page table quadrant for 2-level guests
- *   bit   16 - direct mapping of virtual to physical mapping at gfn
- *              used for real mode and two-dimensional paging
- *   bits 17:19 - common access permissions for all ptes in this shadow page
- */
 union kvm_mmu_page_role {
 	unsigned word;
 	struct {
 		unsigned level:4;
 		unsigned cr4_pae:1;
 		unsigned quadrant:2;
-		unsigned pad_for_nice_hex_output:6;
 		unsigned direct:1;
 		unsigned access:3;
 		unsigned invalid:1;
@@ -208,6 +197,15 @@ union kvm_mmu_page_role {
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
 		unsigned smap_andnot_wp:1;
+		unsigned :8;
+
+		/*
+		 * This is left at the top of the word so that
+		 * kvm_memslots_for_spte_role can extract it with a
+		 * simple shift.  While there is room, give it a whole
+		 * byte so it is also faster to load it from memory.
+		 */
+		unsigned smm:8;
 	};
 };
 
@@ -1120,6 +1118,12 @@ enum {
 #define HF_SMM_MASK		(1 << 6)
 #define HF_SMM_INSIDE_NMI_MASK	(1 << 7)
 
+#define __KVM_VCPU_MULTIPLE_ADDRESS_SPACE
+#define KVM_ADDRESS_SPACE_NUM 2
+
+#define kvm_arch_vcpu_memslots_id(vcpu) ((vcpu)->arch.hflags & HF_SMM_MASK ? 1 : 0)
+#define kvm_memslots_for_spte_role(kvm, role) __kvm_memslots(kvm, (role).smm)
+
 /*
  * Hardware virtualization extension instructions may fault if a
  * reboot turns off virtualization while processes are running.

commit 9da0e4d5ac969909f6b435ce28ea28135a9cbd69
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:33:16 2015 +0200

    KVM: x86: work on all available address spaces
    
    This patch has no semantic change, but it prepares for the introduction
    of a second address space for system management mode.
    
    A new function x86_set_memory_region (and the "slots_lock taken"
    counterpart __x86_set_memory_region) is introduced in order to
    operate on all address spaces when adding or deleting private
    memory slots.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2fd420255c2f..5a5e13af6e03 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1189,4 +1189,9 @@ int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
 void kvm_handle_pmu_event(struct kvm_vcpu *vcpu);
 void kvm_deliver_pmi(struct kvm_vcpu *vcpu);
 
+int __x86_set_memory_region(struct kvm *kvm,
+			    const struct kvm_userspace_memory_region *mem);
+int x86_set_memory_region(struct kvm *kvm,
+			  const struct kvm_userspace_memory_region *mem);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit 54bf36aac520315385fe7623a5c3a698e993ceda
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Apr 8 15:39:23 2015 +0200

    KVM: x86: use vcpu-specific functions to read/write/translate GFNs
    
    We need to hide SMRAM from guests not running in SMM.  Therefore,
    all uses of kvm_read_guest* and kvm_write_guest* must be changed to
    check whether the VCPU is in system management mode and use a
    different set of memslots.  Switch from kvm_* to the newly-introduced
    kvm_vcpu_*, which call into kvm_arch_vcpu_memslots_id.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 12a7318887ad..2fd420255c2f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -887,7 +887,7 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				   struct kvm_memory_slot *slot,
 				   gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
-void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm);
+void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, struct kvm_memslots *slots);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 

commit 64d6067057d9658acb8675afcfba549abdb7fc16
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu May 7 11:36:11 2015 +0200

    KVM: x86: stubs for SMM support
    
    This patch adds the interface between x86.c and the emulator: the
    SMBASE register, a new emulator flag, the RSM instruction.  It also
    adds a new request bit that will be used by the KVM_SMI ioctl.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d52d7aea375f..12a7318887ad 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -368,6 +368,7 @@ struct kvm_vcpu_arch {
 	int32_t apic_arb_prio;
 	int mp_state;
 	u64 ia32_misc_enable_msr;
+	u64 smbase;
 	bool tpr_access_reporting;
 	u64 ia32_xss;
 

commit f077825a8758d79838a757dafb79adcdd047ef3a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Apr 1 15:06:40 2015 +0200

    KVM: x86: API changes for SMM support
    
    This patch includes changes to the external API for SMM support.
    Userspace can predicate the availability of the new fields and
    ioctls on a new capability, KVM_CAP_X86_SMM, which is added at the end
    of the patch series.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4e299fcd0eb6..d52d7aea375f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -471,6 +471,7 @@ struct kvm_vcpu_arch {
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
 	unsigned nmi_pending; /* NMI queued after currently running handler */
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
+	bool smi_pending;    /* SMI queued after currently running handler */
 
 	struct mtrr_state_type mtrr_state;
 	u64 pat;
@@ -1115,6 +1116,8 @@ enum {
 #define HF_NMI_MASK		(1 << 3)
 #define HF_IRET_MASK		(1 << 4)
 #define HF_GUEST_MASK		(1 << 5) /* VCPU is in guest-mode */
+#define HF_SMM_MASK		(1 << 6)
+#define HF_SMM_INSIDE_NMI_MASK	(1 << 7)
 
 /*
  * Hardware virtualization extension instructions may fault if a

commit 609e36d372ad9329269e4a1467bd35311893d1d6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Apr 8 15:30:38 2015 +0200

    KVM: x86: pass host_initiated to functions that read MSRs
    
    SMBASE is only readable from SMM for the VCPU, but it must be always
    accessible if userspace is accessing it.  Thus, all functions that
    read MSRs are changed to accept a struct msr_data; the host_initiated
    and index fields are pre-initialized, while the data field is filled
    on return.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7276107b35df..4e299fcd0eb6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -721,7 +721,7 @@ struct kvm_x86_ops {
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
 
 	void (*update_db_bp_intercept)(struct kvm_vcpu *vcpu);
-	int (*get_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata);
+	int (*get_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);
 	int (*set_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);
 	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);
 	void (*get_segment)(struct kvm_vcpu *vcpu,
@@ -941,7 +941,7 @@ static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 
 void kvm_enable_efer_bits(u64);
 bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);
-int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);
+int kvm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
 int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
 
 struct x86_emulate_ctxt;
@@ -970,7 +970,7 @@ void kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw);
 void kvm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l);
 int kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr);
 
-int kvm_get_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
+int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr);
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr);
 
 unsigned long kvm_get_rflags(struct kvm_vcpu *vcpu);

commit f36f3f2846b5578d62910ee0b6dbef59fdd1cfa4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:20:23 2015 +0200

    KVM: add "new" argument to kvm_arch_commit_memory_region
    
    This lets the function access the new memory slot without going through
    kvm_memslots and id_to_memslot.  It will simplify the code when more
    than one address space will be supported.
    
    Unfortunately, the "const"ness of the new argument must be casted
    away in two places.  Fixing KVM to accept const struct kvm_memory_slot
    pointers would require modifications in pretty much all architectures,
    and is left for later.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1a4d6a054749..7276107b35df 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -874,7 +874,7 @@ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 				      struct kvm_memory_slot *memslot);
 void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
-					struct kvm_memory_slot *memslot);
+				   const struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,

commit 3152657f104ce9b0f80793c92a745d5e97b43812
Merge: b8c1b8ea7b21 ba155e2d21f6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat May 23 16:47:12 2015 +0200

    Merge branch 'linus' into x86/fpu
    
    Resolve semantic conflict in arch/x86/kvm/cpuid.c with:
    
      c447e76b4cab ("kvm/fpu: Enable eager restore kvm FPU for MPX")
    
    By removing the FPU internal include files.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a9b4fb7e79e7624c97c55e9c7562e3fe866ce70f
Merge: ed3cf15271fa c447e76b4cab
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 20 11:46:12 2015 +0200

    Merge branch 'kvm-master' into kvm-next
    
    Grab MPX bugfix, and fix conflicts against Rik's adaptive FPU
    deactivation patch.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit c447e76b4cabb49ddae8e49c5758f031f35d55fb
Author: Liang Li <liang.z.li@intel.com>
Date:   Thu May 21 04:41:25 2015 +0800

    kvm/fpu: Enable eager restore kvm FPU for MPX
    
    The MPX feature requires eager KVM FPU restore support. We have verified
    that MPX cannot work correctly with the current lazy KVM FPU restore
    mechanism. Eager KVM FPU restore should be enabled if the MPX feature is
    exposed to VM.
    
    Signed-off-by: Yang Zhang <yang.z.zhang@intel.com>
    Signed-off-by: Liang Li <liang.z.li@intel.com>
    [Also activate the FPU on AMD processors. - Paolo]
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5a1faf3f043e..f4a555beef19 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -401,6 +401,7 @@ struct kvm_vcpu_arch {
 	struct kvm_mmu_memory_cache mmu_page_header_cache;
 
 	struct fpu guest_fpu;
+	bool eager_fpu;
 	u64 xcr0;
 	u64 guest_supported_xcr0;
 	u32 guest_xstate_size;

commit 0fdd74f7784b5cdff7075736992bbb149b1ae49c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 20 11:33:43 2015 +0200

    Revert "KVM: x86: drop fpu_activate hook"
    
    This reverts commit 4473b570a7ebb502f63f292ccfba7df622e5fdd3.  We'll
    use the hook again.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e61c3a4ee131..5a1faf3f043e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -744,6 +744,7 @@ struct kvm_x86_ops {
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
+	void (*fpu_activate)(struct kvm_vcpu *vcpu);
 	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);

commit edc90b7dc4ceef62ef0ad9cc6c3f5dc770e83ad2
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon May 11 22:55:21 2015 +0800

    KVM: MMU: fix SMAP virtualization
    
    KVM may turn a user page to a kernel page when kernel writes a readonly
    user page if CR0.WP = 1. This shadow page entry will be reused after
    SMAP is enabled so that kernel is allowed to access this user page
    
    Fix it by setting SMAP && !CR0.WP into shadow page's role and reset mmu
    once CR4.SMAP is updated
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8b661d1946b5..bbb8f4e7738a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -207,6 +207,7 @@ union kvm_mmu_page_role {
 		unsigned nxe:1;
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
+		unsigned smap_andnot_wp:1;
 	};
 };
 

commit 0ee6a5172573aea06ef41f4e48737dcfab0099bb
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 06:58:22 2015 +0200

    x86/fpu, kvm: Simplify fx_init()
    
    Now that fpstate_init() cannot fail the error return of fx_init()
    has lost its purpose. Eliminate the error return and propagate this
    change to all callers.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dea2e7e962e3..c29e61a8d6d4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -999,8 +999,6 @@ void kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);
 
 void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
-int fx_init(struct kvm_vcpu *vcpu);
-
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 		       const u8 *new, int bytes);
 int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn);

commit 0be0226f07d14b153a5eedf2bb86e1eb7dcefab5
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon May 11 22:55:21 2015 +0800

    KVM: MMU: fix SMAP virtualization
    
    KVM may turn a user page to a kernel page when kernel writes a readonly
    user page if CR0.WP = 1. This shadow page entry will be reused after
    SMAP is enabled so that kernel is allowed to access this user page
    
    Fix it by setting SMAP && !CR0.WP into shadow page's role and reset mmu
    once CR4.SMAP is updated
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dea2e7e962e3..e61c3a4ee131 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -207,6 +207,7 @@ union kvm_mmu_page_role {
 		unsigned nxe:1;
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
+		unsigned smap_andnot_wp:1;
 	};
 };
 

commit 93bbf0b8bc80f0ee3c629542a4dea14a3537760b
Author: James Sullivan <sullivan.james.f@gmail.com>
Date:   Wed Mar 18 19:26:03 2015 -0600

    kvm: x86: Extended struct kvm_lapic_irq with msi_redir_hint for MSI delivery
    
    Extended struct kvm_lapic_irq with bool msi_redir_hint, which will
    be used to determine if the delivery of the MSI should target only
    the lowest priority CPU in the logical group specified for delivery.
    (In physical dest mode, the RH bit is not relevant). Initialized the value
    of msi_redir_hint to true when RH=1 in kvm_set_msi_irq(), and initialized
    to false in all other cases.
    
    Added value of msi_redir_hint to a debug message dump of an IRQ in
    apic_send_ipi().
    
    Signed-off-by: James Sullivan <sullivan.james.f@gmail.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dc83b43d0850..8b661d1946b5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -695,6 +695,7 @@ struct kvm_lapic_irq {
 	u16 trig_mode;
 	u32 shorthand;
 	u32 dest_id;
+	bool msi_redir_hint;
 };
 
 struct kvm_x86_ops {

commit b7cb22317305883d50f930cd6bf2fdb37df930c1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Apr 21 14:57:05 2015 +0200

    KVM: x86: tweak types of fields in kvm_lapic_irq
    
    Change to u16 if they only contain data in the low 16 bits.
    
    Change the level field to bool, since we assign 1 sometimes, but
    just mask icr_low with APIC_INT_ASSERT in apic_send_ipi.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3a19e30f0be0..dc83b43d0850 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -689,10 +689,10 @@ struct msr_data {
 
 struct kvm_lapic_irq {
 	u32 vector;
-	u32 delivery_mode;
-	u32 dest_mode;
-	u32 level;
-	u32 trig_mode;
+	u16 delivery_mode;
+	u16 dest_mode;
+	bool level;
+	u16 trig_mode;
 	u32 shorthand;
 	u32 dest_id;
 };

commit d28bc9dd25ce023270d2e039e7c98d38ecbf7758
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Mon Apr 13 14:34:08 2015 +0300

    KVM: x86: INIT and reset sequences are different
    
    x86 architecture defines differences between the reset and INIT sequences.
    INIT does not initialize the FPU (including MMX, XMM, YMM, etc.), TSC, PMU,
    MSRs (in general), MTRRs machine-check, APIC ID, APIC arbitration ID and BSP.
    
    References (from Intel SDM):
    
    "If the MP protocol has completed and a BSP is chosen, subsequent INITs (either
    to a specific processor or system wide) do not cause the MP protocol to be
    repeated." [8.4.2: MP Initialization Protocol Requirements and Restrictions]
    
    [Table 9-1. IA-32 Processor States Following Power-up, Reset, or INIT]
    
    "If the processor is reset by asserting the INIT# pin, the x87 FPU state is not
    changed." [9.2: X87 FPU INITIALIZATION]
    
    "The state of the local APIC following an INIT reset is the same as it is after
    a power-up or hardware reset, except that the APIC ID and arbitration ID
    registers are not affected." [10.4.7.3: Local APIC State After an INIT Reset
    ("Wait-for-SIPI" State)]
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Message-Id: <1428924848-28212-1-git-send-email-namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f80ad591aa61..3a19e30f0be0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -711,7 +711,7 @@ struct kvm_x86_ops {
 	/* Create, but do not attach this VCPU */
 	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
-	void (*vcpu_reset)(struct kvm_vcpu *vcpu);
+	void (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);
 
 	void (*prepare_guest_switch)(struct kvm_vcpu *vcpu);
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
@@ -1001,7 +1001,7 @@ void kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);
 
 void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
-int fx_init(struct kvm_vcpu *vcpu);
+int fx_init(struct kvm_vcpu *vcpu, bool init_event);
 
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 		       const u8 *new, int bytes);
@@ -1145,7 +1145,7 @@ int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
-void kvm_vcpu_reset(struct kvm_vcpu *vcpu);
+void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event);
 void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu);
 void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 					   unsigned long address);

commit 90de4a1875180f8347c075319af2cce586c96ab6
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Mon Apr 13 01:53:41 2015 +0300

    KVM: x86: Support for disabling quirks
    
    Introducing KVM_CAP_DISABLE_QUIRKS for disabling x86 quirks that were previous
    created in order to overcome QEMU issues. Those issue were mostly result of
    invalid VM BIOS.  Currently there are two quirks that can be disabled:
    
    1. KVM_QUIRK_LINT0_REENABLED - LINT0 was enabled after boot
    2. KVM_QUIRK_CD_NW_CLEARED - CD and NW are cleared after boot
    
    These two issues are already resolved in recent releases of QEMU, and would
    therefore be disabled by QEMU.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Message-Id: <1428879221-29996-1-git-send-email-namit@cs.technion.ac.il>
    [Report capability from KVM_CHECK_EXTENSION too. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dea2e7e962e3..f80ad591aa61 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -635,6 +635,8 @@ struct kvm_arch {
 	#endif
 
 	bool boot_vcpu_runs_old_kvmclock;
+
+	u64 disabled_quirks;
 };
 
 struct kvm_vm_stat {

commit 3ea3b7fa9af067982f34b6745584558821eea79d
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Fri Apr 3 15:40:25 2015 +0800

    kvm: mmu: lazy collapse small sptes into large sptes
    
    Dirty logging tracks sptes in 4k granularity, meaning that large sptes
    have to be split.  If live migration is successful, the guest in the
    source machine will be destroyed and large sptes will be created in the
    destination. However, the guest continues to run in the source machine
    (for example if live migration fails), small sptes will remain around
    and cause bad performance.
    
    This patch introduce lazy collapsing of small sptes into large sptes.
    The rmap will be scanned in ioctl context when dirty logging is stopped,
    dropping those sptes which can be collapsed into a single large-page spte.
    Later page faults will create the large-page sptes.
    
    Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Message-Id: <1428046825-6905-1-git-send-email-wanpeng.li@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9f1d66e2e3b5..dea2e7e962e3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -867,6 +867,8 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 				      struct kvm_memory_slot *memslot);
+void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
+					struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,

commit ae561edeb421fbc24f97df7af8607c14009c16b2
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Thu Apr 2 03:10:37 2015 +0300

    KVM: x86: DR0-DR3 are not clear on reset
    
    DR0-DR3 are not cleared as they should during reset and when they are set from
    userspace.  It appears to be caused by c77fb5fe6f03 ("KVM: x86: Allow the guest
    to run with dirty debug registers").
    
    Force their reload on these situations.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Message-Id: <1427933438-12782-4-git-send-email-namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d0cfdb08b4c2..9f1d66e2e3b5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -340,6 +340,7 @@ struct kvm_pmu {
 enum {
 	KVM_DEBUGREG_BP_ENABLED = 1,
 	KVM_DEBUGREG_WONT_EXIT = 2,
+	KVM_DEBUGREG_RELOAD = 4,
 };
 
 struct kvm_vcpu_arch {

commit 3b5a5ffa928a3f875b0d5dd284eeb7c322e1688a
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 12 19:41:34 2015 +0100

    KVM: x86: simplify kvm_apic_map
    
    recalculate_apic_map() uses two passes over all VCPUs.  This is a relic
    from time when we selected a global mode in the first pass and set up
    the optimized table in the second pass (to have a consistent mode).
    
    Recent changes made mixed mode unoptimized and we can do it in one pass.
    Format of logical MDA is a function of the mode, so we encode it in
    apic_logical_id() and drop obsoleted variables from the struct.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Message-Id: <1423766494-26150-5-git-send-email-rkrcmar@redhat.com>
    [Add lid_bits temporary in apic_logical_id. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9477b27e8e1e..d0cfdb08b4c2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -562,9 +562,6 @@ struct kvm_arch_memory_slot {
 struct kvm_apic_map {
 	struct rcu_head rcu;
 	u8 mode;
-	u8 ldr_bits;
-	/* fields bellow are used to decode ldr values in different modes */
-	u32 cid_shift, cid_mask, lid_mask;
 	struct kvm_lapic *phys_map[256];
 	/* first index is cluster id second is cpu id in a cluster */
 	struct kvm_lapic *logical_map[16][16];

commit 3548a259f6990d8cb4f520e6c14f4b45b1f2fd38
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 12 19:41:33 2015 +0100

    KVM: x86: avoid logical_map when it is invalid
    
    We want to support mixed modes and the easiest solution is to avoid
    optimizing those weird and unlikely scenarios.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Message-Id: <1423766494-26150-4-git-send-email-rkrcmar@redhat.com>
    [Add comment above KVM_APIC_MODE_* defines. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5b1bc97a258a..9477b27e8e1e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -548,8 +548,20 @@ struct kvm_arch_memory_slot {
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 };
 
+/*
+ * We use as the mode the number of bits allocated in the LDR for the
+ * logical processor ID.  It happens that these are all powers of two.
+ * This makes it is very easy to detect cases where the APICs are
+ * configured for multiple modes; in that case, we cannot use the map and
+ * hence cannot use kvm_irq_delivery_to_apic_fast either.
+ */
+#define KVM_APIC_MODE_XAPIC_CLUSTER          4
+#define KVM_APIC_MODE_XAPIC_FLAT             8
+#define KVM_APIC_MODE_X2APIC                16
+
 struct kvm_apic_map {
 	struct rcu_head rcu;
+	u8 mode;
 	u8 ldr_bits;
 	/* fields bellow are used to decode ldr values in different modes */
 	u32 cid_shift, cid_mask, lid_mask;

commit 9ea369b032d87b88f1a47187b51ad4321dea5766
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 12 19:41:32 2015 +0100

    KVM: x86: fix mixed APIC mode broadcast
    
    Broadcast allowed only one global APIC mode, but mixed modes are
    theoretically possible.  x2APIC IPI doesn't mean 0xff as broadcast,
    the rest does.
    
    x2APIC broadcasts are accepted by xAPIC.  If we take SDM to be logical,
    even addreses beginning with 0xff should be accepted, but real hardware
    disagrees.  This patch aims for simple code by considering most of real
    behavior as undefined.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Message-Id: <1423766494-26150-3-git-send-email-rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8d92e3bab118..5b1bc97a258a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -552,7 +552,7 @@ struct kvm_apic_map {
 	struct rcu_head rcu;
 	u8 ldr_bits;
 	/* fields bellow are used to decode ldr values in different modes */
-	u32 cid_shift, cid_mask, lid_mask, broadcast;
+	u32 cid_shift, cid_mask, lid_mask;
 	struct kvm_lapic *phys_map[256];
 	/* first index is cluster id second is cpu id in a cluster */
 	struct kvm_lapic *logical_map[16][16];

commit 5a4f55cde81f1633cb7ae9f0963b722e47acdc36
Author: Eugene Korenevsky <ekorenevsky@gmail.com>
Date:   Sun Mar 29 23:56:12 2015 +0300

    KVM: x86: cache maxphyaddr CPUID leaf in struct kvm_vcpu
    
    cpuid_maxphyaddr(), which performs lot of memory accesses is called
    extensively across KVM, especially in nVMX code.
    
    This patch adds a cached value of maxphyaddr to vcpu.arch to reduce the
    pressure onto CPU cache and simplify the code of cpuid_maxphyaddr()
    callers. The cached value is initialized in kvm_arch_vcpu_init() and
    reloaded every time CPUID is updated by usermode. It is obvious that
    these reloads occur infrequently.
    
    Signed-off-by: Eugene Korenevsky <ekorenevsky@gmail.com>
    Message-Id: <20150329205612.GA1223@gnote>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 30b28dc76411..8d92e3bab118 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -426,6 +426,9 @@ struct kvm_vcpu_arch {
 
 	int cpuid_nent;
 	struct kvm_cpuid_entry2 cpuid_entries[KVM_MAX_CPUID_ENTRIES];
+
+	int maxphyaddr;
+
 	/* emulate context */
 
 	struct x86_emulate_ctxt emulate_ctxt;
@@ -1124,7 +1127,6 @@ int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end)
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
-int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);
 int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);

commit b32a99180027ec980af971d548781eac1f6bb9b5
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Sun Mar 29 16:33:04 2015 +0300

    KVM: x86: Remove redundant definitions
    
    Some constants are redfined in emulate.c. Avoid it.
    
    s/SELECTOR_RPL_MASK/SEGMENT_RPL_MASK
    s/SELECTOR_TI_MASK/SEGMENT_TI_MASK
    
    No functional change.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Message-Id: <1427635984-8113-3-git-send-email-namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7ba3d9dc7ca2..30b28dc76411 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -81,9 +81,6 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
-#define SELECTOR_TI_MASK (1 << 2)
-#define SELECTOR_RPL_MASK 0x03
-
 #define KVM_PERMILLE_MMU_PAGES 20
 #define KVM_MIN_ALLOC_MMU_PAGES 64
 #define KVM_MMU_HASH_SHIFT 10

commit 0efb04406de834d820f7ba150a00d1d3194aa8a6
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Sun Mar 29 16:33:03 2015 +0300

    KVM: x86: removing redundant eflags bits definitions
    
    The eflags are redefined (using other defines) in emulate.c.
    Use the definition from processor-flags.h as some mess already started.
    No functional change.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Message-Id: <1427635984-8113-2-git-send-email-namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bf5a1606ccd5..7ba3d9dc7ca2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -84,8 +84,6 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 #define SELECTOR_TI_MASK (1 << 2)
 #define SELECTOR_RPL_MASK 0x03
 
-#define IOPL_SHIFT 12
-
 #define KVM_PERMILLE_MMU_PAGES 20
 #define KVM_MIN_ALLOC_MMU_PAGES 64
 #define KVM_MMU_HASH_SHIFT 10

commit 5cb56059c94ddfaf92567a1c6443deec8363ae1c
Author: Joel Schopp <joel.schopp@amd.com>
Date:   Mon Mar 2 13:43:31 2015 -0600

    kvm: x86: make kvm_emulate_* consistant
    
    Currently kvm_emulate() skips the instruction but kvm_emulate_* sometimes
    don't.  The end reult is the caller ends up doing the skip themselves.
    Let's make them consistant.
    
    Signed-off-by: Joel Schopp <joel.schopp@amd.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a236e39cc385..bf5a1606ccd5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -933,6 +933,7 @@ struct x86_emulate_ctxt;
 int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
 void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
+int kvm_vcpu_halt(struct kvm_vcpu *vcpu);
 int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);

commit f7819512996361280b86259222456fcf15aad926
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Feb 4 18:20:58 2015 +0100

    kvm: add halt_poll_ns module parameter
    
    This patch introduces a new module parameter for the KVM module; when it
    is present, KVM attempts a bit of polling on every HLT before scheduling
    itself out via kvm_vcpu_block.
    
    This parameter helps a lot for latency-bound workloads---in particular
    I tested it with O_DSYNC writes with a battery-backed disk in the host.
    In this case, writes are fast (because the data doesn't have to go all
    the way to the platters) but they cannot be merged by either the host or
    the guest.  KVM's performance here is usually around 30% of bare metal,
    or 50% if you use cache=directsync or cache=writethrough (these
    parameters avoid that the guest sends pointless flush requests, and
    at the same time they are not slow because of the battery-backed cache).
    The bad performance happens because on every halt the host CPU decides
    to halt itself too.  When the interrupt comes, the vCPU thread is then
    migrated to a new physical CPU, and in general the latency is horrible
    because the vCPU thread has to be scheduled back in.
    
    With this patch performance reaches 60-65% of bare metal and, more
    important, 99% of what you get if you use idle=poll in the guest.  This
    means that the tunable gets rid of this particular bottleneck, and more
    work can be done to improve performance in the kernel or QEMU.
    
    Of course there is some price to pay; every time an otherwise idle vCPUs
    is interrupted by an interrupt, it will poll unnecessarily and thus
    impose a little load on the host.  The above results were obtained with
    a mostly random value of the parameter (500000), and the load was around
    1.5-2.5% CPU usage on one of the host's core for each idle guest vCPU.
    
    The patch also adds a new stat, /sys/kernel/debug/kvm/halt_successful_poll,
    that can be used to tune the parameter.  It counts how many HLT
    instructions received an interrupt during the polling period; each
    successful poll avoids that Linux schedules the VCPU thread out and back
    in, and may also avoid a likely trip to C1 and back for the physical CPU.
    
    While the VM is idle, a Linux 4 VCPU VM halts around 10 times per second.
    Of these halts, almost all are failed polls.  During the benchmark,
    instead, basically all halts end within the polling period, except a more
    or less constant stream of 50 per second coming from vCPUs that are not
    running the benchmark.  The wasted time is thus very low.  Things may
    be slightly different for Windows VMs, which have a ~10 ms timer tick.
    
    The effect is also visible on Marcelo's recently-introduced latency
    test for the TSC deadline timer.  Though of course a non-RT kernel has
    awful latency bounds, the latency of the timer is around 8000-10000 clock
    cycles compared to 20000-120000 without setting halt_poll_ns.  For the TSC
    deadline timer, thus, the effect is both a smaller average latency and
    a smaller variance.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 848947ac6ade..a236e39cc385 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -655,6 +655,7 @@ struct kvm_vcpu_stat {
 	u32 irq_window_exits;
 	u32 nmi_window_exits;
 	u32 halt_exits;
+	u32 halt_successful_poll;
 	u32 halt_wakeup;
 	u32 request_irq_exits;
 	u32 irq_exits;

commit 1c2b364b225a5a93dbd1f317bd000d2fec2694be
Author: Tiejun Chen <tiejun.chen@intel.com>
Date:   Thu Feb 5 17:22:26 2015 +0800

    kvm: remove KVM_MMIO_SIZE
    
    After f78146b0f923, "KVM: Fix page-crossing MMIO", and
    87da7e66a405, "KVM: x86: fix vcpu->mmio_fragments overflow",
    actually KVM_MMIO_SIZE is gone.
    
    Signed-off-by: Tiejun Chen <tiejun.chen@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9dbc7435cbc2..848947ac6ade 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -38,8 +38,6 @@
 #define KVM_PRIVATE_MEM_SLOTS 3
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 
-#define KVM_MMIO_SIZE 16
-
 #define KVM_PIO_PAGE_OFFSET 1
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 2
 

commit 2e6d015799d523dcce11c7d1465e6feb7b69fab1
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Feb 2 15:26:09 2015 -0200

    KVM: x86: revert "add method to test PIR bitmap vector"
    
    Revert 7c6a98dfa1ba9dc64a62e73624ecea9995736bbd, given
    that testing PIR is not necessary anymore.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 57916ecb9b92..9dbc7435cbc2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -767,7 +767,6 @@ struct kvm_x86_ops {
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
-	bool (*test_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	void (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);

commit 88178fd4f7187bbe290c5d373fd44aabec891934
Author: Kai Huang <kai.huang@linux.intel.com>
Date:   Wed Jan 28 10:54:27 2015 +0800

    KVM: x86: Add new dirty logging kvm_x86_ops for PML
    
    This patch adds new kvm_x86_ops dirty logging hooks to enable/disable dirty
    logging for particular memory slot, and to flush potentially logged dirty GPAs
    before reporting slot->dirty_bitmap to userspace.
    
    kvm x86 common code calls these hooks when they are available so PML logic can
    be hidden to VMX specific. SVM won't be impacted as these hooks remain NULL
    there.
    
    Signed-off-by: Kai Huang <kai.huang@linux.intel.com>
    Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 67a98d793bf2..57916ecb9b92 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -802,6 +802,31 @@ struct kvm_x86_ops {
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
 
 	void (*sched_in)(struct kvm_vcpu *kvm, int cpu);
+
+	/*
+	 * Arch-specific dirty logging hooks. These hooks are only supposed to
+	 * be valid if the specific arch has hardware-accelerated dirty logging
+	 * mechanism. Currently only for PML on VMX.
+	 *
+	 *  - slot_enable_log_dirty:
+	 *	called when enabling log dirty mode for the slot.
+	 *  - slot_disable_log_dirty:
+	 *	called when disabling log dirty mode for the slot.
+	 *	also called when slot is created with log dirty disabled.
+	 *  - flush_log_dirty:
+	 *	called before reporting dirty_bitmap to userspace.
+	 *  - enable_log_dirty_pt_masked:
+	 *	called when reenabling log dirty for the GFNs in the mask after
+	 *	corresponding bits are cleared in slot->dirty_bitmap.
+	 */
+	void (*slot_enable_log_dirty)(struct kvm *kvm,
+				      struct kvm_memory_slot *slot);
+	void (*slot_disable_log_dirty)(struct kvm *kvm,
+				       struct kvm_memory_slot *slot);
+	void (*flush_log_dirty)(struct kvm *kvm);
+	void (*enable_log_dirty_pt_masked)(struct kvm *kvm,
+					   struct kvm_memory_slot *slot,
+					   gfn_t offset, unsigned long mask);
 };
 
 struct kvm_arch_async_pf {

commit 1c91cad42366ce0799ca17e7ad6995418741d012
Author: Kai Huang <kai.huang@linux.intel.com>
Date:   Wed Jan 28 10:54:26 2015 +0800

    KVM: x86: Change parameter of kvm_mmu_slot_remove_write_access
    
    This patch changes the second parameter of kvm_mmu_slot_remove_write_access from
    'slot id' to 'struct kvm_memory_slot *' to align with kvm_x86_ops dirty logging
    hooks, which will be introduced in further patch.
    
    Better way is to change second parameter of kvm_arch_commit_memory_region from
    'struct kvm_userspace_memory_region *' to 'struct kvm_memory_slot * new', but it
    requires changes on other non-x86 ARCH too, so avoid it now.
    
    Signed-off-by: Kai Huang <kai.huang@linux.intel.com>
    Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4f6369b6f7d2..67a98d793bf2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -834,7 +834,8 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask);
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
-void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
+void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
+				      struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,

commit f4b4b1808690c37c7c703d43789c1988c5e7fdeb
Author: Kai Huang <kai.huang@linux.intel.com>
Date:   Wed Jan 28 10:54:24 2015 +0800

    KVM: MMU: Add mmu help functions to support PML
    
    This patch adds new mmu layer functions to clear/set D-bit for memory slot, and
    to write protect superpages for memory slot.
    
    In case of PML, CPU logs the dirty GPA automatically to PML buffer when CPU
    updates D-bit from 0 to 1, therefore we don't have to write protect 4K pages,
    instead, we only need to clear D-bit in order to log that GPA.
    
    For superpages, we still write protect it and let page fault code to handle
    dirty page logging, as we still need to split superpage to 4K pages in PML.
    
    As PML is always enabled during guest's lifetime, to eliminate unnecessary PML
    GPA logging, we set D-bit manually for the slot with dirty logging disabled.
    
    Signed-off-by: Kai Huang <kai.huang@linux.intel.com>
    Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 843bea0e70fd..4f6369b6f7d2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -835,6 +835,15 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
+void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
+				   struct kvm_memory_slot *memslot);
+void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
+					struct kvm_memory_slot *memslot);
+void kvm_mmu_slot_set_dirty(struct kvm *kvm,
+			    struct kvm_memory_slot *memslot);
+void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
+				   struct kvm_memory_slot *slot,
+				   gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);

commit 1c6007d59a20762052cc92c0a2889ff11030d23a
Merge: c6156df9d321 4b990589952f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jan 23 13:39:51 2015 +0100

    Merge tag 'kvm-arm-for-3.20' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into kvm-next
    
    KVM/ARM changes for v3.20 including GICv3 emulation, dirty page logging, added
    trace symbols, and adding an explicit VGIC init device control IOCTL.
    
    Conflicts:
            arch/arm64/include/asm/kvm_arm.h
            arch/arm64/kvm/handle_exit.c

commit cfaa790a3fb8a7efa98f4a6457e19dc3a0db35d3
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jan 15 09:44:56 2015 +0100

    kvm: Fix CR3_PCID_INVD type on 32-bit
    
    arch/x86/kvm/emulate.c: In function ‘check_cr_write’:
    arch/x86/kvm/emulate.c:3552:4: warning: left shift count >= width of type
        rsvd = CR3_L_MODE_RESERVED_BITS & ~CR3_PCID_INVD;
    
    happens because sizeof(UL) on 32-bit is 4 bytes but we shift it 63 bits
    to the left.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 177b2f2ff9fb..4327af53e544 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -51,7 +51,7 @@
 			  | X86_CR0_NW | X86_CR0_CD | X86_CR0_PG))
 
 #define CR3_L_MODE_RESERVED_BITS 0xFFFFFF0000000000ULL
-#define CR3_PCID_INVD		 (1UL << 63)
+#define CR3_PCID_INVD		 BIT_64(63)
 #define CR4_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \

commit 54750f2cf042c42b4223d67b1bd20138464bde0e
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Jan 20 15:54:52 2015 -0200

    KVM: x86: workaround SuSE's 2.6.16 pvclock vs masterclock issue
    
    SuSE's 2.6.16 kernel fails to boot if the delta between tsc_timestamp
    and rdtsc is larger than a given threshold:
    
     * If we get more than the below threshold into the future, we rerequest
     * the real time from the host again which has only little offset then
     * that we need to adjust using the TSC.
     *
     * For now that threshold is 1/5th of a jiffie. That should be good
     * enough accuracy for completely broken systems, but also give us swing
     * to not call out to the host all the time.
     */
    #define PVCLOCK_DELTA_MAX ((1000000000ULL / HZ) / 5)
    
    Disable masterclock support (which increases said delta) in case the
    boot vcpu does not use MSR_KVM_SYSTEM_TIME_NEW.
    
    Upstreams kernels which support pvclock vsyscalls (and therefore make
    use of PVCLOCK_STABLE_BIT) use MSR_KVM_SYSTEM_TIME_NEW.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 97a5dd0222c8..177b2f2ff9fb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -627,6 +627,8 @@ struct kvm_arch {
 	#ifdef CONFIG_KVM_MMU_AUDIT
 	int audit_point;
 	#endif
+
+	bool boot_vcpu_runs_old_kvmclock;
 };
 
 struct kvm_vm_stat {

commit e108ff2f8033a417ee3e517d9f8730f665646076
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 15 15:58:54 2015 -0800

    KVM: x86: switch to kvm_get_dirty_log_protect
    
    We now have a generic function that does most of the work of
    kvm_vm_ioctl_get_dirty_log, now use it.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Mario Smarduch <m.smarduch@samsung.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cb19d05af3cd..3ceddf41ca74 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -821,9 +821,6 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
-void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
-				     struct kvm_memory_slot *slot,
-				     gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);

commit c205fb7d7d4f81e46fc577b707ceb9e356af1456
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Thu Dec 25 02:52:16 2014 +0200

    KVM: x86: #PF error-code on R/W operations is wrong
    
    When emulating an instruction that reads the destination memory operand (i.e.,
    instructions without the Mov flag in the emulator), the operand is first read.
    If a page-fault is detected in this phase, the error-code which would be
    delivered to the VM does not indicate that the access that caused the exception
    is a write one. This does not conform with real hardware, and may cause the VM
    to enter the page-fault handler twice for no reason (once for read, once for
    write).
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cb19d05af3cd..97a5dd0222c8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -160,6 +160,18 @@ enum {
 #define DR7_FIXED_1	0x00000400
 #define DR7_VOLATILE	0xffff2bff
 
+#define PFERR_PRESENT_BIT 0
+#define PFERR_WRITE_BIT 1
+#define PFERR_USER_BIT 2
+#define PFERR_RSVD_BIT 3
+#define PFERR_FETCH_BIT 4
+
+#define PFERR_PRESENT_MASK (1U << PFERR_PRESENT_BIT)
+#define PFERR_WRITE_MASK (1U << PFERR_WRITE_BIT)
+#define PFERR_USER_MASK (1U << PFERR_USER_BIT)
+#define PFERR_RSVD_MASK (1U << PFERR_RSVD_BIT)
+#define PFERR_FETCH_MASK (1U << PFERR_FETCH_BIT)
+
 /* apic attention bits */
 #define KVM_APIC_CHECK_VAPIC	0
 /*

commit 7c6a98dfa1ba9dc64a62e73624ecea9995736bbd
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Dec 16 09:08:14 2014 -0500

    KVM: x86: add method to test PIR bitmap vector
    
    kvm_x86_ops->test_posted_interrupt() returns true/false depending
    whether 'vector' is set.
    
    Next patch makes use of this interface.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d89c6b828c96..cb19d05af3cd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -753,6 +753,7 @@ struct kvm_x86_ops {
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
+	bool (*test_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	void (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);

commit cb5281a57214581902ac06fb83f0d6ea2d440318
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Dec 17 18:17:20 2014 +0100

    KVM: move APIC types to arch/x86/
    
    They are not used anymore by IA64, move them away.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0c4c88c008ce..d89c6b828c96 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -664,6 +664,16 @@ struct msr_data {
 	u64 data;
 };
 
+struct kvm_lapic_irq {
+	u32 vector;
+	u32 delivery_mode;
+	u32 dest_mode;
+	u32 level;
+	u32 trig_mode;
+	u32 shorthand;
+	u32 dest_id;
+};
+
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */

commit 203000993de5098c9b7da3b131cffa7978029994
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Tue Dec 2 19:14:59 2014 +0800

    kvm: vmx: add MSR logic for XSAVES
    
    Add logic to get/set the XSS model-specific register.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0271f6bcf123..0c4c88c008ce 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -362,6 +362,7 @@ struct kvm_vcpu_arch {
 	int mp_state;
 	u64 ia32_misc_enable_msr;
 	bool tpr_access_reporting;
+	u64 ia32_xss;
 
 	/*
 	 * Paging state of the vcpu

commit 55412b2eda2b783ef37316eb06ba91fa63ae049d
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Tue Dec 2 19:21:30 2014 +0800

    kvm: x86: Add kvm_x86_ops hook that enables XSAVES for guest
    
    Expose the XSAVES feature to the guest if the kvm_x86_ops say it is
    available.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2896dbc18987..0271f6bcf123 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -771,6 +771,7 @@ struct kvm_x86_ops {
 			       enum x86_intercept_stage stage);
 	void (*handle_external_intr)(struct kvm_vcpu *vcpu);
 	bool (*mpx_supported)(void);
+	bool (*xsaves_supported)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
 

commit 2b4a273b4266d9928d5b20154fea96f09ea5cb9a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 24 14:35:24 2014 +0100

    kvm: x86: avoid warning about potential shift wrapping bug
    
    cs.base is declared as a __u64 variable and vector is a u32 so this
    causes a static checker warning.  The user indeed can set "sipi_vector"
    to any u32 value in kvm_vcpu_ioctl_x86_set_vcpu_events(), but the
    value should really have 8-bit precision only.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 76ff3e2d8fd2..2896dbc18987 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -880,7 +880,7 @@ int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);
-void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, unsigned int vector);
+void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector);
 
 int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,
 		    int reason, bool has_error_code, u32 error_code);

commit c9eab58f6466cef3d9cd760a96e4de5e060e5195
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 24 15:27:17 2014 +0100

    KVM: x86: move device assignment out of kvm_host.h
    
    Create a new header, and hide the device assignment functions there.
    Move struct kvm_assigned_dev_kernel to assigned-dev.c by modifying
    arch/x86/kvm/iommu.c to take a PCI device struct.
    
    Based on a patch by Radim Krcmar <rkrcmark@redhat.com>.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d549cf8bfb69..76ff3e2d8fd2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1112,27 +1112,4 @@ int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
 void kvm_handle_pmu_event(struct kvm_vcpu *vcpu);
 void kvm_deliver_pmi(struct kvm_vcpu *vcpu);
 
-#ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
-int kvm_iommu_map_guest(struct kvm *kvm);
-int kvm_iommu_unmap_guest(struct kvm *kvm);
-
-long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
-				  unsigned long arg);
-
-void kvm_free_all_assigned_devices(struct kvm *kvm);
-#else
-static inline int kvm_iommu_unmap_guest(struct kvm *kvm)
-{
-	return 0;
-}
-
-static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
-						unsigned long arg)
-{
-	return -ENOTTY;
-}
-
-static inline void kvm_free_all_assigned_devices(struct kvm *kvm) {}
-#endif
-
 #endif /* _ASM_X86_KVM_HOST_H */

commit c274e03af70544506cd7214fcc2d4c4376c2c6f4
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Fri Nov 21 22:21:50 2014 +0100

    kvm: x86: move assigned-dev.c and iommu.c to arch/x86/
    
    Now that ia64 is gone, we can hide deprecated device assignment in x86.
    
    Notable changes:
     - kvm_vm_ioctl_assigned_device() was moved to x86/kvm_arch_vm_ioctl()
    
    The easy parts were removed from generic kvm code, remaining
     - kvm_iommu_(un)map_pages() would require new code to be moved
     - struct kvm_assigned_dev_kernel depends on struct kvm_irq_ack_notifier
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 76ff3e2d8fd2..d549cf8bfb69 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1112,4 +1112,27 @@ int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
 void kvm_handle_pmu_event(struct kvm_vcpu *vcpu);
 void kvm_deliver_pmi(struct kvm_vcpu *vcpu);
 
+#ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
+int kvm_iommu_map_guest(struct kvm *kvm);
+int kvm_iommu_unmap_guest(struct kvm *kvm);
+
+long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
+				  unsigned long arg);
+
+void kvm_free_all_assigned_devices(struct kvm *kvm);
+#else
+static inline int kvm_iommu_unmap_guest(struct kvm *kvm)
+{
+	return 0;
+}
+
+static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
+						unsigned long arg)
+{
+	return -ENOTTY;
+}
+
+static inline void kvm_free_all_assigned_devices(struct kvm *kvm) {}
+#endif
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit 6ef768fac9dfe3404d3fdc09909ea203a88f2f38
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 20 13:45:31 2014 +0100

    kvm: x86: move ioapic.c and irq_comm.c back to arch/x86/
    
    ia64 does not need them anymore.  Ack notifiers become x86-specific
    too.
    
    Suggested-by: Gleb Natapov <gleb@kernel.org>
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 769db36a3001..76ff3e2d8fd2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -603,6 +603,9 @@ struct kvm_arch {
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 
+	/* reads protected by irq_srcu, writes by irq_lock */
+	struct hlist_head mask_notifier_list;
+
 	/* fields used by HYPER-V emulation */
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
@@ -819,6 +822,19 @@ int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
 			  const void *val, int bytes);
 u8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 
+struct kvm_irq_mask_notifier {
+	void (*func)(struct kvm_irq_mask_notifier *kimn, bool masked);
+	int irq;
+	struct hlist_node link;
+};
+
+void kvm_register_irq_mask_notifier(struct kvm *kvm, int irq,
+				    struct kvm_irq_mask_notifier *kimn);
+void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
+				      struct kvm_irq_mask_notifier *kimn);
+void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
+			     bool mask);
+
 extern bool tdp_enabled;
 
 u64 vcpu_tsc_khz(struct kvm_vcpu *vcpu);

commit 1d4e7e3c0bca747d0fc54069a6ab8393349431c0
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Thu Nov 6 15:52:47 2014 +0000

    kvm: x86: increase user memory slots to 509
    
    With the 3 private slots, this gives us 512 slots total.
    Motivation for this is in addition to assigned devices
    support more memory hotplug slots, where 1 slot is
    used by a hotplugged memory stick.
    It will allow to support upto 256 hotplug memory
    slots and leave 253 slots for assigned devices and
    other devices that use them.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dc932d388c43..769db36a3001 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -33,7 +33,7 @@
 
 #define KVM_MAX_VCPUS 255
 #define KVM_SOFT_MAX_VCPUS 160
-#define KVM_USER_MEM_SLOTS 125
+#define KVM_USER_MEM_SLOTS 509
 /* memory slots that are not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 3
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)

commit 9d88fca71a99a65c37cbfe481b4aa4e91a27ff13
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Sun Nov 2 11:54:52 2014 +0200

    KVM: x86: MOV to CR3 can set bit 63
    
    Although Intel SDM mentions bit 63 is reserved, MOV to CR3 can have bit 63 set.
    As Intel SDM states in section 4.10.4 "Invalidation of TLBs and
    Paging-Structure Caches": " MOV to CR3. ... If CR4.PCIDE = 1 and bit 63 of the
    instructionâ€™s source operand is 0 ..."
    
    In other words, bit 63 is not reserved. KVM emulator currently consider bit 63
    as reserved. Fix it.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 904535fe825e..dc932d388c43 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -51,6 +51,7 @@
 			  | X86_CR0_NW | X86_CR0_CD | X86_CR0_PG))
 
 #define CR3_L_MODE_RESERVED_BITS 0xFFFFFF0000000000ULL
+#define CR3_PCID_INVD		 (1UL << 63)
 #define CR4_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \

commit 82b32774c2d00c0a12ab182c67e32e0b5e5e580a
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Sun Nov 2 11:54:45 2014 +0200

    KVM: x86: Breakpoints do not consider CS.base
    
    x86 debug registers hold a linear address. Therefore, breakpoints detection
    should consider CS.base, and check whether instruction linear address equals
    (CS.base + RIP). This patch introduces a function to evaluate RIP linear
    address and uses it for breakpoints detection.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3ac807646911..904535fe825e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1067,6 +1067,7 @@ void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 void kvm_define_shared_msr(unsigned index, u32 msr);
 int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
+unsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu);
 bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
 
 void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,

commit 16f8a6f9798ab9a1fd593b06b78925d02525ab81
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Fri Oct 3 01:10:05 2014 +0300

    KVM: vmx: Unavailable DR4/5 is checked before CPL
    
    If DR4/5 is accessed when it is unavailable (since CR4.DE is set), then #UD
    should be generated even if CPL>0. This is according to Intel SDM Table 6-2:
    "Priority Among Simultaneous Exceptions and Interrupts".
    
    Note, that this may happen on the first DR access, even if the host does not
    sets debug breakpoints. Obviously, it occurs when the host debugs the guest.
    
    This patch moves the DR4/5 checks from __kvm_set_dr/_kvm_get_dr to handle_dr.
    The emulator already checks DR4/5 availability in check_dr_read. Nested
    virutalization related calls to kvm_set_dr/kvm_get_dr would not like to inject
    exceptions to the guest.
    
    As for SVM, the patch follows the previous logic as much as possible. Anyhow,
    it appears the DR interception code might be buggy - even if the DR access
    may cause an exception, the instruction is skipped.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dd7cfc6de4a3..3ac807646911 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -895,6 +895,7 @@ int kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gfn_t gfn, void *data, int offset, int len,
 			    u32 access);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
+bool kvm_require_dr(struct kvm_vcpu *vcpu, int dr);
 
 static inline int __kvm_irq_line_state(unsigned long *irq_state,
 				       int irq_source_id, int level)

commit 394457a928e0f7ff121c375966f5ec1980dabc09
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Fri Oct 3 00:30:52 2014 +0300

    KVM: x86: some apic broadcast modes does not work
    
    KVM does not deliver x2APIC broadcast messages with physical mode.  Intel SDM
    (10.12.9 ICR Operation in x2APIC Mode) states: "A destination ID value of
    FFFF_FFFFH is used for broadcast of interrupts in both logical destination and
    physical destination modes."
    
    In addition, the local-apic enables cluster mode broadcast. As Intel SDM
    10.6.2.2 says: "Broadcast to all local APICs is achieved by setting all
    destination bits to one." This patch enables cluster mode broadcast.
    
    The fix tries to combine broadcast in different modes through a unified code.
    
    One rare case occurs when the source of IPI has its APIC disabled.  In such
    case, the source can still issue IPIs, but since the source is not obliged to
    have the same LAPIC mode as the enabled ones, we cannot rely on it.
    Since it is a rare case, it is unoptimized and done on the slow-path.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Reviewed-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    [As per Radim's review, use unsigned int for X2APIC_BROADCAST, return bool from
     kvm_apic_broadcast. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6ed0c30d6a0c..dd7cfc6de4a3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -542,7 +542,7 @@ struct kvm_apic_map {
 	struct rcu_head rcu;
 	u8 ldr_bits;
 	/* fields bellow are used to decode ldr values in different modes */
-	u32 cid_shift, cid_mask, lid_mask;
+	u32 cid_shift, cid_mask, lid_mask, broadcast;
 	struct kvm_lapic *phys_map[256];
 	/* first index is cluster id second is cpu id in a cluster */
 	struct kvm_lapic *logical_map[16][16];

commit 8b3c3104c3f4f706e99365c3e0d2aa61b95f969f
Author: Andy Honig <ahonig@google.com>
Date:   Wed Aug 27 11:16:44 2014 -0700

    KVM: x86: Prevent host from panicking on shared MSR writes.
    
    The previous patch blocked invalid writes directly when the MSR
    is written.  As a precaution, prevent future similar mistakes by
    gracefulling handle GPs caused by writes to shared MSRs.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Andrew Honig <ahonig@google.com>
    [Remove parts obsoleted by Nadav's patch. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ccc94de4ac49..6ed0c30d6a0c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1064,7 +1064,7 @@ void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 					   unsigned long address);
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
-void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
+int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
 bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
 

commit 854e8bb1aa06c578c2c9145fa6bfe3680ef63b23
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Tue Sep 16 03:24:05 2014 +0300

    KVM: x86: Check non-canonical addresses upon WRMSR
    
    Upon WRMSR, the CPU should inject #GP if a non-canonical value (address) is
    written to certain MSRs. The behavior is "almost" identical for AMD and Intel
    (ignoring MSRs that are not implemented in either architecture since they would
    anyhow #GP). However, IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if
    non-canonical address is written on Intel but not on AMD (which ignores the top
    32-bits).
    
    Accordingly, this patch injects a #GP on the MSRs which behave identically on
    Intel and AMD.  To eliminate the differences between the architecutres, the
    value which is written to IA32_SYSENTER_ESP and IA32_SYSENTER_EIP is turned to
    canonical value before writing instead of injecting a #GP.
    
    Some references from Intel and AMD manuals:
    
    According to Intel SDM description of WRMSR instruction #GP is expected on
    WRMSR "If the source register contains a non-canonical address and ECX
    specifies one of the following MSRs: IA32_DS_AREA, IA32_FS_BASE, IA32_GS_BASE,
    IA32_KERNEL_GS_BASE, IA32_LSTAR, IA32_SYSENTER_EIP, IA32_SYSENTER_ESP."
    
    According to AMD manual instruction manual:
    LSTAR/CSTAR (SYSCALL): "The WRMSR instruction loads the target RIP into the
    LSTAR and CSTAR registers.  If an RIP written by WRMSR is not in canonical
    form, a general-protection exception (#GP) occurs."
    IA32_GS_BASE and IA32_FS_BASE (WRFSBASE/WRGSBASE): "The address written to the
    base field must be in canonical form or a #GP fault will occur."
    IA32_KERNEL_GS_BASE (SWAPGS): "The address stored in the KernelGSbase MSR must
    be in canonical form."
    
    This patch fixes CVE-2014-3610.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7d603a71ab3a..ccc94de4ac49 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -989,6 +989,20 @@ static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 }
 
+static inline u64 get_canonical(u64 la)
+{
+	return ((int64_t)la << 16) >> 16;
+}
+
+static inline bool is_noncanonical_address(u64 la)
+{
+#ifdef CONFIG_X86_64
+	return get_canonical(la) != la;
+#else
+	return false;
+#endif
+}
+
 #define TSS_IOPB_BASE_OFFSET 0x66
 #define TSS_BASE_SIZE 0x68
 #define TSS_IOPB_SIZE (65536 / 8)

commit c24ae0dcd3e8695efa43e71704d1fc4bc7e29e9b
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Sep 24 15:57:58 2014 +0800

    kvm: x86: Unpin and remove kvm_arch->apic_access_page
    
    In order to make the APIC access page migratable, stop pinning it in
    memory.
    
    And because the APIC access page is not pinned in memory, we can
    remove kvm_arch->apic_access_page.  When we need to write its
    physical address into vmcs, we use gfn_to_page() to get its page
    struct, which is needed to call page_to_phys(); the page is then
    immediately unpinned.
    
    Suggested-by: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 60f9d73c6282..7d603a71ab3a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -574,7 +574,7 @@ struct kvm_arch {
 	struct kvm_apic_map *apic_map;
 
 	unsigned int tss_addr;
-	struct page *apic_access_page;
+	bool apic_access_page_done;
 
 	gpa_t wall_clock;
 

commit 4256f43f9fab91e1c17b5846a240cf4b66a768a8
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Sep 24 15:57:54 2014 +0800

    kvm: x86: Add request bit to reload APIC access page address
    
    Currently, the APIC access page is pinned by KVM for the entire life
    of the guest.  We want to make it migratable in order to make memory
    hot-unplug available for machines that run KVM.
    
    This patch prepares to handle this in generic code, through a new
    request bit (that will be set by the MMU notifier) and a new hook
    that is called whenever the request bit is processed.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 022c356e0fed..60f9d73c6282 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -736,6 +736,7 @@ struct kvm_x86_ops {
 	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
+	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu, hpa_t hpa);
 	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
 	void (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
@@ -1044,6 +1045,7 @@ int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu);
+void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu);
 void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 					   unsigned long address);
 

commit fe71557afbec641fee73711e40602bed37f6f33b
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Sep 24 15:57:57 2014 +0800

    kvm: Add arch specific mmu notifier for page invalidation
    
    This will be used to let the guest run while the APIC access page is
    not pinned.  Because subsequent patches will fill in the function
    for x86, place the (still empty) x86 implementation in the x86.c file
    instead of adding an inline function in kvm_host.h.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 763d273cab1d..022c356e0fed 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1044,6 +1044,8 @@ int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu);
+void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+					   unsigned long address);
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
 void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);

commit 57128468080a8b6ea452223036d3e417f748af55
Author: Andres Lagar-Cavilla <andreslc@google.com>
Date:   Mon Sep 22 14:54:42 2014 -0700

    kvm: Fix page ageing bugs
    
    1. We were calling clear_flush_young_notify in unmap_one, but we are
    within an mmu notifier invalidate range scope. The spte exists no more
    (due to range_start) and the accessed bit info has already been
    propagated (due to kvm_pfn_set_accessed). Simply call
    clear_flush_young.
    
    2. We clear_flush_young on a primary MMU PMD, but this may be mapped
    as a collection of PTEs by the secondary MMU (e.g. during log-dirty).
    This required expanding the interface of the clear_flush_young mmu
    notifier, so a lot of code has been trivially touched.
    
    3. In the absence of shadow_accessed_mask (e.g. EPT A bit), we emulate
    the access bit by blowing the spte. This requires proper synchronizing
    with MMU notifier consumers, like every other removal of spte's does.
    
    Signed-off-by: Andres Lagar-Cavilla <andreslc@google.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index eeeb573fcf6f..763d273cab1d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1035,7 +1035,7 @@ asmlinkage void kvm_spurious_fault(void);
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end);
-int kvm_age_hva(struct kvm *kvm, unsigned long hva);
+int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);

commit 77c3913b74212a86027d311f5e81625736816620
Author: Liang Chen <liangchen.linux@gmail.com>
Date:   Thu Sep 18 12:38:37 2014 -0400

    KVM: x86: directly use kvm_make_request again
    
    A one-line wrapper around kvm_make_request is not particularly
    useful. Replace kvm_mmu_flush_tlb() with kvm_make_request().
    
    Signed-off-by: Liang Chen <liangchen.linux@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 028df8dc538e..eeeb573fcf6f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -914,7 +914,6 @@ void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
 int fx_init(struct kvm_vcpu *vcpu);
 
-void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 		       const u8 *new, int bytes);
 int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn);

commit a255d4795f83cf3e6a1c7d5ab998392d9413298c
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Sep 16 18:41:58 2014 +0800

    kvm: Remove ept_identity_pagetable from struct kvm_arch.
    
    kvm_arch->ept_identity_pagetable holds the ept identity pagetable page. But
    it is never used to refer to the page at all.
    
    In vcpu initialization, it indicates two things:
    1. indicates if ept page is allocated
    2. indicates if a memory slot for identity page is initialized
    
    Actually, kvm_arch->ept_identity_pagetable_done is enough to tell if the ept
    identity pagetable is initialized. So we can remove ept_identity_pagetable.
    
    NOTE: In the original code, ept identity pagetable page is pinned in memroy.
          As a result, it cannot be migrated/hot-removed. After this patch, since
          kvm_arch->ept_identity_pagetable is removed, ept identity pagetable page
          is no longer pinned in memory. And it can be migrated/hot-removed.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2dbde3b7446c..028df8dc538e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -578,7 +578,6 @@ struct kvm_arch {
 
 	gpa_t wall_clock;
 
-	struct page *ept_identity_pagetable;
 	bool ept_identity_pagetable_done;
 	gpa_t ept_identity_map_addr;
 

commit 54987b7afa902e886b3a751c056c2a4d4701020e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 2 13:23:06 2014 +0200

    KVM: x86: propagate exception from permission checks on the nested page fault
    
    Currently, if a permission error happens during the translation of
    the final GPA to HPA, walk_addr_generic returns 0 but does not fill
    in walker->fault.  To avoid this, add an x86_exception* argument
    to the translate_gpa function, and let it fill in walker->fault.
    The nested_page_fault field will be true, since the walk_mmu is the
    nested_mmu and translate_gpu instead operates on the "outer" (NPT)
    instance.
    
    Reported-by: Valentine Sinitsyn <valentine.sinitsyn@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c9896518e54d..2dbde3b7446c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -262,7 +262,8 @@ struct kvm_mmu {
 				  struct x86_exception *fault);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    struct x86_exception *exception);
-	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access);
+	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
+			       struct x86_exception *exception);
 	int (*sync_page)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
@@ -923,7 +924,8 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
-gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access);
+gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
+			   struct x86_exception *exception);
 gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,
 			      struct x86_exception *exception);
 gpa_t kvm_mmu_gva_to_gpa_fetch(struct kvm_vcpu *vcpu, gva_t gva,
@@ -943,7 +945,8 @@ void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu);
 void kvm_enable_tdp(void);
 void kvm_disable_tdp(void);
 
-static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access)
+static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
+				  struct x86_exception *exception)
 {
 	return gpa;
 }

commit ef54bcfeea6c8b04e2a4f9396e16d88558aa2eee
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Sep 4 19:46:15 2014 +0200

    KVM: x86: skip writeback on injection of nested exception
    
    If a nested page fault happens during emulation, we will inject a vmexit,
    not a page fault.  However because writeback happens after the injection,
    we will write ctxt->eip from L2 into the L1 EIP.  We do not write back
    if an instruction caused an interception vmexit---do the same for page
    faults.
    
    Suggested-by: Gleb Natapov <gleb@kernel.org>
    Reviewed-by: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 08cc299ec6f4..c9896518e54d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -893,7 +893,6 @@ void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);
 int kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gfn_t gfn, void *data, int offset, int len,
 			    u32 access);
-void kvm_propagate_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
 static inline int __kvm_irq_line_state(unsigned long *irq_state,

commit 56f17dd3fbc44adcdbc3340fe3988ddb833a47a7
Author: David Matlack <dmatlack@google.com>
Date:   Mon Aug 18 15:46:07 2014 -0700

    kvm: x86: fix stale mmio cache bug
    
    The following events can lead to an incorrect KVM_EXIT_MMIO bubbling
    up to userspace:
    
    (1) Guest accesses gpa X without a memory slot. The gfn is cached in
    struct kvm_vcpu_arch (mmio_gfn). On Intel EPT-enabled hosts, KVM sets
    the SPTE write-execute-noread so that future accesses cause
    EPT_MISCONFIGs.
    
    (2) Host userspace creates a memory slot via KVM_SET_USER_MEMORY_REGION
    covering the page just accessed.
    
    (3) Guest attempts to read or write to gpa X again. On Intel, this
    generates an EPT_MISCONFIG. The memory slot generation number that
    was incremented in (2) would normally take care of this but we fast
    path mmio faults through quickly_check_mmio_pf(), which only checks
    the per-vcpu mmio cache. Since we hit the cache, KVM passes a
    KVM_EXIT_MMIO up to userspace.
    
    This patch fixes the issue by using the memslot generation number
    to validate the mmio cache.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: David Matlack <dmatlack@google.com>
    [xiaoguangrong: adjust the code to make it simpler for stable-tree fix.]
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Reviewed-by: David Matlack <dmatlack@google.com>
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Tested-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 73e4149eda33..08cc299ec6f4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -477,6 +477,7 @@ struct kvm_vcpu_arch {
 	u64 mmio_gva;
 	unsigned access;
 	gfn_t mmio_gfn;
+	u64 mmio_gen;
 
 	struct kvm_pmu pmu;
 

commit 13a34e067eab24fec882e1834fbf2cc31911d474
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:03 2014 +0200

    KVM: remove garbage arg to *hardware_{en,dis}able
    
    In the beggining was on_each_cpu(), which required an unused argument to
    kvm_arch_ops.hardware_{en,dis}able, but this was soon forgotten.
    
    Remove unnecessary arguments that stem from this.
    
    Signed-off-by: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 567fface45f8..73e4149eda33 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -661,8 +661,8 @@ struct msr_data {
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
-	int (*hardware_enable)(void *dummy);
-	void (*hardware_disable)(void *dummy);
+	int (*hardware_enable)(void);
+	void (*hardware_disable)(void);
 	void (*check_processor_compatibility)(void *rtn);
 	int (*hardware_setup)(void);               /* __init */
 	void (*hardware_unsetup)(void);            /* __exit */

commit 656473003bc7e056c3bbd4a4d9832dad01e86f76
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Aug 29 14:01:17 2014 +0200

    KVM: forward declare structs in kvm_types.h
    
    Opaque KVM structs are useful for prototypes in asm/kvm_host.h, to avoid
    "'struct foo' declared inside parameter list" warnings (and consequent
    breakage due to conflicting types).
    
    Move them from individual files to a generic place in linux/kvm_types.h.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ac0f90e26a0b..567fface45f8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -99,10 +99,6 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 
 #define ASYNC_PF_PER_VCPU 64
 
-struct kvm_vcpu;
-struct kvm;
-struct kvm_async_pf;
-
 enum kvm_reg {
 	VCPU_REGS_RAX = 0,
 	VCPU_REGS_RCX = 1,

commit ae97a3b818324b92b5b9cc885c63c3f4bd46ee9d
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Aug 21 18:08:06 2014 +0200

    KVM: x86: introduce sched_in to kvm_x86_ops
    
    sched_in preempt notifier is available for x86, allow its use in
    specific virtualization technlogies as well.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4bda61b582e6..ac0f90e26a0b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -771,6 +771,8 @@ struct kvm_x86_ops {
 	bool (*mpx_supported)(void);
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
+
+	void (*sched_in)(struct kvm_vcpu *kvm, int cpu);
 };
 
 struct kvm_arch_async_pf {

commit 4473b570a7ebb502f63f292ccfba7df622e5fdd3
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Mon Aug 18 17:50:28 2014 +0800

    KVM: x86: drop fpu_activate hook
    
    The only user of the fpu_activate hook was dropped in commit
    2d04a05bd7e9 (KVM: x86 emulator: emulate CLTS internally, 2011-04-20).
    vmx_fpu_activate and svm_fpu_activate are still called on #NM (and for
    Intel CLTS), but never from common code; hence, there's no need for
    a hook.
    
    Reviewed-by: Yang Zhang <yang.z.zhang@intel.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7c492ed9087b..4bda61b582e6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -710,7 +710,6 @@ struct kvm_x86_ops {
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
-	void (*fpu_activate)(struct kvm_vcpu *vcpu);
 	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);

commit 0d234daf7e0a3290a3a20c8087eefbd6335a5bd4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Aug 18 16:39:48 2014 +0200

    Revert "KVM: x86: Increase the number of fixed MTRR regs to 10"
    
    This reverts commit 682367c494869008eb89ef733f196e99415ae862,
    which causes 32-bit SMP Windows 7 guests to panic.
    
    SeaBIOS has a limit on the number of MTRRs that it can handle,
    and this patch exceeded the limit.  Better revert it.
    Thanks to Nadav Amit for debugging the cause.
    
    Cc: stable@nongnu.org
    Reported-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 572460175ba5..7c492ed9087b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -95,7 +95,7 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 #define KVM_REFILL_PAGES 25
 #define KVM_MAX_CPUID_ENTRIES 80
 #define KVM_NR_FIXED_MTRR_REGION 88
-#define KVM_NR_VAR_MTRR 10
+#define KVM_NR_VAR_MTRR 8
 
 #define ASYNC_PF_PER_VCPU 64
 

commit 8533ce72718871fb528d853391746f36243273af
Merge: c9b88e958182 42cbc04fd3b5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 12:16:46 2014 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Paolo Bonzini:
     "These are the x86, MIPS and s390 changes; PPC and ARM will come in a
      few days.
    
      MIPS and s390 have little going on this release; just bugfixes, some
      small, some larger.
    
      The highlights for x86 are nested VMX improvements (Jan Kiszka),
      optimizations for old processor (up to Nehalem, by me and Bandan Das),
      and a lot of x86 emulator bugfixes (Nadav Amit).
    
      Stephen Rothwell reported a trivial conflict with the tracing branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (104 commits)
      x86/kvm: Resolve shadow warnings in macro expansion
      KVM: s390: rework broken SIGP STOP interrupt handling
      KVM: x86: always exit on EOIs for interrupts listed in the IOAPIC redir table
      KVM: vmx: remove duplicate vmx_mpx_supported() prototype
      KVM: s390: Fix memory leak on busy SIGP stop
      x86/kvm: Resolve shadow warning from min macro
      kvm: Resolve missing-field-initializers warnings
      Replace NR_VMX_MSR with its definition
      KVM: x86: Assertions to check no overrun in MSR lists
      KVM: x86: set rflags.rf during fault injection
      KVM: x86: Setting rflags.rf during rep-string emulation
      KVM: x86: DR6/7.RTM cannot be written
      KVM: nVMX: clean up nested_release_vmcs12 and code around it
      KVM: nVMX: fix lifetime issues for vmcs02
      KVM: x86: Defining missing x86 vectors
      KVM: x86: emulator injects #DB when RFLAGS.RF is set
      KVM: x86: Cleanup of rflags.rf cleaning
      KVM: x86: Clear rflags.rf on emulated instructions
      KVM: x86: popf emulation should not change RF
      KVM: x86: Clearing rflags.rf upon skipped emulated instruction
      ...

commit 6f43ed01e87c8a8dbd8c826eaf0f714c1342c039
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Tue Jul 15 17:37:46 2014 +0300

    KVM: x86: DR6/7.RTM cannot be written
    
    Haswell and newer Intel CPUs have support for RTM, and in that case DR6.RTM is
    not fixed to 1 and DR7.RTM is not fixed to zero. That is not the case in the
    current KVM implementation. This bug is apparent only if the MOV-DR instruction
    is emulated or the host also debugs the guest.
    
    This patch is a partial fix which enables DR6.RTM and DR7.RTM to be cleared and
    set respectively. It also sets DR6.RTM upon every debug exception. Obviously,
    it is not a complete fix, as debugging of RTM is still unsupported.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b8a4480176b9..a84eaf7ba33f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -152,14 +152,16 @@ enum {
 
 #define DR6_BD		(1 << 13)
 #define DR6_BS		(1 << 14)
-#define DR6_FIXED_1	0xffff0ff0
-#define DR6_VOLATILE	0x0000e00f
+#define DR6_RTM		(1 << 16)
+#define DR6_FIXED_1	0xfffe0ff0
+#define DR6_INIT	0xffff0ff0
+#define DR6_VOLATILE	0x0001e00f
 
 #define DR7_BP_EN_MASK	0x000000ff
 #define DR7_GE		(1 << 9)
 #define DR7_GD		(1 << 13)
 #define DR7_FIXED_1	0x00000400
-#define DR7_VOLATILE	0xffff23ff
+#define DR7_VOLATILE	0xffff2bff
 
 /* apic attention bits */
 #define KVM_APIC_CHECK_VAPIC	0

commit 37ccdcbe0757196ec98c0dcf9754bec8423807a5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 20 14:29:47 2014 +0200

    KVM: x86: return all bits from get_interrupt_shadow
    
    For the next patch we will need to know the full state of the
    interrupt shadow; we will then set KVM_REQ_EVENT when one bit
    is cleared.
    
    However, right now get_interrupt_shadow only returns the one
    corresponding to the emulated instruction, or an unconditional
    0 if the emulated instruction does not have an interrupt shadow.
    This is confusing and does not allow us to check for cleared
    bits as mentioned above.
    
    Clean the callback up, and modify toggle_interruptibility to
    match the comment above the call.  As a small result, the
    call to set_interrupt_shadow will be skipped in the common
    case where int_shadow == 0 && mask == 0.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index af36f89fe67a..b8a4480176b9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -717,7 +717,7 @@ struct kvm_x86_ops {
 	int (*handle_exit)(struct kvm_vcpu *vcpu);
 	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
 	void (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
-	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
+	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu);
 	void (*patch_hypercall)(struct kvm_vcpu *vcpu,
 				unsigned char *hypercall_addr);
 	void (*set_irq)(struct kvm_vcpu *vcpu);

commit 0d3da0d26e3c3515997c99451ce3b0ad1a69a36c
Author: Tomasz Grabiec <tgrabiec@cloudius-systems.com>
Date:   Tue Jun 24 09:42:43 2014 +0200

    KVM: x86: fix TSC matching
    
    I've observed kvmclock being marked as unstable on a modern
    single-socket system with a stable TSC and qemu-1.6.2 or qemu-2.0.0.
    
    The culprit was failure in TSC matching because of overflow of
    kvm_arch::nr_vcpus_matched_tsc in case there were multiple TSC writes
    in a single synchronization cycle.
    
    Turns out that qemu does multiple TSC writes during init, below is the
    evidence of that (qemu-2.0.0):
    
    The first one:
    
     0xffffffffa08ff2b4 : vmx_write_tsc_offset+0xa4/0xb0 [kvm_intel]
     0xffffffffa04c9c05 : kvm_write_tsc+0x1a5/0x360 [kvm]
     0xffffffffa04cfd6b : kvm_arch_vcpu_postcreate+0x4b/0x80 [kvm]
     0xffffffffa04b8188 : kvm_vm_ioctl+0x418/0x750 [kvm]
    
    The second one:
    
     0xffffffffa08ff2b4 : vmx_write_tsc_offset+0xa4/0xb0 [kvm_intel]
     0xffffffffa04c9c05 : kvm_write_tsc+0x1a5/0x360 [kvm]
     0xffffffffa090610d : vmx_set_msr+0x29d/0x350 [kvm_intel]
     0xffffffffa04be83b : do_set_msr+0x3b/0x60 [kvm]
     0xffffffffa04c10a8 : msr_io+0xc8/0x160 [kvm]
     0xffffffffa04caeb6 : kvm_arch_vcpu_ioctl+0xc86/0x1060 [kvm]
     0xffffffffa04b6797 : kvm_vcpu_ioctl+0xc7/0x5a0 [kvm]
    
     #0  kvm_vcpu_ioctl at /build/buildd/qemu-2.0.0+dfsg/kvm-all.c:1780
     #1  kvm_put_msrs at /build/buildd/qemu-2.0.0+dfsg/target-i386/kvm.c:1270
     #2  kvm_arch_put_registers at /build/buildd/qemu-2.0.0+dfsg/target-i386/kvm.c:1909
     #3  kvm_cpu_synchronize_post_init at /build/buildd/qemu-2.0.0+dfsg/kvm-all.c:1641
     #4  cpu_synchronize_post_init at /build/buildd/qemu-2.0.0+dfsg/include/sysemu/kvm.h:330
     #5  cpu_synchronize_all_post_init () at /build/buildd/qemu-2.0.0+dfsg/cpus.c:521
     #6  main at /build/buildd/qemu-2.0.0+dfsg/vl.c:4390
    
    The third one:
    
     0xffffffffa08ff2b4 : vmx_write_tsc_offset+0xa4/0xb0 [kvm_intel]
     0xffffffffa04c9c05 : kvm_write_tsc+0x1a5/0x360 [kvm]
     0xffffffffa090610d : vmx_set_msr+0x29d/0x350 [kvm_intel]
     0xffffffffa04be83b : do_set_msr+0x3b/0x60 [kvm]
     0xffffffffa04c10a8 : msr_io+0xc8/0x160 [kvm]
     0xffffffffa04caeb6 : kvm_arch_vcpu_ioctl+0xc86/0x1060 [kvm]
     0xffffffffa04b6797 : kvm_vcpu_ioctl+0xc7/0x5a0 [kvm]
    
     #0  kvm_vcpu_ioctl at /build/buildd/qemu-2.0.0+dfsg/kvm-all.c:1780
     #1  kvm_put_msrs  at /build/buildd/qemu-2.0.0+dfsg/target-i386/kvm.c:1270
     #2  kvm_arch_put_registers  at /build/buildd/qemu-2.0.0+dfsg/target-i386/kvm.c:1909
     #3  kvm_cpu_synchronize_post_reset  at /build/buildd/qemu-2.0.0+dfsg/kvm-all.c:1635
     #4  cpu_synchronize_post_reset  at /build/buildd/qemu-2.0.0+dfsg/include/sysemu/kvm.h:323
     #5  cpu_synchronize_all_post_reset () at /build/buildd/qemu-2.0.0+dfsg/cpus.c:512
     #6  main  at /build/buildd/qemu-2.0.0+dfsg/vl.c:4482
    
    The fix is to count each vCPU only once when matched, so that
    nr_vcpus_matched_tsc holds the size of the matched set. This is
    achieved by reusing generation counters. Every vCPU with
    this_tsc_generation == cur_tsc_generation is in the matched set. The
    match set is cleared by setting cur_tsc_generation to a value which no
    other vCPU is set to (by incrementing it).
    
    I needed to bump up the counter size form u8 to u64 to ensure it never
    overflows. Otherwise in cases TSC is not written the same number of
    times on each vCPU the counter could overflow and incorrectly indicate
    some vCPUs as being in the matched set. This scenario seems unlikely
    but I'm not sure if it can be disregarded.
    
    Signed-off-by: Tomasz Grabiec <tgrabiec@cloudius-systems.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 63e020be3da7..af36f89fe67a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -448,7 +448,7 @@ struct kvm_vcpu_arch {
 	u64 tsc_offset_adjustment;
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
-	u8  this_tsc_generation;
+	u64 this_tsc_generation;
 	bool tsc_catchup;
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
@@ -591,7 +591,7 @@ struct kvm_arch {
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
 	u64 cur_tsc_offset;
-	u8  cur_tsc_generation;
+	u64 cur_tsc_generation;
 	int nr_vcpus_matched_tsc;
 
 	spinlock_t pvclock_gtod_sync_lock;

commit 7cb060a91c0efc5ff94f83c6df3ed705e143cdb9
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 19 11:40:18 2014 +0200

    KVM: x86: preserve the high 32-bits of the PAT register
    
    KVM does not really do much with the PAT, so this went unnoticed for a
    long time.  It is exposed however if you try to do rdmsr on the PAT
    register.
    
    Reported-by: Valentine Sinitsyn <valentine.sinitsyn@gmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0bab29de7f1b..49205d01b9ad 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -461,7 +461,7 @@ struct kvm_vcpu_arch {
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
 
 	struct mtrr_state_type mtrr_state;
-	u32 pat;
+	u64 pat;
 
 	unsigned switch_db_regs;
 	unsigned long db[KVM_NR_DB_REGS];

commit 682367c494869008eb89ef733f196e99415ae862
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Wed Jun 18 17:21:19 2014 +0300

    KVM: x86: Increase the number of fixed MTRR regs to 10
    
    Recent Intel CPUs have 10 variable range MTRRs. Since operating systems
    sometime make assumptions on CPUs while they ignore capability MSRs, it is
    better for KVM to be consistent with recent CPUs. Reporting more MTRRs than
    actually supported has no functional implications.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 49314155b66c..0bab29de7f1b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -95,7 +95,7 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 #define KVM_REFILL_PAGES 25
 #define KVM_MAX_CPUID_ENTRIES 80
 #define KVM_NR_FIXED_MTRR_REGION 88
-#define KVM_NR_VAR_MTRR 8
+#define KVM_NR_VAR_MTRR 10
 
 #define ASYNC_PF_PER_VCPU 64
 

commit 67f4d4288c353734d29c45f6725971c71af96791
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Mon Jun 2 18:34:09 2014 +0300

    KVM: x86: rdpmc emulation checks the counter incorrectly
    
    The rdpmc emulation checks that the counter (ECX) is not higher than 2, without
    taking into considerations bits 30:31 role (e.g., bit 30 marks whether the
    counter is fixed). The fix uses the pmu information for checking the validity
    of the pmu counter.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 49314155b66c..63e020be3da7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1070,6 +1070,7 @@ void kvm_pmu_cpuid_update(struct kvm_vcpu *vcpu);
 bool kvm_pmu_msr(struct kvm_vcpu *vcpu, u32 msr);
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data);
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info);
+int kvm_pmu_check_pmc(struct kvm_vcpu *vcpu, unsigned pmc);
 int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
 void kvm_handle_pmu_event(struct kvm_vcpu *vcpu);
 void kvm_deliver_pmi(struct kvm_vcpu *vcpu);

commit ae9fedc793c4d98aa9bb298585b2b9246096ce65
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 14 09:39:49 2014 +0200

    KVM: x86: get CPL from SS.DPL
    
    CS.RPL is not equal to the CPL in the few instructions between
    setting CR0.PE and reloading CS.  And CS.DPL is also not equal
    to the CPL for conforming code segments.
    
    However, SS.DPL *is* always equal to the CPL except for the weird
    case of SYSRET on AMD processors, which sets SS.DPL=SS.RPL from the
    value in the STAR MSR, but force CPL=3 (Intel instead forces
    SS.DPL=SS.RPL=CPL=3).
    
    So this patch:
    
    - modifies SVM to update the CPL from SS.DPL rather than CS.RPL;
    the above case with SYSRET is not broken further, and the way
    to fix it would be to pass the CPL to userspace and back
    
    - modifies VMX to always return the CPL from SS.DPL (except
    forcing it to 0 if we are emulating real mode via vm86 mode;
    in vm86 mode all DPLs have to be 3, but real mode does allow
    privileged instructions).  It also removes the CPL cache,
    which becomes a duplicate of the SS access rights cache.
    
    This fixes doing KVM_IOCTL_SET_SREGS exactly after setting
    CR0.PE=1 but before CS has been reloaded.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e21aee98a5c2..49314155b66c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -130,7 +130,6 @@ enum kvm_reg_ex {
 	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
 	VCPU_EXREG_CR3,
 	VCPU_EXREG_RFLAGS,
-	VCPU_EXREG_CPL,
 	VCPU_EXREG_SEGMENTS,
 };
 

commit 346874c9507a2582d0c00021f848de6e115f276c
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Fri Apr 18 03:35:09 2014 +0300

    KVM: x86: Fix CR3 reserved bits
    
    According to Intel specifications, PAE and non-PAE does not have any reserved
    bits.  In long-mode, regardless to PCIDE, only the high bits (above the
    physical address) are reserved.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7de069afb382..e21aee98a5c2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -50,11 +50,7 @@
 			  | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM \
 			  | X86_CR0_NW | X86_CR0_CD | X86_CR0_PG))
 
-#define CR3_PAE_RESERVED_BITS ((X86_CR3_PWT | X86_CR3_PCD) - 1)
-#define CR3_NONPAE_RESERVED_BITS ((PAGE_SIZE-1) & ~(X86_CR3_PWT | X86_CR3_PCD))
-#define CR3_PCID_ENABLED_RESERVED_BITS 0xFFFFFF0000000000ULL
-#define CR3_L_MODE_RESERVED_BITS (CR3_NONPAE_RESERVED_BITS |	\
-				  0xFFFFFF0000000000ULL)
+#define CR3_L_MODE_RESERVED_BITS 0xFFFFFF0000000000ULL
 #define CR4_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \

commit 56d6efc2de5fcf76d3c7b33a7671bc04c53cb0e5
Author: Feng Wu <feng.wu@intel.com>
Date:   Tue Apr 1 17:46:33 2014 +0800

    KVM: Remove SMAP bit from CR4_RESERVED_BITS
    
    This patch removes SMAP bit from CR4_RESERVED_BITS.
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fcaf9c961265..7de069afb382 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -60,7 +60,7 @@
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
 			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR | X86_CR4_PCIDE \
 			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_FSGSBASE \
-			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE))
+			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE | X86_CR4_SMAP))
 
 #define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)
 

commit c77fb5fe6f031bee9403397ae7b94ea22ea19aa7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 21 10:17:24 2014 +0100

    KVM: x86: Allow the guest to run with dirty debug registers
    
    When not running in guest-debug mode, the guest controls the debug
    registers and having to take an exit for each DR access is a waste
    of time.  If the guest gets into a state where each context switch
    causes DR to be saved and restored, this can take away as much as 40%
    of the execution time from the guest.
    
    After this patch, VMX- and SVM-specific code can set a flag in
    switch_db_regs, telling vcpu_enter_guest that on the next exit the debug
    registers might be dirty and need to be reloaded (syncing will be taken
    care of by a new callback in kvm_x86_ops).  This flag can be set on the
    first access to a debug registers, so that multiple accesses to the
    debug registers only cause one vmexit.
    
    Note that since the guest will be able to read debug registers and
    enable breakpoints in DR7, we need to ensure that they are synchronized
    on entry to the guest---including DR6 that was not synced before.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 35f538bda3a9..fcaf9c961265 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -339,6 +339,7 @@ struct kvm_pmu {
 
 enum {
 	KVM_DEBUGREG_BP_ENABLED = 1,
+	KVM_DEBUGREG_WONT_EXIT = 2,
 };
 
 struct kvm_vcpu_arch {
@@ -707,6 +708,7 @@ struct kvm_x86_ops {
 	void (*set_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	u64 (*get_dr6)(struct kvm_vcpu *vcpu);
 	void (*set_dr6)(struct kvm_vcpu *vcpu, unsigned long value);
+	void (*sync_dirty_debug_regs)(struct kvm_vcpu *vcpu);
 	void (*set_dr7)(struct kvm_vcpu *vcpu, unsigned long value);
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);

commit 360b948d88bf30ef4b10b693adf497f51fb46a08
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 21 09:55:56 2014 +0100

    KVM: x86: change vcpu->arch.switch_db_regs to a bit mask
    
    The next patch will add another bit that we can test with the
    same "if".
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7930c294182a..35f538bda3a9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -337,6 +337,10 @@ struct kvm_pmu {
 	u64 reprogram_pmi;
 };
 
+enum {
+	KVM_DEBUGREG_BP_ENABLED = 1,
+};
+
 struct kvm_vcpu_arch {
 	/*
 	 * rip and regs accesses must go through
@@ -463,7 +467,7 @@ struct kvm_vcpu_arch {
 	struct mtrr_state_type mtrr_state;
 	u32 pat;
 
-	int switch_db_regs;
+	unsigned switch_db_regs;
 	unsigned long db[KVM_NR_DB_REGS];
 	unsigned long dr6;
 	unsigned long dr7;

commit c9a7953f09bbe2b66050ebf97e0532eaeefbc9f3
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Fri Mar 7 20:03:15 2014 +0100

    KVM: x86: Remove return code from enable_irq/nmi_window
    
    It's no longer possible to enter enable_irq_window in guest mode when
    L1 intercepts external interrupts and we are entering L2. This is now
    caught in vcpu_enter_guest. So we can remove the check from the VMX
    version of enable_irq_window, thus the need to return an error code from
    both enable_irq_window and enable_nmi_window.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 461d00a554e0..7930c294182a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -729,8 +729,8 @@ struct kvm_x86_ops {
 	int (*nmi_allowed)(struct kvm_vcpu *vcpu);
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
 	void (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);
-	int (*enable_nmi_window)(struct kvm_vcpu *vcpu);
-	int (*enable_irq_window)(struct kvm_vcpu *vcpu);
+	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
+	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
 	int (*vm_has_apicv)(struct kvm *kvm);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);

commit b6b8a1451fc40412c57d10c94b62e22acab28f94
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Fri Mar 7 20:03:12 2014 +0100

    KVM: nVMX: Rework interception of IRQs and NMIs
    
    Move the check for leaving L2 on pending and intercepted IRQs or NMIs
    from the *_allowed handler into a dedicated callback. Invoke this
    callback at the relevant points before KVM checks if IRQs/NMIs can be
    injected. The callback has the task to switch from L2 to L1 if needed
    and inject the proper vmexit events.
    
    The rework fixes L2 wakeups from HLT and provides the foundation for
    preemption timer emulation.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 85be627ef5de..461d00a554e0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -767,6 +767,8 @@ struct kvm_x86_ops {
 			       enum x86_intercept_stage stage);
 	void (*handle_external_intr)(struct kvm_vcpu *vcpu);
 	bool (*mpx_supported)(void);
+
+	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
 };
 
 struct kvm_arch_async_pf {

commit 332967a3eac06f6379283cf155c84fe7cd0537c2
Author: Andrew Jones <drjones@redhat.com>
Date:   Fri Feb 28 12:52:55 2014 +0100

    x86: kvm: introduce periodic global clock updates
    
    commit 0061d53daf26f introduced a mechanism to execute a global clock
    update for a vm. We can apply this periodically in order to propagate
    host NTP corrections. Also, if all vcpus of a vm are pinned, then
    without an additional trigger, no guest NTP corrections can propagate
    either, as the current trigger is only vcpu cpu migration.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9aa09d330a4b..85be627ef5de 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -599,6 +599,7 @@ struct kvm_arch {
 	u64 master_kernel_ns;
 	cycle_t master_cycle_now;
 	struct delayed_work kvmclock_update_work;
+	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 

commit 7e44e4495a398eb553ce561f29f9148f40a3448f
Author: Andrew Jones <drjones@redhat.com>
Date:   Fri Feb 28 12:52:54 2014 +0100

    x86: kvm: rate-limit global clock updates
    
    When we update a vcpu's local clock it may pick up an NTP correction.
    We can't wait an indeterminate amount of time for other vcpus to pick
    up that correction, so commit 0061d53daf26f introduced a global clock
    update. However, we can't request a global clock update on every vcpu
    load either (which is what happens if the tsc is marked as unstable).
    The solution is to rate-limit the global clock updates. Marcelo
    calculated that we should delay the global clock updates no more
    than 0.1s as follows:
    
    Assume an NTP correction c is applied to one vcpu, but not the other,
    then in n seconds the delta of the vcpu system_timestamps will be
    c * n. If we assume a correction of 500ppm (worst-case), then the two
    vcpus will diverge 50us in 0.1s, which is a considerable amount.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e714f8c08ccf..9aa09d330a4b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -598,6 +598,7 @@ struct kvm_arch {
 	bool use_master_clock;
 	u64 master_kernel_ns;
 	cycle_t master_cycle_now;
+	struct delayed_work kvmclock_update_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 

commit da8999d31818fdc8508d527ba3aac2e128005af4
Author: Liu, Jinsong <jinsong.liu@intel.com>
Date:   Mon Feb 24 10:55:46 2014 +0000

    KVM: x86: Intel MPX vmx and msr handle
    
    From caddc009a6d2019034af8f2346b2fd37a81608d0 Mon Sep 17 00:00:00 2001
    From: Liu Jinsong <jinsong.liu@intel.com>
    Date: Mon, 24 Feb 2014 18:11:11 +0800
    Subject: [PATCH v5 1/3] KVM: x86: Intel MPX vmx and msr handle
    
    This patch handle vmx and msr of Intel MPX feature.
    
    Signed-off-by: Xudong Hao <xudong.hao@intel.com>
    Signed-off-by: Liu Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0ffe7140d630..e714f8c08ccf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -764,6 +764,7 @@ struct kvm_x86_ops {
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage);
 	void (*handle_external_intr)(struct kvm_vcpu *vcpu);
+	bool (*mpx_supported)(void);
 };
 
 struct kvm_arch_async_pf {

commit 4f34d683e52271197e1ee17b7095e8ba27761ba6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jan 29 17:31:38 2014 -0200

    KVM: x86: remove unused last_kernel_ns variable
    
    Remove unused last_kernel_ns variable.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fdf83afbb7d9..0ffe7140d630 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -444,7 +444,6 @@ struct kvm_vcpu_arch {
 	} st;
 
 	u64 last_guest_tsc;
-	u64 last_kernel_ns;
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;
 	u64 this_tsc_nsec;

commit 73aaf249ee2287b4686ff079dcbdbbb658156e64
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Sat Jan 4 18:47:16 2014 +0100

    KVM: SVM: Fix reading of DR6
    
    In contrast to VMX, SVM dose not automatically transfer DR6 into the
    VCPU's arch.dr6. So if we face a DR6 read, we must consult a new vendor
    hook to obtain the current value. And as SVM now picks the DR6 state
    from its VMCB, we also need a set callback in order to write updates of
    DR6 back.
    
    Fixes a regression of 020df0794f.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 33fef0738a29..fdf83afbb7d9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -700,6 +700,8 @@ struct kvm_x86_ops {
 	void (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*get_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*set_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
+	u64 (*get_dr6)(struct kvm_vcpu *vcpu);
+	void (*set_dr6)(struct kvm_vcpu *vcpu, unsigned long value);
 	void (*set_dr7)(struct kvm_vcpu *vcpu, unsigned long value);
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);

commit e984097b553ed2d6551c805223e4057421370f00
Author: Vadim Rozenfeld <vrozenfe@redhat.com>
Date:   Thu Jan 16 20:18:37 2014 +1100

    add support for Hyper-V reference time counter
    
    Signed-off: Peter Lieven <pl@kamp.de>
    Signed-off: Gleb Natapov
    Signed-off: Vadim Rozenfeld <vrozenfe@redhat.com>
    
    After some consideration I decided to submit only Hyper-V reference
    counters support this time. I will submit iTSC support as a separate
    patch as soon as it is ready.
    
    v1 -> v2
    1. mark TSC page dirty as suggested by
        Eric Northup <digitaleric@google.com> and Gleb
    2. disable local irq when calling get_kernel_ns,
        as it was done by Peter Lieven <pl@amp.de>
    3. move check for TSC page enable from second patch
        to this one.
    
    v3 -> v4
        Get rid of ref counter offset.
    
    v4 -> v5
        replace __copy_to_user with kvm_write_guest
        when updateing iTSC page.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ae5d7830855c..33fef0738a29 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -605,6 +605,7 @@ struct kvm_arch {
 	/* fields used by HYPER-V emulation */
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
+	u64 hv_tsc_page;
 
 	#ifdef CONFIG_KVM_MMU_AUDIT
 	int audit_point;

commit a890b6fefd1775a1c3a7d8fe8af968a3a7b23c04
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Sun Oct 20 15:30:16 2013 +0100

    kvm: Delete prototype for non-existent function kvm_check_iopl
    
    The prototype for kvm_check_iopl appeared in commit
    f850e2e603bf5a05b0aee7901857cf85715aa694 ("KVM: x86 emulator: Check IOPL
    level during io instruction emulation"), but the function never actually
    existed.  Remove the prototype.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5dc75edaaa1d..ae5d7830855c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -936,8 +936,6 @@ void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu);
 void kvm_enable_tdp(void);
 void kvm_disable_tdp(void);
 
-bool kvm_check_iopl(struct kvm_vcpu *vcpu);
-
 static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access)
 {
 	return gpa;

commit 35a5121b588f88686cab289b29eff3ce011f55a3
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Sun Oct 20 15:29:57 2013 +0100

    kvm: Delete prototype for non-existent function complete_pio
    
    complete_pio ceased to exist in commit
    7972995b0c346de76fe260ce0fd6bcc8ffab724a ("KVM: x86 emulator: Move
    string pio emulation into emulator.c"), but the prototype remained.
    Remove its prototype.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index de388c55e7ec..5dc75edaaa1d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -936,7 +936,6 @@ void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu);
 void kvm_enable_tdp(void);
 void kvm_disable_tdp(void);
 
-int complete_pio(struct kvm_vcpu *vcpu);
 bool kvm_check_iopl(struct kvm_vcpu *vcpu);
 
 static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access)

commit e0f0bbc527f6e9c0261f1d16b2a0b47612b7f235
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed Oct 30 11:02:30 2013 -0600

    kvm: Create non-coherent DMA registeration
    
    We currently use some ad-hoc arch variables tied to legacy KVM device
    assignment to manage emulation of instructions that depend on whether
    non-coherent DMA is present.  Create an interface for this, adapting
    legacy KVM device assignment and adding VFIO via the KVM-VFIO device.
    For now we assume that non-coherent DMA is possible any time we have a
    VFIO group.  Eventually an interface can be developed as part of the
    VFIO external user interface to query the coherency of a group.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 91b35e4005d3..de388c55e7ec 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -565,6 +565,8 @@ struct kvm_arch {
 	struct list_head assigned_dev_head;
 	struct iommu_domain *iommu_domain;
 	bool iommu_noncoherent;
+#define __KVM_HAVE_ARCH_NONCOHERENT_DMA
+	atomic_t noncoherent_dma_count;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;

commit d96eb2c6f480769bff32054e78b964860dae4d56
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed Oct 30 11:02:23 2013 -0600

    kvm/x86: Convert iommu_flags to iommu_noncoherent
    
    Default to operating in coherent mode.  This simplifies the logic when
    we switch to a model of registering and unregistering noncoherent I/O
    with KVM.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5cbf3166257c..91b35e4005d3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -564,7 +564,7 @@ struct kvm_arch {
 
 	struct list_head assigned_dev_head;
 	struct iommu_domain *iommu_domain;
-	int iommu_flags;
+	bool iommu_noncoherent;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;

commit 6d9d41e57440e32a3400f37aa05ef7a1a09ced64
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Wed Oct 2 14:22:28 2013 -0700

    KVM: Move gfn_to_index to x86 specific code
    
    The gfn_to_index function relies on huge page defines which either may
    not make sense on systems that don't support huge pages or are defined
    in an unconvenient way for other architectures.  Since this is
    x86-specific, move the function to arch/x86/include/asm/kvm_host.h.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8dd143a65d60..5cbf3166257c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -79,6 +79,13 @@
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
+static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
+{
+	/* KVM_HPAGE_GFN_SHIFT(PT_PAGE_TABLE_LEVEL) must be 0. */
+	return (gfn >> KVM_HPAGE_GFN_SHIFT(level)) -
+		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
+}
+
 #define SELECTOR_TI_MASK (1 << 2)
 #define SELECTOR_RPL_MASK 0x03
 

commit 8a3c1a33476f6bfebd07954e2277dbc88003bd37
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 2 16:56:13 2013 +0200

    KVM: mmu: change useless int return types to void
    
    kvm_mmu initialization is mostly filling in function pointers, there is
    no way for it to fail.  Clean up unused return values.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6e9785f5029f..8dd143a65d60 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -780,11 +780,11 @@ void kvm_mmu_module_exit(void);
 
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
 int kvm_mmu_create(struct kvm_vcpu *vcpu);
-int kvm_mmu_setup(struct kvm_vcpu *vcpu);
+void kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask);
 
-int kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
+void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
 void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 				     struct kvm_memory_slot *slot,

commit d8d173dab2505e72b62882e5a580862e6ec1c06c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 2 16:56:11 2013 +0200

    KVM: mmu: remove uninteresting MMU "new_cr3" callbacks
    
    The new_cr3 MMU callback has been a wrapper for mmu_free_roots since commit
    e676505 (KVM: MMU: Force cr3 reload with two dimensional paging on mov
    cr3 emulation, 2012-07-08).
    
    The commit message mentioned that "mmu_free_roots() is somewhat of an overkill,
    but fixing that is more complicated and will be done after this minimal fix".
    One year has passed, and no one really felt the need to do a different fix.
    Wrap the call with a kvm_mmu_new_cr3 function for clarity, but remove the
    callback.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 671c6f0bea51..6e9785f5029f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -253,7 +253,6 @@ struct kvm_pio_request {
  * mode.
  */
 struct kvm_mmu {
-	void (*new_cr3)(struct kvm_vcpu *vcpu);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
@@ -923,6 +922,7 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u32 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
+void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu);
 
 void kvm_enable_tdp(void);
 void kvm_disable_tdp(void);

commit 206260941fd4b6f25f28ecf4e267b2f9a0ba72d7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 2 16:56:10 2013 +0200

    KVM: mmu: remove uninteresting MMU "free" callbacks
    
    The free MMU callback has been a wrapper for mmu_free_roots since mmu_free_roots
    itself was introduced (commit 17ac10a, [PATCH] KVM: MU: Special treatment
    for shadow pae root pages, 2007-01-05), and has always been the same for all
    MMU cases.  Remove the indirection as it is useless.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 52110d0ceb13..671c6f0bea51 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -261,7 +261,6 @@ struct kvm_mmu {
 			  bool prefault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
-	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    struct x86_exception *exception);
 	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access);

commit 4344ee981e21990f8ea14d3c9e3890b9b7b06279
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 2 16:06:16 2013 +0200

    KVM: x86: only copy XSAVE state for the supported features
    
    This makes the interface more deterministic for userspace, which can expect
    (after configuring only the features it supports) to get exactly the same
    state from the kernel, independent of the host CPU and kernel version.
    
    Suggested-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 35d10d1a6b58..52110d0ceb13 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -390,6 +390,7 @@ struct kvm_vcpu_arch {
 	struct fpu guest_fpu;
 	u64 xcr0;
 	u64 guest_supported_xcr0;
+	u32 guest_xstate_size;
 
 	struct kvm_pio_request pio;
 	void *pio_data;

commit d7876f1be40a16223a44355740de625849504eb5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 2 16:06:15 2013 +0200

    KVM: x86: prevent setting unsupported XSAVE states
    
    A guest can still attempt to save and restore XSAVE states even if they
    have been masked in CPUID leaf 0Dh.  This usually is not visible to
    the guest, but is still wrong: "Any attempt to set a reserved bit (as
    determined by the contents of EAX and EDX after executing CPUID with
    EAX=0DH, ECX= 0H) in XCR0 for a given processor will result in a #GP
    exception".
    
    The patch also performs the same checks as __kvm_set_xcr in KVM_SET_XSAVE.
    This catches migration from newer to older kernel/processor before the
    guest starts running.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c76ff74a98f2..35d10d1a6b58 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -389,6 +389,7 @@ struct kvm_vcpu_arch {
 
 	struct fpu guest_fpu;
 	u64 xcr0;
+	u64 guest_supported_xcr0;
 
 	struct kvm_pio_request pio;
 	void *pio_data;

commit 6aef266c6e17b798a1740cf70cd34f90664740b3
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Mon Aug 26 14:18:34 2013 +0530

    kvm hypervisor : Add a hypercall to KVM hypervisor to support pv-ticketlocks
    
    kvm_hc_kick_cpu allows the calling vcpu to kick another vcpu out of halt state.
    the presence of these hypercalls is indicated to guest via
    kvm_feature_pv_unhalt.
    
    Fold pv_unhalt flag into GET_MP_STATE ioctl to aid migration
    During migration, any vcpu that got kicked but did not become runnable
    (still in halted state) should be runnable after migration.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Suzuki Poulose <suzuki@in.ibm.com>
    [Raghu: Apic related changes, folding pvunhalted into vcpu_runnable
     Added flags for future use (suggested by Gleb)]
    [ Raghu: fold pv_unhalt flag as suggested by Eric Northup]
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Acked-by: Gleb Natapov <gleb@redhat.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c0efd16bdfa1..c76ff74a98f2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -516,6 +516,11 @@ struct kvm_vcpu_arch {
 
 	/* set at EPT violation at this point */
 	unsigned long exit_qualification;
+
+	/* pv related host specific info */
+	struct {
+		bool pv_unhalted;
+	} pv;
 };
 
 struct kvm_lpage_info {

commit 25d92081ae2ff9858fa733621ef8e91d30fec9d0
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Tue Aug 6 12:00:32 2013 +0300

    nEPT: Add nEPT violation/misconfigration support
    
    Inject nEPT fault to L1 guest. This patch is original from Xinhao.
    
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Jun Nakajima <jun.nakajima@intel.com>
    Signed-off-by: Xinhao Xu <xinhao.xu@intel.com>
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f5df0a84e51c..c0efd16bdfa1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -286,6 +286,7 @@ struct kvm_mmu {
 	u64 *pae_root;
 	u64 *lm_root;
 	u64 rsvd_bits_mask[2][4];
+	u64 bad_mt_xwr;
 
 	/*
 	 * Bitmap: bit set = last pte in walk
@@ -512,6 +513,9 @@ struct kvm_vcpu_arch {
 	 * instruction.
 	 */
 	bool write_fault_to_shadow_pgtable;
+
+	/* set at EPT violation at this point */
+	unsigned long exit_qualification;
 };
 
 struct kvm_lpage_info {

commit ac0a48c39af31fe27bdb1afca7b26f109ff1c704
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jun 25 18:24:41 2013 +0200

    KVM: x86: rename EMULATE_DO_MMIO
    
    The next patch will reuse it for other userspace exits than MMIO,
    namely debug events.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 531f47cbf1f8..f5df0a84e51c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -803,8 +803,8 @@ extern u32  kvm_min_guest_tsc_khz;
 extern u32  kvm_max_guest_tsc_khz;
 
 enum emulation_result {
-	EMULATE_DONE,       /* no further processing */
-	EMULATE_DO_MMIO,      /* kvm_run filled with mmio request */
+	EMULATE_DONE,         /* no further processing */
+	EMULATE_USER_EXIT,    /* kvm_run ready for userspace exit */
 	EMULATE_FAIL,         /* can't emulate this instruction */
 };
 

commit 103af0a98788592b76ee69a13948b6b3036d7e18
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Jul 18 15:57:02 2013 -0700

    perf, kvm: Support the in_tx/in_tx_cp modifiers in KVM arch perfmon emulation v5
    
    [KVM maintainers:
    The underlying support for this is in perf/core now. So please merge
    this patch into the KVM tree.]
    
    This is not arch perfmon, but older CPUs will just ignore it. This makes
    it possible to do at least some TSX measurements from a KVM guest
    
    v2: Various fixes to address review feedback
    v3: Ignore the bits when no CPUID. No #GP. Force raw events with TSX bits.
    v4: Use reserved bits for #GP
    v5: Remove obsolete argument
    Acked-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f87f7fcefa0a..531f47cbf1f8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -323,6 +323,7 @@ struct kvm_pmu {
 	u64 global_ovf_ctrl;
 	u64 counter_bitmask[2];
 	u64 global_ctrl_mask;
+	u64 reserved_bits;
 	u8 version;
 	struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
 	struct kvm_pmc fixed_counters[INTEL_PMC_MAX_FIXED];

commit fe489bf4505ae26d3c6d6a1f1d3064c2a9c5cd85
Merge: 3e34131a6512 a3ff5fbc94a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 13:21:40 2013 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM fixes from Paolo Bonzini:
     "On the x86 side, there are some optimizations and documentation
      updates.  The big ARM/KVM change for 3.11, support for AArch64, will
      come through Catalin Marinas's tree.  s390 and PPC have misc cleanups
      and bugfixes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (87 commits)
      KVM: PPC: Ignore PIR writes
      KVM: PPC: Book3S PR: Invalidate SLB entries properly
      KVM: PPC: Book3S PR: Allow guest to use 1TB segments
      KVM: PPC: Book3S PR: Don't keep scanning HPTEG after we find a match
      KVM: PPC: Book3S PR: Fix invalidation of SLB entry 0 on guest entry
      KVM: PPC: Book3S PR: Fix proto-VSID calculations
      KVM: PPC: Guard doorbell exception with CONFIG_PPC_DOORBELL
      KVM: Fix RTC interrupt coalescing tracking
      kvm: Add a tracepoint write_tsc_offset
      KVM: MMU: Inform users of mmio generation wraparound
      KVM: MMU: document fast invalidate all mmio sptes
      KVM: MMU: document fast invalidate all pages
      KVM: MMU: document fast page fault
      KVM: MMU: document mmio page fault
      KVM: MMU: document write_flooding_count
      KVM: MMU: document clear_spte_count
      KVM: MMU: drop kvm_mmu_zap_mmio_sptes
      KVM: MMU: init kvm generation close to mmio wrap-around value
      KVM: MMU: add tracepoint for check_mmio_spte
      KVM: MMU: fast invalidate all mmio sptes
      ...

commit f6f8adeef542a18b1cb26a0b772c9781a10bb477
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Wed Jun 19 17:09:24 2013 +0800

    KVM: MMU: document fast invalidate all pages
    
    Document it to Documentation/virtual/kvm/mmu.txt
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6b636fd8582f..280e3271b027 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -222,7 +222,10 @@ struct kvm_mmu_page {
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	unsigned long parent_ptes;	/* Reverse mapping for parent_pte */
+
+	/* The page is obsolete if mmu_valid_gen != kvm->arch.mmu_valid_gen.  */
 	unsigned long mmu_valid_gen;
+
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32

commit 0cbf8e437b60b8b12d97589509d3e5a581731d36
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Wed Jun 19 17:09:21 2013 +0800

    KVM: MMU: document write_flooding_count
    
    Document write_flooding_count to Documentation/virtual/kvm/mmu.txt
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5d28c11d5e21..6b636fd8582f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -233,6 +233,7 @@ struct kvm_mmu_page {
 	int clear_spte_count;
 #endif
 
+	/* Number of writes since the last time traversal visited this page.  */
 	int write_flooding_count;
 };
 

commit accaefe07ddbeb12c0de4cec1d62dba6a0ea1605
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Wed Jun 19 17:09:20 2013 +0800

    KVM: MMU: document clear_spte_count
    
    Document it to Documentation/virtual/kvm/mmu.txt
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 966f2650b6ab..5d28c11d5e21 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -226,6 +226,10 @@ struct kvm_mmu_page {
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32
+	/*
+	 * Used out of the mmu-lock to avoid reading spte values while an
+	 * update is in progress; see the comments in __get_spte_lockless().
+	 */
 	int clear_spte_count;
 #endif
 

commit a8eca9dcc656a405a28ffba43f3d86a1ff0eb331
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Mon Jun 10 16:28:55 2013 +0800

    KVM: MMU: drop kvm_mmu_zap_mmio_sptes
    
    Drop kvm_mmu_zap_mmio_sptes and use kvm_mmu_invalidate_zap_all_pages
    instead to handle mmio generation number overflow
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 90d05edbbfe2..966f2650b6ab 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -230,7 +230,6 @@ struct kvm_mmu_page {
 #endif
 
 	int write_flooding_count;
-	bool mmio_cached;
 };
 
 struct kvm_pio_request {

commit f8f559422b6c6a05469dfde614b67789b6142cb5
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Jun 7 16:51:26 2013 +0800

    KVM: MMU: fast invalidate all mmio sptes
    
    This patch tries to introduce a very simple and scale way to invalidate
    all mmio sptes - it need not walk any shadow pages and hold mmu-lock
    
    KVM maintains a global mmio valid generation-number which is stored in
    kvm->memslots.generation and every mmio spte stores the current global
    generation-number into his available bits when it is created
    
    When KVM need zap all mmio sptes, it just simply increase the global
    generation-number. When guests do mmio access, KVM intercepts a MMIO #PF
    then it walks the shadow page table and get the mmio spte. If the
    generation-number on the spte does not equal the global generation-number,
    it will go to the normal #PF handler to update the mmio spte
    
    Since 19 bits are used to store generation-number on mmio spte, we zap all
    mmio sptes when the number is round
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1f98c1bb5b7a..90d05edbbfe2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -773,7 +773,7 @@ void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 				     struct kvm_memory_slot *slot,
 				     gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
-void kvm_mmu_zap_mmio_sptes(struct kvm *kvm);
+void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 

commit afcbf13fa6d53d8a97eafaca1dcb344331d2ce0c
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Sat Apr 27 16:37:47 2013 -0700

    x86: Rename X86_CR4_RDWRGSFS to X86_CR4_FSGSBASE
    
    Rename X86_CR4_RDWRGSFS to X86_CR4_FSGSBASE to match the SDM.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Link: http://lkml.kernel.org/n/tip-buq1evi5dpykxx7ak6amaam0@git.kernel.org

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3741c653767c..af9c5525434d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -59,7 +59,7 @@
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
 			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR | X86_CR4_PCIDE \
-			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_RDWRGSFS \
+			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_FSGSBASE \
 			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE))
 
 #define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)

commit 365c886860c4ba670d245e762b23987c912c129a
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri May 31 08:36:29 2013 +0800

    KVM: MMU: reclaim the zapped-obsolete page first
    
    As Marcelo pointed out that
    | "(retention of large number of pages while zapping)
    | can be fatal, it can lead to OOM and host crash"
    
    We introduce a list, kvm->arch.zapped_obsolete_pages, to link all
    the pages which are deleted from the mmu cache but not actually
    freed. When page reclaiming is needed, we always zap this kind of
    pages first.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bff7d464a6ae..1f98c1bb5b7a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -536,6 +536,8 @@ struct kvm_arch {
 	 * Hash table of struct kvm_mmu_page.
 	 */
 	struct list_head active_mmu_pages;
+	struct list_head zapped_obsolete_pages;
+
 	struct list_head assigned_dev_head;
 	struct iommu_domain *iommu_domain;
 	int iommu_flags;

commit 5304b8d37c2a5ebca48330f5e7868d240eafbed1
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri May 31 08:36:22 2013 +0800

    KVM: MMU: fast invalidate all pages
    
    The current kvm_mmu_zap_all is really slow - it is holding mmu-lock to
    walk and zap all shadow pages one by one, also it need to zap all guest
    page's rmap and all shadow page's parent spte list. Particularly, things
    become worse if guest uses more memory or vcpus. It is not good for
    scalability
    
    In this patch, we introduce a faster way to invalidate all shadow pages.
    KVM maintains a global mmu invalid generation-number which is stored in
    kvm->arch.mmu_valid_gen and every shadow page stores the current global
    generation-number into sp->mmu_valid_gen when it is created
    
    When KVM need zap all shadow pages sptes, it just simply increase the
    global generation-number then reload root shadow pages on all vcpus.
    Vcpu will create a new shadow page table according to current kvm's
    generation-number. It ensures the old pages are not used any more.
    Then the obsolete pages (sp->mmu_valid_gen != kvm->arch.mmu_valid_gen)
    are zapped by using lock-break technique
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3741c653767c..bff7d464a6ae 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -222,6 +222,7 @@ struct kvm_mmu_page {
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	unsigned long parent_ptes;	/* Reverse mapping for parent_pte */
+	unsigned long mmu_valid_gen;
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32
@@ -529,6 +530,7 @@ struct kvm_arch {
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
+	unsigned long mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.

commit 03b28f8133165dbe4cd922054d599e26b8119508
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Apr 29 16:46:42 2013 +0200

    KVM: x86: Account for failing enable_irq_window for NMI window request
    
    With VMX, enable_irq_window can now return -EBUSY, in which case an
    immediate exit shall be requested before entering the guest. Account for
    this also in enable_nmi_window which uses enable_irq_window in absence
    of vnmi support, e.g.
    
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ec14b7245a4e..3741c653767c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -695,7 +695,7 @@ struct kvm_x86_ops {
 	int (*nmi_allowed)(struct kvm_vcpu *vcpu);
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
 	void (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);
-	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
+	int (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	int (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
 	int (*vm_has_apicv)(struct kvm *kvm);

commit cbf64358588ae45dcf0207dbc97fba783577d64a
Author: Chegu Vinod <chegu_vinod@hp.com>
Date:   Sat Apr 27 18:31:04 2013 -0700

    KVM: x86: Increase the "hard" max VCPU limit
    
    KVM guests today use 8bit APIC ids allowing for 256 ID's. Reserving one
    ID for Broadcast interrupts should leave 255 ID's. In case of KVM there
    is no need for reserving another ID for IO-APIC so the hard max limit for
    VCPUS can be increased from 254 to 255. (This was confirmed by Gleb Natapov
    http://article.gmane.org/gmane.comp.emulators.kvm.devel/99713  )
    
    Signed-off-by: Chegu Vinod <chegu_vinod@hp.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 38f2cc0a5a23..ec14b7245a4e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -31,7 +31,7 @@
 #include <asm/msr-index.h>
 #include <asm/asm.h>
 
-#define KVM_MAX_VCPUS 254
+#define KVM_MAX_VCPUS 255
 #define KVM_SOFT_MAX_VCPUS 160
 #define KVM_USER_MEM_SLOTS 125
 /* memory slots that are not exposed to userspace */

commit 064d1afaa5a60fc391d0b4b77599fc8f63f99cd3
Merge: 730dca42c1d3 8b78645c93b5
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Apr 28 12:50:07 2013 +0300

    Merge git://github.com/agraf/linux-2.6.git kvm-ppc-next into queue

commit 730dca42c1d363c939da18c1499c7327c66e2b37
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Sun Apr 28 10:50:52 2013 +0200

    KVM: x86: Rework request for immediate exit
    
    The VMX implementation of enable_irq_window raised
    KVM_REQ_IMMEDIATE_EXIT after we checked it in vcpu_enter_guest. This
    caused infinite loops on vmentry. Fix it by letting enable_irq_window
    signal the need for an immediate exit via its return value and drop
    KVM_REQ_IMMEDIATE_EXIT.
    
    This issue only affects nested VMX scenarios.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 18635ae42a8e..111b4a0c3907 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -694,7 +694,7 @@ struct kvm_x86_ops {
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
 	void (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
-	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
+	int (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
 	int (*vm_has_apicv)(struct kvm *kvm);
 	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);

commit 8175e5b79c38a1d85225da516fa1a0ecbf2fdbca
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Apr 15 10:42:33 2013 +0200

    KVM: Add KVM_IRQCHIP_NUM_PINS in addition to KVM_IOAPIC_NUM_PINS
    
    The concept of routing interrupt lines to an irqchip is nothing
    that is IOAPIC specific. Every irqchip has a maximum number of pins
    that can be linked to irq lines.
    
    So let's add a new define that allows us to reuse generic code for
    non-IOAPIC platforms.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 18635ae42a8e..14337fa464bc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -43,6 +43,8 @@
 #define KVM_PIO_PAGE_OFFSET 1
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 2
 
+#define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS
+
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
 			  | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM \

commit 384bb783275145b70d769acf4c687957d1c61802
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Sat Apr 20 10:52:36 2013 +0200

    KVM: nVMX: Validate EFER values for VM_ENTRY/EXIT_LOAD_IA32_EFER
    
    As we may emulate the loading of EFER on VM-entry and VM-exit, implement
    the checks that VMX performs on the guest and host values on vmlaunch/
    vmresume. Factor out kvm_valid_efer for this purpose which checks for
    set reserved bits.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 599f98b612d4..18635ae42a8e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -809,6 +809,7 @@ static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 }
 
 void kvm_enable_efer_bits(u64);
+bool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);
 int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
 

commit a20ed54d6e470bf0d28921b7aadb6ca0da0ff0c3
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Thu Apr 11 19:25:15 2013 +0800

    KVM: VMX: Add the deliver posted interrupt algorithm
    
    Only deliver the posted interrupt when target vcpu is running
    and there is no previous interrupt pending in pir.
    
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 68d438630dd3..599f98b612d4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -701,6 +701,8 @@ struct kvm_x86_ops {
 	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
+	void (*deliver_posted_interrupt)(struct kvm_vcpu *vcpu, int vector);
+	void (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);

commit a547c6db4d2f16ba5ce8e7054bffad6acc248d40
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Thu Apr 11 19:25:10 2013 +0800

    KVM: VMX: Enable acknowledge interupt on vmexit
    
    The "acknowledge interrupt on exit" feature controls processor behavior
    for external interrupt acknowledgement. When this control is set, the
    processor acknowledges the interrupt controller to acquire the
    interrupt vector on VM exit.
    
    After enabling this feature, an interrupt which arrived when target cpu is
    running in vmx non-root mode will be handled by vmx handler instead of handler
    in idt. Currently, vmx handler only fakes an interrupt stack and jump to idt
    table to let real handler to handle it. Further, we will recognize the interrupt
    and only delivery the interrupt which not belong to current vcpu through idt table.
    The interrupt which belonged to current vcpu will be handled inside vmx handler.
    This will reduce the interrupt handle cost of KVM.
    
    Also, interrupt enable logic is changed if this feature is turnning on:
    Before this patch, hypervior call local_irq_enable() to enable it directly.
    Now IF bit is set on interrupt stack frame, and will be enabled on a return from
    interrupt handler if exterrupt interrupt exists. If no external interrupt, still
    call local_irq_enable() to enable it.
    
    Refer to Intel SDM volum 3, chapter 33.2.
    
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 82f1dc67782f..68d438630dd3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -727,6 +727,7 @@ struct kvm_x86_ops {
 	int (*check_intercept)(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage);
+	void (*handle_external_intr)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {

commit 991eebf9f8e523e7ff1e4d31ac80641582b2e57a
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Apr 11 12:10:51 2013 +0300

    KVM: VMX: do not try to reexecute failed instruction while emulating invalid guest state
    
    During invalid guest state emulation vcpu cannot enter guest mode to try
    to reexecute instruction that emulator failed to emulate, so emulation
    will happen again and again.  Prevent that by telling the emulator that
    instruction reexecution should not be attempted.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b2c7263c93b6..82f1dc67782f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -795,6 +795,7 @@ enum emulation_result {
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_RETRY		    (1 << 3)
+#define EMULTYPE_NO_REEXECUTE	    (1 << 4)
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 			    int emulation_type, void *insn, int insn_len);
 

commit 8b415dcd762607379cf0a69c9dd25940da1d174e
Author: Geoff Levand <geoff@infradead.org>
Date:   Fri Apr 5 19:20:30 2013 +0000

    KVM: Move kvm_rebooting declaration out of x86
    
    The variable kvm_rebooting is a common kvm variable, so move its
    declaration from arch/x86/include/asm/kvm_host.h to
    include/asm/kvm_host.h.
    
    Fixes this sparse warning when building on arm64:
    
      virt/kvm/kvm_main.c:warning: symbol 'kvm_rebooting' was not declared. Should it be static?
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 628163c0c6e4..b2c7263c93b6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -972,7 +972,6 @@ enum {
  * Trap the fault and ignore the instruction if that happens.
  */
 asmlinkage void kvm_spurious_fault(void);
-extern bool kvm_rebooting;
 
 #define ____kvm_handle_fault_on_reboot(insn, cleanup_insn)	\
 	"666: " insn "\n\t" \

commit fc1b74925f87f6aca5432eb73f6a57eff30afde7
Author: Geoff Levand <geoff@infradead.org>
Date:   Fri Apr 5 19:20:30 2013 +0000

    KVM: Move vm_list kvm_lock declarations out of x86
    
    The variables vm_list and kvm_lock are common to all architectures, so
    move the declarations from arch/x86/include/asm/kvm_host.h to
    include/linux/kvm_host.h.
    
    Fixes sparse warnings like these when building for arm64:
    
      virt/kvm/kvm_main.c: warning: symbol 'kvm_lock' was not declared. Should it be static?
      virt/kvm/kvm_main.c: warning: symbol 'vm_list' was not declared. Should it be static?
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3dd84c996d56..628163c0c6e4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -94,9 +94,6 @@
 
 #define ASYNC_PF_PER_VCPU 64
 
-extern raw_spinlock_t kvm_lock;
-extern struct list_head vm_list;
-
 struct kvm_vcpu;
 struct kvm;
 struct kvm_async_pf;

commit afd80d85aefac27e6e2f9dc10f60515357c504d2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 28 17:18:35 2013 +0100

    pmu: prepare for migration support
    
    In order to migrate the PMU state correctly, we need to restore the
    values of MSR_CORE_PERF_GLOBAL_STATUS (a read-only register) and
    MSR_CORE_PERF_GLOBAL_OVF_CTRL (which has side effects when written).
    We also need to write the full 40-bit value of the performance counter,
    which would only be possible with a v3 architectural PMU's full-width
    counter MSRs.
    
    To distinguish host-initiated writes from the guest's, pass the
    full struct msr_data to kvm_pmu_set_msr.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b5a64621d5af..3dd84c996d56 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1030,7 +1030,7 @@ void kvm_pmu_reset(struct kvm_vcpu *vcpu);
 void kvm_pmu_cpuid_update(struct kvm_vcpu *vcpu);
 bool kvm_pmu_msr(struct kvm_vcpu *vcpu, u32 msr);
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data);
-int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info);
 int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
 void kvm_handle_pmu_event(struct kvm_vcpu *vcpu);
 void kvm_deliver_pmi(struct kvm_vcpu *vcpu);

commit 2ae33b389601b86a3d0cfe2d09f5e3189d5322fd
Merge: 04b66839d312 2ffdd7e23cde
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Mar 21 11:11:52 2013 -0300

    Merge remote-tracking branch 'upstream/master' into queue
    
    Merge reason:
    
    From: Alexander Graf <agraf@suse.de>
    
    "Just recently this really important patch got pulled into Linus' tree for 3.9:
    
    commit 1674400aaee5b466c595a8fc310488263ce888c7
    Author: Anton Blanchard <anton <at> samba.org>
    Date:   Tue Mar 12 01:51:51 2013 +0000
    
    Without that commit, I can not boot my G5, thus I can't run automated tests on it against my queue.
    
    Could you please merge kvm/next against linus/master, so that I can base my trees against that?"
    
    * upstream/master: (653 commits)
      PCI: Use ROM images from firmware only if no other ROM source available
      sparc: remove unused "config BITS"
      sparc: delete "if !ULTRA_HAS_POPULATION_COUNT"
      KVM: Fix bounds checking in ioapic indirect register reads (CVE-2013-1798)
      KVM: x86: Convert MSR_KVM_SYSTEM_TIME to use gfn_to_hva_cache functions (CVE-2013-1797)
      KVM: x86: fix for buffer overflow in handling of MSR_KVM_SYSTEM_TIME (CVE-2013-1796)
      arm64: Kconfig.debug: Remove unused CONFIG_DEBUG_ERRORS
      arm64: Do not select GENERIC_HARDIRQS_NO_DEPRECATED
      inet: limit length of fragment queue hash table bucket lists
      qeth: Fix scatter-gather regression
      qeth: Fix invalid router settings handling
      qeth: delay feature trace
      sgy-cts1000: Remove __dev* attributes
      KVM: x86: fix deadlock in clock-in-progress request handling
      KVM: allow host header to be included even for !CONFIG_KVM
      hwmon: (lm75) Fix tcn75 prefix
      hwmon: (lm75.h) Update header inclusion
      MAINTAINERS: Remove Mark M. Hoffman
      xfs: ensure we capture IO errors correctly
      xfs: fix xfs_iomap_eof_prealloc_initial_size type
      ...
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

commit 0b79459b482e85cb7426aa7da683a9f2c97aeae1
Author: Andy Honig <ahonig@google.com>
Date:   Wed Feb 20 14:48:10 2013 -0800

    KVM: x86: Convert MSR_KVM_SYSTEM_TIME to use gfn_to_hva_cache functions (CVE-2013-1797)
    
    There is a potential use after free issue with the handling of
    MSR_KVM_SYSTEM_TIME.  If the guest specifies a GPA in a movable or removable
    memory such as frame buffers then KVM might continue to write to that
    address even after it's removed via KVM_SET_USER_MEMORY_REGION.  KVM pins
    the page in memory so it's unlikely to cause an issue, but if the user
    space component re-purposes the memory previously used for the guest, then
    the guest will be able to corrupt that memory.
    
    Tested: Tested against kvmclock unit test
    
    Signed-off-by: Andrew Honig <ahonig@google.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 635a74d22409..4979778cc7fb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -414,8 +414,8 @@ struct kvm_vcpu_arch {
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
 	unsigned int hw_tsc_khz;
-	unsigned int time_offset;
-	struct page *time_page;
+	struct gfn_to_hva_cache pv_time;
+	bool pv_time_enabled;
 	/* set guest stopped flag in pvclock flags field */
 	bool pvclock_set_guest_stopped_request;
 

commit 982b3394dd23eec6e5a2f7871238435a167b63cc
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Tue Mar 12 17:45:30 2013 +0900

    KVM: x86: Optimize mmio spte zapping when creating/moving memslot
    
    When we create or move a memory slot, we need to zap mmio sptes.
    Currently, zap_all() is used for this and this is causing two problems:
     - extra page faults after zapping mmu pages
     - long mmu_lock hold time during zapping mmu pages
    
    For the latter, Marcelo reported a disastrous mmu_lock hold time during
    hot-plug, which made the guest unresponsive for a long time.
    
    This patch takes a simple way to fix these problems: do not zap mmu
    pages unless they are marked mmio cached.  On our test box, this took
    only 50us for the 4GB guest and we did not see ms of mmu_lock hold time
    any more.
    
    Note that we still need to do zap_all() for other cases.  So another
    work is also needed: Xiao's work may be the one.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9b75cae83d10..3f205c6cde59 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -767,6 +767,7 @@ void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 				     struct kvm_memory_slot *slot,
 				     gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
+void kvm_mmu_zap_mmio_sptes(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 

commit 95b0430d1a53541076ffbaf453f8b49a547cceba
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Tue Mar 12 17:44:40 2013 +0900

    KVM: MMU: Mark sp mmio cached when creating mmio spte
    
    This will be used not to zap unrelated mmu pages when creating/moving
    a memory slot later.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ef7f4a5cf8c7..9b75cae83d10 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -230,6 +230,7 @@ struct kvm_mmu_page {
 #endif
 
 	int write_flooding_count;
+	bool mmio_cached;
 };
 
 struct kvm_pio_request {

commit 66450a21f99636af4fafac2afd33f1a40631bc3a
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Wed Mar 13 12:42:34 2013 +0100

    KVM: x86: Rework INIT and SIPI handling
    
    A VCPU sending INIT or SIPI to some other VCPU races for setting the
    remote VCPU's mp_state. When we were unlucky, KVM_MP_STATE_INIT_RECEIVED
    was overwritten by kvm_emulate_halt and, thus, got lost.
    
    This introduces APIC events for those two signals, keeping them in
    kvm_apic until kvm_apic_accept_events is run over the target vcpu
    context. kvm_apic_has_events reports to kvm_arch_vcpu_runnable if there
    are pending events, thus if vcpu blocking should end.
    
    The patch comes with the side effect of effectively obsoleting
    KVM_MP_STATE_SIPI_RECEIVED. We still accept it from user space, but
    immediately translate it to KVM_MP_STATE_INIT_RECEIVED + KVM_APIC_SIPI.
    The vcpu itself will no longer enter the KVM_MP_STATE_SIPI_RECEIVED
    state. That also means we no longer exit to user space after receiving a
    SIPI event.
    
    Furthermore, we already reset the VCPU on INIT, only fixing up the code
    segment later on when SIPI arrives. Moreover, we fix INIT handling for
    the BSP: it never enter wait-for-SIPI but directly starts over on INIT.
    
    Tested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 348d85965ead..ef7f4a5cf8c7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -345,7 +345,6 @@ struct kvm_vcpu_arch {
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;
 	int mp_state;
-	int sipi_vector;
 	u64 ia32_misc_enable_msr;
 	bool tpr_access_reporting;
 
@@ -819,6 +818,7 @@ int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);
+void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, unsigned int vector);
 
 int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,
 		    int reason, bool has_error_code, u32 error_code);
@@ -1002,6 +1002,7 @@ int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
+void kvm_vcpu_reset(struct kvm_vcpu *vcpu);
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
 void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);

commit 57f252f22908535e04d520f3833a6e3116eb159d
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Mar 12 10:20:24 2013 +0100

    KVM: x86: Drop unused return code from VCPU reset callback
    
    Neither vmx nor svm nor the common part may generate an error on
    kvm_vcpu_reset. So drop the return code.
    
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 635a74d22409..348d85965ead 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -643,7 +643,7 @@ struct kvm_x86_ops {
 	/* Create, but do not attach this VCPU */
 	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
-	int (*vcpu_reset)(struct kvm_vcpu *vcpu);
+	void (*vcpu_reset)(struct kvm_vcpu *vcpu);
 
 	void (*prepare_guest_switch)(struct kvm_vcpu *vcpu);
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);

commit c7c9c56ca26f7b9458711b2d78b60b60e0d38ba7
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Fri Jan 25 10:18:51 2013 +0800

    x86, apicv: add virtual interrupt delivery support
    
    Virtual interrupt delivery avoids KVM to inject vAPIC interrupts
    manually, which is fully taken care of by the hardware. This needs
    some special awareness into existing interrupr injection path:
    
    - for pending interrupt, instead of direct injection, we may need
      update architecture specific indicators before resuming to guest.
    
    - A pending interrupt, which is masked by ISR, should be also
      considered in above update action, since hardware will decide
      when to inject it at right time. Current has_interrupt and
      get_interrupt only returns a valid vector from injection p.o.v.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d42c2839be98..635a74d22409 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -699,6 +699,10 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
+	int (*vm_has_apicv)(struct kvm *kvm);
+	void (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);
+	void (*hwapic_isr_update)(struct kvm *kvm, int isr);
+	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
@@ -994,6 +998,7 @@ int kvm_age_hva(struct kvm *kvm, unsigned long hva);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);
+int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);

commit 8d14695f9542e9e0195d6e41ddaa52c32322adf5
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Fri Jan 25 10:18:50 2013 +0800

    x86, apicv: add virtual x2apic support
    
    basically to benefit from apicv, we need to enable virtualized x2apic mode.
    Currently, we only enable it when guest is really using x2apic.
    
    Also, clear MSR bitmap for corresponding x2apic MSRs when guest enabled x2apic:
    0x800 - 0x8ff: no read intercept for apicv register virtualization,
                   except APIC ID and TMCCT which need software's assistance to
                   get right value.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 77d56a4ba89c..d42c2839be98 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -699,6 +699,7 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
+	void (*set_virtual_x2apic_mode)(struct kvm_vcpu *vcpu, bool set);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);

commit 93c05d3ef25275829d421a255271595ac219a518
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Sun Jan 13 23:49:07 2013 +0800

    KVM: x86: improve reexecute_instruction
    
    The current reexecute_instruction can not well detect the failed instruction
    emulation. It allows guest to retry all the instructions except it accesses
    on error pfn
    
    For example, some cases are nested-write-protect - if the page we want to
    write is used as PDE but it chains to itself. Under this case, we should
    stop the emulation and report the case to userspace
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f75e1feb6ec5..77d56a4ba89c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -497,6 +497,13 @@ struct kvm_vcpu_arch {
 		u64 msr_val;
 		struct gfn_to_hva_cache data;
 	} pv_eoi;
+
+	/*
+	 * Indicate whether the access faults on its page table in guest
+	 * which is set when fix page fault and used to detect unhandeable
+	 * instruction.
+	 */
+	bool write_fault_to_shadow_pgtable;
 };
 
 struct kvm_lpage_info {

commit e12091ce7bdd3c82fa392a868d1bdccecee655d5
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Tue Jan 8 19:45:28 2013 +0900

    KVM: Remove unused slot_bitmap from kvm_mmu_page
    
    Not needed any more.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c431b33271f3..f75e1feb6ec5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -219,11 +219,6 @@ struct kvm_mmu_page {
 	u64 *spt;
 	/* hold the gfn of each spte inside spt */
 	gfn_t *gfns;
-	/*
-	 * One bit set per slot which has memory
-	 * in this shadow page.
-	 */
-	DECLARE_BITMAP(slot_bitmap, KVM_MEM_SLOTS_NUM);
 	bool unsync;
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;

commit 0f888f5acd0cd806d4fd9f4067276b3855a13309
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:38 2012 -0700

    KVM: Increase user memory slots on x86 to 125
    
    With the 3 private slots, this gives us a nice round 128 slots total.
    The primary motivation for this is to support more assigned devices.
    Each assigned device can theoretically use up to 8 slots (6 MMIO BARs,
    1 ROM BAR, 1 spare for a split MSI-X table mapping) though it's far
    more typical for a device to use 3-4 slots.  If we assume a typical VM
    uses a dozen slots for non-assigned devices purposes, we should always
    be able to support 14 worst case assigned devices or 28 to 37 typical
    devices.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 51d52108f109..c431b33271f3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -33,7 +33,7 @@
 
 #define KVM_MAX_VCPUS 254
 #define KVM_SOFT_MAX_VCPUS 160
-#define KVM_USER_MEM_SLOTS 32
+#define KVM_USER_MEM_SLOTS 125
 /* memory slots that are not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 3
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)

commit 0743247fbf0c4a27185b2aa1fdda91d0745dfed1
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:15 2012 -0700

    KVM: Make KVM_PRIVATE_MEM_SLOTS optional
    
    Seems like everyone copied x86 and defined 4 private memory slots
    that never actually get used.  Even x86 only uses 3 of the 4.  These
    aren't exposed so there's no need to add padding.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c7df6ffd2437..51d52108f109 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -34,8 +34,8 @@
 #define KVM_MAX_VCPUS 254
 #define KVM_SOFT_MAX_VCPUS 160
 #define KVM_USER_MEM_SLOTS 32
-/* memory slots that does not exposed to userspace */
-#define KVM_PRIVATE_MEM_SLOTS 4
+/* memory slots that are not exposed to userspace */
+#define KVM_PRIVATE_MEM_SLOTS 3
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 
 #define KVM_MMIO_SIZE 16

commit bbacc0c111c3c5d1f3192b8cc1642b9c3954f80d
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:09 2012 -0700

    KVM: Rename KVM_MEMORY_SLOTS -> KVM_USER_MEM_SLOTS
    
    It's easy to confuse KVM_MEMORY_SLOTS and KVM_MEM_SLOTS_NUM.  One is
    the user accessible slots and the other is user + private.  Make this
    more obvious.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dc87b65e9c3a..c7df6ffd2437 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -33,10 +33,10 @@
 
 #define KVM_MAX_VCPUS 254
 #define KVM_SOFT_MAX_VCPUS 160
-#define KVM_MEMORY_SLOTS 32
+#define KVM_USER_MEM_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
-#define KVM_MEM_SLOTS_NUM (KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
+#define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 
 #define KVM_MMIO_SIZE 16
 

commit ba904635d498fea43fc3610983f9dc430ac324e4
Author: Will Auld <will.auld.intel@gmail.com>
Date:   Thu Nov 29 12:42:50 2012 -0800

    KVM: x86: Emulate IA32_TSC_ADJUST MSR
    
    CPUID.7.0.EBX[1]=1 indicates IA32_TSC_ADJUST MSR 0x3b is supported
    
    Basic design is to emulate the MSR by allowing reads and writes to a guest
    vcpu specific location to store the value of the emulated MSR while adding
    the value to the vmcs tsc_offset. In this way the IA32_TSC_ADJUST value will
    be included in all reads to the TSC MSR whether through rdmsr or rdtsc. This
    is of course as long as the "use TSC counter offsetting" VM-execution control
    is enabled as well as the IA32_TSC_ADJUST control.
    
    However, because hardware will only return the TSC + IA32_TSC_ADJUST +
    vmsc tsc_offset for a guest process when it does and rdtsc (with the correct
    settings) the value of our virtualized IA32_TSC_ADJUST must be stored in one
    of these three locations. The argument against storing it in the actual MSR
    is performance. This is likely to be seldom used while the save/restore is
    required on every transition. IA32_TSC_ADJUST was created as a way to solve
    some issues with writing TSC itself so that is not an option either.
    
    The remaining option, defined above as our solution has the problem of
    returning incorrect vmcs tsc_offset values (unless we intercept and fix, not
    done here) as mentioned above. However, more problematic is that storing the
    data in vmcs tsc_offset will have a different semantic effect on the system
    than does using the actual MSR. This is illustrated in the following example:
    
    The hypervisor set the IA32_TSC_ADJUST, then the guest sets it and a guest
    process performs a rdtsc. In this case the guest process will get
    TSC + IA32_TSC_ADJUST_hyperviser + vmsc tsc_offset including
    IA32_TSC_ADJUST_guest. While the total system semantics changed the semantics
    as seen by the guest do not and hence this will not cause a problem.
    
    Signed-off-by: Will Auld <will.auld@intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 56c5dca9d78d..dc87b65e9c3a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -444,6 +444,7 @@ struct kvm_vcpu_arch {
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
 	u32 virtual_tsc_khz;
+	s64 ia32_tsc_adjust_msr;
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
 	unsigned nmi_pending; /* NMI queued after currently running handler */
@@ -711,6 +712,7 @@ struct kvm_x86_ops {
 	bool (*has_wbinvd_exit)(void);
 
 	void (*set_tsc_khz)(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale);
+	u64 (*read_tsc_offset)(struct kvm_vcpu *vcpu);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	u64 (*compute_tsc_offset)(struct kvm_vcpu *vcpu, u64 target_tsc);

commit 8fe8ab46be06fcd9abfe6fe9928fd95b54ab079a
Author: Will Auld <will.auld.intel@gmail.com>
Date:   Thu Nov 29 12:42:12 2012 -0800

    KVM: x86: Add code to track call origin for msr assignment
    
    In order to track who initiated the call (host or guest) to modify an msr
    value I have changed function call parameters along the call path. The
    specific change is to add a struct pointer parameter that points to (index,
    data, caller) information rather than having this information passed as
    individual parameters.
    
    The initial use for this capability is for updating the IA32_TSC_ADJUST msr
    while setting the tsc value. It is anticipated that this capability is
    useful for other tasks.
    
    Signed-off-by: Will Auld <will.auld@intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9fb6d8da7a43..56c5dca9d78d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -620,6 +620,12 @@ struct kvm_vcpu_stat {
 
 struct x86_instruction_info;
 
+struct msr_data {
+	bool host_initiated;
+	u32 index;
+	u64 data;
+};
+
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
@@ -642,7 +648,7 @@ struct kvm_x86_ops {
 
 	void (*update_db_bp_intercept)(struct kvm_vcpu *vcpu);
 	int (*get_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata);
-	int (*set_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
+	int (*set_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);
 	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);
 	void (*get_segment)(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg);
@@ -793,7 +799,7 @@ static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 
 void kvm_enable_efer_bits(u64);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);
-int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
+int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr);
 
 struct x86_emulate_ctxt;
 
@@ -820,7 +826,7 @@ void kvm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l);
 int kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr);
 
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
-int kvm_set_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr);
 
 unsigned long kvm_get_rflags(struct kvm_vcpu *vcpu);
 void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);

commit b48aa97e38206a84bf8485e7c553412274708ce5
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:29:03 2012 -0200

    KVM: x86: require matched TSC offsets for master clock
    
    With master clock, a pvclock clock read calculates:
    
    ret = system_timestamp + [ (rdtsc + tsc_offset) - tsc_timestamp ]
    
    Where 'rdtsc' is the host TSC.
    
    system_timestamp and tsc_timestamp are unique, one tuple
    per VM: the "master clock".
    
    Given a host with synchronized TSCs, its obvious that
    guest TSC must be matched for the above to guarantee monotonicity.
    
    Allow master clock usage only if guest TSCs are synchronized.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 32f0e4a063b7..9fb6d8da7a43 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -561,6 +561,7 @@ struct kvm_arch {
 	u64 cur_tsc_write;
 	u64 cur_tsc_offset;
 	u8  cur_tsc_generation;
+	int nr_vcpus_matched_tsc;
 
 	spinlock_t pvclock_gtod_sync_lock;
 	bool use_master_clock;

commit d828199e84447795c6669ff0e6c6d55eb9beeff6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:29:01 2012 -0200

    KVM: x86: implement PVCLOCK_TSC_STABLE_BIT pvclock flag
    
    KVM added a global variable to guarantee monotonicity in the guest.
    One of the reasons for that is that the time between
    
            1. ktime_get_ts(&timespec);
            2. rdtscll(tsc);
    
    Is variable. That is, given a host with stable TSC, suppose that
    two VCPUs read the same time via ktime_get_ts() above.
    
    The time required to execute 2. is not the same on those two instances
    executing in different VCPUS (cache misses, interrupts...).
    
    If the TSC value that is used by the host to interpolate when
    calculating the monotonic time is the same value used to calculate
    the tsc_timestamp value stored in the pvclock data structure, and
    a single <system_timestamp, tsc_timestamp> tuple is visible to all
    vcpus simultaneously, this problem disappears. See comment on top
    of pvclock_update_vm_gtod_copy for details.
    
    Monotonicity is then guaranteed by synchronicity of the host TSCs
    and guest TSCs.
    
    Set TSC stable pvclock flag in that case, allowing the guest to read
    clock from userspace.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d60535adec98..32f0e4a063b7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -22,6 +22,8 @@
 #include <linux/kvm_para.h>
 #include <linux/kvm_types.h>
 #include <linux/perf_event.h>
+#include <linux/pvclock_gtod.h>
+#include <linux/clocksource.h>
 
 #include <asm/pvclock-abi.h>
 #include <asm/desc.h>
@@ -560,6 +562,11 @@ struct kvm_arch {
 	u64 cur_tsc_offset;
 	u8  cur_tsc_generation;
 
+	spinlock_t pvclock_gtod_sync_lock;
+	bool use_master_clock;
+	u64 master_kernel_ns;
+	cycle_t master_cycle_now;
+
 	struct kvm_xen_hvm_config xen_hvm_config;
 
 	/* fields used by HYPER-V emulation */

commit 886b470cb14733a0286e365c77f1844c240c33a4
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:28:58 2012 -0200

    KVM: x86: pass host_tsc to read_l1_tsc
    
    Allow the caller to pass host tsc value to kvm_x86_ops->read_l1_tsc().
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b2e11f452435..d60535adec98 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -700,7 +700,7 @@ struct kvm_x86_ops {
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	u64 (*compute_tsc_offset)(struct kvm_vcpu *vcpu, u64 target_tsc);
-	u64 (*read_l1_tsc)(struct kvm_vcpu *vcpu);
+	u64 (*read_l1_tsc)(struct kvm_vcpu *vcpu, u64 host_tsc);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
 

commit ecefbd94b834fa32559d854646d777c56749ef1c
Merge: ce57e981f2b9 3d11df7abbff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 4 09:30:33 2012 -0700

    Merge tag 'kvm-3.7-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Avi Kivity:
     "Highlights of the changes for this release include support for vfio
      level triggered interrupts, improved big real mode support on older
      Intels, a streamlines guest page table walker, guest APIC speedups,
      PIO optimizations, better overcommit handling, and read-only memory."
    
    * tag 'kvm-3.7-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (138 commits)
      KVM: s390: Fix vcpu_load handling in interrupt code
      KVM: x86: Fix guest debug across vcpu INIT reset
      KVM: Add resampling irqfds for level triggered interrupts
      KVM: optimize apic interrupt delivery
      KVM: MMU: Eliminate pointless temporary 'ac'
      KVM: MMU: Avoid access/dirty update loop if all is well
      KVM: MMU: Eliminate eperm temporary
      KVM: MMU: Optimize is_last_gpte()
      KVM: MMU: Simplify walk_addr_generic() loop
      KVM: MMU: Optimize pte permission checks
      KVM: MMU: Update accessed and dirty bits after guest pagetable walk
      KVM: MMU: Move gpte_access() out of paging_tmpl.h
      KVM: MMU: Optimize gpte_access() slightly
      KVM: MMU: Push clean gpte write protection out of gpte_access()
      KVM: clarify kvmclock documentation
      KVM: make processes waiting on vcpu mutex killable
      KVM: SVM: Make use of asm.h
      KVM: VMX: Make use of asm.h
      KVM: VMX: Make lto-friendly
      KVM: x86: lapic: Clean up find_highest_vector() and count_vectors()
      ...
    
    Conflicts:
            arch/s390/include/asm/processor.h
            arch/x86/kvm/i8259.c

commit c863901075a42d50678616d8ee4b96ef13080498
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Fri Sep 21 05:42:55 2012 +0200

    KVM: x86: Fix guest debug across vcpu INIT reset
    
    If we reset a vcpu on INIT, we so far overwrote dr7 as provided by
    KVM_SET_GUEST_DEBUG, and we also cleared switch_db_regs unconditionally.
    
    Fix this by saving the dr7 used for guest debugging and calculating the
    effective register value as well as switch_db_regs on any potential
    change. This will change to focus of the set_guest_debug vendor op to
    update_dp_bp_intercept.
    
    Found while trying to stop on start_secondary.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0b902c98f279..c9a91368fc5e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -471,6 +471,7 @@ struct kvm_vcpu_arch {
 	unsigned long dr6;
 	unsigned long dr7;
 	unsigned long eff_db[KVM_NR_DB_REGS];
+	unsigned long guest_debug_dr7;
 
 	u64 mcg_cap;
 	u64 mcg_status;
@@ -647,8 +648,7 @@ struct kvm_x86_ops {
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
 
-	void (*set_guest_debug)(struct kvm_vcpu *vcpu,
-				struct kvm_guest_debug *dbg);
+	void (*update_db_bp_intercept)(struct kvm_vcpu *vcpu);
 	int (*get_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata);
 	int (*set_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
 	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);

commit 26bf264e871a4b9a8ac09c21a2b518e7f23830d5
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Mon Sep 17 16:31:13 2012 +0800

    KVM: x86: Export svm/vmx exit code and vector code to userspace
    
    Exporting KVM exit information to userspace to be consumed by perf.
    
    Signed-off-by: Dong Hao <haodong@linux.vnet.ibm.com>
    [ Dong Hao <haodong@linux.vnet.ibm.com>: rebase it on acme's git tree ]
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Acked-by: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: Runzhen Wang <runzhen@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1347870675-31495-2-git-send-email-haodong@linux.vnet.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 09155d64cf7e..1eaa6b056670 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -75,22 +75,6 @@
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
-#define DE_VECTOR 0
-#define DB_VECTOR 1
-#define BP_VECTOR 3
-#define OF_VECTOR 4
-#define BR_VECTOR 5
-#define UD_VECTOR 6
-#define NM_VECTOR 7
-#define DF_VECTOR 8
-#define TS_VECTOR 10
-#define NP_VECTOR 11
-#define SS_VECTOR 12
-#define GP_VECTOR 13
-#define PF_VECTOR 14
-#define MF_VECTOR 16
-#define MC_VECTOR 18
-
 #define SELECTOR_TI_MASK (1 << 2)
 #define SELECTOR_RPL_MASK 0x03
 

commit 1e08ec4a130e2745d96df169e67c58df98a07311
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Sep 13 17:19:24 2012 +0300

    KVM: optimize apic interrupt delivery
    
    Most interrupt are delivered to only one vcpu. Use pre-build tables to
    find interrupt destination instead of looping through all vcpus. In case
    of logical mode loop only through vcpus in a logical cluster irq is sent
    to.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 43aeb9422839..0b902c98f279 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -525,6 +525,16 @@ struct kvm_arch_memory_slot {
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 };
 
+struct kvm_apic_map {
+	struct rcu_head rcu;
+	u8 ldr_bits;
+	/* fields bellow are used to decode ldr values in different modes */
+	u32 cid_shift, cid_mask, lid_mask;
+	struct kvm_lapic *phys_map[256];
+	/* first index is cluster id second is cpu id in a cluster */
+	struct kvm_lapic *logical_map[16][16];
+};
+
 struct kvm_arch {
 	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;
@@ -542,6 +552,8 @@ struct kvm_arch {
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;
 	int vapics_in_nmi_mode;
+	struct mutex apic_map_lock;
+	struct kvm_apic_map *apic_map;
 
 	unsigned int tss_addr;
 	struct page *apic_access_page;

commit 6fd01b711bee96ce3356f7b6f370ab708e37504b
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Sep 12 20:46:56 2012 +0300

    KVM: MMU: Optimize is_last_gpte()
    
    Instead of branchy code depending on level, gpte.ps, and mmu configuration,
    prepare everything in a bitmap during mode changes and look it up during
    runtime.
    
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3318bde206a5..43aeb9422839 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -298,6 +298,13 @@ struct kvm_mmu {
 	u64 *lm_root;
 	u64 rsvd_bits_mask[2][4];
 
+	/*
+	 * Bitmap: bit set = last pte in walk
+	 * index[0:1]: level (zero-based)
+	 * index[2]: pte.ps
+	 */
+	u8 last_pte_bitmap;
+
 	bool nx;
 
 	u64 pdptrs[4]; /* pae */

commit 97d64b788114be1c4dc4bfe7a8ba2bf9643fe6af
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Sep 12 14:52:00 2012 +0300

    KVM: MMU: Optimize pte permission checks
    
    walk_addr_generic() permission checks are a maze of branchy code, which is
    performed four times per lookup.  It depends on the type of access, efer.nxe,
    cr0.wp, cr4.smep, and in the near future, cr4.smap.
    
    Optimize this away by precalculating all variants and storing them in a
    bitmap.  The bitmap is recalculated when rarely-changing variables change
    (cr0, cr4) and is indexed by the often-changing variables (page fault error
    code, pte access permissions).
    
    The permission check is moved to the end of the loop, otherwise an SMEP
    fault could be reported as a false positive, when PDE.U=1 but PTE.U=0.
    Noted by Xiao Guangrong.
    
    The result is short, branch-free code.
    
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 64adb6117e19..3318bde206a5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -287,6 +287,13 @@ struct kvm_mmu {
 	union kvm_mmu_page_role base_role;
 	bool direct_map;
 
+	/*
+	 * Bitmap; bit set = permission fault
+	 * Byte index: page fault error code [4:1]
+	 * Bit index: pte permissions in ACC_* format
+	 */
+	u8 permissions[16];
+
 	u64 *pae_root;
 	u64 *lm_root;
 	u64 rsvd_bits_mask[2][4];

commit 716d51abff06f48425cef15d78ca6f36093f6dbf
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Sep 3 15:24:26 2012 +0300

    KVM: Provide userspace IO exit completion callback
    
    Current code assumes that IO exit was due to instruction emulation
    and handles execution back to emulator directly. This patch adds new
    userspace IO exit completion callback that can be set by any other code
    that caused IO exit to userspace.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fc0e752e7564..64adb6117e19 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -414,6 +414,7 @@ struct kvm_vcpu_arch {
 	struct x86_emulate_ctxt emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
+	int (*complete_userspace_io)(struct kvm_vcpu *vcpu);
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;

commit 51d59c6b422f3f95940ae4e5b42f165595906aee
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Aug 3 15:57:49 2012 -0300

    KVM: x86: fix pvclock guest stopped flag reporting
    
    kvm_guest_time_update unconditionally clears hv_clock.flags field,
    so the notification never reaches the guest.
    
    Fix it by allowing PVCLOCK_GUEST_STOPPED to passthrough.
    
    Reviewed-by: Eric B Munson <emunson@mgebm.net>
    Reviewed-by: Amit Shah <amit.shah@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1309e69b57fa..fc0e752e7564 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -420,6 +420,8 @@ struct kvm_vcpu_arch {
 	unsigned int hw_tsc_khz;
 	unsigned int time_offset;
 	struct page *time_page;
+	/* set guest stopped flag in pvclock flags field */
+	bool pvclock_set_guest_stopped_request;
 
 	struct {
 		u64 msr_val;

commit d89cc617b954aff4030fce178f7d86f59aaf713d
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Aug 1 18:03:28 2012 +0900

    KVM: Push rmap into kvm_arch_memory_slot
    
    Two reasons:
     - x86 can integrate rmap and rmap_pde and remove heuristics in
       __gfn_to_rmap().
     - Some architectures do not need rmap.
    
    Since rmap is one of the most memory consuming stuff in KVM, ppc'd
    better restrict the allocation to Book3S HV.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 48e713188469..1309e69b57fa 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -504,7 +504,7 @@ struct kvm_lpage_info {
 };
 
 struct kvm_arch_memory_slot {
-	unsigned long *rmap_pde[KVM_NR_PAGE_SIZES - 1];
+	unsigned long *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 };
 

commit e9bda6f6f902e6b55d9baceb5523468a048cbe56
Merge: bdc0077af574 06e48c510aa3
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jul 26 11:54:21 2012 +0300

    Merge branch 'queue' into next
    
    Merge patches queued during the run-up to the merge window.
    
    * queue: (25 commits)
      KVM: Choose better candidate for directed yield
      KVM: Note down when cpu relax intercepted or pause loop exited
      KVM: Add config to support ple or cpu relax optimzation
      KVM: switch to symbolic name for irq_states size
      KVM: x86: Fix typos in pmu.c
      KVM: x86: Fix typos in lapic.c
      KVM: x86: Fix typos in cpuid.c
      KVM: x86: Fix typos in emulate.c
      KVM: x86: Fix typos in x86.c
      KVM: SVM: Fix typos
      KVM: VMX: Fix typos
      KVM: remove the unused parameter of gfn_to_pfn_memslot
      KVM: remove is_error_hpa
      KVM: make bad_pfn static to kvm_main.c
      KVM: using get_fault_pfn to get the fault pfn
      KVM: MMU: track the refcount when unmap the page
      KVM: x86: remove unnecessary mark_page_dirty
      KVM: MMU: Avoid handling same rmap_pde in kvm_handle_hva_range()
      KVM: MMU: Push trace_kvm_age_page() into kvm_age_rmapp()
      KVM: MMU: Add memslot parameter to hva handlers
      ...
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

commit 5fecc9d8f59e765c2a48379dd7c6f5cf88c7d75a
Merge: 3c4cfadef6a1 1a577b72475d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 24 12:01:20 2012 -0700

    Merge tag 'kvm-3.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Avi Kivity:
     "Highlights include
      - full big real mode emulation on pre-Westmere Intel hosts (can be
        disabled with emulate_invalid_guest_state=0)
      - relatively small ppc and s390 updates
      - PCID/INVPCID support in guests
      - EOI avoidance; 3.6 guests should perform better on 3.6 hosts on
        interrupt intensive workloads)
      - Lockless write faults during live migration
      - EPT accessed/dirty bits support for new Intel processors"
    
    Fix up conflicts in:
     - Documentation/virtual/kvm/api.txt:
    
       Stupid subchapter numbering, added next to each other.
    
     - arch/powerpc/kvm/booke_interrupts.S:
    
       PPC asm changes clashing with the KVM fixes
    
     - arch/s390/include/asm/sigp.h, arch/s390/kvm/sigp.c:
    
       Duplicated commits through the kvm tree and the s390 tree, with
       subsequent edits in the KVM tree.
    
    * tag 'kvm-3.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (93 commits)
      KVM: fix race with level interrupts
      x86, hyper: fix build with !CONFIG_KVM_GUEST
      Revert "apic: fix kvm build on UP without IOAPIC"
      KVM guest: switch to apic_set_eoi_write, apic_write
      apic: add apic_set_eoi_write for PV use
      KVM: VMX: Implement PCID/INVPCID for guests with EPT
      KVM: Add x86_hyper_kvm to complete detect_hypervisor_platform check
      KVM: PPC: Critical interrupt emulation support
      KVM: PPC: e500mc: Fix tlbilx emulation for 64-bit guests
      KVM: PPC64: booke: Set interrupt computation mode for 64-bit host
      KVM: PPC: bookehv: Add ESR flag to Data Storage Interrupt
      KVM: PPC: bookehv64: Add support for std/ld emulation.
      booke: Added crit/mc exception handler for e500v2
      booke/bookehv: Add host crit-watchdog exception support
      KVM: MMU: document mmu-lock and fast page fault
      KVM: MMU: fix kvm_mmu_pagetable_walk tracepoint
      KVM: MMU: trace fast page fault
      KVM: MMU: fast path of handling guest page fault
      KVM: MMU: introduce SPTE_MMU_WRITEABLE bit
      KVM: MMU: fold tlb flush judgement into mmu_spte_update
      ...

commit 1a577b72475d161b6677c05abe57301362023bb2
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Thu Jul 19 13:45:20 2012 +0300

    KVM: fix race with level interrupts
    
    When more than 1 source id is in use for the same GSI, we have the
    following race related to handling irq_states race:
    
    CPU 0 clears bit 0. CPU 0 read irq_state as 0. CPU 1 sets level to 1.
    CPU 1 calls kvm_ioapic_set_irq(1). CPU 0 calls kvm_ioapic_set_irq(0).
    Now ioapic thinks the level is 0 but irq_state is not 0.
    
    Fix by performing all irq_states bitmap handling under pic/ioapic lock.
    This also removes the need for atomics with irq_states handling.
    
    Reported-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a3e9409e90b6..2c75b400e40c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -816,7 +816,20 @@ int kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 void kvm_propagate_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
-int kvm_pic_set_irq(void *opaque, int irq, int level);
+static inline int __kvm_irq_line_state(unsigned long *irq_state,
+				       int irq_source_id, int level)
+{
+	/* Logical OR for level trig interrupt */
+	if (level)
+		__set_bit(irq_source_id, irq_state);
+	else
+		__clear_bit(irq_source_id, irq_state);
+
+	return !!(*irq_state);
+}
+
+int kvm_pic_set_irq(struct kvm_pic *pic, int irq, int irq_source_id, int level);
+void kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);
 
 void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 

commit 77d11309b3a10e1ce112058ec2c9b7b979bcf311
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Mon Jul 2 17:57:17 2012 +0900

    KVM: Separate rmap_pde from kvm_lpage_info->write_count
    
    This makes it possible to loop over rmap_pde arrays in the same way as
    we do over rmap so that we can optimize kvm_handle_hva_range() easily in
    the following patch.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d4aab865606c..4f98da9243fc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -500,11 +500,11 @@ struct kvm_vcpu_arch {
 };
 
 struct kvm_lpage_info {
-	unsigned long rmap_pde;
 	int write_count;
 };
 
 struct kvm_arch_memory_slot {
+	unsigned long *rmap_pde[KVM_NR_PAGE_SIZES - 1];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 };
 

commit b3ae2096974b12c3af2ad1a4e7716b084949867f
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Mon Jul 2 17:56:33 2012 +0900

    KVM: Introduce kvm_unmap_hva_range() for kvm_mmu_notifier_invalidate_range_start()
    
    When we tested KVM under memory pressure, with THP enabled on the host,
    we noticed that MMU notifier took a long time to invalidate huge pages.
    
    Since the invalidation was done with mmu_lock held, it not only wasted
    the CPU but also made the host harder to respond.
    
    This patch mitigates this by using kvm_handle_hva_range().
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a3e9409e90b6..d4aab865606c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -944,6 +944,7 @@ extern bool kvm_rebooting;
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
+int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);

commit ad756a1603c5fac207758faaac7f01c34c9d0b7b
Author: Mao, Junjie <junjie.mao@intel.com>
Date:   Mon Jul 2 01:18:48 2012 +0000

    KVM: VMX: Implement PCID/INVPCID for guests with EPT
    
    This patch handles PCID/INVPCID for guests.
    
    Process-context identifiers (PCIDs) are a facility by which a logical processor
    may cache information for multiple linear-address spaces so that the processor
    may retain cached information when software switches to a different linear
    address space. Refer to section 4.10.1 in IA32 Intel Software Developer's Manual
    Volume 3A for details.
    
    For guests with EPT, the PCID feature is enabled and INVPCID behaves as running
    natively.
    For guests without EPT, the PCID feature is disabled and INVPCID triggers #UD.
    
    Signed-off-by: Junjie Mao <junjie.mao@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 24b76474d9de..a3e9409e90b6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -48,12 +48,13 @@
 
 #define CR3_PAE_RESERVED_BITS ((X86_CR3_PWT | X86_CR3_PCD) - 1)
 #define CR3_NONPAE_RESERVED_BITS ((PAGE_SIZE-1) & ~(X86_CR3_PWT | X86_CR3_PCD))
+#define CR3_PCID_ENABLED_RESERVED_BITS 0xFFFFFF0000000000ULL
 #define CR3_L_MODE_RESERVED_BITS (CR3_NONPAE_RESERVED_BITS |	\
 				  0xFFFFFF0000000000ULL)
 #define CR4_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
-			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR  \
+			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR | X86_CR4_PCIDE \
 			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_RDWRGSFS \
 			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE))
 
@@ -673,6 +674,7 @@ struct kvm_x86_ops {
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
+	bool (*invpcid_supported)(void);
 	void (*adjust_tsc_offset)(struct kvm_vcpu *vcpu, s64 adjustment, bool host);
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);

commit 15c7ad51ad58cbd3b46112c1840bc7228bd354bf
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jun 20 20:46:33 2012 +0200

    perf/x86: Rename Intel specific macros
    
    There are macros that are Intel specific and not x86 generic. Rename
    them into INTEL_*.
    
    This patch removes X86_PMC_IDX_GENERIC and does:
    
     $ sed -i -e 's/X86_PMC_MAX_/INTEL_PMC_MAX_/g'           \
             arch/x86/include/asm/kvm_host.h                 \
             arch/x86/include/asm/perf_event.h               \
             arch/x86/kernel/cpu/perf_event.c                \
             arch/x86/kernel/cpu/perf_event_p4.c             \
             arch/x86/kvm/pmu.c
     $ sed -i -e 's/X86_PMC_IDX_FIXED/INTEL_PMC_IDX_FIXED/g' \
             arch/x86/include/asm/perf_event.h               \
             arch/x86/kernel/cpu/perf_event.c                \
             arch/x86/kernel/cpu/perf_event_intel.c          \
             arch/x86/kernel/cpu/perf_event_intel_ds.c       \
             arch/x86/kvm/pmu.c
     $ sed -i -e 's/X86_PMC_MSK_/INTEL_PMC_MSK_/g'           \
             arch/x86/include/asm/perf_event.h               \
             arch/x86/kernel/cpu/perf_event.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1340217996-2254-2-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index db7c1f2709a2..2da88c0cda14 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -313,8 +313,8 @@ struct kvm_pmu {
 	u64 counter_bitmask[2];
 	u64 global_ctrl_mask;
 	u8 version;
-	struct kvm_pmc gp_counters[X86_PMC_MAX_GENERIC];
-	struct kvm_pmc fixed_counters[X86_PMC_MAX_FIXED];
+	struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+	struct kvm_pmc fixed_counters[INTEL_PMC_MAX_FIXED];
 	struct irq_work irq_work;
 	u64 reprogram_pmi;
 };

commit ae7a2a3fb6f8b784c2752863f4f1f20c656f76fb
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Jun 24 19:25:07 2012 +0300

    KVM: host side for eoi optimization
    
    Implementation of PV EOI using shared memory.
    This reduces the number of exits an interrupt
    causes as much as by half.
    
    The idea is simple: there's a bit, per APIC, in guest memory,
    that tells the guest that it does not need EOI.
    We set it before injecting an interrupt and clear
    before injecting a nested one. Guest tests it using
    a test and clear operation - this is necessary
    so that host can detect interrupt nesting -
    and if set, it can skip the EOI MSR.
    
    There's a new MSR to set the address of said register
    in guest memory. Otherwise not much changed:
    - Guest EOI is not required
    - Register is tested & ISR is automatically cleared on exit
    
    For testing results see description of previous patch
    'kvm_para: guest side for eoi avoidance'.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index db7c1f2709a2..24b76474d9de 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -175,6 +175,13 @@ enum {
 
 /* apic attention bits */
 #define KVM_APIC_CHECK_VAPIC	0
+/*
+ * The following bit is set with PV-EOI, unset on EOI.
+ * We detect PV-EOI changes by guest by comparing
+ * this bit with PV-EOI in guest memory.
+ * See the implementation in apic_update_pv_eoi.
+ */
+#define KVM_APIC_PV_EOI_PENDING	1
 
 /*
  * We don't want allocation failures within the mmu code, so we preallocate
@@ -484,6 +491,11 @@ struct kvm_vcpu_arch {
 		u64 length;
 		u64 status;
 	} osvw;
+
+	struct {
+		u64 msr_val;
+		struct gfn_to_hva_cache data;
+	} pv_eoi;
 };
 
 struct kvm_lpage_info {

commit 07acfc2a9349a8ce45b236c2624dad452001966b
Merge: b5f4035adfff 322728e55aa7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 16:17:30 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Avi Kivity:
     "Changes include additional instruction emulation, page-crossing MMIO,
      faster dirty logging, preventing the watchdog from killing a stopped
      guest, module autoload, a new MSI ABI, and some minor optimizations
      and fixes.  Outside x86 we have a small s390 and a very large ppc
      update.
    
      Regarding the new (for kvm) rebaseless workflow, some of the patches
      that were merged before we switch trees had to be rebased, while
      others are true pulls.  In either case the signoffs should be correct
      now."
    
    Fix up trivial conflicts in Documentation/feature-removal-schedule.txt
    arch/powerpc/kvm/book3s_segment.S and arch/x86/include/asm/kvm_para.h.
    
    I suspect the kvm_para.h resolution ends up doing the "do I have cpuid"
    check effectively twice (it was done differently in two different
    commits), but better safe than sorry ;)
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (125 commits)
      KVM: make asm-generic/kvm_para.h have an ifdef __KERNEL__ block
      KVM: s390: onereg for timer related registers
      KVM: s390: epoch difference and TOD programmable field
      KVM: s390: KVM_GET/SET_ONEREG for s390
      KVM: s390: add capability indicating COW support
      KVM: Fix mmu_reload() clash with nested vmx event injection
      KVM: MMU: Don't use RCU for lockless shadow walking
      KVM: VMX: Optimize %ds, %es reload
      KVM: VMX: Fix %ds/%es clobber
      KVM: x86 emulator: convert bsf/bsr instructions to emulate_2op_SrcV_nobyte()
      KVM: VMX: unlike vmcs on fail path
      KVM: PPC: Emulator: clean up SPR reads and writes
      KVM: PPC: Emulator: clean up instruction parsing
      kvm/powerpc: Add new ioctl to retreive server MMU infos
      kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
      KVM: PPC: bookehv: Fix r8/r13 storing in level exception handler
      KVM: PPC: Book3S: Enable IRQs during exit handling
      KVM: PPC: Fix PR KVM on POWER7 bare metal
      KVM: PPC: Fix stbux emulation
      KVM: PPC: bookehv: Use lwz/stw instead of PPC_LL/PPC_STL for 32-bit fields
      ...

commit c142786c6291189b5c85f53d91743e1eefbd8fe0
Author: Avi Kivity <avi@redhat.com>
Date:   Mon May 14 15:44:06 2012 +0300

    KVM: MMU: Don't use RCU for lockless shadow walking
    
    Using RCU for lockless shadow walking can increase the amount of memory
    in use by the system, since RCU grace periods are unpredictable.  We also
    have an unconditional write to a shared variable (reader_counter), which
    isn't good for scaling.
    
    Replace that with a scheme similar to x86's get_user_pages_fast(): disable
    interrupts during lockless shadow walk to force the freer
    (kvm_mmu_commit_zap_page()) to wait for the TLB flush IPI to find the
    processor with interrupts enabled.
    
    We also add a new vcpu->mode, READING_SHADOW_PAGE_TABLES, to prevent
    kvm_flush_remote_tlbs() from avoiding the IPI.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 69e39bc7e36f..64c8989263f6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -240,8 +240,6 @@ struct kvm_mmu_page {
 #endif
 
 	int write_flooding_count;
-
-	struct rcu_head rcu;
 };
 
 struct kvm_pio_request {
@@ -540,8 +538,6 @@ struct kvm_arch {
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
 
-	atomic_t reader_counter;
-
 	#ifdef CONFIG_KVM_MMU_AUDIT
 	int audit_point;
 	#endif

commit 413837714232b3a4c0705e915d8af75ad521d083
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Apr 19 14:06:29 2012 +0300

    KVM: Introduce bitmask for apic attention reasons
    
    The patch introduces a bitmap that will hold reasons apic should be
    checked during vmexit. This is in a preparation for vp eoi patch
    that will add one more check on vmexit. With the bitmap we can do
    if(apic_attention) to check everything simultaneously which will
    add zero overhead on the fast path.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f624ca72ea24..69e39bc7e36f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -172,6 +172,9 @@ enum {
 #define DR7_FIXED_1	0x00000400
 #define DR7_VOLATILE	0xffff23ff
 
+/* apic attention bits */
+#define KVM_APIC_CHECK_VAPIC	0
+
 /*
  * We don't want allocation failures within the mmu code, so we preallocate
  * enough memory for a single page fault in a cache.
@@ -337,6 +340,7 @@ struct kvm_vcpu_arch {
 	u64 efer;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
+	unsigned long apic_attention;
 	int32_t apic_arb_prio;
 	int mp_state;
 	int sipi_vector;

commit 3ee89722cfb165295cc8eb498018c0bdafc57062
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri Apr 20 13:41:59 2012 -0700

    x86, extable: Remove open-coded exception table entries in arch/x86/include/asm/kvm_host.h
    
    Remove open-coded exception table entries in arch/x86/include/asm/kvm_host.h,
    and replace them with _ASM_EXTABLE() macros; this will allow us to
    change the format and type of the exception table entries.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Link: http://lkml.kernel.org/r/CA%2B55aFyijf43qSu3N9nWHEBwaGbb7T2Oq9A=9EyR=Jtyqfq_cQ@mail.gmail.com

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e216ba066e79..e5b97be12d2a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -27,6 +27,7 @@
 #include <asm/desc.h>
 #include <asm/mtrr.h>
 #include <asm/msr-index.h>
+#include <asm/asm.h>
 
 #define KVM_MAX_VCPUS 254
 #define KVM_SOFT_MAX_VCPUS 160
@@ -921,9 +922,7 @@ extern bool kvm_rebooting;
 	__ASM_SIZE(push) " $666b \n\t"	      \
 	"call kvm_spurious_fault \n\t"	      \
 	".popsection \n\t" \
-	".pushsection __ex_table, \"a\" \n\t" \
-	_ASM_PTR " 666b, 667b \n\t" \
-	".popsection"
+	_ASM_EXTABLE(666b, 667b)
 
 #define __kvm_handle_fault_on_reboot(insn)		\
 	____kvm_handle_fault_on_reboot(insn, "")

commit 5dc99b2380d59b8aeafa98791f92b96400ed3187
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Thu Mar 1 19:32:16 2012 +0900

    KVM: Avoid checking huge page mappings in get_dirty_log()
    
    Dropped such mappings when we enabled dirty logging and we will never
    create new ones until we stop the logging.
    
    For this we introduce a new function which can be used to write protect
    a range of PT level pages: although we do not need to care about a range
    of pages at this point, the following patch will need this feature to
    optimize the write protection of many pages.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e216ba066e79..f624ca72ea24 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -712,8 +712,9 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 
 int kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
-int kvm_mmu_rmap_write_protect(struct kvm *kvm, u64 gfn,
-			       struct kvm_memory_slot *slot);
+void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
+				     struct kvm_memory_slot *slot,
+				     gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);

commit 7f3d35fddd173e52886d03bc34b5b5d6f5bea343
Author: Kevin Wolf <kwolf@redhat.com>
Date:   Wed Feb 8 14:34:38 2012 +0100

    KVM: x86 emulator: Fix task switch privilege checks
    
    Currently, all task switches check privileges against the DPL of the
    TSS. This is only correct for jmp/call to a TSS. If a task gate is used,
    the DPL of this take gate is used for the check instead. Exceptions,
    external interrupts and iret shouldn't perform any check.
    
    [avi: kill kvm-kmod remnants]
    
    Signed-off-by: Kevin Wolf <kwolf@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 74c9edf2bb18..e216ba066e79 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -768,8 +768,8 @@ int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);
 
-int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason,
-		    bool has_error_code, u32 error_code);
+int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,
+		    int reason, bool has_error_code, u32 error_code);
 
 int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 int kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);

commit db3fe4eb45f3555d91a7124e18cf3a2f2a30eb90
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Feb 8 13:02:18 2012 +0900

    KVM: Introduce kvm_memory_slot::arch and move lpage_info into it
    
    Some members of kvm_memory_slot are not used by every architecture.
    
    This patch is the first step to make this difference clear by
    introducing kvm_memory_slot::arch;  lpage_info is moved into it.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c24125cd0c63..74c9edf2bb18 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -483,6 +483,15 @@ struct kvm_vcpu_arch {
 	} osvw;
 };
 
+struct kvm_lpage_info {
+	unsigned long rmap_pde;
+	int write_count;
+};
+
+struct kvm_arch_memory_slot {
+	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+};
+
 struct kvm_arch {
 	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;

commit e26101b116a6235bcd80b3a4c38c9fe91286cd79
Author: Zachary Amsden <zamsden@gmail.com>
Date:   Fri Feb 3 15:43:57 2012 -0200

    KVM: Track TSC synchronization in generations
    
    This allows us to track the original nanosecond and counter values
    at each phase of TSC writing by the guest.  This gets us perfect
    offset matching for stable TSC systems, and perfect software
    computed TSC matching for machines with unstable TSC.
    
    Signed-off-by: Zachary Amsden <zamsden@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4fbeb84b1818..c24125cd0c63 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -420,10 +420,11 @@ struct kvm_vcpu_arch {
 
 	u64 last_guest_tsc;
 	u64 last_kernel_ns;
-	u64 last_tsc_nsec;
-	u64 last_tsc_write;
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;
+	u64 this_tsc_nsec;
+	u64 this_tsc_write;
+	u8  this_tsc_generation;
 	bool tsc_catchup;
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
@@ -513,9 +514,12 @@ struct kvm_arch {
 	s64 kvmclock_offset;
 	raw_spinlock_t tsc_write_lock;
 	u64 last_tsc_nsec;
-	u64 last_tsc_offset;
 	u64 last_tsc_write;
 	u32 last_tsc_khz;
+	u64 cur_tsc_nsec;
+	u64 cur_tsc_write;
+	u64 cur_tsc_offset;
+	u8  cur_tsc_generation;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 

commit 0dd6a6edb0124e6c71931ff575b18e15ed6e8603
Author: Zachary Amsden <zamsden@gmail.com>
Date:   Fri Feb 3 15:43:56 2012 -0200

    KVM: Dont mark TSC unstable due to S4 suspend
    
    During a host suspend, TSC may go backwards, which KVM interprets
    as an unstable TSC.  Technically, KVM should not be marking the
    TSC unstable, which causes the TSC clocksource to go bad, but we
    need to be adjusting the TSC offsets in such a case.
    
    Dealing with this issue is a little tricky as the only place we
    can reliably do it is before much of the timekeeping infrastructure
    is up and running.  On top of this, we are not in a KVM thread
    context, so we may not be able to safely access VCPU fields.
    Instead, we compute our best known hardware offset at power-up and
    stash it to be applied to all VCPUs when they actually start running.
    
    Signed-off-by: Zachary Amsden <zamsden@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dd439f13df84..4fbeb84b1818 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -423,6 +423,7 @@ struct kvm_vcpu_arch {
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
 	u64 last_host_tsc;
+	u64 tsc_offset_adjustment;
 	bool tsc_catchup;
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;

commit f1e2b26003c41e581243c09ceed7567677449468
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Feb 3 15:43:55 2012 -0200

    KVM: Allow adjust_tsc_offset to be in host or guest cycles
    
    Redefine the API to take a parameter indicating whether an
    adjustment is in host or guest cycles.
    
    Signed-off-by: Zachary Amsden <zamsden@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b23682900f41..dd439f13df84 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -646,7 +646,7 @@ struct kvm_x86_ops {
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
-	void (*adjust_tsc_offset)(struct kvm_vcpu *vcpu, s64 adjustment);
+	void (*adjust_tsc_offset)(struct kvm_vcpu *vcpu, s64 adjustment, bool host);
 
 	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 
@@ -676,6 +676,17 @@ struct kvm_arch_async_pf {
 
 extern struct kvm_x86_ops *kvm_x86_ops;
 
+static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
+					   s64 adjustment)
+{
+	kvm_x86_ops->adjust_tsc_offset(vcpu, adjustment, false);
+}
+
+static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
+{
+	kvm_x86_ops->adjust_tsc_offset(vcpu, adjustment, true);
+}
+
 int kvm_mmu_module_init(void);
 void kvm_mmu_module_exit(void);
 

commit 6f526ec5383dcd5fa5ffc7b3ac1d62099a0b46ad
Author: Zachary Amsden <zamsden@gmail.com>
Date:   Fri Feb 3 15:43:54 2012 -0200

    KVM: Add last_host_tsc tracking back to KVM
    
    The variable last_host_tsc was removed from upstream code.  I am adding
    it back for two reasons.  First, it is unnecessary to use guest TSC
    computation to conclude information about the host TSC.  The guest may
    set the TSC backwards (this case handled by the previous patch), but
    the computation of guest TSC (and fetching an MSR) is significanlty more
    work and complexity than simply reading the hardware counter.  In addition,
    we don't actually need the guest TSC for any part of the computation,
    by always recomputing the offset, we can eliminate the need to deal with
    the current offset and any scaling factors that may apply.
    
    The second reason is that later on, we are going to be using the host
    TSC value to restore TSC offsets after a host S4 suspend, so we need to
    be reading the host values, not the guest values here.
    
    Signed-off-by: Zachary Amsden <zamsden@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8a34fca6c572..b23682900f41 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -422,6 +422,7 @@ struct kvm_vcpu_arch {
 	u64 last_kernel_ns;
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
+	u64 last_host_tsc;
 	bool tsc_catchup;
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;

commit 5d3cb0f6a8e3af018a522ae8d36f8f7d2511b5d8
Author: Zachary Amsden <zamsden@gmail.com>
Date:   Fri Feb 3 15:43:51 2012 -0200

    KVM: Improve TSC offset matching
    
    There are a few improvements that can be made to the TSC offset
    matching code.  First, we don't need to call the 128-bit multiply
    (especially on a constant number), the code works much nicer to
    do computation in nanosecond units.
    
    Second, the way everything is setup with software TSC rate scaling,
    we currently have per-cpu rates.  Obviously this isn't too desirable
    to use in practice, but if for some reason we do change the rate of
    all VCPUs at runtime, then reset the TSCs, we will only want to
    match offsets for VCPUs running at the same rate.
    
    Finally, for the case where we have an unstable host TSC, but
    rate scaling is being done in hardware, we should call the platform
    code to compute the TSC offset, so the math is reorganized to recompute
    the base instead, then transform the base into an offset using the
    existing API.
    
    [avi: fix 64-bit division on i386]
    
    Signed-off-by: Zachary Amsden <zamsden@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    
    KVM: Fix 64-bit division in kvm_write_tsc()
    
    Breaks i386 build.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ddebbe01fff9..8a34fca6c572 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -513,6 +513,7 @@ struct kvm_arch {
 	u64 last_tsc_nsec;
 	u64 last_tsc_offset;
 	u64 last_tsc_write;
+	u32 last_tsc_khz;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 

commit cc578287e3224d0da196cc1d226bdae6b068faa7
Author: Zachary Amsden <zamsden@gmail.com>
Date:   Fri Feb 3 15:43:50 2012 -0200

    KVM: Infrastructure for software and hardware based TSC rate scaling
    
    This requires some restructuring; rather than use 'virtual_tsc_khz'
    to indicate whether hardware rate scaling is in effect, we consider
    each VCPU to always have a virtual TSC rate.  Instead, there is new
    logic above the vendor-specific hardware scaling that decides whether
    it is even necessary to use and updates all rate variables used by
    common code.  This means we can simply query the virtual rate at
    any point, which is needed for software rate scaling.
    
    There is also now a threshold added to the TSC rate scaling; minor
    differences and variations of measured TSC rate can accidentally
    provoke rate scaling to be used when it is not needed.  Instead,
    we have a tolerance variable called tsc_tolerance_ppm, which is
    the maximum variation from user requested rate at which scaling
    will be used.  The default is 250ppm, which is the half the
    threshold for NTP adjustment, allowing for some hardware variation.
    
    In the event that hardware rate scaling is not available, we can
    kludge a bit by forcing TSC catchup to turn on when a faster than
    hardware speed has been requested, but there is nothing available
    yet for the reverse case; this requires a trap and emulate software
    implementation for RDTSC, which is still forthcoming.
    
    [avi: fix 64-bit division on i386]
    
    Signed-off-by: Zachary Amsden <zamsden@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 782d973b0719..ddebbe01fff9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -422,10 +422,11 @@ struct kvm_vcpu_arch {
 	u64 last_kernel_ns;
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
-	u32 virtual_tsc_khz;
 	bool tsc_catchup;
-	u32  tsc_catchup_mult;
-	s8   tsc_catchup_shift;
+	bool tsc_always_catchup;
+	s8 virtual_tsc_shift;
+	u32 virtual_tsc_mult;
+	u32 virtual_tsc_khz;
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
 	unsigned nmi_pending; /* NMI queued after currently running handler */
@@ -651,7 +652,7 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
-	void (*set_tsc_khz)(struct kvm_vcpu *vcpu, u32 user_tsc_khz);
+	void (*set_tsc_khz)(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	u64 (*compute_tsc_offset)(struct kvm_vcpu *vcpu, u64 target_tsc);

commit a59cb29e4d81e025192550c2703f305637f016f6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Feb 3 12:28:31 2012 -0200

    KVM: x86: increase recommended max vcpus to 160
    
    Increase recommended max vcpus from 64 to 160 (tested internally
    at Red Hat).
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 461016614324..782d973b0719 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -29,7 +29,7 @@
 #include <asm/msr-index.h>
 
 #define KVM_MAX_VCPUS 254
-#define KVM_SOFT_MAX_VCPUS 64
+#define KVM_SOFT_MAX_VCPUS 160
 #define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4

commit 3ea8b75e47ac70bdd0a2c0492102682d43bfa3c4
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Tue Jan 17 19:50:08 2012 +0900

    KVM: MMU: Remove unused kvm_pte_chain
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bd69c93da8fa..461016614324 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -181,13 +181,6 @@ struct kvm_mmu_memory_cache {
 	void *objects[KVM_NR_MEM_OBJS];
 };
 
-#define NR_PTE_CHAIN_ENTRIES 5
-
-struct kvm_pte_chain {
-	u64 *parent_ptes[NR_PTE_CHAIN_ENTRIES];
-	struct hlist_node link;
-};
-
 /*
  * kvm_mmu_page_role, below, is defined as:
  *

commit 2b036c6b861dc5da295c6fe19a3edcff7093fdeb
Author: Boris Ostrovsky <boris.ostrovsky@amd.com>
Date:   Mon Jan 9 14:00:35 2012 -0500

    KVM: SVM: Add support for AMD's OSVW feature in guests
    
    In some cases guests should not provide workarounds for errata even when the
    physical processor is affected. For example, because of erratum 400 on family
    10h processors a Linux guest will read an MSR (resulting in VMEXIT) before
    going to idle in order to avoid getting stuck in a non-C0 state. This is not
    necessary: HLT and IO instructions are intercepted and therefore there is no
    reason for erratum 400 workaround in the guest.
    
    This patch allows us to present a guest with certain errata as fixed,
    regardless of the state of actual hardware.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 52d6640a5ca1..bd69c93da8fa 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -478,6 +478,12 @@ struct kvm_vcpu_arch {
 		u32 id;
 		bool send_user_only;
 	} apf;
+
+	/* OSVW MSRs (AMD only) */
+	struct {
+		u64 length;
+		u64 status;
+	} osvw;
 };
 
 struct kvm_arch {

commit 022cd0e84020eec8b589bc119699c935c7b29584
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Nov 10 14:57:23 2011 +0200

    KVM: Add generic RDPMC support
    
    Add a helper function that emulates the RDPMC instruction operation.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fb60ffdb4e43..52d6640a5ca1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -760,6 +760,7 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 data);
 
 unsigned long kvm_get_rflags(struct kvm_vcpu *vcpu);
 void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);
+bool kvm_rdpmc(struct kvm_vcpu *vcpu);
 
 void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);

commit f5132b01386b5a67f1ff673bb2b96a507a3f7e41
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Nov 10 14:57:22 2011 +0200

    KVM: Expose a version 2 architectural PMU to a guests
    
    Use perf_events to emulate an architectural PMU, version 2.
    
    Based on PMU version 1 emulation by Avi Kivity.
    
    [avi: adjust for cpuid.c]
    [jan: fix anonymous field initialization for older gcc]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 020413afb285..fb60ffdb4e43 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -16,10 +16,12 @@
 #include <linux/mmu_notifier.h>
 #include <linux/tracepoint.h>
 #include <linux/cpumask.h>
+#include <linux/irq_work.h>
 
 #include <linux/kvm.h>
 #include <linux/kvm_para.h>
 #include <linux/kvm_types.h>
+#include <linux/perf_event.h>
 
 #include <asm/pvclock-abi.h>
 #include <asm/desc.h>
@@ -291,6 +293,37 @@ struct kvm_mmu {
 	u64 pdptrs[4]; /* pae */
 };
 
+enum pmc_type {
+	KVM_PMC_GP = 0,
+	KVM_PMC_FIXED,
+};
+
+struct kvm_pmc {
+	enum pmc_type type;
+	u8 idx;
+	u64 counter;
+	u64 eventsel;
+	struct perf_event *perf_event;
+	struct kvm_vcpu *vcpu;
+};
+
+struct kvm_pmu {
+	unsigned nr_arch_gp_counters;
+	unsigned nr_arch_fixed_counters;
+	unsigned available_event_types;
+	u64 fixed_ctr_ctrl;
+	u64 global_ctrl;
+	u64 global_status;
+	u64 global_ovf_ctrl;
+	u64 counter_bitmask[2];
+	u64 global_ctrl_mask;
+	u8 version;
+	struct kvm_pmc gp_counters[X86_PMC_MAX_GENERIC];
+	struct kvm_pmc fixed_counters[X86_PMC_MAX_FIXED];
+	struct irq_work irq_work;
+	u64 reprogram_pmi;
+};
+
 struct kvm_vcpu_arch {
 	/*
 	 * rip and regs accesses must go through
@@ -424,6 +457,8 @@ struct kvm_vcpu_arch {
 	unsigned access;
 	gfn_t mmio_gfn;
 
+	struct kvm_pmu pmu;
+
 	/* used for guest single stepping over the given code position */
 	unsigned long singlestep_rip;
 
@@ -891,4 +926,17 @@ extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 void kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
 
+int kvm_is_in_guest(void);
+
+void kvm_pmu_init(struct kvm_vcpu *vcpu);
+void kvm_pmu_destroy(struct kvm_vcpu *vcpu);
+void kvm_pmu_reset(struct kvm_vcpu *vcpu);
+void kvm_pmu_cpuid_update(struct kvm_vcpu *vcpu);
+bool kvm_pmu_msr(struct kvm_vcpu *vcpu, u32 msr);
+int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data);
+int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data);
+void kvm_handle_pmu_event(struct kvm_vcpu *vcpu);
+void kvm_deliver_pmi(struct kvm_vcpu *vcpu);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit e459e3228dc57f7160e564ce0f09edb5bee656d3
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Mon Nov 28 20:42:16 2011 +0800

    KVM: MMU: move the relevant mmu code to mmu.c
    
    Move the mmu code in kvm_arch_vcpu_init() to kvm_mmu_create()
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1769f3dde611..020413afb285 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -752,6 +752,7 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
+gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access);
 gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,
 			      struct x86_exception *exception);
 gpa_t kvm_mmu_gva_to_gpa_fetch(struct kvm_vcpu *vcpu, gva_t gva,
@@ -773,6 +774,11 @@ void kvm_disable_tdp(void);
 int complete_pio(struct kvm_vcpu *vcpu);
 bool kvm_check_iopl(struct kvm_vcpu *vcpu);
 
+static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access)
+{
+	return gpa;
+}
+
 static inline struct kvm_mmu_page *page_header(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);

commit 93a5cef07d686a0341d056b0f930a762c7174a13
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Nov 24 17:37:48 2011 +0800

    KVM: introduce KVM_MEM_SLOTS_NUM macro
    
    Introduce KVM_MEM_SLOTS_NUM macro to instead of
    KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 69b652547489..1769f3dde611 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -31,6 +31,8 @@
 #define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
+#define KVM_MEM_SLOTS_NUM (KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
+
 #define KVM_MMIO_SIZE 16
 
 #define KVM_PIO_PAGE_OFFSET 1
@@ -228,7 +230,7 @@ struct kvm_mmu_page {
 	 * One bit set per slot which has memory
 	 * in this shadow page.
 	 */
-	DECLARE_BITMAP(slot_bitmap, KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS);
+	DECLARE_BITMAP(slot_bitmap, KVM_MEM_SLOTS_NUM);
 	bool unsync;
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;

commit 95d4c16ce78cb6b7549a09159c409d52ddd18dae
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Mon Nov 14 18:24:50 2011 +0900

    KVM: Optimize dirty logging by rmap_write_protect()
    
    Currently, write protecting a slot needs to walk all the shadow pages
    and checks ones which have a pte mapping a page in it.
    
    The walk is overly heavy when dirty pages in that slot are not so many
    and checking the shadow pages would result in unwanted cache pollution.
    
    To mitigate this problem, we use rmap_write_protect() and check only
    the sptes which can be reached from gfns marked in the dirty bitmap
    when the number of dirty pages are less than that of shadow pages.
    
    This criterion is reasonable in its meaning and worked well in our test:
    write protection became some times faster than before when the ratio of
    dirty pages are low and was not worse even when the ratio was near the
    criterion.
    
    Note that the locking for this write protection becomes fine grained.
    The reason why this is safe is descripted in the comments.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6d8326409974..69b652547489 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -648,6 +648,8 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 
 int kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
+int kvm_mmu_rmap_write_protect(struct kvm *kvm, u64 gfn,
+			       struct kvm_memory_slot *slot);
 void kvm_mmu_zap_all(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);

commit fb92045843a8cd99c7b843d9b567a680a3854ba1
Author: Chris Wright <chrisw@sous-sol.org>
Date:   Tue Nov 1 17:31:18 2011 -0700

    KVM: MMU: remove KVM host pv mmu support
    
    The host side pv mmu support has been marked for feature removal in
    January 2011.  It's not in use, is slower than shadow or hardware
    assisted paging, and a maintenance burden.  It's November 2011, time to
    remove it.
    
    Signed-off-by: Chris Wright <chrisw@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c1f19de8b51c..6d8326409974 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -244,13 +244,6 @@ struct kvm_mmu_page {
 	struct rcu_head rcu;
 };
 
-struct kvm_pv_mmu_op_buffer {
-	void *ptr;
-	unsigned len;
-	unsigned processed;
-	char buf[512] __aligned(sizeof(long));
-};
-
 struct kvm_pio_request {
 	unsigned long count;
 	int in;
@@ -347,10 +340,6 @@ struct kvm_vcpu_arch {
 	 */
 	struct kvm_mmu *walk_mmu;
 
-	/* only needed in kvm_pv_mmu_op() path, but it's hot so
-	 * put it here to avoid allocation */
-	struct kvm_pv_mmu_op_buffer mmu_op_buffer;
-
 	struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
 	struct kvm_mmu_memory_cache mmu_page_cache;
 	struct kvm_mmu_memory_cache mmu_page_header_cache;
@@ -667,8 +656,6 @@ int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3);
 
 int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
 			  const void *val, int bytes);
-int kvm_pv_mmu_op(struct kvm_vcpu *vcpu, unsigned long bytes,
-		  gpa_t addr, unsigned long *ret);
 u8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 extern bool tdp_enabled;

commit a30f47cb150dd8d109923eeb65fe73e8b3e09046
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Sep 22 16:58:36 2011 +0800

    KVM: MMU: improve write flooding detected
    
    Detecting write-flooding does not work well, when we handle page written, if
    the last speculative spte is not accessed, we treat the page is
    write-flooding, however, we can speculative spte on many path, such as pte
    prefetch, page synced, that means the last speculative spte may be not point
    to the written page and the written page can be accessed via other sptes, so
    depends on the Accessed bit of the last speculative spte is not enough
    
    Instead of detected page accessed, we can detect whether the spte is accessed
    after it is written, if the spte is not accessed but it is written frequently,
    we treat is not a page table or it not used for a long time
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3c9ea26c7aea..c1f19de8b51c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -239,6 +239,8 @@ struct kvm_mmu_page {
 	int clear_spte_count;
 #endif
 
+	int write_flooding_count;
+
 	struct rcu_head rcu;
 };
 
@@ -353,10 +355,6 @@ struct kvm_vcpu_arch {
 	struct kvm_mmu_memory_cache mmu_page_cache;
 	struct kvm_mmu_memory_cache mmu_page_header_cache;
 
-	gfn_t last_pt_write_gfn;
-	int   last_pt_write_count;
-	u64  *last_pte_updated;
-
 	struct fpu guest_fpu;
 	u64 xcr0;
 

commit f57f2ef58f6703e6df70ed52a198920cb3e8edba
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Sep 22 16:56:39 2011 +0800

    KVM: MMU: fast prefetch spte on invlpg path
    
    Fast prefetch spte for the unsync shadow page on invlpg path
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f8ab0d760231..3c9ea26c7aea 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -461,7 +461,6 @@ struct kvm_arch {
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
-	atomic_t invlpg_counter;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.
@@ -757,8 +756,7 @@ int fx_init(struct kvm_vcpu *vcpu);
 
 void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
-		       const u8 *new, int bytes,
-		       bool guest_initiated);
+		       const u8 *new, int bytes);
 int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn);
 int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva);
 void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);

commit d01f8d5e02cc79998e3160f7ad545f77891b00e5
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Sep 22 16:55:36 2011 +0800

    KVM: MMU: do not mark accessed bit on pte write path
    
    In current code, the accessed bit is always set when page fault occurred,
    do not need to set it on pte write path
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4ceefa9567ed..f8ab0d760231 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -356,7 +356,6 @@ struct kvm_vcpu_arch {
 	gfn_t last_pt_write_gfn;
 	int   last_pt_write_count;
 	u64  *last_pte_updated;
-	gfn_t last_pte_gfn;
 
 	struct fpu guest_fpu;
 	u64 xcr0;

commit 1cb3f3ae5a3855ba430430706da4201ace1d6ec4
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Sep 22 17:02:48 2011 +0800

    KVM: x86: retry non-page-table writing instructions
    
    If the emulation is caused by #PF and it is non-page_table writing instruction,
    it means the VM-EXIT is caused by shadow page protected, we can zap the shadow
    page and retry this instruction directly
    
    The idea is from Avi
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b4973f4dab98..4ceefa9567ed 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -444,6 +444,9 @@ struct kvm_vcpu_arch {
 
 	cpumask_var_t wbinvd_dirty_mask;
 
+	unsigned long last_retry_eip;
+	unsigned long last_retry_addr;
+
 	struct {
 		bool halted;
 		gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
@@ -692,6 +695,7 @@ enum emulation_result {
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
+#define EMULTYPE_RETRY		    (1 << 3)
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
 			    int emulation_type, void *insn, int insn_len);
 
@@ -756,6 +760,7 @@ void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 		       const u8 *new, int bytes,
 		       bool guest_initiated);
+int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn);
 int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva);
 void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);

commit a3e06bbe8445f57eb949e6474c5a9b30f24d2057
Author: Liu, Jinsong <jinsong.liu@intel.com>
Date:   Thu Sep 22 16:55:52 2011 +0800

    KVM: emulate lapic tsc deadline timer for guest
    
    This patch emulate lapic tsc deadline timer for guest:
    Enumerate tsc deadline timer capability by CPUID;
    Enable tsc deadline timer mode by lapic MMIO;
    Start tsc deadline timer by WRMSR;
    
    [jan: use do_div()]
    [avi: fix for !irqchip_in_kernel()]
    [marcelo: another fix for !irqchip_in_kernel()]
    
    Signed-off-by: Liu, Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ab62711ccb78..b4973f4dab98 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -674,6 +674,8 @@ u8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 extern bool tdp_enabled;
 
+u64 vcpu_tsc_khz(struct kvm_vcpu *vcpu);
+
 /* control of guest tsc rate supported? */
 extern bool kvm_has_tsc_control;
 /* minimum supported tsc_khz for guests */

commit 7460fb4a340033107530df19e7e125bd0969bfb2
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Sep 20 13:43:14 2011 +0300

    KVM: Fix simultaneous NMIs
    
    If simultaneous NMIs happen, we're supposed to queue the second
    and next (collapsing them), but currently we sometimes collapse
    the second into the first.
    
    Fix by using a counter for pending NMIs instead of a bool; since
    the counter limit depends on whether the processor is currently
    in an NMI handler, which can only be checked in vcpu context
    (via the NMI mask), we add a new KVM_REQ_NMI to request recalculation
    of the counter.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6ab4241c27cb..ab62711ccb78 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -413,8 +413,9 @@ struct kvm_vcpu_arch {
 	u32  tsc_catchup_mult;
 	s8   tsc_catchup_shift;
 
-	bool nmi_pending;
-	bool nmi_injected;
+	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
+	unsigned nmi_pending; /* NMI queued after currently running handler */
+	bool nmi_injected;    /* Trying to inject an NMI this entry */
 
 	struct mtrr_state_type mtrr_state;
 	u32 pat;

commit d5c1785d2f3aabe284d91bc7fc8f0abc58525dc9
Author: Nadav Har'El <nyh@il.ibm.com>
Date:   Tue Aug 2 15:54:20 2011 +0300

    KVM: L1 TSC handling
    
    KVM assumed in several places that reading the TSC MSR returns the value for
    L1. This is incorrect, because when L2 is running, the correct TSC read exit
    emulation is to return L2's value.
    
    We therefore add a new x86_ops function, read_l1_tsc, to use in places that
    specifically need to read the L1 TSC, NOT the TSC of the current level of
    guest.
    
    Note that one change, of one line in kvm_arch_vcpu_load, is made redundant
    by a different patch sent by Zachary Amsden (and not yet applied):
    kvm_arch_vcpu_load() should not read the guest TSC, and if it didn't, of
    course we didn't have to change the call of kvm_get_msr() to read_l1_tsc().
    
    [avi: moved callback to kvm_x86_ops tsc block]
    
    Signed-off-by: Nadav Har'El <nyh@il.ibm.com>
    Acked-by: Zachary Amsdem <zamsden@gmail.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b31a3417a405..6ab4241c27cb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -630,6 +630,7 @@ struct kvm_x86_ops {
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	u64 (*compute_tsc_offset)(struct kvm_vcpu *vcpu, u64 target_tsc);
+	u64 (*read_l1_tsc)(struct kvm_vcpu *vcpu);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
 

commit e4e517b4be019787ada4cbbce2f04570c21b0cbd
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jul 28 11:36:17 2011 +0300

    KVM: MMU: Do not unconditionally read PDPTE from guest memory
    
    Architecturally, PDPTEs are cached in the PDPTRs when CR3 is reloaded.
    On SVM, it is not possible to implement this, but on VMX this is possible
    and was indeed implemented until nested SVM changed this to unconditionally
    read PDPTEs dynamically.  This has noticable impact when running PAE guests.
    
    Fix by changing the MMU to read PDPTRs from the cache, falling back to
    reading from memory for the nested MMU.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Tested-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 307e3cfa28ad..b31a3417a405 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -265,6 +265,7 @@ struct kvm_mmu {
 	void (*new_cr3)(struct kvm_vcpu *vcpu);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
+	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err,
 			  bool prefault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,

commit 0d460ffc0956d2dbe12ca9f5f6aa0f8701ea9d73
Author: Stefan Hajnoczi <stefanha@linux.vnet.ibm.com>
Date:   Fri Jul 22 12:46:53 2011 +0100

    KVM: Use __print_symbolic() for vmexit tracepoints
    
    The vmexit tracepoints format the exit_reason to make it human-readable.
    Since the exit_reason depends on the instruction set (vmx or svm),
    formatting is handled with ftrace_print_symbols_seq() by referring to
    the appropriate exit reason table.
    
    However, the ftrace_print_symbols_seq() function is not meant to be used
    directly in tracepoints since it does not export the formatting table
    which userspace tools like trace-cmd and perf use to format traces.
    
    In practice perf dies when formatting vmexit-related events and
    trace-cmd falls back to printing the numeric value (with extra
    formatting code in the kvm plugin to paper over this limitation).  Other
    userspace consumers of vmexit-related tracepoints would be in similar
    trouble.
    
    To avoid significant changes to the kvm_exit tracepoint, this patch
    moves the vmx and svm exit reason tables into arch/x86/kvm/trace.h and
    selects the right table with __print_symbolic() depending on the
    instruction set.  Note that __print_symbolic() is designed for exporting
    the formatting table to userspace and allows trace-cmd and perf to work.
    
    Signed-off-by: Stefan Hajnoczi <stefanha@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c00ec28e7147..307e3cfa28ad 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -635,8 +635,6 @@ struct kvm_x86_ops {
 	int (*check_intercept)(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage);
-
-	const struct trace_print_flags *exit_reasons_str;
 };
 
 struct kvm_arch_async_pf {

commit 8c3ba334f8588e1d5099f8602cf01897720e0eca
Author: Sasha Levin <levinsasha928@gmail.com>
Date:   Mon Jul 18 17:17:15 2011 +0300

    KVM: x86: Raise the hard VCPU count limit
    
    The patch raises the hard limit of VCPU count to 254.
    
    This will allow developers to easily work on scalability
    and will allow users to test high VCPU setups easily without
    patching the kernel.
    
    To prevent possible issues with current setups, KVM_CAP_NR_VCPUS
    now returns the recommended VCPU limit (which is still 64) - this
    should be a safe value for everybody, while a new KVM_CAP_MAX_VCPUS
    returns the hard limit which is now 254.
    
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Suggested-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dd51c83aa5de..c00ec28e7147 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -26,7 +26,8 @@
 #include <asm/mtrr.h>
 #include <asm/msr-index.h>
 
-#define KVM_MAX_VCPUS 64
+#define KVM_MAX_VCPUS 254
+#define KVM_SOFT_MAX_VCPUS 64
 #define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4

commit c2a2ac2b563ccc3a69540965b5a994c19e3817d7
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Jul 12 03:32:13 2011 +0800

    KVM: MMU: lockless walking shadow page table
    
    Use rcu to protect shadow pages table to be freed, so we can safely walk it,
    it should run fastly and is needed by mmio page fault
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a198a5b2f04e..dd51c83aa5de 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -233,6 +233,12 @@ struct kvm_mmu_page {
 	unsigned int unsync_children;
 	unsigned long parent_ptes;	/* Reverse mapping for parent_pte */
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
+
+#ifdef CONFIG_X86_32
+	int clear_spte_count;
+#endif
+
+	struct rcu_head rcu;
 };
 
 struct kvm_pv_mmu_op_buffer {
@@ -486,6 +492,8 @@ struct kvm_arch {
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
 
+	atomic_t reader_counter;
+
 	#ifdef CONFIG_KVM_MMU_AUDIT
 	int audit_point;
 	#endif

commit c37079586f317d7e7f1a70d36f0e5177691c89c2
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Jul 12 03:28:04 2011 +0800

    KVM: MMU: remove bypass_guest_pf
    
    The idea is from Avi:
    | Maybe it's time to kill off bypass_guest_pf=1.  It's not as effective as
    | it used to be, since unsync pages always use shadow_trap_nonpresent_pte,
    | and since we convert between the two nonpresent_ptes during sync and unsync.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8da1400ab581..a198a5b2f04e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -266,8 +266,6 @@ struct kvm_mmu {
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    struct x86_exception *exception);
 	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access);
-	void (*prefetch_page)(struct kvm_vcpu *vcpu,
-			      struct kvm_mmu_page *page);
 	int (*sync_page)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
@@ -647,7 +645,6 @@ void kvm_mmu_module_exit(void);
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
 int kvm_mmu_create(struct kvm_vcpu *vcpu);
 int kvm_mmu_setup(struct kvm_vcpu *vcpu);
-void kvm_mmu_set_nonpresent_ptes(u64 trap_pte, u64 notrap_pte);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask);
 

commit bebb106a5afa32efdf5332ed4a40bf4d6d06b56e
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Jul 12 03:23:20 2011 +0800

    KVM: MMU: cache mmio info on page fault path
    
    If the page fault is caused by mmio, we can cache the mmio info, later, we do
    not need to walk guest page table and quickly know it is a mmio fault while we
    emulate the mmio instruction
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 59086a77ff13..8da1400ab581 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -424,6 +424,11 @@ struct kvm_vcpu_arch {
 	u64 mcg_ctl;
 	u64 *mce_banks;
 
+	/* Cache MMIO info */
+	u64 mmio_gva;
+	unsigned access;
+	gfn_t mmio_gfn;
+
 	/* used for guest single stepping over the given code position */
 	unsigned long singlestep_rip;
 

commit c9aaa8957f203bd6df83b002fb40b98390bed078
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Jul 11 15:28:14 2011 -0400

    KVM: Steal time implementation
    
    To implement steal time, we need the hypervisor to pass the guest
    information about how much time was spent running other processes
    outside the VM, while the vcpu had meaningful work to do - halt
    time does not count.
    
    This information is acquired through the run_delay field of
    delayacct/schedstats infrastructure, that counts time spent in a
    runqueue but not running.
    
    Steal time is a per-cpu information, so the traditional MSR-based
    infrastructure is used. A new msr, KVM_MSR_STEAL_TIME, holds the
    memory area address containing information about steal time
    
    This patch contains the hypervisor part of the steal time infrasructure,
    and can be backported independently of the guest portion.
    
    [avi, yongjie: export delayacct_on, to avoid build failures in some configs]
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Tested-by: Eric B Munson <emunson@mgebm.net>
    CC: Rik van Riel <riel@redhat.com>
    CC: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Yongjie Ren <yongjie.ren@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index da6bbee878ca..59086a77ff13 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -389,6 +389,15 @@ struct kvm_vcpu_arch {
 	unsigned int hw_tsc_khz;
 	unsigned int time_offset;
 	struct page *time_page;
+
+	struct {
+		u64 msr_val;
+		u64 last_steal;
+		u64 accum_steal;
+		struct gfn_to_hva_cache stime;
+		struct kvm_steal_time steal;
+	} st;
+
 	u64 last_guest_tsc;
 	u64 last_kernel_ns;
 	u64 last_tsc_nsec;

commit 411c588dfb863feee78b721d5e7c86ac38921c49
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Jun 6 16:11:54 2011 +0300

    KVM: MMU: Adjust shadow paging to work when SMEP=1 and CR0.WP=0
    
    When CR0.WP=0, we sometimes map user pages as kernel pages (to allow
    the kernel to write to them).  Unfortunately this also allows the kernel
    to fetch from these pages, even if CR4.SMEP is set.
    
    Adjust for this by also setting NX on the spte in these circumstances.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 554be456f11e..da6bbee878ca 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -205,6 +205,7 @@ union kvm_mmu_page_role {
 		unsigned invalid:1;
 		unsigned nxe:1;
 		unsigned cr0_wp:1;
+		unsigned smep_andnot_wp:1;
 	};
 };
 

commit d9c3476d8a99455cd3af1bd773acd77aa947a934
Author: Yang, Wei <wei.y.yang@intel.com>
Date:   Tue Jun 14 20:10:17 2011 +0800

    KVM: Remove RDWRGSFS bit from CR4_RESERVED_BITS
    
    This patch removes RDWRGSFS bit from CR4_RESERVED_BITS.
    
    Signed-off-by: Yang, Wei <wei.y.yang@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fc38eca116c0..554be456f11e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -48,7 +48,7 @@
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
 			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR  \
-			  | X86_CR4_OSXSAVE | X86_CR4_SMEP              \
+			  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_RDWRGSFS \
 			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE))
 
 #define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)

commit 8d9c975fc5b825cb76953a1b45a84195ffc6f4ab
Author: Yang, Wei Y <wei.y.yang@intel.com>
Date:   Fri Jun 3 11:13:35 2011 +0800

    KVM: Remove SMEP bit from CR4_RESERVED_BITS
    
    This patch removes SMEP bit from CR4_RESERVED_BITS.
    
    Signed-off-by: Yang, Wei <wei.y.yang@intel.com>
    Signed-off-by: Shan, Haitao <haitao.shan@intel.com>
    Signed-off-by: Li, Xin <xin.li@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d167039ecdf4..fc38eca116c0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -48,7 +48,7 @@
 	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
 			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
 			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR  \
-			  | X86_CR4_OSXSAVE \
+			  | X86_CR4_OSXSAVE | X86_CR4_SMEP              \
 			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE))
 
 #define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)

commit 5e1746d6205d1efa3193cc0c67aa2d15e54799bd
Author: Nadav Har'El <nyh@il.ibm.com>
Date:   Wed May 25 23:03:24 2011 +0300

    KVM: nVMX: Allow setting the VMXE bit in CR4
    
    This patch allows the guest to enable the VMXE bit in CR4, which is a
    prerequisite to running VMXON.
    
    Whether to allow setting the VMXE bit now depends on the architecture (svm
    or vmx), so its checking has moved to kvm_x86_ops->set_cr4(). This function
    now returns an int: If kvm_x86_ops->set_cr4() returns 1, __kvm_set_cr4()
    will also return 1, and this will cause kvm_set_cr4() will throw a #GP.
    
    Turning on the VMXE bit is allowed only when the nested VMX feature is
    enabled, and turning it off is forbidden after a vmxon.
    
    Signed-off-by: Nadav Har'El <nyh@il.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ff17deb6e98b..d167039ecdf4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -555,7 +555,7 @@ struct kvm_x86_ops {
 	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
-	void (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
+	int (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
 	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);
 	void (*get_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);

commit 67052b3508f09956427d6476fd35e8fddde6c618
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Sun May 15 23:27:08 2011 +0800

    KVM: MMU: remove the arithmetic of parent pte rmap
    
    Parent pte rmap and page rmap are very similar, so use the same arithmetic
    for them
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e6a4a57e142b..ff17deb6e98b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -227,14 +227,10 @@ struct kvm_mmu_page {
 	 * in this shadow page.
 	 */
 	DECLARE_BITMAP(slot_bitmap, KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS);
-	bool multimapped;         /* More than one parent_pte? */
 	bool unsync;
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
-	union {
-		u64 *parent_pte;               /* !multimapped */
-		struct hlist_head parent_ptes; /* multimapped, kvm_pte_chain */
-	};
+	unsigned long parent_ptes;	/* Reverse mapping for parent_pte */
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 };
 
@@ -346,7 +342,6 @@ struct kvm_vcpu_arch {
 	 * put it here to avoid allocation */
 	struct kvm_pv_mmu_op_buffer mmu_op_buffer;
 
-	struct kvm_mmu_memory_cache mmu_pte_chain_cache;
 	struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
 	struct kvm_mmu_memory_cache mmu_page_cache;
 	struct kvm_mmu_memory_cache mmu_page_header_cache;

commit 53c07b18787d564a105e1aa678795d67eeb27447
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Sun May 15 23:26:20 2011 +0800

    KVM: MMU: abstract the operation of rmap
    
    Abstract the operation of rmap to spte_list, then we can use it for the
    reverse mapping of parent pte in the later patch
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 387780eb97bb..e6a4a57e142b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -347,7 +347,7 @@ struct kvm_vcpu_arch {
 	struct kvm_pv_mmu_op_buffer mmu_op_buffer;
 
 	struct kvm_mmu_memory_cache mmu_pte_chain_cache;
-	struct kvm_mmu_memory_cache mmu_rmap_desc_cache;
+	struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
 	struct kvm_mmu_memory_cache mmu_page_cache;
 	struct kvm_mmu_memory_cache mmu_page_header_cache;
 

commit 332b207d65c1d7982489dbb83e5071c95e19eb75
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Sun May 15 23:20:27 2011 +0800

    KVM: MMU: optimize pte write path if don't have protected sp
    
    Simply return from kvm_mmu_pte_write path if no shadow page is
    write-protected, then we can avoid to walk all shadow pages and hold
    mmu-lock
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index db4b6543b830..387780eb97bb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -441,6 +441,7 @@ struct kvm_arch {
 	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_max_mmu_pages;
+	unsigned int indirect_shadow_pages;
 	atomic_t invlpg_counter;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*

commit 5e520e62787afd8fc28626fd8d4f77491135119d
Author: Avi Kivity <avi@redhat.com>
Date:   Sun May 15 10:13:12 2011 -0400

    KVM: VMX: Move VMREAD cleanup to exception handler
    
    We clean up a failed VMREAD by clearing the output register.  Do
    it in the exception handler instead of unconditionally.  This is
    worthwhile since there are more than a hundred call sites.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d2ac8e2ee897..db4b6543b830 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -830,11 +830,12 @@ enum {
 asmlinkage void kvm_spurious_fault(void);
 extern bool kvm_rebooting;
 
-#define __kvm_handle_fault_on_reboot(insn) \
+#define ____kvm_handle_fault_on_reboot(insn, cleanup_insn)	\
 	"666: " insn "\n\t" \
 	"668: \n\t"                           \
 	".pushsection .fixup, \"ax\" \n" \
 	"667: \n\t" \
+	cleanup_insn "\n\t"		      \
 	"cmpb $0, kvm_rebooting \n\t"	      \
 	"jne 668b \n\t"      		      \
 	__ASM_SIZE(push) " $666b \n\t"	      \
@@ -844,6 +845,9 @@ extern bool kvm_rebooting;
 	_ASM_PTR " 666b, 667b \n\t" \
 	".popsection"
 
+#define __kvm_handle_fault_on_reboot(insn)		\
+	____kvm_handle_fault_on_reboot(insn, "")
+
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);

commit 2fb92db1ec08f3235c500e7f460eeb78092d844e
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 27 19:42:18 2011 +0300

    KVM: VMX: Cache vmcs segment fields
    
    Since the emulator now checks segment limits and access rights, it
    generates a lot more accesses to the vmcs segment fields.  Undo some
    of the performance hit by cacheing those fields in a read-only cache
    (the entire cache is invalidated on any write, or on guest exit).
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index afb0e69bd160..d2ac8e2ee897 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -136,6 +136,7 @@ enum kvm_reg_ex {
 	VCPU_EXREG_CR3,
 	VCPU_EXREG_RFLAGS,
 	VCPU_EXREG_CPL,
+	VCPU_EXREG_SEGMENTS,
 };
 
 enum {

commit 8d7d810255982bfcc355cdb8972d72843acb0cf8
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 12 12:36:21 2011 +0300

    KVM: mmio_fault_cr2 is not used
    
    Remove unused variable mmio_fault_cr2.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6cfc1ab2cdd6..afb0e69bd160 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -358,7 +358,6 @@ struct kvm_vcpu_arch {
 	struct fpu guest_fpu;
 	u64 xcr0;
 
-	gva_t mmio_fault_cr2;
 	struct kvm_pio_request pio;
 	void *pio_data;
 

commit d6aa10003b0cded5a538af0d198460e89dc2d6d2
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 20 15:47:13 2011 +0300

    KVM: x86 emulator: add ->fix_hypercall() callback
    
    Artificial, but needed to remove direct calls to KVM.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d957d0d06562..6cfc1ab2cdd6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -752,8 +752,6 @@ gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 
-int kvm_fix_hypercall(struct kvm_vcpu *vcpu);
-
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u32 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);

commit 3cb16fe78ce91991a876c74fc5dc99419b737b7a
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 20 15:38:44 2011 +0300

    KVM: x86 emulator: make emulate_invlpg() an emulator callback
    
    Removing direct calls to KVM.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9c3567e0f730..d957d0d06562 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -690,7 +690,6 @@ struct x86_emulate_ctxt;
 int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
 void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
-int emulate_invlpg(struct kvm_vcpu *vcpu, gva_t address);
 int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);

commit 2d04a05bd7e93c13f13a82ac40de4065a99d069b
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 20 15:32:49 2011 +0300

    KVM: x86 emulator: emulate CLTS internally
    
    Avoid using ctxt->vcpu; we can do everything with ->get_cr() and ->set_cr().
    
    A side effect is that we no longer activate the fpu on emulated CLTS; but that
    should be very rare.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a8616ca8320e..9c3567e0f730 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -691,7 +691,6 @@ int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
 void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
 int emulate_invlpg(struct kvm_vcpu *vcpu, gva_t address);
-int emulate_clts(struct kvm_vcpu *vcpu);
 int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);

commit 1ac9d0cfb07e8ac3b5007d8279c5bd56e124250c
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 20 15:12:00 2011 +0300

    KVM: x86 emulator: add and use new callbacks set_idt(), set_gdt()
    
    Replacing direct calls to realmode_lgdt(), realmode_lidt().
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e50bffcf3cc0..a8616ca8320e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -681,9 +681,6 @@ static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 
-void realmode_lgdt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
-void realmode_lidt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
-
 void kvm_enable_efer_bits(u64);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);

commit 7c4c0f4fd5c3e82234c0ab61c7e7ffdb8f3af07b
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Apr 18 11:42:53 2011 +0200

    KVM: X86: Update last_guest_tsc in vcpu_put
    
    The last_guest_tsc is used in vcpu_load to adjust the
    tsc_offset since tsc-scaling is merged. So the
    last_guest_tsc needs to be updated in vcpu_put instead of
    the the last_host_tsc. This is fixed with this patch.
    
    Reported-by: Jan Kiszka <jan.kiszka@web.de>
    Tested-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3e03f37f43ea..e50bffcf3cc0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -393,7 +393,6 @@ struct kvm_vcpu_arch {
 	unsigned int hw_tsc_khz;
 	unsigned int time_offset;
 	struct page *time_page;
-	u64 last_host_tsc;
 	u64 last_guest_tsc;
 	u64 last_kernel_ns;
 	u64 last_tsc_nsec;

commit 7ae441eac521b2006c9f03c4f2a23582c07fd76d
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 31 12:06:41 2011 +0200

    KVM: emulator: do not needlesly sync registers from emulator ctxt to vcpu
    
    Currently we sync registers back and forth before/after exiting
    to userspace for IO, but during IO device model shouldn't need to
    read/write the registers, so we can as well skip those sync points. The
    only exaception is broken vmware backdor interface. The new code sync
    registers content during IO only if registers are read from/written to
    by userspace in the middle of the IO operation and this almost never
    happens in practise.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bd57639fd5db..3e03f37f43ea 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -385,6 +385,8 @@ struct kvm_vcpu_arch {
 	/* emulate context */
 
 	struct x86_emulate_ctxt emulate_ctxt;
+	bool emulate_regs_need_sync_to_vcpu;
+	bool emulate_regs_need_sync_from_vcpu;
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;

commit 92a1f12d2598f429bd8639e21d89305e787115c5
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Mar 25 09:44:51 2011 +0100

    KVM: X86: Implement userspace interface to set virtual_tsc_khz
    
    This patch implements two new vm-ioctls to get and set the
    virtual_tsc_khz if the machine supports tsc-scaling. Setting
    the tsc-frequency is only possible before userspace creates
    any vcpu.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index da0a8ce3a139..bd57639fd5db 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -655,6 +655,13 @@ u8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 extern bool tdp_enabled;
 
+/* control of guest tsc rate supported? */
+extern bool kvm_has_tsc_control;
+/* minimum supported tsc_khz for guests */
+extern u32  kvm_min_guest_tsc_khz;
+/* maximum supported tsc_khz for guests */
+extern u32  kvm_max_guest_tsc_khz;
+
 enum emulation_result {
 	EMULATE_DONE,       /* no further processing */
 	EMULATE_DO_MMIO,      /* kvm_run filled with mmio request */

commit 857e40999e35906baa367a79137019912cfb5434
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Mar 25 09:44:50 2011 +0100

    KVM: X86: Delegate tsc-offset calculation to architecture code
    
    With TSC scaling in SVM the tsc-offset needs to be
    calculated differently. This patch propagates this
    calculation into the architecture specific modules so that
    this complexity can be handled there.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f3a7116f802f..da0a8ce3a139 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -609,6 +609,8 @@ struct kvm_x86_ops {
 	void (*set_tsc_khz)(struct kvm_vcpu *vcpu, u32 user_tsc_khz);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
+	u64 (*compute_tsc_offset)(struct kvm_vcpu *vcpu, u64 target_tsc);
+
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
 
 	int (*check_intercept)(struct kvm_vcpu *vcpu,

commit 4051b18801f5b47bb0369feefdc80e57819d0ddf
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Mar 25 09:44:49 2011 +0100

    KVM: X86: Implement call-back to propagate virtual_tsc_khz
    
    This patch implements a call-back into the architecture code
    to allow the propagation of changes to the virtual tsc_khz
    of the vcpu.
    On SVM it updates the tsc_ratio variable, on VMX it does
    nothing.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e3aaa02ca032..f3a7116f802f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -606,6 +606,7 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
+	void (*set_tsc_khz)(struct kvm_vcpu *vcpu, u32 user_tsc_khz);
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);

commit 1e993611d0dc879fde25515dc9867d1cfd4c5137
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Mar 25 09:44:47 2011 +0100

    KVM: X86: Let kvm-clock report the right tsc frequency
    
    This patch changes the kvm_guest_time_update function to use
    TSC frequency the guest actually has for updating its clock.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ecdc562ea3e2..e3aaa02ca032 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -396,7 +396,10 @@ struct kvm_vcpu_arch {
 	u64 last_kernel_ns;
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
+	u32 virtual_tsc_khz;
 	bool tsc_catchup;
+	u32  tsc_catchup_mult;
+	s8   tsc_catchup_shift;
 
 	bool nmi_pending;
 	bool nmi_injected;
@@ -466,9 +469,6 @@ struct kvm_arch {
 	u64 last_tsc_nsec;
 	u64 last_tsc_offset;
 	u64 last_tsc_write;
-	u32 virtual_tsc_khz;
-	u32 virtual_tsc_mult;
-	s8 virtual_tsc_shift;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 

commit 7c5625227ff8c81953e953d8e25c3eba2ab0aeb3
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Mon Mar 28 10:29:27 2011 +0800

    KVM: MMU: remove mmu_seq verification on pte update path
    
    The mmu_seq verification can be removed since we get the pfn in the
    protection of mmu_lock.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f7dfd6479d02..ecdc562ea3e2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -274,7 +274,7 @@ struct kvm_mmu {
 			 struct kvm_mmu_page *sp);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
-			u64 *spte, const void *pte, unsigned long mmu_seq);
+			   u64 *spte, const void *pte);
 	hpa_t root_hpa;
 	int root_level;
 	int shadow_root_level;

commit cfec82cb7d313ae5b2c2dbb974401d7c214c7b09
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Apr 4 12:39:28 2011 +0200

    KVM: SVM: Add intercept check for emulated cr accesses
    
    This patch adds all necessary intercept checks for
    instructions that access the crX registers.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 038562c222e8..f7dfd6479d02 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -35,10 +35,25 @@
 #define KVM_PIO_PAGE_OFFSET 1
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 2
 
+#define CR0_RESERVED_BITS                                               \
+	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
+			  | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM \
+			  | X86_CR0_NW | X86_CR0_CD | X86_CR0_PG))
+
 #define CR3_PAE_RESERVED_BITS ((X86_CR3_PWT | X86_CR3_PCD) - 1)
 #define CR3_NONPAE_RESERVED_BITS ((PAGE_SIZE-1) & ~(X86_CR3_PWT | X86_CR3_PCD))
 #define CR3_L_MODE_RESERVED_BITS (CR3_NONPAE_RESERVED_BITS |	\
 				  0xFFFFFF0000000000ULL)
+#define CR4_RESERVED_BITS                                               \
+	(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\
+			  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \
+			  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR  \
+			  | X86_CR4_OSXSAVE \
+			  | X86_CR4_OSXMMEXCPT | X86_CR4_VMXE))
+
+#define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)
+
+
 
 #define INVALID_PAGE (~(hpa_t)0)
 #define VALID_PAGE(x) ((x) != INVALID_PAGE)

commit 8a76d7f25f8f24fc5a328c8e15e4a7313cf141b9
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Apr 4 12:39:27 2011 +0200

    KVM: x86: Add x86 callback for intercept check
    
    This patch adds a callback into kvm_x86_ops so that svm and
    vmx code can do intercept checks on emulated instructions.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e820c6339b8b..038562c222e8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -505,6 +505,8 @@ struct kvm_vcpu_stat {
 	u32 nmi_injections;
 };
 
+struct x86_instruction_info;
+
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
@@ -592,6 +594,11 @@ struct kvm_x86_ops {
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
 	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
+
+	int (*check_intercept)(struct kvm_vcpu *vcpu,
+			       struct x86_instruction_info *info,
+			       enum x86_intercept_stage stage);
+
 	const struct trace_print_flags *exit_reasons_str;
 };
 

commit cef4dea07f6720b36cc93e18a2e68be4bdb71a92
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Jan 20 12:01:20 2010 +0200

    KVM: 16-byte mmio support
    
    Since sse instructions can issue 16-byte mmios, we need to support them.  We
    can't increase the kvm_run mmio buffer size to 16 bytes without breaking
    compatibility, so instead we break the large mmios into two smaller 8-byte
    ones.  Since the bus is 64-bit we aren't breaking any atomicity guarantees.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 35f81b110260..e820c6339b8b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -30,6 +30,7 @@
 #define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
+#define KVM_MMIO_SIZE 16
 
 #define KVM_PIO_PAGE_OFFSET 1
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 2

commit 69c730289011df706a1c9890d6e6c5ee822623c7
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Mar 7 15:26:44 2011 +0200

    KVM: VMX: Cache cpl
    
    We may read the cpl quite often in the same vmexit (instruction privilege
    check, memory access checks for instruction and operands), so we gain
    a bit if we cache the value.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5af426464954..35f81b110260 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -119,6 +119,7 @@ enum kvm_reg_ex {
 	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
 	VCPU_EXREG_CR3,
 	VCPU_EXREG_RFLAGS,
+	VCPU_EXREG_CPL,
 };
 
 enum {

commit 6de12732c42c7070af42e3d6e42ecee2838fc920
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Mar 7 12:51:22 2011 +0200

    KVM: VMX: Optimize vmx_get_rflags()
    
    If called several times within the same exit, return cached results.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c8af0991fdf0..5af426464954 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -118,6 +118,7 @@ enum kvm_reg {
 enum kvm_reg_ex {
 	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
 	VCPU_EXREG_CR3,
+	VCPU_EXREG_RFLAGS,
 };
 
 enum {

commit 0f53b5b1c0baae4f949ac0721a55b7a2158dda01
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Wed Mar 9 15:43:51 2011 +0800

    KVM: MMU: cleanup pte write path
    
    This patch does:
    - call vcpu->arch.mmu.update_pte directly
    - use gfn_to_pfn_atomic in update_pte path
    
    The suggestion is from Avi.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f08314f303e0..c8af0991fdf0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -255,6 +255,8 @@ struct kvm_mmu {
 	int (*sync_page)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
+	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
+			u64 *spte, const void *pte, unsigned long mmu_seq);
 	hpa_t root_hpa;
 	int root_level;
 	int shadow_root_level;
@@ -335,11 +337,6 @@ struct kvm_vcpu_arch {
 	u64  *last_pte_updated;
 	gfn_t last_pte_gfn;
 
-	struct {
-		pfn_t pfn;	/* pfn corresponding to that gfn */
-		unsigned long mmu_seq;
-	} update_pte;
-
 	struct fpu guest_fpu;
 	u64 xcr0;
 

commit 49b26e26e4b7b94753b39f7edb0c34f3d1c4c167
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Fri Mar 4 19:00:00 2011 +0800

    KVM: MMU: do not record gfn in kvm_mmu_pte_write
    
    No need to record the gfn to verifier the pte has the same mode as
    current vcpu, it's because we only speculatively update the pte only
    if the pte and vcpu have the same mode
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 37bd730ff852..f08314f303e0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -336,7 +336,6 @@ struct kvm_vcpu_arch {
 	gfn_t last_pte_gfn;
 
 	struct {
-		gfn_t gfn;	/* presumed gfn during guest pte update */
 		pfn_t pfn;	/* pfn corresponding to that gfn */
 		unsigned long mmu_seq;
 	} update_pte;

commit 038f8c110eace38d7598e271835ae96ad04a3a26
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Fri Feb 4 10:49:11 2011 +0100

    KVM: x86: Convert tsc_write_lock to raw_spinlock
    
    Code under this lock requires non-preemptibility. Ensure this also over
    -rt by converting it to raw spinlock.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a58aebef5188..37bd730ff852 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -448,7 +448,7 @@ struct kvm_arch {
 
 	unsigned long irq_sources_bitmap;
 	s64 kvmclock_offset;
-	spinlock_t tsc_write_lock;
+	raw_spinlock_t tsc_write_lock;
 	u64 last_tsc_nsec;
 	u64 last_tsc_offset;
 	u64 last_tsc_write;

commit e935b8372cf8c63dc618a9f2b24ab360a225f1cd
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Feb 8 12:55:33 2011 +0100

    KVM: Convert kvm_lock to raw_spinlock
    
    Code under this lock requires non-preemptibility. Ensure this also over
    -rt by converting it to raw spinlock.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ffd7f8d29187..a58aebef5188 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -85,7 +85,7 @@
 
 #define ASYNC_PF_PER_VCPU 64
 
-extern spinlock_t kvm_lock;
+extern raw_spinlock_t kvm_lock;
 extern struct list_head vm_list;
 
 struct kvm_vcpu;

commit 8ee53820edfd1f3b6554c593f337148dd3d7fc91
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:47:10 2011 -0800

    thp: mmu_notifier_test_young
    
    For GRU and EPT, we need gup-fast to set referenced bit too (this is why
    it's correct to return 0 when shadow_access_mask is zero, it requires
    gup-fast to set the referenced bit).  qemu-kvm access already sets the
    young bit in the pte if it isn't zero-copy, if it's zero copy or a shadow
    paging EPT minor fault we relay on gup-fast to signal the page is in
    use...
    
    We also need to check the young bits on the secondary pagetables for NPT
    and not nested shadow mmu as the data may never get accessed again by the
    primary pte.
    
    Without this closer accuracy, we'd have to remove the heuristic that
    avoids collapsing hugepages in hugepage virtual regions that have not even
    a single subpage in use.
    
    ->test_young is full backwards compatible with GRU and other usages that
    don't have young bits in pagetables set by the hardware and that should
    nuke the secondary mmu mappings when ->clear_flush_young runs just like
    EPT does.
    
    Removing the heuristic that checks the young bit in
    khugepaged/collapse_huge_page completely isn't so bad either probably but
    I thought it was worth it and this makes it reliable.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aa75f21a9fba..ffd7f8d29187 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -822,6 +822,7 @@ extern bool kvm_rebooting;
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);
+int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);

commit b034cf0105235e65ee1b0161dbe8fef0338d06e7
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Dec 23 16:08:35 2010 +0800

    KVM: MMU: audit: allow audit more guests at the same time
    
    It only allows to audit one guest in the system since:
    - 'audit_point' is a glob variable
    - mmu_audit_disable() is called in kvm_mmu_destroy(), so audit is disabled
      after a guest exited
    
    this patch fix those issues then allow to audit more guests at the same time
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 95f026be8b5e..aa75f21a9fba 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -461,6 +461,10 @@ struct kvm_arch {
 	/* fields used by HYPER-V emulation */
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
+
+	#ifdef CONFIG_KVM_MMU_AUDIT
+	int audit_point;
+	#endif
 };
 
 struct kvm_vm_stat {

commit aff48baa34c033318ad322ecbf2e4bcd891b29ca
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Dec 5 18:56:11 2010 +0200

    KVM: Fetch guest cr3 from hardware on demand
    
    Instead of syncing the guest cr3 every exit, which is expensince on vmx
    with ept enabled, sync it only on demand.
    
    [sheng: fix incorrect cr3 seen by Windows XP]
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6268f6ce6434..95f026be8b5e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -117,6 +117,7 @@ enum kvm_reg {
 
 enum kvm_reg_ex {
 	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
+	VCPU_EXREG_CR3,
 };
 
 enum {
@@ -533,6 +534,7 @@ struct kvm_x86_ops {
 			    struct kvm_segment *var, int seg);
 	void (*get_cs_db_l_bits)(struct kvm_vcpu *vcpu, int *db, int *l);
 	void (*decache_cr0_guest_bits)(struct kvm_vcpu *vcpu);
+	void (*decache_cr3)(struct kvm_vcpu *vcpu);
 	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);

commit dc25e89e07d5ef31c476117d2c76b34dbb22196c
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Tue Dec 21 11:12:07 2010 +0100

    KVM: SVM: copy instruction bytes from VMCB
    
    In case of a nested page fault or an intercepted #PF newer SVM
    implementations provide a copy of the faulting instruction bytes
    in the VMCB.
    Use these bytes to feed the instruction emulator and avoid the costly
    guest instruction fetch in this case.
    
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index de00b6026b76..6268f6ce6434 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -634,13 +634,13 @@ enum emulation_result {
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
-int x86_emulate_instruction(struct kvm_vcpu *vcpu,
-			unsigned long cr2, int emulation_type);
+int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
+			    int emulation_type, void *insn, int insn_len);
 
 static inline int emulate_instruction(struct kvm_vcpu *vcpu,
 			int emulation_type)
 {
-	return x86_emulate_instruction(vcpu, 0, emulation_type);
+	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 
 void realmode_lgdt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
@@ -721,7 +721,8 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 
 int kvm_fix_hypercall(struct kvm_vcpu *vcpu);
 
-int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u32 error_code);
+int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u32 error_code,
+		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 
 void kvm_enable_tdp(void);

commit 51d8b66199e94284e7725a79eae4a38de4b80d54
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Tue Dec 21 11:12:02 2010 +0100

    KVM: cleanup emulate_instruction
    
    emulate_instruction had many callers, but only one used all
    parameters. One parameter was unused, another one is now
    hidden by a wrapper function (required for a future addition
    anyway), so most callers use now a shorter parameter list.
    
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cd4a990e8a12..de00b6026b76 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -634,8 +634,15 @@ enum emulation_result {
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
-int emulate_instruction(struct kvm_vcpu *vcpu,
-			unsigned long cr2, u16 error_code, int emulation_type);
+int x86_emulate_instruction(struct kvm_vcpu *vcpu,
+			unsigned long cr2, int emulation_type);
+
+static inline int emulate_instruction(struct kvm_vcpu *vcpu,
+			int emulation_type)
+{
+	return x86_emulate_instruction(vcpu, 0, emulation_type);
+}
+
 void realmode_lgdt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
 void realmode_lidt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
 

commit db8fcefaa704ccb40b6dcd24e3b75bad3ce7dde3
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Tue Dec 21 11:12:01 2010 +0100

    KVM: move complete_insn_gp() into x86.c
    
    move the complete_insn_gp() helper function out of the VMX part
    into the generic x86 part to make it usable by SVM.
    
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cb5cad2f2d46..cd4a990e8a12 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -828,4 +828,6 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
+void kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit eea1cff9ab732ea56358ff5e1bd8b99db2e8402d
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Tue Dec 21 11:12:00 2010 +0100

    KVM: x86: fix CR8 handling
    
    The handling of CR8 writes in KVM is currently somewhat cumbersome.
    This patch makes it look like the other CR register handlers
    and fixes a possible issue in VMX, where the RIP would be incremented
    despite an injected #GP.
    
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4461429957a9..cb5cad2f2d46 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -661,7 +661,7 @@ int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason,
 int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 int kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
 int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
-void kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);
+int kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);
 int kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val);
 int kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val);
 unsigned long kvm_get_cr8(struct kvm_vcpu *vcpu);

commit fb67e14fc90f18250259faf61a269320ea8e4d8f
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Dec 7 10:35:25 2010 +0800

    KVM: MMU: retry #PF for softmmu
    
    Retry #PF for softmmu only when the current vcpu has the same cr3 as the time
    when #PF occurs
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aa1518d794cc..4461429957a9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -593,6 +593,7 @@ struct kvm_x86_ops {
 struct kvm_arch_async_pf {
 	u32 token;
 	gfn_t gfn;
+	unsigned long cr3;
 	bool direct_map;
 };
 

commit 78b2c54aa4a7e9e4257d2b8e3a4b96d2d0c6e636
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Dec 7 10:48:06 2010 +0800

    KVM: MMU: rename 'no_apf' to 'prefault'
    
    It's the speculative path if 'no_apf = 1' and we will specially handle this
    speculative path in the later patch, so 'prefault' is better to fit the sense.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d968cc501799..aa1518d794cc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -241,7 +241,8 @@ struct kvm_mmu {
 	void (*new_cr3)(struct kvm_vcpu *vcpu);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
-	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err, bool no_apf);
+	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err,
+			  bool prefault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
 	void (*free)(struct kvm_vcpu *vcpu);

commit b7c4145ba2eb0717db0ddac1b5f7f48012189c53
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Dec 2 17:52:50 2010 +0200

    KVM: Don't spin on virt instruction faults during reboot
    
    Since vmx blocks INIT signals, we disable virtualization extensions during
    reboot.  This leads to virtualization instructions faulting; we trap these
    faults and spin while the reboot continues.
    
    Unfortunately spinning on a non-preemptible kernel may block a task that
    reboot depends on; this causes the reboot to hang.
    
    Fix by skipping over the instruction and hoping for the best.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 56e45a2ed2de..d968cc501799 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -786,14 +786,18 @@ enum {
  * reboot turns off virtualization while processes are running.
  * Trap the fault and ignore the instruction if that happens.
  */
-asmlinkage void kvm_handle_fault_on_reboot(void);
+asmlinkage void kvm_spurious_fault(void);
+extern bool kvm_rebooting;
 
 #define __kvm_handle_fault_on_reboot(insn) \
 	"666: " insn "\n\t" \
+	"668: \n\t"                           \
 	".pushsection .fixup, \"ax\" \n" \
 	"667: \n\t" \
+	"cmpb $0, kvm_rebooting \n\t"	      \
+	"jne 668b \n\t"      		      \
 	__ASM_SIZE(push) " $666b \n\t"	      \
-	"jmp kvm_handle_fault_on_reboot \n\t" \
+	"call kvm_spurious_fault \n\t"	      \
 	".popsection \n\t" \
 	".pushsection __ex_table, \"a\" \n\t" \
 	_ASM_PTR " 666b, 667b \n\t" \

commit ec9e60b21977007e3dfacc2b8fe3a8fbb9276b51
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Nov 29 17:51:47 2010 +0100

    KVM: X86: Introduce generic guest-mode representation
    
    This patch introduces a generic representation of guest-mode
    fpr a vcpu. This currently only exists in the SVM code.
    Having this representation generic will help making the
    non-svm code aware of nesting when this is necessary.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0c0941db31c4..56e45a2ed2de 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -779,6 +779,7 @@ enum {
 #define HF_VINTR_MASK		(1 << 2)
 #define HF_NMI_MASK		(1 << 3)
 #define HF_IRET_MASK		(1 << 4)
+#define HF_GUEST_MASK		(1 << 5) /* VCPU is in guest-mode */
 
 /*
  * Hardware virtualization extension instructions may fault if a

commit 6389ee946303cb4313dba0a49865e495a53351ff
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Nov 29 16:12:30 2010 +0200

    KVM: Pull extra page fault information into struct x86_exception
    
    Currently page fault cr2 and nesting infomation are carried outside
    the fault data structure.  Instead they are placed in the vcpu struct,
    which results in confusion as global variables are manipulated instead
    of passing parameters.
    
    Fix this issue by adding address and nested fields to struct x86_exception,
    so this struct can carry all information associated with a fault.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Tested-by: Joerg Roedel <joerg.roedel@amd.com>
    Tested-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9980a2484624..0c0941db31c4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -242,7 +242,8 @@ struct kvm_mmu {
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err, bool no_apf);
-	void (*inject_page_fault)(struct kvm_vcpu *vcpu);
+	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
+				  struct x86_exception *fault);
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    struct x86_exception *exception);
@@ -318,16 +319,6 @@ struct kvm_vcpu_arch {
 	 */
 	struct kvm_mmu *walk_mmu;
 
-	/*
-	 * This struct is filled with the necessary information to propagate a
-	 * page fault into the guest
-	 */
-	struct {
-		u64      address;
-		unsigned error_code;
-		bool     nested;
-	} fault;
-
 	/* only needed in kvm_pv_mmu_op() path, but it's hot so
 	 * put it here to avoid allocation */
 	struct kvm_pv_mmu_op_buffer mmu_op_buffer;
@@ -686,11 +677,11 @@ void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
-void kvm_inject_page_fault(struct kvm_vcpu *vcpu);
+void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);
 int kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gfn_t gfn, void *data, int offset, int len,
 			    u32 access);
-void kvm_propagate_fault(struct kvm_vcpu *vcpu);
+void kvm_propagate_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
 int kvm_pic_set_irq(void *opaque, int irq, int level);

commit ab9ae3138789afacd133a9c4b3d7a3f1578e25c7
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Nov 22 17:53:26 2010 +0200

    KVM: Push struct x86_exception info the various gva_to_gpa variants
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 14524781de13..9980a2484624 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -245,7 +245,7 @@ struct kvm_mmu {
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu);
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
-			    u32 *error);
+			    struct x86_exception *exception);
 	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access);
 	void (*prefetch_page)(struct kvm_vcpu *vcpu,
 			      struct kvm_mmu_page *page);
@@ -708,10 +708,14 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
-gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
-gpa_t kvm_mmu_gva_to_gpa_fetch(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
-gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
-gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
+gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,
+			      struct x86_exception *exception);
+gpa_t kvm_mmu_gva_to_gpa_fetch(struct kvm_vcpu *vcpu, gva_t gva,
+			       struct x86_exception *exception);
+gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,
+			       struct x86_exception *exception);
+gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
+				struct x86_exception *exception);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 

commit a4a8e6f76ecf963fa7e4d74b3635655a2033a27b
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Fri Nov 19 17:04:03 2010 +0800

    KVM: MMU: remove 'clear_unsync' parameter
    
    Remove it since we can judge it by using sp->unsync
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3cc80c478003..14524781de13 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -250,7 +250,7 @@ struct kvm_mmu {
 	void (*prefetch_page)(struct kvm_vcpu *vcpu,
 			      struct kvm_mmu_page *page);
 	int (*sync_page)(struct kvm_vcpu *vcpu,
-			 struct kvm_mmu_page *sp, bool clear_unsync);
+			 struct kvm_mmu_page *sp);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
 	hpa_t root_hpa;
 	int root_level;

commit 586f9607962cd982293759a4e95ff06e75be5225
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Nov 18 13:09:54 2010 +0200

    KVM: Add instruction-set-specific exit qualifications to kvm_exit trace
    
    The exit reason alone is insufficient to understand exactly why an exit
    occured; add ISA-specific trace parameters for additional information.
    
    Because fetching these parameters is expensive on vmx, and because these
    parameters are fetched even if tracing is disabled, we fetch the
    parameters via a callback instead of as traditional trace arguments.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f1e8d5b99f5d..3cc80c478003 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -594,6 +594,7 @@ struct kvm_x86_ops {
 
 	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
 
+	void (*get_exit_info)(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2);
 	const struct trace_print_flags *exit_reasons_str;
 };
 

commit c4806acdcec020fe5bbb054ce9dc75aaecaf29dd
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Fri Nov 12 14:49:55 2010 +0800

    KVM: MMU: fix apf prefault if nested guest is enabled
    
    If apf is generated in L2 guest and is completed in L1 guest, it will
    prefault this apf in L1 guest's mmu context.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 116dac5e01d6..f1e8d5b99f5d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -600,6 +600,7 @@ struct kvm_x86_ops {
 struct kvm_arch_async_pf {
 	u32 token;
 	gfn_t gfn;
+	bool direct_map;
 };
 
 extern struct kvm_x86_ops *kvm_x86_ops;

commit 2a126faafb840e9a1e46514127cdb88ed998bd64
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Nov 4 18:29:42 2010 +0800

    KVM: remove unused function declaration
    
    Remove the declaration of kvm_mmu_set_base_ptes()
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b2ea42870e47..116dac5e01d6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -611,7 +611,6 @@ void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
 int kvm_mmu_create(struct kvm_vcpu *vcpu);
 int kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_set_nonpresent_ptes(u64 trap_pte, u64 notrap_pte);
-void kvm_mmu_set_base_ptes(u64 base_pte);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask);
 

commit 6adba527420651b6cacaf392541c09fb108711a2
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:55 2010 +0200

    KVM: Let host know whether the guest can handle async PF in non-userspace context.
    
    If guest can detect that it runs in non-preemptable context it can
    handle async PFs at any time, so let host know that it can send async
    PF even if guest cpu is not in userspace.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 167375cc49ff..b2ea42870e47 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -422,6 +422,7 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache data;
 		u64 msr_val;
 		u32 id;
+		bool send_user_only;
 	} apf;
 };
 

commit 7c90705bf2a373aa238661bdb6446f27299ef489
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:53 2010 +0200

    KVM: Inject asynchronous page fault into a PV guest if page is swapped out.
    
    Send async page fault to a PV guest if it accesses swapped out memory.
    Guest will choose another task to run upon receiving the fault.
    
    Allow async page fault injection only when guest is in user mode since
    otherwise guest may be in non-sleepable context and will not be able
    to reschedule.
    
    Vcpu will be halted if guest will fault on the same page again or if
    vcpu executes kernel code.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0d7039804b4c..167375cc49ff 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -421,6 +421,7 @@ struct kvm_vcpu_arch {
 		gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
 		struct gfn_to_hva_cache data;
 		u64 msr_val;
+		u32 id;
 	} apf;
 };
 
@@ -596,6 +597,7 @@ struct kvm_x86_ops {
 };
 
 struct kvm_arch_async_pf {
+	u32 token;
 	gfn_t gfn;
 };
 
@@ -819,6 +821,7 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 			       struct kvm_async_pf *work);
+bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit 344d9588a9df06182684168be4f1408b55c7da3e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:50 2010 +0200

    KVM: Add PV MSR to enable asynchronous page faults delivery.
    
    Guest enables async PF vcpu functionality using this MSR.
    
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c3076bcf5ef7..0d7039804b4c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -419,6 +419,8 @@ struct kvm_vcpu_arch {
 	struct {
 		bool halted;
 		gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
+		struct gfn_to_hva_cache data;
+		u64 msr_val;
 	} apf;
 };
 

commit 56028d0861e48f7cc9c573d79f2d8a0a933a2bba
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Oct 17 18:13:42 2010 +0200

    KVM: Retry fault before vmentry
    
    When page is swapped in it is mapped into guest memory only after guest
    tries to access it again and generate another fault. To save this fault
    we can map it immediately since we know that guest is going to access
    the page. Do it only when tdp is enabled for now. Shadow paging case is
    more complicated. CR[034] and EFER registers should be switched before
    doing mapping and then switched back.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b5f4c1a36d65..c3076bcf5ef7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -241,7 +241,7 @@ struct kvm_mmu {
 	void (*new_cr3)(struct kvm_vcpu *vcpu);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
-	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err);
+	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err, bool no_apf);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu);
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
@@ -815,6 +815,8 @@ void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work);
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);
+void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
+			       struct kvm_async_pf *work);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit af585b921e5d1e919947c4b1164b59507fe7cd7b
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:46 2010 +0200

    KVM: Halt vcpu if page it tries to access is swapped out
    
    If a guest accesses swapped out memory do not swap it in from vcpu thread
    context. Schedule work to do swapping and put vcpu into halted state
    instead.
    
    Interrupts will still be delivered to the guest and if interrupt will
    cause reschedule guest will continue to run another task.
    
    [avi: remove call to get_user_pages_noio(), nacked by Linus; this
          makes everything synchrnous again]
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f702f82aa1eb..b5f4c1a36d65 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -83,11 +83,14 @@
 #define KVM_NR_FIXED_MTRR_REGION 88
 #define KVM_NR_VAR_MTRR 8
 
+#define ASYNC_PF_PER_VCPU 64
+
 extern spinlock_t kvm_lock;
 extern struct list_head vm_list;
 
 struct kvm_vcpu;
 struct kvm;
+struct kvm_async_pf;
 
 enum kvm_reg {
 	VCPU_REGS_RAX = 0,
@@ -412,6 +415,11 @@ struct kvm_vcpu_arch {
 	u64 hv_vapic;
 
 	cpumask_var_t wbinvd_dirty_mask;
+
+	struct {
+		bool halted;
+		gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
+	} apf;
 };
 
 struct kvm_arch {
@@ -585,6 +593,10 @@ struct kvm_x86_ops {
 	const struct trace_print_flags *exit_reasons_str;
 };
 
+struct kvm_arch_async_pf {
+	gfn_t gfn;
+};
+
 extern struct kvm_x86_ops *kvm_x86_ops;
 
 int kvm_mmu_module_init(void);
@@ -799,4 +811,10 @@ void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
 bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
 
+void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
+				     struct kvm_async_pf *work);
+void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
+				 struct kvm_async_pf *work);
+extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit 73c1160ce377d8fc6d84cb630ebf9658808bec49
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Wed Dec 1 12:17:44 2010 +0100

    KVM: enlarge number of possible CPUID leaves
    
    Currently the number of CPUID leaves KVM handles is limited to 40.
    My desktop machine (AthlonII) already has 35 and future CPUs will
    expand this well beyond the limit. Extend the limit to 80 to make
    room for future processors.
    
    KVM-Stable-Tag.
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9e6fe391094e..f702f82aa1eb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -79,7 +79,7 @@
 #define KVM_NUM_MMU_PAGES (1 << KVM_MMU_HASH_SHIFT)
 #define KVM_MIN_FREE_MMU_PAGES 5
 #define KVM_REFILL_PAGES 25
-#define KVM_MAX_CPUID_ENTRIES 40
+#define KVM_MAX_CPUID_ENTRIES 80
 #define KVM_NR_FIXED_MTRR_REGION 88
 #define KVM_NR_VAR_MTRR 8
 

commit c285545f813d7b0ce989fd34e42ad1fe785dc65d
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Sat Sep 18 14:38:15 2010 -1000

    KVM: x86: TSC catchup mode
    
    Negate the effects of AN TYM spell while kvm thread is preempted by tracking
    conversion factor to the highest TSC rate and catching the TSC up when it has
    fallen behind the kernel view of time.  Note that once triggered, we don't
    turn off catchup mode.
    
    A slightly more clever version of this is possible, which only does catchup
    when TSC rate drops, and which specifically targets only CPUs with broken
    TSC, but since these all are considered unstable_tsc(), this patch covers
    all necessary cases.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 519d6f784984..9e6fe391094e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -384,6 +384,9 @@ struct kvm_vcpu_arch {
 	u64 last_host_tsc;
 	u64 last_guest_tsc;
 	u64 last_kernel_ns;
+	u64 last_tsc_nsec;
+	u64 last_tsc_write;
+	bool tsc_catchup;
 
 	bool nmi_pending;
 	bool nmi_injected;
@@ -444,6 +447,9 @@ struct kvm_arch {
 	u64 last_tsc_nsec;
 	u64 last_tsc_offset;
 	u64 last_tsc_write;
+	u32 virtual_tsc_khz;
+	u32 virtual_tsc_mult;
+	s8 virtual_tsc_shift;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 

commit 0959ffacf39b1ae7f56072b0c64429ee528100ca
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Tue Sep 14 17:46:12 2010 +0200

    KVM: MMU: Don't track nested fault info in error-code
    
    This patch moves the detection whether a page-fault was
    nested or not out of the error code and moves it into a
    separate variable in the fault struct.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 80224bf5d4f8..519d6f784984 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -322,6 +322,7 @@ struct kvm_vcpu_arch {
 	struct {
 		u64      address;
 		unsigned error_code;
+		bool     nested;
 	} fault;
 
 	/* only needed in kvm_pv_mmu_op() path, but it's hot so

commit b463a6f744a263fccd7da14db1afdc880371a280
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Jul 20 15:06:17 2010 +0300

    KVM: Non-atomic interrupt injection
    
    Change the interrupt injection code to work from preemptible, interrupts
    enabled context.  This works by adding a ->cancel_injection() operation
    that undoes an injection in case we were not able to actually enter the guest
    (this condition could never happen with atomic injection).
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b43686a44877..80224bf5d4f8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -552,6 +552,7 @@ struct kvm_x86_ops {
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code,
 				bool reinject);
+	void (*cancel_injection)(struct kvm_vcpu *vcpu);
 	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
 	int (*nmi_allowed)(struct kvm_vcpu *vcpu);
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);

commit 2d48a985c7bbcd72b4e92e301ea96bf1252ffc61
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:31:01 2010 +0200

    KVM: MMU: Track NX state in struct kvm_mmu
    
    With Nested Paging emulation the NX state between the two
    MMU contexts may differ. To make sure that always the right
    fault error code is recorded this patch moves the NX state
    into struct kvm_mmu so that the code can distinguish between
    L1 and L2 NX state.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bd59b482f1a8..b43686a44877 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -259,6 +259,8 @@ struct kvm_mmu {
 	u64 *lm_root;
 	u64 rsvd_bits_mask[2][4];
 
+	bool nx;
+
 	u64 pdptrs[4]; /* pae */
 };
 

commit 81407ca553c0c852b8cd3f38f3ec362d307f829b
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:31:00 2010 +0200

    KVM: MMU: Allow long mode shadows for legacy page tables
    
    Currently the KVM softmmu implementation can not shadow a 32
    bit legacy or PAE page table with a long mode page table.
    This is a required feature for nested paging emulation
    because the nested page table must alway be in host format.
    So this patch implements the missing pieces to allow long
    mode page tables for page table types.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9e70de376544..bd59b482f1a8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -256,6 +256,7 @@ struct kvm_mmu {
 	bool direct_map;
 
 	u64 *pae_root;
+	u64 *lm_root;
 	u64 rsvd_bits_mask[2][4];
 
 	u64 pdptrs[4]; /* pae */

commit ff03a073e715d49b5cfeeec862649b1df2481ae0
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:57 2010 +0200

    KVM: MMU: Add kvm_mmu parameter to load_pdptrs function
    
    This function need to be able to load the pdptrs from any
    mmu context currently in use. So change this function to
    take an kvm_mmu parameter to fit these needs.
    As a side effect this patch also moves the cached pdptrs
    from vcpu_arch into the kvm_mmu struct.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 574db6d1532a..9e70de376544 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -257,6 +257,8 @@ struct kvm_mmu {
 
 	u64 *pae_root;
 	u64 rsvd_bits_mask[2][4];
+
+	u64 pdptrs[4]; /* pae */
 };
 
 struct kvm_vcpu_arch {
@@ -276,7 +278,6 @@ struct kvm_vcpu_arch {
 	unsigned long cr4_guest_owned_bits;
 	unsigned long cr8;
 	u32 hflags;
-	u64 pdptrs[4]; /* pae */
 	u64 efer;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
@@ -592,7 +593,7 @@ void kvm_mmu_zap_all(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 
-int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3);
+int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3);
 
 int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
 			  const void *val, int bytes);

commit d4f8cf664e4c1fd579df6b6e6378335c9f79d790
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:55 2010 +0200

    KVM: MMU: Propagate the right fault back to the guest after gva_to_gpa
    
    This patch implements logic to make sure that either a
    page-fault/page-fault-vmexit or a nested-page-fault-vmexit
    is propagated back to the guest.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 08bc383083ff..574db6d1532a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -660,6 +660,7 @@ void kvm_inject_page_fault(struct kvm_vcpu *vcpu);
 int kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gfn_t gfn, void *data, int offset, int len,
 			    u32 access);
+void kvm_propagate_fault(struct kvm_vcpu *vcpu);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
 int kvm_pic_set_irq(void *opaque, int irq, int level);

commit ec92fe44e7ff94d04d8305e49efcffd8773e1cf6
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:51 2010 +0200

    KVM: X86: Add kvm_read_guest_page_mmu function
    
    This patch adds a function which can read from the guests
    physical memory or from the guest's guest physical memory.
    This will be used in the two-dimensional page table walker.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8ec3547c433d..08bc383083ff 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -657,6 +657,9 @@ void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu);
+int kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
+			    gfn_t gfn, void *data, int offset, int len,
+			    u32 access);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
 int kvm_pic_set_irq(void *opaque, int irq, int level);

commit 6539e738f65a8f1fc7806295d5d701fba4008343
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:50 2010 +0200

    KVM: MMU: Implement nested gva_to_gpa functions
    
    This patch adds the functions to do a nested l2_gva to
    l1_gpa page table walk.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1b3eb8a0a1bc..8ec3547c433d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -295,6 +295,16 @@ struct kvm_vcpu_arch {
 	 */
 	struct kvm_mmu mmu;
 
+	/*
+	 * Paging state of an L2 guest (used for nested npt)
+	 *
+	 * This context will save all necessary information to walk page tables
+	 * of the an L2 guest. This context is only initialized for page table
+	 * walking and not for faulting since we never handle l2 page faults on
+	 * the host.
+	 */
+	struct kvm_mmu nested_mmu;
+
 	/*
 	 * Pointer to the mmu context currently used for
 	 * gva_to_gpa translations.

commit 14dfe855f978181cd611ec018e5ceba860a98545
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:49 2010 +0200

    KVM: X86: Introduce pointer to mmu context used for gva_to_gpa
    
    This patch introduces the walk_mmu pointer which points to
    the mmu-context currently used for gva_to_gpa translations.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4915b7c8f2ec..1b3eb8a0a1bc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -286,8 +286,21 @@ struct kvm_vcpu_arch {
 	u64 ia32_misc_enable_msr;
 	bool tpr_access_reporting;
 
+	/*
+	 * Paging state of the vcpu
+	 *
+	 * If the vcpu runs in guest mode with two level paging this still saves
+	 * the paging mode of the l1 guest. This context is always used to
+	 * handle faults.
+	 */
 	struct kvm_mmu mmu;
 
+	/*
+	 * Pointer to the mmu context currently used for
+	 * gva_to_gpa translations.
+	 */
+	struct kvm_mmu *walk_mmu;
+
 	/*
 	 * This struct is filled with the necessary information to propagate a
 	 * page fault into the guest

commit c30a358d33e0e111f06e54a4a4125371e6b6693c
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:48 2010 +0200

    KVM: MMU: Add infrastructure for two-level page walker
    
    This patch introduces a mmu-callback to translate gpa
    addresses in the walk_addr code. This is later used to
    translate l2_gpa addresses into l1_gpa addresses.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3fde5b322534..4915b7c8f2ec 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -243,6 +243,7 @@ struct kvm_mmu {
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    u32 *error);
+	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access);
 	void (*prefetch_page)(struct kvm_vcpu *vcpu,
 			      struct kvm_mmu_page *page);
 	int (*sync_page)(struct kvm_vcpu *vcpu,

commit 8df25a328a6ca3bd0f048278f4d5ae0a1f6fadc1
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:46 2010 +0200

    KVM: MMU: Track page fault data in struct vcpu
    
    This patch introduces a struct with two new fields in
    vcpu_arch for x86:
    
            * fault.address
            * fault.error_code
    
    This will be used to correctly propagate page faults back
    into the guest when we could have either an ordinary page
    fault or a nested page fault. In the case of a nested page
    fault the fault-address is different from the original
    address that should be walked. So we need to keep track
    about the real fault-address.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 009a4a1b370e..3fde5b322534 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -239,9 +239,7 @@ struct kvm_mmu {
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err);
-	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
-				  unsigned long addr,
-				  u32 error_code);
+	void (*inject_page_fault)(struct kvm_vcpu *vcpu);
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    u32 *error);
@@ -288,6 +286,16 @@ struct kvm_vcpu_arch {
 	bool tpr_access_reporting;
 
 	struct kvm_mmu mmu;
+
+	/*
+	 * This struct is filled with the necessary information to propagate a
+	 * page fault into the guest
+	 */
+	struct {
+		u64      address;
+		unsigned error_code;
+	} fault;
+
 	/* only needed in kvm_pv_mmu_op() path, but it's hot so
 	 * put it here to avoid allocation */
 	struct kvm_pv_mmu_op_buffer mmu_op_buffer;
@@ -624,8 +632,7 @@ void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
-void kvm_inject_page_fault(struct kvm_vcpu *vcpu, unsigned long cr2,
-			   u32 error_code);
+void kvm_inject_page_fault(struct kvm_vcpu *vcpu);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
 int kvm_pic_set_irq(void *opaque, int irq, int level);

commit cb659db8a7d1ed558898f533a957dfc342f9499d
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:43 2010 +0200

    KVM: MMU: Introduce inject_page_fault function pointer
    
    This patch introduces an inject_page_fault function pointer
    into struct kvm_mmu which will be used to inject a page
    fault. This will be used later when Nested Nested Paging is
    implemented.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6c97b8debfa8..009a4a1b370e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -239,6 +239,9 @@ struct kvm_mmu {
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err);
+	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
+				  unsigned long addr,
+				  u32 error_code);
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    u32 *error);

commit 5777ed340d89cdc6c76a5c552337a3861b40a806
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:42 2010 +0200

    KVM: MMU: Introduce get_cr3 function pointer
    
    This function pointer in the MMU context is required to
    implement Nested Nested Paging.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 81a51473f745..6c97b8debfa8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -237,6 +237,7 @@ struct kvm_pio_request {
 struct kvm_mmu {
 	void (*new_cr3)(struct kvm_vcpu *vcpu);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
+	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err);
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,

commit 1c97f0a04c74196880f22a563134c8f6d0b9d752
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:41 2010 +0200

    KVM: X86: Introduce a tdp_set_cr3 function
    
    This patch introduces a special set_tdp_cr3 function pointer
    in kvm_x86_ops which is only used for tpd enabled mmu
    contexts. This allows to remove some hacks from svm code.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 53cedede88fa..81a51473f745 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -526,6 +526,8 @@ struct kvm_x86_ops {
 	bool (*rdtscp_supported)(void);
 	void (*adjust_tsc_offset)(struct kvm_vcpu *vcpu, s64 adjustment);
 
+	void (*set_tdp_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
+
 	void (*set_supported_cpuid)(u32 func, struct kvm_cpuid_entry2 *entry);
 
 	bool (*has_wbinvd_exit)(void);

commit f43addd46168110d572dcf69100cb215a4e9fd08
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:40 2010 +0200

    KVM: MMU: Make set_cr3 a function pointer in kvm_mmu
    
    This is necessary to implement Nested Nested Paging. As a
    side effect this allows some cleanups in the SVM nested
    paging code.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 80ef28bddcc3..53cedede88fa 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -236,6 +236,7 @@ struct kvm_pio_request {
  */
 struct kvm_mmu {
 	void (*new_cr3)(struct kvm_vcpu *vcpu);
+	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err);
 	void (*free)(struct kvm_vcpu *vcpu);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,

commit c5a78f2b649ae75ae788e7622ca5a586af2cb35a
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:39 2010 +0200

    KVM: MMU: Make tdp_enabled a mmu-context parameter
    
    This patch changes the tdp_enabled flag from its global
    meaning to the mmu-context and renames it to direct_map
    there. This is necessary for Nested SVM with emulation of
    Nested Paging where we need an extra MMU context to shadow
    the Nested Nested Page Table.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 789e9462668f..80ef28bddcc3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -249,6 +249,7 @@ struct kvm_mmu {
 	int root_level;
 	int shadow_root_level;
 	union kvm_mmu_page_role base_role;
+	bool direct_map;
 
 	u64 *pae_root;
 	u64 rsvd_bits_mask[2][4];

commit 1d5f066e0b63271b67eac6d3752f8aa96adcbddb
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Thu Aug 19 22:07:30 2010 -1000

    KVM: x86: Fix a possible backwards warp of kvmclock
    
    Kernel time, which advances in discrete steps may progress much slower
    than TSC.  As a result, when kvmclock is adjusted to a new base, the
    apparent time to the guest, which runs at a much higher, nsec scaled
    rate based on the current TSC, may have already been observed to have
    a larger value (kernel_ns + scaled tsc) than the value to which we are
    setting it (kernel_ns + 0).
    
    We must instead compute the clock as potentially observed by the guest
    for kernel_ns to make sure it does not go backwards.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5ab1c3fb34ef..789e9462668f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -339,6 +339,8 @@ struct kvm_vcpu_arch {
 	unsigned int time_offset;
 	struct page *time_page;
 	u64 last_host_tsc;
+	u64 last_guest_tsc;
+	u64 last_kernel_ns;
 
 	bool nmi_pending;
 	bool nmi_injected;

commit e48672fa25e879f7ae21785c7efd187738139593
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Thu Aug 19 22:07:23 2010 -1000

    KVM: x86: Unify TSC logic
    
    Move the TSC control logic from the vendor backends into x86.c
    by adding adjust_tsc_offset to x86 ops.  Now all TSC decisions
    can be done in one place.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 57b4394491ec..5ab1c3fb34ef 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -255,7 +255,6 @@ struct kvm_mmu {
 };
 
 struct kvm_vcpu_arch {
-	u64 host_tsc;
 	/*
 	 * rip and regs accesses must go through
 	 * kvm_{register,rip}_{read,write} functions.
@@ -336,9 +335,10 @@ struct kvm_vcpu_arch {
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
-	unsigned int hv_clock_tsc_khz;
+	unsigned int hw_tsc_khz;
 	unsigned int time_offset;
 	struct page *time_page;
+	u64 last_host_tsc;
 
 	bool nmi_pending;
 	bool nmi_injected;
@@ -520,6 +520,7 @@ struct kvm_x86_ops {
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
+	void (*adjust_tsc_offset)(struct kvm_vcpu *vcpu, s64 adjustment);
 
 	void (*set_supported_cpuid)(u32 func, struct kvm_cpuid_entry2 *entry);
 

commit f38e098ff3a315bb74abbb4a35cba11bbea8e2fa
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Thu Aug 19 22:07:20 2010 -1000

    KVM: x86: TSC reset compensation
    
    Attempt to synchronize TSCs which are reset to the same value.  In the
    case of a reliable hardware TSC, we can just re-use the same offset, but
    on non-reliable hardware, we can get closer by adjusting the offset to
    match the elapsed time.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a215153f1ff6..57b4394491ec 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -396,6 +396,9 @@ struct kvm_arch {
 	unsigned long irq_sources_bitmap;
 	s64 kvmclock_offset;
 	spinlock_t tsc_write_lock;
+	u64 last_tsc_nsec;
+	u64 last_tsc_offset;
+	u64 last_tsc_write;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 

commit 99e3e30aee1a326a98bf3a5f47b8622219c685f3
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Thu Aug 19 22:07:17 2010 -1000

    KVM: x86: Move TSC offset writes to common code
    
    Also, ensure that the storing of the offset and the reading of the TSC
    are never preempted by taking a spinlock.  While the lock is overkill
    now, it is useful later in this patch series.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6056a23dc4cf..a215153f1ff6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -395,6 +395,7 @@ struct kvm_arch {
 
 	unsigned long irq_sources_bitmap;
 	s64 kvmclock_offset;
+	spinlock_t tsc_write_lock;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 
@@ -521,6 +522,8 @@ struct kvm_x86_ops {
 
 	bool (*has_wbinvd_exit)(void);
 
+	void (*write_tsc_offset)(struct kvm_vcpu *vcpu, u64 offset);
+
 	const struct trace_print_flags *exit_reasons_str;
 };
 

commit ae38436b78a8abff767e2ac10e2cd663a7eef476
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Thu Aug 19 22:07:15 2010 -1000

    KVM: x86: Drop vm_init_tsc
    
    This is used only by the VMX code, and is not done properly;
    if the TSC is indeed backwards, it is out of sync, and will
    need proper handling in the logic at each and every CPU change.
    For now, drop this test during init as misguided.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e01b72825564..6056a23dc4cf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -394,7 +394,6 @@ struct kvm_arch {
 	gpa_t ept_identity_map_addr;
 
 	unsigned long irq_sources_bitmap;
-	u64 vm_init_tsc;
 	s64 kvmclock_offset;
 
 	struct kvm_xen_hvm_config xen_hvm_config;

commit 49d5ca26636cb8feb05aff92fc4dba3e494ec683
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Thu Aug 19 18:11:28 2010 -0700

    KVM: replace x86 kvm n_free_mmu_pages with n_used_mmu_pages
    
    Doing this makes the code much more readable.  That's
    borne out by the fact that this patch removes code.  "used"
    also happens to be the number that we need to return back to
    the slab code when our shrinker gets called.  Keeping this
    value as opposed to free makes the next patch simpler.
    
    So, 'struct kvm' is kzalloc()'d.  'struct kvm_arch' is a
    structure member (and not a pointer) of 'struct kvm'.  That
    means they start out zeroed.  I _think_ they get initialized
    properly by kvm_mmu_change_mmu_pages().  But, that only happens
    via kvm ioctls.
    
    Another benefit of storing 'used' intead of 'free' is
    that the values are consistent from the moment the structure is
    allocated: no negative "used" value.
    
    Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Tim Pepper <lnxninja@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 02963684cd28..e01b72825564 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -367,7 +367,7 @@ struct kvm_vcpu_arch {
 };
 
 struct kvm_arch {
-	unsigned int n_free_mmu_pages;
+	unsigned int n_used_mmu_pages;
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_max_mmu_pages;
 	atomic_t invlpg_counter;

commit 39de71ec5397f374aed95e99509372d605e1407c
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Thu Aug 19 18:11:14 2010 -0700

    KVM: rename x86 kvm->arch.n_alloc_mmu_pages
    
    arch.n_alloc_mmu_pages is a poor choice of name. This value truly
    means, "the number of pages which _may_ be allocated".  But,
    reading the name, "n_alloc_mmu_pages" implies "the number of allocated
    mmu pages", which is dead wrong.
    
    It's really the high watermark, so let's give it a name to match:
    nr_max_mmu_pages.  This change will make the next few patches
    much more obvious and easy to read.
    
    Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Tim Pepper <lnxninja@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c52e2eb40a1e..02963684cd28 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -369,7 +369,7 @@ struct kvm_vcpu_arch {
 struct kvm_arch {
 	unsigned int n_free_mmu_pages;
 	unsigned int n_requested_mmu_pages;
-	unsigned int n_alloc_mmu_pages;
+	unsigned int n_max_mmu_pages;
 	atomic_t invlpg_counter;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*

commit 9581d442b9058d3699b4be568b6e5eae38a41493
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Oct 19 16:46:55 2010 +0200

    KVM: Fix fs/gs reload oops with invalid ldt
    
    kvm reloads the host's fs and gs blindly, however the underlying segment
    descriptors may be invalid due to the user modifying the ldt after loading
    them.
    
    Fix by using the safe accessors (loadsegment() and load_gs_index()) instead
    of home grown unsafe versions.
    
    This is CVE-2010-3698.
    
    KVM-Stable-Tag.
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 502e53f999cf..c52e2eb40a1e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -652,20 +652,6 @@ static inline struct kvm_mmu_page *page_header(hpa_t shadow_page)
 	return (struct kvm_mmu_page *)page_private(page);
 }
 
-static inline u16 kvm_read_fs(void)
-{
-	u16 seg;
-	asm("mov %%fs, %0" : "=g"(seg));
-	return seg;
-}
-
-static inline u16 kvm_read_gs(void)
-{
-	u16 seg;
-	asm("mov %%gs, %0" : "=g"(seg));
-	return seg;
-}
-
 static inline u16 kvm_read_ldt(void)
 {
 	u16 ldt;
@@ -673,16 +659,6 @@ static inline u16 kvm_read_ldt(void)
 	return ldt;
 }
 
-static inline void kvm_load_fs(u16 sel)
-{
-	asm("mov %0, %%fs" : : "rm"(sel));
-}
-
-static inline void kvm_load_gs(u16 sel)
-{
-	asm("mov %0, %%gs" : : "rm"(sel));
-}
-
 static inline void kvm_load_ldt(u16 sel)
 {
 	asm("lldt %0" : : "rm"(sel));

commit dd180b3e90253cb4ca95d603a8c17413f8daec69
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Sat Jul 3 16:02:42 2010 +0800

    KVM: VMX: fix tlb flush with invalid root
    
    Commit 341d9b535b6c simplify reload logic while entry guest mode, it
    can avoid unnecessary sync-root if KVM_REQ_MMU_RELOAD and
    KVM_REQ_MMU_SYNC both set.
    
    But, it cause a issue that when we handle 'KVM_REQ_TLB_FLUSH', the
    root is invalid, it is triggered during my test:
    
    Kernel BUG at ffffffffa00212b8 [verbose debug info unavailable]
    ......
    
    Fixed by directly return if the root is not ready.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 50c79b9f5c38..502e53f999cf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -40,6 +40,8 @@
 				  0xFFFFFF0000000000ULL)
 
 #define INVALID_PAGE (~(hpa_t)0)
+#define VALID_PAGE(x) ((x) != INVALID_PAGE)
+
 #define UNMAPPED_GVA (~(gpa_t)0)
 
 /* KVM Hugepage definitions for x86 */

commit 828554136bbacae6e39fc31b9cd7e7c660ad7530
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Thu Jul 1 16:00:11 2010 +0200

    KVM: Remove unnecessary divide operations
    
    This patch converts unnecessary divide and modulo operations
    in the KVM large page related code into logical operations.
    This allows to convert gfn_t to u64 while not breaking 32
    bit builds.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2bda62485c4c..50c79b9f5c38 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -44,7 +44,8 @@
 
 /* KVM Hugepage definitions for x86 */
 #define KVM_NR_PAGE_SIZES	3
-#define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + (((x) - 1) * 9))
+#define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+#define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)

commit f5f48ee15c2ee3e44cf429e34b16c6fa9b900246
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Wed Jun 30 12:25:15 2010 +0800

    KVM: VMX: Execute WBINVD to keep data consistency with assigned devices
    
    Some guest device driver may leverage the "Non-Snoop" I/O, and explicitly
    WBINVD or CLFLUSH to a RAM space. Since migration may occur before WBINVD or
    CLFLUSH, we need to maintain data consistency either by:
    1: flushing cache (wbinvd) when the guest is scheduled out if there is no
    wbinvd exit, or
    2: execute wbinvd on all dirty physical CPUs when guest wbinvd exits.
    
    Signed-off-by: Yaozu (Eddie) Dong <eddie.dong@intel.com>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a57cdeacc4d2..2bda62485c4c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -15,6 +15,7 @@
 #include <linux/mm.h>
 #include <linux/mmu_notifier.h>
 #include <linux/tracepoint.h>
+#include <linux/cpumask.h>
 
 #include <linux/kvm.h>
 #include <linux/kvm_para.h>
@@ -358,6 +359,8 @@ struct kvm_vcpu_arch {
 
 	/* fields used by HYPER-V emulation */
 	u64 hv_vapic;
+
+	cpumask_var_t wbinvd_dirty_mask;
 };
 
 struct kvm_arch {
@@ -514,6 +517,8 @@ struct kvm_x86_ops {
 
 	void (*set_supported_cpuid)(u32 func, struct kvm_cpuid_entry2 *entry);
 
+	bool (*has_wbinvd_exit)(void);
+
 	const struct trace_print_flags *exit_reasons_str;
 };
 
@@ -571,6 +576,7 @@ void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
 int emulate_invlpg(struct kvm_vcpu *vcpu, gva_t address);
 int emulate_clts(struct kvm_vcpu *vcpu);
+int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);

commit a1f4d39500ad8ed61825eff061debff42386ab5b
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Jun 21 11:44:20 2010 +0300

    KVM: Remove memory alias support
    
    As advertised in feature-removal-schedule.txt.  Equivalent support is provided
    by overlapping memory regions.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2ec2e27a403e..a57cdeacc4d2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -69,8 +69,6 @@
 
 #define IOPL_SHIFT 12
 
-#define KVM_ALIAS_SLOTS 4
-
 #define KVM_PERMILLE_MMU_PAGES 20
 #define KVM_MIN_ALLOC_MMU_PAGES 64
 #define KVM_MMU_HASH_SHIFT 10
@@ -362,24 +360,7 @@ struct kvm_vcpu_arch {
 	u64 hv_vapic;
 };
 
-struct kvm_mem_alias {
-	gfn_t base_gfn;
-	unsigned long npages;
-	gfn_t target_gfn;
-#define KVM_ALIAS_INVALID     1UL
-	unsigned long flags;
-};
-
-#define KVM_ARCH_HAS_UNALIAS_INSTANTIATION
-
-struct kvm_mem_aliases {
-	struct kvm_mem_alias aliases[KVM_ALIAS_SLOTS];
-	int naliases;
-};
-
 struct kvm_arch {
-	struct kvm_mem_aliases *aliases;
-
 	unsigned int n_free_mmu_pages;
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_alloc_mmu_pages;
@@ -655,8 +636,6 @@ void kvm_disable_tdp(void);
 int complete_pio(struct kvm_vcpu *vcpu);
 bool kvm_check_iopl(struct kvm_vcpu *vcpu);
 
-struct kvm_memory_slot *gfn_to_memslot_unaliased(struct kvm *kvm, gfn_t gfn);
-
 static inline struct kvm_mmu_page *page_header(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);

commit be71e061d15c0aad4f8c2606f76c57b8a19792fd
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Fri Jun 11 21:31:38 2010 +0800

    KVM: MMU: don't mark pte notrap if it's just sync transient
    
    If the sync-sp just sync transient, don't mark its pte notrap
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c2813d658f3e..2ec2e27a403e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -241,7 +241,7 @@ struct kvm_mmu {
 	void (*prefetch_page)(struct kvm_vcpu *vcpu,
 			      struct kvm_mmu_page *page);
 	int (*sync_page)(struct kvm_vcpu *vcpu,
-			 struct kvm_mmu_page *sp);
+			 struct kvm_mmu_page *sp, bool clear_unsync);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
 	hpa_t root_hpa;
 	int root_level;

commit 2390218b6aa2eb3784b0a82fa811c19097dc793a
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jun 10 17:02:16 2010 +0300

    KVM: Fix mov cr3 #GP at wrong instruction
    
    On Intel, we call skip_emulated_instruction() even if we injected a #GP,
    resulting in the #GP pointing at the wrong address.
    
    Fix by injecting the exception and skipping the instruction at the same place,
    so we can do just one or the other.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ea8c319cdffc..c2813d658f3e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -598,7 +598,7 @@ int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason,
 		    bool has_error_code, u32 error_code);
 
 int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
-void kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
+int kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
 int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
 void kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);
 int kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val);

commit a83b29c6ad6d6497e569edbc29e556a384cebddd
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jun 10 17:02:15 2010 +0300

    KVM: Fix mov cr4 #GP at wrong instruction
    
    On Intel, we call skip_emulated_instruction() even if we injected a #GP,
    resulting in the #GP pointing at the wrong address.
    
    Fix by injecting the exception and skipping the instruction at the same place,
    so we can do just one or the other.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b23708450210..ea8c319cdffc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -599,7 +599,7 @@ int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason,
 
 int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 void kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
-void kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
+int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
 void kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);
 int kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val);
 int kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val);

commit 49a9b07edcf4aff159c1f3d3a27e58cf38bc27cd
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jun 10 17:02:14 2010 +0300

    KVM: Fix mov cr0 #GP at wrong instruction
    
    On Intel, we call skip_emulated_instruction() even if we injected a #GP,
    resulting in the #GP pointing at the wrong address.
    
    Fix by injecting the exception and skipping the instruction at the same place,
    so we can do just one or the other.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 91631b8b2090..b23708450210 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -597,7 +597,7 @@ int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);
 int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason,
 		    bool has_error_code, u32 error_code);
 
-void kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
+int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 void kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
 void kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
 void kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);

commit 2acf923e38fb6a4ce0c57115decbb38d334902ac
Author: Dexuan Cui <dexuan.cui@intel.com>
Date:   Thu Jun 10 11:27:12 2010 +0800

    KVM: VMX: Enable XSAVE/XRSTOR for guest
    
    This patch enable guest to use XSAVE/XRSTOR instructions.
    
    We assume that host_xcr0 would use all possible bits that OS supported.
    
    And we loaded xcr0 in the same way we handled fpu - do it as late as we can.
    
    Signed-off-by: Dexuan Cui <dexuan.cui@intel.com>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0cd0f2923af5..91631b8b2090 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -302,6 +302,7 @@ struct kvm_vcpu_arch {
 	} update_pte;
 
 	struct fpu guest_fpu;
+	u64 xcr0;
 
 	gva_t mmio_fault_cr2;
 	struct kvm_pio_request pio;
@@ -605,6 +606,7 @@ int kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val);
 unsigned long kvm_get_cr8(struct kvm_vcpu *vcpu);
 void kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw);
 void kvm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l);
+int kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr);
 
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 data);

commit 10ab25cd6bf7ee4e5a55d81f203f7dc1a855c27e
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue May 25 16:01:50 2010 +0200

    KVM: x86: Propagate fpu_alloc errors
    
    Memory allocation may fail. Propagate such errors.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Reviewed-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d08bb4a202de..0cd0f2923af5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -624,7 +624,7 @@ int kvm_pic_set_irq(void *opaque, int irq, int level);
 
 void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
-void fx_init(struct kvm_vcpu *vcpu);
+int fx_init(struct kvm_vcpu *vcpu);
 
 void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,

commit 98918833a3e21ffc5619535955e7a003cb788163
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon May 17 17:08:28 2010 +0800

    KVM: x86: Use FPU API
    
    Convert KVM to use generic FPU API.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d93601c52902..d08bb4a202de 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -301,7 +301,7 @@ struct kvm_vcpu_arch {
 		unsigned long mmu_seq;
 	} update_pte;
 
-	struct i387_fxsave_struct guest_fx_image;
+	struct fpu guest_fpu;
 
 	gva_t mmio_fault_cr2;
 	struct kvm_pio_request pio;
@@ -708,21 +708,6 @@ static inline unsigned long read_msr(unsigned long msr)
 }
 #endif
 
-static inline void kvm_fx_save(struct i387_fxsave_struct *image)
-{
-	asm("fxsave (%0)":: "r" (image));
-}
-
-static inline void kvm_fx_restore(struct i387_fxsave_struct *image)
-{
-	asm("fxrstor (%0)":: "r" (image));
-}
-
-static inline void kvm_fx_finit(void)
-{
-	asm("finit");
-}
-
 static inline u32 get_rdx_init_val(void)
 {
 	return 0x600; /* P6 family */

commit 7cf30855e02be7a207ffebb8b9350986f2ba83e9
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon May 17 17:08:27 2010 +0800

    KVM: x86: Use unlazy_fpu() for host FPU
    
    We can avoid unnecessary fpu load when userspace process
    didn't use FPU frequently.
    
    Derived from Avi's idea.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0c06148fa3b1..d93601c52902 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -301,7 +301,6 @@ struct kvm_vcpu_arch {
 		unsigned long mmu_seq;
 	} update_pte;
 
-	struct i387_fxsave_struct host_fx_image;
 	struct i387_fxsave_struct guest_fx_image;
 
 	gva_t mmio_fault_cr2;

commit 6d77dbfc88e37c9efd5c5dd18445cfe819ae17ea
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon May 10 11:16:56 2010 +0300

    KVM: inject #UD if instruction emulation fails and exit to userspace
    
    Do not kill VM when instruction emulation fails. Inject #UD and report
    failure to userspace instead. Userspace may choose to reenter guest if
    vcpu is in userspace (cpl == 3) in which case guest OS will kill
    offending process and continue running.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2ca1867ed97a..0c06148fa3b1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -576,7 +576,6 @@ enum emulation_result {
 #define EMULTYPE_SKIP		    (1 << 2)
 int emulate_instruction(struct kvm_vcpu *vcpu,
 			unsigned long cr2, u16 error_code, int emulation_type);
-void kvm_report_emulation_failure(struct kvm_vcpu *cvpu, const char *context);
 void realmode_lgdt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
 void realmode_lidt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
 

commit f181b96d4c769b8915849eb9070c18116fd8d44e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Apr 28 19:15:36 2010 +0300

    KVM: remove export of emulator_write_emulated()
    
    It is not called directly outside of the file it's defined in anymore.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 97774ae3c874..2ca1867ed97a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -628,11 +628,6 @@ void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
 void fx_init(struct kvm_vcpu *vcpu);
 
-int emulator_write_emulated(unsigned long addr,
-			    const void *val,
-			    unsigned int bytes,
-			    struct kvm_vcpu *vcpu);
-
 void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 		       const u8 *new, int bytes,

commit 35aa5375d407ecadcc3adb5cb31d27044bf7f29f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Apr 28 19:15:27 2010 +0300

    KVM: x86 emulator: add (set|get)_dr callbacks to x86_emulate_ops
    
    Add (set|get)_dr callbacks to x86_emulate_ops instead of calling
    them directly.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 76f5483cffec..97774ae3c874 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -591,10 +591,6 @@ void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
 int emulate_invlpg(struct kvm_vcpu *vcpu, gva_t address);
 int emulate_clts(struct kvm_vcpu *vcpu);
-int emulator_get_dr(struct x86_emulate_ctxt *ctxt, int dr,
-		    unsigned long *dest);
-int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,
-		    unsigned long value);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);

commit 3dbe141595faa48a067add3e47bba3205b79d33c
Author: Avi Kivity <avi@redhat.com>
Date:   Wed May 12 11:48:18 2010 +0300

    KVM: MMU: Segregate shadow pages with different cr0.wp
    
    When cr0.wp=0, we may shadow a gpte having u/s=1 and r/w=0 with an spte
    having u/s=0 and r/w=1.  This allows excessive access if the guest sets
    cr0.wp=1 and accesses through this spte.
    
    Fix by making cr0.wp part of the base role; we'll have different sptes for
    the two cases and the problem disappears.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3f0007b076da..76f5483cffec 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -179,6 +179,7 @@ union kvm_mmu_page_role {
 		unsigned access:3;
 		unsigned invalid:1;
 		unsigned nxe:1;
+		unsigned cr0_wp:1;
 	};
 };
 

commit ce7ddec4bbbc08f0c2901cc103773aed864b09fd
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Thu Apr 22 12:33:13 2010 +0200

    KVM: x86: Allow marking an exception as reinjected
    
    This patch adds logic to kvm/x86 which allows to mark an
    injected exception as reinjected. This allows to remove an
    ugly hack from svm_complete_interrupts that prevented
    exceptions from being reinjected at all in the nested case.
    The hack was necessary because an reinjected exception into
    the nested guest could cause a nested vmexit emulation. But
    reinjected exceptions must not intercept. The downside of
    the hack is that a exception that in injected could get
    lost.
    This patch fixes the problem and puts the code for it into
    generic x86 files because. Nested-VMX will likely have the
    same problem and could reuse the code.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 357573af974f..3f0007b076da 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -312,6 +312,7 @@ struct kvm_vcpu_arch {
 	struct kvm_queued_exception {
 		bool pending;
 		bool has_error_code;
+		bool reinject;
 		u8 nr;
 		u32 error_code;
 	} exception;
@@ -514,7 +515,8 @@ struct kvm_x86_ops {
 	void (*set_irq)(struct kvm_vcpu *vcpu);
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
-				bool has_error_code, u32 error_code);
+				bool has_error_code, u32 error_code,
+				bool reinject);
 	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
 	int (*nmi_allowed)(struct kvm_vcpu *vcpu);
 	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
@@ -617,6 +619,8 @@ void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);
 
 void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
+void kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr);
+void kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, unsigned long cr2,
 			   u32 error_code);
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);

commit d4330ef2fb2236a1e3a176f0f68360f4c0a8661b
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Thu Apr 22 12:33:11 2010 +0200

    KVM: x86: Add callback to let modules decide over some supported cpuid bits
    
    This patch adds the get_supported_cpuid callback to
    kvm_x86_ops. It will be used in do_cpuid_ent to delegate the
    decission about some supported cpuid bits to the
    architecture modules.
    
    Cc: stable@kernel.org
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d47d087568fe..357573af974f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -528,6 +528,8 @@ struct kvm_x86_ops {
 	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
 
+	void (*set_supported_cpuid)(u32 func, struct kvm_cpuid_entry2 *entry);
+
 	const struct trace_print_flags *exit_reasons_str;
 };
 

commit 87bc3bf972af0585ba5415aebbc8bd09b6a2ee94
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Apr 19 17:25:53 2010 +0300

    KVM: MMU: Drop cr4.pge from shadow page role
    
    Since commit bf47a760f66ad, we no longer handle ptes with the global bit
    set specially, so there is no reason to distinguish between shadow pages
    created with cr4.gpe set and clear.
    
    Such tracking is expensive when the guest toggles cr4.pge, so drop it.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3c31c5ad37ab..d47d087568fe 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -178,7 +178,6 @@ union kvm_mmu_page_role {
 		unsigned direct:1;
 		unsigned access:3;
 		unsigned invalid:1;
-		unsigned cr4_pge:1;
 		unsigned nxe:1;
 	};
 };

commit 0571d366e0be571be14581cb5e28d9c3f6e0d0b1
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Fri Apr 16 21:27:54 2010 +0800

    KVM: MMU: reduce 'struct kvm_mmu_page' size
    
    Define 'multimapped' as 'bool'.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 707d272ae4a1..3c31c5ad37ab 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -202,9 +202,9 @@ struct kvm_mmu_page {
 	 * in this shadow page.
 	 */
 	DECLARE_BITMAP(slot_bitmap, KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS);
-	int multimapped;         /* More than one parent_pte? */
-	int root_count;          /* Currently serving as active root */
+	bool multimapped;         /* More than one parent_pte? */
 	bool unsync;
+	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	union {
 		u64 *parent_pte;               /* !multimapped */

commit 5b7e0102ae744e9175b905f4267a81393bdb7a75
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 14 19:20:03 2010 +0300

    KVM: MMU: Replace role.glevels with role.cr4_pae
    
    There is no real distinction between glevels=3 and glevels=4; both have
    exactly the same format and the code is treated exactly the same way.  Drop
    role.glevels and replace is with role.cr4_pae (which is meaningful).  This
    simplifies the code a bit.
    
    As a side effect, it allows sharing shadow page tables between pae and
    longmode guest page tables at the same guest page.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3602728d54de..707d272ae4a1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -171,8 +171,8 @@ struct kvm_pte_chain {
 union kvm_mmu_page_role {
 	unsigned word;
 	struct {
-		unsigned glevels:4;
 		unsigned level:4;
+		unsigned cr4_pae:1;
 		unsigned quadrant:2;
 		unsigned pad_for_nice_hex_output:6;
 		unsigned direct:1;

commit e269fb2189fb86d79d64c0ca74c6c1a549ad4aa3
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Wed Apr 14 15:51:09 2010 +0200

    KVM: x86: Push potential exception error code on task switches
    
    When a fault triggers a task switch, the error code, if existent, has to
    be pushed on the new task's stack. Implement the missing bits.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5d5e0a9afcf2..3602728d54de 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -595,7 +595,8 @@ int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);
 
-int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason);
+int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason,
+		    bool has_error_code, u32 error_code);
 
 void kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 void kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);

commit 020df0794f5764e742feaa718be88b8f1b4ce04f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 13 10:05:23 2010 +0300

    KVM: move DR register access handling into generic code
    
    Currently both SVM and VMX have their own DR handling code. Move it to
    x86.c.
    
    Acked-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0c49c888be6b..5d5e0a9afcf2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -496,8 +496,7 @@ struct kvm_x86_ops {
 	void (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*get_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*set_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
-	int (*get_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long *dest);
-	int (*set_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long value);
+	void (*set_dr7)(struct kvm_vcpu *vcpu, unsigned long value);
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
@@ -602,6 +601,8 @@ void kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 void kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
 void kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
 void kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);
+int kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val);
+int kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val);
 unsigned long kvm_get_cr8(struct kvm_vcpu *vcpu);
 void kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw);
 void kvm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l);

commit f84cbb0561d7cefb5fc050ee99d8c82ec9ce883d
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Apr 6 18:29:05 2010 +0800

    KVM: MMU: remove unused field
    
    kvm_mmu_page.oos_link is not used, so remove it
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 26c629a062db..0c49c888be6b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -187,8 +187,6 @@ struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
 
-	struct list_head oos_link;
-
 	/*
 	 * The following two entries are used to key the shadow page in the
 	 * hash table.

commit 7972995b0c346de76fe260ce0fd6bcc8ffab724a
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 18 15:20:24 2010 +0200

    KVM: x86 emulator: Move string pio emulation into emulator.c
    
    Currently emulation is done outside of emulator so things like doing
    ins/outs to/from mmio are broken it also makes it hard (if not impossible)
    to implement single stepping in the future. The implementation in this
    patch is not efficient since it exits to userspace for each IO while
    previous implementation did 'ins' in batches. Further patch that
    implements pio in string read ahead address this problem.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 776d3e202b56..26c629a062db 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -224,14 +224,9 @@ struct kvm_pv_mmu_op_buffer {
 
 struct kvm_pio_request {
 	unsigned long count;
-	int cur_count;
-	gva_t guest_gva;
 	int in;
 	int port;
 	int size;
-	int string;
-	int down;
-	int rep;
 };
 
 /*
@@ -591,9 +586,6 @@ int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
 struct x86_emulate_ctxt;
 
 int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
-int kvm_emulate_pio_string(struct kvm_vcpu *vcpu, int in,
-			   int size, unsigned long count, int down,
-			    gva_t address, int rep, unsigned port);
 void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
 int kvm_emulate_halt(struct kvm_vcpu *vcpu);
 int emulate_invlpg(struct kvm_vcpu *vcpu, gva_t address);

commit cf8f70bfe38b326bb80b10f76d6544f571040229
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 18 15:20:23 2010 +0200

    KVM: x86 emulator: fix in/out emulation.
    
    in/out emulation is broken now. The breakage is different depending
    on where IO device resides. If it is in userspace emulator reports
    emulation failure since it incorrectly interprets kvm_emulate_pio()
    return value. If IO device is in the kernel emulation of 'in' will do
    nothing since kvm_emulate_pio() stores result directly into vcpu
    registers, so emulator will overwrite result of emulation during
    commit of shadowed register.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b99cec1547c6..776d3e202b56 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -590,8 +590,7 @@ int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
 
 struct x86_emulate_ctxt;
 
-int kvm_emulate_pio(struct kvm_vcpu *vcpu, int in,
-		     int size, unsigned port);
+int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size, unsigned short port);
 int kvm_emulate_pio_string(struct kvm_vcpu *vcpu, int in,
 			   int size, unsigned long count, int down,
 			    gva_t address, int rep, unsigned port);

commit 93a152be5af3d651ff0ab5459f5e0f9662b22438
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 18 15:20:04 2010 +0200

    KVM: remove realmode_lmsw function.
    
    Use (get|set)_cr callback to emulate lmsw inside emulator.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9d474c7ae261..b99cec1547c6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -583,8 +583,6 @@ int emulate_instruction(struct kvm_vcpu *vcpu,
 void kvm_report_emulation_failure(struct kvm_vcpu *cvpu, const char *context);
 void realmode_lgdt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
 void realmode_lidt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
-void realmode_lmsw(struct kvm_vcpu *vcpu, unsigned long msw,
-		   unsigned long *rflags);
 
 void kvm_enable_efer_bits(u64);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);

commit 52a4661737ecc918633f6b05c611a4af4b5eae5a
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 18 15:20:03 2010 +0200

    KVM: Provide callback to get/set control registers in emulator ops.
    
    Use this callback instead of directly call kvm function. Also rename
    realmode_(set|get)_cr to emulator_(set|get)_cr since function has nothing
    to do with real mode.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 53f520259471..9d474c7ae261 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -586,8 +586,6 @@ void realmode_lidt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
 void realmode_lmsw(struct kvm_vcpu *vcpu, unsigned long msw,
 		   unsigned long *rflags);
 
-unsigned long realmode_get_cr(struct kvm_vcpu *vcpu, int cr);
-void realmode_set_cr(struct kvm_vcpu *vcpu, int cr, unsigned long value);
 void kvm_enable_efer_bits(u64);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);

commit 49c6799a2ce3a6a4dd66021dabeb468901c7a700
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Mar 15 16:38:31 2010 +0200

    KVM: Remove pointer to rflags from realmode_set_cr parameters.
    
    Mov reg, cr instruction doesn't change flags in any meaningful way, so
    no need to update rflags after instruction execution.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 28826c82d1e2..53f520259471 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -587,8 +587,7 @@ void realmode_lmsw(struct kvm_vcpu *vcpu, unsigned long msw,
 		   unsigned long *rflags);
 
 unsigned long realmode_get_cr(struct kvm_vcpu *vcpu, int cr);
-void realmode_set_cr(struct kvm_vcpu *vcpu, int cr, unsigned long value,
-		     unsigned long *rflags);
+void realmode_set_cr(struct kvm_vcpu *vcpu, int cr, unsigned long value);
 void kvm_enable_efer_bits(u64);
 int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);

commit 08e850c6536db302050c0287649e68e3bbdfe2c7
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Mar 15 13:59:57 2010 +0200

    KVM: MMU: Reinstate pte prefetch on invlpg
    
    Commit fb341f57 removed the pte prefetch on guest invlpg, citing guest races.
    However, the SDM is adamant that prefetch is allowed:
    
      "The processor may create entries in paging-structure caches for
       translations required for prefetches and for accesses that are a
       result of speculative execution that would never actually occur
       in the executed code path."
    
    And, in fact, there was a race in the prefetch code: we picked up the pte
    without the mmu lock held, so an older invlpg could install the pte over
    a newer invlpg.
    
    Reinstate the prefetch logic, but this time note whether another invlpg has
    executed using a counter.  If a race occured, do not install the pte.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ea1b6c615f9f..28826c82d1e2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -389,6 +389,7 @@ struct kvm_arch {
 	unsigned int n_free_mmu_pages;
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_alloc_mmu_pages;
+	atomic_t invlpg_counter;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.

commit ec68798c8fd0f01cdbd3f3e1a970e76a644cf08e
Author: Wei Yongjun <yjwei@cn.fujitsu.com>
Date:   Fri Mar 5 12:11:48 2010 +0800

    KVM: x86: Use native_store_idt() instead of kvm_get_idt()
    
    This patch use generic linux function native_store_idt()
    instead of kvm_get_idt(), and also removed the useless
    function kvm_get_idt().
    
    Signed-off-by: Wei Yongjun <yjwei@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ec891a2ce86e..ea1b6c615f9f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -716,11 +716,6 @@ static inline void kvm_load_ldt(u16 sel)
 	asm("lldt %0" : : "rm"(sel));
 }
 
-static inline void kvm_get_idt(struct desc_ptr *table)
-{
-	asm("sidt %0" : "=m"(*table));
-}
-
 #ifdef CONFIG_X86_64
 static inline unsigned long read_msr(unsigned long msr)
 {

commit 2d49ec72d3fab0aa90510a64a973d594c48b1fd1
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Feb 25 12:43:09 2010 +0200

    KVM: move segment_base() into vmx.c
    
    segment_base() is used only by vmx so move it there.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e3167224d936..ec891a2ce86e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -644,8 +644,6 @@ int emulator_write_emulated(unsigned long addr,
 			    unsigned int bytes,
 			    struct kvm_vcpu *vcpu);
 
-unsigned long segment_base(u16 selector);
-
 void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 		       const u8 *new, int bytes,
@@ -723,13 +721,6 @@ static inline void kvm_get_idt(struct desc_ptr *table)
 	asm("sidt %0" : "=m"(*table));
 }
 
-static inline unsigned long kvm_read_tr_base(void)
-{
-	u16 tr;
-	asm("str %0" : "=g"(tr));
-	return segment_base(tr);
-}
-
 #ifdef CONFIG_X86_64
 static inline unsigned long read_msr(unsigned long msr)
 {

commit d6ab1ed44627c91d0a857a430b7ec4ed8648c7a5
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Feb 25 12:43:07 2010 +0200

    KVM: Drop kvm_get_gdt() in favor of generic linux function
    
    Linux now has native_store_gdt() to do the same. Use it instead of
    kvm local version.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 502fff123e29..e3167224d936 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -723,11 +723,6 @@ static inline void kvm_get_idt(struct desc_ptr *table)
 	asm("sidt %0" : "=m"(*table));
 }
 
-static inline void kvm_get_gdt(struct desc_ptr *table)
-{
-	asm("sgdt %0" : "=m"(*table));
-}
-
 static inline unsigned long kvm_read_tr_base(void)
 {
 	u16 tr;

commit f92653eeb496fe8624ac4b0d628c916a06a3d25c
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Feb 23 17:47:55 2010 +0100

    KVM: x86: Add kvm_is_linear_rip
    
    Based on Gleb's suggestion: Add a helper kvm_is_linear_rip that matches
    a given linear RIP against the current one. Use this for guest
    single-stepping, more users will follow.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d46e791de129..502fff123e29 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -362,8 +362,8 @@ struct kvm_vcpu_arch {
 	u64 *mce_banks;
 
 	/* used for guest single stepping over the given code position */
-	u16 singlestep_cs;
 	unsigned long singlestep_rip;
+
 	/* fields used by HYPER-V emulation */
 	u64 hv_vapic;
 };
@@ -820,4 +820,6 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_define_shared_msr(unsigned index, u32 msr);
 void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
+bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit ad91f8ffbb18413e79f9f976a55b4e11d02e6a6d
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Fri Feb 12 16:02:54 2010 +0900

    KVM: remove redundant prototype of load_pdptrs()
    
    This patch removes redundant prototype of load_pdptrs().
    
    I found load_pdptrs() twice in kvm_host.h. Let's remove one.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cf392dfb8000..d46e791de129 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -670,7 +670,6 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_enable_tdp(void);
 void kvm_disable_tdp(void);
 
-int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3);
 int complete_pio(struct kvm_vcpu *vcpu);
 bool kvm_check_iopl(struct kvm_vcpu *vcpu);
 

commit 89a27f4d0e042a2fa3391a76b652aec3e16ef200
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Feb 16 10:51:48 2010 +0200

    KVM: use desc_ptr struct instead of kvm private descriptor_table
    
    x86 arch defines desc_ptr for idt/gdt pointers, no need to define
    another structure in kvm code.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 06d9e79ca37d..cf392dfb8000 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -461,11 +461,6 @@ struct kvm_vcpu_stat {
 	u32 nmi_injections;
 };
 
-struct descriptor_table {
-	u16 limit;
-	unsigned long base;
-} __attribute__((packed));
-
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
@@ -503,10 +498,10 @@ struct kvm_x86_ops {
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 	void (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
 	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);
-	void (*get_idt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
-	void (*set_idt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
-	void (*get_gdt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
-	void (*set_gdt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
+	void (*get_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
+	void (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
+	void (*get_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
+	void (*set_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	int (*get_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long *dest);
 	int (*set_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long value);
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
@@ -724,12 +719,12 @@ static inline void kvm_load_ldt(u16 sel)
 	asm("lldt %0" : : "rm"(sel));
 }
 
-static inline void kvm_get_idt(struct descriptor_table *table)
+static inline void kvm_get_idt(struct desc_ptr *table)
 {
 	asm("sidt %0" : "=m"(*table));
 }
 
-static inline void kvm_get_gdt(struct descriptor_table *table)
+static inline void kvm_get_gdt(struct desc_ptr *table)
 {
 	asm("sgdt %0" : "=m"(*table));
 }

commit c697518a861e6c43b92b848895f9926580ee63c3
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Feb 18 12:15:01 2010 +0200

    KVM: Fix segment descriptor loading
    
    Add proper error and permission checking. This patch also change task
    switching code to load segment selectors before segment descriptors, like
    SDM requires, otherwise permission checking during segment descriptor
    loading will be incorrect.
    
    Cc: stable@kernel.org (2.6.33, 2.6.32)
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f9a2f66530cf..06d9e79ca37d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -614,8 +614,7 @@ int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,
 		    unsigned long value);
 
 void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
-int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector,
-				int type_bits, int seg);
+int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);
 
 int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason);
 

commit f850e2e603bf5a05b0aee7901857cf85715aa694
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Feb 10 14:21:33 2010 +0200

    KVM: x86 emulator: Check IOPL level during io instruction emulation
    
    Make emulator check that vcpu is allowed to execute IN, INS, OUT,
    OUTS, CLI, STI.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Cc: stable@kernel.org
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c07c16f64015..f9a2f66530cf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -678,6 +678,7 @@ void kvm_disable_tdp(void);
 
 int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3);
 int complete_pio(struct kvm_vcpu *vcpu);
+bool kvm_check_iopl(struct kvm_vcpu *vcpu);
 
 struct kvm_memory_slot *gfn_to_memslot_unaliased(struct kvm *kvm, gfn_t gfn);
 

commit 1871c6020d7308afb99127bba51f04548e7ca84e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Feb 10 14:21:32 2010 +0200

    KVM: x86 emulator: fix memory access during x86 emulation
    
    Currently when x86 emulator needs to access memory, page walk is done with
    broadest permission possible, so if emulated instruction was executed
    by userspace process it can still access kernel memory. Fix that by
    providing correct memory access to page walker during emulation.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Cc: stable@kernel.org
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 152233723844..c07c16f64015 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -243,7 +243,8 @@ struct kvm_mmu {
 	void (*new_cr3)(struct kvm_vcpu *vcpu);
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err);
 	void (*free)(struct kvm_vcpu *vcpu);
-	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva);
+	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
+			    u32 *error);
 	void (*prefetch_page)(struct kvm_vcpu *vcpu,
 			      struct kvm_mmu_page *page);
 	int (*sync_page)(struct kvm_vcpu *vcpu,
@@ -660,6 +661,10 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
+gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
+gpa_t kvm_mmu_gva_to_gpa_fetch(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
+gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
+gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva, u32 *error);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 

commit f6801dff23bd1902473902194667f4ac1eb6ea26
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jan 21 15:31:50 2010 +0200

    KVM: Rename vcpu->shadow_efer to efer
    
    None of the other registers have the shadow_ prefix.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7ebf9fe670cd..152233723844 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -277,7 +277,7 @@ struct kvm_vcpu_arch {
 	unsigned long cr8;
 	u32 hflags;
 	u64 pdptrs[4]; /* pae */
-	u64 shadow_efer;
+	u64 efer;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
 	int32_t apic_arb_prio;

commit 6b52d18605f580bdffaffd48c8da228c3e848deb
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jan 21 15:31:47 2010 +0200

    KVM: Activate fpu on clts
    
    Assume that if the guest executes clts, it knows what it's doing, and load the
    guest fpu to prevent an #NM exception.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d73ed48587e4..7ebf9fe670cd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -511,6 +511,7 @@ struct kvm_x86_ops {
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
+	void (*fpu_activate)(struct kvm_vcpu *vcpu);
 	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);

commit c76de350c8a3ba770becc17eaa744dc3c7642295
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Wed Jan 20 18:20:20 2010 +0100

    KVM: SVM: Clean up and enhance mov dr emulation
    
    Enhance mov dr instruction emulation used by SVM so that it properly
    handles dr4/5: alias to dr6/7 if cr4.de is cleared. Otherwise return
    EMULATE_FAIL which will let our only possible caller in that scenario,
    ud_interception, re-inject UD.
    
    We do not need to inject faults, SVM does this for us (exceptions take
    precedence over instruction interceptions). For the same reason, the
    value overflow checks can be removed.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a1f0b5dd7d75..d73ed48587e4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -506,9 +506,8 @@ struct kvm_x86_ops {
 	void (*set_idt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
 	void (*get_gdt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
 	void (*set_gdt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
-	unsigned long (*get_dr)(struct kvm_vcpu *vcpu, int dr);
-	void (*set_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long value,
-		       int *exception);
+	int (*get_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long *dest);
+	int (*set_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long value);
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);

commit 10388a07164c1512b3a3d0273b9adc230f82790e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Jan 17 15:51:23 2010 +0200

    KVM: Add HYPER-V apic access MSRs
    
    Implement HYPER-V apic MSRs. Spec defines three MSRs that speed-up
    access to EOI/TPR/ICR apic registers for PV guests.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Vadim Rozenfeld <vrozenfe@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 67d19e422006..a1f0b5dd7d75 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -363,6 +363,8 @@ struct kvm_vcpu_arch {
 	/* used for guest single stepping over the given code position */
 	u16 singlestep_cs;
 	unsigned long singlestep_rip;
+	/* fields used by HYPER-V emulation */
+	u64 hv_vapic;
 };
 
 struct kvm_mem_alias {

commit 55cd8e5a4edb8e235163ffe8264b9aaa8d7c050f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Jan 17 15:51:22 2010 +0200

    KVM: Implement bare minimum of HYPER-V MSRs
    
    Minimum HYPER-V implementation should have GUEST_OS_ID, HYPERCALL and
    VP_INDEX MSRs.
    
    [avi: fix build on i386]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Vadim Rozenfeld <vrozenfe@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 93bee7abb71c..67d19e422006 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -413,6 +413,10 @@ struct kvm_arch {
 	s64 kvmclock_offset;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
+
+	/* fields used by HYPER-V emulation */
+	u64 hv_guest_os_id;
+	u64 hv_hypercall;
 };
 
 struct kvm_vm_stat {

commit 02daab21d94dc4cf01b2fd09863d59a436900322
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Dec 30 12:40:26 2009 +0200

    KVM: Lazify fpu activation and deactivation
    
    Defer fpu deactivation as much as possible - if the guest fpu is loaded, keep
    it loaded until the next heavyweight exit (where we are forced to unload it).
    This reduces unnecessary exits.
    
    We also defer fpu activation on clts; while clts signals the intent to use the
    fpu, we can't be sure the guest will actually use it.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 693046a7a12d..93bee7abb71c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -506,6 +506,7 @@ struct kvm_x86_ops {
 	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
+	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);
 

commit e8467fda83cdc9de53972fee0cd2e6916cf66f41
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Dec 29 18:43:06 2009 +0200

    KVM: VMX: Allow the guest to own some cr0 bits
    
    We will use this later to give the guest ownership of cr0.ts.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a4de557ad733..693046a7a12d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -269,6 +269,7 @@ struct kvm_vcpu_arch {
 	u32 regs_dirty;
 
 	unsigned long cr0;
+	unsigned long cr0_guest_owned_bits;
 	unsigned long cr2;
 	unsigned long cr3;
 	unsigned long cr4;
@@ -489,6 +490,7 @@ struct kvm_x86_ops {
 	void (*set_segment)(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg);
 	void (*get_cs_db_l_bits)(struct kvm_vcpu *vcpu, int *db, int *l);
+	void (*decache_cr0_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);

commit 17cc393596823f4bbab81e68a9e23e7beadbcfca
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Jan 5 19:02:27 2010 +0800

    KVM: x86: Rename gb_page_enable() to get_lpage_level() in kvm_x86_ops
    
    Then the callback can provide the maximum supported large page level, which
    is more flexible.
    
    Also move the gb page support into x86_64 specific.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 741b8972a3a5..a4de557ad733 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -528,7 +528,7 @@ struct kvm_x86_ops {
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
-	bool (*gb_page_enable)(void);
+	int (*get_lpage_level)(void);
 	bool (*rdtscp_supported)(void);
 
 	const struct trace_print_flags *exit_reasons_str;

commit 0680fe52753381cb7154beeb01ef3e48f2cdeec6
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Dec 27 17:00:46 2009 +0200

    KVM: Bump maximum vcpu count to 64
    
    With slots_lock converted to rcu, the entire kvm hotpath on modern processors
    (with npt or ept) now scales beautifully.  Increase the maximum vcpu count to
    64 to reflect this.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6c8c7c578c46..741b8972a3a5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -25,7 +25,7 @@
 #include <asm/mtrr.h>
 #include <asm/msr-index.h>
 
-#define KVM_MAX_VCPUS 16
+#define KVM_MAX_VCPUS 64
 #define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4

commit a983fb238728e1123177e8058d4f644b949a7d05
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:23 2009 -0200

    KVM: x86: switch kvm_set_memory_alias to SRCU update
    
    Using a similar two-step procedure as for memslots.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7cdcb3d0f770..6c8c7c578c46 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -368,8 +368,12 @@ struct kvm_mem_alias {
 	gfn_t base_gfn;
 	unsigned long npages;
 	gfn_t target_gfn;
+#define KVM_ALIAS_INVALID     1UL
+	unsigned long flags;
 };
 
+#define KVM_ARCH_HAS_UNALIAS_INSTANTIATION
+
 struct kvm_mem_aliases {
 	struct kvm_mem_alias aliases[KVM_ALIAS_SLOTS];
 	int naliases;

commit fef9cce0eb28a67e688a411cc30b73625e49002b
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:17 2009 -0200

    KVM: modify alias layout in x86s struct kvm_arch
    
    Have a pointer to an allocated region inside x86's kvm_arch.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fe4df464fb39..7cdcb3d0f770 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -370,9 +370,13 @@ struct kvm_mem_alias {
 	gfn_t target_gfn;
 };
 
-struct kvm_arch{
-	int naliases;
+struct kvm_mem_aliases {
 	struct kvm_mem_alias aliases[KVM_ALIAS_SLOTS];
+	int naliases;
+};
+
+struct kvm_arch {
+	struct kvm_mem_aliases *aliases;
 
 	unsigned int n_free_mmu_pages;
 	unsigned int n_requested_mmu_pages;

commit 4e47c7a6d714cf352b719db92a924b6ec487acc5
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Fri Dec 18 16:48:47 2009 +0800

    KVM: VMX: Add instruction rdtscp support for guest
    
    Before enabling, execution of "rdtscp" in guest would result in #UD.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7ff0ea371e3c..fe4df464fb39 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -521,6 +521,7 @@ struct kvm_x86_ops {
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 	bool (*gb_page_enable)(void);
+	bool (*rdtscp_supported)(void);
 
 	const struct trace_print_flags *exit_reasons_str;
 };

commit 0e85188049afacdfce9c026144142264981bbabb
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Fri Dec 18 16:48:46 2009 +0800

    KVM: Add cpuid_update() callback to kvm_x86_ops
    
    Sometime, we need to adjust some state in order to reflect guest CPUID
    setting, e.g. if we don't expose rdtscp to guest, we won't want to enable
    it on hardware. cpuid_update() is introduced for this purpose.
    
    Also export kvm_find_cpuid_entry() for later use.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e9f4f12ec3c4..7ff0ea371e3c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -459,6 +459,7 @@ struct kvm_x86_ops {
 	int (*hardware_setup)(void);               /* __init */
 	void (*hardware_unsetup)(void);            /* __exit */
 	bool (*cpu_has_accelerated_tpr)(void);
+	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
 	/* Create, but do not attach this VCPU */
 	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);

commit fc78f51938e1ea866daa2045851b2e5681371668
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Dec 7 12:16:48 2009 +0200

    KVM: Add accessor for reading cr4 (or some bits of cr4)
    
    Some bits of cr4 can be owned by the guest on vmx, so when we read them,
    we copy them to the vcpu structure.  In preparation for making the set of
    guest-owned bits dynamic, use helpers to access these bits so we don't need
    to know where the bit resides.
    
    No changes to svm since all bits are host-owned there.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index da6dee862763..e9f4f12ec3c4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -272,6 +272,7 @@ struct kvm_vcpu_arch {
 	unsigned long cr2;
 	unsigned long cr3;
 	unsigned long cr4;
+	unsigned long cr4_guest_owned_bits;
 	unsigned long cr8;
 	u32 hflags;
 	u64 pdptrs[4]; /* pae */

commit cdc0e24456bf5678f63497569c3676c9019f82c1
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Dec 6 17:21:14 2009 +0200

    KVM: VMX: Move some cr[04] related constants to vmx.c
    
    They have no place in common code.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4f865e8b8540..da6dee862763 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -38,19 +38,6 @@
 #define CR3_L_MODE_RESERVED_BITS (CR3_NONPAE_RESERVED_BITS |	\
 				  0xFFFFFF0000000000ULL)
 
-#define KVM_GUEST_CR0_MASK_UNRESTRICTED_GUEST				\
-	(X86_CR0_WP | X86_CR0_NE | X86_CR0_NW | X86_CR0_CD)
-#define KVM_GUEST_CR0_MASK						\
-	(KVM_GUEST_CR0_MASK_UNRESTRICTED_GUEST | X86_CR0_PG | X86_CR0_PE)
-#define KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST				\
-	(X86_CR0_WP | X86_CR0_NE | X86_CR0_TS | X86_CR0_MP)
-#define KVM_VM_CR0_ALWAYS_ON						\
-	(KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST | X86_CR0_PG | X86_CR0_PE)
-#define KVM_GUEST_CR4_MASK						\
-	(X86_CR4_VME | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_PGE | X86_CR4_VMXE)
-#define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)
-#define KVM_RMODE_VM_CR4_ALWAYS_ON (X86_CR4_VME | X86_CR4_PAE | X86_CR4_VMXE)
-
 #define INVALID_PAGE (~(hpa_t)0)
 #define UNMAPPED_GVA (~(gpa_t)0)
 

commit d5696725b2a4c59503f5e0bc33adeee7f30cd45b
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Dec 2 12:28:47 2009 +0200

    KVM: VMX: Fix comparison of guest efer with stale host value
    
    update_transition_efer() masks out some efer bits when deciding whether
    to switch the msr during guest entry; for example, NX is emulated using the
    mmu so we don't need to disable it, and LMA/LME are handled by the hardware.
    
    However, with shared msrs, the comparison is made against a stale value;
    at the time of the guest switch we may be running with another guest's efer.
    
    Fix by deferring the mask/compare to the actual point of guest entry.
    
    Noted by Marcelo.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 06e085614dad..4f865e8b8540 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -812,6 +812,6 @@ int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
-void kvm_set_shared_msr(unsigned index, u64 val);
+void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit 3cfc3092f40bc37c57ba556cfd8de4218f2135ab
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Thu Nov 12 01:04:25 2009 +0100

    KVM: x86: Add KVM_GET/SET_VCPU_EVENTS
    
    This new IOCTL exports all yet user-invisible states related to
    exceptions, interrupts, and NMIs. Together with appropriate user space
    changes, this fixes sporadic problems of vmsave/restore, live migration
    and system reset.
    
    [avi: future-proof abi by adding a flags field]
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 26a74b7bb6bc..06e085614dad 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -523,6 +523,8 @@ struct kvm_x86_ops {
 				bool has_error_code, u32 error_code);
 	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
 	int (*nmi_allowed)(struct kvm_vcpu *vcpu);
+	bool (*get_nmi_mask)(struct kvm_vcpu *vcpu);
+	void (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);

commit 18863bdd60f895f3b3ba16b15e8331aee781e8ec
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Sep 7 11:12:18 2009 +0300

    KVM: x86 shared msr infrastructure
    
    The various syscall-related MSRs are fairly expensive to switch.  Currently
    we switch them on every vcpu preemption, which is far too often:
    
    - if we're switching to a kernel thread (idle task, threaded interrupt,
      kernel-mode virtio server (vhost-net), for example) and back, then
      there's no need to switch those MSRs since kernel threasd won't
      be exiting to userspace.
    
    - if we're switching to another guest running an identical OS, most likely
      those MSRs will have the same value, so there's little point in reloading
      them.
    
    - if we're running the same OS on the guest and host, the MSRs will have
      identical values and reloading is unnecessary.
    
    This patch uses the new user return notifiers to implement last-minute
    switching, and checks the msr values to avoid unnecessary reloading.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0558ff8c32ae..26a74b7bb6bc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -809,4 +809,7 @@ int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 
+void kvm_define_shared_msr(unsigned index, u32 msr);
+void kvm_set_shared_msr(unsigned index, u64 val);
+
 #endif /* _ASM_X86_KVM_HOST_H */

commit afbcf7ab8d1bc8c2d04792f6d9e786e0adeb328d
Author: Glauber Costa <glommer@redhat.com>
Date:   Fri Oct 16 15:28:36 2009 -0400

    KVM: allow userspace to adjust kvmclock offset
    
    When we migrate a kvm guest that uses pvclock between two hosts, we may
    suffer a large skew. This is because there can be significant differences
    between the monotonic clock of the hosts involved. When a new host with
    a much larger monotonic time starts running the guest, the view of time
    will be significantly impacted.
    
    Situation is much worse when we do the opposite, and migrate to a host with
    a smaller monotonic clock.
    
    This proposed ioctl will allow userspace to inform us what is the monotonic
    clock value in the source host, so we can keep the time skew short, and
    more importantly, never goes backwards. Userspace may also need to trigger
    the current data, since from the first migration onwards, it won't be
    reflected by a simple call to clock_gettime() anymore.
    
    [marcelo: future-proof abi with a flags field]
    [jan: fix KVM_GET_CLOCK by clearing flags field instead of checking it]
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4d994ad5051a..0558ff8c32ae 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -413,6 +413,7 @@ struct kvm_arch{
 
 	unsigned long irq_sources_bitmap;
 	u64 vm_init_tsc;
+	s64 kvmclock_offset;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
 };

commit 6be7d3062b59af891be7e40c6802350de5f78cef
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Sun Oct 18 13:24:54 2009 +0200

    KVM: SVM: Cleanup NMI singlestep
    
    Push the NMI-related singlestep variable into vcpu_svm. It's dealing
    with an AMD-specific deficit, nothing generic for x86.
    
    Acked-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    
     arch/x86/include/asm/kvm_host.h |    1 -
     arch/x86/kvm/svm.c              |   12 +++++++-----
     2 files changed, 7 insertions(+), 6 deletions(-)
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2536fbd85b3a..4d994ad5051a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -354,7 +354,6 @@ struct kvm_vcpu_arch {
 	unsigned int time_offset;
 	struct page *time_page;
 
-	bool singlestep; /* guest is single stepped by KVM */
 	bool nmi_pending;
 	bool nmi_injected;
 

commit 94fe45da48f921d01d8ff02a0ad54ee9c326d7f0
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Sun Oct 18 13:24:44 2009 +0200

    KVM: x86: Fix guest single-stepping while interruptible
    
    Commit 705c5323 opened the doors of hell by unconditionally injecting
    single-step flags as long as guest_debug signaled this. This doesn't
    work when the guest branches into some interrupt or exception handler
    and triggers a vmexit with flag reloading.
    
    Fix it by saving cs:rip when user space requests single-stepping and
    restricting the trace flag injection to this guest code position.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 36f3b53f5c27..2536fbd85b3a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -371,6 +371,10 @@ struct kvm_vcpu_arch {
 	u64 mcg_status;
 	u64 mcg_ctl;
 	u64 *mce_banks;
+
+	/* used for guest single stepping over the given code position */
+	u16 singlestep_cs;
+	unsigned long singlestep_rip;
 };
 
 struct kvm_mem_alias {

commit ffde22ac53b6d6b1d7206f1172176a667eead778
Author: Ed Swierk <eswierk@aristanetworks.com>
Date:   Thu Oct 15 15:21:43 2009 -0700

    KVM: Xen PV-on-HVM guest support
    
    Support for Xen PV-on-HVM guests can be implemented almost entirely in
    userspace, except for handling one annoying MSR that maps a Xen
    hypercall blob into guest address space.
    
    A generic mechanism to delegate MSR writes to userspace seems overkill
    and risks encouraging similar MSR abuse in the future.  Thus this patch
    adds special support for the Xen HVM MSR.
    
    I implemented a new ioctl, KVM_XEN_HVM_CONFIG, that lets userspace tell
    KVM which MSR the guest will write to, as well as the starting address
    and size of the hypercall blobs (one each for 32-bit and 64-bit) that
    userspace has loaded from files.  When the guest writes to the MSR, KVM
    copies one page of the blob from userspace to the guest.
    
    I've tested this patch with a hacked-up version of Gerd's userspace
    code, booting a number of guests (CentOS 5.3 i386 and x86_64, and
    FreeBSD 8.0-RC1 amd64) and exercising PV network and block devices.
    
    [jan: fix i386 build warning]
    [avi: future proof abi with a flags field]
    
    Signed-off-by: Ed Swierk <eswierk@aristanetworks.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 179a919f53a4..36f3b53f5c27 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -410,6 +410,8 @@ struct kvm_arch{
 
 	unsigned long irq_sources_bitmap;
 	u64 vm_init_tsc;
+
+	struct kvm_xen_hvm_config xen_hvm_config;
 };
 
 struct kvm_vm_stat {

commit 91586a3b7d79432772a3cdcb81473cd08a237c79
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Oct 5 13:07:21 2009 +0200

    KVM: x86: Rework guest single-step flag injection and filtering
    
    Push TF and RF injection and filtering on guest single-stepping into the
    vender get/set_rflags callbacks. This makes the whole mechanism more
    robust wrt user space IOCTL order and instruction emulations.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e7f870832603..179a919f53a4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -614,6 +614,9 @@ void kvm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l);
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 data);
 
+unsigned long kvm_get_rflags(struct kvm_vcpu *vcpu);
+void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);
+
 void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, unsigned long cr2,

commit 355be0b9300579e02275d7d19374806a974ce622
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Sat Oct 3 00:31:21 2009 +0200

    KVM: x86: Refactor guest debug IOCTL handling
    
    Much of so far vendor-specific code for setting up guest debug can
    actually be handled by the generic code. This also fixes a minor deficit
    in the SVM part /wrt processing KVM_GUESTDBG_ENABLE.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 295c7c4d9c90..e7f870832603 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -475,8 +475,8 @@ struct kvm_x86_ops {
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
 
-	int (*set_guest_debug)(struct kvm_vcpu *vcpu,
-			       struct kvm_guest_debug *dbg);
+	void (*set_guest_debug)(struct kvm_vcpu *vcpu,
+				struct kvm_guest_debug *dbg);
 	int (*get_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata);
 	int (*set_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
 	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);

commit 10474ae8945ce08622fd1f3464e55bd817bf2376
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Sep 15 11:37:46 2009 +0200

    KVM: Activate Virtualization On Demand
    
    X86 CPUs need to have some magic happening to enable the virtualization
    extensions on them. This magic can result in unpleasant results for
    users, like blocking other VMMs from working (vmx) or using invalid TLB
    entries (svm).
    
    Currently KVM activates virtualization when the respective kernel module
    is loaded. This blocks us from autoloading KVM modules without breaking
    other VMMs.
    
    To circumvent this problem at least a bit, this patch introduces on
    demand activation of virtualization. This means, that instead
    virtualization is enabled on creation of the first virtual machine
    and disabled on destruction of the last one.
    
    So using this, KVM can be easily autoloaded, while keeping other
    hypervisors usable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a46e2dd9aca8..295c7c4d9c90 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -459,7 +459,7 @@ struct descriptor_table {
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
-	void (*hardware_enable)(void *dummy);      /* __init */
+	int (*hardware_enable)(void *dummy);
 	void (*hardware_disable)(void *dummy);
 	void (*check_processor_compatibility)(void *rtn);
 	int (*hardware_setup)(void);               /* __init */

commit 136bdfeee7b5bc986fc94af3a40d7d13ea37bb95
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Aug 24 11:54:23 2009 +0300

    KVM: Move irq ack notifier list to arch independent code
    
    Mask irq notifier list is already there.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 35d3236c9de4..a46e2dd9aca8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -397,7 +397,6 @@ struct kvm_arch{
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;
-	struct hlist_head irq_ack_notifier_list;
 	int vapics_in_nmi_mode;
 
 	unsigned int tss_addr;

commit 1a6e4a8c276e122dbeb6f9c610f29735e4236bfd
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Aug 24 11:54:19 2009 +0300

    KVM: Move irq sharing information to irqchip level
    
    This removes assumptions that max GSIs is smaller than number of pins.
    Sharing is tracked on pin level not GSI level.
    
    [avi: no PIC on ia64]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0b113f2b58cf..35d3236c9de4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -410,7 +410,6 @@ struct kvm_arch{
 	gpa_t ept_identity_map_addr;
 
 	unsigned long irq_sources_bitmap;
-	unsigned long irq_states[KVM_IOAPIC_NUM_PINS];
 	u64 vm_init_tsc;
 };
 

commit 851ba6922ac575b749f63dee0ae072808163ba6a
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Aug 24 11:10:17 2009 +0300

    KVM: Don't pass kvm_run arguments
    
    They're just copies of vcpu->run, which is readily accessible.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d83892226f73..0b113f2b58cf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -506,8 +506,8 @@ struct kvm_x86_ops {
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);
 
-	void (*run)(struct kvm_vcpu *vcpu, struct kvm_run *run);
-	int (*handle_exit)(struct kvm_run *run, struct kvm_vcpu *vcpu);
+	void (*run)(struct kvm_vcpu *vcpu);
+	int (*handle_exit)(struct kvm_vcpu *vcpu);
 	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
 	void (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
 	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
@@ -568,7 +568,7 @@ enum emulation_result {
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
 #define EMULTYPE_SKIP		    (1 << 2)
-int emulate_instruction(struct kvm_vcpu *vcpu, struct kvm_run *run,
+int emulate_instruction(struct kvm_vcpu *vcpu,
 			unsigned long cr2, u16 error_code, int emulation_type);
 void kvm_report_emulation_failure(struct kvm_vcpu *cvpu, const char *context);
 void realmode_lgdt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
@@ -585,9 +585,9 @@ int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
 
 struct x86_emulate_ctxt;
 
-int kvm_emulate_pio(struct kvm_vcpu *vcpu, struct kvm_run *run, int in,
+int kvm_emulate_pio(struct kvm_vcpu *vcpu, int in,
 		     int size, unsigned port);
-int kvm_emulate_pio_string(struct kvm_vcpu *vcpu, struct kvm_run *run, int in,
+int kvm_emulate_pio_string(struct kvm_vcpu *vcpu, int in,
 			   int size, unsigned long count, int down,
 			    gva_t address, int rep, unsigned port);
 void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);

commit 3da0dd433dc399a8c0124d0614d82a09b6a49bce
Author: Izik Eidus <ieidus@redhat.com>
Date:   Wed Sep 23 21:47:18 2009 +0300

    KVM: add support for change_pte mmu notifiers
    
    this is needed for kvm if it want ksm to directly map pages into its
    shadow page tables.
    
    [marcelo: cast pfn assignment to u64]
    
    Signed-off-by: Izik Eidus <ieidus@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3be000435fad..d83892226f73 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -796,6 +796,7 @@ asmlinkage void kvm_handle_fault_on_reboot(void);
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);
+void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
 int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);

commit 0a79b009525b160081d75cef5dbf45817956acf2
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Sep 1 12:03:25 2009 +0300

    KVM: VMX: Check cpl before emulating debug register access
    
    Debug registers may only be accessed from cpl 0.  Unfortunately, vmx will
    code to emulate the instruction even though it was issued from guest
    userspace, possibly leading to an unexpected trap later.
    
    Cc: stable@kernel.org
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e8f166a02c79..3be000435fad 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -620,6 +620,7 @@ void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
 void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, unsigned long cr2,
 			   u32 error_code);
+bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);
 
 int kvm_pic_set_irq(void *opaque, int irq, int level);
 

commit 3d53c27d05950390712f92c5ad1604c60190ed64
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Sep 1 12:34:07 2009 +0300

    KVM: Use thread debug register storage instead of kvm specific data
    
    Instead of saving the debug registers from the processor to a kvm data
    structure, rely in the debug registers stored in the thread structure.
    This allows us not to save dr6 and dr7.
    
    Reduces lightweight vmexit cost by 350 cycles, or 11 percent.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 33901be75a36..e8f166a02c79 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -362,9 +362,6 @@ struct kvm_vcpu_arch {
 	u32 pat;
 
 	int switch_db_regs;
-	unsigned long host_db[KVM_NR_DB_REGS];
-	unsigned long host_dr6;
-	unsigned long host_dr7;
 	unsigned long db[KVM_NR_DB_REGS];
 	unsigned long dr6;
 	unsigned long dr7;

commit 56e8231841301ad38e347e33fd4319c89f697045
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Aug 12 15:04:37 2009 +0300

    KVM: Rename x86_emulate.c to emulate.c
    
    We're in arch/x86, what could we possibly be emulating?
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b17d845897b7..33901be75a36 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -138,7 +138,7 @@ enum {
 	VCPU_SREG_LDTR,
 };
 
-#include <asm/kvm_x86_emulate.h>
+#include <asm/kvm_emulate.h>
 
 #define KVM_NR_MEM_OBJS 40
 

commit 344f414fa0f16254dd07195d4cd11b2f92931d3d
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Jul 27 16:30:48 2009 +0200

    KVM: report 1GB page support to userspace
    
    If userspace knows that the kernel part supports 1GB pages it can enable
    the corresponding cpuid bit so that guests actually use GB pages.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3315efaacf9e..b17d845897b7 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -528,6 +528,8 @@ struct kvm_x86_ops {
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
+	bool (*gb_page_enable)(void);
+
 	const struct trace_print_flags *exit_reasons_str;
 };
 

commit 04326caacff2b162d359c15a2edf634448897d1a
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Jul 27 16:30:47 2009 +0200

    KVM: MMU: enable gbpages by increasing nr of pagesizes
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c9fb2bc13a81..3315efaacf9e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -55,7 +55,7 @@
 #define UNMAPPED_GVA (~(gpa_t)0)
 
 /* KVM Hugepage definitions for x86 */
-#define KVM_NR_PAGE_SIZES	2
+#define KVM_NR_PAGE_SIZES	3
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + (((x) - 1) * 9))
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))

commit 7e4e4056f72da51c5dede48515df0ecd20eaf8ca
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Jul 27 16:30:46 2009 +0200

    KVM: MMU: shadow support for 1gb pages
    
    This patch adds support for shadow paging to the 1gb page table code in KVM.
    With this code the guest can use 1gb pages even if the host does not support
    them.
    
    [ Marcelo: fix shadow page collision on pmd level if a guest 1gb page is mapped
               with 4kb ptes on host level ]
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e09dc26d96bd..c9fb2bc13a81 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -315,7 +315,6 @@ struct kvm_vcpu_arch {
 	struct {
 		gfn_t gfn;	/* presumed gfn during guest pte update */
 		pfn_t pfn;	/* pfn corresponding to that gfn */
-		int level;
 		unsigned long mmu_seq;
 	} update_pte;
 

commit 852e3c19ac64b7c3912e8efe42d3ce090ebc0161
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Jul 27 16:30:44 2009 +0200

    KVM: MMU: make direct mapping paths aware of mapping levels
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e210b218df44..e09dc26d96bd 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -315,7 +315,7 @@ struct kvm_vcpu_arch {
 	struct {
 		gfn_t gfn;	/* presumed gfn during guest pte update */
 		pfn_t pfn;	/* pfn corresponding to that gfn */
-		int largepage;
+		int level;
 		unsigned long mmu_seq;
 	} update_pte;
 

commit b927a3cec081a605142f5b7e90b730611bee28b1
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Jul 21 10:42:48 2009 +0800

    KVM: VMX: Introduce KVM_SET_IDENTITY_MAP_ADDR ioctl
    
    Now KVM allow guest to modify guest's physical address of EPT's identity mapping page.
    
    (change from v1, discard unnecessary check, change ioctl to accept parameter
    address rather than value)
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 08732d7b6d98..e210b218df44 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -411,6 +411,7 @@ struct kvm_arch{
 
 	struct page *ept_identity_pagetable;
 	bool ept_identity_pagetable_done;
+	gpa_t ept_identity_map_addr;
 
 	unsigned long irq_sources_bitmap;
 	unsigned long irq_states[KVM_IOAPIC_NUM_PINS];

commit a1b37100d9e29c1f8dc3e2f5490a205c80180e01
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Jul 9 15:33:52 2009 +0300

    KVM: Reduce runnability interface with arch support code
    
    Remove kvm_cpu_has_interrupt() and kvm_arch_interrupt_allowed() from
    interface between general code and arch code. kvm_arch_vcpu_runnable()
    checks for interrupts instead.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3f4f00a23536..08732d7b6d98 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -797,6 +797,8 @@ asmlinkage void kvm_handle_fault_on_reboot(void);
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);
 int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);
+int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);
+int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit 0b71785dc05f1f66e6268022b9953c0d6a9985c6
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Jul 9 15:33:53 2009 +0300

    KVM: Move kvm_cpu_get_interrupt() declaration to x86 code
    
    It is implemented only by x86.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 30b625d8e5f0..3f4f00a23536 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -797,5 +797,6 @@ asmlinkage void kvm_handle_fault_on_reboot(void);
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);
 int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);
+int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit ec04b2604c3707a46db1d26d98f82b11d0844669
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Jun 19 15:16:23 2009 +0200

    KVM: Prepare memslot data structures for multiple hugepage sizes
    
    [avi: fix build on non-x86]
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 19027ab20412..30b625d8e5f0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -54,12 +54,12 @@
 #define INVALID_PAGE (~(hpa_t)0)
 #define UNMAPPED_GVA (~(gpa_t)0)
 
-/* shadow tables are PAE even on non-PAE hosts */
-#define KVM_HPAGE_SHIFT 21
-#define KVM_HPAGE_SIZE (1UL << KVM_HPAGE_SHIFT)
-#define KVM_HPAGE_MASK (~(KVM_HPAGE_SIZE - 1))
-
-#define KVM_PAGES_PER_HPAGE (KVM_HPAGE_SIZE / PAGE_SIZE)
+/* KVM Hugepage definitions for x86 */
+#define KVM_NR_PAGE_SIZES	2
+#define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + (((x) - 1) * 9))
+#define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+#define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+#define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define DE_VECTOR 0
 #define DB_VECTOR 1

commit 229456fc34b1c9031b04f7581e7b755d1cebfe9c
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jun 17 09:22:14 2009 -0300

    KVM: convert custom marker based tracing to event traces
    
    This allows use of the powerful ftrace infrastructure.
    
    See Documentation/trace/ for usage information.
    
    [avi, stephen: various build fixes]
    [sheng: fix control register breakage]
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c7b0cc2b7020..19027ab20412 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -14,6 +14,7 @@
 #include <linux/types.h>
 #include <linux/mm.h>
 #include <linux/mmu_notifier.h>
+#include <linux/tracepoint.h>
 
 #include <linux/kvm.h>
 #include <linux/kvm_para.h>
@@ -527,6 +528,7 @@ struct kvm_x86_ops {
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
+	const struct trace_print_flags *exit_reasons_str;
 };
 
 extern struct kvm_x86_ops *kvm_x86_ops;

commit 7ffd92c53c5ebd0ad5a68ac3ca033c3a06374d19
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Jun 9 14:10:45 2009 +0300

    KVM: VMX: Move rmode structure to vmx-specific code
    
    rmode is only used in vmx, so move it to vmx.c
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a1a96a57bb9d..c7b0cc2b7020 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -340,16 +340,6 @@ struct kvm_vcpu_arch {
 		u8 nr;
 	} interrupt;
 
-	struct {
-		int vm86_active;
-		u8 save_iopl;
-		struct kvm_save_segment {
-			u16 selector;
-			unsigned long base;
-			u32 limit;
-			u32 ar;
-		} tr, es, ds, fs, gs;
-	} rmode;
 	int halt_request; /* real mode on Intel only */
 
 	int cpuid_nent;

commit 3a624e29c7587b79abab60e279f9d1a62a3d4716
Author: Nitin A Kamble <nitin.a.kamble@intel.com>
Date:   Mon Jun 8 11:34:16 2009 -0700

    KVM: VMX: Support Unrestricted Guest feature
    
    "Unrestricted Guest" feature is added in the VMX specification.
    Intel Westmere and onwards processors will support this feature.
    
        It allows kvm guests to run real mode and unpaged mode
    code natively in the VMX mode when EPT is turned on. With the
    unrestricted guest there is no need to emulate the guest real mode code
    in the vm86 container or in the emulator. Also the guest big real mode
    code works like native.
    
      The attached patch enhances KVM to use the unrestricted guest feature
    if available on the processor. It also adds a new kernel/module
    parameter to disable the unrestricted guest feature at the boot time.
    
    Signed-off-by: Nitin A Kamble <nitin.a.kamble@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1cc901ec4ba5..a1a96a57bb9d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -37,12 +37,14 @@
 #define CR3_L_MODE_RESERVED_BITS (CR3_NONPAE_RESERVED_BITS |	\
 				  0xFFFFFF0000000000ULL)
 
-#define KVM_GUEST_CR0_MASK				   \
-	(X86_CR0_PG | X86_CR0_PE | X86_CR0_WP | X86_CR0_NE \
-	 | X86_CR0_NW | X86_CR0_CD)
+#define KVM_GUEST_CR0_MASK_UNRESTRICTED_GUEST				\
+	(X86_CR0_WP | X86_CR0_NE | X86_CR0_NW | X86_CR0_CD)
+#define KVM_GUEST_CR0_MASK						\
+	(KVM_GUEST_CR0_MASK_UNRESTRICTED_GUEST | X86_CR0_PG | X86_CR0_PE)
+#define KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST				\
+	(X86_CR0_WP | X86_CR0_NE | X86_CR0_TS | X86_CR0_MP)
 #define KVM_VM_CR0_ALWAYS_ON						\
-	(X86_CR0_PG | X86_CR0_PE | X86_CR0_WP | X86_CR0_NE | X86_CR0_TS \
-	 | X86_CR0_MP)
+	(KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST | X86_CR0_PG | X86_CR0_PE)
 #define KVM_GUEST_CR4_MASK						\
 	(X86_CR4_VME | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_PGE | X86_CR4_VMXE)
 #define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)

commit 6de4f3ada40b336522250a7832a0cc4de8856589
Author: Avi Kivity <avi@redhat.com>
Date:   Sun May 31 22:58:47 2009 +0300

    KVM: Cache pdptrs
    
    Instead of reloading the pdptrs on every entry and exit (vmcs writes on vmx,
    guest memory access on svm) extract them on demand.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 81c68f630b14..1cc901ec4ba5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -120,6 +120,10 @@ enum kvm_reg {
 	NR_VCPU_REGS
 };
 
+enum kvm_reg_ex {
+	VCPU_EXREG_PDPTR = NR_VCPU_REGS,
+};
+
 enum {
 	VCPU_SREG_ES,
 	VCPU_SREG_CS,

commit 890ca9aefa78f7831f8f633cab9e4803636dffe4
Author: Huang Ying <ying.huang@intel.com>
Date:   Mon May 11 16:48:15 2009 +0800

    KVM: Add MCE support
    
    The related MSRs are emulated. MCE capability is exported via
    extension KVM_CAP_MCE and ioctl KVM_X86_GET_MCE_CAP_SUPPORTED.  A new
    vcpu ioctl command KVM_X86_SETUP_MCE is used to setup MCE emulation
    such as the mcg_cap. MCE is injected via vcpu ioctl command
    KVM_X86_SET_MCE. Extended machine-check state (MCG_EXT_P) and CMCI are
    not implemented.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 79561752af97..81c68f630b14 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -373,6 +373,11 @@ struct kvm_vcpu_arch {
 	unsigned long dr6;
 	unsigned long dr7;
 	unsigned long eff_db[KVM_NR_DB_REGS];
+
+	u64 mcg_cap;
+	u64 mcg_status;
+	u64 mcg_ctl;
+	u64 *mce_banks;
 };
 
 struct kvm_mem_alias {

commit af24a4e4aec77ef16c1971cf4465f767ba946034
Author: Jaswinder Singh Rajput <jaswinder@kernel.org>
Date:   Fri May 15 18:42:05 2009 +0530

    KVM: Replace MSR_IA32_TIME_STAMP_COUNTER with MSR_IA32_TSC of msr-index.h
    
    Use standard msr-index.h's MSR declaration.
    
    MSR_IA32_TSC is better than MSR_IA32_TIME_STAMP_COUNTER as it also solves
    80 column issue.
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index eabdc1cfab5c..79561752af97 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -752,8 +752,6 @@ static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 }
 
-#define MSR_IA32_TIME_STAMP_COUNTER		0x010
-
 #define TSS_IOPB_BASE_OFFSET 0x66
 #define TSS_BASE_SIZE 0x68
 #define TSS_IOPB_SIZE (65536 / 8)

commit 56b237e31abf4d6dbc6e2a0214049b9a23be4883
Author: Nitin A Kamble <nitin.a.kamble@intel.com>
Date:   Thu Jun 4 15:04:08 2009 -0700

    KVM: VMX: Rename rmode.active to rmode.vm86_active
    
    That way the interpretation of rmode.active becomes more clear with
    unrestricted guest code.
    
    Signed-off-by: Nitin A Kamble <nitin.a.kamble@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 82129437e873..eabdc1cfab5c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -335,7 +335,7 @@ struct kvm_vcpu_arch {
 	} interrupt;
 
 	struct {
-		int active;
+		int vm86_active;
 		u8 save_iopl;
 		struct kvm_save_segment {
 			u16 selector;

commit 44c11430b52cbad0a467bc023a802d122dfd285c
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon May 11 13:35:52 2009 +0300

    KVM: inject NMI after IRET from a previous NMI, not before.
    
    If NMI is received during handling of another NMI it should be injected
    immediately after IRET from previous NMI handler, but SVM intercept IRET
    before instruction execution so we can't inject pending NMI at this
    point and there is not way to request exit when NMI window opens. This
    patch fix SVM code to open NMI window after IRET by single stepping over
    IRET instruction.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1d6c3f757cb6..82129437e873 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -358,6 +358,7 @@ struct kvm_vcpu_arch {
 	unsigned int time_offset;
 	struct page *time_page;
 
+	bool singlestep; /* guest is single stepped by KVM */
 	bool nmi_pending;
 	bool nmi_injected;
 
@@ -771,6 +772,7 @@ enum {
 #define HF_HIF_MASK		(1 << 1)
 #define HF_VINTR_MASK		(1 << 2)
 #define HF_NMI_MASK		(1 << 3)
+#define HF_IRET_MASK		(1 << 4)
 
 /*
  * Hardware virtualization extension instructions may fault if a

commit 66fd3f7f901f29a557a473af595bf11b270b9ac2
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon May 11 13:35:50 2009 +0300

    KVM: Do not re-execute INTn instruction.
    
    Re-inject event instead. This is what Intel suggest. Also use correct
    instruction length when re-injecting soft fault/interrupt.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 977a785a9d75..1d6c3f757cb6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -319,6 +319,8 @@ struct kvm_vcpu_arch {
 	struct kvm_pio_request pio;
 	void *pio_data;
 
+	u8 event_exit_inst_len;
+
 	struct kvm_queued_exception {
 		bool pending;
 		bool has_error_code;
@@ -328,6 +330,7 @@ struct kvm_vcpu_arch {
 
 	struct kvm_queued_interrupt {
 		bool pending;
+		bool soft;
 		u8 nr;
 	} interrupt;
 
@@ -510,7 +513,7 @@ struct kvm_x86_ops {
 	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
 	void (*patch_hypercall)(struct kvm_vcpu *vcpu,
 				unsigned char *hypercall_addr);
-	void (*set_irq)(struct kvm_vcpu *vcpu, int vec);
+	void (*set_irq)(struct kvm_vcpu *vcpu);
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code);

commit 923c61bbc6413e87e5f6b0bae663d202a8de0537
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon May 11 13:35:48 2009 +0300

    KVM: Remove irq_pending bitmap
    
    Only one interrupt vector can be injected from userspace irqchip at
    any given time so no need to store it in a bitmap. Put it into interrupt
    queue directly.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 16d1481aa231..977a785a9d75 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -266,8 +266,6 @@ struct kvm_mmu {
 
 struct kvm_vcpu_arch {
 	u64 host_tsc;
-	unsigned long irq_summary; /* bit vector: 1 per word in irq_pending */
-	DECLARE_BITMAP(irq_pending, KVM_NR_INTERRUPTS);
 	/*
 	 * rip and regs accesses must go through
 	 * kvm_{register,rip}_{read,write} functions.

commit 2809f5d2c4cfad171167b131bb2a21ab65eba40f
Author: Glauber Costa <glommer@redhat.com>
Date:   Tue May 12 16:21:05 2009 -0400

    KVM: Replace ->drop_interrupt_shadow() by ->set_interrupt_shadow()
    
    This patch replaces drop_interrupt_shadow with the more
    general set_interrupt_shadow, that can either drop or raise
    it, depending on its parameter.  It also adds ->get_interrupt_shadow()
    for future use.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    CC: H. Peter Anvin <hpa@zytor.com>
    CC: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ab7de4a11955..16d1481aa231 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -508,6 +508,8 @@ struct kvm_x86_ops {
 	void (*run)(struct kvm_vcpu *vcpu, struct kvm_run *run);
 	int (*handle_exit)(struct kvm_run *run, struct kvm_vcpu *vcpu);
 	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
+	void (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
+	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
 	void (*patch_hypercall)(struct kvm_vcpu *vcpu,
 				unsigned char *hypercall_addr);
 	void (*set_irq)(struct kvm_vcpu *vcpu, int vec);
@@ -519,7 +521,6 @@ struct kvm_x86_ops {
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
-	void (*drop_interrupt_shadow)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);

commit d6a8c875f35a6e1b3fb3f21e93eabb183b1f39ee
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Apr 20 18:10:07 2009 +0200

    KVM: Drop request_nmi from stats
    
    The stats entry request_nmi is no longer used as the related user space
    interface was dropped. So clean it up.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 253d8f669cf6..ab7de4a11955 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -441,7 +441,6 @@ struct kvm_vcpu_stat {
 	u32 halt_exits;
 	u32 halt_wakeup;
 	u32 request_irq_exits;
-	u32 request_nmi_exits;
 	u32 irq_exits;
 	u32 host_state_reload;
 	u32 efer_reload;

commit 522c68c4416de3cd3e11a9ff10d58e776a69ae1e
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon Apr 27 20:35:43 2009 +0800

    KVM: Enable snooping control for supported hardware
    
    Memory aliases with different memory type is a problem for guest. For the guest
    without assigned device, the memory type of guest memory would always been the
    same as host(WB); but for the assigned device, some part of memory may be used
    as DMA and then set to uncacheable memory type(UC/WC), which would be a conflict of
    host memory type then be a potential issue.
    
    Snooping control can guarantee the cache correctness of memory go through the
    DMA engine of VT-d.
    
    [avi: fix build on ia64]
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8a6f6b643dfe..253d8f669cf6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -393,6 +393,7 @@ struct kvm_arch{
 	struct list_head active_mmu_pages;
 	struct list_head assigned_dev_head;
 	struct iommu_domain *iommu_domain;
+	int iommu_flags;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;

commit 4b12f0de33a64dfc624b2480f55b674f7fa23ef2
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon Apr 27 20:35:42 2009 +0800

    KVM: Replace get_mt_mask_shift with get_mt_mask
    
    Shadow_mt_mask is out of date, now it have only been used as a flag to indicate
    if TDP enabled. Get rid of it and use tdp_enabled instead.
    
    Also put memory type logical in kvm_x86_ops->get_mt_mask().
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3e94d0513208..8a6f6b643dfe 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -522,7 +522,7 @@ struct kvm_x86_ops {
 	void (*drop_interrupt_shadow)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
-	int (*get_mt_mask_shift)(void);
+	u64 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);
 };
 
 extern struct kvm_x86_ops *kvm_x86_ops;
@@ -536,7 +536,7 @@ int kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_set_nonpresent_ptes(u64 trap_pte, u64 notrap_pte);
 void kvm_mmu_set_base_ptes(u64 base_pte);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
-		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 mt_mask);
+		u64 dirty_mask, u64 nx_mask, u64 x_mask);
 
 int kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
@@ -550,6 +550,7 @@ int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
 			  const void *val, int bytes);
 int kvm_pv_mmu_op(struct kvm_vcpu *vcpu, unsigned long bytes,
 		  gpa_t addr, unsigned long *ret);
+u8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 extern bool tdp_enabled;
 

commit 14d0bc1f7c8226d5088e7182c3b53e0c7e91d1af
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 21 17:45:11 2009 +0300

    KVM: Get rid of get_irq() callback
    
    It just returns pending IRQ vector from the queue for VMX/SVM.
    Get IRQ directly from the queue before migration and put it back
    after.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index dd9ecd3de90d..3e94d0513208 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -510,7 +510,6 @@ struct kvm_x86_ops {
 	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
 	void (*patch_hypercall)(struct kvm_vcpu *vcpu,
 				unsigned char *hypercall_addr);
-	int (*get_irq)(struct kvm_vcpu *vcpu);
 	void (*set_irq)(struct kvm_vcpu *vcpu, int vec);
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,

commit 95ba82731374eb1c2af4dd442526c4b314f0e8b6
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 21 17:45:08 2009 +0300

    KVM: SVM: Add NMI injection support
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 53533ea17555..dd9ecd3de90d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -512,10 +512,15 @@ struct kvm_x86_ops {
 				unsigned char *hypercall_addr);
 	int (*get_irq)(struct kvm_vcpu *vcpu);
 	void (*set_irq)(struct kvm_vcpu *vcpu, int vec);
+	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code);
-	void (*inject_pending_irq)(struct kvm_vcpu *vcpu, struct kvm_run *run);
 	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
+	int (*nmi_allowed)(struct kvm_vcpu *vcpu);
+	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
+	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
+	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
+	void (*drop_interrupt_shadow)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	int (*get_mt_mask_shift)(void);
@@ -763,6 +768,7 @@ enum {
 #define HF_GIF_MASK		(1 << 0)
 #define HF_HIF_MASK		(1 << 1)
 #define HF_VINTR_MASK		(1 << 2)
+#define HF_NMI_MASK		(1 << 3)
 
 /*
  * Hardware virtualization extension instructions may fault if a

commit c4282df98ae0993983924c00ed76428a6609d68b
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 21 17:45:07 2009 +0300

    KVM: Get rid of arch.interrupt_window_open & arch.nmi_window_open
    
    They are recalculated before each use anyway.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aa5a54eb4da4..53533ea17555 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -266,7 +266,6 @@ struct kvm_mmu {
 
 struct kvm_vcpu_arch {
 	u64 host_tsc;
-	int interrupt_window_open;
 	unsigned long irq_summary; /* bit vector: 1 per word in irq_pending */
 	DECLARE_BITMAP(irq_pending, KVM_NR_INTERRUPTS);
 	/*
@@ -360,7 +359,6 @@ struct kvm_vcpu_arch {
 
 	bool nmi_pending;
 	bool nmi_injected;
-	bool nmi_window_open;
 
 	struct mtrr_state_type mtrr_state;
 	u32 pat;

commit 1d6ed0cb95a2f0839e1a31f1971dc37cd60c258a
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 21 17:45:03 2009 +0300

    KVM: Remove inject_pending_vectors() callback
    
    It is the same as inject_pending_irq() for VMX/SVM now.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ea3741edbec3..aa5a54eb4da4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -517,8 +517,6 @@ struct kvm_x86_ops {
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code);
 	void (*inject_pending_irq)(struct kvm_vcpu *vcpu, struct kvm_run *run);
-	void (*inject_pending_vectors)(struct kvm_vcpu *vcpu,
-				       struct kvm_run *run);
 	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);

commit 1cb948ae86f3d95cce58fac51d00766825f5f783
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 21 17:45:02 2009 +0300

    KVM: Remove exception_injected() callback.
    
    It always return false for VMX/SVM now.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5edae351b5dc..ea3741edbec3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -516,7 +516,6 @@ struct kvm_x86_ops {
 	void (*set_irq)(struct kvm_vcpu *vcpu, int vec);
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code);
-	bool (*exception_injected)(struct kvm_vcpu *vcpu);
 	void (*inject_pending_irq)(struct kvm_vcpu *vcpu, struct kvm_run *run);
 	void (*inject_pending_vectors)(struct kvm_vcpu *vcpu,
 				       struct kvm_run *run);

commit 863e8e658ee9ac6e5931b295eb7428456e450a0f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 21 17:44:57 2009 +0300

    KVM: VMX: Consolidate userspace and kernel interrupt injection for VMX
    
    Use the same callback to inject irq/nmi events no matter what irqchip is
    in use. Only from VMX for now.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index cb306cff2b49..5edae351b5dc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -517,7 +517,7 @@ struct kvm_x86_ops {
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code);
 	bool (*exception_injected)(struct kvm_vcpu *vcpu);
-	void (*inject_pending_irq)(struct kvm_vcpu *vcpu);
+	void (*inject_pending_irq)(struct kvm_vcpu *vcpu, struct kvm_run *run);
 	void (*inject_pending_vectors)(struct kvm_vcpu *vcpu,
 				       struct kvm_run *run);
 	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);

commit ba8afb6b0a2c7e06da760ffe5d078245058619b5
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Apr 12 13:36:57 2009 +0300

    KVM: x86 emulator: Add new mode of instruction emulation: skip
    
    In the new mode instruction is decoded, but not executed. The EIP
    is moved to point after the instruction.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0e3a7c6e522c..cb306cff2b49 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -562,6 +562,7 @@ enum emulation_result {
 
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
+#define EMULTYPE_SKIP		    (1 << 2)
 int emulate_instruction(struct kvm_vcpu *vcpu, struct kvm_run *run,
 			unsigned long cr2, u16 error_code, int emulation_type);
 void kvm_report_emulation_failure(struct kvm_vcpu *cvpu, const char *context);

commit c2d0ee46e6e633a3c23ecbcb9b03ad731906cd79
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Sun Apr 5 14:54:47 2009 -0300

    KVM: MMU: remove global page optimization logic
    
    Complexity to fix it not worthwhile the gains, as discussed
    in http://article.gmane.org/gmane.comp.emulators.kvm.devel/28649.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3fc46238476c..0e3a7c6e522c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -213,7 +213,6 @@ struct kvm_mmu_page {
 	int multimapped;         /* More than one parent_pte? */
 	int root_count;          /* Currently serving as active root */
 	bool unsync;
-	bool global;
 	unsigned int unsync_children;
 	union {
 		u64 *parent_pte;               /* !multimapped */
@@ -395,7 +394,6 @@ struct kvm_arch{
 	 */
 	struct list_head active_mmu_pages;
 	struct list_head assigned_dev_head;
-	struct list_head oos_global_pages;
 	struct iommu_domain *iommu_domain;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
@@ -425,7 +423,6 @@ struct kvm_vm_stat {
 	u32 mmu_recycled;
 	u32 mmu_cache_miss;
 	u32 mmu_unsync;
-	u32 mmu_unsync_global;
 	u32 remote_tlb_flush;
 	u32 lpages;
 };
@@ -640,7 +637,6 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
-void kvm_mmu_sync_global(struct kvm_vcpu *vcpu);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 

commit 9645bb56b31a1b70ab9e470387b5264cafc04aa9
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Mar 31 11:31:54 2009 +0300

    KVM: MMU: Use different shadows when EFER.NXE changes
    
    A pte that is shadowed when the guest EFER.NXE=1 is not valid when
    EFER.NXE=0; if bit 63 is set, the pte should cause a fault, and since the
    shadow EFER always has NX enabled, this won't happen.
    
    Fix by using a different shadow page table for different EFER.NXE bits.  This
    allows vcpus to run correctly with different values of EFER.NXE, and for
    transitions on this bit to be handled correctly without requiring a full
    flush.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 548b97d284d3..3fc46238476c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -185,6 +185,7 @@ union kvm_mmu_page_role {
 		unsigned access:3;
 		unsigned invalid:1;
 		unsigned cr4_pge:1;
+		unsigned nxe:1;
 	};
 };
 

commit 82725b20e22fb85377f61a16f6d0d5cfc28b45d3
Author: Dong, Eddie <eddie.dong@intel.com>
Date:   Mon Mar 30 16:21:08 2009 +0800

    KVM: MMU: Emulate #PF error code of reserved bits violation
    
    Detect, indicate, and propagate page faults where reserved bits are set.
    Take care to handle the different paging modes, each of which has different
    sets of reserved bits.
    
    [avi: fix pte reserved bits for efer.nxe=0]
    
    Signed-off-by: Eddie Dong <eddie.dong@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8351c4d00ac0..548b97d284d3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -261,6 +261,7 @@ struct kvm_mmu {
 	union kvm_mmu_page_role base_role;
 
 	u64 *pae_root;
+	u64 rsvd_bits_mask[2][4];
 };
 
 struct kvm_vcpu_arch {
@@ -791,5 +792,6 @@ asmlinkage void kvm_handle_fault_on_reboot(void);
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);
+int cpuid_maxphyaddr(struct kvm_vcpu *vcpu);
 
 #endif /* _ASM_X86_KVM_HOST_H */

commit 78646121e9a2fcf7977cc15966420e572a450bc3
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Mar 23 12:12:11 2009 +0200

    KVM: Fix interrupt unhalting a vcpu when it shouldn't
    
    kvm_vcpu_block() unhalts vpu on an interrupt/timer without checking
    if interrupt window is actually opened.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 46276273a1a1..8351c4d00ac0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -521,7 +521,7 @@ struct kvm_x86_ops {
 	void (*inject_pending_irq)(struct kvm_vcpu *vcpu);
 	void (*inject_pending_vectors)(struct kvm_vcpu *vcpu,
 				       struct kvm_run *run);
-
+	int (*interrupt_allowed)(struct kvm_vcpu *vcpu);
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
 	int (*get_mt_mask_shift)(void);

commit e1035715ef8d3171e29f9c6aee6f40d57b3fead5
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 5 16:34:59 2009 +0200

    KVM: change the way how lowest priority vcpu is calculated
    
    The new way does not require additional loop over vcpus to calculate
    the one with lowest priority as one is chosen during delivery bitmap
    construction.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f0faf58044ff..46276273a1a1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -286,6 +286,7 @@ struct kvm_vcpu_arch {
 	u64 shadow_efer;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
+	int32_t apic_arb_prio;
 	int mp_state;
 	int sipi_vector;
 	u64 ia32_misc_enable_msr;
@@ -400,7 +401,6 @@ struct kvm_arch{
 	struct hlist_head irq_ack_notifier_list;
 	int vapics_in_nmi_mode;
 
-	int round_robin_prev_vcpu;
 	unsigned int tss_addr;
 	struct page *apic_access_page;
 

commit 4925663a079c77d95d8685228ad6675fc5639c8e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Feb 4 17:28:14 2009 +0200

    KVM: Report IRQ injection status to userspace.
    
    IRQ injection status is either -1 (if there was no CPU found
    that should except the interrupt because IRQ was masked or
    ioapic was misconfigured or ...) or >= 0 in that case the
    number indicates to how many CPUs interrupt was injected.
    If the value is 0 it means that the interrupt was coalesced
    and probably should be reinjected.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 55fd4c5fd388..f0faf58044ff 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -616,7 +616,7 @@ void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, unsigned long cr2,
 			   u32 error_code);
 
-void kvm_pic_set_irq(void *opaque, int irq, int level);
+int kvm_pic_set_irq(void *opaque, int irq, int level);
 
 void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 

commit f6e2c02b6d28ddabe99377c5640a833407a62632
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Jan 11 13:02:10 2009 +0200

    KVM: MMU: Rename "metaphysical" attribute to "direct"
    
    This actually describes what is going on, rather than alerting the reader
    that something strange is going on.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 863ea73431ad..55fd4c5fd388 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -170,7 +170,8 @@ struct kvm_pte_chain {
  *   bits 0:3 - total guest paging levels (2-4, or zero for real mode)
  *   bits 4:7 - page table level for this shadow (1-4)
  *   bits 8:9 - page table quadrant for 2-level guests
- *   bit   16 - "metaphysical" - gfn is not a real page (huge page/real mode)
+ *   bit   16 - direct mapping of virtual to physical mapping at gfn
+ *              used for real mode and two-dimensional paging
  *   bits 17:19 - common access permissions for all ptes in this shadow page
  */
 union kvm_mmu_page_role {
@@ -180,7 +181,7 @@ union kvm_mmu_page_role {
 		unsigned level:4;
 		unsigned quadrant:2;
 		unsigned pad_for_nice_hex_output:6;
-		unsigned metaphysical:1;
+		unsigned direct:1;
 		unsigned access:3;
 		unsigned invalid:1;
 		unsigned cr4_pge:1;

commit 1c08364c3565242f1e1bd585bc2ce458967941af
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Jan 4 12:39:07 2009 +0200

    KVM: Move struct kvm_pio_request into x86 kvm_host.h
    
    This is an x86 specific stucture and has no business living in common code.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b74576aec19a..863ea73431ad 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -227,6 +227,18 @@ struct kvm_pv_mmu_op_buffer {
 	char buf[512] __aligned(sizeof(long));
 };
 
+struct kvm_pio_request {
+	unsigned long count;
+	int cur_count;
+	gva_t guest_gva;
+	int in;
+	int port;
+	int size;
+	int string;
+	int down;
+	int rep;
+};
+
 /*
  * x86 supports 3 paging modes (4-level 64-bit, 3-level 64-bit, and 2-level
  * 32-bit).  The kvm_mmu structure abstracts the details of the current mmu

commit 77c2002e7c6f019f59a6f3cc5f8b16b41748dbe1
Author: Izik Eidus <ieidus@redhat.com>
Date:   Mon Dec 29 01:42:19 2008 +0200

    KVM: introduce kvm_read_guest_virt, kvm_write_guest_virt
    
    This commit change the name of emulator_read_std into kvm_read_guest_virt,
    and add new function name kvm_write_guest_virt that allow writing into a
    guest virtual address.
    
    Signed-off-by: Izik Eidus <ieidus@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9efc446b5ac6..b74576aec19a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -609,10 +609,6 @@ void kvm_inject_nmi(struct kvm_vcpu *vcpu);
 
 void fx_init(struct kvm_vcpu *vcpu);
 
-int emulator_read_std(unsigned long addr,
-		      void *val,
-		      unsigned int bytes,
-		      struct kvm_vcpu *vcpu);
 int emulator_write_emulated(unsigned long addr,
 			    const void *val,
 			    unsigned int bytes,

commit 53f658b3c33616a4997ee254311b335e59063289
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Dec 11 20:45:05 2008 +0100

    KVM: VMX: initialize TSC offset relative to vm creation time
    
    VMX initializes the TSC offset for each vcpu at different times, and
    also reinitializes it for vcpus other than 0 on APIC SIPI message.
    
    This bug causes the TSC's to appear unsynchronized in the guest, even if
    the host is good.
    
    Older Linux kernels don't handle the situation very well, so
    gettimeofday is likely to go backwards in time:
    
    http://www.mail-archive.com/kvm@vger.kernel.org/msg02955.html
    http://sourceforge.net/tracker/index.php?func=detail&aid=2025534&group_id=180599&atid=893831
    
    Fix it by initializating the offset of each vcpu relative to vm creation
    time, and moving it from vmx_vcpu_reset to vmx_vcpu_setup, out of the
    APIC MP init path.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c2a01d0513f5..9efc446b5ac6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -398,6 +398,7 @@ struct kvm_arch{
 
 	unsigned long irq_sources_bitmap;
 	unsigned long irq_states[KVM_IOAPIC_NUM_PINS];
+	u64 vm_init_tsc;
 };
 
 struct kvm_vm_stat {

commit 2f0b3d60b2c43aef7cd10169c425c052169c622a
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Dec 21 19:27:36 2008 +0200

    KVM: MMU: Segregate mmu pages created with different cr4.pge settings
    
    Don't allow a vcpu with cr4.pge cleared to use a shadow page created with
    cr4.pge set; this might cause a cr3 switch not to sync ptes that have the
    global bit set (the global bit has no effect if !cr4.pge).
    
    This can only occur on smp with different cr4.pge settings for different
    vcpus (since a cr4 change will resync the shadow ptes), but there's no
    cost to being correct here.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 28f875f28f58..c2a01d0513f5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -183,6 +183,7 @@ union kvm_mmu_page_role {
 		unsigned metaphysical:1;
 		unsigned access:3;
 		unsigned invalid:1;
+		unsigned cr4_pge:1;
 	};
 };
 

commit a770f6f28b1a9287189f3dc8333eb694d9a2f0ab
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Dec 21 19:20:09 2008 +0200

    KVM: MMU: Inherit a shadow page's guest level count from vcpu setup
    
    Instead of "calculating" it on every shadow page allocation, set it once
    when switching modes, and copy it when allocating pages.
    
    This doesn't buy us much, but sets up the stage for inheriting more
    information related to the mmu setup.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0a4dab25a919..28f875f28f58 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -244,6 +244,7 @@ struct kvm_mmu {
 	hpa_t root_hpa;
 	int root_level;
 	int shadow_root_level;
+	union kvm_mmu_page_role base_role;
 
 	u64 *pae_root;
 };

commit 42dbaa5a057736bf8b5c22aa42dbe975bf1080e5
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Dec 15 13:52:10 2008 +0100

    KVM: x86: Virtualize debug registers
    
    So far KVM only had basic x86 debug register support, once introduced to
    realize guest debugging that way. The guest itself was not able to use
    those registers.
    
    This patch now adds (almost) full support for guest self-debugging via
    hardware registers. It refactors the code, moving generic parts out of
    SVM (VMX was already cleaned up by the KVM_SET_GUEST_DEBUG patches), and
    it ensures that the registers are properly switched between host and
    guest.
    
    This patch also prepares debug register usage by the host. The latter
    will (once wired-up by the following patch) allow for hardware
    breakpoints/watchpoints in guest code. If this is enabled, the guest
    will only see faked debug registers without functionality, but with
    content reflecting the guest's modifications.
    
    Tested on Intel only, but SVM /should/ work as well, but who knows...
    
    Known limitations: Trapping on tss switch won't work - most probably on
    Intel.
    
    Credits also go to Joerg Roedel - I used his once posted debugging
    series as platform for this patch.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c430cd580ee2..0a4dab25a919 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -135,6 +135,19 @@ enum {
 
 #define KVM_NR_MEM_OBJS 40
 
+#define KVM_NR_DB_REGS	4
+
+#define DR6_BD		(1 << 13)
+#define DR6_BS		(1 << 14)
+#define DR6_FIXED_1	0xffff0ff0
+#define DR6_VOLATILE	0x0000e00f
+
+#define DR7_BP_EN_MASK	0x000000ff
+#define DR7_GE		(1 << 9)
+#define DR7_GD		(1 << 13)
+#define DR7_FIXED_1	0x00000400
+#define DR7_VOLATILE	0xffff23ff
+
 /*
  * We don't want allocation failures within the mmu code, so we preallocate
  * enough memory for a single page fault in a cache.
@@ -334,6 +347,15 @@ struct kvm_vcpu_arch {
 
 	struct mtrr_state_type mtrr_state;
 	u32 pat;
+
+	int switch_db_regs;
+	unsigned long host_db[KVM_NR_DB_REGS];
+	unsigned long host_dr6;
+	unsigned long host_dr7;
+	unsigned long db[KVM_NR_DB_REGS];
+	unsigned long dr6;
+	unsigned long dr7;
+	unsigned long eff_db[KVM_NR_DB_REGS];
 };
 
 struct kvm_mem_alias {

commit d0bfb940ecabf0b44fb1fd80d8d60594e569e5ec
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Dec 15 13:52:10 2008 +0100

    KVM: New guest debug interface
    
    This rips out the support for KVM_DEBUG_GUEST and introduces a new IOCTL
    instead: KVM_SET_GUEST_DEBUG. The IOCTL payload consists of a generic
    part, controlling the "main switch" and the single-step feature. The
    arch specific part adds an x86 interface for intercepting both types of
    debug exceptions separately and re-injecting them when the host was not
    interested. Moveover, the foundation for guest debugging via debug
    registers is layed.
    
    To signal breakpoint events properly back to userland, an arch-specific
    data block is now returned along KVM_EXIT_DEBUG. For x86, the arch block
    contains the PC, the debug exception, and relevant debug registers to
    tell debug events properly apart.
    
    The availability of this new interface is signaled by
    KVM_CAP_SET_GUEST_DEBUG. Empty stubs for not yet supported archs are
    provided.
    
    Note that both SVM and VTX are supported, but only the latter was tested
    yet. Based on the experience with all those VTX corner case, I would be
    fairly surprised if SVM will work out of the box.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 53779309514a..c430cd580ee2 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -135,12 +135,6 @@ enum {
 
 #define KVM_NR_MEM_OBJS 40
 
-struct kvm_guest_debug {
-	int enabled;
-	unsigned long bp[4];
-	int singlestep;
-};
-
 /*
  * We don't want allocation failures within the mmu code, so we preallocate
  * enough memory for a single page fault in a cache.
@@ -448,8 +442,7 @@ struct kvm_x86_ops {
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
 
 	int (*set_guest_debug)(struct kvm_vcpu *vcpu,
-			       struct kvm_debug_guest *dbg);
-	void (*guest_debug_pre)(struct kvm_vcpu *vcpu);
+			       struct kvm_guest_debug *dbg);
 	int (*get_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata);
 	int (*set_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
 	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);

commit 3d6368ef580a4dff012960834bba4e28d3c1430c
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Nov 25 20:17:07 2008 +0100

    KVM: SVM: Add VMRUN handler
    
    This patch implements VMRUN. VMRUN enters a virtual CPU and runs that
    in the same context as the normal guest CPU would run.
    So basically it is implemented the same way, a normal CPU would do it.
    
    We also prepare all intercepts that get OR'ed with the original
    intercepts, as we do not allow a level 2 guest to be intercepted less
    than the first level guest.
    
    v2 implements the following improvements:
    
    - fixes the CPL check
    - does not allocate iopm when not used
    - remembers the host's IF in the HIF bit in the hflags
    
    v3:
    
    - make use of the new permission checking
    - add support for V_INTR_MASKING_MASK
    
    v4:
    
    - use host page backed hsave
    
    v5:
    
    - remove IOPM merging code
    
    v6:
    
    - save cr4 so PAE l1 guests work
    
    v7:
    
    - return 0 on vmrun so we check the MSRs too
    - fix MSR check to use the correct variable
    
    Acked-by: Joerg Roedel <joro@8bytes.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 29e4157732db..53779309514a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -740,6 +740,8 @@ enum {
 };
 
 #define HF_GIF_MASK		(1 << 0)
+#define HF_HIF_MASK		(1 << 1)
+#define HF_VINTR_MASK		(1 << 2)
 
 /*
  * Hardware virtualization extension instructions may fault if a

commit 1371d90460189d02bf1bcca19dbfe6bd10dc6031
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Nov 25 20:17:04 2008 +0100

    KVM: SVM: Implement GIF, clgi and stgi
    
    This patch implements the GIF flag and the clgi and stgi instructions that
    set this flag. Only if the flag is set (default), interrupts can be received by
    the CPU.
    
    To keep the information about that somewhere, this patch adds a new hidden
    flags vector. that is used to store information that does not go into the
    vmcb, but is SVM specific.
    
    I tried to write some code to make -no-kvm-irqchip work too, but the first
    level guest won't even boot with that atm, so I ditched it.
    
    v2 moves the hflags to x86 generic code
    v3 makes use of the new permission helper
    v6 only enables interrupt_window if GIF=1
    
    Acked-by: Joerg Roedel <joro@8bytes.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 2998efe89278..29e4157732db 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -259,6 +259,7 @@ struct kvm_vcpu_arch {
 	unsigned long cr3;
 	unsigned long cr4;
 	unsigned long cr8;
+	u32 hflags;
 	u64 pdptrs[4]; /* pae */
 	u64 shadow_efer;
 	u64 apic_base;
@@ -738,6 +739,8 @@ enum {
 	TASK_SWITCH_GATE = 3,
 };
 
+#define HF_GIF_MASK		(1 << 0)
+
 /*
  * Hardware virtualization extension instructions may fault if a
  * reboot turns off virtualization while processes are running.

commit 9962d032bbff0268f22068787831405f8468c8b4
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Nov 25 20:17:02 2008 +0100

    KVM: SVM: Move EFER and MSR constants to generic x86 code
    
    MSR_EFER_SVME_MASK, MSR_VM_CR and MSR_VM_HSAVE_PA are set in KVM
    specific headers. Linux does have nice header files to collect
    EFER bits and MSR IDs, so IMHO we should put them there.
    
    While at it, I also changed the naming scheme to match that
    of the other defines.
    
    (introduced in v6)
    
    Acked-by: Joerg Roedel <joro@8bytes.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 730843d1d2fb..2998efe89278 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -22,6 +22,7 @@
 #include <asm/pvclock-abi.h>
 #include <asm/desc.h>
 #include <asm/mtrr.h>
+#include <asm/msr-index.h>
 
 #define KVM_MAX_VCPUS 16
 #define KVM_MEMORY_SLOTS 32

commit 19de40a8472fa64693eab844911eec277d489f6c
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Wed Dec 3 14:43:34 2008 +0100

    KVM: change KVM to use IOMMU API
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 97215a458e5f..730843d1d2fb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -360,7 +360,7 @@ struct kvm_arch{
 	struct list_head active_mmu_pages;
 	struct list_head assigned_dev_head;
 	struct list_head oos_global_pages;
-	struct dmar_domain *intel_iommu_domain;
+	struct iommu_domain *iommu_domain;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;

commit ad218f85e388e8ca816ff09d91c246cd014c53a8
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Dec 1 22:32:05 2008 -0200

    KVM: MMU: prepopulate the shadow on invlpg
    
    If the guest executes invlpg, peek into the pagetable and attempt to
    prepopulate the shadow entry.
    
    Also stop dirty fault updates from interfering with the fork detector.
    
    2% improvement on RHEL3/AIM7.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 65b1ed295698..97215a458e5f 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -602,7 +602,8 @@ unsigned long segment_base(u16 selector);
 
 void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
 void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
-		       const u8 *new, int bytes);
+		       const u8 *new, int bytes,
+		       bool guest_initiated);
 int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva);
 void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);

commit 6cffe8ca4a2adf1ac5003d9cad08fe4434d6eee0
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Dec 1 22:32:04 2008 -0200

    KVM: MMU: skip global pgtables on sync due to cr3 switch
    
    Skip syncing global pages on cr3 switch (but not on cr4/cr0). This is
    important for Linux 32-bit guests with PAE, where the kmap page is
    marked as global.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 93d0aed35880..65b1ed295698 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -182,6 +182,8 @@ struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
 
+	struct list_head oos_link;
+
 	/*
 	 * The following two entries are used to key the shadow page in the
 	 * hash table.
@@ -200,6 +202,7 @@ struct kvm_mmu_page {
 	int multimapped;         /* More than one parent_pte? */
 	int root_count;          /* Currently serving as active root */
 	bool unsync;
+	bool global;
 	unsigned int unsync_children;
 	union {
 		u64 *parent_pte;               /* !multimapped */
@@ -356,6 +359,7 @@ struct kvm_arch{
 	 */
 	struct list_head active_mmu_pages;
 	struct list_head assigned_dev_head;
+	struct list_head oos_global_pages;
 	struct dmar_domain *intel_iommu_domain;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
@@ -385,6 +389,7 @@ struct kvm_vm_stat {
 	u32 mmu_recycled;
 	u32 mmu_cache_miss;
 	u32 mmu_unsync;
+	u32 mmu_unsync_global;
 	u32 remote_tlb_flush;
 	u32 lpages;
 };
@@ -603,6 +608,7 @@ void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
 int kvm_mmu_load(struct kvm_vcpu *vcpu);
 void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
+void kvm_mmu_sync_global(struct kvm_vcpu *vcpu);
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 

commit 60c8aec6e2c9923492dabbd6b67e34692bd26c20
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Dec 1 22:32:02 2008 -0200

    KVM: MMU: use page array in unsync walk
    
    Instead of invoking the handler directly collect pages into
    an array so the caller can work with it.
    
    Simplifies TLB flush collapsing.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f58f7ebdea81..93d0aed35880 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -200,7 +200,7 @@ struct kvm_mmu_page {
 	int multimapped;         /* More than one parent_pte? */
 	int root_count;          /* Currently serving as active root */
 	bool unsync;
-	bool unsync_children;
+	unsigned int unsync_children;
 	union {
 		u64 *parent_pte;               /* !multimapped */
 		struct hlist_head parent_ptes; /* multimapped, kvm_pte_chain */

commit eca70fc5671b226966dfb7ee9953d59199288566
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Mon Nov 17 19:03:15 2008 -0200

    KVM: VMX: move ASM_VMX_* definitions from asm/kvm_host.h to asm/vmx.h
    
    Those definitions will be used by code outside KVM, so move it outside
    of a KVM-specific source file.
    
    Those definitions are used only on kvm/vmx.c, that already includes
    asm/vmx.h, so they can be moved safely.
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 99e3cc149d21..f58f7ebdea81 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -714,18 +714,6 @@ static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 }
 
-#define ASM_VMX_VMCLEAR_RAX       ".byte 0x66, 0x0f, 0xc7, 0x30"
-#define ASM_VMX_VMLAUNCH          ".byte 0x0f, 0x01, 0xc2"
-#define ASM_VMX_VMRESUME          ".byte 0x0f, 0x01, 0xc3"
-#define ASM_VMX_VMPTRLD_RAX       ".byte 0x0f, 0xc7, 0x30"
-#define ASM_VMX_VMREAD_RDX_RAX    ".byte 0x0f, 0x78, 0xd0"
-#define ASM_VMX_VMWRITE_RAX_RDX   ".byte 0x0f, 0x79, 0xd0"
-#define ASM_VMX_VMWRITE_RSP_RDX   ".byte 0x0f, 0x79, 0xd4"
-#define ASM_VMX_VMXOFF            ".byte 0x0f, 0x01, 0xc4"
-#define ASM_VMX_VMXON_RAX         ".byte 0xf3, 0x0f, 0xc7, 0x30"
-#define ASM_VMX_INVEPT		  ".byte 0x66, 0x0f, 0x38, 0x80, 0x08"
-#define ASM_VMX_INVVPID		  ".byte 0x66, 0x0f, 0x38, 0x81, 0x08"
-
 #define MSR_IA32_TIME_STAMP_COUNTER		0x010
 
 #define TSS_IOPB_BASE_OFFSET 0x66

commit 2843099fee32a6020e1caa95c6026f28b5d43bff
Author: Izik Eidus <ieidus@redhat.com>
Date:   Fri Oct 3 17:40:32 2008 +0300

    KVM: MMU: Fix aliased gfns treated as unaliased
    
    Some areas of kvm x86 mmu are using gfn offset inside a slot without
    unaliasing the gfn first.  This patch makes sure that the gfn will be
    unaliased and add gfn_to_memslot_unaliased() to save the calculating
    of the gfn unaliasing in case we have it unaliased already.
    
    Signed-off-by: Izik Eidus <ieidus@redhat.com>
    Acked-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 09e6c56572cb..99e3cc149d21 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -617,6 +617,8 @@ void kvm_disable_tdp(void);
 int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3);
 int complete_pio(struct kvm_vcpu *vcpu);
 
+struct kvm_memory_slot *gfn_to_memslot_unaliased(struct kvm *kvm, gfn_t gfn);
+
 static inline struct kvm_mmu_page *page_header(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);

commit cc6e462cd54e64858ea25816df87d033229efe56
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Oct 20 10:20:03 2008 +0200

    KVM: x86: Optimize NMI watchdog delivery
    
    As suggested by Avi, this patch introduces a counter of VCPUs that have
    LVT0 set to NMI mode. Only if the counter > 0, we push the PIT ticks via
    all LAPIC LVT0 lines to enable NMI watchdog support.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Acked-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 59c3ae10de6c..09e6c56572cb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -361,6 +361,7 @@ struct kvm_arch{
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;
 	struct hlist_head irq_ack_notifier_list;
+	int vapics_in_nmi_mode;
 
 	int round_robin_prev_vcpu;
 	unsigned int tss_addr;

commit 291f26bc0f89518ad7ee3207c09eb8a743ac8fcc
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Thu Oct 16 17:30:57 2008 +0800

    KVM: MMU: Extend kvm_mmu_page->slot_bitmap size
    
    Otherwise set_bit() for private memory slot(above KVM_MEMORY_SLOTS) would
    corrupted memory in 32bit host.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 93040b5eed96..59c3ae10de6c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -192,9 +192,11 @@ struct kvm_mmu_page {
 	u64 *spt;
 	/* hold the gfn of each spte inside spt */
 	gfn_t *gfns;
-	unsigned long slot_bitmap; /* One bit set per slot which has memory
-				    * in this shadow page.
-				    */
+	/*
+	 * One bit set per slot which has memory
+	 * in this shadow page.
+	 */
+	DECLARE_BITMAP(slot_bitmap, KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS);
 	int multimapped;         /* More than one parent_pte? */
 	int root_count;          /* Currently serving as active root */
 	bool unsync;

commit 64d4d521757117aa5c1cfe79d3baa6cf57703f81
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Thu Oct 9 16:01:57 2008 +0800

    KVM: Enable MTRR for EPT
    
    The effective memory type of EPT is the mixture of MSR_IA32_CR_PAT and memory
    type field of EPT entry.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8082e87f628d..93040b5eed96 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -483,6 +483,7 @@ struct kvm_x86_ops {
 
 	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
 	int (*get_tdp_level)(void);
+	int (*get_mt_mask_shift)(void);
 };
 
 extern struct kvm_x86_ops *kvm_x86_ops;
@@ -496,7 +497,7 @@ int kvm_mmu_setup(struct kvm_vcpu *vcpu);
 void kvm_mmu_set_nonpresent_ptes(u64 trap_pte, u64 notrap_pte);
 void kvm_mmu_set_base_ptes(u64 base_pte);
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
-		u64 dirty_mask, u64 nx_mask, u64 x_mask);
+		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 mt_mask);
 
 int kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);

commit 0bed3b568b68e5835ef5da888a372b9beabf7544
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Thu Oct 9 16:01:54 2008 +0800

    KVM: Improve MTRR structure
    
    As well as reset mmu context when set MTRR.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index a40fa8478920..8082e87f628d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -21,6 +21,7 @@
 
 #include <asm/pvclock-abi.h>
 #include <asm/desc.h>
+#include <asm/mtrr.h>
 
 #define KVM_MAX_VCPUS 16
 #define KVM_MEMORY_SLOTS 32
@@ -86,6 +87,7 @@
 #define KVM_MIN_FREE_MMU_PAGES 5
 #define KVM_REFILL_PAGES 25
 #define KVM_MAX_CPUID_ENTRIES 40
+#define KVM_NR_FIXED_MTRR_REGION 88
 #define KVM_NR_VAR_MTRR 8
 
 extern spinlock_t kvm_lock;
@@ -329,7 +331,8 @@ struct kvm_vcpu_arch {
 	bool nmi_injected;
 	bool nmi_window_open;
 
-	u64 mtrr[0x100];
+	struct mtrr_state_type mtrr_state;
+	u32 pat;
 };
 
 struct kvm_mem_alias {

commit c4abb7c9cde24b7351a47328ef866e6a2bbb1ad0
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Fri Sep 26 09:30:55 2008 +0200

    KVM: x86: Support for user space injected NMIs
    
    Introduces the KVM_NMI IOCTL to the generic x86 part of KVM for
    injecting NMIs from user space and also extends the statistic report
    accordingly.
    
    Based on the original patch by Sheng Yang.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Sheng Yang <sheng.yang@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bfbbdea869bf..a40fa8478920 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -398,6 +398,7 @@ struct kvm_vcpu_stat {
 	u32 halt_exits;
 	u32 halt_wakeup;
 	u32 request_irq_exits;
+	u32 request_nmi_exits;
 	u32 irq_exits;
 	u32 host_state_reload;
 	u32 efer_reload;
@@ -406,6 +407,7 @@ struct kvm_vcpu_stat {
 	u32 insn_emulation_fail;
 	u32 hypercalls;
 	u32 irq_injections;
+	u32 nmi_injections;
 };
 
 struct descriptor_table {

commit 33f089ca5a61f7aead26e8e1866dfc961dd88a9e
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Fri Sep 26 09:30:49 2008 +0200

    KVM: VMX: refactor/fix IRQ and NMI injectability determination
    
    There are currently two ways in VMX to check if an IRQ or NMI can be
    injected:
     - vmx_{nmi|irq}_enabled and
     - vcpu.arch.{nmi|interrupt}_window_open.
    Even worse, one test (at the end of vmx_vcpu_run) uses an inconsistent,
    likely incorrect logic.
    
    This patch consolidates and unifies the tests over
    {nmi|interrupt}_window_open as cache + vmx_update_window_states
    for updating the cache content.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8346be87cfa1..bfbbdea869bf 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -327,6 +327,7 @@ struct kvm_vcpu_arch {
 
 	bool nmi_pending;
 	bool nmi_injected;
+	bool nmi_window_open;
 
 	u64 mtrr[0x100];
 };

commit 5550af4df179e52753d3a43a788a113ad8cd95cd
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Wed Oct 15 20:15:06 2008 +0800

    KVM: Fix guest shared interrupt with in-kernel irqchip
    
    Every call of kvm_set_irq() should offer an irq_source_id, which is
    allocated by kvm_request_irq_source_id(). Based on irq_source_id, we
    identify the irq source and implement logical OR for shared level
    interrupts.
    
    The allocated irq_source_id can be freed by kvm_free_irq_source_id().
    
    Currently, we support at most sizeof(unsigned long) different irq sources.
    
    [Amit: - rebase to kvm.git HEAD
           - move definition of KVM_USERSPACE_IRQ_SOURCE_ID to common file
           - move kvm_request_irq_source_id to the update_irq ioctl]
    
    [Xiantao: - Add kvm/ia64 stuff and make it work for kvm/ia64 guests]
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Amit Shah <amit.shah@redhat.com>
    Signed-off-by: Xiantao Zhang <xiantao.zhang@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 65679d006337..8346be87cfa1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -364,6 +364,9 @@ struct kvm_arch{
 
 	struct page *ept_identity_pagetable;
 	bool ept_identity_pagetable_done;
+
+	unsigned long irq_sources_bitmap;
+	unsigned long irq_states[KVM_IOAPIC_NUM_PINS];
 };
 
 struct kvm_vm_stat {

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 411fb8cfb24e..65679d006337 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -8,8 +8,8 @@
  *
  */
 
-#ifndef ASM_X86__KVM_HOST_H
-#define ASM_X86__KVM_HOST_H
+#ifndef _ASM_X86_KVM_HOST_H
+#define _ASM_X86_KVM_HOST_H
 
 #include <linux/types.h>
 #include <linux/mm.h>
@@ -749,4 +749,4 @@ asmlinkage void kvm_handle_fault_on_reboot(void);
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_age_hva(struct kvm *kvm, unsigned long hva);
 
-#endif /* ASM_X86__KVM_HOST_H */
+#endif /* _ASM_X86_KVM_HOST_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
new file mode 100644
index 000000000000..411fb8cfb24e
--- /dev/null
+++ b/arch/x86/include/asm/kvm_host.h
@@ -0,0 +1,752 @@
+/*
+ * Kernel-based Virtual Machine driver for Linux
+ *
+ * This header defines architecture specific interfaces, x86 version
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef ASM_X86__KVM_HOST_H
+#define ASM_X86__KVM_HOST_H
+
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <linux/mmu_notifier.h>
+
+#include <linux/kvm.h>
+#include <linux/kvm_para.h>
+#include <linux/kvm_types.h>
+
+#include <asm/pvclock-abi.h>
+#include <asm/desc.h>
+
+#define KVM_MAX_VCPUS 16
+#define KVM_MEMORY_SLOTS 32
+/* memory slots that does not exposed to userspace */
+#define KVM_PRIVATE_MEM_SLOTS 4
+
+#define KVM_PIO_PAGE_OFFSET 1
+#define KVM_COALESCED_MMIO_PAGE_OFFSET 2
+
+#define CR3_PAE_RESERVED_BITS ((X86_CR3_PWT | X86_CR3_PCD) - 1)
+#define CR3_NONPAE_RESERVED_BITS ((PAGE_SIZE-1) & ~(X86_CR3_PWT | X86_CR3_PCD))
+#define CR3_L_MODE_RESERVED_BITS (CR3_NONPAE_RESERVED_BITS |	\
+				  0xFFFFFF0000000000ULL)
+
+#define KVM_GUEST_CR0_MASK				   \
+	(X86_CR0_PG | X86_CR0_PE | X86_CR0_WP | X86_CR0_NE \
+	 | X86_CR0_NW | X86_CR0_CD)
+#define KVM_VM_CR0_ALWAYS_ON						\
+	(X86_CR0_PG | X86_CR0_PE | X86_CR0_WP | X86_CR0_NE | X86_CR0_TS \
+	 | X86_CR0_MP)
+#define KVM_GUEST_CR4_MASK						\
+	(X86_CR4_VME | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_PGE | X86_CR4_VMXE)
+#define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)
+#define KVM_RMODE_VM_CR4_ALWAYS_ON (X86_CR4_VME | X86_CR4_PAE | X86_CR4_VMXE)
+
+#define INVALID_PAGE (~(hpa_t)0)
+#define UNMAPPED_GVA (~(gpa_t)0)
+
+/* shadow tables are PAE even on non-PAE hosts */
+#define KVM_HPAGE_SHIFT 21
+#define KVM_HPAGE_SIZE (1UL << KVM_HPAGE_SHIFT)
+#define KVM_HPAGE_MASK (~(KVM_HPAGE_SIZE - 1))
+
+#define KVM_PAGES_PER_HPAGE (KVM_HPAGE_SIZE / PAGE_SIZE)
+
+#define DE_VECTOR 0
+#define DB_VECTOR 1
+#define BP_VECTOR 3
+#define OF_VECTOR 4
+#define BR_VECTOR 5
+#define UD_VECTOR 6
+#define NM_VECTOR 7
+#define DF_VECTOR 8
+#define TS_VECTOR 10
+#define NP_VECTOR 11
+#define SS_VECTOR 12
+#define GP_VECTOR 13
+#define PF_VECTOR 14
+#define MF_VECTOR 16
+#define MC_VECTOR 18
+
+#define SELECTOR_TI_MASK (1 << 2)
+#define SELECTOR_RPL_MASK 0x03
+
+#define IOPL_SHIFT 12
+
+#define KVM_ALIAS_SLOTS 4
+
+#define KVM_PERMILLE_MMU_PAGES 20
+#define KVM_MIN_ALLOC_MMU_PAGES 64
+#define KVM_MMU_HASH_SHIFT 10
+#define KVM_NUM_MMU_PAGES (1 << KVM_MMU_HASH_SHIFT)
+#define KVM_MIN_FREE_MMU_PAGES 5
+#define KVM_REFILL_PAGES 25
+#define KVM_MAX_CPUID_ENTRIES 40
+#define KVM_NR_VAR_MTRR 8
+
+extern spinlock_t kvm_lock;
+extern struct list_head vm_list;
+
+struct kvm_vcpu;
+struct kvm;
+
+enum kvm_reg {
+	VCPU_REGS_RAX = 0,
+	VCPU_REGS_RCX = 1,
+	VCPU_REGS_RDX = 2,
+	VCPU_REGS_RBX = 3,
+	VCPU_REGS_RSP = 4,
+	VCPU_REGS_RBP = 5,
+	VCPU_REGS_RSI = 6,
+	VCPU_REGS_RDI = 7,
+#ifdef CONFIG_X86_64
+	VCPU_REGS_R8 = 8,
+	VCPU_REGS_R9 = 9,
+	VCPU_REGS_R10 = 10,
+	VCPU_REGS_R11 = 11,
+	VCPU_REGS_R12 = 12,
+	VCPU_REGS_R13 = 13,
+	VCPU_REGS_R14 = 14,
+	VCPU_REGS_R15 = 15,
+#endif
+	VCPU_REGS_RIP,
+	NR_VCPU_REGS
+};
+
+enum {
+	VCPU_SREG_ES,
+	VCPU_SREG_CS,
+	VCPU_SREG_SS,
+	VCPU_SREG_DS,
+	VCPU_SREG_FS,
+	VCPU_SREG_GS,
+	VCPU_SREG_TR,
+	VCPU_SREG_LDTR,
+};
+
+#include <asm/kvm_x86_emulate.h>
+
+#define KVM_NR_MEM_OBJS 40
+
+struct kvm_guest_debug {
+	int enabled;
+	unsigned long bp[4];
+	int singlestep;
+};
+
+/*
+ * We don't want allocation failures within the mmu code, so we preallocate
+ * enough memory for a single page fault in a cache.
+ */
+struct kvm_mmu_memory_cache {
+	int nobjs;
+	void *objects[KVM_NR_MEM_OBJS];
+};
+
+#define NR_PTE_CHAIN_ENTRIES 5
+
+struct kvm_pte_chain {
+	u64 *parent_ptes[NR_PTE_CHAIN_ENTRIES];
+	struct hlist_node link;
+};
+
+/*
+ * kvm_mmu_page_role, below, is defined as:
+ *
+ *   bits 0:3 - total guest paging levels (2-4, or zero for real mode)
+ *   bits 4:7 - page table level for this shadow (1-4)
+ *   bits 8:9 - page table quadrant for 2-level guests
+ *   bit   16 - "metaphysical" - gfn is not a real page (huge page/real mode)
+ *   bits 17:19 - common access permissions for all ptes in this shadow page
+ */
+union kvm_mmu_page_role {
+	unsigned word;
+	struct {
+		unsigned glevels:4;
+		unsigned level:4;
+		unsigned quadrant:2;
+		unsigned pad_for_nice_hex_output:6;
+		unsigned metaphysical:1;
+		unsigned access:3;
+		unsigned invalid:1;
+	};
+};
+
+struct kvm_mmu_page {
+	struct list_head link;
+	struct hlist_node hash_link;
+
+	/*
+	 * The following two entries are used to key the shadow page in the
+	 * hash table.
+	 */
+	gfn_t gfn;
+	union kvm_mmu_page_role role;
+
+	u64 *spt;
+	/* hold the gfn of each spte inside spt */
+	gfn_t *gfns;
+	unsigned long slot_bitmap; /* One bit set per slot which has memory
+				    * in this shadow page.
+				    */
+	int multimapped;         /* More than one parent_pte? */
+	int root_count;          /* Currently serving as active root */
+	bool unsync;
+	bool unsync_children;
+	union {
+		u64 *parent_pte;               /* !multimapped */
+		struct hlist_head parent_ptes; /* multimapped, kvm_pte_chain */
+	};
+	DECLARE_BITMAP(unsync_child_bitmap, 512);
+};
+
+struct kvm_pv_mmu_op_buffer {
+	void *ptr;
+	unsigned len;
+	unsigned processed;
+	char buf[512] __aligned(sizeof(long));
+};
+
+/*
+ * x86 supports 3 paging modes (4-level 64-bit, 3-level 64-bit, and 2-level
+ * 32-bit).  The kvm_mmu structure abstracts the details of the current mmu
+ * mode.
+ */
+struct kvm_mmu {
+	void (*new_cr3)(struct kvm_vcpu *vcpu);
+	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err);
+	void (*free)(struct kvm_vcpu *vcpu);
+	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva);
+	void (*prefetch_page)(struct kvm_vcpu *vcpu,
+			      struct kvm_mmu_page *page);
+	int (*sync_page)(struct kvm_vcpu *vcpu,
+			 struct kvm_mmu_page *sp);
+	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
+	hpa_t root_hpa;
+	int root_level;
+	int shadow_root_level;
+
+	u64 *pae_root;
+};
+
+struct kvm_vcpu_arch {
+	u64 host_tsc;
+	int interrupt_window_open;
+	unsigned long irq_summary; /* bit vector: 1 per word in irq_pending */
+	DECLARE_BITMAP(irq_pending, KVM_NR_INTERRUPTS);
+	/*
+	 * rip and regs accesses must go through
+	 * kvm_{register,rip}_{read,write} functions.
+	 */
+	unsigned long regs[NR_VCPU_REGS];
+	u32 regs_avail;
+	u32 regs_dirty;
+
+	unsigned long cr0;
+	unsigned long cr2;
+	unsigned long cr3;
+	unsigned long cr4;
+	unsigned long cr8;
+	u64 pdptrs[4]; /* pae */
+	u64 shadow_efer;
+	u64 apic_base;
+	struct kvm_lapic *apic;    /* kernel irqchip context */
+	int mp_state;
+	int sipi_vector;
+	u64 ia32_misc_enable_msr;
+	bool tpr_access_reporting;
+
+	struct kvm_mmu mmu;
+	/* only needed in kvm_pv_mmu_op() path, but it's hot so
+	 * put it here to avoid allocation */
+	struct kvm_pv_mmu_op_buffer mmu_op_buffer;
+
+	struct kvm_mmu_memory_cache mmu_pte_chain_cache;
+	struct kvm_mmu_memory_cache mmu_rmap_desc_cache;
+	struct kvm_mmu_memory_cache mmu_page_cache;
+	struct kvm_mmu_memory_cache mmu_page_header_cache;
+
+	gfn_t last_pt_write_gfn;
+	int   last_pt_write_count;
+	u64  *last_pte_updated;
+	gfn_t last_pte_gfn;
+
+	struct {
+		gfn_t gfn;	/* presumed gfn during guest pte update */
+		pfn_t pfn;	/* pfn corresponding to that gfn */
+		int largepage;
+		unsigned long mmu_seq;
+	} update_pte;
+
+	struct i387_fxsave_struct host_fx_image;
+	struct i387_fxsave_struct guest_fx_image;
+
+	gva_t mmio_fault_cr2;
+	struct kvm_pio_request pio;
+	void *pio_data;
+
+	struct kvm_queued_exception {
+		bool pending;
+		bool has_error_code;
+		u8 nr;
+		u32 error_code;
+	} exception;
+
+	struct kvm_queued_interrupt {
+		bool pending;
+		u8 nr;
+	} interrupt;
+
+	struct {
+		int active;
+		u8 save_iopl;
+		struct kvm_save_segment {
+			u16 selector;
+			unsigned long base;
+			u32 limit;
+			u32 ar;
+		} tr, es, ds, fs, gs;
+	} rmode;
+	int halt_request; /* real mode on Intel only */
+
+	int cpuid_nent;
+	struct kvm_cpuid_entry2 cpuid_entries[KVM_MAX_CPUID_ENTRIES];
+	/* emulate context */
+
+	struct x86_emulate_ctxt emulate_ctxt;
+
+	gpa_t time;
+	struct pvclock_vcpu_time_info hv_clock;
+	unsigned int hv_clock_tsc_khz;
+	unsigned int time_offset;
+	struct page *time_page;
+
+	bool nmi_pending;
+	bool nmi_injected;
+
+	u64 mtrr[0x100];
+};
+
+struct kvm_mem_alias {
+	gfn_t base_gfn;
+	unsigned long npages;
+	gfn_t target_gfn;
+};
+
+struct kvm_arch{
+	int naliases;
+	struct kvm_mem_alias aliases[KVM_ALIAS_SLOTS];
+
+	unsigned int n_free_mmu_pages;
+	unsigned int n_requested_mmu_pages;
+	unsigned int n_alloc_mmu_pages;
+	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
+	/*
+	 * Hash table of struct kvm_mmu_page.
+	 */
+	struct list_head active_mmu_pages;
+	struct list_head assigned_dev_head;
+	struct dmar_domain *intel_iommu_domain;
+	struct kvm_pic *vpic;
+	struct kvm_ioapic *vioapic;
+	struct kvm_pit *vpit;
+	struct hlist_head irq_ack_notifier_list;
+
+	int round_robin_prev_vcpu;
+	unsigned int tss_addr;
+	struct page *apic_access_page;
+
+	gpa_t wall_clock;
+
+	struct page *ept_identity_pagetable;
+	bool ept_identity_pagetable_done;
+};
+
+struct kvm_vm_stat {
+	u32 mmu_shadow_zapped;
+	u32 mmu_pte_write;
+	u32 mmu_pte_updated;
+	u32 mmu_pde_zapped;
+	u32 mmu_flooded;
+	u32 mmu_recycled;
+	u32 mmu_cache_miss;
+	u32 mmu_unsync;
+	u32 remote_tlb_flush;
+	u32 lpages;
+};
+
+struct kvm_vcpu_stat {
+	u32 pf_fixed;
+	u32 pf_guest;
+	u32 tlb_flush;
+	u32 invlpg;
+
+	u32 exits;
+	u32 io_exits;
+	u32 mmio_exits;
+	u32 signal_exits;
+	u32 irq_window_exits;
+	u32 nmi_window_exits;
+	u32 halt_exits;
+	u32 halt_wakeup;
+	u32 request_irq_exits;
+	u32 irq_exits;
+	u32 host_state_reload;
+	u32 efer_reload;
+	u32 fpu_reload;
+	u32 insn_emulation;
+	u32 insn_emulation_fail;
+	u32 hypercalls;
+	u32 irq_injections;
+};
+
+struct descriptor_table {
+	u16 limit;
+	unsigned long base;
+} __attribute__((packed));
+
+struct kvm_x86_ops {
+	int (*cpu_has_kvm_support)(void);          /* __init */
+	int (*disabled_by_bios)(void);             /* __init */
+	void (*hardware_enable)(void *dummy);      /* __init */
+	void (*hardware_disable)(void *dummy);
+	void (*check_processor_compatibility)(void *rtn);
+	int (*hardware_setup)(void);               /* __init */
+	void (*hardware_unsetup)(void);            /* __exit */
+	bool (*cpu_has_accelerated_tpr)(void);
+
+	/* Create, but do not attach this VCPU */
+	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);
+	void (*vcpu_free)(struct kvm_vcpu *vcpu);
+	int (*vcpu_reset)(struct kvm_vcpu *vcpu);
+
+	void (*prepare_guest_switch)(struct kvm_vcpu *vcpu);
+	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
+	void (*vcpu_put)(struct kvm_vcpu *vcpu);
+
+	int (*set_guest_debug)(struct kvm_vcpu *vcpu,
+			       struct kvm_debug_guest *dbg);
+	void (*guest_debug_pre)(struct kvm_vcpu *vcpu);
+	int (*get_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata);
+	int (*set_msr)(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
+	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);
+	void (*get_segment)(struct kvm_vcpu *vcpu,
+			    struct kvm_segment *var, int seg);
+	int (*get_cpl)(struct kvm_vcpu *vcpu);
+	void (*set_segment)(struct kvm_vcpu *vcpu,
+			    struct kvm_segment *var, int seg);
+	void (*get_cs_db_l_bits)(struct kvm_vcpu *vcpu, int *db, int *l);
+	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
+	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
+	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
+	void (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
+	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);
+	void (*get_idt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
+	void (*set_idt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
+	void (*get_gdt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
+	void (*set_gdt)(struct kvm_vcpu *vcpu, struct descriptor_table *dt);
+	unsigned long (*get_dr)(struct kvm_vcpu *vcpu, int dr);
+	void (*set_dr)(struct kvm_vcpu *vcpu, int dr, unsigned long value,
+		       int *exception);
+	void (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);
+	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
+	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
+
+	void (*tlb_flush)(struct kvm_vcpu *vcpu);
+
+	void (*run)(struct kvm_vcpu *vcpu, struct kvm_run *run);
+	int (*handle_exit)(struct kvm_run *run, struct kvm_vcpu *vcpu);
+	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
+	void (*patch_hypercall)(struct kvm_vcpu *vcpu,
+				unsigned char *hypercall_addr);
+	int (*get_irq)(struct kvm_vcpu *vcpu);
+	void (*set_irq)(struct kvm_vcpu *vcpu, int vec);
+	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
+				bool has_error_code, u32 error_code);
+	bool (*exception_injected)(struct kvm_vcpu *vcpu);
+	void (*inject_pending_irq)(struct kvm_vcpu *vcpu);
+	void (*inject_pending_vectors)(struct kvm_vcpu *vcpu,
+				       struct kvm_run *run);
+
+	int (*set_tss_addr)(struct kvm *kvm, unsigned int addr);
+	int (*get_tdp_level)(void);
+};
+
+extern struct kvm_x86_ops *kvm_x86_ops;
+
+int kvm_mmu_module_init(void);
+void kvm_mmu_module_exit(void);
+
+void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
+int kvm_mmu_create(struct kvm_vcpu *vcpu);
+int kvm_mmu_setup(struct kvm_vcpu *vcpu);
+void kvm_mmu_set_nonpresent_ptes(u64 trap_pte, u64 notrap_pte);
+void kvm_mmu_set_base_ptes(u64 base_pte);
+void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
+		u64 dirty_mask, u64 nx_mask, u64 x_mask);
+
+int kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
+void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
+void kvm_mmu_zap_all(struct kvm *kvm);
+unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
+void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
+
+int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3);
+
+int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
+			  const void *val, int bytes);
+int kvm_pv_mmu_op(struct kvm_vcpu *vcpu, unsigned long bytes,
+		  gpa_t addr, unsigned long *ret);
+
+extern bool tdp_enabled;
+
+enum emulation_result {
+	EMULATE_DONE,       /* no further processing */
+	EMULATE_DO_MMIO,      /* kvm_run filled with mmio request */
+	EMULATE_FAIL,         /* can't emulate this instruction */
+};
+
+#define EMULTYPE_NO_DECODE	    (1 << 0)
+#define EMULTYPE_TRAP_UD	    (1 << 1)
+int emulate_instruction(struct kvm_vcpu *vcpu, struct kvm_run *run,
+			unsigned long cr2, u16 error_code, int emulation_type);
+void kvm_report_emulation_failure(struct kvm_vcpu *cvpu, const char *context);
+void realmode_lgdt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
+void realmode_lidt(struct kvm_vcpu *vcpu, u16 size, unsigned long address);
+void realmode_lmsw(struct kvm_vcpu *vcpu, unsigned long msw,
+		   unsigned long *rflags);
+
+unsigned long realmode_get_cr(struct kvm_vcpu *vcpu, int cr);
+void realmode_set_cr(struct kvm_vcpu *vcpu, int cr, unsigned long value,
+		     unsigned long *rflags);
+void kvm_enable_efer_bits(u64);
+int kvm_get_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *data);
+int kvm_set_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data);
+
+struct x86_emulate_ctxt;
+
+int kvm_emulate_pio(struct kvm_vcpu *vcpu, struct kvm_run *run, int in,
+		     int size, unsigned port);
+int kvm_emulate_pio_string(struct kvm_vcpu *vcpu, struct kvm_run *run, int in,
+			   int size, unsigned long count, int down,
+			    gva_t address, int rep, unsigned port);
+void kvm_emulate_cpuid(struct kvm_vcpu *vcpu);
+int kvm_emulate_halt(struct kvm_vcpu *vcpu);
+int emulate_invlpg(struct kvm_vcpu *vcpu, gva_t address);
+int emulate_clts(struct kvm_vcpu *vcpu);
+int emulator_get_dr(struct x86_emulate_ctxt *ctxt, int dr,
+		    unsigned long *dest);
+int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,
+		    unsigned long value);
+
+void kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
+int kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector,
+				int type_bits, int seg);
+
+int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int reason);
+
+void kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
+void kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
+void kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
+void kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);
+unsigned long kvm_get_cr8(struct kvm_vcpu *vcpu);
+void kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw);
+void kvm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l);
+
+int kvm_get_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
+int kvm_set_msr_common(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+
+void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);
+void kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);
+void kvm_inject_page_fault(struct kvm_vcpu *vcpu, unsigned long cr2,
+			   u32 error_code);
+
+void kvm_pic_set_irq(void *opaque, int irq, int level);
+
+void kvm_inject_nmi(struct kvm_vcpu *vcpu);
+
+void fx_init(struct kvm_vcpu *vcpu);
+
+int emulator_read_std(unsigned long addr,
+		      void *val,
+		      unsigned int bytes,
+		      struct kvm_vcpu *vcpu);
+int emulator_write_emulated(unsigned long addr,
+			    const void *val,
+			    unsigned int bytes,
+			    struct kvm_vcpu *vcpu);
+
+unsigned long segment_base(u16 selector);
+
+void kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu);
+void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
+		       const u8 *new, int bytes);
+int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva);
+void __kvm_mmu_free_some_pages(struct kvm_vcpu *vcpu);
+int kvm_mmu_load(struct kvm_vcpu *vcpu);
+void kvm_mmu_unload(struct kvm_vcpu *vcpu);
+void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
+
+int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
+
+int kvm_fix_hypercall(struct kvm_vcpu *vcpu);
+
+int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u32 error_code);
+void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
+
+void kvm_enable_tdp(void);
+void kvm_disable_tdp(void);
+
+int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3);
+int complete_pio(struct kvm_vcpu *vcpu);
+
+static inline struct kvm_mmu_page *page_header(hpa_t shadow_page)
+{
+	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);
+
+	return (struct kvm_mmu_page *)page_private(page);
+}
+
+static inline u16 kvm_read_fs(void)
+{
+	u16 seg;
+	asm("mov %%fs, %0" : "=g"(seg));
+	return seg;
+}
+
+static inline u16 kvm_read_gs(void)
+{
+	u16 seg;
+	asm("mov %%gs, %0" : "=g"(seg));
+	return seg;
+}
+
+static inline u16 kvm_read_ldt(void)
+{
+	u16 ldt;
+	asm("sldt %0" : "=g"(ldt));
+	return ldt;
+}
+
+static inline void kvm_load_fs(u16 sel)
+{
+	asm("mov %0, %%fs" : : "rm"(sel));
+}
+
+static inline void kvm_load_gs(u16 sel)
+{
+	asm("mov %0, %%gs" : : "rm"(sel));
+}
+
+static inline void kvm_load_ldt(u16 sel)
+{
+	asm("lldt %0" : : "rm"(sel));
+}
+
+static inline void kvm_get_idt(struct descriptor_table *table)
+{
+	asm("sidt %0" : "=m"(*table));
+}
+
+static inline void kvm_get_gdt(struct descriptor_table *table)
+{
+	asm("sgdt %0" : "=m"(*table));
+}
+
+static inline unsigned long kvm_read_tr_base(void)
+{
+	u16 tr;
+	asm("str %0" : "=g"(tr));
+	return segment_base(tr);
+}
+
+#ifdef CONFIG_X86_64
+static inline unsigned long read_msr(unsigned long msr)
+{
+	u64 value;
+
+	rdmsrl(msr, value);
+	return value;
+}
+#endif
+
+static inline void kvm_fx_save(struct i387_fxsave_struct *image)
+{
+	asm("fxsave (%0)":: "r" (image));
+}
+
+static inline void kvm_fx_restore(struct i387_fxsave_struct *image)
+{
+	asm("fxrstor (%0)":: "r" (image));
+}
+
+static inline void kvm_fx_finit(void)
+{
+	asm("finit");
+}
+
+static inline u32 get_rdx_init_val(void)
+{
+	return 0x600; /* P6 family */
+}
+
+static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
+{
+	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
+}
+
+#define ASM_VMX_VMCLEAR_RAX       ".byte 0x66, 0x0f, 0xc7, 0x30"
+#define ASM_VMX_VMLAUNCH          ".byte 0x0f, 0x01, 0xc2"
+#define ASM_VMX_VMRESUME          ".byte 0x0f, 0x01, 0xc3"
+#define ASM_VMX_VMPTRLD_RAX       ".byte 0x0f, 0xc7, 0x30"
+#define ASM_VMX_VMREAD_RDX_RAX    ".byte 0x0f, 0x78, 0xd0"
+#define ASM_VMX_VMWRITE_RAX_RDX   ".byte 0x0f, 0x79, 0xd0"
+#define ASM_VMX_VMWRITE_RSP_RDX   ".byte 0x0f, 0x79, 0xd4"
+#define ASM_VMX_VMXOFF            ".byte 0x0f, 0x01, 0xc4"
+#define ASM_VMX_VMXON_RAX         ".byte 0xf3, 0x0f, 0xc7, 0x30"
+#define ASM_VMX_INVEPT		  ".byte 0x66, 0x0f, 0x38, 0x80, 0x08"
+#define ASM_VMX_INVVPID		  ".byte 0x66, 0x0f, 0x38, 0x81, 0x08"
+
+#define MSR_IA32_TIME_STAMP_COUNTER		0x010
+
+#define TSS_IOPB_BASE_OFFSET 0x66
+#define TSS_BASE_SIZE 0x68
+#define TSS_IOPB_SIZE (65536 / 8)
+#define TSS_REDIRECTION_SIZE (256 / 8)
+#define RMODE_TSS_SIZE							\
+	(TSS_BASE_SIZE + TSS_REDIRECTION_SIZE + TSS_IOPB_SIZE + 1)
+
+enum {
+	TASK_SWITCH_CALL = 0,
+	TASK_SWITCH_IRET = 1,
+	TASK_SWITCH_JMP = 2,
+	TASK_SWITCH_GATE = 3,
+};
+
+/*
+ * Hardware virtualization extension instructions may fault if a
+ * reboot turns off virtualization while processes are running.
+ * Trap the fault and ignore the instruction if that happens.
+ */
+asmlinkage void kvm_handle_fault_on_reboot(void);
+
+#define __kvm_handle_fault_on_reboot(insn) \
+	"666: " insn "\n\t" \
+	".pushsection .fixup, \"ax\" \n" \
+	"667: \n\t" \
+	__ASM_SIZE(push) " $666b \n\t"	      \
+	"jmp kvm_handle_fault_on_reboot \n\t" \
+	".popsection \n\t" \
+	".pushsection __ex_table, \"a\" \n\t" \
+	_ASM_PTR " 666b, 667b \n\t" \
+	".popsection"
+
+#define KVM_ARCH_WANT_MMU_NOTIFIER
+int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
+int kvm_age_hva(struct kvm *kvm, unsigned long hva);
+
+#endif /* ASM_X86__KVM_HOST_H */
