commit 4d65adfcd1196818659d3bd9b42dccab291e1751
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Fri Sep 6 22:14:10 2019 +0900

    x86: xen: insn: Decode Xen and KVM emulate-prefix signature
    
    Decode Xen and KVM's emulate-prefix signature by x86 insn decoder.
    It is called "prefix" but actually not x86 instruction prefix, so
    this adds insn.emulate_prefix_size field instead of reusing
    insn.prefixes.
    
    If x86 decoder finds a special sequence of instructions of
    XEN_EMULATE_PREFIX and 'ud2a; .ascii "kvm"', it just counts the
    length, set insn.emulate_prefix_size and fold it with the next
    instruction. In other words, the signature and the next instruction
    is treated as a single instruction.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: x86@kernel.org
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Link: https://lkml.kernel.org/r/156777564986.25081.4964537658500952557.stgit@devnote2

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index 154f27be8bfc..5c1ae3eff9d4 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -45,6 +45,7 @@ struct insn {
 		struct insn_field immediate2;	/* for 64bit imm or seg16 */
 	};
 
+	int	emulate_prefix_size;
 	insn_attr_t attr;
 	unsigned char opnd_bytes;
 	unsigned char addr_bytes;
@@ -128,6 +129,11 @@ static inline int insn_is_evex(struct insn *insn)
 	return (insn->vex_prefix.nbytes == 4);
 }
 
+static inline int insn_has_emulate_prefix(struct insn *insn)
+{
+	return !!insn->emulate_prefix_size;
+}
+
 /* Ensure this instruction is decoded completely */
 static inline int insn_complete(struct insn *insn)
 {

commit 1a59d1b8e05ea6ab45f7e18897de1ef0e6bc3da6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:05 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 156
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details you
      should have received a copy of the gnu general public license along
      with this program if not write to the free software foundation inc
      59 temple place suite 330 boston ma 02111 1307 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 1334 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070033.113240726@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index c2c01f84df75..154f27be8bfc 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -1,22 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 #ifndef _ASM_X86_INSN_H
 #define _ASM_X86_INSN_H
 /*
  * x86 instruction analysis
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
- *
  * Copyright (C) IBM Corporation, 2009
  */
 

commit ee6a7354a3629f9b65bc18dbe393503e9440d6f5
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Wed May 9 21:58:15 2018 +0900

    kprobes/x86: Prohibit probing on exception masking instructions
    
    Since MOV SS and POP SS instructions will delay the exceptions until the
    next instruction is executed, single-stepping on it by kprobes must be
    prohibited.
    
    However, kprobes usually executes those instructions directly on trampoline
    buffer (a.k.a. kprobe-booster), except for the kprobes which has
    post_handler. Thus if kprobe user probes MOV SS with post_handler, it will
    do single-stepping on the MOV SS.
    
    This means it is safe that if it is used via ftrace or perf/bpf since those
    don't use the post_handler.
    
    Anyway, since the stack switching is a rare case, it is safer just
    rejecting kprobes on such instructions.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: Francis Deslauriers <francis.deslauriers@efficios.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Yonghong Song <yhs@fb.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S . Miller" <davem@davemloft.net>
    Link: https://lkml.kernel.org/r/152587069574.17316.3311695234863248641.stgit@devbox

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index b3e32b010ab1..c2c01f84df75 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -208,4 +208,22 @@ static inline int insn_offset_immediate(struct insn *insn)
 	return insn_offset_displacement(insn) + insn->displacement.nbytes;
 }
 
+#define POP_SS_OPCODE 0x1f
+#define MOV_SREG_OPCODE 0x8e
+
+/*
+ * Intel SDM Vol.3A 6.8.3 states;
+ * "Any single-step trap that would be delivered following the MOV to SS
+ * instruction or POP to SS instruction (because EFLAGS.TF is 1) is
+ * suppressed."
+ * This function returns true if @insn is MOV SS or POP SS. On these
+ * instructions, single stepping is suppressed.
+ */
+static inline int insn_masking_exception(struct insn *insn)
+{
+	return insn->opcode.bytes[0] == POP_SS_OPCODE ||
+		(insn->opcode.bytes[0] == MOV_SREG_OPCODE &&
+		 X86_MODRM_REG(insn->modrm.bytes[0]) == 2);
+}
+
 #endif /* _ASM_X86_INSN_H */

commit 25af37f4e1e0a747824e3713b80d6b97dad28b7c
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Wed Jul 20 11:30:35 2016 +0300

    x86/insn: Add AVX-512 support to the instruction decoder
    
    Add support for Intel's AVX-512 instructions to the instruction decoder.
    
    AVX-512 instructions are documented in Intel Architecture Instruction
    Set Extensions Programming Reference (February 2016).
    
    AVX-512 instructions are identified by a EVEX prefix which, for the
    purpose of instruction decoding, can be treated as though it were a
    4-byte VEX prefix.
    
    Existing instructions which can now accept an EVEX prefix need not be
    further annotated in the op code map (x86-opcode-map.txt). In the case
    of new instructions, the op code map is updated accordingly.
    
    Also add associated Mask Instructions that are used to manipulate mask
    registers used in AVX-512 instructions.
    
    The 'perf tools' instruction decoder is updated in a subsequent patch.
    And a representative set of instructions is added to the perf tools new
    instructions test in a subsequent patch.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: X86 ML <x86@kernel.org>
    Link: http://lkml.kernel.org/r/1469003437-32706-3-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index e7814b74caf8..b3e32b010ab1 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -91,6 +91,7 @@ struct insn {
 #define X86_VEX_B(vex)	((vex) & 0x20)	/* VEX3 Byte1 */
 #define X86_VEX_L(vex)	((vex) & 0x04)	/* VEX3 Byte2, VEX2 Byte1 */
 /* VEX bit fields */
+#define X86_EVEX_M(vex)	((vex) & 0x03)		/* EVEX Byte1 */
 #define X86_VEX3_M(vex)	((vex) & 0x1f)		/* VEX3 Byte1 */
 #define X86_VEX2_M	1			/* VEX2.M always 1 */
 #define X86_VEX_V(vex)	(((vex) & 0x78) >> 3)	/* VEX3 Byte2, VEX2 Byte1 */
@@ -133,6 +134,13 @@ static inline int insn_is_avx(struct insn *insn)
 	return (insn->vex_prefix.value != 0);
 }
 
+static inline int insn_is_evex(struct insn *insn)
+{
+	if (!insn->prefixes.got)
+		insn_get_prefixes(insn);
+	return (insn->vex_prefix.nbytes == 4);
+}
+
 /* Ensure this instruction is decoded completely */
 static inline int insn_complete(struct insn *insn)
 {
@@ -144,8 +152,10 @@ static inline insn_byte_t insn_vex_m_bits(struct insn *insn)
 {
 	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
 		return X86_VEX2_M;
-	else
+	else if (insn->vex_prefix.nbytes == 3)	/* 3 bytes VEX */
 		return X86_VEX3_M(insn->vex_prefix.bytes[1]);
+	else					/* EVEX */
+		return X86_EVEX_M(insn->vex_prefix.bytes[1]);
 }
 
 static inline insn_byte_t insn_vex_p_bits(struct insn *insn)

commit 91e5ed49fca09c2b83b262b9757d1376ee2b46c3
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Jan 27 16:06:02 2015 -0800

    x86/asm/decoder: Fix and enforce max instruction size in the insn decoder
    
    x86 instructions cannot exceed 15 bytes, and the instruction
    decoder should enforce that.  Prior to 6ba48ff46f76, the
    instruction length limit was implicitly set to 16, which was an
    approximation of 15, but there is currently no limit at all.
    
    Fix MAX_INSN_SIZE (it should be 15, not 16), and fix the decoder
    to reject instructions that exceed MAX_INSN_SIZE.
    
    Other than potentially confusing some of the decoder sanity
    checks, I'm not aware of any actual problems that omitting this
    check would cause, nor am I aware of any practical problems
    caused by the MAX_INSN_SIZE error.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Fixes: 6ba48ff46f76 ("x86: Remove arbitrary instruction size limit ...
    Link: http://lkml.kernel.org/r/f8f0bc9b8c58cfd6830f7d88400bf1396cbdcd0f.1422403511.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index 47f29b1d1846..e7814b74caf8 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -69,7 +69,7 @@ struct insn {
 	const insn_byte_t *next_byte;
 };
 
-#define MAX_INSN_SIZE	16
+#define MAX_INSN_SIZE	15
 
 #define X86_MODRM_MOD(modrm) (((modrm) & 0xc0) >> 6)
 #define X86_MODRM_REG(modrm) (((modrm) & 0x38) >> 3)

commit 6ba48ff46f764414f979d2eacb23c4e6296bcc95
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Nov 14 07:39:57 2014 -0800

    x86: Remove arbitrary instruction size limit in instruction decoder
    
    The current x86 instruction decoder steps along through the
    instruction stream but always ensures that it never steps farther
    than the largest possible instruction size (MAX_INSN_SIZE).
    
    The MPX code is now going to be doing some decoding of userspace
    instructions.  We copy those from userspace in to the kernel and
    they're obviously completely untrusted coming from userspace.  In
    addition to the constraint that instructions can only be so long,
    we also have to be aware of how long the buffer is that came in
    from userspace.  This _looks_ to be similar to what the perf and
    kprobes is doing, but it's unclear to me whether they are
    affected.
    
    The whole reason we need this is that it is perfectly valid to be
    executing an instruction within MAX_INSN_SIZE bytes of an
    unreadable page. We should be able to gracefully handle short
    reads in those cases.
    
    This adds support to the decoder to record how long the buffer
    being decoded is and to refuse to "validate" the instruction if
    we would have gone over the end of the buffer to decode it.
    
    The kprobes code probably needs to be looked at here a bit more
    carefully.  This patch still respects the MAX_INSN_SIZE limit
    there but the kprobes code does look like it might be able to
    be a bit more strict than it currently is.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Acked-by: Jim Keniston <jkenisto@us.ibm.com>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: x86@kernel.org
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Link: http://lkml.kernel.org/r/20141114153957.E6B01535@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index 48eb30a86062..47f29b1d1846 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -65,6 +65,7 @@ struct insn {
 	unsigned char x86_64;
 
 	const insn_byte_t *kaddr;	/* kernel address of insn to analyze */
+	const insn_byte_t *end_kaddr;	/* kernel address of last insn in buffer */
 	const insn_byte_t *next_byte;
 };
 
@@ -96,7 +97,7 @@ struct insn {
 #define X86_VEX_P(vex)	((vex) & 0x03)		/* VEX3 Byte2, VEX2 Byte1 */
 #define X86_VEX_M_MAX	0x1f			/* VEX3.M Maximum value */
 
-extern void insn_init(struct insn *insn, const void *kaddr, int x86_64);
+extern void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64);
 extern void insn_get_prefixes(struct insn *insn);
 extern void insn_get_opcode(struct insn *insn);
 extern void insn_get_modrm(struct insn *insn);
@@ -115,12 +116,13 @@ static inline void insn_get_attribute(struct insn *insn)
 extern int insn_rip_relative(struct insn *insn);
 
 /* Init insn for kernel text */
-static inline void kernel_insn_init(struct insn *insn, const void *kaddr)
+static inline void kernel_insn_init(struct insn *insn,
+				    const void *kaddr, int buf_len)
 {
 #ifdef CONFIG_X86_64
-	insn_init(insn, kaddr, 1);
+	insn_init(insn, kaddr, buf_len, 1);
 #else /* CONFIG_X86_32 */
-	insn_init(insn, kaddr, 0);
+	insn_init(insn, kaddr, buf_len, 0);
 #endif
 }
 

commit f8d98f1095210da708a59f3a0b6fd267ad8f3f03
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Fri Feb 10 14:33:40 2012 +0900

    x86: Fix to decode grouped AVX with VEX pp bits
    
    Fix to decode grouped AVX with VEX pp bits which should be
    handled as same as last-prefixes. This fixes below warnings
    in posttest with CONFIG_CRYPTO_SHA1_SSSE3=y.
    
     Warning: arch/x86/tools/test_get_len found difference at <sha1_transform_avx>:ffffffff810d5fc0
     Warning: ffffffff810d6069:     c5 f9 73 de 04          vpsrldq $0x4,%xmm6,%xmm0
     Warning: objdump says 5 bytes, but insn_get_length() says 4
     ...
    
    With this change, test_get_len can decode it correctly.
    
     $ arch/x86/tools/test_get_len -v -y
     ffffffff810d6069:       c5 f9 73 de 04          vpsrldq $0x4,%xmm6,%xmm0
     Succeed: decoded and checked 1 instructions
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: yrl.pp-manager.tt@hitachi.com
    Link: http://lkml.kernel.org/r/20120210053340.30429.73410.stgit@localhost.localdomain
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index 74df3f1eddfd..48eb30a86062 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -96,12 +96,6 @@ struct insn {
 #define X86_VEX_P(vex)	((vex) & 0x03)		/* VEX3 Byte2, VEX2 Byte1 */
 #define X86_VEX_M_MAX	0x1f			/* VEX3.M Maximum value */
 
-/* The last prefix is needed for two-byte and three-byte opcodes */
-static inline insn_byte_t insn_last_prefix(struct insn *insn)
-{
-	return insn->prefixes.bytes[3];
-}
-
 extern void insn_init(struct insn *insn, const void *kaddr, int x86_64);
 extern void insn_get_prefixes(struct insn *insn);
 extern void insn_get_opcode(struct insn *insn);
@@ -160,6 +154,18 @@ static inline insn_byte_t insn_vex_p_bits(struct insn *insn)
 		return X86_VEX_P(insn->vex_prefix.bytes[2]);
 }
 
+/* Get the last prefix id from last prefix or VEX prefix */
+static inline int insn_last_prefix_id(struct insn *insn)
+{
+	if (insn_is_avx(insn))
+		return insn_vex_p_bits(insn);	/* VEX_p is a SIMD prefix id */
+
+	if (insn->prefixes.bytes[3])
+		return inat_get_last_prefix_id(insn->prefixes.bytes[3]);
+
+	return 0;
+}
+
 /* Offset of each field from kaddr */
 static inline int insn_offset_rex_prefix(struct insn *insn)
 {

commit 1ec454baf1245df4fdb5dae728da3363630ce6de
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Oct 20 23:01:09 2011 +0900

    x86, perf: Add a build-time sanity test to the x86 decoder
    
    Add a sanity test of x86 insn decoder against a stream
    of randomly generated input, at build time.
    
    This test is also able to reproduce any bug that might
    trigger by allowing the passing of random-seed and
    iteration-number to the test, or by passing input
    which has invalid byte code.
    
    Changes in V2:
     - Code cleanup.
     - Show how to reproduce the error by insn_sanity test.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: acme@redhat.com
    Cc: ming.m.lin@intel.com
    Cc: robert.richter@amd.com
    Cc: ravitillo@lbl.gov
    Cc: yrl.pp-manager.tt@hitachi.com
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20111020140109.20938.92572.stgit@localhost.localdomain
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index 88c765e16410..74df3f1eddfd 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -137,6 +137,13 @@ static inline int insn_is_avx(struct insn *insn)
 	return (insn->vex_prefix.value != 0);
 }
 
+/* Ensure this instruction is decoded completely */
+static inline int insn_complete(struct insn *insn)
+{
+	return insn->opcode.got && insn->modrm.got && insn->sib.got &&
+		insn->displacement.got && insn->immediate.got;
+}
+
 static inline insn_byte_t insn_vex_m_bits(struct insn *insn)
 {
 	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */

commit 30a813ae035d3e220a89609adce878e045c49547
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 4 13:49:21 2010 +0100

    x86: Move MAX_INSN_SIZE into asm/insn.h
    
    Since there's now two users for this, place it in a common header.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: paulus@samba.org
    Cc: eranian@google.com
    Cc: robert.richter@amd.com
    Cc: fweisbec@gmail.com
    LKML-Reference: <20100304140100.923774125@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index 96c2e0ad04ca..88c765e16410 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -68,6 +68,8 @@ struct insn {
 	const insn_byte_t *next_byte;
 };
 
+#define MAX_INSN_SIZE	16
+
 #define X86_MODRM_MOD(modrm) (((modrm) & 0xc0) >> 6)
 #define X86_MODRM_REG(modrm) (((modrm) & 0x38) >> 3)
 #define X86_MODRM_RM(modrm) ((modrm) & 0x07)

commit e0e492e99b372c6990a5daca9e4683c341f1330e
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Tue Oct 27 16:42:27 2009 -0400

    x86: AVX instruction set decoder support
    
    Add Intel AVX(Advanced Vector Extensions) instruction set
    support to x86 instruction decoder. This adds insn.vex_prefix
    field for storing VEX prefixes, and introduces some original
    tags for expressing opcodes attributes.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    LKML-Reference: <20091027204226.30545.23451.stgit@harusame>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
index 12b4e3751d3f..96c2e0ad04ca 100644
--- a/arch/x86/include/asm/insn.h
+++ b/arch/x86/include/asm/insn.h
@@ -39,6 +39,7 @@ struct insn {
 					 * prefixes.bytes[3]: last prefix
 					 */
 	struct insn_field rex_prefix;	/* REX prefix */
+	struct insn_field vex_prefix;	/* VEX prefix */
 	struct insn_field opcode;	/*
 					 * opcode.bytes[0]: opcode1
 					 * opcode.bytes[1]: opcode2
@@ -80,6 +81,19 @@ struct insn {
 #define X86_REX_X(rex) ((rex) & 2)
 #define X86_REX_B(rex) ((rex) & 1)
 
+/* VEX bit flags  */
+#define X86_VEX_W(vex)	((vex) & 0x80)	/* VEX3 Byte2 */
+#define X86_VEX_R(vex)	((vex) & 0x80)	/* VEX2/3 Byte1 */
+#define X86_VEX_X(vex)	((vex) & 0x40)	/* VEX3 Byte1 */
+#define X86_VEX_B(vex)	((vex) & 0x20)	/* VEX3 Byte1 */
+#define X86_VEX_L(vex)	((vex) & 0x04)	/* VEX3 Byte2, VEX2 Byte1 */
+/* VEX bit fields */
+#define X86_VEX3_M(vex)	((vex) & 0x1f)		/* VEX3 Byte1 */
+#define X86_VEX2_M	1			/* VEX2.M always 1 */
+#define X86_VEX_V(vex)	(((vex) & 0x78) >> 3)	/* VEX3 Byte2, VEX2 Byte1 */
+#define X86_VEX_P(vex)	((vex) & 0x03)		/* VEX3 Byte2, VEX2 Byte1 */
+#define X86_VEX_M_MAX	0x1f			/* VEX3.M Maximum value */
+
 /* The last prefix is needed for two-byte and three-byte opcodes */
 static inline insn_byte_t insn_last_prefix(struct insn *insn)
 {
@@ -114,15 +128,42 @@ static inline void kernel_insn_init(struct insn *insn, const void *kaddr)
 #endif
 }
 
+static inline int insn_is_avx(struct insn *insn)
+{
+	if (!insn->prefixes.got)
+		insn_get_prefixes(insn);
+	return (insn->vex_prefix.value != 0);
+}
+
+static inline insn_byte_t insn_vex_m_bits(struct insn *insn)
+{
+	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
+		return X86_VEX2_M;
+	else
+		return X86_VEX3_M(insn->vex_prefix.bytes[1]);
+}
+
+static inline insn_byte_t insn_vex_p_bits(struct insn *insn)
+{
+	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
+		return X86_VEX_P(insn->vex_prefix.bytes[1]);
+	else
+		return X86_VEX_P(insn->vex_prefix.bytes[2]);
+}
+
 /* Offset of each field from kaddr */
 static inline int insn_offset_rex_prefix(struct insn *insn)
 {
 	return insn->prefixes.nbytes;
 }
-static inline int insn_offset_opcode(struct insn *insn)
+static inline int insn_offset_vex_prefix(struct insn *insn)
 {
 	return insn_offset_rex_prefix(insn) + insn->rex_prefix.nbytes;
 }
+static inline int insn_offset_opcode(struct insn *insn)
+{
+	return insn_offset_vex_prefix(insn) + insn->vex_prefix.nbytes;
+}
 static inline int insn_offset_modrm(struct insn *insn)
 {
 	return insn_offset_opcode(insn) + insn->opcode.nbytes;

commit eb13296cfaf6c699566473669a96a38a90562384
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Aug 13 16:34:13 2009 -0400

    x86: Instruction decoder API
    
    Add x86 instruction decoder to arch-specific libraries. This decoder
    can decode x86 instructions used in kernel into prefix, opcode, modrm,
    sib, displacement and immediates. This can also show the length of
    instructions.
    
    This version introduces instruction attributes for decoding
    instructions.
    The instruction attribute tables are generated from the opcode map file
    (x86-opcode-map.txt) by the generator script(gen-insn-attr-x86.awk).
    
    Currently, the opcode maps are based on opcode maps in Intel(R) 64 and
    IA-32 Architectures Software Developers Manual Vol.2: Appendix.A,
    and consist of below two types of opcode tables.
    
    1-byte/2-bytes/3-bytes opcodes, which has 256 elements, are
    written as below;
    
     Table: table-name
     Referrer: escaped-name
     opcode: mnemonic|GrpXXX [operand1[,operand2...]] [(extra1)[,(extra2)...] [| 2nd-mnemonic ...]
      (or)
     opcode: escape # escaped-name
     EndTable
    
    Group opcodes, which has 8 elements, are written as below;
    
     GrpTable: GrpXXX
     reg:  mnemonic [operand1[,operand2...]] [(extra1)[,(extra2)...] [| 2nd-mnemonic ...]
     EndTable
    
    These opcode maps include a few SSE and FP opcodes (for setup), because
    those opcodes are used in the kernel.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Jim Keniston <jkenisto@us.ibm.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Przemysław Pawełczyk <przemyslaw@pawelczyk.it>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    LKML-Reference: <20090813203413.31965.49709.stgit@localhost.localdomain>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/include/asm/insn.h b/arch/x86/include/asm/insn.h
new file mode 100644
index 000000000000..12b4e3751d3f
--- /dev/null
+++ b/arch/x86/include/asm/insn.h
@@ -0,0 +1,143 @@
+#ifndef _ASM_X86_INSN_H
+#define _ASM_X86_INSN_H
+/*
+ * x86 instruction analysis
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (C) IBM Corporation, 2009
+ */
+
+/* insn_attr_t is defined in inat.h */
+#include <asm/inat.h>
+
+struct insn_field {
+	union {
+		insn_value_t value;
+		insn_byte_t bytes[4];
+	};
+	/* !0 if we've run insn_get_xxx() for this field */
+	unsigned char got;
+	unsigned char nbytes;
+};
+
+struct insn {
+	struct insn_field prefixes;	/*
+					 * Prefixes
+					 * prefixes.bytes[3]: last prefix
+					 */
+	struct insn_field rex_prefix;	/* REX prefix */
+	struct insn_field opcode;	/*
+					 * opcode.bytes[0]: opcode1
+					 * opcode.bytes[1]: opcode2
+					 * opcode.bytes[2]: opcode3
+					 */
+	struct insn_field modrm;
+	struct insn_field sib;
+	struct insn_field displacement;
+	union {
+		struct insn_field immediate;
+		struct insn_field moffset1;	/* for 64bit MOV */
+		struct insn_field immediate1;	/* for 64bit imm or off16/32 */
+	};
+	union {
+		struct insn_field moffset2;	/* for 64bit MOV */
+		struct insn_field immediate2;	/* for 64bit imm or seg16 */
+	};
+
+	insn_attr_t attr;
+	unsigned char opnd_bytes;
+	unsigned char addr_bytes;
+	unsigned char length;
+	unsigned char x86_64;
+
+	const insn_byte_t *kaddr;	/* kernel address of insn to analyze */
+	const insn_byte_t *next_byte;
+};
+
+#define X86_MODRM_MOD(modrm) (((modrm) & 0xc0) >> 6)
+#define X86_MODRM_REG(modrm) (((modrm) & 0x38) >> 3)
+#define X86_MODRM_RM(modrm) ((modrm) & 0x07)
+
+#define X86_SIB_SCALE(sib) (((sib) & 0xc0) >> 6)
+#define X86_SIB_INDEX(sib) (((sib) & 0x38) >> 3)
+#define X86_SIB_BASE(sib) ((sib) & 0x07)
+
+#define X86_REX_W(rex) ((rex) & 8)
+#define X86_REX_R(rex) ((rex) & 4)
+#define X86_REX_X(rex) ((rex) & 2)
+#define X86_REX_B(rex) ((rex) & 1)
+
+/* The last prefix is needed for two-byte and three-byte opcodes */
+static inline insn_byte_t insn_last_prefix(struct insn *insn)
+{
+	return insn->prefixes.bytes[3];
+}
+
+extern void insn_init(struct insn *insn, const void *kaddr, int x86_64);
+extern void insn_get_prefixes(struct insn *insn);
+extern void insn_get_opcode(struct insn *insn);
+extern void insn_get_modrm(struct insn *insn);
+extern void insn_get_sib(struct insn *insn);
+extern void insn_get_displacement(struct insn *insn);
+extern void insn_get_immediate(struct insn *insn);
+extern void insn_get_length(struct insn *insn);
+
+/* Attribute will be determined after getting ModRM (for opcode groups) */
+static inline void insn_get_attribute(struct insn *insn)
+{
+	insn_get_modrm(insn);
+}
+
+/* Instruction uses RIP-relative addressing */
+extern int insn_rip_relative(struct insn *insn);
+
+/* Init insn for kernel text */
+static inline void kernel_insn_init(struct insn *insn, const void *kaddr)
+{
+#ifdef CONFIG_X86_64
+	insn_init(insn, kaddr, 1);
+#else /* CONFIG_X86_32 */
+	insn_init(insn, kaddr, 0);
+#endif
+}
+
+/* Offset of each field from kaddr */
+static inline int insn_offset_rex_prefix(struct insn *insn)
+{
+	return insn->prefixes.nbytes;
+}
+static inline int insn_offset_opcode(struct insn *insn)
+{
+	return insn_offset_rex_prefix(insn) + insn->rex_prefix.nbytes;
+}
+static inline int insn_offset_modrm(struct insn *insn)
+{
+	return insn_offset_opcode(insn) + insn->opcode.nbytes;
+}
+static inline int insn_offset_sib(struct insn *insn)
+{
+	return insn_offset_modrm(insn) + insn->modrm.nbytes;
+}
+static inline int insn_offset_displacement(struct insn *insn)
+{
+	return insn_offset_sib(insn) + insn->sib.nbytes;
+}
+static inline int insn_offset_immediate(struct insn *insn)
+{
+	return insn_offset_displacement(insn) + insn->displacement.nbytes;
+}
+
+#endif /* _ASM_X86_INSN_H */
