commit 2853d5fafb1e21707e89aa2e227c90bb2c1ea4a9
Merge: d5f744f9a2ac a6a60741035b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 19:35:52 2020 -0700

    Merge tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 splitlock updates from Thomas Gleixner:
     "Support for 'split lock' detection:
    
      Atomic operations (lock prefixed instructions) which span two cache
      lines have to acquire the global bus lock. This is at least 1k cycles
      slower than an atomic operation within a cache line and disrupts
      performance on other cores. Aside of performance disruption this is a
      unpriviledged form of DoS.
    
      Some newer CPUs have the capability to raise an #AC trap when such an
      operation is attempted. The detection is by default enabled in warning
      mode which will warn once when a user space application is caught. A
      command line option allows to disable the detection or to select fatal
      mode which will terminate offending applications with SIGBUS"
    
    * tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/split_lock: Avoid runtime reads of the TEST_CTRL MSR
      x86/split_lock: Rework the initialization flow of split lock detection
      x86/split_lock: Enable split lock detection by kernel

commit 6650cdd9a8ccf00555dbbe743d58541ad8feb6a7
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Sun Jan 26 12:05:35 2020 -0800

    x86/split_lock: Enable split lock detection by kernel
    
    A split-lock occurs when an atomic instruction operates on data that spans
    two cache lines. In order to maintain atomicity the core takes a global bus
    lock.
    
    This is typically >1000 cycles slower than an atomic operation within a
    cache line. It also disrupts performance on other cores (which must wait
    for the bus lock to be released before their memory operations can
    complete). For real-time systems this may mean missing deadlines. For other
    systems it may just be very annoying.
    
    Some CPUs have the capability to raise an #AC trap when a split lock is
    attempted.
    
    Provide a command line option to give the user choices on how to handle
    this:
    
    split_lock_detect=
            off     - not enabled (no traps for split locks)
            warn    - warn once when an application does a
                      split lock, but allow it to continue
                      running.
            fatal   - Send SIGBUS to applications that cause split lock
    
    On systems that support split lock detection the default is "warn". Note
    that if the kernel hits a split lock in any mode other than "off" it will
    OOPs.
    
    One implementation wrinkle is that the MSR to control the split lock
    detection is per-core, not per thread. This might result in some short
    lived races on HT systems in "warn" mode if Linux tries to enable on one
    thread while disabling on the other. Race analysis by Sean Christopherson:
    
      - Toggling of split-lock is only done in "warn" mode.  Worst case
        scenario of a race is that a misbehaving task will generate multiple
        #AC exceptions on the same instruction.  And this race will only occur
        if both siblings are running tasks that generate split-lock #ACs, e.g.
        a race where sibling threads are writing different values will only
        occur if CPUx is disabling split-lock after an #AC and CPUy is
        re-enabling split-lock after *its* previous task generated an #AC.
      - Transitioning between off/warn/fatal modes at runtime isn't supported
        and disabling is tracked per task, so hardware will always reach a steady
        state that matches the configured mode.  I.e. split-lock is guaranteed to
        be enabled in hardware once all _TIF_SLD threads have been scheduled out.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Co-developed-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20200126200535.GB30377@agluck-desk2.amr.corp.intel.com

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index cf4327986e98..f807930bd763 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -92,6 +92,7 @@ struct thread_info {
 #define TIF_NOCPUID		15	/* CPUID is not accessible in userland */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
+#define TIF_SLD			18	/* Restore split lock detection on context switch */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
@@ -122,6 +123,7 @@ struct thread_info {
 #define _TIF_NOCPUID		(1 << TIF_NOCPUID)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
+#define _TIF_SLD		(1 << TIF_SLD)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
@@ -145,7 +147,7 @@ struct thread_info {
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW_BASE					\
 	(_TIF_NOCPUID | _TIF_NOTSC | _TIF_BLOCKSTEP |		\
-	 _TIF_SSBD | _TIF_SPEC_FORCE_UPDATE)
+	 _TIF_SSBD | _TIF_SPEC_FORCE_UPDATE | _TIF_SLD)
 
 /*
  * Avoid calls to __switch_to_xtra() on UP as STIBP is not evaluated.

commit 68d875131e43a8eed193810c1dfebb027fe2450e
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Tue Jan 28 13:50:32 2020 +0100

    x86: Remove TIF_NOHZ
    
    Static keys have replaced TIF_NOHZ to optimize the calls to context
    tracking. We can now safely remove that thread flag.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 6cb9d1b0d1e6..384cdde10680 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -92,7 +92,6 @@ struct thread_info {
 #define TIF_NOCPUID		15	/* CPUID is not accessible in userland */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
-#define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
@@ -122,7 +121,6 @@ struct thread_info {
 #define _TIF_NOCPUID		(1 << TIF_NOCPUID)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
-#define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)

commit 7c805795307b40af50a45b7db44dd09ac1700947
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 23 14:27:14 2019 +0200

    x86/entry: Remove _TIF_NOHZ from _TIF_WORK_SYSCALL_ENTRY
    
    Evaluating _TIF_NOHZ to decide whether to use the slow syscall entry path
    is not only pointless, it's actually counterproductive:
    
     1) Context tracking code is invoked unconditionally before that flag is
        evaluated.
    
     2) If the flag is set the slow path is invoked for nothing due to #1
    
    Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index cf4327986e98..6cb9d1b0d1e6 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -133,14 +133,10 @@ struct thread_info {
 #define _TIF_X32		(1 << TIF_X32)
 #define _TIF_FSCHECK		(1 << TIF_FSCHECK)
 
-/*
- * work to do in syscall_trace_enter().  Also includes TIF_NOHZ for
- * enter_from_user_mode()
- */
+/* Work to do before invoking the actual syscall. */
 #define _TIF_WORK_SYSCALL_ENTRY	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_AUDIT |	\
-	 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT |	\
-	 _TIF_NOHZ)
+	 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT)
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW_BASE					\

commit 2b10906f2d25515bba58070b8183babc89063597
Author: Brian Gerst <brgerst@gmail.com>
Date:   Thu Dec 19 06:58:12 2019 -0500

    x86: Remove force_iret()
    
    force_iret() was originally intended to prevent the return to user mode with
    the SYSRET or SYSEXIT instructions, in cases where the register state could
    have been changed to be incompatible with those instructions.  The entry code
    has been significantly reworked since then, and register state is validated
    before SYSRET or SYSEXIT are used.  force_iret() no longer serves its original
    purpose and can be eliminated.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Link: https://lkml.kernel.org/r/20191219115812.102620-1-brgerst@gmail.com

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index d779366ce3f8..cf4327986e98 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -239,15 +239,6 @@ static inline int arch_within_stack_frames(const void * const stack,
 			   current_thread_info()->status & TS_COMPAT)
 #endif
 
-/*
- * Force syscall return via IRET by making it look as if there was
- * some work pending. IRET is our most capable (but slowest) syscall
- * return path, which is able to restore modified SS, CS and certain
- * EFLAGS values that other (fast) syscall return instructions
- * are not able to restore properly.
- */
-#define force_iret() set_thread_flag(TIF_NOTIFY_RESUME)
-
 extern void arch_task_cache_init(void);
 extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
 extern void arch_release_task_struct(struct task_struct *tsk);

commit 111e7b15cf10f6e973ccf537c70c66a5de539060
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 12 21:40:33 2019 +0100

    x86/ioperm: Extend IOPL config to control ioperm() as well
    
    If iopl() is disabled, then providing ioperm() does not make much sense.
    
    Rename the config option and disable/enable both syscalls with it. Guard
    the code with #ifdefs where appropriate.
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 0accf44878a5..d779366ce3f8 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -156,8 +156,13 @@ struct thread_info {
 # define _TIF_WORK_CTXSW	(_TIF_WORK_CTXSW_BASE)
 #endif
 
-#define _TIF_WORK_CTXSW_PREV	(_TIF_WORK_CTXSW| _TIF_USER_RETURN_NOTIFY | \
+#ifdef CONFIG_X86_IOPL_IOPERM
+# define _TIF_WORK_CTXSW_PREV	(_TIF_WORK_CTXSW| _TIF_USER_RETURN_NOTIFY | \
 				 _TIF_IO_BITMAP)
+#else
+# define _TIF_WORK_CTXSW_PREV	(_TIF_WORK_CTXSW| _TIF_USER_RETURN_NOTIFY)
+#endif
+
 #define _TIF_WORK_CTXSW_NEXT	(_TIF_WORK_CTXSW)
 
 #define STACK_WARN		(THREAD_SIZE/8)

commit 22fe5b0439dd53643fd6f4c582c46c6dba0fde53
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:23 2019 +0100

    x86/ioperm: Move TSS bitmap update to exit to user work
    
    There is no point to update the TSS bitmap for tasks which use I/O bitmaps
    on every context switch. It's enough to update it right before exiting to
    user space.
    
    That reduces the context switch bitmap handling to invalidating the io
    bitmap base offset in the TSS when the outgoing task has TIF_IO_BITMAP
    set. The invaldiation is done on purpose when a task with an IO bitmap
    switches out to prevent any possible leakage of an activated IO bitmap.
    
    It also removes the requirement to update the tasks bitmap atomically in
    ioperm().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index f9453536f9bb..0accf44878a5 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -143,8 +143,8 @@ struct thread_info {
 	 _TIF_NOHZ)
 
 /* flags to check in __switch_to() */
-#define _TIF_WORK_CTXSW_BASE						\
-	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|		\
+#define _TIF_WORK_CTXSW_BASE					\
+	(_TIF_NOCPUID | _TIF_NOTSC | _TIF_BLOCKSTEP |		\
 	 _TIF_SSBD | _TIF_SPEC_FORCE_UPDATE)
 
 /*
@@ -156,8 +156,9 @@ struct thread_info {
 # define _TIF_WORK_CTXSW	(_TIF_WORK_CTXSW_BASE)
 #endif
 
-#define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
-#define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
+#define _TIF_WORK_CTXSW_PREV	(_TIF_WORK_CTXSW| _TIF_USER_RETURN_NOTIFY | \
+				 _TIF_IO_BITMAP)
+#define _TIF_WORK_CTXSW_NEXT	(_TIF_WORK_CTXSW)
 
 #define STACK_WARN		(THREAD_SIZE/8)
 

commit 383c252545edcc708128e2028a2318b05c45ede4
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:45 2019 +0200

    x86/entry: Add TIF_NEED_FPU_LOAD
    
    Add TIF_NEED_FPU_LOAD. This flag is used for loading the FPU registers
    before returning to userland. It must not be set on systems without a
    FPU.
    
    If this flag is cleared, the CPU's FPU registers hold the latest,
    up-to-date content of the current task's (current()) FPU registers.
    The in-memory copy (union fpregs_state) is not valid.
    
    If this flag is set, then all of CPU's FPU registers may hold a random
    value (except for PKRU) and it is required to load the content of the
    FPU registers on return to userland.
    
    Introduce it now as a preparatory change before adding the main feature.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-17-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e0eccbcb8447..f9453536f9bb 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -88,6 +88,7 @@ struct thread_info {
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
 #define TIF_PATCH_PENDING	13	/* pending live patching update */
+#define TIF_NEED_FPU_LOAD	14	/* load FPU on return to userspace */
 #define TIF_NOCPUID		15	/* CPUID is not accessible in userland */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
@@ -117,6 +118,7 @@ struct thread_info {
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_PATCH_PENDING	(1 << TIF_PATCH_PENDING)
+#define _TIF_NEED_FPU_LOAD	(1 << TIF_NEED_FPU_LOAD)
 #define _TIF_NOCPUID		(1 << TIF_NOCPUID)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)

commit d23650e06299e9b2d04e090ab8c625cdf24465bf
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Nov 28 23:20:09 2018 +0100

    x86/thread_info: Remove _TIF_ALLWORK_MASK
    
    There is no user of _TIF_ALLWORK_MASK since commit
    
      21d375b6b34ff ("x86/entry/64: Remove the SYSCALL64 fast path").
    
    Remove the unused define _TIF_ALLWORK_MASK.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20181128222035.2996-4-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 82b73b75d67c..e0eccbcb8447 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -140,14 +140,6 @@ struct thread_info {
 	 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT |	\
 	 _TIF_NOHZ)
 
-/* work to do on any return to user space */
-#define _TIF_ALLWORK_MASK						\
-	(_TIF_SYSCALL_TRACE | _TIF_NOTIFY_RESUME | _TIF_SIGPENDING |	\
-	 _TIF_NEED_RESCHED | _TIF_SINGLESTEP | _TIF_SYSCALL_EMU |	\
-	 _TIF_SYSCALL_AUDIT | _TIF_USER_RETURN_NOTIFY | _TIF_UPROBE |	\
-	 _TIF_PATCH_PENDING | _TIF_NOHZ | _TIF_SYSCALL_TRACEPOINT |	\
-	 _TIF_FSCHECK)
-
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW_BASE						\
 	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|		\

commit 6d991ba509ebcfcc908e009d1db51972a4f7a064
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 28 10:56:57 2018 +0100

    x86/speculation: Prevent stale SPEC_CTRL msr content
    
    The seccomp speculation control operates on all tasks of a process, but
    only the current task of a process can update the MSR immediately. For the
    other threads the update is deferred to the next context switch.
    
    This creates the following situation with Process A and B:
    
    Process A task 2 and Process B task 1 are pinned on CPU1. Process A task 2
    does not have the speculation control TIF bit set. Process B task 1 has the
    speculation control TIF bit set.
    
    CPU0                                    CPU1
                                            MSR bit is set
                                            ProcB.T1 schedules out
                                            ProcA.T2 schedules in
                                            MSR bit is cleared
    ProcA.T1
      seccomp_update()
      set TIF bit on ProcA.T2
                                            ProcB.T1 schedules in
                                            MSR is not updated  <-- FAIL
    
    This happens because the context switch code tries to avoid the MSR update
    if the speculation control TIF bits of the incoming and the outgoing task
    are the same. In the worst case ProcB.T1 and ProcA.T2 are the only tasks
    scheduling back and forth on CPU1, which keeps the MSR stale forever.
    
    In theory this could be remedied by IPIs, but chasing the remote task which
    could be migrated is complex and full of races.
    
    The straight forward solution is to avoid the asychronous update of the TIF
    bit and defer it to the next context switch. The speculation control state
    is stored in task_struct::atomic_flags by the prctl and seccomp updates
    already.
    
    Add a new TIF_SPEC_FORCE_UPDATE bit and set this after updating the
    atomic_flags. Check the bit on context switch and force a synchronous
    update of the speculation control if set. Use the same mechanism for
    updating the current task.
    
    Reported-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1811272247140.1875@nanos.tec.linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 6d201699c651..82b73b75d67c 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -84,6 +84,7 @@ struct thread_info {
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
 #define TIF_SPEC_IB		9	/* Indirect branch speculation mitigation */
+#define TIF_SPEC_FORCE_UPDATE	10	/* Force speculation MSR update in context switch */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
 #define TIF_PATCH_PENDING	13	/* pending live patching update */
@@ -112,6 +113,7 @@ struct thread_info {
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_SPEC_IB		(1 << TIF_SPEC_IB)
+#define _TIF_SPEC_FORCE_UPDATE	(1 << TIF_SPEC_FORCE_UPDATE)
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_PATCH_PENDING	(1 << TIF_PATCH_PENDING)
@@ -149,7 +151,7 @@ struct thread_info {
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW_BASE						\
 	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|		\
-	 _TIF_SSBD)
+	 _TIF_SSBD | _TIF_SPEC_FORCE_UPDATE)
 
 /*
  * Avoid calls to __switch_to_xtra() on UP as STIBP is not evaluated.

commit 5635d99953f04b550738f6f4c1c532667c3fd872
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 25 19:33:48 2018 +0100

    x86/speculation: Avoid __switch_to_xtra() calls
    
    The TIF_SPEC_IB bit does not need to be evaluated in the decision to invoke
    __switch_to_xtra() when:
    
     - CONFIG_SMP is disabled
    
     - The conditional STIPB mode is disabled
    
    The TIF_SPEC_IB bit still controls IBPB in both cases so the TIF work mask
    checks might invoke __switch_to_xtra() for nothing if TIF_SPEC_IB is the
    only set bit in the work masks.
    
    Optimize it out by masking the bit at compile time for CONFIG_SMP=n and at
    run time when the static key controlling the conditional STIBP mode is
    disabled.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185005.374062201@linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index fa583ec99e3e..6d201699c651 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -147,9 +147,18 @@ struct thread_info {
 	 _TIF_FSCHECK)
 
 /* flags to check in __switch_to() */
-#define _TIF_WORK_CTXSW							\
+#define _TIF_WORK_CTXSW_BASE						\
 	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|		\
-	 _TIF_SSBD|_TIF_SPEC_IB)
+	 _TIF_SSBD)
+
+/*
+ * Avoid calls to __switch_to_xtra() on UP as STIBP is not evaluated.
+ */
+#ifdef CONFIG_SMP
+# define _TIF_WORK_CTXSW	(_TIF_WORK_CTXSW_BASE | _TIF_SPEC_IB)
+#else
+# define _TIF_WORK_CTXSW	(_TIF_WORK_CTXSW_BASE)
+#endif
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)

commit 5bfbe3ad5840d941b89bcac54b821ba14f50a0ba
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Sun Nov 25 19:33:46 2018 +0100

    x86/speculation: Prepare for per task indirect branch speculation control
    
    To avoid the overhead of STIBP always on, it's necessary to allow per task
    control of STIBP.
    
    Add a new task flag TIF_SPEC_IB and evaluate it during context switch if
    SMT is active and flag evaluation is enabled by the speculation control
    code. Add the conditional evaluation to x86_virt_spec_ctrl() as well so the
    guest/host switch works properly.
    
    This has no effect because TIF_SPEC_IB cannot be set yet and the static key
    which controls evaluation is off. Preparatory patch for adding the control
    code.
    
    [ tglx: Simplify the context switch logic and make the TIF evaluation
            depend on SMP=y and on the static key controlling the conditional
            update. Rename it to TIF_SPEC_IB because it controls both STIBP and
            IBPB ]
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185005.176917199@linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 523c69efc38a..fa583ec99e3e 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -83,6 +83,7 @@ struct thread_info {
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
+#define TIF_SPEC_IB		9	/* Indirect branch speculation mitigation */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
 #define TIF_PATCH_PENDING	13	/* pending live patching update */
@@ -110,6 +111,7 @@ struct thread_info {
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
+#define _TIF_SPEC_IB		(1 << TIF_SPEC_IB)
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_PATCH_PENDING	(1 << TIF_PATCH_PENDING)
@@ -146,7 +148,8 @@ struct thread_info {
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
-	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|_TIF_SSBD)
+	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|		\
+	 _TIF_SSBD|_TIF_SPEC_IB)
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)

commit 8eb729b77faf83ac4c1f363a9ad68d042415f24c
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Sun Nov 25 19:33:29 2018 +0100

    x86/speculation: Update the TIF_SSBD comment
    
    "Reduced Data Speculation" is an obsolete term. The correct new name is
    "Speculative store bypass disable" - which is abbreviated into SSBD.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185003.593893901@linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 2ff2a30a264f..523c69efc38a 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -79,7 +79,7 @@ struct thread_info {
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
-#define TIF_SSBD			5	/* Reduced data speculation */
+#define TIF_SSBD		5	/* Speculative store bypass disable */
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */

commit 9f65fb29374ee37856dbad847b4e121aab72b510
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 9 21:41:38 2018 +0200

    x86/bugs: Rename _RDS to _SSBD
    
    Intel collateral will reference the SSB mitigation bit in IA32_SPEC_CTL[2]
    as SSBD (Speculative Store Bypass Disable).
    
    Hence changing it.
    
    It is unclear yet what the MSR_IA32_ARCH_CAPABILITIES (0x10a) Bit(4) name
    is going to be. Following the rename it would be SSBD_NO but that rolls out
    to Speculative Store Bypass Disable No.
    
    Also fixed the missing space in X86_FEATURE_AMD_SSBD.
    
    [ tglx: Fixup x86_amd_rds_enable() and rds_tif_to_amd_ls_cfg() as well ]
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e5c26cc59619..2ff2a30a264f 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -79,7 +79,7 @@ struct thread_info {
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
-#define TIF_RDS			5	/* Reduced data speculation */
+#define TIF_SSBD			5	/* Reduced data speculation */
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
@@ -106,7 +106,7 @@ struct thread_info {
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
-#define _TIF_RDS		(1 << TIF_RDS)
+#define _TIF_SSBD		(1 << TIF_SSBD)
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
@@ -146,7 +146,7 @@ struct thread_info {
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
-	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|_TIF_RDS)
+	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|_TIF_SSBD)
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)

commit 885f82bfbc6fefb6664ea27965c3ab9ac4194b8c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 29 15:21:42 2018 +0200

    x86/process: Allow runtime control of Speculative Store Bypass
    
    The Speculative Store Bypass vulnerability can be mitigated with the
    Reduced Data Speculation (RDS) feature. To allow finer grained control of
    this eventually expensive mitigation a per task mitigation control is
    required.
    
    Add a new TIF_RDS flag and put it into the group of TIF flags which are
    evaluated for mismatch in switch_to(). If these bits differ in the previous
    and the next task, then the slow path function __switch_to_xtra() is
    invoked. Implement the TIF_RDS dependent mitigation control in the slow
    path.
    
    If the prctl for controlling Speculative Store Bypass is disabled or no
    task uses the prctl then there is no overhead in the switch_to() fast
    path.
    
    Update the KVM related speculation control functions to take TID_RDS into
    account as well.
    
    Based on a patch from Tim Chen. Completely rewritten.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index a5d9521bb2cb..e5c26cc59619 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -79,6 +79,7 @@ struct thread_info {
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
+#define TIF_RDS			5	/* Reduced data speculation */
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
@@ -105,6 +106,7 @@ struct thread_info {
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
+#define _TIF_RDS		(1 << TIF_RDS)
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
@@ -144,7 +146,7 @@ struct thread_info {
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
-	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP)
+	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|_TIF_RDS)
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)

commit 35277995e17919ab838beae765f440674e8576eb
Merge: 0a646e9c992e b2ac58f90540
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 4 11:45:55 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull spectre/meltdown updates from Thomas Gleixner:
     "The next round of updates related to melted spectrum:
    
       - The initial set of spectre V1 mitigations:
    
           - Array index speculation blocker and its usage for syscall,
             fdtable and the n180211 driver.
    
           - Speculation barrier and its usage in user access functions
    
       - Make indirect calls in KVM speculation safe
    
       - Blacklisting of known to be broken microcodes so IPBP/IBSR are not
         touched.
    
       - The initial IBPB support and its usage in context switch
    
       - The exposure of the new speculation MSRs to KVM guests.
    
       - A fix for a regression in x86/32 related to the cpu entry area
    
       - Proper whitelisting for known to be safe CPUs from the mitigations.
    
       - objtool fixes to deal proper with retpolines and alternatives
    
       - Exclude __init functions from retpolines which speeds up the boot
         process.
    
       - Removal of the syscall64 fast path and related cleanups and
         simplifications
    
       - Removal of the unpatched paravirt mode which is yet another source
         of indirect unproteced calls.
    
       - A new and undisputed version of the module mismatch warning
    
       - A couple of cleanup and correctness fixes all over the place
    
      Yet another step towards full mitigation. There are a few things still
      missing like the RBS underflow mitigation for Skylake and other small
      details, but that's being worked on.
    
      That said, I'm taking a belated christmas vacation for a week and hope
      that everything is magically solved when I'm back on Feb 12th"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      KVM/SVM: Allow direct access to MSR_IA32_SPEC_CTRL
      KVM/VMX: Allow direct access to MSR_IA32_SPEC_CTRL
      KVM/VMX: Emulate MSR_IA32_ARCH_CAPABILITIES
      KVM/x86: Add IBPB support
      KVM/x86: Update the reverse_cpuid list to include CPUID_7_EDX
      x86/speculation: Fix typo IBRS_ATT, which should be IBRS_ALL
      x86/pti: Mark constant arrays as __initconst
      x86/spectre: Simplify spectre_v2 command line parsing
      x86/retpoline: Avoid retpolines for built-in __init functions
      x86/kvm: Update spectre-v1 mitigation
      KVM: VMX: make MSR bitmaps per-VCPU
      x86/paravirt: Remove 'noreplace-paravirt' cmdline option
      x86/speculation: Use Indirect Branch Prediction Barrier in context switch
      x86/cpuid: Fix up "virtual" IBRS/IBPB/STIBP feature bits on Intel
      x86/spectre: Fix spelling mistake: "vunerable"-> "vulnerable"
      x86/spectre: Report get_user mitigation for spectre_v1
      nl80211: Sanitize array index in parse_txq_params
      vfs, fdtable: Prevent bounds-check bypass via speculative execution
      x86/syscall: Sanitize syscall table de-references under speculation
      x86/get_user: Use pointer masking to limit speculation
      ...

commit 37a8f7c38339b22b69876d6f5a0ab851565284e3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Jan 28 10:38:50 2018 -0800

    x86/asm: Move 'status' from thread_struct to thread_info
    
    The TS_COMPAT bit is very hot and is accessed from code paths that mostly
    also touch thread_info::flags.  Move it into struct thread_info to improve
    cache locality.
    
    The only reason it was in thread_struct is that there was a brief period
    during which arch-specific fields were not allowed in struct thread_info.
    
    Linus suggested further changing:
    
      ti->status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
    
    to:
    
      if (unlikely(ti->status & (TS_COMPAT|TS_I386_REGS_POKED)))
              ti->status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
    
    on the theory that frequently dirtying the cacheline even in pure 64-bit
    code that never needs to modify status hurts performance.  That could be a
    reasonable followup patch, but I suspect it matters less on top of this
    patch.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Kernel Hardening <kernel-hardening@lists.openwall.com>
    Link: https://lkml.kernel.org/r/03148bcc1b217100e6e8ecf6a5468c45cf4304b6.1517164461.git.luto@kernel.org

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 00223333821a..eda3b6823ca4 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -55,6 +55,7 @@ struct task_struct;
 
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
+	u32			status;		/* thread synchronous flags */
 };
 
 #define INIT_THREAD_INFO(tsk)			\
@@ -221,7 +222,7 @@ static inline int arch_within_stack_frames(const void * const stack,
 #define in_ia32_syscall() true
 #else
 #define in_ia32_syscall() (IS_ENABLED(CONFIG_IA32_EMULATION) && \
-			   current->thread.status & TS_COMPAT)
+			   current_thread_info()->status & TS_COMPAT)
 #endif
 
 /*

commit 0500871f21b237b2bea2d9db405eadf78e5aab05
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Construct init thread stack in the linker script rather than by union
    
    Construct the init thread stack in the linker script rather than doing it
    by means of a union so that ia64's init_task.c can be got rid of.
    
    The following symbols are then made available from INIT_TASK_DATA() linker
    script macro:
    
            init_thread_union
            init_stack
    
    INIT_TASK_DATA() also expands the region to THREAD_SIZE to accommodate the
    size of the init stack.  init_thread_union is given its own section so that
    it can be placed into the stack space in the right order.  I'm assuming
    that the ia64 ordering is correct and that the task_struct is first and the
    thread_info second.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 00223333821a..d25a638a2720 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -62,8 +62,6 @@ struct thread_info {
 	.flags		= 0,			\
 }
 
-#define init_stack		(init_thread_union.stack)
-
 #else /* !__ASSEMBLY__ */
 
 #include <asm/asm-offsets.h>

commit c482feefe1aeb150156248ba0fd3e029bc886605
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:29 2017 +0100

    x86/entry/64: Make cpu_entry_area.tss read-only
    
    The TSS is a fairly juicy target for exploits, and, now that the TSS
    is in the cpu_entry_area, it's no longer protected by kASLR.  Make it
    read-only on x86_64.
    
    On x86_32, it can't be RO because it's written by the CPU during task
    switches, and we use a task gate for double faults.  I'd also be
    nervous about errata if we tried to make it RO even on configurations
    without double fault handling.
    
    [ tglx: AMD confirmed that there is no problem on 64-bit with TSS RO.  So
            it's probably safe to assume that it's a non issue, though Intel
            might have been creative in that area. Still waiting for
            confirmation. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.733700132@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 44a04999791e..00223333821a 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -207,7 +207,7 @@ static inline int arch_within_stack_frames(const void * const stack,
 #else /* !__ASSEMBLY__ */
 
 #ifdef CONFIG_X86_64
-# define cpu_current_top_of_stack (cpu_tss + TSS_sp1)
+# define cpu_current_top_of_stack (cpu_tss_rw + TSS_sp1)
 #endif
 
 #endif

commit 9aaefe7b59ae00605256a7d6bd1c1456432495fc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:21 2017 +0100

    x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
    
    On 64-bit kernels, we used to assume that TSS.sp0 was the current
    top of stack.  With the addition of an entry trampoline, this will
    no longer be the case.  Store the current top of stack in TSS.sp1,
    which is otherwise unused but shares the same cacheline.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.050864668@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 70f425947dc5..44a04999791e 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -207,7 +207,7 @@ static inline int arch_within_stack_frames(const void * const stack,
 #else /* !__ASSEMBLY__ */
 
 #ifdef CONFIG_X86_64
-# define cpu_current_top_of_stack (cpu_tss + TSS_sp0)
+# define cpu_current_top_of_stack (cpu_tss + TSS_sp1)
 #endif
 
 #endif

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 89e7eeb5cec1..70f425947dc5 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /* thread_info.h: low-level thread information
  *
  * Copyright (C) 2002  David Howells (dhowells@redhat.com)

commit 196bd485ee4f03ce4c690bfcf38138abfcd0a4bc
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Fri Sep 29 17:15:36 2017 +0300

    x86/asm: Use register variable to get stack pointer value
    
    Currently we use current_stack_pointer() function to get the value
    of the stack pointer register. Since commit:
    
      f5caf621ee35 ("x86/asm: Fix inline asm call constraints for Clang")
    
    ... we have a stack register variable declared. It can be used instead of
    current_stack_pointer() function which allows to optimize away some
    excessive "mov %rsp, %<dst>" instructions:
    
     -mov    %rsp,%rdx
     -sub    %rdx,%rax
     -cmp    $0x3fff,%rax
     -ja     ffffffff810722fd <ist_begin_non_atomic+0x2d>
    
     +sub    %rsp,%rax
     +cmp    $0x3fff,%rax
     +ja     ffffffff810722fa <ist_begin_non_atomic+0x2a>
    
    Remove current_stack_pointer(), rename __asm_call_sp to current_stack_pointer
    and use it instead of the removed function.
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170929141537.29167-1-aryabinin@virtuozzo.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 5161da1a0fa0..89e7eeb5cec1 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -158,17 +158,6 @@ struct thread_info {
  */
 #ifndef __ASSEMBLY__
 
-static inline unsigned long current_stack_pointer(void)
-{
-	unsigned long sp;
-#ifdef CONFIG_X86_64
-	asm("mov %%rsp,%0" : "=g" (sp));
-#else
-	asm("mov %%esp,%0" : "=g" (sp));
-#endif
-	return sp;
-}
-
 /*
  * Walks up the stack frames to make sure that the specified object is
  * entirely contained by a single stack frame.

commit 5ea0727b163cb5575e36397a12eade68a1f35f24
Author: Thomas Garnier <thgarnie@google.com>
Date:   Wed Jun 14 18:12:01 2017 -0700

    x86/syscalls: Check address limit on user-mode return
    
    Ensure the address limit is a user-mode segment before returning to
    user-mode. Otherwise a process can corrupt kernel-mode memory and elevate
    privileges [1].
    
    The set_fs function sets the TIF_SETFS flag to force a slow path on
    return. In the slow path, the address limit is checked to be USER_DS if
    needed.
    
    The addr_limit_user_check function is added as a cross-architecture
    function to check the address limit.
    
    [1] https://bugs.chromium.org/p/project-zero/issues/detail?id=990
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: kernel-hardening@lists.openwall.com
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Pratyush Anand <panand@redhat.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Will Drewry <wad@chromium.org>
    Cc: linux-api@vger.kernel.org
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170615011203.144108-1-thgarnie@google.com

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e00e1bd6e7b3..5161da1a0fa0 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -98,6 +98,7 @@ struct thread_info {
 #define TIF_SYSCALL_TRACEPOINT	28	/* syscall tracepoint instrumentation */
 #define TIF_ADDR32		29	/* 32-bit address space on 64 bits */
 #define TIF_X32			30	/* 32-bit native x86-64 binary */
+#define TIF_FSCHECK		31	/* Check FS is USER_DS on return */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -122,6 +123,7 @@ struct thread_info {
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 #define _TIF_ADDR32		(1 << TIF_ADDR32)
 #define _TIF_X32		(1 << TIF_X32)
+#define _TIF_FSCHECK		(1 << TIF_FSCHECK)
 
 /*
  * work to do in syscall_trace_enter().  Also includes TIF_NOHZ for
@@ -137,7 +139,8 @@ struct thread_info {
 	(_TIF_SYSCALL_TRACE | _TIF_NOTIFY_RESUME | _TIF_SIGPENDING |	\
 	 _TIF_NEED_RESCHED | _TIF_SINGLESTEP | _TIF_SYSCALL_EMU |	\
 	 _TIF_SYSCALL_AUDIT | _TIF_USER_RETURN_NOTIFY | _TIF_UPROBE |	\
-	 _TIF_PATCH_PENDING | _TIF_NOHZ | _TIF_SYSCALL_TRACEPOINT)
+	 _TIF_PATCH_PENDING | _TIF_NOHZ | _TIF_SYSCALL_TRACEPOINT |	\
+	 _TIF_FSCHECK)
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\

commit 76f1948a79b26d5f57a5ee9941876b745c6baaea
Merge: 7af4c727c7b6 a0841609f658
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 2 18:24:16 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching
    
    Pull livepatch updates from Jiri Kosina:
    
     - a per-task consistency model is being added for architectures that
       support reliable stack dumping (extending this, currently rather
       trivial set, is currently in the works).
    
       This extends the nature of the types of patches that can be applied
       by live patching infrastructure. The code stems from the design
       proposal made [1] back in November 2014. It's a hybrid of SUSE's
       kGraft and RH's kpatch, combining advantages of both: it uses
       kGraft's per-task consistency and syscall barrier switching combined
       with kpatch's stack trace switching. There are also a number of
       fallback options which make it quite flexible.
    
       Most of the heavy lifting done by Josh Poimboeuf with help from
       Miroslav Benes and Petr Mladek
    
       [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
     - module load time patch optimization from Zhou Chengming
    
     - a few assorted small fixes
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching:
      livepatch: add missing printk newlines
      livepatch: Cancel transition a safe way for immediate patches
      livepatch: Reduce the time of finding module symbols
      livepatch: make klp_mutex proper part of API
      livepatch: allow removal of a disabled patch
      livepatch: add /proc/<pid>/patch_state
      livepatch: change to a per-task consistency model
      livepatch: store function sizes
      livepatch: use kstrtobool() in enabled_store()
      livepatch: move patching functions into patch.c
      livepatch: remove unnecessary object loaded check
      livepatch: separate enabled and patched states
      livepatch/s390: add TIF_PATCH_PENDING thread flag
      livepatch/s390: reorganize TIF thread flag bits
      livepatch/powerpc: add TIF_PATCH_PENDING thread flag
      livepatch/x86: add TIF_PATCH_PENDING thread flag
      livepatch: create temporary klp_update_patch_state() stub
      x86/entry: define _TIF_ALLWORK_MASK flags explicitly
      stacktrace/x86: add function for detecting reliable stack traces

commit 5958cc49ed2961a059d92ae55afeeaba64a783a0
Merge: 2575be8ad32f 517e1fbeb65f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 2 10:45:15 2017 -0700

    Merge tag 'usercopy-v4.12-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull hardened usercopy updates from Kees Cook:
     "A couple hardened usercopy changes:
    
       - drop now unneeded is_vmalloc_or_module() check (Laura Abbott)
    
       - use enum instead of literals for stack frame API (Sahara)"
    
    * tag 'usercopy-v4.12-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      mm/usercopy: Drop extra is_vmalloc_or_module() check
      usercopy: Move enum for arch_within_stack_frames()

commit 96dc4f9fb64690fc34410415fd1fc609cf803f61
Author: Sahara <keun-o.park@darkmatter.ae>
Date:   Thu Feb 16 18:29:15 2017 +0000

    usercopy: Move enum for arch_within_stack_frames()
    
    This patch moves the arch_within_stack_frames() return value enum up in
    the header files so that per-architecture implementations can reuse the
    same return values.
    
    Signed-off-by: Sahara <keun-o.park@darkmatter.ae>
    Signed-off-by: James Morse <james.morse@arm.com>
    [kees: adjusted naming and commit log]
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ad6f5eb07a95..920ca1f7adea 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -168,9 +168,9 @@ static inline unsigned long current_stack_pointer(void)
  * entirely contained by a single stack frame.
  *
  * Returns:
- *		 1 if within a frame
- *		-1 if placed across a frame boundary (or outside stack)
- *		 0 unable to determine (no frame pointers, etc)
+ *	GOOD_FRAME	if within a frame
+ *	BAD_STACK	if placed across a frame boundary (or outside stack)
+ *	NOT_STACK	unable to determine (no frame pointers, etc)
  */
 static inline int arch_within_stack_frames(const void * const stack,
 					   const void * const stackend,
@@ -197,13 +197,14 @@ static inline int arch_within_stack_frames(const void * const stack,
 		 * the copy as invalid.
 		 */
 		if (obj + len <= frame)
-			return obj >= oldframe + 2 * sizeof(void *) ? 1 : -1;
+			return obj >= oldframe + 2 * sizeof(void *) ?
+				GOOD_FRAME : BAD_STACK;
 		oldframe = frame;
 		frame = *(const void * const *)frame;
 	}
-	return -1;
+	return BAD_STACK;
 #else
-	return 0;
+	return NOT_STACK;
 #endif
 }
 

commit e9ea1e7f53b852147cbd568b0568c7ad97ec21a3
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:26 2017 -0700

    x86/arch_prctl: Add ARCH_[GET|SET]_CPUID
    
    Intel supports faulting on the CPUID instruction beginning with Ivy Bridge.
    When enabled, the processor will fault on attempts to execute the CPUID
    instruction with CPL>0. Exposing this feature to userspace will allow a
    ptracer to trap and emulate the CPUID instruction.
    
    When supported, this feature is controlled by toggling bit 0 of
    MSR_MISC_FEATURES_ENABLES. It is documented in detail in Section 2.3.2 of
    https://bugzilla.kernel.org/attachment.cgi?id=243991
    
    Implement a new pair of arch_prctls, available on both x86-32 and x86-64.
    
    ARCH_GET_CPUID: Returns the current CPUID state, either 0 if CPUID faulting
        is enabled (and thus the CPUID instruction is not available) or 1 if
        CPUID faulting is not enabled.
    
    ARCH_SET_CPUID: Set the CPUID state to the second argument. If
        cpuid_enabled is 0 CPUID faulting will be activated, otherwise it will
        be deactivated. Returns ENODEV if CPUID faulting is not supported on
        this system.
    
    The state of the CPUID faulting flag is propagated across forks, but reset
    upon exec.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-9-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ad6f5eb07a95..9fc44b95f7cb 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -87,6 +87,7 @@ struct thread_info {
 #define TIF_SECCOMP		8	/* secure computing */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
+#define TIF_NOCPUID		15	/* CPUID is not accessible in userland */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
@@ -110,6 +111,7 @@ struct thread_info {
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
+#define _TIF_NOCPUID		(1 << TIF_NOCPUID)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
@@ -138,7 +140,7 @@ struct thread_info {
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
-	(_TIF_IO_BITMAP|_TIF_NOTSC|_TIF_BLOCKSTEP)
+	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP)
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
@@ -239,6 +241,8 @@ static inline int arch_within_stack_frames(const void * const stack,
 extern void arch_task_cache_init(void);
 extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
 extern void arch_release_task_struct(struct task_struct *tsk);
+extern void arch_setup_new_exec(void);
+#define arch_setup_new_exec arch_setup_new_exec
 #endif	/* !__ASSEMBLY__ */
 
 #endif /* _ASM_X86_THREAD_INFO_H */

commit afb94c9e0b413bbdea779192eaca076c43ede031
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:31 2017 -0600

    livepatch/x86: add TIF_PATCH_PENDING thread flag
    
    Add the TIF_PATCH_PENDING thread flag to enable the new livepatch
    per-task consistency model for x86_64.  The bit getting set indicates
    the thread has a pending patch which needs to be applied when the thread
    exits the kernel.
    
    The bit is placed in the _TIF_ALLWORK_MASK macro, which results in
    exit_to_usermode_loop() calling klp_update_patch_state() when it's set.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Reviewed-by: Miroslav Benes <mbenes@suse.cz>
    Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the x86 changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 207d0d981287..83372dc43943 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -84,6 +84,7 @@ struct thread_info {
 #define TIF_SECCOMP		8	/* secure computing */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
+#define TIF_PATCH_PENDING	13	/* pending live patching update */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
@@ -107,6 +108,7 @@ struct thread_info {
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
+#define _TIF_PATCH_PENDING	(1 << TIF_PATCH_PENDING)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
@@ -133,7 +135,7 @@ struct thread_info {
 	(_TIF_SYSCALL_TRACE | _TIF_NOTIFY_RESUME | _TIF_SIGPENDING |	\
 	 _TIF_NEED_RESCHED | _TIF_SINGLESTEP | _TIF_SYSCALL_EMU |	\
 	 _TIF_SYSCALL_AUDIT | _TIF_USER_RETURN_NOTIFY | _TIF_UPROBE |	\
-	 _TIF_NOHZ | _TIF_SYSCALL_TRACEPOINT)
+	 _TIF_PATCH_PENDING | _TIF_NOHZ | _TIF_SYSCALL_TRACEPOINT)
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\

commit 3a404842547c92e71127870a613319a29cdebe49
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:29 2017 -0600

    x86/entry: define _TIF_ALLWORK_MASK flags explicitly
    
    The _TIF_ALLWORK_MASK macro automatically includes the least-significant
    16 bits of the thread_info flags, which is less than obvious and tends
    to create confusion and surprises when reading or modifying the code.
    
    Define the flags explicitly.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Reviewed-by: Miroslav Benes <mbenes@suse.cz>
    Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the x86 changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ad6f5eb07a95..207d0d981287 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -73,9 +73,6 @@ struct thread_info {
  * thread information flags
  * - these are process state flags that various assembly files
  *   may need to access
- * - pending work-to-be-done flags are in LSW
- * - other flags in MSW
- * Warning: layout of LSW is hardcoded in entry.S
  */
 #define TIF_SYSCALL_TRACE	0	/* syscall trace active */
 #define TIF_NOTIFY_RESUME	1	/* callback before returning to user */
@@ -103,8 +100,8 @@ struct thread_info {
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
-#define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
+#define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
@@ -133,8 +130,10 @@ struct thread_info {
 
 /* work to do on any return to user space */
 #define _TIF_ALLWORK_MASK						\
-	((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_TRACEPOINT |	\
-	_TIF_NOHZ)
+	(_TIF_SYSCALL_TRACE | _TIF_NOTIFY_RESUME | _TIF_SIGPENDING |	\
+	 _TIF_NEED_RESCHED | _TIF_SINGLESTEP | _TIF_SYSCALL_EMU |	\
+	 _TIF_SYSCALL_AUDIT | _TIF_USER_RETURN_NOTIFY | _TIF_UPROBE |	\
+	 _TIF_NOHZ | _TIF_SYSCALL_TRACEPOINT)
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\

commit c8061485a0d7569a865a3cc3c63347b0f42b3765
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Oct 19 19:28:11 2016 +0100

    sched/core, x86: Make struct thread_info arch specific again
    
    The following commit:
    
      c65eacbe290b ("sched/core: Allow putting thread_info into task_struct")
    
    ... made 'struct thread_info' a generic struct with only a
    single ::flags member, if CONFIG_THREAD_INFO_IN_TASK_STRUCT=y is
    selected.
    
    This change however seems to be quite x86 centric, since at least the
    generic preemption code (asm-generic/preempt.h) assumes that struct
    thread_info also has a preempt_count member, which apparently was not
    true for x86.
    
    We could add a bit more #ifdefs to solve this problem too, but it seems
    to be much simpler to make struct thread_info arch specific
    again. This also makes the conversion to THREAD_INFO_IN_TASK_STRUCT a
    bit easier for architectures that have a couple of arch specific stuff
    in their thread_info definition.
    
    The arch specific stuff _could_ be moved to thread_struct. However
    keeping them in thread_info makes it easier: accessing thread_info
    members is simple, since it is at the beginning of the task_struct,
    while the thread_struct is at the end. At least on s390 the offsets
    needed to access members of the thread_struct (with task_struct as
    base) are too large for various asm instructions.  This is not a
    problem when keeping these members within thread_info.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: keescook@chromium.org
    Cc: linux-arch@vger.kernel.org
    Link: http://lkml.kernel.org/r/1476901693-8492-2-git-send-email-mark.rutland@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 2aaca53c0974..ad6f5eb07a95 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -52,6 +52,15 @@ struct task_struct;
 #include <asm/cpufeature.h>
 #include <linux/atomic.h>
 
+struct thread_info {
+	unsigned long		flags;		/* low level flags */
+};
+
+#define INIT_THREAD_INFO(tsk)			\
+{						\
+	.flags		= 0,			\
+}
+
 #define init_stack		(init_thread_union.stack)
 
 #else /* !__ASSEMBLY__ */

commit 15f4eae70d365bba26854c90b6002aaabb18c8aa
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:25 2016 -0700

    x86: Move thread_info into task_struct
    
    Now that most of the thread_info users have been cleaned up,
    this is straightforward.
    
    Most of this code was written by Linus.
    
    Originally-from: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a50eab40abeaec9cb9a9e3cbdeafd32190206654.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index c9dcfe7c7e4b..2aaca53c0974 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -52,20 +52,6 @@ struct task_struct;
 #include <asm/cpufeature.h>
 #include <linux/atomic.h>
 
-struct thread_info {
-	struct task_struct	*task;		/* main task structure */
-	__u32			flags;		/* low level flags */
-	__u32			cpu;		/* current CPU */
-};
-
-#define INIT_THREAD_INFO(tsk)			\
-{						\
-	.task		= &tsk,			\
-	.flags		= 0,			\
-	.cpu		= 0,			\
-}
-
-#define init_thread_info	(init_thread_union.thread_info)
 #define init_stack		(init_thread_union.stack)
 
 #else /* !__ASSEMBLY__ */
@@ -157,11 +143,6 @@ struct thread_info {
  */
 #ifndef __ASSEMBLY__
 
-static inline struct thread_info *current_thread_info(void)
-{
-	return (struct thread_info *)(current_top_of_stack() - THREAD_SIZE);
-}
-
 static inline unsigned long current_stack_pointer(void)
 {
 	unsigned long sp;
@@ -223,33 +204,6 @@ static inline int arch_within_stack_frames(const void * const stack,
 # define cpu_current_top_of_stack (cpu_tss + TSS_sp0)
 #endif
 
-/*
- * ASM operand which evaluates to a 'thread_info' address of
- * the current task, if it is known that "reg" is exactly "off"
- * bytes below the top of the stack currently.
- *
- * ( The kernel stack's size is known at build time, it is usually
- *   2 or 4 pages, and the bottom  of the kernel stack contains
- *   the thread_info structure. So to access the thread_info very
- *   quickly from assembly code we can calculate down from the
- *   top of the kernel stack to the bottom, using constant,
- *   build-time calculations only. )
- *
- * For example, to fetch the current thread_info->flags value into %eax
- * on x86-64 defconfig kernels, in syscall entry code where RSP is
- * currently at exactly SIZEOF_PTREGS bytes away from the top of the
- * stack:
- *
- *      mov ASM_THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS), %eax
- *
- * will translate to:
- *
- *      8b 84 24 b8 c0 ff ff      mov    -0x3f48(%rsp), %eax
- *
- * which is below the current RSP by almost 16K.
- */
-#define ASM_THREAD_INFO(field, reg, off) ((field)+(off)-THREAD_SIZE)(reg)
-
 #endif
 
 #ifdef CONFIG_COMPAT

commit b9d989c7218ac922185d82ad46f3e58b27a4bea9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:21 2016 -0700

    x86/asm: Move the thread_info::status field to thread_struct
    
    Because sched.h and thread_info.h are a tangled mess, I turned
    in_compat_syscall() into a macro.  If we had current_thread_struct()
    or similar and we could use it from thread_info.h, then this would
    be a bit cleaner.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/ccc8a1b2f41f9c264a41f771bb4a6539a642ad72.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 494c4b5ada34..c9dcfe7c7e4b 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -55,7 +55,6 @@ struct task_struct;
 struct thread_info {
 	struct task_struct	*task;		/* main task structure */
 	__u32			flags;		/* low level flags */
-	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
 };
 
@@ -253,31 +252,17 @@ static inline int arch_within_stack_frames(const void * const stack,
 
 #endif
 
-/*
- * Thread-synchronous status.
- *
- * This is different from the flags in that nobody else
- * ever touches our thread-synchronous status, so we don't
- * have to worry about atomic accesses.
- */
-#define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
 #ifdef CONFIG_COMPAT
 #define TS_I386_REGS_POKED	0x0004	/* regs poked by 32-bit ptracer */
 #endif
-
 #ifndef __ASSEMBLY__
 
-static inline bool in_ia32_syscall(void)
-{
 #ifdef CONFIG_X86_32
-	return true;
-#endif
-#ifdef CONFIG_IA32_EMULATION
-	if (current_thread_info()->status & TS_COMPAT)
-		return true;
+#define in_ia32_syscall() true
+#else
+#define in_ia32_syscall() (IS_ENABLED(CONFIG_IA32_EMULATION) && \
+			   current->thread.status & TS_COMPAT)
 #endif
-	return false;
-}
 
 /*
  * Force syscall return via IRET by making it look as if there was

commit 0100301bfdf56a2a370c7157b5ab0fbf9313e1cd
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Aug 13 12:38:19 2016 -0400

    sched/x86: Rewrite the switch_to() code
    
    Move the low-level context switch code to an out-of-line asm stub instead of
    using complex inline asm.  This allows constructing a new stack frame for the
    child process to make it seamlessly flow to ret_from_fork without an extra
    test and branch in __switch_to().  It also improves code generation for
    __schedule() by using the C calling convention instead of clobbering all
    registers.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1471106302-10159-5-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 8b7c8d8e0852..494c4b5ada34 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -95,7 +95,6 @@ struct thread_info {
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
-#define TIF_FORK		18	/* ret_from_fork */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
@@ -119,7 +118,6 @@ struct thread_info {
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
-#define _TIF_FORK		(1 << TIF_FORK)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)

commit 1eccfa090eaea22558570054bbdc147817e1df5e
Merge: 1bd4403d86a1 ed18adc1cdd0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 8 14:48:14 2016 -0700

    Merge tag 'usercopy-v4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull usercopy protection from Kees Cook:
     "Tbhis implements HARDENED_USERCOPY verification of copy_to_user and
      copy_from_user bounds checking for most architectures on SLAB and
      SLUB"
    
    * tag 'usercopy-v4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      mm: SLUB hardened usercopy support
      mm: SLAB hardened usercopy support
      s390/uaccess: Enable hardened usercopy
      sparc/uaccess: Enable hardened usercopy
      powerpc/uaccess: Enable hardened usercopy
      ia64/uaccess: Enable hardened usercopy
      arm64/uaccess: Enable hardened usercopy
      ARM: uaccess: Enable hardened usercopy
      x86/uaccess: Enable hardened usercopy
      mm: Hardened usercopy
      mm: Implement stack frame object validation
      mm: Add is_migrate_cma_page

commit c98f5827f8f0e0cb075075def7f0d210992ef06d
Merge: 1630e843e104 f7d665627e10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Aug 6 09:04:35 2016 -0400

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Two fixes and a cleanup-fix, to the syscall entry code and to ptrace"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/syscalls/64: Add compat_sys_keyctl for 32-bit userspace
      x86/ptrace: Stop setting TS_COMPAT in ptrace code
      x86/vdso: Error out if the vDSO isn't a valid DSO

commit 7e7814180b334dff97ef8f56c7c40c277ad4531c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Aug 2 14:05:36 2016 -0700

    signal: consolidate {TS,TLF}_RESTORE_SIGMASK code
    
    In general, there's no need for the "restore sigmask" flag to live in
    ti->flags.  alpha, ia64, microblaze, powerpc, sh, sparc (64-bit only),
    tile, and x86 use essentially identical alternative implementations,
    placing the flag in ti->status.
    
    Replace those optimized implementations with an equally good common
    implementation that stores it in a bitfield in struct task_struct and
    drop the custom implementations.
    
    Additional architectures can opt in by removing their
    TIF_RESTORE_SIGMASK defines.
    
    Link: http://lkml.kernel.org/r/8a14321d64a28e40adfddc90e18a96c086a6d6f9.1468522723.git.luto@kernel.org
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>        [powerpc]
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 89bff044a6f5..b45ffdda3549 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -219,32 +219,8 @@ static inline unsigned long current_stack_pointer(void)
  * have to worry about atomic accesses.
  */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
-#define TS_RESTORE_SIGMASK	0x0008	/* restore signal mask in do_signal() */
 
 #ifndef __ASSEMBLY__
-#define HAVE_SET_RESTORE_SIGMASK	1
-static inline void set_restore_sigmask(void)
-{
-	struct thread_info *ti = current_thread_info();
-	ti->status |= TS_RESTORE_SIGMASK;
-	WARN_ON(!test_bit(TIF_SIGPENDING, (unsigned long *)&ti->flags));
-}
-static inline void clear_restore_sigmask(void)
-{
-	current_thread_info()->status &= ~TS_RESTORE_SIGMASK;
-}
-static inline bool test_restore_sigmask(void)
-{
-	return current_thread_info()->status & TS_RESTORE_SIGMASK;
-}
-static inline bool test_and_clear_restore_sigmask(void)
-{
-	struct thread_info *ti = current_thread_info();
-	if (!(ti->status & TS_RESTORE_SIGMASK))
-		return false;
-	ti->status &= ~TS_RESTORE_SIGMASK;
-	return true;
-}
 
 static inline bool in_ia32_syscall(void)
 {

commit 609c19a385c8744c41f944e2a2d8afe8e8fb860e
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Jul 26 23:12:22 2016 -0700

    x86/ptrace: Stop setting TS_COMPAT in ptrace code
    
    Setting TS_COMPAT in ptrace is wrong: if we happen to do it during
    syscall entry, then we'll confuse seccomp and audit.  (The former
    isn't a security problem: seccomp is currently entirely insecure if a
    malicious ptracer is attached.)  As a minimal fix, this patch adds a
    new flag TS_I386_REGS_POKED that handles the ptrace special case.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pedro Alves <palves@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/5383ebed38b39fa37462139e337aff7f2314d1ca.1469599803.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 89bff044a6f5..f856c590c309 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -219,6 +219,9 @@ static inline unsigned long current_stack_pointer(void)
  * have to worry about atomic accesses.
  */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
+#ifdef CONFIG_COMPAT
+#define TS_I386_REGS_POKED	0x0004	/* regs poked by 32-bit ptracer */
+#endif
 #define TS_RESTORE_SIGMASK	0x0008	/* restore signal mask in do_signal() */
 
 #ifndef __ASSEMBLY__

commit 0f60a8efe4005ab5e65ce000724b04d4ca04a199
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jul 12 16:19:48 2016 -0700

    mm: Implement stack frame object validation
    
    This creates per-architecture function arch_within_stack_frames() that
    should validate if a given object is contained by a kernel stack frame.
    Initial implementation is on x86.
    
    This is based on code from PaX.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 30c133ac05cd..ab386f1336f2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -180,6 +180,50 @@ static inline unsigned long current_stack_pointer(void)
 	return sp;
 }
 
+/*
+ * Walks up the stack frames to make sure that the specified object is
+ * entirely contained by a single stack frame.
+ *
+ * Returns:
+ *		 1 if within a frame
+ *		-1 if placed across a frame boundary (or outside stack)
+ *		 0 unable to determine (no frame pointers, etc)
+ */
+static inline int arch_within_stack_frames(const void * const stack,
+					   const void * const stackend,
+					   const void *obj, unsigned long len)
+{
+#if defined(CONFIG_FRAME_POINTER)
+	const void *frame = NULL;
+	const void *oldframe;
+
+	oldframe = __builtin_frame_address(1);
+	if (oldframe)
+		frame = __builtin_frame_address(2);
+	/*
+	 * low ----------------------------------------------> high
+	 * [saved bp][saved ip][args][local vars][saved bp][saved ip]
+	 *                     ^----------------^
+	 *               allow copies only within here
+	 */
+	while (stack <= frame && frame < stackend) {
+		/*
+		 * If obj + len extends past the last frame, this
+		 * check won't pass and the next frame will be 0,
+		 * causing us to bail out and correctly report
+		 * the copy as invalid.
+		 */
+		if (obj + len <= frame)
+			return obj >= oldframe + 2 * sizeof(void *) ? 1 : -1;
+		oldframe = frame;
+		frame = *(const void * const *)frame;
+	}
+	return -1;
+#else
+	return 0;
+#endif
+}
+
 #else /* !__ASSEMBLY__ */
 
 #ifdef CONFIG_X86_64

commit 13d4ea097d18b419ad2a2b696063d44bf59acec0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 14 13:22:57 2016 -0700

    x86/uaccess: Move thread_info::addr_limit to thread_struct
    
    struct thread_info is a legacy mess.  To prepare for its partial removal,
    move thread_info::addr_limit out.
    
    As an added benefit, this way is simpler.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/15bee834d09402b47ac86f2feccdf6529f9bc5b0.1468527351.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 7c47bb659ecd..89bff044a6f5 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -57,7 +57,6 @@ struct thread_info {
 	__u32			flags;		/* low level flags */
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
-	mm_segment_t		addr_limit;
 };
 
 #define INIT_THREAD_INFO(tsk)			\
@@ -65,7 +64,6 @@ struct thread_info {
 	.task		= &tsk,			\
 	.flags		= 0,			\
 	.cpu		= 0,			\
-	.addr_limit	= KERNEL_DS,		\
 }
 
 #define init_thread_info	(init_thread_union.thread_info)
@@ -184,11 +182,6 @@ static inline unsigned long current_stack_pointer(void)
 # define cpu_current_top_of_stack (cpu_tss + TSS_sp0)
 #endif
 
-/* Load thread_info address into "reg" */
-#define GET_THREAD_INFO(reg) \
-	_ASM_MOV PER_CPU_VAR(cpu_current_top_of_stack),reg ; \
-	_ASM_SUB $(THREAD_SIZE),reg ;
-
 /*
  * ASM operand which evaluates to a 'thread_info' address of
  * the current task, if it is known that "reg" is exactly "off"

commit dfa9a942fd7951c8f333cf3f377dde51ebd21685
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 14 13:22:56 2016 -0700

    x86/uaccess: Move thread_info::uaccess_err and thread_info::sig_on_uaccess_err to thread_struct
    
    struct thread_info is a legacy mess.  To prepare for its partial removal,
    move the uaccess control fields out -- they're straightforward.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/d0ac4d01c8e4d4d756264604e47445d5acc7900e.1468527351.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 30c133ac05cd..7c47bb659ecd 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -58,8 +58,6 @@ struct thread_info {
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
 	mm_segment_t		addr_limit;
-	unsigned int		sig_on_uaccess_error:1;
-	unsigned int		uaccess_err:1;	/* uaccess failed */
 };
 
 #define INIT_THREAD_INFO(tsk)			\

commit abfb9498ee1327f534df92a7ecaea81a85913bae
Author: Dmitry Safonov <dsafonov@virtuozzo.com>
Date:   Mon Apr 18 16:43:43 2016 +0300

    x86/entry: Rename is_{ia32,x32}_task() to in_{ia32,x32}_syscall()
    
    The is_ia32_task()/is_x32_task() function names are a big misnomer: they
    suggests that the compat-ness of a system call is a task property, which
    is not true, the compatness of a system call purely depends on how it
    was invoked through the system call layer.
    
    A task may call 32-bit and 64-bit and x32 system calls without changing
    any of its kernel visible state.
    
    This specific minomer is also actively dangerous, as it might cause kernel
    developers to use the wrong kind of security checks within system calls.
    
    So rename it to in_{ia32,x32}_syscall().
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Dmitry Safonov <dsafonov@virtuozzo.com>
    [ Expanded the changelog. ]
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: 0x7f454c46@gmail.com
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/1460987025-30360-1-git-send-email-dsafonov@virtuozzo.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ffae84df8a93..30c133ac05cd 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -255,7 +255,7 @@ static inline bool test_and_clear_restore_sigmask(void)
 	return true;
 }
 
-static inline bool is_ia32_task(void)
+static inline bool in_ia32_syscall(void)
 {
 #ifdef CONFIG_X86_32
 	return true;

commit 5f870a3f7188065e13efafe1faeb01c136173bc4
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Mar 28 20:20:17 2016 +0200

    x86/thread_info: Merge two !__ASSEMBLY__ sections
    
    We have
    
      #ifndef __ASSEMBLY__
      ...
      #endif
    
      #ifndef __ASSEMBLY__
      ...
      #endif
    
    Merge the two.
    
    No functionality change.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1459189217-25532-1-git-send-email-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 82866697fcf1..ffae84df8a93 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -276,11 +276,9 @@ static inline bool is_ia32_task(void)
  */
 #define force_iret() set_thread_flag(TIF_NOTIFY_RESUME)
 
-#endif	/* !__ASSEMBLY__ */
-
-#ifndef __ASSEMBLY__
 extern void arch_task_cache_init(void);
 extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
 extern void arch_release_task_struct(struct task_struct *tsk);
-#endif
+#endif	/* !__ASSEMBLY__ */
+
 #endif /* _ASM_X86_THREAD_INFO_H */

commit 9999c8c01f34c918a57d6e5ba2f5d8b79aa04801
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 13:24:33 2016 -0800

    x86/entry: Call enter_from_user_mode() with IRQs off
    
    Now that slow-path syscalls always enter C before enabling
    interrupts, it's straightforward to call enter_from_user_mode() before
    enabling interrupts rather than doing it as part of entry tracing.
    
    With this change, we should finally be able to retire exception_enter().
    
    This will also enable optimizations based on knowing that we never
    change context tracking state with interrupts on.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/bc376ecf87921a495e874ff98139b1ca2f5c5dd7.1457558566.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index f2e2302c406f..82866697fcf1 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -134,7 +134,10 @@ struct thread_info {
 #define _TIF_ADDR32		(1 << TIF_ADDR32)
 #define _TIF_X32		(1 << TIF_X32)
 
-/* work to do in syscall_trace_enter() */
+/*
+ * work to do in syscall_trace_enter().  Also includes TIF_NOHZ for
+ * enter_from_user_mode()
+ */
 #define _TIF_WORK_SYSCALL_ENTRY	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_AUDIT |	\
 	 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT |	\

commit 392a62549fbb80da48811d04391615a25c39d091
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 9 19:00:34 2016 -0800

    x86/entry: Remove TIF_SINGLESTEP entry work
    
    Now that SYSENTER with TF set puts X86_EFLAGS_TF directly into
    regs->flags, we don't need a TIF_SINGLESTEP fixup in the syscall
    entry code.  Remove it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2d15f24da52dafc9d2f0b8d76f55544f4779c517.1457578375.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index c0778fcab06d..f2e2302c406f 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -137,7 +137,7 @@ struct thread_info {
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_AUDIT |	\
-	 _TIF_SECCOMP | _TIF_SINGLESTEP | _TIF_SYSCALL_TRACEPOINT |	\
+	 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT |	\
 	 _TIF_NOHZ)
 
 /* work to do on any return to user space */

commit cd4d09ec6f6c12a2cc3db5b7d8876a325a53545b
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 26 22:12:04 2016 +0100

    x86/cpufeature: Carve out X86_FEATURE_*
    
    Move them to a separate header and have the following
    dependency:
    
      x86/cpufeatures.h <- x86/processor.h <- x86/cpufeature.h
    
    This makes it easier to use the header in asm code and not
    include the whole cpufeature.h and add guards for asm.
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453842730-28463-5-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index c7b551028740..c0778fcab06d 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -49,7 +49,7 @@
  */
 #ifndef __ASSEMBLY__
 struct task_struct;
-#include <asm/processor.h>
+#include <asm/cpufeature.h>
 #include <linux/atomic.h>
 
 struct thread_info {

commit a75a3f6fc92888e4119744d8594ffdf748c3d444
Merge: d2bea739f8b4 3bd29515d1ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 18:59:10 2015 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm changes from Ingo Molnar:
     "The main change in this cycle is another step in the big x86 system
      call interface rework by Andy Lutomirski, which moves most of the low
      level x86 entry code from assembly to C, for all syscall entries
      except native 64-bit system calls:
    
        arch/x86/entry/entry_32.S        | 182 ++++------
        arch/x86/entry/entry_64_compat.S | 547 ++++++++-----------------------
        194 insertions(+), 535 deletions(-)
    
      ... our hope is that the final remaining step (converting native
      64-bit system calls) will be less painful as all the previous steps,
      given that most of the legacies and quirks are concentrated around
      native 32-bit and compat environments"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (47 commits)
      x86/entry/32: Fix FS and GS restore in opportunistic SYSEXIT
      x86/entry/32: Fix entry_INT80_32() to expect interrupts to be on
      um/x86: Fix build after x86 syscall changes
      x86/asm: Remove the xyz_cfi macros from dwarf2.h
      selftests/x86: Style fixes for the 'unwind_vdso' test
      x86/entry/64/compat: Document sysenter_fix_flags's reason for existence
      x86/entry: Split and inline syscall_return_slowpath()
      x86/entry: Split and inline prepare_exit_to_usermode()
      x86/entry: Use pt_regs_to_thread_info() in syscall entry tracing
      x86/entry: Hide two syscall entry assertions behind CONFIG_DEBUG_ENTRY
      x86/entry: Micro-optimize compat fast syscall arg fetch
      x86/entry: Force inlining of 32-bit syscall code
      x86/entry: Make irqs_disabled checks in exit code depend on lockdep
      x86/entry: Remove unnecessary IRQ twiddling in fast 32-bit syscalls
      x86/asm: Remove thread_info.sysenter_return
      x86/entry/32: Re-implement SYSENTER using the new C path
      x86/entry/32: Switch INT80 to the new C syscall path
      x86/entry/32: Open-code return tracking from fork and kthreads
      x86/entry/compat: Implement opportunistic SYSRETL for compat syscalls
      x86/vdso/compat: Wire up SYSENTER and SYSCSALL for compat userspace
      ...

commit 487e3bf4f77699160aa81a414200060a78a67c3d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Oct 5 17:48:16 2015 -0700

    x86/asm: Remove thread_info.sysenter_return
    
    It's no longer needed.
    
    We could reinstate something like it as an optimization, which
    would remove two cachelines from the fast syscall entry working
    set.  I benchmarked it, and it makes no difference whatsoever to
    the performance of cache-hot compat syscalls on Sandy Bridge.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/f08cc0cff30201afe9bb565c47134c0a6c1a96a2.1444091585.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 8afdc3e44247..a1ecd214d227 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -59,7 +59,6 @@ struct thread_info {
 	__u32			cpu;		/* current CPU */
 	int			saved_preempt_count;
 	mm_segment_t		addr_limit;
-	void __user		*sysenter_return;
 	unsigned int		sig_on_uaccess_error:1;
 	unsigned int		uaccess_err:1;	/* uaccess failed */
 };

commit d87b7a33794f52226131f93cbc9db03274d9fecf
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 28 18:11:18 2015 +0200

    sched/core, sched/x86: Kill thread_info::saved_preempt_count
    
    With the introduction of the context switch preempt_count invariant,
    and the demise of PREEMPT_ACTIVE, its pointless to save/restore the
    per-cpu preemption count, it must always be 2.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 8afdc3e44247..809877e9030b 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -57,7 +57,6 @@ struct thread_info {
 	__u32			flags;		/* low level flags */
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
-	int			saved_preempt_count;
 	mm_segment_t		addr_limit;
 	void __user		*sysenter_return;
 	unsigned int		sig_on_uaccess_error:1;
@@ -69,7 +68,6 @@ struct thread_info {
 	.task		= &tsk,			\
 	.flags		= 0,			\
 	.cpu		= 0,			\
-	.saved_preempt_count = INIT_PREEMPT_COUNT,	\
 	.addr_limit	= KERNEL_DS,		\
 }
 

commit 88cd622f9299c4c9e61e978bb9ef9d7599769ed0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jul 31 14:41:10 2015 -0700

    x86/entry: Remove do_notify_resume(), syscall_trace_leave(), and their TIF masks
    
    They are no longer used. Good riddance!
    
    Deleting the TIF_ macros is really nice. It was never clear why
    there were so many variants.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric Paris <eparis@parisplace.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/22c61682f446628573dde0f1d573ab821677e06da.1438378274.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index fdad5c244350..8afdc3e44247 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -143,27 +143,11 @@ struct thread_info {
 	 _TIF_SECCOMP | _TIF_SINGLESTEP | _TIF_SYSCALL_TRACEPOINT |	\
 	 _TIF_NOHZ)
 
-/* work to do in syscall_trace_leave() */
-#define _TIF_WORK_SYSCALL_EXIT	\
-	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP |	\
-	 _TIF_SYSCALL_TRACEPOINT | _TIF_NOHZ)
-
-/* work to do on interrupt/exception return */
-#define _TIF_WORK_MASK							\
-	(0x0000FFFF &							\
-	 ~(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|			\
-	   _TIF_SINGLESTEP|_TIF_SECCOMP|_TIF_SYSCALL_EMU))
-
 /* work to do on any return to user space */
 #define _TIF_ALLWORK_MASK						\
 	((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_TRACEPOINT |	\
 	_TIF_NOHZ)
 
-/* Only used for 64 bit */
-#define _TIF_DO_NOTIFY_MASK						\
-	(_TIF_SIGPENDING | _TIF_NOTIFY_RESUME |				\
-	 _TIF_USER_RETURN_NOTIFY | _TIF_UPROBE)
-
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
 	(_TIF_IO_BITMAP|_TIF_NOTSC|_TIF_BLOCKSTEP)

commit 5ed92a8ab71f8865ba07811429c988c72299b315
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jul 29 01:41:19 2015 -0400

    x86/vm86: Use the normal pt_regs area for vm86
    
    Change to use the normal pt_regs area to enter and exit vm86
    mode.  This is done by increasing the padding at the top of the
    stack to make room for the extra vm86 segment slots in the IRET
    frame.  It then saves the 32-bit regs in the off-stack vm86
    data, and copies in the vm86 regs.  Exiting back to 32-bit mode
    does the reverse.  This allows removing the hacks to jump
    directly into the exit asm code due to having to change the
    stack pointer.  Returning normally from the vm86 syscall and the
    exception handlers allows things like ptrace and auditing to work properly.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1438148483-11932-5-git-send-email-brgerst@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 225ee545e1a0..fdad5c244350 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -27,14 +27,17 @@
  * Without this offset, that can result in a page fault.  (We are
  * careful that, in this case, the value we read doesn't matter.)
  *
- * In vm86 mode, the hardware frame is much longer still, but we neither
- * access the extra members from NMI context, nor do we write such a
- * frame at sp0 at all.
+ * In vm86 mode, the hardware frame is much longer still, so add 16
+ * bytes to make room for the real-mode segments.
  *
  * x86_64 has a fixed-length stack frame.
  */
 #ifdef CONFIG_X86_32
-# define TOP_OF_KERNEL_STACK_PADDING 8
+# ifdef CONFIG_VM86
+#  define TOP_OF_KERNEL_STACK_PADDING 16
+# else
+#  define TOP_OF_KERNEL_STACK_PADDING 8
+# endif
 #else
 # define TOP_OF_KERNEL_STACK_PADDING 0
 #endif

commit 3a23208e69679597e767cf3547b1a30dd845d9b5
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Apr 24 17:31:35 2015 +0200

    x86/entry: Define 'cpu_current_top_of_stack' for 64-bit code
    
    32-bit code has PER_CPU_VAR(cpu_current_top_of_stack).
    64-bit code uses somewhat more obscure: PER_CPU_VAR(cpu_tss + TSS_sp0).
    
    Define the 'cpu_current_top_of_stack' macro on CONFIG_X86_64
    as well so that the PER_CPU_VAR(cpu_current_top_of_stack)
    expression can be used in both 32-bit and 64-bit code.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1429889495-27850-3-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 472288962c99..225ee545e1a0 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -195,16 +195,14 @@ static inline unsigned long current_stack_pointer(void)
 
 #else /* !__ASSEMBLY__ */
 
+#ifdef CONFIG_X86_64
+# define cpu_current_top_of_stack (cpu_tss + TSS_sp0)
+#endif
+
 /* Load thread_info address into "reg" */
-#ifdef CONFIG_X86_32
 #define GET_THREAD_INFO(reg) \
 	_ASM_MOV PER_CPU_VAR(cpu_current_top_of_stack),reg ; \
 	_ASM_SUB $(THREAD_SIZE),reg ;
-#else
-#define GET_THREAD_INFO(reg) \
-	_ASM_MOV PER_CPU_VAR(cpu_tss + TSS_sp0),reg ; \
-	_ASM_SUB $(THREAD_SIZE),reg ;
-#endif
 
 /*
  * ASM operand which evaluates to a 'thread_info' address of

commit fed7c3f0f750f225317828d691e9eb76eec887b3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Apr 24 17:31:34 2015 +0200

    x86/entry: Remove unused 'kernel_stack' per-cpu variable
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1429889495-27850-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index d656a363e1eb..472288962c99 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -177,8 +177,6 @@ struct thread_info {
  */
 #ifndef __ASSEMBLY__
 
-DECLARE_PER_CPU(unsigned long, kernel_stack);
-
 static inline struct thread_info *current_thread_info(void)
 {
 	return (struct thread_info *)(current_top_of_stack() - THREAD_SIZE);

commit 63332a8455d8310b77d38779c6c21a660a8d9feb
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Apr 24 17:31:33 2015 +0200

    x86/entry: Stop using PER_CPU_VAR(kernel_stack)
    
    PER_CPU_VAR(kernel_stack) is redundant:
    
      - On the 64-bit build, we can use PER_CPU_VAR(cpu_tss + TSS_sp0).
      - On the 32-bit build, we can use PER_CPU_VAR(cpu_current_top_of_stack).
    
    PER_CPU_VAR(kernel_stack) will be deleted by a separate change.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1429889495-27850-1-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index b4bdec3e9523..d656a363e1eb 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -198,9 +198,15 @@ static inline unsigned long current_stack_pointer(void)
 #else /* !__ASSEMBLY__ */
 
 /* Load thread_info address into "reg" */
+#ifdef CONFIG_X86_32
 #define GET_THREAD_INFO(reg) \
-	_ASM_MOV PER_CPU_VAR(kernel_stack),reg ; \
+	_ASM_MOV PER_CPU_VAR(cpu_current_top_of_stack),reg ; \
 	_ASM_SUB $(THREAD_SIZE),reg ;
+#else
+#define GET_THREAD_INFO(reg) \
+	_ASM_MOV PER_CPU_VAR(cpu_tss + TSS_sp0),reg ; \
+	_ASM_SUB $(THREAD_SIZE),reg ;
+#endif
 
 /*
  * ASM operand which evaluates to a 'thread_info' address of

commit fa2e5c073a355465a2a8c9a2fbecf404f9857c3a
Merge: e44740c1a94b 97b2f0dc3314
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 15 13:53:55 2015 -0700

    Merge branch 'exec_domain_rip_v2' of git://git.kernel.org/pub/scm/linux/kernel/git/rw/misc
    
    Pull exec domain removal from Richard Weinberger:
     "This series removes execution domain support from Linux.
    
      The idea behind exec domains was to support different ABIs.  The
      feature was never complete nor stable.  Let's rip it out and make the
      kernel signal handling code less complicated"
    
    * 'exec_domain_rip_v2' of git://git.kernel.org/pub/scm/linux/kernel/git/rw/misc: (27 commits)
      arm64: Removed unused variable
      sparc: Fix execution domain removal
      Remove rest of exec domains.
      arch: Remove exec_domain from remaining archs
      arc: Remove signal translation and exec_domain
      xtensa: Remove signal translation and exec_domain
      xtensa: Autogenerate offsets in struct thread_info
      x86: Remove signal translation and exec_domain
      unicore32: Remove signal translation and exec_domain
      um: Remove signal translation and exec_domain
      tile: Remove signal translation and exec_domain
      sparc: Remove signal translation and exec_domain
      sh: Remove signal translation and exec_domain
      s390: Remove signal translation and exec_domain
      mn10300: Remove signal translation and exec_domain
      microblaze: Remove signal translation and exec_domain
      m68k: Remove signal translation and exec_domain
      m32r: Remove signal translation and exec_domain
      m32r: Autogenerate offsets in struct thread_info
      frv: Remove signal translation and exec_domain
      ...

commit 3050a35fba296196cb00e87f4a96aa7d9ed17a7b
Author: Richard Weinberger <richard@nod.at>
Date:   Sun Jul 13 17:43:51 2014 +0200

    x86: Remove signal translation and exec_domain
    
    As execution domain support is gone we can remove
    signal translation from the signal code and remove
    exec_domain from thread_info.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 1d4e4f279a32..2df52baf5228 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -19,13 +19,11 @@
  */
 #ifndef __ASSEMBLY__
 struct task_struct;
-struct exec_domain;
 #include <asm/processor.h>
 #include <linux/atomic.h>
 
 struct thread_info {
 	struct task_struct	*task;		/* main task structure */
-	struct exec_domain	*exec_domain;	/* execution domain */
 	__u32			flags;		/* low level flags */
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
@@ -39,7 +37,6 @@ struct thread_info {
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.task		= &tsk,			\
-	.exec_domain	= &default_exec_domain,	\
 	.flags		= 0,			\
 	.cpu		= 0,			\
 	.saved_preempt_count = INIT_PREEMPT_COUNT,	\

commit dca5b52ad76b10c3adc29e2a006d4b1721c44a8d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 24 19:44:42 2015 +0100

    x86/asm/entry/64: Rename THREAD_INFO() to ASM_THREAD_INFO()
    
    The THREAD_INFO() macro has a somewhat confusingly generic name,
    defined in a generic .h C header file. It also does not make it
    clear that it constructs a memory operand for use in assembly
    code.
    
    Rename it to ASM_THREAD_INFO() to make it all glaringly
    obvious on first glance.
    
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/20150324184442.GC14760@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 224285b674ca..ea2dbe82cba3 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -222,7 +222,7 @@ static inline unsigned long current_stack_pointer(void)
  * currently at exactly SIZEOF_PTREGS bytes away from the top of the
  * stack:
  *
- *      mov THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS), %eax
+ *      mov ASM_THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS), %eax
  *
  * will translate to:
  *
@@ -230,7 +230,7 @@ static inline unsigned long current_stack_pointer(void)
  *
  * which is below the current RSP by almost 16K.
  */
-#define THREAD_INFO(field, reg, off) ((field)+(off)-THREAD_SIZE)(reg)
+#define ASM_THREAD_INFO(field, reg, off) ((field)+(off)-THREAD_SIZE)(reg)
 
 #endif
 

commit f9d71854b4fe9b22ca199c4676da5a6ece1e5c17
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 24 19:44:11 2015 +0100

    x86/asm/entry/64: Merge the field offset into the THREAD_INFO() macro
    
    Before:
    
       TI_sysenter_return+THREAD_INFO(%rsp,3*8),%r10d
    
    After:
    
       movl    THREAD_INFO(TI_sysenter_return, %rsp, 3*8), %r10d
    
    to turn it into a clear thread_info accessor.
    
    No code changed:
    
     md5:
       fb4cb2b3ce05d89940ca304efc8ff183  ia32entry.o.before.asm
       fb4cb2b3ce05d89940ca304efc8ff183  ia32entry.o.after.asm
    
       e39f2958a5d1300158e276e4f7663263  entry_64.o.before.asm
       e39f2958a5d1300158e276e4f7663263  entry_64.o.after.asm
    
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/20150324184411.GB14760@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 813dfbb867a7..224285b674ca 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -222,7 +222,7 @@ static inline unsigned long current_stack_pointer(void)
  * currently at exactly SIZEOF_PTREGS bytes away from the top of the
  * stack:
  *
- *      mov TI_flags+THREAD_INFO(%rsp, SIZEOF_PTREGS), %eax
+ *      mov THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS), %eax
  *
  * will translate to:
  *
@@ -230,7 +230,7 @@ static inline unsigned long current_stack_pointer(void)
  *
  * which is below the current RSP by almost 16K.
  */
-#define THREAD_INFO(reg, off) ((off)-THREAD_SIZE)(reg)
+#define THREAD_INFO(field, reg, off) ((field)+(off)-THREAD_SIZE)(reg)
 
 #endif
 

commit 1ddc6f3c60d75a7577dd33bc441e309febe2fc76
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 24 19:43:11 2015 +0100

    x86/asm/entry/64: Improve the THREAD_INFO() macro explanation
    
    Explain the background, and add a real example.
    
    Acked-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/20150324184311.GA14760@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ad0ee3423da5..813dfbb867a7 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -206,10 +206,29 @@ static inline unsigned long current_stack_pointer(void)
 	_ASM_SUB $(THREAD_SIZE),reg ;
 
 /*
- * ASM operand which evaluates to thread_info address
- * if it is known that "reg" is exactly "off" bytes below stack top.
- * Example (fetch thread_info->fieldname):
- *  mov TI_fieldname+THREAD_INFO(reg, off),%eax
+ * ASM operand which evaluates to a 'thread_info' address of
+ * the current task, if it is known that "reg" is exactly "off"
+ * bytes below the top of the stack currently.
+ *
+ * ( The kernel stack's size is known at build time, it is usually
+ *   2 or 4 pages, and the bottom  of the kernel stack contains
+ *   the thread_info structure. So to access the thread_info very
+ *   quickly from assembly code we can calculate down from the
+ *   top of the kernel stack to the bottom, using constant,
+ *   build-time calculations only. )
+ *
+ * For example, to fetch the current thread_info->flags value into %eax
+ * on x86-64 defconfig kernels, in syscall entry code where RSP is
+ * currently at exactly SIZEOF_PTREGS bytes away from the top of the
+ * stack:
+ *
+ *      mov TI_flags+THREAD_INFO(%rsp, SIZEOF_PTREGS), %eax
+ *
+ * will translate to:
+ *
+ *      8b 84 24 b8 c0 ff ff      mov    -0x3f48(%rsp), %eax
+ *
+ * which is below the current RSP by almost 16K.
  */
 #define THREAD_INFO(reg, off) ((off)-THREAD_SIZE)(reg)
 

commit ef593260f0cae2699874f098fb5b19fb46502cb3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Thu Mar 19 18:17:46 2015 +0100

    x86/asm/entry: Get rid of KERNEL_STACK_OFFSET
    
    PER_CPU_VAR(kernel_stack) was set up in a way where it points
    five stack slots below the top of stack.
    
    Presumably, it was done to avoid one "sub $5*8,%rsp"
    in syscall/sysenter code paths, where iret frame needs to be
    created by hand.
    
    Ironically, none of them benefits from this optimization,
    since all of them need to allocate additional data on stack
    (struct pt_regs), so they still have to perform subtraction.
    
    This patch eliminates KERNEL_STACK_OFFSET.
    
    PER_CPU_VAR(kernel_stack) now points directly to top of stack.
    pt_regs allocations are adjusted to allocate iret frame as well.
    Hopefully we can merge it later with 32-bit specific
    PER_CPU_VAR(cpu_current_top_of_stack) variable...
    
    Net result in generated code is that constants in several insns
    are changed.
    
    This change is necessary for changing struct pt_regs creation
    in SYSCALL64 code path from MOV to PUSH instructions.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426785469-15125-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ae9c2f13b476..ad0ee3423da5 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -172,7 +172,6 @@ struct thread_info {
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
 
 #define STACK_WARN		(THREAD_SIZE/8)
-#define KERNEL_STACK_OFFSET	(5*(BITS_PER_LONG/8))
 
 /*
  * macros/functions for gaining access to the thread information structure
@@ -201,10 +200,10 @@ static inline unsigned long current_stack_pointer(void)
 
 #else /* !__ASSEMBLY__ */
 
-/* how to get the thread information struct from ASM */
+/* Load thread_info address into "reg" */
 #define GET_THREAD_INFO(reg) \
 	_ASM_MOV PER_CPU_VAR(kernel_stack),reg ; \
-	_ASM_SUB $(THREAD_SIZE-KERNEL_STACK_OFFSET),reg ;
+	_ASM_SUB $(THREAD_SIZE),reg ;
 
 /*
  * ASM operand which evaluates to thread_info address

commit b3fe8ba320ace38cee6859b4c015d81627254ddb
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Thu Mar 19 18:17:45 2015 +0100

    x86/asm/entry/64: Change the THREAD_INFO() definition to not depend on KERNEL_STACK_OFFSET
    
    This changes the THREAD_INFO() definition and all its callsites
    so that they do not count stack position from
    (top of stack - KERNEL_STACK_OFFSET), but from top of stack.
    
    Semi-mysterious expressions THREAD_INFO(%rsp,RIP) - "why RIP??"
    are now replaced by more logical THREAD_INFO(%rsp,SIZEOF_PTREGS)
    - "calculate thread_info's address using information that
    rsp is SIZEOF_PTREGS bytes below top of stack".
    
    While at it, replace "(off)-THREAD_SIZE(reg)" with equivalent
    "((off)-THREAD_SIZE)(reg)". The form without parentheses
    falsely looks like we invoke THREAD_SIZE() macro.
    
    Improve comment atop THREAD_INFO macro definition.
    
    This patch does not change generated code (verified by objdump).
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426785469-15125-1-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 0abf7ab20ce2..ae9c2f13b476 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -207,10 +207,12 @@ static inline unsigned long current_stack_pointer(void)
 	_ASM_SUB $(THREAD_SIZE-KERNEL_STACK_OFFSET),reg ;
 
 /*
- * Same if PER_CPU_VAR(kernel_stack) is, perhaps with some offset, already in
- * a certain register (to be used in assembler memory operands).
+ * ASM operand which evaluates to thread_info address
+ * if it is known that "reg" is exactly "off" bytes below stack top.
+ * Example (fetch thread_info->fieldname):
+ *  mov TI_fieldname+THREAD_INFO(reg, off),%eax
  */
-#define THREAD_INFO(reg, off) KERNEL_STACK_OFFSET+(off)-THREAD_SIZE(reg)
+#define THREAD_INFO(reg, off) ((off)-THREAD_SIZE)(reg)
 
 #endif
 

commit 1daeaa315164c60b937f56fe3848d4328c358eba
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sat Mar 21 18:54:21 2015 -0400

    x86/asm/entry: Fix execve() and sigreturn() syscalls to always return via IRET
    
    Both the execve() and sigreturn() family of syscalls have the
    ability to change registers in ways that may not be compatabile
    with the syscall path they were called from.
    
    In particular, SYSRET and SYSEXIT can't handle non-default %cs and %ss,
    and some bits in eflags.
    
    These syscalls have stubs that are hardcoded to jump to the IRET path,
    and not return to the original syscall path.
    
    The following commit:
    
       76f5df43cab5e76 ("Always allocate a complete "struct pt_regs" on the kernel stack")
    
    recently changed this for some 32-bit compat syscalls, but introduced a bug where
    execve from a 32-bit program to a 64-bit program would fail because it still returned
    via SYSRETL. This caused Wine to fail when built for both 32-bit and 64-bit.
    
    This patch sets TIF_NOTIFY_RESUME for execve() and sigreturn() so
    that the IRET path is always taken on exit to userspace.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1426978461-32089-1-git-send-email-brgerst@gmail.com
    [ Improved the changelog and comments. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ba115eb6fbcf..0abf7ab20ce2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -260,6 +260,16 @@ static inline bool is_ia32_task(void)
 #endif
 	return false;
 }
+
+/*
+ * Force syscall return via IRET by making it look as if there was
+ * some work pending. IRET is our most capable (but slowest) syscall
+ * return path, which is able to restore modified SS, CS and certain
+ * EFLAGS values that other (fast) syscall return instructions
+ * are not able to restore properly.
+ */
+#define force_iret() set_thread_flag(TIF_NOTIFY_RESUME)
+
 #endif	/* !__ASSEMBLY__ */
 
 #ifndef __ASSEMBLY__

commit 3ee4298f440c81638cbb5ec06f2497fb7a9a9eb4
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Tue Mar 10 11:05:58 2015 -0700

    x86/asm/entry: Create and use a 'TOP_OF_KERNEL_STACK_PADDING' macro
    
    x86_32, unlike x86_64, pads the top of the kernel stack, because the
    hardware stack frame formats are variable in size.
    
    Document this padding and give it a name.
    
    This should make no change whatsoever to the compiled kernel
    image. It also doesn't fix any of the current bugs in this area.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/02bf2f54b8dcb76a62a142b6dfe07d4ef7fc582e.1426009661.git.luto@amacapital.net
    [ Fixed small details, such as a missed magic constant in entry_32.S pointed out by Denys Vlasenko. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 7740edd56fed..ba115eb6fbcf 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -12,6 +12,33 @@
 #include <asm/percpu.h>
 #include <asm/types.h>
 
+/*
+ * TOP_OF_KERNEL_STACK_PADDING is a number of unused bytes that we
+ * reserve at the top of the kernel stack.  We do it because of a nasty
+ * 32-bit corner case.  On x86_32, the hardware stack frame is
+ * variable-length.  Except for vm86 mode, struct pt_regs assumes a
+ * maximum-length frame.  If we enter from CPL 0, the top 8 bytes of
+ * pt_regs don't actually exist.  Ordinarily this doesn't matter, but it
+ * does in at least one case:
+ *
+ * If we take an NMI early enough in SYSENTER, then we can end up with
+ * pt_regs that extends above sp0.  On the way out, in the espfix code,
+ * we can read the saved SS value, but that value will be above sp0.
+ * Without this offset, that can result in a page fault.  (We are
+ * careful that, in this case, the value we read doesn't matter.)
+ *
+ * In vm86 mode, the hardware frame is much longer still, but we neither
+ * access the extra members from NMI context, nor do we write such a
+ * frame at sp0 at all.
+ *
+ * x86_64 has a fixed-length stack frame.
+ */
+#ifdef CONFIG_X86_32
+# define TOP_OF_KERNEL_STACK_PADDING 8
+#else
+# define TOP_OF_KERNEL_STACK_PADDING 0
+#endif
+
 /*
  * low level task data that entry.S needs immediate access to
  * - this struct should fit entirely inside of one cache line

commit a7fcf28d431ef70afaa91496e64e16dc51dccec4
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Mar 6 17:50:19 2015 -0800

    x86/asm/entry: Replace this_cpu_sp0() with current_top_of_stack() and fix it on x86_32
    
    I broke 32-bit kernels.  The implementation of sp0 was correct
    as far as I can tell, but sp0 was much weirder on x86_32 than I
    realized.  It has the following issues:
    
     - Init's sp0 is inconsistent with everything else's: non-init tasks
       are offset by 8 bytes.  (I have no idea why, and the comment is unhelpful.)
    
     - vm86 does crazy things to sp0.
    
    Fix it up by replacing this_cpu_sp0() with
    current_top_of_stack() and using a new percpu variable to track
    the top of the stack on x86_32.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 75182b1632a8 ("x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()")
    Link: http://lkml.kernel.org/r/d09dbe270883433776e0cbee3c7079433349e96d.1425692936.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index a2fa1899494e..7740edd56fed 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -158,9 +158,7 @@ DECLARE_PER_CPU(unsigned long, kernel_stack);
 
 static inline struct thread_info *current_thread_info(void)
 {
-	struct thread_info *ti;
-	ti = (void *)(this_cpu_sp0() - THREAD_SIZE);
-	return ti;
+	return (struct thread_info *)(current_top_of_stack() - THREAD_SIZE);
 }
 
 static inline unsigned long current_stack_pointer(void)

commit 75182b1632a89f12540baa1806a7c5c180db620c
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:03 2015 -0800

    x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()
    
    This will make modifying the semantics of kernel_stack easier.
    
    The change to ist_begin_non_atomic() is necessary because sp0 no
    longer points to the same THREAD_SIZE-aligned region as RSP;
    it's one byte too high for that.  At Denys' suggestion, rather
    than offsetting it, just check explicitly that we're in the
    correct range ending at sp0.  This has the added benefit that we
    no longer assume that the thread stack is aligned to
    THREAD_SIZE.
    
    Suggested-by: Denys Vlasenko <dvlasenk@redhat.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/ef8254ad414cbb8034c9a56396eeb24f5dd5b0de.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 1d4e4f279a32..a2fa1899494e 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -159,8 +159,7 @@ DECLARE_PER_CPU(unsigned long, kernel_stack);
 static inline struct thread_info *current_thread_info(void)
 {
 	struct thread_info *ti;
-	ti = (void *)(this_cpu_read_stable(kernel_stack) +
-		      KERNEL_STACK_OFFSET - THREAD_SIZE);
+	ti = (void *)(this_cpu_sp0() - THREAD_SIZE);
 	return ti;
 }
 

commit f56141e3e2d9aabf7e6b89680ab572c2cdbb2a24
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Feb 12 15:01:14 2015 -0800

    all arches, signal: move restart_block to struct task_struct
    
    If an attacker can cause a controlled kernel stack overflow, overwriting
    the restart block is a very juicy exploit target.  This is because the
    restart_block is held in the same memory allocation as the kernel stack.
    
    Moving the restart block to struct task_struct prevents this exploit by
    making the restart_block harder to locate.
    
    Note that there are other fields in thread_info that are also easy
    targets, at least on some architectures.
    
    It's also a decent simplification, since the restart code is more or less
    identical on all architectures.
    
    [james.hogan@imgtec.com: metag: align thread_info::supervisor_stack]
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: David Miller <davem@davemloft.net>
    Acked-by: Richard Weinberger <richard@nod.at>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e82e95abc92b..1d4e4f279a32 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -31,7 +31,6 @@ struct thread_info {
 	__u32			cpu;		/* current CPU */
 	int			saved_preempt_count;
 	mm_segment_t		addr_limit;
-	struct restart_block    restart_block;
 	void __user		*sysenter_return;
 	unsigned int		sig_on_uaccess_error:1;
 	unsigned int		uaccess_err:1;	/* uaccess failed */
@@ -45,9 +44,6 @@ struct thread_info {
 	.cpu		= 0,			\
 	.saved_preempt_count = INIT_PREEMPT_COUNT,	\
 	.addr_limit	= KERNEL_DS,		\
-	.restart_block = {			\
-		.fn = do_no_restart_syscall,	\
-	},					\
 }
 
 #define init_thread_info	(init_thread_union.thread_info)

commit d4812e169de44f4ab53ff671c6193c67de24da62
Author: Luck, Tony <tony.luck@intel.com>
Date:   Mon Jan 5 16:44:42 2015 -0800

    x86, mce: Get rid of TIF_MCE_NOTIFY and associated mce tricks
    
    We now switch to the kernel stack when a machine check interrupts
    during user mode.  This means that we can perform recovery actions
    in the tail of do_machine_check()
    
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 8b13b0fbda8e..e82e95abc92b 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -75,7 +75,6 @@ struct thread_info {
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
-#define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
@@ -100,7 +99,6 @@ struct thread_info {
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
-#define _TIF_MCE_NOTIFY		(1 << TIF_MCE_NOTIFY)
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
@@ -140,7 +138,7 @@ struct thread_info {
 
 /* Only used for 64 bit */
 #define _TIF_DO_NOTIFY_MASK						\
-	(_TIF_SIGPENDING | _TIF_MCE_NOTIFY | _TIF_NOTIFY_RESUME |	\
+	(_TIF_SIGPENDING | _TIF_NOTIFY_RESUME |				\
 	 _TIF_USER_RETURN_NOTIFY | _TIF_UPROBE)
 
 /* flags to check in __switch_to() */

commit 83653c16da91112236292871b820cb8b367220e3
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Nov 13 15:57:07 2014 -0800

    x86: Clean up current_stack_pointer
    
    There's no good reason for it to be a macro, and x86_64 will want to
    use it, so it should be in a header.
    
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 547e344a6dc6..8b13b0fbda8e 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -170,6 +170,17 @@ static inline struct thread_info *current_thread_info(void)
 	return ti;
 }
 
+static inline unsigned long current_stack_pointer(void)
+{
+	unsigned long sp;
+#ifdef CONFIG_X86_64
+	asm("mov %%rsp,%0" : "=g" (sp));
+#else
+	asm("mov %%esp,%0" : "=g" (sp));
+#endif
+	return sp;
+}
+
 #else /* !__ASSEMBLY__ */
 
 /* how to get the thread information struct from ASM */

commit 82975bc6a6df743b9a01810fb32cb65d0ec5d60b
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Nov 21 13:26:07 2014 -0800

    uprobes, x86: Fix _TIF_UPROBE vs _TIF_NOTIFY_RESUME
    
    x86 call do_notify_resume on paranoid returns if TIF_UPROBE is set but
    not on non-paranoid returns.  I suspect that this is a mistake and that
    the code only works because int3 is paranoid.
    
    Setting _TIF_NOTIFY_RESUME in the uprobe code was probably a workaround
    for the x86 bug.  With that bug fixed, we can remove _TIF_NOTIFY_RESUME
    from the uprobes code.
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 854053889d4d..547e344a6dc6 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -141,7 +141,7 @@ struct thread_info {
 /* Only used for 64 bit */
 #define _TIF_DO_NOTIFY_MASK						\
 	(_TIF_SIGPENDING | _TIF_MCE_NOTIFY | _TIF_NOTIFY_RESUME |	\
-	 _TIF_USER_RETURN_NOTIFY)
+	 _TIF_USER_RETURN_NOTIFY | _TIF_UPROBE)
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\

commit f80c5b39b80ab4d52acd940c03b89948cafdbd18
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 11 09:59:05 2014 +0200

    sched/idle, x86: Switch from TS_POLLING to TIF_POLLING_NRFLAG
    
    Standardize the idle polling indicator to TIF_POLLING_NRFLAG such that
    both TIF_NEED_RESCHED and TIF_POLLING_NRFLAG are in the same word.
    This will allow us, using fetch_or(), to both set NEED_RESCHED and
    check for POLLING_NRFLAG in a single operation and avoid pointless
    wakeups.
    
    Changing from the non-atomic thread_info::status flags to the atomic
    thread_info::flags shouldn't be a big issue since most polling state
    changes were followed/preceded by a full memory barrier anyway.
    
    Also, fix up the apm_32 idle function, clearly that was forgotten in
    the last conversion. The default idle state is !POLLING so just kill
    the lot.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Link: http://lkml.kernel.org/n/tip-7yksmqtlv4nfowmlqr1rifoi@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 47e5de25ba79..854053889d4d 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -83,6 +83,7 @@ struct thread_info {
 #define TIF_FORK		18	/* ret_from_fork */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
+#define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_BLOCKSTEP		25	/* set when we want DEBUGCTLMSR_BTF */
@@ -106,6 +107,7 @@ struct thread_info {
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_FORK		(1 << TIF_FORK)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
+#define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
 #define _TIF_BLOCKSTEP		(1 << TIF_BLOCKSTEP)
@@ -191,8 +193,6 @@ static inline struct thread_info *current_thread_info(void)
  * have to worry about atomic accesses.
  */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
-#define TS_POLLING		0x0004	/* idle task polling need_resched,
-					   skip sending interrupt */
 #define TS_RESTORE_SIGMASK	0x0008	/* restore signal mask in do_signal() */
 
 #ifndef __ASSEMBLY__

commit 198d208df4371734ac4728f69cb585c284d20a15
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 6 09:41:31 2014 -0500

    x86: Keep thread_info on thread stack in x86_32
    
    x86_64 uses a per_cpu variable kernel_stack to always point to
    the thread stack of current. This is where the thread_info is stored
    and is accessed from this location even when the irq or exception stack
    is in use. This removes the complexity of having to maintain the
    thread info on the stack when interrupts are running and having to
    copy the preempt_count and other fields to the interrupt stack.
    
    x86_32 uses the old method of copying the thread_info from the thread
    stack to the exception stack just before executing the exception.
    
    Having the two different requires #ifdefs and also the x86_32 way
    is a bit of a pain to maintain. By converting x86_32 to the same
    method of x86_64, we can remove #ifdefs, clean up the x86_32 code
    a little, and remove the overhead of the copy.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20110806012354.263834829@goodmis.org
    Link: http://lkml.kernel.org/r/20140206144321.852942014@goodmis.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ca2de1b48fac..47e5de25ba79 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -9,6 +9,7 @@
 
 #include <linux/compiler.h>
 #include <asm/page.h>
+#include <asm/percpu.h>
 #include <asm/types.h>
 
 /*
@@ -34,12 +35,6 @@ struct thread_info {
 	void __user		*sysenter_return;
 	unsigned int		sig_on_uaccess_error:1;
 	unsigned int		uaccess_err:1;	/* uaccess failed */
-#ifdef CONFIG_X86_32
-	unsigned long           previous_esp;   /* ESP of the previous stack in
-						   case of nested (IRQ) stacks
-						   (Moved to end, to be removed soon)
-						*/
-#endif
 };
 
 #define INIT_THREAD_INFO(tsk)			\
@@ -153,9 +148,9 @@ struct thread_info {
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
 
-#ifdef CONFIG_X86_32
+#define STACK_WARN		(THREAD_SIZE/8)
+#define KERNEL_STACK_OFFSET	(5*(BITS_PER_LONG/8))
 
-#define STACK_WARN	(THREAD_SIZE/8)
 /*
  * macros/functions for gaining access to the thread information structure
  *
@@ -163,38 +158,6 @@ struct thread_info {
  */
 #ifndef __ASSEMBLY__
 
-#define current_stack_pointer ({		\
-	unsigned long sp;			\
-	asm("mov %%esp,%0" : "=g" (sp));	\
-	sp;					\
-})
-
-/* how to get the thread information struct from C */
-static inline struct thread_info *current_thread_info(void)
-{
-	return (struct thread_info *)
-		(current_stack_pointer & ~(THREAD_SIZE - 1));
-}
-
-#else /* !__ASSEMBLY__ */
-
-/* how to get the thread information struct from ASM */
-#define GET_THREAD_INFO(reg)	 \
-	movl $-THREAD_SIZE, reg; \
-	andl %esp, reg
-
-#endif
-
-#else /* X86_32 */
-
-#include <asm/percpu.h>
-#define KERNEL_STACK_OFFSET (5*8)
-
-/*
- * macros/functions for gaining access to the thread information structure
- * preempt_count needs to be 1 initially, until the scheduler is functional.
- */
-#ifndef __ASSEMBLY__
 DECLARE_PER_CPU(unsigned long, kernel_stack);
 
 static inline struct thread_info *current_thread_info(void)
@@ -209,8 +172,8 @@ static inline struct thread_info *current_thread_info(void)
 
 /* how to get the thread information struct from ASM */
 #define GET_THREAD_INFO(reg) \
-	movq PER_CPU_VAR(kernel_stack),reg ; \
-	subq $(THREAD_SIZE-KERNEL_STACK_OFFSET),reg
+	_ASM_MOV PER_CPU_VAR(kernel_stack),reg ; \
+	_ASM_SUB $(THREAD_SIZE-KERNEL_STACK_OFFSET),reg ;
 
 /*
  * Same if PER_CPU_VAR(kernel_stack) is, perhaps with some offset, already in
@@ -220,8 +183,6 @@ static inline struct thread_info *current_thread_info(void)
 
 #endif
 
-#endif /* !X86_32 */
-
 /*
  * Thread-synchronous status.
  *

commit 0788aa6a23cb9d693fc5040ec774b979f1e906cd
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 6 09:41:30 2014 -0500

    x86: Prepare removal of previous_esp from i386 thread_info structure
    
    The i386 thread_info contains a previous_esp field that is used
    to daisy chain the different stacks for dump_stack()
    (ie. irq, softirq, thread stacks).
    
    The goal is to eventual make i386 handling of thread_info the same
    as x86_64, which means that the thread_info will not be in the stack
    but as a per_cpu variable. We will no longer depend on thread_info
    being able to daisy chain different stacks as it will only exist
    in one location (the thread stack).
    
    By moving previous_esp to the end of thread_info and referencing
    it as an offset instead of using a thread_info field, this becomes
    a stepping stone to moving the thread_info.
    
    The offset to get to the previous stack is rather ugly in this
    patch, but this is only temporary and the prev_esp will be changed
    in the next commit. This commit is more for sanity checks of the
    change.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Robert Richter <rric@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20110806012353.891757693@goodmis.org
    Link: http://lkml.kernel.org/r/20140206144321.608754481@goodmis.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 1cb3501636d2..ca2de1b48fac 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -32,13 +32,14 @@ struct thread_info {
 	mm_segment_t		addr_limit;
 	struct restart_block    restart_block;
 	void __user		*sysenter_return;
+	unsigned int		sig_on_uaccess_error:1;
+	unsigned int		uaccess_err:1;	/* uaccess failed */
 #ifdef CONFIG_X86_32
 	unsigned long           previous_esp;   /* ESP of the previous stack in
 						   case of nested (IRQ) stacks
+						   (Moved to end, to be removed soon)
 						*/
 #endif
-	unsigned int		sig_on_uaccess_error:1;
-	unsigned int		uaccess_err:1;	/* uaccess failed */
 };
 
 #define INIT_THREAD_INFO(tsk)			\

commit b807902a88c470eb06d0acdf3b6590f27f5dce81
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Feb 6 09:41:29 2014 -0500

    x86: Nuke GET_THREAD_INFO_WITH_ESP() macro for i386
    
    According to a git log -p, GET_THREAD_INFO_WITH_ESP() has only been defined
    and never been used. Get rid of it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20140206144321.409045251@goodmis.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index b7aa975561e2..1cb3501636d2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -182,10 +182,6 @@ static inline struct thread_info *current_thread_info(void)
 	movl $-THREAD_SIZE, reg; \
 	andl %esp, reg
 
-/* use this one if reg already contains %esp */
-#define GET_THREAD_INFO_WITH_ESP(reg) \
-	andl $-THREAD_SIZE, reg
-
 #endif
 
 #else /* X86_32 */

commit 2432e1364bbee83cb6e63e026c91785031704ba5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 6 09:41:28 2014 -0500

    x86: Nuke the supervisor_stack field in i386 thread_info
    
    Nothing references the supervisor_stack in the thread_info field,
    and it does not exist in x86_64. To make the two more the same,
    it is being removed.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20110806012353.546183789@goodmis.org
    Link: http://lkml.kernel.org/r/20140206144321.203619611@goodmis.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e1940c06ed02..b7aa975561e2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -36,7 +36,6 @@ struct thread_info {
 	unsigned long           previous_esp;   /* ESP of the previous stack in
 						   case of nested (IRQ) stacks
 						*/
-	__u8			supervisor_stack[0];
 #endif
 	unsigned int		sig_on_uaccess_error:1;
 	unsigned int		uaccess_err:1;	/* uaccess failed */

commit dff38e3e93bbc10653a232f68077e5d031624464
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Oct 22 09:07:57 2013 -0700

    x86: Use inline assembler instead of global register variable to get sp
    
    LTO in gcc 4.6/47. has trouble with global register variables. They were used
    to read the stack pointer. Use a simple inline assembler statement with
    a mov instead.
    
    This also helps LLVM/clang, which does not support global register
    variables.
    
    [ hpa: Ideally this should become a builtin in both gcc and clang. ]
    
    v2: More general asm constraint. Fix description (Jan Beulich)
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1382458079-24450-6-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 3ba3de457d05..e1940c06ed02 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -163,9 +163,11 @@ struct thread_info {
  */
 #ifndef __ASSEMBLY__
 
-
-/* how to get the current stack pointer from C */
-register unsigned long current_stack_pointer asm("esp") __used;
+#define current_stack_pointer ({		\
+	unsigned long sp;			\
+	asm("mov %%esp,%0" : "=g" (sp));	\
+	sp;					\
+})
 
 /* how to get the thread information struct from C */
 static inline struct thread_info *current_thread_info(void)

commit 00d1a39e69d5afa7523dad515a05b21abd17c389
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 17 18:53:09 2013 +0000

    preempt: Make PREEMPT_ACTIVE generic
    
    No point in having this bit defined by architecture.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20130917183629.090698799@linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index c46a46be1ec6..3ba3de457d05 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -153,8 +153,6 @@ struct thread_info {
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
 
-#define PREEMPT_ACTIVE		0x10000000
-
 #ifdef CONFIG_X86_32
 
 #define STACK_WARN	(THREAD_SIZE/8)

commit c2daa3bed53a81171cf8c1a36db798e82b91afe8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 14 14:51:00 2013 +0200

    sched, x86: Provide a per-cpu preempt_count implementation
    
    Convert x86 to use a per-cpu preemption count. The reason for doing so
    is that accessing per-cpu variables is a lot cheaper than accessing
    thread_info variables.
    
    We still need to save/restore the actual preemption count due to
    PREEMPT_ACTIVE so we place the per-cpu __preempt_count variable in the
    same cache-line as the other hot __switch_to() variables such as
    current_task.
    
    NOTE: this save/restore is required even for !PREEMPT kernels as
    cond_resched() also relies on preempt_count's PREEMPT_ACTIVE to ignore
    task_struct::state.
    
    Also rename thread_info::preempt_count to ensure nobody is
    'accidentally' still poking at it.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-gzn5rfsf8trgjoqx8hyayy3q@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 27811190cbd7..c46a46be1ec6 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -28,8 +28,7 @@ struct thread_info {
 	__u32			flags;		/* low level flags */
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
-	int			preempt_count;	/* 0 => preemptable,
-						   <0 => BUG */
+	int			saved_preempt_count;
 	mm_segment_t		addr_limit;
 	struct restart_block    restart_block;
 	void __user		*sysenter_return;
@@ -49,7 +48,7 @@ struct thread_info {
 	.exec_domain	= &default_exec_domain,	\
 	.flags		= 0,			\
 	.cpu		= 0,			\
-	.preempt_count	= INIT_PREEMPT_COUNT,	\
+	.saved_preempt_count = INIT_PREEMPT_COUNT,	\
 	.addr_limit	= KERNEL_DS,		\
 	.restart_block = {			\
 		.fn = do_no_restart_syscall,	\

commit 37f07655522042399db7a4b54aee830bdb52ce2c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 3 15:08:11 2013 -0700

    x86: kill TIF_DEBUG
    
    Because it is not used.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index a1df6e84691f..27811190cbd7 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -89,7 +89,6 @@ struct thread_info {
 #define TIF_FORK		18	/* ret_from_fork */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
-#define TIF_DEBUG		21	/* uses debug registers */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_BLOCKSTEP		25	/* set when we want DEBUGCTLMSR_BTF */
@@ -113,7 +112,6 @@ struct thread_info {
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_FORK		(1 << TIF_FORK)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
-#define _TIF_DEBUG		(1 << TIF_DEBUG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
 #define _TIF_BLOCKSTEP		(1 << TIF_BLOCKSTEP)
@@ -154,7 +152,7 @@ struct thread_info {
 	(_TIF_IO_BITMAP|_TIF_NOTSC|_TIF_BLOCKSTEP)
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
-#define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)
+#define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
 
 #define PREEMPT_ACTIVE		0x10000000
 

commit ee761f629d598579594d7e1eb8c552f3c5f71e4d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:32 2013 +0100

    arch: Consolidate tsk_is_polling()
    
    Move it to a common place. Preparatory patch for implementing
    set/clear for the idle need_resched poll implementation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130321215233.446034505@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 2cd056e3ada3..a1df6e84691f 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -241,8 +241,6 @@ static inline struct thread_info *current_thread_info(void)
 					   skip sending interrupt */
 #define TS_RESTORE_SIGMASK	0x0008	/* restore signal mask in do_signal() */
 
-#define tsk_is_polling(t) (task_thread_info(t)->status & TS_POLLING)
-
 #ifndef __ASSEMBLY__
 #define HAVE_SET_RESTORE_SIGMASK	1
 static inline void set_restore_sigmask(void)

commit f431b634f24d099872e78acc356c7fd35913b36b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Feb 12 16:18:59 2013 -0500

    tracing/syscalls: Allow archs to ignore tracing compat syscalls
    
    The tracing of ia32 compat system calls has been a bit of a pain as they
    use different system call numbers than the 64bit equivalents.
    
    I wrote a simple 'lls' program that lists files. I compiled it as a i686
    ELF binary and ran it under a x86_64 box. This is the result:
    
    echo 0 > /debug/tracing/tracing_on
    echo 1 > /debug/tracing/events/syscalls/enable
    echo 1 > /debug/tracing/tracing_on ; ./lls ; echo 0 > /debug/tracing/tracing_on
    
    grep lls /debug/tracing/trace
    
    [.. skipping calls before TS_COMPAT is set ...]
    
                 lls-1127  [005] d...   936.409188: sys_recvfrom(fd: 0, ubuf: 4d560fc4, size: 0, flags: 8048034, addr: 8, addr_len: f7700420)
                 lls-1127  [005] d...   936.409190: sys_recvfrom -> 0x8a77000
                 lls-1127  [005] d...   936.409211: sys_lgetxattr(pathname: 0, name: 1000, value: 3, size: 22)
                 lls-1127  [005] d...   936.409215: sys_lgetxattr -> 0xf76ff000
                 lls-1127  [005] d...   936.409223: sys_dup2(oldfd: 4d55ae9b, newfd: 4)
                 lls-1127  [005] d...   936.409228: sys_dup2 -> 0xfffffffffffffffe
                 lls-1127  [005] d...   936.409236: sys_newfstat(fd: 4d55b085, statbuf: 80000)
                 lls-1127  [005] d...   936.409242: sys_newfstat -> 0x3
                 lls-1127  [005] d...   936.409243: sys_removexattr(pathname: 3, name: ffcd0060)
                 lls-1127  [005] d...   936.409244: sys_removexattr -> 0x0
                 lls-1127  [005] d...   936.409245: sys_lgetxattr(pathname: 0, name: 19614, value: 1, size: 2)
                 lls-1127  [005] d...   936.409248: sys_lgetxattr -> 0xf76e5000
                 lls-1127  [005] d...   936.409248: sys_newlstat(filename: 3, statbuf: 19614)
                 lls-1127  [005] d...   936.409249: sys_newlstat -> 0x0
                 lls-1127  [005] d...   936.409262: sys_newfstat(fd: f76fb588, statbuf: 80000)
                 lls-1127  [005] d...   936.409279: sys_newfstat -> 0x3
                 lls-1127  [005] d...   936.409279: sys_close(fd: 3)
                 lls-1127  [005] d...   936.421550: sys_close -> 0x200
                 lls-1127  [005] d...   936.421558: sys_removexattr(pathname: 3, name: ffcd00d0)
                 lls-1127  [005] d...   936.421560: sys_removexattr -> 0x0
                 lls-1127  [005] d...   936.421569: sys_lgetxattr(pathname: 4d564000, name: 1b1abc, value: 5, size: 802)
                 lls-1127  [005] d...   936.421574: sys_lgetxattr -> 0x4d564000
                 lls-1127  [005] d...   936.421575: sys_capget(header: 4d70f000, dataptr: 1000)
                 lls-1127  [005] d...   936.421580: sys_capget -> 0x0
                 lls-1127  [005] d...   936.421580: sys_lgetxattr(pathname: 4d710000, name: 3000, value: 3, size: 812)
                 lls-1127  [005] d...   936.421589: sys_lgetxattr -> 0x4d710000
                 lls-1127  [005] d...   936.426130: sys_lgetxattr(pathname: 4d713000, name: 2abc, value: 3, size: 32)
                 lls-1127  [005] d...   936.426141: sys_lgetxattr -> 0x4d713000
                 lls-1127  [005] d...   936.426145: sys_newlstat(filename: 3, statbuf: f76ff3f0)
                 lls-1127  [005] d...   936.426146: sys_newlstat -> 0x0
                 lls-1127  [005] d...   936.431748: sys_lgetxattr(pathname: 0, name: 1000, value: 3, size: 22)
    
    Obviously I'm not calling newfstat with a fd of 4d55b085. The calls are
    obviously incorrect, and confusing.
    
    Other efforts have been made to fix this:
    
    https://lkml.org/lkml/2012/3/26/367
    
    But the real solution is to rewrite the syscall internals and come up
    with a fixed solution. One that doesn't require all the kluge that the
    current solution has.
    
    Thus for now, instead of outputting incorrect data, simply ignore them.
    With this patch the changes now have:
    
     #> grep lls /debug/tracing/trace
     #>
    
    Compat system calls simply are not traced. If users need compat
    syscalls, then they should just use the raw syscall tracepoints.
    
    For an architecture to make their compat syscalls ignored, it must
    define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS (done in asm/ftrace.h) and also
    define an arch_trace_is_compat_syscall() function that will return true
    if the current task should ignore tracing the syscall.
    
    I want to stress that this change does not affect actual syscalls in any
    way, shape or form. It is only used within the tracing system and
    doesn't interfere with the syscall logic at all. The changes are
    consolidated nicely into trace_syscalls.c and asm/ftrace.h.
    
    I had to make one small modification to asm/thread_info.h and that was
    to remove the include of asm/ftrace.h. As asm/ftrace.h required the
    current_thread_info() it was causing include hell. That include was
    added back in 2008 when the function graph tracer was added:
    
     commit caf4b323 "tracing, x86: add low level support for ftrace return tracing"
    
    It does not need to be included there.
    
    Link: http://lkml.kernel.org/r/1360703939.21867.99.camel@gandalf.local.home
    
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 2d946e63ee82..2cd056e3ada3 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -20,7 +20,6 @@
 struct task_struct;
 struct exec_domain;
 #include <asm/processor.h>
-#include <asm/ftrace.h>
 #include <linux/atomic.h>
 
 struct thread_info {

commit 42859eea96ba6beabfb0369a1eeffa3c7d2bd9cb
Merge: f59b51fe3d30 f322220d6159
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 10 12:02:25 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull generic execve() changes from Al Viro:
     "This introduces the generic kernel_thread() and kernel_execve()
      functions, and switches x86, arm, alpha, um and s390 over to them."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (26 commits)
      s390: convert to generic kernel_execve()
      s390: switch to generic kernel_thread()
      s390: fold kernel_thread_helper() into ret_from_fork()
      s390: fold execve_tail() into start_thread(), convert to generic sys_execve()
      um: switch to generic kernel_thread()
      x86, um/x86: switch to generic sys_execve and kernel_execve
      x86: split ret_from_fork
      alpha: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      alpha: switch to generic kernel_thread()
      alpha: switch to generic sys_execve()
      arm: get rid of execve wrapper, switch to generic execve() implementation
      arm: optimized current_pt_regs()
      arm: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      arm: split ret_from_fork, simplify kernel_thread() [based on patch by rmk]
      generic sys_execve()
      generic kernel_execve()
      new helper: current_pt_regs()
      preparation for generic kernel_thread()
      um: kill thread->forking
      um: let signal_delivered() do SIGTRAP on singlestepping into handler
      ...

commit bf5a3c13b939813d28ce26c01425054c740d6731
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 11 20:26:34 2012 +0200

    x86: Syscall hooks for userspace RCU extended QS
    
    Add syscall slow path hooks to notify syscall entry
    and exit on CPUs that want to support userspace RCU
    extended quiescent state.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 89f794f007ec..c535d847e3b5 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -89,6 +89,7 @@ struct thread_info {
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
 #define TIF_FORK		18	/* ret_from_fork */
+#define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_DEBUG		21	/* uses debug registers */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
@@ -114,6 +115,7 @@ struct thread_info {
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_FORK		(1 << TIF_FORK)
+#define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_DEBUG		(1 << TIF_DEBUG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
@@ -126,12 +128,13 @@ struct thread_info {
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_AUDIT |	\
-	 _TIF_SECCOMP | _TIF_SINGLESTEP | _TIF_SYSCALL_TRACEPOINT)
+	 _TIF_SECCOMP | _TIF_SINGLESTEP | _TIF_SYSCALL_TRACEPOINT |	\
+	 _TIF_NOHZ)
 
 /* work to do in syscall_trace_leave() */
 #define _TIF_WORK_SYSCALL_EXIT	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP |	\
-	 _TIF_SYSCALL_TRACEPOINT)
+	 _TIF_SYSCALL_TRACEPOINT | _TIF_NOHZ)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK							\
@@ -141,7 +144,8 @@ struct thread_info {
 
 /* work to do on any return to user space */
 #define _TIF_ALLWORK_MASK						\
-	((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_TRACEPOINT)
+	((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_TRACEPOINT |	\
+	_TIF_NOHZ)
 
 /* Only used for 64 bit */
 #define _TIF_DO_NOTIFY_MASK						\

commit e76623d69408d0bd66a296c6ee5eae1b17a6adfc
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Aug 2 22:12:06 2012 +0400

    x86: get rid of TIF_IRET hackery
    
    TIF_NOTIFY_RESUME will work in precisely the same way; all that
    is achieved by TIF_IRET is appearing that there's some work to be
    done, so we end up on the iret exit path.  Just use NOTIFY_RESUME.
    And for execve() do that in 32bit start_thread(), not sys_execve()
    itself.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 89f794f007ec..c509d07bdbd7 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -79,7 +79,6 @@ struct thread_info {
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
-#define TIF_IRET		5	/* force IRET */
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
@@ -104,7 +103,6 @@ struct thread_info {
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
-#define _TIF_IRET		(1 << TIF_IRET)
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)

commit edd63a2763bdae0daa4f0a4d4c5d61d1154352a5
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Apr 27 13:42:45 2012 -0400

    set_restore_sigmask() is never called without SIGPENDING (and never should be)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 8f3f1ff69fa9..89f794f007ec 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -248,7 +248,7 @@ static inline void set_restore_sigmask(void)
 {
 	struct thread_info *ti = current_thread_info();
 	ti->status |= TS_RESTORE_SIGMASK;
-	set_bit(TIF_SIGPENDING, (unsigned long *)&ti->flags);
+	WARN_ON(!test_bit(TIF_SIGPENDING, (unsigned long *)&ti->flags));
 }
 static inline void clear_restore_sigmask(void)
 {

commit 4ebefe3ec729003443daf153ed6fad1739271283
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 26 22:29:20 2012 -0400

    new helpers: {clear,test,test_and_clear}_restore_sigmask()
    
    helpers parallel to set_restore_sigmask(), used in the next commits
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 5c25de07cba8..8f3f1ff69fa9 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -250,6 +250,22 @@ static inline void set_restore_sigmask(void)
 	ti->status |= TS_RESTORE_SIGMASK;
 	set_bit(TIF_SIGPENDING, (unsigned long *)&ti->flags);
 }
+static inline void clear_restore_sigmask(void)
+{
+	current_thread_info()->status &= ~TS_RESTORE_SIGMASK;
+}
+static inline bool test_restore_sigmask(void)
+{
+	return current_thread_info()->status & TS_RESTORE_SIGMASK;
+}
+static inline bool test_and_clear_restore_sigmask(void)
+{
+	struct thread_info *ti = current_thread_info();
+	if (!(ti->status & TS_RESTORE_SIGMASK))
+		return false;
+	ti->status &= ~TS_RESTORE_SIGMASK;
+	return true;
+}
 
 static inline bool is_ia32_task(void)
 {

commit 654443e20dfc0617231f28a07c96a979ee1a0239
Merge: 2c01e7bc46f1 9cba26e66d09
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 11:39:34 2012 -0700

    Merge branch 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull user-space probe instrumentation from Ingo Molnar:
     "The uprobes code originates from SystemTap and has been used for years
      in Fedora and RHEL kernels.  This version is much rewritten, reviews
      from PeterZ, Oleg and myself shaped the end result.
    
      This tree includes uprobes support in 'perf probe' - but SystemTap
      (and other tools) can take advantage of user probe points as well.
    
      Sample usage of uprobes via perf, for example to profile malloc()
      calls without modifying user-space binaries.
    
      First boot a new kernel with CONFIG_UPROBE_EVENT=y enabled.
    
      If you don't know which function you want to probe you can pick one
      from 'perf top' or can get a list all functions that can be probed
      within libc (binaries can be specified as well):
    
            $ perf probe -F -x /lib/libc.so.6
    
      To probe libc's malloc():
    
            $ perf probe -x /lib64/libc.so.6 malloc
            Added new event:
            probe_libc:malloc    (on 0x7eac0)
    
      You can now use it in all perf tools, such as:
    
            perf record -e probe_libc:malloc -aR sleep 1
    
      Make use of it to create a call graph (as the flat profile is going to
      look very boring):
    
            $ perf record -e probe_libc:malloc -gR make
            [ perf record: Woken up 173 times to write data ]
            [ perf record: Captured and wrote 44.190 MB perf.data (~1930712
    
            $ perf report | less
    
              32.03%            git  libc-2.15.so   [.] malloc
                                |
                                --- malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                                |
                                --- malloc
                                   |
                                   |--0.95%-- 0x208eb1000000000
                                   |
                                   |--0.63%-- htab_traverse_noresize
    
              11.04%             as  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               7.15%             ld  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               5.07%             sh  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
               4.99%  python-config  libc-2.15.so   [.] malloc
                      |
                      --- malloc
                         |
               4.54%           make  libc-2.15.so   [.] malloc
                               |
                               --- malloc
                                  |
                                  |--7.34%-- glob
                                  |          |
                                  |          |--93.18%-- 0x41588f
                                  |          |
                                  |           --6.82%-- glob
                                  |                     0x41588f
    
               ...
    
      Or:
    
            $ perf report -g flat | less
    
            # Overhead        Command  Shared Object      Symbol
            # ........  .............  .............  ..........
            #
              32.03%            git  libc-2.15.so   [.] malloc
                      27.19%
                          malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                      24.77%
                          malloc
    
              11.04%             as  libc-2.15.so   [.] malloc
                      11.02%
                          malloc
    
               7.15%             ld  libc-2.15.so   [.] malloc
                       6.57%
                          malloc
    
             ...
    
      The core uprobes design is fairly straightforward: uprobes probe
      points register themselves at (inode:offset) addresses of
      libraries/binaries, after which all existing (or new) vmas that map
      that address will have a software breakpoint injected at that address.
      vmas are COW-ed to preserve original content.  The probe points are
      kept in an rbtree.
    
      If user-space executes the probed inode:offset instruction address
      then an event is generated which can be recovered from the regular
      perf event channels and mmap-ed ring-buffer.
    
      Multiple probes at the same address are supported, they create a
      dynamic callback list of event consumers.
    
      The basic model is further complicated by the XOL speedup: the
      original instruction that is probed is copied (in an architecture
      specific fashion) and executed out of line when the probe triggers.
      The XOL area is a single vma per process, with a fixed number of
      entries (which limits probe execution parallelism).
    
      The API: uprobes are installed/removed via
      /sys/kernel/debug/tracing/uprobe_events, the API is integrated to
      align with the kprobes interface as much as possible, but is separate
      to it.
    
      Injecting a probe point is privileged operation, which can be relaxed
      by setting perf_paranoid to -1.
    
      You can use multiple probes as well and mix them with kprobes and
      regular PMU events or tracepoints, when instrumenting a task."
    
    Fix up trivial conflicts in mm/memory.c due to previous cleanup of
    unmap_single_vma().
    
    * 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      perf probe: Detect probe target when m/x options are absent
      perf probe: Provide perf interface for uprobes
      tracing: Fix kconfig warning due to a typo
      tracing: Provide trace events interface for uprobes
      tracing: Extract out common code for kprobes/uprobes trace events
      tracing: Modify is_delete, is_return from int to bool
      uprobes/core: Decrement uprobe count before the pages are unmapped
      uprobes/core: Make background page replacement logic account for rss_stat counters
      uprobes/core: Optimize probe hits with the help of a counter
      uprobes/core: Allocate XOL slots for uprobes use
      uprobes/core: Handle breakpoint and singlestep exceptions
      uprobes/core: Rename bkpt to swbp
      uprobes/core: Make order of function parameters consistent across functions
      uprobes/core: Make macro names consistent
      uprobes: Update copyright notices
      uprobes/core: Move insn to arch specific structure
      uprobes/core: Remove uprobe_opcode_sz
      uprobes/core: Make instruction tables volatile
      uprobes: Move to kernel/events/
      uprobes/core: Clean up, refactor and improve the code
      ...

commit f5c101892fbd3d2f6d2729bc7eb7b3f6c31dbddd
Merge: c54894cd4672 641b695c2f11
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 17:37:47 2012 -0700

    Merge branch 'for-3.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    Pull percpu updates from Tejun Heo:
     "Contains Alex Shi's three patches to remove percpu_xxx() which overlap
      with this_cpu_xxx().  There shouldn't be any functional change."
    
    * 'for-3.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu:
      percpu: remove percpu_xxx() functions
      x86: replace percpu_xxx funcs with this_cpu_xxx
      net: replace percpu_xxx funcs with this_cpu_xxx or __this_cpu_xxx

commit c6ae41e7d469f00d9c92a2b2887c7235d121c009
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri May 11 15:35:27 2012 +0800

    x86: replace percpu_xxx funcs with this_cpu_xxx
    
    Since percpu_xxx() serial functions are duplicated with this_cpu_xxx().
    Removing percpu_xxx() definition and replacing them by this_cpu_xxx()
    in code. There is no function change in this patch, just preparation for
    later percpu_xxx serial function removing.
    
    On x86 machine the this_cpu_xxx() serial functions are same as
    __this_cpu_xxx() without no unnecessary premmpt enable/disable.
    
    Thanks for Stephen Rothwell, he found and fixed a i386 build error in
    the patch.
    
    Also thanks for Andrew Morton, he kept updating the patchset in Linus'
    tree.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Christoph Lameter <cl@gentwo.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ad6df8ccd715..f67fd89c874b 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -222,7 +222,7 @@ DECLARE_PER_CPU(unsigned long, kernel_stack);
 static inline struct thread_info *current_thread_info(void)
 {
 	struct thread_info *ti;
-	ti = (void *)(percpu_read_stable(kernel_stack) +
+	ti = (void *)(this_cpu_read_stable(kernel_stack) +
 		      KERNEL_STACK_OFFSET - THREAD_SIZE);
 	return ti;
 }

commit 38e7c572ce7310def003d8bb7c34260f5d8118cb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:42 2012 +0000

    x86: Use common threadinfo allocator
    
    The only difference is the free_thread_info function, which frees
    xstate.
    
    Use the new arch_release_task_struct() function instead and switch
    over to the core allocator.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120505150141.559556763@linutronix.de
    Cc: x86@kernel.org

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 8692a166dd4e..73cfe0d309c9 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -155,24 +155,6 @@ struct thread_info {
 
 #define PREEMPT_ACTIVE		0x10000000
 
-/* thread information allocation */
-#ifdef CONFIG_DEBUG_STACK_USAGE
-#define THREAD_FLAGS (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO)
-#else
-#define THREAD_FLAGS (GFP_KERNEL | __GFP_NOTRACK)
-#endif
-
-#define __HAVE_ARCH_THREAD_INFO_ALLOCATOR
-
-#define alloc_thread_info_node(tsk, node)				\
-({									\
-	struct page *page = alloc_pages_node(node, THREAD_FLAGS,	\
-					     THREAD_ORDER);		\
-	struct thread_info *ret = page ? page_address(page) : NULL;	\
-									\
-	ret;								\
-})
-
 #ifdef CONFIG_X86_32
 
 #define STACK_WARN	(THREAD_SIZE/8)
@@ -282,7 +264,7 @@ static inline bool is_ia32_task(void)
 
 #ifndef __ASSEMBLY__
 extern void arch_task_cache_init(void);
-extern void free_thread_info(struct thread_info *ti);
 extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
+extern void arch_release_task_struct(struct task_struct *tsk);
 #endif
 #endif /* _ASM_X86_THREAD_INFO_H */

commit 6c0a9fa62feb7e9fdefa9720bcc03040c9b0b311
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:40 2012 +0000

    fork: Remove the weak insanity
    
    We error out when compiling with gcc4.1.[01] as it miscompiles
    __weak. The workaround with magic defines is not longer
    necessary. Make it __weak again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120505150141.306358267@linutronix.de

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ad6df8ccd715..8692a166dd4e 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -284,6 +284,5 @@ static inline bool is_ia32_task(void)
 extern void arch_task_cache_init(void);
 extern void free_thread_info(struct thread_info *ti);
 extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
-#define arch_task_cache_init arch_task_cache_init
 #endif
 #endif /* _ASM_X86_THREAD_INFO_H */

commit 0326f5a94ddea33fa331b2519f4172f4fb387baa
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Mar 13 23:30:11 2012 +0530

    uprobes/core: Handle breakpoint and singlestep exceptions
    
    Uprobes uses exception notifiers to get to know if a thread hit
    a breakpoint or a singlestep exception.
    
    When a thread hits a uprobe or is singlestepping post a uprobe
    hit, the uprobe exception notifier sets its TIF_UPROBE bit,
    which will then be checked on its return to userspace path
    (do_notify_resume() ->uprobe_notify_resume()), where the
    consumers handlers are run (in task context) based on the
    defined filters.
    
    Uprobe hits are thread specific and hence we need to maintain
    information about if a task hit a uprobe, what uprobe was hit,
    the slot where the original instruction was copied for xol so
    that it can be singlestepped with appropriate fixups.
    
    In some cases, special care is needed for instructions that are
    executed out of line (xol). These are architecture specific
    artefacts, such as handling RIP relative instructions on x86_64.
    
    Since the instruction at which the uprobe was inserted is
    executed out of line, architecture specific fixups are added so
    that the thread continues normal execution in the presence of a
    uprobe.
    
    Postpone the signals until we execute the probed insn.
    post_xol() path does a recalc_sigpending() before return to
    user-mode, this ensures the signal can't be lost.
    
    Uprobes relies on DIE_DEBUG notification to notify if a
    singlestep is complete.
    
    Adds x86 specific uprobe exception notifiers and appropriate
    hooks needed to determine a uprobe hit and subsequent post
    processing.
    
    Add requisite x86 fixups for xol for uprobes. Specific cases
    needing fixups include relative jumps (x86_64), calls, etc.
    
    Where possible, we check and skip singlestepping the
    breakpointed instructions. For now we skip single byte as well
    as few multibyte nop instructions. However this can be extended
    to other instructions too.
    
    Credits to Oleg Nesterov for suggestions/patches related to
    signal, breakpoint, singlestep handling code.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120313180011.29771.89027.sendpatchset@srdronam.in.ibm.com
    [ Performed various cleanliness edits ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index ad6df8ccd715..0710c11305d4 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -85,6 +85,7 @@ struct thread_info {
 #define TIF_SECCOMP		8	/* secure computing */
 #define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
+#define TIF_UPROBE		12	/* breakpointed or singlestepping */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
 #define TIF_FORK		18	/* ret_from_fork */
@@ -109,6 +110,7 @@ struct thread_info {
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_MCE_NOTIFY		(1 << TIF_MCE_NOTIFY)
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
+#define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_FORK		(1 << TIF_FORK)

commit ef334a20d84f52407a8a2afd02ddeaecbef0ad3d
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Mar 13 19:33:03 2012 +0530

    x86: Move is_ia32_task to asm/thread_info.h from asm/compat.h
    
    is_ia32_task() is useful even in !CONFIG_COMPAT cases - utrace will
    use it for example. Hence move it to a more generic file: asm/thread_info.h
    
    Also now is_ia32_task() returns true if CONFIG_X86_32 is defined.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120313140303.17134.1401.sendpatchset@srdronam.in.ibm.com
    [ Performed minor cleanup ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index af1db7e722f4..ad6df8ccd715 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -266,6 +266,18 @@ static inline void set_restore_sigmask(void)
 	ti->status |= TS_RESTORE_SIGMASK;
 	set_bit(TIF_SIGPENDING, (unsigned long *)&ti->flags);
 }
+
+static inline bool is_ia32_task(void)
+{
+#ifdef CONFIG_X86_32
+	return true;
+#endif
+#ifdef CONFIG_IA32_EMULATION
+	if (current_thread_info()->status & TS_COMPAT)
+		return true;
+#endif
+	return false;
+}
 #endif	/* !__ASSEMBLY__ */
 
 #ifndef __ASSEMBLY__

commit e898c6706869fdcbd68b1e7fb0ac7461d98710fe
Merge: c94082656dac 55283e253771
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 13 05:54:39 2012 +0100

    Merge branch 'x86/x32' into x86/cleanups
    
    Merge reason: We are going to merge a dependent patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit bb2127240c5595ae4ef7115494f51e973692f64e
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Tue Feb 14 13:56:49 2012 -0800

    x32: Add a thread flag for x32 processes
    
    An x32 process is *almost* the same thing as a 64-bit process with a
    32-bit address limit, but there are a few minor differences -- in
    particular core dumps are 32 bits and signal handling is different.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index d1803a495b35..912e93511466 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -96,6 +96,7 @@ struct thread_info {
 #define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
 #define TIF_SYSCALL_TRACEPOINT	28	/* syscall tracepoint instrumentation */
 #define TIF_ADDR32		29	/* 32-bit address space on 64 bits */
+#define TIF_X32			30	/* 32-bit native x86-64 binary */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -118,6 +119,7 @@ struct thread_info {
 #define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 #define _TIF_ADDR32		(1 << TIF_ADDR32)
+#define _TIF_X32		(1 << TIF_X32)
 
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\

commit 6bd330083e0e97b7ddc053459190bf3d5768ca83
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Feb 6 13:03:09 2012 -0800

    x86: Factor out TIF_IA32 from 32-bit address space
    
    Factor out IA32 (compatibility instruction set) from 32-bit address
    space in the thread_info flags; this is a precondition patch for x32
    support.
    
    Originally-by: H. J. Lu <hjl.tools@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/n/tip-4pr1xnnksprt7t0h3w5fw4rv@git.kernel.org

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index bc817cd8b443..d1803a495b35 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -86,7 +86,7 @@ struct thread_info {
 #define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
-#define TIF_IA32		17	/* 32bit process */
+#define TIF_IA32		17	/* IA32 compatibility process */
 #define TIF_FORK		18	/* ret_from_fork */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_DEBUG		21	/* uses debug registers */
@@ -95,6 +95,7 @@ struct thread_info {
 #define TIF_BLOCKSTEP		25	/* set when we want DEBUGCTLMSR_BTF */
 #define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
 #define TIF_SYSCALL_TRACEPOINT	28	/* syscall tracepoint instrumentation */
+#define TIF_ADDR32		29	/* 32-bit address space on 64 bits */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -116,6 +117,7 @@ struct thread_info {
 #define _TIF_BLOCKSTEP		(1 << TIF_BLOCKSTEP)
 #define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
+#define _TIF_ADDR32		(1 << TIF_ADDR32)
 
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\

commit f94edacf998516ac9d849f7bc6949a703977a7f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 17 21:48:54 2012 -0800

    i387: move TS_USEDFPU flag from thread_info to task_struct
    
    This moves the bit that indicates whether a thread has ownership of the
    FPU from the TS_USEDFPU bit in thread_info->status to a word of its own
    (called 'has_fpu') in task_struct->thread.has_fpu.
    
    This fixes two independent bugs at the same time:
    
     - changing 'thread_info->status' from the scheduler causes nasty
       problems for the other users of that variable, since it is defined to
       be thread-synchronous (that's what the "TS_" part of the naming was
       supposed to indicate).
    
       So perfectly valid code could (and did) do
    
            ti->status |= TS_RESTORE_SIGMASK;
    
       and the compiler was free to do that as separate load, or and store
       instructions.  Which can cause problems with preemption, since a task
       switch could happen in between, and change the TS_USEDFPU bit. The
       change to TS_USEDFPU would be overwritten by the final store.
    
       In practice, this seldom happened, though, because the 'status' field
       was seldom used more than once, so gcc would generally tend to
       generate code that used a read-modify-write instruction and thus
       happened to avoid this problem - RMW instructions are naturally low
       fat and preemption-safe.
    
     - On x86-32, the current_thread_info() pointer would, during interrupts
       and softirqs, point to a *copy* of the real thread_info, because
       x86-32 uses %esp to calculate the thread_info address, and thus the
       separate irq (and softirq) stacks would cause these kinds of odd
       thread_info copy aliases.
    
       This is normally not a problem, since interrupts aren't supposed to
       look at thread information anyway (what thread is running at
       interrupt time really isn't very well-defined), but it confused the
       heck out of irq_fpu_usable() and the code that tried to squirrel
       away the FPU state.
    
       (It also caused untold confusion for us poor kernel developers).
    
    It also turns out that using 'task_struct' is actually much more natural
    for most of the call sites that care about the FPU state, since they
    tend to work with the task struct for other reasons anyway (ie
    scheduling).  And the FPU data that we are going to save/restore is
    found there too.
    
    Thanks to Arjan Van De Ven <arjan@linux.intel.com> for pointing us to
    the %esp issue.
    
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Reported-and-tested-by: Raphael Prevost <raphael@buro.asia>
    Acked-and-tested-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Tested-by: Peter Anvin <hpa@zytor.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index bc817cd8b443..cfd8144d5527 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -247,8 +247,6 @@ static inline struct thread_info *current_thread_info(void)
  * ever touches our thread-synchronous status, so we don't
  * have to worry about atomic accesses.
  */
-#define TS_USEDFPU		0x0001	/* FPU was used by this task
-					   this quantum (SMP) */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
 #define TS_POLLING		0x0004	/* idle task polling need_resched,
 					   skip sending interrupt */

commit bccd17294a26b67a8a19aaa120e3eeaa7da49281
Author: Anton Vorontsov <cbouatmailru@gmail.com>
Date:   Wed Jan 11 05:11:46 2012 +0400

    x86: Get rid of 'dubious one-bit signed bitfield' sprase warning
    
    This very noisy sparse warning appears on almost every file in the
    kernel:
    
      CHECK   init/main.c
      arch/x86/include/asm/thread_info.h:43:55: error: dubious one-bit signed bitfield
      arch/x86/include/asm/thread_info.h:44:46: error: dubious one-bit signed bitfield
    
    This patch changes sig_on_uaccess_error and uaccess_err flags to unsigned
    type and thus fixes the warning.
    
    Signed-off-by: Anton Vorontsov <cbouatmailru@gmail.com>
    Acked-by: Andy Lutomirski <luto@mit.edu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 74047159d0ab..bc817cd8b443 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -40,8 +40,8 @@ struct thread_info {
 						*/
 	__u8			supervisor_stack[0];
 #endif
-	int			sig_on_uaccess_error:1;
-	int			uaccess_err:1;	/* uaccess failed */
+	unsigned int		sig_on_uaccess_error:1;
+	unsigned int		uaccess_err:1;	/* uaccess failed */
 };
 
 #define INIT_THREAD_INFO(tsk)			\

commit eb59c505f8a5906ad2e053d14fab50eb8574fd6f
Merge: 1619ed8f6095 c233523b3d39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 8 13:10:57 2012 -0800

    Merge branch 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    * 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (76 commits)
      PM / Hibernate: Implement compat_ioctl for /dev/snapshot
      PM / Freezer: fix return value of freezable_schedule_timeout_killable()
      PM / shmobile: Allow the A4R domain to be turned off at run time
      PM / input / touchscreen: Make st1232 use device PM QoS constraints
      PM / QoS: Introduce dev_pm_qos_add_ancestor_request()
      PM / shmobile: Remove the stay_on flag from SH7372's PM domains
      PM / shmobile: Don't include SH7372's INTCS in syscore suspend/resume
      PM / shmobile: Add support for the sh7372 A4S power domain / sleep mode
      PM: Drop generic_subsys_pm_ops
      PM / Sleep: Remove forward-only callbacks from AMBA bus type
      PM / Sleep: Remove forward-only callbacks from platform bus type
      PM: Run the driver callback directly if the subsystem one is not there
      PM / Sleep: Make pm_op() and pm_noirq_op() return callback pointers
      PM/Devfreq: Add Exynos4-bus device DVFS driver for Exynos4210/4212/4412.
      PM / Sleep: Merge internal functions in generic_ops.c
      PM / Sleep: Simplify generic system suspend callbacks
      PM / Hibernate: Remove deprecated hibernation snapshot ioctls
      PM / Sleep: Fix freezer failures due to racy usermodehelper_is_disabled()
      ARM: S3C64XX: Implement basic power domain support
      PM / shmobile: Use common always on power domain governor
      ...
    
    Fix up trivial conflict in fs/xfs/xfs_buf.c due to removal of unused
    XBT_FORCE_SLEEP bit

commit 46db09d3fd847f185a7d23a96bc8fe7a4be0cd05
Author: Jan Beulich <JBeulich@suse.com>
Date:   Tue Nov 29 11:17:45 2011 +0000

    x86-64: Slightly shorten line system call entry and exit paths
    
    GET_THREAD_INFO() involves a memory read immediately followed by
    an "sub" on the value read, in turn (in several cases)
    immediately followed by a use of the calculated value as the
    base address of a memory access. This combination of
    instructions has a non-negligible potential for stalls.
    
    In the system call entry point code, however, the (fixed) offset
    of the stack pointer from the end of the stack is generally
    known, and hence we can instead avoid the memory load and
    subtract, and instead do the memory reference using %rsp as the
    base register. To do so in a legible fashion, introduce a
    THREAD_INFO() macro which, provided a register (generally %rsp)
    and the known offset from the end of the stack, produces a
    suitable memory access operand.
    
    The patch attempts to only touch the fast paths (no auditing and
    alike), but manages to do so only in the 64-bit entry point
    case; the compatibility mode entry points have so many
    interdependencies between their various branch targets that it
    was necessary to also adjust the slow paths to eliminate the
    risk of having missed some register dependency during code
    analysis.
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/4ED4CD690200007800064075@nat28.tlf.novell.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 25ebd792725b..185b719ec61a 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -232,6 +232,12 @@ static inline struct thread_info *current_thread_info(void)
 	movq PER_CPU_VAR(kernel_stack),reg ; \
 	subq $(THREAD_SIZE-KERNEL_STACK_OFFSET),reg
 
+/*
+ * Same if PER_CPU_VAR(kernel_stack) is, perhaps with some offset, already in
+ * a certain register (to be used in assembler memory operands).
+ */
+#define THREAD_INFO(reg, off) KERNEL_STACK_OFFSET+(off)-THREAD_SIZE(reg)
+
 #endif
 
 #endif /* !X86_32 */

commit 4fc3490114bb159bd4fff1b3c96f4320fe6fb08f
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon Nov 7 16:33:40 2011 -0800

    x86-64: Set siginfo and context on vsyscall emulation faults
    
    To make this work, we teach the page fault handler how to send
    signals on failed uaccess.  This only works for user addresses
    (kernel addresses will never hit the page fault handler in the
    first place), so we need to generate signals for those
    separately.
    
    This gets the tricky case right: if the user buffer spans
    multiple pages and only the second page is invalid, we set
    cr2 and si_addr correctly.  UML relies on this behavior to
    "fault in" pages as needed.
    
    We steal a bit from thread_info.uaccess_err to enable this.
    Before this change, uaccess_err was a 32-bit boolean value.
    
    This fixes issues with UML when vsyscall=emulate.
    
    Reported-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: richard -rw- weinberger <richard.weinberger@gmail.com>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/4c8f91de7ec5cd2ef0f59521a04e1015f11e42b4.1320712291.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index a1fe5c127b52..25ebd792725b 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -40,7 +40,8 @@ struct thread_info {
 						*/
 	__u8			supervisor_stack[0];
 #endif
-	int			uaccess_err;
+	int			sig_on_uaccess_error:1;
+	int			uaccess_err:1;	/* uaccess failed */
 };
 
 #define INIT_THREAD_INFO(tsk)			\

commit d88e4cb67197d007fb778d62fe17360e970d5bfa
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 21 12:32:25 2011 -0800

    freezer: remove now unused TIF_FREEZE
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linux-arch@vger.kernel.org

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index a1fe5c127b52..32125af20d32 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -90,7 +90,6 @@ struct thread_info {
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_DEBUG		21	/* uses debug registers */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
-#define TIF_FREEZE		23	/* is freezing for suspend */
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_BLOCKSTEP		25	/* set when we want DEBUGCTLMSR_BTF */
 #define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
@@ -112,7 +111,6 @@ struct thread_info {
 #define _TIF_FORK		(1 << TIF_FORK)
 #define _TIF_DEBUG		(1 << TIF_DEBUG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
-#define _TIF_FREEZE		(1 << TIF_FREEZE)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
 #define _TIF_BLOCKSTEP		(1 << TIF_BLOCKSTEP)
 #define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 1f2e61e28981..a1fe5c127b52 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -21,7 +21,7 @@ struct task_struct;
 struct exec_domain;
 #include <asm/processor.h>
 #include <asm/ftrace.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 
 struct thread_info {
 	struct task_struct	*task;		/* main task structure */

commit b6a84016bd2598e35ead635147fa53619982648d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:42 2011 -0700

    mm: NUMA aware alloc_thread_info_node()
    
    Add a node parameter to alloc_thread_info(), and change its name to
    alloc_thread_info_node()
    
    This change is needed to allow NUMA aware kthread_create_on_cpu()
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index f0b6e5dbc5a0..1f2e61e28981 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -161,8 +161,14 @@ struct thread_info {
 
 #define __HAVE_ARCH_THREAD_INFO_ALLOCATOR
 
-#define alloc_thread_info(tsk)						\
-	((struct thread_info *)__get_free_pages(THREAD_FLAGS, THREAD_ORDER))
+#define alloc_thread_info_node(tsk, node)				\
+({									\
+	struct page *page = alloc_pages_node(node, THREAD_FLAGS,	\
+					     THREAD_ORDER);		\
+	struct thread_info *ret = page ? page_address(page) : NULL;	\
+									\
+	ret;								\
+})
 
 #ifdef CONFIG_X86_32
 

commit e4f2e5eaac8f5f903ca4a8cc944d26e68745d6bb
Merge: 9a90e09854a3 2671717265ae
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 28 16:14:17 2010 -0700

    Merge branch 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6
    
    * 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6:
      intel_idle: native hardware cpuidle driver for latest Intel processors
      ACPI: acpi_idle: touch TS_POLLING only in the non-MWAIT case
      acpi_pad: uses MONITOR/MWAIT, so it doesn't need to clear TS_POLLING
      sched: clarify commment for TS_POLLING
      ACPI: allow a native cpuidle driver to displace ACPI
      cpuidle: make cpuidle_curr_driver static
      cpuidle: add cpuidle_unregister_driver() error check
      cpuidle: fail to register if !CONFIG_CPU_IDLE

commit e45b7fa23097332508730123ac6d59227e7bd7f8
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 24 11:34:36 2010 -0400

    sched: clarify commment for TS_POLLING
    
    TS_POLLING set tells the scheduler an idle_task will poll
    need_resched() to look for work.
    
    TS_POLLING clear tells resched_task() and wake_up_idle_cpu()
    that the remote CPU's idle_task is now sleeping in idle,
    and thus requires a reschedule interrupt notice work.
    
    Update the description of TS_POLLING to reflect how it works.
    "idle task polling need_resched, skip sending interrupt"
    
    Wordsmithing-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e0d28901e969..812186919bba 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -241,8 +241,8 @@ static inline struct thread_info *current_thread_info(void)
 #define TS_USEDFPU		0x0001	/* FPU was used by this task
 					   this quantum (SMP) */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
-#define TS_POLLING		0x0004	/* true if in idle loop
-					   and not sleeping */
+#define TS_POLLING		0x0004	/* idle task polling need_resched,
+					   skip sending interrupt */
 #define TS_RESTORE_SIGMASK	0x0008	/* restore signal mask in do_signal() */
 #define TS_XSAVE		0x0010	/* Use xsave/xrstor */
 

commit f39d01be4c59a61a08d0cb53f615e7016b85d339
Merge: 54291263519a 7db82437cfca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 20 09:20:59 2010 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (44 commits)
      vlynq: make whole Kconfig-menu dependant on architecture
      add descriptive comment for TIF_MEMDIE task flag declaration.
      EEPROM: max6875: Header file cleanup
      EEPROM: 93cx6: Header file cleanup
      EEPROM: Header file cleanup
      agp: use NULL instead of 0 when pointer is needed
      rtc-v3020: make bitfield unsigned
      PCI: make bitfield unsigned
      jbd2: use NULL instead of 0 when pointer is needed
      cciss: fix shadows sparse warning
      doc: inode uses a mutex instead of a semaphore.
      uml: i386: Avoid redefinition of NR_syscalls
      fix "seperate" typos in comments
      cocbalt_lcdfb: correct sections
      doc: Change urls for sparse
      Powerpc: wii: Fix typo in comment
      i2o: cleanup some exit paths
      Documentation/: it's -> its where appropriate
      UML: Fix compiler warning due to missing task_struct declaration
      UML: add kernel.h include to signal.c
      ...

commit 41d59102e146a4423a490b8eca68a5860af4fe1c
Merge: 3e1dd193edef c9775b4cc522
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:58:16 2010 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, fpu: Use static_cpu_has() to implement use_xsave()
      x86: Add new static_cpu_has() function using alternatives
      x86, fpu: Use the proper asm constraint in use_xsave()
      x86, fpu: Unbreak FPU emulation
      x86: Introduce 'struct fpu' and related API
      x86: Eliminate TS_XSAVE
      x86-32: Don't set ignore_fpu_irq in simd exception
      x86: Merge kernel_math_error() into math_error()
      x86: Merge simd_math_error() into math_error()
      x86-32: Rework cache flush denied handler
    
    Fix trivial conflict in arch/x86/kernel/process.c

commit 0ddc9324b1a842afd77e8e86698b1d1d2ffed022
Author: Andreas Dilger <adilger@dilger.ca>
Date:   Fri May 14 11:13:27 2010 +0200

    add descriptive comment for TIF_MEMDIE task flag declaration.
    
    Signed-off-by: Andreas Dilger <adilger@dilger.ca>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e0d28901e969..4cc1be269757 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -87,7 +87,7 @@ struct thread_info {
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* 32bit process */
 #define TIF_FORK		18	/* ret_from_fork */
-#define TIF_MEMDIE		20
+#define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_DEBUG		21	/* uses debug registers */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
 #define TIF_FREEZE		23	/* is freezing for suspend */

commit c9ad488289144ae5ef53b012e15895ef1f5e4bb6
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 6 11:45:45 2010 +0300

    x86: Eliminate TS_XSAVE
    
    The fpu code currently uses current->thread_info->status & TS_XSAVE as
    a way to distinguish between XSAVE capable processors and older processors.
    The decision is not really task specific; instead we use the task status to
    avoid a global memory reference - the value should be the same across all
    threads.
    
    Eliminate this tie-in into the task structure by using an alternative
    instruction keyed off the XSAVE cpu feature; this results in shorter and
    faster code, without introducing a global memory reference.
    
    [ hpa: in the future, this probably should use an asm jmp ]
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1273135546-29690-2-git-send-email-avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e0d28901e969..e9e341505ab3 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -244,7 +244,6 @@ static inline struct thread_info *current_thread_info(void)
 #define TS_POLLING		0x0004	/* true if in idle loop
 					   and not sleeping */
 #define TS_RESTORE_SIGMASK	0x0008	/* restore signal mask in do_signal() */
-#define TS_XSAVE		0x0010	/* Use xsave/xrstor */
 
 #define tsk_is_polling(t) (task_thread_info(t)->status & TS_POLLING)
 

commit ea8e61b7bbc4a2faef77db34eb2db2a2c2372ff6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:51 2010 +0100

    x86, ptrace: Fix block-step
    
    Implement ptrace-block-step using TIF_BLOCKSTEP which will set
    DEBUGCTLMSR_BTF when set for a task while preserving any other
    DEBUGCTLMSR bits.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100325135414.017536066@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index dc85e12d1405..d017ed5502e2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -92,6 +92,7 @@ struct thread_info {
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
 #define TIF_FREEZE		23	/* is freezing for suspend */
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
+#define TIF_BLOCKSTEP		25	/* set when we want DEBUGCTLMSR_BTF */
 #define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
 #define TIF_SYSCALL_TRACEPOINT	28	/* syscall tracepoint instrumentation */
 
@@ -113,6 +114,7 @@ struct thread_info {
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
+#define _TIF_BLOCKSTEP		(1 << TIF_BLOCKSTEP)
 #define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 
@@ -143,7 +145,7 @@ struct thread_info {
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
-	(_TIF_IO_BITMAP|_TIF_NOTSC)
+	(_TIF_IO_BITMAP|_TIF_NOTSC|_TIF_BLOCKSTEP)
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e0d28901e969..dc85e12d1405 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -92,8 +92,6 @@ struct thread_info {
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
 #define TIF_FREEZE		23	/* is freezing for suspend */
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
-#define TIF_DEBUGCTLMSR		25	/* uses thread_struct.debugctlmsr */
-#define TIF_DS_AREA_MSR		26      /* uses thread_struct.ds_area_msr */
 #define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
 #define TIF_SYSCALL_TRACEPOINT	28	/* syscall tracepoint instrumentation */
 
@@ -115,8 +113,6 @@ struct thread_info {
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
-#define _TIF_DEBUGCTLMSR	(1 << TIF_DEBUGCTLMSR)
-#define _TIF_DS_AREA_MSR	(1 << TIF_DS_AREA_MSR)
 #define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 
@@ -147,7 +143,7 @@ struct thread_info {
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
-	(_TIF_IO_BITMAP|_TIF_DEBUGCTLMSR|_TIF_DS_AREA_MSR|_TIF_NOTSC)
+	(_TIF_IO_BITMAP|_TIF_NOTSC)
 
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)

commit 05d43ed8a89c159ff641d472f970e3f1baa66318
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jan 28 22:14:43 2010 -0800

    x86: get rid of the insane TIF_ABI_PENDING bit
    
    Now that the previous commit made it possible to do the personality
    setting at the point of no return, we do just that for ELF binaries.
    And suddenly all the reasons for that insane TIF_ABI_PENDING bit go
    away, and we can just make SET_PERSONALITY() just do the obvious thing
    for a 32-bit compat process.
    
    Everything becomes much more straightforward this way.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 375c917c37d2..e0d28901e969 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -87,7 +87,6 @@ struct thread_info {
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* 32bit process */
 #define TIF_FORK		18	/* ret_from_fork */
-#define TIF_ABI_PENDING		19
 #define TIF_MEMDIE		20
 #define TIF_DEBUG		21	/* uses debug registers */
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
@@ -112,7 +111,6 @@ struct thread_info {
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_FORK		(1 << TIF_FORK)
-#define _TIF_ABI_PENDING	(1 << TIF_ABI_PENDING)
 #define _TIF_DEBUG		(1 << TIF_DEBUG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)

commit 7c68af6e32c73992bad24107311f3433c89016e2
Author: Avi Kivity <avi@redhat.com>
Date:   Sat Sep 19 09:40:22 2009 +0300

    core, x86: Add user return notifiers
    
    Add a general per-cpu notifier that is called whenever the kernel is
    about to return to userspace.  The notifier uses a thread_info flag
    and existing checks, so there is no impact on user return or context
    switch fast paths.
    
    This will be used initially to speed up KVM task switching by lazily
    updating MSRs.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    LKML-Reference: <1253342422-13811-1-git-send-email-avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index d27d0a2fec4c..375c917c37d2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -83,6 +83,7 @@ struct thread_info {
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
 #define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
+#define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* 32bit process */
 #define TIF_FORK		18	/* ret_from_fork */
@@ -107,6 +108,7 @@ struct thread_info {
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_MCE_NOTIFY		(1 << TIF_MCE_NOTIFY)
+#define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
 #define _TIF_FORK		(1 << TIF_FORK)
@@ -142,13 +144,14 @@ struct thread_info {
 
 /* Only used for 64 bit */
 #define _TIF_DO_NOTIFY_MASK						\
-	(_TIF_SIGPENDING|_TIF_MCE_NOTIFY|_TIF_NOTIFY_RESUME)
+	(_TIF_SIGPENDING | _TIF_MCE_NOTIFY | _TIF_NOTIFY_RESUME |	\
+	 _TIF_USER_RETURN_NOTIFY)
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
 	(_TIF_IO_BITMAP|_TIF_DEBUGCTLMSR|_TIF_DS_AREA_MSR|_TIF_NOTSC)
 
-#define _TIF_WORK_CTXSW_PREV _TIF_WORK_CTXSW
+#define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)
 
 #define PREEMPT_ACTIVE		0x10000000

commit 55e0715f612f19b44c17497929091df2f3357e5d
Merge: 7dfd54a905be bdf977b37418
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 08:01:28 2009 -0700

    Merge branch 'x86-percpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-percpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, percpu: Collect hot percpu variables into one cacheline
      x86, percpu: Fix DECLARE/DEFINE_PER_CPU_PAGE_ALIGNED()
      x86, percpu: Add 'percpu_read_stable()' interface for cacheable accesses

commit 667000011927b4fcc359beac4a2447889db6d349
Author: Josh Stone <jistone@redhat.com>
Date:   Mon Aug 24 14:43:11 2009 -0700

    tracing: Rename FTRACE_SYSCALLS for tracepoints
    
    s/HAVE_FTRACE_SYSCALLS/HAVE_SYSCALL_TRACEPOINTS/g
    s/TIF_SYSCALL_FTRACE/TIF_SYSCALL_TRACEPOINT/g
    
    The syscall enter/exit tracing is no longer specific to just ftrace, so
    they now have names that reflect their tie to tracepoints instead.
    
    Signed-off-by: Josh Stone <jistone@redhat.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    LKML-Reference: <1251150194-1713-2-git-send-email-jistone@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index fad7d40b75f8..6f7786aea4fc 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -95,7 +95,7 @@ struct thread_info {
 #define TIF_DEBUGCTLMSR		25	/* uses thread_struct.debugctlmsr */
 #define TIF_DS_AREA_MSR		26      /* uses thread_struct.ds_area_msr */
 #define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
-#define TIF_SYSCALL_FTRACE	28	/* for ftrace syscall instrumentation */
+#define TIF_SYSCALL_TRACEPOINT	28	/* syscall tracepoint instrumentation */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -118,17 +118,17 @@ struct thread_info {
 #define _TIF_DEBUGCTLMSR	(1 << TIF_DEBUGCTLMSR)
 #define _TIF_DS_AREA_MSR	(1 << TIF_DS_AREA_MSR)
 #define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)
-#define _TIF_SYSCALL_FTRACE	(1 << TIF_SYSCALL_FTRACE)
+#define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\
-	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_FTRACE |	\
-	 _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | _TIF_SINGLESTEP)
+	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_AUDIT |	\
+	 _TIF_SECCOMP | _TIF_SINGLESTEP | _TIF_SYSCALL_TRACEPOINT)
 
 /* work to do in syscall_trace_leave() */
 #define _TIF_WORK_SYSCALL_EXIT	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP |	\
-	 _TIF_SYSCALL_FTRACE)
+	 _TIF_SYSCALL_TRACEPOINT)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK							\
@@ -137,7 +137,8 @@ struct thread_info {
 	   _TIF_SINGLESTEP|_TIF_SECCOMP|_TIF_SYSCALL_EMU))
 
 /* work to do on any return to user space */
-#define _TIF_ALLWORK_MASK ((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_FTRACE)
+#define _TIF_ALLWORK_MASK						\
+	((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_TRACEPOINT)
 
 /* Only used for 64 bit */
 #define _TIF_DO_NOTIFY_MASK						\

commit ed8d9adf357ec331603fa1049510399812cea7e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 3 14:08:48 2009 +0900

    x86, percpu: Add 'percpu_read_stable()' interface for cacheable accesses
    
    This is very useful for some common things like 'get_current()' and
    'get_thread_info()', which can be used multiple times in a function, and
    where the result is cacheable.
    
    tj: Added the magical undocumented "P" modifier to UP __percpu_arg()
        to force gcc to dereference the pointer value passed in via the
        "p" input constraint.  Without this, percpu_read_stable() returns
        the address of the percpu variable.  Also added comment explaining
        the difference between percpu_read() and percpu_read_stable().
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index fad7d40b75f8..a1bb5a114bf2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -213,7 +213,7 @@ DECLARE_PER_CPU(unsigned long, kernel_stack);
 static inline struct thread_info *current_thread_info(void)
 {
 	struct thread_info *ti;
-	ti = (void *)(percpu_read(kernel_stack) +
+	ti = (void *)(percpu_read_stable(kernel_stack) +
 		      KERNEL_STACK_OFFSET - THREAD_SIZE);
 	return ti;
 }

commit c99e6efe1ba04561e7d93a81f0be07e37427e835
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jul 10 14:57:56 2009 +0200

    sched: INIT_PREEMPT_COUNT
    
    Pull the initial preempt_count value into a single
    definition site.
    
    Maintainers for: alpha, ia64 and m68k, please have a look,
    your arch code is funny.
    
    The header magic is a bit odd, but similar to the KERNEL_DS
    one, CPP waits with expanding these macros until the
    INIT_THREAD_INFO macro itself is expanded, which is in
    arch/*/kernel/init_task.c where we've already included
    sched.h so we're good.
    
    Cc: tony.luck@intel.com
    Cc: rth@twiddle.net
    Cc: geert@linux-m68k.org
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index b0783520988b..fad7d40b75f8 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -49,7 +49,7 @@ struct thread_info {
 	.exec_domain	= &default_exec_domain,	\
 	.flags		= 0,			\
 	.cpu		= 0,			\
-	.preempt_count	= 1,			\
+	.preempt_count	= INIT_PREEMPT_COUNT,	\
 	.addr_limit	= KERNEL_DS,		\
 	.restart_block = {			\
 		.fn = do_no_restart_syscall,	\

commit b1eeab67682a5e397aecf172046b3a8bd4808ae4
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Tue Nov 25 16:55:53 2008 +0100

    kmemcheck: add hooks for the page allocator
    
    This adds support for tracking the initializedness of memory that
    was allocated with the page allocator. Highmem requests are not
    tracked.
    
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    
    [build fix for !CONFIG_KMEMCHECK]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    [rebased for mainline inclusion]
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 602c769fc98c..b0783520988b 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -154,9 +154,9 @@ struct thread_info {
 
 /* thread information allocation */
 #ifdef CONFIG_DEBUG_STACK_USAGE
-#define THREAD_FLAGS (GFP_KERNEL | __GFP_ZERO)
+#define THREAD_FLAGS (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO)
 #else
-#define THREAD_FLAGS GFP_KERNEL
+#define THREAD_FLAGS (GFP_KERNEL | __GFP_NOTRACK)
 #endif
 
 #define __HAVE_ARCH_THREAD_INFO_ALLOCATOR

commit 38f4b8c0da01ae7cd9b93386842ce272d6fde9ab
Merge: a81145402735 8e2c4f2844c0
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Apr 7 13:34:16 2009 -0700

    Merge commit 'origin/master' into for-linus/xen/master
    
    * commit 'origin/master': (4825 commits)
      Fix build errors due to CONFIG_BRANCH_TRACER=y
      parport: Use the PCI IRQ if offered
      tty: jsm cleanups
      Adjust path to gpio headers
      KGDB_SERIAL_CONSOLE check for module
      Change KCONFIG name
      tty: Blackin CTS/RTS
      Change hardware flow control from poll to interrupt driven
      Add support for the MAX3100 SPI UART.
      lanana: assign a device name and numbering for MAX3100
      serqt: initial clean up pass for tty side
      tty: Use the generic RS485 ioctl on CRIS
      tty: Correct inline types for tty_driver_kref_get()
      splice: fix deadlock in splicing to file
      nilfs2: support nanosecond timestamp
      nilfs2: introduce secondary super block
      nilfs2: simplify handling of active state of segments
      nilfs2: mark minor flag for checkpoint created by internal operation
      nilfs2: clean up sketch file
      nilfs2: super block operations fix endian bug
      ...
    
    Conflicts:
            arch/x86/include/asm/thread_info.h
            arch/x86/lguest/boot.c
            drivers/xen/manage.c

commit b407fc57b815b2016186220baabc76cc8264206e
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Feb 17 23:46:21 2009 -0800

    x86/paravirt: flush pending mmu updates on context switch
    
    Impact: allow preemption during lazy mmu updates
    
    If we're in lazy mmu mode when context switching, leave
    lazy mmu mode, but remember the task's state in
    TIF_LAZY_MMU_UPDATES.  When we resume the task, check this
    flag and re-enter lazy mmu mode if its set.
    
    This sets things up for allowing lazy mmu mode while preemptible,
    though that won't actually be active until the next change.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index df9d5f78385e..2f34d643b567 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -94,6 +94,7 @@ struct thread_info {
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_DEBUGCTLMSR		25	/* uses thread_struct.debugctlmsr */
 #define TIF_DS_AREA_MSR		26      /* uses thread_struct.ds_area_msr */
+#define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -115,6 +116,7 @@ struct thread_info {
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
 #define _TIF_DEBUGCTLMSR	(1 << TIF_DEBUGCTLMSR)
 #define _TIF_DS_AREA_MSR	(1 << TIF_DS_AREA_MSR)
+#define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)
 
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\

commit 1b3fa2ce64363c289b3b14723cca7290bf91cfce
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Mar 7 05:53:00 2009 +0100

    tracing/x86: basic implementation of syscall tracing for x86
    
    Provide the x86 trace callbacks to trace syscalls.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <1236401580-5758-3-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index df9d5f78385e..8820a73ae090 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -94,6 +94,7 @@ struct thread_info {
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_DEBUGCTLMSR		25	/* uses thread_struct.debugctlmsr */
 #define TIF_DS_AREA_MSR		26      /* uses thread_struct.ds_area_msr */
+#define TIF_SYSCALL_FTRACE	27	/* for ftrace syscall instrumentation */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -115,15 +116,17 @@ struct thread_info {
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
 #define _TIF_DEBUGCTLMSR	(1 << TIF_DEBUGCTLMSR)
 #define _TIF_DS_AREA_MSR	(1 << TIF_DS_AREA_MSR)
+#define _TIF_SYSCALL_FTRACE	(1 << TIF_SYSCALL_FTRACE)
 
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\
-	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | \
+	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_FTRACE |	\
 	 _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | _TIF_SINGLESTEP)
 
 /* work to do in syscall_trace_leave() */
 #define _TIF_WORK_SYSCALL_EXIT	\
-	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP)
+	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP |	\
+	 _TIF_SYSCALL_FTRACE)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK							\
@@ -132,7 +135,7 @@ struct thread_info {
 	   _TIF_SINGLESTEP|_TIF_SECCOMP|_TIF_SYSCALL_EMU))
 
 /* work to do on any return to user space */
-#define _TIF_ALLWORK_MASK (0x0000FFFF & ~_TIF_SECCOMP)
+#define _TIF_ALLWORK_MASK ((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_FTRACE)
 
 /* Only used for 64 bit */
 #define _TIF_DO_NOTIFY_MASK						\

commit 5d96218b4a5ee0c5ff0ac87f3ba90cfa86ca0ca1
Merge: 249d51b53aea 18114f61359a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 10 00:40:48 2009 +0100

    Merge branch 'x86/uaccess' into core/percpu

commit fe40c0af3cff3ea461cf25bddb979abc7279d4df
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Fri Jan 23 15:49:41 2009 -0800

    x86: uaccess: introduce try and catch framework
    
    Impact: introduce new uaccess exception handling framework
    
    Introduce {get|put}_user_try and {get|put}_user_catch as new uaccess exception
    handling framework.
    {get|put}_user_try begins exception block and {get|put}_user_catch(err) ends
    the block and gets err if an exception occured in {get|put}_user_ex() in the
    block. The exception is stored thread_info->uaccess_err.
    
    The example usage of this framework is below;
    int func()
    {
            int err = 0;
    
            get_user_try {
                    get_user_ex(...);
                    get_user_ex(...);
                    :
            } get_user_catch(err);
    
            return err;
    }
    
    Note: get_user_ex() is not clear the value when an exception occurs, it's
    different from the behavior of __get_user(), but I think it doesn't matter.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 98789647baa9..3f90aeb456bc 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -40,6 +40,7 @@ struct thread_info {
 						*/
 	__u8			supervisor_stack[0];
 #endif
+	int			uaccess_err;
 };
 
 #define INIT_THREAD_INFO(tsk)			\

commit 9af45651f1f7c89942e016a1a00a7ebddfa727f8
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move kernelstack from PDA to per-cpu.
    
    Also clean up PER_CPU_VAR usage in xen-asm_64.S
    
    tj: * remove now unused stack_thread_info()
        * s/kernelstack/kernel_stack/
        * added FIXME comment in xen-asm_64.S
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 98789647baa9..b46f8ca007b5 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -194,25 +194,21 @@ static inline struct thread_info *current_thread_info(void)
 
 #else /* X86_32 */
 
-#include <asm/pda.h>
+#include <asm/percpu.h>
+#define KERNEL_STACK_OFFSET (5*8)
 
 /*
  * macros/functions for gaining access to the thread information structure
  * preempt_count needs to be 1 initially, until the scheduler is functional.
  */
 #ifndef __ASSEMBLY__
-static inline struct thread_info *current_thread_info(void)
-{
-	struct thread_info *ti;
-	ti = (void *)(read_pda(kernelstack) + PDA_STACKOFFSET - THREAD_SIZE);
-	return ti;
-}
+DECLARE_PER_CPU(unsigned long, kernel_stack);
 
-/* do not use in interrupt context */
-static inline struct thread_info *stack_thread_info(void)
+static inline struct thread_info *current_thread_info(void)
 {
 	struct thread_info *ti;
-	asm("andq %%rsp,%0; " : "=r" (ti) : "0" (~(THREAD_SIZE - 1)));
+	ti = (void *)(percpu_read(kernel_stack) +
+		      KERNEL_STACK_OFFSET - THREAD_SIZE);
 	return ti;
 }
 
@@ -220,8 +216,8 @@ static inline struct thread_info *stack_thread_info(void)
 
 /* how to get the thread information struct from ASM */
 #define GET_THREAD_INFO(reg) \
-	movq %gs:pda_kernelstack,reg ; \
-	subq $(THREAD_SIZE-PDA_STACKOFFSET),reg
+	movq PER_CPU_VAR(kernel_stack),reg ; \
+	subq $(THREAD_SIZE-KERNEL_STACK_OFFSET),reg
 
 #endif
 

commit b0f4b285d7ed174804658539129a834270f4829a
Merge: be9c5ae4eeec 5250d329e38c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:21:10 2008 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (241 commits)
      sched, trace: update trace_sched_wakeup()
      tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
      Revert "x86: disable X86_PTRACE_BTS"
      ring-buffer: prevent false positive warning
      ring-buffer: fix dangling commit race
      ftrace: enable format arguments checking
      x86, bts: memory accounting
      x86, bts: add fork and exit handling
      ftrace: introduce tracing_reset_online_cpus() helper
      tracing: fix warnings in kernel/trace/trace_sched_switch.c
      tracing: fix warning in kernel/trace/trace.c
      tracing/ring-buffer: remove unused ring_buffer size
      trace: fix task state printout
      ftrace: add not to regex on filtering functions
      trace: better use of stack_trace_enabled for boot up code
      trace: add a way to enable or disable the stack tracer
      x86: entry_64 - introduce FTRACE_ frame macro v2
      tracing/ftrace: add the printk-msg-only option
      tracing/ftrace: use preempt_enable_no_resched_notrace in ring_buffer_time_stamp()
      x86, bts: correctly report invalid bts records
      ...
    
    Fixed up trivial conflict in scripts/recordmcount.pl due to SH bits
    being already partly merged by the SH merge.

commit c2724775ce57c98b8af9694857b941dc61056516
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Thu Dec 11 13:49:59 2008 +0100

    x86, bts: provide in-kernel branch-trace interface
    
    Impact: cleanup
    
    Move the BTS bits from ptrace.c into ds.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 0921b4018c11..bf8113d16a33 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -93,7 +93,6 @@ struct thread_info {
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_DEBUGCTLMSR		25	/* uses thread_struct.debugctlmsr */
 #define TIF_DS_AREA_MSR		26      /* uses thread_struct.ds_area_msr */
-#define TIF_BTS_TRACE_TS	27      /* record scheduling event timestamps */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
@@ -115,7 +114,6 @@ struct thread_info {
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
 #define _TIF_DEBUGCTLMSR	(1 << TIF_DEBUGCTLMSR)
 #define _TIF_DS_AREA_MSR	(1 << TIF_DS_AREA_MSR)
-#define _TIF_BTS_TRACE_TS	(1 << TIF_BTS_TRACE_TS)
 
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\
@@ -141,8 +139,7 @@ struct thread_info {
 
 /* flags to check in __switch_to() */
 #define _TIF_WORK_CTXSW							\
-	(_TIF_IO_BITMAP|_TIF_DEBUGCTLMSR|_TIF_DS_AREA_MSR|_TIF_BTS_TRACE_TS| \
-								_TIF_NOTSC)
+	(_TIF_IO_BITMAP|_TIF_DEBUGCTLMSR|_TIF_DS_AREA_MSR|_TIF_NOTSC)
 
 #define _TIF_WORK_CTXSW_PREV _TIF_WORK_CTXSW
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)

commit affa219b60a11b3295637a97f5b1b8ef231490fc
Author: Joe Korty <joe.korty@ccur.com>
Date:   Wed Dec 3 18:58:19 2008 -0500

    x86: change thread_info's flag field back to 32 bits
    
    Impact: pack struct thread_info more tightly
    
    Change x86_64's thread_info 'flags' field back to __u32.
    
    This was changed to 'unsigned long' when the thread_info*.h
    for i386 and x86_64 were merged.  Change it back.  We can
    do this as only 27 bits of 'flags' are actually used.
    
    This change actually packs down thread_info by 64 bits:
    32 bits are saved by the smaller flags, and 32 bits are
    saved by the following 'mm_segment_t field' becoming
    naturally 64-bit aligned.
    
    Signed-off-by: Joe Korty <joe.korty@ccur.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e44d379faad2..8dbc57390d25 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -24,7 +24,7 @@ struct exec_domain;
 struct thread_info {
 	struct task_struct	*task;		/* main task structure */
 	struct exec_domain	*exec_domain;	/* execution domain */
-	unsigned long		flags;		/* low level flags */
+	__u32			flags;		/* low level flags */
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
 	int			preempt_count;	/* 0 => preemptable,

commit f201ae2356c74bcae130b2177b3dca903ea98071
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 06:22:56 2008 +0100

    tracing/function-return-tracer: store return stack into task_struct and allocate it dynamically
    
    Impact: use deeper function tracing depth safely
    
    Some tests showed that function return tracing needed a more deeper depth
    of function calls. But it could be unsafe to store these return addresses
    to the stack.
    
    So these arrays will now be allocated dynamically into task_struct of current
    only when the tracer is activated.
    
    Typical scheme when tracer is activated:
    - allocate a return stack for each task in global list.
    - fork: allocate the return stack for the newly created task
    - exit: free return stack of current
    - idle init: same as fork
    
    I chose a default depth of 50. I don't have overruns anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e90e81ef6ab9..0921b4018c11 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -40,36 +40,8 @@ struct thread_info {
 						*/
 	__u8			supervisor_stack[0];
 #endif
-
-#ifdef CONFIG_FUNCTION_RET_TRACER
-	/* Index of current stored adress in ret_stack */
-	int		curr_ret_stack;
-	/* Stack of return addresses for return function tracing */
-	struct ftrace_ret_stack	ret_stack[FTRACE_RET_STACK_SIZE];
-	/*
-	 * Number of functions that haven't been traced
-	 * because of depth overrun.
-	 */
-	atomic_t	trace_overrun;
-#endif
 };
 
-#ifdef CONFIG_FUNCTION_RET_TRACER
-#define INIT_THREAD_INFO(tsk)			\
-{						\
-	.task		= &tsk,			\
-	.exec_domain	= &default_exec_domain,	\
-	.flags		= 0,			\
-	.cpu		= 0,			\
-	.preempt_count	= 1,			\
-	.addr_limit	= KERNEL_DS,		\
-	.restart_block = {			\
-		.fn = do_no_restart_syscall,	\
-	},					\
-	.curr_ret_stack = -1,\
-	.trace_overrun	= ATOMIC_INIT(0)	\
-}
-#else
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.task		= &tsk,			\
@@ -82,7 +54,6 @@ struct thread_info {
 		.fn = do_no_restart_syscall,	\
 	},					\
 }
-#endif
 
 #define init_thread_info	(init_thread_union.thread_info)
 #define init_stack		(init_thread_union.stack)

commit 0231022cc32d5f2e7f3c06b75691dda0ad6aec33
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Nov 17 03:22:41 2008 +0100

    tracing/function-return-tracer: add the overrun field
    
    Impact: help to find the better depth of trace
    
    We decided to arbitrary define the depth of function return trace as
    "20". Perhaps this is not enough. To help finding an optimal depth, we
    measure now the overrun: the number of functions that have been missed
    for the current thread. By default this is not displayed, we have to
    do set a particular flag on the return tracer: echo overrun >
    /debug/tracing/trace_options And the overrun will be printed on the
    right.
    
    As the trace shows below, the current 20 depth is not enough.
    
    update_wall_time+0x37f/0x8c0 -> update_xtime_cache (345 ns) (Overruns: 2838)
    update_wall_time+0x384/0x8c0 -> clocksource_get_next (1141 ns) (Overruns: 2838)
    do_timer+0x23/0x100 -> update_wall_time (3882 ns) (Overruns: 2838)
    tick_do_update_jiffies64+0xbf/0x160 -> do_timer (5339 ns) (Overruns: 2838)
    tick_sched_timer+0x6a/0xf0 -> tick_do_update_jiffies64 (7209 ns) (Overruns: 2838)
    vgacon_set_cursor_size+0x98/0x120 -> native_io_delay (2613 ns) (Overruns: 274)
    vgacon_cursor+0x16e/0x1d0 -> vgacon_set_cursor_size (33151 ns) (Overruns: 274)
    set_cursor+0x5f/0x80 -> vgacon_cursor (36432 ns) (Overruns: 274)
    con_flush_chars+0x34/0x40 -> set_cursor (38790 ns) (Overruns: 274)
    release_console_sem+0x1ec/0x230 -> up (721 ns) (Overruns: 274)
    release_console_sem+0x225/0x230 -> wake_up_klogd (316 ns) (Overruns: 274)
    con_flush_chars+0x39/0x40 -> release_console_sem (2996 ns) (Overruns: 274)
    con_write+0x22/0x30 -> con_flush_chars (46067 ns) (Overruns: 274)
    n_tty_write+0x1cc/0x360 -> con_write (292670 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2a/0x90 -> native_apic_mem_write (330 ns) (Overruns: 274)
    irq_enter+0x17/0x70 -> idle_cpu (413 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2f/0x90 -> irq_enter (1525 ns) (Overruns: 274)
    ktime_get_ts+0x40/0x70 -> getnstimeofday (465 ns) (Overruns: 274)
    ktime_get_ts+0x60/0x70 -> set_normalized_timespec (436 ns) (Overruns: 274)
    ktime_get+0x16/0x30 -> ktime_get_ts (2501 ns) (Overruns: 274)
    hrtimer_interrupt+0x77/0x1a0 -> ktime_get (3439 ns) (Overruns: 274)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index a71158369fd4..e90e81ef6ab9 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -21,6 +21,7 @@ struct task_struct;
 struct exec_domain;
 #include <asm/processor.h>
 #include <asm/ftrace.h>
+#include <asm/atomic.h>
 
 struct thread_info {
 	struct task_struct	*task;		/* main task structure */
@@ -45,6 +46,11 @@ struct thread_info {
 	int		curr_ret_stack;
 	/* Stack of return addresses for return function tracing */
 	struct ftrace_ret_stack	ret_stack[FTRACE_RET_STACK_SIZE];
+	/*
+	 * Number of functions that haven't been traced
+	 * because of depth overrun.
+	 */
+	atomic_t	trace_overrun;
 #endif
 };
 
@@ -61,6 +67,7 @@ struct thread_info {
 		.fn = do_no_restart_syscall,	\
 	},					\
 	.curr_ret_stack = -1,\
+	.trace_overrun	= ATOMIC_INIT(0)	\
 }
 #else
 #define INIT_THREAD_INFO(tsk)			\

commit caf4b323b02a16c92fba449952ac6515ddc76d7a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 07:03:45 2008 +0100

    tracing, x86: add low level support for ftrace return tracing
    
    Impact: add infrastructure for function-return tracing
    
    Add low level support for ftrace return tracing.
    
    This plug-in stores return addresses on the thread_info structure of
    the current task.
    
    The index of the current return address is initialized when the task
    is the first one (init) and when a process forks (the child). It is
    not needed when a task does a sys_execve because after this syscall,
    it still needs to return on the kernel functions it called.
    
    Note that the code of return_to_handler has been suggested by Steven
    Rostedt as almost all of the ideas of improvements in this V3.
    
    For purpose of security, arch/x86/kernel/process_32.c is not traced
    because __switch_to() changes the current task during its execution.
    That could cause inconsistency in the stored return address of this
    function even if I didn't have any crash after testing with tracing on
    this function enabled.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index e44d379faad2..a71158369fd4 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -20,6 +20,7 @@
 struct task_struct;
 struct exec_domain;
 #include <asm/processor.h>
+#include <asm/ftrace.h>
 
 struct thread_info {
 	struct task_struct	*task;		/* main task structure */
@@ -38,8 +39,30 @@ struct thread_info {
 						*/
 	__u8			supervisor_stack[0];
 #endif
+
+#ifdef CONFIG_FUNCTION_RET_TRACER
+	/* Index of current stored adress in ret_stack */
+	int		curr_ret_stack;
+	/* Stack of return addresses for return function tracing */
+	struct ftrace_ret_stack	ret_stack[FTRACE_RET_STACK_SIZE];
+#endif
 };
 
+#ifdef CONFIG_FUNCTION_RET_TRACER
+#define INIT_THREAD_INFO(tsk)			\
+{						\
+	.task		= &tsk,			\
+	.exec_domain	= &default_exec_domain,	\
+	.flags		= 0,			\
+	.cpu		= 0,			\
+	.preempt_count	= 1,			\
+	.addr_limit	= KERNEL_DS,		\
+	.restart_block = {			\
+		.fn = do_no_restart_syscall,	\
+	},					\
+	.curr_ret_stack = -1,\
+}
+#else
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.task		= &tsk,			\
@@ -52,6 +75,7 @@ struct thread_info {
 		.fn = do_no_restart_syscall,	\
 	},					\
 }
+#endif
 
 #define init_thread_info	(init_thread_union.thread_info)
 #define init_stack		(init_thread_union.stack)

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 3f4e52bb77f5..e44d379faad2 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -4,8 +4,8 @@
  * - Incorporating suggestions made by Linus Torvalds and Dave Miller
  */
 
-#ifndef ASM_X86__THREAD_INFO_H
-#define ASM_X86__THREAD_INFO_H
+#ifndef _ASM_X86_THREAD_INFO_H
+#define _ASM_X86_THREAD_INFO_H
 
 #include <linux/compiler.h>
 #include <asm/page.h>
@@ -261,4 +261,4 @@ extern void free_thread_info(struct thread_info *ti);
 extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
 #define arch_task_cache_init arch_task_cache_init
 #endif
-#endif /* ASM_X86__THREAD_INFO_H */
+#endif /* _ASM_X86_THREAD_INFO_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
new file mode 100644
index 000000000000..3f4e52bb77f5
--- /dev/null
+++ b/arch/x86/include/asm/thread_info.h
@@ -0,0 +1,264 @@
+/* thread_info.h: low-level thread information
+ *
+ * Copyright (C) 2002  David Howells (dhowells@redhat.com)
+ * - Incorporating suggestions made by Linus Torvalds and Dave Miller
+ */
+
+#ifndef ASM_X86__THREAD_INFO_H
+#define ASM_X86__THREAD_INFO_H
+
+#include <linux/compiler.h>
+#include <asm/page.h>
+#include <asm/types.h>
+
+/*
+ * low level task data that entry.S needs immediate access to
+ * - this struct should fit entirely inside of one cache line
+ * - this struct shares the supervisor stack pages
+ */
+#ifndef __ASSEMBLY__
+struct task_struct;
+struct exec_domain;
+#include <asm/processor.h>
+
+struct thread_info {
+	struct task_struct	*task;		/* main task structure */
+	struct exec_domain	*exec_domain;	/* execution domain */
+	unsigned long		flags;		/* low level flags */
+	__u32			status;		/* thread synchronous flags */
+	__u32			cpu;		/* current CPU */
+	int			preempt_count;	/* 0 => preemptable,
+						   <0 => BUG */
+	mm_segment_t		addr_limit;
+	struct restart_block    restart_block;
+	void __user		*sysenter_return;
+#ifdef CONFIG_X86_32
+	unsigned long           previous_esp;   /* ESP of the previous stack in
+						   case of nested (IRQ) stacks
+						*/
+	__u8			supervisor_stack[0];
+#endif
+};
+
+#define INIT_THREAD_INFO(tsk)			\
+{						\
+	.task		= &tsk,			\
+	.exec_domain	= &default_exec_domain,	\
+	.flags		= 0,			\
+	.cpu		= 0,			\
+	.preempt_count	= 1,			\
+	.addr_limit	= KERNEL_DS,		\
+	.restart_block = {			\
+		.fn = do_no_restart_syscall,	\
+	},					\
+}
+
+#define init_thread_info	(init_thread_union.thread_info)
+#define init_stack		(init_thread_union.stack)
+
+#else /* !__ASSEMBLY__ */
+
+#include <asm/asm-offsets.h>
+
+#endif
+
+/*
+ * thread information flags
+ * - these are process state flags that various assembly files
+ *   may need to access
+ * - pending work-to-be-done flags are in LSW
+ * - other flags in MSW
+ * Warning: layout of LSW is hardcoded in entry.S
+ */
+#define TIF_SYSCALL_TRACE	0	/* syscall trace active */
+#define TIF_NOTIFY_RESUME	1	/* callback before returning to user */
+#define TIF_SIGPENDING		2	/* signal pending */
+#define TIF_NEED_RESCHED	3	/* rescheduling necessary */
+#define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
+#define TIF_IRET		5	/* force IRET */
+#define TIF_SYSCALL_EMU		6	/* syscall emulation active */
+#define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
+#define TIF_SECCOMP		8	/* secure computing */
+#define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
+#define TIF_NOTSC		16	/* TSC is not accessible in userland */
+#define TIF_IA32		17	/* 32bit process */
+#define TIF_FORK		18	/* ret_from_fork */
+#define TIF_ABI_PENDING		19
+#define TIF_MEMDIE		20
+#define TIF_DEBUG		21	/* uses debug registers */
+#define TIF_IO_BITMAP		22	/* uses I/O bitmap */
+#define TIF_FREEZE		23	/* is freezing for suspend */
+#define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
+#define TIF_DEBUGCTLMSR		25	/* uses thread_struct.debugctlmsr */
+#define TIF_DS_AREA_MSR		26      /* uses thread_struct.ds_area_msr */
+#define TIF_BTS_TRACE_TS	27      /* record scheduling event timestamps */
+
+#define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
+#define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
+#define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
+#define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
+#define _TIF_IRET		(1 << TIF_IRET)
+#define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
+#define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
+#define _TIF_SECCOMP		(1 << TIF_SECCOMP)
+#define _TIF_MCE_NOTIFY		(1 << TIF_MCE_NOTIFY)
+#define _TIF_NOTSC		(1 << TIF_NOTSC)
+#define _TIF_IA32		(1 << TIF_IA32)
+#define _TIF_FORK		(1 << TIF_FORK)
+#define _TIF_ABI_PENDING	(1 << TIF_ABI_PENDING)
+#define _TIF_DEBUG		(1 << TIF_DEBUG)
+#define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
+#define _TIF_FREEZE		(1 << TIF_FREEZE)
+#define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
+#define _TIF_DEBUGCTLMSR	(1 << TIF_DEBUGCTLMSR)
+#define _TIF_DS_AREA_MSR	(1 << TIF_DS_AREA_MSR)
+#define _TIF_BTS_TRACE_TS	(1 << TIF_BTS_TRACE_TS)
+
+/* work to do in syscall_trace_enter() */
+#define _TIF_WORK_SYSCALL_ENTRY	\
+	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | \
+	 _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | _TIF_SINGLESTEP)
+
+/* work to do in syscall_trace_leave() */
+#define _TIF_WORK_SYSCALL_EXIT	\
+	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP)
+
+/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK							\
+	(0x0000FFFF &							\
+	 ~(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|			\
+	   _TIF_SINGLESTEP|_TIF_SECCOMP|_TIF_SYSCALL_EMU))
+
+/* work to do on any return to user space */
+#define _TIF_ALLWORK_MASK (0x0000FFFF & ~_TIF_SECCOMP)
+
+/* Only used for 64 bit */
+#define _TIF_DO_NOTIFY_MASK						\
+	(_TIF_SIGPENDING|_TIF_MCE_NOTIFY|_TIF_NOTIFY_RESUME)
+
+/* flags to check in __switch_to() */
+#define _TIF_WORK_CTXSW							\
+	(_TIF_IO_BITMAP|_TIF_DEBUGCTLMSR|_TIF_DS_AREA_MSR|_TIF_BTS_TRACE_TS| \
+								_TIF_NOTSC)
+
+#define _TIF_WORK_CTXSW_PREV _TIF_WORK_CTXSW
+#define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)
+
+#define PREEMPT_ACTIVE		0x10000000
+
+/* thread information allocation */
+#ifdef CONFIG_DEBUG_STACK_USAGE
+#define THREAD_FLAGS (GFP_KERNEL | __GFP_ZERO)
+#else
+#define THREAD_FLAGS GFP_KERNEL
+#endif
+
+#define __HAVE_ARCH_THREAD_INFO_ALLOCATOR
+
+#define alloc_thread_info(tsk)						\
+	((struct thread_info *)__get_free_pages(THREAD_FLAGS, THREAD_ORDER))
+
+#ifdef CONFIG_X86_32
+
+#define STACK_WARN	(THREAD_SIZE/8)
+/*
+ * macros/functions for gaining access to the thread information structure
+ *
+ * preempt_count needs to be 1 initially, until the scheduler is functional.
+ */
+#ifndef __ASSEMBLY__
+
+
+/* how to get the current stack pointer from C */
+register unsigned long current_stack_pointer asm("esp") __used;
+
+/* how to get the thread information struct from C */
+static inline struct thread_info *current_thread_info(void)
+{
+	return (struct thread_info *)
+		(current_stack_pointer & ~(THREAD_SIZE - 1));
+}
+
+#else /* !__ASSEMBLY__ */
+
+/* how to get the thread information struct from ASM */
+#define GET_THREAD_INFO(reg)	 \
+	movl $-THREAD_SIZE, reg; \
+	andl %esp, reg
+
+/* use this one if reg already contains %esp */
+#define GET_THREAD_INFO_WITH_ESP(reg) \
+	andl $-THREAD_SIZE, reg
+
+#endif
+
+#else /* X86_32 */
+
+#include <asm/pda.h>
+
+/*
+ * macros/functions for gaining access to the thread information structure
+ * preempt_count needs to be 1 initially, until the scheduler is functional.
+ */
+#ifndef __ASSEMBLY__
+static inline struct thread_info *current_thread_info(void)
+{
+	struct thread_info *ti;
+	ti = (void *)(read_pda(kernelstack) + PDA_STACKOFFSET - THREAD_SIZE);
+	return ti;
+}
+
+/* do not use in interrupt context */
+static inline struct thread_info *stack_thread_info(void)
+{
+	struct thread_info *ti;
+	asm("andq %%rsp,%0; " : "=r" (ti) : "0" (~(THREAD_SIZE - 1)));
+	return ti;
+}
+
+#else /* !__ASSEMBLY__ */
+
+/* how to get the thread information struct from ASM */
+#define GET_THREAD_INFO(reg) \
+	movq %gs:pda_kernelstack,reg ; \
+	subq $(THREAD_SIZE-PDA_STACKOFFSET),reg
+
+#endif
+
+#endif /* !X86_32 */
+
+/*
+ * Thread-synchronous status.
+ *
+ * This is different from the flags in that nobody else
+ * ever touches our thread-synchronous status, so we don't
+ * have to worry about atomic accesses.
+ */
+#define TS_USEDFPU		0x0001	/* FPU was used by this task
+					   this quantum (SMP) */
+#define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
+#define TS_POLLING		0x0004	/* true if in idle loop
+					   and not sleeping */
+#define TS_RESTORE_SIGMASK	0x0008	/* restore signal mask in do_signal() */
+#define TS_XSAVE		0x0010	/* Use xsave/xrstor */
+
+#define tsk_is_polling(t) (task_thread_info(t)->status & TS_POLLING)
+
+#ifndef __ASSEMBLY__
+#define HAVE_SET_RESTORE_SIGMASK	1
+static inline void set_restore_sigmask(void)
+{
+	struct thread_info *ti = current_thread_info();
+	ti->status |= TS_RESTORE_SIGMASK;
+	set_bit(TIF_SIGPENDING, (unsigned long *)&ti->flags);
+}
+#endif	/* !__ASSEMBLY__ */
+
+#ifndef __ASSEMBLY__
+extern void arch_task_cache_init(void);
+extern void free_thread_info(struct thread_info *ti);
+extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
+#define arch_task_cache_init arch_task_cache_init
+#endif
+#endif /* ASM_X86__THREAD_INFO_H */
