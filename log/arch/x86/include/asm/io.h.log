commit 232bb01bb8ad2f88aefbd88b0d3fe3f9a253502b
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Jan 21 16:43:41 2020 -0700

    x86/asm: add iosubmit_cmds512() based on MOVDIR64B CPU instruction
    
    With the introduction of MOVDIR64B instruction, there is now an instruction
    that can write 64 bytes of data atomically.
    
    Quoting from Intel SDM:
    "There is no atomicity guarantee provided for the 64-byte load operation
    from source address, and processor implementations may use multiple
    load operations to read the 64-bytes. The 64-byte direct-store issued
    by MOVDIR64B guarantees 64-byte write-completion atomicity. This means
    that the data arrives at the destination in a single undivided 64-byte
    write transaction."
    
    We have identified at least 3 different use cases for this instruction in
    the format of func(dst, src, count):
    1) Clear poison / Initialize MKTME memory
       @dst is normal memory.
       @src in normal memory. Does not increment. (Copy same line to all
       targets)
       @count (to clear/init multiple lines)
    2) Submit command(s) to new devices
       @dst is a special MMIO region for a device. Does not increment.
       @src is normal memory. Increments.
       @count usually is 1, but can be multiple.
    3) Copy to iomem in big chunks
       @dst is iomem and increments
       @src in normal memory and increments
       @count is number of chunks to copy
    
    Add support for case #2 to support device that will accept commands via
    this instruction. We provide a @count in order to submit a batch of
    preprogrammed descriptors in virtually contiguous memory. This
    allows the caller to submit multiple descriptors to a device with a single
    submission. The special device requires the entire 64bytes descriptor to
    be written atomically and will accept MOVDIR64B instruction.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/157965022175.73301.10174614665472962675.stgit@djiang5-desk3.ch.intel.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 9997521fc5cd..e1aa17a468a8 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -399,4 +399,40 @@ extern bool arch_memremap_can_ram_remap(resource_size_t offset,
 extern bool phys_mem_access_encrypted(unsigned long phys_addr,
 				      unsigned long size);
 
+/**
+ * iosubmit_cmds512 - copy data to single MMIO location, in 512-bit units
+ * @__dst: destination, in MMIO space (must be 512-bit aligned)
+ * @src: source
+ * @count: number of 512 bits quantities to submit
+ *
+ * Submit data from kernel space to MMIO space, in units of 512 bits at a
+ * time.  Order of access is not guaranteed, nor is a memory barrier
+ * performed afterwards.
+ *
+ * Warning: Do not use this helper unless your driver has checked that the CPU
+ * instruction is supported on the platform.
+ */
+static inline void iosubmit_cmds512(void __iomem *__dst, const void *src,
+				    size_t count)
+{
+	/*
+	 * Note that this isn't an "on-stack copy", just definition of "dst"
+	 * as a pointer to 64-bytes of stuff that is going to be overwritten.
+	 * In the MOVDIR64B case that may be needed as you can use the
+	 * MOVDIR64B instruction to copy arbitrary memory around. This trick
+	 * lets the compiler know how much gets clobbered.
+	 */
+	volatile struct { char _[64]; } *dst = __dst;
+	const u8 *from = src;
+	const u8 *end = from + count * 64;
+
+	while (from < end) {
+		/* MOVDIR64B [rdx], rax */
+		asm volatile(".byte 0x66, 0x0f, 0x38, 0xf8, 0x02"
+			     : "=m" (dst)
+			     : "d" (from), "a" (dst));
+		from += 64;
+	}
+}
+
 #endif /* _ASM_X86_IO_H */

commit d092a87073269677b7ff09e71a8d91912b7f969a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Oct 16 08:09:38 2019 +0200

    arch: rely on asm-generic/io.h for default ioremap_* definitions
    
    Various architectures that use asm-generic/io.h still defined their
    own default versions of ioremap_nocache, ioremap_wt and ioremap_wc
    that point back to plain ioremap directly or indirectly.  Remove these
    definitions and rely on asm-generic/io.h instead.  For this to work
    the backup ioremap_* defintions needs to be changed to purely cpp
    macros instea of inlines to cover for architectures like openrisc
    that only define ioremap after including <asm-generic/io.h>.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: Palmer Dabbelt <palmer@dabbelt.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 6b5cc41319a7..9997521fc5cd 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -205,7 +205,6 @@ extern void __iomem *ioremap_encrypted(resource_size_t phys_addr, unsigned long
  */
 void __iomem *ioremap(resource_size_t offset, unsigned long size);
 #define ioremap ioremap
-#define ioremap_nocache ioremap
 
 extern void iounmap(volatile void __iomem *addr);
 #define iounmap iounmap

commit c0d94aa54bd893bd41ca35e2a2de332742bb167d
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Aug 12 23:35:59 2019 +0200

    x86: Clean up ioremap()
    
    Use ioremap() as the main implemented function, and defines
    ioremap_nocache() as a deprecated alias of ioremap() in
    preparation of removing ioremap_nocache() entirely.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 6bed97ff6db2..6b5cc41319a7 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -180,8 +180,6 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
  * The default ioremap() behavior is non-cached; if you need something
  * else, you probably want one of the following.
  */
-extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
-#define ioremap_nocache ioremap_nocache
 extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
 #define ioremap_uc ioremap_uc
 extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
@@ -205,11 +203,9 @@ extern void __iomem *ioremap_encrypted(resource_size_t phys_addr, unsigned long
  * If the area you are trying to map is a PCI BAR you should have a
  * look at pci_iomap().
  */
-static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
-{
-	return ioremap_nocache(offset, size);
-}
+void __iomem *ioremap(resource_size_t offset, unsigned long size);
 #define ioremap ioremap
+#define ioremap_nocache ioremap
 
 extern void iounmap(volatile void __iomem *addr);
 #define iounmap iounmap

commit 3a7f0adfe7c27cdaf6dc3456226a430398732e2c
Author: Stephen Kitt <steve@sk2.org>
Date:   Tue Jul 16 16:27:04 2019 -0700

    arch/*: remove unused isa_page_to_bus()
    
    isa_page_to_bus() is deprecated and is no longer used anywhere.  Remove
    it entirely.
    
    Link: http://lkml.kernel.org/r/20190613161155.16946-1-steve@sk2.org
    Signed-off-by: Stephen Kitt <steve@sk2.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index a06a9f8294ea..6bed97ff6db2 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -165,7 +165,6 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
 {
 	return (unsigned int)virt_to_phys(address);
 }
-#define isa_page_to_bus(page)	((unsigned int)page_to_phys(page))
 #define isa_bus_to_virt		phys_to_virt
 
 /*

commit 08f1f3a72f4cea136686585b81a251baa3539f12
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Feb 22 13:03:18 2019 +0000

    x86/io: Remove useless definition of mmiowb()
    
    x86 maps mmiowb() to barrier(), but this is superfluous because a
    compiler barrier is already implied by spin_unlock(). Since x86 also
    includes asm-generic/io.h in its asm/io.h file, remove the definition
    entirely and pick up the dummy definition from core code.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 686247db3106..a06a9f8294ea 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -90,8 +90,6 @@ build_mmio_write(__writel, "l", unsigned int, "r", )
 #define __raw_writew __writew
 #define __raw_writel __writel
 
-#define mmiowb() barrier()
-
 #ifdef CONFIG_X86_64
 
 build_mmio_read(readq, "q", u64, "=r", :"memory")

commit 170d13ca3a2fdaaa0283399247631b76b441cca2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 4 17:52:49 2019 -0800

    x86: re-introduce non-generic memcpy_{to,from}io
    
    This has been broken forever, and nobody ever really noticed because
    it's purely a performance issue.
    
    Long long ago, in commit 6175ddf06b61 ("x86: Clean up mem*io functions")
    Brian Gerst simplified the memory copies to and from iomem, since on
    x86, the instructions to access iomem are exactly the same as the
    regular instructions.
    
    That is technically true, and things worked, and nobody said anything.
    Besides, back then the regular memcpy was pretty simple and worked fine.
    
    Nobody noticed except for David Laight, that is.  David has a testing a
    TLP monitor he was writing for an FPGA, and has been occasionally
    complaining about how memcpy_toio() writes things one byte at a time.
    
    Which is completely unacceptable from a performance standpoint, even if
    it happens to technically work.
    
    The reason it's writing one byte at a time is because while it's
    technically true that accesses to iomem are the same as accesses to
    regular memory on x86, the _granularity_ (and ordering) of accesses
    matter to iomem in ways that they don't matter to regular cached memory.
    
    In particular, when ERMS is set, we default to using "rep movsb" for
    larger memory copies.  That is indeed perfectly fine for real memory,
    since the whole point is that the CPU is going to do cacheline
    optimizations and executes the memory copy efficiently for cached
    memory.
    
    With iomem? Not so much.  With iomem, "rep movsb" will indeed work, but
    it will copy things one byte at a time. Slowly and ponderously.
    
    Now, originally, back in 2010 when commit 6175ddf06b61 was done, we
    didn't use ERMS, and this was much less noticeable.
    
    Our normal memcpy() was simpler in other ways too.
    
    Because in fact, it's not just about using the string instructions.  Our
    memcpy() these days does things like "read and write overlapping values"
    to handle the last bytes of the copy.  Again, for normal memory,
    overlapping accesses isn't an issue.  For iomem? It can be.
    
    So this re-introduces the specialized memcpy_toio(), memcpy_fromio() and
    memset_io() functions.  It doesn't particularly optimize them, but it
    tries to at least not be horrid, or do overlapping accesses.  In fact,
    this uses the existing __inline_memcpy() function that we still had
    lying around that uses our very traditional "rep movsl" loop followed by
    movsw/movsb for the final bytes.
    
    Somebody may decide to try to improve on it, but if we've gone almost a
    decade with only one person really ever noticing and complaining, maybe
    it's not worth worrying about further, once it's not _completely_ broken?
    
    Reported-by: David Laight <David.Laight@aculab.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 832da8229cc7..686247db3106 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -221,6 +221,14 @@ extern void set_iounmap_nonlazy(void);
 
 #ifdef __KERNEL__
 
+void memcpy_fromio(void *, const volatile void __iomem *, size_t);
+void memcpy_toio(volatile void __iomem *, const void *, size_t);
+void memset_io(volatile void __iomem *, int, size_t);
+
+#define memcpy_fromio memcpy_fromio
+#define memcpy_toio memcpy_toio
+#define memset_io memset_io
+
 #include <asm-generic/iomap.h>
 
 /*

commit 99792e0cea1ed733cdc8d0758677981e0cbebfed
Merge: 382d72a9aa52 977e4be5eb71
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:05:28 2018 +0100

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "Lots of changes in this cycle:
    
       - Lots of CPA (change page attribute) optimizations and related
         cleanups (Thomas Gleixner, Peter Zijstra)
    
       - Make lazy TLB mode even lazier (Rik van Riel)
    
       - Fault handler cleanups and improvements (Dave Hansen)
    
       - kdump, vmcore: Enable kdumping encrypted memory with AMD SME
         enabled (Lianbo Jiang)
    
       - Clean up VM layout documentation (Baoquan He, Ingo Molnar)
    
       - ... plus misc other fixes and enhancements"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (51 commits)
      x86/stackprotector: Remove the call to boot_init_stack_canary() from cpu_startup_entry()
      x86/mm: Kill stray kernel fault handling comment
      x86/mm: Do not warn about PCI BIOS W+X mappings
      resource: Clean it up a bit
      resource: Fix find_next_iomem_res() iteration issue
      resource: Include resource end in walk_*() interfaces
      x86/kexec: Correct KEXEC_BACKUP_SRC_END off-by-one error
      x86/mm: Remove spurious fault pkey check
      x86/mm/vsyscall: Consider vsyscall page part of user address space
      x86/mm: Add vsyscall address helper
      x86/mm: Fix exception table comments
      x86/mm: Add clarifying comments for user addr space
      x86/mm: Break out user address space handling
      x86/mm: Break out kernel address space handling
      x86/mm: Clarify hardware vs. software "error_code"
      x86/mm/tlb: Make lazy TLB mode lazier
      x86/mm/tlb: Add freed_tables element to flush_tlb_info
      x86/mm/tlb: Add freed_tables argument to flush_tlb_mm_range
      smp,cpumask: introduce on_each_cpu_cond_mask
      smp: use __cpumask_set_cpu in on_each_cpu_cond
      ...

commit c3a7a61c192ec350330128edb13db33a9bc0ace1
Author: Lianbo Jiang <lijiang@redhat.com>
Date:   Thu Sep 27 15:19:51 2018 +0800

    x86/ioremap: Add an ioremap_encrypted() helper
    
    When SME is enabled, the memory is encrypted in the first kernel. In
    this case, SME also needs to be enabled in the kdump kernel, and we have
    to remap the old memory with the memory encryption mask.
    
    The case of concern here is if SME is active in the first kernel,
    and it is active too in the kdump kernel. There are four cases to be
    considered:
    
    a. dump vmcore
       It is encrypted in the first kernel, and needs be read out in the
       kdump kernel.
    
    b. crash notes
       When dumping vmcore, the people usually need to read useful
       information from notes, and the notes is also encrypted.
    
    c. iommu device table
       It's encrypted in the first kernel, kdump kernel needs to access its
       content to analyze and get information it needs.
    
    d. mmio of AMD iommu
       not encrypted in both kernels
    
    Add a new bool parameter @encrypted to __ioremap_caller(). If set,
    memory will be remapped with the SME mask.
    
    Add a new function ioremap_encrypted() to explicitly pass in a true
    value for @encrypted. Use ioremap_encrypted() for the above a, b, c
    cases.
    
     [ bp: cleanup commit message, extern defs in io.h and drop forgotten
       include. ]
    
    Signed-off-by: Lianbo Jiang <lijiang@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: kexec@lists.infradead.org
    Cc: tglx@linutronix.de
    Cc: mingo@redhat.com
    Cc: hpa@zytor.com
    Cc: akpm@linux-foundation.org
    Cc: dan.j.williams@intel.com
    Cc: bhelgaas@google.com
    Cc: baiyaowei@cmss.chinamobile.com
    Cc: tiwai@suse.de
    Cc: brijesh.singh@amd.com
    Cc: dyoung@redhat.com
    Cc: bhe@redhat.com
    Cc: jroedel@suse.de
    Link: https://lkml.kernel.org/r/20180927071954.29615-2-lijiang@redhat.com

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 6de64840dd22..6df53efcecfd 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -187,11 +187,12 @@ extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size)
 #define ioremap_nocache ioremap_nocache
 extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
 #define ioremap_uc ioremap_uc
-
 extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
 #define ioremap_cache ioremap_cache
 extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size, unsigned long prot_val);
 #define ioremap_prot ioremap_prot
+extern void __iomem *ioremap_encrypted(resource_size_t phys_addr, unsigned long size);
+#define ioremap_encrypted ioremap_encrypted
 
 /**
  * ioremap     -   map bus memory into CPU space

commit 3cfa210bf3fe0803cca17f3775d8cf2360d5f443
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 25 13:30:09 2018 -0700

    xen: don't include <xen/xen.h> from <asm/io.h> and <asm/dma-mapping.h>
    
    Nothing Xen specific in these headers, which get included from a lot
    of code in the kernel.  So prune the includes and move them to the
    Xen-specific files that actually use them instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 232d8e9ee8a0..9a92a3ac2ac5 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -369,10 +369,6 @@ extern void __iomem *ioremap_wt(resource_size_t offset, unsigned long size);
 
 extern bool is_early_ioremap_ptep(pte_t *ptep);
 
-#ifdef CONFIG_XEN
-#include <xen/xen.h>
-#endif	/* CONFIG_XEN */
-
 #define IO_SPACE_LIMIT 0xffff
 
 #include <asm-generic/io.h>

commit c39ae60dfbda66922f644193b91850abcd4d588c
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 25 13:30:08 2018 -0700

    block: remove ARCH_BIOVEC_PHYS_MERGEABLE
    
    Take the Xen check into the core code instead of delegating it to
    the architectures.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index abdb501a551d..232d8e9ee8a0 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -371,9 +371,6 @@ extern bool is_early_ioremap_ptep(pte_t *ptep);
 
 #ifdef CONFIG_XEN
 #include <xen/xen.h>
-
-#define ARCH_BIOVEC_PHYS_MERGEABLE(vec1, vec2)				\
-	 (!xen_domain() || xen_biovec_phys_mergeable(vec1, vec2))
 #endif	/* CONFIG_XEN */
 
 #define IO_SPACE_LIMIT 0xffff

commit 20e3267601f95ff62d7a3116a17a680e9f5cbcc9
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 25 13:30:07 2018 -0700

    xen: provide a prototype for xen_biovec_phys_mergeable in xen.h
    
    Having multiple externs in arch headers is not a good way to provide
    a common interface.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 7c6106216d9c..abdb501a551d 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -371,10 +371,6 @@ extern bool is_early_ioremap_ptep(pte_t *ptep);
 
 #ifdef CONFIG_XEN
 #include <xen/xen.h>
-struct bio_vec;
-
-extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
-				      const struct bio_vec *vec2);
 
 #define ARCH_BIOVEC_PHYS_MERGEABLE(vec1, vec2)				\
 	 (!xen_domain() || xen_biovec_phys_mergeable(vec1, vec2))

commit 6a9f5f240adfdced863a098d34f8f05ca6ab9d5f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 24 09:43:50 2018 +0200

    block: simplify BIOVEC_PHYS_MERGEABLE
    
    Turn the macro into an inline, move it to blk.h and simplify the
    arch hooks a bit.
    
    Also rename the function to biovec_phys_mergeable as there is no need
    to shout.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 6de64840dd22..7c6106216d9c 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -376,9 +376,8 @@ struct bio_vec;
 extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
 				      const struct bio_vec *vec2);
 
-#define BIOVEC_PHYS_MERGEABLE(vec1, vec2)				\
-	(__BIOVEC_PHYS_MERGEABLE(vec1, vec2) &&				\
-	 (!xen_domain() || xen_biovec_phys_mergeable(vec1, vec2)))
+#define ARCH_BIOVEC_PHYS_MERGEABLE(vec1, vec2)				\
+	 (!xen_domain() || xen_biovec_phys_mergeable(vec1, vec2))
 #endif	/* CONFIG_XEN */
 
 #define IO_SPACE_LIMIT 0xffff

commit 6469a0ee0a06b2ea1f5afbb1d5a3feed017d4c7a
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Tue May 15 14:52:11 2018 +0300

    x86/io: Define readq()/writeq() to use 64-bit type
    
    Since non atomic readq() and writeq() were added some of the drivers
    would like to use it in a manner of:
    
     #include <io-64-nonatomic-lo-hi.h>
     ...
     pr_debug("Debug value of some register: %016llx\n", readq(addr));
    
    However, lo_hi_readq() always returns __u64 data, while readq()
    on x86_64 defines it as unsigned long. and thus compiler warns
    about type mismatch, although they are both 64-bit on x86_64.
    
    Convert readq() and writeq() on x86 to operate on deterministic
    64-bit type. The most of architectures in the kernel already are using
    either unsigned long long, or u64 type for readq() / writeq().
    This change propagates consistency in that sense.
    
    While this is not an issue per se, though if someone wants to address it,
    the anchor could be the commit:
    
      797a796a13df ("asm-generic: architecture independent readq/writeq for 32bit environment")
    
    where non-atomic variants had been introduced.
    
    Note, there are only few users of above pattern and they will not be
    affected because they do cast returned value. The actual warning has
    been issued on not-yet-upstreamed code.
    
    Potentially we might get a new warnings if some 64-bit only code
    assigns returned value to unsigned long type of variable. This is
    assumed to be addressed on case-by-case basis.
    
    Reported-by: lkp <lkp@intel.com>
    Tested-by: Sohil Mehta <sohil.mehta@intel.com>
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180515115211.55050-1-andriy.shevchenko@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index f6e5b9375d8c..6de64840dd22 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -94,10 +94,10 @@ build_mmio_write(__writel, "l", unsigned int, "r", )
 
 #ifdef CONFIG_X86_64
 
-build_mmio_read(readq, "q", unsigned long, "=r", :"memory")
-build_mmio_read(__readq, "q", unsigned long, "=r", )
-build_mmio_write(writeq, "q", unsigned long, "r", :"memory")
-build_mmio_write(__writeq, "q", unsigned long, "r", )
+build_mmio_read(readq, "q", u64, "=r", :"memory")
+build_mmio_read(__readq, "q", u64, "=r", )
+build_mmio_write(writeq, "q", u64, "r", :"memory")
+build_mmio_write(__writeq, "q", u64, "r", )
 
 #define readq_relaxed(a)	__readq(a)
 #define writeq_relaxed(v, a)	__writeq(v, a)

commit 5927145efd5de71976e62e2822511b13014d7e56
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Mar 19 11:38:13 2018 +0100

    x86/cpu: Remove the CONFIG_X86_PPRO_FENCE=y quirk
    
    There were only a few Pentium Pro multiprocessors systems where this
    errata applied. They are more than 20 years old now, and we've slowly
    dropped places which put the workarounds in and discouraged anyone
    from enabling the workaround.
    
    Get rid of it for good.
    
    Tested-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jon Mason <jdmason@kudzu.us>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Muli Ben-Yehuda <mulix@mulix.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: iommu@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20180319103826.12853-2-hch@lst.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 95e948627fd0..f6e5b9375d8c 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -232,21 +232,6 @@ extern void set_iounmap_nonlazy(void);
  */
 #define __ISA_IO_base ((char __iomem *)(PAGE_OFFSET))
 
-/*
- *	Cache management
- *
- *	This needed for two cases
- *	1. Out of order aware processors
- *	2. Accidentally out of order processors (PPro errata #51)
- */
-
-static inline void flush_write_buffers(void)
-{
-#if defined(CONFIG_X86_PPRO_FENCE)
-	asm volatile("lock; addl $0,0(%%esp)": : :"memory");
-#endif
-}
-
 #endif /* __KERNEL__ */
 
 extern void native_io_delay(void);

commit be62a32044061cb4a3b70a10598e093f1319102e
Author: Craig Bergstrom <craigb@google.com>
Date:   Wed Nov 15 15:29:51 2017 -0700

    x86/mm: Limit mmap() of /dev/mem to valid physical addresses
    
    One thing /dev/mem access APIs should verify is that there's no way
    that excessively large pfn's can leak into the high bits of the
    page table entry.
    
    In particular, if people can use "very large physical page addresses"
    through /dev/mem to set the bits past bit 58 - SOFTW4 and permission
    key bits and NX bit, that could *really* confuse the kernel.
    
    We had an earlier attempt:
    
      ce56a86e2ade ("x86/mm: Limit mmap() of /dev/mem to valid physical addresses")
    
    ... which turned out to be too restrictive (breaking mem=... bootups for example) and
    had to be reverted in:
    
      90edaac62729 ("Revert "x86/mm: Limit mmap() of /dev/mem to valid physical addresses"")
    
    This v2 attempt modifies the original patch and makes sure that mmap(/dev/mem)
    limits the pfns so that it at least fits in the actual pteval_t architecturally:
    
     - Make sure mmap_mem() actually validates that the offset fits in phys_addr_t
    
        ( This may be indirectly true due to some other check, but it's not
          entirely obvious. )
    
     - Change valid_mmap_phys_addr_range() to just use phys_addr_valid()
       on the top byte
    
        ( Top byte is sufficient, because mmap_mem() has already checked that
          it cannot wrap. )
    
     - Add a few comments about what the valid_phys_addr_range() vs.
       valid_mmap_phys_addr_range() difference is.
    
    Signed-off-by: Craig Bergstrom <craigb@google.com>
    [ Fixed the checks and added comments. ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    [ Collected the discussion and patches into a commit. ]
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Hans Verkuil <hans.verkuil@cisco.com>
    Cc: Mauro Carvalho Chehab <mchehab@s-opensource.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sander Eikelenboom <linux@eikelenboom.it>
    Cc: Sean Young <sean@mess.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/CA+55aFyEcOMb657vWSmrM13OxmHxC-XxeBmNis=DwVvpJUOogQ@mail.gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 93ae8aee1780..95e948627fd0 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -111,6 +111,10 @@ build_mmio_write(__writeq, "q", unsigned long, "r", )
 
 #endif
 
+#define ARCH_HAS_VALID_PHYS_ADDR_RANGE
+extern int valid_phys_addr_range(phys_addr_t addr, size_t size);
+extern int valid_mmap_phys_addr_range(unsigned long pfn, size_t size);
+
 /**
  *	virt_to_phys	-	map virtual addresses to physical
  *	@address: address to remap

commit 606b21d4a6498c23632a4693c81b7b24feedd038
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Fri Oct 20 09:30:55 2017 -0500

    x86/io: Unroll string I/O when SEV is active
    
    Secure Encrypted Virtualization (SEV) does not support string I/O, so
    unroll the string I/O operation into a loop operating on one element at
    a time.
    
    [ tglx: Gave the static key a real name instead of the obscure __sev ]
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Tested-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: kvm@vger.kernel.org
    Cc: David Laight <David.Laight@ACULAB.COM>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20171020143059.3291-14-brijesh.singh@amd.com

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 11398d55aefa..93ae8aee1780 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -266,6 +266,21 @@ static inline void slow_down_io(void)
 
 #endif
 
+#ifdef CONFIG_AMD_MEM_ENCRYPT
+#include <linux/jump_label.h>
+
+extern struct static_key_false sev_enable_key;
+static inline bool sev_key_active(void)
+{
+	return static_branch_unlikely(&sev_enable_key);
+}
+
+#else /* !CONFIG_AMD_MEM_ENCRYPT */
+
+static inline bool sev_key_active(void) { return false; }
+
+#endif /* CONFIG_AMD_MEM_ENCRYPT */
+
 #define BUILDIO(bwl, bw, type)						\
 static inline void out##bwl(unsigned type value, int port)		\
 {									\
@@ -296,14 +311,34 @@ static inline unsigned type in##bwl##_p(int port)			\
 									\
 static inline void outs##bwl(int port, const void *addr, unsigned long count) \
 {									\
-	asm volatile("rep; outs" #bwl					\
-		     : "+S"(addr), "+c"(count) : "d"(port) : "memory");	\
+	if (sev_key_active()) {						\
+		unsigned type *value = (unsigned type *)addr;		\
+		while (count) {						\
+			out##bwl(*value, port);				\
+			value++;					\
+			count--;					\
+		}							\
+	} else {							\
+		asm volatile("rep; outs" #bwl				\
+			     : "+S"(addr), "+c"(count)			\
+			     : "d"(port) : "memory");			\
+	}								\
 }									\
 									\
 static inline void ins##bwl(int port, void *addr, unsigned long count)	\
 {									\
-	asm volatile("rep; ins" #bwl					\
-		     : "+D"(addr), "+c"(count) : "d"(port) : "memory");	\
+	if (sev_key_active()) {						\
+		unsigned type *value = (unsigned type *)addr;		\
+		while (count) {						\
+			*value = in##bwl(port);				\
+			value++;					\
+			count--;					\
+		}							\
+	} else {							\
+		asm volatile("rep; ins" #bwl				\
+			     : "+D"(addr), "+c"(count)			\
+			     : "d"(port) : "memory");			\
+	}								\
 }
 
 BUILDIO(b, b, char)

commit ead751507de86d90fa250431e9990a8b881f713c
Merge: fdebad11e50e e2be04c7f995
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 2 10:04:46 2017 -0700

    Merge tag 'spdx_identifiers-4.14-rc8' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core
    
    Pull initial SPDX identifiers from Greg KH:
     "License cleanup: add SPDX license identifiers to some files
    
      Many source files in the tree are missing licensing information, which
      makes it harder for compliance tools to determine the correct license.
    
      By default all files without license information are under the default
      license of the kernel, which is GPL version 2.
    
      Update the files which contain no license information with the
      'GPL-2.0' SPDX license identifier. The SPDX identifier is a legally
      binding shorthand, which can be used instead of the full boiler plate
      text.
    
      This patch is based on work done by Thomas Gleixner and Kate Stewart
      and Philippe Ombredanne.
    
      How this work was done:
    
      Patches were generated and checked against linux-4.14-rc6 for a subset
      of the use cases:
    
       - file had no licensing information it it.
    
       - file was a */uapi/* one with no licensing information in it,
    
       - file was a */uapi/* one with existing licensing information,
    
      Further patches will be generated in subsequent months to fix up cases
      where non-standard license headers were used, and references to
      license had to be inferred by heuristics based on keywords.
    
      The analysis to determine which SPDX License Identifier to be applied
      to a file was done in a spreadsheet of side by side results from of
      the output of two independent scanners (ScanCode & Windriver)
      producing SPDX tag:value files created by Philippe Ombredanne.
      Philippe prepared the base worksheet, and did an initial spot review
      of a few 1000 files.
    
      The 4.13 kernel was the starting point of the analysis with 60,537
      files assessed. Kate Stewart did a file by file comparison of the
      scanner results in the spreadsheet to determine which SPDX license
      identifier(s) to be applied to the file. She confirmed any
      determination that was not immediately clear with lawyers working with
      the Linux Foundation.
    
      Criteria used to select files for SPDX license identifier tagging was:
    
       - Files considered eligible had to be source code files.
    
       - Make and config files were included as candidates if they contained
         >5 lines of source
    
       - File already had some variant of a license header in it (even if <5
         lines).
    
      All documentation files were explicitly excluded.
    
      The following heuristics were used to determine which SPDX license
      identifiers to apply.
    
       - when both scanners couldn't find any license traces, file was
         considered to have no license information in it, and the top level
         COPYING file license applied.
    
         For non */uapi/* files that summary was:
    
           SPDX license identifier                            # files
           ---------------------------------------------------|-------
           GPL-2.0                                              11139
    
         and resulted in the first patch in this series.
    
         If that file was a */uapi/* path one, it was "GPL-2.0 WITH
         Linux-syscall-note" otherwise it was "GPL-2.0". Results of that
         was:
    
           SPDX license identifier                            # files
           ---------------------------------------------------|-------
           GPL-2.0 WITH Linux-syscall-note                        930
    
         and resulted in the second patch in this series.
    
       - if a file had some form of licensing information in it, and was one
         of the */uapi/* ones, it was denoted with the Linux-syscall-note if
         any GPL family license was found in the file or had no licensing in
         it (per prior point). Results summary:
    
           SPDX license identifier                            # files
           ---------------------------------------------------|------
           GPL-2.0 WITH Linux-syscall-note                       270
           GPL-2.0+ WITH Linux-syscall-note                      169
           ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
           ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
           LGPL-2.1+ WITH Linux-syscall-note                      15
           GPL-1.0+ WITH Linux-syscall-note                       14
           ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
           LGPL-2.0+ WITH Linux-syscall-note                       4
           LGPL-2.1 WITH Linux-syscall-note                        3
           ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
           ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
         and that resulted in the third patch in this series.
    
       - when the two scanners agreed on the detected license(s), that
         became the concluded license(s).
    
       - when there was disagreement between the two scanners (one detected
         a license but the other didn't, or they both detected different
         licenses) a manual inspection of the file occurred.
    
       - In most cases a manual inspection of the information in the file
         resulted in a clear resolution of the license that should apply
         (and which scanner probably needed to revisit its heuristics).
    
       - When it was not immediately clear, the license identifier was
         confirmed with lawyers working with the Linux Foundation.
    
       - If there was any question as to the appropriate license identifier,
         the file was flagged for further research and to be revisited later
         in time.
    
      In total, over 70 hours of logged manual review was done on the
      spreadsheet to determine the SPDX license identifiers to apply to the
      source files by Kate, Philippe, Thomas and, in some cases,
      confirmation by lawyers working with the Linux Foundation.
    
      Kate also obtained a third independent scan of the 4.13 code base from
      FOSSology, and compared selected files where the other two scanners
      disagreed against that SPDX file, to see if there was new insights.
      The Windriver scanner is based on an older version of FOSSology in
      part, so they are related.
    
      Thomas did random spot checks in about 500 files from the spreadsheets
      for the uapi headers and agreed with SPDX license identifier in the
      files he inspected. For the non-uapi files Thomas did random spot
      checks in about 15000 files.
    
      In initial set of patches against 4.14-rc6, 3 files were found to have
      copy/paste license identifier errors, and have been fixed to reflect
      the correct identifier.
    
      Additionally Philippe spent 10 hours this week doing a detailed manual
      inspection and review of the 12,461 patched files from the initial
      patch version early this week with:
    
       - a full scancode scan run, collecting the matched texts, detected
         license ids and scores
    
       - reviewing anything where there was a license detected (about 500+
         files) to ensure that the applied SPDX license was correct
    
       - reviewing anything where there was no detection but the patch
         license was not GPL-2.0 WITH Linux-syscall-note to ensure that the
         applied SPDX license was correct
    
      This produced a worksheet with 20 files needing minor correction. This
      worksheet was then exported into 3 different .csv files for the
      different types of files to be modified.
    
      These .csv files were then reviewed by Greg. Thomas wrote a script to
      parse the csv files and add the proper SPDX tag to the file, in the
      format that the file expected. This script was further refined by Greg
      based on the output to detect more types of files automatically and to
      distinguish between header and source .c files (which need different
      comment types.) Finally Greg ran the script using the .csv files to
      generate the patches.
    
      Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
      Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
      Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
      Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>"
    
    * tag 'spdx_identifiers-4.14-rc8' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core:
      License cleanup: add SPDX license identifier to uapi header files with a license
      License cleanup: add SPDX license identifier to uapi header files with no license
      License cleanup: add SPDX GPL-2.0 license identifier to files with no license

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 322d25ae23ab..efb6c17ca902 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_IO_H
 #define _ASM_X86_IO_H
 

commit 90edaac62729d3b9cbb97756261a0049a7fdd6a0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Oct 27 10:03:13 2017 +0200

    Revert "x86/mm: Limit mmap() of /dev/mem to valid physical addresses"
    
    This reverts commit ce56a86e2ade45d052b3228cdfebe913a1ae7381.
    
    There's unanticipated interaction with some boot parameters like 'mem=',
    which now cause the new checks via valid_mmap_phys_addr_range() to be too
    restrictive, crashing a Qemu bootup in fact, as reported by Fengguang Wu.
    
    So while the motivation of the change is still entirely valid, we
    need a few more rounds of testing to get it right - it's way too late
    after -rc6, so revert it for now.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Craig Bergstrom <craigb@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: dsafonov@virtuozzo.com
    Cc: kirill.shutemov@linux.intel.com
    Cc: mhocko@suse.com
    Cc: oleg@redhat.com
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 322d25ae23ab..c40a95c33bb8 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -110,10 +110,6 @@ build_mmio_write(__writeq, "q", unsigned long, "r", )
 
 #endif
 
-#define ARCH_HAS_VALID_PHYS_ADDR_RANGE
-extern int valid_phys_addr_range(phys_addr_t addr, size_t size);
-extern int valid_mmap_phys_addr_range(unsigned long pfn, size_t size);
-
 /**
  *	virt_to_phys	-	map virtual addresses to physical
  *	@address: address to remap

commit ce56a86e2ade45d052b3228cdfebe913a1ae7381
Author: Craig Bergstrom <craigb@google.com>
Date:   Thu Oct 19 13:28:56 2017 -0600

    x86/mm: Limit mmap() of /dev/mem to valid physical addresses
    
    Currently, it is possible to mmap() any offset from /dev/mem.  If a
    program mmaps() /dev/mem offsets outside of the addressable limits
    of a system, the page table can be corrupted by setting reserved bits.
    
    For example if you mmap() offset 0x0001000000000000 of /dev/mem on an
    x86_64 system with a 48-bit bus, the page fault handler will be called
    with error_code set to RSVD.  The kernel then crashes with a page table
    corruption error.
    
    This change prevents this page table corruption on x86 by refusing
    to mmap offsets higher than the highest valid address in the system.
    
    Signed-off-by: Craig Bergstrom <craigb@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: dsafonov@virtuozzo.com
    Cc: kirill.shutemov@linux.intel.com
    Cc: mhocko@suse.com
    Cc: oleg@redhat.com
    Link: http://lkml.kernel.org/r/20171019192856.39672-1-craigb@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index c40a95c33bb8..322d25ae23ab 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -110,6 +110,10 @@ build_mmio_write(__writeq, "q", unsigned long, "r", )
 
 #endif
 
+#define ARCH_HAS_VALID_PHYS_ADDR_RANGE
+extern int valid_phys_addr_range(phys_addr_t addr, size_t size);
+extern int valid_mmap_phys_addr_range(unsigned long pfn, size_t size);
+
 /**
  *	virt_to_phys	-	map virtual addresses to physical
  *	@address: address to remap

commit b1b6f83ac938d176742c85757960dec2cf10e468
Merge: 5f82e71a001d 9e52fc2b50de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 12:21:28 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "PCID support, 5-level paging support, Secure Memory Encryption support
    
      The main changes in this cycle are support for three new, complex
      hardware features of x86 CPUs:
    
       - Add 5-level paging support, which is a new hardware feature on
         upcoming Intel CPUs allowing up to 128 PB of virtual address space
         and 4 PB of physical RAM space - a 512-fold increase over the old
         limits. (Supercomputers of the future forecasting hurricanes on an
         ever warming planet can certainly make good use of more RAM.)
    
         Many of the necessary changes went upstream in previous cycles,
         v4.14 is the first kernel that can enable 5-level paging.
    
         This feature is activated via CONFIG_X86_5LEVEL=y - disabled by
         default.
    
         (By Kirill A. Shutemov)
    
       - Add 'encrypted memory' support, which is a new hardware feature on
         upcoming AMD CPUs ('Secure Memory Encryption', SME) allowing system
         RAM to be encrypted and decrypted (mostly) transparently by the
         CPU, with a little help from the kernel to transition to/from
         encrypted RAM. Such RAM should be more secure against various
         attacks like RAM access via the memory bus and should make the
         radio signature of memory bus traffic harder to intercept (and
         decrypt) as well.
    
         This feature is activated via CONFIG_AMD_MEM_ENCRYPT=y - disabled
         by default.
    
         (By Tom Lendacky)
    
       - Enable PCID optimized TLB flushing on newer Intel CPUs: PCID is a
         hardware feature that attaches an address space tag to TLB entries
         and thus allows to skip TLB flushing in many cases, even if we
         switch mm's.
    
         (By Andy Lutomirski)
    
      All three of these features were in the works for a long time, and
      it's coincidence of the three independent development paths that they
      are all enabled in v4.14 at once"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (65 commits)
      x86/mm: Enable RCU based page table freeing (CONFIG_HAVE_RCU_TABLE_FREE=y)
      x86/mm: Use pr_cont() in dump_pagetable()
      x86/mm: Fix SME encryption stack ptr handling
      kvm/x86: Avoid clearing the C-bit in rsvd_bits()
      x86/CPU: Align CR3 defines
      x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages
      acpi, x86/mm: Remove encryption mask from ACPI page protection type
      x86/mm, kexec: Fix memory corruption with SME on successive kexecs
      x86/mm/pkeys: Fix typo in Documentation/x86/protection-keys.txt
      x86/mm/dump_pagetables: Speed up page tables dump for CONFIG_KASAN=y
      x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID
      x86: Enable 5-level paging support via CONFIG_X86_5LEVEL=y
      x86/mm: Allow userspace have mappings above 47-bit
      x86/mm: Prepare to expose larger address space to userspace
      x86/mpx: Do not allow MPX if we have mappings above 47-bit
      x86/mm: Rename tasksize_32bit/64bit to task_size_32bit/64bit()
      x86/xen: Redefine XEN_ELFNOTE_INIT_P2M using PUD_SIZE * PTRS_PER_PUD
      x86/mm/dump_pagetables: Fix printout of p4d level
      x86/mm/dump_pagetables: Generalize address normalization
      x86/boot: Fix memremap() related build failure
      ...

commit 413d63d71b222108d19703f3fd5cf9108652a730
Merge: d6c8103b0265 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Aug 26 09:19:13 2017 +0200

    Merge branch 'linus' into x86/mm to pick up fixes and to fix conflicts
    
    Conflicts:
            arch/x86/kernel/head64.c
            arch/x86/mm/mmap.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1d0f49e14007a5426eb7e9e5808168cdb77b3e7f
Merge: 99504819fc64 e93c17301ac5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Aug 10 13:14:15 2017 +0200

    Merge branch 'x86/urgent' into x86/asm, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9683a64fc3cb67e663859a6bb2e0db5dcee9ed32
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Fri Jun 30 20:09:34 2017 +0300

    x86/io: Make readq() / writeq() API consistent
    
    Despite the following commit:
    
      93093d099e5d ("x86: provide readq()/writeq() on 32-bit too, complete")
    
    which says:
    
      ...Also, map all the APIs to the strongest ordering variant. It's way
      too easy to mess such details up in drivers and the difference between
      "memory" and "" constrained asm() constructs is in the noise range.
    
    ... we have for now only one user of this API (i.e. writeq_relaxed() in
    drivers/hwtracing/intel_th/sth.c) on x86 and it does care about
    "relaxed" part of it.
    
    Moreover 32-bit support has been removed from that header, though appeared
    later in specific headers that emphasizes its non-atomic context.
    
    The rest should keep in mind a consistent picture of the __raw_IO() vs. IO()
    vs. IO_relaxed() API.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Baolin Wang <baolin.wang@spreadtrum.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mika Westerberg <mika.westerberg@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: linux-i2c@vger.kernel.org
    Cc: wsa@the-dreams.de
    Link: http://lkml.kernel.org/r/20170630170934.83028-6-andriy.shevchenko@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index b3bba2f87e18..9ada93f01524 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -94,13 +94,15 @@ build_mmio_write(__writel, "l", unsigned int, "r", )
 #ifdef CONFIG_X86_64
 
 build_mmio_read(readq, "q", unsigned long, "=r", :"memory")
+build_mmio_read(__readq, "q", unsigned long, "=r", )
 build_mmio_write(writeq, "q", unsigned long, "r", :"memory")
+build_mmio_write(__writeq, "q", unsigned long, "r", )
 
-#define readq_relaxed(a)	readq(a)
-#define writeq_relaxed(v, a)	writeq(v, a)
+#define readq_relaxed(a)	__readq(a)
+#define writeq_relaxed(v, a)	__writeq(v, a)
 
-#define __raw_readq(a)		readq(a)
-#define __raw_writeq(val, addr)	writeq(val, addr)
+#define __raw_readq		__readq
+#define __raw_writeq		__writeq
 
 /* Let people know that we have them */
 #define readq			readq

commit eabc2a7c49c01fc97ff8c764ef7d74276b904af6
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Fri Jun 30 20:09:33 2017 +0300

    x86/io: Remove xlate_dev_kmem_ptr() duplication
    
    Generic header defines xlate_dev_kmem_ptr().
    
    Reuse it from generic header and remove in x86 code.
    Move a description to the generic header as well.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Baolin Wang <baolin.wang@spreadtrum.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mika Westerberg <mika.westerberg@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: linux-i2c@vger.kernel.org
    Cc: wsa@the-dreams.de
    Link: http://lkml.kernel.org/r/20170630170934.83028-5-andriy.shevchenko@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 252434b00fdb..b3bba2f87e18 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -215,11 +215,6 @@ extern void set_iounmap_nonlazy(void);
 
 #include <asm-generic/iomap.h>
 
-/*
- * Convert a virtual cached pointer to an uncached pointer
- */
-#define xlate_dev_kmem_ptr(p)	p
-
 /*
  * ISA space is 'always mapped' on a typical x86 system, no need to
  * explicitly ioremap() it. The fact that the ISA IO space is mapped

commit c2327da06b33d8e1093ce2c28f395bc500d1b0d3
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Fri Jun 30 20:09:32 2017 +0300

    x86/io: Remove mem*io() duplications
    
    Generic header defines memset_io, memcpy_fromio(). and memcpy_toio().
    
    Reuse them from generic header and remove in x86 code.
    Move the descriptions to the generic header as well.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Baolin Wang <baolin.wang@spreadtrum.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mika Westerberg <mika.westerberg@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: linux-i2c@vger.kernel.org
    Cc: wsa@the-dreams.de
    Link: http://lkml.kernel.org/r/20170630170934.83028-4-andriy.shevchenko@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 0558d81c177e..252434b00fdb 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -220,51 +220,6 @@ extern void set_iounmap_nonlazy(void);
  */
 #define xlate_dev_kmem_ptr(p)	p
 
-/**
- * memset_io	Set a range of I/O memory to a constant value
- * @addr:	The beginning of the I/O-memory range to set
- * @val:	The value to set the memory to
- * @count:	The number of bytes to set
- *
- * Set a range of I/O memory to a given value.
- */
-static inline void
-memset_io(volatile void __iomem *addr, unsigned char val, size_t count)
-{
-	memset((void __force *)addr, val, count);
-}
-#define memset_io(dst,c,count) memset_io(dst,c,count)
-
-/**
- * memcpy_fromio	Copy a block of data from I/O memory
- * @dst:		The (RAM) destination for the copy
- * @src:		The (I/O memory) source for the data
- * @count:		The number of bytes to copy
- *
- * Copy a block of data from I/O memory.
- */
-static inline void
-memcpy_fromio(void *dst, const volatile void __iomem *src, size_t count)
-{
-	memcpy(dst, (const void __force *)src, count);
-}
-#define memcpy_fromio(to,from,count) memcpy_fromio(to,from,count)
-
-/**
- * memcpy_toio		Copy a block of data into I/O memory
- * @dst:		The (I/O memory) destination for the copy
- * @src:		The (RAM) source for the data
- * @count:		The number of bytes to copy
- *
- * Copy a block of data to I/O memory.
- */
-static inline void
-memcpy_toio(volatile void __iomem *dst, const void *src, size_t count)
-{
-	memcpy((void __force *)dst, src, count);
-}
-#define memcpy_toio(to,from,count) memcpy_toio(to,from,count)
-
 /*
  * ISA space is 'always mapped' on a typical x86 system, no need to
  * explicitly ioremap() it. The fact that the ISA IO space is mapped

commit 3195201198aad06c151dcfcb09e2fff79b2fda0e
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Fri Jun 30 20:09:31 2017 +0300

    x86/io: Include asm-generic/io.h to architectural code
    
    asm-generic/io.h defines few helpers which would be useful in the drivers,
    such as writesb() and readsb().
    
    Include it to the asm/io.h in architectural folder.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Acked-by: Wolfram Sang <wsa@the-dreams.de>
    Cc: Baolin Wang <baolin.wang@spreadtrum.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mika Westerberg <mika.westerberg@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: linux-i2c@vger.kernel.org
    Link: http://lkml.kernel.org/r/20170630170934.83028-3-andriy.shevchenko@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index c1c0880768f7..0558d81c177e 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -406,6 +406,9 @@ extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
 
 #define IO_SPACE_LIMIT 0xffff
 
+#include <asm-generic/io.h>
+#undef PCI_IOBASE
+
 #ifdef CONFIG_MTRR
 extern int __must_check arch_phys_wc_index(int handle);
 #define arch_phys_wc_index arch_phys_wc_index

commit 80b9ece1339374444f08da46fa2383019fcf4374
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Fri Jun 30 20:09:30 2017 +0300

    x86/io: Define IO accessors by preprocessor
    
    As a preparatory to use generic IO accessor helpers we need to define
    architecture dependent functions via preprocessor to let world know we
    have them.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Acked-by: Wolfram Sang <wsa@the-dreams.de>
    Cc: Baolin Wang <baolin.wang@spreadtrum.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mika Westerberg <mika.westerberg@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: linux-i2c@vger.kernel.org
    Link: http://lkml.kernel.org/r/20170630170934.83028-2-andriy.shevchenko@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 7afb0e2f07f4..c1c0880768f7 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -69,6 +69,9 @@ build_mmio_write(__writeb, "b", unsigned char, "q", )
 build_mmio_write(__writew, "w", unsigned short, "r", )
 build_mmio_write(__writel, "l", unsigned int, "r", )
 
+#define readb readb
+#define readw readw
+#define readl readl
 #define readb_relaxed(a) __readb(a)
 #define readw_relaxed(a) __readw(a)
 #define readl_relaxed(a) __readl(a)
@@ -76,6 +79,9 @@ build_mmio_write(__writel, "l", unsigned int, "r", )
 #define __raw_readw __readw
 #define __raw_readl __readl
 
+#define writeb writeb
+#define writew writew
+#define writel writel
 #define writeb_relaxed(v, a) __writeb(v, a)
 #define writew_relaxed(v, a) __writew(v, a)
 #define writel_relaxed(v, a) __writel(v, a)
@@ -119,6 +125,7 @@ static inline phys_addr_t virt_to_phys(volatile void *address)
 {
 	return __pa(address);
 }
+#define virt_to_phys virt_to_phys
 
 /**
  *	phys_to_virt	-	map physical address to virtual
@@ -137,6 +144,7 @@ static inline void *phys_to_virt(phys_addr_t address)
 {
 	return __va(address);
 }
+#define phys_to_virt phys_to_virt
 
 /*
  * Change "struct page" to physical address.
@@ -169,11 +177,14 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
  * else, you probably want one of the following.
  */
 extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
+#define ioremap_nocache ioremap_nocache
 extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
 #define ioremap_uc ioremap_uc
 
 extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
+#define ioremap_cache ioremap_cache
 extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size, unsigned long prot_val);
+#define ioremap_prot ioremap_prot
 
 /**
  * ioremap     -   map bus memory into CPU space
@@ -193,8 +204,10 @@ static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
 {
 	return ioremap_nocache(offset, size);
 }
+#define ioremap ioremap
 
 extern void iounmap(volatile void __iomem *addr);
+#define iounmap iounmap
 
 extern void set_iounmap_nonlazy(void);
 
@@ -220,6 +233,7 @@ memset_io(volatile void __iomem *addr, unsigned char val, size_t count)
 {
 	memset((void __force *)addr, val, count);
 }
+#define memset_io(dst,c,count) memset_io(dst,c,count)
 
 /**
  * memcpy_fromio	Copy a block of data from I/O memory
@@ -234,6 +248,7 @@ memcpy_fromio(void *dst, const volatile void __iomem *src, size_t count)
 {
 	memcpy(dst, (const void __force *)src, count);
 }
+#define memcpy_fromio(to,from,count) memcpy_fromio(to,from,count)
 
 /**
  * memcpy_toio		Copy a block of data into I/O memory
@@ -248,6 +263,7 @@ memcpy_toio(volatile void __iomem *dst, const void *src, size_t count)
 {
 	memcpy((void __force *)dst, src, count);
 }
+#define memcpy_toio(to,from,count) memcpy_toio(to,from,count)
 
 /*
  * ISA space is 'always mapped' on a typical x86 system, no need to
@@ -341,13 +357,38 @@ BUILDIO(b, b, char)
 BUILDIO(w, w, short)
 BUILDIO(l, , int)
 
+#define inb inb
+#define inw inw
+#define inl inl
+#define inb_p inb_p
+#define inw_p inw_p
+#define inl_p inl_p
+#define insb insb
+#define insw insw
+#define insl insl
+
+#define outb outb
+#define outw outw
+#define outl outl
+#define outb_p outb_p
+#define outw_p outw_p
+#define outl_p outl_p
+#define outsb outsb
+#define outsw outsw
+#define outsl outsl
+
 extern void *xlate_dev_mem_ptr(phys_addr_t phys);
 extern void unxlate_dev_mem_ptr(phys_addr_t phys, void *addr);
 
+#define xlate_dev_mem_ptr xlate_dev_mem_ptr
+#define unxlate_dev_mem_ptr unxlate_dev_mem_ptr
+
 extern int ioremap_change_attr(unsigned long vaddr, unsigned long size,
 				enum page_cache_mode pcm);
 extern void __iomem *ioremap_wc(resource_size_t offset, unsigned long size);
+#define ioremap_wc ioremap_wc
 extern void __iomem *ioremap_wt(resource_size_t offset, unsigned long size);
+#define ioremap_wt ioremap_wt
 
 extern bool is_early_ioremap_ptep(pte_t *ptep);
 

commit 7206f9bf108eb9513d170c73f151367a1bdf3dbf
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Jul 19 14:53:02 2017 +0200

    x86/io: Add "memory" clobber to insb/insw/insl/outsb/outsw/outsl
    
    The x86 version of insb/insw/insl uses an inline assembly that does
    not have the target buffer listed as an output. This can confuse
    the compiler, leading it to think that a subsequent access of the
    buffer is uninitialized:
    
      drivers/net/wireless/wl3501_cs.c: In function wl3501_mgmt_scan_confirm:
      drivers/net/wireless/wl3501_cs.c:665:9: error: sig.status is used uninitialized in this function [-Werror=uninitialized]
      drivers/net/wireless/wl3501_cs.c:668:12: error: sig.cap_info may be used uninitialized in this function [-Werror=maybe-uninitialized]
      drivers/net/sb1000.c: In function 'sb1000_rx':
      drivers/net/sb1000.c:775:9: error: 'st[0]' is used uninitialized in this function [-Werror=uninitialized]
      drivers/net/sb1000.c:776:10: error: 'st[1]' may be used uninitialized in this function [-Werror=maybe-uninitialized]
      drivers/net/sb1000.c:784:11: error: 'st[1]' may be used uninitialized in this function [-Werror=maybe-uninitialized]
    
    I tried to mark the exact input buffer as an output here, but couldn't
    figure it out. As suggested by Linus, marking all memory as clobbered
    however is good enough too. For the outs operations, I also add the
    memory clobber, to force the input to be written to local variables.
    This is probably already guaranteed by the "asm volatile", but it can't
    hurt to do this for symmetry.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Link: http://lkml.kernel.org/r/20170719125310.2487451-5-arnd@arndb.de
    Link: https://lkml.org/lkml/2017/7/12/605
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 7afb0e2f07f4..48febf07e828 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -328,13 +328,13 @@ static inline unsigned type in##bwl##_p(int port)			\
 static inline void outs##bwl(int port, const void *addr, unsigned long count) \
 {									\
 	asm volatile("rep; outs" #bwl					\
-		     : "+S"(addr), "+c"(count) : "d"(port));		\
+		     : "+S"(addr), "+c"(count) : "d"(port) : "memory");	\
 }									\
 									\
 static inline void ins##bwl(int port, void *addr, unsigned long count)	\
 {									\
 	asm volatile("rep; ins" #bwl					\
-		     : "+D"(addr), "+c"(count) : "d"(port));		\
+		     : "+D"(addr), "+c"(count) : "d"(port) : "memory");	\
 }
 
 BUILDIO(b, b, char)

commit 8458bf94b0399cd1bca6c437366bcafb29c230c5
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:30 2017 -0500

    x86/mm: Use proper encryption attributes with /dev/mem
    
    When accessing memory using /dev/mem (or /dev/kmem) use the proper
    encryption attributes when mapping the memory.
    
    To insure the proper attributes are applied when reading or writing
    /dev/mem, update the xlate_dev_mem_ptr() function to use memremap()
    which will essentially perform the same steps of applying __va for
    RAM or using ioremap() if not RAM.
    
    To insure the proper attributes are applied when mmapping /dev/mem,
    update the phys_mem_access_prot() to call phys_mem_access_encrypted(),
    a new function which will check if the memory should be mapped encrypted
    or not. If it is not to be mapped encrypted then the VMA protection
    value is updated to remove the encryption bit.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/c917f403ab9f61cbfd455ad6425ed8429a5e7b54.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 09c5557b1454..e080a39b2108 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -386,4 +386,7 @@ extern bool arch_memremap_can_ram_remap(resource_size_t offset,
 					unsigned long flags);
 #define arch_memremap_can_ram_remap arch_memremap_can_ram_remap
 
+extern bool phys_mem_access_encrypted(unsigned long phys_addr,
+				      unsigned long size);
+
 #endif /* _ASM_X86_IO_H */

commit 8f716c9b5febf6ed0f5fedb7c9407cd0c25b2796
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:16 2017 -0500

    x86/mm: Add support to access boot related data in the clear
    
    Boot data (such as EFI related data) is not encrypted when the system is
    booted because UEFI/BIOS does not run with SME active. In order to access
    this data properly it needs to be mapped decrypted.
    
    Update early_memremap() to provide an arch specific routine to modify the
    pagetable protection attributes before they are applied to the new
    mapping. This is used to remove the encryption mask for boot related data.
    
    Update memremap() to provide an arch specific routine to determine if RAM
    remapping is allowed.  RAM remapping will cause an encrypted mapping to be
    generated. By preventing RAM remapping, ioremap_cache() will be used
    instead, which will provide a decrypted mapping of the boot related data.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/81fb6b4117a5df6b9f2eda342f81bbef4b23d2e5.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 7afb0e2f07f4..09c5557b1454 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -381,4 +381,9 @@ extern void arch_io_free_memtype_wc(resource_size_t start, resource_size_t size)
 #define arch_io_reserve_memtype_wc arch_io_reserve_memtype_wc
 #endif
 
+extern bool arch_memremap_can_ram_remap(resource_size_t offset,
+					unsigned long size,
+					unsigned long flags);
+#define arch_memremap_can_ram_remap arch_memremap_can_ram_remap
+
 #endif /* _ASM_X86_IO_H */

commit f58576666ccdcfb9cf7cae8669dffe1eed844f88
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Fri Jan 27 16:17:52 2017 -0700

    x86/mm: Improve documentation for low-level device I/O functions
    
    Add kerneldoc comments for memcpy_{to,from}io() and memset_io().  The
    existing documentation for ioremap() was distant from the definition,
    causing kernel-doc to miss it; move it appropriately.
    
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170127161752.0b95e95b@lwn.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index d34bd370074b..7afb0e2f07f4 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -164,6 +164,17 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
 #define virt_to_bus virt_to_phys
 #define bus_to_virt phys_to_virt
 
+/*
+ * The default ioremap() behavior is non-cached; if you need something
+ * else, you probably want one of the following.
+ */
+extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
+#define ioremap_uc ioremap_uc
+
+extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size, unsigned long prot_val);
+
 /**
  * ioremap     -   map bus memory into CPU space
  * @offset:    bus address of the memory
@@ -178,17 +189,6 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
  * If the area you are trying to map is a PCI BAR you should have a
  * look at pci_iomap().
  */
-extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
-extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
-#define ioremap_uc ioremap_uc
-
-extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
-extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size,
-				unsigned long prot_val);
-
-/*
- * The default ioremap() behavior is non-cached:
- */
 static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
 {
 	return ioremap_nocache(offset, size);
@@ -207,18 +207,42 @@ extern void set_iounmap_nonlazy(void);
  */
 #define xlate_dev_kmem_ptr(p)	p
 
+/**
+ * memset_io	Set a range of I/O memory to a constant value
+ * @addr:	The beginning of the I/O-memory range to set
+ * @val:	The value to set the memory to
+ * @count:	The number of bytes to set
+ *
+ * Set a range of I/O memory to a given value.
+ */
 static inline void
 memset_io(volatile void __iomem *addr, unsigned char val, size_t count)
 {
 	memset((void __force *)addr, val, count);
 }
 
+/**
+ * memcpy_fromio	Copy a block of data from I/O memory
+ * @dst:		The (RAM) destination for the copy
+ * @src:		The (I/O memory) source for the data
+ * @count:		The number of bytes to copy
+ *
+ * Copy a block of data from I/O memory.
+ */
 static inline void
 memcpy_fromio(void *dst, const volatile void __iomem *src, size_t count)
 {
 	memcpy(dst, (const void __force *)src, count);
 }
 
+/**
+ * memcpy_toio		Copy a block of data into I/O memory
+ * @dst:		The (I/O memory) destination for the copy
+ * @src:		The (RAM) source for the data
+ * @count:		The number of bytes to copy
+ *
+ * Copy a block of data to I/O memory.
+ */
 static inline void
 memcpy_toio(volatile void __iomem *dst, const void *src, size_t count)
 {

commit 8ef4227615e158faa4ee85a1d6466782f7e22f2f
Author: Dave Airlie <airlied@redhat.com>
Date:   Mon Oct 24 15:27:59 2016 +1000

    x86/io: add interface to reserve io memtype for a resource range. (v1.1)
    
    A recent change to the mm code in:
    87744ab3832b mm: fix cache mode tracking in vm_insert_mixed()
    
    started enforcing checking the memory type against the registered list for
    amixed pfn insertion mappings. It happens that the drm drivers for a number
    of gpus relied on this being broken. Currently the driver only inserted
    VRAM mappings into the tracking table when they came from the kernel,
    and userspace mappings never landed in the table. This led to a regression
    where all the mapping end up as UC instead of WC now.
    
    I've considered a number of solutions but since this needs to be fixed
    in fixes and not next, and some of the solutions were going to introduce
    overhead that hadn't been there before I didn't consider them viable at
    this stage. These mainly concerned hooking into the TTM io reserve APIs,
    but these API have a bunch of fast paths I didn't want to unwind to add
    this to.
    
    The solution I've decided on is to add a new API like the arch_phys_wc
    APIs (these would have worked but wc_del didn't take a range), and
    use them from the drivers to add a WC compatible mapping to the table
    for all VRAM on those GPUs. This means we can then create userspace
    mapping that won't get degraded to UC.
    
    v1.1: use CONFIG_X86_PAT + add some comments in io.h
    
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: x86@kernel.org
    Cc: mcgrof@suse.com
    Cc: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index de25aad07853..d34bd370074b 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -351,4 +351,10 @@ extern void arch_phys_wc_del(int handle);
 #define arch_phys_wc_add arch_phys_wc_add
 #endif
 
+#ifdef CONFIG_X86_PAT
+extern int arch_io_reserve_memtype_wc(resource_size_t start, resource_size_t size);
+extern void arch_io_free_memtype_wc(resource_size_t start, resource_size_t size);
+#define arch_io_reserve_memtype_wc arch_io_reserve_memtype_wc
+#endif
+
 #endif /* _ASM_X86_IO_H */

commit 12f03ee606914317e7e6a0815e53a48205c31dae
Merge: d9241b22b58e 004f1afbe199
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 8 14:35:59 2015 -0700

    Merge tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "This update has successfully completed a 0day-kbuild run and has
      appeared in a linux-next release.  The changes outside of the typical
      drivers/nvdimm/ and drivers/acpi/nfit.[ch] paths are related to the
      removal of IORESOURCE_CACHEABLE, the introduction of memremap(), and
      the introduction of ZONE_DEVICE + devm_memremap_pages().
    
      Summary:
    
       - Introduce ZONE_DEVICE and devm_memremap_pages() as a generic
         mechanism for adding device-driver-discovered memory regions to the
         kernel's direct map.
    
         This facility is used by the pmem driver to enable pfn_to_page()
         operations on the page frames returned by DAX ('direct_access' in
         'struct block_device_operations').
    
         For now, the 'memmap' allocation for these "device" pages comes
         from "System RAM".  Support for allocating the memmap from device
         memory will arrive in a later kernel.
    
       - Introduce memremap() to replace usages of ioremap_cache() and
         ioremap_wt().  memremap() drops the __iomem annotation for these
         mappings to memory that do not have i/o side effects.  The
         replacement of ioremap_cache() with memremap() is limited to the
         pmem driver to ease merging the api change in v4.3.
    
         Completion of the conversion is targeted for v4.4.
    
       - Similar to the usage of memcpy_to_pmem() + wmb_pmem() in the pmem
         driver, update the VFS DAX implementation and PMEM api to provide
         persistence guarantees for kernel operations on a DAX mapping.
    
       - Convert the ACPI NFIT 'BLK' driver to map the block apertures as
         cacheable to improve performance.
    
       - Miscellaneous updates and fixes to libnvdimm including support for
         issuing "address range scrub" commands, clarifying the optimal
         'sector size' of pmem devices, a clarification of the usage of the
         ACPI '_STA' (status) property for DIMM devices, and other minor
         fixes"
    
    * tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (34 commits)
      libnvdimm, pmem: direct map legacy pmem by default
      libnvdimm, pmem: 'struct page' for pmem
      libnvdimm, pfn: 'struct page' provider infrastructure
      x86, pmem: clarify that ARCH_HAS_PMEM_API implies PMEM mapped WB
      add devm_memremap_pages
      mm: ZONE_DEVICE for "device memory"
      mm: move __phys_to_pfn and __pfn_to_phys to asm/generic/memory_model.h
      dax: drop size parameter to ->direct_access()
      nd_blk: change aperture mapping from WC to WB
      nvdimm: change to use generic kvfree()
      pmem, dax: have direct_access use __pmem annotation
      dax: update I/O path to do proper PMEM flushing
      pmem: add copy_from_iter_pmem() and clear_pmem()
      pmem, x86: clean up conditional pmem includes
      pmem: remove layer when calling arch_has_wmb_pmem()
      pmem, x86: move x86 PMEM API to new pmem.h header
      libnvdimm, e820: make CONFIG_X86_PMEM_LEGACY a tristate option
      pmem: switch to devm_ allocations
      devres: add devm_memremap
      libnvdimm, btt: write and validate parent_uuid
      ...

commit 67a3e8fe90156d41cd480d3dfbb40f3bc007c262
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Thu Aug 27 13:14:20 2015 -0600

    nd_blk: change aperture mapping from WC to WB
    
    This should result in a pretty sizeable performance gain for reads.  For
    rough comparison I did some simple read testing using PMEM to compare
    reads of write combining (WC) mappings vs write-back (WB).  This was
    done on a random lab machine.
    
    PMEM reads from a write combining mapping:
            # dd of=/dev/null if=/dev/pmem0 bs=4096 count=100000
            100000+0 records in
            100000+0 records out
            409600000 bytes (410 MB) copied, 9.2855 s, 44.1 MB/s
    
    PMEM reads from a write-back mapping:
            # dd of=/dev/null if=/dev/pmem0 bs=4096 count=1000000
            1000000+0 records in
            1000000+0 records out
            4096000000 bytes (4.1 GB) copied, 3.44034 s, 1.2 GB/s
    
    To be able to safely support a write-back aperture I needed to add
    support for the "read flush" _DSM flag, as outlined in the DSM spec:
    
    http://pmem.io/documents/NVDIMM_DSM_Interface_Example.pdf
    
    This flag tells the ND BLK driver that it needs to flush the cache lines
    associated with the aperture after the aperture is moved but before any
    new data is read.  This ensures that any stale cache lines from the
    previous contents of the aperture will be discarded from the processor
    cache, and the new data will be read properly from the DIMM.  We know
    that the cache lines are clean and will be discarded without any
    writeback because either a) the previous aperture operation was a read,
    and we never modified the contents of the aperture, or b) the previous
    aperture operation was a write and we must have written back the dirtied
    contents of the aperture to the DIMM before the I/O was completed.
    
    In order to add support for the "read flush" flag I needed to add a
    generic routine to invalidate cache lines, mmio_flush_range().  This is
    protected by the ARCH_HAS_MMIO_FLUSH Kconfig variable, and is currently
    only supported on x86.
    
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index d241fbd5c87b..83ec9b1d77cc 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -248,8 +248,6 @@ static inline void flush_write_buffers(void)
 #endif
 }
 
-#define ARCH_MEMREMAP_PMEM MEMREMAP_WB
-
 #endif /* __KERNEL__ */
 
 extern void native_io_delay(void);

commit 8d58b66ed2b000f27658c88a4ed70e8042e86a58
Merge: 13fe86f465b7 c13dcf9f2d6f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Aug 25 09:59:19 2015 +0200

    Merge tag 'v4.2-rc8' into x86/mm, before applying new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e836a256e8fd579c9d7a3685f22981225a1ca451
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Aug 12 18:42:56 2015 -0400

    pmem: convert to generic memremap
    
    Kill arch_memremap_pmem() and just let the architecture specify the
    flags to be passed to memremap().  Default to writethrough by default.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index cc9c61bc1abe..d241fbd5c87b 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -248,11 +248,7 @@ static inline void flush_write_buffers(void)
 #endif
 }
 
-static inline void __pmem *arch_memremap_pmem(resource_size_t offset,
-	unsigned long size)
-{
-	return (void __force __pmem *) ioremap_cache(offset, size);
-}
+#define ARCH_MEMREMAP_PMEM MEMREMAP_WB
 
 #endif /* __KERNEL__ */
 

commit 8c7ea50c010b2f1e006ad37c43f98202a31de2cb
Author: Luis R. Rodriguez <mcgrof@suse.com>
Date:   Thu Jul 9 17:28:16 2015 -0700

    x86/mm, asm-generic: Add IOMMU ioremap_uc() variant default
    
    We currently have no safe way of currently defining architecture
    agnostic IOMMU ioremap_*() variants. The trend is for folks to
    *assume* that ioremap_nocache() should be the default everywhere
    and then add this mapping on each architectures -- this is not
    correct today for a variety of reasons.
    
    We have two options:
    
      1) Sit and wait for every architecture in Linux to get a
         an ioremap_*() variant defined before including it upstream.
    
      2) Gather consensus on a safe architecture agnostic ioremap_*()
         default.
    
    Approach 1) introduces development latencies, and since 2) will
    take time and work on clarifying semantics the only remaining
    sensible thing to do to avoid issues is returning NULL on
    ioremap_*() variants.
    
    In order for this to work we must have all architectures declare
    their own ioremap_*() variants as defined. This will take some
    work, do this for ioremp_uc() to set the example as its only
    currently implemented on x86. Document all this.
    
    We only provide implementation support for ioremap_uc() as the
    other ioremap_*() variants are well defined all over the kernel
    for other architectures already.
    
    Signed-off-by: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arnd@arndb.de
    Cc: benh@kernel.crashing.org
    Cc: bp@suse.de
    Cc: dan.j.williams@intel.com
    Cc: geert@linux-m68k.org
    Cc: hch@lst.de
    Cc: hmh@hmh.eng.br
    Cc: jgross@suse.com
    Cc: linux-mm@kvack.org
    Cc: luto@amacapital.net
    Cc: mpe@ellerman.id.au
    Cc: mst@redhat.com
    Cc: ralf@linux-mips.org
    Cc: ross.zwisler@linux.intel.com
    Cc: stefan.bader@canonical.com
    Cc: tj@kernel.org
    Cc: tomi.valkeinen@ti.com
    Cc: toshi.kani@hp.com
    Link: http://lkml.kernel.org/r/1436488096-3165-1-git-send-email-mcgrof@do-not-panic.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 83ec9b1d77cc..de25aad07853 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -180,6 +180,8 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
  */
 extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
 extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
+#define ioremap_uc ioremap_uc
+
 extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
 extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size,
 				unsigned long prot_val);

commit 88793e5c774ec69351ef6b5200bb59f532e41bca
Merge: 1bc5e157ed2b 61031952f4c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 29 10:34:42 2015 -0700

    Merge tag 'libnvdimm-for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/nvdimm
    
    Pull libnvdimm subsystem from Dan Williams:
     "The libnvdimm sub-system introduces, in addition to the
      libnvdimm-core, 4 drivers / enabling modules:
    
      NFIT:
        Instantiates an "nvdimm bus" with the core and registers memory
        devices (NVDIMMs) enumerated by the ACPI 6.0 NFIT (NVDIMM Firmware
        Interface table).
    
        After registering NVDIMMs the NFIT driver then registers "region"
        devices.  A libnvdimm-region defines an access mode and the
        boundaries of persistent memory media.  A region may span multiple
        NVDIMMs that are interleaved by the hardware memory controller.  In
        turn, a libnvdimm-region can be carved into a "namespace" device and
        bound to the PMEM or BLK driver which will attach a Linux block
        device (disk) interface to the memory.
    
      PMEM:
        Initially merged in v4.1 this driver for contiguous spans of
        persistent memory address ranges is re-worked to drive
        PMEM-namespaces emitted by the libnvdimm-core.
    
        In this update the PMEM driver, on x86, gains the ability to assert
        that writes to persistent memory have been flushed all the way
        through the caches and buffers in the platform to persistent media.
        See memcpy_to_pmem() and wmb_pmem().
    
      BLK:
        This new driver enables access to persistent memory media through
        "Block Data Windows" as defined by the NFIT.  The primary difference
        of this driver to PMEM is that only a small window of persistent
        memory is mapped into system address space at any given point in
        time.
    
        Per-NVDIMM windows are reprogrammed at run time, per-I/O, to access
        different portions of the media.  BLK-mode, by definition, does not
        support DAX.
    
      BTT:
        This is a library, optionally consumed by either PMEM or BLK, that
        converts a byte-accessible namespace into a disk with atomic sector
        update semantics (prevents sector tearing on crash or power loss).
    
        The sinister aspect of sector tearing is that most applications do
        not know they have a atomic sector dependency.  At least today's
        disk's rarely ever tear sectors and if they do one almost certainly
        gets a CRC error on access.  NVDIMMs will always tear and always
        silently.  Until an application is audited to be robust in the
        presence of sector-tearing the usage of BTT is recommended.
    
      Thanks to: Ross Zwisler, Jeff Moyer, Vishal Verma, Christoph Hellwig,
      Ingo Molnar, Neil Brown, Boaz Harrosh, Robert Elliott, Matthew Wilcox,
      Andy Rudoff, Linda Knippers, Toshi Kani, Nicholas Moulin, Rafael
      Wysocki, and Bob Moore"
    
    * tag 'libnvdimm-for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/nvdimm: (33 commits)
      arch, x86: pmem api for ensuring durability of persistent memory updates
      libnvdimm: Add sysfs numa_node to NVDIMM devices
      libnvdimm: Set numa_node to NVDIMM devices
      acpi: Add acpi_map_pxm_to_online_node()
      libnvdimm, nfit: handle unarmed dimms, mark namespaces read-only
      pmem: flag pmem block devices as non-rotational
      libnvdimm: enable iostat
      pmem: make_request cleanups
      libnvdimm, pmem: fix up max_hw_sectors
      libnvdimm, blk: add support for blk integrity
      libnvdimm, btt: add support for blk integrity
      fs/block_dev.c: skip rw_page if bdev has integrity
      libnvdimm: Non-Volatile Devices
      tools/testing/nvdimm: libnvdimm unit test infrastructure
      libnvdimm, nfit, nd_blk: driver for BLK-mode access persistent memory
      nd_btt: atomic sector updates
      libnvdimm: infrastructure for btt devices
      libnvdimm: write blk label set
      libnvdimm: write pmem label set
      libnvdimm: blk labels and namespace instantiation
      ...

commit 61031952f4c89dba1065f7a5b9419badb112554c
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Thu Jun 25 03:08:39 2015 -0400

    arch, x86: pmem api for ensuring durability of persistent memory updates
    
    Based on an original patch by Ross Zwisler [1].
    
    Writes to persistent memory have the potential to be posted to cpu
    cache, cpu write buffers, and platform write buffers (memory controller)
    before being committed to persistent media.  Provide apis,
    memcpy_to_pmem(), wmb_pmem(), and memremap_pmem(), to write data to
    pmem and assert that it is durable in PMEM (a persistent linear address
    range).  A '__pmem' attribute is added so sparse can track proper usage
    of pointers to pmem.
    
    This continues the status quo of pmem being x86 only for 4.2, but
    reworks to ioremap, and wider implementation of memremap() will enable
    other archs in 4.3.
    
    [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-May/000932.html
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    [djbw: various reworks]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 34a5b93704d3..c60c3f3b0183 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -247,6 +247,12 @@ static inline void flush_write_buffers(void)
 #endif
 }
 
+static inline void __pmem *arch_memremap_pmem(resource_size_t offset,
+	unsigned long size)
+{
+	return (void __force __pmem *) ioremap_cache(offset, size);
+}
+
 #endif /* __KERNEL__ */
 
 extern void native_io_delay(void);

commit d838270e2516db11084bed4e294017eb7b646a75
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Thu Jun 4 18:55:15 2015 +0200

    x86/mm, asm-generic: Add ioremap_wt() for creating Write-Through mappings
    
    Add ioremap_wt() for creating Write-Through mappings on x86. It
    follows the same model as ioremap_wc() for multi-arch support.
    Define ARCH_HAS_IOREMAP_WT in the x86 version of io.h to
    indicate that ioremap_wt() is implemented on x86.
    
    Also update the PAT documentation file to cover ioremap_wt().
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Elliott@hp.com
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arnd@arndb.de
    Cc: hch@lst.de
    Cc: hmh@hmh.eng.br
    Cc: jgross@suse.com
    Cc: konrad.wilk@oracle.com
    Cc: linux-mm <linux-mm@kvack.org>
    Cc: linux-nvdimm@lists.01.org
    Cc: stefan.bader@canonical.com
    Cc: yigal@plexistor.com
    Link: http://lkml.kernel.org/r/1433436928-31903-8-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index a94463063b46..83ec9b1d77cc 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -35,6 +35,7 @@
   */
 
 #define ARCH_HAS_IOREMAP_WC
+#define ARCH_HAS_IOREMAP_WT
 
 #include <linux/string.h>
 #include <linux/compiler.h>
@@ -320,6 +321,7 @@ extern void unxlate_dev_mem_ptr(phys_addr_t phys, void *addr);
 extern int ioremap_change_attr(unsigned long vaddr, unsigned long size,
 				enum page_cache_mode pcm);
 extern void __iomem *ioremap_wc(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_wt(resource_size_t offset, unsigned long size);
 
 extern bool is_early_ioremap_ptep(pte_t *ptep);
 

commit d6472302f242559d45dcf4ebace62508dc4d8aeb
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Jun 2 19:01:38 2015 +1000

    x86/mm: Decouple <linux/vmalloc.h> from <asm/io.h>
    
    Nothing in <asm/io.h> uses anything from <linux/vmalloc.h>, so
    remove it from there and fix up the resulting build problems
    triggered on x86 {64|32}-bit {def|allmod|allno}configs.
    
    The breakages were triggering in places where x86 builds relied
    on vmalloc() facilities but did not include <linux/vmalloc.h>
    explicitly and relied on the implicit inclusion via <asm/io.h>.
    
    Also add:
    
      - <linux/init.h> to <linux/io.h>
      - <asm/pgtable_types> to <asm/io.h>
    
    ... which were two other implicit header file dependencies.
    
    Suggested-by: David Miller <davem@davemloft.net>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    [ Tidied up the changelog. ]
    Acked-by: David Miller <davem@davemloft.net>
    Acked-by: Takashi Iwai <tiwai@suse.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Vinod Koul <vinod.koul@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anton Vorontsov <anton@enomsg.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Colin Cross <ccross@android.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: James E.J. Bottomley <JBottomley@odin.com>
    Cc: Jaroslav Kysela <perex@perex.cz>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Kristen Carlson Accardi <kristen@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Suma Ramars <sramars@cisco.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index a2b97404922d..a94463063b46 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -40,6 +40,7 @@
 #include <linux/compiler.h>
 #include <asm/page.h>
 #include <asm/early_ioremap.h>
+#include <asm/pgtable_types.h>
 
 #define build_mmio_read(name, size, type, reg, barrier) \
 static inline type name(const volatile void __iomem *addr) \
@@ -198,8 +199,6 @@ extern void set_iounmap_nonlazy(void);
 
 #include <asm-generic/iomap.h>
 
-#include <linux/vmalloc.h>
-
 /*
  * Convert a virtual cached pointer to an uncached pointer
  */

commit 7d010fdf299929f9583ce5e17da629dcd83c36ef
Author: Luis R. Rodriguez <mcgrof@suse.com>
Date:   Tue May 26 10:28:13 2015 +0200

    x86/mm/mtrr: Avoid #ifdeffery with phys_wc_to_mtrr_index()
    
    There is only one user but since we're going to bury MTRR next
    out of access to drivers, expose this last piece of API to
    drivers in a general fashion only needing io.h for access to
    helpers.
    
    Signed-off-by: Luis R. Rodriguez <mcgrof@suse.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Abhilash Kesavan <a.kesavan@samsung.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Antonino Daplas <adaplas@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Cristian Stoica <cristian.stoica@freescale.com>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Dave Airlie <airlied@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jean-Christophe Plagniol-Villard <plagnioj@jcrosoft.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthias Brugger <matthias.bgg@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Cc: Thierry Reding <treding@nvidia.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tomi Valkeinen <tomi.valkeinen@ti.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Ville Syrjl <syrjala@sci.fi>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: dri-devel@lists.freedesktop.org
    Link: http://lkml.kernel.org/r/1429722736-4473-1-git-send-email-mcgrof@do-not-panic.com
    Link: http://lkml.kernel.org/r/1432628901-18044-11-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 4afc05ffa566..a2b97404922d 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -339,6 +339,9 @@ extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
 #define IO_SPACE_LIMIT 0xffff
 
 #ifdef CONFIG_MTRR
+extern int __must_check arch_phys_wc_index(int handle);
+#define arch_phys_wc_index arch_phys_wc_index
+
 extern int __must_check arch_phys_wc_add(unsigned long base,
 					 unsigned long size);
 extern void arch_phys_wc_del(int handle);

commit e4b6be33c28923d8cde53023e0888b1c5d1a9027
Author: Luis R. Rodriguez <mcgrof@suse.com>
Date:   Mon May 11 10:15:53 2015 +0200

    x86/mm: Add ioremap_uc() helper to map memory uncacheable (not UC-)
    
    ioremap_nocache() currently uses UC- by default. Our goal is to
    eventually make UC the default. Linux maps UC- to PCD=1, PWT=0
    page attributes on non-PAT systems. Linux maps UC to PCD=1,
    PWT=1 page attributes on non-PAT systems. On non-PAT and PAT
    systems a WC MTRR has different effects on pages with either of
    these attributes. In order to help with a smooth transition its
    best to enable use of UC (PCD,1, PWT=1) on a region as that
    ensures a WC MTRR will have no effect on a region, this however
    requires us to have an way to declare a region as UC and we
    currently do not have a way to do this.
    
      WC MTRR on non-PAT system with PCD=1, PWT=0 (UC-) yields WC.
      WC MTRR on non-PAT system with PCD=1, PWT=1 (UC)  yields UC.
    
      WC MTRR on PAT system with PCD=1, PWT=0 (UC-) yields WC.
      WC MTRR on PAT system with PCD=1, PWT=1 (UC)  yields UC.
    
    A flip of the default ioremap_nocache() behaviour from UC- to UC
    can therefore regress a memory region from effective memory type
    WC to UC if MTRRs are used. Use of MTRRs should be phased out
    and in the best case only arch_phys_wc_add() use will remain,
    even if this happens arch_phys_wc_add() will have an effect on
    non-PAT systems and changes to default ioremap_nocache()
    behaviour could regress drivers.
    
    Now, ideally we'd use ioremap_nocache() on the regions in which
    we'd need uncachable memory types and avoid any MTRRs on those
    regions. There are however some restrictions on MTRRs use, such
    as the requirement of having the base and size of variable sized
    MTRRs to be powers of two, which could mean having to use a WC
    MTRR over a large area which includes a region in which
    write-combining effects are undesirable.
    
    Add ioremap_uc() to help with the both phasing out of MTRR use
    and also provide a way to blacklist small WC undesirable regions
    in devices with mixed regions which are size-implicated to use
    large WC MTRRs. Use of ioremap_uc() helps phase out MTRR use by
    avoiding regressions with an eventual flip of default behaviour
    or ioremap_nocache() from UC- to UC.
    
    Drivers working with WC MTRRs can use the below table to review
    and consider the use of ioremap*() and similar helpers to ensure
    appropriate behaviour long term even if default
    ioremap_nocache() behaviour changes from UC- to UC.
    
    Although ioremap_uc() is being added we leave set_memory_uc() to
    use UC- as only initial memory type setup is required to be able
    to accommodate existing device drivers and phase out MTRR use.
    It should also be clarified that set_memory_uc() cannot be used
    with IO memory, even though its use will not return any errors,
    it really has no effect.
    
      ----------------------------------------------------------------------
      MTRR Non-PAT   PAT    Linux ioremap value        Effective memory type
      ----------------------------------------------------------------------
                                                        Non-PAT |  PAT
           PAT
           |PCD
           ||PWT
           |||
      WC   000      WB      _PAGE_CACHE_MODE_WB            WC   |   WC
      WC   001      WC      _PAGE_CACHE_MODE_WC            WC*  |   WC
      WC   010      UC-     _PAGE_CACHE_MODE_UC_MINUS      WC*  |   WC
      WC   011      UC      _PAGE_CACHE_MODE_UC            UC   |   UC
      ----------------------------------------------------------------------
    
    Signed-off-by: Luis R. Rodriguez <mcgrof@suse.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Antonino Daplas <adaplas@gmail.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Dave Airlie <airlied@redhat.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Jean-Christophe Plagniol-Villard <plagnioj@jcrosoft.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Travis <travis@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Suresh Siddha <sbsiddha@gmail.com>
    Cc: Thierry Reding <treding@nvidia.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tomi Valkeinen <tomi.valkeinen@ti.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Ville Syrjl <syrjala@sci.fi>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-fbdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/1430343851-967-2-git-send-email-mcgrof@do-not-panic.com
    Link: http://lkml.kernel.org/r/1431332153-18566-9-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 34a5b93704d3..4afc05ffa566 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -177,6 +177,7 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
  * look at pci_iomap().
  */
 extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_uc(resource_size_t offset, unsigned long size);
 extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
 extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size,
 				unsigned long prot_val);

commit a023748d53c10850650fe86b1c4a7d421d576451
Merge: 773fed910d41 0dbcae884779
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 13:59:34 2014 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm tree changes from Ingo Molnar:
     "The biggest change is full PAT support from Jrgen Gross:
    
         The x86 architecture offers via the PAT (Page Attribute Table) a
         way to specify different caching modes in page table entries.  The
         PAT MSR contains 8 entries each specifying one of 6 possible cache
         modes.  A pte references one of those entries via 3 bits:
         _PAGE_PAT, _PAGE_PWT and _PAGE_PCD.
    
         The Linux kernel currently supports only 4 different cache modes.
         The PAT MSR is set up in a way that the setting of _PAGE_PAT in a
         pte doesn't matter: the top 4 entries in the PAT MSR are the same
         as the 4 lower entries.
    
         This results in the kernel not supporting e.g. write-through mode.
         Especially this cache mode would speed up drivers of video cards
         which now have to use uncached accesses.
    
         OTOH some old processors (Pentium) don't support PAT correctly and
         the Xen hypervisor has been using a different PAT MSR configuration
         for some time now and can't change that as this setting is part of
         the ABI.
    
         This patch set abstracts the cache mode from the pte and introduces
         tables to translate between cache mode and pte bits (the default
         cache mode "write back" is hard-wired to PAT entry 0).  The tables
         are statically initialized with values being compatible to old
         processors and current usage.  As soon as the PAT MSR is changed
         (or - in case of Xen - is read at boot time) the tables are changed
         accordingly.  Requests of mappings with special cache modes are
         always possible now, in case they are not supported there will be a
         fallback to a compatible but slower mode.
    
         Summing it up, this patch set adds the following features:
    
          - capability to support WT and WP cache modes on processors with
            full PAT support
    
          - processors with no or uncorrect PAT support are still working as
            today, even if WT or WP cache mode are selected by drivers for
            some pages
    
          - reduction of Xen special handling regarding cache mode
    
      Another change is a boot speedup on ridiculously large RAM systems,
      plus other smaller fixes"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      x86: mm: Move PAT only functions to mm/pat.c
      xen: Support Xen pv-domains using PAT
      x86: Enable PAT to use cache mode translation tables
      x86: Respect PAT bit when copying pte values between large and normal pages
      x86: Support PAT bit in pagetable dump for lower levels
      x86: Clean up pgtable_types.h
      x86: Use new cache mode type in memtype related functions
      x86: Use new cache mode type in mm/ioremap.c
      x86: Use new cache mode type in setting page attributes
      x86: Remove looking for setting of _PAGE_PAT_LARGE in pageattr.c
      x86: Use new cache mode type in track_pfn_remap() and track_pfn_insert()
      x86: Use new cache mode type in mm/iomap_32.c
      x86: Use new cache mode type in asm/pgtable.h
      x86: Use new cache mode type in arch/x86/mm/init_64.c
      x86: Use new cache mode type in arch/x86/pci
      x86: Use new cache mode type in drivers/video/fbdev/vermilion
      x86: Use new cache mode type in drivers/video/fbdev/gbefb.c
      x86: Use new cache mode type in include/asm/fb.h
      x86: Make page cache mode a real type
      x86: mm: Use 2GB memory block size on large-memory x86-64 systems
      ...

commit b14097bd911c2554b0b5271b3a6b2d84044d1843
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Nov 3 14:01:58 2014 +0100

    x86: Use new cache mode type in mm/ioremap.c
    
    Instead of directly using the cache mode bits in the pte switch to
    using the cache mode type.
    
    Based-on-patch-by: Stefan Bader <stefan.bader@canonical.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stefan.bader@canonical.com
    Cc: xen-devel@lists.xensource.com
    Cc: konrad.wilk@oracle.com
    Cc: ville.syrjala@linux.intel.com
    Cc: david.vrabel@citrix.com
    Cc: jbeulich@suse.com
    Cc: toshi.kani@hp.com
    Cc: plagnioj@jcrosoft.com
    Cc: tomi.valkeinen@ti.com
    Cc: bhelgaas@google.com
    Link: http://lkml.kernel.org/r/1415019724-4317-13-git-send-email-jgross@suse.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index b8237d8a1e0c..71b9e65daf25 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -314,7 +314,7 @@ extern void *xlate_dev_mem_ptr(unsigned long phys);
 extern void unxlate_dev_mem_ptr(unsigned long phys, void *addr);
 
 extern int ioremap_change_attr(unsigned long vaddr, unsigned long size,
-				unsigned long prot_val);
+				enum page_cache_mode pcm);
 extern void __iomem *ioremap_wc(resource_size_t offset, unsigned long size);
 
 extern bool is_early_ioremap_ptep(pte_t *ptep);

commit 1c8d29696f0d79902962526d6c54ebfeb842c61d
Merge: 3ba5acf368ae a8e0aead70b4
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Nov 11 19:55:45 2014 +0100

    Merge branch 'io' of git://git.kernel.org/pub/scm/linux/kernel/git/will/linux into asm-generic
    
    * 'io' of git://git.kernel.org/pub/scm/linux/kernel/git/will/linux:
      documentation: memory-barriers: clarify relaxed io accessor semantics
      x86: io: implement dummy relaxed accessor macros for writes
      tile: io: implement dummy relaxed accessor macros for writes
      sparc: io: implement dummy relaxed accessor macros for writes
      powerpc: io: implement dummy relaxed accessor macros for writes
      parisc: io: implement dummy relaxed accessor macros for writes
      mn10300: io: implement dummy relaxed accessor macros for writes
      m68k: io: implement dummy relaxed accessor macros for writes
      m32r: io: implement dummy relaxed accessor macros for writes
      ia64: io: implement dummy relaxed accessor macros for writes
      cris: io: implement dummy relaxed accessor macros for writes
      frv: io: implement dummy relaxed accessor macros for writes
      xtensa: io: remove dummy relaxed accessor macros for reads
      s390: io: remove dummy relaxed accessor macros for reads
      microblaze: io: remove dummy relaxed accessor macros
      asm-generic: io: implement relaxed accessor macros as conditional wrappers
    
    Conflicts:
            include/asm-generic/io.h
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 4707a341b4af57c72c1573a89d303559cf7bcf88
Author: Thierry Reding <treding@nvidia.com>
Date:   Mon Jul 28 17:20:33 2014 +0200

    /dev/mem: Use more consistent data types
    
    The xlate_dev_{kmem,mem}_ptr() functions take either a physical address
    or a kernel virtual address, so data types should be phys_addr_t and
    void *. They both return a kernel virtual address which is only ever
    used in calls to copy_{from,to}_user(), so make variables that store it
    void * rather than char * for consistency.
    
    Also only define a weak unxlate_dev_mem_ptr() function if architectures
    haven't overridden them in the asm/io.h header file.
    
    Signed-off-by: Thierry Reding <treding@nvidia.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index b8237d8a1e0c..ae2b593e7c6e 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -310,8 +310,8 @@ BUILDIO(b, b, char)
 BUILDIO(w, w, short)
 BUILDIO(l, , int)
 
-extern void *xlate_dev_mem_ptr(unsigned long phys);
-extern void unxlate_dev_mem_ptr(unsigned long phys, void *addr);
+extern void *xlate_dev_mem_ptr(phys_addr_t phys);
+extern void unxlate_dev_mem_ptr(phys_addr_t phys, void *addr);
 
 extern int ioremap_change_attr(unsigned long vaddr, unsigned long size,
 				unsigned long prot_val);

commit cbc908ef8e6babc40cb929f46ea1b8a26cbdbce0
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Sep 4 11:34:08 2013 +0100

    x86: io: implement dummy relaxed accessor macros for writes
    
    write{b,w,l,q}_relaxed are implemented by some architectures in order to
    permit memory-mapped I/O accesses with weaker barrier semantics than the
    non-relaxed variants.
    
    This patch adds dummy macros for the write accessors to x86, in the
    same vein as the dummy definitions for the relaxed read accessors.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index b8237d8a1e0c..2ea07f5ec7b7 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -74,6 +74,9 @@ build_mmio_write(__writel, "l", unsigned int, "r", )
 #define __raw_readw __readw
 #define __raw_readl __readl
 
+#define writeb_relaxed(v, a) __writeb(v, a)
+#define writew_relaxed(v, a) __writew(v, a)
+#define writel_relaxed(v, a) __writel(v, a)
 #define __raw_writeb __writeb
 #define __raw_writew __writew
 #define __raw_writel __writel
@@ -86,6 +89,7 @@ build_mmio_read(readq, "q", unsigned long, "=r", :"memory")
 build_mmio_write(writeq, "q", unsigned long, "r", :"memory")
 
 #define readq_relaxed(a)	readq(a)
+#define writeq_relaxed(v, a)	writeq(v, a)
 
 #define __raw_readq(a)		readq(a)
 #define __raw_writeq(val, addr)	writeq(val, addr)

commit 5b7c73e00968c7fdf908c3ced31e1cc83c01ba14
Author: Mark Salter <msalter@redhat.com>
Date:   Mon Apr 7 15:39:49 2014 -0700

    x86: use generic early_ioremap
    
    Move x86 over to the generic early ioremap implementation.
    
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 7cec9ef3a73a..b8237d8a1e0c 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -39,6 +39,7 @@
 #include <linux/string.h>
 #include <linux/compiler.h>
 #include <asm/page.h>
+#include <asm/early_ioremap.h>
 
 #define build_mmio_read(name, size, type, reg, barrier) \
 static inline type name(const volatile void __iomem *addr) \
@@ -316,20 +317,6 @@ extern int ioremap_change_attr(unsigned long vaddr, unsigned long size,
 				unsigned long prot_val);
 extern void __iomem *ioremap_wc(resource_size_t offset, unsigned long size);
 
-/*
- * early_ioremap() and early_iounmap() are for temporary early boot-time
- * mappings, before the real ioremap() is functional.
- * A boot-time mapping is currently limited to at most 16 pages.
- */
-extern void early_ioremap_init(void);
-extern void early_ioremap_reset(void);
-extern void __iomem *early_ioremap(resource_size_t phys_addr,
-				   unsigned long size);
-extern void *early_memremap(resource_size_t phys_addr,
-				    unsigned long size);
-extern void early_iounmap(void __iomem *addr, unsigned long size);
-extern void early_memunmap(void *addr, unsigned long size);
-extern void fixup_early_ioremap(void);
 extern bool is_early_ioremap_ptep(pte_t *ptep);
 
 #ifdef CONFIG_XEN

commit 6b550f6f2004017f1b4633d2c9e39b610bfe84f0
Author: Dave Young <dyoung@redhat.com>
Date:   Mon Apr 7 15:39:46 2014 -0700

    x86/mm: sparse warning fix for early_memremap
    
    This patch series takes the common bits from the x86 early ioremap
    implementation and creates a generic implementation which may be used by
    other architectures.  The early ioremap interfaces are intended for
    situations where boot code needs to make temporary virtual mappings
    before the normal ioremap interfaces are available.  Typically, this
    means before paging_init() has run.
    
    This patch (of 6):
    
    There's a lot of sparse warnings for code like below: void *a =
    early_memremap(phys_addr, size);
    
    early_memremap intend to map kernel memory with ioremap facility, the
    return pointer should be a kernel ram pointer instead of iomem one.
    
    For making the function clearer and supressing sparse warnings this patch
    do below two things:
    1. cast to (__force void *) for the return value of early_memremap
    2. add early_memunmap function and pass (__force void __iomem *) to iounmap
    
    From Boris:
      "Ingo told me yesterday, it makes sense too.  I'd guess we can try it.
       FWIW, all callers of early_memremap use the memory they get remapped
       as normal memory so we should be safe"
    
    Signed-off-by: Dave Young <dyoung@redhat.com>
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 91d9c69a629e..7cec9ef3a73a 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -325,9 +325,10 @@ extern void early_ioremap_init(void);
 extern void early_ioremap_reset(void);
 extern void __iomem *early_ioremap(resource_size_t phys_addr,
 				   unsigned long size);
-extern void __iomem *early_memremap(resource_size_t phys_addr,
+extern void *early_memremap(resource_size_t phys_addr,
 				    unsigned long size);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
+extern void early_memunmap(void *addr, unsigned long size);
 extern void fixup_early_ioremap(void);
 extern bool is_early_ioremap_ptep(pte_t *ptep);
 

commit 09df7c4c8097ca4a11393b1edd4997d786daad52
Author: Dave Jones <davej@redhat.com>
Date:   Mon Mar 10 19:32:22 2014 -0400

    x86: Remove CONFIG_X86_OOSTORE
    
    This was an optimization that made memcpy type benchmarks a little
    faster on ancient (Circa 1998) IDT Winchip CPUs.  In real-life
    workloads, it wasn't even noticable, and I doubt anyone is running
    benchmarks on 16 year old silicon any more.
    
    Given this code has likely seen very little use over the last decade,
    let's just remove it.
    
    Signed-off-by: Dave Jones <davej@fedoraproject.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 34f69cb9350a..91d9c69a629e 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -237,7 +237,7 @@ memcpy_toio(volatile void __iomem *dst, const void *src, size_t count)
 
 static inline void flush_write_buffers(void)
 {
-#if defined(CONFIG_X86_OOSTORE) || defined(CONFIG_X86_PPRO_FENCE)
+#if defined(CONFIG_X86_PPRO_FENCE)
 	asm volatile("lock; addl $0,0(%%esp)": : :"memory");
 #endif
 }

commit d0d98eedee2178c803dd824bb09f52b0e2ac1811
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon May 13 23:58:40 2013 +0000

    Add arch_phys_wc_{add, del} to manipulate WC MTRRs if needed
    
    Several drivers currently use mtrr_add through various #ifdef guards
    and/or drm wrappers.  The vast majority of them want to add WC MTRRs
    on x86 systems and don't actually need the MTRR if PAT (i.e.
    ioremap_wc, etc) are working.
    
    arch_phys_wc_add and arch_phys_wc_del are new functions, available
    on all architectures and configurations, that add WC MTRRs on x86 if
    needed (and handle errors) and do nothing at all otherwise.  They're
    also easier to use than mtrr_add and mtrr_del, so the call sites can
    be simplified.
    
    As an added benefit, this will avoid wasting MTRRs and possibly
    warning pointlessly on PAT-supporting systems.
    
    Reviewed-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index d8e8eefbe24c..34f69cb9350a 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -345,4 +345,11 @@ extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
 
 #define IO_SPACE_LIMIT 0xffff
 
+#ifdef CONFIG_MTRR
+extern int __must_check arch_phys_wc_add(unsigned long base,
+					 unsigned long size);
+extern void arch_phys_wc_del(int handle);
+#define arch_phys_wc_add arch_phys_wc_add
+#endif
+
 #endif /* _ASM_X86_IO_H */

commit 33f35f2a4ee3abfc0f87990058aa1b6b5092f725
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 3 22:00:38 2011 -1000

    x86: don't include xen/xen.h in <asm/io.h> unless XEN is enabled
    
    Dmitry Kasatkin reports:
      "kernel-devel package with kernel headers have no <include/xen>
       directory if XEN is disabled.  Modules which inclide asm/io.h won't
       compile.
    
       XEN related content is behind the CONFIG_XEN flag in the io.h.  And
       <xen/xen.h> should be also behind CONFIG_XEN flag."
    
    So move the include of <xen/xen.h> down into the section that is
    conditional on CONFIG_XEN.
    
    Reported-by: Dmitry Kasatkin <dmitry.kasatkin@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index d02804d650c4..d8e8eefbe24c 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -40,8 +40,6 @@
 #include <linux/compiler.h>
 #include <asm/page.h>
 
-#include <xen/xen.h>
-
 #define build_mmio_read(name, size, type, reg, barrier) \
 static inline type name(const volatile void __iomem *addr) \
 { type ret; asm volatile("mov" size " %1,%0":reg (ret) \
@@ -334,6 +332,7 @@ extern void fixup_early_ioremap(void);
 extern bool is_early_ioremap_ptep(pte_t *ptep);
 
 #ifdef CONFIG_XEN
+#include <xen/xen.h>
 struct bio_vec;
 
 extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,

commit dbee8a0affd5e6eaa5d7c816c4bc233f6f110f50
Author: Roland Dreier <roland@purestorage.com>
Date:   Tue May 24 17:13:09 2011 -0700

    x86: remove 32-bit versions of readq()/writeq()
    
    The presense of a writeq() implementation on 32-bit x86 that splits the
    64-bit write into two 32-bit writes turns out to break the mpt2sas driver
    (and in general is risky for drivers as was discussed in
    <http://lkml.kernel.org/r/adaab6c1h7c.fsf@cisco.com>).  To fix this,
    revert 2c5643b1c5c7 ("x86: provide readq()/writeq() on 32-bit too") and
    follow-on cleanups.
    
    This unfortunately leads to pushing non-atomic definitions of readq() and
    write() to various x86-only drivers that in the meantime started using the
    definitions in the x86 version of <asm/io.h>.  However as discussed
    exhaustively, this is actually the right thing to do, because the right
    way to split a 64-bit transaction is hardware dependent and therefore
    belongs in the hardware driver (eg mpt2sas needs a spinlock to make sure
    no other accesses occur in between the two halves of the access).
    
    Build tested on 32- and 64-bit x86 allmodconfig.
    
    Link: http://lkml.kernel.org/r/x86-32-writeq-is-broken@mdm.bga.com
    Acked-by: Hitoshi Mitake <h.mitake@gmail.com>
    Cc: Kashyap Desai <Kashyap.Desai@lsi.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Ravi Anand <ravi.anand@qlogic.com>
    Cc: Vikas Chaudhary <vikas.chaudhary@qlogic.com>
    Cc: Matthew Garrett <mjg@redhat.com>
    Cc: Jason Uhlenkott <juhlenko@akamai.com>
    Acked-by: James Bottomley <James.Bottomley@parallels.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 072273082528..d02804d650c4 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -38,7 +38,6 @@
 
 #include <linux/string.h>
 #include <linux/compiler.h>
-#include <asm-generic/int-ll64.h>
 #include <asm/page.h>
 
 #include <xen/xen.h>
@@ -87,27 +86,6 @@ build_mmio_write(__writel, "l", unsigned int, "r", )
 build_mmio_read(readq, "q", unsigned long, "=r", :"memory")
 build_mmio_write(writeq, "q", unsigned long, "r", :"memory")
 
-#else
-
-static inline __u64 readq(const volatile void __iomem *addr)
-{
-	const volatile u32 __iomem *p = addr;
-	u32 low, high;
-
-	low = readl(p);
-	high = readl(p + 1);
-
-	return low + ((u64)high << 32);
-}
-
-static inline void writeq(__u64 val, volatile void __iomem *addr)
-{
-	writel(val, addr);
-	writel(val >> 32, addr+4);
-}
-
-#endif
-
 #define readq_relaxed(a)	readq(a)
 
 #define __raw_readq(a)		readq(a)
@@ -117,6 +95,8 @@ static inline void writeq(__u64 val, volatile void __iomem *addr)
 #define readq			readq
 #define writeq			writeq
 
+#endif
+
 /**
  *	virt_to_phys	-	map virtual addresses to physical
  *	@address: address to remap

commit 18cb657ca1bafe635f368346a1676fb04c512edf
Merge: 2301b65b86df e28c31a96b15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 28 17:11:17 2010 -0700

    Merge branch 'stable/xen-pcifront-0.8.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
      and branch 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm
    
    * 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm:
      xen: register xen pci notifier
      xen: initialize cpu masks for pv guests in xen_smp_init
      xen: add a missing #include to arch/x86/pci/xen.c
      xen: mask the MTRR feature from the cpuid
      xen: make hvc_xen console work for dom0.
      xen: add the direct mapping area for ISA bus access
      xen: Initialize xenbus for dom0.
      xen: use vcpu_ops to setup cpu masks
      xen: map a dummy page for local apic and ioapic in xen_set_fixmap
      xen: remap MSIs into pirqs when running as initial domain
      xen: remap GSIs as pirqs when running as initial domain
      xen: introduce XEN_DOM0 as a silent option
      xen: map MSIs into pirqs
      xen: support GSI -> pirq remapping in PV on HVM guests
      xen: add xen hvm acpi_register_gsi variant
      acpi: use indirect call to register gsi in different modes
      xen: implement xen_hvm_register_pirq
      xen: get the maximum number of pirqs from xen
      xen: support pirq != irq
    
    * 'stable/xen-pcifront-0.8.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen: (27 commits)
      X86/PCI: Remove the dependency on isapnp_disable.
      xen: Update Makefile with CONFIG_BLOCK dependency for biomerge.c
      MAINTAINERS: Add myself to the Xen Hypervisor Interface and remove Chris Wright.
      x86: xen: Sanitse irq handling (part two)
      swiotlb-xen: On x86-32 builts, select SWIOTLB instead of depending on it.
      MAINTAINERS: Add myself for Xen PCI and Xen SWIOTLB maintainer.
      xen/pci: Request ACS when Xen-SWIOTLB is activated.
      xen-pcifront: Xen PCI frontend driver.
      xenbus: prevent warnings on unhandled enumeration values
      xenbus: Xen paravirtualised PCI hotplug support.
      xen/x86/PCI: Add support for the Xen PCI subsystem
      x86: Introduce x86_msi_ops
      msi: Introduce default_[teardown|setup]_msi_irqs with fallback.
      x86/PCI: Export pci_walk_bus function.
      x86/PCI: make sure _PAGE_IOMAP it set on pci mappings
      x86/PCI: Clean up pci_cache_line_size
      xen: fix shared irq device passthrough
      xen: Provide a variant of xen_poll_irq with timeout.
      xen: Find an unbound irq number in reverse order (high to low).
      xen: statically initialize cpu_evtchn_mask_p
      ...
    
    Fix up trivial conflicts in drivers/pci/Makefile

commit 3044100e58c84e133791c8b60a2f5bef69d732e4
Merge: b5153163ed58 67e87f0a1c5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 18:52:11 2010 -0700

    Merge branch 'core-memblock-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-memblock-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (74 commits)
      x86-64: Only set max_pfn_mapped to 512 MiB if we enter via head_64.S
      xen: Cope with unmapped pages when initializing kernel pagetable
      memblock, bootmem: Round pfn properly for memory and reserved regions
      memblock: Annotate memblock functions with __init_memblock
      memblock: Allow memblock_init to be called early
      memblock/arm: Fix memblock_region_is_memory() typo
      x86, memblock: Remove __memblock_x86_find_in_range_size()
      memblock: Fix wraparound in find_region()
      x86-32, memblock: Make add_highpages honor early reserved ranges
      x86, memblock: Fix crashkernel allocation
      arm, memblock: Fix the sparsemem build
      memblock: Fix section mismatch warnings
      powerpc, memblock: Fix memblock API change fallout
      memblock, microblaze: Fix memblock API change fallout
      x86: Remove old bootmem code
      x86, memblock: Use memblock_memory_size()/memblock_free_memory_size() to get correct dma_reserve
      x86: Remove not used early_res code
      x86, memblock: Replace e820_/_early string with memblock_
      x86: Use memblock to replace early_res
      x86, memblock: Use memblock_debug to control debug message print out
      ...
    
    Fix up trivial conflicts in arch/x86/kernel/setup.c and kernel/Makefile

commit d8e0420603cf1ce9cb459c00ea0b7337de41b968
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Feb 9 12:05:46 2009 -0800

    xen: define BIOVEC_PHYS_MERGEABLE()
    
    Impact: allow Xen control of bio merging
    
    When running in Xen domain with device access, we need to make sure
    the block subsystem doesn't merge requests across pages which aren't
    machine physically contiguous.  To do this, we define our own
    BIOVEC_PHYS_MERGEABLE.  When CONFIG_XEN isn't enabled, or we're not
    running in a Xen domain, this has identical behaviour to the normal
    implementation.  When running under Xen, we also make sure the
    underlying machine pages are the same or adjacent.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 30a3e9776123..0ad29d401565 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -41,6 +41,8 @@
 #include <asm-generic/int-ll64.h>
 #include <asm/page.h>
 
+#include <xen/xen.h>
+
 #define build_mmio_read(name, size, type, reg, barrier) \
 static inline type name(const volatile void __iomem *addr) \
 { type ret; asm volatile("mov" size " %1,%0":reg (ret) \
@@ -349,6 +351,17 @@ extern void __iomem *early_memremap(resource_size_t phys_addr,
 extern void early_iounmap(void __iomem *addr, unsigned long size);
 extern void fixup_early_ioremap(void);
 
+#ifdef CONFIG_XEN
+struct bio_vec;
+
+extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
+				      const struct bio_vec *vec2);
+
+#define BIOVEC_PHYS_MERGEABLE(vec1, vec2)				\
+	(__BIOVEC_PHYS_MERGEABLE(vec1, vec2) &&				\
+	 (!xen_domain() || xen_biovec_phys_mergeable(vec1, vec2)))
+#endif	/* CONFIG_XEN */
+
 #define IO_SPACE_LIMIT 0xffff
 
 #endif /* _ASM_X86_IO_H */

commit fef5ba797991f9335bcfc295942b684f9bf613a1
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Oct 13 16:02:24 2010 -0700

    xen: Cope with unmapped pages when initializing kernel pagetable
    
    Xen requires that all pages containing pagetable entries to be mapped
    read-only.  If pages used for the initial pagetable are already mapped
    then we can change the mapping to RO.  However, if they are initially
    unmapped, we need to make sure that when they are later mapped, they
    are also mapped RO.
    
    We do this by knowing that the kernel pagetable memory is pre-allocated
    in the range e820_table_start - e820_table_end, so any pfn within this
    range should be mapped read-only.  However, the pagetable setup code
    early_ioremaps the pages to write their entries, so we must make sure
    that mappings created in the early_ioremap fixmap area are mapped RW.
    (Those mappings are removed before the pages are presented to Xen
    as pagetable pages.)
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    LKML-Reference: <4CB63A80.8060702@goop.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 30a3e9776123..66aee6c4123b 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -348,6 +348,7 @@ extern void __iomem *early_memremap(resource_size_t phys_addr,
 				    unsigned long size);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
 extern void fixup_early_ioremap(void);
+extern bool is_early_ioremap_ptep(pte_t *ptep);
 
 #define IO_SPACE_LIMIT 0xffff
 

commit 3ee48b6af49cf534ca2f481ecc484b156a41451d
Author: Cliff Wickman <cpw@sgi.com>
Date:   Thu Sep 16 11:44:02 2010 -0500

    mm, x86: Saving vmcore with non-lazy freeing of vmas
    
    During the reading of /proc/vmcore the kernel is doing
    ioremap()/iounmap() repeatedly. And the buildup of un-flushed
    vm_area_struct's is causing a great deal of overhead. (rb_next()
    is chewing up most of that time).
    
    This solution is to provide function set_iounmap_nonlazy(). It
    causes a subsequent call to iounmap() to immediately purge the
    vma area (with try_purge_vmap_area_lazy()).
    
    With this patch we have seen the time for writing a 250MB
    compressed dump drop from 71 seconds to 44 seconds.
    
    Signed-off-by: Cliff Wickman <cpw@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: kexec@lists.infradead.org
    Cc: <stable@kernel.org>
    LKML-Reference: <E1OwHZ4-0005WK-Tw@eag09.americas.sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 30a3e9776123..6a45ec41ec26 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -206,6 +206,7 @@ static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
 
 extern void iounmap(volatile void __iomem *addr);
 
+extern void set_iounmap_nonlazy(void);
 
 #ifdef __KERNEL__
 

commit e67a807f3d9a82fa91817871f1c0e2e04da993b8
Author: Liang Li <liang.li@windriver.com>
Date:   Fri Apr 30 18:01:51 2010 +0800

    x86: Fix 'reservetop=' functionality
    
    When specifying the 'reservetop=0xbadc0de' kernel parameter,
    the kernel will stop booting due to a early_ioremap bug that
    relates to commit 8827247ff.
    
    The root cause of boot failure problem is the value of
    'slot_virt[i]' was initialized in setup_arch->early_ioremap_init().
    But later in setup_arch, the function 'parse_early_param' will
    modify 'FIXADDR_TOP' when 'reservetop=0xbadc0de' being specified.
    
    The simplest fix might be use __fix_to_virt(idx0) to get updated
    value of 'FIXADDR_TOP' in '__early_ioremap' instead of reference
    old value from slot_virt[slot] directly.
    
    Changelog since v0:
    
    -v1: When reservetop being handled then FIXADDR_TOP get
         adjusted, Hence check prev_map then re-initialize slot_virt and
         PMD based on new FIXADDR_TOP.
    
    -v2: place fixup_early_ioremap hence call early_ioremap_init in
         reserve_top_address  to re-initialize slot_virt and
         corresponding PMD when parse_reservertop
    
    -v3: move fixup_early_ioremap out of reserve_top_address to make
         sure other clients of reserve_top_address like xen/lguest won't
         broken
    
    Signed-off-by: Liang Li <liang.li@windriver.com>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Wang Chen <wangchen@cn.fujitsu.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <1272621711-8683-1-git-send-email-liang.li@windriver.com>
    [ fixed three small cleanliness details in fixup_early_ioremap() ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index a1dcfa3ab17d..30a3e9776123 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -347,6 +347,7 @@ extern void __iomem *early_ioremap(resource_size_t phys_addr,
 extern void __iomem *early_memremap(resource_size_t phys_addr,
 				    unsigned long size);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
+extern void fixup_early_ioremap(void);
 
 #define IO_SPACE_LIMIT 0xffff
 

commit 1c5b9069e12e20d2fe883076ae0bf73966492108
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Feb 5 09:37:09 2010 -0500

    x86: Merge io.h
    
    io_32.h and io_64.h are now identical.  Merge them into io.h.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1265380629-3212-8-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 73739322b6d0..a1dcfa3ab17d 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -1,8 +1,42 @@
 #ifndef _ASM_X86_IO_H
 #define _ASM_X86_IO_H
 
+/*
+ * This file contains the definitions for the x86 IO instructions
+ * inb/inw/inl/outb/outw/outl and the "string versions" of the same
+ * (insb/insw/insl/outsb/outsw/outsl). You can also use "pausing"
+ * versions of the single-IO instructions (inb_p/inw_p/..).
+ *
+ * This file is not meant to be obfuscating: it's just complicated
+ * to (a) handle it all in a way that makes gcc able to optimize it
+ * as well as possible and (b) trying to avoid writing the same thing
+ * over and over again with slight variations and possibly making a
+ * mistake somewhere.
+ */
+
+/*
+ * Thanks to James van Artsdalen for a better timing-fix than
+ * the two short jumps: using outb's to a nonexistent port seems
+ * to guarantee better timings even on fast machines.
+ *
+ * On the other hand, I'd like to be sure of a non-existent port:
+ * I feel a bit unsafe about using 0x80 (should be safe, though)
+ *
+ *		Linus
+ */
+
+ /*
+  *  Bit simplified and optimized by Jan Hubicka
+  *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999.
+  *
+  *  isa_memset_io, isa_memcpy_fromio, isa_memcpy_toio added,
+  *  isa_read[wl] and isa_write[wl] fixed
+  *  - Arnaldo Carvalho de Melo <acme@conectiva.com.br>
+  */
+
 #define ARCH_HAS_IOREMAP_WC
 
+#include <linux/string.h>
 #include <linux/compiler.h>
 #include <asm-generic/int-ll64.h>
 #include <asm/page.h>
@@ -173,11 +207,126 @@ static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
 extern void iounmap(volatile void __iomem *addr);
 
 
-#ifdef CONFIG_X86_32
-# include "io_32.h"
+#ifdef __KERNEL__
+
+#include <asm-generic/iomap.h>
+
+#include <linux/vmalloc.h>
+
+/*
+ * Convert a virtual cached pointer to an uncached pointer
+ */
+#define xlate_dev_kmem_ptr(p)	p
+
+static inline void
+memset_io(volatile void __iomem *addr, unsigned char val, size_t count)
+{
+	memset((void __force *)addr, val, count);
+}
+
+static inline void
+memcpy_fromio(void *dst, const volatile void __iomem *src, size_t count)
+{
+	memcpy(dst, (const void __force *)src, count);
+}
+
+static inline void
+memcpy_toio(volatile void __iomem *dst, const void *src, size_t count)
+{
+	memcpy((void __force *)dst, src, count);
+}
+
+/*
+ * ISA space is 'always mapped' on a typical x86 system, no need to
+ * explicitly ioremap() it. The fact that the ISA IO space is mapped
+ * to PAGE_OFFSET is pure coincidence - it does not mean ISA values
+ * are physical addresses. The following constant pointer can be
+ * used as the IO-area pointer (it can be iounmapped as well, so the
+ * analogy with PCI is quite large):
+ */
+#define __ISA_IO_base ((char __iomem *)(PAGE_OFFSET))
+
+/*
+ *	Cache management
+ *
+ *	This needed for two cases
+ *	1. Out of order aware processors
+ *	2. Accidentally out of order processors (PPro errata #51)
+ */
+
+static inline void flush_write_buffers(void)
+{
+#if defined(CONFIG_X86_OOSTORE) || defined(CONFIG_X86_PPRO_FENCE)
+	asm volatile("lock; addl $0,0(%%esp)": : :"memory");
+#endif
+}
+
+#endif /* __KERNEL__ */
+
+extern void native_io_delay(void);
+
+extern int io_delay_type;
+extern void io_delay_init(void);
+
+#if defined(CONFIG_PARAVIRT)
+#include <asm/paravirt.h>
 #else
-# include "io_64.h"
+
+static inline void slow_down_io(void)
+{
+	native_io_delay();
+#ifdef REALLY_SLOW_IO
+	native_io_delay();
+	native_io_delay();
+	native_io_delay();
 #endif
+}
+
+#endif
+
+#define BUILDIO(bwl, bw, type)						\
+static inline void out##bwl(unsigned type value, int port)		\
+{									\
+	asm volatile("out" #bwl " %" #bw "0, %w1"			\
+		     : : "a"(value), "Nd"(port));			\
+}									\
+									\
+static inline unsigned type in##bwl(int port)				\
+{									\
+	unsigned type value;						\
+	asm volatile("in" #bwl " %w1, %" #bw "0"			\
+		     : "=a"(value) : "Nd"(port));			\
+	return value;							\
+}									\
+									\
+static inline void out##bwl##_p(unsigned type value, int port)		\
+{									\
+	out##bwl(value, port);						\
+	slow_down_io();							\
+}									\
+									\
+static inline unsigned type in##bwl##_p(int port)			\
+{									\
+	unsigned type value = in##bwl(port);				\
+	slow_down_io();							\
+	return value;							\
+}									\
+									\
+static inline void outs##bwl(int port, const void *addr, unsigned long count) \
+{									\
+	asm volatile("rep; outs" #bwl					\
+		     : "+S"(addr), "+c"(count) : "d"(port));		\
+}									\
+									\
+static inline void ins##bwl(int port, void *addr, unsigned long count)	\
+{									\
+	asm volatile("rep; ins" #bwl					\
+		     : "+D"(addr), "+c"(count) : "d"(port));		\
+}
+
+BUILDIO(b, b, char)
+BUILDIO(w, w, short)
+BUILDIO(l, , int)
 
 extern void *xlate_dev_mem_ptr(unsigned long phys);
 extern void unxlate_dev_mem_ptr(unsigned long phys, void *addr);

commit 9b987aeb4a7bc42a3eb8361030b820b0263c31f1
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Apr 9 10:55:33 2009 -0700

    x86: fix set_fixmap to use phys_addr_t
    
    Impact: fix kprobes crash on 32-bit with RAM above 4G
    
    Use phys_addr_t for receiving a physical address argument
    instead of unsigned long. This allows fixmap to handle
    pages higher than 4GB on x86-32.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: systemtap-ml <systemtap@sources.redhat.com>
    Cc: Gary Hade <garyhade@us.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <49DE3695.6040800@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index e5383e3d2f8c..73739322b6d0 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -193,8 +193,10 @@ extern void __iomem *ioremap_wc(resource_size_t offset, unsigned long size);
  */
 extern void early_ioremap_init(void);
 extern void early_ioremap_reset(void);
-extern void __iomem *early_ioremap(unsigned long offset, unsigned long size);
-extern void __iomem *early_memremap(unsigned long offset, unsigned long size);
+extern void __iomem *early_ioremap(resource_size_t phys_addr,
+				   unsigned long size);
+extern void __iomem *early_memremap(resource_size_t phys_addr,
+				    unsigned long size);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
 
 #define IO_SPACE_LIMIT 0xffff

commit 4e8304758cc09a6097dbd2c4f44a5369e5c1edb0
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Mar 3 11:51:39 2009 -0800

    x86: remove vestigial fix_ioremap prototypes
    
    The function seems to have disappeared at some point, leaving
    some vestigial prototypes behind...
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 683d0b4c00fc..e5383e3d2f8c 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -172,8 +172,6 @@ static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
 
 extern void iounmap(volatile void __iomem *addr);
 
-extern void __iomem *fix_ioremap(unsigned idx, unsigned long phys);
-
 
 #ifdef CONFIG_X86_32
 # include "io_32.h"
@@ -198,7 +196,6 @@ extern void early_ioremap_reset(void);
 extern void __iomem *early_ioremap(unsigned long offset, unsigned long size);
 extern void __iomem *early_memremap(unsigned long offset, unsigned long size);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
-extern void __iomem *fix_ioremap(unsigned idx, unsigned long phys);
 
 #define IO_SPACE_LIMIT 0xffff
 

commit a7eb518998529c08cc53fef17756d9fe433b0c23
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Feb 17 13:01:51 2009 -0800

    x86: truncate ISA addresses to unsigned int
    
    Impact: Cleanup; fix inappropriate macro use
    
    ISA addresses on x86 are mapped 1:1 with the physical address space.
    Since the ISA address space is only 24 bits (32 for VLB or LPC) it
    will always fit in an unsigned int, and at least in the aha1542 driver
    using a wider type would cause an undesirable promotion.  Hence
    explicitly cast the ISA bus addresses to unsigned int.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: James Bottomley <James.Bottomley@hansenpartnership.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 4f8e820cf38f..683d0b4c00fc 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -124,10 +124,15 @@ static inline void *phys_to_virt(phys_addr_t address)
 
 /*
  * ISA I/O bus memory addresses are 1:1 with the physical address.
+ * However, we truncate the address to unsigned int to avoid undesirable
+ * promitions in legacy drivers.
  */
-#define isa_virt_to_bus (unsigned long)virt_to_phys
-#define isa_page_to_bus page_to_phys
-#define isa_bus_to_virt phys_to_virt
+static inline unsigned int isa_virt_to_bus(volatile void *address)
+{
+	return (unsigned int)virt_to_phys(address);
+}
+#define isa_page_to_bus(page)	((unsigned int)page_to_phys(page))
+#define isa_bus_to_virt		phys_to_virt
 
 /*
  * However PCI ones are not necessarily 1:1 and therefore these interfaces

commit bf33a70a73876b163d62612e9567cbac6604ba7e
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Fri Feb 13 12:52:44 2009 -0600

    x86: fix "__udivdi3" [drivers/scsi/aha1542.ko] undefined
    
    Commit 976e8f677e42757e5586ea04a9ac8bb8ddaa037e ("x86: asm/io.h: unify
    virt_to_phys/phys_to_virt") changed the return of virt_to_phys from long
    to phys_addr_t which is unsigned long long on a PAE platform.
    
    So, I could suggest a fix below since isa addresses may never be above
    32 bits.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index e5a2ab44cd5c..4f8e820cf38f 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -125,7 +125,7 @@ static inline void *phys_to_virt(phys_addr_t address)
 /*
  * ISA I/O bus memory addresses are 1:1 with the physical address.
  */
-#define isa_virt_to_bus virt_to_phys
+#define isa_virt_to_bus (unsigned long)virt_to_phys
 #define isa_page_to_bus page_to_phys
 #define isa_bus_to_virt phys_to_virt
 

commit a56cdcb662032a732f7c4f35cc5a9acf37759d8c
Merge: 881c47760bc6 8e1568f35002 063f8913afb4 f5deb79679af 2c344e9d6e19 bd282422fe95 d88316c243e5 39ba5d43fc91
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:46:36 2009 +0100

    Merge branches 'x86/acpi', 'x86/asm', 'x86/cpudetect', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/doc', 'x86/header-fixes', 'x86/headers' and 'x86/minor-fixes' into x86/core

commit 881c47760bc66b43360337da37d2a9de4af865b0
Merge: ab639f3593f0 891393745aad
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:45:42 2009 +0100

    Merge branch 'x86/cleanups' into x86/core

commit 891393745aad5c5acdb01b6ce61c08d4cc064649
Merge: a448720ca324 8e4921515c1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 11 11:38:55 2009 +0100

    Merge commit 'v2.6.29-rc4' into x86/cleanups

commit 133822c5c038b265ddb6545cda3a4c88815c7d3d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 6 13:29:52 2009 -0800

    x86: asm/io.h: unify ioremap prototypes
    
    Impact: unify identical code
    
    asm/io_32.h and _64.h have identical prototypes for the ioremap family
    of functions.  The 32-bit header had a more descriptive comment.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 919e3b19f3ca..f150b1ecf920 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -138,6 +138,37 @@ static inline void *phys_to_virt(phys_addr_t address)
 #define virt_to_bus virt_to_phys
 #define bus_to_virt phys_to_virt
 
+/**
+ * ioremap     -   map bus memory into CPU space
+ * @offset:    bus address of the memory
+ * @size:      size of the resource to map
+ *
+ * ioremap performs a platform specific sequence of operations to
+ * make bus memory CPU accessible via the readb/readw/readl/writeb/
+ * writew/writel functions and the other mmio helpers. The returned
+ * address is not guaranteed to be usable directly as a virtual
+ * address.
+ *
+ * If the area you are trying to map is a PCI BAR you should have a
+ * look at pci_iomap().
+ */
+extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size,
+				unsigned long prot_val);
+
+/*
+ * The default ioremap() behavior is non-cached:
+ */
+static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
+{
+	return ioremap_nocache(offset, size);
+}
+
+extern void iounmap(volatile void __iomem *addr);
+
+extern void __iomem *fix_ioremap(unsigned idx, unsigned long phys);
+
 
 #ifdef CONFIG_X86_32
 # include "io_32.h"

commit 976e8f677e42757e5586ea04a9ac8bb8ddaa037e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 6 13:29:44 2009 -0800

    x86: asm/io.h: unify virt_to_phys/phys_to_virt
    
    Impact: unify identical code
    
    asm/io_32.h and _64.h has functionally identical definitions for
    virt_to_phys, phys_to_virt, page_to_phys, and the isa_* variants, so
    just unify them.
    
    The only slightly functional change is using phys_addr_t for the
    physical address argument and return val.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 1dbbdf4be9b4..919e3b19f3ca 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -5,6 +5,7 @@
 
 #include <linux/compiler.h>
 #include <asm-generic/int-ll64.h>
+#include <asm/page.h>
 
 #define build_mmio_read(name, size, type, reg, barrier) \
 static inline type name(const volatile void __iomem *addr) \
@@ -80,6 +81,64 @@ static inline void writeq(__u64 val, volatile void __iomem *addr)
 #define readq			readq
 #define writeq			writeq
 
+/**
+ *	virt_to_phys	-	map virtual addresses to physical
+ *	@address: address to remap
+ *
+ *	The returned physical address is the physical (CPU) mapping for
+ *	the memory address given. It is only valid to use this function on
+ *	addresses directly mapped or allocated via kmalloc.
+ *
+ *	This function does not give bus mappings for DMA transfers. In
+ *	almost all conceivable cases a device driver should not be using
+ *	this function
+ */
+
+static inline phys_addr_t virt_to_phys(volatile void *address)
+{
+	return __pa(address);
+}
+
+/**
+ *	phys_to_virt	-	map physical address to virtual
+ *	@address: address to remap
+ *
+ *	The returned virtual address is a current CPU mapping for
+ *	the memory address given. It is only valid to use this function on
+ *	addresses that have a kernel mapping
+ *
+ *	This function does not handle bus mappings for DMA transfers. In
+ *	almost all conceivable cases a device driver should not be using
+ *	this function
+ */
+
+static inline void *phys_to_virt(phys_addr_t address)
+{
+	return __va(address);
+}
+
+/*
+ * Change "struct page" to physical address.
+ */
+#define page_to_phys(page)    ((dma_addr_t)page_to_pfn(page) << PAGE_SHIFT)
+
+/*
+ * ISA I/O bus memory addresses are 1:1 with the physical address.
+ */
+#define isa_virt_to_bus virt_to_phys
+#define isa_page_to_bus page_to_phys
+#define isa_bus_to_virt phys_to_virt
+
+/*
+ * However PCI ones are not necessarily 1:1 and therefore these interfaces
+ * are forbidden in portable PCI drivers.
+ *
+ * Allow them on x86 for legacy drivers, though.
+ */
+#define virt_to_bus virt_to_phys
+#define bus_to_virt phys_to_virt
+
+
 #ifdef CONFIG_X86_32
 # include "io_32.h"
 #else

commit a448720ca3248e8a7a426336885549d6e923fd8e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jan 28 15:42:23 2009 -0800

    x86: unify asm/io.h: IO_SPACE_LIMIT
    
    Impact: Cleanup (trivial unification)
    
    Move common define IO_SPACE_LIMIT from <asm/io_*.h> to <asm/io.h>.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 05cfed4485fa..975207c08b3e 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -106,5 +106,6 @@ extern void __iomem *early_memremap(unsigned long offset, unsigned long size);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
 extern void __iomem *fix_ioremap(unsigned idx, unsigned long phys);
 
+#define IO_SPACE_LIMIT 0xffff
 
 #endif /* _ASM_X86_IO_H */

commit 74b6eb6b937df07d0757e8642b7538b07da4290f
Merge: 6a385db5ce7f 2d4d57db692e 8f6d86dc4178 b38b06659055 d5e397cb49b5 e56d0cfe7790 dbca1df48e89 fb746d0e1365 6522869c3466 d639bab8da86 042cbaf88ab4 5662a2f8e731 3b4b75700a24 30a0fb947a68
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 23:13:53 2009 +0100

    Merge branches 'x86/asm', 'x86/cleanups', 'x86/cpudetect', 'x86/debug', 'x86/doc', 'x86/header-fixes', 'x86/mm', 'x86/paravirt', 'x86/pat', 'x86/setup-v2', 'x86/subarch', 'x86/uaccess' and 'x86/urgent' into x86/core

commit d639bab8da86d330493487e8c0fea8ca31f53427
Author: venkatesh.pallipadi@intel.com <venkatesh.pallipadi@intel.com>
Date:   Fri Jan 9 16:13:13 2009 -0800

    x86 PAT: ioremap_wc should take resource_size_t parameter
    
    Impact: fix/extend ioremap_wc() beyond 4GB aperture on 32-bit
    
    ioremap_wc() was taking in unsigned long parameter, where as it should take
    64-bit resource_size_t parameter like other ioremap variants.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 05cfed4485fa..bdbb4b961605 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -91,7 +91,7 @@ extern void unxlate_dev_mem_ptr(unsigned long phys, void *addr);
 
 extern int ioremap_change_attr(unsigned long vaddr, unsigned long size,
 				unsigned long prot_val);
-extern void __iomem *ioremap_wc(unsigned long offset, unsigned long size);
+extern void __iomem *ioremap_wc(resource_size_t offset, unsigned long size);
 
 /*
  * early_ioremap() and early_iounmap() are for temporary early boot-time

commit a3c6018e565dc07cf3738ace6bbe412f97b1bba8
Author: Jan Beulich <jbeulich@novell.com>
Date:   Fri Jan 16 11:59:33 2009 +0000

    x86: fix assumed to be contiguous leaf page tables for kmap_atomic region (take 2)
    
    Debugging and original patch from Nick Piggin <npiggin@suse.de>
    
    The early fixmap pmd entry inserted at the very top of the KVA is causing the
    subsequent fixmap mapping code to not provide physically linear pte pages over
    the kmap atomic portion of the fixmap (which relies on said property to
    calculate pte addresses).
    
    This has caused weird boot failures in kmap_atomic much later in the boot
    process (initial userspace faults) on a 32-bit PAE system with a larger number
    of CPUs (smaller CPU counts tend not to run over into the next page so don't
    show up the problem).
    
    Solve this by attempting to clear out the page table, and copy any of its
    entries to the new one. Also, add a bug if a nonlinear condition is encountered
    and can't be resolved, which might save some hours of debugging if this fragile
    scheme ever breaks again...
    
    Once we have such logic, we can also use it to eliminate the early ioremap
    trickery around the page table setup for the fixmap area. This also fixes
    potential issues with FIX_* entries sharing the leaf page table with the early
    ioremap ones getting discarded by early_ioremap_clear() and not restored by
    early_ioremap_reset(). It at once eliminates the temporary (and configuration,
    namely NR_CPUS, dependent) unavailability of early fixed mappings during the
    time the fixmap area page tables get constructed.
    
    Finally, also replace the hard coded calculation of the initial table space
    needed for the fixmap area with a proper one, allowing kernels configured for
    large CPU counts to actually boot.
    
    Based-on: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 05cfed4485fa..1dbbdf4be9b4 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -99,7 +99,6 @@ extern void __iomem *ioremap_wc(unsigned long offset, unsigned long size);
  * A boot-time mapping is currently limited to at most 16 pages.
  */
 extern void early_ioremap_init(void);
-extern void early_ioremap_clear(void);
 extern void early_ioremap_reset(void);
 extern void __iomem *early_ioremap(unsigned long offset, unsigned long size);
 extern void __iomem *early_memremap(unsigned long offset, unsigned long size);

commit 181de82ee3ffda1175f89d50c991dae31b79280c
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed Dec 3 14:53:04 2008 +0900

    x86: remove dead BIO_VMERGE_BOUNDARY definition
    
    Impact: cleanup, remove dead code
    
    The block layer dropped the virtual merge feature
    (b8b3e16cfe6435d961f6aaebcfd52a1ff2a988c5).
    
    BIO_VMERGE_BOUNDARY definition is meaningless now.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 33513b9a67f3..05cfed4485fa 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -80,8 +80,6 @@ static inline void writeq(__u64 val, volatile void __iomem *addr)
 #define readq			readq
 #define writeq			writeq
 
-extern int iommu_bio_merge;
-
 #ifdef CONFIG_X86_32
 # include "io_32.h"
 #else

commit 93093d099e5dd0c258fd530c12668e828c20df41
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 30 10:20:20 2008 +0100

    x86: provide readq()/writeq() on 32-bit too, complete
    
    if HAVE_READQ/HAVE_WRITEQ are defined, the full range of readq/writeq
    APIs has to be provided to drivers:
    
     drivers/infiniband/hw/amso1100/c2.c: In function 'c2_tx_ring_alloc':
     drivers/infiniband/hw/amso1100/c2.c:133: error: implicit declaration of function '__raw_writeq'
    
    So provide them on 32-bit as well. Also, map all the APIs to the
    strongest ordering variant. It's way too easy to mess such details
    up in drivers and the difference between "memory" and "" constrained
    asm() constructs is in the noise range.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 3ccfaf610c89..33513b9a67f3 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -46,16 +46,11 @@ build_mmio_write(__writel, "l", unsigned int, "r", )
 #define mmiowb() barrier()
 
 #ifdef CONFIG_X86_64
+
 build_mmio_read(readq, "q", unsigned long, "=r", :"memory")
-build_mmio_read(__readq, "q", unsigned long, "=r", )
 build_mmio_write(writeq, "q", unsigned long, "r", :"memory")
-build_mmio_write(__writeq, "q", unsigned long, "r", )
-
-#define readq_relaxed(a) __readq(a)
-#define __raw_readq __readq
-#define __raw_writeq writeq
 
-#else  /* CONFIG_X86_32 from here */
+#else
 
 static inline __u64 readq(const volatile void __iomem *addr)
 {
@@ -76,9 +71,14 @@ static inline void writeq(__u64 val, volatile void __iomem *addr)
 
 #endif
 
+#define readq_relaxed(a)	readq(a)
+
+#define __raw_readq(a)		readq(a)
+#define __raw_writeq(val, addr)	writeq(val, addr)
+
 /* Let people know that we have them */
-#define readq		readq
-#define writeq		writeq
+#define readq			readq
+#define writeq			writeq
 
 extern int iommu_bio_merge;
 

commit a0b1131e479e5af32eefac8bc54c9742e23d638e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 30 09:33:55 2008 +0100

    x86: provide readq()/writeq() on 32-bit too, cleanup
    
    Impact: cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 25946449df4f..3ccfaf610c89 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -55,21 +55,17 @@ build_mmio_write(__writeq, "q", unsigned long, "r", )
 #define __raw_readq __readq
 #define __raw_writeq writeq
 
-/* Let people know we have them */
-#define readq readq
-#define writeq writeq
-
 #else  /* CONFIG_X86_32 from here */
 
 static inline __u64 readq(const volatile void __iomem *addr)
 {
 	const volatile u32 __iomem *p = addr;
-	u32 l, h;
+	u32 low, high;
 
-	l = readl(p);
-	h = readl(p + 1);
+	low = readl(p);
+	high = readl(p + 1);
 
-	return l + ((u64)h << 32);
+	return low + ((u64)high << 32);
 }
 
 static inline void writeq(__u64 val, volatile void __iomem *addr)
@@ -78,11 +74,12 @@ static inline void writeq(__u64 val, volatile void __iomem *addr)
 	writel(val >> 32, addr+4);
 }
 
+#endif
+
+/* Let people know that we have them */
 #define readq		readq
 #define writeq		writeq
 
-#endif
-
 extern int iommu_bio_merge;
 
 #ifdef CONFIG_X86_32

commit 2c5643b1c5c7fbb13f340d4c58944d9642f41796
Author: Hitoshi Mitake <h.mitake@gmail.com>
Date:   Sun Nov 30 17:16:04 2008 +0900

    x86: provide readq()/writeq() on 32-bit too
    
    Impact: add new API for drivers
    
    Add implementation of readq/writeq to x86_32, and add config value to
    the x86 architecture to determine existence of readq/writeq.
    
    Signed-off-by: Hitoshi Mitake <h.mitake@gmail.com>
    Acked-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index ac2abc88cd95..25946449df4f 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -4,6 +4,7 @@
 #define ARCH_HAS_IOREMAP_WC
 
 #include <linux/compiler.h>
+#include <asm-generic/int-ll64.h>
 
 #define build_mmio_read(name, size, type, reg, barrier) \
 static inline type name(const volatile void __iomem *addr) \
@@ -57,6 +58,29 @@ build_mmio_write(__writeq, "q", unsigned long, "r", )
 /* Let people know we have them */
 #define readq readq
 #define writeq writeq
+
+#else  /* CONFIG_X86_32 from here */
+
+static inline __u64 readq(const volatile void __iomem *addr)
+{
+	const volatile u32 __iomem *p = addr;
+	u32 l, h;
+
+	l = readl(p);
+	h = readl(p + 1);
+
+	return l + ((u64)h << 32);
+}
+
+static inline void writeq(__u64 val, volatile void __iomem *addr)
+{
+	writel(val, addr);
+	writel(val >> 32, addr+4);
+}
+
+#define readq		readq
+#define writeq		writeq
+
 #endif
 
 extern int iommu_bio_merge;

commit 1d6cf1feb854c53c6d59e0d879603692b379e208
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Tue Oct 28 22:46:04 2008 -0700

    x86: start annotating early ioremap pointers with __iomem
    
    Impact: some new sparse warnings in e820.c etc, but no functional change.
    
    As with regular ioremap, iounmap etc, annotate with __iomem.
    
    Fixes the following sparse warnings, will produce some new ones
    elsewhere in arch/x86 that will get worked out over time.
    
    arch/x86/mm/ioremap.c:402:9: warning: cast removes address space of expression
    arch/x86/mm/ioremap.c:406:10: warning: cast adds address space to expression (<asn:2>)
    arch/x86/mm/ioremap.c:782:19: warning: Using plain integer as NULL pointer
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 5618a103f395..ac2abc88cd95 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -82,9 +82,9 @@ extern void __iomem *ioremap_wc(unsigned long offset, unsigned long size);
 extern void early_ioremap_init(void);
 extern void early_ioremap_clear(void);
 extern void early_ioremap_reset(void);
-extern void *early_ioremap(unsigned long offset, unsigned long size);
-extern void *early_memremap(unsigned long offset, unsigned long size);
-extern void early_iounmap(void *addr, unsigned long size);
+extern void __iomem *early_ioremap(unsigned long offset, unsigned long size);
+extern void __iomem *early_memremap(unsigned long offset, unsigned long size);
+extern void early_iounmap(void __iomem *addr, unsigned long size);
 extern void __iomem *fix_ioremap(unsigned idx, unsigned long phys);
 
 

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index a233f835e0b5..5618a103f395 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -1,5 +1,5 @@
-#ifndef ASM_X86__IO_H
-#define ASM_X86__IO_H
+#ifndef _ASM_X86_IO_H
+#define _ASM_X86_IO_H
 
 #define ARCH_HAS_IOREMAP_WC
 
@@ -88,4 +88,4 @@ extern void early_iounmap(void *addr, unsigned long size);
 extern void __iomem *fix_ioremap(unsigned idx, unsigned long phys);
 
 
-#endif /* ASM_X86__IO_H */
+#endif /* _ASM_X86_IO_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
new file mode 100644
index 000000000000..a233f835e0b5
--- /dev/null
+++ b/arch/x86/include/asm/io.h
@@ -0,0 +1,91 @@
+#ifndef ASM_X86__IO_H
+#define ASM_X86__IO_H
+
+#define ARCH_HAS_IOREMAP_WC
+
+#include <linux/compiler.h>
+
+#define build_mmio_read(name, size, type, reg, barrier) \
+static inline type name(const volatile void __iomem *addr) \
+{ type ret; asm volatile("mov" size " %1,%0":reg (ret) \
+:"m" (*(volatile type __force *)addr) barrier); return ret; }
+
+#define build_mmio_write(name, size, type, reg, barrier) \
+static inline void name(type val, volatile void __iomem *addr) \
+{ asm volatile("mov" size " %0,%1": :reg (val), \
+"m" (*(volatile type __force *)addr) barrier); }
+
+build_mmio_read(readb, "b", unsigned char, "=q", :"memory")
+build_mmio_read(readw, "w", unsigned short, "=r", :"memory")
+build_mmio_read(readl, "l", unsigned int, "=r", :"memory")
+
+build_mmio_read(__readb, "b", unsigned char, "=q", )
+build_mmio_read(__readw, "w", unsigned short, "=r", )
+build_mmio_read(__readl, "l", unsigned int, "=r", )
+
+build_mmio_write(writeb, "b", unsigned char, "q", :"memory")
+build_mmio_write(writew, "w", unsigned short, "r", :"memory")
+build_mmio_write(writel, "l", unsigned int, "r", :"memory")
+
+build_mmio_write(__writeb, "b", unsigned char, "q", )
+build_mmio_write(__writew, "w", unsigned short, "r", )
+build_mmio_write(__writel, "l", unsigned int, "r", )
+
+#define readb_relaxed(a) __readb(a)
+#define readw_relaxed(a) __readw(a)
+#define readl_relaxed(a) __readl(a)
+#define __raw_readb __readb
+#define __raw_readw __readw
+#define __raw_readl __readl
+
+#define __raw_writeb __writeb
+#define __raw_writew __writew
+#define __raw_writel __writel
+
+#define mmiowb() barrier()
+
+#ifdef CONFIG_X86_64
+build_mmio_read(readq, "q", unsigned long, "=r", :"memory")
+build_mmio_read(__readq, "q", unsigned long, "=r", )
+build_mmio_write(writeq, "q", unsigned long, "r", :"memory")
+build_mmio_write(__writeq, "q", unsigned long, "r", )
+
+#define readq_relaxed(a) __readq(a)
+#define __raw_readq __readq
+#define __raw_writeq writeq
+
+/* Let people know we have them */
+#define readq readq
+#define writeq writeq
+#endif
+
+extern int iommu_bio_merge;
+
+#ifdef CONFIG_X86_32
+# include "io_32.h"
+#else
+# include "io_64.h"
+#endif
+
+extern void *xlate_dev_mem_ptr(unsigned long phys);
+extern void unxlate_dev_mem_ptr(unsigned long phys, void *addr);
+
+extern int ioremap_change_attr(unsigned long vaddr, unsigned long size,
+				unsigned long prot_val);
+extern void __iomem *ioremap_wc(unsigned long offset, unsigned long size);
+
+/*
+ * early_ioremap() and early_iounmap() are for temporary early boot-time
+ * mappings, before the real ioremap() is functional.
+ * A boot-time mapping is currently limited to at most 16 pages.
+ */
+extern void early_ioremap_init(void);
+extern void early_ioremap_clear(void);
+extern void early_ioremap_reset(void);
+extern void *early_ioremap(unsigned long offset, unsigned long size);
+extern void *early_memremap(unsigned long offset, unsigned long size);
+extern void early_iounmap(void *addr, unsigned long size);
+extern void __iomem *fix_ioremap(unsigned idx, unsigned long phys);
+
+
+#endif /* ASM_X86__IO_H */
