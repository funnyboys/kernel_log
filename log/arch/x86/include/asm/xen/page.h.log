commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 790ce08e41f2..5941e18edd5a 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -11,7 +11,6 @@
 
 #include <asm/extable.h>
 #include <asm/page.h>
-#include <asm/pgtable.h>
 
 #include <xen/interface/xen.h>
 #include <xen/interface/grant_table.h>

commit 1457d8cf7664f34c4ba534c1073821a559a2f6f9
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Nov 7 18:01:00 2018 +0100

    x86/xen: fix pv boot
    
    Commit 9da3f2b7405440 ("x86/fault: BUG() when uaccess helpers fault on
    kernel addresses") introduced a regression for booting Xen PV guests.
    
    Xen PV guests are using __put_user() and __get_user() for accessing the
    p2m map (physical to machine frame number map) as accesses might fail
    in case of not populated areas of the map.
    
    With above commit using __put_user() and __get_user() for accessing
    kernel pages is no longer valid. So replace the Xen hack by adding
    appropriate p2m access functions using the default fixup handler.
    
    Fixes: 9da3f2b7405440 ("x86/fault: BUG() when uaccess helpers fault on kernel addresses")
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 123e669bf363..790ce08e41f2 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -9,7 +9,7 @@
 #include <linux/mm.h>
 #include <linux/device.h>
 
-#include <linux/uaccess.h>
+#include <asm/extable.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
 
@@ -93,12 +93,39 @@ clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
  */
 static inline int xen_safe_write_ulong(unsigned long *addr, unsigned long val)
 {
-	return __put_user(val, (unsigned long __user *)addr);
+	int ret = 0;
+
+	asm volatile("1: mov %[val], %[ptr]\n"
+		     "2:\n"
+		     ".section .fixup, \"ax\"\n"
+		     "3: sub $1, %[ret]\n"
+		     "   jmp 2b\n"
+		     ".previous\n"
+		     _ASM_EXTABLE(1b, 3b)
+		     : [ret] "+r" (ret), [ptr] "=m" (*addr)
+		     : [val] "r" (val));
+
+	return ret;
 }
 
-static inline int xen_safe_read_ulong(unsigned long *addr, unsigned long *val)
+static inline int xen_safe_read_ulong(const unsigned long *addr,
+				      unsigned long *val)
 {
-	return __get_user(*val, (unsigned long __user *)addr);
+	int ret = 0;
+	unsigned long rval = ~0ul;
+
+	asm volatile("1: mov %[ptr], %[rval]\n"
+		     "2:\n"
+		     ".section .fixup, \"ax\"\n"
+		     "3: sub $1, %[ret]\n"
+		     "   jmp 2b\n"
+		     ".previous\n"
+		     _ASM_EXTABLE(1b, 3b)
+		     : [ret] "+r" (ret), [rval] "+r" (rval)
+		     : [ptr] "m" (*addr));
+	*val = rval;
+
+	return ret;
 }
 
 #ifdef CONFIG_XEN_PV

commit 051089a2eed9a9977080774f3793ff2688cd3878
Merge: 974aa5630b31 646d944c2ef5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 13:06:27 2017 -0800

    Merge tag 'for-linus-4.15-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from Juergen Gross:
     "Xen features and fixes for v4.15-rc1
    
      Apart from several small fixes it contains the following features:
    
       - a series by Joao Martins to add vdso support of the pv clock
         interface
    
       - a series by Juergen Gross to add support for Xen pv guests to be
         able to run on 5 level paging hosts
    
       - a series by Stefano Stabellini adding the Xen pvcalls frontend
         driver using a paravirtualized socket interface"
    
    * tag 'for-linus-4.15-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (34 commits)
      xen/pvcalls: fix potential endless loop in pvcalls-front.c
      xen/pvcalls: Add MODULE_LICENSE()
      MAINTAINERS: xen, kvm: track pvclock-abi.h changes
      x86/xen/time: setup vcpu 0 time info page
      x86/xen/time: set pvclock flags on xen_time_init()
      x86/pvclock: add setter for pvclock_pvti_cpu0_va
      ptp_kvm: probe for kvm guest availability
      xen/privcmd: remove unused variable pageidx
      xen: select grant interface version
      xen: update arch/x86/include/asm/xen/cpuid.h
      xen: add grant interface version dependent constants to gnttab_ops
      xen: limit grant v2 interface to the v1 functionality
      xen: re-introduce support for grant v2 interface
      xen: support priv-mapping in an HVM tools domain
      xen/pvcalls: remove redundant check for irq >= 0
      xen/pvcalls: fix unsigned less than zero error check
      xen/time: Return -ENODEV from xen_get_wallclock()
      xen/pvcalls-front: mark expected switch fall-through
      xen: xenbus_probe_frontend: mark expected switch fall-throughs
      xen/time: do not decrease steal time after live migration on xen
      ...

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 07b6531813c4..c6b84245e5ab 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_XEN_PAGE_H
 #define _ASM_X86_XEN_PAGE_H
 

commit 6f0e8bf16730a36ff6773802d8c8df56d10e60cd
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Oct 27 19:49:37 2017 +0200

    xen: support 52 bit physical addresses in pv guests
    
    Physical addresses on processors supporting 5 level paging can be up to
    52 bits wide. For a Xen pv guest running on such a machine those
    physical addresses have to be supported in order to be able to use any
    memory on the machine even if the guest itself does not support 5 level
    paging.
    
    So when reading/writing a MFN from/to a pte don't use the kernel's
    PTE_PFN_MASK but a new XEN_PTE_MFN_MASK allowing full 40 bit wide MFNs.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 07b6531813c4..90e91003fd9d 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -26,6 +26,15 @@ typedef struct xpaddr {
 	phys_addr_t paddr;
 } xpaddr_t;
 
+#ifdef CONFIG_X86_64
+#define XEN_PHYSICAL_MASK	__sme_clr((1UL << 52) - 1)
+#else
+#define XEN_PHYSICAL_MASK	__PHYSICAL_MASK
+#endif
+
+#define XEN_PTE_MFN_MASK	((pteval_t)(((signed long)PAGE_MASK) & \
+					    XEN_PHYSICAL_MASK))
+
 #define XMADDR(x)	((xmaddr_t) { .maddr = (x) })
 #define XPADDR(x)	((xpaddr_t) { .paddr = (x) })
 
@@ -277,7 +286,7 @@ static inline unsigned long bfn_to_local_pfn(unsigned long mfn)
 
 static inline unsigned long pte_mfn(pte_t pte)
 {
-	return (pte.pte & PTE_PFN_MASK) >> PAGE_SHIFT;
+	return (pte.pte & XEN_PTE_MFN_MASK) >> PAGE_SHIFT;
 }
 
 static inline pte_t mfn_pte(unsigned long page_nr, pgprot_t pgprot)

commit 882bbe56aed0cbb00b374454f6c069919c9228dd
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Aug 4 13:36:12 2017 +0200

    xen: remove unused function xen_set_domain_pte()
    
    The function xen_set_domain_pte() is used nowhere in the kernel.
    Remove it.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 497f7d28c1d6..07b6531813c4 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -314,8 +314,6 @@ static inline pte_t __pte_ma(pteval_t x)
 #define p4d_val_ma(x)	((x).p4d)
 #endif
 
-void xen_set_domain_pte(pte_t *ptep, pte_t pteval, unsigned domid);
-
 xmaddr_t arbitrary_virt_to_machine(void *address);
 unsigned long arbitrary_virt_to_mfn(void *vaddr);
 void make_lowmem_page_readonly(void *vaddr);

commit 82616f9599a707e8225ceca6000dc5ea5aa78e11
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Aug 4 13:36:11 2017 +0200

    xen: remove tests for pvh mode in pure pv paths
    
    Remove the last tests for XENFEAT_auto_translated_physmap in pure
    PV-domain specific paths. PVH V1 is gone and the feature will always
    be "false" in PV guests.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 8417ef7c3885..497f7d28c1d6 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -158,9 +158,6 @@ static inline unsigned long mfn_to_pfn_no_overrides(unsigned long mfn)
 	unsigned long pfn;
 	int ret;
 
-	if (xen_feature(XENFEAT_auto_translated_physmap))
-		return mfn;
-
 	if (unlikely(mfn >= machine_to_phys_nr))
 		return ~0;
 

commit 3d4ebdb26f6f3573d68d7957f66a89e9401fc4b1
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 14 18:35:52 2017 +0100

    x86/xen: create stubs for HVM-only builds in page.h
    
    __pfn_to_mfn() is only used from PV code (mmu_pv.c, p2m.c) and from
    page.h where all functions calling it check for
    xen_feature(XENFEAT_auto_translated_physmap) first so we can replace
    it with any stub to make build happy.
    
    set_foreign_p2m_mapping()/clear_foreign_p2m_mapping() are used from
    grant-table.c but only if !xen_feature(XENFEAT_auto_translated_physmap).
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 8a5a02b1dfba..8417ef7c3885 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -52,12 +52,30 @@ extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern unsigned long __init set_phys_range_identity(unsigned long pfn_s,
 						    unsigned long pfn_e);
 
+#ifdef CONFIG_XEN_PV
 extern int set_foreign_p2m_mapping(struct gnttab_map_grant_ref *map_ops,
 				   struct gnttab_map_grant_ref *kmap_ops,
 				   struct page **pages, unsigned int count);
 extern int clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
 				     struct gnttab_unmap_grant_ref *kunmap_ops,
 				     struct page **pages, unsigned int count);
+#else
+static inline int
+set_foreign_p2m_mapping(struct gnttab_map_grant_ref *map_ops,
+			struct gnttab_map_grant_ref *kmap_ops,
+			struct page **pages, unsigned int count)
+{
+	return 0;
+}
+
+static inline int
+clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
+			  struct gnttab_unmap_grant_ref *kunmap_ops,
+			  struct page **pages, unsigned int count)
+{
+	return 0;
+}
+#endif
 
 /*
  * Helper functions to write or read unsigned long values to/from
@@ -73,6 +91,7 @@ static inline int xen_safe_read_ulong(unsigned long *addr, unsigned long *val)
 	return __get_user(*val, (unsigned long __user *)addr);
 }
 
+#ifdef CONFIG_XEN_PV
 /*
  * When to use pfn_to_mfn(), __pfn_to_mfn() or get_phys_to_machine():
  * - pfn_to_mfn() returns either INVALID_P2M_ENTRY or the mfn. No indicator
@@ -99,6 +118,12 @@ static inline unsigned long __pfn_to_mfn(unsigned long pfn)
 
 	return mfn;
 }
+#else
+static inline unsigned long __pfn_to_mfn(unsigned long pfn)
+{
+	return pfn;
+}
+#endif
 
 static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {

commit e5185a76a23b2d56fb2327ad8bd58fb1bcaa52b1
Merge: b678c91aefa7 4729277156cf
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 11 08:56:05 2017 +0200

    Merge branch 'x86/boot' into x86/mm, to avoid conflict
    
    There's a conflict between ongoing level-5 paging support and
    the E820 rewrite. Since the E820 rewrite is essentially ready,
    merge it into x86/mm to reduce tree conflicts.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f2a6a7050109e0a5c7a84c70aa6010f682b2f1ee
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Mar 17 21:55:15 2017 +0300

    x86: Convert the rest of the code to support p4d_t
    
    This patch converts x86 to use proper folding of a new (fifth) page table level
    with <asm-generic/pgtable-nop4d.h>.
    
    That's a bit of a kitchen sink patch, but I don't see how to split it further
    without hurting bisectability.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170317185515.8636-7-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 33cbd3db97b9..bf2ca56fba11 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -279,13 +279,17 @@ static inline pte_t __pte_ma(pteval_t x)
 
 #define pmd_val_ma(v) ((v).pmd)
 #ifdef __PAGETABLE_PUD_FOLDED
-#define pud_val_ma(v) ((v).pgd.pgd)
+#define pud_val_ma(v) ((v).p4d.pgd.pgd)
 #else
 #define pud_val_ma(v) ((v).pud)
 #endif
 #define __pmd_ma(x)	((pmd_t) { (x) } )
 
-#define pgd_val_ma(x)	((x).pgd)
+#ifdef __PAGETABLE_P4D_FOLDED
+#define p4d_val_ma(x)	((x).pgd.pgd)
+#else
+#define p4d_val_ma(x)	((x).p4d)
+#endif
 
 void xen_set_domain_pte(pte_t *ptep, pte_t pteval, unsigned domid);
 

commit 7fd5cf0b25181effd952e0c3d279395e35e0e9b7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jan 30 10:01:20 2017 +0100

    xen, x86/headers: Add <linux/device.h> dependency to <asm/xen/page.h>
    
    The following patch (not upstream yet):
    
      "x86/boot/e820: Remove spurious asm/e820/api.h inclusions"
    
    Removed the (spurious) <asm/e820.h> include line from <asm/pgtable.h> to
    reduce header file dependencies - but a Xen header has (unintentionally)
    learned to rely on the indirect inclusion of <linux/device.h>.
    
    This resulted in the following (harmless) build warning:
    
       arch/x86/include/asm/xen/page.h:302:7: warning: 'struct device' declared inside parameter list
    
    Include <linux/device.h> explicitly.
    
    No change in functionality.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: <stefano.stabellini@eu.citrix.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 33cbd3db97b9..64c5e745ebad 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -6,6 +6,7 @@
 #include <linux/spinlock.h>
 #include <linux/pfn.h>
 #include <linux/mm.h>
+#include <linux/device.h>
 
 #include <linux/uaccess.h>
 #include <asm/page.h>

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index f5fb840b43e8..33cbd3db97b9 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -7,7 +7,7 @@
 #include <linux/pfn.h>
 #include <linux/mm.h>
 
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
 

commit 291be10fd7511101d44cf98166d049bd31bc7600
Author: Julien Grall <julien.grall@citrix.com>
Date:   Wed Sep 9 15:17:33 2015 +0100

    xen/swiotlb: Pass addresses rather than frame numbers to xen_arch_need_swiotlb
    
    With 64KB page granularity support, the frame number will be different.
    
    It will be easier to modify the behavior in a single place rather than
    in each caller.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index fe58e3a935de..f5fb840b43e8 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -298,8 +298,8 @@ void make_lowmem_page_readwrite(void *vaddr);
 #define xen_unmap(cookie) iounmap((cookie))
 
 static inline bool xen_arch_need_swiotlb(struct device *dev,
-					 unsigned long pfn,
-					 unsigned long bfn)
+					 phys_addr_t phys,
+					 dma_addr_t dev_addr)
 {
 	return false;
 }

commit 008c320a96d218712043f8db0111d5472697785c
Author: Julien Grall <julien.grall@citrix.com>
Date:   Fri Jun 19 17:49:03 2015 +0100

    xen/grant: Introduce helpers to split a page into grant
    
    Currently, a grant is always based on the Xen page granularity (i.e
    4KB). When Linux is using a different page granularity, a single page
    will be split between multiple grants.
    
    The new helpers will be in charge of splitting the Linux page into grants
    and call a function given by the caller on each grant.
    
    Also provide an helper to count the number of grants within a given
    contiguous region.
    
    Note that the x86/include/asm/xen/page.h is now including
    xen/interface/grant_table.h rather than xen/grant_table.h. It's
    necessary because xen/grant_table.h depends on asm/xen/page.h and will
    break the compilation. Furthermore, only definition in
    interface/grant_table.h is required.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index b922fa4bb4a1..fe58e3a935de 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -12,7 +12,7 @@
 #include <asm/pgtable.h>
 
 #include <xen/interface/xen.h>
-#include <xen/grant_table.h>
+#include <xen/interface/grant_table.h>
 #include <xen/features.h>
 
 /* Xen machine address */

commit 8edfcf882eb91ec9028c7334f90f6ef3db5b0fcf
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Jul 22 14:48:09 2015 +0100

    x86/xen: export xen_alloc_p2m_entry()
    
    Rename alloc_p2m() to xen_alloc_p2m_entry() and export it.
    
    This is useful for ensuring that a p2m entry is allocated (i.e., not a
    shared missing or identity entry) so that subsequent set_phys_to_machine()
    calls will require no further allocations.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Daniel Kiper <daniel.kiper@oracle.com>
    ---
    v3:
    - Make xen_alloc_p2m_entry() a nop on auto-xlate guests.

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 0679e11d2cf7..b922fa4bb4a1 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -43,6 +43,8 @@ extern unsigned long *xen_p2m_addr;
 extern unsigned long  xen_p2m_size;
 extern unsigned long  xen_max_p2m_pfn;
 
+extern int xen_alloc_p2m_entry(unsigned long pfn);
+
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);

commit 0df4f266b3af90442bbeb5e685a84a80745beba0
Author: Julien Grall <julien.grall@citrix.com>
Date:   Fri Aug 7 17:34:37 2015 +0100

    xen: Use correctly the Xen memory terminologies
    
    Based on include/xen/mm.h [1], Linux is mistakenly using MFN when GFN
    is meant, I suspect this is because the first support for Xen was for
    PV. This resulted in some misimplementation of helpers on ARM and
    confused developers about the expected behavior.
    
    For instance, with pfn_to_mfn, we expect to get an MFN based on the name.
    Although, if we look at the implementation on x86, it's returning a GFN.
    
    For clarity and avoid new confusion, replace any reference to mfn with
    gfn in any helpers used by PV drivers. The x86 code will still keep some
    reference of pfn_to_mfn which may be used by all kind of guests
    No changes as been made in the hypercall field, even
    though they may be invalid, in order to keep the same as the defintion
    in xen repo.
    
    Note that page_to_mfn has been renamed to xen_page_to_gfn to avoid a
    name to close to the KVM function gfn_to_page.
    
    Take also the opportunity to simplify simple construction such
    as pfn_to_mfn(page_to_pfn(page)) into xen_page_to_gfn. More complex clean up
    will come in follow-up patches.
    
    [1] http://xenbits.xen.org/gitweb/?p=xen.git;a=commitdiff;h=e758ed14f390342513405dd766e874934573e6cb
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Acked-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 724bffbde54d..0679e11d2cf7 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -101,6 +101,11 @@ static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {
 	unsigned long mfn;
 
+	/*
+	 * Some x86 code are still using pfn_to_mfn instead of
+	 * pfn_to_mfn. This will have to be removed when we figured
+	 * out which call.
+	 */
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return pfn;
 
@@ -147,6 +152,11 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 {
 	unsigned long pfn;
 
+	/*
+	 * Some x86 code are still using mfn_to_pfn instead of
+	 * gfn_to_pfn. This will have to be removed when we figure
+	 * out which call.
+	 */
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return mfn;
 
@@ -176,9 +186,26 @@ static inline xpaddr_t machine_to_phys(xmaddr_t machine)
 	return XPADDR(PFN_PHYS(mfn_to_pfn(PFN_DOWN(machine.maddr))) | offset);
 }
 
+/* Pseudo-physical <-> Guest conversion */
+static inline unsigned long pfn_to_gfn(unsigned long pfn)
+{
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return pfn;
+	else
+		return pfn_to_mfn(pfn);
+}
+
+static inline unsigned long gfn_to_pfn(unsigned long gfn)
+{
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return gfn;
+	else
+		return mfn_to_pfn(gfn);
+}
+
 /* Pseudo-physical <-> Bus conversion */
-#define pfn_to_bfn(pfn)		pfn_to_mfn(pfn)
-#define bfn_to_pfn(bfn)		mfn_to_pfn(bfn)
+#define pfn_to_bfn(pfn)		pfn_to_gfn(pfn)
+#define bfn_to_pfn(bfn)		gfn_to_pfn(bfn)
 
 /*
  * We detect special mappings in one of two ways:
@@ -219,6 +246,10 @@ static inline unsigned long bfn_to_local_pfn(unsigned long mfn)
 #define virt_to_mfn(v)		(pfn_to_mfn(virt_to_pfn(v)))
 #define mfn_to_virt(m)		(__va(mfn_to_pfn(m) << PAGE_SHIFT))
 
+/* VIRT <-> GUEST conversion */
+#define virt_to_gfn(v)		(pfn_to_gfn(virt_to_pfn(v)))
+#define gfn_to_virt(g)		(__va(gfn_to_pfn(g) << PAGE_SHIFT))
+
 static inline unsigned long pte_mfn(pte_t pte)
 {
 	return (pte.pte & PTE_PFN_MASK) >> PAGE_SHIFT;

commit 32e09870eedfb501a6cb5729d8c23f44f8a7cbdd
Author: Julien Grall <julien.grall@citrix.com>
Date:   Fri Aug 7 17:34:35 2015 +0100

    xen: Make clear that swiotlb and biomerge are dealing with DMA address
    
    The swiotlb is required when programming a DMA address on ARM when a
    device is not protected by an IOMMU.
    
    In this case, the DMA address should always be equal to the machine address.
    For DOM0 memory, Xen ensure it by have an identity mapping between the
    guest address and host address. However, when mapping a foreign grant
    reference, the 1:1 model doesn't work.
    
    For ARM guest, most of the callers of pfn_to_mfn expects to get a GFN
    (Guest Frame Number), i.e a PFN (Page Frame Number) from the Linux point
    of view given that all ARM guest are auto-translated.
    
    Even though the name pfn_to_mfn is misleading, we need to ensure that
    those caller get a GFN and not by mistake a MFN. In pratical, I haven't
    seen error related to this but we should fix it for the sake of
    correctness.
    
    In order to fix the implementation of pfn_to_mfn on ARM in a follow-up
    patch, we have to introduce new helpers to return the DMA from a PFN and
    the invert.
    
    On x86, the new helpers will be an alias of pfn_to_mfn and mfn_to_pfn.
    
    The helpers will be used in swiotlb and xen_biovec_phys_mergeable.
    
    This is necessary in the latter because we have to ensure that the
    biovec code will not try to merge a biovec using foreign page and
    another using Linux memory.
    
    Lastly, the helper mfn_to_local_pfn has been renamed to bfn_to_local_pfn
    given that the only usage was in swiotlb.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index a3804fbe1f36..724bffbde54d 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -176,6 +176,10 @@ static inline xpaddr_t machine_to_phys(xmaddr_t machine)
 	return XPADDR(PFN_PHYS(mfn_to_pfn(PFN_DOWN(machine.maddr))) | offset);
 }
 
+/* Pseudo-physical <-> Bus conversion */
+#define pfn_to_bfn(pfn)		pfn_to_mfn(pfn)
+#define bfn_to_pfn(bfn)		mfn_to_pfn(bfn)
+
 /*
  * We detect special mappings in one of two ways:
  *  1. If the MFN is an I/O page then Xen will set the m2p entry
@@ -196,7 +200,7 @@ static inline xpaddr_t machine_to_phys(xmaddr_t machine)
  *      require. In all the cases we care about, the FOREIGN_FRAME bit is
  *      masked (e.g., pfn_to_mfn()) so behaviour there is correct.
  */
-static inline unsigned long mfn_to_local_pfn(unsigned long mfn)
+static inline unsigned long bfn_to_local_pfn(unsigned long mfn)
 {
 	unsigned long pfn;
 
@@ -262,7 +266,7 @@ void make_lowmem_page_readwrite(void *vaddr);
 
 static inline bool xen_arch_need_swiotlb(struct device *dev,
 					 unsigned long pfn,
-					 unsigned long mfn)
+					 unsigned long bfn)
 {
 	return false;
 }

commit cb3eb850137cd43fc3e25d2062525f5ba5fd884a
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:37 2015 +0200

    xen: remove no longer needed p2m.h
    
    Cleanup by removing arch/x86/xen/p2m.h as it isn't needed any more.
    
    Most definitions in this file are used in p2m.c only. Move those into
    p2m.c.
    
    set_phys_range_identity() is already declared in
    arch/x86/include/asm/xen/page.h, add __init annotation there.
    
    MAX_REMAP_RANGES isn't used at all, just delete it.
    
    The only define left is P2M_PER_PAGE which is moved to page.h as well.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 10ef14b2616d..a3804fbe1f36 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -35,6 +35,8 @@ typedef struct xpaddr {
 #define FOREIGN_FRAME(m)	((m) | FOREIGN_FRAME_BIT)
 #define IDENTITY_FRAME(m)	((m) | IDENTITY_FRAME_BIT)
 
+#define P2M_PER_PAGE		(PAGE_SIZE / sizeof(unsigned long))
+
 extern unsigned long *machine_to_phys_mapping;
 extern unsigned long  machine_to_phys_nr;
 extern unsigned long *xen_p2m_addr;
@@ -44,8 +46,8 @@ extern unsigned long  xen_max_p2m_pfn;
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
-extern unsigned long set_phys_range_identity(unsigned long pfn_s,
-					     unsigned long pfn_e);
+extern unsigned long __init set_phys_range_identity(unsigned long pfn_s,
+						    unsigned long pfn_e);
 
 extern int set_foreign_p2m_mapping(struct gnttab_map_grant_ref *map_ops,
 				   struct gnttab_map_grant_ref *kmap_ops,

commit c70727a5bc18a5a233fddc6056d1de9144d7a293
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:36 2015 +0200

    xen: allow more than 512 GB of RAM for 64 bit pv-domains
    
    64 bit pv-domains under Xen are limited to 512 GB of RAM today. The
    main reason has been the 3 level p2m tree, which was replaced by the
    virtual mapped linear p2m list. Parallel to the p2m list which is
    being used by the kernel itself there is a 3 level mfn tree for usage
    by the Xen tools and eventually for crash dump analysis. For this tree
    the linear p2m list can serve as a replacement, too. As the kernel
    can't know whether the tools are capable of dealing with the p2m list
    instead of the mfn tree, the limit of 512 GB can't be dropped in all
    cases.
    
    This patch replaces the hard limit by a kernel parameter which tells
    the kernel to obey the 512 GB limit or not. The default is selected by
    a configuration parameter which specifies whether the 512 GB limit
    should be active per default for domUs (domain save/restore/migration
    and crash dump analysis are affected).
    
    Memory above the domain limit is returned to the hypervisor instead of
    being identity mapped, which was wrong anyway.
    
    The kernel configuration parameter to specify the maximum size of a
    domain can be deleted, as it is not relevant any more.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index c44a5d53e464..10ef14b2616d 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -35,10 +35,6 @@ typedef struct xpaddr {
 #define FOREIGN_FRAME(m)	((m) | FOREIGN_FRAME_BIT)
 #define IDENTITY_FRAME(m)	((m) | IDENTITY_FRAME_BIT)
 
-/* Maximum amount of memory we can handle in a domain in pages */
-#define MAX_DOMAIN_PAGES						\
-    ((unsigned long)((u64)CONFIG_XEN_MAX_DOMAIN_MEMORY * 1024 * 1024 * 1024 / PAGE_SIZE))
-
 extern unsigned long *machine_to_phys_mapping;
 extern unsigned long  machine_to_phys_nr;
 extern unsigned long *xen_p2m_addr;

commit 8746515d7f04c9ea94cf43e2db1fd2cfca93276d
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Apr 24 10:16:40 2015 +0100

    xen: Add __GFP_DMA flag when xen_swiotlb_init gets free pages on ARM
    
    Make sure that xen_swiotlb_init allocates buffers that are DMA capable
    when at least one memblock is available below 4G. Otherwise we assume
    that all devices on the SoC can cope with >4G addresses. We do this on
    ARM and ARM64, where dom0 is mapped 1:1, so pfn == mfn in this case.
    
    No functional changes on x86.
    
    From: Chen Baozi <baozich@gmail.com>
    
    Signed-off-by: Chen Baozi <baozich@gmail.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Tested-by: Chen Baozi <baozich@gmail.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 358dcd338915..c44a5d53e464 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -269,4 +269,9 @@ static inline bool xen_arch_need_swiotlb(struct device *dev,
 	return false;
 }
 
+static inline unsigned long xen_get_swiotlb_free_pages(unsigned int order)
+{
+	return __get_free_pages(__GFP_NOWARN, order);
+}
+
 #endif /* _ASM_X86_XEN_PAGE_H */

commit 0bb599fd30108883b00c7d4a226eeb49111e6932
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jan 5 17:06:01 2015 +0000

    xen: remove scratch frames for ballooned pages and m2p override
    
    The scratch frame mappings for ballooned pages and the m2p override
    are broken.  Remove them in preparation for replacing them with
    simpler mechanisms that works.
    
    The scratch pages did not ensure that the page was not in use.  In
    particular, the foreign page could still be in use by hardware.  If
    the guest reused the frame the hardware could read or write that
    frame.
    
    The m2p override did not handle the same frame being granted by two
    different grant references.  Trying an M2P override lookup in this
    case is impossible.
    
    With the m2p override removed, the grant map/unmap for the kernel
    mappings (for x86 PV) can be easily batched in
    set_foreign_p2m_mapping() and clear_foreign_p2m_mapping().
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index e9f52fe2d56a..358dcd338915 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -57,7 +57,6 @@ extern int set_foreign_p2m_mapping(struct gnttab_map_grant_ref *map_ops,
 extern int clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
 				     struct gnttab_unmap_grant_ref *kunmap_ops,
 				     struct page **pages, unsigned int count);
-extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
 /*
  * Helper functions to write or read unsigned long values to/from
@@ -154,21 +153,12 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 		return mfn;
 
 	pfn = mfn_to_pfn_no_overrides(mfn);
-	if (__pfn_to_mfn(pfn) != mfn) {
-		/*
-		 * If this appears to be a foreign mfn (because the pfn
-		 * doesn't map back to the mfn), then check the local override
-		 * table to see if there's a better pfn to use.
-		 *
-		 * m2p_find_override_pfn returns ~0 if it doesn't find anything.
-		 */
-		pfn = m2p_find_override_pfn(mfn, ~0);
-	}
+	if (__pfn_to_mfn(pfn) != mfn)
+		pfn = ~0;
 
 	/*
-	 * pfn is ~0 if there are no entries in the m2p for mfn or if the
-	 * entry doesn't map back to the mfn and m2p_override doesn't have a
-	 * valid entry for it.
+	 * pfn is ~0 if there are no entries in the m2p for mfn or the
+	 * entry doesn't map back to the mfn.
 	 */
 	if (pfn == ~0 && __pfn_to_mfn(mfn) == IDENTITY_FRAME(mfn))
 		pfn = mfn;

commit 853d0289340026b30f93fd0e768340221d4e605c
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jan 5 14:13:41 2015 +0000

    xen/grant-table: pre-populate kernel unmap ops for xen_gnttab_unmap_refs()
    
    When unmapping grants, instead of converting the kernel map ops to
    unmap ops on the fly, pre-populate the set of unmap ops.
    
    This allows the grant unmap for the kernel mappings to be trivially
    batched in the future.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 5eea09915a15..e9f52fe2d56a 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -55,7 +55,7 @@ extern int set_foreign_p2m_mapping(struct gnttab_map_grant_ref *map_ops,
 				   struct gnttab_map_grant_ref *kmap_ops,
 				   struct page **pages, unsigned int count);
 extern int clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
-				     struct gnttab_map_grant_ref *kmap_ops,
+				     struct gnttab_unmap_grant_ref *kunmap_ops,
 				     struct page **pages, unsigned int count);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 

commit 90fff3ea15a8fa6d2bd60cc0538d8ac33f14b692
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Dec 5 13:28:04 2014 +0100

    xen: introduce helper functions to do safe read and write accesses
    
    Introduce two helper functions to safely read and write unsigned long
    values from or to memory when the access may fault because the mapping
    is non-present or read-only.
    
    These helpers can be used instead of open coded uses of __get_user()
    and __put_user() avoiding the need to do casts to fix sparse warnings.
    
    Use the helpers in page.h and p2m.c. This will fix the sparse
    warnings when doing "make C=1".
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index b54a3d20d6b2..5eea09915a15 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -59,6 +59,20 @@ extern int clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
 				     struct page **pages, unsigned int count);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
+/*
+ * Helper functions to write or read unsigned long values to/from
+ * memory, when the access may fault.
+ */
+static inline int xen_safe_write_ulong(unsigned long *addr, unsigned long val)
+{
+	return __put_user(val, (unsigned long __user *)addr);
+}
+
+static inline int xen_safe_read_ulong(unsigned long *addr, unsigned long *val)
+{
+	return __get_user(*val, (unsigned long __user *)addr);
+}
+
 /*
  * When to use pfn_to_mfn(), __pfn_to_mfn() or get_phys_to_machine():
  * - pfn_to_mfn() returns either INVALID_P2M_ENTRY or the mfn. No indicator
@@ -125,7 +139,7 @@ static inline unsigned long mfn_to_pfn_no_overrides(unsigned long mfn)
 	 * In such cases it doesn't matter what we return (we return garbage),
 	 * but we must handle the fault without crashing!
 	 */
-	ret = __get_user(pfn, &machine_to_phys_mapping[mfn]);
+	ret = xen_safe_read_ulong(&machine_to_phys_mapping[mfn], &pfn);
 	if (ret < 0)
 		return ~0;
 

commit 054954eb051f35e74b75a566a96fe756015352c8
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Nov 28 11:53:58 2014 +0100

    xen: switch to linear virtual mapped sparse p2m list
    
    At start of the day the Xen hypervisor presents a contiguous mfn list
    to a pv-domain. In order to support sparse memory this mfn list is
    accessed via a three level p2m tree built early in the boot process.
    Whenever the system needs the mfn associated with a pfn this tree is
    used to find the mfn.
    
    Instead of using a software walked tree for accessing a specific mfn
    list entry this patch is creating a virtual address area for the
    entire possible mfn list including memory holes. The holes are
    covered by mapping a pre-defined  page consisting only of "invalid
    mfn" entries. Access to a mfn entry is possible by just using the
    virtual base address of the mfn list and the pfn as index into that
    list. This speeds up the (hot) path of determining the mfn of a
    pfn.
    
    Kernel build on a Dell Latitude E6440 (2 cores, HT) in 64 bit Dom0
    showed following improvements:
    
    Elapsed time: 32:50 ->  32:35
    System:       18:07 ->  17:47
    User:        104:00 -> 103:30
    
    Tested with following configurations:
    - 64 bit dom0, 8GB RAM
    - 64 bit dom0, 128 GB RAM, PCI-area above 4 GB
    - 32 bit domU, 512 MB, 8 GB, 43 GB (more wouldn't work even without
                                        the patch)
    - 32 bit domU, ballooning up and down
    - 32 bit domU, save and restore
    - 32 bit domU with PCI passthrough
    - 64 bit domU, 8 GB, 2049 MB, 5000 MB
    - 64 bit domU, ballooning up and down
    - 64 bit domU, save and restore
    - 64 bit domU with PCI passthrough
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 57aba6ba6f92..b54a3d20d6b2 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -65,13 +65,25 @@ extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn)
  *   bits (identity or foreign) are set.
  * - __pfn_to_mfn() returns the found entry of the p2m table. A possibly set
  *   identity or foreign indicator will be still set. __pfn_to_mfn() is
- *   encapsulating get_phys_to_machine().
- * - get_phys_to_machine() is to be called by __pfn_to_mfn() only to allow
- *   for future optimizations.
+ *   encapsulating get_phys_to_machine() which is called in special cases only.
+ * - get_phys_to_machine() is to be called by __pfn_to_mfn() only in special
+ *   cases needing an extended handling.
  */
 static inline unsigned long __pfn_to_mfn(unsigned long pfn)
 {
-	return get_phys_to_machine(pfn);
+	unsigned long mfn;
+
+	if (pfn < xen_p2m_size)
+		mfn = xen_p2m_addr[pfn];
+	else if (unlikely(pfn < xen_max_p2m_pfn))
+		return get_phys_to_machine(pfn);
+	else
+		return IDENTITY_FRAME(pfn);
+
+	if (unlikely(mfn == INVALID_P2M_ENTRY))
+		return get_phys_to_machine(pfn);
+
+	return mfn;
 }
 
 static inline unsigned long pfn_to_mfn(unsigned long pfn)

commit 0aad5689837c882d2539f50f42f686b74046c0a0
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Nov 28 11:53:57 2014 +0100

    xen: Hide get_phys_to_machine() to be able to tune common path
    
    Today get_phys_to_machine() is always called when the mfn for a pfn
    is to be obtained. Add a wrapper __pfn_to_mfn() as inline function
    to be able to avoid calling get_phys_to_machine() when possible as
    soon as the switch to a linear mapped p2m list has been done.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index a4a265fcab17..57aba6ba6f92 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -59,6 +59,21 @@ extern int clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
 				     struct page **pages, unsigned int count);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
+/*
+ * When to use pfn_to_mfn(), __pfn_to_mfn() or get_phys_to_machine():
+ * - pfn_to_mfn() returns either INVALID_P2M_ENTRY or the mfn. No indicator
+ *   bits (identity or foreign) are set.
+ * - __pfn_to_mfn() returns the found entry of the p2m table. A possibly set
+ *   identity or foreign indicator will be still set. __pfn_to_mfn() is
+ *   encapsulating get_phys_to_machine().
+ * - get_phys_to_machine() is to be called by __pfn_to_mfn() only to allow
+ *   for future optimizations.
+ */
+static inline unsigned long __pfn_to_mfn(unsigned long pfn)
+{
+	return get_phys_to_machine(pfn);
+}
+
 static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {
 	unsigned long mfn;
@@ -66,7 +81,7 @@ static inline unsigned long pfn_to_mfn(unsigned long pfn)
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return pfn;
 
-	mfn = get_phys_to_machine(pfn);
+	mfn = __pfn_to_mfn(pfn);
 
 	if (mfn != INVALID_P2M_ENTRY)
 		mfn &= ~(FOREIGN_FRAME_BIT | IDENTITY_FRAME_BIT);
@@ -79,7 +94,7 @@ static inline int phys_to_machine_mapping_valid(unsigned long pfn)
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return 1;
 
-	return get_phys_to_machine(pfn) != INVALID_P2M_ENTRY;
+	return __pfn_to_mfn(pfn) != INVALID_P2M_ENTRY;
 }
 
 static inline unsigned long mfn_to_pfn_no_overrides(unsigned long mfn)
@@ -113,7 +128,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 		return mfn;
 
 	pfn = mfn_to_pfn_no_overrides(mfn);
-	if (get_phys_to_machine(pfn) != mfn) {
+	if (__pfn_to_mfn(pfn) != mfn) {
 		/*
 		 * If this appears to be a foreign mfn (because the pfn
 		 * doesn't map back to the mfn), then check the local override
@@ -129,8 +144,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	 * entry doesn't map back to the mfn and m2p_override doesn't have a
 	 * valid entry for it.
 	 */
-	if (pfn == ~0 &&
-			get_phys_to_machine(mfn) == IDENTITY_FRAME(mfn))
+	if (pfn == ~0 && __pfn_to_mfn(mfn) == IDENTITY_FRAME(mfn))
 		pfn = mfn;
 
 	return pfn;
@@ -176,7 +190,7 @@ static inline unsigned long mfn_to_local_pfn(unsigned long mfn)
 		return mfn;
 
 	pfn = mfn_to_pfn(mfn);
-	if (get_phys_to_machine(pfn) != mfn)
+	if (__pfn_to_mfn(pfn) != mfn)
 		return -1; /* force !pfn_valid() */
 	return pfn;
 }

commit 5b8e7d80542487ff1bf17b4cf2922a01dee13d3a
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Nov 28 11:53:55 2014 +0100

    xen: Delay invalidating extra memory
    
    When the physical memory configuration is initialized the p2m entries
    for not pouplated memory pages are set to "invalid". As those pages
    are beyond the hypervisor built p2m list the p2m tree has to be
    extended.
    
    This patch delays processing the extra memory related p2m entries
    during the boot process until some more basic memory management
    functions are callable. This removes the need to create new p2m
    entries until virtual memory management is available.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index eb89c7ddae16..a4a265fcab17 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -41,6 +41,9 @@ typedef struct xpaddr {
 
 extern unsigned long *machine_to_phys_mapping;
 extern unsigned long  machine_to_phys_nr;
+extern unsigned long *xen_p2m_addr;
+extern unsigned long  xen_p2m_size;
+extern unsigned long  xen_max_p2m_pfn;
 
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);

commit 1f3ac86b4c45a146e090d24bf66c49b95e72f071
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Nov 28 11:53:53 2014 +0100

    xen: Delay remapping memory of pv-domain
    
    Early in the boot process the memory layout of a pv-domain is changed
    to match the E820 map (either the host one for Dom0 or the Xen one)
    regarding placement of RAM and PCI holes. This requires removing memory
    pages initially located at positions not suitable for RAM and adding
    them later at higher addresses where no restrictions apply.
    
    To be able to operate on the hypervisor supported p2m list until a
    virtual mapped linear p2m list can be constructed, remapping must
    be delayed until virtual memory management is initialized, as the
    initial p2m list can't be extended unlimited at physical memory
    initialization time due to it's fixed structure.
    
    A further advantage is the reduction in complexity and code volume as
    we don't have to be careful regarding memory restrictions during p2m
    updates.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 5a65a7551698..eb89c7ddae16 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -44,7 +44,6 @@ extern unsigned long  machine_to_phys_nr;
 
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
-extern bool __init early_set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 					     unsigned long pfn_e);

commit 820c4db2be4ec179210b5c69103a5b2858513e8a
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Nov 28 11:53:51 2014 +0100

    xen: Make functions static
    
    Some functions in arch/x86/xen/p2m.c are used locally only. Make them
    static. Rearrange the functions in p2m.c to avoid forward declarations.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index f58ef6c0613b..5a65a7551698 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -52,15 +52,9 @@ extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 extern int set_foreign_p2m_mapping(struct gnttab_map_grant_ref *map_ops,
 				   struct gnttab_map_grant_ref *kmap_ops,
 				   struct page **pages, unsigned int count);
-extern int m2p_add_override(unsigned long mfn, struct page *page,
-			    struct gnttab_map_grant_ref *kmap_op);
 extern int clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
 				     struct gnttab_map_grant_ref *kmap_ops,
 				     struct page **pages, unsigned int count);
-extern int m2p_remove_override(struct page *page,
-			       struct gnttab_map_grant_ref *kmap_op,
-			       unsigned long mfn);
-extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
 static inline unsigned long pfn_to_mfn(unsigned long pfn)

commit a4dba130891271084344c12537731542ec77cb85
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Nov 21 11:07:39 2014 +0000

    xen/arm/arm64: introduce xen_arch_need_swiotlb
    
    Introduce an arch specific function to find out whether a particular dma
    mapping operation needs to bounce on the swiotlb buffer.
    
    On ARM and ARM64, if the page involved is a foreign page and the device
    is not coherent, we need to bounce because at unmap time we cannot
    execute any required cache maintenance operations (we don't know how to
    find the pfn from the mfn).
    
    No change of behaviour for x86.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index c949923a5668..f58ef6c0613b 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -236,4 +236,11 @@ void make_lowmem_page_readwrite(void *vaddr);
 #define xen_remap(cookie, size) ioremap((cookie), (size));
 #define xen_unmap(cookie) iounmap((cookie))
 
+static inline bool xen_arch_need_swiotlb(struct device *dev,
+					 unsigned long pfn,
+					 unsigned long mfn)
+{
+	return false;
+}
+
 #endif /* _ASM_X86_XEN_PAGE_H */

commit 1429d46df4c538d28460d0b493997006a62a1093
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Feb 27 15:55:30 2014 +0000

    xen/grant-table: Refactor gnttab_[un]map_refs to avoid m2p_override
    
    The grant mapping API does m2p_override unnecessarily: only gntdev needs it,
    for blkback and future netback patches it just cause a lock contention, as
    those pages never go to userspace. Therefore this series does the following:
    - the bulk of the original function (everything after the mapping hypercall)
      is moved to arch-dependent set/clear_foreign_p2m_mapping
    - the "if (xen_feature(XENFEAT_auto_translated_physmap))" branch goes to ARM
    - therefore the ARM function could be much smaller, the m2p_override stubs
      could be also removed
    - on x86 the set_phys_to_machine calls were moved up to this new funcion
      from m2p_override functions
    - and m2p_override functions are only called when there is a kmap_ops param
    
    It also removes a stray space from arch/x86/include/asm/xen/page.h.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Suggested-by: Anthony Liguori <aliguori@amazon.com>
    Suggested-by: David Vrabel <david.vrabel@citrix.com>
    Suggested-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 3e276eb23d1b..c949923a5668 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -49,10 +49,17 @@ extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 					     unsigned long pfn_e);
 
+extern int set_foreign_p2m_mapping(struct gnttab_map_grant_ref *map_ops,
+				   struct gnttab_map_grant_ref *kmap_ops,
+				   struct page **pages, unsigned int count);
 extern int m2p_add_override(unsigned long mfn, struct page *page,
 			    struct gnttab_map_grant_ref *kmap_op);
+extern int clear_foreign_p2m_mapping(struct gnttab_unmap_grant_ref *unmap_ops,
+				     struct gnttab_map_grant_ref *kmap_ops,
+				     struct page **pages, unsigned int count);
 extern int m2p_remove_override(struct page *page,
-				struct gnttab_map_grant_ref *kmap_op);
+			       struct gnttab_map_grant_ref *kmap_op,
+			       unsigned long mfn);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
@@ -121,7 +128,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 		pfn = m2p_find_override_pfn(mfn, ~0);
 	}
 
-	/* 
+	/*
 	 * pfn is ~0 if there are no entries in the m2p for mfn or if the
 	 * entry doesn't map back to the mfn and m2p_override doesn't have a
 	 * valid entry for it.

commit e85fc9805591a17ca8af50023ee8e2b61d9a123b
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Feb 3 06:43:59 2014 -0500

    Revert "xen/grant-table: Avoid m2p_override during mapping"
    
    This reverts commit 08ece5bb2312b4510b161a6ef6682f37f4eac8a1.
    
    As it breaks ARM builds and needs more attention
    on the ARM side.
    
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 787e1bb5aafc..3e276eb23d1b 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -52,8 +52,7 @@ extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 extern int m2p_add_override(unsigned long mfn, struct page *page,
 			    struct gnttab_map_grant_ref *kmap_op);
 extern int m2p_remove_override(struct page *page,
-			       struct gnttab_map_grant_ref *kmap_op,
-			       unsigned long mfn);
+				struct gnttab_map_grant_ref *kmap_op);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
@@ -122,7 +121,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 		pfn = m2p_find_override_pfn(mfn, ~0);
 	}
 
-	/*
+	/* 
 	 * pfn is ~0 if there are no entries in the m2p for mfn or if the
 	 * entry doesn't map back to the mfn and m2p_override doesn't have a
 	 * valid entry for it.

commit 08ece5bb2312b4510b161a6ef6682f37f4eac8a1
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Jan 23 21:23:44 2014 +0000

    xen/grant-table: Avoid m2p_override during mapping
    
    The grant mapping API does m2p_override unnecessarily: only gntdev needs it,
    for blkback and future netback patches it just cause a lock contention, as
    those pages never go to userspace. Therefore this series does the following:
    - the original functions were renamed to __gnttab_[un]map_refs, with a new
      parameter m2p_override
    - based on m2p_override either they follow the original behaviour, or just set
      the private flag and call set_phys_to_machine
    - gnttab_[un]map_refs are now a wrapper to call __gnttab_[un]map_refs with
      m2p_override false
    - a new function gnttab_[un]map_refs_userspace provides the old behaviour
    
    It also removes a stray space from page.h and change ret to 0 if
    XENFEAT_auto_translated_physmap, as that is the only possible return value
    there.
    
    v2:
    - move the storing of the old mfn in page->index to gnttab_map_refs
    - move the function header update to a separate patch
    
    v3:
    - a new approach to retain old behaviour where it needed
    - squash the patches into one
    
    v4:
    - move out the common bits from m2p* functions, and pass pfn/mfn as parameter
    - clear page->private before doing anything with the page, so m2p_find_override
      won't race with this
    
    v5:
    - change return value handling in __gnttab_[un]map_refs
    - remove a stray space in page.h
    - add detail why ret = 0 now at some places
    
    v6:
    - don't pass pfn to m2p* functions, just get it locally
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Suggested-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 3e276eb23d1b..787e1bb5aafc 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -52,7 +52,8 @@ extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 extern int m2p_add_override(unsigned long mfn, struct page *page,
 			    struct gnttab_map_grant_ref *kmap_op);
 extern int m2p_remove_override(struct page *page,
-				struct gnttab_map_grant_ref *kmap_op);
+			       struct gnttab_map_grant_ref *kmap_op,
+			       unsigned long mfn);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
@@ -121,7 +122,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 		pfn = m2p_find_override_pfn(mfn, ~0);
 	}
 
-	/* 
+	/*
 	 * pfn is ~0 if there are no entries in the m2p for mfn or if the
 	 * entry doesn't map back to the mfn and m2p_override doesn't have a
 	 * valid entry for it.

commit efaf30a3357872cf0fc7d555b1f9968ec71535d3
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Jan 6 10:40:36 2014 -0500

    xen/grant: Implement an grant frame array struct (v3).
    
    The 'xen_hvm_resume_frames' used to be an 'unsigned long'
    and contain the virtual address of the grants. That was OK
    for most architectures (PVHVM, ARM) were the grants are contiguous
    in memory. That however is not the case for PVH - in which case
    we will have to do a lookup for each virtual address for the PFN.
    
    Instead of doing that, lets make it a structure which will contain
    the array of PFNs, the virtual address and the count of said PFNs.
    
    Also provide a generic functions: gnttab_setup_auto_xlat_frames and
    gnttab_free_auto_xlat_frames to populate said structure with
    appropriate values for PVHVM and ARM.
    
    To round it off, change the name from 'xen_hvm_resume_frames' to
    a more descriptive one - 'xen_auto_xlat_grant_frames'.
    
    For PVH, in patch "xen/pvh: Piggyback on PVHVM for grant driver"
    we will populate the 'xen_auto_xlat_grant_frames' by ourselves.
    
    v2 moves the xen_remap in the gnttab_setup_auto_xlat_frames
    and also introduces xen_unmap for gnttab_free_auto_xlat_frames.
    
    Suggested-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    [v3: Based on top of 'asm/xen/page.h: remove redundant semicolon']
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 4a092ccdd147..3e276eb23d1b 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -227,5 +227,6 @@ void make_lowmem_page_readonly(void *vaddr);
 void make_lowmem_page_readwrite(void *vaddr);
 
 #define xen_remap(cookie, size) ioremap((cookie), (size));
+#define xen_unmap(cookie) iounmap((cookie))
 
 #endif /* _ASM_X86_XEN_PAGE_H */

commit fc590efe667338f7da250cf2d2eb6fdd486e1b97
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Fri Dec 13 12:09:28 2013 -0500

    xen/p2m: Check for auto-xlat when doing mfn_to_local_pfn.
    
    Most of the functions in page.h are prefaced with
            if (xen_feature(XENFEAT_auto_translated_physmap))
                    return mfn;
    
    Except the mfn_to_local_pfn. At a first sight, the function
    should work without this patch - as the 'mfn_to_mfn' has
    a similar check. But there are no such check in the
    'get_phys_to_machine' function - so we would crash in there.
    
    This fixes it by following the convention of having the
    check for auto-xlat in these static functions.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index b913915e8e63..4a092ccdd147 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -167,7 +167,12 @@ static inline xpaddr_t machine_to_phys(xmaddr_t machine)
  */
 static inline unsigned long mfn_to_local_pfn(unsigned long mfn)
 {
-	unsigned long pfn = mfn_to_pfn(mfn);
+	unsigned long pfn;
+
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return mfn;
+
+	pfn = mfn_to_pfn(mfn);
 	if (get_phys_to_machine(pfn) != mfn)
 		return -1; /* force !pfn_valid() */
 	return pfn;

commit 0160676bba69523e8b0ac83f306cce7d342ed7c8
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Sep 13 15:13:30 2013 +0100

    xen/p2m: check MFN is in range before using the m2p table
    
    On hosts with more than 168 GB of memory, a 32-bit guest may attempt
    to grant map an MFN that is error cannot lookup in its mapping of the
    m2p table.  There is an m2p lookup as part of m2p_add_override() and
    m2p_remove_override().  The lookup falls off the end of the mapped
    portion of the m2p and (because the mapping is at the highest virtual
    address) wraps around and the lookup causes a fault on what appears to
    be a user space address.
    
    do_page_fault() (thinking it's a fault to a userspace address), tries
    to lock mm->mmap_sem.  If the gntdev device is used for the grant map,
    m2p_add_override() is called from from gnttab_mmap() with mm->mmap_sem
    already locked.  do_page_fault() then deadlocks.
    
    The deadlock would most commonly occur when a 64-bit guest is started
    and xenconsoled attempts to grant map its console ring.
    
    Introduce mfn_to_pfn_no_overrides() which checks the MFN is within the
    mapped portion of the m2p table before accessing the table and use
    this in m2p_add_override(), m2p_remove_override(), and mfn_to_pfn()
    (which already had the correct range check).
    
    All faults caused by accessing the non-existant parts of the m2p are
    thus within the kernel address space and exception_fixup() is called
    without trying to lock mm->mmap_sem.
    
    This means that for MFNs that are outside the mapped range of the m2p
    then mfn_to_pfn() will always look in the m2p overrides.  This is
    correct because it must be a foreign MFN (and the PFN in the m2p in
    this case is only relevant for the other domain).
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Stefano Stabellini <stefano.stabellini@citrix.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    --
    v3: check for auto_translated_physmap in mfn_to_pfn_no_overrides()
    v2: in mfn_to_pfn() look in m2p_overrides if the MFN is out of
        range as it's probably foreign.
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 6aef9fbc09b7..b913915e8e63 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -79,30 +79,38 @@ static inline int phys_to_machine_mapping_valid(unsigned long pfn)
 	return get_phys_to_machine(pfn) != INVALID_P2M_ENTRY;
 }
 
-static inline unsigned long mfn_to_pfn(unsigned long mfn)
+static inline unsigned long mfn_to_pfn_no_overrides(unsigned long mfn)
 {
 	unsigned long pfn;
-	int ret = 0;
+	int ret;
 
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return mfn;
 
-	if (unlikely(mfn >= machine_to_phys_nr)) {
-		pfn = ~0;
-		goto try_override;
-	}
-	pfn = 0;
+	if (unlikely(mfn >= machine_to_phys_nr))
+		return ~0;
+
 	/*
 	 * The array access can fail (e.g., device space beyond end of RAM).
 	 * In such cases it doesn't matter what we return (we return garbage),
 	 * but we must handle the fault without crashing!
 	 */
 	ret = __get_user(pfn, &machine_to_phys_mapping[mfn]);
-try_override:
-	/* ret might be < 0 if there are no entries in the m2p for mfn */
 	if (ret < 0)
-		pfn = ~0;
-	else if (get_phys_to_machine(pfn) != mfn)
+		return ~0;
+
+	return pfn;
+}
+
+static inline unsigned long mfn_to_pfn(unsigned long mfn)
+{
+	unsigned long pfn;
+
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return mfn;
+
+	pfn = mfn_to_pfn_no_overrides(mfn);
+	if (get_phys_to_machine(pfn) != mfn) {
 		/*
 		 * If this appears to be a foreign mfn (because the pfn
 		 * doesn't map back to the mfn), then check the local override
@@ -111,6 +119,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 		 * m2p_find_override_pfn returns ~0 if it doesn't find anything.
 		 */
 		pfn = m2p_find_override_pfn(mfn, ~0);
+	}
 
 	/* 
 	 * pfn is ~0 if there are no entries in the m2p for mfn or if the

commit 3216dceb31c08be08ea98814a9ca5775fa680389
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Tue Feb 19 13:59:19 2013 +0000

    xen: introduce xen_remap, use it instead of ioremap
    
    ioremap can't be used to map ring pages on ARM because it uses device
    memory caching attributes (MT_DEVICE*).
    
    Introduce a Xen specific abstraction to map ring pages, called
    xen_remap, that is defined as ioremap on x86 (no behavioral changes).
    On ARM it explicitly calls __arm_ioremap with the right caching
    attributes: MT_MEMORY.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 472b9b783019..6aef9fbc09b7 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -212,4 +212,6 @@ unsigned long arbitrary_virt_to_mfn(void *vaddr);
 void make_lowmem_page_readonly(void *vaddr);
 void make_lowmem_page_readwrite(void *vaddr);
 
+#define xen_remap(cookie, size) ioremap((cookie), (size));
+
 #endif /* _ASM_X86_XEN_PAGE_H */

commit 2fc136eecd0c647a6b13fcd00d0c41a1a28f35a5
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Wed Sep 12 12:44:30 2012 +0100

    xen/m2p: do not reuse kmap_op->dev_bus_addr
    
    If the caller passes a valid kmap_op to m2p_add_override, we use
    kmap_op->dev_bus_addr to store the original mfn, but dev_bus_addr is
    part of the interface with Xen and if we are batching the hypercalls it
    might not have been written by the hypervisor yet. That means that later
    on Xen will write to it and we'll think that the original mfn is
    actually what Xen has written to it.
    
    Rather than "stealing" struct members from kmap_op, keep using
    page->index to store the original mfn and add another parameter to
    m2p_remove_override to get the corresponding kmap_op instead.
    It is now responsibility of the caller to keep track of which kmap_op
    corresponds to a particular page in the m2p_override (gntdev, the only
    user of this interface that passes a valid kmap_op, is already doing that).
    
    CC: stable@kernel.org
    Reported-and-Tested-By: Sander Eikelenboom <linux@eikelenboom.it>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 93971e841dd5..472b9b783019 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -51,7 +51,8 @@ extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 
 extern int m2p_add_override(unsigned long mfn, struct page *page,
 			    struct gnttab_map_grant_ref *kmap_op);
-extern int m2p_remove_override(struct page *page, bool clear_pte);
+extern int m2p_remove_override(struct page *page,
+				struct gnttab_map_grant_ref *kmap_op);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 

commit 940713bb2ce3033f468a220094a07250a2f69bdd
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Mar 30 14:33:14 2012 -0400

    xen/p2m: An early bootup variant of set_phys_to_machine
    
    During early bootup we can't use alloc_page, so to allocate
    leaf pages in the P2M we need to use extend_brk. For that
    we are utilizing the early_alloc_p2m and early_alloc_p2m_middle
    functions to do the job for us. This function follows the
    same logic as set_phys_to_machine.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index c34f96c2f7a0..93971e841dd5 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -44,6 +44,7 @@ extern unsigned long  machine_to_phys_nr;
 
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
+extern bool __init early_set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 					     unsigned long pfn_e);

commit 31018acd4c77f0e4b90f870011249f32c5e3d5b6
Merge: 5eef150c1d7e a491dbef56f2 38a1ed4f039d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 25 09:17:47 2011 +0200

    Merge branches 'stable/bug.fixes-3.2' and 'stable/mmu.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/bug.fixes-3.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/p2m/debugfs: Make type_name more obvious.
      xen/p2m/debugfs: Fix potential pointer exception.
      xen/enlighten: Fix compile warnings and set cx to known value.
      xen/xenbus: Remove the unnecessary check.
      xen/irq: If we fail during msi_capability_init return proper error code.
      xen/events: Don't check the info for NULL as it is already done.
      xen/events: BUG() when we can't allocate our event->irq array.
    
    * 'stable/mmu.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: Fix selfballooning and ensure it doesn't go too far
      xen/gntdev: Fix sleep-inside-spinlock
      xen: modify kernel mappings corresponding to granted pages
      xen: add an "highmem" parameter to alloc_xenballooned_pages
      xen/p2m: Use SetPagePrivate and its friends for M2P overrides.
      xen/p2m: Make debug/xen/mmu/p2m visible again.
      Revert "xen/debug: WARN_ON when identity PFN has no _PAGE_IOMAP flag set."

commit 0930bba674e248b921ea659b036ff02564e5a5f4
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Sep 29 11:57:56 2011 +0100

    xen: modify kernel mappings corresponding to granted pages
    
    If we want to use granted pages for AIO, changing the mappings of a user
    vma and the corresponding p2m is not enough, we also need to update the
    kernel mappings accordingly.
    Currently this is only needed for pages that are created for user usages
    through /dev/xen/gntdev. As in, pages that have been in use by the
    kernel and use the P2M will not need this special mapping.
    However there are no guarantees that in the future the kernel won't
    start accessing pages through the 1:1 even for internal usage.
    
    In order to avoid the complexity of dealing with highmem, we allocated
    the pages lowmem.
    We issue a HYPERVISOR_grant_table_op right away in
    m2p_add_override and we remove the mappings using another
    HYPERVISOR_grant_table_op in m2p_remove_override.
    Considering that m2p_add_override and m2p_remove_override are called
    once per page we use multicalls and hypercall batching.
    
    Use the kmap_op pointer directly as argument to do the mapping as it is
    guaranteed to be present up until the unmapping is done.
    Before issuing any unmapping multicalls, we need to make sure that the
    mapping has already being done, because we need the kmap->handle to be
    set correctly.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    [v1: Removed GRANT_FRAME_BIT usage]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index bc12c12299c3..3138d33b8949 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -12,6 +12,7 @@
 #include <asm/pgtable.h>
 
 #include <xen/interface/xen.h>
+#include <xen/grant_table.h>
 #include <xen/features.h>
 
 /* Xen machine address */
@@ -48,7 +49,7 @@ extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 					     unsigned long pfn_e);
 
 extern int m2p_add_override(unsigned long mfn, struct page *page,
-			    bool clear_pte);
+			    struct gnttab_map_grant_ref *kmap_op);
 extern int m2p_remove_override(struct page *page, bool clear_pte);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);

commit a867db10e89e12a3d97dedafdd411aa1527a6540
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Sep 23 16:32:47 2011 -0400

    xen/p2m: Make debug/xen/mmu/p2m visible again.
    
    We dropped a lot of the MMU debugfs in favour of using
    tracing API - but there is one which just provides
    mostly static information that was made invisible by this change.
    
    Bring it back.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 64a619d47d34..bc12c12299c3 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -53,9 +53,6 @@ extern int m2p_remove_override(struct page *page, bool clear_pte);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
-#ifdef CONFIG_XEN_DEBUG_FS
-extern int p2m_dump_show(struct seq_file *m, void *v);
-#endif
 static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {
 	unsigned long mfn;

commit ccbcdf7cf1b5f6c6db30d84095b9c6c53043af55
Author: Jan Beulich <JBeulich@novell.com>
Date:   Tue Aug 16 15:07:41 2011 +0100

    xen/x86: replace order-based range checking of M2P table by linear one
    
    The order-based approach is not only less efficient (requiring a shift
    and a compare, typical generated code looking like this
    
            mov     eax, [machine_to_phys_order]
            mov     ecx, eax
            shr     ebx, cl
            test    ebx, ebx
            jnz     ...
    
    whereas a direct check requires just a compare, like in
    
            cmp     ebx, [machine_to_phys_nr]
            jae     ...
    
    ), but also slightly dangerous in the 32-on-64 case - the element
    address calculation can wrap if the next power of two boundary is
    sufficiently far away from the actual upper limit of the table, and
    hence can result in user space addresses being accessed (with it being
    unknown what may actually be mapped there).
    
    Additionally, the elimination of the mistaken use of fls() here (should
    have been __fls()) fixes a latent issue on x86-64 that would trigger
    if the code was run on a system with memory extending beyond the 44-bit
    boundary.
    
    CC: stable@kernel.org
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    [v1: Based on Jeremy's feedback]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 64a619d47d34..7ff4669580cf 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -39,7 +39,7 @@ typedef struct xpaddr {
     ((unsigned long)((u64)CONFIG_XEN_MAX_DOMAIN_MEMORY * 1024 * 1024 * 1024 / PAGE_SIZE))
 
 extern unsigned long *machine_to_phys_mapping;
-extern unsigned int   machine_to_phys_order;
+extern unsigned long  machine_to_phys_nr;
 
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
@@ -87,7 +87,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return mfn;
 
-	if (unlikely((mfn >> machine_to_phys_order) != 0)) {
+	if (unlikely(mfn >= machine_to_phys_nr)) {
 		pfn = ~0;
 		goto try_override;
 	}

commit cf8d91633ddef9e816ccbf3da833c79ce508988d
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Feb 28 17:58:48 2011 -0500

    xen/p2m/m2p/gnttab: Support GNTMAP_host_map in the M2P override.
    
    We only supported the M2P (and P2M) override only for the
    GNTMAP_contains_pte type mappings. Meaning that we grants
    operations would "contain the machine address of the PTE to update"
    If the flag is unset, then the grant operation is
    "contains a host virtual address". The latter case means that
    the Hypervisor takes care of updating our page table
    (specifically the PTE entry) with the guest's MFN. As such we should
    not try to do anything with the PTE. Previous to this patch
    we would try to clear the PTE which resulted in Xen hypervisor
    being upset with us:
    
    (XEN) mm.c:1066:d0 Attempt to implicitly unmap a granted PTE c0100000ccc59067
    (XEN) domain_crash called from mm.c:1067
    (XEN) Domain 0 (vcpu#0) crashed on cpu#3:
    (XEN) ----[ Xen-4.0-110228  x86_64  debug=y  Not tainted ]----
    
    and crashing us.
    
    This patch allows us to inhibit the PTE clearing in the PV guest
    if the GNTMAP_contains_pte is not set.
    
    On the m2p_remove_override path we provide the same parameter.
    
    Sadly in the grant-table driver we do not have a mechanism to
    tell m2p_remove_override whether to clear the PTE or not. Since
    the grant-table driver is used by user-space, we can safely assume
    that it operates only on PTE's. Hence the implementation for
    it to work on !GNTMAP_contains_pte returns -EOPNOTSUPP. In the future
    we can implement the support for this. It will require some extra
    accounting structure to keep track of the page[i], and the flag.
    
    [v1: Added documentation details, made it return -EOPNOTSUPP instead
     of trying to do a half-way implementation]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index c61934fbf22a..64a619d47d34 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -47,8 +47,9 @@ extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern unsigned long set_phys_range_identity(unsigned long pfn_s,
 					     unsigned long pfn_e);
 
-extern int m2p_add_override(unsigned long mfn, struct page *page);
-extern int m2p_remove_override(struct page *page);
+extern int m2p_add_override(unsigned long mfn, struct page *page,
+			    bool clear_pte);
+extern int m2p_remove_override(struct page *page, bool clear_pte);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 

commit 706cc9d2a4cb9b03217e15b0bb3d117f4d5109ee
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Wed Feb 2 18:32:59 2011 +0000

    xen/m2p: Check whether the MFN has IDENTITY_FRAME bit set..
    
    If there is no proper PFN value in the M2P for the MFN
    (so we get 0xFFFFF.. or 0x55555, or 0x0), we should
    consult the M2P override to see if there is an entry for this.
    [Note: we also consult the M2P override if the MFN
    is past our machine_to_phys size].
    
    We consult the P2M with the PFN. In case the returned
    MFN is one of the special values: 0xFFF.., 0x5555
    (which signify that the MFN can be either "missing" or it
    belongs to DOMID_IO) or the p2m(m2p(mfn)) != mfn, we check
    the M2P override. If we fail the M2P override check, we reset
    the PFN value to INVALID_P2M_ENTRY.
    
    Next we try to find the MFN in the P2M using the MFN
    value (not the PFN value) and if found, we know
    that this MFN is an identity value and return it as so.
    
    Otherwise we have exhausted all the posibilities and we
    return the PFN, which at this stage can either be a real
    PFN value found in the machine_to_phys.. array, or
    INVALID_P2M_ENTRY value.
    
    [v1: Added Review-by tag]
    
    Reviewed-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 195707060493..c61934fbf22a 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -81,6 +81,7 @@ static inline int phys_to_machine_mapping_valid(unsigned long pfn)
 static inline unsigned long mfn_to_pfn(unsigned long mfn)
 {
 	unsigned long pfn;
+	int ret = 0;
 
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return mfn;
@@ -95,15 +96,29 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	 * In such cases it doesn't matter what we return (we return garbage),
 	 * but we must handle the fault without crashing!
 	 */
-	__get_user(pfn, &machine_to_phys_mapping[mfn]);
+	ret = __get_user(pfn, &machine_to_phys_mapping[mfn]);
 try_override:
-	/*
-	 * If this appears to be a foreign mfn (because the pfn
-	 * doesn't map back to the mfn), then check the local override
-	 * table to see if there's a better pfn to use.
+	/* ret might be < 0 if there are no entries in the m2p for mfn */
+	if (ret < 0)
+		pfn = ~0;
+	else if (get_phys_to_machine(pfn) != mfn)
+		/*
+		 * If this appears to be a foreign mfn (because the pfn
+		 * doesn't map back to the mfn), then check the local override
+		 * table to see if there's a better pfn to use.
+		 *
+		 * m2p_find_override_pfn returns ~0 if it doesn't find anything.
+		 */
+		pfn = m2p_find_override_pfn(mfn, ~0);
+
+	/* 
+	 * pfn is ~0 if there are no entries in the m2p for mfn or if the
+	 * entry doesn't map back to the mfn and m2p_override doesn't have a
+	 * valid entry for it.
 	 */
-	if (get_phys_to_machine(pfn) != mfn)
-		pfn = m2p_find_override_pfn(mfn, pfn);
+	if (pfn == ~0 &&
+			get_phys_to_machine(mfn) == IDENTITY_FRAME(mfn))
+		pfn = mfn;
 
 	return pfn;
 }

commit 146c4e511717e581065800938537b276173d8548
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Jan 14 17:55:44 2011 -0500

    xen/m2p: No need to catch exceptions when we know that there is no RAM
    
    .. beyound what we think is the end of memory. However there might
    be more System RAM - but assigned to a guest. Hence jump to the
    M2P override check and consult.
    
    [v1: Added Review-by tag]
    
    Reviewed-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 78ebbeb88d9c..195707060493 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -85,6 +85,10 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return mfn;
 
+	if (unlikely((mfn >> machine_to_phys_order) != 0)) {
+		pfn = ~0;
+		goto try_override;
+	}
 	pfn = 0;
 	/*
 	 * The array access can fail (e.g., device space beyond end of RAM).
@@ -92,7 +96,7 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	 * but we must handle the fault without crashing!
 	 */
 	__get_user(pfn, &machine_to_phys_mapping[mfn]);
-
+try_override:
 	/*
 	 * If this appears to be a foreign mfn (because the pfn
 	 * doesn't map back to the mfn), then check the local override

commit 2222e71bd6eff7b2ad026d4ee663b6327c5a49f5
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Dec 22 08:57:30 2010 -0500

    xen/debugfs: Add 'p2m' file for printing out the P2M layout.
    
    We walk over the whole P2M tree and construct a simplified view of
    which PFN regions belong to what level and what type they are.
    
    Only enabled if CONFIG_XEN_DEBUG_FS is set.
    
    [v2: UNKN->UNKNOWN, use uninitialized_var]
    [v3: Rebased on top of mmu->p2m code split]
    [v4: Fixed the else if]
    Reviewed-by: Ian Campbell <Ian.Campbell@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 65fa4f26aa34..78ebbeb88d9c 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -52,6 +52,9 @@ extern int m2p_remove_override(struct page *page);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 
+#ifdef CONFIG_XEN_DEBUG_FS
+extern int p2m_dump_show(struct seq_file *m, void *v);
+#endif
 static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {
 	unsigned long mfn;

commit f4cec35b0d4b90d96e3770a3d1e68ea882e7a7c8
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Jan 18 20:15:21 2011 -0500

    xen/mmu: Add the notion of identity (1-1) mapping.
    
    Our P2M tree structure is a three-level. On the leaf nodes
    we set the Machine Frame Number (MFN) of the PFN. What this means
    is that when one does: pfn_to_mfn(pfn), which is used when creating
    PTE entries, you get the real MFN of the hardware. When Xen sets
    up a guest it initially populates a array which has descending
    (or ascending) MFN values, as so:
    
     idx: 0,  1,       2
     [0x290F, 0x290E, 0x290D, ..]
    
    so pfn_to_mfn(2)==0x290D. If you start, restart many guests that list
    starts looking quite random.
    
    We graft this structure on our P2M tree structure and stick in
    those MFN in the leafs. But for all other leaf entries, or for the top
    root, or middle one, for which there is a void entry, we assume it is
    "missing". So
     pfn_to_mfn(0xc0000)=INVALID_P2M_ENTRY.
    
    We add the possibility of setting 1-1 mappings on certain regions, so
    that:
     pfn_to_mfn(0xc0000)=0xc0000
    
    The benefit of this is, that we can assume for non-RAM regions (think
    PCI BARs, or ACPI spaces), we can create mappings easily b/c we
    get the PFN value to match the MFN.
    
    For this to work efficiently we introduce one new page p2m_identity and
    allocate (via reserved_brk) any other pages we need to cover the sides
    (1GB or 4MB boundary violations). All entries in p2m_identity are set to
    INVALID_P2M_ENTRY type (Xen toolstack only recognizes that and MFNs,
    no other fancy value).
    
    On lookup we spot that the entry points to p2m_identity and return the identity
    value instead of dereferencing and returning INVALID_P2M_ENTRY. If the entry
    points to an allocated page, we just proceed as before and return the PFN.
    If the PFN has IDENTITY_FRAME_BIT set we unmask that in appropriate functions
    (pfn_to_mfn).
    
    The reason for having the IDENTITY_FRAME_BIT instead of just returning the
    PFN is that we could find ourselves where pfn_to_mfn(pfn)==pfn for a
    non-identity pfn. To protect ourselves against we elect to set (and get) the
    IDENTITY_FRAME_BIT on all identity mapped PFNs.
    
    This simplistic diagram is used to explain the more subtle piece of code.
    There is also a digram of the P2M at the end that can help.
    Imagine your E820 looking as so:
    
                       1GB                                           2GB
    /-------------------+---------\/----\         /----------\    /---+-----\
    | System RAM        | Sys RAM ||ACPI|         | reserved |    | Sys RAM |
    \-------------------+---------/\----/         \----------/    \---+-----/
                                  ^- 1029MB                       ^- 2001MB
    
    [1029MB = 263424 (0x40500), 2001MB = 512256 (0x7D100), 2048MB = 524288 (0x80000)]
    
    And dom0_mem=max:3GB,1GB is passed in to the guest, meaning memory past 1GB
    is actually not present (would have to kick the balloon driver to put it in).
    
    When we are told to set the PFNs for identity mapping (see patch: "xen/setup:
    Set identity mapping for non-RAM E820 and E820 gaps.") we pass in the start
    of the PFN and the end PFN (263424 and 512256 respectively). The first step is
    to reserve_brk a top leaf page if the p2m[1] is missing. The top leaf page
    covers 512^2 of page estate (1GB) and in case the start or end PFN is not
    aligned on 512^2*PAGE_SIZE (1GB) we loop on aligned 1GB PFNs from start pfn to
    end pfn.  We reserve_brk top leaf pages if they are missing (means they point
    to p2m_mid_missing).
    
    With the E820 example above, 263424 is not 1GB aligned so we allocate a
    reserve_brk page which will cover the PFNs estate from 0x40000 to 0x80000.
    Each entry in the allocate page is "missing" (points to p2m_missing).
    
    Next stage is to determine if we need to do a more granular boundary check
    on the 4MB (or 2MB depending on architecture) off the start and end pfn's.
    We check if the start pfn and end pfn violate that boundary check, and if
    so reserve_brk a middle (p2m[x][y]) leaf page. This way we have a much finer
    granularity of setting which PFNs are missing and which ones are identity.
    In our example 263424 and 512256 both fail the check so we reserve_brk two
    pages. Populate them with INVALID_P2M_ENTRY (so they both have "missing" values)
    and assign them to p2m[1][2] and p2m[1][488] respectively.
    
    At this point we would at minimum reserve_brk one page, but could be up to
    three. Each call to set_phys_range_identity has at maximum a three page
    cost. If we were to query the P2M at this stage, all those entries from
    start PFN through end PFN (so 1029MB -> 2001MB) would return INVALID_P2M_ENTRY
    ("missing").
    
    The next step is to walk from the start pfn to the end pfn setting
    the IDENTITY_FRAME_BIT on each PFN. This is done in 'set_phys_range_identity'.
    If we find that the middle leaf is pointing to p2m_missing we can swap it over
    to p2m_identity - this way covering 4MB (or 2MB) PFN space.  At this point we
    do not need to worry about boundary aligment (so no need to reserve_brk a middle
    page, figure out which PFNs are "missing" and which ones are identity), as that
    has been done earlier.  If we find that the middle leaf is not occupied by
    p2m_identity or p2m_missing, we dereference that page (which covers
    512 PFNs) and set the appropriate PFN with IDENTITY_FRAME_BIT. In our example
    263424 and 512256 end up there, and we set from p2m[1][2][256->511] and
    p2m[1][488][0->256] with IDENTITY_FRAME_BIT set.
    
    All other regions that are void (or not filled) either point to p2m_missing
    (considered missing) or have the default value of INVALID_P2M_ENTRY (also
    considered missing). In our case, p2m[1][2][0->255] and p2m[1][488][257->511]
    contain the INVALID_P2M_ENTRY value and are considered "missing."
    
    This is what the p2m ends up looking (for the E820 above) with this
    fabulous drawing:
    
       p2m         /--------------\
     /-----\       | &mfn_list[0],|                           /-----------------\
     |  0  |------>| &mfn_list[1],|    /---------------\      | ~0, ~0, ..      |
     |-----|       |  ..., ~0, ~0 |    | ~0, ~0, [x]---+----->| IDENTITY [@256] |
     |  1  |---\   \--------------/    | [p2m_identity]+\     | IDENTITY [@257] |
     |-----|    \                      | [p2m_identity]+\\    | ....            |
     |  2  |--\  \-------------------->|  ...          | \\   \----------------/
     |-----|   \                       \---------------/  \\
     |  3  |\   \                                          \\  p2m_identity
     |-----| \   \-------------------->/---------------\   /-----------------\
     | ..  +->+                        | [p2m_identity]+-->| ~0, ~0, ~0, ... |
     \-----/ /                         | [p2m_identity]+-->| ..., ~0         |
            / /---------------\        | ....          |   \-----------------/
           /  | IDENTITY[@0]  |      /-+-[x], ~0, ~0.. |
          /   | IDENTITY[@256]|<----/  \---------------/
         /    | ~0, ~0, ....  |
        |     \---------------/
        |
        p2m_missing             p2m_missing
    /------------------\     /------------\
    | [p2m_mid_missing]+---->| ~0, ~0, ~0 |
    | [p2m_mid_missing]+---->| ..., ~0    |
    \------------------/     \------------/
    
    where ~0 is INVALID_P2M_ENTRY. IDENTITY is (PFN | IDENTITY_BIT)
    
    Reviewed-by: Ian Campbell <ian.campbell@citrix.com>
    [v5: Changed code to use ranges, added ASCII art]
    [v6: Rebased on top of xen->p2m code split]
    [v4: Squished patches in just this one]
    [v7: Added RESERVE_BRK for potentially allocated pages]
    [v8: Fixed alignment problem]
    [v9: Changed 1<<3X to 1<<BITS_PER_LONG-X]
    [v10: Copied git commit description in the p2m code + Add Review tag]
    [v11: Title had '2-1' - should be '1-1' mapping]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 8ea977277c55..65fa4f26aa34 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -29,8 +29,10 @@ typedef struct xpaddr {
 
 /**** MACHINE <-> PHYSICAL CONVERSION MACROS ****/
 #define INVALID_P2M_ENTRY	(~0UL)
-#define FOREIGN_FRAME_BIT	(1UL<<31)
+#define FOREIGN_FRAME_BIT	(1UL<<(BITS_PER_LONG-1))
+#define IDENTITY_FRAME_BIT	(1UL<<(BITS_PER_LONG-2))
 #define FOREIGN_FRAME(m)	((m) | FOREIGN_FRAME_BIT)
+#define IDENTITY_FRAME(m)	((m) | IDENTITY_FRAME_BIT)
 
 /* Maximum amount of memory we can handle in a domain in pages */
 #define MAX_DOMAIN_PAGES						\
@@ -42,6 +44,8 @@ extern unsigned int   machine_to_phys_order;
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
+extern unsigned long set_phys_range_identity(unsigned long pfn_s,
+					     unsigned long pfn_e);
 
 extern int m2p_add_override(unsigned long mfn, struct page *page);
 extern int m2p_remove_override(struct page *page);
@@ -58,7 +62,7 @@ static inline unsigned long pfn_to_mfn(unsigned long pfn)
 	mfn = get_phys_to_machine(pfn);
 
 	if (mfn != INVALID_P2M_ENTRY)
-		mfn &= ~FOREIGN_FRAME_BIT;
+		mfn &= ~(FOREIGN_FRAME_BIT | IDENTITY_FRAME_BIT);
 
 	return mfn;
 }

commit 6eaa412f2753d98566b777836a98c6e7f672a3bb
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Jan 18 20:09:41 2011 -0500

    xen: Mark all initial reserved pages for the balloon as INVALID_P2M_ENTRY.
    
    With this patch, we diligently set regions that will be used by the
    balloon driver to be INVALID_P2M_ENTRY and under the ownership
    of the balloon driver. We are OK using the __set_phys_to_machine
    as we do not expect to be allocating any P2M middle or entries pages.
    The set_phys_to_machine has the side-effect of potentially allocating
    new pages and we do not want that at this stage.
    
    We can do this because xen_build_mfn_list_list will have already
    allocated all such pages up to xen_max_p2m_pfn.
    
    We also move the check for auto translated physmap down the
    stack so it is present in __set_phys_to_machine.
    
    [v2: Rebased with mmu->p2m code split]
    Reviewed-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index f25bdf238a33..8ea977277c55 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -41,6 +41,7 @@ extern unsigned int   machine_to_phys_order;
 
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
+extern bool __set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 
 extern int m2p_add_override(unsigned long mfn, struct page *page);
 extern int m2p_remove_override(struct page *page);

commit 87f1d40a706bdebdc8f959b9ac291d0d8fdfcc7e
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Dec 13 14:42:30 2010 +0000

    xen p2m: clear the old pte when adding a page to m2p_override
    
    When adding a page to m2p_override we change the p2m of the page so we
    need to also clear the old pte of the kernel linear mapping because it
    doesn't correspond anymore.
    
    When we remove the page from m2p_override we restore the original p2m of
    the page and we also restore the old pte of the kernel linear mapping.
    
    Before changing the p2m mappings in m2p_add_override and
    m2p_remove_override, check that the page passed as argument is valid and
    return an error if it is not.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 50f0a0f6bd6a..f25bdf238a33 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -42,8 +42,8 @@ extern unsigned int   machine_to_phys_order;
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 
-extern void m2p_add_override(unsigned long mfn, struct page *page);
-extern void m2p_remove_override(struct page *page);
+extern int m2p_add_override(unsigned long mfn, struct page *page);
+extern int m2p_remove_override(struct page *page);
 extern struct page *m2p_find_override(unsigned long mfn);
 extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
 

commit 448f2831934381e9d3c4d93e700ba7bbe14612dc
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Dec 15 13:19:33 2010 +0000

    xen: add m2p override mechanism
    
    Add a simple hashtable based mechanism to override some portions of the
    m2p, so that we can find out the pfn corresponding to an mfn of a
    granted page. In fact entries corresponding to granted pages in the m2p
    hold the original pfn value of the page in the source domain that
    granted it.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 8760cc60a21c..50f0a0f6bd6a 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -42,6 +42,11 @@ extern unsigned int   machine_to_phys_order;
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 
+extern void m2p_add_override(unsigned long mfn, struct page *page);
+extern void m2p_remove_override(struct page *page);
+extern struct page *m2p_find_override(unsigned long mfn);
+extern unsigned long m2p_find_override_pfn(unsigned long mfn, unsigned long pfn);
+
 static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {
 	unsigned long mfn;
@@ -72,9 +77,6 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return mfn;
 
-	if (unlikely((mfn >> machine_to_phys_order) != 0))
-		return ~0;
-
 	pfn = 0;
 	/*
 	 * The array access can fail (e.g., device space beyond end of RAM).
@@ -83,6 +85,14 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	 */
 	__get_user(pfn, &machine_to_phys_mapping[mfn]);
 
+	/*
+	 * If this appears to be a foreign mfn (because the pfn
+	 * doesn't map back to the mfn), then check the local override
+	 * table to see if there's a better pfn to use.
+	 */
+	if (get_phys_to_machine(pfn) != mfn)
+		pfn = m2p_find_override_pfn(mfn, pfn);
+
 	return pfn;
 }
 

commit 7e77506a5918d82cafa2ffa783ab57c23f9e9817
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Thu Sep 30 12:37:26 2010 +0100

    xen: implement XENMEM_machphys_mapping
    
    This hypercall allows Xen to specify a non-default location for the
    machine to physical mapping. This capability is used when running a 32
    bit domain 0 on a 64 bit hypervisor to shrink the hypervisor hole to
    exactly the size required.
    
    [ Impact: add Xen hypercall definitions ]
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index dd8c1414b3d5..8760cc60a21c 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -5,6 +5,7 @@
 #include <linux/types.h>
 #include <linux/spinlock.h>
 #include <linux/pfn.h>
+#include <linux/mm.h>
 
 #include <asm/uaccess.h>
 #include <asm/page.h>
@@ -35,6 +36,8 @@ typedef struct xpaddr {
 #define MAX_DOMAIN_PAGES						\
     ((unsigned long)((u64)CONFIG_XEN_MAX_DOMAIN_MEMORY * 1024 * 1024 * 1024 / PAGE_SIZE))
 
+extern unsigned long *machine_to_phys_mapping;
+extern unsigned int   machine_to_phys_order;
 
 extern unsigned long get_phys_to_machine(unsigned long pfn);
 extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
@@ -69,10 +72,8 @@ static inline unsigned long mfn_to_pfn(unsigned long mfn)
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return mfn;
 
-#if 0
 	if (unlikely((mfn >> machine_to_phys_order) != 0))
-		return max_mapnr;
-#endif
+		return ~0;
 
 	pfn = 0;
 	/*

commit 520045db940a381d2bee1c1b2179f7921b40fb10
Merge: 426e1f5cec48 9387377eb79a 45263cb0993d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 26 18:20:19 2010 -0700

    Merge branches 'upstream/xenfs' and 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'upstream/xenfs' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen/privcmd: make privcmd visible in domU
      xen/privcmd: move remap_domain_mfn_range() to core xen code and export.
      privcmd: MMAPBATCH: Fix error handling/reporting
      xenbus: export xen_store_interface for xenfs
      xen/privcmd: make sure vma is ours before doing anything to it
      xen/privcmd: print SIGBUS faults
      xen/xenfs: set_page_dirty is supposed to return true if it dirties
      xen/privcmd: create address space to allow writable mmaps
      xen: add privcmd driver
      xen: add variable hypercall caller
      xen: add xen_set_domain_pte()
      xen: add /proc/xen/xsd_{kva,port} to xenfs
    
    * 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen: (29 commits)
      xen: include xen/xen.h for definition of xen_initial_domain()
      xen: use host E820 map for dom0
      xen: correctly rebuild mfn list list after migration.
      xen: improvements to VIRQ_DEBUG output
      xen: set up IRQ before binding virq to evtchn
      xen: ensure that all event channels start off bound to VCPU 0
      xen/hvc: only notify if we actually sent something
      xen: don't add extra_pages for RAM after mem_end
      xen: add support for PAT
      xen: make sure xen_max_p2m_pfn is up to date
      xen: limit extra memory to a certain ratio of base
      xen: add extra pages for E820 RAM regions, even if beyond mem_end
      xen: make sure xen_extra_mem_start is beyond all non-RAM e820
      xen: implement "extra" memory to reserve space for pages not present at boot
      xen: Use host-provided E820 map
      xen: don't map missing memory
      xen: defer building p2m mfn structures until kernel is mapped
      xen: add return value to set_phys_to_machine()
      xen: convert p2m to a 3 level tree
      xen: make install_p2mtop_page() static
      ...
    
    Fix up trivial conflict in arch/x86/xen/mmu.c, and fix the use of
    'reserve_early()' - in the new memblock world order it is now
    'memblock_x86_reserve_range()' instead. Pointed out by Jeremy.

commit cfd8951e082a589637f9de3c33efd3218fdb3c03
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Aug 31 14:06:22 2010 -0700

    xen: don't map missing memory
    
    When setting up a pte for a missing pfn (no matching mfn), just create
    an empty pte rather than a junk mapping.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index e40ca6e67bb5..875f5a08a6c7 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -41,10 +41,17 @@ extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 
 static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {
+	unsigned long mfn;
+
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return pfn;
 
-	return get_phys_to_machine(pfn) & ~FOREIGN_FRAME_BIT;
+	mfn = get_phys_to_machine(pfn);
+
+	if (mfn != INVALID_P2M_ENTRY)
+		mfn &= ~FOREIGN_FRAME_BIT;
+
+	return mfn;
 }
 
 static inline int phys_to_machine_mapping_valid(unsigned long pfn)

commit c3798062f100c3e1d4ae1241bc536f3b1f28a6ca
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Aug 27 13:42:04 2010 -0700

    xen: add return value to set_phys_to_machine()
    
    set_phys_to_machine() can return false on failure, which means a memory
    allocation failure for the p2m structure.  It can only fail if setting
    the mfn for a pfn in previously unused address space.  It is guaranteed
    to succeed if you're setting a mapping to INVALID_P2M_ENTRY or updating
    the mfn for an existing pfn.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index bf5f7d32bd08..e40ca6e67bb5 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -37,7 +37,7 @@ typedef struct xpaddr {
 
 
 extern unsigned long get_phys_to_machine(unsigned long pfn);
-extern void set_phys_to_machine(unsigned long pfn, unsigned long mfn);
+extern bool set_phys_to_machine(unsigned long pfn, unsigned long mfn);
 
 static inline unsigned long pfn_to_mfn(unsigned long pfn)
 {

commit eba3ff8b99863bcc9e66b8d528e4750229e29693
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Feb 9 12:05:49 2009 -0800

    xen: add xen_set_domain_pte()
    
    Add xen_set_domain_pte() to allow setting a pte mapping a page from
    another domain.  The common case is to map from DOMID_IO, the pseudo
    domain which owns all IO pages, but will also be used in the privcmd
    interface to map other domain pages.
    
    [ Impact: new Xen-internal API for cross-domain mappings ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index bf5f7d32bd08..5e0eb8758919 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -159,6 +159,7 @@ static inline pte_t __pte_ma(pteval_t x)
 
 #define pgd_val_ma(x)	((x).pgd)
 
+void xen_set_domain_pte(pte_t *ptep, pte_t pteval, unsigned domid);
 
 xmaddr_t arbitrary_virt_to_machine(void *address);
 unsigned long arbitrary_virt_to_mfn(void *vaddr);

commit c0011dbfce69467b23b08fb4a64c39a409a935fb
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Thu Feb 4 14:46:34 2010 -0800

    xen: use _PAGE_IOMAP in ioremap to do machine mappings
    
    In a Xen domain, ioremap operates on machine addresses, not
    pseudo-physical addresses.  We use _PAGE_IOMAP to determine whether a
    mapping is intended for machine addresses.
    
    [ Impact: allow Xen domain to map real hardware ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 018a0a400799..bf5f7d32bd08 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -112,13 +112,9 @@ static inline xpaddr_t machine_to_phys(xmaddr_t machine)
  */
 static inline unsigned long mfn_to_local_pfn(unsigned long mfn)
 {
-	extern unsigned long max_mapnr;
 	unsigned long pfn = mfn_to_pfn(mfn);
-	if ((pfn < max_mapnr)
-	    && !xen_feature(XENFEAT_auto_translated_physmap)
-	    && (get_phys_to_machine(pfn) != mfn))
-		return max_mapnr; /* force !pfn_valid() */
-	/* XXX fixme; not true with sparsemem */
+	if (get_phys_to_machine(pfn) != mfn)
+		return -1; /* force !pfn_valid() */
 	return pfn;
 }
 

commit b40bf53effc0338ad7926aa1abce703af372cbd4
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Mon Feb 9 12:05:46 2009 -0800

    Xen: Add virt_to_pfn helper function
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 1a918dde46b5..018a0a400799 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -124,7 +124,8 @@ static inline unsigned long mfn_to_local_pfn(unsigned long mfn)
 
 /* VIRT <-> MACHINE conversion */
 #define virt_to_machine(v)	(phys_to_machine(XPADDR(__pa(v))))
-#define virt_to_mfn(v)		(pfn_to_mfn(PFN_DOWN(__pa(v))))
+#define virt_to_pfn(v)          (PFN_DOWN(__pa(v)))
+#define virt_to_mfn(v)		(pfn_to_mfn(virt_to_pfn(v)))
 #define mfn_to_virt(m)		(__va(mfn_to_pfn(m) << PAGE_SHIFT))
 
 static inline unsigned long pte_mfn(pte_t pte)

commit 9976b39b5031bbf76f715893cf080b6a17683881
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 27 09:19:26 2009 -0800

    xen: deal with virtually mapped percpu data
    
    The virtually mapped percpu space causes us two problems:
    
     - for hypercalls which take an mfn, we need to do a full pagetable
       walk to convert the percpu va into an mfn, and
    
     - when a hypercall requires a page to be mapped RO via all its aliases,
       we need to make sure its RO in both the percpu mapping and in the
       linear mapping
    
    This primarily affects the gdt and the vcpu info structure.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Xen-devel <xen-devel@lists.xensource.com>
    Cc: Gerd Hoffmann <kraxel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 4bd990ee43df..1a918dde46b5 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -164,6 +164,7 @@ static inline pte_t __pte_ma(pteval_t x)
 
 
 xmaddr_t arbitrary_virt_to_machine(void *address);
+unsigned long arbitrary_virt_to_mfn(void *vaddr);
 void make_lowmem_page_readonly(void *vaddr);
 void make_lowmem_page_readwrite(void *vaddr);
 

commit b534816b552d35bbd3c60702139ed5c7da2f55c2
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Feb 4 18:33:38 2009 -0800

    x86: don't apply __supported_pte_mask to non-present ptes
    
    On an x86 system which doesn't support global mappings,
    __supported_pte_mask has _PAGE_GLOBAL clear, to make sure it never
    appears in the PTE.  pfn_pte() and so on will enforce it with:
    
    static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
    {
            return __pte((((phys_addr_t)page_nr << PAGE_SHIFT) |
                          pgprot_val(pgprot)) & __supported_pte_mask);
    }
    
    However, we overload _PAGE_GLOBAL with _PAGE_PROTNONE on non-present
    ptes to distinguish them from swap entries.  However, applying
    __supported_pte_mask indiscriminately will clear the bit and corrupt the
    pte.
    
    I guess the best fix is to only apply __supported_pte_mask to present
    ptes.  This seems like the right solution to me, as it means we can
    completely ignore the issue of overlaps between the present pte bits and
    the non-present pte-as-swap entry use of the bits.
    
    __supported_pte_mask contains the set of flags we support on the
    current hardware.  We also use bits in the pte for things like
    logically present ptes with no permissions, and swap entries for
    swapped out pages.  We should only apply __supported_pte_mask to
    present ptes, because otherwise we may destroy other information being
    stored in the ptes.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 7ef617ef1df3..4bd990ee43df 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -137,7 +137,7 @@ static inline pte_t mfn_pte(unsigned long page_nr, pgprot_t pgprot)
 	pte_t pte;
 
 	pte.pte = ((phys_addr_t)page_nr << PAGE_SHIFT) |
-		(pgprot_val(pgprot) & __supported_pte_mask);
+			massage_pgprot(pgprot);
 
 	return pte;
 }

commit ecbf29cdb3990c83d90d0c4187c89fb2ce423367
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Dec 16 12:37:07 2008 -0800

    xen: clean up asm/xen/hypervisor.h
    
    Impact: cleanup
    
    hypervisor.h had accumulated a lot of crud, including lots of spurious
    #includes.  Clean it all up, and go around fixing up everything else
    accordingly.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index bc628998a1b9..7ef617ef1df3 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -1,11 +1,16 @@
 #ifndef _ASM_X86_XEN_PAGE_H
 #define _ASM_X86_XEN_PAGE_H
 
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/spinlock.h>
 #include <linux/pfn.h>
 
 #include <asm/uaccess.h>
+#include <asm/page.h>
 #include <asm/pgtable.h>
 
+#include <xen/interface/xen.h>
 #include <xen/features.h>
 
 /* Xen machine address */

commit 05e4d3169bd16229d84a2ef095e1ba2cd3873baa
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Oct 23 00:01:39 2008 -0700

    x86: drop double underscores from header guards
    
    Drop double underscores from header guards in arch/x86/include.  They
    are used inconsistently, and are not necessary.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index 4f483e4fcab3..bc628998a1b9 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -1,5 +1,5 @@
-#ifndef _ASM_X86_XEN__PAGE_H
-#define _ASM_X86_XEN__PAGE_H
+#ifndef _ASM_X86_XEN_PAGE_H
+#define _ASM_X86_XEN_PAGE_H
 
 #include <linux/pfn.h>
 
@@ -162,4 +162,4 @@ xmaddr_t arbitrary_virt_to_machine(void *address);
 void make_lowmem_page_readonly(void *vaddr);
 void make_lowmem_page_readwrite(void *vaddr);
 
-#endif /* _ASM_X86_XEN__PAGE_H */
+#endif /* _ASM_X86_XEN_PAGE_H */

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index d5eada0a48d9..4f483e4fcab3 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -1,5 +1,5 @@
-#ifndef ASM_X86__XEN__PAGE_H
-#define ASM_X86__XEN__PAGE_H
+#ifndef _ASM_X86_XEN__PAGE_H
+#define _ASM_X86_XEN__PAGE_H
 
 #include <linux/pfn.h>
 
@@ -162,4 +162,4 @@ xmaddr_t arbitrary_virt_to_machine(void *address);
 void make_lowmem_page_readonly(void *vaddr);
 void make_lowmem_page_readwrite(void *vaddr);
 
-#endif /* ASM_X86__XEN__PAGE_H */
+#endif /* _ASM_X86_XEN__PAGE_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
new file mode 100644
index 000000000000..d5eada0a48d9
--- /dev/null
+++ b/arch/x86/include/asm/xen/page.h
@@ -0,0 +1,165 @@
+#ifndef ASM_X86__XEN__PAGE_H
+#define ASM_X86__XEN__PAGE_H
+
+#include <linux/pfn.h>
+
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+
+#include <xen/features.h>
+
+/* Xen machine address */
+typedef struct xmaddr {
+	phys_addr_t maddr;
+} xmaddr_t;
+
+/* Xen pseudo-physical address */
+typedef struct xpaddr {
+	phys_addr_t paddr;
+} xpaddr_t;
+
+#define XMADDR(x)	((xmaddr_t) { .maddr = (x) })
+#define XPADDR(x)	((xpaddr_t) { .paddr = (x) })
+
+/**** MACHINE <-> PHYSICAL CONVERSION MACROS ****/
+#define INVALID_P2M_ENTRY	(~0UL)
+#define FOREIGN_FRAME_BIT	(1UL<<31)
+#define FOREIGN_FRAME(m)	((m) | FOREIGN_FRAME_BIT)
+
+/* Maximum amount of memory we can handle in a domain in pages */
+#define MAX_DOMAIN_PAGES						\
+    ((unsigned long)((u64)CONFIG_XEN_MAX_DOMAIN_MEMORY * 1024 * 1024 * 1024 / PAGE_SIZE))
+
+
+extern unsigned long get_phys_to_machine(unsigned long pfn);
+extern void set_phys_to_machine(unsigned long pfn, unsigned long mfn);
+
+static inline unsigned long pfn_to_mfn(unsigned long pfn)
+{
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return pfn;
+
+	return get_phys_to_machine(pfn) & ~FOREIGN_FRAME_BIT;
+}
+
+static inline int phys_to_machine_mapping_valid(unsigned long pfn)
+{
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return 1;
+
+	return get_phys_to_machine(pfn) != INVALID_P2M_ENTRY;
+}
+
+static inline unsigned long mfn_to_pfn(unsigned long mfn)
+{
+	unsigned long pfn;
+
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return mfn;
+
+#if 0
+	if (unlikely((mfn >> machine_to_phys_order) != 0))
+		return max_mapnr;
+#endif
+
+	pfn = 0;
+	/*
+	 * The array access can fail (e.g., device space beyond end of RAM).
+	 * In such cases it doesn't matter what we return (we return garbage),
+	 * but we must handle the fault without crashing!
+	 */
+	__get_user(pfn, &machine_to_phys_mapping[mfn]);
+
+	return pfn;
+}
+
+static inline xmaddr_t phys_to_machine(xpaddr_t phys)
+{
+	unsigned offset = phys.paddr & ~PAGE_MASK;
+	return XMADDR(PFN_PHYS(pfn_to_mfn(PFN_DOWN(phys.paddr))) | offset);
+}
+
+static inline xpaddr_t machine_to_phys(xmaddr_t machine)
+{
+	unsigned offset = machine.maddr & ~PAGE_MASK;
+	return XPADDR(PFN_PHYS(mfn_to_pfn(PFN_DOWN(machine.maddr))) | offset);
+}
+
+/*
+ * We detect special mappings in one of two ways:
+ *  1. If the MFN is an I/O page then Xen will set the m2p entry
+ *     to be outside our maximum possible pseudophys range.
+ *  2. If the MFN belongs to a different domain then we will certainly
+ *     not have MFN in our p2m table. Conversely, if the page is ours,
+ *     then we'll have p2m(m2p(MFN))==MFN.
+ * If we detect a special mapping then it doesn't have a 'struct page'.
+ * We force !pfn_valid() by returning an out-of-range pointer.
+ *
+ * NB. These checks require that, for any MFN that is not in our reservation,
+ * there is no PFN such that p2m(PFN) == MFN. Otherwise we can get confused if
+ * we are foreign-mapping the MFN, and the other domain as m2p(MFN) == PFN.
+ * Yikes! Various places must poke in INVALID_P2M_ENTRY for safety.
+ *
+ * NB2. When deliberately mapping foreign pages into the p2m table, you *must*
+ *      use FOREIGN_FRAME(). This will cause pte_pfn() to choke on it, as we
+ *      require. In all the cases we care about, the FOREIGN_FRAME bit is
+ *      masked (e.g., pfn_to_mfn()) so behaviour there is correct.
+ */
+static inline unsigned long mfn_to_local_pfn(unsigned long mfn)
+{
+	extern unsigned long max_mapnr;
+	unsigned long pfn = mfn_to_pfn(mfn);
+	if ((pfn < max_mapnr)
+	    && !xen_feature(XENFEAT_auto_translated_physmap)
+	    && (get_phys_to_machine(pfn) != mfn))
+		return max_mapnr; /* force !pfn_valid() */
+	/* XXX fixme; not true with sparsemem */
+	return pfn;
+}
+
+/* VIRT <-> MACHINE conversion */
+#define virt_to_machine(v)	(phys_to_machine(XPADDR(__pa(v))))
+#define virt_to_mfn(v)		(pfn_to_mfn(PFN_DOWN(__pa(v))))
+#define mfn_to_virt(m)		(__va(mfn_to_pfn(m) << PAGE_SHIFT))
+
+static inline unsigned long pte_mfn(pte_t pte)
+{
+	return (pte.pte & PTE_PFN_MASK) >> PAGE_SHIFT;
+}
+
+static inline pte_t mfn_pte(unsigned long page_nr, pgprot_t pgprot)
+{
+	pte_t pte;
+
+	pte.pte = ((phys_addr_t)page_nr << PAGE_SHIFT) |
+		(pgprot_val(pgprot) & __supported_pte_mask);
+
+	return pte;
+}
+
+static inline pteval_t pte_val_ma(pte_t pte)
+{
+	return pte.pte;
+}
+
+static inline pte_t __pte_ma(pteval_t x)
+{
+	return (pte_t) { .pte = x };
+}
+
+#define pmd_val_ma(v) ((v).pmd)
+#ifdef __PAGETABLE_PUD_FOLDED
+#define pud_val_ma(v) ((v).pgd.pgd)
+#else
+#define pud_val_ma(v) ((v).pud)
+#endif
+#define __pmd_ma(x)	((pmd_t) { (x) } )
+
+#define pgd_val_ma(x)	((x).pgd)
+
+
+xmaddr_t arbitrary_virt_to_machine(void *address);
+void make_lowmem_page_readonly(void *vaddr);
+void make_lowmem_page_readwrite(void *vaddr);
+
+#endif /* ASM_X86__XEN__PAGE_H */
