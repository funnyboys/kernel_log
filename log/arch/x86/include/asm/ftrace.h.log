commit f44d5c489051c2127189abb25d3d1625d9564c2d
Merge: f492de9dcf04 da4d401a6b8f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 14 11:46:52 2020 -0700

    Merge tag 'trace-v5.7-rc5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull more tracing fixes from Steven Rostedt:
     "Various tracing fixes:
    
       - Fix a crash when having function tracing and function stack tracing
         on the command line.
    
         The ftrace trampolines are created as executable and read only. But
         the stack tracer tries to modify them with text_poke() which
         expects all kernel text to still be writable at boot. Keep the
         trampolines writable at boot, and convert them to read-only with
         the rest of the kernel.
    
       - A selftest was triggering in the ring buffer iterator code, that is
         no longer valid with the update of keeping the ring buffer writable
         while a iterator is reading.
    
         Just bail after three failed attempts to get an event and remove
         the warning and disabling of the ring buffer.
    
       - While modifying the ring buffer code, decided to remove all the
         unnecessary BUG() calls"
    
    * tag 'trace-v5.7-rc5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      ring-buffer: Remove all BUG() calls
      ring-buffer: Don't deactivate the ring buffer on failed iterator reads
      x86/ftrace: Have ftrace trampolines turn read-only at the end of system boot up

commit 59566b0b622e3e6ea928c0b8cac8a5601b00b383
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 30 20:21:47 2020 -0400

    x86/ftrace: Have ftrace trampolines turn read-only at the end of system boot up
    
    Booting one of my machines, it triggered the following crash:
    
     Kernel/User page tables isolation: enabled
     ftrace: allocating 36577 entries in 143 pages
     Starting tracer 'function'
     BUG: unable to handle page fault for address: ffffffffa000005c
     #PF: supervisor write access in kernel mode
     #PF: error_code(0x0003) - permissions violation
     PGD 2014067 P4D 2014067 PUD 2015063 PMD 7b253067 PTE 7b252061
     Oops: 0003 [#1] PREEMPT SMP PTI
     CPU: 0 PID: 0 Comm: swapper Not tainted 5.4.0-test+ #24
     Hardware name: To Be Filled By O.E.M. To Be Filled By O.E.M./To be filled by O.E.M., BIOS SDBLI944.86P 05/08/2007
     RIP: 0010:text_poke_early+0x4a/0x58
     Code: 34 24 48 89 54 24 08 e8 bf 72 0b 00 48 8b 34 24 48 8b 4c 24 08 84 c0 74 0b 48 89 df f3 a4 48 83 c4 10 5b c3 9c 58 fa 48 89 df <f3> a4 50 9d 48 83 c4 10 5b e9 d6 f9 ff ff
    0 41 57 49
     RSP: 0000:ffffffff82003d38 EFLAGS: 00010046
     RAX: 0000000000000046 RBX: ffffffffa000005c RCX: 0000000000000005
     RDX: 0000000000000005 RSI: ffffffff825b9a90 RDI: ffffffffa000005c
     RBP: ffffffffa000005c R08: 0000000000000000 R09: ffffffff8206e6e0
     R10: ffff88807b01f4c0 R11: ffffffff8176c106 R12: ffffffff8206e6e0
     R13: ffffffff824f2440 R14: 0000000000000000 R15: ffffffff8206eac0
     FS:  0000000000000000(0000) GS:ffff88807d400000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     CR2: ffffffffa000005c CR3: 0000000002012000 CR4: 00000000000006b0
     Call Trace:
      text_poke_bp+0x27/0x64
      ? mutex_lock+0x36/0x5d
      arch_ftrace_update_trampoline+0x287/0x2d5
      ? ftrace_replace_code+0x14b/0x160
      ? ftrace_update_ftrace_func+0x65/0x6c
      __register_ftrace_function+0x6d/0x81
      ftrace_startup+0x23/0xc1
      register_ftrace_function+0x20/0x37
      func_set_flag+0x59/0x77
      __set_tracer_option.isra.19+0x20/0x3e
      trace_set_options+0xd6/0x13e
      apply_trace_boot_options+0x44/0x6d
      register_tracer+0x19e/0x1ac
      early_trace_init+0x21b/0x2c9
      start_kernel+0x241/0x518
      ? load_ucode_intel_bsp+0x21/0x52
      secondary_startup_64+0xa4/0xb0
    
    I was able to trigger it on other machines, when I added to the kernel
    command line of both "ftrace=function" and "trace_options=func_stack_trace".
    
    The cause is the "ftrace=function" would register the function tracer
    and create a trampoline, and it will set it as executable and
    read-only. Then the "trace_options=func_stack_trace" would then update
    the same trampoline to include the stack tracer version of the function
    tracer. But since the trampoline already exists, it updates it with
    text_poke_bp(). The problem is that text_poke_bp() called while
    system_state == SYSTEM_BOOTING, it will simply do a memcpy() and not
    the page mapping, as it would think that the text is still read-write.
    But in this case it is not, and we take a fault and crash.
    
    Instead, lets keep the ftrace trampolines read-write during boot up,
    and then when the kernel executable text is set to read-only, the
    ftrace trampolines get set to read-only as well.
    
    Link: https://lkml.kernel.org/r/20200430202147.4dc6e2de@oasis.local.home
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: stable@vger.kernel.org
    Fixes: 768ae4406a5c ("x86/ftrace: Use text_poke()")
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 85be2f506272..89af0d2c62aa 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -56,6 +56,12 @@ struct dyn_arch_ftrace {
 
 #ifndef __ASSEMBLY__
 
+#if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_DYNAMIC_FTRACE)
+extern void set_ftrace_ops_ro(void);
+#else
+static inline void set_ftrace_ops_ro(void) { }
+#endif
+
 #define ARCH_HAS_SYSCALL_MATCH_SYM_NAME
 static inline bool arch_syscall_match_sym_name(const char *sym, const char *name)
 {

commit fdc63ff0e49c588884992b4b2656345a5e878b32
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Wed Apr 8 21:13:10 2020 +0300

    ftrace/x86: Fix trace event registration for syscalls without arguments
    
    The refactoring of SYSCALL_DEFINE0() macros removed the ABI stubs and
    simply defines __abi_sys_$NAME as alias of __do_sys_$NAME.
    
    As a result kallsyms_lookup() returns "__do_sys_$NAME" which does not match
    with the declared trace event name.
    
    See also commit 1c758a2202a6 ("tracing/x86: Update syscall trace events to
    handle new prefixed syscall func names").
    
    Add __do_sys_ to the valid prefixes which are checked in
    arch_syscall_match_sym_name().
    
    Fixes: d2b5de495ee9 ("x86/entry: Refactor SYSCALL_DEFINE0 macros")
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Link: https://lkml.kernel.org/r/158636958997.7900.16485049455470033557.stgit@buzz

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 85be2f506272..70b96cae5b42 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -61,11 +61,12 @@ static inline bool arch_syscall_match_sym_name(const char *sym, const char *name
 {
 	/*
 	 * Compare the symbol name with the system call name. Skip the
-	 * "__x64_sys", "__ia32_sys" or simple "sys" prefix.
+	 * "__x64_sys", "__ia32_sys", "__do_sys" or simple "sys" prefix.
 	 */
 	return !strcmp(sym + 3, name + 3) ||
 		(!strncmp(sym, "__x64_", 6) && !strcmp(sym + 9, name + 3)) ||
-		(!strncmp(sym, "__ia32_", 7) && !strcmp(sym + 10, name + 3));
+		(!strncmp(sym, "__ia32_", 7) && !strcmp(sym + 10, name + 3)) ||
+		(!strncmp(sym, "__do_sys", 8) && !strcmp(sym + 8, name + 3));
 }
 
 #ifndef COMPILE_OFFSETS

commit 2040cf9f59037aa8aec749363e69ead165b67b43
Merge: f66c0447cca1 e42617b825f8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 10 10:11:00 2019 +0100

    Merge tag 'v5.5-rc1' into core/kprobes, to resolve conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 768ae4406a5cab7e8702550f2446dbeb377b798d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 26 14:48:23 2019 +0200

    x86/ftrace: Use text_poke()
    
    Move ftrace over to using the generic x86 text_poke functions; this
    avoids having a second/different copy of that code around.
    
    This also avoids ftrace violating the (new) W^X rule and avoids
    fragmenting the kernel text page-tables, due to no longer having to
    toggle them RW.
    
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191111132457.761255803@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index c38a66661576..dbd9b08bf173 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -34,8 +34,6 @@ struct dyn_arch_ftrace {
 	/* No extra data needed for x86 */
 };
 
-int ftrace_int3_handler(struct pt_regs *regs);
-
 #define FTRACE_GRAPH_TRAMP_ADDR FTRACE_GRAPH_ADDR
 
 #endif /*  CONFIG_DYNAMIC_FTRACE */

commit 562955fe6a558b9ef98ad87c470314946338cb2f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Nov 8 13:11:39 2019 -0500

    ftrace/x86: Add register_ftrace_direct() for custom trampolines
    
    Enable x86 to allow for register_ftrace_direct(), where a custom trampoline
    may be called directly from an ftrace mcount/fentry location.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index c38a66661576..c2a7458f912c 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -28,6 +28,19 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 	return addr;
 }
 
+/*
+ * When a ftrace registered caller is tracing a function that is
+ * also set by a register_ftrace_direct() call, it needs to be
+ * differentiated in the ftrace_caller trampoline. To do this, we
+ * place the direct caller in the ORIG_AX part of pt_regs. This
+ * tells the ftrace_caller that there's a direct caller.
+ */
+static inline void arch_ftrace_set_direct_caller(struct pt_regs *regs, unsigned long addr)
+{
+	/* Emulate a call */
+	regs->orig_ax = addr;
+}
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 
 struct dyn_arch_ftrace {

commit 2e815627318910fb2ab004670a83ba27ac2228b6
Author: Jisheng Zhang <Jisheng.Zhang@synaptics.com>
Date:   Mon Aug 26 09:13:12 2019 +0000

    ftrace/x86: Remove mcount() declaration
    
    Commit 562e14f72292 ("ftrace/x86: Remove mcount support") removed the
    support for using mcount, so we could remove the mcount() declaration
    to clean up.
    
    Link: http://lkml.kernel.org/r/20190826170150.10f101ba@xhacker.debian
    
    Signed-off-by: Jisheng Zhang <Jisheng.Zhang@synaptics.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 287f1f7b2e52..c38a66661576 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -16,7 +16,6 @@
 #define HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
 
 #ifndef __ASSEMBLY__
-extern void mcount(void);
 extern atomic_t modifying_ftrace_code;
 extern void __fentry__(void);
 

commit 562e14f72292249e52e6346a9e3a30be652b0cf6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu May 9 15:32:05 2019 -0400

    ftrace/x86: Remove mcount support
    
    There's two methods of enabling function tracing in Linux on x86. One is
    with just "gcc -pg" and the other is "gcc -pg -mfentry". The former will use
    calls to a special function "mcount" after the frame is set up in all C
    functions. The latter will add calls to a special function called "fentry"
    as the very first instruction of all C functions.
    
    At compile time, there is a check to see if gcc supports, -mfentry, and if
    it does, it will use that, because it is more versatile and less error prone
    for function tracing.
    
    Starting with v4.19, the minimum gcc supported to build the Linux kernel,
    was raised to version 4.6. That also happens to be the first gcc version to
    support -mfentry. Since on x86, using gcc versions from 4.6 and beyond will
    unconditionally enable the -mfentry, it will no longer use mcount as the
    method for inserting calls into the C functions of the kernel. This means
    that there is no point in continuing to maintain mcount in x86.
    
    Remove support for using mcount. This makes the code less complex, and will
    also allow it to be simplified in the future.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Jiri Kosina <jkosina@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index cf350639e76d..287f1f7b2e52 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -3,12 +3,10 @@
 #define _ASM_X86_FTRACE_H
 
 #ifdef CONFIG_FUNCTION_TRACER
-#ifdef CC_USING_FENTRY
-# define MCOUNT_ADDR		((unsigned long)(__fentry__))
-#else
-# define MCOUNT_ADDR		((unsigned long)(mcount))
-# define HAVE_FUNCTION_GRAPH_FP_TEST
+#ifndef CC_USING_FENTRY
+# error Compiler does not support fentry?
 #endif
+# define MCOUNT_ADDR		((unsigned long)(__fentry__))
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit a846446b1914d1e3d996d657754f43fde89bab51
Author: Dmitry Safonov <dima@arista.com>
Date:   Fri Oct 12 14:42:52 2018 +0100

    x86/compat: Adjust in_compat_syscall() to generic code under !COMPAT
    
    The result of in_compat_syscall() can be pictured as:
    
    x86 platform:
        ---------------------------------------------------
        |  Arch\syscall  |  64-bit  |   ia32   |   x32    |
        |-------------------------------------------------|
        |     x86_64     |  false   |   true   |   true   |
        |-------------------------------------------------|
        |      i686      |          |  <true>  |          |
        ---------------------------------------------------
    
    Other platforms:
        -------------------------------------------
        |  Arch\syscall  |  64-bit  |   compat    |
        |-----------------------------------------|
        |     64-bit     |  false   |    true     |
        |-----------------------------------------|
        |    32-bit(?)   |          |   <false>   |
        -------------------------------------------
    
    As seen, the result of in_compat_syscall() on generic 32-bit platform
    differs from i686.
    
    There is no reason for in_compat_syscall() == true on native i686.  It also
    easy to misread code if the result on native 32-bit platform differs
    between arches.
    
    Because of that non arch-specific code has many places with:
        if (IS_ENABLED(CONFIG_COMPAT) && in_compat_syscall())
    in different variations.
    
    It looks-like the only non-x86 code which uses in_compat_syscall() not
    under CONFIG_COMPAT guard is in amd/amdkfd. But according to the commit
    a18069c132cb ("amdkfd: Disable support for 32-bit user processes"), it
    actually should be disabled on native i686.
    
    Rename in_compat_syscall() to in_32bit_syscall() for x86-specific code
    and make in_compat_syscall() false under !CONFIG_COMPAT.
    
    A follow on patch will clean up generic users which were forced to check
    IS_ENABLED(CONFIG_COMPAT) with in_compat_syscall().
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Dmitry Safonov <0x7f454c46@gmail.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Stephen Boyd <sboyd@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linux-efi@vger.kernel.org
    Cc: netdev@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181012134253.23266-2-dima@arista.com

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index c18ed65287d5..cf350639e76d 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -76,9 +76,7 @@ static inline bool arch_syscall_match_sym_name(const char *sym, const char *name
 #define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS 1
 static inline bool arch_trace_is_compat_syscall(struct pt_regs *regs)
 {
-	if (in_compat_syscall())
-		return true;
-	return false;
+	return in_32bit_syscall();
 }
 #endif /* CONFIG_FTRACE_SYSCALLS && CONFIG_IA32_EMULATION */
 #endif /* !COMPILE_OFFSETS */

commit 604a98f1df2897f9ea6ca6bdab8e1c2d6844be01
Merge: 1cfd904f1674 7dba33c6346c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 2 16:11:12 2018 +0200

    Merge branch 'timers/urgent' into timers/core
    
    Pick up urgent fixes to apply dependent cleanup patch

commit 1c758a2202a6b4624d0703013a2c6cfa6e7455aa
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 17 17:41:28 2018 -0400

    tracing/x86: Update syscall trace events to handle new prefixed syscall func names
    
    Arnaldo noticed that the latest kernel is missing the syscall event system
    directory in x86. I bisected it down to d5a00528b58c ("syscalls/core,
    syscalls/x86: Rename struct pt_regs-based sys_*() to __x64_sys_*()").
    
    The system call trace events are special, as there is only one trace event
    for all system calls (the raw_syscalls). But a macro that wraps the system
    calls creates meta data for them that copies the name to find the system
    call that maps to the system call table (the number). At boot up, it does a
    kallsyms lookup of the system call table to find the function that maps to
    the meta data of the system call. If it does not find a function, then that
    system call is ignored.
    
    Because the x86 system calls had "__x64_", or "__ia32_" prefixed to the
    "sys" for the names, they do not match the default compare algorithm. As
    this was a problem for power pc, the algorithm can be overwritten by the
    architecture. The solution is to have x86 have its own algorithm to do the
    compare and this brings back the system call trace events.
    
    Link: http://lkml.kernel.org/r/20180417174128.0f3457f0@gandalf.local.home
    
    Reported-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Fixes: d5a00528b58c ("syscalls/core, syscalls/x86: Rename struct pt_regs-based sys_*() to __x64_sys_*()")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 09ad88572746..cc8f8fcf9b4a 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -46,7 +46,21 @@ int ftrace_int3_handler(struct pt_regs *regs);
 #endif /* CONFIG_FUNCTION_TRACER */
 
 
-#if !defined(__ASSEMBLY__) && !defined(COMPILE_OFFSETS)
+#ifndef __ASSEMBLY__
+
+#define ARCH_HAS_SYSCALL_MATCH_SYM_NAME
+static inline bool arch_syscall_match_sym_name(const char *sym, const char *name)
+{
+	/*
+	 * Compare the symbol name with the system call name. Skip the
+	 * "__x64_sys", "__ia32_sys" or simple "sys" prefix.
+	 */
+	return !strcmp(sym + 3, name + 3) ||
+		(!strncmp(sym, "__x64_", 6) && !strcmp(sym + 9, name + 3)) ||
+		(!strncmp(sym, "__ia32_", 7) && !strcmp(sym + 10, name + 3));
+}
+
+#ifndef COMPILE_OFFSETS
 
 #if defined(CONFIG_FTRACE_SYSCALLS) && defined(CONFIG_IA32_EMULATION)
 #include <asm/compat.h>
@@ -67,6 +81,7 @@ static inline bool arch_trace_is_compat_syscall(struct pt_regs *regs)
 	return false;
 }
 #endif /* CONFIG_FTRACE_SYSCALLS && CONFIG_IA32_EMULATION */
-#endif /* !__ASSEMBLY__  && !COMPILE_OFFSETS */
+#endif /* !COMPILE_OFFSETS */
+#endif /* !__ASSEMBLY__ */
 
 #endif /* _ASM_X86_FTRACE_H */

commit 0d55303c51a4f35f674617e415632d492b596c26
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Tue Mar 13 21:03:25 2018 -0700

    compat: Move compat_timespec/ timeval to compat_time.h
    
    All the current architecture specific defines for these
    are the same. Refactor these common defines to a common
    header file.
    
    The new common linux/compat_time.h is also useful as it
    will eventually be used to hold all the defines that
    are needed for compat time types that support non y2038
    safe types. New architectures need not have to define these
    new types as they will only use new y2038 safe syscalls.
    This file can be deleted after y2038 when we stop supporting
    non y2038 safe syscalls.
    
    The patch also requires an operation similar to:
    
    git grep "asm/compat\.h" | cut -d ":" -f 1 |  xargs -n 1 sed -i -e "s%asm/compat.h%linux/compat.h%g"
    
    Cc: acme@kernel.org
    Cc: benh@kernel.crashing.org
    Cc: borntraeger@de.ibm.com
    Cc: catalin.marinas@arm.com
    Cc: cmetcalf@mellanox.com
    Cc: cohuck@redhat.com
    Cc: davem@davemloft.net
    Cc: deller@gmx.de
    Cc: devel@driverdev.osuosl.org
    Cc: gerald.schaefer@de.ibm.com
    Cc: gregkh@linuxfoundation.org
    Cc: heiko.carstens@de.ibm.com
    Cc: hoeppner@linux.vnet.ibm.com
    Cc: hpa@zytor.com
    Cc: jejb@parisc-linux.org
    Cc: jwi@linux.vnet.ibm.com
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: mark.rutland@arm.com
    Cc: mingo@redhat.com
    Cc: mpe@ellerman.id.au
    Cc: oberpar@linux.vnet.ibm.com
    Cc: oprofile-list@lists.sf.net
    Cc: paulus@samba.org
    Cc: peterz@infradead.org
    Cc: ralf@linux-mips.org
    Cc: rostedt@goodmis.org
    Cc: rric@kernel.org
    Cc: schwidefsky@de.ibm.com
    Cc: sebott@linux.vnet.ibm.com
    Cc: sparclinux@vger.kernel.org
    Cc: sth@linux.vnet.ibm.com
    Cc: ubraun@linux.vnet.ibm.com
    Cc: will.deacon@arm.com
    Cc: x86@kernel.org
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: James Hogan <jhogan@kernel.org>
    Acked-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 09ad88572746..db25aa15b705 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -49,7 +49,7 @@ int ftrace_int3_handler(struct pt_regs *regs);
 #if !defined(__ASSEMBLY__) && !defined(COMPILE_OFFSETS)
 
 #if defined(CONFIG_FTRACE_SYSCALLS) && defined(CONFIG_IA32_EMULATION)
-#include <asm/compat.h>
+#include <linux/compat.h>
 
 /*
  * Because ia32 syscalls do not map to x86_64 syscall numbers

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index eccd0ac6bc38..09ad88572746 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_FTRACE_H
 #define _ASM_X86_FTRACE_H
 

commit 471bd10f5e2880bd91a2627d887f6062494cfe9c
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Aug 19 06:53:00 2016 -0500

    ftrace/x86: Implement HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
    
    Use the more reliable version of ftrace_graph_ret_addr() so we no longer
    have to worry about the unwinder getting out of sync with the function
    graph ret_stack index, which can happen if the unwinder skips any frames
    before calling ftrace_graph_ret_addr().
    
    This fixes this issue (and several others like it):
    
      $ cat /proc/self/stack
      [<ffffffff810489a2>] save_stack_trace_tsk+0x22/0x40
      [<ffffffff81311a89>] proc_pid_stack+0xb9/0x110
      [<ffffffff813127c4>] proc_single_show+0x54/0x80
      [<ffffffff812be088>] seq_read+0x108/0x3e0
      [<ffffffff812923d7>] __vfs_read+0x37/0x140
      [<ffffffff812929d9>] vfs_read+0x99/0x140
      [<ffffffff81293f28>] SyS_read+0x58/0xc0
      [<ffffffff818af97c>] entry_SYSCALL_64_fastpath+0x1f/0xbd
      [<ffffffffffffffff>] 0xffffffffffffffff
    
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
    
      $ cat /proc/self/stack
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff810394cc>] print_context_stack+0xfc/0x100
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff8103891b>] dump_trace+0x12b/0x350
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff810489a2>] save_stack_trace_tsk+0x22/0x40
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff81311a89>] proc_pid_stack+0xb9/0x110
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff813127c4>] proc_single_show+0x54/0x80
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff812be088>] seq_read+0x108/0x3e0
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff812923d7>] __vfs_read+0x37/0x140
      [<ffffffff818b2428>] return_to_handler+0x0/0x27
      [<ffffffff812929d9>] vfs_read+0x99/0x140
      [<ffffffffffffffff>] 0xffffffffffffffff
    
    Enabling function graph tracing causes the stack trace to change in two
    ways:
    
    First, the real call addresses are confusingly interspersed with
    'return_to_handler' addresses.  This issue will be fixed by the next
    patch.
    
    Second, the stack trace is offset by two frames, because the unwinder
    skipped the first two frames and got out of sync with the ret_stack
    index.  This patch fixes this issue.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a6d623e36f8d08f9a17bd74d804d201177a23afd.1471607358.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 37f67cbba1c6..eccd0ac6bc38 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -14,6 +14,8 @@
 #define ARCH_SUPPORTS_FTRACE_OPS 1
 #endif
 
+#define HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
+
 #ifndef __ASSEMBLY__
 extern void mcount(void);
 extern atomic_t modifying_ftrace_code;

commit e4a744ef2fef5c803348b650a3a2d01da7797a9b
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Aug 19 06:52:55 2016 -0500

    ftrace: Remove CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST from config
    
    Make HAVE_FUNCTION_GRAPH_FP_TEST a normal define, independent from
    kconfig.  This removes some config file pollution and simplifies the
    checking for the fp test.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2c4e5f05054d6d367f702fd153af7a0109dd5c81.1471607358.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index a4820d4df617..37f67cbba1c6 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -6,6 +6,7 @@
 # define MCOUNT_ADDR		((unsigned long)(__fentry__))
 #else
 # define MCOUNT_ADDR		((unsigned long)(mcount))
+# define HAVE_FUNCTION_GRAPH_FP_TEST
 #endif
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
 

commit d88f48e12821ab4b2244124d50ac094568f48db5
Merge: be53f58fa0fc 9da77666d697
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 09:47:32 2016 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes:
    
       - fix hotplug bugs
       - fix irq live lock
       - fix various topology handling bugs
       - fix APIC ACK ordering
       - fix PV iopl handling
       - fix speling
       - fix/tweak memcpy_mcsafe() return value
       - fix fbcon bug
       - remove stray prototypes"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/msr: Remove unused native_read_tscp()
      x86/apic: Remove declaration of unused hw_nmi_is_cpu_stuck
      x86/oprofile/nmi: Add missing hotplug FROZEN handling
      x86/hpet: Use proper mask to modify hotplug action
      x86/apic/uv: Fix the hotplug notifier
      x86/apb/timer: Use proper mask to modify hotplug action
      x86/topology: Use total_cpus not nr_cpu_ids for logical packages
      x86/topology: Fix Intel HT disable
      x86/topology: Fix logical package mapping
      x86/irq: Cure live lock in fixup_irqs()
      x86/tsc: Prevent NULL pointer deref in calibrate_delay_is_known()
      x86/apic: Fix suspicious RCU usage in smp_trace_call_function_interrupt()
      x86/iopl: Fix iopl capability check on Xen PV
      x86/iopl/64: Properly context-switch IOPL on Xen PV
      selftests/x86: Add an iopl test
      x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe()
      x86/video: Don't assume all FB devices are PCI devices
      arch/x86/irq: Purge useless handler declarations from hw_irq.h
      x86: Fix misspellings in comments

commit f970165beeaa4803438e435d68022b3680a1a0b0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Mar 22 14:25:27 2016 -0700

    x86/compat: remove is_compat_task()
    
    x86's is_compat_task always checked the current syscall type, not the
    task type.  It has no non-arch users any more, so just remove it to
    avoid confusion.
    
    On x86, nothing should really be checking the task ABI.  There are
    legitimate users for the syscall ABI and for the mm ABI.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 24938852db30..21b66dbf3601 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -58,7 +58,7 @@ int ftrace_int3_handler(struct pt_regs *regs);
 #define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS 1
 static inline bool arch_trace_is_compat_syscall(struct pt_regs *regs)
 {
-	if (is_compat_task())
+	if (in_compat_syscall())
 		return true;
 	return false;
 }

commit 6a6256f9e0ebaabf7ded1fef8977a4352dbe7784
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Tue Feb 23 15:34:30 2016 -0800

    x86: Fix misspellings in comments
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: trivial@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 24938852db30..abbad505dd24 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -52,7 +52,7 @@ int ftrace_int3_handler(struct pt_regs *regs);
  * this screws up the trace output when tracing a ia32 task.
  * Instead of reporting bogus syscalls, just do not trace them.
  *
- * If the user realy wants these, then they should use the
+ * If the user really wants these, then they should use the
  * raw syscall tracepoints with filtering.
  */
 #define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS 1

commit c93bf928fea22c61f6b5c04786b325c9bfbc0462
Author: Minfei Huang <mnfhuang@gmail.com>
Date:   Sun Jul 12 17:52:24 2015 +0800

    ftrace: Format MCOUNT_ADDR address as type unsigned long
    
    Always we use type unsigned long to format the ip address, since the
    value of ip address is never the negative.
    
    This patch uses type unsigned long, instead of long, to format the ip
    address. The code is more clearly to be viewed by using type unsigned
    long, although it is correct by using either unsigned long or long.
    
    Link: http://lkml.kernel.org/r/1436694744-16747-1-git-send-email-mhuang@redhat.com
    
    Cc: Minfei Huang <mnfhuang@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Minfei Huang <mnfhuang@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index f45acad3c4b6..24938852db30 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -3,9 +3,9 @@
 
 #ifdef CONFIG_FUNCTION_TRACER
 #ifdef CC_USING_FENTRY
-# define MCOUNT_ADDR		((long)(__fentry__))
+# define MCOUNT_ADDR		((unsigned long)(__fentry__))
 #else
-# define MCOUNT_ADDR		((long)(mcount))
+# define MCOUNT_ADDR		((unsigned long)(mcount))
 #endif
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
 

commit 4bcdf1522fb11b6c3a3dabe4432b54da2bd6bc0e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 24 11:30:58 2014 -0500

    ftrace/x86: Move MCOUNT_SAVE_FRAME out of header file
    
    Linus pointed out that MCOUNT_SAVE_FRAME is used in only a single file
    and that there's no reason that it should be in a header file.
    Move the macro to the code that uses it.
    
    Link: http://lkml.kernel.org/r/CA+55aFwF+qCGSKdGaEgW4p6N65GZ5_XTV=1NbtWDvxnd5yYLiw@mail.gmail.com
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1411262304010.3961@nanos
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index e1f7fecaa7d6..f45acad3c4b6 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -1,39 +1,6 @@
 #ifndef _ASM_X86_FTRACE_H
 #define _ASM_X86_FTRACE_H
 
-#ifdef __ASSEMBLY__
-
-	/* skip is set if the stack was already partially adjusted */
-	.macro MCOUNT_SAVE_FRAME skip=0
-	 /*
-	  * We add enough stack to save all regs.
-	  */
-	subq $(SS+8-\skip), %rsp
-	movq %rax, RAX(%rsp)
-	movq %rcx, RCX(%rsp)
-	movq %rdx, RDX(%rsp)
-	movq %rsi, RSI(%rsp)
-	movq %rdi, RDI(%rsp)
-	movq %r8, R8(%rsp)
-	movq %r9, R9(%rsp)
-	 /* Move RIP to its proper location */
-	movq SS+8(%rsp), %rdx
-	movq %rdx, RIP(%rsp)
-	.endm
-
-	.macro MCOUNT_RESTORE_FRAME skip=0
-	movq R9(%rsp), %r9
-	movq R8(%rsp), %r8
-	movq RDI(%rsp), %rdi
-	movq RSI(%rsp), %rsi
-	movq RDX(%rsp), %rdx
-	movq RCX(%rsp), %rcx
-	movq RAX(%rsp), %rax
-	addq $(SS+8-\skip), %rsp
-	.endm
-
-#endif
-
 #ifdef CONFIG_FUNCTION_TRACER
 #ifdef CC_USING_FENTRY
 # define MCOUNT_ADDR		((long)(__fentry__))

commit 1026ff9b8e6cbb112fd708b2e62f1812ce9a4e01
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jul 11 23:23:53 2014 -0400

    ftrace/x86: Have function graph tracer use its own trampoline
    
    The function graph trampoline is called from the function trampoline
    and both do a save and restore of registers. The save of registers
    done by the function trampoline when only the function graph tracer
    is running is a waste of CPU cycles.
    
    As the function graph tracer trampoline in x86 is dependent from
    the function trampoline, we can call it directly when a function
    is only being traced by the function graph trampoline.
    
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 0525a8bdf65d..e1f7fecaa7d6 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -68,6 +68,8 @@ struct dyn_arch_ftrace {
 
 int ftrace_int3_handler(struct pt_regs *regs);
 
+#define FTRACE_GRAPH_TRAMP_ADDR FTRACE_GRAPH_ADDR
+
 #endif /*  CONFIG_DYNAMIC_FTRACE */
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_FUNCTION_TRACER */

commit f431b634f24d099872e78acc356c7fd35913b36b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Feb 12 16:18:59 2013 -0500

    tracing/syscalls: Allow archs to ignore tracing compat syscalls
    
    The tracing of ia32 compat system calls has been a bit of a pain as they
    use different system call numbers than the 64bit equivalents.
    
    I wrote a simple 'lls' program that lists files. I compiled it as a i686
    ELF binary and ran it under a x86_64 box. This is the result:
    
    echo 0 > /debug/tracing/tracing_on
    echo 1 > /debug/tracing/events/syscalls/enable
    echo 1 > /debug/tracing/tracing_on ; ./lls ; echo 0 > /debug/tracing/tracing_on
    
    grep lls /debug/tracing/trace
    
    [.. skipping calls before TS_COMPAT is set ...]
    
                 lls-1127  [005] d...   936.409188: sys_recvfrom(fd: 0, ubuf: 4d560fc4, size: 0, flags: 8048034, addr: 8, addr_len: f7700420)
                 lls-1127  [005] d...   936.409190: sys_recvfrom -> 0x8a77000
                 lls-1127  [005] d...   936.409211: sys_lgetxattr(pathname: 0, name: 1000, value: 3, size: 22)
                 lls-1127  [005] d...   936.409215: sys_lgetxattr -> 0xf76ff000
                 lls-1127  [005] d...   936.409223: sys_dup2(oldfd: 4d55ae9b, newfd: 4)
                 lls-1127  [005] d...   936.409228: sys_dup2 -> 0xfffffffffffffffe
                 lls-1127  [005] d...   936.409236: sys_newfstat(fd: 4d55b085, statbuf: 80000)
                 lls-1127  [005] d...   936.409242: sys_newfstat -> 0x3
                 lls-1127  [005] d...   936.409243: sys_removexattr(pathname: 3, name: ffcd0060)
                 lls-1127  [005] d...   936.409244: sys_removexattr -> 0x0
                 lls-1127  [005] d...   936.409245: sys_lgetxattr(pathname: 0, name: 19614, value: 1, size: 2)
                 lls-1127  [005] d...   936.409248: sys_lgetxattr -> 0xf76e5000
                 lls-1127  [005] d...   936.409248: sys_newlstat(filename: 3, statbuf: 19614)
                 lls-1127  [005] d...   936.409249: sys_newlstat -> 0x0
                 lls-1127  [005] d...   936.409262: sys_newfstat(fd: f76fb588, statbuf: 80000)
                 lls-1127  [005] d...   936.409279: sys_newfstat -> 0x3
                 lls-1127  [005] d...   936.409279: sys_close(fd: 3)
                 lls-1127  [005] d...   936.421550: sys_close -> 0x200
                 lls-1127  [005] d...   936.421558: sys_removexattr(pathname: 3, name: ffcd00d0)
                 lls-1127  [005] d...   936.421560: sys_removexattr -> 0x0
                 lls-1127  [005] d...   936.421569: sys_lgetxattr(pathname: 4d564000, name: 1b1abc, value: 5, size: 802)
                 lls-1127  [005] d...   936.421574: sys_lgetxattr -> 0x4d564000
                 lls-1127  [005] d...   936.421575: sys_capget(header: 4d70f000, dataptr: 1000)
                 lls-1127  [005] d...   936.421580: sys_capget -> 0x0
                 lls-1127  [005] d...   936.421580: sys_lgetxattr(pathname: 4d710000, name: 3000, value: 3, size: 812)
                 lls-1127  [005] d...   936.421589: sys_lgetxattr -> 0x4d710000
                 lls-1127  [005] d...   936.426130: sys_lgetxattr(pathname: 4d713000, name: 2abc, value: 3, size: 32)
                 lls-1127  [005] d...   936.426141: sys_lgetxattr -> 0x4d713000
                 lls-1127  [005] d...   936.426145: sys_newlstat(filename: 3, statbuf: f76ff3f0)
                 lls-1127  [005] d...   936.426146: sys_newlstat -> 0x0
                 lls-1127  [005] d...   936.431748: sys_lgetxattr(pathname: 0, name: 1000, value: 3, size: 22)
    
    Obviously I'm not calling newfstat with a fd of 4d55b085. The calls are
    obviously incorrect, and confusing.
    
    Other efforts have been made to fix this:
    
    https://lkml.org/lkml/2012/3/26/367
    
    But the real solution is to rewrite the syscall internals and come up
    with a fixed solution. One that doesn't require all the kluge that the
    current solution has.
    
    Thus for now, instead of outputting incorrect data, simply ignore them.
    With this patch the changes now have:
    
     #> grep lls /debug/tracing/trace
     #>
    
    Compat system calls simply are not traced. If users need compat
    syscalls, then they should just use the raw syscall tracepoints.
    
    For an architecture to make their compat syscalls ignored, it must
    define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS (done in asm/ftrace.h) and also
    define an arch_trace_is_compat_syscall() function that will return true
    if the current task should ignore tracing the syscall.
    
    I want to stress that this change does not affect actual syscalls in any
    way, shape or form. It is only used within the tracing system and
    doesn't interfere with the syscall logic at all. The changes are
    consolidated nicely into trace_syscalls.c and asm/ftrace.h.
    
    I had to make one small modification to asm/thread_info.h and that was
    to remove the include of asm/ftrace.h. As asm/ftrace.h required the
    current_thread_info() it was causing include hell. That include was
    added back in 2008 when the function graph tracer was added:
    
     commit caf4b323 "tracing, x86: add low level support for ftrace return tracing"
    
    It does not need to be included there.
    
    Link: http://lkml.kernel.org/r/1360703939.21867.99.camel@gandalf.local.home
    
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 86cb51e1ca96..0525a8bdf65d 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -72,4 +72,28 @@ int ftrace_int3_handler(struct pt_regs *regs);
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_FUNCTION_TRACER */
 
+
+#if !defined(__ASSEMBLY__) && !defined(COMPILE_OFFSETS)
+
+#if defined(CONFIG_FTRACE_SYSCALLS) && defined(CONFIG_IA32_EMULATION)
+#include <asm/compat.h>
+
+/*
+ * Because ia32 syscalls do not map to x86_64 syscall numbers
+ * this screws up the trace output when tracing a ia32 task.
+ * Instead of reporting bogus syscalls, just do not trace them.
+ *
+ * If the user realy wants these, then they should use the
+ * raw syscall tracepoints with filtering.
+ */
+#define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS 1
+static inline bool arch_trace_is_compat_syscall(struct pt_regs *regs)
+{
+	if (is_compat_task())
+		return true;
+	return false;
+}
+#endif /* CONFIG_FTRACE_SYSCALLS && CONFIG_IA32_EMULATION */
+#endif /* !__ASSEMBLY__  && !COMPILE_OFFSETS */
+
 #endif /* _ASM_X86_FTRACE_H */

commit 06aeaaeabf69da4a3e86df532425640f51b01cef
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Fri Sep 28 17:15:17 2012 +0900

    ftrace: Move ARCH_SUPPORTS_FTRACE_SAVE_REGS in Kconfig
    
    Move SAVE_REGS support flag into Kconfig and rename
    it to CONFIG_DYNAMIC_FTRACE_WITH_REGS. This also introduces
    CONFIG_HAVE_DYNAMIC_FTRACE_WITH_REGS which indicates
    the architecture depending part of ftrace has a code
    that saves full registers.
    On the other hand, CONFIG_DYNAMIC_FTRACE_WITH_REGS indicates
    the code is enabled.
    
    Link: http://lkml.kernel.org/r/20120928081516.3560.72534.stgit@ltc138.sdl.hitachi.co.jp
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 9a25b522d377..86cb51e1ca96 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -44,7 +44,6 @@
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 #define ARCH_SUPPORTS_FTRACE_OPS 1
-#define ARCH_SUPPORTS_FTRACE_SAVE_REGS
 #endif
 
 #ifndef __ASSEMBLY__

commit d57c5d51a30152f3175d2344cb6395f08bf8ee0c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Feb 9 13:32:18 2011 -0500

    ftrace/x86: Add support for -mfentry to x86_64
    
    If the kernel is compiled with gcc 4.6.0 which supports -mfentry,
    then use that instead of mcount.
    
    With mcount, frame pointers are forced with the -pg option and we
    get something like:
    
    <can_vma_merge_before>:
           55                      push   %rbp
           48 89 e5                mov    %rsp,%rbp
           53                      push   %rbx
           41 51                   push   %r9
           e8 fe 6a 39 00          callq  ffffffff81483d00 <mcount>
           31 c0                   xor    %eax,%eax
           48 89 fb                mov    %rdi,%rbx
           48 89 d7                mov    %rdx,%rdi
           48 33 73 30             xor    0x30(%rbx),%rsi
           48 f7 c6 ff ff ff f7    test   $0xfffffffff7ffffff,%rsi
    
    With -mfentry, frame pointers are no longer forced and the call looks
    like this:
    
    <can_vma_merge_before>:
           e8 33 af 37 00          callq  ffffffff81461b40 <__fentry__>
           53                      push   %rbx
           48 89 fb                mov    %rdi,%rbx
           31 c0                   xor    %eax,%eax
           48 89 d7                mov    %rdx,%rdi
           41 51                   push   %r9
           48 33 73 30             xor    0x30(%rbx),%rsi
           48 f7 c6 ff ff ff f7    test   $0xfffffffff7ffffff,%rsi
    
    This adds the ftrace hook at the beginning of the function before a
    frame is set up, and allows the function callbacks to be able to access
    parameters. As kprobes now can use function tracing (at least on x86)
    this speeds up the kprobe hooks that are at the beginning of the
    function.
    
    Link: http://lkml.kernel.org/r/20120807194100.130477900@goodmis.org
    
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index a6cae0c1720c..9a25b522d377 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -35,7 +35,11 @@
 #endif
 
 #ifdef CONFIG_FUNCTION_TRACER
-#define MCOUNT_ADDR		((long)(mcount))
+#ifdef CC_USING_FENTRY
+# define MCOUNT_ADDR		((long)(__fentry__))
+#else
+# define MCOUNT_ADDR		((long)(mcount))
+#endif
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -46,6 +50,7 @@
 #ifndef __ASSEMBLY__
 extern void mcount(void);
 extern atomic_t modifying_ftrace_code;
+extern void __fentry__(void);
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {

commit 4de72395ff4cf48e23b61986dbc90b99a7c4ed97
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 5 20:00:11 2012 -0400

    ftrace/x86: Add save_regs for i386 function calls
    
    Add saving full regs for function tracing on i386.
    The saving of regs was influenced by patches sent out by
    Masami Hiramatsu.
    
    Link: Link: http://lkml.kernel.org/r/20120711195745.379060003@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index a8475014c4ad..a6cae0c1720c 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -40,10 +40,8 @@
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 #define ARCH_SUPPORTS_FTRACE_OPS 1
-#ifdef CONFIG_X86_64
 #define ARCH_SUPPORTS_FTRACE_SAVE_REGS
 #endif
-#endif
 
 #ifndef __ASSEMBLY__
 extern void mcount(void);

commit 08f6fba503111e0336f2b4d6915a4a18f9b60e51
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Apr 30 16:20:23 2012 -0400

    ftrace/x86: Add separate function to save regs
    
    Add a way to have different functions calling different trampolines.
    If a ftrace_ops wants regs saved on the return, then have only the
    functions with ops registered to save regs. Functions registered by
    other ops would not be affected, unless the functions overlap.
    
    If one ftrace_ops registered functions A, B and C and another ops
    registered fucntions to save regs on A, and D, then only functions
    A and D would be saving regs. Function B and C would work as normal.
    Although A is registered by both ops: normal and saves regs; this is fine
    as saving the regs is needed to satisfy one of the ops that calls it
    but the regs are ignored by the other ops function.
    
    x86_64 implements the full regs saving, and i386 just passes a NULL
    for regs to satisfy the ftrace_ops passing. Where an arch must supply
    both regs and ftrace_ops parameters, even if regs is just NULL.
    
    It is OK for an arch to pass NULL regs. All function trace users that
    require regs passing must add the flag FTRACE_OPS_FL_SAVE_REGS when
    registering the ftrace_ops. If the arch does not support saving regs
    then the ftrace_ops will fail to register. The flag
    FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED may be set that will prevent the
    ftrace_ops from failing to register. In this case, the handler may
    either check if regs is not NULL or check if ARCH_SUPPORTS_FTRACE_SAVE_REGS.
    If the arch supports passing regs it will set this macro and pass regs
    for ops that request them. All other archs will just pass NULL.
    
    Link: Link: http://lkml.kernel.org/r/20120711195745.107705970@goodmis.org
    
    Cc: Alexander van Heukelum <heukelum@fastmail.fm>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index b3bb1f3f2773..a8475014c4ad 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -3,27 +3,33 @@
 
 #ifdef __ASSEMBLY__
 
-	.macro MCOUNT_SAVE_FRAME
-	/* taken from glibc */
-	subq $0x38, %rsp
-	movq %rax, (%rsp)
-	movq %rcx, 8(%rsp)
-	movq %rdx, 16(%rsp)
-	movq %rsi, 24(%rsp)
-	movq %rdi, 32(%rsp)
-	movq %r8, 40(%rsp)
-	movq %r9, 48(%rsp)
+	/* skip is set if the stack was already partially adjusted */
+	.macro MCOUNT_SAVE_FRAME skip=0
+	 /*
+	  * We add enough stack to save all regs.
+	  */
+	subq $(SS+8-\skip), %rsp
+	movq %rax, RAX(%rsp)
+	movq %rcx, RCX(%rsp)
+	movq %rdx, RDX(%rsp)
+	movq %rsi, RSI(%rsp)
+	movq %rdi, RDI(%rsp)
+	movq %r8, R8(%rsp)
+	movq %r9, R9(%rsp)
+	 /* Move RIP to its proper location */
+	movq SS+8(%rsp), %rdx
+	movq %rdx, RIP(%rsp)
 	.endm
 
-	.macro MCOUNT_RESTORE_FRAME
-	movq 48(%rsp), %r9
-	movq 40(%rsp), %r8
-	movq 32(%rsp), %rdi
-	movq 24(%rsp), %rsi
-	movq 16(%rsp), %rdx
-	movq 8(%rsp), %rcx
-	movq (%rsp), %rax
-	addq $0x38, %rsp
+	.macro MCOUNT_RESTORE_FRAME skip=0
+	movq R9(%rsp), %r9
+	movq R8(%rsp), %r8
+	movq RDI(%rsp), %rdi
+	movq RSI(%rsp), %rsi
+	movq RDX(%rsp), %rdx
+	movq RCX(%rsp), %rcx
+	movq RAX(%rsp), %rax
+	addq $(SS+8-\skip), %rsp
 	.endm
 
 #endif
@@ -34,6 +40,9 @@
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 #define ARCH_SUPPORTS_FTRACE_OPS 1
+#ifdef CONFIG_X86_64
+#define ARCH_SUPPORTS_FTRACE_SAVE_REGS
+#endif
 #endif
 
 #ifndef __ASSEMBLY__

commit 28fb5dfa783c25dbeeb25a72663f8066a3a517f5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Aug 10 22:00:55 2011 -0400

    ftrace/x86_32: Push ftrace_ops in as 3rd parameter to function tracer
    
    Add support of passing the current ftrace_ops into the 3rd parameter
    of the callback to the function tracer.
    
    Link: http://lkml.kernel.org/r/20120612225424.942411318@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 783b107eacbc..b3bb1f3f2773 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -32,7 +32,7 @@
 #define MCOUNT_ADDR		((long)(mcount))
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
 
-#if defined(CONFIG_DYNAMIC_FTRACE) && defined(CONFIG_X86_64)
+#ifdef CONFIG_DYNAMIC_FTRACE
 #define ARCH_SUPPORTS_FTRACE_OPS 1
 #endif
 

commit 2f5f6ad9390c1ebbf738d130dbfe80b60eaa167e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 8 16:57:47 2011 -0400

    ftrace: Pass ftrace_ops as third parameter to function trace callback
    
    Currently the function trace callback receives only the ip and parent_ip
    of the function that it traced. It would be more powerful to also return
    the ops that registered the function as well. This allows the same function
    to act differently depending on what ftrace_ops registered it.
    
    Link: http://lkml.kernel.org/r/20120612225424.267254552@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index b0767bc08740..783b107eacbc 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -32,6 +32,10 @@
 #define MCOUNT_ADDR		((long)(mcount))
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
 
+#if defined(CONFIG_DYNAMIC_FTRACE) && defined(CONFIG_X86_64)
+#define ARCH_SUPPORTS_FTRACE_OPS 1
+#endif
+
 #ifndef __ASSEMBLY__
 extern void mcount(void);
 extern atomic_t modifying_ftrace_code;

commit a192cd0413b71c2a3e4e48dd365af704be72b748
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 30 13:26:37 2012 -0400

    ftrace: Synchronize variable setting with breakpoints
    
    When the function tracer starts modifying the code via breakpoints
    it sets a variable (modifying_ftrace_code) to inform the breakpoint
    handler to call the ftrace int3 code.
    
    But there's no synchronization between setting this code and the
    handler, thus it is possible for the handler to be called on another
    CPU before it sees the variable. This will cause a kernel crash as
    the int3 handler will not know what to do with it.
    
    I originally added smp_mb()'s to force the visibility of the variable
    but H. Peter Anvin suggested that I just make it atomic.
    
    [ Added comments as suggested by Peter Zijlstra ]
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 18d9005d9e4f..b0767bc08740 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -34,7 +34,7 @@
 
 #ifndef __ASSEMBLY__
 extern void mcount(void);
-extern int modifying_ftrace_code;
+extern atomic_t modifying_ftrace_code;
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {

commit 08d636b6d4fb80647fe8869ea1cd97b2c26a4751
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 16 09:57:10 2011 -0400

    ftrace/x86: Have arch x86_64 use breakpoints instead of stop machine
    
    This method changes x86 to add a breakpoint to the mcount locations
    instead of calling stop machine.
    
    Now that iret can be handled by NMIs, we perform the following to
    update code:
    
    1) Add a breakpoint to all locations that will be modified
    
    2) Sync all cores
    
    3) Update all locations to be either a nop or call (except breakpoint
       op)
    
    4) Sync all cores
    
    5) Remove the breakpoint with the new code.
    
    6) Sync all cores
    
    [
      Added updates that Masami suggested:
       Use unlikely(modifying_ftrace_code) in int3 trap to keep kprobes efficient.
       Don't use NOTIFY_* in ftrace handler in int3 as it is not a notifier.
    ]
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 268c783ab1c0..18d9005d9e4f 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -34,6 +34,7 @@
 
 #ifndef __ASSEMBLY__
 extern void mcount(void);
+extern int modifying_ftrace_code;
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
@@ -50,6 +51,8 @@ struct dyn_arch_ftrace {
 	/* No extra data needed for x86 */
 };
 
+int ftrace_int3_handler(struct pt_regs *regs);
+
 #endif /*  CONFIG_DYNAMIC_FTRACE */
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_FUNCTION_TRACER */

commit 521ccb5c4aece609311bfa7157910a8f0c942af5
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue May 10 10:10:41 2011 +0200

    ftrace/x86: mcount offset calculation
    
    Do the mcount offset adjustment in the recordmcount.pl/recordmcount.[ch]
    at compile time and not in ftrace_call_adjust at run time.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index db24c2278be0..268c783ab1c0 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -38,11 +38,10 @@ extern void mcount(void);
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
 	/*
-	 * call mcount is "e8 <4 byte offset>"
-	 * The addr points to the 4 byte offset and the caller of this
-	 * function wants the pointer to e8. Simply subtract one.
+	 * addr is the address of the mcount call instruction.
+	 * recordmcount does the necessary offset calculation.
 	 */
-	return addr - 1;
+	return addr;
 }
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 117226d15850387b55fd01675917ee4fcb9699e8
Author: Jason Baron <jbaron@redhat.com>
Date:   Mon Aug 24 17:40:26 2009 -0400

    tracing: Remove FTRACE_SYSCALL_MAX definitions
    
    Remove the FTRACE_SYSCALL_MAX definitions now that we have converted the
    syscall event tracing code to use NR_syscalls.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Josh Stone <jistone@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anwin <hpa@zytor.com>
    Cc: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    LKML-Reference: <f2240cdc8f0b1ca7617390c8f5ec90ba2bd348cf.1251146513.git.jbaron@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 71136545187a..db24c2278be0 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -28,13 +28,6 @@
 
 #endif
 
-/* FIXME: I don't want to stay hardcoded */
-#ifdef CONFIG_X86_64
-# define FTRACE_SYSCALL_MAX     299
-#else
-# define FTRACE_SYSCALL_MAX     337
-#endif
-
 #ifdef CONFIG_FUNCTION_TRACER
 #define MCOUNT_ADDR		((long)(mcount))
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */

commit 9daa77e2e9a6b8b859660d5e24d0f8cd77c2af39
Author: Jason Baron <jbaron@redhat.com>
Date:   Mon Aug 10 16:52:35 2009 -0400

    tracing: Update FTRACE_SYSCALL_MAX
    
    update FTRACE_SYSCALL_MAX to the current number of syscalls
    
    FTRACE_SYSCALL_MAX is a temporary solution to get the number of
    syscalls supported by the arch until we find a more dynamic way
    to get this number.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index bd2c6511c887..71136545187a 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -30,9 +30,9 @@
 
 /* FIXME: I don't want to stay hardcoded */
 #ifdef CONFIG_X86_64
-# define FTRACE_SYSCALL_MAX     296
+# define FTRACE_SYSCALL_MAX     299
 #else
-# define FTRACE_SYSCALL_MAX     333
+# define FTRACE_SYSCALL_MAX     337
 #endif
 
 #ifdef CONFIG_FUNCTION_TRACER

commit f58ba100678f421bdcb000a3c71793f432dfab93
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 13 15:42:12 2009 +0100

    tracing/syscalls: support for syscalls tracing on x86
    
    Extend x86 architecture syscall tracing support with syscall
    metadata table details.
    
    (The upcoming core syscall tracing modifications rely on this.)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236955332-10133-3-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index db24c2278be0..bd2c6511c887 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -28,6 +28,13 @@
 
 #endif
 
+/* FIXME: I don't want to stay hardcoded */
+#ifdef CONFIG_X86_64
+# define FTRACE_SYSCALL_MAX     296
+#else
+# define FTRACE_SYSCALL_MAX     333
+#endif
+
 #ifdef CONFIG_FUNCTION_TRACER
 #define MCOUNT_ADDR		((long)(mcount))
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */

commit 712406a6bf59ebf4a00358bb59a4a2a1b2953d90
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Feb 9 10:54:03 2009 -0800

    tracing/function-graph-tracer: make arch generic push pop functions
    
    There is nothing really arch specific of the push and pop functions
    used by the function graph tracer. This patch moves them to generic
    code.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index b55b4a7fbefd..db24c2278be0 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -55,29 +55,4 @@ struct dyn_arch_ftrace {
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_FUNCTION_TRACER */
 
-#ifdef CONFIG_FUNCTION_GRAPH_TRACER
-
-#ifndef __ASSEMBLY__
-
-/*
- * Stack of return addresses for functions
- * of a thread.
- * Used in struct thread_info
- */
-struct ftrace_ret_stack {
-	unsigned long ret;
-	unsigned long func;
-	unsigned long long calltime;
-};
-
-/*
- * Primary handler of a function return.
- * It relays on ftrace_return_to_handler.
- * Defined in entry_32/64.S
- */
-extern void return_to_handler(void);
-
-#endif /* __ASSEMBLY__ */
-#endif /* CONFIG_FUNCTION_GRAPH_TRACER */
-
 #endif /* _ASM_X86_FTRACE_H */

commit d680fe44775ed17a80035462d9898f5e77bfd7dd
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Sat Dec 13 00:09:08 2008 +0300

    x86: entry_64 - introduce FTRACE_ frame macro v2
    
    Impact: clean up
    
    Itroduce MCOUNT_SAVE/RESTORE_FRAME which allow us to
    save a number of lines on source level.
    
    Also fix a comment in ftrace.h.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 7e61b4ceb9a4..b55b4a7fbefd 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -1,6 +1,33 @@
 #ifndef _ASM_X86_FTRACE_H
 #define _ASM_X86_FTRACE_H
 
+#ifdef __ASSEMBLY__
+
+	.macro MCOUNT_SAVE_FRAME
+	/* taken from glibc */
+	subq $0x38, %rsp
+	movq %rax, (%rsp)
+	movq %rcx, 8(%rsp)
+	movq %rdx, 16(%rsp)
+	movq %rsi, 24(%rsp)
+	movq %rdi, 32(%rsp)
+	movq %r8, 40(%rsp)
+	movq %r9, 48(%rsp)
+	.endm
+
+	.macro MCOUNT_RESTORE_FRAME
+	movq 48(%rsp), %r9
+	movq 40(%rsp), %r8
+	movq 32(%rsp), %rdi
+	movq 24(%rsp), %rsi
+	movq 16(%rsp), %rdx
+	movq 8(%rsp), %rcx
+	movq (%rsp), %rax
+	addq $0x38, %rsp
+	.endm
+
+#endif
+
 #ifdef CONFIG_FUNCTION_TRACER
 #define MCOUNT_ADDR		((long)(mcount))
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
@@ -46,7 +73,7 @@ struct ftrace_ret_stack {
 /*
  * Primary handler of a function return.
  * It relays on ftrace_return_to_handler.
- * Defined in entry32.S
+ * Defined in entry_32/64.S
  */
 extern void return_to_handler(void);
 

commit fb52607afcd0629776f1dc9e657647ceae81dd50
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 25 21:07:04 2008 +0100

    tracing/function-return-tracer: change the name into function-graph-tracer
    
    Impact: cleanup
    
    This patch changes the name of the "return function tracer" into
    function-graph-tracer which is a more suitable name for a tracing
    which makes one able to retrieve the ordered call stack during
    the code flow.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 754a3e082f94..7e61b4ceb9a4 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -28,7 +28,7 @@ struct dyn_arch_ftrace {
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_FUNCTION_TRACER */
 
-#ifdef CONFIG_FUNCTION_RET_TRACER
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
 #ifndef __ASSEMBLY__
 
@@ -51,6 +51,6 @@ struct ftrace_ret_stack {
 extern void return_to_handler(void);
 
 #endif /* __ASSEMBLY__ */
-#endif /* CONFIG_FUNCTION_RET_TRACER */
+#endif /* CONFIG_FUNCTION_GRAPH_TRACER */
 
 #endif /* _ASM_X86_FTRACE_H */

commit f201ae2356c74bcae130b2177b3dca903ea98071
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 06:22:56 2008 +0100

    tracing/function-return-tracer: store return stack into task_struct and allocate it dynamically
    
    Impact: use deeper function tracing depth safely
    
    Some tests showed that function return tracing needed a more deeper depth
    of function calls. But it could be unsafe to store these return addresses
    to the stack.
    
    So these arrays will now be allocated dynamically into task_struct of current
    only when the tracer is activated.
    
    Typical scheme when tracer is activated:
    - allocate a return stack for each task in global list.
    - fork: allocate the return stack for the newly created task
    - exit: free return stack of current
    - idle init: same as fork
    
    I chose a default depth of 50. I don't have overruns anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 2bb43b433e07..754a3e082f94 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -29,7 +29,6 @@ struct dyn_arch_ftrace {
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #ifdef CONFIG_FUNCTION_RET_TRACER
-#define FTRACE_RET_STACK_SIZE 20
 
 #ifndef __ASSEMBLY__
 

commit 31e889098a80ceb3e9e3c555d522b2686a6663c6
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 14 16:21:19 2008 -0800

    ftrace: pass module struct to arch dynamic ftrace functions
    
    Impact: allow archs more flexibility on dynamic ftrace implementations
    
    Dynamic ftrace has largly been developed on x86. Since x86 does not
    have the same limitations as other architectures, the ftrace interaction
    between the generic code and the architecture specific code was not
    flexible enough to handle some of the issues that other architectures
    have.
    
    Most notably, module trampolines. Due to the limited branch distance
    that archs make in calling kernel core code from modules, the module
    load code must create a trampoline to jump to what will make the
    larger jump into core kernel code.
    
    The problem arises when this happens to a call to mcount. Ftrace checks
    all code before modifying it and makes sure the current code is what
    it expects. Right now, there is not enough information to handle modifying
    module trampolines.
    
    This patch changes the API between generic dynamic ftrace code and
    the arch dependent code. There is now two functions for modifying code:
    
      ftrace_make_nop(mod, rec, addr) - convert the code at rec->ip into
           a nop, where the original text is calling addr. (mod is the
           module struct if called by module init)
    
      ftrace_make_caller(rec, addr) - convert the code rec->ip that should
           be a nop into a caller to addr.
    
    The record "rec" now has a new field called "arch" where the architecture
    can add any special attributes to each call site record.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 9b6a1fa19e70..2bb43b433e07 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -17,6 +17,14 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 	 */
 	return addr - 1;
 }
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+
+struct dyn_arch_ftrace {
+	/* No extra data needed for x86 */
+};
+
+#endif /*  CONFIG_DYNAMIC_FTRACE */
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_FUNCTION_TRACER */
 

commit caf4b323b02a16c92fba449952ac6515ddc76d7a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 07:03:45 2008 +0100

    tracing, x86: add low level support for ftrace return tracing
    
    Impact: add infrastructure for function-return tracing
    
    Add low level support for ftrace return tracing.
    
    This plug-in stores return addresses on the thread_info structure of
    the current task.
    
    The index of the current return address is initialized when the task
    is the first one (init) and when a process forks (the child). It is
    not needed when a task does a sys_execve because after this syscall,
    it still needs to return on the kernel functions it called.
    
    Note that the code of return_to_handler has been suggested by Steven
    Rostedt as almost all of the ideas of improvements in this V3.
    
    For purpose of security, arch/x86/kernel/process_32.c is not traced
    because __switch_to() changes the current task during its execution.
    That could cause inconsistency in the stored return address of this
    function even if I didn't have any crash after testing with tracing on
    this function enabled.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index f8173ed1c970..9b6a1fa19e70 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -20,4 +20,30 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_FUNCTION_TRACER */
 
+#ifdef CONFIG_FUNCTION_RET_TRACER
+#define FTRACE_RET_STACK_SIZE 20
+
+#ifndef __ASSEMBLY__
+
+/*
+ * Stack of return addresses for functions
+ * of a thread.
+ * Used in struct thread_info
+ */
+struct ftrace_ret_stack {
+	unsigned long ret;
+	unsigned long func;
+	unsigned long long calltime;
+};
+
+/*
+ * Primary handler of a function return.
+ * It relays on ftrace_return_to_handler.
+ * Defined in entry32.S
+ */
+extern void return_to_handler(void);
+
+#endif /* __ASSEMBLY__ */
+#endif /* CONFIG_FUNCTION_RET_TRACER */
+
 #endif /* _ASM_X86_FTRACE_H */

commit 7e5e26a3d8ac4bcadb380073dc9604c07a9a6198
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Oct 31 09:36:38 2008 -0400

    ftrace: fix hardirq header for non ftrace archs
    
    Impact: build fix for non-ftrace architectures
    
    Not all archs implement ftrace, and therefore do not have an asm/ftrace.h.
    This patch corrects the problem.
    
    The ftrace_nmi_enter/exit now must be defined for all archs that implement
    dynamic ftrace. Currently, only x86 does.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index a23468194b8c..f8173ed1c970 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -17,23 +17,7 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 	 */
 	return addr - 1;
 }
-
-#ifdef CONFIG_DYNAMIC_FTRACE
-extern void ftrace_nmi_enter(void);
-extern void ftrace_nmi_exit(void);
-#else
-static inline void ftrace_nmi_enter(void) { }
-static inline void ftrace_nmi_exit(void) { }
-#endif
 #endif /* __ASSEMBLY__ */
-
-#else /* CONFIG_FUNCTION_TRACER */
-
-#ifndef __ASSEMBLY__
-static inline void ftrace_nmi_enter(void) { }
-static inline void ftrace_nmi_exit(void) { }
-#endif
-
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #endif /* _ASM_X86_FTRACE_H */

commit a26a2a27396c0a0877aa701f8f92d08ba550a6c9
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Oct 31 00:03:22 2008 -0400

    ftrace: nmi safe code clean ups
    
    Impact: cleanup
    
    This patch cleans up the NMI safe code for dynamic ftrace as suggested
    by Andrew Morton.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index f2ed6b704a75..a23468194b8c 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -22,16 +22,16 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 extern void ftrace_nmi_enter(void);
 extern void ftrace_nmi_exit(void);
 #else
-#define ftrace_nmi_enter()	do { } while (0)
-#define ftrace_nmi_exit()	do { } while (0)
-#endif
+static inline void ftrace_nmi_enter(void) { }
+static inline void ftrace_nmi_exit(void) { }
 #endif
+#endif /* __ASSEMBLY__ */
 
 #else /* CONFIG_FUNCTION_TRACER */
 
 #ifndef __ASSEMBLY__
-#define ftrace_nmi_enter()	do { } while (0)
-#define ftrace_nmi_exit()	do { } while (0)
+static inline void ftrace_nmi_enter(void) { }
+static inline void ftrace_nmi_exit(void) { }
 #endif
 
 #endif /* CONFIG_FUNCTION_TRACER */

commit 17666f02b118099028522dfc3df00a235700e216
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 30 16:08:32 2008 -0400

    ftrace: nmi safe code modification
    
    Impact: fix crashes that can occur in NMI handlers, if their code is modified
    
    Modifying code is something that needs special care. On SMP boxes,
    if code that is being modified is also being executed on another CPU,
    that CPU will have undefined results.
    
    The dynamic ftrace uses kstop_machine to make the system act like a
    uniprocessor system. But this does not address NMIs, that can still
    run on other CPUs.
    
    One approach to handle this is to make all code that are used by NMIs
    not be traced. But NMIs can call notifiers that spread throughout the
    kernel and this will be very hard to maintain, and the chance of missing
    a function is very high.
    
    The approach that this patch takes is to have the NMIs modify the code
    if the modification is taking place. The way this works is that just
    writing to code executing on another CPU is not harmful if what is
    written is the same as what exists.
    
    Two buffers are used: an IP buffer and a "code" buffer.
    
    The steps that the patcher takes are:
    
     1) Put in the instruction pointer into the IP buffer
        and the new code into the "code" buffer.
     2) Set a flag that says we are modifying code
     3) Wait for any running NMIs to finish.
     4) Write the code
     5) clear the flag.
     6) Wait for any running NMIs to finish.
    
    If an NMI is executed, it will also write the pending code.
    Multiple writes are OK, because what is being written is the same.
    Then the patcher must wait for all running NMIs to finish before
    going to the next line that must be patched.
    
    This is basically the RCU approach to code modification.
    
    Thanks to Ingo Molnar for suggesting the idea, and to Arjan van de Ven
    for his guidence on what is safe and what is not.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 9e8bc29b8b17..f2ed6b704a75 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -17,6 +17,21 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 	 */
 	return addr - 1;
 }
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+extern void ftrace_nmi_enter(void);
+extern void ftrace_nmi_exit(void);
+#else
+#define ftrace_nmi_enter()	do { } while (0)
+#define ftrace_nmi_exit()	do { } while (0)
+#endif
+#endif
+
+#else /* CONFIG_FUNCTION_TRACER */
+
+#ifndef __ASSEMBLY__
+#define ftrace_nmi_enter()	do { } while (0)
+#define ftrace_nmi_exit()	do { } while (0)
 #endif
 
 #endif /* CONFIG_FUNCTION_TRACER */

commit 4944dd62de21230af039eda7cd218e9a09021d11
Merge: f17845e5d97e 0173a3265b22
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 27 10:50:54 2008 +0100

    Merge commit 'v2.6.28-rc2' into tracing/urgent

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
index 1bb6f9bbe1ab..47f7e65e6c1d 100644
--- a/arch/x86/include/asm/ftrace.h
+++ b/arch/x86/include/asm/ftrace.h
@@ -1,5 +1,5 @@
-#ifndef ASM_X86__FTRACE_H
-#define ASM_X86__FTRACE_H
+#ifndef _ASM_X86_FTRACE_H
+#define _ASM_X86_FTRACE_H
 
 #ifdef CONFIG_FTRACE
 #define MCOUNT_ADDR		((long)(mcount))
@@ -21,4 +21,4 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 
 #endif /* CONFIG_FTRACE */
 
-#endif /* ASM_X86__FTRACE_H */
+#endif /* _ASM_X86_FTRACE_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h
new file mode 100644
index 000000000000..1bb6f9bbe1ab
--- /dev/null
+++ b/arch/x86/include/asm/ftrace.h
@@ -0,0 +1,24 @@
+#ifndef ASM_X86__FTRACE_H
+#define ASM_X86__FTRACE_H
+
+#ifdef CONFIG_FTRACE
+#define MCOUNT_ADDR		((long)(mcount))
+#define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
+
+#ifndef __ASSEMBLY__
+extern void mcount(void);
+
+static inline unsigned long ftrace_call_adjust(unsigned long addr)
+{
+	/*
+	 * call mcount is "e8 <4 byte offset>"
+	 * The addr points to the 4 byte offset and the caller of this
+	 * function wants the pointer to e8. Simply subtract one.
+	 */
+	return addr - 1;
+}
+#endif
+
+#endif /* CONFIG_FTRACE */
+
+#endif /* ASM_X86__FTRACE_H */
