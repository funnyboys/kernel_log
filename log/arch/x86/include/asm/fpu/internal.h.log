commit 7ad816762f9bf89e940e618ea40c43138b479e10
Author: Petteri Aimonen <jpa@git.mail.kapsi.fi>
Date:   Tue Jun 16 11:12:57 2020 +0200

    x86/fpu: Reset MXCSR to default in kernel_fpu_begin()
    
    Previously, kernel floating point code would run with the MXCSR control
    register value last set by userland code by the thread that was active
    on the CPU core just before kernel call. This could affect calculation
    results if rounding mode was changed, or a crash if a FPU/SIMD exception
    was unmasked.
    
    Restore MXCSR to the kernel's default value.
    
     [ bp: Carve out from a bigger patch by Petteri, add feature check, add
       FNINIT call too (amluto). ]
    
    Signed-off-by: Petteri Aimonen <jpa@git.mail.kapsi.fi>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=207979
    Link: https://lkml.kernel.org/r/20200624114646.28953-2-bp@alien8.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 42159f45bf9c..845e7481ab77 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -623,6 +623,11 @@ static inline void switch_fpu_finish(struct fpu *new_fpu)
  * MXCSR and XCR definitions:
  */
 
+static inline void ldmxcsr(u32 mxcsr)
+{
+	asm volatile("ldmxcsr %0" :: "m" (mxcsr));
+}
+
 extern unsigned int mxcsr_feature_mask;
 
 #define XCR_XFEATURE_ENABLED_MASK	0x00000000

commit c95473e175dd1234b7440daa6eb2670ebf529653
Author: Yu-cheng Yu <yu-cheng.yu@intel.com>
Date:   Tue May 12 07:54:41 2020 -0700

    x86/fpu/xstate: Update copy_kernel_to_xregs_err() for supervisor states
    
    The function copy_kernel_to_xregs_err() uses XRSTOR which can work with
    standard or compacted format without supervisor xstates. However, when
    supervisor xstates are present, XRSTORS must be used. Fix it by using
    XRSTORS when supervisor state handling is enabled.
    
    I also considered if there were additional cases where XRSTOR might be
    mistakenly called instead of XRSTORS.  There are only three XRSTOR sites
    in the kernel:
    
    1. copy_kernel_to_xregs_booting(), already switches between XRSTOR and
       XRSTORS based on X86_FEATURE_XSAVES.
    
    2. copy_user_to_xregs(), which *needs* XRSTOR because it is copying from
       userspace and must never copy supervisor state with XRSTORS.
    
    3. copy_kernel_to_xregs_err() mistakenly used XRSTOR only.  Fix it.
    
     [ bp: Massage commit message. ]
    
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: https://lkml.kernel.org/r/20200512145444.15483-8-yu-cheng.yu@intel.com

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a42fcb4b690d..42159f45bf9c 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -400,7 +400,10 @@ static inline int copy_kernel_to_xregs_err(struct xregs_state *xstate, u64 mask)
 	u32 hmask = mask >> 32;
 	int err;
 
-	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+	if (static_cpu_has(X86_FEATURE_XSAVES))
+		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
+	else
+		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
 
 	return err;
 }

commit b860eb8dce5906b14e3a7f3c771e0b3d6ef61b94
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue May 12 07:54:39 2020 -0700

    x86/fpu/xstate: Define new functions for clearing fpregs and xstates
    
    Currently, fpu__clear() clears all fpregs and xstates.  Once XSAVES
    supervisor states are introduced, supervisor settings (e.g. CET xstates)
    must remain active for signals; It is necessary to have separate functions:
    
    - Create fpu__clear_user_states(): clear only user settings for signals;
    - Create fpu__clear_all(): clear both user and supervisor settings in
       flush_thread().
    
    Also modify copy_init_fpstate_to_fpregs() to take a mask from above two
    functions.
    
    Remove obvious side-comment in fpu__clear(), while at it.
    
     [ bp: Make the second argument of fpu__clear() bool after requesting it
       a bunch of times during review.
      - Add a comment about copy_init_fpstate_to_fpregs() locking needs. ]
    
    Co-developed-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/20200512145444.15483-6-yu-cheng.yu@intel.com

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index ccb1bb32ad7d..a42fcb4b690d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -31,7 +31,8 @@ extern void fpu__save(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
 extern void fpu__drop(struct fpu *fpu);
 extern int  fpu__copy(struct task_struct *dst, struct task_struct *src);
-extern void fpu__clear(struct fpu *fpu);
+extern void fpu__clear_user_states(struct fpu *fpu);
+extern void fpu__clear_all(struct fpu *fpu);
 extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
 extern int  dump_fpu(struct pt_regs *ptregs, struct user_i387_struct *fpstate);
 

commit 524bb73bc15c56f5587e33c817e103a259b019d2
Author: Yu-cheng Yu <yu-cheng.yu@intel.com>
Date:   Tue May 12 07:54:37 2020 -0700

    x86/fpu/xstate: Separate user and supervisor xfeatures mask
    
    Before the introduction of XSAVES supervisor states, 'xfeatures_mask' is
    used at various places to determine XSAVE buffer components and XCR0 bits.
    It contains only user xstates.  To support supervisor xstates, it is
    necessary to separate user and supervisor xstates:
    
    - First, change 'xfeatures_mask' to 'xfeatures_mask_all', which represents
      the full set of bits that should ever be set in a kernel XSAVE buffer.
    - Introduce xfeatures_mask_supervisor() and xfeatures_mask_user() to
      extract relevant xfeatures from xfeatures_mask_all.
    
    Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/20200512145444.15483-4-yu-cheng.yu@intel.com

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 44c48e34d799..ccb1bb32ad7d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -92,7 +92,7 @@ static inline void fpstate_init_xstate(struct xregs_state *xsave)
 	 * XRSTORS requires these bits set in xcomp_bv, or it will
 	 * trigger #GP:
 	 */
-	xsave->header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT | xfeatures_mask;
+	xsave->header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT | xfeatures_mask_all;
 }
 
 static inline void fpstate_init_fxstate(struct fxregs_state *fx)

commit 59c4bd853abcea95eccc167a7d7fd5f1a5f47b98
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Nov 28 09:53:06 2019 +0100

    x86/fpu: Don't cache access to fpu_fpregs_owner_ctx
    
    The state/owner of the FPU is saved to fpu_fpregs_owner_ctx by pointing
    to the context that is currently loaded. It never changed during the
    lifetime of a task - it remained stable/constant.
    
    After deferred FPU registers loading until return to userland was
    implemented, the content of fpu_fpregs_owner_ctx may change during
    preemption and must not be cached.
    
    This went unnoticed for some time and was now noticed, in particular
    since gcc 9 is caching that load in copy_fpstate_to_sigframe() and
    reusing it in the retry loop:
    
      copy_fpstate_to_sigframe()
        load fpu_fpregs_owner_ctx and save on stack
        fpregs_lock()
        copy_fpregs_to_sigframe() /* failed */
        fpregs_unlock()
             *** PREEMPTION, another uses FPU, changes fpu_fpregs_owner_ctx ***
    
        fault_in_pages_writeable() /* succeed, retry */
    
        fpregs_lock()
            __fpregs_load_activate()
              fpregs_state_valid() /* uses fpu_fpregs_owner_ctx from stack */
        copy_fpregs_to_sigframe() /* succeeds, random FPU content */
    
    This is a comparison of the assembly produced by gcc 9, without vs with this
    patch:
    
    | # arch/x86/kernel/fpu/signal.c:173:      if (!access_ok(buf, size))
    |        cmpq    %rdx, %rax      # tmp183, _4
    |        jb      .L190   #,
    |-# arch/x86/include/asm/fpu/internal.h:512:       return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
    |-#APP
    |-# 512 "arch/x86/include/asm/fpu/internal.h" 1
    |-       movq %gs:fpu_fpregs_owner_ctx,%rax      #, pfo_ret__
    |-# 0 "" 2
    |-#NO_APP
    |-       movq    %rax, -88(%rbp) # pfo_ret__, %sfp
    …
    |-# arch/x86/include/asm/fpu/internal.h:512:       return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
    |-       movq    -88(%rbp), %rcx # %sfp, pfo_ret__
    |-       cmpq    %rcx, -64(%rbp) # pfo_ret__, %sfp
    |+# arch/x86/include/asm/fpu/internal.h:512:       return fpu == this_cpu_read(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
    |+#APP
    |+# 512 "arch/x86/include/asm/fpu/internal.h" 1
    |+       movq %gs:fpu_fpregs_owner_ctx(%rip),%rax        # fpu_fpregs_owner_ctx, pfo_ret__
    |+# 0 "" 2
    |+# arch/x86/include/asm/fpu/internal.h:512:       return fpu == this_cpu_read(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
    |+#NO_APP
    |+       cmpq    %rax, -64(%rbp) # pfo_ret__, %sfp
    
    Use this_cpu_read() instead this_cpu_read_stable() to avoid caching of
    fpu_fpregs_owner_ctx during preemption points.
    
    The Fixes: tag points to the commit where deferred FPU loading was
    added. Since this commit, the compiler is no longer allowed to move the
    load of fpu_fpregs_owner_ctx somewhere else / outside of the locked
    section. A task preemption will change its value and stale content will
    be observed.
    
     [ bp: Massage. ]
    
    Debugged-by: Austin Clements <austin@google.com>
    Debugged-by: David Chase <drchase@golang.org>
    Debugged-by: Ian Lance Taylor <ian@airs.com>
    Fixes: 5f409e20b7945 ("x86/fpu: Defer FPU state load until return to userspace")
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Tested-by: Borislav Petkov <bp@suse.de>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Austin Clements <austin@google.com>
    Cc: Barret Rhoden <brho@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: David Chase <drchase@golang.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: ian@airs.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Bleecher Snyder <josharian@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20191128085306.hxfa2o3knqtu4wfn@linutronix.de
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=205663

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 4c95c365058a..44c48e34d799 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -509,7 +509,7 @@ static inline void __fpu_invalidate_fpregs_state(struct fpu *fpu)
 
 static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)
 {
-	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
+	return fpu == this_cpu_read(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
 }
 
 /*

commit 8d3289f2fa1e0c7e2f72c7352f1efb75d2ad7c76
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jun 4 19:54:12 2019 +0200

    x86/fpu: Don't use current->mm to check for a kthread
    
    current->mm can be non-NULL if a kthread calls use_mm(). Check for
    PF_KTHREAD instead to decide when to store user mode FP state.
    
    Fixes: 2722146eb784 ("x86/fpu: Remove fpu->initialized")
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190604175411.GA27477@lst.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 9e27fa05a7ae..4c95c365058a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -536,7 +536,7 @@ static inline void __fpregs_load_activate(void)
 	struct fpu *fpu = &current->thread.fpu;
 	int cpu = smp_processor_id();
 
-	if (WARN_ON_ONCE(current->mm == NULL))
+	if (WARN_ON_ONCE(current->flags & PF_KTHREAD))
 		return;
 
 	if (!fpregs_state_valid(fpu, cpu)) {
@@ -567,11 +567,11 @@ static inline void __fpregs_load_activate(void)
  * otherwise.
  *
  * The FPU context is only stored/restored for a user task and
- * ->mm is used to distinguish between kernel and user threads.
+ * PF_KTHREAD is used to distinguish between kernel and user threads.
  */
 static inline void switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	if (static_cpu_has(X86_FEATURE_FPU) && current->mm) {
+	if (static_cpu_has(X86_FEATURE_FPU) && !(current->flags & PF_KTHREAD)) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
 		else

commit 8ff468c29e9a9c3afe9152c10c7b141343270bf3
Merge: 68253e718c27 d9c9ce34ed5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 10:24:10 2019 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU state handling updates from Borislav Petkov:
     "This contains work started by Rik van Riel and brought to fruition by
      Sebastian Andrzej Siewior with the main goal to optimize when to load
      FPU registers: only when returning to userspace and not on every
      context switch (while the task remains in the kernel).
    
      In addition, this optimization makes kernel_fpu_begin() cheaper by
      requiring registers saving only on the first invocation and skipping
      that in following ones.
    
      What is more, this series cleans up and streamlines many aspects of
      the already complex FPU code, hopefully making it more palatable for
      future improvements and simplifications.
    
      Finally, there's a __user annotations fix from Jann Horn"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (29 commits)
      x86/fpu: Fault-in user stack if copy_fpstate_to_sigframe() fails
      x86/pkeys: Add PKRU value to init_fpstate
      x86/fpu: Restore regs in copy_fpstate_to_sigframe() in order to use the fastpath
      x86/fpu: Add a fastpath to copy_fpstate_to_sigframe()
      x86/fpu: Add a fastpath to __fpu__restore_sig()
      x86/fpu: Defer FPU state load until return to userspace
      x86/fpu: Merge the two code paths in __fpu__restore_sig()
      x86/fpu: Restore from kernel memory on the 64-bit path too
      x86/fpu: Inline copy_user_to_fpregs_zeroing()
      x86/fpu: Update xstate's PKRU value on write_pkru()
      x86/fpu: Prepare copy_fpstate_to_sigframe() for TIF_NEED_FPU_LOAD
      x86/fpu: Always store the registers in copy_fpstate_to_sigframe()
      x86/entry: Add TIF_NEED_FPU_LOAD
      x86/fpu: Eager switch PKRU state
      x86/pkeys: Don't check if PKRU is zero before writing it
      x86/fpu: Only write PKRU if it is different from current
      x86/pkeys: Provide *pkru() helpers
      x86/fpu: Use a feature number instead of mask in two more helpers
      x86/fpu: Make __raw_xsave_addr() use a feature number instead of mask
      x86/fpu: Add an __fpregs_load_activate() internal helper
      ...

commit 5f409e20b794565e2d60ad333e79334630a6c798
Author: Rik van Riel <riel@surriel.com>
Date:   Wed Apr 3 18:41:52 2019 +0200

    x86/fpu: Defer FPU state load until return to userspace
    
    Defer loading of FPU state until return to userspace. This gives
    the kernel the potential to skip loading FPU state for tasks that
    stay in kernel mode, or for tasks that end up with repeated
    invocations of kernel_fpu_begin() & kernel_fpu_end().
    
    The fpregs_lock/unlock() section ensures that the registers remain
    unchanged. Otherwise a context switch or a bottom half could save the
    registers to its FPU context and the processor's FPU registers would
    became random if modified at the same time.
    
    KVM swaps the host/guest registers on entry/exit path. This flow has
    been kept as is. First it ensures that the registers are loaded and then
    saves the current (host) state before it loads the guest's registers. The
    swap is done at the very end with disabled interrupts so it should not
    change anymore before theg guest is entered. The read/save version seems
    to be cheaper compared to memcpy() in a micro benchmark.
    
    Each thread gets TIF_NEED_FPU_LOAD set as part of fork() / fpu__copy().
    For kernel threads, this flag gets never cleared which avoids saving /
    restoring the FPU state for kernel threads and during in-kernel usage of
    the FPU registers.
    
     [
       bp: Correct and update commit message and fix checkpatch warnings.
       s/register/registers/ where it is used in plural.
       minor comment corrections.
       remove unused trace_x86_fpu_activate_state() TP.
     ]
    
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Link: https://lkml.kernel.org/r/20190403164156.19645-24-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 2cf04fbcba5d..0c8a5093647a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -30,7 +30,7 @@ extern void fpu__prepare_write(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
 extern void fpu__drop(struct fpu *fpu);
-extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
+extern int  fpu__copy(struct task_struct *dst, struct task_struct *src);
 extern void fpu__clear(struct fpu *fpu);
 extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
 extern int  dump_fpu(struct pt_regs *ptregs, struct user_i387_struct *fpstate);
@@ -531,13 +531,20 @@ static inline void fpregs_activate(struct fpu *fpu)
 /*
  * Internal helper, do not use directly. Use switch_fpu_return() instead.
  */
-static inline void __fpregs_load_activate(struct fpu *fpu, int cpu)
+static inline void __fpregs_load_activate(void)
 {
+	struct fpu *fpu = &current->thread.fpu;
+	int cpu = smp_processor_id();
+
+	if (WARN_ON_ONCE(current->mm == NULL))
+		return;
+
 	if (!fpregs_state_valid(fpu, cpu)) {
-		if (current->mm)
-			copy_kernel_to_fpregs(&fpu->state);
+		copy_kernel_to_fpregs(&fpu->state);
 		fpregs_activate(fpu);
+		fpu->last_cpu = cpu;
 	}
+	clear_thread_flag(TIF_NEED_FPU_LOAD);
 }
 
 /*
@@ -548,8 +555,8 @@ static inline void __fpregs_load_activate(struct fpu *fpu, int cpu)
  *  - switch_fpu_prepare() saves the old state.
  *    This is done within the context of the old process.
  *
- *  - switch_fpu_finish() restores the new state as
- *    necessary.
+ *  - switch_fpu_finish() sets TIF_NEED_FPU_LOAD; the floating point state
+ *    will get loaded on return to userspace, or when the kernel needs it.
  *
  * If TIF_NEED_FPU_LOAD is cleared then the CPU's FPU registers
  * are saved in the current thread's FPU register state.
@@ -581,10 +588,10 @@ switch_fpu_prepare(struct fpu *old_fpu, int cpu)
  */
 
 /*
- * Set up the userspace FPU context for the new task, if the task
- * has used the FPU.
+ * Load PKRU from the FPU context if available. Delay loading of the
+ * complete FPU state until the return to userland.
  */
-static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
+static inline void switch_fpu_finish(struct fpu *new_fpu)
 {
 	u32 pkru_val = init_pkru_value;
 	struct pkru_state *pk;
@@ -592,7 +599,7 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
 	if (!static_cpu_has(X86_FEATURE_FPU))
 		return;
 
-	__fpregs_load_activate(new_fpu, cpu);
+	set_thread_flag(TIF_NEED_FPU_LOAD);
 
 	if (!cpu_feature_enabled(X86_FEATURE_OSPKE))
 		return;

commit 926b21f37b072ae4c117052de45a975c6d468fec
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:50 2019 +0200

    x86/fpu: Restore from kernel memory on the 64-bit path too
    
    The 64-bit case (both 64-bit and 32-bit frames) loads the new state from
    user memory.
    
    However, doing this is not desired if the FPU state is going to be
    restored on return to userland: it would be required to disable
    preemption in order to avoid a context switch which would set
    TIF_NEED_FPU_LOAD. If this happens before the restore operation then the
    loaded registers would become volatile.
    
    Furthermore, disabling preemption while accessing user memory requires
    to disable the pagefault handler. An error during FXRSTOR would then
    mean that either a page fault occurred (and it would have to be retried
    with enabled page fault handler) or a #GP occurred because the xstate is
    bogus (after all, the signal handler can modify it).
    
    In order to avoid that mess, copy the FPU state from userland, validate
    it and then load it. The copy_kernel_…() helpers are basically just
    like the old helpers except that they operate on kernel memory and the
    fault handler just sets the error value and the caller handles it.
    
    copy_user_to_fpregs_zeroing() and its helpers remain and will be used
    later for a fastpath optimisation.
    
     [ bp: Clarify commit message. ]
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-22-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index da75d7b3e37d..2cf04fbcba5d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -121,6 +121,21 @@ extern void fpstate_sanitize_xstate(struct fpu *fpu);
 	err;								\
 })
 
+#define kernel_insn_err(insn, output, input...)				\
+({									\
+	int err;							\
+	asm volatile("1:" #insn "\n\t"					\
+		     "2:\n"						\
+		     ".section .fixup,\"ax\"\n"				\
+		     "3:  movl $-1,%[err]\n"				\
+		     "    jmp  2b\n"					\
+		     ".previous\n"					\
+		     _ASM_EXTABLE(1b, 3b)				\
+		     : [err] "=r" (err), output				\
+		     : "0"(0), input);					\
+	err;								\
+})
+
 #define kernel_insn(insn, output, input...)				\
 	asm volatile("1:" #insn "\n\t"					\
 		     "2:\n"						\
@@ -149,6 +164,14 @@ static inline void copy_kernel_to_fxregs(struct fxregs_state *fx)
 		kernel_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
 }
 
+static inline int copy_kernel_to_fxregs_err(struct fxregs_state *fx)
+{
+	if (IS_ENABLED(CONFIG_X86_32))
+		return kernel_insn_err(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+	else
+		return kernel_insn_err(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
+}
+
 static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
 {
 	if (IS_ENABLED(CONFIG_X86_32))
@@ -162,6 +185,11 @@ static inline void copy_kernel_to_fregs(struct fregs_state *fx)
 	kernel_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 }
 
+static inline int copy_kernel_to_fregs_err(struct fregs_state *fx)
+{
+	return kernel_insn_err(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+}
+
 static inline int copy_user_to_fregs(struct fregs_state __user *fx)
 {
 	return user_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
@@ -361,6 +389,21 @@ static inline int copy_user_to_xregs(struct xregs_state __user *buf, u64 mask)
 	return err;
 }
 
+/*
+ * Restore xstate from kernel space xsave area, return an error code instead of
+ * an exception.
+ */
+static inline int copy_kernel_to_xregs_err(struct xregs_state *xstate, u64 mask)
+{
+	u32 lmask = mask;
+	u32 hmask = mask >> 32;
+	int err;
+
+	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+
+	return err;
+}
+
 /*
  * These must be called with preempt disabled. Returns
  * 'true' if the FPU state is still intact and we can

commit 383c252545edcc708128e2028a2318b05c45ede4
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:45 2019 +0200

    x86/entry: Add TIF_NEED_FPU_LOAD
    
    Add TIF_NEED_FPU_LOAD. This flag is used for loading the FPU registers
    before returning to userland. It must not be set on systems without a
    FPU.
    
    If this flag is cleared, the CPU's FPU registers hold the latest,
    up-to-date content of the current task's (current()) FPU registers.
    The in-memory copy (union fpregs_state) is not valid.
    
    If this flag is set, then all of CPU's FPU registers may hold a random
    value (except for PKRU) and it is required to load the content of the
    FPU registers on return to userland.
    
    Introduce it now as a preparatory change before adding the main feature.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-17-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 6eb4a0b1ad0e..da75d7b3e37d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -508,6 +508,14 @@ static inline void __fpregs_load_activate(struct fpu *fpu, int cpu)
  *  - switch_fpu_finish() restores the new state as
  *    necessary.
  *
+ * If TIF_NEED_FPU_LOAD is cleared then the CPU's FPU registers
+ * are saved in the current thread's FPU register state.
+ *
+ * If TIF_NEED_FPU_LOAD is set then CPU's FPU registers may not
+ * hold current()'s FPU registers. It is required to load the
+ * registers before returning to userland or using the content
+ * otherwise.
+ *
  * The FPU context is only stored/restored for a user task and
  * ->mm is used to distinguish between kernel and user threads.
  */

commit 0cecca9d03c964abbd2b7927d0670eb70db4ebf2
Author: Rik van Riel <riel@surriel.com>
Date:   Wed Apr 3 18:41:44 2019 +0200

    x86/fpu: Eager switch PKRU state
    
    While most of a task's FPU state is only needed in user space, the
    protection keys need to be in place immediately after a context switch.
    
    The reason is that any access to userspace memory while running in
    kernel mode also needs to abide by the memory permissions specified in
    the protection keys.
    
    The "eager switch" is a preparation for loading the FPU state on return
    to userland. Instead of decoupling PKRU state from xstate, update PKRU
    within xstate on write operations by the kernel.
    
    For user tasks the PKRU should be always read from the xsave area and it
    should not change anything because the PKRU value was loaded as part of
    FPU restore.
    
    For kernel threads the default "init_pkru_value" will be written. Before
    this commit, the kernel thread would end up with a random value which it
    inherited from the previous user task.
    
     [ bigeasy: save pkru to xstate, no cache, don't use __raw_xsave_addr() ]
    
     [ bp: update commit message, sort headers properly in asm/fpu/xstate.h ]
    
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-16-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 3e0c2c496f2d..6eb4a0b1ad0e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -14,6 +14,7 @@
 #include <linux/compat.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
+#include <linux/mm.h>
 
 #include <asm/user.h>
 #include <asm/fpu/api.h>
@@ -534,8 +535,27 @@ switch_fpu_prepare(struct fpu *old_fpu, int cpu)
  */
 static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
 {
-	if (static_cpu_has(X86_FEATURE_FPU))
-		__fpregs_load_activate(new_fpu, cpu);
+	u32 pkru_val = init_pkru_value;
+	struct pkru_state *pk;
+
+	if (!static_cpu_has(X86_FEATURE_FPU))
+		return;
+
+	__fpregs_load_activate(new_fpu, cpu);
+
+	if (!cpu_feature_enabled(X86_FEATURE_OSPKE))
+		return;
+
+	/*
+	 * PKRU state is switched eagerly because it needs to be valid before we
+	 * return to userland e.g. for a copy_to_user() operation.
+	 */
+	if (current->mm) {
+		pk = get_xsave_addr(&new_fpu->state.xsave, XFEATURE_PKRU);
+		if (pk)
+			pkru_val = pk->pkru;
+	}
+	__write_pkru(pkru_val);
 }
 
 /*

commit 4ee91519e1dccc175665fe24bb20a47c6053575c
Author: Rik van Riel <riel@surriel.com>
Date:   Wed Apr 3 18:41:38 2019 +0200

    x86/fpu: Add an __fpregs_load_activate() internal helper
    
    Add a helper function that ensures the floating point registers for the
    current task are active. Use with preemption disabled.
    
    While at it, add fpregs_lock/unlock() helpers too, to be used in later
    patches.
    
     [ bp: Add a comment about its intended usage. ]
    
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-10-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 54f70cae2f15..3e0c2c496f2d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -484,6 +484,18 @@ static inline void fpregs_activate(struct fpu *fpu)
 	trace_x86_fpu_regs_activated(fpu);
 }
 
+/*
+ * Internal helper, do not use directly. Use switch_fpu_return() instead.
+ */
+static inline void __fpregs_load_activate(struct fpu *fpu, int cpu)
+{
+	if (!fpregs_state_valid(fpu, cpu)) {
+		if (current->mm)
+			copy_kernel_to_fpregs(&fpu->state);
+		fpregs_activate(fpu);
+	}
+}
+
 /*
  * FPU state switching for scheduling.
  *
@@ -522,14 +534,8 @@ switch_fpu_prepare(struct fpu *old_fpu, int cpu)
  */
 static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
 {
-	if (static_cpu_has(X86_FEATURE_FPU)) {
-		if (!fpregs_state_valid(new_fpu, cpu)) {
-			if (current->mm)
-				copy_kernel_to_fpregs(&new_fpu->state);
-		}
-
-		fpregs_activate(new_fpu);
-	}
+	if (static_cpu_has(X86_FEATURE_FPU))
+		__fpregs_load_activate(new_fpu, cpu);
 }
 
 /*

commit 0169f53e0d97bb675075506810494bd86b8c934e
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:37 2019 +0200

    x86/fpu: Remove user_fpu_begin()
    
    user_fpu_begin() sets fpu_fpregs_owner_ctx to task's fpu struct. This is
    always the case since there is no lazy FPU anymore.
    
    fpu_fpregs_owner_ctx is used during context switch to decide if it needs
    to load the saved registers or if the currently loaded registers are
    valid. It could be skipped during a
    
      taskA -> kernel thread -> taskA
    
    switch because the switch to the kernel thread would not alter the CPU's
    sFPU tate.
    
    Since this field is always updated during context switch and
    never invalidated, setting it manually (in user context) makes no
    difference. A kernel thread with kernel_fpu_begin() block could
    set fpu_fpregs_owner_ctx to NULL but a kernel thread does not use
    user_fpu_begin().
    
    This is a leftover from the lazy-FPU time.
    
    Remove user_fpu_begin(), it does not change fpu_fpregs_owner_ctx's
    content.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-9-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 04042eacc852..54f70cae2f15 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -532,23 +532,6 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
 	}
 }
 
-/*
- * Needs to be preemption-safe.
- *
- * NOTE! user_fpu_begin() must be used only immediately before restoring
- * the save state. It does not do any saving/restoring on its own. In
- * lazy FPU mode, it is just an optimization to avoid a #NM exception,
- * the task can lose the FPU right after preempt_enable().
- */
-static inline void user_fpu_begin(void)
-{
-	struct fpu *fpu = &current->thread.fpu;
-
-	preempt_disable();
-	fpregs_activate(fpu);
-	preempt_enable();
-}
-
 /*
  * MXCSR and XCR definitions:
  */

commit 2722146eb78451b30e4717a267a3a2b44e4ad317
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:36 2019 +0200

    x86/fpu: Remove fpu->initialized
    
    The struct fpu.initialized member is always set to one for user tasks
    and zero for kernel tasks. This avoids saving/restoring the FPU
    registers for kernel threads.
    
    The ->initialized = 0 case for user tasks has been removed in previous
    changes, for instance, by doing an explicit unconditional init at fork()
    time for FPU-less systems which was otherwise delayed until the emulated
    opcode.
    
    The context switch code (switch_fpu_prepare() + switch_fpu_finish())
    can't unconditionally save/restore registers for kernel threads. Not
    only would it slow down the switch but also load a zeroed xcomp_bv for
    XSAVES.
    
    For kernel_fpu_begin() (+end) the situation is similar: EFI with runtime
    services uses this before alternatives_patched is true. Which means that
    this function is used too early and it wasn't the case before.
    
    For those two cases, use current->mm to distinguish between user and
    kernel thread. For kernel_fpu_begin() skip save/restore of the FPU
    registers.
    
    During the context switch into a kernel thread don't do anything. There
    is no reason to save the FPU state of a kernel thread.
    
    The reordering in __switch_to() is important because the current()
    pointer needs to be valid before switch_fpu_finish() is invoked so ->mm
    is seen of the new task instead the old one.
    
    N.B.: fpu__save() doesn't need to check ->mm because it is called by
    user tasks only.
    
     [ bp: Massage. ]
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-8-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 70ecb7c032cb..04042eacc852 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -494,11 +494,14 @@ static inline void fpregs_activate(struct fpu *fpu)
  *
  *  - switch_fpu_finish() restores the new state as
  *    necessary.
+ *
+ * The FPU context is only stored/restored for a user task and
+ * ->mm is used to distinguish between kernel and user threads.
  */
 static inline void
 switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	if (static_cpu_has(X86_FEATURE_FPU) && old_fpu->initialized) {
+	if (static_cpu_has(X86_FEATURE_FPU) && current->mm) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
 		else
@@ -506,8 +509,7 @@ switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 
 		/* But leave fpu_fpregs_owner_ctx! */
 		trace_x86_fpu_regs_deactivated(old_fpu);
-	} else
-		old_fpu->last_cpu = -1;
+	}
 }
 
 /*
@@ -520,12 +522,12 @@ switch_fpu_prepare(struct fpu *old_fpu, int cpu)
  */
 static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
 {
-	bool preload = static_cpu_has(X86_FEATURE_FPU) &&
-		       new_fpu->initialized;
+	if (static_cpu_has(X86_FEATURE_FPU)) {
+		if (!fpregs_state_valid(new_fpu, cpu)) {
+			if (current->mm)
+				copy_kernel_to_fpregs(&new_fpu->state);
+		}
 
-	if (preload) {
-		if (!fpregs_state_valid(new_fpu, cpu))
-			copy_kernel_to_fpregs(&new_fpu->state);
 		fpregs_activate(new_fpu);
 	}
 }

commit 88f5260a3bf9bfb276b5b4aac2e81587e425a1d7
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:33 2019 +0200

    x86/fpu: Always init the state in fpu__clear()
    
    fpu__clear() only initializes the state if the CPU has FPU support.
    This initialisation is also required for FPU-less systems and takes
    place in math_emulate(). Since fpu__initialize() only performs the
    initialization if ->initialized is zero it does not matter that it
    is invoked each time an opcode is emulated. It makes the removal of
    ->initialized easier if the struct is also initialized in the FPU-less
    case at the same time.
    
    Move fpu__initialize() before the FPU feature check so it is also
    performed in the FPU-less case too.
    
     [ bp: Massage a bit. ]
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Bill Metzenthen <billm@melbpc.org.au>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-5-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 75a1d5f712ee..70ecb7c032cb 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -24,7 +24,6 @@
 /*
  * High level FPU state handling functions:
  */
-extern void fpu__initialize(struct fpu *fpu);
 extern void fpu__prepare_read(struct fpu *fpu);
 extern void fpu__prepare_write(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);

commit 6dd677a044e606fd343e31c2108b13d74aec1ca5
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:31 2019 +0200

    x86/fpu: Remove fpu__restore()
    
    There are no users of fpu__restore() so it is time to remove it. The
    comment regarding fpu__restore() and TS bit is stale since commit
    
      b3b0870ef3ffe ("i387: do not preload FPU state at task switch time")
    
    and has no meaning since.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: linux-doc@vger.kernel.org
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-3-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index fb04a3ded7dd..75a1d5f712ee 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -28,7 +28,6 @@ extern void fpu__initialize(struct fpu *fpu);
 extern void fpu__prepare_read(struct fpu *fpu);
 extern void fpu__prepare_write(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
-extern void fpu__restore(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
 extern void fpu__drop(struct fpu *fpu);
 extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);

commit 67e87d43b794a8886b5d075b3e0fdd0c615a595f
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 29 19:52:59 2019 +0100

    x86: Convert some slow-path static_cpu_has() callers to boot_cpu_has()
    
    Using static_cpu_has() is pointless on those paths, convert them to the
    boot_cpu_has() variant.
    
    No functional changes.
    
    Reported-by: Nadav Amit <nadav.amit@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Reviewed-by: Juergen Gross <jgross@suse.com> # for paravirt
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: linux-edac@vger.kernel.org
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: virtualization@lists.linux-foundation.org
    Cc: x86@kernel.org
    Link: https://lkml.kernel.org/r/20190330112022.28888-3-bp@alien8.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index fb04a3ded7dd..745a19d34f23 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -253,7 +253,7 @@ static inline void copy_xregs_to_kernel_booting(struct xregs_state *xstate)
 
 	WARN_ON(system_state != SYSTEM_BOOTING);
 
-	if (static_cpu_has(X86_FEATURE_XSAVES))
+	if (boot_cpu_has(X86_FEATURE_XSAVES))
 		XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
 	else
 		XSTATE_OP(XSAVE, xstate, lmask, hmask, err);
@@ -275,7 +275,7 @@ static inline void copy_kernel_to_xregs_booting(struct xregs_state *xstate)
 
 	WARN_ON(system_state != SYSTEM_BOOTING);
 
-	if (static_cpu_has(X86_FEATURE_XSAVES))
+	if (boot_cpu_has(X86_FEATURE_XSAVES))
 		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
 	else
 		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
@@ -497,8 +497,7 @@ static inline void fpregs_activate(struct fpu *fpu)
  *  - switch_fpu_finish() restores the new state as
  *    necessary.
  */
-static inline void
-switch_fpu_prepare(struct fpu *old_fpu, int cpu)
+static inline void switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
 	if (static_cpu_has(X86_FEATURE_FPU) && old_fpu->initialized) {
 		if (!copy_fpregs_to_fpstate(old_fpu))

commit 2f7726f955572e587d5f50fbe9b2deed5334bd90
Author: Aubrey Li <aubrey.li@intel.com>
Date:   Fri Jan 18 02:38:20 2019 +0800

    x86/fpu: Track AVX-512 usage of tasks
    
    User space tools which do automated task placement need information
    about AVX-512 usage of tasks, because AVX-512 usage could cause core
    turbo frequency drop and impact the running task on the sibling CPU.
    
    The XSAVE hardware structure has bits that indicate when valid state
    is present in registers unique to AVX-512 use.  Use these bits to
    indicate when AVX-512 has been in use and add per-task AVX-512 state
    timestamp tracking to context switch.
    
    Well-written AVX-512 applications are expected to clear the AVX-512
    state when not actively using AVX-512 registers, so the tracking
    mechanism is imprecise and can theoretically miss AVX-512 usage during
    context switch. But it has been measured to be precise enough to be
    useful under real-world workloads like tensorflow and linpack.
    
    If higher precision is required, suggest user space tools to use the
    PMU-based mechanisms in combination.
    
    Signed-off-by: Aubrey Li <aubrey.li@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: aubrey.li@intel.com
    Link: http://lkml.kernel.org/r/20190117183822.31333-1-aubrey.li@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 5d536e3dcc6d..fb04a3ded7dd 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -376,6 +376,13 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 {
 	if (likely(use_xsave())) {
 		copy_xregs_to_kernel(&fpu->state.xsave);
+
+		/*
+		 * AVX512 state is tracked here because its use is
+		 * known to slow the max clock speed of the core.
+		 */
+		if (fpu->state.xsave.header.xfeatures & XFEATURE_MASK_AVX512)
+			fpu->avx512_timestamp = jiffies;
 		return 1;
 	}
 

commit bae54dc4f353653bbd3e3081754adad05da1d4dd
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Jan 18 00:05:40 2019 +0100

    x86/fpu: Get rid of CONFIG_AS_FXSAVEQ
    
    This was a "workaround" to probe for binutils which could generate
    FXSAVEQ, apparently gas with min version 2.16. In the meantime, minimal
    required gas version is 2.20 so all those workarounds for older binutils
    can be dropped.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20190117232408.GH5023@zn.tnic

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index fa2c93cb42a2..5d536e3dcc6d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -137,37 +137,25 @@ static inline int copy_fxregs_to_user(struct fxregs_state __user *fx)
 {
 	if (IS_ENABLED(CONFIG_X86_32))
 		return user_insn(fxsave %[fx], [fx] "=m" (*fx), "m" (*fx));
-	else if (IS_ENABLED(CONFIG_AS_FXSAVEQ))
+	else
 		return user_insn(fxsaveq %[fx], [fx] "=m" (*fx), "m" (*fx));
 
-	/* See comment in copy_fxregs_to_kernel() below. */
-	return user_insn(rex64/fxsave (%[fx]), "=m" (*fx), [fx] "R" (fx));
 }
 
 static inline void copy_kernel_to_fxregs(struct fxregs_state *fx)
 {
-	if (IS_ENABLED(CONFIG_X86_32)) {
+	if (IS_ENABLED(CONFIG_X86_32))
 		kernel_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
-	} else {
-		if (IS_ENABLED(CONFIG_AS_FXSAVEQ)) {
-			kernel_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
-		} else {
-			/* See comment in copy_fxregs_to_kernel() below. */
-			kernel_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx), "m" (*fx));
-		}
-	}
+	else
+		kernel_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
 }
 
 static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
 {
 	if (IS_ENABLED(CONFIG_X86_32))
 		return user_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
-	else if (IS_ENABLED(CONFIG_AS_FXSAVEQ))
+	else
 		return user_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
-
-	/* See comment in copy_fxregs_to_kernel() below. */
-	return user_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx),
-			  "m" (*fx));
 }
 
 static inline void copy_kernel_to_fregs(struct fregs_state *fx)
@@ -184,34 +172,8 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 {
 	if (IS_ENABLED(CONFIG_X86_32))
 		asm volatile( "fxsave %[fx]" : [fx] "=m" (fpu->state.fxsave));
-	else if (IS_ENABLED(CONFIG_AS_FXSAVEQ))
+	else
 		asm volatile("fxsaveq %[fx]" : [fx] "=m" (fpu->state.fxsave));
-	else {
-		/* Using "rex64; fxsave %0" is broken because, if the memory
-		 * operand uses any extended registers for addressing, a second
-		 * REX prefix will be generated (to the assembler, rex64
-		 * followed by semicolon is a separate instruction), and hence
-		 * the 64-bitness is lost.
-		 *
-		 * Using "fxsaveq %0" would be the ideal choice, but is only
-		 * supported starting with gas 2.16.
-		 *
-		 * Using, as a workaround, the properly prefixed form below
-		 * isn't accepted by any binutils version so far released,
-		 * complaining that the same type of prefix is used twice if
-		 * an extended register is needed for addressing (fix submitted
-		 * to mainline 2005-11-21).
-		 *
-		 *  asm volatile("rex64/fxsave %0" : "=m" (fpu->state.fxsave));
-		 *
-		 * This, however, we can work around by forcing the compiler to
-		 * select an addressing mode that doesn't require extended
-		 * registers.
-		 */
-		asm volatile( "rex64/fxsave (%[fx])"
-			     : "=m" (fpu->state.fxsave)
-			     : [fx] "R" (&fpu->state.fxsave));
-	}
 }
 
 /* These macros all use (%edi)/(%rdi) as the single memory argument. */

commit 6637401c35b2f327a35d27f44bda05e327f2f017
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Nov 28 23:20:11 2018 +0100

    x86/fpu: Add might_fault() to user_insn()
    
    Every user of user_insn() passes an user memory pointer to this macro.
    
    Add might_fault() to user_insn() so we can spot users which are using
    this macro in sections where page faulting is not allowed.
    
     [ bp: Space it out to make it more visible. ]
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20181128222035.2996-6-bigeasy@linutronix.de

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 69dcdf195b61..fa2c93cb42a2 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -106,6 +106,9 @@ extern void fpstate_sanitize_xstate(struct fpu *fpu);
 #define user_insn(insn, output, input...)				\
 ({									\
 	int err;							\
+									\
+	might_fault();							\
+									\
 	asm volatile(ASM_STAC "\n"					\
 		     "1:" #insn "\n\t"					\
 		     "2: " ASM_CLAC "\n"				\

commit ac26d1f74cfc19c8dc9d533b5f20e99dbee3d9bd
Author: Jann Horn <jannh@google.com>
Date:   Tue Nov 27 14:32:00 2018 +0100

    x86/fpu: Use the correct exception table macro in the XSTATE_OP wrapper
    
    Commit
    
      75045f77f7a7 ("x86/extable: Introduce _ASM_EXTABLE_UA for uaccess fixups")
    
    incorrectly replaced the fixup entry for XSTATE_OP with a user-#PF-only
    fixup. XRSTOR can also raise #GP if the xstate content is invalid,
    and _ASM_EXTABLE_UA doesn't expect that. Change this fixup back to
    _ASM_EXTABLE so that #GP gets fixed up.
    
    Fixes: 75045f77f7a7 ("x86/extable: Introduce _ASM_EXTABLE_UA for uaccess fixups")
    Reported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Tested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Naveen N. Rao" <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-hardening@lists.openwall.com
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20181126165957.xhsyu2dhyy45mrjo@linutronix.de
    Link: https://lkml.kernel.org/r/20181127133200.38322-1-jannh@google.com

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 5f7290e6e954..69dcdf195b61 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -226,7 +226,7 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 		     "3: movl $-2,%[err]\n\t"				\
 		     "jmp 2b\n\t"					\
 		     ".popsection\n\t"					\
-		     _ASM_EXTABLE_UA(1b, 3b)				\
+		     _ASM_EXTABLE(1b, 3b)				\
 		     : [err] "=r" (err)					\
 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
 		     : "memory")

commit 0200fbdd431519d730b5d399a12840ec832b27cc
Merge: de3fbb2aa802 01a14bda11ad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 13:08:53 2018 +0100

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking and misc x86 updates from Ingo Molnar:
     "Lots of changes in this cycle - in part because locking/core attracted
      a number of related x86 low level work which was easier to handle in a
      single tree:
    
       - Linux Kernel Memory Consistency Model updates (Alan Stern, Paul E.
         McKenney, Andrea Parri)
    
       - lockdep scalability improvements and micro-optimizations (Waiman
         Long)
    
       - rwsem improvements (Waiman Long)
    
       - spinlock micro-optimization (Matthew Wilcox)
    
       - qspinlocks: Provide a liveness guarantee (more fairness) on x86.
         (Peter Zijlstra)
    
       - Add support for relative references in jump tables on arm64, x86
         and s390 to optimize jump labels (Ard Biesheuvel, Heiko Carstens)
    
       - Be a lot less permissive on weird (kernel address) uaccess faults
         on x86: BUG() when uaccess helpers fault on kernel addresses (Jann
         Horn)
    
       - macrofy x86 asm statements to un-confuse the GCC inliner. (Nadav
         Amit)
    
       - ... and a handful of other smaller changes as well"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (57 commits)
      locking/lockdep: Make global debug_locks* variables read-mostly
      locking/lockdep: Fix debug_locks off performance problem
      locking/pvqspinlock: Extend node size when pvqspinlock is configured
      locking/qspinlock_stat: Count instances of nested lock slowpaths
      locking/qspinlock, x86: Provide liveness guarantee
      x86/asm: 'Simplify' GEN_*_RMWcc() macros
      locking/qspinlock: Rework some comments
      locking/qspinlock: Re-order code
      locking/lockdep: Remove duplicated 'lock_class_ops' percpu array
      x86/defconfig: Enable CONFIG_USB_XHCI_HCD=y
      futex: Replace spin_is_locked() with lockdep
      locking/lockdep: Make class->ops a percpu counter and move it under CONFIG_DEBUG_LOCKDEP=y
      x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs
      x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs
      x86/extable: Macrofy inline assembly code to work around GCC inlining bugs
      x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops
      x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs
      x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs
      x86/refcount: Work around GCC inlining bug
      x86/objtool: Use asm macros to work around GCC inlining bugs
      ...

commit 2224d616528194b02424c91c2ee254b3d29942c3
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Oct 16 22:25:25 2018 +0200

    x86/fpu: Fix i486 + no387 boot crash by only saving FPU registers on context switch if there is an FPU
    
    Booting an i486 with "no387 nofxsr" ends with with the following crash:
    
       math_emulate: 0060:c101987d
       Kernel panic - not syncing: Math emulation needed in kernel
    
    on the first context switch in user land.
    
    The reason is that copy_fpregs_to_fpstate() tries FNSAVE which does not work
    as the FPU is turned off.
    
    This bug was introduced in:
    
      f1c8cd0176078 ("x86/fpu: Change fpu->fpregs_active users to fpu->fpstate_active")
    
    Add a check for X86_FEATURE_FPU before trying to save FPU registers (we
    have such a check in switch_fpu_finish() already).
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Fixes: f1c8cd0176078 ("x86/fpu: Change fpu->fpregs_active users to fpu->fpstate_active")
    Link: http://lkml.kernel.org/r/20181016202525.29437-4-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a38bf5a1e37a..69dcdf195b61 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -528,7 +528,7 @@ static inline void fpregs_activate(struct fpu *fpu)
 static inline void
 switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	if (old_fpu->initialized) {
+	if (static_cpu_has(X86_FEATURE_FPU) && old_fpu->initialized) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
 		else

commit 75045f77f7a73e617494d7a1fcf4e9c1849cec39
Author: Jann Horn <jannh@google.com>
Date:   Tue Aug 28 22:14:18 2018 +0200

    x86/extable: Introduce _ASM_EXTABLE_UA for uaccess fixups
    
    Currently, most fixups for attempting to access userspace memory are
    handled using _ASM_EXTABLE, which is also used for various other types of
    fixups (e.g. safe MSR access, IRET failures, and a bunch of other things).
    In order to make it possible to add special safety checks to uaccess fixups
    (in particular, checking whether the fault address is actually in
    userspace), introduce a new exception table handler ex_handler_uaccess()
    and wire it up to all the user access fixups (excluding ones that
    already use _ASM_EXTABLE_EX).
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: kernel-hardening@lists.openwall.com
    Cc: dvyukov@google.com
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: "Naveen N. Rao" <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20180828201421.157735-5-jannh@google.com

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a38bf5a1e37a..068ff7689268 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -226,7 +226,7 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 		     "3: movl $-2,%[err]\n\t"				\
 		     "jmp 2b\n\t"					\
 		     ".popsection\n\t"					\
-		     _ASM_EXTABLE(1b, 3b)				\
+		     _ASM_EXTABLE_UA(1b, 3b)				\
 		     : [err] "=r" (err)					\
 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
 		     : "memory")

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index e3221ffa304e..a38bf5a1e37a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Copyright (C) 1994 Linus Torvalds
  *

commit 369a036de206710ff27a66f9bffe78ef657648c3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 23 13:37:45 2017 +0200

    x86/fpu: Rename fpu__activate_fpstate_read/write() to fpu__prepare_[read|write]()
    
    As per the new nomenclature we don't 'activate' the FPU state
    anymore, we initialize it. So drop the _activate_fpstate name
    from these functions, which were a bit of a mouthful anyway,
    and name them:
    
            fpu__prepare_read()
            fpu__prepare_write()
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 7c980aafb8aa..e3221ffa304e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -24,8 +24,8 @@
  * High level FPU state handling functions:
  */
 extern void fpu__initialize(struct fpu *fpu);
-extern void fpu__activate_fpstate_read(struct fpu *fpu);
-extern void fpu__activate_fpstate_write(struct fpu *fpu);
+extern void fpu__prepare_read(struct fpu *fpu);
+extern void fpu__prepare_write(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);

commit 2ce03d850b9a2f17d55596ecfa86e72b5687a627
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 23 15:00:15 2017 +0200

    x86/fpu: Rename fpu__activate_curr() to fpu__initialize()
    
    Rename this function to better express that it's all about
    initializing the FPU state of a task which goes hand in hand
    with the fpu::initialized field.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170923130016.21448-33-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index b26ae05da18a..7c980aafb8aa 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -23,7 +23,7 @@
 /*
  * High level FPU state handling functions:
  */
-extern void fpu__activate_curr(struct fpu *fpu);
+extern void fpu__initialize(struct fpu *fpu);
 extern void fpu__activate_fpstate_read(struct fpu *fpu);
 extern void fpu__activate_fpstate_write(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);

commit e4a81bfcaae1ebbdc6efe74e8ea563144d90e9a9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Sep 26 09:43:36 2017 +0200

    x86/fpu: Rename fpu::fpstate_active to fpu::initialized
    
    The x86 FPU code used to have a complex state machine where both the FPU
    registers and the FPU state context could be 'active' (or inactive)
    independently of each other - which enabled features like lazy FPU restore.
    
    Much of this complexity is gone in the current code: now we basically can
    have FPU-less tasks (kernel threads) that don't use (and save/restore) FPU
    state at all, plus full FPU users that save/restore directly with no laziness
    whatsoever.
    
    But the fpu::fpstate_active still carries bits of the old complexity - meanwhile
    this flag has become a simple flag that shows whether the FPU context saving
    area in the thread struct is initialized and used, or not.
    
    Rename it to fpu::initialized to express this simplicity in the name as well.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170923130016.21448-30-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 508e4181c4af..b26ae05da18a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -527,7 +527,7 @@ static inline void fpregs_activate(struct fpu *fpu)
 static inline void
 switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	if (old_fpu->fpstate_active) {
+	if (old_fpu->initialized) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
 		else
@@ -550,7 +550,7 @@ switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
 {
 	bool preload = static_cpu_has(X86_FEATURE_FPU) &&
-		       new_fpu->fpstate_active;
+		       new_fpu->initialized;
 
 	if (preload) {
 		if (!fpregs_state_valid(new_fpu, cpu))

commit 685c930d6e58e31e251ec354f9dca3958a4c5040
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 23 15:00:11 2017 +0200

    x86/fpu: Remove fpu__current_fpstate_write_begin/end()
    
    These functions are not used anymore, so remove them.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170923130016.21448-29-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index cf290d424e48..508e4181c4af 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -26,8 +26,6 @@
 extern void fpu__activate_curr(struct fpu *fpu);
 extern void fpu__activate_fpstate_read(struct fpu *fpu);
 extern void fpu__activate_fpstate_write(struct fpu *fpu);
-extern void fpu__current_fpstate_write_begin(void);
-extern void fpu__current_fpstate_write_end(void);
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);

commit d5c8028b4788f62b31fb79a331b3ad3e041fa366
Author: Eric Biggers <ebiggers@google.com>
Date:   Sat Sep 23 15:00:09 2017 +0200

    x86/fpu: Reinitialize FPU registers if restoring FPU state fails
    
    Userspace can change the FPU state of a task using the ptrace() or
    rt_sigreturn() system calls.  Because reserved bits in the FPU state can
    cause the XRSTOR instruction to fail, the kernel has to carefully
    validate that no reserved bits or other invalid values are being set.
    
    Unfortunately, there have been bugs in this validation code.  For
    example, we were not checking that the 'xcomp_bv' field in the
    xstate_header was 0.  As-is, such bugs are exploitable to read the FPU
    registers of other processes on the system.  To do so, an attacker can
    create a task, assign to it an invalid FPU state, then spin in a loop
    and monitor the values of the FPU registers.  Because the task's FPU
    registers are not being restored, sometimes the FPU registers will have
    the values from another process.
    
    This is likely to continue to be a problem in the future because the
    validation done by the CPU instructions like XRSTOR is not immediately
    visible to kernel developers.  Nor will invalid FPU states ever be
    encountered during ordinary use --- they will only be seen during
    fuzzing or exploits.  There can even be reserved bits outside the
    xstate_header which are easy to forget about.  For example, the MXCSR
    register contains reserved bits, which were not validated by the
    KVM_SET_XSAVE ioctl until commit a575813bfe4b ("KVM: x86: Fix load
    damaged SSEx MXCSR register").
    
    Therefore, mitigate this class of vulnerability by restoring the FPU
    registers from init_fpstate if restoring from the task's state fails.
    
    We actually used to do this, but it was (perhaps unwisely) removed by
    commit 9ccc27a5d297 ("x86/fpu: Remove error return values from
    copy_kernel_to_*regs() functions").  This new patch is also a bit
    different.  First, it only clears the registers, not also the bad
    in-memory state; this is simpler and makes it easier to make the
    mitigation cover all callers of __copy_kernel_to_fpregs().  Second, it
    does the register clearing in an exception handler so that no extra
    instructions are added to context switches.  In fact, we *remove*
    instructions, since previously we were always zeroing the register
    containing 'err' even if CONFIG_X86_DEBUG_FPU was disabled.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Kevin Hao <haokexin@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Halcrow <mhalcrow@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Cc: kernel-hardening@lists.openwall.com
    Link: http://lkml.kernel.org/r/20170922174156.16780-4-ebiggers3@gmail.com
    Link: http://lkml.kernel.org/r/20170923130016.21448-27-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 2dca7c65319c..cf290d424e48 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -120,20 +120,11 @@ extern void fpstate_sanitize_xstate(struct fpu *fpu);
 	err;								\
 })
 
-#define check_insn(insn, output, input...)				\
-({									\
-	int err;							\
+#define kernel_insn(insn, output, input...)				\
 	asm volatile("1:" #insn "\n\t"					\
 		     "2:\n"						\
-		     ".section .fixup,\"ax\"\n"				\
-		     "3:  movl $-1,%[err]\n"				\
-		     "    jmp  2b\n"					\
-		     ".previous\n"					\
-		     _ASM_EXTABLE(1b, 3b)				\
-		     : [err] "=r" (err), output				\
-		     : "0"(0), input);					\
-	err;								\
-})
+		     _ASM_EXTABLE_HANDLE(1b, 2b, ex_handler_fprestore)	\
+		     : output : input)
 
 static inline int copy_fregs_to_user(struct fregs_state __user *fx)
 {
@@ -153,20 +144,16 @@ static inline int copy_fxregs_to_user(struct fxregs_state __user *fx)
 
 static inline void copy_kernel_to_fxregs(struct fxregs_state *fx)
 {
-	int err;
-
 	if (IS_ENABLED(CONFIG_X86_32)) {
-		err = check_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+		kernel_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 	} else {
 		if (IS_ENABLED(CONFIG_AS_FXSAVEQ)) {
-			err = check_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
+			kernel_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
 		} else {
 			/* See comment in copy_fxregs_to_kernel() below. */
-			err = check_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx), "m" (*fx));
+			kernel_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx), "m" (*fx));
 		}
 	}
-	/* Copying from a kernel buffer to FPU registers should never fail: */
-	WARN_ON_FPU(err);
 }
 
 static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
@@ -183,9 +170,7 @@ static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
 
 static inline void copy_kernel_to_fregs(struct fregs_state *fx)
 {
-	int err = check_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
-
-	WARN_ON_FPU(err);
+	kernel_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 }
 
 static inline int copy_user_to_fregs(struct fregs_state __user *fx)
@@ -281,18 +266,13 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
  * Use XRSTORS to restore context if it is enabled. XRSTORS supports compact
  * XSAVE area format.
  */
-#define XSTATE_XRESTORE(st, lmask, hmask, err)				\
+#define XSTATE_XRESTORE(st, lmask, hmask)				\
 	asm volatile(ALTERNATIVE(XRSTOR,				\
 				 XRSTORS, X86_FEATURE_XSAVES)		\
 		     "\n"						\
-		     "xor %[err], %[err]\n"				\
 		     "3:\n"						\
-		     ".pushsection .fixup,\"ax\"\n"			\
-		     "4: movl $-2, %[err]\n"				\
-		     "jmp 3b\n"						\
-		     ".popsection\n"					\
-		     _ASM_EXTABLE(661b, 4b)				\
-		     : [err] "=r" (err)					\
+		     _ASM_EXTABLE_HANDLE(661b, 3b, ex_handler_fprestore)\
+		     :							\
 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
 		     : "memory")
 
@@ -336,7 +316,10 @@ static inline void copy_kernel_to_xregs_booting(struct xregs_state *xstate)
 	else
 		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
 
-	/* We should never fault when copying from a kernel buffer: */
+	/*
+	 * We should never fault when copying from a kernel buffer, and the FPU
+	 * state we set at boot time should be valid.
+	 */
 	WARN_ON_FPU(err);
 }
 
@@ -365,12 +348,8 @@ static inline void copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
 {
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
-	int err;
-
-	XSTATE_XRESTORE(xstate, lmask, hmask, err);
 
-	/* We should never fault when copying from a kernel buffer: */
-	WARN_ON_FPU(err);
+	XSTATE_XRESTORE(xstate, lmask, hmask);
 }
 
 /*

commit 03eaec81ac09814817e9f0307d572ffe8365f980
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sat Sep 23 15:00:06 2017 +0200

    x86/fpu: Turn WARN_ON() in context switch into WARN_ON_FPU()
    
    copy_xregs_to_kernel checks if the alternatives have been already
    patched.
    
    This WARN_ON() is always executed in every context switch.
    
    All the other checks in fpu internal.h are WARN_ON_FPU(), but
    this one is plain WARN_ON(). I assume it was forgotten to switch it.
    
    So switch it to WARN_ON_FPU() too to avoid some unnecessary code
    in the context switch, and a potentially expensive cache line miss for the
    global variable.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170329062605.4970-1-andi@firstfloor.org
    Link: http://lkml.kernel.org/r/20170923130016.21448-24-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 629e7abcd6c9..2dca7c65319c 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -350,7 +350,7 @@ static inline void copy_xregs_to_kernel(struct xregs_state *xstate)
 	u32 hmask = mask >> 32;
 	int err;
 
-	WARN_ON(!alternatives_patched);
+	WARN_ON_FPU(!alternatives_patched);
 
 	XSTATE_XSAVE(xstate, lmask, hmask, err);
 

commit 99dc26bda233ee722bbd370bddf20beece3ffb93
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 23 15:00:03 2017 +0200

    x86/fpu: Remove struct fpu::fpregs_active
    
    The previous changes paved the way for the removal of the
    fpu::fpregs_active state flag - we now only have the
    fpu::fpstate_active and fpu::last_cpu fields left.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170923130016.21448-21-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 42a601673c09..629e7abcd6c9 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -526,14 +526,12 @@ static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)
  */
 static inline void fpregs_deactivate(struct fpu *fpu)
 {
-	fpu->fpregs_active = 0;
 	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
 	trace_x86_fpu_regs_deactivated(fpu);
 }
 
 static inline void fpregs_activate(struct fpu *fpu)
 {
-	fpu->fpregs_active = 1;
 	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
 	trace_x86_fpu_regs_activated(fpu);
 }
@@ -552,8 +550,6 @@ static inline void fpregs_activate(struct fpu *fpu)
 static inline void
 switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	WARN_ON_FPU(old_fpu->fpregs_active != old_fpu->fpstate_active);
-
 	if (old_fpu->fpstate_active) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
@@ -561,7 +557,6 @@ switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 			old_fpu->last_cpu = cpu;
 
 		/* But leave fpu_fpregs_owner_ctx! */
-		old_fpu->fpregs_active = 0;
 		trace_x86_fpu_regs_deactivated(old_fpu);
 	} else
 		old_fpu->last_cpu = -1;

commit 6cf4edbe0526db311a28734609da888fdfcb3604
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 23 15:00:02 2017 +0200

    x86/fpu: Decouple fpregs_activate()/fpregs_deactivate() from fpu->fpregs_active
    
    The fpregs_activate()/fpregs_deactivate() are currently called in such a pattern:
    
            if (!fpu->fpregs_active)
                    fpregs_activate(fpu);
    
            ...
    
            if (fpu->fpregs_active)
                    fpregs_deactivate(fpu);
    
    But note that it's actually safe to call them without checking the flag first.
    
    This further decouples the fpu->fpregs_active flag from actual FPU logic.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170923130016.21448-20-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 7fa676f93ac1..42a601673c09 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -526,8 +526,6 @@ static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)
  */
 static inline void fpregs_deactivate(struct fpu *fpu)
 {
-	WARN_ON_FPU(!fpu->fpregs_active);
-
 	fpu->fpregs_active = 0;
 	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
 	trace_x86_fpu_regs_deactivated(fpu);
@@ -535,8 +533,6 @@ static inline void fpregs_deactivate(struct fpu *fpu)
 
 static inline void fpregs_activate(struct fpu *fpu)
 {
-	WARN_ON_FPU(fpu->fpregs_active);
-
 	fpu->fpregs_active = 1;
 	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
 	trace_x86_fpu_regs_activated(fpu);
@@ -604,8 +600,7 @@ static inline void user_fpu_begin(void)
 	struct fpu *fpu = &current->thread.fpu;
 
 	preempt_disable();
-	if (!fpu->fpregs_active)
-		fpregs_activate(fpu);
+	fpregs_activate(fpu);
 	preempt_enable();
 }
 

commit f1c8cd0176078c7bcafdc89cac447cab672a0b5e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 23 15:00:01 2017 +0200

    x86/fpu: Change fpu->fpregs_active users to fpu->fpstate_active
    
    We want to simplify the FPU state machine by eliminating fpu->fpregs_active,
    and we can do that because the two state flags (::fpregs_active and
    ::fpstate_active) are set essentially together.
    
    The old lazy FPU switching code used to make a distinction - but there's
    no lazy switching code anymore, we always switch in an 'eager' fashion.
    
    Do this by first changing all substantial uses of fpu->fpregs_active
    to fpu->fpstate_active and adding a few debug checks to double check
    our assumption is correct.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170923130016.21448-19-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index b223c57dd5dc..7fa676f93ac1 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -556,7 +556,9 @@ static inline void fpregs_activate(struct fpu *fpu)
 static inline void
 switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	if (old_fpu->fpregs_active) {
+	WARN_ON_FPU(old_fpu->fpregs_active != old_fpu->fpstate_active);
+
+	if (old_fpu->fpstate_active) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
 		else

commit b3a163081c28d1a4d1ad76259a9d93b34a82f1da
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Sep 23 14:59:58 2017 +0200

    x86/fpu: Simplify fpu->fpregs_active use
    
    The fpregs_active() inline function is pretty pointless - in almost
    all the callsites it can be replaced with a direct fpu->fpregs_active
    access.
    
    Do so and eliminate the extra layer of obfuscation.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/20170923130016.21448-16-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 554cdb205d17..b223c57dd5dc 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -542,21 +542,6 @@ static inline void fpregs_activate(struct fpu *fpu)
 	trace_x86_fpu_regs_activated(fpu);
 }
 
-/*
- * The question "does this thread have fpu access?"
- * is slightly racy, since preemption could come in
- * and revoke it immediately after the test.
- *
- * However, even in that very unlikely scenario,
- * we can just assume we have FPU access - typically
- * to save the FP state - we'll just take a #NM
- * fault and get the FPU access back.
- */
-static inline int fpregs_active(void)
-{
-	return current->thread.fpu.fpregs_active;
-}
-
 /*
  * FPU state switching for scheduling.
  *
@@ -617,7 +602,7 @@ static inline void user_fpu_begin(void)
 	struct fpu *fpu = &current->thread.fpu;
 
 	preempt_disable();
-	if (!fpregs_active())
+	if (!fpu->fpregs_active)
 		fpregs_activate(fpu);
 	preempt_enable();
 }

commit 38cfd5e3df9c4f88e76b547eee2087ee5c042ae2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Aug 23 23:16:29 2017 +0200

    KVM, pkeys: do not use PKRU value in vcpu->arch.guest_fpu.state
    
    The host pkru is restored right after vcpu exit (commit 1be0e61), so
    KVM_GET_XSAVE will return the host PKRU value instead.  Fix this by
    using the guest PKRU explicitly in fill_xsave and load_xsave.  This
    part is based on a patch by Junkang Fu.
    
    The host PKRU data may also not match the value in vcpu->arch.guest_fpu.state,
    because it could have been changed by userspace since the last time
    it was saved, so skip loading it in kvm_load_guest_fpu.
    
    Reported-by: Junkang Fu <junkang.fjk@alibaba-inc.com>
    Cc: Yang Zhang <zy107165@alibaba-inc.com>
    Fixes: 1be0e61c1f255faaeab04a390e00c8b9b9042870
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 255645f60ca2..554cdb205d17 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -450,10 +450,10 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 	return 0;
 }
 
-static inline void __copy_kernel_to_fpregs(union fpregs_state *fpstate)
+static inline void __copy_kernel_to_fpregs(union fpregs_state *fpstate, u64 mask)
 {
 	if (use_xsave()) {
-		copy_kernel_to_xregs(&fpstate->xsave, -1);
+		copy_kernel_to_xregs(&fpstate->xsave, mask);
 	} else {
 		if (use_fxsr())
 			copy_kernel_to_fxregs(&fpstate->fxsave);
@@ -477,7 +477,7 @@ static inline void copy_kernel_to_fpregs(union fpregs_state *fpstate)
 			: : [addr] "m" (fpstate));
 	}
 
-	__copy_kernel_to_fpregs(fpstate);
+	__copy_kernel_to_fpregs(fpstate, -1);
 }
 
 extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size);

commit a5828ed3d03d38399159dc17a98cefde3109a66b
Author: Yu-cheng Yu <yu-cheng.yu@intel.com>
Date:   Tue Jan 24 10:25:46 2017 -0800

    x86/fpu/xstate: Move XSAVES state init to a function
    
    Make XSTATE init similar to existing code; move it to a separate function.
    There is no functionality change.
    
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi V. Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1485282346-15437-1-git-send-email-yu-cheng.yu@intel.com
    [ Minor cleanliness edits. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d4a684997497..255645f60ca2 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -87,6 +87,16 @@ extern void fpstate_init_soft(struct swregs_state *soft);
 #else
 static inline void fpstate_init_soft(struct swregs_state *soft) {}
 #endif
+
+static inline void fpstate_init_xstate(struct xregs_state *xsave)
+{
+	/*
+	 * XRSTORS requires these bits set in xcomp_bv, or it will
+	 * trigger #GP:
+	 */
+	xsave->header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT | xfeatures_mask;
+}
+
 static inline void fpstate_init_fxstate(struct fxregs_state *fx)
 {
 	fx->cwd = 0x37f;

commit c474e50711aa79b7bd0ea30b44744baca5650375
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Oct 14 08:15:31 2016 -0400

    x86/fpu: Split old_fpu & new_fpu handling into separate functions
    
    By moving all of the new_fpu state handling into switch_fpu_finish(),
    the code can be simplified some more.
    
    This gets rid of the prefetch, but given the size of the FPU register
    state on modern CPUs, and the amount of work done by __switch_to()
    inbetween both functions, the value of a single cache line prefetch
    seems somewhat dubious anyway.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1476447331-21566-3-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 590f27488fca..d4a684997497 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -552,27 +552,15 @@ static inline int fpregs_active(void)
  *
  * This is a two-stage process:
  *
- *  - switch_fpu_prepare() saves the old state and
- *    sets the new state of the CR0.TS bit. This is
- *    done within the context of the old process.
+ *  - switch_fpu_prepare() saves the old state.
+ *    This is done within the context of the old process.
  *
  *  - switch_fpu_finish() restores the new state as
  *    necessary.
  */
-typedef struct { int preload; } fpu_switch_t;
-
-static inline fpu_switch_t
-switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
+static inline void
+switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	fpu_switch_t fpu;
-
-	/*
-	 * If the task has used the math, pre-load the FPU on xsave processors
-	 * or if the past 5 consecutive context-switches used math.
-	 */
-	fpu.preload = static_cpu_has(X86_FEATURE_FPU) &&
-		      new_fpu->fpstate_active;
-
 	if (old_fpu->fpregs_active) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
@@ -584,16 +572,6 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 		trace_x86_fpu_regs_deactivated(old_fpu);
 	} else
 		old_fpu->last_cpu = -1;
-
-	if (fpu.preload) {
-		if (fpregs_state_valid(new_fpu, cpu))
-			fpu.preload = 0;
-		else
-			prefetch(&new_fpu->state);
-		fpregs_activate(new_fpu);
-	}
-
-	return fpu;
 }
 
 /*
@@ -601,15 +579,19 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
  */
 
 /*
- * By the time this gets called, we've already cleared CR0.TS and
- * given the process the FPU if we are going to preload the FPU
- * state - all we need to do is to conditionally restore the register
- * state itself.
+ * Set up the userspace FPU context for the new task, if the task
+ * has used the FPU.
  */
-static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)
+static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)
 {
-	if (fpu_switch.preload)
-		copy_kernel_to_fpregs(&new_fpu->state);
+	bool preload = static_cpu_has(X86_FEATURE_FPU) &&
+		       new_fpu->fpstate_active;
+
+	if (preload) {
+		if (!fpregs_state_valid(new_fpu, cpu))
+			copy_kernel_to_fpregs(&new_fpu->state);
+		fpregs_activate(new_fpu);
+	}
 }
 
 /*

commit 317b622cb2fda1812d8646e211cdb23dce2564d0
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Oct 14 08:15:30 2016 -0400

    x86/fpu: Remove 'cpu' argument from __cpu_invalidate_fpregs_state()
    
    The __{fpu,cpu}_invalidate_fpregs_state() functions can only be used
    to invalidate a resource they control.  Document that, and change
    the API a little bit to reflect that.
    
    Go back to open coding the fpu_fpregs_owner_ctx write in the CPU
    hotplug code, which should be the exception, and move __kernel_fpu_begin()
    to this API.
    
    This patch has no functional changes to the current code.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1476447331-21566-2-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 1dcb29e8d92c..590f27488fca 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -488,15 +488,16 @@ DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
  *
  * Any code that clobbers the FPU registers or updates the in-memory
  * FPU state for a task MUST let the rest of the kernel know that the
- * FPU registers are no longer valid for this task. Calling either of
- * these two invalidate functions is enough, use whichever is convenient.
+ * FPU registers are no longer valid for this task.
  *
- * Must be run with preemption disabled: this clears the fpu_fpregs_owner_ctx,
- * on this CPU.
+ * Either one of these invalidation functions is enough. Invalidate
+ * a resource you control: CPU if using the CPU for something else
+ * (with preemption disabled), FPU for the current task, or a task that
+ * is prevented from running by the current task.
  */
-static inline void __cpu_invalidate_fpregs_state(unsigned int cpu)
+static inline void __cpu_invalidate_fpregs_state(void)
 {
-	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;
+	__this_cpu_write(fpu_fpregs_owner_ctx, NULL);
 }
 
 static inline void __fpu_invalidate_fpregs_state(struct fpu *fpu)

commit 9ad93fe35aff616fca4e2b9581fdeed498605f9e
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Oct 4 20:34:38 2016 -0400

    x86/fpu: Split old & new FPU code paths
    
    Now that CR0.TS is no longer being manipulated, we can simplify
    switch_fpu_prepare() by no longer nesting the handling of new_fpu
    inside the two branches for the old_fpu.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-10-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d0324bcdd3d6..1dcb29e8d92c 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -581,23 +581,17 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 		/* But leave fpu_fpregs_owner_ctx! */
 		old_fpu->fpregs_active = 0;
 		trace_x86_fpu_regs_deactivated(old_fpu);
+	} else
+		old_fpu->last_cpu = -1;
 
-		/* Don't change CR0.TS if we just switch! */
-		if (fpu.preload) {
-			fpregs_activate(new_fpu);
-			trace_x86_fpu_regs_activated(new_fpu);
+	if (fpu.preload) {
+		if (fpregs_state_valid(new_fpu, cpu))
+			fpu.preload = 0;
+		else
 			prefetch(&new_fpu->state);
-		}
-	} else {
-		old_fpu->last_cpu = -1;
-		if (fpu.preload) {
-			if (fpregs_state_valid(new_fpu, cpu))
-				fpu.preload = 0;
-			else
-				prefetch(&new_fpu->state);
-			fpregs_activate(new_fpu);
-		}
+		fpregs_activate(new_fpu);
 	}
+
 	return fpu;
 }
 

commit 66f314efca3843a8874405ab015e354d041f86dd
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Oct 4 20:34:37 2016 -0400

    x86/fpu: Remove __fpregs_(de)activate()
    
    Now that fpregs_activate() and fpregs_deactivate() do nothing except
    call the double underscored versions of themselves, we can get
    rid of the double underscore version.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-9-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d2cfe16dd9fa..d0324bcdd3d6 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -509,8 +509,11 @@ static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)
 	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
 }
 
-
-static inline void __fpregs_deactivate(struct fpu *fpu)
+/*
+ * These generally need preemption protection to work,
+ * do try to avoid using these on their own:
+ */
+static inline void fpregs_deactivate(struct fpu *fpu)
 {
 	WARN_ON_FPU(!fpu->fpregs_active);
 
@@ -519,7 +522,7 @@ static inline void __fpregs_deactivate(struct fpu *fpu)
 	trace_x86_fpu_regs_deactivated(fpu);
 }
 
-static inline void __fpregs_activate(struct fpu *fpu)
+static inline void fpregs_activate(struct fpu *fpu)
 {
 	WARN_ON_FPU(fpu->fpregs_active);
 
@@ -543,20 +546,6 @@ static inline int fpregs_active(void)
 	return current->thread.fpu.fpregs_active;
 }
 
-/*
- * These generally need preemption protection to work,
- * do try to avoid using these on their own.
- */
-static inline void fpregs_activate(struct fpu *fpu)
-{
-	__fpregs_activate(fpu);
-}
-
-static inline void fpregs_deactivate(struct fpu *fpu)
-{
-	__fpregs_deactivate(fpu);
-}
-
 /*
  * FPU state switching for scheduling.
  *
@@ -595,7 +584,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 
 		/* Don't change CR0.TS if we just switch! */
 		if (fpu.preload) {
-			__fpregs_activate(new_fpu);
+			fpregs_activate(new_fpu);
 			trace_x86_fpu_regs_activated(new_fpu);
 			prefetch(&new_fpu->state);
 		}

commit 25d83b531c1aa4fca5b4e24ed10f493268f162bc
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Oct 4 20:34:36 2016 -0400

    x86/fpu: Rename lazy restore functions to "register state valid"
    
    Name the functions after the state they track, rather than the function
    they currently enable. This should make it more obvious when we use the
    fpu_register_state_valid() function for something else in the future.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-8-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 499d6ed0e376..d2cfe16dd9fa 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -479,18 +479,32 @@ extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size)
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
 /*
+ * The in-register FPU state for an FPU context on a CPU is assumed to be
+ * valid if the fpu->last_cpu matches the CPU, and the fpu_fpregs_owner_ctx
+ * matches the FPU.
+ *
+ * If the FPU register state is valid, the kernel can skip restoring the
+ * FPU state from memory.
+ *
+ * Any code that clobbers the FPU registers or updates the in-memory
+ * FPU state for a task MUST let the rest of the kernel know that the
+ * FPU registers are no longer valid for this task. Calling either of
+ * these two invalidate functions is enough, use whichever is convenient.
+ *
  * Must be run with preemption disabled: this clears the fpu_fpregs_owner_ctx,
  * on this CPU.
- *
- * This will disable any lazy FPU state restore of the current FPU state,
- * but if the current thread owns the FPU, it will still be saved by.
  */
-static inline void __cpu_disable_lazy_restore(unsigned int cpu)
+static inline void __cpu_invalidate_fpregs_state(unsigned int cpu)
 {
 	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;
 }
 
-static inline int fpu_want_lazy_restore(struct fpu *fpu, unsigned int cpu)
+static inline void __fpu_invalidate_fpregs_state(struct fpu *fpu)
+{
+	fpu->last_cpu = -1;
+}
+
+static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)
 {
 	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
 }
@@ -588,7 +602,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 	} else {
 		old_fpu->last_cpu = -1;
 		if (fpu.preload) {
-			if (fpu_want_lazy_restore(new_fpu, cpu))
+			if (fpregs_state_valid(new_fpu, cpu))
 				fpu.preload = 0;
 			else
 				prefetch(&new_fpu->state);

commit 3913cc3507575273beb165a5e027a081913ed507
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Oct 4 20:34:34 2016 -0400

    x86/fpu: Remove struct fpu::counter
    
    With the lazy FPU code gone, we no longer use the counter field
    in struct fpu for anything. Get rid it.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-6-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 7801d32347a2..499d6ed0e376 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -581,16 +581,13 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 
 		/* Don't change CR0.TS if we just switch! */
 		if (fpu.preload) {
-			new_fpu->counter++;
 			__fpregs_activate(new_fpu);
 			trace_x86_fpu_regs_activated(new_fpu);
 			prefetch(&new_fpu->state);
 		}
 	} else {
-		old_fpu->counter = 0;
 		old_fpu->last_cpu = -1;
 		if (fpu.preload) {
-			new_fpu->counter++;
 			if (fpu_want_lazy_restore(new_fpu, cpu))
 				fpu.preload = 0;
 			else

commit c592b57347069abfc0dcad3b3a302cf882602597
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Oct 4 20:34:33 2016 -0400

    x86/fpu: Remove use_eager_fpu()
    
    This removes all the obvious code paths that depend on lazy FPU mode.
    It shouldn't change the generated code at all.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-5-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 8852e3afa1ad..7801d32347a2 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -60,11 +60,6 @@ extern u64 fpu__get_supported_xfeatures_mask(void);
 /*
  * FPU related CPU feature flag helper routines:
  */
-static __always_inline __pure bool use_eager_fpu(void)
-{
-	return true;
-}
-
 static __always_inline __pure bool use_xsaveopt(void)
 {
 	return static_cpu_has(X86_FEATURE_XSAVEOPT);
@@ -501,24 +496,6 @@ static inline int fpu_want_lazy_restore(struct fpu *fpu, unsigned int cpu)
 }
 
 
-/*
- * Wrap lazy FPU TS handling in a 'hw fpregs activation/deactivation'
- * idiom, which is then paired with the sw-flag (fpregs_active) later on:
- */
-
-static inline void __fpregs_activate_hw(void)
-{
-	if (!use_eager_fpu())
-		clts();
-}
-
-static inline void __fpregs_deactivate_hw(void)
-{
-	if (!use_eager_fpu())
-		stts();
-}
-
-/* Must be paired with an 'stts' (fpregs_deactivate_hw()) after! */
 static inline void __fpregs_deactivate(struct fpu *fpu)
 {
 	WARN_ON_FPU(!fpu->fpregs_active);
@@ -528,7 +505,6 @@ static inline void __fpregs_deactivate(struct fpu *fpu)
 	trace_x86_fpu_regs_deactivated(fpu);
 }
 
-/* Must be paired with a 'clts' (fpregs_activate_hw()) before! */
 static inline void __fpregs_activate(struct fpu *fpu)
 {
 	WARN_ON_FPU(fpu->fpregs_active);
@@ -554,22 +530,17 @@ static inline int fpregs_active(void)
 }
 
 /*
- * Encapsulate the CR0.TS handling together with the
- * software flag.
- *
  * These generally need preemption protection to work,
  * do try to avoid using these on their own.
  */
 static inline void fpregs_activate(struct fpu *fpu)
 {
-	__fpregs_activate_hw();
 	__fpregs_activate(fpu);
 }
 
 static inline void fpregs_deactivate(struct fpu *fpu)
 {
 	__fpregs_deactivate(fpu);
-	__fpregs_deactivate_hw();
 }
 
 /*
@@ -596,8 +567,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 	 * or if the past 5 consecutive context-switches used math.
 	 */
 	fpu.preload = static_cpu_has(X86_FEATURE_FPU) &&
-		      new_fpu->fpstate_active &&
-		      (use_eager_fpu() || new_fpu->counter > 5);
+		      new_fpu->fpstate_active;
 
 	if (old_fpu->fpregs_active) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
@@ -615,8 +585,6 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 			__fpregs_activate(new_fpu);
 			trace_x86_fpu_regs_activated(new_fpu);
 			prefetch(&new_fpu->state);
-		} else {
-			__fpregs_deactivate_hw();
 		}
 	} else {
 		old_fpu->counter = 0;

commit ca6938a1cd8a1c5e861a99b67f84ac166fc2b9e7
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Oct 4 20:34:31 2016 -0400

    x86/fpu: Hard-disable lazy FPU mode
    
    Since commit:
    
      58122bf1d856 ("x86/fpu: Default eagerfpu=on on all CPUs")
    
    ... in Linux 4.6, eager FPU mode has been the default on all x86
    systems, and no one has reported any regressions.
    
    This patch removes the ability to enable lazy mode: use_eager_fpu()
    becomes "return true" and all of the FPU mode selection machinery is
    removed.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-3-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 2737366ea583..8852e3afa1ad 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -62,7 +62,7 @@ extern u64 fpu__get_supported_xfeatures_mask(void);
  */
 static __always_inline __pure bool use_eager_fpu(void)
 {
-	return static_cpu_has(X86_FEATURE_EAGER_FPU);
+	return true;
 }
 
 static __always_inline __pure bool use_xsaveopt(void)

commit 97f2645f358b411ba2afb22e5966753f0ad92916
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Wed Aug 3 13:45:50 2016 -0700

    tree-wide: replace config_enabled() with IS_ENABLED()
    
    The use of config_enabled() against config options is ambiguous.  In
    practical terms, config_enabled() is equivalent to IS_BUILTIN(), but the
    author might have used it for the meaning of IS_ENABLED().  Using
    IS_ENABLED(), IS_BUILTIN(), IS_MODULE() etc.  makes the intention
    clearer.
    
    This commit replaces config_enabled() with IS_ENABLED() where possible.
    This commit is only touching bool config options.
    
    I noticed two cases where config_enabled() is used against a tristate
    option:
    
     - config_enabled(CONFIG_HWMON)
      [ drivers/net/wireless/ath/ath10k/thermal.c ]
    
     - config_enabled(CONFIG_BACKLIGHT_CLASS_DEVICE)
      [ drivers/gpu/drm/gma500/opregion.c ]
    
    I did not touch them because they should be converted to IS_BUILTIN()
    in order to keep the logic, but I was not sure it was the authors'
    intention.
    
    Link: http://lkml.kernel.org/r/1465215656-20569-1-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Stas Sergeev <stsp@list.ru>
    Cc: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Joshua Kinard <kumba@gentoo.org>
    Cc: Jiri Slaby <jslaby@suse.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Markos Chandras <markos.chandras@imgtec.com>
    Cc: "Dmitry V. Levin" <ldv@altlinux.org>
    Cc: yu-cheng yu <yu-cheng.yu@intel.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Nikolay Martynov <mar.kolya@gmail.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Leonid Yegoshin <Leonid.Yegoshin@imgtec.com>
    Cc: Rafal Milecki <zajec5@gmail.com>
    Cc: James Cowgill <James.Cowgill@imgtec.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Alex Smith <alex.smith@imgtec.com>
    Cc: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Mikko Rapeli <mikko.rapeli@iki.fi>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Brian Norris <computersforpeace@gmail.com>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: "Luis R. Rodriguez" <mcgrof@do-not-panic.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Kalle Valo <kvalo@qca.qualcomm.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Tony Wu <tung7970@gmail.com>
    Cc: Huaitong Han <huaitong.han@intel.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrea Gelmini <andrea.gelmini@gelma.net>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Rabin Vincent <rabin@rab.in>
    Cc: "Maciej W. Rozycki" <macro@imgtec.com>
    Cc: David Daney <david.daney@cavium.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 116b58347501..2737366ea583 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -137,9 +137,9 @@ static inline int copy_fregs_to_user(struct fregs_state __user *fx)
 
 static inline int copy_fxregs_to_user(struct fxregs_state __user *fx)
 {
-	if (config_enabled(CONFIG_X86_32))
+	if (IS_ENABLED(CONFIG_X86_32))
 		return user_insn(fxsave %[fx], [fx] "=m" (*fx), "m" (*fx));
-	else if (config_enabled(CONFIG_AS_FXSAVEQ))
+	else if (IS_ENABLED(CONFIG_AS_FXSAVEQ))
 		return user_insn(fxsaveq %[fx], [fx] "=m" (*fx), "m" (*fx));
 
 	/* See comment in copy_fxregs_to_kernel() below. */
@@ -150,10 +150,10 @@ static inline void copy_kernel_to_fxregs(struct fxregs_state *fx)
 {
 	int err;
 
-	if (config_enabled(CONFIG_X86_32)) {
+	if (IS_ENABLED(CONFIG_X86_32)) {
 		err = check_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 	} else {
-		if (config_enabled(CONFIG_AS_FXSAVEQ)) {
+		if (IS_ENABLED(CONFIG_AS_FXSAVEQ)) {
 			err = check_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
 		} else {
 			/* See comment in copy_fxregs_to_kernel() below. */
@@ -166,9 +166,9 @@ static inline void copy_kernel_to_fxregs(struct fxregs_state *fx)
 
 static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
 {
-	if (config_enabled(CONFIG_X86_32))
+	if (IS_ENABLED(CONFIG_X86_32))
 		return user_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
-	else if (config_enabled(CONFIG_AS_FXSAVEQ))
+	else if (IS_ENABLED(CONFIG_AS_FXSAVEQ))
 		return user_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
 
 	/* See comment in copy_fxregs_to_kernel() below. */
@@ -190,9 +190,9 @@ static inline int copy_user_to_fregs(struct fregs_state __user *fx)
 
 static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 {
-	if (config_enabled(CONFIG_X86_32))
+	if (IS_ENABLED(CONFIG_X86_32))
 		asm volatile( "fxsave %[fx]" : [fx] "=m" (fpu->state.fxsave));
-	else if (config_enabled(CONFIG_AS_FXSAVEQ))
+	else if (IS_ENABLED(CONFIG_AS_FXSAVEQ))
 		asm volatile("fxsaveq %[fx]" : [fx] "=m" (fpu->state.fxsave));
 	else {
 		/* Using "rex64; fxsave %0" is broken because, if the memory

commit d1898b733619bd46194bd25aa6452d238ff2dc4e
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Wed Jun 1 10:42:20 2016 -0700

    x86/fpu: Add tracepoints to dump FPU state at key points
    
    I've been carrying this patch around for a bit and it's helped me
    solve at least a couple FPU-related bugs.  In addition to using
    it for debugging, I also drug it out because using AVX (and
    AVX2/AVX-512) can have serious power consequences for a modern
    core.  It's very important to be able to figure out who is using
    it.
    
    It's also insanely useful to go out and see who is using a given
    feature, like MPX or Memory Protection Keys.  If you, for
    instance, want to find all processes using protection keys, you
    can do:
    
            echo 'xfeatures & 0x200' > filter
    
    Since 0x200 is the protection keys feature bit.
    
    Note that this touches the KVM code.  KVM did a CREATE_TRACE_POINTS
    and then included a bunch of random headers.  If anyone one of
    those included other tracepoints, it would have defined the *OTHER*
    tracepoints.  That's bogus, so move it to the right place.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160601174220.3CDFB90E@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 31ac8e6d9f36..116b58347501 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -18,6 +18,7 @@
 #include <asm/fpu/api.h>
 #include <asm/fpu/xstate.h>
 #include <asm/cpufeature.h>
+#include <asm/trace/fpu.h>
 
 /*
  * High level FPU state handling functions:
@@ -524,6 +525,7 @@ static inline void __fpregs_deactivate(struct fpu *fpu)
 
 	fpu->fpregs_active = 0;
 	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
+	trace_x86_fpu_regs_deactivated(fpu);
 }
 
 /* Must be paired with a 'clts' (fpregs_activate_hw()) before! */
@@ -533,6 +535,7 @@ static inline void __fpregs_activate(struct fpu *fpu)
 
 	fpu->fpregs_active = 1;
 	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
+	trace_x86_fpu_regs_activated(fpu);
 }
 
 /*
@@ -604,11 +607,13 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 
 		/* But leave fpu_fpregs_owner_ctx! */
 		old_fpu->fpregs_active = 0;
+		trace_x86_fpu_regs_deactivated(old_fpu);
 
 		/* Don't change CR0.TS if we just switch! */
 		if (fpu.preload) {
 			new_fpu->counter++;
 			__fpregs_activate(new_fpu);
+			trace_x86_fpu_regs_activated(new_fpu);
 			prefetch(&new_fpu->state);
 		} else {
 			__fpregs_deactivate_hw();

commit b8b9b6ba9dec3f155c7555cb208ba4078e97aedb
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Feb 12 13:02:35 2016 -0800

    x86/fpu: Allow setting of XSAVE state
    
    We want to modify the Protection Key rights inside the kernel, so
    we need to change PKRU's contents.  But, if we do a plain
    'wrpkru', when we return to userspace we might do an XRSTOR and
    wipe out the kernel's 'wrpkru'.  So, we need to go after PKRU in
    the xsave buffer.
    
    We do this by:
    
      1. Ensuring that we have the XSAVE registers (fpregs) in the
         kernel FPU buffer (fpstate)
      2. Looking up the location of a given state in the buffer
      3. Filling in the stat
      4. Ensuring that the hardware knows that state is present there
         (basically that the 'init optimization' is not in place).
      5. Copying the newly-modified state back to the registers if
         necessary.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20160212210235.5A3139BF@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a2124343edf5..31ac8e6d9f36 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -25,6 +25,8 @@
 extern void fpu__activate_curr(struct fpu *fpu);
 extern void fpu__activate_fpstate_read(struct fpu *fpu);
 extern void fpu__activate_fpstate_write(struct fpu *fpu);
+extern void fpu__current_fpstate_write_begin(void);
+extern void fpu__current_fpstate_write_end(void);
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);

commit 1fe3f29e4a908461be16a9388e73837157cc7942
Merge: 1926e54f1157 58122bf1d856 e2c7698cd61f f2cc8e0791c7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 16 09:37:37 2016 +0100

    Merge branches 'x86/fpu', 'x86/mm' and 'x86/asm' into x86/pkeys
    
    Provide a stable basis for the pkeys patches, which touches various
    x86 details.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 4ecd16ec7059390b430af34bd8bc3ca2b5dcef9a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Jan 24 14:38:06 2016 -0800

    x86/fpu: Fix math emulation in eager fpu mode
    
    Systems without an FPU are generally old and therefore use lazy FPU
    switching. Unsurprisingly, math emulation in eager FPU mode is a
    bit buggy. Fix it.
    
    There were two bugs involving kernel code trying to use the FPU
    registers in eager mode even if they didn't exist and one BUG_ON()
    that was incorrect.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: yu-cheng yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/b4b8d112436bd6fab866e1b4011131507e8d7fbe.1453675014.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 0fd440df63f1..a1f78a9fbf41 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -589,7 +589,8 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 	 * If the task has used the math, pre-load the FPU on xsave processors
 	 * or if the past 5 consecutive context-switches used math.
 	 */
-	fpu.preload = new_fpu->fpstate_active &&
+	fpu.preload = static_cpu_has(X86_FEATURE_FPU) &&
+		      new_fpu->fpstate_active &&
 		      (use_eager_fpu() || new_fpu->counter > 5);
 
 	if (old_fpu->fpregs_active) {

commit bc696ca05f5a8927329ec276a892341e006b00ba
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 26 22:12:05 2016 +0100

    x86/cpufeature: Replace the old static_cpu_has() with safe variant
    
    So the old one didn't work properly before alternatives had run.
    And it was supposed to provide an optimized JMP because the
    assumption was that the offset it is jumping to is within a
    signed byte and thus a two-byte JMP.
    
    So I did an x86_64 allyesconfig build and dumped all possible
    sites where static_cpu_has() was used. The optimization amounted
    to all in all 12(!) places where static_cpu_has() had generated
    a 2-byte JMP. Which has saved us a whopping 36 bytes!
    
    This clearly is not worth the trouble so we can remove it. The
    only place where the optimization might count - in __switch_to()
    - we will handle differently. But that's not subject of this
    patch.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453842730-28463-6-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d01199def781..c2e46eb96b6d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -59,22 +59,22 @@ extern u64 fpu__get_supported_xfeatures_mask(void);
  */
 static __always_inline __pure bool use_eager_fpu(void)
 {
-	return static_cpu_has_safe(X86_FEATURE_EAGER_FPU);
+	return static_cpu_has(X86_FEATURE_EAGER_FPU);
 }
 
 static __always_inline __pure bool use_xsaveopt(void)
 {
-	return static_cpu_has_safe(X86_FEATURE_XSAVEOPT);
+	return static_cpu_has(X86_FEATURE_XSAVEOPT);
 }
 
 static __always_inline __pure bool use_xsave(void)
 {
-	return static_cpu_has_safe(X86_FEATURE_XSAVE);
+	return static_cpu_has(X86_FEATURE_XSAVE);
 }
 
 static __always_inline __pure bool use_fxsr(void)
 {
-	return static_cpu_has_safe(X86_FEATURE_FXSR);
+	return static_cpu_has(X86_FEATURE_FXSR);
 }
 
 /*
@@ -301,7 +301,7 @@ static inline void copy_xregs_to_kernel_booting(struct xregs_state *xstate)
 
 	WARN_ON(system_state != SYSTEM_BOOTING);
 
-	if (static_cpu_has_safe(X86_FEATURE_XSAVES))
+	if (static_cpu_has(X86_FEATURE_XSAVES))
 		XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
 	else
 		XSTATE_OP(XSAVE, xstate, lmask, hmask, err);
@@ -323,7 +323,7 @@ static inline void copy_kernel_to_xregs_booting(struct xregs_state *xstate)
 
 	WARN_ON(system_state != SYSTEM_BOOTING);
 
-	if (static_cpu_has_safe(X86_FEATURE_XSAVES))
+	if (static_cpu_has(X86_FEATURE_XSAVES))
 		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
 	else
 		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
@@ -461,7 +461,7 @@ static inline void copy_kernel_to_fpregs(union fpregs_state *fpstate)
 	 * pending. Clear the x87 state here by setting it to fixed values.
 	 * "m" is a random variable that should be in L1.
 	 */
-	if (unlikely(static_cpu_has_bug_safe(X86_BUG_FXSAVE_LEAK))) {
+	if (unlikely(static_cpu_has_bug(X86_BUG_FXSAVE_LEAK))) {
 		asm volatile(
 			"fnclex\n\t"
 			"emms\n\t"

commit cd4d09ec6f6c12a2cc3db5b7d8876a325a53545b
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 26 22:12:04 2016 +0100

    x86/cpufeature: Carve out X86_FEATURE_*
    
    Move them to a separate header and have the following
    dependency:
    
      x86/cpufeatures.h <- x86/processor.h <- x86/cpufeature.h
    
    This makes it easier to use the header in asm code and not
    include the whole cpufeature.h and add guards for asm.
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453842730-28463-5-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 0fd440df63f1..d01199def781 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -17,6 +17,7 @@
 #include <asm/user.h>
 #include <asm/fpu/api.h>
 #include <asm/fpu/xstate.h>
+#include <asm/cpufeature.h>
 
 /*
  * High level FPU state handling functions:

commit a5fe93a549c54838063d2952dd9643b0b18aa67f
Author: yu-cheng yu <yu-cheng.yu@intel.com>
Date:   Wed Jan 6 14:24:53 2016 -0800

    x86/fpu: Disable MPX when eagerfpu is off
    
    This issue is a fallout from the command-line parsing move.
    
    When "eagerfpu=off" is given as a command-line input, the kernel
    should disable MPX support. The decision for turning off MPX was
    made in fpu__init_system_ctx_switch(), which is after the
    selection of the XSAVE format. This patch fixes it by getting
    that decision done earlier in fpu__init_system_xstate().
    
    Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Ravi V. Shankar <ravi.v.shankar@intel.com>
    Cc: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: yu-cheng yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/1452119094-7252-4-git-send-email-yu-cheng.yu@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index eadcdd5bb946..0fd440df63f1 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -42,6 +42,7 @@ extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 extern void fpu__init_check_bugs(void);
 extern void fpu__resume_cpu(void);
+extern u64 fpu__get_supported_xfeatures_mask(void);
 
 /*
  * Debugging facility:

commit b7106fa0f29f9fd83d2d1905ab690d334ef855c1
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Nov 19 12:25:26 2015 +0100

    x86/fpu: Get rid of xstate_fault()
    
    Add macros for the alternative XSAVE*/XRSTOR* operations which
    contain the fault handling and use them. Kill xstate_fault().
    
    Also, copy_xregs_to_kernel() didn't have the extended state as
    memory reference in the asm.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447932326-4371-3-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 709a3df8ab37..eadcdd5bb946 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -224,19 +224,6 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 #define XRSTOR		".byte " REX_PREFIX "0x0f,0xae,0x2f"
 #define XRSTORS		".byte " REX_PREFIX "0x0f,0xc7,0x1f"
 
-/* xstate instruction fault handler: */
-#define xstate_fault(__err)		\
-					\
-	".section .fixup,\"ax\"\n"	\
-					\
-	"3:  movl $-2,%[_err]\n"	\
-	"    jmp  2b\n"			\
-					\
-	".previous\n"			\
-					\
-	_ASM_EXTABLE(1b, 3b)		\
-	: [_err] "=r" (__err)
-
 #define XSTATE_OP(op, st, lmask, hmask, err)				\
 	asm volatile("1:" op "\n\t"					\
 		     "xor %[err], %[err]\n"				\
@@ -250,6 +237,54 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
 		     : "memory")
 
+/*
+ * If XSAVES is enabled, it replaces XSAVEOPT because it supports a compact
+ * format and supervisor states in addition to modified optimization in
+ * XSAVEOPT.
+ *
+ * Otherwise, if XSAVEOPT is enabled, XSAVEOPT replaces XSAVE because XSAVEOPT
+ * supports modified optimization which is not supported by XSAVE.
+ *
+ * We use XSAVE as a fallback.
+ *
+ * The 661 label is defined in the ALTERNATIVE* macros as the address of the
+ * original instruction which gets replaced. We need to use it here as the
+ * address of the instruction where we might get an exception at.
+ */
+#define XSTATE_XSAVE(st, lmask, hmask, err)				\
+	asm volatile(ALTERNATIVE_2(XSAVE,				\
+				   XSAVEOPT, X86_FEATURE_XSAVEOPT,	\
+				   XSAVES,   X86_FEATURE_XSAVES)	\
+		     "\n"						\
+		     "xor %[err], %[err]\n"				\
+		     "3:\n"						\
+		     ".pushsection .fixup,\"ax\"\n"			\
+		     "4: movl $-2, %[err]\n"				\
+		     "jmp 3b\n"						\
+		     ".popsection\n"					\
+		     _ASM_EXTABLE(661b, 4b)				\
+		     : [err] "=r" (err)					\
+		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+		     : "memory")
+
+/*
+ * Use XRSTORS to restore context if it is enabled. XRSTORS supports compact
+ * XSAVE area format.
+ */
+#define XSTATE_XRESTORE(st, lmask, hmask, err)				\
+	asm volatile(ALTERNATIVE(XRSTOR,				\
+				 XRSTORS, X86_FEATURE_XSAVES)		\
+		     "\n"						\
+		     "xor %[err], %[err]\n"				\
+		     "3:\n"						\
+		     ".pushsection .fixup,\"ax\"\n"			\
+		     "4: movl $-2, %[err]\n"				\
+		     "jmp 3b\n"						\
+		     ".popsection\n"					\
+		     _ASM_EXTABLE(661b, 4b)				\
+		     : [err] "=r" (err)					\
+		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+		     : "memory")
 
 /*
  * This function is called only during boot time when x86 caps are not set
@@ -303,33 +338,11 @@ static inline void copy_xregs_to_kernel(struct xregs_state *xstate)
 	u64 mask = -1;
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
-	int err = 0;
+	int err;
 
 	WARN_ON(!alternatives_patched);
 
-	/*
-	 * If xsaves is enabled, xsaves replaces xsaveopt because
-	 * it supports compact format and supervisor states in addition to
-	 * modified optimization in xsaveopt.
-	 *
-	 * Otherwise, if xsaveopt is enabled, xsaveopt replaces xsave
-	 * because xsaveopt supports modified optimization which is not
-	 * supported by xsave.
-	 *
-	 * If none of xsaves and xsaveopt is enabled, use xsave.
-	 */
-	alternative_input_2(
-		"1:"XSAVE,
-		XSAVEOPT,
-		X86_FEATURE_XSAVEOPT,
-		XSAVES,
-		X86_FEATURE_XSAVES,
-		[xstate] "D" (xstate), "a" (lmask), "d" (hmask) :
-		"memory");
-	asm volatile("2:\n\t"
-		     xstate_fault(err)
-		     : "0" (err)
-		     : "memory");
+	XSTATE_XSAVE(xstate, lmask, hmask, err);
 
 	/* We should never fault when copying to a kernel buffer: */
 	WARN_ON_FPU(err);
@@ -342,23 +355,9 @@ static inline void copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
 {
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
-	int err = 0;
+	int err;
 
-	/*
-	 * Use xrstors to restore context if it is enabled. xrstors supports
-	 * compacted format of xsave area which is not supported by xrstor.
-	 */
-	alternative_input(
-		"1: " XRSTOR,
-		XRSTORS,
-		X86_FEATURE_XSAVES,
-		"D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
-		: "memory");
-
-	asm volatile("2:\n"
-		     xstate_fault(err)
-		     : "0" (err)
-		     : "memory");
+	XSTATE_XRESTORE(xstate, lmask, hmask, err);
 
 	/* We should never fault when copying from a kernel buffer: */
 	WARN_ON_FPU(err);

commit b74a0cf1b3db30173eefa00c411775d2b1697700
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Nov 19 12:25:25 2015 +0100

    x86/fpu: Add an XSTATE_OP() macro
    
    Add an XSTATE_OP() macro which contains the XSAVE* fault handling
    and replace all non-alternatives users of xstate_fault() with
    it.
    
    This fixes also the buglet in copy_xregs_to_user() and
    copy_user_to_xregs() where the inline asm didn't have @xstate as
    memory reference and thus potentially causing unwanted
    reordering of accesses to the extended state.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447932326-4371-2-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 3c3550c3a4a3..709a3df8ab37 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -237,6 +237,20 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 	_ASM_EXTABLE(1b, 3b)		\
 	: [_err] "=r" (__err)
 
+#define XSTATE_OP(op, st, lmask, hmask, err)				\
+	asm volatile("1:" op "\n\t"					\
+		     "xor %[err], %[err]\n"				\
+		     "2:\n\t"						\
+		     ".pushsection .fixup,\"ax\"\n\t"			\
+		     "3: movl $-2,%[err]\n\t"				\
+		     "jmp 2b\n\t"					\
+		     ".popsection\n\t"					\
+		     _ASM_EXTABLE(1b, 3b)				\
+		     : [err] "=r" (err)					\
+		     : "D" (st), "m" (*st), "a" (lmask), "d" (hmask)	\
+		     : "memory")
+
+
 /*
  * This function is called only during boot time when x86 caps are not set
  * up and alternative can not be used yet.
@@ -246,22 +260,14 @@ static inline void copy_xregs_to_kernel_booting(struct xregs_state *xstate)
 	u64 mask = -1;
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
-	int err = 0;
+	int err;
 
 	WARN_ON(system_state != SYSTEM_BOOTING);
 
-	if (boot_cpu_has(X86_FEATURE_XSAVES))
-		asm volatile("1:"XSAVES"\n\t"
-			"2:\n\t"
-			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
-			: "memory");
+	if (static_cpu_has_safe(X86_FEATURE_XSAVES))
+		XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
 	else
-		asm volatile("1:"XSAVE"\n\t"
-			"2:\n\t"
-			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
-			: "memory");
+		XSTATE_OP(XSAVE, xstate, lmask, hmask, err);
 
 	/* We should never fault when copying to a kernel buffer: */
 	WARN_ON_FPU(err);
@@ -276,22 +282,14 @@ static inline void copy_kernel_to_xregs_booting(struct xregs_state *xstate)
 	u64 mask = -1;
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
-	int err = 0;
+	int err;
 
 	WARN_ON(system_state != SYSTEM_BOOTING);
 
-	if (boot_cpu_has(X86_FEATURE_XSAVES))
-		asm volatile("1:"XRSTORS"\n\t"
-			"2:\n\t"
-			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
-			: "memory");
+	if (static_cpu_has_safe(X86_FEATURE_XSAVES))
+		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
 	else
-		asm volatile("1:"XRSTOR"\n\t"
-			"2:\n\t"
-			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
-			: "memory");
+		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
 
 	/* We should never fault when copying from a kernel buffer: */
 	WARN_ON_FPU(err);
@@ -388,12 +386,10 @@ static inline int copy_xregs_to_user(struct xregs_state __user *buf)
 	if (unlikely(err))
 		return -EFAULT;
 
-	__asm__ __volatile__(ASM_STAC "\n"
-			     "1:"XSAVE"\n"
-			     "2: " ASM_CLAC "\n"
-			     xstate_fault(err)
-			     : "D" (buf), "a" (-1), "d" (-1), "0" (err)
-			     : "memory");
+	stac();
+	XSTATE_OP(XSAVE, buf, -1, -1, err);
+	clac();
+
 	return err;
 }
 
@@ -405,14 +401,12 @@ static inline int copy_user_to_xregs(struct xregs_state __user *buf, u64 mask)
 	struct xregs_state *xstate = ((__force struct xregs_state *)buf);
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
-	int err = 0;
+	int err;
+
+	stac();
+	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+	clac();
 
-	__asm__ __volatile__(ASM_STAC "\n"
-			     "1:"XRSTOR"\n"
-			     "2: " ASM_CLAC "\n"
-			     xstate_fault(err)
-			     : "D" (xstate), "a" (lmask), "d" (hmask), "0" (err)
-			     : "memory");	/* memory required? */
 	return err;
 }
 

commit 83242c515881cc8642d726c3e648e41bf6c24551
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 12:22:29 2015 +0200

    x86/fpu: Make WARN_ON_FPU() more robust in the !CONFIG_X86_DEBUG_FPU case
    
    Make sure the WARN_ON_FPU() macro consumes the macro argument,
    to avoid 'unused variable' build warnings if the only use of
    a variable is in debugging code.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 9f3863872007..3c3550c3a4a3 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -49,7 +49,7 @@ extern void fpu__resume_cpu(void);
 #ifdef CONFIG_X86_DEBUG_FPU
 # define WARN_ON_FPU(x) WARN_ON_ONCE(x)
 #else
-# define WARN_ON_FPU(x) ({ 0; })
+# define WARN_ON_FPU(x) ({ (void)(x); 0; })
 #endif
 
 /*

commit d65fcd608f6a9ac0316bd3efc75956ca329b6571
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 14:04:44 2015 +0200

    x86/fpu: Simplify copy_kernel_to_xregs_booting()
    
    copy_kernel_to_xregs_booting() has a second parameter that is the mask
    of xfeatures that should be copied - but this parameter is always -1.
    
    Simplify the call site of this function, this also makes it more
    similar to the function call signature of other copy_kernel_to*regs()
    functions.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 12acbb32a561..9f3863872007 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -271,8 +271,9 @@ static inline void copy_xregs_to_kernel_booting(struct xregs_state *xstate)
  * This function is called only during boot time when x86 caps are not set
  * up and alternative can not be used yet.
  */
-static inline void copy_kernel_to_xregs_booting(struct xregs_state *xstate, u64 mask)
+static inline void copy_kernel_to_xregs_booting(struct xregs_state *xstate)
 {
+	u64 mask = -1;
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
 	int err = 0;

commit 003e2e8b57e79709e4973f6cb48381b2ba396680
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 11:59:35 2015 +0200

    x86/fpu: Standardize the parameter type of copy_kernel_to_fpregs()
    
    Bring the __copy_fpstate_to_fpregs() and copy_fpstate_to_fpregs() functions
    in line with the parameter passing convention of other kernel-to-FPU-registers
    copying functions: pass around an in-memory FPU register state pointer,
    instead of struct fpu *.
    
    NOTE: This patch also changes the assembly constraint of the FXSAVE-leak
          workaround from 'fpu->fpregs_active' to 'fpstate' - but that is fine,
          as we only need a valid memory address there for the FILDL instruction.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index da71d41227ff..12acbb32a561 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -446,19 +446,19 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 	return 0;
 }
 
-static inline void __copy_kernel_to_fpregs(struct fpu *fpu)
+static inline void __copy_kernel_to_fpregs(union fpregs_state *fpstate)
 {
 	if (use_xsave()) {
-		copy_kernel_to_xregs(&fpu->state.xsave, -1);
+		copy_kernel_to_xregs(&fpstate->xsave, -1);
 	} else {
 		if (use_fxsr())
-			copy_kernel_to_fxregs(&fpu->state.fxsave);
+			copy_kernel_to_fxregs(&fpstate->fxsave);
 		else
-			copy_kernel_to_fregs(&fpu->state.fsave);
+			copy_kernel_to_fregs(&fpstate->fsave);
 	}
 }
 
-static inline void copy_kernel_to_fpregs(struct fpu *fpu)
+static inline void copy_kernel_to_fpregs(union fpregs_state *fpstate)
 {
 	/*
 	 * AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception is
@@ -470,10 +470,10 @@ static inline void copy_kernel_to_fpregs(struct fpu *fpu)
 			"fnclex\n\t"
 			"emms\n\t"
 			"fildl %P[addr]"	/* set F?P to defined value */
-			: : [addr] "m" (fpu->fpregs_active));
+			: : [addr] "m" (fpstate));
 	}
 
-	__copy_kernel_to_fpregs(fpu);
+	__copy_kernel_to_fpregs(fpstate);
 }
 
 extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size);
@@ -642,7 +642,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)
 {
 	if (fpu_switch.preload)
-		copy_kernel_to_fpregs(new_fpu);
+		copy_kernel_to_fpregs(&new_fpu->state);
 }
 
 /*

commit 9ccc27a5d297503e485373b69688d038a1d8e662
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 11:27:46 2015 +0200

    x86/fpu: Remove error return values from copy_kernel_to_*regs() functions
    
    None of the copy_kernel_to_*regs() FPU register copying functions are
    supposed to fail, and all of them have debugging checks that enforce
    this.
    
    Remove their return values and simplify their call sites, which have
    redundant error checks and error handling code paths.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 6193b7a9cf00..da71d41227ff 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -141,7 +141,7 @@ static inline int copy_fxregs_to_user(struct fxregs_state __user *fx)
 	return user_insn(rex64/fxsave (%[fx]), "=m" (*fx), [fx] "R" (fx));
 }
 
-static inline int copy_kernel_to_fxregs(struct fxregs_state *fx)
+static inline void copy_kernel_to_fxregs(struct fxregs_state *fx)
 {
 	int err;
 
@@ -157,8 +157,6 @@ static inline int copy_kernel_to_fxregs(struct fxregs_state *fx)
 	}
 	/* Copying from a kernel buffer to FPU registers should never fail: */
 	WARN_ON_FPU(err);
-
-	return err;
 }
 
 static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
@@ -173,13 +171,11 @@ static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
 			  "m" (*fx));
 }
 
-static inline int copy_kernel_to_fregs(struct fregs_state *fx)
+static inline void copy_kernel_to_fregs(struct fregs_state *fx)
 {
 	int err = check_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 
 	WARN_ON_FPU(err);
-
-	return err;
 }
 
 static inline int copy_user_to_fregs(struct fregs_state __user *fx)
@@ -450,20 +446,19 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 	return 0;
 }
 
-static inline int __copy_kernel_to_fpregs(struct fpu *fpu)
+static inline void __copy_kernel_to_fpregs(struct fpu *fpu)
 {
 	if (use_xsave()) {
 		copy_kernel_to_xregs(&fpu->state.xsave, -1);
-		return 0;
 	} else {
 		if (use_fxsr())
-			return copy_kernel_to_fxregs(&fpu->state.fxsave);
+			copy_kernel_to_fxregs(&fpu->state.fxsave);
 		else
-			return copy_kernel_to_fregs(&fpu->state.fsave);
+			copy_kernel_to_fregs(&fpu->state.fsave);
 	}
 }
 
-static inline int copy_kernel_to_fpregs(struct fpu *fpu)
+static inline void copy_kernel_to_fpregs(struct fpu *fpu)
 {
 	/*
 	 * AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception is
@@ -478,7 +473,7 @@ static inline int copy_kernel_to_fpregs(struct fpu *fpu)
 			: : [addr] "m" (fpu->fpregs_active));
 	}
 
-	return __copy_kernel_to_fpregs(fpu);
+	__copy_kernel_to_fpregs(fpu);
 }
 
 extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size);
@@ -646,12 +641,8 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
  */
 static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)
 {
-	if (fpu_switch.preload) {
-		if (unlikely(copy_kernel_to_fpregs(new_fpu))) {
-			WARN_ON_FPU(1);
-			fpu__clear(new_fpu);
-		}
-	}
+	if (fpu_switch.preload)
+		copy_kernel_to_fpregs(new_fpu);
 }
 
 /*

commit 3e1bf47e5c81c2b895db4bea67f70c3ca8e5b984
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 13:47:01 2015 +0200

    x86/fpu: Rename copy_fpstate_to_fpregs() to copy_kernel_to_fpregs()
    
    Bring the __copy_fpstate_to_fpregs() and copy_fpstate_to_fpregs() functions
    in line with the naming of other kernel-to-FPU-registers copying functions.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index eb8fa0f9d279..6193b7a9cf00 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -450,7 +450,7 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 	return 0;
 }
 
-static inline int __copy_fpstate_to_fpregs(struct fpu *fpu)
+static inline int __copy_kernel_to_fpregs(struct fpu *fpu)
 {
 	if (use_xsave()) {
 		copy_kernel_to_xregs(&fpu->state.xsave, -1);
@@ -463,7 +463,7 @@ static inline int __copy_fpstate_to_fpregs(struct fpu *fpu)
 	}
 }
 
-static inline int copy_fpstate_to_fpregs(struct fpu *fpu)
+static inline int copy_kernel_to_fpregs(struct fpu *fpu)
 {
 	/*
 	 * AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception is
@@ -478,7 +478,7 @@ static inline int copy_fpstate_to_fpregs(struct fpu *fpu)
 			: : [addr] "m" (fpu->fpregs_active));
 	}
 
-	return __copy_fpstate_to_fpregs(fpu);
+	return __copy_kernel_to_fpregs(fpu);
 }
 
 extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size);
@@ -647,7 +647,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)
 {
 	if (fpu_switch.preload) {
-		if (unlikely(copy_fpstate_to_fpregs(new_fpu))) {
+		if (unlikely(copy_kernel_to_fpregs(new_fpu))) {
 			WARN_ON_FPU(1);
 			fpu__clear(new_fpu);
 		}

commit 43b287b3f4d8665cd5a4909132259b663cc1c0e3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 10:59:31 2015 +0200

    x86/fpu: Add debugging checks to all copy_kernel_to_*() functions
    
    Copying from in-kernel FPU context buffers to FPU registers are
    never supposed to fault.
    
    Add debugging checks to copy_kernel_to_fxregs() and copy_kernel_to_fregs()
    to double check this assumption.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index e3bd93c84928..eb8fa0f9d279 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -143,14 +143,22 @@ static inline int copy_fxregs_to_user(struct fxregs_state __user *fx)
 
 static inline int copy_kernel_to_fxregs(struct fxregs_state *fx)
 {
-	if (config_enabled(CONFIG_X86_32))
-		return check_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
-	else if (config_enabled(CONFIG_AS_FXSAVEQ))
-		return check_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
+	int err;
 
-	/* See comment in copy_fxregs_to_kernel() below. */
-	return check_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx),
-			  "m" (*fx));
+	if (config_enabled(CONFIG_X86_32)) {
+		err = check_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+	} else {
+		if (config_enabled(CONFIG_AS_FXSAVEQ)) {
+			err = check_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
+		} else {
+			/* See comment in copy_fxregs_to_kernel() below. */
+			err = check_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx), "m" (*fx));
+		}
+	}
+	/* Copying from a kernel buffer to FPU registers should never fail: */
+	WARN_ON_FPU(err);
+
+	return err;
 }
 
 static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
@@ -167,7 +175,11 @@ static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
 
 static inline int copy_kernel_to_fregs(struct fregs_state *fx)
 {
-	return check_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+	int err = check_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+
+	WARN_ON_FPU(err);
+
+	return err;
 }
 
 static inline int copy_user_to_fregs(struct fregs_state __user *fx)

commit 6a81d7eb330479c908dab3a47ac33cfca8af5a67
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 12:22:29 2015 +0200

    x86/fpu: Rename fpu__activate_fpstate() to fpu__activate_fpstate_write()
    
    Remaining users of fpu__activate_fpstate() are all places that want to modify
    FPU registers, rename the function to fpu__activate_fpstate_write() according
    to this usage.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 3cc2086b97f8..e3bd93c84928 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -23,7 +23,7 @@
  */
 extern void fpu__activate_curr(struct fpu *fpu);
 extern void fpu__activate_fpstate_read(struct fpu *fpu);
-extern void fpu__activate_fpstate(struct fpu *fpu);
+extern void fpu__activate_fpstate_write(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);

commit 0560281266b313400b622c5ddfafb0ee8e59c702
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 12:22:29 2015 +0200

    x86/fpu: Split out the fpu__activate_fpstate_read() method
    
    Currently fpu__activate_fpstate() is used for two distinct purposes:
    
      - read access by ptrace and core dumping, where in the core dumping
        case the current task's FPU state may be examined as well.
    
      - write access by ptrace, which modifies FPU registers and expects
        the modified registers to be reloaded on the next context switch.
    
    Split out the reading side into fpu__activate_fpstate_read().
    
    ( Note that this is just a pure duplication of fpu__activate_fpstate()
      for the time being, we'll optimize the new function in the next patch. )
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 62d13d515f95..3cc2086b97f8 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -22,6 +22,7 @@
  * High level FPU state handling functions:
  */
 extern void fpu__activate_curr(struct fpu *fpu);
+extern void fpu__activate_fpstate_read(struct fpu *fpu);
 extern void fpu__activate_fpstate(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(struct fpu *fpu);

commit 47f01e8cc23f3d041f6b9fb97627369eaf75ba7f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 12:22:29 2015 +0200

    x86/fpu: Fix FPU register read access to the current task
    
    Bobby Powers reported the following FPU warning during ELF coredumping:
    
       WARNING: CPU: 0 PID: 27452 at arch/x86/kernel/fpu/core.c:324 fpu__activate_stopped+0x8a/0xa0()
    
    This warning unearthed an invalid assumption about fpu__activate_stopped()
    that I added in:
    
      67e97fc2ec57 ("x86/fpu: Rename init_fpu() to fpu__unlazy_stopped() and add debugging check")
    
    the old init_fpu() function had an (intentional but obscure) side effect:
    when FPU registers are accessed for the current task, for reading, then
    it synchronized live in-register FPU state with the fpstate by saving it.
    
    So fix this bug by saving the FPU if we are the current task. We'll
    still warn in fpu__save() if this is called for not yet stopped
    child tasks, so the debugging check is still preserved.
    
    Also rename the function to fpu__activate_fpstate(), because it's not
    exclusively used for stopped tasks, but for the current task as well.
    
    ( Note that this bug calls for a cleaner separation of access-for-read
      and access-for-modification FPU methods, but we'll do that in separate
      patches. )
    
    Reported-by: Bobby Powers <bobbypowers@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 99690bed920a..62d13d515f95 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -22,7 +22,7 @@
  * High level FPU state handling functions:
  */
 extern void fpu__activate_curr(struct fpu *fpu);
-extern void fpu__activate_stopped(struct fpu *fpu);
+extern void fpu__activate_fpstate(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);

commit 8c05f05edb7795ecd1fa95d5d44bc5b22fd85287
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun May 24 09:23:25 2015 +0200

    x86/fpu: Micro-optimize the copy_xregs_to_kernel*() and copy_kernel_to_xregs*() functions
    
    The copy_xregs_to_kernel*() and copy_kernel_to_xregs*() functions are used
    to copy FPU registers to kernel memory and vice versa.
    
    They are never expected to fail, yet they have a return code, mostly because
    that way they can share the assembly macros with the copy*user*() functions.
    
    This error code is then silently ignored by the context switching
    and other code - which made the bug in:
    
      b8c1b8ea7b21 ("x86/fpu: Fix FPU state save area alignment bug")
    
    harder to fix than necessary.
    
    So remove the return values and check for no faults when FPU debugging
    is enabled in the .config.
    
    This improves the eagerfpu context switching fast path by a couple of
    instructions, when FPU debugging is disabled:
    
       ffffffff810407fa:      89 c2                   mov    %eax,%edx
       ffffffff810407fc:      48 0f ae 2f             xrstor64 (%rdi)
       ffffffff81040800:      31 c0                   xor    %eax,%eax
      -ffffffff81040802:      eb 0a                   jmp    ffffffff8104080e <__switch_to+0x321>
      +ffffffff81040802:      eb 16                   jmp    ffffffff8104081a <__switch_to+0x32d>
       ffffffff81040804:      31 c0                   xor    %eax,%eax
       ffffffff81040806:      48 0f ae 8b c0 05 00    fxrstor64 0x5c0(%rbx)
       ffffffff8104080d:      00
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 1352d380bd46..99690bed920a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -232,7 +232,7 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
  * This function is called only during boot time when x86 caps are not set
  * up and alternative can not be used yet.
  */
-static inline int copy_xregs_to_kernel_booting(struct xregs_state *xstate)
+static inline void copy_xregs_to_kernel_booting(struct xregs_state *xstate)
 {
 	u64 mask = -1;
 	u32 lmask = mask;
@@ -253,14 +253,16 @@ static inline int copy_xregs_to_kernel_booting(struct xregs_state *xstate)
 			     xstate_fault(err)
 			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
 			: "memory");
-	return err;
+
+	/* We should never fault when copying to a kernel buffer: */
+	WARN_ON_FPU(err);
 }
 
 /*
  * This function is called only during boot time when x86 caps are not set
  * up and alternative can not be used yet.
  */
-static inline int copy_kernel_to_xregs_booting(struct xregs_state *xstate, u64 mask)
+static inline void copy_kernel_to_xregs_booting(struct xregs_state *xstate, u64 mask)
 {
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
@@ -280,13 +282,15 @@ static inline int copy_kernel_to_xregs_booting(struct xregs_state *xstate, u64 m
 			     xstate_fault(err)
 			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
 			: "memory");
-	return err;
+
+	/* We should never fault when copying from a kernel buffer: */
+	WARN_ON_FPU(err);
 }
 
 /*
  * Save processor xstate to xsave area.
  */
-static inline int copy_xregs_to_kernel(struct xregs_state *xstate)
+static inline void copy_xregs_to_kernel(struct xregs_state *xstate)
 {
 	u64 mask = -1;
 	u32 lmask = mask;
@@ -319,13 +323,14 @@ static inline int copy_xregs_to_kernel(struct xregs_state *xstate)
 		     : "0" (err)
 		     : "memory");
 
-	return err;
+	/* We should never fault when copying to a kernel buffer: */
+	WARN_ON_FPU(err);
 }
 
 /*
  * Restore processor xstate from xsave area.
  */
-static inline int copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
+static inline void copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
 {
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
@@ -347,7 +352,8 @@ static inline int copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
 		     : "0" (err)
 		     : "memory");
 
-	return err;
+	/* We should never fault when copying from a kernel buffer: */
+	WARN_ON_FPU(err);
 }
 
 /*
@@ -433,12 +439,15 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 
 static inline int __copy_fpstate_to_fpregs(struct fpu *fpu)
 {
-	if (use_xsave())
-		return copy_kernel_to_xregs(&fpu->state.xsave, -1);
-	else if (use_fxsr())
-		return copy_kernel_to_fxregs(&fpu->state.fxsave);
-	else
-		return copy_kernel_to_fregs(&fpu->state.fsave);
+	if (use_xsave()) {
+		copy_kernel_to_xregs(&fpu->state.xsave, -1);
+		return 0;
+	} else {
+		if (use_fxsr())
+			return copy_kernel_to_fxregs(&fpu->state.fxsave);
+		else
+			return copy_kernel_to_fregs(&fpu->state.fsave);
+	}
 }
 
 static inline int copy_fpstate_to_fpregs(struct fpu *fpu)

commit 685c9616248c4f0d57e0d81d3236c80bdce1af46
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 11:59:35 2015 +0200

    x86/fpu: Improve the initialization logic of 'err' around xstate_fault() constraints
    
    There's a confusing aspect of how xstate_fault() constraints are
    handled by the FPU register/memory copying functions in
    fpu/internal.h: they use "0" (0) to signal that the asm code
    will not always set 'err' to a valid value.
    
    But 'err' is already initialized to 0 in C code, which is duplicated
    by the asm() constraint. Should the initialization value ever be
    changed, it might become subtly inconsistent with the not too clear
    asm() constraint.
    
    Use 'err' as the value of the input variable instead, to clarify
    this all.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 5370500d479e..1352d380bd46 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -316,7 +316,7 @@ static inline int copy_xregs_to_kernel(struct xregs_state *xstate)
 		"memory");
 	asm volatile("2:\n\t"
 		     xstate_fault(err)
-		     : "0" (0)
+		     : "0" (err)
 		     : "memory");
 
 	return err;
@@ -327,9 +327,9 @@ static inline int copy_xregs_to_kernel(struct xregs_state *xstate)
  */
 static inline int copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
 {
-	int err = 0;
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
+	int err = 0;
 
 	/*
 	 * Use xrstors to restore context if it is enabled. xrstors supports
@@ -344,7 +344,7 @@ static inline int copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
 
 	asm volatile("2:\n"
 		     xstate_fault(err)
-		     : "0" (0)
+		     : "0" (err)
 		     : "memory");
 
 	return err;
@@ -376,7 +376,7 @@ static inline int copy_xregs_to_user(struct xregs_state __user *buf)
 			     "1:"XSAVE"\n"
 			     "2: " ASM_CLAC "\n"
 			     xstate_fault(err)
-			     : "D" (buf), "a" (-1), "d" (-1), "0" (0)
+			     : "D" (buf), "a" (-1), "d" (-1), "0" (err)
 			     : "memory");
 	return err;
 }
@@ -386,16 +386,16 @@ static inline int copy_xregs_to_user(struct xregs_state __user *buf)
  */
 static inline int copy_user_to_xregs(struct xregs_state __user *buf, u64 mask)
 {
-	int err = 0;
 	struct xregs_state *xstate = ((__force struct xregs_state *)buf);
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
+	int err = 0;
 
 	__asm__ __volatile__(ASM_STAC "\n"
 			     "1:"XRSTOR"\n"
 			     "2: " ASM_CLAC "\n"
 			     xstate_fault(err)
-			     : "D" (xstate), "a" (lmask), "d" (hmask), "0" (0)
+			     : "D" (xstate), "a" (lmask), "d" (hmask), "0" (err)
 			     : "memory");	/* memory required? */
 	return err;
 }

commit 87b6559d0a37cd82b4b2ffe38f88c0d4ac6ee7e2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 11:27:46 2015 +0200

    x86/fpu: Improve xstate_fault() handling
    
    There are two problems with xstate_fault handling:
    
     - The xstate_fault() macro takes an argument, but that's
       propagated into the assembly named label as well. This
       is technically correct currently but might result in
       failures if anytime a more complex argument is used.
       So use a separate '_err' name instead for the label.
    
     - All the xstate_fault() using functions have an error
       variable named 'err', which is an output variable to
       the asm() they are using. The problem is, it's not always
       set by the asm(), in which case the compiler might
       optimize out its initialization, so that the C variable
       'err' might become corrupted after the asm() - confusing
       anyone who tries to take advantage of this variable
       after the asm(). Mark it an input variable as well.
    
       This is a latent bug currently, but an upcoming debug
       patch will make use of 'err'.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d142ecb067b8..5370500d479e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -220,13 +220,13 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 					\
 	".section .fixup,\"ax\"\n"	\
 					\
-	"3:  movl $-1,%[err]\n"		\
+	"3:  movl $-2,%[_err]\n"	\
 	"    jmp  2b\n"			\
 					\
 	".previous\n"			\
 					\
 	_ASM_EXTABLE(1b, 3b)		\
-	: [err] "=r" (__err)
+	: [_err] "=r" (__err)
 
 /*
  * This function is called only during boot time when x86 caps are not set
@@ -245,14 +245,14 @@ static inline int copy_xregs_to_kernel_booting(struct xregs_state *xstate)
 		asm volatile("1:"XSAVES"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
-			:   "memory");
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
+			: "memory");
 	else
 		asm volatile("1:"XSAVE"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
-			:   "memory");
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
+			: "memory");
 	return err;
 }
 
@@ -272,14 +272,14 @@ static inline int copy_kernel_to_xregs_booting(struct xregs_state *xstate, u64 m
 		asm volatile("1:"XRSTORS"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
-			:   "memory");
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
+			: "memory");
 	else
 		asm volatile("1:"XRSTOR"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
-			:   "memory");
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask), "0" (err)
+			: "memory");
 	return err;
 }
 

commit 87dafd41a4423c6f730e5f4b0d56a1aa3fdcf3fc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 10:57:06 2015 +0200

    x86/fpu: Rename xstate related 'fx' references to 'xstate'
    
    So the xstate code was probably first copied from the fxregs code,
    hence it carried over the 'fx' naming for the state pointer variable.
    
    But this is slightly confusing, as we usually on call the (legacy)
    MMX/SSE state 'fx', both in data structures and in the functions
    build around FXSAVE/FXRSTOR.
    
    So rename it to 'xstate' to make it more apparent what it is related to.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index bcc4173a888a..d142ecb067b8 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -232,7 +232,7 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
  * This function is called only during boot time when x86 caps are not set
  * up and alternative can not be used yet.
  */
-static inline int copy_xregs_to_kernel_booting(struct xregs_state *fx)
+static inline int copy_xregs_to_kernel_booting(struct xregs_state *xstate)
 {
 	u64 mask = -1;
 	u32 lmask = mask;
@@ -245,13 +245,13 @@ static inline int copy_xregs_to_kernel_booting(struct xregs_state *fx)
 		asm volatile("1:"XSAVES"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
 			:   "memory");
 	else
 		asm volatile("1:"XSAVE"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
 			:   "memory");
 	return err;
 }
@@ -260,7 +260,7 @@ static inline int copy_xregs_to_kernel_booting(struct xregs_state *fx)
  * This function is called only during boot time when x86 caps are not set
  * up and alternative can not be used yet.
  */
-static inline int copy_kernel_to_xregs_booting(struct xregs_state *fx, u64 mask)
+static inline int copy_kernel_to_xregs_booting(struct xregs_state *xstate, u64 mask)
 {
 	u32 lmask = mask;
 	u32 hmask = mask >> 32;
@@ -272,13 +272,13 @@ static inline int copy_kernel_to_xregs_booting(struct xregs_state *fx, u64 mask)
 		asm volatile("1:"XRSTORS"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
 			:   "memory");
 	else
 		asm volatile("1:"XRSTOR"\n\t"
 			"2:\n\t"
 			     xstate_fault(err)
-			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			: "D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
 			:   "memory");
 	return err;
 }
@@ -286,7 +286,7 @@ static inline int copy_kernel_to_xregs_booting(struct xregs_state *fx, u64 mask)
 /*
  * Save processor xstate to xsave area.
  */
-static inline int copy_xregs_to_kernel(struct xregs_state *fx)
+static inline int copy_xregs_to_kernel(struct xregs_state *xstate)
 {
 	u64 mask = -1;
 	u32 lmask = mask;
@@ -312,7 +312,7 @@ static inline int copy_xregs_to_kernel(struct xregs_state *fx)
 		X86_FEATURE_XSAVEOPT,
 		XSAVES,
 		X86_FEATURE_XSAVES,
-		[fx] "D" (fx), "a" (lmask), "d" (hmask) :
+		[xstate] "D" (xstate), "a" (lmask), "d" (hmask) :
 		"memory");
 	asm volatile("2:\n\t"
 		     xstate_fault(err)
@@ -325,7 +325,7 @@ static inline int copy_xregs_to_kernel(struct xregs_state *fx)
 /*
  * Restore processor xstate from xsave area.
  */
-static inline int copy_kernel_to_xregs(struct xregs_state *fx, u64 mask)
+static inline int copy_kernel_to_xregs(struct xregs_state *xstate, u64 mask)
 {
 	int err = 0;
 	u32 lmask = mask;
@@ -339,7 +339,7 @@ static inline int copy_kernel_to_xregs(struct xregs_state *fx, u64 mask)
 		"1: " XRSTOR,
 		XRSTORS,
 		X86_FEATURE_XSAVES,
-		"D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+		"D" (xstate), "m" (*xstate), "a" (lmask), "d" (hmask)
 		: "memory");
 
 	asm volatile("2:\n"
@@ -426,7 +426,7 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 	 * Legacy FPU register saving, FNSAVE always clears FPU registers,
 	 * so we have to mark them inactive:
 	 */
-	asm volatile("fnsave %[fx]; fwait" : [fx] "=m" (fpu->state.fsave));
+	asm volatile("fnsave %[fp]; fwait" : [fp] "=m" (fpu->state.fsave));
 
 	return 0;
 }
@@ -459,7 +459,7 @@ static inline int copy_fpstate_to_fpregs(struct fpu *fpu)
 	return __copy_fpstate_to_fpregs(fpu);
 }
 
-extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fx, int size);
+extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size);
 
 /*
  * FPU context switch related helper methods:

commit fd169b05415485bf47606d820194c39cb2f53201
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 25 09:55:39 2015 +0200

    x86/fpu: Move the xstate copying functions into fpu/internal.h
    
    All the other register<-> memory copying functions are defined
    in fpu/internal.h, so move the xstate variants there too.
    
    Beyond being more consistent, this also allows FPU debugging
    checks to be added to them. (Because they can now use the
    macros defined in fpu/internal.h.)
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a98a08d1efa9..bcc4173a888a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -208,6 +208,198 @@ static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 	}
 }
 
+/* These macros all use (%edi)/(%rdi) as the single memory argument. */
+#define XSAVE		".byte " REX_PREFIX "0x0f,0xae,0x27"
+#define XSAVEOPT	".byte " REX_PREFIX "0x0f,0xae,0x37"
+#define XSAVES		".byte " REX_PREFIX "0x0f,0xc7,0x2f"
+#define XRSTOR		".byte " REX_PREFIX "0x0f,0xae,0x2f"
+#define XRSTORS		".byte " REX_PREFIX "0x0f,0xc7,0x1f"
+
+/* xstate instruction fault handler: */
+#define xstate_fault(__err)		\
+					\
+	".section .fixup,\"ax\"\n"	\
+					\
+	"3:  movl $-1,%[err]\n"		\
+	"    jmp  2b\n"			\
+					\
+	".previous\n"			\
+					\
+	_ASM_EXTABLE(1b, 3b)		\
+	: [err] "=r" (__err)
+
+/*
+ * This function is called only during boot time when x86 caps are not set
+ * up and alternative can not be used yet.
+ */
+static inline int copy_xregs_to_kernel_booting(struct xregs_state *fx)
+{
+	u64 mask = -1;
+	u32 lmask = mask;
+	u32 hmask = mask >> 32;
+	int err = 0;
+
+	WARN_ON(system_state != SYSTEM_BOOTING);
+
+	if (boot_cpu_has(X86_FEATURE_XSAVES))
+		asm volatile("1:"XSAVES"\n\t"
+			"2:\n\t"
+			     xstate_fault(err)
+			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			:   "memory");
+	else
+		asm volatile("1:"XSAVE"\n\t"
+			"2:\n\t"
+			     xstate_fault(err)
+			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			:   "memory");
+	return err;
+}
+
+/*
+ * This function is called only during boot time when x86 caps are not set
+ * up and alternative can not be used yet.
+ */
+static inline int copy_kernel_to_xregs_booting(struct xregs_state *fx, u64 mask)
+{
+	u32 lmask = mask;
+	u32 hmask = mask >> 32;
+	int err = 0;
+
+	WARN_ON(system_state != SYSTEM_BOOTING);
+
+	if (boot_cpu_has(X86_FEATURE_XSAVES))
+		asm volatile("1:"XRSTORS"\n\t"
+			"2:\n\t"
+			     xstate_fault(err)
+			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			:   "memory");
+	else
+		asm volatile("1:"XRSTOR"\n\t"
+			"2:\n\t"
+			     xstate_fault(err)
+			: "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+			:   "memory");
+	return err;
+}
+
+/*
+ * Save processor xstate to xsave area.
+ */
+static inline int copy_xregs_to_kernel(struct xregs_state *fx)
+{
+	u64 mask = -1;
+	u32 lmask = mask;
+	u32 hmask = mask >> 32;
+	int err = 0;
+
+	WARN_ON(!alternatives_patched);
+
+	/*
+	 * If xsaves is enabled, xsaves replaces xsaveopt because
+	 * it supports compact format and supervisor states in addition to
+	 * modified optimization in xsaveopt.
+	 *
+	 * Otherwise, if xsaveopt is enabled, xsaveopt replaces xsave
+	 * because xsaveopt supports modified optimization which is not
+	 * supported by xsave.
+	 *
+	 * If none of xsaves and xsaveopt is enabled, use xsave.
+	 */
+	alternative_input_2(
+		"1:"XSAVE,
+		XSAVEOPT,
+		X86_FEATURE_XSAVEOPT,
+		XSAVES,
+		X86_FEATURE_XSAVES,
+		[fx] "D" (fx), "a" (lmask), "d" (hmask) :
+		"memory");
+	asm volatile("2:\n\t"
+		     xstate_fault(err)
+		     : "0" (0)
+		     : "memory");
+
+	return err;
+}
+
+/*
+ * Restore processor xstate from xsave area.
+ */
+static inline int copy_kernel_to_xregs(struct xregs_state *fx, u64 mask)
+{
+	int err = 0;
+	u32 lmask = mask;
+	u32 hmask = mask >> 32;
+
+	/*
+	 * Use xrstors to restore context if it is enabled. xrstors supports
+	 * compacted format of xsave area which is not supported by xrstor.
+	 */
+	alternative_input(
+		"1: " XRSTOR,
+		XRSTORS,
+		X86_FEATURE_XSAVES,
+		"D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
+		: "memory");
+
+	asm volatile("2:\n"
+		     xstate_fault(err)
+		     : "0" (0)
+		     : "memory");
+
+	return err;
+}
+
+/*
+ * Save xstate to user space xsave area.
+ *
+ * We don't use modified optimization because xrstor/xrstors might track
+ * a different application.
+ *
+ * We don't use compacted format xsave area for
+ * backward compatibility for old applications which don't understand
+ * compacted format of xsave area.
+ */
+static inline int copy_xregs_to_user(struct xregs_state __user *buf)
+{
+	int err;
+
+	/*
+	 * Clear the xsave header first, so that reserved fields are
+	 * initialized to zero.
+	 */
+	err = __clear_user(&buf->header, sizeof(buf->header));
+	if (unlikely(err))
+		return -EFAULT;
+
+	__asm__ __volatile__(ASM_STAC "\n"
+			     "1:"XSAVE"\n"
+			     "2: " ASM_CLAC "\n"
+			     xstate_fault(err)
+			     : "D" (buf), "a" (-1), "d" (-1), "0" (0)
+			     : "memory");
+	return err;
+}
+
+/*
+ * Restore xstate from user space xsave area.
+ */
+static inline int copy_user_to_xregs(struct xregs_state __user *buf, u64 mask)
+{
+	int err = 0;
+	struct xregs_state *xstate = ((__force struct xregs_state *)buf);
+	u32 lmask = mask;
+	u32 hmask = mask >> 32;
+
+	__asm__ __volatile__(ASM_STAC "\n"
+			     "1:"XRSTOR"\n"
+			     "2: " ASM_CLAC "\n"
+			     xstate_fault(err)
+			     : "D" (xstate), "a" (lmask), "d" (hmask), "0" (0)
+			     : "memory");	/* memory required? */
+	return err;
+}
+
 /*
  * These must be called with preempt disabled. Returns
  * 'true' if the FPU state is still intact and we can

commit b1b64dc3558b7bde2917f9fc4f573f6a4787a95c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue May 5 15:56:33 2015 +0200

    x86/fpu: Reorganize fpu/internal.h
    
    fpu/internal.h has grown organically, with not much high level structure,
    which hurts its readability.
    
    Organize the various definitions into 5 sections:
    
     - high level FPU state functions
     - FPU/CPU feature flag helpers
     - fpstate handling functions
     - FPU context switching helpers
     - misc helper functions
    
    Other related changes:
    
     - Move MXCSR_DEFAULT to fpu/types.h.
     - drop the unused X87_FSW_ES define
    
    No change in functionality.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d2a281bd5f45..a98a08d1efa9 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -18,32 +18,6 @@
 #include <asm/fpu/api.h>
 #include <asm/fpu/xstate.h>
 
-#define	MXCSR_DEFAULT		0x1f80
-
-extern unsigned int mxcsr_feature_mask;
-
-extern union fpregs_state init_fpstate;
-
-extern void fpu__init_cpu(void);
-extern void fpu__init_system_xstate(void);
-extern void fpu__init_cpu_xstate(void);
-extern void fpu__init_system(struct cpuinfo_x86 *c);
-
-extern void fpstate_init(union fpregs_state *state);
-#ifdef CONFIG_MATH_EMULATION
-extern void fpstate_init_soft(struct swregs_state *soft);
-#else
-static inline void fpstate_init_soft(struct swregs_state *soft) {}
-#endif
-static inline void fpstate_init_fxstate(struct fxregs_state *fx)
-{
-	fx->cwd = 0x37f;
-	fx->mxcsr = MXCSR_DEFAULT;
-}
-
-extern int  dump_fpu(struct pt_regs *, struct user_i387_struct *);
-extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
-
 /*
  * High level FPU state handling functions:
  */
@@ -55,7 +29,16 @@ extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
 extern void fpu__drop(struct fpu *fpu);
 extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
 extern void fpu__clear(struct fpu *fpu);
+extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
+extern int  dump_fpu(struct pt_regs *ptregs, struct user_i387_struct *fpstate);
 
+/*
+ * Boot time FPU initialization functions:
+ */
+extern void fpu__init_cpu(void);
+extern void fpu__init_system_xstate(void);
+extern void fpu__init_cpu_xstate(void);
+extern void fpu__init_system(struct cpuinfo_x86 *c);
 extern void fpu__init_check_bugs(void);
 extern void fpu__resume_cpu(void);
 
@@ -68,27 +51,9 @@ extern void fpu__resume_cpu(void);
 # define WARN_ON_FPU(x) ({ 0; })
 #endif
 
-DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
-
 /*
- * Must be run with preemption disabled: this clears the fpu_fpregs_owner_ctx,
- * on this CPU.
- *
- * This will disable any lazy FPU state restore of the current FPU state,
- * but if the current thread owns the FPU, it will still be saved by.
+ * FPU related CPU feature flag helper routines:
  */
-static inline void __cpu_disable_lazy_restore(unsigned int cpu)
-{
-	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;
-}
-
-static inline int fpu_want_lazy_restore(struct fpu *fpu, unsigned int cpu)
-{
-	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
-}
-
-#define X87_FSW_ES (1 << 7)	/* Exception Summary */
-
 static __always_inline __pure bool use_eager_fpu(void)
 {
 	return static_cpu_has_safe(X86_FEATURE_EAGER_FPU);
@@ -109,6 +74,23 @@ static __always_inline __pure bool use_fxsr(void)
 	return static_cpu_has_safe(X86_FEATURE_FXSR);
 }
 
+/*
+ * fpstate handling functions:
+ */
+
+extern union fpregs_state init_fpstate;
+
+extern void fpstate_init(union fpregs_state *state);
+#ifdef CONFIG_MATH_EMULATION
+extern void fpstate_init_soft(struct swregs_state *soft);
+#else
+static inline void fpstate_init_soft(struct swregs_state *soft) {}
+#endif
+static inline void fpstate_init_fxstate(struct fxregs_state *fx)
+{
+	fx->cwd = 0x37f;
+	fx->mxcsr = MXCSR_DEFAULT;
+}
 extern void fpstate_sanitize_xstate(struct fpu *fpu);
 
 #define user_insn(insn, output, input...)				\
@@ -285,6 +267,32 @@ static inline int copy_fpstate_to_fpregs(struct fpu *fpu)
 	return __copy_fpstate_to_fpregs(fpu);
 }
 
+extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fx, int size);
+
+/*
+ * FPU context switch related helper methods:
+ */
+
+DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
+
+/*
+ * Must be run with preemption disabled: this clears the fpu_fpregs_owner_ctx,
+ * on this CPU.
+ *
+ * This will disable any lazy FPU state restore of the current FPU state,
+ * but if the current thread owns the FPU, it will still be saved by.
+ */
+static inline void __cpu_disable_lazy_restore(unsigned int cpu)
+{
+	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;
+}
+
+static inline int fpu_want_lazy_restore(struct fpu *fpu, unsigned int cpu)
+{
+	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
+}
+
+
 /*
  * Wrap lazy FPU TS handling in a 'hw fpregs activation/deactivation'
  * idiom, which is then paired with the sw-flag (fpregs_active) later on:
@@ -354,31 +362,6 @@ static inline void fpregs_deactivate(struct fpu *fpu)
 	__fpregs_deactivate_hw();
 }
 
-/*
- * Definitions for the eXtended Control Register instructions
- */
-
-#define XCR_XFEATURE_ENABLED_MASK	0x00000000
-
-static inline u64 xgetbv(u32 index)
-{
-	u32 eax, edx;
-
-	asm volatile(".byte 0x0f,0x01,0xd0" /* xgetbv */
-		     : "=a" (eax), "=d" (edx)
-		     : "c" (index));
-	return eax + ((u64)edx << 32);
-}
-
-static inline void xsetbv(u32 index, u64 value)
-{
-	u32 eax = value;
-	u32 edx = value >> 32;
-
-	asm volatile(".byte 0x0f,0x01,0xd1" /* xsetbv */
-		     : : "a" (eax), "d" (edx), "c" (index));
-}
-
 /*
  * FPU state switching for scheduling.
  *
@@ -437,6 +420,10 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 	return fpu;
 }
 
+/*
+ * Misc helper functions:
+ */
+
 /*
  * By the time this gets called, we've already cleared CR0.TS and
  * given the process the FPU if we are going to preload the FPU
@@ -453,11 +440,6 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switc
 	}
 }
 
-/*
- * Signal frame handlers...
- */
-extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fx, int size);
-
 /*
  * Needs to be preemption-safe.
  *
@@ -476,4 +458,31 @@ static inline void user_fpu_begin(void)
 	preempt_enable();
 }
 
+/*
+ * MXCSR and XCR definitions:
+ */
+
+extern unsigned int mxcsr_feature_mask;
+
+#define XCR_XFEATURE_ENABLED_MASK	0x00000000
+
+static inline u64 xgetbv(u32 index)
+{
+	u32 eax, edx;
+
+	asm volatile(".byte 0x0f,0x01,0xd0" /* xgetbv */
+		     : "=a" (eax), "=d" (edx)
+		     : "c" (index));
+	return eax + ((u64)edx << 32);
+}
+
+static inline void xsetbv(u32 index, u64 value)
+{
+	u32 eax = value;
+	u32 edx = value >> 32;
+
+	asm volatile(".byte 0x0f,0x01,0xd1" /* xsetbv */
+		     : : "a" (eax), "d" (edx), "c" (index));
+}
+
 #endif /* _ASM_X86_FPU_INTERNAL_H */

commit e97131a8391e9fce5126ed54dc66c9d8965d3b4e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue May 5 11:34:49 2015 +0200

    x86/fpu: Add CONFIG_X86_DEBUG_FPU=y FPU debugging code
    
    There are various internal FPU state debugging checks that never
    trigger in practice, but which are useful for FPU code development.
    
    Separate these out into CONFIG_X86_DEBUG_FPU=y, and also add a
    couple of new ones.
    
    The size difference is about 0.5K of code on defconfig:
    
       text        data     bss          filename
       15028906    2578816  1638400      vmlinux
       15029430    2578816  1638400      vmlinux
    
    ( Keep this enabled by default until the new FPU code is debugged. )
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a4c1b7dbf70e..d2a281bd5f45 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -59,6 +59,15 @@ extern void fpu__clear(struct fpu *fpu);
 extern void fpu__init_check_bugs(void);
 extern void fpu__resume_cpu(void);
 
+/*
+ * Debugging facility:
+ */
+#ifdef CONFIG_X86_DEBUG_FPU
+# define WARN_ON_FPU(x) WARN_ON_ONCE(x)
+#else
+# define WARN_ON_FPU(x) ({ 0; })
+#endif
+
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
 /*
@@ -296,6 +305,8 @@ static inline void __fpregs_deactivate_hw(void)
 /* Must be paired with an 'stts' (fpregs_deactivate_hw()) after! */
 static inline void __fpregs_deactivate(struct fpu *fpu)
 {
+	WARN_ON_FPU(!fpu->fpregs_active);
+
 	fpu->fpregs_active = 0;
 	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
 }
@@ -303,6 +314,8 @@ static inline void __fpregs_deactivate(struct fpu *fpu)
 /* Must be paired with a 'clts' (fpregs_activate_hw()) before! */
 static inline void __fpregs_activate(struct fpu *fpu)
 {
+	WARN_ON_FPU(fpu->fpregs_active);
+
 	fpu->fpregs_active = 1;
 	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
 }
@@ -433,8 +446,10 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)
 {
 	if (fpu_switch.preload) {
-		if (unlikely(copy_fpstate_to_fpregs(new_fpu)))
+		if (unlikely(copy_fpstate_to_fpregs(new_fpu))) {
+			WARN_ON_FPU(1);
 			fpu__clear(new_fpu);
+		}
 	}
 }
 

commit e1884d69f643c743806ebb9bc9292863ef39e894
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 4 11:49:58 2015 +0200

    x86/fpu: Pass 'struct fpu' to fpu__restore()
    
    This cleans up the call sites and the function a bit,
    and also makes it more symmetric with the other high
    level FPU state handling functions.
    
    It's still only valid for the current task, as we copy
    to the FPU registers of the current CPU.
    
    No change in functionality.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 7fdc90b9dd86..a4c1b7dbf70e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -50,7 +50,7 @@ extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
 extern void fpu__activate_curr(struct fpu *fpu);
 extern void fpu__activate_stopped(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
-extern void fpu__restore(void);
+extern void fpu__restore(struct fpu *fpu);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
 extern void fpu__drop(struct fpu *fpu);
 extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);

commit c47ada305de3803517ae64aa50686f644c5456fa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 17:15:32 2015 +0200

    x86/fpu: Harmonize FPU register state types
    
    Use these consistent names:
    
        struct fregs_state           # was: i387_fsave_struct
        struct fxregs_state          # was: i387_fxsave_struct
        struct swregs_state          # was: i387_soft_struct
        struct xregs_state           # was: xsave_struct
        union  fpregs_state          # was: thread_xstate
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index db6c24ba6d3d..7fdc90b9dd86 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -22,20 +22,20 @@
 
 extern unsigned int mxcsr_feature_mask;
 
-extern union thread_xstate init_fpstate;
+extern union fpregs_state init_fpstate;
 
 extern void fpu__init_cpu(void);
 extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 
-extern void fpstate_init(union thread_xstate *state);
+extern void fpstate_init(union fpregs_state *state);
 #ifdef CONFIG_MATH_EMULATION
-extern void fpstate_init_soft(struct i387_soft_struct *soft);
+extern void fpstate_init_soft(struct swregs_state *soft);
 #else
-static inline void fpstate_init_soft(struct i387_soft_struct *soft) {}
+static inline void fpstate_init_soft(struct swregs_state *soft) {}
 #endif
-static inline void fpstate_init_fxstate(struct i387_fxsave_struct *fx)
+static inline void fpstate_init_fxstate(struct fxregs_state *fx)
 {
 	fx->cwd = 0x37f;
 	fx->mxcsr = MXCSR_DEFAULT;
@@ -133,12 +133,12 @@ extern void fpstate_sanitize_xstate(struct fpu *fpu);
 	err;								\
 })
 
-static inline int copy_fregs_to_user(struct i387_fsave_struct __user *fx)
+static inline int copy_fregs_to_user(struct fregs_state __user *fx)
 {
 	return user_insn(fnsave %[fx]; fwait,  [fx] "=m" (*fx), "m" (*fx));
 }
 
-static inline int copy_fxregs_to_user(struct i387_fxsave_struct __user *fx)
+static inline int copy_fxregs_to_user(struct fxregs_state __user *fx)
 {
 	if (config_enabled(CONFIG_X86_32))
 		return user_insn(fxsave %[fx], [fx] "=m" (*fx), "m" (*fx));
@@ -149,7 +149,7 @@ static inline int copy_fxregs_to_user(struct i387_fxsave_struct __user *fx)
 	return user_insn(rex64/fxsave (%[fx]), "=m" (*fx), [fx] "R" (fx));
 }
 
-static inline int copy_kernel_to_fxregs(struct i387_fxsave_struct *fx)
+static inline int copy_kernel_to_fxregs(struct fxregs_state *fx)
 {
 	if (config_enabled(CONFIG_X86_32))
 		return check_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
@@ -161,7 +161,7 @@ static inline int copy_kernel_to_fxregs(struct i387_fxsave_struct *fx)
 			  "m" (*fx));
 }
 
-static inline int copy_user_to_fxregs(struct i387_fxsave_struct __user *fx)
+static inline int copy_user_to_fxregs(struct fxregs_state __user *fx)
 {
 	if (config_enabled(CONFIG_X86_32))
 		return user_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
@@ -173,12 +173,12 @@ static inline int copy_user_to_fxregs(struct i387_fxsave_struct __user *fx)
 			  "m" (*fx));
 }
 
-static inline int copy_kernel_to_fregs(struct i387_fsave_struct *fx)
+static inline int copy_kernel_to_fregs(struct fregs_state *fx)
 {
 	return check_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 }
 
-static inline int copy_user_to_fregs(struct i387_fsave_struct __user *fx)
+static inline int copy_user_to_fregs(struct fregs_state __user *fx)
 {
 	return user_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 }

commit 0c306bcfba288296dc34d00d514546915234bc90
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 12:59:30 2015 +0200

    x86/fpu: Factor out the FPU regset code into fpu/regset.c
    
    So much of fpu/core.c is the regset code, but it just obscures the generic
    FPU state machine logic. Factor out the regset code into fpu/regset.c, where
    it can be read in isolation.
    
    This affects one API: fpu__activate_stopped() has to be made available
    from the core to fpu/regset.c.
    
    No change in functionality.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index f23ea10d3a1f..db6c24ba6d3d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -29,8 +29,6 @@ extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 
-extern void fpu__activate_curr(struct fpu *fpu);
-
 extern void fpstate_init(union thread_xstate *state);
 #ifdef CONFIG_MATH_EMULATION
 extern void fpstate_init_soft(struct i387_soft_struct *soft);
@@ -49,6 +47,8 @@ extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
 /*
  * High level FPU state handling functions:
  */
+extern void fpu__activate_curr(struct fpu *fpu);
+extern void fpu__activate_stopped(struct fpu *fpu);
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(void);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);

commit c681314421c7c70c418190f3b4ffb4d3257ea5be
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 11:34:09 2015 +0200

    x86/fpu: Rename all the fpregs, xregs, fxregs and fregs handling functions
    
    Standardize the naming of the various functions that copy register
    content in specific FPU context formats:
    
      copy_fxregs_to_kernel()         # was: fpu_fxsave()
      copy_xregs_to_kernel()          # was: xsave_state()
    
      copy_kernel_to_fregs()          # was: frstor_checking()
      copy_kernel_to_fxregs()         # was: fxrstor_checking()
      copy_kernel_to_xregs()          # was: fpu_xrstor_checking()
      copy_kernel_to_xregs_booting()  # was: xrstor_state_booting()
    
      copy_fregs_to_user()            # was: fsave_user()
      copy_fxregs_to_user()           # was: fxsave_user()
      copy_xregs_to_user()            # was: xsave_user()
    
      copy_user_to_fregs()            # was: frstor_user()
      copy_user_to_fxregs()           # was: fxrstor_user()
      copy_user_to_xregs()            # was: xrestore_user()
      copy_user_to_fpregs_zeroing()   # was: restore_user_xstate()
    
    Eliminate fpu_xrstor_checking(), because it was just a wrapper.
    
    No change in functionality.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a1810eb39afa..f23ea10d3a1f 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -133,57 +133,57 @@ extern void fpstate_sanitize_xstate(struct fpu *fpu);
 	err;								\
 })
 
-static inline int fsave_user(struct i387_fsave_struct __user *fx)
+static inline int copy_fregs_to_user(struct i387_fsave_struct __user *fx)
 {
 	return user_insn(fnsave %[fx]; fwait,  [fx] "=m" (*fx), "m" (*fx));
 }
 
-static inline int fxsave_user(struct i387_fxsave_struct __user *fx)
+static inline int copy_fxregs_to_user(struct i387_fxsave_struct __user *fx)
 {
 	if (config_enabled(CONFIG_X86_32))
 		return user_insn(fxsave %[fx], [fx] "=m" (*fx), "m" (*fx));
 	else if (config_enabled(CONFIG_AS_FXSAVEQ))
 		return user_insn(fxsaveq %[fx], [fx] "=m" (*fx), "m" (*fx));
 
-	/* See comment in fpu_fxsave() below. */
+	/* See comment in copy_fxregs_to_kernel() below. */
 	return user_insn(rex64/fxsave (%[fx]), "=m" (*fx), [fx] "R" (fx));
 }
 
-static inline int fxrstor_checking(struct i387_fxsave_struct *fx)
+static inline int copy_kernel_to_fxregs(struct i387_fxsave_struct *fx)
 {
 	if (config_enabled(CONFIG_X86_32))
 		return check_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 	else if (config_enabled(CONFIG_AS_FXSAVEQ))
 		return check_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
 
-	/* See comment in fpu_fxsave() below. */
+	/* See comment in copy_fxregs_to_kernel() below. */
 	return check_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx),
 			  "m" (*fx));
 }
 
-static inline int fxrstor_user(struct i387_fxsave_struct __user *fx)
+static inline int copy_user_to_fxregs(struct i387_fxsave_struct __user *fx)
 {
 	if (config_enabled(CONFIG_X86_32))
 		return user_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 	else if (config_enabled(CONFIG_AS_FXSAVEQ))
 		return user_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
 
-	/* See comment in fpu_fxsave() below. */
+	/* See comment in copy_fxregs_to_kernel() below. */
 	return user_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx),
 			  "m" (*fx));
 }
 
-static inline int frstor_checking(struct i387_fsave_struct *fx)
+static inline int copy_kernel_to_fregs(struct i387_fsave_struct *fx)
 {
 	return check_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 }
 
-static inline int frstor_user(struct i387_fsave_struct __user *fx)
+static inline int copy_user_to_fregs(struct i387_fsave_struct __user *fx)
 {
 	return user_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
 }
 
-static inline void fpu_fxsave(struct fpu *fpu)
+static inline void copy_fxregs_to_kernel(struct fpu *fpu)
 {
 	if (config_enabled(CONFIG_X86_32))
 		asm volatile( "fxsave %[fx]" : [fx] "=m" (fpu->state.fxsave));
@@ -230,12 +230,12 @@ static inline void fpu_fxsave(struct fpu *fpu)
 static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 {
 	if (likely(use_xsave())) {
-		xsave_state(&fpu->state.xsave);
+		copy_xregs_to_kernel(&fpu->state.xsave);
 		return 1;
 	}
 
 	if (likely(use_fxsr())) {
-		fpu_fxsave(fpu);
+		copy_fxregs_to_kernel(fpu);
 		return 1;
 	}
 
@@ -251,11 +251,11 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 static inline int __copy_fpstate_to_fpregs(struct fpu *fpu)
 {
 	if (use_xsave())
-		return fpu_xrstor_checking(&fpu->state.xsave);
+		return copy_kernel_to_xregs(&fpu->state.xsave, -1);
 	else if (use_fxsr())
-		return fxrstor_checking(&fpu->state.fxsave);
+		return copy_kernel_to_fxregs(&fpu->state.fxsave);
 	else
-		return frstor_checking(&fpu->state.fsave);
+		return copy_kernel_to_fregs(&fpu->state.fsave);
 }
 
 static inline int copy_fpstate_to_fpregs(struct fpu *fpu)

commit 815418890e2a3984d8b04c433072df1a42573f96
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 11:21:59 2015 +0200

    x86/fpu: Move restore_init_xstate() out of fpu/internal.h
    
    Move restore_init_xstate() next to its sole caller.
    
    Also rename it to copy_init_fpstate_to_fpregs() and add
    some comments about what it does.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 792fdbe64179..a1810eb39afa 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -341,14 +341,6 @@ static inline void fpregs_deactivate(struct fpu *fpu)
 	__fpregs_deactivate_hw();
 }
 
-static inline void restore_init_xstate(void)
-{
-	if (use_xsave())
-		xrstor_state(&init_fpstate.xsave, -1);
-	else
-		fxrstor_checking(&init_fpstate.fxsave);
-}
-
 /*
  * Definitions for the eXtended Control Register instructions
  */

commit 6f57502310c85b60bdea78228e9b5bb3e82dc3b7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 11:07:06 2015 +0200

    x86/fpu: Generalize 'init_xstate_ctx'
    
    So the handling of init_xstate_ctx has a layering violation: both
    'struct xsave_struct' and 'union thread_xstate' have a
    'struct i387_fxsave_struct' member:
    
       xsave_struct::i387
       thread_xstate::fxsave
    
    The handling of init_xstate_ctx is generic, it is used on all
    CPUs, with or without XSAVE instruction. So it's confusing how
    the generic code passes around and handles an XSAVE specific
    format.
    
    What we really want is for init_xstate_ctx to be a proper
    fpstate and we use its ::fxsave and ::xsave members, as
    appropriate.
    
    Since the xsave_struct::i387 and thread_xstate::fxsave aliases
    each other this is not a functional problem.
    
    So implement this, and move init_xstate_ctx to the generic FPU
    code in the process.
    
    Also, since init_xstate_ctx is not XSAVE specific anymore,
    rename it to init_fpstate, and mark it __read_mostly,
    because it's only modified once during bootup, and used
    as a reference fpstate later on.
    
    There's no change in functionality.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index b74aa4329aeb..792fdbe64179 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -22,6 +22,8 @@
 
 extern unsigned int mxcsr_feature_mask;
 
+extern union thread_xstate init_fpstate;
+
 extern void fpu__init_cpu(void);
 extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
@@ -342,9 +344,9 @@ static inline void fpregs_deactivate(struct fpu *fpu)
 static inline void restore_init_xstate(void)
 {
 	if (use_xsave())
-		xrstor_state(&init_xstate_ctx, -1);
+		xrstor_state(&init_fpstate.xsave, -1);
 	else
-		fxrstor_checking(&init_xstate_ctx.i387);
+		fxrstor_checking(&init_fpstate.fxsave);
 }
 
 /*

commit bf935b0b526ffa0607476dfc6198593553957dd9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 10:23:42 2015 +0200

    x86/fpu: Create 'union thread_xstate' helper for fpstate_init()
    
    fpstate_init() only uses fpu->state, so pass that in to it.
    
    This enables the cleanup we will do in the next patch.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 0236ae6ffc26..b74aa4329aeb 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -29,7 +29,7 @@ extern void fpu__init_system(struct cpuinfo_x86 *c);
 
 extern void fpu__activate_curr(struct fpu *fpu);
 
-extern void fpstate_init(struct fpu *fpu);
+extern void fpstate_init(union thread_xstate *state);
 #ifdef CONFIG_MATH_EMULATION
 extern void fpstate_init_soft(struct i387_soft_struct *soft);
 #else

commit 0aba69789452faab6f6bd7cd293489bab66352bc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 10:08:36 2015 +0200

    x86/fpu: Harmonize the names of the fpstate_init() helper functions
    
    Harmonize the inconsistent naming of these related functions:
    
                              fpstate_init()
      finit_soft_fpu()   =>   fpstate_init_fsoft()
      fx_finit()         =>   fpstate_init_fxstate()
      fx_finit()         =>   fpstate_init_fstate()       # split out
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index dfdafea6e56f..0236ae6ffc26 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -28,7 +28,18 @@ extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 
 extern void fpu__activate_curr(struct fpu *fpu);
+
 extern void fpstate_init(struct fpu *fpu);
+#ifdef CONFIG_MATH_EMULATION
+extern void fpstate_init_soft(struct i387_soft_struct *soft);
+#else
+static inline void fpstate_init_soft(struct i387_soft_struct *soft) {}
+#endif
+static inline void fpstate_init_fxstate(struct i387_fxsave_struct *fx)
+{
+	fx->cwd = 0x37f;
+	fx->mxcsr = MXCSR_DEFAULT;
+}
 
 extern int  dump_fpu(struct pt_regs *, struct user_i387_struct *);
 extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
@@ -48,12 +59,6 @@ extern void fpu__resume_cpu(void);
 
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
-#ifdef CONFIG_MATH_EMULATION
-extern void finit_soft_fpu(struct i387_soft_struct *soft);
-#else
-static inline void finit_soft_fpu(struct i387_soft_struct *soft) {}
-#endif
-
 /*
  * Must be run with preemption disabled: this clears the fpu_fpregs_owner_ctx,
  * on this CPU.
@@ -93,12 +98,6 @@ static __always_inline __pure bool use_fxsr(void)
 	return static_cpu_has_safe(X86_FEATURE_FXSR);
 }
 
-static inline void fx_finit(struct i387_fxsave_struct *fx)
-{
-	fx->cwd = 0x37f;
-	fx->mxcsr = MXCSR_DEFAULT;
-}
-
 extern void fpstate_sanitize_xstate(struct fpu *fpu);
 
 #define user_insn(insn, output, input...)				\

commit e1cebad49c54e0241e101ebf63d5238fe1137749
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 09:29:38 2015 +0200

    x86/fpu: Factor out the exception error code handling code
    
    Factor out the FPU error code handling code from traps.c and fpu/internal.h
    and move them close to each other.
    
    Also convert the helper functions to 'struct fpu *', which further simplifies
    them.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 7b62d9032623..dfdafea6e56f 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -30,7 +30,8 @@ extern void fpu__init_system(struct cpuinfo_x86 *c);
 extern void fpu__activate_curr(struct fpu *fpu);
 extern void fpstate_init(struct fpu *fpu);
 
-extern int dump_fpu(struct pt_regs *, struct user_i387_struct *);
+extern int  dump_fpu(struct pt_regs *, struct user_i387_struct *);
+extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
 
 /*
  * High level FPU state handling functions:
@@ -467,34 +468,4 @@ static inline void user_fpu_begin(void)
 	preempt_enable();
 }
 
-/*
- * i387 state interaction
- */
-static inline unsigned short get_fpu_cwd(struct task_struct *tsk)
-{
-	if (cpu_has_fxsr) {
-		return tsk->thread.fpu.state.fxsave.cwd;
-	} else {
-		return (unsigned short)tsk->thread.fpu.state.fsave.cwd;
-	}
-}
-
-static inline unsigned short get_fpu_swd(struct task_struct *tsk)
-{
-	if (cpu_has_fxsr) {
-		return tsk->thread.fpu.state.fxsave.swd;
-	} else {
-		return (unsigned short)tsk->thread.fpu.state.fsave.swd;
-	}
-}
-
-static inline unsigned short get_fpu_mxcsr(struct task_struct *tsk)
-{
-	if (cpu_has_xmm) {
-		return tsk->thread.fpu.state.fxsave.mxcsr;
-	} else {
-		return MXCSR_DEFAULT;
-	}
-}
-
 #endif /* _ASM_X86_FPU_INTERNAL_H */

commit 59a36d16be8f9f68410f1bd396577fb7f31ae877
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 08:53:18 2015 +0200

    x86/fpu: Factor out fpu/regset.h from fpu/internal.h
    
    Only a few places use the regset definitions, so factor them out.
    
    Also fix related header dependency assumptions.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index e2ceb49d310d..7b62d9032623 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -10,7 +10,6 @@
 #ifndef _ASM_X86_FPU_INTERNAL_H
 #define _ASM_X86_FPU_INTERNAL_H
 
-#include <linux/regset.h>
 #include <linux/compat.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
@@ -48,18 +47,6 @@ extern void fpu__resume_cpu(void);
 
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
-extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active;
-extern user_regset_get_fn fpregs_get, xfpregs_get, fpregs_soft_get,
-				xstateregs_get;
-extern user_regset_set_fn fpregs_set, xfpregs_set, fpregs_soft_set,
-				 xstateregs_set;
-
-/*
- * xstateregs_active == regset_fpregs_active. Please refer to the comment
- * at the definition of regset_fpregs_active.
- */
-#define xstateregs_active	regset_fpregs_active
-
 #ifdef CONFIG_MATH_EMULATION
 extern void finit_soft_fpu(struct i387_soft_struct *soft);
 #else

commit fcbc99c403c4a1a24ac4744e08c04da3ec18a68c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 08:45:02 2015 +0200

    x86/fpu: Split out fpu/signal.h from fpu/internal.h for signal frame handling functions
    
    Most of the FPU does not use them, so split it out and include
    them in signal.c and ia32_signal.c
    
    Also fix header file dependency assumption in fpu/core.c.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index dc4842b0831b..e2ceb49d310d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -19,21 +19,6 @@
 #include <asm/fpu/api.h>
 #include <asm/fpu/xstate.h>
 
-#ifdef CONFIG_X86_64
-# include <asm/sigcontext32.h>
-# include <asm/user32.h>
-struct ksignal;
-int ia32_setup_rt_frame(int sig, struct ksignal *ksig,
-			compat_sigset_t *set, struct pt_regs *regs);
-int ia32_setup_frame(int sig, struct ksignal *ksig,
-		     compat_sigset_t *set, struct pt_regs *regs);
-#else
-# define user_i387_ia32_struct	user_i387_struct
-# define user32_fxsr_struct	user_fxsr_struct
-# define ia32_setup_frame	__setup_frame
-# define ia32_setup_rt_frame	__setup_rt_frame
-#endif
-
 #define	MXCSR_DEFAULT		0x1f80
 
 extern unsigned int mxcsr_feature_mask;
@@ -63,11 +48,6 @@ extern void fpu__resume_cpu(void);
 
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
-extern void convert_from_fxsr(struct user_i387_ia32_struct *env,
-			      struct task_struct *tsk);
-extern void convert_to_fxsr(struct task_struct *tsk,
-			    const struct user_i387_ia32_struct *env);
-
 extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active;
 extern user_regset_get_fn fpregs_get, xfpregs_get, fpregs_soft_get,
 				xstateregs_get;
@@ -530,8 +510,4 @@ static inline unsigned short get_fpu_mxcsr(struct task_struct *tsk)
 	}
 }
 
-unsigned long
-fpu__alloc_mathframe(unsigned long sp, int ia32_frame,
-		     unsigned long *buf_fx, unsigned long *size);
-
 #endif /* _ASM_X86_FPU_INTERNAL_H */

commit 05012c13f69d67be8a6a7e65726eeb70899ad6ad
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 07:26:04 2015 +0200

    x86/fpu: Move is_ia32*frame() helpers out of fpu/internal.h
    
    Move them to their only user. This makes the code easier to read,
    the header is less cluttered, and it also speeds up the build a bit.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index a55d63efab0f..dc4842b0831b 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -103,22 +103,6 @@ static inline int fpu_want_lazy_restore(struct fpu *fpu, unsigned int cpu)
 	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
 }
 
-static inline int is_ia32_compat_frame(void)
-{
-	return config_enabled(CONFIG_IA32_EMULATION) &&
-	       test_thread_flag(TIF_IA32);
-}
-
-static inline int is_ia32_frame(void)
-{
-	return config_enabled(CONFIG_X86_32) || is_ia32_compat_frame();
-}
-
-static inline int is_x32_frame(void)
-{
-	return config_enabled(CONFIG_X86_X32_ABI) && test_thread_flag(TIF_X32);
-}
-
 #define X87_FSW_ES (1 << 7)	/* Exception Summary */
 
 static __always_inline __pure bool use_eager_fpu(void)

commit fbce7782467553d09cfde39473d23bde4ad78270
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 30 07:12:46 2015 +0200

    x86/fpu: Merge fpu__reset() and fpu__clear()
    
    With recent cleanups and fixes the fpu__reset() and fpu__clear()
    functions have become almost identical in functionality: the only
    difference is that fpu__reset() assumed that the fpstate
    was already active in the eagerfpu case, while fpu__clear()
    activated it if it was inactive.
    
    This distinction almost never matters, the only case where such
    fpstate activation happens if if the init thread (PID 1) gets exec()-ed
    for the first time.
    
    So keep fpu__clear() and change all fpu__reset() uses to
    fpu__clear() to simpify the logic.
    
    ( In a later patch we'll further simplify fpu__clear() by making
      sure that all contexts it is called on are already active. )
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 34fbf95bbe14..a55d63efab0f 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -56,7 +56,6 @@ extern void fpu__restore(void);
 extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
 extern void fpu__drop(struct fpu *fpu);
 extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
-extern void fpu__reset(struct fpu *fpu);
 extern void fpu__clear(struct fpu *fpu);
 
 extern void fpu__init_check_bugs(void);
@@ -490,7 +489,7 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switc
 {
 	if (fpu_switch.preload) {
 		if (unlikely(copy_fpstate_to_fpregs(new_fpu)))
-			fpu__reset(new_fpu);
+			fpu__clear(new_fpu);
 	}
 }
 

commit 82c0e45eb5c839a8cee3d8d8a82d0592c4f42773
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 21:09:18 2015 +0200

    x86/fpu: Move the signal frame handling code closer to each other
    
    Consolidate more signal frame related functions:
    
       text      data    bss     dec       filename
       14108070  2575280 1634304 18317654  vmlinux.before
       14107944  2575344 1634304 18317592  vmlinux.after
    
    Also, while moving it, rename alloc_mathframe() to fpu__alloc_mathframe().
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d6cfbdafbab2..34fbf95bbe14 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -53,6 +53,7 @@ extern int dump_fpu(struct pt_regs *, struct user_i387_struct *);
  */
 extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(void);
+extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
 extern void fpu__drop(struct fpu *fpu);
 extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
 extern void fpu__reset(struct fpu *fpu);
@@ -497,25 +498,6 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switc
  * Signal frame handlers...
  */
 extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fx, int size);
-extern int __fpu__restore_sig(void __user *buf, void __user *fx, int size);
-
-static inline int xstate_sigframe_size(void)
-{
-	return use_xsave() ? xstate_size + FP_XSTATE_MAGIC2_SIZE : xstate_size;
-}
-
-static inline int fpu__restore_sig(void __user *buf, int ia32_frame)
-{
-	void __user *buf_fx = buf;
-	int size = xstate_sigframe_size();
-
-	if (ia32_frame && use_fxsr()) {
-		buf_fx = buf + sizeof(struct i387_fsave_struct);
-		size += sizeof(struct i387_fsave_struct);
-	}
-
-	return __fpu__restore_sig(buf, buf_fx, size);
-}
 
 /*
  * Needs to be preemption-safe.
@@ -565,20 +547,8 @@ static inline unsigned short get_fpu_mxcsr(struct task_struct *tsk)
 	}
 }
 
-static inline unsigned long
-alloc_mathframe(unsigned long sp, int ia32_frame, unsigned long *buf_fx,
-		unsigned long *size)
-{
-	unsigned long frame_size = xstate_sigframe_size();
-
-	*buf_fx = sp = round_down(sp - frame_size, 64);
-	if (ia32_frame && use_fxsr()) {
-		frame_size += sizeof(struct i387_fsave_struct);
-		sp -= sizeof(struct i387_fsave_struct);
-	}
-
-	*size = frame_size;
-	return sp;
-}
+unsigned long
+fpu__alloc_mathframe(unsigned long sp, int ia32_frame,
+		     unsigned long *buf_fx, unsigned long *size);
 
 #endif /* _ASM_X86_FPU_INTERNAL_H */

commit 9dfe99b755a263f9f6ac1dfdb5512bd6e22c28e8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 20:55:19 2015 +0200

    x86/fpu: Rename restore_xstate_sig() to fpu__restore_sig()
    
    restore_xstate_sig() is a misnomer: it's not limited to 'xstate' at all,
    it is the high level 'restore FPU state from a signal frame' function
    that works with all legacy FPU formats as well.
    
    Rename it (and its helper) accordingly, and also move it to the
    fpu__*() namespace.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index d6ac4611f05e..d6cfbdafbab2 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -497,14 +497,14 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switc
  * Signal frame handlers...
  */
 extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fx, int size);
-extern int __restore_xstate_sig(void __user *buf, void __user *fx, int size);
+extern int __fpu__restore_sig(void __user *buf, void __user *fx, int size);
 
 static inline int xstate_sigframe_size(void)
 {
 	return use_xsave() ? xstate_size + FP_XSTATE_MAGIC2_SIZE : xstate_size;
 }
 
-static inline int restore_xstate_sig(void __user *buf, int ia32_frame)
+static inline int fpu__restore_sig(void __user *buf, int ia32_frame)
 {
 	void __user *buf_fx = buf;
 	int size = xstate_sigframe_size();
@@ -514,7 +514,7 @@ static inline int restore_xstate_sig(void __user *buf, int ia32_frame)
 		size += sizeof(struct i387_fsave_struct);
 	}
 
-	return __restore_xstate_sig(buf, buf_fx, size);
+	return __fpu__restore_sig(buf, buf_fx, size);
 }
 
 /*

commit 04c8e01d50ae1f2b33a46459e8b1e776b747e97d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 20:35:33 2015 +0200

    x86/fpu: Move fpu__clear() to 'struct fpu *' parameter passing
    
    Do it like all other high level FPU state handling functions: they
    only know about struct fpu, not about the task.
    
    (Also remove a dead prototype while at it.)
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index f20a0030f6a1..d6ac4611f05e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -37,9 +37,8 @@ int ia32_setup_frame(int sig, struct ksignal *ksig,
 #define	MXCSR_DEFAULT		0x1f80
 
 extern unsigned int mxcsr_feature_mask;
-extern void fpu__init_cpu(void);
-extern void eager_fpu_init(void);
 
+extern void fpu__init_cpu(void);
 extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
@@ -57,7 +56,7 @@ extern void fpu__restore(void);
 extern void fpu__drop(struct fpu *fpu);
 extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
 extern void fpu__reset(struct fpu *fpu);
-extern void fpu__clear(struct task_struct *tsk);
+extern void fpu__clear(struct fpu *fpu);
 
 extern void fpu__init_check_bugs(void);
 extern void fpu__resume_cpu(void);

commit 6ffc152e4606c7f9149940d36a83e786f7f0a4f9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 20:24:14 2015 +0200

    x86/fpu: Move all the fpu__*() high level methods closer to each other
    
    The fpu__*() methods are closely related, but they are defined
    in scattered places within the FPU code.
    
    Concentrate them, and also uninline fpu__save(), fpu__drop()
    and fpu__reset() to save about 5K of kernel text on 64-bit kernels:
    
       text            data    bss     dec        filename
       14113063        2575280 1634304 18322647   vmlinux.before
       14108070        2575280 1634304 18317654   vmlinux.after
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index c09aea145e09..f20a0030f6a1 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -46,10 +46,19 @@ extern void fpu__init_system(struct cpuinfo_x86 *c);
 
 extern void fpu__activate_curr(struct fpu *fpu);
 extern void fpstate_init(struct fpu *fpu);
-extern void fpu__clear(struct task_struct *tsk);
 
 extern int dump_fpu(struct pt_regs *, struct user_i387_struct *);
+
+/*
+ * High level FPU state handling functions:
+ */
+extern void fpu__save(struct fpu *fpu);
 extern void fpu__restore(void);
+extern void fpu__drop(struct fpu *fpu);
+extern int  fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
+extern void fpu__reset(struct fpu *fpu);
+extern void fpu__clear(struct task_struct *tsk);
+
 extern void fpu__init_check_bugs(void);
 extern void fpu__resume_cpu(void);
 
@@ -287,8 +296,6 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 	return 0;
 }
 
-extern void fpu__save(struct fpu *fpu);
-
 static inline int __copy_fpstate_to_fpregs(struct fpu *fpu)
 {
 	if (use_xsave())
@@ -382,33 +389,6 @@ static inline void fpregs_deactivate(struct fpu *fpu)
 	__fpregs_deactivate_hw();
 }
 
-/*
- * Drops current FPU state: deactivates the fpregs and
- * the fpstate. NOTE: it still leaves previous contents
- * in the fpregs in the eager-FPU case.
- *
- * This function can be used in cases where we know that
- * a state-restore is coming: either an explicit one,
- * or a reschedule.
- */
-static inline void fpu__drop(struct fpu *fpu)
-{
-	preempt_disable();
-	fpu->counter = 0;
-
-	if (fpu->fpregs_active) {
-		/* Ignore delayed exceptions from user space */
-		asm volatile("1: fwait\n"
-			     "2:\n"
-			     _ASM_EXTABLE(1b, 2b));
-		fpregs_deactivate(fpu);
-	}
-
-	fpu->fpstate_active = 0;
-
-	preempt_enable();
-}
-
 static inline void restore_init_xstate(void)
 {
 	if (use_xsave())
@@ -417,17 +397,6 @@ static inline void restore_init_xstate(void)
 		fxrstor_checking(&init_xstate_ctx.i387);
 }
 
-/*
- * Reset the FPU state back to init state.
- */
-static inline void fpu__reset(struct fpu *fpu)
-{
-	if (!use_eager_fpu())
-		fpu__drop(fpu);
-	else
-		restore_init_xstate();
-}
-
 /*
  * Definitions for the eXtended Control Register instructions
  */
@@ -597,8 +566,6 @@ static inline unsigned short get_fpu_mxcsr(struct task_struct *tsk)
 	}
 }
 
-extern int fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
-
 static inline unsigned long
 alloc_mathframe(unsigned long sp, int ia32_frame, unsigned long *buf_fx,
 		unsigned long *size)

commit 0e75c54f1703e83e6cdf239491bf7294f6c34777
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 20:10:43 2015 +0200

    x86/fpu: Rename restore_fpu_checking() to copy_fpstate_to_fpregs()
    
    fpu_restore_checking() is a helper function of restore_fpu_checking(),
    but this is not apparent from the naming.
    
    Both copy fpstate contents to fpregs, while the fuller variant does
    a full copy without leaking information.
    
    So rename them to:
    
        copy_fpstate_to_fpregs()
      __copy_fpstate_to_fpregs()
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 31bfda818f30..c09aea145e09 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -289,7 +289,7 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 
 extern void fpu__save(struct fpu *fpu);
 
-static inline int fpu_restore_checking(struct fpu *fpu)
+static inline int __copy_fpstate_to_fpregs(struct fpu *fpu)
 {
 	if (use_xsave())
 		return fpu_xrstor_checking(&fpu->state.xsave);
@@ -299,7 +299,7 @@ static inline int fpu_restore_checking(struct fpu *fpu)
 		return frstor_checking(&fpu->state.fsave);
 }
 
-static inline int restore_fpu_checking(struct fpu *fpu)
+static inline int copy_fpstate_to_fpregs(struct fpu *fpu)
 {
 	/*
 	 * AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception is
@@ -314,7 +314,7 @@ static inline int restore_fpu_checking(struct fpu *fpu)
 			: : [addr] "m" (fpu->fpregs_active));
 	}
 
-	return fpu_restore_checking(fpu);
+	return __copy_fpstate_to_fpregs(fpu);
 }
 
 /*
@@ -520,7 +520,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)
 {
 	if (fpu_switch.preload) {
-		if (unlikely(restore_fpu_checking(new_fpu)))
+		if (unlikely(copy_fpstate_to_fpregs(new_fpu)))
 			fpu__reset(new_fpu);
 	}
 }

commit 5033861575df08a04090cc7b785b2b7aadcbde82
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 29 19:04:31 2015 +0200

    x86/fpu: Synchronize the naming of drop_fpu() and fpu_reset_state()
    
    drop_fpu() and fpu_reset_state() are similar in functionality
    and in scope, yet this is not apparent from their names.
    
    drop_fpu() deactivates FPU contents (both the fpregs and the fpstate),
    but leaves register contents intact in the eager-FPU case, mostly as an
    optimization. It disables fpregs in the lazy FPU case. The drop_fpu()
    method can be used to destroy FPU state in an optimized way, when we
    know that a new state will be loaded before user-space might see
    any remains of the old FPU state:
    
         - such as in sys_exit()'s exit_thread() where we know this task
           won't execute any user-space instructions anymore and the
           next context switch cleans up the FPU. The old FPU state
           might still be around in the eagerfpu case but won't be
           saved.
    
         - in __restore_xstate_sig(), where we use drop_fpu() before
           copying a new state into the fpstate and activating that one.
           No user-pace instructions can execute between those steps.
    
         - in sys_execve()'s fpu__clear(): there we use drop_fpu() in
           the !eagerfpu case, where it's equivalent to a full reinit.
    
    fpu_reset_state() is a stronger version of drop_fpu(): both in
    the eagerfpu and the lazy-FPU case it guarantees that fpregs
    are reinitialized to init state. This method is used in cases
    where we need a full reset:
    
         - handle_signal() uses fpu_reset_state() to reset the FPU state
           to init before executing a user-space signal handler. While we
           have already saved the original FPU state at this point, and
           always restore the original state, the signal handling code
           still has to do this reinit, because signals may interrupt
           any user-space instruction, and the FPU might be in various
           intermediate states (such as an unbalanced x87 stack) that is
           not immediately usable for general C signal handler code.
    
         - __restore_xstate_sig() uses fpu_reset_state() when the signal
           frame has no FP context. Since the signal handler may have
           modified the FPU state, it gets reset back to init state.
    
         - in another branch __restore_xstate_sig() uses fpu_reset_state()
           to handle a restoration error: when restore_user_xstate() fails
           to restore FPU state and we might have inconsistent FPU data,
           fpu_reset_state() is used to reset it back to a known good
           state.
    
         - __kernel_fpu_end() uses fpu_reset_state() in an error branch.
           This is in a 'must not trigger' error branch, so on bug-free
           kernels this never triggers.
    
         - fpu__restore() uses fpu_reset_state() in an error path
           as well: if the fpstate was set up with invalid FPU state
           (via ptrace or via a signal handler), then it's reset back
           to init state.
    
         - likewise, the scheduler's switch_fpu_finish() uses it in a
           restoration error path too.
    
    Move both drop_fpu() and fpu_reset_state() to the fpu__*() namespace
    and harmonize their naming with their function:
    
        fpu__drop()
        fpu__reset()
    
    This clearly shows that both methods operate on the full state of the
    FPU, just like fpu__restore().
    
    Also add comments to explain what each function does.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 0f17cd4e4e58..31bfda818f30 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -382,11 +382,17 @@ static inline void fpregs_deactivate(struct fpu *fpu)
 	__fpregs_deactivate_hw();
 }
 
-static inline void drop_fpu(struct fpu *fpu)
+/*
+ * Drops current FPU state: deactivates the fpregs and
+ * the fpstate. NOTE: it still leaves previous contents
+ * in the fpregs in the eager-FPU case.
+ *
+ * This function can be used in cases where we know that
+ * a state-restore is coming: either an explicit one,
+ * or a reschedule.
+ */
+static inline void fpu__drop(struct fpu *fpu)
 {
-	/*
-	 * Forget coprocessor state..
-	 */
 	preempt_disable();
 	fpu->counter = 0;
 
@@ -412,13 +418,12 @@ static inline void restore_init_xstate(void)
 }
 
 /*
- * Reset the FPU state in the eager case and drop it in the lazy case (later use
- * will reinit it).
+ * Reset the FPU state back to init state.
  */
-static inline void fpu_reset_state(struct fpu *fpu)
+static inline void fpu__reset(struct fpu *fpu)
 {
 	if (!use_eager_fpu())
-		drop_fpu(fpu);
+		fpu__drop(fpu);
 	else
 		restore_init_xstate();
 }
@@ -516,7 +521,7 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switc
 {
 	if (fpu_switch.preload) {
 		if (unlikely(restore_fpu_checking(new_fpu)))
-			fpu_reset_state(new_fpu);
+			fpu__reset(new_fpu);
 	}
 }
 

commit 3c6dffa93be9f82f2566dcc948285d6f79fb9ce2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 28 12:28:08 2015 +0200

    x86/fpu: Rename user_has_fpu() to fpregs_active()
    
    Rename this function in line with the new FPU nomenclature.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 58c274dfcb62..0f17cd4e4e58 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -358,7 +358,7 @@ static inline void __fpregs_activate(struct fpu *fpu)
  * to save the FP state - we'll just take a #NM
  * fault and get the FPU access back.
  */
-static inline int user_has_fpu(void)
+static inline int fpregs_active(void)
 {
 	return current->thread.fpu.fpregs_active;
 }
@@ -557,7 +557,7 @@ static inline void user_fpu_begin(void)
 	struct fpu *fpu = &current->thread.fpu;
 
 	preempt_disable();
-	if (!user_has_fpu())
+	if (!fpregs_active())
 		fpregs_activate(fpu);
 	preempt_enable();
 }

commit c8e1404120d55876d2dedd3a541bc484ef692c58
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 28 11:35:20 2015 +0200

    x86/fpu: Rename save_xstate_sig() to copy_fpstate_to_sigframe()
    
    Standardize the naming of save_xstate_sig() by renaming it to
    copy_fpstate_to_sigframe(): this tells us at a glance that
    the function copies an FPU fpstate to a signal frame.
    
    This naming also follows the naming of copy_fpregs_to_fpstate().
    
    Don't put 'xstate' into the name: since this is a generic name,
    it's expected that the function is able to handle xstate frames
    as well, beyond legacy frames.
    
    xstate used to be the odd case in the x86 FPU code - now it's the
    common case.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index da96b0cbfcb3..58c274dfcb62 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -523,7 +523,7 @@ static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switc
 /*
  * Signal frame handlers...
  */
-extern int save_xstate_sig(void __user *buf, void __user *fx, int size);
+extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fx, int size);
 extern int __restore_xstate_sig(void __user *buf, void __user *fx, int size);
 
 static inline int xstate_sigframe_size(void)

commit 36e49e7f2ec8fa2cdf1ec0439374583ea0a82c47
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 28 11:25:02 2015 +0200

    x86/fpu: Pass 'struct fpu' to fpstate_sanitize_xstate()
    
    Currently fpstate_sanitize_xstate() has a task_struct input parameter,
    but it only uses the fpu structure from it - so pass in a 'struct fpu'
    pointer only and update all call sites.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 88fec3f108de..da96b0cbfcb3 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -139,7 +139,7 @@ static inline void fx_finit(struct i387_fxsave_struct *fx)
 	fx->mxcsr = MXCSR_DEFAULT;
 }
 
-extern void fpstate_sanitize_xstate(struct task_struct *);
+extern void fpstate_sanitize_xstate(struct fpu *fpu);
 
 #define user_insn(insn, output, input...)				\
 ({									\

commit 1ac91a767f1d2ac049dc11e5b7e4342c63c21538
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 28 11:17:55 2015 +0200

    x86/fpu: Simplify fpstate_sanitize_xstate() calls
    
    Remove the extra layer of __fpstate_sanitize_xstate():
    
            if (!use_xsaveopt())
                    return;
            __fpstate_sanitize_xstate(tsk);
    
    and move the check for use_xsaveopt() into fpstate_sanitize_xstate().
    
    In general we optimize for the presence of CPU features, not for
    the absence of them. Furthermore there's little point in this inlining,
    as the call sites are not super hot code paths.
    
    Doing this uninlining shrinks the code a bit:
    
       text    data     bss     dec     hex filename
       14108751        2573624 1634304 18316679        1177d87 vmlinux.before
       14108627        2573624 1634304 18316555        1177d0b vmlinux.after
    
    Also remove a pointless '!fx' check from fpstate_sanitize_xstate().
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 6b6fa46037f8..88fec3f108de 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -139,14 +139,7 @@ static inline void fx_finit(struct i387_fxsave_struct *fx)
 	fx->mxcsr = MXCSR_DEFAULT;
 }
 
-extern void __fpstate_sanitize_xstate(struct task_struct *);
-
-static inline void fpstate_sanitize_xstate(struct task_struct *tsk)
-{
-	if (!use_xsaveopt())
-		return;
-	__fpstate_sanitize_xstate(tsk);
-}
+extern void fpstate_sanitize_xstate(struct task_struct *);
 
 #define user_insn(insn, output, input...)				\
 ({									\

commit d0903193124132c6bb59a895eeb0656f86013da1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 28 11:11:10 2015 +0200

    x86/fpu: Rename sanitize_i387_state() to fpstate_sanitize_xstate()
    
    So the sanitize_i387_state() function has the following purpose:
    on CPUs that support optimized xstate saving instructions, an
    FPU fpstate might end up having partially uninitialized data.
    
    This function initializes that data.
    
    Note that the function name is a misnomer and confusing on two levels,
    not only is it not i387 specific at all, but it is the exact opposite:
    it only matters on xstate CPUs.
    
    So rename sanitize_i387_state() and __sanitize_i387_state() to
    fpstate_sanitize_xstate() and __fpstate_sanitize_xstate(),
    to clearly express the purpose and usage of the function.
    
    We'll further clean up this function in the next patch.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 161b51bf267e..6b6fa46037f8 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -139,13 +139,13 @@ static inline void fx_finit(struct i387_fxsave_struct *fx)
 	fx->mxcsr = MXCSR_DEFAULT;
 }
 
-extern void __sanitize_i387_state(struct task_struct *);
+extern void __fpstate_sanitize_xstate(struct task_struct *);
 
-static inline void sanitize_i387_state(struct task_struct *tsk)
+static inline void fpstate_sanitize_xstate(struct task_struct *tsk)
 {
 	if (!use_xsaveopt())
 		return;
-	__sanitize_i387_state(tsk);
+	__fpstate_sanitize_xstate(tsk);
 }
 
 #define user_insn(insn, output, input...)				\

commit befc61ad3c097bb6ace3da0c73ad56272ccee02d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 28 10:56:54 2015 +0200

    x86/fpu: Move asm/xcr.h to asm/fpu/internal.h
    
    Now that all FPU internals using drivers are converted to public APIs,
    move xcr.h's definitions into fpu/internal.h and remove xcr.h.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 8ec785ecce81..161b51bf267e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -430,6 +430,31 @@ static inline void fpu_reset_state(struct fpu *fpu)
 		restore_init_xstate();
 }
 
+/*
+ * Definitions for the eXtended Control Register instructions
+ */
+
+#define XCR_XFEATURE_ENABLED_MASK	0x00000000
+
+static inline u64 xgetbv(u32 index)
+{
+	u32 eax, edx;
+
+	asm volatile(".byte 0x0f,0x01,0xd0" /* xgetbv */
+		     : "=a" (eax), "=d" (edx)
+		     : "c" (index));
+	return eax + ((u64)edx << 32);
+}
+
+static inline void xsetbv(u32 index, u64 value)
+{
+	u32 eax = value;
+	u32 edx = value >> 32;
+
+	asm volatile(".byte 0x0f,0x01,0xd1" /* xsetbv */
+		     : : "a" (eax), "d" (edx), "c" (index));
+}
+
 /*
  * FPU state switching for scheduling.
  *

commit 669ebabb79906302ba6e6922a683893788a134e8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 28 08:41:33 2015 +0200

    x86/fpu: Rename fpu/xsave.h to fpu/xstate.h
    
    'xsave' is an x86 instruction name to most people - but xsave.h is
    about a lot more than just the XSAVE instruction: it includes
    definitions and support, both internal and external, related to
    xstate and xfeatures support.
    
    As a first step in cleaning up the various xstate uses rename this
    header to 'fpu/xstate.h' to better reflect what this header file
    is about.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 28556c6671c3..8ec785ecce81 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -17,7 +17,7 @@
 
 #include <asm/user.h>
 #include <asm/fpu/api.h>
-#include <asm/fpu/xsave.h>
+#include <asm/fpu/xstate.h>
 
 #ifdef CONFIG_X86_64
 # include <asm/sigcontext32.h>

commit 32b49b3c83cad1ba60494a00dad2f511a647fb5a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 08:58:45 2015 +0200

    x86/fpu: Factor out FPU hw activation/deactivation
    
    We have repeat patterns of:
    
            if (!use_eager_fpu())
                    clts();
    
    ... to activate FPU registers, and:
    
            if (!use_eager_fpu())
                    stts();
    
    ... to deactivate them.
    
    Encapsulate these in:
    
            __fpregs_activate_hw();
            __fpregs_activate_hw();
    
    and use them accordingly.
    
    Doing this synchronizes the idiom with the fpu->fpregs_active
    software-flag's handling functions, creating clear patterns of:
    
            __fpregs_activate_hw();
            __fpregs_activate(fpu);
    
    etc., which improves readability.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index de19fc53f54e..28556c6671c3 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -324,14 +324,31 @@ static inline int restore_fpu_checking(struct fpu *fpu)
 	return fpu_restore_checking(fpu);
 }
 
-/* Must be paired with an 'stts' after! */
+/*
+ * Wrap lazy FPU TS handling in a 'hw fpregs activation/deactivation'
+ * idiom, which is then paired with the sw-flag (fpregs_active) later on:
+ */
+
+static inline void __fpregs_activate_hw(void)
+{
+	if (!use_eager_fpu())
+		clts();
+}
+
+static inline void __fpregs_deactivate_hw(void)
+{
+	if (!use_eager_fpu())
+		stts();
+}
+
+/* Must be paired with an 'stts' (fpregs_deactivate_hw()) after! */
 static inline void __fpregs_deactivate(struct fpu *fpu)
 {
 	fpu->fpregs_active = 0;
 	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
 }
 
-/* Must be paired with a 'clts' before! */
+/* Must be paired with a 'clts' (fpregs_activate_hw()) before! */
 static inline void __fpregs_activate(struct fpu *fpu)
 {
 	fpu->fpregs_active = 1;
@@ -362,16 +379,14 @@ static inline int user_has_fpu(void)
  */
 static inline void fpregs_activate(struct fpu *fpu)
 {
-	if (!use_eager_fpu())
-		clts();
+	__fpregs_activate_hw();
 	__fpregs_activate(fpu);
 }
 
 static inline void fpregs_deactivate(struct fpu *fpu)
 {
 	__fpregs_deactivate(fpu);
-	if (!use_eager_fpu())
-		stts();
+	__fpregs_deactivate_hw();
 }
 
 static inline void drop_fpu(struct fpu *fpu)
@@ -455,8 +470,9 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 			new_fpu->counter++;
 			__fpregs_activate(new_fpu);
 			prefetch(&new_fpu->state);
-		} else if (!use_eager_fpu())
-			stts();
+		} else {
+			__fpregs_deactivate_hw();
+		}
 	} else {
 		old_fpu->counter = 0;
 		old_fpu->last_cpu = -1;

commit c4d72e2db3a36bf560b506df8a3490f140aeae26
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 07:18:17 2015 +0200

    x86/fpu: Simplify fpstate_init_curr() usage
    
    Now that fpstate_init_curr() is not doing implicit allocations
    anymore, almost all uses of it involve a very simple pattern:
    
            if (!fpu->fpstate_active)
                    fpstate_init_curr(fpu);
    
    which is basically activating the FPU fpstate if it was not active
    before.
    
    So propagate the check into the function itself, and rename the
    function according to its new purpose:
    
            fpu__activate_curr(fpu);
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 1345ab3dd273..de19fc53f54e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -44,7 +44,7 @@ extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 
-extern void fpstate_init_curr(struct fpu *fpu);
+extern void fpu__activate_curr(struct fpu *fpu);
 extern void fpstate_init(struct fpu *fpu);
 extern void fpu__clear(struct task_struct *tsk);
 

commit e62bb3d894d312a37009fb07dc83fd1cc7bed37c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 06:50:29 2015 +0200

    x86/fpu: Rename fpstate_alloc_init() to fpstate_init_curr()
    
    Now that there are no FPU context allocations, rename fpstate_alloc_init()
    to fpstate_init_curr(), to signal that it initializes the fpstate and
    marks it active, for the current task.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 1d0c5cee29eb..1345ab3dd273 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -44,7 +44,7 @@ extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 
-extern void fpstate_alloc_init(struct fpu *fpu);
+extern void fpstate_init_curr(struct fpu *fpu);
 extern void fpstate_init(struct fpu *fpu);
 extern void fpu__clear(struct task_struct *tsk);
 

commit 91d93d0e206432b9fe4c88e64577b93aef018f98
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 06:46:52 2015 +0200

    x86/fpu: Remove failure return from fpstate_alloc_init()
    
    Remove the failure code and propagate this down to callers.
    
    Note that this function still has an 'init' aspect, which must be
    called.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 9454f21f0edf..1d0c5cee29eb 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -44,7 +44,7 @@ extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 
-extern int fpstate_alloc_init(struct fpu *fpu);
+extern void fpstate_alloc_init(struct fpu *fpu);
 extern void fpstate_init(struct fpu *fpu);
 extern void fpu__clear(struct task_struct *tsk);
 

commit c4d6ee6e2e52ec604cc1d76877791f8e8f5c79b5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 05:52:40 2015 +0200

    x86/fpu: Remove failure paths from fpstate-alloc low level functions
    
    Now that we always allocate the FPU context as part of task_struct there's
    no need for separate allocations - remove them and their primary failure
    handling code.
    
    ( Note that there's still secondary error codes that have become superfluous,
      those will be removed in separate patches. )
    
    Move the somewhat misplaced setup_xstate_comp() call to the core.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 4ce830fb3f31..9454f21f0edf 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -558,10 +558,6 @@ static inline unsigned short get_fpu_mxcsr(struct task_struct *tsk)
 	}
 }
 
-extern void fpstate_cache_init(void);
-
-extern int fpstate_alloc(struct fpu *fpu);
-extern void fpstate_free(struct fpu *fpu);
 extern int fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
 
 static inline unsigned long

commit 7366ed771f6ed95e4c4525c335722888a83b4b6c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 04:19:39 2015 +0200

    x86/fpu: Simplify FPU handling by embedding the fpstate in task_struct (again)
    
    So 6 years ago we made the FPU fpstate dynamically allocated:
    
      aa283f49276e ("x86, fpu: lazy allocation of FPU area - v5")
      61c4628b5386 ("x86, fpu: split FPU state from task struct - v5")
    
    In hindsight this was a mistake:
    
       - it complicated context allocation failure handling, such as:
    
                    /* kthread execs. TODO: cleanup this horror. */
                    if (WARN_ON(fpstate_alloc_init(fpu)))
                            force_sig(SIGKILL, tsk);
    
       - it caused us to enable irqs in fpu__restore():
    
                    local_irq_enable();
                    /*
                     * does a slab alloc which can sleep
                     */
                    if (fpstate_alloc_init(fpu)) {
                            /*
                             * ran out of memory!
                             */
                            do_group_exit(SIGKILL);
                            return;
                    }
                    local_irq_disable();
    
       - it (slightly) slowed down task creation/destruction by adding
         slab allocation/free pattens.
    
       - it made access to context contents (slightly) slower by adding
         one more pointer dereference.
    
    The motivation for the dynamic allocation was two-fold:
    
       - reduce memory consumption by non-FPU tasks
    
       - allocate and handle only the necessary amount of context for
         various XSAVE processors that have varying hardware frame
         sizes.
    
    These days, with glibc using SSE memcpy by default and GCC optimizing
    for SSE/AVX by default, the scope of FPU using apps on an x86 system is
    much larger than it was 6 years ago.
    
    For example on a freshly installed Fedora 21 desktop system, with a
    recent kernel, all non-kthread tasks have used the FPU shortly after
    bootup.
    
    Also, even modern embedded x86 CPUs try to support the latest vector
    instruction set - so they'll too often use the larger xstate frame
    sizes.
    
    So remove the dynamic allocation complication by embedding the FPU
    fpstate in task_struct again. This should make the FPU a lot more
    accessible to all sorts of atomic contexts.
    
    We could still optimize for the xstate frame size in the future,
    by moving the state structure to the last element of task_struct,
    and allocating only a part of that.
    
    This change is kept minimal by still keeping the ctx_alloc()/free()
    routines (that now do nothing substantial) - we'll remove them in
    the following patches.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 10663b02ee22..4ce830fb3f31 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -232,9 +232,9 @@ static inline int frstor_user(struct i387_fsave_struct __user *fx)
 static inline void fpu_fxsave(struct fpu *fpu)
 {
 	if (config_enabled(CONFIG_X86_32))
-		asm volatile( "fxsave %[fx]" : [fx] "=m" (fpu->state->fxsave));
+		asm volatile( "fxsave %[fx]" : [fx] "=m" (fpu->state.fxsave));
 	else if (config_enabled(CONFIG_AS_FXSAVEQ))
-		asm volatile("fxsaveq %[fx]" : [fx] "=m" (fpu->state->fxsave));
+		asm volatile("fxsaveq %[fx]" : [fx] "=m" (fpu->state.fxsave));
 	else {
 		/* Using "rex64; fxsave %0" is broken because, if the memory
 		 * operand uses any extended registers for addressing, a second
@@ -251,15 +251,15 @@ static inline void fpu_fxsave(struct fpu *fpu)
 		 * an extended register is needed for addressing (fix submitted
 		 * to mainline 2005-11-21).
 		 *
-		 *  asm volatile("rex64/fxsave %0" : "=m" (fpu->state->fxsave));
+		 *  asm volatile("rex64/fxsave %0" : "=m" (fpu->state.fxsave));
 		 *
 		 * This, however, we can work around by forcing the compiler to
 		 * select an addressing mode that doesn't require extended
 		 * registers.
 		 */
 		asm volatile( "rex64/fxsave (%[fx])"
-			     : "=m" (fpu->state->fxsave)
-			     : [fx] "R" (&fpu->state->fxsave));
+			     : "=m" (fpu->state.fxsave)
+			     : [fx] "R" (&fpu->state.fxsave));
 	}
 }
 
@@ -276,7 +276,7 @@ static inline void fpu_fxsave(struct fpu *fpu)
 static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 {
 	if (likely(use_xsave())) {
-		xsave_state(&fpu->state->xsave);
+		xsave_state(&fpu->state.xsave);
 		return 1;
 	}
 
@@ -289,7 +289,7 @@ static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 	 * Legacy FPU register saving, FNSAVE always clears FPU registers,
 	 * so we have to mark them inactive:
 	 */
-	asm volatile("fnsave %[fx]; fwait" : [fx] "=m" (fpu->state->fsave));
+	asm volatile("fnsave %[fx]; fwait" : [fx] "=m" (fpu->state.fsave));
 
 	return 0;
 }
@@ -299,11 +299,11 @@ extern void fpu__save(struct fpu *fpu);
 static inline int fpu_restore_checking(struct fpu *fpu)
 {
 	if (use_xsave())
-		return fpu_xrstor_checking(&fpu->state->xsave);
+		return fpu_xrstor_checking(&fpu->state.xsave);
 	else if (use_fxsr())
-		return fxrstor_checking(&fpu->state->fxsave);
+		return fxrstor_checking(&fpu->state.fxsave);
 	else
-		return frstor_checking(&fpu->state->fsave);
+		return frstor_checking(&fpu->state.fsave);
 }
 
 static inline int restore_fpu_checking(struct fpu *fpu)
@@ -454,7 +454,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 		if (fpu.preload) {
 			new_fpu->counter++;
 			__fpregs_activate(new_fpu);
-			prefetch(new_fpu->state);
+			prefetch(&new_fpu->state);
 		} else if (!use_eager_fpu())
 			stts();
 	} else {
@@ -465,7 +465,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 			if (fpu_want_lazy_restore(new_fpu, cpu))
 				fpu.preload = 0;
 			else
-				prefetch(new_fpu->state);
+				prefetch(&new_fpu->state);
 			fpregs_activate(new_fpu);
 		}
 	}
@@ -534,25 +534,25 @@ static inline void user_fpu_begin(void)
 static inline unsigned short get_fpu_cwd(struct task_struct *tsk)
 {
 	if (cpu_has_fxsr) {
-		return tsk->thread.fpu.state->fxsave.cwd;
+		return tsk->thread.fpu.state.fxsave.cwd;
 	} else {
-		return (unsigned short)tsk->thread.fpu.state->fsave.cwd;
+		return (unsigned short)tsk->thread.fpu.state.fsave.cwd;
 	}
 }
 
 static inline unsigned short get_fpu_swd(struct task_struct *tsk)
 {
 	if (cpu_has_fxsr) {
-		return tsk->thread.fpu.state->fxsave.swd;
+		return tsk->thread.fpu.state.fxsave.swd;
 	} else {
-		return (unsigned short)tsk->thread.fpu.state->fsave.swd;
+		return (unsigned short)tsk->thread.fpu.state.fsave.swd;
 	}
 }
 
 static inline unsigned short get_fpu_mxcsr(struct task_struct *tsk)
 {
 	if (cpu_has_xmm) {
-		return tsk->thread.fpu.state->fxsave.mxcsr;
+		return tsk->thread.fpu.state.fxsave.mxcsr;
 	} else {
 		return MXCSR_DEFAULT;
 	}

commit 1bc6b056d8f90f0a710ff1201964c1fffc9ddd3c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 03:32:18 2015 +0200

    x86/fpu: Optimize copy_fpregs_to_fpstate() by removing the FNCLEX synchronization with FP exceptions
    
    So we have the following ancient code in copy_fpregs_to_fpstate():
    
            if (unlikely(fpu->state->fxsave.swd & X87_FSW_ES)) {
                    asm volatile("fnclex");
                    goto drop_fpregs;
            }
    
    which clears pending FPU exceptions and then drops registers, which
    causes the next FP instruction of the saved context to re-load the
    saved FPU state, with all pending exceptions marked properly, and
    will re-start the exception handling mechanism in the hardware.
    
    Since FPU exceptions are always issued on instruction boundaries,
    in particular on the next FP instruction following the exception
    generating instruction, there's no fear of getting an FP exception
    asynchronously.
    
    They were truly asynchronous back in the IRQ13 days, when the FPU was
    a weird and expensive co-processor that did its own processing, and we
    had to synchronize with them, but that code is not working anymore:
    we don't have IRQ13 mapped in the IDT anymore.
    
    With the introduction of optimized XSAVE support there's a new
    complication: if the xstate features bit indicates that a particular
    state component is unused (in 'init state'), then the hardware does
    not guarantee that the XSAVE (et al) instruction keeps the underlying
    FPU state image in memory valid and current. In practice this means
    that the hardware won't write it, and the exceptions flag in the
    state might be an older version, with it still being set. This
    meant that we had to check the xfeatures flag as well, adding
    another memory load and branch to a critical hot path of the scheduler.
    
    So optimize all this by removing both the old quirk and the new check,
    and straight-line optimizing the most common cases with likely()
    hints. Quite a bit of code gets removed this way:
    
      arch/x86/kernel/process_64.o:
    
        text    data     bss     dec     filename
        5484       8       0    5492     process_64.o.before
        5416       8       0    5424     process_64.o.after
    
    Now there's also a chance that some weird behavior or erratum was
    masked by our IRQ13 handling quirk (or that I misunderstood the
    nature of the quirk), and that this change triggers some badness.
    
    There's no real good way to protect against that possibility other
    than keeping this change well isolated, well commented and well
    bisectable. If you bisect a weird (or not so weird) breakage to
    this commit then please let us know!
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 11055f51e67a..10663b02ee22 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -271,46 +271,26 @@ static inline void fpu_fxsave(struct fpu *fpu)
  * The legacy FNSAVE instruction cleared all FPU state
  * unconditionally, so registers are essentially destroyed.
  * Modern FPU state can be kept in registers, if there are
- * no pending FP exceptions. (Note the FIXME below.)
+ * no pending FP exceptions.
  */
 static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 {
-	if (use_xsave()) {
+	if (likely(use_xsave())) {
 		xsave_state(&fpu->state->xsave);
+		return 1;
+	}
 
-		/*
-		 * xsave header may indicate the init state of the FP.
-		 */
-		if (!(fpu->state->xsave.header.xfeatures & XSTATE_FP))
-			goto keep_fpregs;
-	} else {
-		if (use_fxsr()) {
-			fpu_fxsave(fpu);
-		} else {
-			/* FNSAVE always clears FPU registers: */
-			asm volatile("fnsave %[fx]; fwait"
-				     : [fx] "=m" (fpu->state->fsave));
-			goto drop_fpregs;
-		}
+	if (likely(use_fxsr())) {
+		fpu_fxsave(fpu);
+		return 1;
 	}
 
 	/*
-	 * If exceptions are pending, we need to clear them so
-	 * that we don't randomly get exceptions later.
-	 *
-	 * FIXME! Is this perhaps only true for the old-style
-	 * irq13 case? Maybe we could leave the x87 state
-	 * intact otherwise?
+	 * Legacy FPU register saving, FNSAVE always clears FPU registers,
+	 * so we have to mark them inactive:
 	 */
-	if (unlikely(fpu->state->fxsave.swd & X87_FSW_ES)) {
-		asm volatile("fnclex");
-		goto drop_fpregs;
-	}
-
-keep_fpregs:
-	return 1;
+	asm volatile("fnsave %[fx]; fwait" : [fx] "=m" (fpu->state->fsave));
 
-drop_fpregs:
 	return 0;
 }
 

commit 4f83634710a1a7024b8acaa3b589dc5d8ca03ab0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 27 02:53:16 2015 +0200

    x86/fpu: Rename fpu_save_init() to copy_fpregs_to_fpstate()
    
    So fpu_save_init() is a historic name that got its name when the only
    way the FPU state was FNSAVE, which cleared (well, destroyed) the FPU
    state after saving it.
    
    Nowadays the name is misleading, because ever since the introduction of
    FXSAVE (and more modern FPU saving instructions) the 'we need to reload
    the FPU state' part is only true if there's a pending FPU exception [*],
    which is almost never the case.
    
    So rename it to copy_fpregs_to_fpstate() to make it clear what's
    happening. Also add a few comments about why we cannot keep registers
    in certain cases.
    
    Also clean up the control flow a bit, to make it more apparent when
    we are dropping/keeping FP registers, and to optimize the common
    case (of keeping fpregs) some more.
    
    [*] Probably not true anymore, modern instructions always leave the FPU
        state intact, even if exceptions are pending: because pending FP
        exceptions are posted on the next FP instruction, not asynchronously.
    
        They were truly asynchronous back in the IRQ13 case, and we had to
        synchronize with them, but that code is not working anymore: we don't
        have IRQ13 mapped in the IDT anymore.
    
        But a cleanup patch is obviously not the place to change subtle behavior.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 89c6ec80c1ac..11055f51e67a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -265,9 +265,15 @@ static inline void fpu_fxsave(struct fpu *fpu)
 
 /*
  * These must be called with preempt disabled. Returns
- * 'true' if the FPU state is still intact.
+ * 'true' if the FPU state is still intact and we can
+ * keep registers active.
+ *
+ * The legacy FNSAVE instruction cleared all FPU state
+ * unconditionally, so registers are essentially destroyed.
+ * Modern FPU state can be kept in registers, if there are
+ * no pending FP exceptions. (Note the FIXME below.)
  */
-static inline int fpu_save_init(struct fpu *fpu)
+static inline int copy_fpregs_to_fpstate(struct fpu *fpu)
 {
 	if (use_xsave()) {
 		xsave_state(&fpu->state->xsave);
@@ -276,13 +282,16 @@ static inline int fpu_save_init(struct fpu *fpu)
 		 * xsave header may indicate the init state of the FP.
 		 */
 		if (!(fpu->state->xsave.header.xfeatures & XSTATE_FP))
-			return 1;
-	} else if (use_fxsr()) {
-		fpu_fxsave(fpu);
+			goto keep_fpregs;
 	} else {
-		asm volatile("fnsave %[fx]; fwait"
-			     : [fx] "=m" (fpu->state->fsave));
-		return 0;
+		if (use_fxsr()) {
+			fpu_fxsave(fpu);
+		} else {
+			/* FNSAVE always clears FPU registers: */
+			asm volatile("fnsave %[fx]; fwait"
+				     : [fx] "=m" (fpu->state->fsave));
+			goto drop_fpregs;
+		}
 	}
 
 	/*
@@ -295,9 +304,14 @@ static inline int fpu_save_init(struct fpu *fpu)
 	 */
 	if (unlikely(fpu->state->fxsave.swd & X87_FSW_ES)) {
 		asm volatile("fnclex");
-		return 0;
+		goto drop_fpregs;
 	}
+
+keep_fpregs:
 	return 1;
+
+drop_fpregs:
+	return 0;
 }
 
 extern void fpu__save(struct fpu *fpu);
@@ -448,7 +462,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 		      (use_eager_fpu() || new_fpu->counter > 5);
 
 	if (old_fpu->fpregs_active) {
-		if (!fpu_save_init(old_fpu))
+		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
 		else
 			old_fpu->last_cpu = cpu;

commit 952f07ecbd4d9bac77c003ba136f8ee8ce631591
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 16:56:05 2015 +0200

    x86/fpu: Move various internal function prototypes to fpu/internal.h
    
    There are a number of FPU internal function prototypes and an inline function
    in fpu/api.h, mostly placed so historically as the code grew over the years.
    
    Move them over into fpu/internal.h where they belong. (Add sched.h include
    to stackprotector.h which incorrectly relied on getting it from fpu/api.h.)
    
    fpu/api.h is now a pure file that only contains FPU APIs intended for driver
    use.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 0c8c812d23b4..89c6ec80c1ac 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -12,6 +12,7 @@
 
 #include <linux/regset.h>
 #include <linux/compat.h>
+#include <linux/sched.h>
 #include <linux/slab.h>
 
 #include <asm/user.h>
@@ -43,6 +44,15 @@ extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 
+extern int fpstate_alloc_init(struct fpu *fpu);
+extern void fpstate_init(struct fpu *fpu);
+extern void fpu__clear(struct task_struct *tsk);
+
+extern int dump_fpu(struct pt_regs *, struct user_i387_struct *);
+extern void fpu__restore(void);
+extern void fpu__init_check_bugs(void);
+extern void fpu__resume_cpu(void);
+
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
 extern void convert_from_fxsr(struct user_i387_ia32_struct *env,
@@ -334,6 +344,21 @@ static inline void __fpregs_activate(struct fpu *fpu)
 	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
 }
 
+/*
+ * The question "does this thread have fpu access?"
+ * is slightly racy, since preemption could come in
+ * and revoke it immediately after the test.
+ *
+ * However, even in that very unlikely scenario,
+ * we can just assume we have FPU access - typically
+ * to save the FP state - we'll just take a #NM
+ * fault and get the FPU access back.
+ */
+static inline int user_has_fpu(void)
+{
+	return current->thread.fpu.fpregs_active;
+}
+
 /*
  * Encapsulate the CR0.TS handling together with the
  * software flag.

commit e229537543fba3de4026cb6e12b75946d9f3430f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 16:43:43 2015 +0200

    x86/fpu: Move fpu__save() to fpu/internals.h
    
    It's an internal method, not a driver API, so move it from fpu/api.h
    to fpu/internal.h.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 5a1fa5bc2c27..0c8c812d23b4 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -290,6 +290,8 @@ static inline int fpu_save_init(struct fpu *fpu)
 	return 1;
 }
 
+extern void fpu__save(struct fpu *fpu);
+
 static inline int fpu_restore_checking(struct fpu *fpu)
 {
 	if (use_xsave())

commit dd863880acd24a23d71576e402f999375d0b4b80
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 15:07:18 2015 +0200

    x86/fpu: Move fpu__init_system_early_generic() out of fpu__detect()
    
    Move the fpu__init_system_early_generic() call into fpu__init_system(),
    which hosts all the system init calls.
    
    Expose fpu__init_system() to other modules - this will be our main and only
    system init function.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 4617eeb57004..5a1fa5bc2c27 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -41,6 +41,7 @@ extern void eager_fpu_init(void);
 
 extern void fpu__init_system_xstate(void);
 extern void fpu__init_cpu_xstate(void);
+extern void fpu__init_system(struct cpuinfo_x86 *c);
 
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 

commit 21c4cd108a1b144ad645355bfee1f8be937f03a2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 14:27:17 2015 +0200

    x86/fpu: Simplify fpu__cpu_init()
    
    After the latest round of cleanups, fpu__cpu_init() has become
    a simple call to fpu__init_cpu().
    
    Rename fpu__init_cpu() to fpu__cpu_init() and remove the
    extra layer.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 71d44be5acb1..4617eeb57004 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -36,7 +36,7 @@ int ia32_setup_frame(int sig, struct ksignal *ksig,
 #define	MXCSR_DEFAULT		0x1f80
 
 extern unsigned int mxcsr_feature_mask;
-extern void fpu__cpu_init(void);
+extern void fpu__init_cpu(void);
 extern void eager_fpu_init(void);
 
 extern void fpu__init_system_xstate(void);

commit 55cc4678b7ee2edd3e6a9411250530eb871bc61d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Apr 25 06:26:36 2015 +0200

    x86/fpu: Make the system/cpu init distinction clear in the xstate code as well
    
    Rename existing xstate init functions along the system/cpu init principles:
    
            fpu__init_system_xstate(): called once per system bootup
            fpu__init_cpu_xstate():    called per CPU onlining
    
    Also make the fpu__init_cpu_xstate() early code invariant:
    if xfeatures_mask is not set yet then don't crash but return.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 19b7cdf73efd..71d44be5acb1 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -39,6 +39,9 @@ extern unsigned int mxcsr_feature_mask;
 extern void fpu__cpu_init(void);
 extern void eager_fpu_init(void);
 
+extern void fpu__init_system_xstate(void);
+extern void fpu__init_cpu_xstate(void);
+
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
 extern void convert_from_fxsr(struct user_i387_ia32_struct *env,

commit 3e5e1267740f47b1616aff5187b668cadd950047
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Apr 25 05:08:17 2015 +0200

    x86/fpu: Remove 'init_xstate_buf' bootmem allocation
    
    Make init_xstate_buf allocated statically at build time.
    
    This structure's maximum size is around 1KB - and it's allocated even on
    most modern embedded x86 CPUs which strive for FPU instruction set parity
    with desktop and server CPUs, so it's not like we can save much on smaller
    systems.
    
    This removes the last bootmem allocation from the FPU init path, allowing
    it to be called earlier in the boot sequence.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 0292fcc4d441..19b7cdf73efd 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -373,9 +373,9 @@ static inline void drop_fpu(struct fpu *fpu)
 static inline void restore_init_xstate(void)
 {
 	if (use_xsave())
-		xrstor_state(init_xstate_buf, -1);
+		xrstor_state(&init_xstate_ctx, -1);
 	else
-		fxrstor_checking(&init_xstate_buf->i387);
+		fxrstor_checking(&init_xstate_ctx.i387);
 }
 
 /*

commit 66af8e276409196abd59e33919f928e4d002d4f8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 14:31:27 2015 +0200

    x86/fpu: Rename __thread_fpu_end() to fpregs_deactivate()
    
    Propagate the 'fpu->fpregs_active' naming to the high level function that
    clears it.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 18a62239c73d..0292fcc4d441 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -335,18 +335,18 @@ static inline void __fpregs_activate(struct fpu *fpu)
  * These generally need preemption protection to work,
  * do try to avoid using these on their own.
  */
-static inline void __thread_fpu_end(struct fpu *fpu)
+static inline void fpregs_activate(struct fpu *fpu)
 {
-	__fpregs_deactivate(fpu);
 	if (!use_eager_fpu())
-		stts();
+		clts();
+	__fpregs_activate(fpu);
 }
 
-static inline void fpregs_activate(struct fpu *fpu)
+static inline void fpregs_deactivate(struct fpu *fpu)
 {
+	__fpregs_deactivate(fpu);
 	if (!use_eager_fpu())
-		clts();
-	__fpregs_activate(fpu);
+		stts();
 }
 
 static inline void drop_fpu(struct fpu *fpu)
@@ -362,7 +362,7 @@ static inline void drop_fpu(struct fpu *fpu)
 		asm volatile("1: fwait\n"
 			     "2:\n"
 			     _ASM_EXTABLE(1b, 2b));
-		__thread_fpu_end(fpu);
+		fpregs_deactivate(fpu);
 	}
 
 	fpu->fpstate_active = 0;

commit 232f62cdd7c7162168a445cbc718a82e7f6e36c1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 14:30:38 2015 +0200

    x86/fpu: Rename __thread_fpu_begin() to fpregs_activate()
    
    Propagate the 'fpu->fpregs_active' naming to the high level
    function that sets it.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 7a235171be6c..18a62239c73d 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -342,7 +342,7 @@ static inline void __thread_fpu_end(struct fpu *fpu)
 		stts();
 }
 
-static inline void __thread_fpu_begin(struct fpu *fpu)
+static inline void fpregs_activate(struct fpu *fpu)
 {
 	if (!use_eager_fpu())
 		clts();
@@ -441,7 +441,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 				fpu.preload = 0;
 			else
 				prefetch(new_fpu->state);
-			__thread_fpu_begin(new_fpu);
+			fpregs_activate(new_fpu);
 		}
 	}
 	return fpu;
@@ -499,7 +499,7 @@ static inline void user_fpu_begin(void)
 
 	preempt_disable();
 	if (!user_has_fpu())
-		__thread_fpu_begin(fpu);
+		fpregs_activate(fpu);
 	preempt_enable();
 }
 

commit 723c58e428fbcbc9f16864edf740ec3bfaf3703c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 14:28:01 2015 +0200

    x86/fpu: Rename __thread_clear_has_fpu() to __fpregs_deactivate()
    
    Propagate the 'fpu->fpregs_active' naming to the functions that
    clears it.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 3554a8cdaece..7a235171be6c 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -315,7 +315,7 @@ static inline int restore_fpu_checking(struct fpu *fpu)
 }
 
 /* Must be paired with an 'stts' after! */
-static inline void __thread_clear_has_fpu(struct fpu *fpu)
+static inline void __fpregs_deactivate(struct fpu *fpu)
 {
 	fpu->fpregs_active = 0;
 	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
@@ -337,7 +337,7 @@ static inline void __fpregs_activate(struct fpu *fpu)
  */
 static inline void __thread_fpu_end(struct fpu *fpu)
 {
-	__thread_clear_has_fpu(fpu);
+	__fpregs_deactivate(fpu);
 	if (!use_eager_fpu())
 		stts();
 }

commit dfaea4e6a27afffb0cc6da7aaaae81abc127fdb3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 14:26:47 2015 +0200

    x86/fpu: Rename __thread_set_has_fpu() to __fpregs_activate()
    
    Propagate the 'fpu->fpregs_active' naming to the functions that
    sets it.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index b546ec816fd6..3554a8cdaece 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -322,7 +322,7 @@ static inline void __thread_clear_has_fpu(struct fpu *fpu)
 }
 
 /* Must be paired with a 'clts' before! */
-static inline void __thread_set_has_fpu(struct fpu *fpu)
+static inline void __fpregs_activate(struct fpu *fpu)
 {
 	fpu->fpregs_active = 1;
 	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
@@ -346,7 +346,7 @@ static inline void __thread_fpu_begin(struct fpu *fpu)
 {
 	if (!use_eager_fpu())
 		clts();
-	__thread_set_has_fpu(fpu);
+	__fpregs_activate(fpu);
 }
 
 static inline void drop_fpu(struct fpu *fpu)
@@ -428,7 +428,7 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 		/* Don't change CR0.TS if we just switch! */
 		if (fpu.preload) {
 			new_fpu->counter++;
-			__thread_set_has_fpu(new_fpu);
+			__fpregs_activate(new_fpu);
 			prefetch(new_fpu->state);
 		} else if (!use_eager_fpu())
 			stts();

commit d5cea9b0af1509f170337ba8f47160d0699ff374
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 14:19:26 2015 +0200

    x86/fpu: Rename fpu->has_fpu to fpu->fpregs_active
    
    So the current code uses fpu->has_cpu to determine whether a given
    user FPU context is actively loaded into the FPU's registers [*] and
    that those registers represent the task's current FPU state.
    
    But this term is not unambiguous: especially the distinction between
    fpu->has_fpu, PF_USED_MATH and fpu_fpregs_owner_ctx is not clear.
    
    Increase clarity by unambigously signalling that it's about
    hardware registers being active right now, by renaming it to
    fpu->fpregs_active.
    
    ( In later patches we'll use more of the 'fpregs' naming, which will
      make it easier to grep for as well. )
    
    [*] There's the kernel_fpu_begin()/end() primitive that also
        activates FPU hw registers as well and uses them, without
        touching the fpu->fpregs_active flag.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 6eea81c068fb..b546ec816fd6 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -308,7 +308,7 @@ static inline int restore_fpu_checking(struct fpu *fpu)
 			"fnclex\n\t"
 			"emms\n\t"
 			"fildl %P[addr]"	/* set F?P to defined value */
-			: : [addr] "m" (fpu->has_fpu));
+			: : [addr] "m" (fpu->fpregs_active));
 	}
 
 	return fpu_restore_checking(fpu);
@@ -317,14 +317,14 @@ static inline int restore_fpu_checking(struct fpu *fpu)
 /* Must be paired with an 'stts' after! */
 static inline void __thread_clear_has_fpu(struct fpu *fpu)
 {
-	fpu->has_fpu = 0;
+	fpu->fpregs_active = 0;
 	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
 }
 
 /* Must be paired with a 'clts' before! */
 static inline void __thread_set_has_fpu(struct fpu *fpu)
 {
-	fpu->has_fpu = 1;
+	fpu->fpregs_active = 1;
 	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
 }
 
@@ -357,7 +357,7 @@ static inline void drop_fpu(struct fpu *fpu)
 	preempt_disable();
 	fpu->counter = 0;
 
-	if (fpu->has_fpu) {
+	if (fpu->fpregs_active) {
 		/* Ignore delayed exceptions from user space */
 		asm volatile("1: fwait\n"
 			     "2:\n"
@@ -416,14 +416,14 @@ switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
 	fpu.preload = new_fpu->fpstate_active &&
 		      (use_eager_fpu() || new_fpu->counter > 5);
 
-	if (old_fpu->has_fpu) {
+	if (old_fpu->fpregs_active) {
 		if (!fpu_save_init(old_fpu))
 			old_fpu->last_cpu = -1;
 		else
 			old_fpu->last_cpu = cpu;
 
 		/* But leave fpu_fpregs_owner_ctx! */
-		old_fpu->has_fpu = 0;
+		old_fpu->fpregs_active = 0;
 
 		/* Don't change CR0.TS if we just switch! */
 		if (fpu.preload) {

commit 678eaf603460180260a645de359050fd6568cf74
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 14:48:24 2015 +0200

    x86/fpu: Rename regset FPU register accessors
    
    Rename regset accessors to prefix them with 'regset_', because we
    want to start using the 'fpregs_active' name elsewhere.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 07c6adc02f68..6eea81c068fb 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -46,17 +46,17 @@ extern void convert_from_fxsr(struct user_i387_ia32_struct *env,
 extern void convert_to_fxsr(struct task_struct *tsk,
 			    const struct user_i387_ia32_struct *env);
 
-extern user_regset_active_fn fpregs_active, xfpregs_active;
+extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active;
 extern user_regset_get_fn fpregs_get, xfpregs_get, fpregs_soft_get,
 				xstateregs_get;
 extern user_regset_set_fn fpregs_set, xfpregs_set, fpregs_soft_set,
 				 xstateregs_set;
 
 /*
- * xstateregs_active == fpregs_active. Please refer to the comment
- * at the definition of fpregs_active.
+ * xstateregs_active == regset_fpregs_active. Please refer to the comment
+ * at the definition of regset_fpregs_active.
  */
-#define xstateregs_active	fpregs_active
+#define xstateregs_active	regset_fpregs_active
 
 #ifdef CONFIG_MATH_EMULATION
 extern void finit_soft_fpu(struct i387_soft_struct *soft);

commit 400e4b209166dcd3e3a155401c57bdc6413bf715
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 10:19:47 2015 +0200

    x86/fpu: Rename xsave.header::xstate_bv to 'xfeatures'
    
    'xsave.header::xstate_bv' is a misnomer - what does 'bv' stand for?
    
    It probably comes from the 'XGETBV' instruction name, but I could
    not find in the Intel documentation where that abbreviation comes
    from. It could mean 'bit vector' - or something else?
    
    But how about - instead of guessing about a weird name - we named
    the field in an obvious and descriptive way that tells us exactly
    what it does?
    
    So rename it to 'xfeatures', which is a bitmask of the
    xfeatures that are fpstate_active in that context structure.
    
    Eyesore like:
    
               fpu->state->xsave.xsave_hdr.xstate_bv |= XSTATE_FP;
    
    is now much more readable:
    
               fpu->state->xsave.header.xfeatures |= XSTATE_FP;
    
    Which form is not just infinitely more readable, but is also
    shorter as well.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 3007df99833e..07c6adc02f68 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -261,7 +261,7 @@ static inline int fpu_save_init(struct fpu *fpu)
 		/*
 		 * xsave header may indicate the init state of the FP.
 		 */
-		if (!(fpu->state->xsave.header.xstate_bv & XSTATE_FP))
+		if (!(fpu->state->xsave.header.xfeatures & XSTATE_FP))
 			return 1;
 	} else if (use_fxsr()) {
 		fpu_fxsave(fpu);

commit 3a54450b5ed1671a6adecf501a0b4d4c1d27235d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 10:14:36 2015 +0200

    x86/fpu: Rename 'xsave_hdr' to 'header'
    
    Code like:
    
               fpu->state->xsave.xsave_hdr.xstate_bv |= XSTATE_FP;
    
    is an eyesore, because not only is the words 'xsave' and 'state'
    are repeated twice times (!), but also because of the 'hdr' and 'bv'
    abbreviations that are pretty meaningless at a first glance.
    
    Start cleaning this up by renaming 'xsave_hdr' to 'header'.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 0e9a7a37801a..3007df99833e 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -261,7 +261,7 @@ static inline int fpu_save_init(struct fpu *fpu)
 		/*
 		 * xsave header may indicate the init state of the FP.
 		 */
-		if (!(fpu->state->xsave.xsave_hdr.xstate_bv & XSTATE_FP))
+		if (!(fpu->state->xsave.header.xstate_bv & XSTATE_FP))
 			return 1;
 	} else if (use_fxsr()) {
 		fpu_fxsave(fpu);

commit df6397525cbe096d0eab0c1530fd25429d26f11b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 03:06:56 2015 +0200

    x86/fpu: Move MXCSR_DEFAULT to fpu/internal.h
    
    fpu/types.h gets included everywhere, move the MXCSR_DEFAULT to
    fpu/internal.h, the place where it's used.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 386a8837c358..0e9a7a37801a 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -33,6 +33,8 @@ int ia32_setup_frame(int sig, struct ksignal *ksig,
 # define ia32_setup_rt_frame	__setup_rt_frame
 #endif
 
+#define	MXCSR_DEFAULT		0x1f80
+
 extern unsigned int mxcsr_feature_mask;
 extern void fpu__cpu_init(void);
 extern void eager_fpu_init(void);

commit 78f7f1e54bac032b98956862a5bcf8c28ed22d07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:54:44 2015 +0200

    x86/fpu: Rename fpu-internal.h to fpu/internal.h
    
    This unifies all the FPU related header files under a unified, hiearchical
    naming scheme:
    
     - asm/fpu/types.h:      FPU related data types, needed for 'struct task_struct',
                             widely included in almost all kernel code, and hence kept
                             as small as possible.
    
     - asm/fpu/api.h:        FPU related 'public' methods exported to other subsystems.
    
     - asm/fpu/internal.h:   FPU subsystem internal methods
    
     - asm/fpu/xsave.h:      XSAVE support internal methods
    
    (Also standardize the header guard in asm/fpu/internal.h.)
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
new file mode 100644
index 000000000000..386a8837c358
--- /dev/null
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -0,0 +1,556 @@
+/*
+ * Copyright (C) 1994 Linus Torvalds
+ *
+ * Pentium III FXSR, SSE support
+ * General FPU state handling cleanups
+ *	Gareth Hughes <gareth@valinux.com>, May 2000
+ * x86-64 work by Andi Kleen 2002
+ */
+
+#ifndef _ASM_X86_FPU_INTERNAL_H
+#define _ASM_X86_FPU_INTERNAL_H
+
+#include <linux/regset.h>
+#include <linux/compat.h>
+#include <linux/slab.h>
+
+#include <asm/user.h>
+#include <asm/fpu/api.h>
+#include <asm/fpu/xsave.h>
+
+#ifdef CONFIG_X86_64
+# include <asm/sigcontext32.h>
+# include <asm/user32.h>
+struct ksignal;
+int ia32_setup_rt_frame(int sig, struct ksignal *ksig,
+			compat_sigset_t *set, struct pt_regs *regs);
+int ia32_setup_frame(int sig, struct ksignal *ksig,
+		     compat_sigset_t *set, struct pt_regs *regs);
+#else
+# define user_i387_ia32_struct	user_i387_struct
+# define user32_fxsr_struct	user_fxsr_struct
+# define ia32_setup_frame	__setup_frame
+# define ia32_setup_rt_frame	__setup_rt_frame
+#endif
+
+extern unsigned int mxcsr_feature_mask;
+extern void fpu__cpu_init(void);
+extern void eager_fpu_init(void);
+
+DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
+
+extern void convert_from_fxsr(struct user_i387_ia32_struct *env,
+			      struct task_struct *tsk);
+extern void convert_to_fxsr(struct task_struct *tsk,
+			    const struct user_i387_ia32_struct *env);
+
+extern user_regset_active_fn fpregs_active, xfpregs_active;
+extern user_regset_get_fn fpregs_get, xfpregs_get, fpregs_soft_get,
+				xstateregs_get;
+extern user_regset_set_fn fpregs_set, xfpregs_set, fpregs_soft_set,
+				 xstateregs_set;
+
+/*
+ * xstateregs_active == fpregs_active. Please refer to the comment
+ * at the definition of fpregs_active.
+ */
+#define xstateregs_active	fpregs_active
+
+#ifdef CONFIG_MATH_EMULATION
+extern void finit_soft_fpu(struct i387_soft_struct *soft);
+#else
+static inline void finit_soft_fpu(struct i387_soft_struct *soft) {}
+#endif
+
+/*
+ * Must be run with preemption disabled: this clears the fpu_fpregs_owner_ctx,
+ * on this CPU.
+ *
+ * This will disable any lazy FPU state restore of the current FPU state,
+ * but if the current thread owns the FPU, it will still be saved by.
+ */
+static inline void __cpu_disable_lazy_restore(unsigned int cpu)
+{
+	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;
+}
+
+static inline int fpu_want_lazy_restore(struct fpu *fpu, unsigned int cpu)
+{
+	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
+}
+
+static inline int is_ia32_compat_frame(void)
+{
+	return config_enabled(CONFIG_IA32_EMULATION) &&
+	       test_thread_flag(TIF_IA32);
+}
+
+static inline int is_ia32_frame(void)
+{
+	return config_enabled(CONFIG_X86_32) || is_ia32_compat_frame();
+}
+
+static inline int is_x32_frame(void)
+{
+	return config_enabled(CONFIG_X86_X32_ABI) && test_thread_flag(TIF_X32);
+}
+
+#define X87_FSW_ES (1 << 7)	/* Exception Summary */
+
+static __always_inline __pure bool use_eager_fpu(void)
+{
+	return static_cpu_has_safe(X86_FEATURE_EAGER_FPU);
+}
+
+static __always_inline __pure bool use_xsaveopt(void)
+{
+	return static_cpu_has_safe(X86_FEATURE_XSAVEOPT);
+}
+
+static __always_inline __pure bool use_xsave(void)
+{
+	return static_cpu_has_safe(X86_FEATURE_XSAVE);
+}
+
+static __always_inline __pure bool use_fxsr(void)
+{
+	return static_cpu_has_safe(X86_FEATURE_FXSR);
+}
+
+static inline void fx_finit(struct i387_fxsave_struct *fx)
+{
+	fx->cwd = 0x37f;
+	fx->mxcsr = MXCSR_DEFAULT;
+}
+
+extern void __sanitize_i387_state(struct task_struct *);
+
+static inline void sanitize_i387_state(struct task_struct *tsk)
+{
+	if (!use_xsaveopt())
+		return;
+	__sanitize_i387_state(tsk);
+}
+
+#define user_insn(insn, output, input...)				\
+({									\
+	int err;							\
+	asm volatile(ASM_STAC "\n"					\
+		     "1:" #insn "\n\t"					\
+		     "2: " ASM_CLAC "\n"				\
+		     ".section .fixup,\"ax\"\n"				\
+		     "3:  movl $-1,%[err]\n"				\
+		     "    jmp  2b\n"					\
+		     ".previous\n"					\
+		     _ASM_EXTABLE(1b, 3b)				\
+		     : [err] "=r" (err), output				\
+		     : "0"(0), input);					\
+	err;								\
+})
+
+#define check_insn(insn, output, input...)				\
+({									\
+	int err;							\
+	asm volatile("1:" #insn "\n\t"					\
+		     "2:\n"						\
+		     ".section .fixup,\"ax\"\n"				\
+		     "3:  movl $-1,%[err]\n"				\
+		     "    jmp  2b\n"					\
+		     ".previous\n"					\
+		     _ASM_EXTABLE(1b, 3b)				\
+		     : [err] "=r" (err), output				\
+		     : "0"(0), input);					\
+	err;								\
+})
+
+static inline int fsave_user(struct i387_fsave_struct __user *fx)
+{
+	return user_insn(fnsave %[fx]; fwait,  [fx] "=m" (*fx), "m" (*fx));
+}
+
+static inline int fxsave_user(struct i387_fxsave_struct __user *fx)
+{
+	if (config_enabled(CONFIG_X86_32))
+		return user_insn(fxsave %[fx], [fx] "=m" (*fx), "m" (*fx));
+	else if (config_enabled(CONFIG_AS_FXSAVEQ))
+		return user_insn(fxsaveq %[fx], [fx] "=m" (*fx), "m" (*fx));
+
+	/* See comment in fpu_fxsave() below. */
+	return user_insn(rex64/fxsave (%[fx]), "=m" (*fx), [fx] "R" (fx));
+}
+
+static inline int fxrstor_checking(struct i387_fxsave_struct *fx)
+{
+	if (config_enabled(CONFIG_X86_32))
+		return check_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+	else if (config_enabled(CONFIG_AS_FXSAVEQ))
+		return check_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
+
+	/* See comment in fpu_fxsave() below. */
+	return check_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx),
+			  "m" (*fx));
+}
+
+static inline int fxrstor_user(struct i387_fxsave_struct __user *fx)
+{
+	if (config_enabled(CONFIG_X86_32))
+		return user_insn(fxrstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+	else if (config_enabled(CONFIG_AS_FXSAVEQ))
+		return user_insn(fxrstorq %[fx], "=m" (*fx), [fx] "m" (*fx));
+
+	/* See comment in fpu_fxsave() below. */
+	return user_insn(rex64/fxrstor (%[fx]), "=m" (*fx), [fx] "R" (fx),
+			  "m" (*fx));
+}
+
+static inline int frstor_checking(struct i387_fsave_struct *fx)
+{
+	return check_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+}
+
+static inline int frstor_user(struct i387_fsave_struct __user *fx)
+{
+	return user_insn(frstor %[fx], "=m" (*fx), [fx] "m" (*fx));
+}
+
+static inline void fpu_fxsave(struct fpu *fpu)
+{
+	if (config_enabled(CONFIG_X86_32))
+		asm volatile( "fxsave %[fx]" : [fx] "=m" (fpu->state->fxsave));
+	else if (config_enabled(CONFIG_AS_FXSAVEQ))
+		asm volatile("fxsaveq %[fx]" : [fx] "=m" (fpu->state->fxsave));
+	else {
+		/* Using "rex64; fxsave %0" is broken because, if the memory
+		 * operand uses any extended registers for addressing, a second
+		 * REX prefix will be generated (to the assembler, rex64
+		 * followed by semicolon is a separate instruction), and hence
+		 * the 64-bitness is lost.
+		 *
+		 * Using "fxsaveq %0" would be the ideal choice, but is only
+		 * supported starting with gas 2.16.
+		 *
+		 * Using, as a workaround, the properly prefixed form below
+		 * isn't accepted by any binutils version so far released,
+		 * complaining that the same type of prefix is used twice if
+		 * an extended register is needed for addressing (fix submitted
+		 * to mainline 2005-11-21).
+		 *
+		 *  asm volatile("rex64/fxsave %0" : "=m" (fpu->state->fxsave));
+		 *
+		 * This, however, we can work around by forcing the compiler to
+		 * select an addressing mode that doesn't require extended
+		 * registers.
+		 */
+		asm volatile( "rex64/fxsave (%[fx])"
+			     : "=m" (fpu->state->fxsave)
+			     : [fx] "R" (&fpu->state->fxsave));
+	}
+}
+
+/*
+ * These must be called with preempt disabled. Returns
+ * 'true' if the FPU state is still intact.
+ */
+static inline int fpu_save_init(struct fpu *fpu)
+{
+	if (use_xsave()) {
+		xsave_state(&fpu->state->xsave);
+
+		/*
+		 * xsave header may indicate the init state of the FP.
+		 */
+		if (!(fpu->state->xsave.xsave_hdr.xstate_bv & XSTATE_FP))
+			return 1;
+	} else if (use_fxsr()) {
+		fpu_fxsave(fpu);
+	} else {
+		asm volatile("fnsave %[fx]; fwait"
+			     : [fx] "=m" (fpu->state->fsave));
+		return 0;
+	}
+
+	/*
+	 * If exceptions are pending, we need to clear them so
+	 * that we don't randomly get exceptions later.
+	 *
+	 * FIXME! Is this perhaps only true for the old-style
+	 * irq13 case? Maybe we could leave the x87 state
+	 * intact otherwise?
+	 */
+	if (unlikely(fpu->state->fxsave.swd & X87_FSW_ES)) {
+		asm volatile("fnclex");
+		return 0;
+	}
+	return 1;
+}
+
+static inline int fpu_restore_checking(struct fpu *fpu)
+{
+	if (use_xsave())
+		return fpu_xrstor_checking(&fpu->state->xsave);
+	else if (use_fxsr())
+		return fxrstor_checking(&fpu->state->fxsave);
+	else
+		return frstor_checking(&fpu->state->fsave);
+}
+
+static inline int restore_fpu_checking(struct fpu *fpu)
+{
+	/*
+	 * AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception is
+	 * pending. Clear the x87 state here by setting it to fixed values.
+	 * "m" is a random variable that should be in L1.
+	 */
+	if (unlikely(static_cpu_has_bug_safe(X86_BUG_FXSAVE_LEAK))) {
+		asm volatile(
+			"fnclex\n\t"
+			"emms\n\t"
+			"fildl %P[addr]"	/* set F?P to defined value */
+			: : [addr] "m" (fpu->has_fpu));
+	}
+
+	return fpu_restore_checking(fpu);
+}
+
+/* Must be paired with an 'stts' after! */
+static inline void __thread_clear_has_fpu(struct fpu *fpu)
+{
+	fpu->has_fpu = 0;
+	this_cpu_write(fpu_fpregs_owner_ctx, NULL);
+}
+
+/* Must be paired with a 'clts' before! */
+static inline void __thread_set_has_fpu(struct fpu *fpu)
+{
+	fpu->has_fpu = 1;
+	this_cpu_write(fpu_fpregs_owner_ctx, fpu);
+}
+
+/*
+ * Encapsulate the CR0.TS handling together with the
+ * software flag.
+ *
+ * These generally need preemption protection to work,
+ * do try to avoid using these on their own.
+ */
+static inline void __thread_fpu_end(struct fpu *fpu)
+{
+	__thread_clear_has_fpu(fpu);
+	if (!use_eager_fpu())
+		stts();
+}
+
+static inline void __thread_fpu_begin(struct fpu *fpu)
+{
+	if (!use_eager_fpu())
+		clts();
+	__thread_set_has_fpu(fpu);
+}
+
+static inline void drop_fpu(struct fpu *fpu)
+{
+	/*
+	 * Forget coprocessor state..
+	 */
+	preempt_disable();
+	fpu->counter = 0;
+
+	if (fpu->has_fpu) {
+		/* Ignore delayed exceptions from user space */
+		asm volatile("1: fwait\n"
+			     "2:\n"
+			     _ASM_EXTABLE(1b, 2b));
+		__thread_fpu_end(fpu);
+	}
+
+	fpu->fpstate_active = 0;
+
+	preempt_enable();
+}
+
+static inline void restore_init_xstate(void)
+{
+	if (use_xsave())
+		xrstor_state(init_xstate_buf, -1);
+	else
+		fxrstor_checking(&init_xstate_buf->i387);
+}
+
+/*
+ * Reset the FPU state in the eager case and drop it in the lazy case (later use
+ * will reinit it).
+ */
+static inline void fpu_reset_state(struct fpu *fpu)
+{
+	if (!use_eager_fpu())
+		drop_fpu(fpu);
+	else
+		restore_init_xstate();
+}
+
+/*
+ * FPU state switching for scheduling.
+ *
+ * This is a two-stage process:
+ *
+ *  - switch_fpu_prepare() saves the old state and
+ *    sets the new state of the CR0.TS bit. This is
+ *    done within the context of the old process.
+ *
+ *  - switch_fpu_finish() restores the new state as
+ *    necessary.
+ */
+typedef struct { int preload; } fpu_switch_t;
+
+static inline fpu_switch_t
+switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)
+{
+	fpu_switch_t fpu;
+
+	/*
+	 * If the task has used the math, pre-load the FPU on xsave processors
+	 * or if the past 5 consecutive context-switches used math.
+	 */
+	fpu.preload = new_fpu->fpstate_active &&
+		      (use_eager_fpu() || new_fpu->counter > 5);
+
+	if (old_fpu->has_fpu) {
+		if (!fpu_save_init(old_fpu))
+			old_fpu->last_cpu = -1;
+		else
+			old_fpu->last_cpu = cpu;
+
+		/* But leave fpu_fpregs_owner_ctx! */
+		old_fpu->has_fpu = 0;
+
+		/* Don't change CR0.TS if we just switch! */
+		if (fpu.preload) {
+			new_fpu->counter++;
+			__thread_set_has_fpu(new_fpu);
+			prefetch(new_fpu->state);
+		} else if (!use_eager_fpu())
+			stts();
+	} else {
+		old_fpu->counter = 0;
+		old_fpu->last_cpu = -1;
+		if (fpu.preload) {
+			new_fpu->counter++;
+			if (fpu_want_lazy_restore(new_fpu, cpu))
+				fpu.preload = 0;
+			else
+				prefetch(new_fpu->state);
+			__thread_fpu_begin(new_fpu);
+		}
+	}
+	return fpu;
+}
+
+/*
+ * By the time this gets called, we've already cleared CR0.TS and
+ * given the process the FPU if we are going to preload the FPU
+ * state - all we need to do is to conditionally restore the register
+ * state itself.
+ */
+static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)
+{
+	if (fpu_switch.preload) {
+		if (unlikely(restore_fpu_checking(new_fpu)))
+			fpu_reset_state(new_fpu);
+	}
+}
+
+/*
+ * Signal frame handlers...
+ */
+extern int save_xstate_sig(void __user *buf, void __user *fx, int size);
+extern int __restore_xstate_sig(void __user *buf, void __user *fx, int size);
+
+static inline int xstate_sigframe_size(void)
+{
+	return use_xsave() ? xstate_size + FP_XSTATE_MAGIC2_SIZE : xstate_size;
+}
+
+static inline int restore_xstate_sig(void __user *buf, int ia32_frame)
+{
+	void __user *buf_fx = buf;
+	int size = xstate_sigframe_size();
+
+	if (ia32_frame && use_fxsr()) {
+		buf_fx = buf + sizeof(struct i387_fsave_struct);
+		size += sizeof(struct i387_fsave_struct);
+	}
+
+	return __restore_xstate_sig(buf, buf_fx, size);
+}
+
+/*
+ * Needs to be preemption-safe.
+ *
+ * NOTE! user_fpu_begin() must be used only immediately before restoring
+ * the save state. It does not do any saving/restoring on its own. In
+ * lazy FPU mode, it is just an optimization to avoid a #NM exception,
+ * the task can lose the FPU right after preempt_enable().
+ */
+static inline void user_fpu_begin(void)
+{
+	struct fpu *fpu = &current->thread.fpu;
+
+	preempt_disable();
+	if (!user_has_fpu())
+		__thread_fpu_begin(fpu);
+	preempt_enable();
+}
+
+/*
+ * i387 state interaction
+ */
+static inline unsigned short get_fpu_cwd(struct task_struct *tsk)
+{
+	if (cpu_has_fxsr) {
+		return tsk->thread.fpu.state->fxsave.cwd;
+	} else {
+		return (unsigned short)tsk->thread.fpu.state->fsave.cwd;
+	}
+}
+
+static inline unsigned short get_fpu_swd(struct task_struct *tsk)
+{
+	if (cpu_has_fxsr) {
+		return tsk->thread.fpu.state->fxsave.swd;
+	} else {
+		return (unsigned short)tsk->thread.fpu.state->fsave.swd;
+	}
+}
+
+static inline unsigned short get_fpu_mxcsr(struct task_struct *tsk)
+{
+	if (cpu_has_xmm) {
+		return tsk->thread.fpu.state->fxsave.mxcsr;
+	} else {
+		return MXCSR_DEFAULT;
+	}
+}
+
+extern void fpstate_cache_init(void);
+
+extern int fpstate_alloc(struct fpu *fpu);
+extern void fpstate_free(struct fpu *fpu);
+extern int fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu);
+
+static inline unsigned long
+alloc_mathframe(unsigned long sp, int ia32_frame, unsigned long *buf_fx,
+		unsigned long *size)
+{
+	unsigned long frame_size = xstate_sigframe_size();
+
+	*buf_fx = sp = round_down(sp - frame_size, 64);
+	if (ia32_frame && use_fxsr()) {
+		frame_size += sizeof(struct i387_fsave_struct);
+		sp -= sizeof(struct i387_fsave_struct);
+	}
+
+	*size = frame_size;
+	return sp;
+}
+
+#endif /* _ASM_X86_FPU_INTERNAL_H */
