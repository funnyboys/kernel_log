commit 186525bd6b83efc592672e2d6185e4d7c810d2b4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Nov 29 08:17:25 2019 +0100

    mm, x86/mm: Untangle address space layout definitions from basic pgtable type definitions
    
    - Untangle the somewhat incestous way of how VMALLOC_START is used all across the
      kernel, but is, on x86, defined deep inside one of the lowest level page table headers.
      It doesn't help that vmalloc.h only includes a single asm header:
    
         #include <asm/page.h>           /* pgprot_t */
    
      So there was no existing cross-arch way to decouple address layout
      definitions from page.h details. I used this:
    
       #ifndef VMALLOC_START
       # include <asm/vmalloc.h>
       #endif
    
      This way every architecture that wants to simplify page.h can do so.
    
    - Also on x86 we had a couple of LDT related inline functions that used
      the late-stage address space layout positions - but these could be
      uninlined without real trouble - the end result is cleaner this way as
      well.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 0416d42e5bdd..5356a46b0373 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -1,6 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-#ifndef _ASM_X86_PGTABLE_32_DEFS_H
-#define _ASM_X86_PGTABLE_32_DEFS_H
+#ifndef _ASM_X86_PGTABLE_32_TYPES_H
+#define _ASM_X86_PGTABLE_32_TYPES_H
 
 /*
  * The Linux x86 paging architecture is 'compile-time dual-mode', it
@@ -20,55 +20,4 @@
 #define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE - 1))
 
-/* Just any arbitrary offset to the start of the vmalloc VM area: the
- * current 8MB value just means that there will be a 8MB "hole" after the
- * physical memory until the kernel virtual memory starts.  That means that
- * any out-of-bounds memory accesses will hopefully be caught.
- * The vmalloc() routines leaves a hole of 4kB between each vmalloced
- * area for the same reason. ;)
- */
-#define VMALLOC_OFFSET	(8 * 1024 * 1024)
-
-#ifndef __ASSEMBLY__
-extern bool __vmalloc_start_set; /* set once high_memory is set */
-#endif
-
-#define VMALLOC_START	((unsigned long)high_memory + VMALLOC_OFFSET)
-#ifdef CONFIG_X86_PAE
-#define LAST_PKMAP 512
-#else
-#define LAST_PKMAP 1024
-#endif
-
-/*
- * This is an upper bound on sizeof(struct cpu_entry_area) / PAGE_SIZE.
- * Define this here and validate with BUILD_BUG_ON() in cpu_entry_area.c
- * to avoid include recursion hell.
- */
-#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 43)
-
-/* The +1 is for the readonly IDT page: */
-#define CPU_ENTRY_AREA_BASE	\
-	((FIXADDR_TOT_START - PAGE_SIZE*(CPU_ENTRY_AREA_PAGES+1)) & PMD_MASK)
-
-#define LDT_BASE_ADDR		\
-	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) & PMD_MASK)
-
-#define LDT_END_ADDR		(LDT_BASE_ADDR + PMD_SIZE)
-
-#define PKMAP_BASE		\
-	((LDT_BASE_ADDR - PAGE_SIZE) & PMD_MASK)
-
-#ifdef CONFIG_HIGHMEM
-# define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
-#else
-# define VMALLOC_END	(LDT_BASE_ADDR - 2 * PAGE_SIZE)
-#endif
-
-#define MODULES_VADDR	VMALLOC_START
-#define MODULES_END	VMALLOC_END
-#define MODULES_LEN	(MODULES_VADDR - MODULES_END)
-
-#define MAXMEM	(VMALLOC_END - PAGE_OFFSET - __VMALLOC_RESERVE)
-
-#endif /* _ASM_X86_PGTABLE_32_DEFS_H */
+#endif /* _ASM_X86_PGTABLE_32_TYPES_H */

commit dc4e0021b00b5a4ecba56fae509217776592b0aa
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Nov 26 18:27:16 2019 +0100

    x86/doublefault/32: Move #DF stack and TSS to cpu_entry_area
    
    There are three problems with the current layout of the doublefault
    stack and TSS.  First, the TSS is only cacheline-aligned, which is
    not enough -- if the hardware portion of the TSS (struct x86_hw_tss)
    crosses a page boundary, horrible things happen [0].  Second, the
    stack and TSS are global, so simultaneous double faults on different
    CPUs will cause massive corruption.  Third, the whole mechanism
    won't work if user CR3 is loaded, resulting in a triple fault [1].
    
    Let the doublefault stack and TSS share a page (which prevents the
    TSS from spanning a page boundary), make it percpu, and move it into
    cpu_entry_area.  Teach the stack dump code about the doublefault
    stack.
    
    [0] Real hardware will read past the end of the page onto the next
        *physical* page if a task switch happens.  Virtual machines may
        have any number of bugs, and I would consider it reasonable for
        a VM to summarily kill the guest if it tries to task-switch to
        a page-spanning TSS.
    
    [1] Real hardware triple faults.  At least some VMs seem to hang.
        I'm not sure what's going on.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 19f5807260c3..0416d42e5bdd 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -41,10 +41,11 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
 #endif
 
 /*
- * Define this here and validate with BUILD_BUG_ON() in pgtable_32.c
- * to avoid include recursion hell
+ * This is an upper bound on sizeof(struct cpu_entry_area) / PAGE_SIZE.
+ * Define this here and validate with BUILD_BUG_ON() in cpu_entry_area.c
+ * to avoid include recursion hell.
  */
-#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 41)
+#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 43)
 
 /* The +1 is for the readonly IDT page: */
 #define CPU_ENTRY_AREA_BASE	\

commit ab851d49f6bfc781edd8bd44c72ec1e49211670b
Merge: 1d87200446f1 e3cb0c7102f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 11:12:02 2019 -0800

    Merge branch 'x86-iopl-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 iopl updates from Ingo Molnar:
     "This implements a nice simplification of the iopl and ioperm code that
      Thomas Gleixner discovered: we can implement the IO privilege features
      of the iopl system call by using the IO permission bitmap in
      permissive mode, while trapping CLI/STI/POPF/PUSHF uses in user-space
      if they change the interrupt flag.
    
      This implements that feature, with testing facilities and related
      cleanups"
    
    [ "Simplification" may be an over-statement. The main goal is to avoid
      the cli/sti of iopl by effectively implementing the IO port access
      parts of iopl in terms of ioperm.
    
      This may end up not workign well in case people actually depend on
      cli/sti being available, or if there are mixed uses of iopl and
      ioperm. We will see..       - Linus ]
    
    * 'x86-iopl-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      x86/ioperm: Fix use of deprecated config option
      x86/entry/32: Clarify register saving in __switch_to_asm()
      selftests/x86/iopl: Extend test to cover IOPL emulation
      x86/ioperm: Extend IOPL config to control ioperm() as well
      x86/iopl: Remove legacy IOPL option
      x86/iopl: Restrict iopl() permission scope
      x86/iopl: Fixup misleading comment
      selftests/x86/ioperm: Extend testing so the shared bitmap is exercised
      x86/ioperm: Share I/O bitmap if identical
      x86/ioperm: Remove bitmap if all permissions dropped
      x86/ioperm: Move TSS bitmap update to exit to user work
      x86/ioperm: Add bitmap sequence number
      x86/ioperm: Move iobitmap data into a struct
      x86/tss: Move I/O bitmap data into a seperate struct
      x86/io: Speedup schedule out of I/O bitmap user
      x86/ioperm: Avoid bitmap allocation if no permissions are set
      x86/ioperm: Simplify first ioperm() invocation logic
      x86/iopl: Cleanup include maze
      x86/tss: Fix and move VMX BUILD_BUG_ON()
      x86/cpu: Unify cpu_init()
      ...

commit 05b042a1944322844eaae7ea596d5f154166d68a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Nov 24 11:21:44 2019 +0100

    x86/pti/32: Calculate the various PTI cpu_entry_area sizes correctly, make the CPU_ENTRY_AREA_PAGES assert precise
    
    When two recent commits that increased the size of the 'struct cpu_entry_area'
    were merged in -tip, the 32-bit defconfig build started failing on the following
    build time assert:
    
      ./include/linux/compiler.h:391:38: error: call to ‘__compiletime_assert_189’ declared with attribute error: BUILD_BUG_ON failed: CPU_ENTRY_AREA_PAGES * PAGE_SIZE < CPU_ENTRY_AREA_MAP_SIZE
      arch/x86/mm/cpu_entry_area.c:189:2: note: in expansion of macro ‘BUILD_BUG_ON’
      In function ‘setup_cpu_entry_area_ptes’,
    
    Which corresponds to the following build time assert:
    
            BUILD_BUG_ON(CPU_ENTRY_AREA_PAGES * PAGE_SIZE < CPU_ENTRY_AREA_MAP_SIZE);
    
    The purpose of this assert is to sanity check the fixed-value definition of
    CPU_ENTRY_AREA_PAGES arch/x86/include/asm/pgtable_32_types.h:
    
            #define CPU_ENTRY_AREA_PAGES    (NR_CPUS * 41)
    
    The '41' is supposed to match sizeof(struct cpu_entry_area)/PAGE_SIZE, which value
    we didn't want to define in such a low level header, because it would cause
    dependency hell.
    
    Every time the size of cpu_entry_area is changed, we have to adjust CPU_ENTRY_AREA_PAGES
    accordingly - and this assert is checking that constraint.
    
    But the assert is both imprecise and buggy, primarily because it doesn't
    include the single readonly IDT page that is mapped at CPU_ENTRY_AREA_BASE
    (which begins at a PMD boundary).
    
    This bug was hidden by the fact that by accident CPU_ENTRY_AREA_PAGES is defined
    too large upstream (v5.4-rc8):
    
            #define CPU_ENTRY_AREA_PAGES    (NR_CPUS * 40)
    
    While 'struct cpu_entry_area' is 155648 bytes, or 38 pages. So we had two extra
    pages, which hid the bug.
    
    The following commit (not yet upstream) increased the size to 40 pages:
    
      x86/iopl: ("Restrict iopl() permission scope")
    
    ... but increased CPU_ENTRY_AREA_PAGES only 41 - i.e. shortening the gap
    to just 1 extra page.
    
    Then another not-yet-upstream commit changed the size again:
    
      880a98c33996: ("x86/cpu_entry_area: Add guard page for entry stack on 32bit")
    
    Which increased the cpu_entry_area size from 38 to 39 pages, but
    didn't change CPU_ENTRY_AREA_PAGES (kept it at 40). This worked
    fine, because we still had a page left from the accidental 'reserve'.
    
    But when these two commits were merged into the same tree, the
    combined size of cpu_entry_area grew from 38 to 40 pages, while
    CPU_ENTRY_AREA_PAGES finally caught up to 40 as well.
    
    Which is fine in terms of functionality, but the assert broke:
    
            BUILD_BUG_ON(CPU_ENTRY_AREA_PAGES * PAGE_SIZE < CPU_ENTRY_AREA_MAP_SIZE);
    
    because CPU_ENTRY_AREA_MAP_SIZE is the total size of the area,
    which is 1 page larger due to the IDT page.
    
    To fix all this, change the assert to two precise asserts:
    
            BUILD_BUG_ON((CPU_ENTRY_AREA_PAGES+1)*PAGE_SIZE != CPU_ENTRY_AREA_MAP_SIZE);
            BUILD_BUG_ON(CPU_ENTRY_AREA_TOTAL_SIZE != CPU_ENTRY_AREA_MAP_SIZE);
    
    This takes the IDT page into account, and also connects the size-based
    define of CPU_ENTRY_AREA_TOTAL_SIZE with the address-subtraction based
    define of CPU_ENTRY_AREA_MAP_SIZE.
    
    Also clean up some of the names which made it rather confusing:
    
     - 'CPU_ENTRY_AREA_TOT_SIZE' wasn't actually the 'total' size of
       the cpu-entry-area, but the per-cpu array size, so rename this
       to CPU_ENTRY_AREA_ARRAY_SIZE.
    
     - Introduce CPU_ENTRY_AREA_TOTAL_SIZE that _is_ the total mapping
       size, with the IDT included.
    
     - Add comments where '+1' denotes the IDT mapping - it wasn't
       obvious and took me about 3 hours to decode...
    
    Finally, because this particular commit is actually applied after
    this patch:
    
      880a98c33996: ("x86/cpu_entry_area: Add guard page for entry stack on 32bit")
    
    Fix the CPU_ENTRY_AREA_PAGES value from 40 pages to the correct 39 pages.
    
    All future commits that change cpu_entry_area will have to adjust
    this value precisely.
    
    As a side note, we should probably attempt to remove CPU_ENTRY_AREA_PAGES
    and derive its value directly from the structure, without causing
    header hell - but that is an adventure for another day! :-)
    
    Fixes: 880a98c33996: ("x86/cpu_entry_area: Add guard page for entry stack on 32bit")
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: stable@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index b0bc0fff5f1f..1636eb8e5a5b 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -44,11 +44,11 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
  * Define this here and validate with BUILD_BUG_ON() in pgtable_32.c
  * to avoid include recursion hell
  */
-#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 40)
+#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 39)
 
-#define CPU_ENTRY_AREA_BASE						\
-	((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1))   \
-	 & PMD_MASK)
+/* The +1 is for the readonly IDT page: */
+#define CPU_ENTRY_AREA_BASE	\
+	((FIXADDR_TOT_START - PAGE_SIZE*(CPU_ENTRY_AREA_PAGES+1)) & PMD_MASK)
 
 #define LDT_BASE_ADDR		\
 	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) & PMD_MASK)

commit c8137ace56383688af911fea5934c71ad158135e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:28 2019 +0100

    x86/iopl: Restrict iopl() permission scope
    
    The access to the full I/O port range can be also provided by the TSS I/O
    bitmap, but that would require to copy 8k of data on scheduling in the
    task. As shown with the sched out optimization TSS.io_bitmap_base can be
    used to switch the incoming task to a preallocated I/O bitmap which has all
    bits zero, i.e. allows access to all I/O ports.
    
    Implementing this allows to provide an iopl() emulation mode which restricts
    the IOPL level 3 permissions to I/O port access but removes the STI/CLI
    permission which is coming with the hardware IOPL mechansim.
    
    Provide a config option to switch IOPL to emulation mode, make it the
    default and while at it also provide an option to disable IOPL completely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index b0bc0fff5f1f..0fab4bfb4df2 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -44,7 +44,7 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
  * Define this here and validate with BUILD_BUG_ON() in pgtable_32.c
  * to avoid include recursion hell
  */
-#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 40)
+#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 41)
 
 #define CPU_ENTRY_AREA_BASE						\
 	((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1))   \

commit 8195d869d118bc30bf0be8d0c5d8849d6f58529b
Author: Joerg Roedel <jroedel@suse.de>
Date:   Wed Jul 18 11:41:11 2018 +0200

    x86/ldt: Define LDT_END_ADDR
    
    It marks the end of the address-space range reserved for the LDT. The
    LDT-code will use it when unmapping the LDT for user-space.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Pavel Machek <pavel@ucw.cz>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: linux-mm@kvack.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Waiman Long <llong@redhat.com>
    Cc: "David H . Gutteridge" <dhgutteridge@sympatico.ca>
    Cc: joro@8bytes.org
    Link: https://lkml.kernel.org/r/1531906876-13451-35-git-send-email-joro@8bytes.org

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 7297810747cf..b0bc0fff5f1f 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -53,6 +53,8 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
 #define LDT_BASE_ADDR		\
 	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) & PMD_MASK)
 
+#define LDT_END_ADDR		(LDT_BASE_ADDR + PMD_SIZE)
+
 #define PKMAP_BASE		\
 	((LDT_BASE_ADDR - PAGE_SIZE) & PMD_MASK)
 

commit f3e48e546c42e31c0c095a6f917a4ad64668608c
Author: Joerg Roedel <jroedel@suse.de>
Date:   Wed Jul 18 11:41:10 2018 +0200

    x86/ldt: Reserve address-space range on 32 bit for the LDT
    
    Reserve 2MB/4MB of address-space for mapping the LDT to user-space on 32
    bit PTI kernels.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Pavel Machek <pavel@ucw.cz>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: linux-mm@kvack.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Waiman Long <llong@redhat.com>
    Cc: "David H . Gutteridge" <dhgutteridge@sympatico.ca>
    Cc: joro@8bytes.org
    Link: https://lkml.kernel.org/r/1531906876-13451-34-git-send-email-joro@8bytes.org

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index d9a001a4a872..7297810747cf 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -50,13 +50,16 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
 	((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1))   \
 	 & PMD_MASK)
 
-#define PKMAP_BASE		\
+#define LDT_BASE_ADDR		\
 	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) & PMD_MASK)
 
+#define PKMAP_BASE		\
+	((LDT_BASE_ADDR - PAGE_SIZE) & PMD_MASK)
+
 #ifdef CONFIG_HIGHMEM
 # define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
 #else
-# define VMALLOC_END	(CPU_ENTRY_AREA_BASE - 2 * PAGE_SIZE)
+# define VMALLOC_END	(LDT_BASE_ADDR - 2 * PAGE_SIZE)
 #endif
 
 #define MODULES_VADDR	VMALLOC_START

commit ed7588d5dc6f5e7202fb9bbeb14d94706ba225d7
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri May 18 13:35:24 2018 +0300

    x86/mm: Stop pretending pgtable_l5_enabled is a variable
    
    pgtable_l5_enabled is defined using cpu_feature_enabled() but we refer
    to it as a variable. This is misleading.
    
    Make pgtable_l5_enabled() a function.
    
    We cannot literally define it as a function due to circular dependencies
    between header files. Function-alike macros is close enough.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20180518103528.59260-4-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index e3225e83db7d..d9a001a4a872 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -15,7 +15,7 @@
 # include <asm/pgtable-2level_types.h>
 #endif
 
-#define pgtable_l5_enabled 0
+#define pgtable_l5_enabled() 0
 
 #define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE - 1))

commit e626e6bb0dfaca41487241d49ce0ae827716101a
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Feb 14 14:16:51 2018 +0300

    x86/mm: Introduce 'pgtable_l5_enabled'
    
    The new flag would indicate what paging mode we are in.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20180214111656.88514-5-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 0777e18a1d23..e3225e83db7d 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -15,6 +15,8 @@
 # include <asm/pgtable-2level_types.h>
 #endif
 
+#define pgtable_l5_enabled 0
+
 #define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE - 1))
 

commit 55f49fcb879fbeebf2a8c1ac7c9e6d90df55f798
Author: William Grant <william.grant@canonical.com>
Date:   Tue Jan 30 22:22:55 2018 +1100

    x86/mm: Fix overlap of i386 CPU_ENTRY_AREA with FIX_BTMAP
    
    Since commit 92a0f81d8957 ("x86/cpu_entry_area: Move it out of the
    fixmap"), i386's CPU_ENTRY_AREA has been mapped to the memory area just
    below FIXADDR_START. But already immediately before FIXADDR_START is the
    FIX_BTMAP area, which means that early_ioremap can collide with the entry
    area.
    
    It's especially bad on PAE where FIX_BTMAP_BEGIN gets aligned to exactly
    match CPU_ENTRY_AREA_BASE, so the first early_ioremap slot clobbers the
    IDT and causes interrupts during early boot to reset the system.
    
    The overlap wasn't a problem before the CPU entry area was introduced,
    as the fixmap has classically been preceded by the pkmap or vmalloc
    areas, neither of which is used until early_ioremap is out of the
    picture.
    
    Relocate CPU_ENTRY_AREA to below FIX_BTMAP, not just below the permanent
    fixmap area.
    
    Fixes: commit 92a0f81d8957 ("x86/cpu_entry_area: Move it out of the fixmap")
    Signed-off-by: William Grant <william.grant@canonical.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/7041d181-a019-e8b9-4e4e-48215f841e2c@canonical.com

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index ce245b0cdfca..0777e18a1d23 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -44,8 +44,9 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
  */
 #define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 40)
 
-#define CPU_ENTRY_AREA_BASE				\
-	((FIXADDR_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) & PMD_MASK)
+#define CPU_ENTRY_AREA_BASE						\
+	((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1))   \
+	 & PMD_MASK)
 
 #define PKMAP_BASE		\
 	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) & PMD_MASK)

commit 92a0f81d89571e3e8759366e050ee05cc545ef99
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 20 18:51:31 2017 +0100

    x86/cpu_entry_area: Move it out of the fixmap
    
    Put the cpu_entry_area into a separate P4D entry. The fixmap gets too big
    and 0-day already hit a case where the fixmap PTEs were cleared by
    cleanup_highmap().
    
    Aside of that the fixmap API is a pain as it's all backwards.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index f2ca9b28fd68..ce245b0cdfca 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -38,13 +38,22 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
 #define LAST_PKMAP 1024
 #endif
 
-#define PKMAP_BASE ((FIXADDR_START - PAGE_SIZE * (LAST_PKMAP + 1))	\
-		    & PMD_MASK)
+/*
+ * Define this here and validate with BUILD_BUG_ON() in pgtable_32.c
+ * to avoid include recursion hell
+ */
+#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 40)
+
+#define CPU_ENTRY_AREA_BASE				\
+	((FIXADDR_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) & PMD_MASK)
+
+#define PKMAP_BASE		\
+	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) & PMD_MASK)
 
 #ifdef CONFIG_HIGHMEM
 # define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
 #else
-# define VMALLOC_END	(FIXADDR_START - 2 * PAGE_SIZE)
+# define VMALLOC_END	(CPU_ENTRY_AREA_BASE - 2 * PAGE_SIZE)
 #endif
 
 #define MODULES_VADDR	VMALLOC_START

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 9fb2f2bc8245..f2ca9b28fd68 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_PGTABLE_32_DEFS_H
 #define _ASM_X86_PGTABLE_32_DEFS_H
 

commit 63e7b6d90c1ebdb73d82eedb945b62ceedc3f963
Author: Minfei Huang <mnfhuang@gmail.com>
Date:   Wed Oct 29 19:38:37 2014 +0800

    x86: mm: Re-use the early_ioremap fixed area
    
    The temp fixed area is only used during boot for early_ioremap(), and
    it is unused when ioremap() is functional. vmalloc/pkmap area become
    available after early boot so the temp fixed area is available for
    re-use.
    
    The virtual address is more precious on i386, especially turning on
    high memory. So we can re-use the virtual address space.
    
    Remove the now unused defines FIXADDR_BOOT_START and FIXADDR_BOOT_SIZE.
    
    Signed-off-by: Minfei Huang <mnfhuang@gmail.com>
    Cc: pbonzini@redhat.com
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/1414582717-32729-1-git-send-email-mnfhuang@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 2ac5fc86abb3..9fb2f2bc8245 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -37,13 +37,13 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
 #define LAST_PKMAP 1024
 #endif
 
-#define PKMAP_BASE ((FIXADDR_BOOT_START - PAGE_SIZE * (LAST_PKMAP + 1))	\
+#define PKMAP_BASE ((FIXADDR_START - PAGE_SIZE * (LAST_PKMAP + 1))	\
 		    & PMD_MASK)
 
 #ifdef CONFIG_HIGHMEM
 # define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
 #else
-# define VMALLOC_END	(FIXADDR_BOOT_START - 2 * PAGE_SIZE)
+# define VMALLOC_END	(FIXADDR_START - 2 * PAGE_SIZE)
 #endif
 
 #define MODULES_VADDR	VMALLOC_START

commit 96e70f83285676d8794f62f3c294d0247bef6b21
Author: Minfei Huang <mnfhuang@gmail.com>
Date:   Thu Oct 23 23:36:17 2014 +0800

    x86/mm: Avoid overlap the fixmap area on i386
    
    It is a problem when configuring high memory off where the
    vmalloc reserve area could end up overlapping the early_ioremap
    fixmap area on i386.
    
    The ordering of the VMALLOC_RESERVE space is:
    
     FIXADDR_TOP
                           fixed_addresses
     FIXADDR_START
                           early_ioremap fixed addresses
     FIXADDR_BOOT_START
                           Persistent kmap area
     PKMAP_BASE
     VMALLOC_END
                           Vmalloc area
     VMALLOC_START
     high_memory
    
    The available address we can use is lower than
    FIXADDR_BOOT_START. So we will set the kmap boundary below the
    FIXADDR_BOOT_START, if we configure high memory.
    
    If we configure high memory, the vmalloc reserve area should
    end up to PKMAP_BASE, otherwise should end up to
    FIXADDR_BOOT_START.
    
    Signed-off-by: Minfei Huang <mnfhuang@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/6B680A9E-6CE9-4C96-934B-CB01DCB58278@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index ed5903be26fe..2ac5fc86abb3 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -43,7 +43,7 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
 #ifdef CONFIG_HIGHMEM
 # define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
 #else
-# define VMALLOC_END	(FIXADDR_START - 2 * PAGE_SIZE)
+# define VMALLOC_END	(FIXADDR_BOOT_START - 2 * PAGE_SIZE)
 #endif
 
 #define MODULES_VADDR	VMALLOC_START

commit fc4ac7a5f5996712d9123ae4850948c640edb315
Author: Andres Salomon <dilinger@queued.net>
Date:   Sat Jun 5 11:40:42 2010 -0400

    x86: use __ASSEMBLY__ rather than __ASSEMBLER__
    
    As Ingo pointed out in a separate patch, we should be using __ASSEMBLY__.
    Make that the case in pgtable headers.
    
    Signed-off-by: Andres Salomon <dilinger@queued.net>
    LKML-Reference: <20100605114042.35ac69c1@dev.queued.net>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 5e67c1532314..ed5903be26fe 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -26,7 +26,7 @@
  */
 #define VMALLOC_OFFSET	(8 * 1024 * 1024)
 
-#ifndef __ASSEMBLER__
+#ifndef __ASSEMBLY__
 extern bool __vmalloc_start_set; /* set once high_memory is set */
 #endif
 

commit c398df30d5caad626ac72bfab0361a7b0f67a661
Author: Amerigo Wang <amwang@redhat.com>
Date:   Wed Jun 3 21:46:46 2009 -0400

    module: merge module_alloc() finally
    
    As Christoph Hellwig suggested, module_alloc() actually can be
    unified for i386 and x86_64 (of course, also UML).
    
    Signed-off-by: WANG Cong <amwang@redhat.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: 'Ingo Molnar' <mingo@elte.hu>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index 2733fad45f98..5e67c1532314 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -46,6 +46,10 @@ extern bool __vmalloc_start_set; /* set once high_memory is set */
 # define VMALLOC_END	(FIXADDR_START - 2 * PAGE_SIZE)
 #endif
 
+#define MODULES_VADDR	VMALLOC_START
+#define MODULES_END	VMALLOC_END
+#define MODULES_LEN	(MODULES_VADDR - MODULES_END)
+
 #define MAXMEM	(VMALLOC_END - PAGE_OFFSET - __VMALLOC_RESERVE)
 
 #endif /* _ASM_X86_PGTABLE_32_DEFS_H */

commit dc16ecf7fd1fad7436832121435d4926a81d469e
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 16:10:44 2009 -0800

    x86-32: use specific __vmalloc_start_set flag in __virt_addr_valid
    
    Rather than relying on the ever-unreliable system_state,
    add a specific __vmalloc_start_set flag to indicate whether
    the vmalloc area has meaningful boundaries yet, and use that
    in x86-32's __phys_addr and __virt_addr_valid.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
index bd8df3b2fe04..2733fad45f98 100644
--- a/arch/x86/include/asm/pgtable_32_types.h
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -25,6 +25,11 @@
  * area for the same reason. ;)
  */
 #define VMALLOC_OFFSET	(8 * 1024 * 1024)
+
+#ifndef __ASSEMBLER__
+extern bool __vmalloc_start_set; /* set once high_memory is set */
+#endif
+
 #define VMALLOC_START	((unsigned long)high_memory + VMALLOC_OFFSET)
 #ifdef CONFIG_X86_PAE
 #define LAST_PKMAP 512

commit f402a65f93c7127b2bd93a4b2fe182cd859fb4c1
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sun Feb 8 18:49:05 2009 -0800

    x86: Split pgtable_32.h into pgtable_32.h and pgtable_32_types.h
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>

diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h
new file mode 100644
index 000000000000..bd8df3b2fe04
--- /dev/null
+++ b/arch/x86/include/asm/pgtable_32_types.h
@@ -0,0 +1,46 @@
+#ifndef _ASM_X86_PGTABLE_32_DEFS_H
+#define _ASM_X86_PGTABLE_32_DEFS_H
+
+/*
+ * The Linux x86 paging architecture is 'compile-time dual-mode', it
+ * implements both the traditional 2-level x86 page tables and the
+ * newer 3-level PAE-mode page tables.
+ */
+#ifdef CONFIG_X86_PAE
+# include <asm/pgtable-3level_types.h>
+# define PMD_SIZE	(1UL << PMD_SHIFT)
+# define PMD_MASK	(~(PMD_SIZE - 1))
+#else
+# include <asm/pgtable-2level_types.h>
+#endif
+
+#define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
+#define PGDIR_MASK	(~(PGDIR_SIZE - 1))
+
+/* Just any arbitrary offset to the start of the vmalloc VM area: the
+ * current 8MB value just means that there will be a 8MB "hole" after the
+ * physical memory until the kernel virtual memory starts.  That means that
+ * any out-of-bounds memory accesses will hopefully be caught.
+ * The vmalloc() routines leaves a hole of 4kB between each vmalloced
+ * area for the same reason. ;)
+ */
+#define VMALLOC_OFFSET	(8 * 1024 * 1024)
+#define VMALLOC_START	((unsigned long)high_memory + VMALLOC_OFFSET)
+#ifdef CONFIG_X86_PAE
+#define LAST_PKMAP 512
+#else
+#define LAST_PKMAP 1024
+#endif
+
+#define PKMAP_BASE ((FIXADDR_BOOT_START - PAGE_SIZE * (LAST_PKMAP + 1))	\
+		    & PMD_MASK)
+
+#ifdef CONFIG_HIGHMEM
+# define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
+#else
+# define VMALLOC_END	(FIXADDR_START - 2 * PAGE_SIZE)
+#endif
+
+#define MAXMEM	(VMALLOC_END - PAGE_OFFSET - __VMALLOC_RESERVE)
+
+#endif /* _ASM_X86_PGTABLE_32_DEFS_H */
