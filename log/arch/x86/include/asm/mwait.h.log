commit bf09fb6cba4f7099620cc9ed32d94c27c4af992e
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Jun 22 17:51:35 2020 -0700

    KVM: VMX: Stop context switching MSR_IA32_UMWAIT_CONTROL
    
    Remove support for context switching between the guest's and host's
    desired UMWAIT_CONTROL.  Propagating the guest's value to hardware isn't
    required for correct functionality, e.g. KVM intercepts reads and writes
    to the MSR, and the latency effects of the settings controlled by the
    MSR are not architecturally visible.
    
    As a general rule, KVM should not allow the guest to control power
    management settings unless explicitly enabled by userspace, e.g. see
    KVM_CAP_X86_DISABLE_EXITS.  E.g. Intel's SDM explicitly states that C0.2
    can improve the performance of SMT siblings.  A devious guest could
    disable C0.2 so as to improve the performance of their workloads at the
    detriment to workloads running in the host or on other VMs.
    
    Wholesale removal of UMWAIT_CONTROL context switching also fixes a race
    condition where updates from the host may cause KVM to enter the guest
    with the incorrect value.  Because updates are are propagated to all
    CPUs via IPI (SMP function callback), the value in hardware may be
    stale with respect to the cached value and KVM could enter the guest
    with the wrong value in hardware.  As above, the guest can't observe the
    bad value, but it's a weird and confusing wart in the implementation.
    
    Removal also fixes the unnecessary usage of VMX's atomic load/store MSR
    lists.  Using the lists is only necessary for MSRs that are required for
    correct functionality immediately upon VM-Enter/VM-Exit, e.g. EFER on
    old hardware, or for MSRs that need to-the-uop precision, e.g. perf
    related MSRs.  For UMWAIT_CONTROL, the effects are only visible in the
    kernel via TPAUSE/delay(), and KVM doesn't do any form of delay in
    vcpu_vmx_run().  Using the atomic lists is undesirable as they are more
    expensive than direct RDMSR/WRMSR.
    
    Furthermore, even if giving the guest control of the MSR is legitimate,
    e.g. in pass-through scenarios, it's not clear that the benefits would
    outweigh the overhead.  E.g. saving and restoring an MSR across a VMX
    roundtrip costs ~250 cycles, and if the guest diverged from the host
    that cost would be paid on every run of the guest.  In other words, if
    there is a legitimate use case then it should be enabled by a new
    per-VM capability.
    
    Note, KVM still needs to emulate MSR_IA32_UMWAIT_CONTROL so that it can
    correctly expose other WAITPKG features to the guest, e.g. TPAUSE,
    UMWAIT and UMONITOR.
    
    Fixes: 6e3ba4abcea56 ("KVM: vmx: Emulate MSR IA32_UMWAIT_CONTROL")
    Cc: stable@vger.kernel.org
    Cc: Jingqi Liu <jingqi.liu@intel.com>
    Cc: Tao Xu <tao3.xu@intel.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200623005135.10414-1-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 73d997aa2966..e039a933aca3 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -25,8 +25,6 @@
 #define TPAUSE_C01_STATE		1
 #define TPAUSE_C02_STATE		0
 
-u32 get_umwait_control_msr(void);
-
 static inline void __monitor(const void *eax, unsigned long ecx,
 			     unsigned long edx)
 {

commit cec5f268cd02d25d2d74807843d8ae0292fe0fb7
Author: Kyung Min Park <kyung.min.park@intel.com>
Date:   Fri Apr 24 12:37:56 2020 -0700

    x86/delay: Introduce TPAUSE delay
    
    TPAUSE instructs the processor to enter an implementation-dependent
    optimized state. The instruction execution wakes up when the time-stamp
    counter reaches or exceeds the implicit EDX:EAX 64-bit input value.
    The instruction execution also wakes up due to the expiration of
    the operating system time-limit or by an external interrupt
    or exceptions such as a debug exception or a machine check exception.
    
    TPAUSE offers a choice of two lower power states:
     1. Light-weight power/performance optimized state C0.1
     2. Improved power/performance optimized state C0.2
    
    This way, it can save power with low wake-up latency in comparison to
    spinloop based delay. The selection between the two is governed by the
    input register.
    
    TPAUSE is available on processors with X86_FEATURE_WAITPKG.
    
    Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Kyung Min Park <kyung.min.park@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/1587757076-30337-4-git-send-email-kyung.min.park@intel.com

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index a43b35b35049..73d997aa2966 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -22,6 +22,8 @@
 #define MWAITX_ECX_TIMER_ENABLE		BIT(1)
 #define MWAITX_MAX_WAIT_CYCLES		UINT_MAX
 #define MWAITX_DISABLE_CSTATES		0xf0
+#define TPAUSE_C01_STATE		1
+#define TPAUSE_C02_STATE		0
 
 u32 get_umwait_control_msr(void);
 
@@ -122,4 +124,24 @@ static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
 	current_clr_polling();
 }
 
+/*
+ * Caller can specify whether to enter C0.1 (low latency, less
+ * power saving) or C0.2 state (saves more power, but longer wakeup
+ * latency). This may be overridden by the IA32_UMWAIT_CONTROL MSR
+ * which can force requests for C0.2 to be downgraded to C0.1.
+ */
+static inline void __tpause(u32 ecx, u32 edx, u32 eax)
+{
+	/* "tpause %ecx, %edx, %eax;" */
+	#ifdef CONFIG_AS_TPAUSE
+	asm volatile("tpause %%ecx\n"
+		     :
+		     : "c"(ecx), "d"(edx), "a"(eax));
+	#else
+	asm volatile(".byte 0x66, 0x0f, 0xae, 0xf1\t\n"
+		     :
+		     : "c"(ecx), "d"(edx), "a"(eax));
+	#endif
+}
+
 #endif /* _ASM_X86_MWAIT_H */

commit e8824890249355656968d8846908a313fe231f11
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 24 12:37:54 2020 -0700

    x86/delay: Preparatory code cleanup
    
    The naming conventions in the delay code are confusing at best.
    
    All delay variants use a loops argument and or variable which originates
    from the original delay_loop() implementation. But all variants except
    delay_loop() are based on TSC cycles.
    
    Rename the argument to cycles and make it type u64 to avoid these weird
    expansions to u64 in the functions.
    
    Rename MWAITX_MAX_LOOPS to MWAITX_MAX_WAIT_CYCLES for the same reason
    and fixup the comment of delay_mwaitx() as well.
    
    Mark the delay_fn function pointer __ro_after_init and fixup the comment
    for it.
    
    No functional change and preparation for the upcoming TPAUSE based delay
    variant.
    
    [ Kyung Min Park: Added __init to use_tsc_delay() ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Kyung Min Park <kyung.min.park@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1587757076-30337-2-git-send-email-kyung.min.park@intel.com

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index b809f117f3f4..a43b35b35049 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -20,7 +20,7 @@
 
 #define MWAIT_ECX_INTERRUPT_BREAK	0x1
 #define MWAITX_ECX_TIMER_ENABLE		BIT(1)
-#define MWAITX_MAX_LOOPS		((u32)-1)
+#define MWAITX_MAX_WAIT_CYCLES		UINT_MAX
 #define MWAITX_DISABLE_CSTATES		0xf0
 
 u32 get_umwait_control_msr(void);

commit b10c307f6f314c068814d0e23c86f06d5d57004b
Author: Benjamin Thiel <b.thiel@posteo.de>
Date:   Thu Jan 23 18:29:45 2020 +0100

    x86/cpu: Move prototype for get_umwait_control_msr() to a global location
    
    .. in order to fix a -Wmissing-prototypes warning.
    
    No functional change.
    
    Signed-off-by: Benjamin Thiel <b.thiel@posteo.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: kvm@vger.kernel.org
    Link: https://lkml.kernel.org/r/20200123172945.7235-1-b.thiel@posteo.de

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 9d5252c9685c..b809f117f3f4 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -23,6 +23,8 @@
 #define MWAITX_MAX_LOOPS		((u32)-1)
 #define MWAITX_DISABLE_CSTATES		0xf0
 
+u32 get_umwait_control_msr(void);
+
 static inline void __monitor(const void *eax, unsigned long ecx,
 			     unsigned long edx)
 {

commit 454de1e7d970d6bc567686052329e4814842867c
Author: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
Date:   Mon Oct 7 19:00:22 2019 +0000

    x86/asm: Fix MWAITX C-state hint value
    
    As per "AMD64 Architecture Programmer's Manual Volume 3: General-Purpose
    and System Instructions", MWAITX EAX[7:4]+1 specifies the optional hint
    of the optimized C-state. For C0 state, EAX[7:4] should be set to 0xf.
    
    Currently, a value of 0xf is set for EAX[3:0] instead of EAX[7:4]. Fix
    this by changing MWAITX_DISABLE_CSTATES from 0xf to 0xf0.
    
    This hasn't had any implications so far because setting reserved bits in
    EAX is simply ignored by the CPU.
    
     [ bp: Fixup comment in delay_mwaitx() and massage. ]
    
    Signed-off-by: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "x86@kernel.org" <x86@kernel.org>
    Cc: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20191007190011.4859-1-Janakarajan.Natarajan@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index e28f8b723b5c..9d5252c9685c 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -21,7 +21,7 @@
 #define MWAIT_ECX_INTERRUPT_BREAK	0x1
 #define MWAITX_ECX_TIMER_ENABLE		BIT(1)
 #define MWAITX_MAX_LOOPS		((u32)-1)
-#define MWAITX_DISABLE_CSTATES		0xf
+#define MWAITX_DISABLE_CSTATES		0xf0
 
 static inline void __monitor(const void *eax, unsigned long ecx,
 			     unsigned long edx)

commit ab3765a050f7bea942f114d07278e1775e38199b
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Sun Jun 23 11:35:04 2019 +0800

    x86/speculation/mds: Eliminate leaks by trace_hardirqs_on()
    
    Move mds_idle_clear_cpu_buffers() after trace_hardirqs_on() to ensure
    all store buffer entries are flushed.
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: hpa@zytor.com
    Cc: jgross@suse.com
    Cc: ndesaulniers@google.com
    Cc: gregkh@linuxfoundation.org
    Link: https://lkml.kernel.org/r/1561260904-29669-2-git-send-email-zhenzhong.duan@oracle.com

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index eb0f80ce8524..e28f8b723b5c 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -86,9 +86,9 @@ static inline void __mwaitx(unsigned long eax, unsigned long ebx,
 
 static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
 {
-	mds_idle_clear_cpu_buffers();
-
 	trace_hardirqs_on();
+
+	mds_idle_clear_cpu_buffers();
 	/* "mwait %eax, %ecx;" */
 	asm volatile("sti; .byte 0x0f, 0x01, 0xc9;"
 		     :: "a" (eax), "c" (ecx));

commit 07f07f55a29cb705e221eda7894dd67ab81ef343
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 18 23:04:01 2019 +0100

    x86/speculation/mds: Conditionally clear CPU buffers on idle entry
    
    Add a static key which controls the invocation of the CPU buffer clear
    mechanism on idle entry. This is independent of other MDS mitigations
    because the idle entry invocation to mitigate the potential leakage due to
    store buffer repartitioning is only necessary on SMT systems.
    
    Add the actual invocations to the different halt/mwait variants which
    covers all usage sites. mwaitx is not patched as it's not available on
    Intel CPUs.
    
    The buffer clear is only invoked before entering the C-State to prevent
    that stale data from the idling CPU is spilled to the Hyper-Thread sibling
    after the Store buffer got repartitioned and all entries are available to
    the non idle sibling.
    
    When coming out of idle the store buffer is partitioned again so each
    sibling has half of it available. Now CPU which returned from idle could be
    speculatively exposed to contents of the sibling, but the buffers are
    flushed either on exit to user space or on VMENTER.
    
    When later on conditional buffer clearing is implemented on top of this,
    then there is no action required either because before returning to user
    space the context switch will set the condition flag which causes a flush
    on the return to user path.
    
    Note, that the buffer clearing on idle is only sensible on CPUs which are
    solely affected by MSBDS and not any other variant of MDS because the other
    MDS variants cannot be mitigated when SMT is enabled, so the buffer
    clearing on idle would be a window dressing exercise.
    
    This intentionally does not handle the case in the acpi/processor_idle
    driver which uses the legacy IO port interface for C-State transitions for
    two reasons:
    
     - The acpi/processor_idle driver was replaced by the intel_idle driver
       almost a decade ago. Anything Nehalem upwards supports it and defaults
       to that new driver.
    
     - The legacy IO port interface is likely to be used on older and therefore
       unaffected CPUs or on systems which do not receive microcode updates
       anymore, so there is no point in adding that.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Jon Masters <jcm@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 39a2fb29378a..eb0f80ce8524 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -6,6 +6,7 @@
 #include <linux/sched/idle.h>
 
 #include <asm/cpufeature.h>
+#include <asm/nospec-branch.h>
 
 #define MWAIT_SUBSTATE_MASK		0xf
 #define MWAIT_CSTATE_MASK		0xf
@@ -40,6 +41,8 @@ static inline void __monitorx(const void *eax, unsigned long ecx,
 
 static inline void __mwait(unsigned long eax, unsigned long ecx)
 {
+	mds_idle_clear_cpu_buffers();
+
 	/* "mwait %eax, %ecx;" */
 	asm volatile(".byte 0x0f, 0x01, 0xc9;"
 		     :: "a" (eax), "c" (ecx));
@@ -74,6 +77,8 @@ static inline void __mwait(unsigned long eax, unsigned long ecx)
 static inline void __mwaitx(unsigned long eax, unsigned long ebx,
 			    unsigned long ecx)
 {
+	/* No MDS buffer clear as this is AMD/HYGON only */
+
 	/* "mwaitx %eax, %ebx, %ecx;" */
 	asm volatile(".byte 0x0f, 0x01, 0xfb;"
 		     :: "a" (eax), "b" (ebx), "c" (ecx));
@@ -81,6 +86,8 @@ static inline void __mwaitx(unsigned long eax, unsigned long ebx,
 
 static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
 {
+	mds_idle_clear_cpu_buffers();
+
 	trace_hardirqs_on();
 	/* "mwait %eax, %ecx;" */
 	asm volatile("sti; .byte 0x0f, 0x01, 0xc9;"

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index bda3c27f0da0..39a2fb29378a 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_MWAIT_H
 #define _ASM_X86_MWAIT_H
 

commit 4c822698cba8bdd93724117eded12bf34eb80252
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/idle.h>
    
    We are going to split  <linux/sched/idle.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/idle.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index f37f2d8a2989..bda3c27f0da0 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -2,6 +2,7 @@
 #define _ASM_X86_MWAIT_H
 
 #include <linux/sched.h>
+#include <linux/sched/idle.h>
 
 #include <asm/cpufeature.h>
 

commit 08e237fa56a1d95c1372033bc29c4a2517b3c0fa
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jul 18 11:41:10 2016 -0700

    x86/cpu: Add workaround for MONITOR instruction erratum on Goldmont based CPUs
    
    Monitored cached line may not wake up from mwait on certain
    Goldmont based CPUs. This patch will avoid calling
    current_set_polling_and_test() and thereby not set the TIF_ flag.
    The result is that we'll always send IPIs for wakeups.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1468867270-18493-1-git-send-email-jacob.jun.pan@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 0deeb2d26df7..f37f2d8a2989 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -97,7 +97,7 @@ static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
  */
 static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
 {
-	if (!current_set_polling_and_test()) {
+	if (static_cpu_has_bug(X86_BUG_MONITOR) || !current_set_polling_and_test()) {
 		if (static_cpu_has_bug(X86_BUG_CLFLUSH_MONITOR)) {
 			mb();
 			clflush((void *)&current_thread_info()->flags);

commit cd4d09ec6f6c12a2cc3db5b7d8876a325a53545b
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 26 22:12:04 2016 +0100

    x86/cpufeature: Carve out X86_FEATURE_*
    
    Move them to a separate header and have the following
    dependency:
    
      x86/cpufeatures.h <- x86/processor.h <- x86/cpufeature.h
    
    This makes it easier to use the header in asm code and not
    include the whole cpufeature.h and add guards for asm.
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453842730-28463-5-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index c70689b5e5aa..0deeb2d26df7 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -3,6 +3,8 @@
 
 #include <linux/sched.h>
 
+#include <asm/cpufeature.h>
+
 #define MWAIT_SUBSTATE_MASK		0xf
 #define MWAIT_CSTATE_MASK		0xf
 #define MWAIT_SUBSTATE_SIZE		4

commit f96756746c7909de37db3d03ac5fd5cfb2757f38
Author: Huang Rui <ray.huang@amd.com>
Date:   Mon Aug 10 12:19:53 2015 +0200

    x86/asm: Add MONITORX/MWAITX instruction support
    
    AMD Carrizo processors (Family 15h, Models 60h-6fh) added a new
    feature called MWAITX (MWAIT with extensions) as an extension to
    MONITOR/MWAIT.
    
    This new instruction controls a configurable timer which causes
    the core to exit wait state on timer expiration, in addition to
    "normal" MWAIT condition of reading from a monitored VA.
    
    Compared to MONITOR/MWAIT, there are minor differences in opcode
    and input parameters:
    
    MWAITX ECX[1]: enable timer if set
    MWAITX EBX[31:0]: max wait time expressed in SW P0 clocks ==
    TSC. The software P0 frequency is the same as the TSC frequency.
    
                    MWAIT                           MWAITX
    opcode          0f 01 c9           |            0f 01 fb
    ECX[0]                  value of RFLAGS.IF seen by instruction
    ECX[1]          unused/#GP if set  |            enable timer if set
    ECX[31:2]                     unused/#GP if set
    EAX                           unused (reserve for hint)
    EBX[31:0]       unused             |            max wait time (SW P0 == TSC)
    
                    MONITOR                         MONITORX
    opcode          0f 01 c8           |            0f 01 fa
    EAX                     (logical) address to monitor
    ECX                     #GP if not zero
    
    Max timeout = EBX/(TSC frequency)
    
    Signed-off-by: Huang Rui <ray.huang@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andreas Herrmann <herrmann.der.user@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dirk Brandewie <dirk.j.brandewie@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Li <tony.li@amd.com>
    Link: http://lkml.kernel.org/r/1439201994-28067-3-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 653dfa7662e1..c70689b5e5aa 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -14,6 +14,9 @@
 #define CPUID5_ECX_INTERRUPT_BREAK	0x2
 
 #define MWAIT_ECX_INTERRUPT_BREAK	0x1
+#define MWAITX_ECX_TIMER_ENABLE		BIT(1)
+#define MWAITX_MAX_LOOPS		((u32)-1)
+#define MWAITX_DISABLE_CSTATES		0xf
 
 static inline void __monitor(const void *eax, unsigned long ecx,
 			     unsigned long edx)
@@ -23,6 +26,14 @@ static inline void __monitor(const void *eax, unsigned long ecx,
 		     :: "a" (eax), "c" (ecx), "d"(edx));
 }
 
+static inline void __monitorx(const void *eax, unsigned long ecx,
+			      unsigned long edx)
+{
+	/* "monitorx %eax, %ecx, %edx;" */
+	asm volatile(".byte 0x0f, 0x01, 0xfa;"
+		     :: "a" (eax), "c" (ecx), "d"(edx));
+}
+
 static inline void __mwait(unsigned long eax, unsigned long ecx)
 {
 	/* "mwait %eax, %ecx;" */
@@ -30,6 +41,40 @@ static inline void __mwait(unsigned long eax, unsigned long ecx)
 		     :: "a" (eax), "c" (ecx));
 }
 
+/*
+ * MWAITX allows for a timer expiration to get the core out a wait state in
+ * addition to the default MWAIT exit condition of a store appearing at a
+ * monitored virtual address.
+ *
+ * Registers:
+ *
+ * MWAITX ECX[1]: enable timer if set
+ * MWAITX EBX[31:0]: max wait time expressed in SW P0 clocks. The software P0
+ * frequency is the same as the TSC frequency.
+ *
+ * Below is a comparison between MWAIT and MWAITX on AMD processors:
+ *
+ *                 MWAIT                           MWAITX
+ * opcode          0f 01 c9           |            0f 01 fb
+ * ECX[0]                  value of RFLAGS.IF seen by instruction
+ * ECX[1]          unused/#GP if set  |            enable timer if set
+ * ECX[31:2]                     unused/#GP if set
+ * EAX                           unused (reserve for hint)
+ * EBX[31:0]       unused             |            max wait time (P0 clocks)
+ *
+ *                 MONITOR                         MONITORX
+ * opcode          0f 01 c8           |            0f 01 fa
+ * EAX                     (logical) address to monitor
+ * ECX                     #GP if not zero
+ */
+static inline void __mwaitx(unsigned long eax, unsigned long ebx,
+			    unsigned long ecx)
+{
+	/* "mwaitx %eax, %ebx, %ecx;" */
+	asm volatile(".byte 0x0f, 0x01, 0xfb;"
+		     :: "a" (eax), "b" (ebx), "c" (ecx));
+}
+
 static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
 {
 	trace_hardirqs_on();

commit b253149b843f89cd300cbdbea27ce1f847506f99
Author: Len Brown <len.brown@intel.com>
Date:   Wed Jan 15 00:37:34 2014 -0500

    sched/idle/x86: Restore mwait_idle() to fix boot hangs, to improve power savings and to improve performance
    
    In Linux-3.9 we removed the mwait_idle() loop:
    
      69fb3676df33 ("x86 idle: remove mwait_idle() and "idle=mwait" cmdline param")
    
    The reasoning was that modern machines should be sufficiently
    happy during the boot process using the default_idle() HALT
    loop, until cpuidle loads and either acpi_idle or intel_idle
    invoke the newer MWAIT-with-hints idle loop.
    
    But two machines reported problems:
    
     1. Certain Core2-era machines support MWAIT-C1 and HALT only.
        MWAIT-C1 is preferred for optimal power and performance.
        But if they support just C1, cpuidle never loads and
        so they use the boot-time default idle loop forever.
    
     2. Some laptops will boot-hang if HALT is used,
        but will boot successfully if MWAIT is used.
        This appears to be a hidden assumption in BIOS SMI,
        that is presumably valid on the proprietary OS
        where the BIOS was validated.
    
           https://bugzilla.kernel.org/show_bug.cgi?id=60770
    
    So here we effectively revert the patch above, restoring
    the mwait_idle() loop.  However, we don't bother restoring
    the idle=mwait cmdline parameter, since it appears to add
    no value.
    
    Maintainer notes:
    
      For 3.9, simply revert 69fb3676df
      for 3.10, patch -F3 applies, fuzz needed due to __cpuinit use in
      context For 3.11, 3.12, 3.13, this patch applies cleanly
    
    Tested-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Mike Galbraith <bitbucket@online.de>
    Cc: <stable@vger.kernel.org> # 3.9+
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ian Malone <ibmalone@gmail.com>
    Cc: Josh Boyer <jwboyer@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/345254a551eb5a6a866e048d7ab570fd2193aca4.1389763084.git.len.brown@intel.com
    [ Ported to recent kernels. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index a1410db38a1a..653dfa7662e1 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -30,6 +30,14 @@ static inline void __mwait(unsigned long eax, unsigned long ecx)
 		     :: "a" (eax), "c" (ecx));
 }
 
+static inline void __sti_mwait(unsigned long eax, unsigned long ecx)
+{
+	trace_hardirqs_on();
+	/* "mwait %eax, %ecx;" */
+	asm volatile("sti; .byte 0x0f, 0x01, 0xc9;"
+		     :: "a" (eax), "c" (ecx));
+}
+
 /*
  * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
  * which can obviate IPI to trigger checking of need_resched.

commit 9b13a93df267af681a66a6a738bf1af10102da7d
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jun 18 00:06:23 2014 +0200

    x86, cpufeature: Convert more "features" to bugs
    
    X86_FEATURE_FXSAVE_LEAK, X86_FEATURE_11AP and
    X86_FEATURE_CLFLUSH_MONITOR are not really features but synthetic bits
    we use for applying different bug workarounds. Call them what they
    really are, and make sure they get the proper cross-CPU behavior (OR
    rather than AND).
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1403042783-23278-1-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 1da25a5f96f9..a1410db38a1a 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -43,7 +43,7 @@ static inline void __mwait(unsigned long eax, unsigned long ecx)
 static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
 {
 	if (!current_set_polling_and_test()) {
-		if (static_cpu_has(X86_FEATURE_CLFLUSH_MONITOR)) {
+		if (static_cpu_has_bug(X86_BUG_CLFLUSH_MONITOR)) {
 			mb();
 			clflush((void *)&current_thread_info()->flags);
 			mb();

commit 8cb75e0c4ec9786b81439761eac1d18d4a931af3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 20 12:22:37 2013 +0100

    sched/preempt: Fix up missed PREEMPT_NEED_RESCHED folding
    
    With various drivers wanting to inject idle time; we get people
    calling idle routines outside of the idle loop proper.
    
    Therefore we need to be extra careful about not missing
    TIF_NEED_RESCHED -> PREEMPT_NEED_RESCHED propagations.
    
    While looking at this, I also realized there's a small window in the
    existing idle loop where we can miss TIF_NEED_RESCHED; when it hits
    right after the tif_need_resched() test at the end of the loop but
    right before the need_resched() test at the start of the loop.
    
    So move preempt_fold_need_resched() out of the loop where we're
    guaranteed to have TIF_NEED_RESCHED set.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-x9jgh45oeayzajz2mjt0y7d6@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 19b71c439256..1da25a5f96f9 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -53,7 +53,7 @@ static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
 		if (!need_resched())
 			__mwait(eax, ecx);
 	}
-	__current_clr_polling();
+	current_clr_polling();
 }
 
 #endif /* _ASM_X86_MWAIT_H */

commit 7e98b71920464b8d15fa95c74366416cd3c88861
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Dec 19 11:58:16 2013 -0800

    x86, idle: Use static_cpu_has() for CLFLUSH workaround, add barriers
    
    Use static_cpu_has() to conditionalize the CLFLUSH workaround, and add
    memory barriers around it since the documentation is explicit that
    CLFLUSH is only ordered with respect to MFENCE.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Len Brown <len.brown@intel.com>
    Link: http://lkml.kernel.org/r/CA%2B55aFzGxcML7j8CEvQPYzh0W81uVoAAVmGctMOUZ7CZ1yYd2A@mail.gmail.com

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 361b02ef128c..19b71c439256 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -43,8 +43,11 @@ static inline void __mwait(unsigned long eax, unsigned long ecx)
 static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
 {
 	if (!current_set_polling_and_test()) {
-		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
+		if (static_cpu_has(X86_FEATURE_CLFLUSH_MONITOR)) {
+			mb();
 			clflush((void *)&current_thread_info()->flags);
+			mb();
+		}
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
 		if (!need_resched())

commit 16824255394f55adf31b9a96a9965d8c15bdac4c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 12 15:08:36 2013 +0100

    x86, acpi, idle: Restructure the mwait idle routines
    
    People seem to delight in writing wrong and broken mwait idle routines;
    collapse the lot.
    
    This leaves mwait_play_dead() the sole remaining user of __mwait() and
    new __mwait() users are probably doing it wrong.
    
    Also remove __sti_mwait() as its unused.
    
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jacob Jun Pan <jacob.jun.pan@linux.intel.com>
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Acked-by: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20131212141654.616820819@infradead.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 2f366d0ac6b4..361b02ef128c 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -1,6 +1,8 @@
 #ifndef _ASM_X86_MWAIT_H
 #define _ASM_X86_MWAIT_H
 
+#include <linux/sched.h>
+
 #define MWAIT_SUBSTATE_MASK		0xf
 #define MWAIT_CSTATE_MASK		0xf
 #define MWAIT_SUBSTATE_SIZE		4
@@ -13,4 +15,42 @@
 
 #define MWAIT_ECX_INTERRUPT_BREAK	0x1
 
+static inline void __monitor(const void *eax, unsigned long ecx,
+			     unsigned long edx)
+{
+	/* "monitor %eax, %ecx, %edx;" */
+	asm volatile(".byte 0x0f, 0x01, 0xc8;"
+		     :: "a" (eax), "c" (ecx), "d"(edx));
+}
+
+static inline void __mwait(unsigned long eax, unsigned long ecx)
+{
+	/* "mwait %eax, %ecx;" */
+	asm volatile(".byte 0x0f, 0x01, 0xc9;"
+		     :: "a" (eax), "c" (ecx));
+}
+
+/*
+ * This uses new MONITOR/MWAIT instructions on P4 processors with PNI,
+ * which can obviate IPI to trigger checking of need_resched.
+ * We execute MONITOR against need_resched and enter optimized wait state
+ * through MWAIT. Whenever someone changes need_resched, we would be woken
+ * up from MWAIT (without an IPI).
+ *
+ * New with Core Duo processors, MWAIT can take some hints based on CPU
+ * capability.
+ */
+static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
+{
+	if (!current_set_polling_and_test()) {
+		if (this_cpu_has(X86_FEATURE_CLFLUSH_MONITOR))
+			clflush((void *)&current_thread_info()->flags);
+
+		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		if (!need_resched())
+			__mwait(eax, ecx);
+	}
+	__current_clr_polling();
+}
+
 #endif /* _ASM_X86_MWAIT_H */

commit e022e7eb90f3edb83f9ff77825eda3d1b3a2f2e0
Author: Len Brown <len.brown@intel.com>
Date:   Fri Feb 1 23:37:30 2013 -0500

    intel_idle: remove assumption of one C-state per MWAIT flag
    
    Remove the assumption that cstate_tables are
    indexed by MWAIT flag values.  Each entry
    identifies itself via its own flags value.
    This change is needed to support multiple states
    that share the same MWAIT flags.
    
    Note that this can have an effect on what state is described
    by 'N' on cmdline intel_idle.max_cstate=N on some systems.
    
    intel_idle.max_cstate=0 still disables the driver
    intel_idle.max_cstate=1 still results in just C1(E)
    However, "place holders" in the sparse C-state name-space
    (eg. Atom) have been removed.
    
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index 3f447320ce87..2f366d0ac6b4 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -4,6 +4,8 @@
 #define MWAIT_SUBSTATE_MASK		0xf
 #define MWAIT_CSTATE_MASK		0xf
 #define MWAIT_SUBSTATE_SIZE		4
+#define MWAIT_HINT2CSTATE(hint)		(((hint) >> MWAIT_SUBSTATE_SIZE) & MWAIT_CSTATE_MASK)
+#define MWAIT_HINT2SUBSTATE(hint)	((hint) & MWAIT_CSTATE_MASK)
 
 #define CPUID_MWAIT_LEAF		5
 #define CPUID5_ECX_EXTENSIONS_SUPPORTED 0x1

commit 137ecc779c80138723677209730738d76262e810
Author: Len Brown <len.brown@intel.com>
Date:   Fri Feb 1 21:35:35 2013 -0500

    intel_idle: remove use and definition of MWAIT_MAX_NUM_CSTATES
    
    Cosmetic only.
    
    Replace use of MWAIT_MAX_NUM_CSTATES with CPUIDLE_STATE_MAX.
    They are both 8, so this patch has no functional change.
    
    The reason to change is that intel_idle will soon be able
    to export more than the 8 "major" states supported by MWAIT.
    When we hit that limit, it is important to know
    where the limit comes from.
    
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
index bcdff997668c..3f447320ce87 100644
--- a/arch/x86/include/asm/mwait.h
+++ b/arch/x86/include/asm/mwait.h
@@ -4,7 +4,6 @@
 #define MWAIT_SUBSTATE_MASK		0xf
 #define MWAIT_CSTATE_MASK		0xf
 #define MWAIT_SUBSTATE_SIZE		4
-#define MWAIT_MAX_NUM_CSTATES		8
 
 #define CPUID_MWAIT_LEAF		5
 #define CPUID5_ECX_EXTENSIONS_SUPPORTED 0x1

commit bc83cccc761953f878088cdfa682de0970b5561f
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 17 15:36:40 2010 -0700

    x86, mwait: Move mwait constants to a common header file
    
    We have MWAIT constants spread across three different .c files, for no
    good reason.  Move them all into a common header file.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <lenb@kernel.org>
    LKML-Reference: <tip-*@git.kernel.org>

diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h
new file mode 100644
index 000000000000..bcdff997668c
--- /dev/null
+++ b/arch/x86/include/asm/mwait.h
@@ -0,0 +1,15 @@
+#ifndef _ASM_X86_MWAIT_H
+#define _ASM_X86_MWAIT_H
+
+#define MWAIT_SUBSTATE_MASK		0xf
+#define MWAIT_CSTATE_MASK		0xf
+#define MWAIT_SUBSTATE_SIZE		4
+#define MWAIT_MAX_NUM_CSTATES		8
+
+#define CPUID_MWAIT_LEAF		5
+#define CPUID5_ECX_EXTENSIONS_SUPPORTED 0x1
+#define CPUID5_ECX_INTERRUPT_BREAK	0x2
+
+#define MWAIT_ECX_INTERRUPT_BREAK	0x1
+
+#endif /* _ASM_X86_MWAIT_H */
