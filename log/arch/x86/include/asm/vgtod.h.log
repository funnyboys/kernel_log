commit 1c1a18b00d7e25d1bed3507880de2da07be704a2
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Mon Mar 23 12:41:09 2020 +0000

    um: Fix header inclusion
    
    User Mode Linux is a flavor of x86 that from the vDSO prospective always
    falls back on system calls. This implies that it does not require any
    of the unified vDSO definitions and their inclusion causes side effects
    like this:
    
      In file included from include/vdso/processor.h:10:0,
                          from include/vdso/datapage.h:17,
                          from arch/x86/include/asm/vgtod.h:7,
                          from arch/x86/um/../kernel/sys_ia32.c:49:
      >> arch/x86/include/asm/vdso/processor.h:11:29: error: redefinition of 'rep_nop'
          static __always_inline void rep_nop(void)
                                      ^~~~~~~
         In file included from include/linux/rcupdate.h:30:0,
                          from include/linux/rculist.h:11,
                          from include/linux/pid.h:5,
                          from include/linux/sched.h:14,
                          from arch/x86/um/../kernel/sys_ia32.c:25:
         arch/x86/um/asm/processor.h:24:20: note: previous definition of 'rep_nop' was here
          static inline void rep_nop(void)
    
    Make sure that the unnecessary headers are not included when um is built
    to address the problem.
    
    Fixes: abc22418db02 ("x86/vdso: Enable x86 to use common headers")
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20200323124109.7104-1-vincenzo.frascino@arm.com

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index fc8e4cd342cc..7aa38b2ad8a9 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -2,6 +2,11 @@
 #ifndef _ASM_X86_VGTOD_H
 #define _ASM_X86_VGTOD_H
 
+/*
+ * This check is required to prevent ARCH=um to include
+ * unwanted headers.
+ */
+#ifdef CONFIG_GENERIC_GETTIMEOFDAY
 #include <linux/compiler.h>
 #include <asm/clocksource.h>
 #include <vdso/datapage.h>
@@ -14,5 +19,6 @@ typedef u64 gtod_long_t;
 #else
 typedef unsigned long gtod_long_t;
 #endif
+#endif /* CONFIG_GENERIC_GETTIMEOFDAY */
 
 #endif /* _ASM_X86_VGTOD_H */

commit eec399dd862762b9594df3659f15839a4e12f17a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 13:38:54 2020 +0100

    x86/vdso: Move VDSO clocksource state tracking to callback
    
    All architectures which use the generic VDSO code have their own storage
    for the VDSO clock mode. That's pointless and just requires duplicate code.
    
    X86 abuses the function which retrieves the architecture specific clock
    mode storage to mark the clocksource as used in the VDSO. That's silly
    because this is invoked on every tick when the VDSO data is updated.
    
    Move this functionality to the clocksource::enable() callback so it gets
    invoked once when the clocksource is installed. This allows to make the
    clock mode storage generic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>  (Hyper-V parts)
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com> (VDSO parts)
    Acked-by: Juergen Gross <jgross@suse.com> (Xen parts)
    Link: https://lkml.kernel.org/r/20200207124402.934519777@linutronix.de

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index a2638c6124ed..fc8e4cd342cc 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -15,10 +15,4 @@ typedef u64 gtod_long_t;
 typedef unsigned long gtod_long_t;
 #endif
 
-extern int vclocks_used;
-static inline bool vclock_was_used(int vclock)
-{
-	return READ_ONCE(vclocks_used) & (1 << vclock);
-}
-
 #endif /* _ASM_X86_VGTOD_H */

commit 7ac8707479886c75f353bfb6a8273f423cfccb23
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Jun 21 10:52:49 2019 +0100

    x86/vdso: Switch to generic vDSO implementation
    
    The x86 vDSO library requires some adaptations to take advantage of the
    newly introduced generic vDSO library.
    
    Introduce the following changes:
     - Modification of vdso.c to be compliant with the common vdso datapage
     - Use of lib/vdso for gettimeofday
    
    [ tglx: Massaged changelog and cleaned up the function signature formatting ]
    
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kselftest@vger.kernel.org
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Mark Salyzyn <salyzyn@android.com>
    Cc: Peter Collingbourne <pcc@google.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Dmitry Safonov <0x7f454c46@gmail.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Huw Davies <huw@codeweavers.com>
    Cc: Shijith Thotton <sthotton@marvell.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Link: https://lkml.kernel.org/r/20190621095252.32307-23-vincenzo.frascino@arm.com

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 913a133f8e6f..a2638c6124ed 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -3,7 +3,9 @@
 #define _ASM_X86_VGTOD_H
 
 #include <linux/compiler.h>
-#include <linux/clocksource.h>
+#include <asm/clocksource.h>
+#include <vdso/datapage.h>
+#include <vdso/helpers.h>
 
 #include <uapi/linux/time.h>
 
@@ -13,81 +15,10 @@ typedef u64 gtod_long_t;
 typedef unsigned long gtod_long_t;
 #endif
 
-/*
- * There is one of these objects in the vvar page for each
- * vDSO-accelerated clockid.  For high-resolution clocks, this encodes
- * the time corresponding to vsyscall_gtod_data.cycle_last.  For coarse
- * clocks, this encodes the actual time.
- *
- * To confuse the reader, for high-resolution clocks, nsec is left-shifted
- * by vsyscall_gtod_data.shift.
- */
-struct vgtod_ts {
-	u64		sec;
-	u64		nsec;
-};
-
-#define VGTOD_BASES	(CLOCK_TAI + 1)
-#define VGTOD_HRES	(BIT(CLOCK_REALTIME) | BIT(CLOCK_MONOTONIC) | BIT(CLOCK_TAI))
-#define VGTOD_COARSE	(BIT(CLOCK_REALTIME_COARSE) | BIT(CLOCK_MONOTONIC_COARSE))
-
-/*
- * vsyscall_gtod_data will be accessed by 32 and 64 bit code at the same time
- * so be carefull by modifying this structure.
- */
-struct vsyscall_gtod_data {
-	unsigned int	seq;
-
-	int		vclock_mode;
-	u64		cycle_last;
-	u64		mask;
-	u32		mult;
-	u32		shift;
-
-	struct vgtod_ts	basetime[VGTOD_BASES];
-
-	int		tz_minuteswest;
-	int		tz_dsttime;
-};
-extern struct vsyscall_gtod_data vsyscall_gtod_data;
-
 extern int vclocks_used;
 static inline bool vclock_was_used(int vclock)
 {
 	return READ_ONCE(vclocks_used) & (1 << vclock);
 }
 
-static inline unsigned int gtod_read_begin(const struct vsyscall_gtod_data *s)
-{
-	unsigned int ret;
-
-repeat:
-	ret = READ_ONCE(s->seq);
-	if (unlikely(ret & 1)) {
-		cpu_relax();
-		goto repeat;
-	}
-	smp_rmb();
-	return ret;
-}
-
-static inline int gtod_read_retry(const struct vsyscall_gtod_data *s,
-				  unsigned int start)
-{
-	smp_rmb();
-	return unlikely(s->seq != start);
-}
-
-static inline void gtod_write_begin(struct vsyscall_gtod_data *s)
-{
-	++s->seq;
-	smp_wmb();
-}
-
-static inline void gtod_write_end(struct vsyscall_gtod_data *s)
-{
-	smp_wmb();
-	++s->seq;
-}
-
 #endif /* _ASM_X86_VGTOD_H */

commit 034bda1cd5abbe7b170ce76b618768d164030bbd
Merge: d82924c3b8d0 99c19e6a8fe4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 19:07:25 2018 +0100

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 vdso updates from Ingo Molnar:
     "Two main changes:
    
       - Cleanups, simplifications and CLOCK_TAI support (Thomas Gleixner)
    
       - Improve code generation (Andy Lutomirski)"
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vdso: Rearrange do_hres() to improve code generation
      x86/vdso: Document vgtod_ts better
      x86/vdso: Remove "memory" clobbers in the vDSO syscall fallbacks
      x66/vdso: Add CLOCK_TAI support
      x86/vdso: Move cycle_last handling into the caller
      x86/vdso: Simplify the invalid vclock case
      x86/vdso: Replace the clockid switch case
      x86/vdso: Collapse coarse functions
      x86/vdso: Collapse high resolution functions
      x86/vdso: Introduce and use vgtod_ts
      x86/vdso: Use unsigned int consistently for vsyscall_gtod_data:: Seq
      x86/vdso: Enforce 64bit clocksource
      x86/time: Implement clocksource_arch_init()
      clocksource: Provide clocksource_arch_init()

commit ffebbaedc8616cffe648202e364dce6a045d65a2
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Tue Sep 18 16:08:58 2018 -0700

    x86/vdso: Introduce helper functions for CPU and node number
    
    Clean up the CPU/node number related code a bit, to make it more apparent
    how we are encoding/extracting the CPU and node fields from the
    segment limit.
    
    No change in functionality intended.
    
    [ mingo: Wrote new changelog. ]
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Link: http://lkml.kernel.org/r/1537312139-5580-8-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 4e81ea920722..056a61c8c5c7 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -77,30 +77,4 @@ static inline void gtod_write_end(struct vsyscall_gtod_data *s)
 	++s->seq;
 }
 
-#ifdef CONFIG_X86_64
-
-#define VGETCPU_CPU_MASK 0xfff
-
-static inline unsigned int __getcpu(void)
-{
-	unsigned int p;
-
-	/*
-	 * Load CPU (and node) number from GDT.  LSL is faster than RDTSCP
-	 * and works on all CPUs.  This is volatile so that it orders
-	 * correctly with respect to barrier() and to keep GCC from cleverly
-	 * hoisting it out of the calling function.
-	 *
-	 * If RDPID is available, use it.
-	 */
-	alternative_io ("lsl %[seg],%[p]",
-			".byte 0xf3,0x0f,0xc7,0xf8", /* RDPID %eax/rax */
-			X86_FEATURE_RDPID,
-			[p] "=a" (p), [seg] "r" (__CPU_NUMBER_SEG));
-
-	return p;
-}
-
-#endif /* CONFIG_X86_64 */
-
 #endif /* _ASM_X86_VGTOD_H */

commit c4755613a1339ea77dbb15de75c9f74217209265
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Tue Sep 18 16:08:57 2018 -0700

    x86/segments/64: Rename the GDT PER_CPU entry to CPU_NUMBER
    
    The old 'per CPU' naming was misleading: 64-bit kernels don't use this
    GDT entry for per CPU data, but to store the CPU (and node) ID.
    
    [ mingo: Wrote new changelog. ]
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Link: http://lkml.kernel.org/r/1537312139-5580-7-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 53748541c487..4e81ea920722 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -86,9 +86,9 @@ static inline unsigned int __getcpu(void)
 	unsigned int p;
 
 	/*
-	 * Load per CPU data from GDT.  LSL is faster than RDTSCP and
-	 * works on all CPUs.  This is volatile so that it orders
-	 * correctly wrt barrier() and to keep gcc from cleverly
+	 * Load CPU (and node) number from GDT.  LSL is faster than RDTSCP
+	 * and works on all CPUs.  This is volatile so that it orders
+	 * correctly with respect to barrier() and to keep GCC from cleverly
 	 * hoisting it out of the calling function.
 	 *
 	 * If RDPID is available, use it.
@@ -96,7 +96,7 @@ static inline unsigned int __getcpu(void)
 	alternative_io ("lsl %[seg],%[p]",
 			".byte 0xf3,0x0f,0xc7,0xf8", /* RDPID %eax/rax */
 			X86_FEATURE_RDPID,
-			[p] "=a" (p), [seg] "r" (__PER_CPU_SEG));
+			[p] "=a" (p), [seg] "r" (__CPU_NUMBER_SEG));
 
 	return p;
 }

commit bcc4a62a73cb65327d7268fbfa3a786d603f52dc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Oct 4 14:44:45 2018 -0700

    x86/vdso: Document vgtod_ts better
    
    After reading do_hres() and do_course() and scratching my head a
    bit, I figured out why the arithmetic is strange.  Document it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/f66f53d81150bbad47d7b282c9207a71a3ce1c16.1538689401.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index d17b092b9f1b..69d05c6d47f5 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -13,6 +13,15 @@ typedef u64 gtod_long_t;
 typedef unsigned long gtod_long_t;
 #endif
 
+/*
+ * There is one of these objects in the vvar page for each
+ * vDSO-accelerated clockid.  For high-resolution clocks, this encodes
+ * the time corresponding to vsyscall_gtod_data.cycle_last.  For coarse
+ * clocks, this encodes the actual time.
+ *
+ * To confuse the reader, for high-resolution clocks, nsec is left-shifted
+ * by vsyscall_gtod_data.shift.
+ */
 struct vgtod_ts {
 	u64		sec;
 	u64		nsec;

commit 315f28fa3a7bfbbb6233d800dffa327e87f96129
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 17 14:45:44 2018 +0200

    x66/vdso: Add CLOCK_TAI support
    
    With the storage array in place it's now trivial to support CLOCK_TAI in
    the vdso. Extend the base time storage array and add the update code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Matt Rickard <matt@softrans.com.au>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Boyd <sboyd@kernel.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: devel@linuxdriverproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Juergen Gross <jgross@suse.com>
    Link: https://lkml.kernel.org/r/20180917130707.823878601@linutronix.de

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 10e534a1a51a..d17b092b9f1b 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -18,8 +18,8 @@ struct vgtod_ts {
 	u64		nsec;
 };
 
-#define VGTOD_BASES	(CLOCK_MONOTONIC_COARSE + 1)
-#define VGTOD_HRES	(BIT(CLOCK_REALTIME) | BIT(CLOCK_MONOTONIC))
+#define VGTOD_BASES	(CLOCK_TAI + 1)
+#define VGTOD_HRES	(BIT(CLOCK_REALTIME) | BIT(CLOCK_MONOTONIC) | BIT(CLOCK_TAI))
 #define VGTOD_COARSE	(BIT(CLOCK_REALTIME_COARSE) | BIT(CLOCK_MONOTONIC_COARSE))
 
 /*

commit 49116f2081eeaf75316956705d46602b7eb735cc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 17 14:45:38 2018 +0200

    x86/vdso: Introduce and use vgtod_ts
    
    It's desired to support more clocks in the VDSO, e.g. CLOCK_TAI. This
    results either in indirect calls due to the larger switch case, which then
    requires retpolines or when the compiler is forced to avoid jump tables it
    results in even more conditionals.
    
    To avoid both variants which are bad for performance the high resolution
    functions and the coarse grained functions will be collapsed into one for
    each. That requires to store the clock specific base time in an array.
    
    Introcude struct vgtod_ts for storage and convert the data store, the
    update function and the individual clock functions over to use it.
    
    The new storage does not longer use gtod_long_t for seconds depending on 32
    or 64 bit compile because this needs to be the full 64bit value even for
    32bit when a Y2038 function is added. No point in keeping the distinction
    alive in the internal representation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Matt Rickard <matt@softrans.com.au>
    Cc: Stephen Boyd <sboyd@kernel.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: devel@linuxdriverproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Juergen Gross <jgross@suse.com>
    Link: https://lkml.kernel.org/r/20180917130707.324679401@linutronix.de

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 91cad1f01027..10e534a1a51a 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -5,33 +5,37 @@
 #include <linux/compiler.h>
 #include <linux/clocksource.h>
 
+#include <uapi/linux/time.h>
+
 #ifdef BUILD_VDSO32_64
 typedef u64 gtod_long_t;
 #else
 typedef unsigned long gtod_long_t;
 #endif
+
+struct vgtod_ts {
+	u64		sec;
+	u64		nsec;
+};
+
+#define VGTOD_BASES	(CLOCK_MONOTONIC_COARSE + 1)
+#define VGTOD_HRES	(BIT(CLOCK_REALTIME) | BIT(CLOCK_MONOTONIC))
+#define VGTOD_COARSE	(BIT(CLOCK_REALTIME_COARSE) | BIT(CLOCK_MONOTONIC_COARSE))
+
 /*
  * vsyscall_gtod_data will be accessed by 32 and 64 bit code at the same time
  * so be carefull by modifying this structure.
  */
 struct vsyscall_gtod_data {
-	unsigned int seq;
-
-	int	vclock_mode;
-	u64	cycle_last;
-	u64	mask;
-	u32	mult;
-	u32	shift;
-
-	/* open coded 'struct timespec' */
-	u64		wall_time_snsec;
-	gtod_long_t	wall_time_sec;
-	gtod_long_t	monotonic_time_sec;
-	u64		monotonic_time_snsec;
-	gtod_long_t	wall_time_coarse_sec;
-	gtod_long_t	wall_time_coarse_nsec;
-	gtod_long_t	monotonic_time_coarse_sec;
-	gtod_long_t	monotonic_time_coarse_nsec;
+	unsigned int	seq;
+
+	int		vclock_mode;
+	u64		cycle_last;
+	u64		mask;
+	u32		mult;
+	u32		shift;
+
+	struct vgtod_ts	basetime[VGTOD_BASES];
 
 	int		tz_minuteswest;
 	int		tz_dsttime;

commit 77e9c678c54f2d9214796c1c5bd0c7c7ccedd932
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 17 14:45:37 2018 +0200

    x86/vdso: Use unsigned int consistently for vsyscall_gtod_data:: Seq
    
    The sequence count in vgtod_data is unsigned int, but the call sites use
    unsigned long, which is a pointless exercise. Fix the call sites and
    replace 'unsigned' with unsinged 'int' while at it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Matt Rickard <matt@softrans.com.au>
    Cc: Stephen Boyd <sboyd@kernel.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: devel@linuxdriverproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Juergen Gross <jgross@suse.com>
    Link: https://lkml.kernel.org/r/20180917130707.236250416@linutronix.de

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 53748541c487..91cad1f01027 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -15,9 +15,9 @@ typedef unsigned long gtod_long_t;
  * so be carefull by modifying this structure.
  */
 struct vsyscall_gtod_data {
-	unsigned seq;
+	unsigned int seq;
 
-	int vclock_mode;
+	int	vclock_mode;
 	u64	cycle_last;
 	u64	mask;
 	u32	mult;
@@ -44,9 +44,9 @@ static inline bool vclock_was_used(int vclock)
 	return READ_ONCE(vclocks_used) & (1 << vclock);
 }
 
-static inline unsigned gtod_read_begin(const struct vsyscall_gtod_data *s)
+static inline unsigned int gtod_read_begin(const struct vsyscall_gtod_data *s)
 {
-	unsigned ret;
+	unsigned int ret;
 
 repeat:
 	ret = READ_ONCE(s->seq);
@@ -59,7 +59,7 @@ static inline unsigned gtod_read_begin(const struct vsyscall_gtod_data *s)
 }
 
 static inline int gtod_read_retry(const struct vsyscall_gtod_data *s,
-					unsigned start)
+				  unsigned int start)
 {
 	smp_rmb();
 	return unlikely(s->seq != start);

commit e78e5a91456fcecaa2efbb3706572fe043766f4d
Author: Samuel Neves <sneves@dei.uc.pt>
Date:   Sat Sep 1 21:14:52 2018 +0100

    x86/vdso: Fix lsl operand order
    
    In the __getcpu function, lsl is using the wrong target and destination
    registers. Luckily, the compiler tends to choose %eax for both variables,
    so it has been working so far.
    
    Fixes: a582c540ac1b ("x86/vdso: Use RDPID in preference to LSL when available")
    Signed-off-by: Samuel Neves <sneves@dei.uc.pt>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180901201452.27828-1-sneves@dei.uc.pt

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index fb856c9f0449..53748541c487 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -93,7 +93,7 @@ static inline unsigned int __getcpu(void)
 	 *
 	 * If RDPID is available, use it.
 	 */
-	alternative_io ("lsl %[p],%[seg]",
+	alternative_io ("lsl %[seg],%[p]",
 			".byte 0xf3,0x0f,0xc7,0xf8", /* RDPID %eax/rax */
 			X86_FEATURE_RDPID,
 			[p] "=a" (p), [seg] "r" (__PER_CPU_SEG));

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 022e59714562..52250681f68c 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_VGTOD_H
 #define _ASM_X86_VGTOD_H
 

commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 23 14:07:29 2017 -0700

    locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()
    
    Please do not apply this to mainline directly, instead please re-run the
    coccinelle script shown below and apply its output.
    
    For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
    preference to ACCESS_ONCE(), and new code is expected to use one of the
    former. So far, there's been no reason to change most existing uses of
    ACCESS_ONCE(), as these aren't harmful, and changing them results in
    churn.
    
    However, for some features, the read/write distinction is critical to
    correct operation. To distinguish these cases, separate read/write
    accessors must be used. This patch migrates (most) remaining
    ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
    coccinelle script:
    
    ----
    // Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
    // WRITE_ONCE()
    
    // $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch
    
    virtual patch
    
    @ depends on patch @
    expression E1, E2;
    @@
    
    - ACCESS_ONCE(E1) = E2
    + WRITE_ONCE(E1, E2)
    
    @ depends on patch @
    expression E;
    @@
    
    - ACCESS_ONCE(E)
    + READ_ONCE(E)
    ----
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: davem@davemloft.net
    Cc: linux-arch@vger.kernel.org
    Cc: mpe@ellerman.id.au
    Cc: shuah@kernel.org
    Cc: snitzer@redhat.com
    Cc: thor.thayer@linux.intel.com
    Cc: tj@kernel.org
    Cc: viro@zeniv.linux.org.uk
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 022e59714562..53dd162576a8 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -48,7 +48,7 @@ static inline unsigned gtod_read_begin(const struct vsyscall_gtod_data *s)
 	unsigned ret;
 
 repeat:
-	ret = ACCESS_ONCE(s->seq);
+	ret = READ_ONCE(s->seq);
 	if (unlikely(ret & 1)) {
 		cpu_relax();
 		goto repeat;

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 3a01996db58f..022e59714562 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -17,8 +17,8 @@ struct vsyscall_gtod_data {
 	unsigned seq;
 
 	int vclock_mode;
-	cycle_t	cycle_last;
-	cycle_t	mask;
+	u64	cycle_last;
+	u64	mask;
 	u32	mult;
 	u32	shift;
 

commit a582c540ac1b10f0a7d37415e04c4af42409fd08
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Nov 16 10:23:27 2016 -0800

    x86/vdso: Use RDPID in preference to LSL when available
    
    RDPID is a new instruction that reads MSR_TSC_AUX quickly.  This
    should be considerably faster than reading the GDT.  Add a
    cpufeature for it and use it from __vdso_getcpu() when available.
    
    Tested-by: Megha Dey <megha.dey@intel.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/4f6c3a22012d10f1c65b9ca15800e01b42c7d39d.1479320367.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index e728699db774..3a01996db58f 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -89,8 +89,13 @@ static inline unsigned int __getcpu(void)
 	 * works on all CPUs.  This is volatile so that it orders
 	 * correctly wrt barrier() and to keep gcc from cleverly
 	 * hoisting it out of the calling function.
+	 *
+	 * If RDPID is available, use it.
 	 */
-	asm volatile ("lsl %1,%0" : "=r" (p) : "r" (__PER_CPU_SEG));
+	alternative_io ("lsl %[p],%[seg]",
+			".byte 0xf3,0x0f,0xc7,0xf8", /* RDPID %eax/rax */
+			X86_FEATURE_RDPID,
+			[p] "=a" (p), [seg] "r" (__PER_CPU_SEG));
 
 	return p;
 }

commit bd902c536298830e4d126dcf6491b46d3f1bf96e
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Dec 29 20:12:24 2015 -0800

    x86/vdso: Disallow vvar access to vclock IO for never-used vclocks
    
    It makes me uncomfortable that even modern systems grant every
    process direct read access to the HPET.
    
    While fixing this for real without regressing anything is a mess
    (unmapping the HPET is tricky because we don't adequately track
    all the mappings), we can do almost as well by tracking which
    vclocks have ever been used and only allowing pages associated
    with used vclocks to be faulted in.
    
    This will cause rogue programs that try to peek at the HPET to
    get SIGBUS instead on most systems.
    
    We can't restrict faults to vclock pages that are associated
    with the currently selected vclock due to a race: a process
    could start to access the HPET for the first time and race
    against a switch away from the HPET as the current clocksource.
    We can't segfault the process trying to peek at the HPET in this
    case, even though the process isn't going to do anything useful
    with the data.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/e79d06295625c02512277737ab55085a498ac5d8.1451446564.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index f556c4843aa1..e728699db774 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -37,6 +37,12 @@ struct vsyscall_gtod_data {
 };
 extern struct vsyscall_gtod_data vsyscall_gtod_data;
 
+extern int vclocks_used;
+static inline bool vclock_was_used(int vclock)
+{
+	return READ_ONCE(vclocks_used) & (1 << vclock);
+}
+
 static inline unsigned gtod_read_begin(const struct vsyscall_gtod_data *s)
 {
 	unsigned ret;

commit 1ddf0b1b11aa8a90cef6706e935fc31c75c406ba
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Sun Dec 21 08:57:46 2014 -0800

    x86, vdso: Use asm volatile in __getcpu
    
    In Linux 3.18 and below, GCC hoists the lsl instructions in the
    pvclock code all the way to the beginning of __vdso_clock_gettime,
    slowing the non-paravirt case significantly.  For unknown reasons,
    presumably related to the removal of a branch, the performance issue
    is gone as of
    
    e76b027e6408 x86,vdso: Use LSL unconditionally for vgetcpu
    
    but I don't trust GCC enough to expect the problem to stay fixed.
    
    There should be no correctness issue, because the __getcpu calls in
    __vdso_vlock_gettime were never necessary in the first place.
    
    Note to stable maintainers: In 3.18 and below, depending on
    configuration, gcc 4.9.2 generates code like this:
    
         9c3:       44 0f 03 e8             lsl    %ax,%r13d
         9c7:       45 89 eb                mov    %r13d,%r11d
         9ca:       0f 03 d8                lsl    %ax,%ebx
    
    This patch won't apply as is to any released kernel, but I'll send a
    trivial backported version if needed.
    
    Fixes: 51c19b4f5927 x86: vdso: pvclock gettime support
    Cc: stable@vger.kernel.org # 3.8+
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index e7e9682a33e9..f556c4843aa1 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -80,9 +80,11 @@ static inline unsigned int __getcpu(void)
 
 	/*
 	 * Load per CPU data from GDT.  LSL is faster than RDTSCP and
-	 * works on all CPUs.
+	 * works on all CPUs.  This is volatile so that it orders
+	 * correctly wrt barrier() and to keep gcc from cleverly
+	 * hoisting it out of the calling function.
 	 */
-	asm("lsl %1,%0" : "=r" (p) : "r" (__PER_CPU_SEG));
+	asm volatile ("lsl %1,%0" : "=r" (p) : "r" (__PER_CPU_SEG));
 
 	return p;
 }

commit e76b027e6408f5570dc940b731ec9ae870c6188a
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Oct 30 14:58:01 2014 -0700

    x86,vdso: Use LSL unconditionally for vgetcpu
    
    LSL is faster than RDTSCP and works everywhere; there's no need to
    switch between them depending on CPU.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Andi Kleen <andi@firstfloor.org>
    Link: http://lkml.kernel.org/r/72f73d5ec4514e02bba345b9759177ef03742efb.1414706021.git.luto@amacapital.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 3c3366c2e37f..e7e9682a33e9 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -70,4 +70,23 @@ static inline void gtod_write_end(struct vsyscall_gtod_data *s)
 	++s->seq;
 }
 
+#ifdef CONFIG_X86_64
+
+#define VGETCPU_CPU_MASK 0xfff
+
+static inline unsigned int __getcpu(void)
+{
+	unsigned int p;
+
+	/*
+	 * Load per CPU data from GDT.  LSL is faster than RDTSCP and
+	 * works on all CPUs.
+	 */
+	asm("lsl %1,%0" : "=r" (p) : "r" (__PER_CPU_SEG));
+
+	return p;
+}
+
+#endif /* CONFIG_X86_64 */
+
 #endif /* _ASM_X86_VGTOD_H */

commit 7c03156f34d113f885f045d8fb8cc3efd9e64751
Author: Stefani Seibold <stefani@seibold.net>
Date:   Mon Mar 17 23:22:10 2014 +0100

    x86, vdso: Add 32 bit VDSO time support for 64 bit kernel
    
    This patch add the VDSO time support for the IA32 Emulation Layer.
    
    Due the nature of the kernel headers and the LP64 compiler where the
    size of a long and a pointer differs against a 32 bit compiler, there
    is some type hacking necessary for optimal performance.
    
    The vsyscall_gtod_data struture must be a rearranged to serve 32- and
    64-bit code access at the same time:
    
    - The seqcount_t was replaced by an unsigned, this makes the
      vsyscall_gtod_data intedepend of kernel configuration and internal functions.
    - All kernel internal structures are replaced by fix size elements
      which works for 32- and 64-bit access
    - The inner struct clock was removed to pack the whole struct.
    
    The "unsigned seq" would be handled by functions derivated from seqcount_t.
    
    Signed-off-by: Stefani Seibold <stefani@seibold.net>
    Link: http://lkml.kernel.org/r/1395094933-14252-11-git-send-email-stefani@seibold.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 46e24d36b7da..3c3366c2e37f 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -1,30 +1,73 @@
 #ifndef _ASM_X86_VGTOD_H
 #define _ASM_X86_VGTOD_H
 
-#include <asm/vsyscall.h>
+#include <linux/compiler.h>
 #include <linux/clocksource.h>
 
+#ifdef BUILD_VDSO32_64
+typedef u64 gtod_long_t;
+#else
+typedef unsigned long gtod_long_t;
+#endif
+/*
+ * vsyscall_gtod_data will be accessed by 32 and 64 bit code at the same time
+ * so be carefull by modifying this structure.
+ */
 struct vsyscall_gtod_data {
-	seqcount_t	seq;
+	unsigned seq;
 
-	struct { /* extract of a clocksource struct */
-		int vclock_mode;
-		cycle_t	cycle_last;
-		cycle_t	mask;
-		u32	mult;
-		u32	shift;
-	} clock;
+	int vclock_mode;
+	cycle_t	cycle_last;
+	cycle_t	mask;
+	u32	mult;
+	u32	shift;
 
 	/* open coded 'struct timespec' */
-	time_t		wall_time_sec;
 	u64		wall_time_snsec;
+	gtod_long_t	wall_time_sec;
+	gtod_long_t	monotonic_time_sec;
 	u64		monotonic_time_snsec;
-	time_t		monotonic_time_sec;
+	gtod_long_t	wall_time_coarse_sec;
+	gtod_long_t	wall_time_coarse_nsec;
+	gtod_long_t	monotonic_time_coarse_sec;
+	gtod_long_t	monotonic_time_coarse_nsec;
 
-	struct timezone sys_tz;
-	struct timespec wall_time_coarse;
-	struct timespec monotonic_time_coarse;
+	int		tz_minuteswest;
+	int		tz_dsttime;
 };
 extern struct vsyscall_gtod_data vsyscall_gtod_data;
 
+static inline unsigned gtod_read_begin(const struct vsyscall_gtod_data *s)
+{
+	unsigned ret;
+
+repeat:
+	ret = ACCESS_ONCE(s->seq);
+	if (unlikely(ret & 1)) {
+		cpu_relax();
+		goto repeat;
+	}
+	smp_rmb();
+	return ret;
+}
+
+static inline int gtod_read_retry(const struct vsyscall_gtod_data *s,
+					unsigned start)
+{
+	smp_rmb();
+	return unlikely(s->seq != start);
+}
+
+static inline void gtod_write_begin(struct vsyscall_gtod_data *s)
+{
+	++s->seq;
+	smp_wmb();
+}
+
+static inline void gtod_write_end(struct vsyscall_gtod_data *s)
+{
+	smp_wmb();
+	++s->seq;
+}
+
 #endif /* _ASM_X86_VGTOD_H */

commit 650ea02475106e8d6bdf561896d2ffe0d1c0ebb4
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Sep 4 16:14:46 2012 -0400

    time: Convert x86_64 to using new update_vsyscall
    
    Switch x86_64 to using sub-ns precise vsyscall
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 8b38be2de9e1..46e24d36b7da 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -17,8 +17,8 @@ struct vsyscall_gtod_data {
 
 	/* open coded 'struct timespec' */
 	time_t		wall_time_sec;
-	u32		wall_time_nsec;
-	u32		monotonic_time_nsec;
+	u64		wall_time_snsec;
+	u64		monotonic_time_snsec;
 	time_t		monotonic_time_sec;
 
 	struct timezone sys_tz;

commit 91ec87d57fc38c529034e853687dfb7756de5406
Author: Andy Lutomirski <luto@mit.edu>
Date:   Thu Mar 22 21:15:51 2012 -0700

    x86-64: Simplify and optimize vdso clock_gettime monotonic variants
    
    We used to store the wall-to-monotonic offset and the realtime base.
    It's faster to precompute the monotonic base.
    
    This is about a 3% speedup on Sandy Bridge for CLOCK_MONOTONIC.
    It's much more impressive for CLOCK_MONOTONIC_COARSE.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 1f007178c813..8b38be2de9e1 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -7,11 +7,6 @@
 struct vsyscall_gtod_data {
 	seqcount_t	seq;
 
-	/* open coded 'struct timespec' */
-	time_t		wall_time_sec;
-	u32		wall_time_nsec;
-
-	struct timezone sys_tz;
 	struct { /* extract of a clocksource struct */
 		int vclock_mode;
 		cycle_t	cycle_last;
@@ -19,8 +14,16 @@ struct vsyscall_gtod_data {
 		u32	mult;
 		u32	shift;
 	} clock;
-	struct timespec wall_to_monotonic;
+
+	/* open coded 'struct timespec' */
+	time_t		wall_time_sec;
+	u32		wall_time_nsec;
+	u32		monotonic_time_nsec;
+	time_t		monotonic_time_sec;
+
+	struct timezone sys_tz;
 	struct timespec wall_time_coarse;
+	struct timespec monotonic_time_coarse;
 };
 extern struct vsyscall_gtod_data vsyscall_gtod_data;
 

commit 2ab516575f2f273b19d95140d02c54612201e80a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 28 19:46:04 2012 +0000

    x86: vdso: Use seqcount instead of seqlock
    
    The update of the vdso data happens under xtime_lock, so adding a
    nested lock is pointless. Just use a seqcount to sync the readers.
    
    Reviewed-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 815285bcaceb..1f007178c813 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -5,7 +5,7 @@
 #include <linux/clocksource.h>
 
 struct vsyscall_gtod_data {
-	seqlock_t	lock;
+	seqcount_t	seq;
 
 	/* open coded 'struct timespec' */
 	time_t		wall_time_sec;

commit 98d0ac38ca7b1b7a552c9a2359174ff84decb600
Author: Andy Lutomirski <luto@mit.edu>
Date:   Thu Jul 14 06:47:22 2011 -0400

    x86-64: Move vread_tsc and vread_hpet into the vDSO
    
    The vsyscall page now consists entirely of trap instructions.
    
    Cc: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Link: http://lkml.kernel.org/r/637648f303f2ef93af93bae25186e9a1bea093f5.1310639973.git.luto@mit.edu
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index aa5add855a91..815285bcaceb 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -13,7 +13,7 @@ struct vsyscall_gtod_data {
 
 	struct timezone sys_tz;
 	struct { /* extract of a clocksource struct */
-		cycle_t (*vread)(void);
+		int vclock_mode;
 		cycle_t	cycle_last;
 		cycle_t	mask;
 		u32	mult;

commit 0d7b8547fb67d5c2a7d954c56b3715b0e708be4a
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Sun Jun 5 13:50:20 2011 -0400

    x86-64: Remove kernel.vsyscall64 sysctl
    
    It's unnecessary overhead in code that's supposed to be highly
    optimized.  Removing it allows us to remove one of the two
    syscall instructions in the vsyscall page.
    
    The only sensible use for it is for UML users, and it doesn't
    fully address inconsistent vsyscall results on UML.  The real
    fix for UML is to stop using vsyscalls entirely.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Cc: Jesper Juhl <jj@chaosbits.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Jan Beulich <JBeulich@novell.com>
    Cc: richard -rw- weinberger <richard.weinberger@gmail.com>
    Cc: Mikael Pettersson <mikpe@it.uu.se>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Louis Rilling <Louis.Rilling@kerlabs.com>
    Cc: Valdis.Kletnieks@vt.edu
    Cc: pageexec@freemail.hu
    Link: http://lkml.kernel.org/r/973ae803fe76f712da4b2740e66dccf452d3b1e4.1307292171.git.luto@mit.edu
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 646b4c1ca695..aa5add855a91 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -11,7 +11,6 @@ struct vsyscall_gtod_data {
 	time_t		wall_time_sec;
 	u32		wall_time_nsec;
 
-	int		sysctl_enabled;
 	struct timezone sys_tz;
 	struct { /* extract of a clocksource struct */
 		cycle_t (*vread)(void);

commit 8c49d9a74bac5ea3f18480307057241b808fcc0c
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Mon May 23 09:31:24 2011 -0400

    x86-64: Clean up vdso/kernel shared variables
    
    Variables that are shared between the vdso and the kernel are
    currently a bit of a mess.  They are each defined with their own
    magic, they are accessed differently in the kernel, the vsyscall page,
    and the vdso, and one of them (vsyscall_clock) doesn't even really
    exist.
    
    This changes them all to use a common mechanism.  All of them are
    delcared in vvar.h with a fixed address (validated by the linker
    script).  In the kernel (as before), they look like ordinary
    read-write variables.  In the vsyscall page and the vdso, they are
    accessed through a new macro VVAR, which gives read-only access.
    
    The vdso is now loaded verbatim into memory without any fixups.  As a
    side bonus, access from the vdso is faster because a level of
    indirection is removed.
    
    While we're at it, pack jiffies and vgetcpu_mode into the same
    cacheline.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@amd64.org>
    Link: http://lkml.kernel.org/r/%3C7357882fbb51fa30491636a7b6528747301b7ee9.1306156808.git.luto%40mit.edu%3E
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 3d61e204826f..646b4c1ca695 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -23,8 +23,6 @@ struct vsyscall_gtod_data {
 	struct timespec wall_to_monotonic;
 	struct timespec wall_time_coarse;
 };
-extern struct vsyscall_gtod_data __vsyscall_gtod_data
-__section_vsyscall_gtod_data;
 extern struct vsyscall_gtod_data vsyscall_gtod_data;
 
 #endif /* _ASM_X86_VGTOD_H */

commit da15cfdae03351c689736f8d142618592e3cebc3
Author: john stultz <johnstul@us.ibm.com>
Date:   Wed Aug 19 19:13:34 2009 -0700

    time: Introduce CLOCK_REALTIME_COARSE
    
    After talking with some application writers who want very fast, but not
    fine-grained timestamps, I decided to try to implement new clock_ids
    to clock_gettime(): CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE
    which returns the time at the last tick. This is very fast as we don't
    have to access any hardware (which can be very painful if you're using
    something like the acpi_pm clocksource), and we can even use the vdso
    clock_gettime() method to avoid the syscall. The only trade off is you
    only get low-res tick grained time resolution.
    
    This isn't a new idea, I know Ingo has a patch in the -rt tree that made
    the vsyscall gettimeofday() return coarse grained time when the
    vsyscall64 sysctrl was set to 2. However this affects all applications
    on a system.
    
    With this method, applications can choose the proper speed/granularity
    trade-off for themselves.
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: nikolag@ca.ibm.com
    Cc: Darren Hart <dvhltc@us.ibm.com>
    Cc: arjan@infradead.org
    Cc: jonathan@jonmasters.org
    LKML-Reference: <1250734414.6897.5.camel@localhost.localdomain>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index dc27a69e5d2a..3d61e204826f 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -21,6 +21,7 @@ struct vsyscall_gtod_data {
 		u32	shift;
 	} clock;
 	struct timespec wall_to_monotonic;
+	struct timespec wall_time_coarse;
 };
 extern struct vsyscall_gtod_data __vsyscall_gtod_data
 __section_vsyscall_gtod_data;

commit 1965aae3c98397aad957412413c07e97b1bd4e64
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Oct 22 22:26:29 2008 -0700

    x86: Fix ASM_X86__ header guards
    
    Change header guards named "ASM_X86__*" to "_ASM_X86_*" since:
    
    a. the double underscore is ugly and pointless.
    b. no leading underscore violates namespace constraints.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 38fd13364021..dc27a69e5d2a 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -1,5 +1,5 @@
-#ifndef ASM_X86__VGTOD_H
-#define ASM_X86__VGTOD_H
+#ifndef _ASM_X86_VGTOD_H
+#define _ASM_X86_VGTOD_H
 
 #include <asm/vsyscall.h>
 #include <linux/clocksource.h>
@@ -26,4 +26,4 @@ extern struct vsyscall_gtod_data __vsyscall_gtod_data
 __section_vsyscall_gtod_data;
 extern struct vsyscall_gtod_data vsyscall_gtod_data;
 
-#endif /* ASM_X86__VGTOD_H */
+#endif /* _ASM_X86_VGTOD_H */

commit bb8985586b7a906e116db835c64773b7a7d51663
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 21:05:42 2008 -0400

    x86, um: ... and asm-x86 move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
new file mode 100644
index 000000000000..38fd13364021
--- /dev/null
+++ b/arch/x86/include/asm/vgtod.h
@@ -0,0 +1,29 @@
+#ifndef ASM_X86__VGTOD_H
+#define ASM_X86__VGTOD_H
+
+#include <asm/vsyscall.h>
+#include <linux/clocksource.h>
+
+struct vsyscall_gtod_data {
+	seqlock_t	lock;
+
+	/* open coded 'struct timespec' */
+	time_t		wall_time_sec;
+	u32		wall_time_nsec;
+
+	int		sysctl_enabled;
+	struct timezone sys_tz;
+	struct { /* extract of a clocksource struct */
+		cycle_t (*vread)(void);
+		cycle_t	cycle_last;
+		cycle_t	mask;
+		u32	mult;
+		u32	shift;
+	} clock;
+	struct timespec wall_to_monotonic;
+};
+extern struct vsyscall_gtod_data __vsyscall_gtod_data
+__section_vsyscall_gtod_data;
+extern struct vsyscall_gtod_data vsyscall_gtod_data;
+
+#endif /* ASM_X86__VGTOD_H */
