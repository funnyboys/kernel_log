commit e48667b865480d8bf0f1171a8b474ffc785b9ace
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Fri Mar 13 18:10:24 2020 -0500

    perf/amd/uncore: Add support for Family 19h L3 PMU
    
    Family 19h introduces change in slice, core and thread specification in
    its L3 Performance Event Select (ChL3PmcCfg) h/w register. The change is
    incompatible with Family 17h's version of the register.
    
    Introduce a new path in l3_thread_slice_mask() to do things differently
    for Family 19h vs. Family 17h, otherwise the new hardware doesn't get
    programmed correctly.
    
    Instead of a linear core--thread bitmask, Family 19h takes an encoded
    core number, and a separate thread mask. There are new bits that are set
    for all cores and all slices, of which only the latter is used, since
    the driver counts events for all slices on behalf of the specified CPU.
    
    Also update amd_uncore_init() to base its L2/NB vs. L3/Data Fabric mode
    decision based on Family 17h or above, not just 17h and 18h: the Family
    19h Data Fabric PMC is compatible with the Family 17h DF PMC.
    
     [ bp: Touchups. ]
    
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200313231024.17601-3-kim.phillips@amd.com

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 29964b0e1075..e855e9cf2c37 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -50,11 +50,22 @@
 
 #define AMD64_L3_SLICE_SHIFT				48
 #define AMD64_L3_SLICE_MASK				\
-	((0xFULL) << AMD64_L3_SLICE_SHIFT)
+	(0xFULL << AMD64_L3_SLICE_SHIFT)
+#define AMD64_L3_SLICEID_MASK				\
+	(0x7ULL << AMD64_L3_SLICE_SHIFT)
 
 #define AMD64_L3_THREAD_SHIFT				56
 #define AMD64_L3_THREAD_MASK				\
-	((0xFFULL) << AMD64_L3_THREAD_SHIFT)
+	(0xFFULL << AMD64_L3_THREAD_SHIFT)
+#define AMD64_L3_F19H_THREAD_MASK			\
+	(0x3ULL << AMD64_L3_THREAD_SHIFT)
+
+#define AMD64_L3_EN_ALL_CORES				BIT_ULL(47)
+#define AMD64_L3_EN_ALL_SLICES				BIT_ULL(46)
+
+#define AMD64_L3_COREID_SHIFT				42
+#define AMD64_L3_COREID_MASK				\
+	(0x7ULL << AMD64_L3_COREID_SHIFT)
 
 #define X86_RAW_EVENT_MASK		\
 	(ARCH_PERFMON_EVENTSEL_EVENT |	\

commit 616c59b52342b0370ab822ce88fa0ff7f3671e4a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 20 20:45:12 2019 -0800

    perf/x86: Provide stubs of KVM helpers for non-Intel CPUs
    
    Provide stubs for perf_guest_get_msrs() and intel_pt_handle_vmx() when
    building without support for Intel CPUs, i.e. CPU_SUP_INTEL=n.  Lack of
    stubs is not currently a problem as the only user, KVM_INTEL, takes a
    dependency on CPU_SUP_INTEL=y.  Provide the stubs for all CPUs so that
    KVM_INTEL can be built for any CPU with compatible hardware support,
    e.g. Centuar and Zhaoxin CPUs.
    
    Note, the existing stub for perf_guest_get_msrs() is essentially dead
    code as KVM selects CONFIG_PERF_EVENTS, i.e. the only user guarantees
    the full implementation is built.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20191221044513.21680-19-sean.j.christopherson@intel.com

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index ee26e9215f18..29964b0e1075 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -322,17 +322,10 @@ struct perf_guest_switch_msr {
 	u64 host, guest;
 };
 
-extern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr);
 extern void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap);
 extern void perf_check_microcode(void);
 extern int x86_perf_rdpmc_index(struct perf_event *event);
 #else
-static inline struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
-{
-	*nr = 0;
-	return NULL;
-}
-
 static inline void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
 {
 	memset(cap, 0, sizeof(*cap));
@@ -342,8 +335,23 @@ static inline void perf_events_lapic_init(void)	{ }
 static inline void perf_check_microcode(void) { }
 #endif
 
+#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)
+extern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr);
+#else
+static inline struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
+{
+	*nr = 0;
+	return NULL;
+}
+#endif
+
 #ifdef CONFIG_CPU_SUP_INTEL
  extern void intel_pt_handle_vmx(int on);
+#else
+static inline void intel_pt_handle_vmx(int on)
+{
+
+}
 #endif
 
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD)

commit 0f4cd769c410e2285a4e9873a684d90423f03090
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Mon Aug 26 14:57:30 2019 -0500

    perf/x86/amd/ibs: Fix sample bias for dispatched micro-ops
    
    When counting dispatched micro-ops with cnt_ctl=1, in order to prevent
    sample bias, IBS hardware preloads the least significant 7 bits of
    current count (IbsOpCurCnt) with random values, such that, after the
    interrupt is handled and counting resumes, the next sample taken
    will be slightly perturbed.
    
    The current count bitfield is in the IBS execution control h/w register,
    alongside the maximum count field.
    
    Currently, the IBS driver writes that register with the maximum count,
    leaving zeroes to fill the current count field, thereby overwriting
    the random bits the hardware preloaded for itself.
    
    Fix the driver to actually retain and carry those random bits from the
    read of the IBS control register, through to its write, instead of
    overwriting the lower current count bits with zeroes.
    
    Tested with:
    
    perf record -c 100001 -e ibs_op/cnt_ctl=1/pp -a -C 0 taskset -c 0 <workload>
    
    'perf annotate' output before:
    
     15.70  65:   addsd     %xmm0,%xmm1
     17.30        add       $0x1,%rax
     15.88        cmp       %rdx,%rax
                  je        82
     17.32  72:   test      $0x1,%al
                  jne       7c
      7.52        movapd    %xmm1,%xmm0
      5.90        jmp       65
      8.23  7c:   sqrtsd    %xmm1,%xmm0
     12.15        jmp       65
    
    'perf annotate' output after:
    
     16.63  65:   addsd     %xmm0,%xmm1
     16.82        add       $0x1,%rax
     16.81        cmp       %rdx,%rax
                  je        82
     16.69  72:   test      $0x1,%al
                  jne       7c
      8.30        movapd    %xmm1,%xmm0
      8.13        jmp       65
      8.24  7c:   sqrtsd    %xmm1,%xmm0
      8.39        jmp       65
    
    Tested on Family 15h and 17h machines.
    
    Machines prior to family 10h Rev. C don't have the RDWROPCNT capability,
    and have the IbsOpCurCnt bitfield reserved, so this patch shouldn't
    affect their operation.
    
    It is unknown why commit db98c5faf8cb ("perf/x86: Implement 64-bit
    counter support for IBS") ignored the lower 4 bits of the IbsOpCurCnt
    field; the number of preloaded random bits has always been 7, AFAICT.
    
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Arnaldo Carvalho de Melo" <acme@kernel.org>
    Cc: <x86@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Borislav Petkov" <bp@alien8.de>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: "Namhyung Kim" <namhyung@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/20190826195730.30614-1-kim.phillips@amd.com

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 1392d5e6e8d6..ee26e9215f18 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -252,16 +252,20 @@ struct pebs_lbr {
 #define IBSCTL_LVT_OFFSET_VALID		(1ULL<<8)
 #define IBSCTL_LVT_OFFSET_MASK		0x0F
 
-/* ibs fetch bits/masks */
+/* IBS fetch bits/masks */
 #define IBS_FETCH_RAND_EN	(1ULL<<57)
 #define IBS_FETCH_VAL		(1ULL<<49)
 #define IBS_FETCH_ENABLE	(1ULL<<48)
 #define IBS_FETCH_CNT		0xFFFF0000ULL
 #define IBS_FETCH_MAX_CNT	0x0000FFFFULL
 
-/* ibs op bits/masks */
-/* lower 4 bits of the current count are ignored: */
-#define IBS_OP_CUR_CNT		(0xFFFF0ULL<<32)
+/*
+ * IBS op bits/masks
+ * The lower 7 bits of the current count are random bits
+ * preloaded by hardware and ignored in software
+ */
+#define IBS_OP_CUR_CNT		(0xFFF80ULL<<32)
+#define IBS_OP_CUR_CNT_RAND	(0x0007FULL<<32)
 #define IBS_OP_CNT_CTL		(1ULL<<19)
 #define IBS_OP_VAL		(1ULL<<18)
 #define IBS_OP_ENABLE		(1ULL<<17)

commit d15d356887e770c5f2dcf963b52c7cb510c9e42d
Author: Kairui Song <kasong@redhat.com>
Date:   Tue Apr 23 00:26:52 2019 +0800

    perf/x86: Make perf callchains work without CONFIG_FRAME_POINTER
    
    Currently perf callchain doesn't work well with ORC unwinder
    when sampling from trace point. We'll get useless in kernel callchain
    like this:
    
    perf  6429 [000]    22.498450:             kmem:mm_page_alloc: page=0x176a17 pfn=1534487 order=0 migratetype=0 gfp_flags=GFP_KERNEL
        ffffffffbe23e32e __alloc_pages_nodemask+0x22e (/lib/modules/5.1.0-rc3+/build/vmlinux)
            7efdf7f7d3e8 __poll+0x18 (/usr/lib64/libc-2.28.so)
            5651468729c1 [unknown] (/usr/bin/perf)
            5651467ee82a main+0x69a (/usr/bin/perf)
            7efdf7eaf413 __libc_start_main+0xf3 (/usr/lib64/libc-2.28.so)
        5541f689495641d7 [unknown] ([unknown])
    
    The root cause is that, for trace point events, it doesn't provide a
    real snapshot of the hardware registers. Instead perf tries to get
    required caller's registers and compose a fake register snapshot
    which suppose to contain enough information for start a unwinding.
    However without CONFIG_FRAME_POINTER, if failed to get caller's BP as the
    frame pointer, so current frame pointer is returned instead. We get
    a invalid register combination which confuse the unwinder, and end the
    stacktrace early.
    
    So in such case just don't try dump BP, and let the unwinder start
    directly when the register is not a real snapshot. Use SP
    as the skip mark, unwinder will skip all the frames until it meet
    the frame of the trace point caller.
    
    Tested with frame pointer unwinder and ORC unwinder, this makes perf
    callchain get the full kernel space stacktrace again like this:
    
    perf  6503 [000]  1567.570191:             kmem:mm_page_alloc: page=0x16c904 pfn=1493252 order=0 migratetype=0 gfp_flags=GFP_KERNEL
        ffffffffb523e2ae __alloc_pages_nodemask+0x22e (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52383bd __get_free_pages+0xd (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52fd28a __pollwait+0x8a (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb521426f perf_poll+0x2f (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52fe3e2 do_sys_poll+0x252 (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52ff027 __x64_sys_poll+0x37 (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb500418b do_syscall_64+0x5b (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb5a0008c entry_SYSCALL_64_after_hwframe+0x44 (/lib/modules/5.1.0-rc3+/build/vmlinux)
            7f71e92d03e8 __poll+0x18 (/usr/lib64/libc-2.28.so)
            55a22960d9c1 [unknown] (/usr/bin/perf)
            55a22958982a main+0x69a (/usr/bin/perf)
            7f71e9202413 __libc_start_main+0xf3 (/usr/lib64/libc-2.28.so)
        5541f689495641d7 [unknown] ([unknown])
    
    Co-developed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Kairui Song <kasong@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190422162652.15483-1-kasong@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 04768a3a5454..1392d5e6e8d6 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -308,14 +308,9 @@ extern unsigned long perf_misc_flags(struct pt_regs *regs);
  */
 #define perf_arch_fetch_caller_regs(regs, __ip)		{	\
 	(regs)->ip = (__ip);					\
-	(regs)->bp = caller_frame_pointer();			\
+	(regs)->sp = (unsigned long)__builtin_frame_address(0);	\
 	(regs)->cs = __KERNEL_CS;				\
 	regs->flags = 0;					\
-	asm volatile(						\
-		_ASM_MOV "%%"_ASM_SP ", %0\n"			\
-		: "=m" ((regs)->sp)				\
-		:: "memory"					\
-	);							\
 }
 
 struct perf_guest_switch_msr {

commit 6017608936c1825ff5d7325270484042f597edff
Author: Kan Liang <kan.liang@linux.intel.com>
Date:   Tue Apr 2 12:45:05 2019 -0700

    perf/x86/intel: Add Icelake support
    
    Add Icelake core PMU perf code, including constraint tables and the main
    enable code.
    
    Icelake expanded the generic counters to always 8 even with HT on, but a
    range of events cannot be scheduled on the extra 4 counters.
    Add new constraint ranges to describe this to the scheduler.
    The number of constraints that need to be checked is larger now than
    with earlier CPUs.
    At some point we may need a new data structure to look them up more
    efficiently than with linear search. So far it still seems to be
    acceptable however.
    
    Icelake added a new fixed counter SLOTS. Full support for it is added
    later in the patch series.
    
    The cache events table is identical to Skylake.
    
    Compare to PEBS instruction event on generic counter, fixed counter 0
    has less skid. Force instruction:ppp always in fixed counter 0.
    
    Originally-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: jolsa@kernel.org
    Link: https://lkml.kernel.org/r/20190402194509.2832-9-kan.liang@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 997a6587d7cf..04768a3a5454 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -7,7 +7,7 @@
  */
 
 #define INTEL_PMC_MAX_GENERIC				       32
-#define INTEL_PMC_MAX_FIXED					3
+#define INTEL_PMC_MAX_FIXED					4
 #define INTEL_PMC_IDX_FIXED				       32
 
 #define X86_PMC_IDX_MAX					       64

commit c22497f5838c237e3094a4dfb99d1c5de6353239
Author: Kan Liang <kan.liang@linux.intel.com>
Date:   Tue Apr 2 12:45:02 2019 -0700

    perf/x86/intel: Support adaptive PEBS v4
    
    Adaptive PEBS is a new way to report PEBS sampling information. Instead
    of a fixed size record for all PEBS events it allows to configure the
    PEBS record to only include the information needed. Events can then opt
    in to use such an extended record, or stay with a basic record which
    only contains the IP.
    
    The major new feature is to support LBRs in PEBS record.
    Besides normal LBR, this allows (much faster) large PEBS, while still
    supporting callstacks through callstack LBR. So essentially a lot of
    profiling can now be done without frequent interrupts, dropping the
    overhead significantly.
    
    The main requirement still is to use a period, and not use frequency
    mode, because frequency mode requires reevaluating the frequency on each
    overflow.
    
    The floating point state (XMM) is also supported, which allows efficient
    profiling of FP function arguments.
    
    Introduce specific drain function to handle variable length records.
    Use a new callback to parse the new record format, and also handle the
    STATUS field now being at a different offset.
    
    Add code to set up the configuration register. Since there is only a
    single register, all events either get the full super set of all events,
    or only the basic record.
    
    Originally-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: jolsa@kernel.org
    Link: https://lkml.kernel.org/r/20190402194509.2832-6-kan.liang@linux.intel.com
    [ Renamed GPRS => GP. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index d9f5bbe44b3c..997a6587d7cf 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -32,6 +32,8 @@
 
 #define HSW_IN_TX					(1ULL << 32)
 #define HSW_IN_TX_CHECKPOINTED				(1ULL << 33)
+#define ICL_EVENTSEL_ADAPTIVE				(1ULL << 34)
+#define ICL_FIXED_0_ADAPTIVE				(1ULL << 32)
 
 #define AMD64_EVENTSEL_INT_CORE_ENABLE			(1ULL << 36)
 #define AMD64_EVENTSEL_GUESTONLY			(1ULL << 40)
@@ -87,6 +89,12 @@
 #define ARCH_PERFMON_BRANCH_MISSES_RETIRED		6
 #define ARCH_PERFMON_EVENTS_COUNT			7
 
+#define PEBS_DATACFG_MEMINFO	BIT_ULL(0)
+#define PEBS_DATACFG_GP	BIT_ULL(1)
+#define PEBS_DATACFG_XMMS	BIT_ULL(2)
+#define PEBS_DATACFG_LBRS	BIT_ULL(3)
+#define PEBS_DATACFG_LBR_SHIFT	24
+
 /*
  * Intel "Architectural Performance Monitoring" CPUID
  * detection/enumeration details:
@@ -176,6 +184,41 @@ struct x86_pmu_capability {
 #define GLOBAL_STATUS_LBRS_FROZEN			BIT_ULL(58)
 #define GLOBAL_STATUS_TRACE_TOPAPMI			BIT_ULL(55)
 
+/*
+ * Adaptive PEBS v4
+ */
+
+struct pebs_basic {
+	u64 format_size;
+	u64 ip;
+	u64 applicable_counters;
+	u64 tsc;
+};
+
+struct pebs_meminfo {
+	u64 address;
+	u64 aux;
+	u64 latency;
+	u64 tsx_tuning;
+};
+
+struct pebs_gprs {
+	u64 flags, ip, ax, cx, dx, bx, sp, bp, si, di;
+	u64 r8, r9, r10, r11, r12, r13, r14, r15;
+};
+
+struct pebs_xmm {
+	u64 xmm[16*2];	/* two entries for each register */
+};
+
+struct pebs_lbr_entry {
+	u64 from, to, info;
+};
+
+struct pebs_lbr {
+	struct pebs_lbr_entry lbr[0]; /* Variable length */
+};
+
 /*
  * IBS cpuid feature detection
  */

commit 878068ea270ea82767ff1d26c91583263c81fba0
Author: Kan Liang <kan.liang@linux.intel.com>
Date:   Tue Apr 2 12:44:59 2019 -0700

    perf/x86: Support outputting XMM registers
    
    Starting from Icelake, XMM registers can be collected in PEBS record.
    But current code only output the pt_regs.
    
    Add a new struct x86_perf_regs for both pt_regs and xmm_regs. The
    xmm_regs will be used later to keep a pointer to PEBS record which has
    XMM information.
    
    XMM registers are 128 bit. To simplify the code, they are handled like
    two different registers, which means setting two bits in the register
    bitmap. This also allows only sampling the lower 64bit bits in XMM.
    
    The index of XMM registers starts from 32. There are 16 XMM registers.
    So all reserved space for regs are used. Remove REG_RESERVED.
    
    Add PERF_REG_X86_XMM_MAX, which stands for the max number of all x86
    regs including both GPRs and XMM.
    
    Add REG_NOSUPPORT for 32bit to exclude unsupported registers.
    
    Previous platforms can not collect XMM information in PEBS record.
    Adding pebs_no_xmm_regs to indicate the unsupported platforms.
    
    The common code still validates the supported registers. However, it
    cannot check model specific registers, e.g. XMM. Add extra check in
    x86_pmu_hw_config() to reject invalid config of regs_user and regs_intr.
    The regs_user never supports XMM collection.
    The regs_intr only supports XMM collection when sampling PEBS event on
    icelake and later platforms.
    
    Originally-by: Andi Kleen <ak@linux.intel.com>
    Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: jolsa@kernel.org
    Link: https://lkml.kernel.org/r/20190402194509.2832-3-kan.liang@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 8bdf74902293..d9f5bbe44b3c 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -248,6 +248,11 @@ extern void perf_events_lapic_init(void);
 #define PERF_EFLAGS_VM		(1UL << 5)
 
 struct pt_regs;
+struct x86_perf_regs {
+	struct pt_regs	regs;
+	u64		*xmm_regs;
+};
+
 extern unsigned long perf_instruction_pointer(struct pt_regs *regs);
 extern unsigned long perf_misc_flags(struct pt_regs *regs);
 #define perf_misc_flags(regs)	perf_misc_flags(regs)

commit a4c9f26533eb547c8123e9a5f77517f61d19d2c2
Merge: 97e831e13015 dd45407c0b24
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Oct 2 09:51:41 2018 +0200

    Merge branch 'x86/cache' into perf/core, to resolve conflicts
    
    Avoid conflict with upcoming perf/core patches, merge in the RDT perf work.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d7cbbe49a9304520181fb8c9272d1327deec8453
Author: Natarajan, Janakarajan <Janakarajan.Natarajan@amd.com>
Date:   Thu Sep 27 15:51:55 2018 +0000

    perf/x86/amd/uncore: Set ThreadMask and SliceMask for L3 Cache perf events
    
    In Family 17h, some L3 Cache Performance events require the ThreadMask
    and SliceMask to be set. For other events, these fields do not affect
    the count either way.
    
    Set ThreadMask and SliceMask to 0xFF and 0xF respectively.
    
    Signed-off-by: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H . Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Suravee <Suravee.Suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/Message-ID:
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 12f54082f4c8..78241b736f2a 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -46,6 +46,14 @@
 #define INTEL_ARCH_EVENT_MASK	\
 	(ARCH_PERFMON_EVENTSEL_UMASK | ARCH_PERFMON_EVENTSEL_EVENT)
 
+#define AMD64_L3_SLICE_SHIFT				48
+#define AMD64_L3_SLICE_MASK				\
+	((0xFULL) << AMD64_L3_SLICE_SHIFT)
+
+#define AMD64_L3_THREAD_SHIFT				56
+#define AMD64_L3_THREAD_MASK				\
+	((0xFFULL) << AMD64_L3_THREAD_SHIFT)
+
 #define X86_RAW_EVENT_MASK		\
 	(ARCH_PERFMON_EVENTSEL_EVENT |	\
 	 ARCH_PERFMON_EVENTSEL_UMASK |	\

commit 1182a49529edde899be4b4f0e1ab76e626976eb6
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Wed Sep 19 10:29:07 2018 -0700

    perf/x86: Add helper to obtain performance counter index
    
    perf_event_read_local() is the safest way to obtain measurements
    associated with performance events. In some cases the overhead
    introduced by perf_event_read_local() affects the measurements and the
    use of rdpmcl() is needed. rdpmcl() requires the index
    of the performance counter used so a helper is introduced to determine
    the index used by a provided performance event.
    
    The index used by a performance event may change when interrupts are
    enabled. A check is added to ensure that the index is only accessed
    with interrupts disabled. Even with this check the use of this counter
    needs to be done with care to ensure it is queried and used within the
    same disabled interrupts section.
    
    This change introduces a new checkpatch warning:
    CHECK: extern prototypes should be avoided in .h files
    +extern int x86_perf_rdpmc_index(struct perf_event *event);
    
    This warning was discussed and designated as a false positive in
    http://lkml.kernel.org/r/20180919091759.GZ24124@hirez.programming.kicks-ass.net
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: fenghua.yu@intel.com
    Cc: tony.luck@intel.com
    Cc: acme@kernel.org
    Cc: gavin.hindman@intel.com
    Cc: jithu.joseph@intel.com
    Cc: dave.hansen@intel.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/b277ffa78a51254f5414f7b1bc1923826874566e.1537377064.git.reinette.chatre@intel.com

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 12f54082f4c8..b2cf84c35a6d 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -270,6 +270,7 @@ struct perf_guest_switch_msr {
 extern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr);
 extern void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap);
 extern void perf_check_microcode(void);
+extern int x86_perf_rdpmc_index(struct perf_event *event);
 #else
 static inline struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
 {

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index f353061bba1d..12f54082f4c8 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_PERF_EVENT_H
 #define _ASM_X86_PERF_EVENT_H
 

commit 1c5ac21a0e9bab7fc45d0ba9e11623e9ad99d02e
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Tue Mar 29 17:43:10 2016 +0300

    perf/x86/intel/pt: Don't die on VMXON
    
    Some versions of Intel PT do not support tracing across VMXON, more
    specifically, VMXON will clear TraceEn control bit and any attempt to
    set it before VMXOFF will throw a #GP, which in the current state of
    things will crash the kernel. Namely:
    
      $ perf record -e intel_pt// kvm -nographic
    
    on such a machine will kill it.
    
    To avoid this, notify the intel_pt driver before VMXON and after
    VMXOFF so that it knows when not to enable itself.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: hpa@zytor.com
    Link: http://lkml.kernel.org/r/87oa9dwrfk.fsf@ashishki-desk.ger.corp.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 5a2ed3ed2f26..f353061bba1d 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -285,6 +285,10 @@ static inline void perf_events_lapic_init(void)	{ }
 static inline void perf_check_microcode(void) { }
 #endif
 
+#ifdef CONFIG_CPU_SUP_INTEL
+ extern void intel_pt_handle_vmx(int on);
+#endif
+
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD)
  extern void amd_pmu_enable_virt(void);
  extern void amd_pmu_disable_virt(void);

commit 5690ae28e472d25e330ad0c637a5cea3fc39fb32
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Mar 3 20:50:40 2016 +0100

    perf/x86/intel: Add definition for PT PMI bit
    
    This patch adds a definition for GLOBAL_OVFL_STATUS bit 55
    which is used with the Processor Trace (PT) feature.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: namhyung@kernel.org
    Link: http://lkml.kernel.org/r/1457034642-21837-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 7bcb861a04e5..5a2ed3ed2f26 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -165,6 +165,7 @@ struct x86_pmu_capability {
 #define GLOBAL_STATUS_ASIF				BIT_ULL(60)
 #define GLOBAL_STATUS_COUNTERS_FROZEN			BIT_ULL(59)
 #define GLOBAL_STATUS_LBRS_FROZEN			BIT_ULL(58)
+#define GLOBAL_STATUS_TRACE_TOPAPMI			BIT_ULL(55)
 
 /*
  * IBS cpuid feature detection

commit b83ff1c8617aac03a1cf807aafa848fe0f0908f2
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sun May 10 12:22:41 2015 -0700

    x86: Add new MSRs and MSR bits used for Intel Skylake PMU support
    
    Add new MSRs (LBR_INFO) and some new MSR bits used by the Intel Skylake
    PMU driver.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/1431285767-27027-4-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index dc0f6ed35b08..7bcb861a04e5 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -159,6 +159,13 @@ struct x86_pmu_capability {
  */
 #define INTEL_PMC_IDX_FIXED_BTS				(INTEL_PMC_IDX_FIXED + 16)
 
+#define GLOBAL_STATUS_COND_CHG				BIT_ULL(63)
+#define GLOBAL_STATUS_BUFFER_OVF			BIT_ULL(62)
+#define GLOBAL_STATUS_UNC_OVF				BIT_ULL(61)
+#define GLOBAL_STATUS_ASIF				BIT_ULL(60)
+#define GLOBAL_STATUS_COUNTERS_FROZEN			BIT_ULL(59)
+#define GLOBAL_STATUS_LBRS_FROZEN			BIT_ULL(58)
+
 /*
  * IBS cpuid feature detection
  */

commit 904cb3677f3adcd3d837be0a0d0b14251ba8d6f7
Author: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
Date:   Mon Nov 10 14:24:26 2014 -0600

    perf/x86/amd/ibs: Update IBS MSRs and feature definitions
    
    New Fam15h models carry extra feature bits and extend
    the MSR register space for IBS ops. Adding them here.
    
    While at it, add functionality to read IbsBrTarget and
    OpData4 depending on their availability if user wants a
    PERF_SAMPLE_RAW.
    
    Signed-off-by: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Jan Kiszka <jan.kiszka@siemens.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <paulus@samba.org>
    Cc: <acme@kernel.org>
    Link: http://lkml.kernel.org/r/1415651066-13523-1-git-send-email-Aravind.Gopalakrishnan@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 8dfc9fd094a3..dc0f6ed35b08 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -177,6 +177,9 @@ struct x86_pmu_capability {
 #define IBS_CAPS_BRNTRGT		(1U<<5)
 #define IBS_CAPS_OPCNTEXT		(1U<<6)
 #define IBS_CAPS_RIPINVALIDCHK		(1U<<7)
+#define IBS_CAPS_OPBRNFUSE		(1U<<8)
+#define IBS_CAPS_FETCHCTLEXTD		(1U<<9)
+#define IBS_CAPS_OPDATA4		(1U<<10)
 
 #define IBS_CAPS_DEFAULT		(IBS_CAPS_AVAIL		\
 					 | IBS_CAPS_FETCHSAM	\

commit 86a04461a99fb857bd7d7f87b234cae27df07f8a
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 11 21:27:10 2014 +0200

    perf/x86: Revamp PEBS event selection
    
    The basic idea is that it does not make sense to list all PEBS
    events individually. The list is very long, sometimes outdated
    and the hardware doesn't need it. If an event does not support
    PEBS it will just not count, there is no security issue.
    
    We need to only list events that something special, like
    supporting load or store addresses.
    
    This vastly simplifies the PEBS event selection. It also
    speeds up the scheduling because the scheduler doesn't
    have to walk as many constraints.
    
    Bugs fixed:
    
     - We do not allow setting forbidden flags with PEBS anymore
       (SDM 18.9.4), except for the special cycle event.
       This is done using a new constraint macro that also
       matches on the event flags.
    
     - Correct DataLA and load/store/na flags reporting on Haswell
       [Requires a followon patch]
    
     - We did not allow all PEBS events on Haswell:
       We were missing some valid subevents in d1-d2 (MEM_LOAD_UOPS_RETIRED.*,
       MEM_LOAD_UOPS_RETIRED_L3_HIT_RETIRED.*)
    
    This includes the changes proposed by Stephane earlier and obsoletes
    his patchkit (except for some changes on pre Sandy Bridge/Silvermont
    CPUs)
    
    I only did Sandy Bridge and Silvermont and later so far, mostly because these
    are the parts I could directly confirm the hardware behavior with hardware
    architects. Also I do not believe the older CPUs have any
    missing events in their PEBS list, so there's no pressing
    need to change them.
    
    I did not implement the flag proposed by Peter to allow
    setting forbidden flags. If really needed this could
    be implemented on to of this patch.
    
    v2: Fix broken store events on SNB/IVB (Stephane Eranian)
    v3: More fixes. Rename some arguments (Stephane Eranian)
    v4: List most Haswell events individually again to report
    memory operation type correctly.
    Add new flags to describe load/store/na for datala.
    Update description.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1407785233-32193-2-git-send-email-eranian@google.com
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Maria Dimakopoulou <maria.n.dimakopoulou@gmail.com>
    Cc: Mark Davies <junk@eslaf.co.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 8249df45d2f2..8dfc9fd094a3 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -51,6 +51,14 @@
 	 ARCH_PERFMON_EVENTSEL_EDGE  |	\
 	 ARCH_PERFMON_EVENTSEL_INV   |	\
 	 ARCH_PERFMON_EVENTSEL_CMASK)
+#define X86_ALL_EVENT_FLAGS  			\
+	(ARCH_PERFMON_EVENTSEL_EDGE |  		\
+	 ARCH_PERFMON_EVENTSEL_INV | 		\
+	 ARCH_PERFMON_EVENTSEL_CMASK | 		\
+	 ARCH_PERFMON_EVENTSEL_ANY | 		\
+	 ARCH_PERFMON_EVENTSEL_PIN_CONTROL | 	\
+	 HSW_IN_TX | 				\
+	 HSW_IN_TX_CHECKPOINTED)
 #define AMD64_RAW_EVENT_MASK		\
 	(X86_RAW_EVENT_MASK          |  \
 	 AMD64_EVENTSEL_EVENT)

commit 3a632cb229bfb18b6d09822cc842451ea46c013e
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Jun 17 17:36:48 2013 -0700

    perf/x86/intel: Add simple Haswell PMU support
    
    Similar to SandyBridge, but has a few new events and two
    new counter bits.
    
    There are some new counter flags that need to be prevented
    from being set on fixed counters, and allowed to be set
    for generic counters.
    
    Also we add support for the counter 2 constraint to handle
    all raw events.
    
    (Contains fixes from Stephane Eranian.)
    
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Andi Kleen <ak@linux.jf.intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Link: http://lkml.kernel.org/r/1371515812-9646-3-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 57cb63402213..8249df45d2f2 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -29,6 +29,9 @@
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
 
+#define HSW_IN_TX					(1ULL << 32)
+#define HSW_IN_TX_CHECKPOINTED				(1ULL << 33)
+
 #define AMD64_EVENTSEL_INT_CORE_ENABLE			(1ULL << 36)
 #define AMD64_EVENTSEL_GUESTONLY			(1ULL << 40)
 #define AMD64_EVENTSEL_HOSTONLY				(1ULL << 41)

commit e259514eef764a5286873618e34c560ecb6cff13
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Feb 6 11:26:29 2013 -0600

    perf/x86/amd: Enable northbridge performance counters on AMD family 15h
    
    On AMD family 15h processors, there are 4 new performance
    counters (in addition to 6 core performance counters) that can
    be used for counting northbridge events (i.e. DRAM accesses).
    
    Their bit fields are almost identical to the core performance
    counters. However, unlike the core performance counters, these
    MSRs are shared between multiple cores (that share the same
    northbridge).
    
    We will reuse the same code path as existing family 10h
    northbridge event constraints handler logic to enforce
    this sharing.
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Stephane Eranian <eranian@google.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Jacob Shin <jacob.shin@amd.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1360171589-6381-7-git-send-email-jacob.shin@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 2234eaaecb52..57cb63402213 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -29,9 +29,14 @@
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
 
+#define AMD64_EVENTSEL_INT_CORE_ENABLE			(1ULL << 36)
 #define AMD64_EVENTSEL_GUESTONLY			(1ULL << 40)
 #define AMD64_EVENTSEL_HOSTONLY				(1ULL << 41)
 
+#define AMD64_EVENTSEL_INT_CORE_SEL_SHIFT		37
+#define AMD64_EVENTSEL_INT_CORE_SEL_MASK		\
+	(0xFULL << AMD64_EVENTSEL_INT_CORE_SEL_SHIFT)
+
 #define AMD64_EVENTSEL_EVENT	\
 	(ARCH_PERFMON_EVENTSEL_EVENT | (0x0FULL << 32))
 #define INTEL_ARCH_EVENT_MASK	\
@@ -46,8 +51,12 @@
 #define AMD64_RAW_EVENT_MASK		\
 	(X86_RAW_EVENT_MASK          |  \
 	 AMD64_EVENTSEL_EVENT)
+#define AMD64_RAW_EVENT_MASK_NB		\
+	(AMD64_EVENTSEL_EVENT        |  \
+	 ARCH_PERFMON_EVENTSEL_UMASK)
 #define AMD64_NUM_COUNTERS				4
 #define AMD64_NUM_COUNTERS_CORE				6
+#define AMD64_NUM_COUNTERS_NB				4
 
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)

commit 9f19010af8c651879ac2c36f1a808a3a4419cd40
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Feb 6 11:26:26 2013 -0600

    perf/x86/amd: Use proper naming scheme for AMD bit field definitions
    
    Update these AMD bit field names to be consistent with naming
    convention followed by the rest of the file.
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Stephane Eranian <eranian@google.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1360171589-6381-4-git-send-email-jacob.shin@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 4fabcdf1cfa7..2234eaaecb52 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -29,8 +29,8 @@
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
 
-#define AMD_PERFMON_EVENTSEL_GUESTONLY			(1ULL << 40)
-#define AMD_PERFMON_EVENTSEL_HOSTONLY			(1ULL << 41)
+#define AMD64_EVENTSEL_GUESTONLY			(1ULL << 40)
+#define AMD64_EVENTSEL_HOSTONLY				(1ULL << 41)
 
 #define AMD64_EVENTSEL_EVENT	\
 	(ARCH_PERFMON_EVENTSEL_EVENT | (0x0FULL << 32))

commit 91d7753a45f8525dc75b6be01e427dc1c378dc16
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Aug 7 15:20:38 2012 +0200

    perf: Factor __output_copy to be usable with specific copy function
    
    Adding a generic way to use __output_copy function with specific copy
    function via DEFINE_PERF_OUTPUT_COPY macro.
    
    Using this to add new __output_copy_user function, that provides output
    copy from user pointers. For x86 the copy_from_user_nmi function is used
    and __copy_from_user_inatomic for the rest of the architectures.
    
    This new function will be used in user stack dump on sample, coming in
    next patches.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Benjamin Redelings <benjamin.redelings@nescent.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Ulrich Drepper <drepper@gmail.com>
    Link: http://lkml.kernel.org/r/1344345647-11536-4-git-send-email-jolsa@redhat.com
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index cb4e43bce98a..4fabcdf1cfa7 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -262,4 +262,6 @@ static inline void perf_check_microcode(void) { }
  static inline void amd_pmu_disable_virt(void) { }
 #endif
 
+#define arch_perf_out_copy_user copy_from_user_nmi
+
 #endif /* _ASM_X86_PERF_EVENT_H */

commit d07bdfd322d307789f15b427dbcc39257665356f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Jul 10 09:42:15 2012 +0200

    perf/x86: Fix USER/KERNEL tagging of samples properly
    
    Some PMUs don't provide a full register set for their sample,
    specifically 'advanced' PMUs like AMD IBS and Intel PEBS which provide
    'better' than regular interrupt accuracy.
    
    In this case we use the interrupt regs as basis and over-write some
    fields (typically IP) with different information.
    
    The perf core however uses user_mode() to distinguish user/kernel
    samples, user_mode() relies on regs->cs. If the interrupt skid pushed
    us over a boundary the new IP might not be in the same domain as the
    interrupt.
    
    Commit ce5c1fe9a9e ("perf/x86: Fix USER/KERNEL tagging of samples")
    tried to fix this by making the perf core use kernel_ip(). This
    however is wrong (TM), as pointed out by Linus, since it doesn't allow
    for VM86 and non-zero based segments in IA32 mode.
    
    Therefore, provide a new helper to set the regs->ip field,
    set_linear_ip(), which massages the regs into a suitable state
    assuming the provided IP is in fact a linear address.
    
    Also modify perf_instruction_pointer() and perf_callchain_user() to
    deal with segments base offsets.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1341910954.3462.102.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index dab39350e51e..cb4e43bce98a 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -196,11 +196,16 @@ static inline u32 get_ibs_caps(void) { return 0; }
 extern void perf_events_lapic_init(void);
 
 /*
- * Abuse bit 3 of the cpu eflags register to indicate proper PEBS IP fixups.
- * This flag is otherwise unused and ABI specified to be 0, so nobody should
- * care what we do with it.
+ * Abuse bits {3,5} of the cpu eflags register. These flags are otherwise
+ * unused and ABI specified to be 0, so nobody should care what we do with
+ * them.
+ *
+ * EXACT - the IP points to the exact instruction that triggered the
+ *         event (HW bugs exempt).
+ * VM    - original X86_VM_MASK; see set_linear_ip().
  */
 #define PERF_EFLAGS_EXACT	(1UL << 3)
+#define PERF_EFLAGS_VM		(1UL << 5)
 
 struct pt_regs;
 extern unsigned long perf_instruction_pointer(struct pt_regs *regs);

commit 35d56ca9d401d9d0ac8d91e4db1485af5f38f6fd
Author: Jovi Zhang <bookjovi@gmail.com>
Date:   Tue Jul 17 10:14:41 2012 +0800

    perf/x86: Fix missing struct before structure name
    
    When CONFIG_PERF_EVENTS disabled, there will have a compiliation
    error, because missing struct before structure name.
    
    Signed-off-by: Jovi Zhang <bookjovi@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jiri Kosina <trivial@kernel.org>
    Link: http://lkml.kernel.org/r/CACV3sbKF%3DCX%2B2jWEWesfCA6rBoQ3wDM4-5ac9MuBtVbCtMRHdQ@mail.gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index c78f14a0df00..dab39350e51e 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -234,7 +234,7 @@ extern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr);
 extern void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap);
 extern void perf_check_microcode(void);
 #else
-static inline perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
+static inline struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
 {
 	*nr = 0;
 	return NULL;

commit c93dc84cbe32435be3ffa2fbde355eff94955c32
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 8 14:50:50 2012 +0200

    perf/x86: Add a microcode revision check for SNB-PEBS
    
    Recent Intel microcode resolved the SNB-PEBS issues, so conditionally
    enable PEBS on SNB hardware depending on the microcode revision.
    
    Thanks to Stephane for figuring out the various microcode revisions.
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-v3672ziwh9damwqwh1uz3krm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index ffdf5e0991c4..c78f14a0df00 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -232,6 +232,7 @@ struct perf_guest_switch_msr {
 
 extern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr);
 extern void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap);
+extern void perf_check_microcode(void);
 #else
 static inline perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
 {
@@ -245,6 +246,7 @@ static inline void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
 }
 
 static inline void perf_events_lapic_init(void)	{ }
+static inline void perf_check_microcode(void) { }
 #endif
 
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD)

commit b1dc3c4820428ac6216537416b2fcd140fdc52e5
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jun 20 20:46:35 2012 +0200

    perf/x86/amd: Unify AMD's generic and family 15h pmus
    
    There is no need for keeping separate pmu structs. We can enable
    amd_{get,put}_event_constraints() functions also for family 15h event.
    
    The advantage is that there is only a single pmu struct for all AMD
    cpus. This patch introduces functions to setup the pmu to enabe core
    performance counters or counter constraints.
    
    Also, cpuid checks are used instead of family checks where
    possible. Thus, it enables the code independently of cpu families if
    the feature flag is set.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1340217996-2254-4-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 3b31248caf60..ffdf5e0991c4 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -47,8 +47,7 @@
 	(X86_RAW_EVENT_MASK          |  \
 	 AMD64_EVENTSEL_EVENT)
 #define AMD64_NUM_COUNTERS				4
-#define AMD64_NUM_COUNTERS_F15H				6
-#define AMD64_NUM_COUNTERS_MAX				AMD64_NUM_COUNTERS_F15H
+#define AMD64_NUM_COUNTERS_CORE				6
 
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)

commit 15c7ad51ad58cbd3b46112c1840bc7228bd354bf
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jun 20 20:46:33 2012 +0200

    perf/x86: Rename Intel specific macros
    
    There are macros that are Intel specific and not x86 generic. Rename
    them into INTEL_*.
    
    This patch removes X86_PMC_IDX_GENERIC and does:
    
     $ sed -i -e 's/X86_PMC_MAX_/INTEL_PMC_MAX_/g'           \
             arch/x86/include/asm/kvm_host.h                 \
             arch/x86/include/asm/perf_event.h               \
             arch/x86/kernel/cpu/perf_event.c                \
             arch/x86/kernel/cpu/perf_event_p4.c             \
             arch/x86/kvm/pmu.c
     $ sed -i -e 's/X86_PMC_IDX_FIXED/INTEL_PMC_IDX_FIXED/g' \
             arch/x86/include/asm/perf_event.h               \
             arch/x86/kernel/cpu/perf_event.c                \
             arch/x86/kernel/cpu/perf_event_intel.c          \
             arch/x86/kernel/cpu/perf_event_intel_ds.c       \
             arch/x86/kvm/pmu.c
     $ sed -i -e 's/X86_PMC_MSK_/INTEL_PMC_MSK_/g'           \
             arch/x86/include/asm/perf_event.h               \
             arch/x86/kernel/cpu/perf_event.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1340217996-2254-2-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 588f52ea810e..3b31248caf60 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -5,11 +5,10 @@
  * Performance event hw details:
  */
 
-#define X86_PMC_MAX_GENERIC				       32
-#define X86_PMC_MAX_FIXED					3
+#define INTEL_PMC_MAX_GENERIC				       32
+#define INTEL_PMC_MAX_FIXED					3
+#define INTEL_PMC_IDX_FIXED				       32
 
-#define X86_PMC_IDX_GENERIC				        0
-#define X86_PMC_IDX_FIXED				       32
 #define X86_PMC_IDX_MAX					       64
 
 #define MSR_ARCH_PERFMON_PERFCTR0			      0xc1
@@ -121,16 +120,16 @@ struct x86_pmu_capability {
 
 /* Instr_Retired.Any: */
 #define MSR_ARCH_PERFMON_FIXED_CTR0	0x309
-#define X86_PMC_IDX_FIXED_INSTRUCTIONS	(X86_PMC_IDX_FIXED + 0)
+#define INTEL_PMC_IDX_FIXED_INSTRUCTIONS	(INTEL_PMC_IDX_FIXED + 0)
 
 /* CPU_CLK_Unhalted.Core: */
 #define MSR_ARCH_PERFMON_FIXED_CTR1	0x30a
-#define X86_PMC_IDX_FIXED_CPU_CYCLES	(X86_PMC_IDX_FIXED + 1)
+#define INTEL_PMC_IDX_FIXED_CPU_CYCLES	(INTEL_PMC_IDX_FIXED + 1)
 
 /* CPU_CLK_Unhalted.Ref: */
 #define MSR_ARCH_PERFMON_FIXED_CTR2	0x30b
-#define X86_PMC_IDX_FIXED_REF_CYCLES	(X86_PMC_IDX_FIXED + 2)
-#define X86_PMC_MSK_FIXED_REF_CYCLES	(1ULL << X86_PMC_IDX_FIXED_REF_CYCLES)
+#define INTEL_PMC_IDX_FIXED_REF_CYCLES	(INTEL_PMC_IDX_FIXED + 2)
+#define INTEL_PMC_MSK_FIXED_REF_CYCLES	(1ULL << INTEL_PMC_IDX_FIXED_REF_CYCLES)
 
 /*
  * We model BTS tracing as another fixed-mode PMC.
@@ -139,7 +138,7 @@ struct x86_pmu_capability {
  * values are used by actual fixed events and higher values are used
  * to indicate other overflow conditions in the PERF_GLOBAL_STATUS msr.
  */
-#define X86_PMC_IDX_FIXED_BTS				(X86_PMC_IDX_FIXED + 16)
+#define INTEL_PMC_IDX_FIXED_BTS				(INTEL_PMC_IDX_FIXED + 16)
 
 /*
  * IBS cpuid feature detection

commit 978da300c7a65494692b329a6a4cbf364afc37c5
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri May 11 11:44:59 2012 +0200

    perf/x86/ibs: Fix undefined reference to `get_ibs_caps'
    
    Fixing i386 allnoconfig built errors:
    
     arch/x86/built-in.o: In function `amd_pmu_hw_config':
     perf_event_amd.c:(.text+0xc3e1): undefined reference to `get_ibs_caps'
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 4e40a64315c9..588f52ea810e 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -188,7 +188,11 @@ struct x86_pmu_capability {
 #define IBS_OP_MAX_CNT_EXT	0x007FFFFFULL	/* not a register bit mask */
 #define IBS_RIP_INVALID		(1ULL<<38)
 
+#ifdef CONFIG_X86_LOCAL_APIC
 extern u32 get_ibs_caps(void);
+#else
+static inline u32 get_ibs_caps(void) { return 0; }
+#endif
 
 #ifdef CONFIG_PERF_EVENTS
 extern void perf_events_lapic_init(void);

commit d47e8238cd76f1ffa7c8cd30e08b8e9074fd597e
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Apr 2 20:19:11 2012 +0200

    perf/x86-ibs: Take instruction pointer from ibs sample
    
    Each IBS sample contains a linear address of the instruction that
    caused the sample to trigger. This address is more precise than the
    rip that was taken from the interrupt handler's stack. Update the rip
    with that address. We use this in the next patch to implement
    precise-event sampling on AMD systems using IBS.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1333390758-10893-6-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 8a3c75d824b7..4e40a64315c9 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -158,6 +158,7 @@ struct x86_pmu_capability {
 #define IBS_CAPS_OPCNT			(1U<<4)
 #define IBS_CAPS_BRNTRGT		(1U<<5)
 #define IBS_CAPS_OPCNTEXT		(1U<<6)
+#define IBS_CAPS_RIPINVALIDCHK		(1U<<7)
 
 #define IBS_CAPS_DEFAULT		(IBS_CAPS_AVAIL		\
 					 | IBS_CAPS_FETCHSAM	\
@@ -170,14 +171,14 @@ struct x86_pmu_capability {
 #define IBSCTL_LVT_OFFSET_VALID		(1ULL<<8)
 #define IBSCTL_LVT_OFFSET_MASK		0x0F
 
-/* IbsFetchCtl bits/masks */
+/* ibs fetch bits/masks */
 #define IBS_FETCH_RAND_EN	(1ULL<<57)
 #define IBS_FETCH_VAL		(1ULL<<49)
 #define IBS_FETCH_ENABLE	(1ULL<<48)
 #define IBS_FETCH_CNT		0xFFFF0000ULL
 #define IBS_FETCH_MAX_CNT	0x0000FFFFULL
 
-/* IbsOpCtl bits */
+/* ibs op bits/masks */
 /* lower 4 bits of the current count are ignored: */
 #define IBS_OP_CUR_CNT		(0xFFFF0ULL<<32)
 #define IBS_OP_CNT_CTL		(1ULL<<19)
@@ -185,6 +186,7 @@ struct x86_pmu_capability {
 #define IBS_OP_ENABLE		(1ULL<<17)
 #define IBS_OP_MAX_CNT		0x0000FFFFULL
 #define IBS_OP_MAX_CNT_EXT	0x007FFFFFULL	/* not a register bit mask */
+#define IBS_RIP_INVALID		(1ULL<<38)
 
 extern u32 get_ibs_caps(void);
 

commit ad8537cda68a8fe81776cceca6c22d54dd652ea5
Merge: 149936a068d8 fab06992de64
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 9 15:22:23 2012 +0200

    Merge branch 'perf/x86-ibs' into perf/core

commit 2e7580b0e75d771d93e24e681031a165b1d31071
Merge: d25413efa953 cf9eeac46350
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 14:35:31 2012 -0700

    Merge branch 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Avi Kivity:
     "Changes include timekeeping improvements, support for assigning host
      PCI devices that share interrupt lines, s390 user-controlled guests, a
      large ppc update, and random fixes."
    
    This is with the sign-off's fixed, hopefully next merge window we won't
    have rebased commits.
    
    * 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: Convert intx_mask_lock to spin lock
      KVM: x86: fix kvm_write_tsc() TSC matching thinko
      x86: kvmclock: abstract save/restore sched_clock_state
      KVM: nVMX: Fix erroneous exception bitmap check
      KVM: Ignore the writes to MSR_K7_HWCR(3)
      KVM: MMU: make use of ->root_level in reset_rsvds_bits_mask
      KVM: PMU: add proper support for fixed counter 2
      KVM: PMU: Fix raw event check
      KVM: PMU: warn when pin control is set in eventsel msr
      KVM: VMX: Fix delayed load of shared MSRs
      KVM: use correct tlbs dirty type in cmpxchg
      KVM: Allow host IRQ sharing for assigned PCI 2.3 devices
      KVM: Ensure all vcpus are consistent with in-kernel irqchip settings
      KVM: x86 emulator: Allow PM/VM86 switch during task switch
      KVM: SVM: Fix CPL updates
      KVM: x86 emulator: VM86 segments must have DPL 3
      KVM: x86 emulator: Fix task switch privilege checks
      arch/powerpc/kvm/book3s_hv.c: included linux/sched.h twice
      KVM: x86 emulator: correctly mask pmc index bits in RDPMC instruction emulation
      KVM: mmu_notifier: Flush TLBs before releasing mmu_lock
      ...

commit a7b9d2ccc3d86303ee9314612d301966e04011c7
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Feb 26 16:55:40 2012 +0200

    KVM: PMU: warn when pin control is set in eventsel msr
    
    Print warning once if pin control bit is set in eventsel msr since
    emulation does not support it yet.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 096c975e099f..f1f71823f682 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -23,6 +23,7 @@
 #define ARCH_PERFMON_EVENTSEL_USR			(1ULL << 16)
 #define ARCH_PERFMON_EVENTSEL_OS			(1ULL << 17)
 #define ARCH_PERFMON_EVENTSEL_EDGE			(1ULL << 18)
+#define ARCH_PERFMON_EVENTSEL_PIN_CONTROL		(1ULL << 19)
 #define ARCH_PERFMON_EVENTSEL_INT			(1ULL << 20)
 #define ARCH_PERFMON_EVENTSEL_ANY			(1ULL << 21)
 #define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)

commit db98c5faf8cb350212ea3af786cb3ba0d4e7a01e
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Dec 15 17:56:39 2011 +0100

    perf/x86: Implement 64-bit counter support for IBS
    
    This patch implements 64 bit counter support for IBS. The
    sampling period is no longer limited to the hw counter width.
    
    The functions perf_event_set_period() and
    perf_event_try_update() can be used as generic functions. They
    can replace similar code that is duplicate across architectures.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1323968199-9326-5-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index e8fb2c7a5f4f..9cf66965141d 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -177,6 +177,8 @@ struct x86_pmu_capability {
 #define IBS_FETCH_MAX_CNT	0x0000FFFFULL
 
 /* IbsOpCtl bits */
+/* lower 4 bits of the current count are ignored: */
+#define IBS_OP_CUR_CNT		(0xFFFF0ULL<<32)
 #define IBS_OP_CNT_CTL		(1ULL<<19)
 #define IBS_OP_VAL		(1ULL<<18)
 #define IBS_OP_ENABLE		(1ULL<<17)

commit 737f24bda723fdf89ecaacb99fa2bf5683c32799
Merge: 8eedce996556 b7c924274c45
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 5 09:20:08 2012 +0100

    Merge branch 'perf/urgent' into perf/core
    
    Conflicts:
            tools/perf/builtin-record.c
            tools/perf/builtin-top.c
            tools/perf/perf.h
            tools/perf/util/top.h
    
    Merge reason: resolve these cherry-picking conflicts.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 1018faa6cf23b256bf25919ef203cd7c129f06f2
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Wed Feb 29 14:57:32 2012 +0100

    perf/x86/kvm: Fix Host-Only/Guest-Only counting with SVM disabled
    
    It turned out that a performance counter on AMD does not
    count at all when the GO or HO bit is set in the control
    register and SVM is disabled in EFER.
    
    This patch works around this issue by masking out the HO bit
    in the performance counter control register when SVM is not
    enabled.
    
    The GO bit is not touched because it is only set when the
    user wants to count in guest-mode only. So when SVM is
    disabled the counter should not run at all and the
    not-counting is the intended behaviour.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: stable@vger.kernel.org # v3.2
    Link: http://lkml.kernel.org/r/1330523852-19566-1-git-send-email-joerg.roedel@amd.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 096c975e099f..461ce432b1c2 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -242,4 +242,12 @@ static inline void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
 static inline void perf_events_lapic_init(void)	{ }
 #endif
 
+#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD)
+ extern void amd_pmu_enable_virt(void);
+ extern void amd_pmu_disable_virt(void);
+#else
+ static inline void amd_pmu_enable_virt(void) { }
+ static inline void amd_pmu_disable_virt(void) { }
+#endif
+
 #endif /* _ASM_X86_PERF_EVENT_H */

commit 35edc2a5095efb189e60dc32bbb9d2663aec6d24
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sun Nov 20 20:36:02 2011 +0100

    perf, arch: Rework perf_event_index()
    
    Put the logic to compute the event index into a per pmu method. This
    is required because the x86 rules are weird and wonderful and don't
    match the capabilities of the current scheme.
    
    AFAIK only powerpc actually has a usable userspace read of the PMCs
    but I'm not at all sure anybody actually used that.
    
    ARM is restored to the default since it currently does not support
    userspace access at all. And all software events are provided with a
    method that reports their index as 0 (disabled).
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Michael Cree <mcree@orcon.net.nz>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Arun Sharma <asharma@fb.com>
    Link: http://lkml.kernel.org/n/tip-dfydxodki16lylkt3gl2j7cw@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 096c975e099f..9b922c136254 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -188,8 +188,6 @@ extern u32 get_ibs_caps(void);
 #ifdef CONFIG_PERF_EVENTS
 extern void perf_events_lapic_init(void);
 
-#define PERF_EVENT_INDEX_OFFSET			0
-
 /*
  * Abuse bit 3 of the cpu eflags register to indicate proper PEBS IP fixups.
  * This flag is otherwise unused and ABI specified to be 0, so nobody should

commit cd09c0c40a971549800ce6a7e53c63f5139dd175
Author: Stephane Eranian <eranian@google.com>
Date:   Sun Dec 11 00:28:51 2011 +0100

    perf events: Enable raw event support for Intel unhalted_reference_cycles event
    
    This patch adds the encoding and definitions necessary for the
    unhalted_reference_cycles event avaialble since Intel Core 2 processors.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1323559734-3488-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index b50e9d15aae0..096c975e099f 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -112,23 +112,24 @@ struct x86_pmu_capability {
 /*
  * All 3 fixed-mode PMCs are configured via this single MSR:
  */
-#define MSR_ARCH_PERFMON_FIXED_CTR_CTRL			0x38d
+#define MSR_ARCH_PERFMON_FIXED_CTR_CTRL	0x38d
 
 /*
  * The counts are available in three separate MSRs:
  */
 
 /* Instr_Retired.Any: */
-#define MSR_ARCH_PERFMON_FIXED_CTR0			0x309
-#define X86_PMC_IDX_FIXED_INSTRUCTIONS			(X86_PMC_IDX_FIXED + 0)
+#define MSR_ARCH_PERFMON_FIXED_CTR0	0x309
+#define X86_PMC_IDX_FIXED_INSTRUCTIONS	(X86_PMC_IDX_FIXED + 0)
 
 /* CPU_CLK_Unhalted.Core: */
-#define MSR_ARCH_PERFMON_FIXED_CTR1			0x30a
-#define X86_PMC_IDX_FIXED_CPU_CYCLES			(X86_PMC_IDX_FIXED + 1)
+#define MSR_ARCH_PERFMON_FIXED_CTR1	0x30a
+#define X86_PMC_IDX_FIXED_CPU_CYCLES	(X86_PMC_IDX_FIXED + 1)
 
 /* CPU_CLK_Unhalted.Ref: */
-#define MSR_ARCH_PERFMON_FIXED_CTR2			0x30b
-#define X86_PMC_IDX_FIXED_BUS_CYCLES			(X86_PMC_IDX_FIXED + 2)
+#define MSR_ARCH_PERFMON_FIXED_CTR2	0x30b
+#define X86_PMC_IDX_FIXED_REF_CYCLES	(X86_PMC_IDX_FIXED + 2)
+#define X86_PMC_MSK_FIXED_REF_CYCLES	(1ULL << X86_PMC_IDX_FIXED_REF_CYCLES)
 
 /*
  * We model BTS tracing as another fixed-mode PMC.

commit b3d9468a8bd218a695e3a0ff112cd4efd27b670a
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Nov 10 14:57:27 2011 +0200

    perf, x86: Expose perf capability to other modules
    
    KVM needs to know perf capability to decide which PMU it can expose to a
    guest.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1320929850-10480-8-git-send-email-gleb@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index c6998bc75456..b50e9d15aae0 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -95,6 +95,15 @@ union cpuid10_edx {
 	unsigned int full;
 };
 
+struct x86_pmu_capability {
+	int		version;
+	int		num_counters_gp;
+	int		num_counters_fixed;
+	int		bit_width_gp;
+	int		bit_width_fixed;
+	unsigned int	events_mask;
+	int		events_mask_len;
+};
 
 /*
  * Fixed-purpose performance events:
@@ -216,6 +225,7 @@ struct perf_guest_switch_msr {
 };
 
 extern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr);
+extern void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap);
 #else
 static inline perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
 {
@@ -223,6 +233,11 @@ static inline perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
 	return NULL;
 }
 
+static inline void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
+{
+	memset(cap, 0, sizeof(*cap));
+}
+
 static inline void perf_events_lapic_init(void)	{ }
 #endif
 

commit ffb871bc9156ee2e5cf442f61250c5bd6aad17e3
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Nov 10 14:57:26 2011 +0200

    x86, perf: Disable non available architectural events
    
    Intel CPUs report non-available architectural events in cpuid leaf
    0AH.EBX. Use it to disable events that are not available according
    to CPU.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1320929850-10480-7-git-send-email-gleb@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index f61c62f7d5d8..c6998bc75456 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -57,6 +57,7 @@
 		(1 << (ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX))
 
 #define ARCH_PERFMON_BRANCH_MISSES_RETIRED		6
+#define ARCH_PERFMON_EVENTS_COUNT			7
 
 /*
  * Intel "Architectural Performance Monitoring" CPUID
@@ -72,6 +73,19 @@ union cpuid10_eax {
 	unsigned int full;
 };
 
+union cpuid10_ebx {
+	struct {
+		unsigned int no_unhalted_core_cycles:1;
+		unsigned int no_instructions_retired:1;
+		unsigned int no_unhalted_reference_cycles:1;
+		unsigned int no_llc_reference:1;
+		unsigned int no_llc_misses:1;
+		unsigned int no_branch_instruction_retired:1;
+		unsigned int no_branch_misses_retired:1;
+	} split;
+	unsigned int full;
+};
+
 union cpuid10_edx {
 	struct {
 		unsigned int num_counters_fixed:5;

commit b716916679e72054d436afadce2f94dcad71cfad
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Sep 21 11:30:18 2011 +0200

    perf, x86: Implement IBS initialization
    
    This patch implements IBS feature detection and initialzation. The
    code is shared between perf and oprofile. If IBS is available on the
    system for perf, a pmu is setup.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1316597423-25723-3-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index e9e277c6991b..f61c62f7d5d8 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -159,6 +159,8 @@ union cpuid10_edx {
 #define IBS_OP_MAX_CNT		0x0000FFFFULL
 #define IBS_OP_MAX_CNT_EXT	0x007FFFFFULL	/* not a register bit mask */
 
+extern u32 get_ibs_caps(void);
+
 #ifdef CONFIG_PERF_EVENTS
 extern void perf_events_lapic_init(void);
 

commit ee5789dbcc800ba7d641443e53f60d53977f9747
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Sep 21 11:30:17 2011 +0200

    perf, x86: Share IBS macros between perf and oprofile
    
    Moving IBS macros from oprofile to <asm/perf_event.h> to make it
    available to perf. No additional changes.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1316597423-25723-2-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index e47cb6167e8f..e9e277c6991b 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -46,14 +46,17 @@
 #define AMD64_RAW_EVENT_MASK		\
 	(X86_RAW_EVENT_MASK          |  \
 	 AMD64_EVENTSEL_EVENT)
+#define AMD64_NUM_COUNTERS				4
+#define AMD64_NUM_COUNTERS_F15H				6
+#define AMD64_NUM_COUNTERS_MAX				AMD64_NUM_COUNTERS_F15H
 
-#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		      0x3c
+#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)
-#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX			 0
+#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX		0
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_PRESENT \
 		(1 << (ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX))
 
-#define ARCH_PERFMON_BRANCH_MISSES_RETIRED			 6
+#define ARCH_PERFMON_BRANCH_MISSES_RETIRED		6
 
 /*
  * Intel "Architectural Performance Monitoring" CPUID
@@ -113,6 +116,35 @@ union cpuid10_edx {
  */
 #define X86_PMC_IDX_FIXED_BTS				(X86_PMC_IDX_FIXED + 16)
 
+/*
+ * IBS cpuid feature detection
+ */
+
+#define IBS_CPUID_FEATURES		0x8000001b
+
+/*
+ * Same bit mask as for IBS cpuid feature flags (Fn8000_001B_EAX), but
+ * bit 0 is used to indicate the existence of IBS.
+ */
+#define IBS_CAPS_AVAIL			(1U<<0)
+#define IBS_CAPS_FETCHSAM		(1U<<1)
+#define IBS_CAPS_OPSAM			(1U<<2)
+#define IBS_CAPS_RDWROPCNT		(1U<<3)
+#define IBS_CAPS_OPCNT			(1U<<4)
+#define IBS_CAPS_BRNTRGT		(1U<<5)
+#define IBS_CAPS_OPCNTEXT		(1U<<6)
+
+#define IBS_CAPS_DEFAULT		(IBS_CAPS_AVAIL		\
+					 | IBS_CAPS_FETCHSAM	\
+					 | IBS_CAPS_OPSAM)
+
+/*
+ * IBS APIC setup
+ */
+#define IBSCTL				0x1cc
+#define IBSCTL_LVT_OFFSET_VALID		(1ULL<<8)
+#define IBSCTL_LVT_OFFSET_MASK		0x0F
+
 /* IbsFetchCtl bits/masks */
 #define IBS_FETCH_RAND_EN	(1ULL<<57)
 #define IBS_FETCH_VAL		(1ULL<<49)

commit 144d31e6f1902a39bc95754d820d356722697850
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Oct 5 14:01:21 2011 +0200

    perf, intel: Use GO/HO bits in perf-ctr
    
    Intel does not have guest/host-only bit in perf counters like AMD
    does.  To support GO/HO bits KVM needs to switch EVENTSELn values
    (or PERF_GLOBAL_CTRL if available) at a guest entry. If a counter is
    configured to count only in a guest mode it stays disabled in a host,
    but VMX is configured to switch it to enabled value during guest entry.
    
    This patch adds GO/HO tracking to Intel perf code and provides interface
    for KVM to get a list of MSRs that need to be switched on a guest entry.
    
    Only cpus with architectural PMU (v1 or later) are supported with this
    patch.  To my knowledge there is not p6 models with VMX but without
    architectural PMU and p4 with VMX are rare and the interface is general
    enough to support them if need arise.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1317816084-18026-7-git-send-email-gleb@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index ce2bfb335384..e47cb6167e8f 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -162,7 +162,19 @@ extern unsigned long perf_misc_flags(struct pt_regs *regs);
 	);							\
 }
 
+struct perf_guest_switch_msr {
+	unsigned msr;
+	u64 host, guest;
+};
+
+extern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr);
 #else
+static inline perf_guest_switch_msr *perf_guest_get_msrs(int *nr)
+{
+	*nr = 0;
+	return NULL;
+}
+
 static inline void perf_events_lapic_init(void)	{ }
 #endif
 

commit 011af857847a7ebe1f45342b29ff9f390515a4b8
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Wed Oct 5 14:01:17 2011 +0200

    perf, amd: Use GO/HO bits in perf-ctr
    
    The AMD perf-counters support counting in guest or host-mode
    only. Make use of that feature when user-space specified
    guest/host-mode only counting.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1317816084-18026-3-git-send-email-gleb@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 094fb30817ab..ce2bfb335384 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -29,6 +29,9 @@
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
 
+#define AMD_PERFMON_EVENTSEL_GUESTONLY			(1ULL << 40)
+#define AMD_PERFMON_EVENTSEL_HOSTONLY			(1ULL << 41)
+
 #define AMD64_EVENTSEL_EVENT	\
 	(ARCH_PERFMON_EVENTSEL_EVENT | (0x0FULL << 32))
 #define INTEL_ARCH_EVENT_MASK	\

commit 9e46294dadedc0c04adcb8ce760bd2cd74f7332d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Jul 2 15:00:52 2011 +0200

    x86: Save stack pointer in perf live regs savings
    
    In order to prepare for fetching the stack pointer from the
    regs when possible in dump_trace() instead of taking the
    local one, save the current stack pointer in perf live regs saving.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index d9d4dae305f6..094fb30817ab 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -152,6 +152,11 @@ extern unsigned long perf_misc_flags(struct pt_regs *regs);
 	(regs)->bp = caller_frame_pointer();			\
 	(regs)->cs = __KERNEL_CS;				\
 	regs->flags = 0;					\
+	asm volatile(						\
+		_ASM_MOV "%%"_ASM_SP ", %0\n"			\
+		: "=m" ((regs)->sp)				\
+		:: "memory"					\
+	);							\
 }
 
 #else

commit 004417a6d468e24399e383645c068b498eed84ad
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Nov 25 18:38:29 2010 +0100

    perf, arch: Cleanup perf-pmu init vs lockup-detector
    
    The perf hardware pmu got initialized at various points in the boot,
    some before early_initcall() some after (notably arch_initcall).
    
    The problem is that the NMI lockup detector is ran from early_initcall()
    and expects the hardware pmu to be present.
    
    Sanitize this by moving all architecture hardware pmu implementations to
    initialize at early_initcall() and move the lockup detector to an explicit
    initcall right after that.
    
    Cc: paulus <paulus@samba.org>
    Cc: davem <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1290707759.2145.119.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 550e26b1dbb3..d9d4dae305f6 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -125,7 +125,6 @@ union cpuid10_edx {
 #define IBS_OP_MAX_CNT_EXT	0x007FFFFFULL	/* not a register bit mask */
 
 #ifdef CONFIG_PERF_EVENTS
-extern void init_hw_perf_events(void);
 extern void perf_events_lapic_init(void);
 
 #define PERF_EVENT_INDEX_OFFSET			0
@@ -156,7 +155,6 @@ extern unsigned long perf_misc_flags(struct pt_regs *regs);
 }
 
 #else
-static inline void init_hw_perf_events(void)		{ }
 static inline void perf_events_lapic_init(void)	{ }
 #endif
 

commit b47fad3bfb5940cc3e28a1c69716f6dc44e4b7e6
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Sep 22 17:45:39 2010 +0200

    oprofile, x86: Add support for IBS periodic op counter extension
    
    The count value for IBS op sampling has been extended by 7 bits. The
    feature is reflected in bit 6 (OpCntExt) of the IBS capability
    register (CPUID Fn8000_001B_EAX).
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 6e742cc4251b..550e26b1dbb3 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -111,17 +111,18 @@ union cpuid10_edx {
 #define X86_PMC_IDX_FIXED_BTS				(X86_PMC_IDX_FIXED + 16)
 
 /* IbsFetchCtl bits/masks */
-#define IBS_FETCH_RAND_EN		(1ULL<<57)
-#define IBS_FETCH_VAL			(1ULL<<49)
-#define IBS_FETCH_ENABLE		(1ULL<<48)
-#define IBS_FETCH_CNT			0xFFFF0000ULL
-#define IBS_FETCH_MAX_CNT		0x0000FFFFULL
+#define IBS_FETCH_RAND_EN	(1ULL<<57)
+#define IBS_FETCH_VAL		(1ULL<<49)
+#define IBS_FETCH_ENABLE	(1ULL<<48)
+#define IBS_FETCH_CNT		0xFFFF0000ULL
+#define IBS_FETCH_MAX_CNT	0x0000FFFFULL
 
 /* IbsOpCtl bits */
-#define IBS_OP_CNT_CTL			(1ULL<<19)
-#define IBS_OP_VAL			(1ULL<<18)
-#define IBS_OP_ENABLE			(1ULL<<17)
-#define IBS_OP_MAX_CNT			0x0000FFFFULL
+#define IBS_OP_CNT_CTL		(1ULL<<19)
+#define IBS_OP_VAL		(1ULL<<18)
+#define IBS_OP_ENABLE		(1ULL<<17)
+#define IBS_OP_MAX_CNT		0x0000FFFFULL
+#define IBS_OP_MAX_CNT_EXT	0x007FFFFFULL	/* not a register bit mask */
 
 #ifdef CONFIG_PERF_EVENTS
 extern void init_hw_perf_events(void);

commit c726b61c6a5acc54c55ed7a0e7638cc4c5a100a8
Merge: 7be7923633a1 018378c55b03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 9 18:55:20 2010 +0200

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/random-tracing into perf/core

commit b0f82b81fe6bbcf78d478071f33e44554726bc81
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 20 07:47:21 2010 +0200

    perf: Drop the skip argument from perf_arch_fetch_regs_caller
    
    Drop this argument now that we always want to rewind only to the
    state of the first caller.
    It means frame pointers are not necessary anymore to reliably get
    the source of an event. But this also means we need this helper
    to be a macro now, as an inline function is not an option since
    we need to know when to provide a default implentation.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 254883d0c7e0..02de29830ffe 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -140,6 +140,19 @@ extern unsigned long perf_instruction_pointer(struct pt_regs *regs);
 extern unsigned long perf_misc_flags(struct pt_regs *regs);
 #define perf_misc_flags(regs)	perf_misc_flags(regs)
 
+#include <asm/stacktrace.h>
+
+/*
+ * We abuse bit 3 from flags to pass exact information, see perf_misc_flags
+ * and the comment with PERF_EFLAGS_EXACT.
+ */
+#define perf_arch_fetch_caller_regs(regs, __ip)		{	\
+	(regs)->ip = (__ip);					\
+	(regs)->bp = caller_frame_pointer();			\
+	(regs)->cs = __KERNEL_CS;				\
+	regs->flags = 0;					\
+}
+
 #else
 static inline void init_hw_perf_events(void)		{ }
 static inline void perf_events_lapic_init(void)	{ }

commit e768aee89c687a50e6a2110e30c5cae1fbf0d2da
Author: Livio Soares <livio@eecg.toronto.edu>
Date:   Thu Jun 3 15:00:31 2010 -0400

    perf, x86: Small fix to cpuid10_edx
    
    Fixes to 'cpuid10_edx' to comply with Intel documentation.
    According to the Intel Manual, Volume 2A, Table 3-12, the cpuid for
    architecture performance monitoring returns, in EDX, two pieces of
    information:
    
      1) Number of fixed-function counters (5 bits, not 4)
      2) Width of fixed-function counters (8 bits)
    
    Signed-off-by: Livio Soares <livio@eecg.toronto.edu>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 254883d0c7e0..6ed3ae4f5482 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -68,8 +68,9 @@ union cpuid10_eax {
 
 union cpuid10_edx {
 	struct {
-		unsigned int num_counters_fixed:4;
-		unsigned int reserved:28;
+		unsigned int num_counters_fixed:5;
+		unsigned int bit_width_fixed:8;
+		unsigned int reserved:19;
 	} split;
 	unsigned int full;
 };

commit 39447b386c846bbf1c56f6403c5282837486200f
Author: Zhang, Yanmin <yanmin_zhang@linux.intel.com>
Date:   Mon Apr 19 13:32:41 2010 +0800

    perf: Enhance perf to allow for guest statistic collection from host
    
    Below patch introduces perf_guest_info_callbacks and related
    register/unregister functions. Add more PERF_RECORD_MISC_XXX bits
    meaning guest kernel and guest user space.
    
    Signed-off-by: Zhang Yanmin <yanmin_zhang@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index f6d43dbfd8e7..254883d0c7e0 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -135,17 +135,10 @@ extern void perf_events_lapic_init(void);
  */
 #define PERF_EFLAGS_EXACT	(1UL << 3)
 
-#define perf_misc_flags(regs)				\
-({	int misc = 0;					\
-	if (user_mode(regs))				\
-		misc |= PERF_RECORD_MISC_USER;		\
-	else						\
-		misc |= PERF_RECORD_MISC_KERNEL;	\
-	if (regs->flags & PERF_EFLAGS_EXACT)		\
-		misc |= PERF_RECORD_MISC_EXACT;		\
-	misc; })
-
-#define perf_instruction_pointer(regs)	((regs)->ip)
+struct pt_regs;
+extern unsigned long perf_instruction_pointer(struct pt_regs *regs);
+extern unsigned long perf_misc_flags(struct pt_regs *regs);
+#define perf_misc_flags(regs)	perf_misc_flags(regs)
 
 #else
 static inline void init_hw_perf_events(void)		{ }

commit a098f4484bc7dae23f5b62360954007b99b64600
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Mar 30 11:28:21 2010 +0200

    perf, x86: implement ARCH_PERFMON_EVENTSEL bit masks
    
    ARCH_PERFMON_EVENTSEL bit masks are often used in the kernel. This
    patch adds macros for the bit masks and removes local defines. The
    function intel_pmu_raw_event() becomes x86_pmu_raw_event() which is
    generic for x86 models and same also for p6. Duplicate code is
    removed.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100330092821.GH11907@erda.amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 987bf673141e..f6d43dbfd8e7 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -18,39 +18,31 @@
 #define MSR_ARCH_PERFMON_EVENTSEL0			     0x186
 #define MSR_ARCH_PERFMON_EVENTSEL1			     0x187
 
-#define ARCH_PERFMON_EVENTSEL_ENABLE			  (1 << 22)
-#define ARCH_PERFMON_EVENTSEL_ANY			  (1 << 21)
-#define ARCH_PERFMON_EVENTSEL_INT			  (1 << 20)
-#define ARCH_PERFMON_EVENTSEL_OS			  (1 << 17)
-#define ARCH_PERFMON_EVENTSEL_USR			  (1 << 16)
-
-/*
- * Includes eventsel and unit mask as well:
- */
-
-
-#define INTEL_ARCH_EVTSEL_MASK		0x000000FFULL
-#define INTEL_ARCH_UNIT_MASK		0x0000FF00ULL
-#define INTEL_ARCH_EDGE_MASK		0x00040000ULL
-#define INTEL_ARCH_INV_MASK		0x00800000ULL
-#define INTEL_ARCH_CNT_MASK		0xFF000000ULL
-#define INTEL_ARCH_EVENT_MASK	(INTEL_ARCH_UNIT_MASK|INTEL_ARCH_EVTSEL_MASK)
-
-/*
- * filter mask to validate fixed counter events.
- * the following filters disqualify for fixed counters:
- *  - inv
- *  - edge
- *  - cnt-mask
- *  The other filters are supported by fixed counters.
- *  The any-thread option is supported starting with v3.
- */
-#define INTEL_ARCH_FIXED_MASK \
-	(INTEL_ARCH_CNT_MASK| \
-	 INTEL_ARCH_INV_MASK| \
-	 INTEL_ARCH_EDGE_MASK|\
-	 INTEL_ARCH_UNIT_MASK|\
-	 INTEL_ARCH_EVENT_MASK)
+#define ARCH_PERFMON_EVENTSEL_EVENT			0x000000FFULL
+#define ARCH_PERFMON_EVENTSEL_UMASK			0x0000FF00ULL
+#define ARCH_PERFMON_EVENTSEL_USR			(1ULL << 16)
+#define ARCH_PERFMON_EVENTSEL_OS			(1ULL << 17)
+#define ARCH_PERFMON_EVENTSEL_EDGE			(1ULL << 18)
+#define ARCH_PERFMON_EVENTSEL_INT			(1ULL << 20)
+#define ARCH_PERFMON_EVENTSEL_ANY			(1ULL << 21)
+#define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)
+#define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
+#define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
+
+#define AMD64_EVENTSEL_EVENT	\
+	(ARCH_PERFMON_EVENTSEL_EVENT | (0x0FULL << 32))
+#define INTEL_ARCH_EVENT_MASK	\
+	(ARCH_PERFMON_EVENTSEL_UMASK | ARCH_PERFMON_EVENTSEL_EVENT)
+
+#define X86_RAW_EVENT_MASK		\
+	(ARCH_PERFMON_EVENTSEL_EVENT |	\
+	 ARCH_PERFMON_EVENTSEL_UMASK |	\
+	 ARCH_PERFMON_EVENTSEL_EDGE  |	\
+	 ARCH_PERFMON_EVENTSEL_INV   |	\
+	 ARCH_PERFMON_EVENTSEL_CMASK)
+#define AMD64_RAW_EVENT_MASK		\
+	(X86_RAW_EVENT_MASK          |  \
+	 AMD64_EVENTSEL_EVENT)
 
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		      0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)

commit 948b1bb89a44561560531394c18da4a99215f772
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Mar 29 18:36:50 2010 +0200

    perf, x86: Undo some some *_counter* -> *_event* renames
    
    The big rename:
    
     cdd6c48 perf: Do the big rename: Performance Counters -> Performance Events
    
    accidentally renamed some members of stucts that were named after
    registers in the spec. To avoid confusion this patch reverts some
    changes. The related specs are MSR descriptions in AMD's BKDGs and the
    ARCHITECTURAL PERFORMANCE MONITORING section in the Intel 64 and IA-32
    Architectures Software Developer's Manuals.
    
    This patch does:
    
     $ sed -i -e 's:num_events:num_counters:g' \
       arch/x86/include/asm/perf_event.h \
       arch/x86/kernel/cpu/perf_event_amd.c \
       arch/x86/kernel/cpu/perf_event.c \
       arch/x86/kernel/cpu/perf_event_intel.c \
       arch/x86/kernel/cpu/perf_event_p6.c \
       arch/x86/kernel/cpu/perf_event_p4.c \
       arch/x86/oprofile/op_model_ppro.c
    
     $ sed -i -e 's:event_bits:cntval_bits:g' -e 's:event_mask:cntval_mask:g' \
       arch/x86/kernel/cpu/perf_event_amd.c \
       arch/x86/kernel/cpu/perf_event.c \
       arch/x86/kernel/cpu/perf_event_intel.c \
       arch/x86/kernel/cpu/perf_event_p6.c \
       arch/x86/kernel/cpu/perf_event_p4.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1269880612-25800-2-git-send-email-robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 124dddd598f3..987bf673141e 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -67,7 +67,7 @@
 union cpuid10_eax {
 	struct {
 		unsigned int version_id:8;
-		unsigned int num_events:8;
+		unsigned int num_counters:8;
 		unsigned int bit_width:8;
 		unsigned int mask_length:8;
 	} split;
@@ -76,7 +76,7 @@ union cpuid10_eax {
 
 union cpuid10_edx {
 	struct {
-		unsigned int num_events_fixed:4;
+		unsigned int num_counters_fixed:4;
 		unsigned int reserved:28;
 	} split;
 	unsigned int full;

commit a072738e04f0eb26370e39ec679e9a0d65e49aea
Author: Cyrill Gorcunov <gorcunov@openvz.org>
Date:   Thu Mar 11 19:54:39 2010 +0300

    perf, x86: Implement initial P4 PMU driver
    
    The netburst PMU is way different from the "architectural
    perfomance monitoring" specification that current CPUs use.
    P4 uses a tuple of ESCR+CCCR+COUNTER MSR registers to handle
    perfomance monitoring events.
    
    A few implementational details:
    
    1) We need a separate x86_pmu::hw_config helper in struct
       x86_pmu since register bit-fields are quite different from P6,
       Core and later cpu series.
    
    2) For the same reason is a x86_pmu::schedule_events helper
       introduced.
    
    3) hw_perf_event::config consists of packed ESCR+CCCR values.
       It's allowed since in reality both registers only use a half
       of their size. Of course before making a real write into a
       particular MSR we need to unpack the value and extend it to
       a proper size.
    
    4) The tuple of packed ESCR+CCCR in hw_perf_event::config
       doesn't describe the memory address of ESCR MSR register
       so that we need to keep a mapping between these tuples
       used and available ESCR (various P4 events may use same
       ESCRs but not simultaneously), for this sake every active
       event has a per-cpu map of hw_perf_event::idx <--> ESCR
       addresses.
    
    5) Since hw_perf_event::idx is an offset to counter/control register
       we need to lift X86_PMC_MAX_GENERIC up, otherwise kernel
       strips it down to 8 registers and event armed may never be turned
       off (ie the bit in active_mask is set but the loop never reaches
       this index to check), thanks to Peter Zijlstra
    
    Restrictions:
    
     - No cascaded counters support (do we ever need them?)
     - No dependent events support (so PERF_COUNT_HW_INSTRUCTIONS
       doesn't work for now)
     - There are events with same counters which can't work simultaneously
       (need to use intersected ones due to broken counter 1)
     - No PERF_COUNT_HW_CACHE_ events yet
    
    Todo:
    
     - Implement dependent events
     - Need proper hashing for event opcodes (no linear search, good for
       debugging stage but not in real loads)
     - Some events counted during a clock cycle -- need to set threshold
       for them and count every clock cycle just to get summary statistics
       (ie to behave the same way as other PMUs do)
     - Need to swicth to use event_constraints
     - To support RAW events we need to encode a global list of P4 events
       into p4_templates
     - Cache events need to be added
    
    Event support status matrix:
    
     Event                  status
     -----------------------------
     cycles                 works
     cache-references       works
     cache-misses           works
     branch-misses          works
     bus-cycles             partially (does not work on 64bit cpu with HT enabled)
     instruction            doesnt work (needs dependent event [mop tagging])
     branches               doesnt work
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Signed-off-by: Lin Ming <ming.m.lin@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20100311165439.GB5129@lenovo>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index a9038c951619..124dddd598f3 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -5,7 +5,7 @@
  * Performance event hw details:
  */
 
-#define X86_PMC_MAX_GENERIC					8
+#define X86_PMC_MAX_GENERIC				       32
 #define X86_PMC_MAX_FIXED					3
 
 #define X86_PMC_IDX_GENERIC				        0

commit ef21f683a045a79b6aa86ad81e5fdfc0d5ddd250
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Mar 3 13:12:23 2010 +0100

    perf, x86: use LBR for PEBS IP+1 fixup
    
    Use the LBR to fix up the PEBS IP+1 issue.
    
    As said, PEBS reports the next instruction, here we use the LBR to find
    the last branch and from that construct the actual IP. If the IP matches
    the LBR-TO, we use LBR-FROM, otherwise we use the LBR-TO address as the
    beginning of the last basic block and decode forward.
    
    Once we find a match to the current IP, we use the previous location.
    
    This patch introduces a new ABI element: PERF_RECORD_MISC_EXACT, which
    conveys that the reported IP (PERF_SAMPLE_IP) is the exact instruction
    that caused the event (barring CPU errata).
    
    The fixup can fail due to various reasons:
    
     1) LBR contains invalid data (quite possible)
     2) part of the basic block got paged out
     3) the reported IP isn't part of the basic block (see 1)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Cc: paulus@samba.org
    Cc: eranian@google.com
    Cc: robert.richter@amd.com
    Cc: fweisbec@gmail.com
    LKML-Reference: <20100304140100.619375431@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index db6109a885a7..a9038c951619 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -136,6 +136,25 @@ extern void perf_events_lapic_init(void);
 
 #define PERF_EVENT_INDEX_OFFSET			0
 
+/*
+ * Abuse bit 3 of the cpu eflags register to indicate proper PEBS IP fixups.
+ * This flag is otherwise unused and ABI specified to be 0, so nobody should
+ * care what we do with it.
+ */
+#define PERF_EFLAGS_EXACT	(1UL << 3)
+
+#define perf_misc_flags(regs)				\
+({	int misc = 0;					\
+	if (user_mode(regs))				\
+		misc |= PERF_RECORD_MISC_USER;		\
+	else						\
+		misc |= PERF_RECORD_MISC_KERNEL;	\
+	if (regs->flags & PERF_EFLAGS_EXACT)		\
+		misc |= PERF_RECORD_MISC_EXACT;		\
+	misc; })
+
+#define perf_instruction_pointer(regs)	((regs)->ip)
+
 #else
 static inline void init_hw_perf_events(void)		{ }
 static inline void perf_events_lapic_init(void)	{ }

commit b622d644c7d61a5cb95b74e7b143c263bed21f0a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Feb 1 15:36:30 2010 +0100

    perf_events, x86: Fixup fixed counter constraints
    
    Patch 1da53e0230 ("perf_events, x86: Improve x86 event scheduling")
    lost us one of the fixed purpose counters and then ed8777fc13
    ("perf_events, x86: Fix event constraint masks") broke it even
    further.
    
    Widen the fixed event mask to event+umask and specify the full config
    for each of the 3 fixed purpose counters. Then let the init code fill
    out the placement for the GP regs based on the cpuid info.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 80e693684f18..db6109a885a7 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -50,7 +50,7 @@
 	 INTEL_ARCH_INV_MASK| \
 	 INTEL_ARCH_EDGE_MASK|\
 	 INTEL_ARCH_UNIT_MASK|\
-	 INTEL_ARCH_EVTSEL_MASK)
+	 INTEL_ARCH_EVENT_MASK)
 
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		      0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)

commit bb1165d6882f423f90fc7007a88c6c993b7c2ac4
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Mar 1 14:21:23 2010 +0100

    perf, x86: rename macro in ARCH_PERFMON_EVENTSEL_ENABLE
    
    For consistency reasons this patch renames
    ARCH_PERFMON_EVENTSEL0_ENABLE to ARCH_PERFMON_EVENTSEL_ENABLE.
    
    The following is performed:
    
     $ sed -i -e s/ARCH_PERFMON_EVENTSEL0_ENABLE/ARCH_PERFMON_EVENTSEL_ENABLE/g \
       arch/x86/include/asm/perf_event.h arch/x86/kernel/cpu/perf_event.c \
       arch/x86/kernel/cpu/perf_event_p6.c \
       arch/x86/kernel/cpu/perfctr-watchdog.c \
       arch/x86/oprofile/op_model_amd.c arch/x86/oprofile/op_model_ppro.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index c7f60e1297ab..80e693684f18 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -18,7 +18,7 @@
 #define MSR_ARCH_PERFMON_EVENTSEL0			     0x186
 #define MSR_ARCH_PERFMON_EVENTSEL1			     0x187
 
-#define ARCH_PERFMON_EVENTSEL0_ENABLE			  (1 << 22)
+#define ARCH_PERFMON_EVENTSEL_ENABLE			  (1 << 22)
 #define ARCH_PERFMON_EVENTSEL_ANY			  (1 << 21)
 #define ARCH_PERFMON_EVENTSEL_INT			  (1 << 20)
 #define ARCH_PERFMON_EVENTSEL_OS			  (1 << 17)

commit a163b1099dc7016704043c7fc572ae42519f08f7
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Feb 25 19:43:07 2010 +0100

    perf, x86: add some IBS macros to perf_event.h
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 4933ccde96c4..c7f60e1297ab 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -121,12 +121,14 @@ union cpuid10_edx {
 #define IBS_FETCH_RAND_EN		(1ULL<<57)
 #define IBS_FETCH_VAL			(1ULL<<49)
 #define IBS_FETCH_ENABLE		(1ULL<<48)
-#define IBS_FETCH_CNT_MASK		0xFFFF0000ULL
+#define IBS_FETCH_CNT			0xFFFF0000ULL
+#define IBS_FETCH_MAX_CNT		0x0000FFFFULL
 
 /* IbsOpCtl bits */
 #define IBS_OP_CNT_CTL			(1ULL<<19)
 #define IBS_OP_VAL			(1ULL<<18)
 #define IBS_OP_ENABLE			(1ULL<<17)
+#define IBS_OP_MAX_CNT			0x0000FFFFULL
 
 #ifdef CONFIG_PERF_EVENTS
 extern void init_hw_perf_events(void);

commit 1d6040f17d12a65b9f7ab4cb9fd6d721206b79ec
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Feb 25 19:40:46 2010 +0100

    perf, x86: make IBS macros available in perf_event.h
    
    This patch moves code from oprofile to perf_event.h to make it also
    available for usage by perf.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index befd172c82ad..4933ccde96c4 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -117,6 +117,16 @@ union cpuid10_edx {
  */
 #define X86_PMC_IDX_FIXED_BTS				(X86_PMC_IDX_FIXED + 16)
 
+/* IbsFetchCtl bits/masks */
+#define IBS_FETCH_RAND_EN		(1ULL<<57)
+#define IBS_FETCH_VAL			(1ULL<<49)
+#define IBS_FETCH_ENABLE		(1ULL<<48)
+#define IBS_FETCH_CNT_MASK		0xFFFF0000ULL
+
+/* IbsOpCtl bits */
+#define IBS_OP_CNT_CTL			(1ULL<<19)
+#define IBS_OP_VAL			(1ULL<<18)
+#define IBS_OP_ENABLE			(1ULL<<17)
 
 #ifdef CONFIG_PERF_EVENTS
 extern void init_hw_perf_events(void);

commit ae7f6711d6231c9ba54feb5ba9856c3775e482f8
Merge: 64abebf731df b23ff0e9330e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 29 09:24:57 2010 +0100

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: We want to queue up a dependent patch. Also update to
                  later -rc's.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ed8777fc132e589d48a0ba854fdbb5d8203b58e5
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jan 27 23:07:46 2010 +0100

    perf_events, x86: Fix event constraint masks
    
    Since constraints are specified on the event number, not number
    and unit mask shorten the constraint masks so that we'll
    actually match something.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    LKML-Reference: <20100127221121.967610372@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index dbc082685d52..ff5ede128bae 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -49,7 +49,7 @@
 	 INTEL_ARCH_INV_MASK| \
 	 INTEL_ARCH_EDGE_MASK|\
 	 INTEL_ARCH_UNIT_MASK|\
-	 INTEL_ARCH_EVENT_MASK)
+	 INTEL_ARCH_EVTSEL_MASK)
 
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		      0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)

commit 1da53e023029c067ba1277a33038c65d6e4c99b3
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Jan 18 10:58:01 2010 +0200

    perf_events, x86: Improve x86 event scheduling
    
    This patch improves event scheduling by maximizing the use of PMU
    registers regardless of the order in which events are created in a group.
    
    The algorithm takes into account the list of counter constraints for each
    event. It assigns events to counters from the most constrained, i.e.,
    works on only one counter, to the least constrained, i.e., works on any
    counter.
    
    Intel Fixed counter events and the BTS special event are also handled via
    this algorithm which is designed to be fairly generic.
    
    The patch also updates the validation of an event to use the scheduling
    algorithm. This will cause early failure in perf_event_open().
    
    The 2nd version of this patch follows the model used by PPC, by running
    the scheduling algorithm and the actual assignment separately. Actual
    assignment takes place in hw_perf_enable() whereas scheduling is
    implemented in hw_perf_group_sched_in() and x86_pmu_enable().
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    [ fixup whitespace and style nits as well as adding is_x86_event() ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4b5430c6.0f975e0a.1bf9.ffff85fe@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 8d9f8548a870..dbc082685d52 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -26,7 +26,14 @@
 /*
  * Includes eventsel and unit mask as well:
  */
-#define ARCH_PERFMON_EVENT_MASK				    0xffff
+
+
+#define INTEL_ARCH_EVTSEL_MASK		0x000000FFULL
+#define INTEL_ARCH_UNIT_MASK		0x0000FF00ULL
+#define INTEL_ARCH_EDGE_MASK		0x00040000ULL
+#define INTEL_ARCH_INV_MASK		0x00800000ULL
+#define INTEL_ARCH_CNT_MASK		0xFF000000ULL
+#define INTEL_ARCH_EVENT_MASK	(INTEL_ARCH_UNIT_MASK|INTEL_ARCH_EVTSEL_MASK)
 
 /*
  * filter mask to validate fixed counter events.
@@ -37,7 +44,12 @@
  *  The other filters are supported by fixed counters.
  *  The any-thread option is supported starting with v3.
  */
-#define ARCH_PERFMON_EVENT_FILTER_MASK			0xff840000
+#define INTEL_ARCH_FIXED_MASK \
+	(INTEL_ARCH_CNT_MASK| \
+	 INTEL_ARCH_INV_MASK| \
+	 INTEL_ARCH_EDGE_MASK|\
+	 INTEL_ARCH_UNIT_MASK|\
+	 INTEL_ARCH_EVENT_MASK)
 
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		      0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)

commit b27d515a49169e5e2a92d621faac761074a8c5b1
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Jan 18 10:58:01 2010 +0200

    perf: x86: Add support for the ANY bit
    
    Propagate the ANY bit into the fixed counter config for v3 and higher.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    [a.p.zijlstra@chello.nl: split from larger patch]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4b5430c6.0f975e0a.1bf9.ffff85fe@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 8d9f8548a870..1380367dabd9 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -19,6 +19,7 @@
 #define MSR_ARCH_PERFMON_EVENTSEL1			     0x187
 
 #define ARCH_PERFMON_EVENTSEL0_ENABLE			  (1 << 22)
+#define ARCH_PERFMON_EVENTSEL_ANY			  (1 << 21)
 #define ARCH_PERFMON_EVENTSEL_INT			  (1 << 20)
 #define ARCH_PERFMON_EVENTSEL_OS			  (1 << 17)
 #define ARCH_PERFMON_EVENTSEL_USR			  (1 << 16)

commit 04a705df47d1ea27ca2b066f24b1951c51792d0d
Author: Stephane Eranian <eranian@googlemail.com>
Date:   Tue Oct 6 16:42:08 2009 +0200

    perf_events: Check for filters on fixed counter events
    
    Intel fixed counters do not support all the filters possible with a
    generic counter. Thus, if a fixed counter event is passed but with
    certain filters set, then the fixed_mode_idx() function must fail
    and the event must be measured in a generic counter instead.
    
    Reject filters are: inv, edge, cnt-mask.
    
    Signed-off-by: Stephane Eranian <eranian@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1254840129-6198-2-git-send-email-eranian@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index ad7ce3fd5065..8d9f8548a870 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -28,9 +28,20 @@
  */
 #define ARCH_PERFMON_EVENT_MASK				    0xffff
 
+/*
+ * filter mask to validate fixed counter events.
+ * the following filters disqualify for fixed counters:
+ *  - inv
+ *  - edge
+ *  - cnt-mask
+ *  The other filters are supported by fixed counters.
+ *  The any-thread option is supported starting with v3.
+ */
+#define ARCH_PERFMON_EVENT_FILTER_MASK			0xff840000
+
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		      0x3c
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)
-#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX 		 0
+#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX			 0
 #define ARCH_PERFMON_UNHALTED_CORE_CYCLES_PRESENT \
 		(1 << (ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX))
 

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
new file mode 100644
index 000000000000..ad7ce3fd5065
--- /dev/null
+++ b/arch/x86/include/asm/perf_event.h
@@ -0,0 +1,108 @@
+#ifndef _ASM_X86_PERF_EVENT_H
+#define _ASM_X86_PERF_EVENT_H
+
+/*
+ * Performance event hw details:
+ */
+
+#define X86_PMC_MAX_GENERIC					8
+#define X86_PMC_MAX_FIXED					3
+
+#define X86_PMC_IDX_GENERIC				        0
+#define X86_PMC_IDX_FIXED				       32
+#define X86_PMC_IDX_MAX					       64
+
+#define MSR_ARCH_PERFMON_PERFCTR0			      0xc1
+#define MSR_ARCH_PERFMON_PERFCTR1			      0xc2
+
+#define MSR_ARCH_PERFMON_EVENTSEL0			     0x186
+#define MSR_ARCH_PERFMON_EVENTSEL1			     0x187
+
+#define ARCH_PERFMON_EVENTSEL0_ENABLE			  (1 << 22)
+#define ARCH_PERFMON_EVENTSEL_INT			  (1 << 20)
+#define ARCH_PERFMON_EVENTSEL_OS			  (1 << 17)
+#define ARCH_PERFMON_EVENTSEL_USR			  (1 << 16)
+
+/*
+ * Includes eventsel and unit mask as well:
+ */
+#define ARCH_PERFMON_EVENT_MASK				    0xffff
+
+#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL		      0x3c
+#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK		(0x00 << 8)
+#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX 		 0
+#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_PRESENT \
+		(1 << (ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX))
+
+#define ARCH_PERFMON_BRANCH_MISSES_RETIRED			 6
+
+/*
+ * Intel "Architectural Performance Monitoring" CPUID
+ * detection/enumeration details:
+ */
+union cpuid10_eax {
+	struct {
+		unsigned int version_id:8;
+		unsigned int num_events:8;
+		unsigned int bit_width:8;
+		unsigned int mask_length:8;
+	} split;
+	unsigned int full;
+};
+
+union cpuid10_edx {
+	struct {
+		unsigned int num_events_fixed:4;
+		unsigned int reserved:28;
+	} split;
+	unsigned int full;
+};
+
+
+/*
+ * Fixed-purpose performance events:
+ */
+
+/*
+ * All 3 fixed-mode PMCs are configured via this single MSR:
+ */
+#define MSR_ARCH_PERFMON_FIXED_CTR_CTRL			0x38d
+
+/*
+ * The counts are available in three separate MSRs:
+ */
+
+/* Instr_Retired.Any: */
+#define MSR_ARCH_PERFMON_FIXED_CTR0			0x309
+#define X86_PMC_IDX_FIXED_INSTRUCTIONS			(X86_PMC_IDX_FIXED + 0)
+
+/* CPU_CLK_Unhalted.Core: */
+#define MSR_ARCH_PERFMON_FIXED_CTR1			0x30a
+#define X86_PMC_IDX_FIXED_CPU_CYCLES			(X86_PMC_IDX_FIXED + 1)
+
+/* CPU_CLK_Unhalted.Ref: */
+#define MSR_ARCH_PERFMON_FIXED_CTR2			0x30b
+#define X86_PMC_IDX_FIXED_BUS_CYCLES			(X86_PMC_IDX_FIXED + 2)
+
+/*
+ * We model BTS tracing as another fixed-mode PMC.
+ *
+ * We choose a value in the middle of the fixed event range, since lower
+ * values are used by actual fixed events and higher values are used
+ * to indicate other overflow conditions in the PERF_GLOBAL_STATUS msr.
+ */
+#define X86_PMC_IDX_FIXED_BTS				(X86_PMC_IDX_FIXED + 16)
+
+
+#ifdef CONFIG_PERF_EVENTS
+extern void init_hw_perf_events(void);
+extern void perf_events_lapic_init(void);
+
+#define PERF_EVENT_INDEX_OFFSET			0
+
+#else
+static inline void init_hw_perf_events(void)		{ }
+static inline void perf_events_lapic_init(void)	{ }
+#endif
+
+#endif /* _ASM_X86_PERF_EVENT_H */
