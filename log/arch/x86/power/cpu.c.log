commit 5d5103595e9e53048bb7e70ee2673c897ab38300
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Jun 8 10:41:34 2020 -0700

    x86/cpu: Reinitialize IA32_FEAT_CTL MSR on BSP during wakeup
    
    Reinitialize IA32_FEAT_CTL on the BSP during wakeup to handle the case
    where firmware doesn't initialize or save/restore across S3.  This fixes
    a bug where IA32_FEAT_CTL is left uninitialized and results in VMXON
    taking a #GP due to VMX not being fully enabled, i.e. breaks KVM.
    
    Use init_ia32_feat_ctl() to "restore" IA32_FEAT_CTL as it already deals
    with the case where the MSR is locked, and because APs already redo
    init_ia32_feat_ctl() during suspend by virtue of the SMP boot flow being
    used to reinitialize APs upon wakeup.  Do the call in the early wakeup
    flow to avoid dependencies in the syscore_ops chain, e.g. simply adding
    a resume hook is not guaranteed to work, as KVM does VMXON in its own
    resume hook, kvm_resume(), when KVM has active guests.
    
    Fixes: 21bd3467a58e ("KVM: VMX: Drop initialization of IA32_FEAT_CTL MSR")
    Reported-by: Brad Campbell <lists2009@fnarfbargle.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Liam Merwick <liam.merwick@oracle.com>
    Reviewed-by: Maxim Levitsky <mlevitsk@redhat.com>
    Tested-by: Brad Campbell <lists2009@fnarfbargle.com>
    Cc: stable@vger.kernel.org # v5.6
    Link: https://lkml.kernel.org/r/20200608174134.11157-1-sean.j.christopherson@intel.com

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 7c65102debaf..db1378c6ff26 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -193,6 +193,8 @@ static void fix_processor_context(void)
  */
 static void notrace __restore_processor_state(struct saved_context *ctxt)
 {
+	struct cpuinfo_x86 *c;
+
 	if (ctxt->misc_enable_saved)
 		wrmsrl(MSR_IA32_MISC_ENABLE, ctxt->misc_enable);
 	/*
@@ -263,6 +265,10 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	mtrr_bp_restore();
 	perf_restore_debug_store();
 	msr_restore_context(ctxt);
+
+	c = &cpu_data(smp_processor_id());
+	if (cpu_has(c, X86_FEATURE_MSR_IA32_FEAT_CTL))
+		init_ia32_feat_ctl(c);
 }
 
 /* Needed by apm.c */

commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 9559080695bf..7c65102debaf 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -13,8 +13,8 @@
 #include <linux/perf_event.h>
 #include <linux/tboot.h>
 #include <linux/dmi.h>
-
 #include <linux/pgtable.h>
+
 #include <asm/proto.h>
 #include <asm/mtrr.h>
 #include <asm/page.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index fc3b757afb2c..9559080695bf 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -14,7 +14,7 @@
 #include <linux/tboot.h>
 #include <linux/dmi.h>
 
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/proto.h>
 #include <asm/mtrr.h>
 #include <asm/page.h>

commit 565558558985b1d7cd43b21f18c1ad6b232788d0
Author: Qais Yousef <qais.yousef@arm.com>
Date:   Thu Apr 30 12:40:03 2020 +0100

    cpu/hotplug: Remove disable_nonboot_cpus()
    
    The single user could have called freeze_secondary_cpus() directly.
    
    Since this function was a source of confusion, remove it as it's
    just a pointless wrapper.
    
    While at it, rename enable_nonboot_cpus() to thaw_secondary_cpus() to
    preserve the naming symmetry.
    
    Done automatically via:
    
            git grep -l enable_nonboot_cpus | xargs sed -i 's/enable_nonboot_cpus/thaw_secondary_cpus/g'
    
    Signed-off-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Link: https://lkml.kernel.org/r/20200430114004.17477-1-qais.yousef@arm.com

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index aaff9ed7ff45..fc3b757afb2c 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -307,7 +307,7 @@ int hibernate_resume_nonboot_cpu_disable(void)
 	if (ret)
 		return ret;
 	smp_ops.play_dead = resume_play_dead;
-	ret = disable_nonboot_cpus();
+	ret = freeze_secondary_cpus(0);
 	smp_ops.play_dead = play_dead;
 	return ret;
 }

commit adefe55e725821e8ae23207992ded5994f1650a9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 20 14:13:51 2020 +0100

    x86/kernel: Convert to new CPU match macros
    
    The new macro set has a consistent namespace and uses C99 initializers
    instead of the grufty C89 ones.
    
    Get rid the of the local macro wrappers for consistency.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Link: https://lkml.kernel.org/r/20200320131509.250559388@linutronix.de

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 915bb1639763..aaff9ed7ff45 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -475,20 +475,8 @@ static int msr_save_cpuid_features(const struct x86_cpu_id *c)
 }
 
 static const struct x86_cpu_id msr_save_cpu_table[] = {
-	{
-		.vendor = X86_VENDOR_AMD,
-		.family = 0x15,
-		.model = X86_MODEL_ANY,
-		.feature = X86_FEATURE_ANY,
-		.driver_data = (kernel_ulong_t)msr_save_cpuid_features,
-	},
-	{
-		.vendor = X86_VENDOR_AMD,
-		.family = 0x16,
-		.model = X86_MODEL_ANY,
-		.feature = X86_FEATURE_ANY,
-		.driver_data = (kernel_ulong_t)msr_save_cpuid_features,
-	},
+	X86_MATCH_VENDOR_FAM(AMD, 0x15, &msr_save_cpuid_features),
+	X86_MATCH_VENDOR_FAM(AMD, 0x16, &msr_save_cpuid_features),
 	{}
 };
 

commit c5f12fdb8bd873aa3ffdb79512e6bdac92b257b0
Merge: a572ba63298d 743dac494d61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 12:04:39 2019 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic updates from Thomas Gleixner:
    
     - Cleanup the apic IPI implementation by removing duplicated code and
       consolidating the functions into the APIC core.
    
     - Implement a safe variant of the IPI broadcast mode. Contrary to
       earlier attempts this uses the core tracking of which CPUs have been
       brought online at least once so that a broadcast does not end up in
       some dead end in BIOS/SMM code when the CPU is still waiting for
       init. Once all CPUs have been brought up once, IPI broadcasting is
       enabled. Before that regular one by one IPIs are issued.
    
     - Drop the paravirt CR8 related functions as they have no user anymore
    
     - Initialize the APIC TPR to block interrupt 16-31 as they are reserved
       for CPU exceptions and should never be raised by any well behaving
       device.
    
     - Emit a warning when vector space exhaustion breaks the admin set
       affinity of an interrupt.
    
     - Make sure to use the NMI fallback when shutdown via reboot vector IPI
       fails. The original code had conditions which prevent the code path
       to be reached.
    
     - Annotate various APIC config variables as RO after init.
    
    [ The ipi broadcase change came in earlier through the cpu hotplug
      branch, but I left the explanation in the commit message since it was
      shared between the two different branches    - Linus ]
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (28 commits)
      x86/apic/vector: Warn when vector space exhaustion breaks affinity
      x86/apic: Annotate global config variables as "read-only after init"
      x86/apic/x2apic: Implement IPI shorthands support
      x86/apic/flat64: Remove the IPI shorthand decision logic
      x86/apic: Share common IPI helpers
      x86/apic: Remove the shorthand decision logic
      x86/smp: Enhance native_send_call_func_ipi()
      x86/smp: Move smp_function_call implementations into IPI code
      x86/apic: Provide and use helper for send_IPI_allbutself()
      x86/apic: Add static key to Control IPI shorthands
      x86/apic: Move no_ipi_broadcast() out of 32bit
      x86/apic: Add NMI_VECTOR wait to IPI shorthand
      x86/apic: Remove dest argument from __default_send_IPI_shortcut()
      x86/hotplug: Silence APIC and NMI when CPU is dead
      x86/cpu: Move arch_smt_update() to a neutral place
      x86/apic/uv: Make x2apic_extra_bits static
      x86/apic: Consolidate the apic local headers
      x86/apic: Move apic_flat_64 header into apic directory
      x86/apic: Move ipi header into apic directory
      x86/apic: Cleanup the include maze
      ...

commit c49a0a80137c7ca7d6ced4c812c9e07a949f6f24
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Aug 19 15:52:35 2019 +0000

    x86/CPU/AMD: Clear RDRAND CPUID bit on AMD family 15h/16h
    
    There have been reports of RDRAND issues after resuming from suspend on
    some AMD family 15h and family 16h systems. This issue stems from a BIOS
    not performing the proper steps during resume to ensure RDRAND continues
    to function properly.
    
    RDRAND support is indicated by CPUID Fn00000001_ECX[30]. This bit can be
    reset by clearing MSR C001_1004[62]. Any software that checks for RDRAND
    support using CPUID, including the kernel, will believe that RDRAND is
    not supported.
    
    Update the CPU initialization to clear the RDRAND CPUID bit for any family
    15h and 16h processor that supports RDRAND. If it is known that the family
    15h or family 16h system does not have an RDRAND resume issue or that the
    system will not be placed in suspend, the "rdrand=force" kernel parameter
    can be used to stop the clearing of the RDRAND CPUID bit.
    
    Additionally, update the suspend and resume path to save and restore the
    MSR C001_1004 value to ensure that the RDRAND CPUID setting remains in
    place after resuming from suspend.
    
    Note, that clearing the RDRAND CPUID bit does not prevent a processor
    that normally supports the RDRAND instruction from executing it. So any
    code that determined the support based on family and model won't #UD.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chen Yu <yu.c.chen@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: "linux-doc@vger.kernel.org" <linux-doc@vger.kernel.org>
    Cc: "linux-pm@vger.kernel.org" <linux-pm@vger.kernel.org>
    Cc: Nathan Chancellor <natechancellor@gmail.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: <stable@vger.kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "x86@kernel.org" <x86@kernel.org>
    Link: https://lkml.kernel.org/r/7543af91666f491547bd86cebb1e17c66824ab9f.1566229943.git.thomas.lendacky@amd.com

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 24b079e94bc2..c9ef6a7a4a1a 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -12,6 +12,7 @@
 #include <linux/smp.h>
 #include <linux/perf_event.h>
 #include <linux/tboot.h>
+#include <linux/dmi.h>
 
 #include <asm/pgtable.h>
 #include <asm/proto.h>
@@ -23,7 +24,7 @@
 #include <asm/debugreg.h>
 #include <asm/cpu.h>
 #include <asm/mmu_context.h>
-#include <linux/dmi.h>
+#include <asm/cpu_device_id.h>
 
 #ifdef CONFIG_X86_32
 __visible unsigned long saved_context_ebx;
@@ -397,15 +398,14 @@ static int __init bsp_pm_check_init(void)
 
 core_initcall(bsp_pm_check_init);
 
-static int msr_init_context(const u32 *msr_id, const int total_num)
+static int msr_build_context(const u32 *msr_id, const int num)
 {
-	int i = 0;
+	struct saved_msrs *saved_msrs = &saved_context.saved_msrs;
 	struct saved_msr *msr_array;
+	int total_num;
+	int i, j;
 
-	if (saved_context.saved_msrs.array || saved_context.saved_msrs.num > 0) {
-		pr_err("x86/pm: MSR quirk already applied, please check your DMI match table.\n");
-		return -EINVAL;
-	}
+	total_num = saved_msrs->num + num;
 
 	msr_array = kmalloc_array(total_num, sizeof(struct saved_msr), GFP_KERNEL);
 	if (!msr_array) {
@@ -413,19 +413,30 @@ static int msr_init_context(const u32 *msr_id, const int total_num)
 		return -ENOMEM;
 	}
 
-	for (i = 0; i < total_num; i++) {
-		msr_array[i].info.msr_no	= msr_id[i];
+	if (saved_msrs->array) {
+		/*
+		 * Multiple callbacks can invoke this function, so copy any
+		 * MSR save requests from previous invocations.
+		 */
+		memcpy(msr_array, saved_msrs->array,
+		       sizeof(struct saved_msr) * saved_msrs->num);
+
+		kfree(saved_msrs->array);
+	}
+
+	for (i = saved_msrs->num, j = 0; i < total_num; i++, j++) {
+		msr_array[i].info.msr_no	= msr_id[j];
 		msr_array[i].valid		= false;
 		msr_array[i].info.reg.q		= 0;
 	}
-	saved_context.saved_msrs.num	= total_num;
-	saved_context.saved_msrs.array	= msr_array;
+	saved_msrs->num   = total_num;
+	saved_msrs->array = msr_array;
 
 	return 0;
 }
 
 /*
- * The following section is a quirk framework for problematic BIOSen:
+ * The following sections are a quirk framework for problematic BIOSen:
  * Sometimes MSRs are modified by the BIOSen after suspended to
  * RAM, this might cause unexpected behavior after wakeup.
  * Thus we save/restore these specified MSRs across suspend/resume
@@ -440,7 +451,7 @@ static int msr_initialize_bdw(const struct dmi_system_id *d)
 	u32 bdw_msr_id[] = { MSR_IA32_THERM_CONTROL };
 
 	pr_info("x86/pm: %s detected, MSR saving is needed during suspending.\n", d->ident);
-	return msr_init_context(bdw_msr_id, ARRAY_SIZE(bdw_msr_id));
+	return msr_build_context(bdw_msr_id, ARRAY_SIZE(bdw_msr_id));
 }
 
 static const struct dmi_system_id msr_save_dmi_table[] = {
@@ -455,9 +466,58 @@ static const struct dmi_system_id msr_save_dmi_table[] = {
 	{}
 };
 
+static int msr_save_cpuid_features(const struct x86_cpu_id *c)
+{
+	u32 cpuid_msr_id[] = {
+		MSR_AMD64_CPUID_FN_1,
+	};
+
+	pr_info("x86/pm: family %#hx cpu detected, MSR saving is needed during suspending.\n",
+		c->family);
+
+	return msr_build_context(cpuid_msr_id, ARRAY_SIZE(cpuid_msr_id));
+}
+
+static const struct x86_cpu_id msr_save_cpu_table[] = {
+	{
+		.vendor = X86_VENDOR_AMD,
+		.family = 0x15,
+		.model = X86_MODEL_ANY,
+		.feature = X86_FEATURE_ANY,
+		.driver_data = (kernel_ulong_t)msr_save_cpuid_features,
+	},
+	{
+		.vendor = X86_VENDOR_AMD,
+		.family = 0x16,
+		.model = X86_MODEL_ANY,
+		.feature = X86_FEATURE_ANY,
+		.driver_data = (kernel_ulong_t)msr_save_cpuid_features,
+	},
+	{}
+};
+
+typedef int (*pm_cpu_match_t)(const struct x86_cpu_id *);
+static int pm_cpu_check(const struct x86_cpu_id *c)
+{
+	const struct x86_cpu_id *m;
+	int ret = 0;
+
+	m = x86_match_cpu(msr_save_cpu_table);
+	if (m) {
+		pm_cpu_match_t fn;
+
+		fn = (pm_cpu_match_t)m->driver_data;
+		ret = fn(m);
+	}
+
+	return ret;
+}
+
 static int pm_check_save_msr(void)
 {
 	dmi_check_system(msr_save_dmi_table);
+	pm_cpu_check(msr_save_cpu_table);
+
 	return 0;
 }
 

commit 83b584d9c6a1494170abd3a8b24f41939b23d625
Author: Andrew Cooper <andrew.cooper3@citrix.com>
Date:   Mon Jul 15 16:16:41 2019 +0100

    x86/paravirt: Drop {read,write}_cr8() hooks
    
    There is a lot of infrastructure for functionality which is used
    exclusively in __{save,restore}_processor_state() on the suspend/resume
    path.
    
    cr8 is an alias of APIC_TASKPRI, and APIC_TASKPRI is saved/restored by
    lapic_{suspend,resume}().  Saving and restoring cr8 independently of the
    rest of the Local APIC state isn't a clever thing to be doing.
    
    Delete the suspend/resume cr8 handling, which shrinks the size of struct
    saved_context, and allows for the removal of both PVOPS.
    
    Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Link: https://lkml.kernel.org/r/20190715151641.29210-1-andrew.cooper3@citrix.com

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 24b079e94bc2..1c58d8982728 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -122,9 +122,6 @@ static void __save_processor_state(struct saved_context *ctxt)
 	ctxt->cr2 = read_cr2();
 	ctxt->cr3 = __read_cr3();
 	ctxt->cr4 = __read_cr4();
-#ifdef CONFIG_X86_64
-	ctxt->cr8 = read_cr8();
-#endif
 	ctxt->misc_enable_saved = !rdmsrl_safe(MSR_IA32_MISC_ENABLE,
 					       &ctxt->misc_enable);
 	msr_save_context(ctxt);
@@ -207,7 +204,6 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 #else
 /* CONFIG X86_64 */
 	wrmsrl(MSR_EFER, ctxt->efer);
-	write_cr8(ctxt->cr8);
 	__write_cr4(ctxt->cr4);
 #endif
 	write_cr3(ctxt->cr3);

commit 9331b6740f86163908de69f4008e434fe0c27691
Merge: 1ce2c85137b1 d925da5c7b09
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 8 12:52:42 2019 -0700

    Merge tag 'spdx-5.2-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core
    
    Pull yet more SPDX updates from Greg KH:
     "Another round of SPDX header file fixes for 5.2-rc4
    
      These are all more "GPL-2.0-or-later" or "GPL-2.0-only" tags being
      added, based on the text in the files. We are slowly chipping away at
      the 700+ different ways people tried to write the license text. All of
      these were reviewed on the spdx mailing list by a number of different
      people.
    
      We now have over 60% of the kernel files covered with SPDX tags:
            $ ./scripts/spdxcheck.py -v 2>&1 | grep Files
            Files checked:            64533
            Files with SPDX:          40392
            Files with errors:            0
    
      I think the majority of the "easy" fixups are now done, it's now the
      start of the longer-tail of crazy variants to wade through"
    
    * tag 'spdx-5.2-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core: (159 commits)
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 450
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 449
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 448
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 446
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 445
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 444
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 443
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 442
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 441
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 440
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 438
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 437
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 436
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 435
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 434
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 433
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 432
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 431
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 430
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 429
      ...

commit 767a67b0b35520348dc3b28dcba06454b0f9023d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 1 10:08:44 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 430
    
    Based on 1 normalized pattern(s):
    
      distribute under gplv2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 8 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190531190114.475576622@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index a7d966964c6f..0bc07a9bc27b 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -1,8 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Suspend support specific for i386/x86-64.
  *
- * Distribute under GPLv2
- *
  * Copyright (c) 2007 Rafael J. Wysocki <rjw@sisk.pl>
  * Copyright (c) 2002 Pavel Machek <pavel@ucw.cz>
  * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>

commit ec527c318036a65a083ef68d8ba95789d2212246
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Thu May 30 00:09:39 2019 +0200

    x86/power: Fix 'nosmt' vs hibernation triple fault during resume
    
    As explained in
    
            0cc3cd21657b ("cpu/hotplug: Boot HT siblings at least once")
    
    we always, no matter what, have to bring up x86 HT siblings during boot at
    least once in order to avoid first MCE bringing the system to its knees.
    
    That means that whenever 'nosmt' is supplied on the kernel command-line,
    all the HT siblings are as a result sitting in mwait or cpudile after
    going through the online-offline cycle at least once.
    
    This causes a serious issue though when a kernel, which saw 'nosmt' on its
    commandline, is going to perform resume from hibernation: if the resume
    from the hibernated image is successful, cr3 is flipped in order to point
    to the address space of the kernel that is being resumed, which in turn
    means that all the HT siblings are all of a sudden mwaiting on address
    which is no longer valid.
    
    That results in triple fault shortly after cr3 is switched, and machine
    reboots.
    
    Fix this by always waking up all the SMT siblings before initiating the
    'restore from hibernation' process; this guarantees that all the HT
    siblings will be properly carried over to the resumed kernel waiting in
    resume_play_dead(), and acted upon accordingly afterwards, based on the
    target kernel configuration.
    
    Symmetricaly, the resumed kernel has to push the SMT siblings to mwait
    again in case it has SMT disabled; this means it has to online all
    the siblings when resuming (so that they come out of hlt) and offline
    them again to let them reach mwait.
    
    Cc: 4.19+ <stable@vger.kernel.org> # v4.19+
    Debugged-by: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 0cc3cd21657b ("cpu/hotplug: Boot HT siblings at least once")
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index a7d966964c6f..513ce09e9950 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -299,7 +299,17 @@ int hibernate_resume_nonboot_cpu_disable(void)
 	 * address in its instruction pointer may not be possible to resolve
 	 * any more at that point (the page tables used by it previously may
 	 * have been overwritten by hibernate image data).
+	 *
+	 * First, make sure that we wake up all the potentially disabled SMT
+	 * threads which have been initially brought up and then put into
+	 * mwait/cpuidle sleep.
+	 * Those will be put to proper (not interfering with hibernation
+	 * resume) sleep afterwards, and the resumed kernel will decide itself
+	 * what to do with them.
 	 */
+	ret = cpuhp_smt_enable();
+	if (ret)
+		return ret;
 	smp_ops.play_dead = resume_play_dead;
 	ret = disable_nonboot_cpus();
 	smp_ops.play_dead = play_dead;

commit 64a48099b3b31568ac45716b7fafcb74a0c2fcfe
Merge: 1291a0d5049d 6cbd2171e89b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 18 08:59:15 2017 -0800

    Merge branch 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 syscall entry code changes for PTI from Ingo Molnar:
     "The main changes here are Andy Lutomirski's changes to switch the
      x86-64 entry code to use the 'per CPU entry trampoline stack'. This,
      besides helping fix KASLR leaks (the pending Page Table Isolation
      (PTI) work), also robustifies the x86 entry code"
    
    * 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      x86/cpufeatures: Make CPU bugs sticky
      x86/paravirt: Provide a way to check for hypervisors
      x86/paravirt: Dont patch flush_tlb_single
      x86/entry/64: Make cpu_entry_area.tss read-only
      x86/entry: Clean up the SYSENTER_stack code
      x86/entry/64: Remove the SYSENTER stack canary
      x86/entry/64: Move the IST stacks into struct cpu_entry_area
      x86/entry/64: Create a per-CPU SYSCALL entry trampoline
      x86/entry/64: Return to userspace from the trampoline stack
      x86/entry/64: Use a per-CPU trampoline stack for IDT entries
      x86/espfix/64: Stop assuming that pt_regs is on the entry stack
      x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
      x86/entry: Remap the TSS into the CPU entry area
      x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct
      x86/dumpstack: Handle stack overflow on all stacks
      x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
      x86/kasan/64: Teach KASAN about the cpu_entry_area
      x86/mm/fixmap: Generalize the GDT fixmap mechanism, introduce struct cpu_entry_area
      x86/entry/gdt: Put per-CPU GDT remaps in ascending order
      x86/dumpstack: Add get_stack_info() support for the SYSENTER stack
      ...

commit 72f5e08dbba2d01aa90b592cf76c378ea233b00b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:20 2017 +0100

    x86/entry: Remap the TSS into the CPU entry area
    
    This has a secondary purpose: it puts the entry stack into a region
    with a well-controlled layout.  A subsequent patch will take
    advantage of this to streamline the SYSCALL entry code to be able to
    find it more easily.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.962042855@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 50593e138281..04d5157fe7f8 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -160,18 +160,19 @@ static void do_fpu_end(void)
 static void fix_processor_context(void)
 {
 	int cpu = smp_processor_id();
-	struct tss_struct *t = &per_cpu(cpu_tss, cpu);
 #ifdef CONFIG_X86_64
 	struct desc_struct *desc = get_cpu_gdt_rw(cpu);
 	tss_desc tss;
 #endif
 
 	/*
-	 * This just modifies memory; should not be necessary. But... This is
-	 * necessary, because 386 hardware has concept of busy TSS or some
-	 * similar stupidity.
+	 * We need to reload TR, which requires that we change the
+	 * GDT entry to indicate "available" first.
+	 *
+	 * XXX: This could probably all be replaced by a call to
+	 * force_reload_TR().
 	 */
-	set_tss_desc(cpu, &t->x86_tss);
+	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 
 #ifdef CONFIG_X86_64
 	memcpy(&tss, &desc[GDT_ENTRY_TSS], sizeof(tss_desc));

commit 7fb983b4dd569e08564134a850dfd4eb1c63d9b8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:17 2017 +0100

    x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
    
    A future patch will move SYSENTER_stack to the beginning of cpu_tss
    to help detect overflow.  Before this can happen, fix several code
    paths that hardcode assumptions about the old layout.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.722425540@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 84fcfde53f8f..50593e138281 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -165,12 +165,13 @@ static void fix_processor_context(void)
 	struct desc_struct *desc = get_cpu_gdt_rw(cpu);
 	tss_desc tss;
 #endif
-	set_tss_desc(cpu, t);	/*
-				 * This just modifies memory; should not be
-				 * necessary. But... This is necessary, because
-				 * 386 hardware has concept of busy TSS or some
-				 * similar stupidity.
-				 */
+
+	/*
+	 * This just modifies memory; should not be necessary. But... This is
+	 * necessary, because 386 hardware has concept of busy TSS or some
+	 * similar stupidity.
+	 */
+	set_tss_desc(cpu, &t->x86_tss);
 
 #ifdef CONFIG_X86_64
 	memcpy(&tss, &desc[GDT_ENTRY_TSS], sizeof(tss_desc));

commit 7ee18d677989e99635027cee04c878950e0752b9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Dec 14 13:19:07 2017 -0800

    x86/power: Make restore_processor_context() sane
    
    My previous attempt to fix a couple of bugs in __restore_processor_context():
    
      5b06bbcfc2c6 ("x86/power: Fix some ordering bugs in __restore_processor_context()")
    
    ... introduced yet another bug, breaking suspend-resume.
    
    Rather than trying to come up with a minimal fix, let's try to clean it up
    for real.  This patch fixes quite a few things:
    
     - The old code saved a nonsensical subset of segment registers.
       The only registers that need to be saved are those that contain
       userspace state or those that can't be trivially restored without
       percpu access working.  (On x86_32, we can restore percpu access
       by writing __KERNEL_PERCPU to %fs.  On x86_64, it's easier to
       save and restore the kernel's GSBASE.)  With this patch, we
       restore hardcoded values to the kernel state where applicable and
       explicitly restore the user state after fixing all the descriptor
       tables.
    
     - We used to use an unholy mix of inline asm and C helpers for
       segment register access.  Let's get rid of the inline asm.
    
    This fixes the reported s2ram hangs and make the code all around
    more logical.
    
    Analyzed-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Reported-by: Pavel Machek <pavel@ucw.cz>
    Tested-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Tested-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Zhang Rui <rui.zhang@intel.com>
    Fixes: 5b06bbcfc2c6 ("x86/power: Fix some ordering bugs in __restore_processor_context()")
    Link: http://lkml.kernel.org/r/398ee68e5c0f766425a7b746becfc810840770ff.1513286253.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 033c61e6891b..36a28eddb435 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -99,22 +99,18 @@ static void __save_processor_state(struct saved_context *ctxt)
 	/*
 	 * segment registers
 	 */
-#ifdef CONFIG_X86_32
-	savesegment(es, ctxt->es);
-	savesegment(fs, ctxt->fs);
+#ifdef CONFIG_X86_32_LAZY_GS
 	savesegment(gs, ctxt->gs);
-	savesegment(ss, ctxt->ss);
-#else
-/* CONFIG_X86_64 */
-	asm volatile ("movw %%ds, %0" : "=m" (ctxt->ds));
-	asm volatile ("movw %%es, %0" : "=m" (ctxt->es));
-	asm volatile ("movw %%fs, %0" : "=m" (ctxt->fs));
-	asm volatile ("movw %%gs, %0" : "=m" (ctxt->gs));
-	asm volatile ("movw %%ss, %0" : "=m" (ctxt->ss));
+#endif
+#ifdef CONFIG_X86_64
+	savesegment(gs, ctxt->gs);
+	savesegment(fs, ctxt->fs);
+	savesegment(ds, ctxt->ds);
+	savesegment(es, ctxt->es);
 
 	rdmsrl(MSR_FS_BASE, ctxt->fs_base);
-	rdmsrl(MSR_GS_BASE, ctxt->gs_base);
-	rdmsrl(MSR_KERNEL_GS_BASE, ctxt->gs_kernel_base);
+	rdmsrl(MSR_GS_BASE, ctxt->kernelmode_gs_base);
+	rdmsrl(MSR_KERNEL_GS_BASE, ctxt->usermode_gs_base);
 	mtrr_save_fixed_ranges(NULL);
 
 	rdmsrl(MSR_EFER, ctxt->efer);
@@ -189,9 +185,12 @@ static void fix_processor_context(void)
 }
 
 /**
- *	__restore_processor_state - restore the contents of CPU registers saved
- *		by __save_processor_state()
- *	@ctxt - structure to load the registers contents from
+ * __restore_processor_state - restore the contents of CPU registers saved
+ *                             by __save_processor_state()
+ * @ctxt - structure to load the registers contents from
+ *
+ * The asm code that gets us here will have restored a usable GDT, although
+ * it will be pointing to the wrong alias.
  */
 static void notrace __restore_processor_state(struct saved_context *ctxt)
 {
@@ -214,46 +213,50 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	write_cr2(ctxt->cr2);
 	write_cr0(ctxt->cr0);
 
+	/* Restore the IDT. */
+	load_idt(&ctxt->idt);
+
 	/*
-	 * now restore the descriptor tables to their proper values
-	 * ltr is done i fix_processor_context().
+	 * Just in case the asm code got us here with the SS, DS, or ES
+	 * out of sync with the GDT, update them.
 	 */
-	load_idt(&ctxt->idt);
+	loadsegment(ss, __KERNEL_DS);
+	loadsegment(ds, __USER_DS);
+	loadsegment(es, __USER_DS);
 
-#ifdef CONFIG_X86_64
 	/*
-	 * We need GSBASE restored before percpu access can work.
-	 * percpu access can happen in exception handlers or in complicated
-	 * helpers like load_gs_index().
+	 * Restore percpu access.  Percpu access can happen in exception
+	 * handlers or in complicated helpers like load_gs_index().
 	 */
-	wrmsrl(MSR_GS_BASE, ctxt->gs_base);
+#ifdef CONFIG_X86_64
+	wrmsrl(MSR_GS_BASE, ctxt->kernelmode_gs_base);
+#else
+	loadsegment(fs, __KERNEL_PERCPU);
+	loadsegment(gs, __KERNEL_STACK_CANARY);
 #endif
 
+	/* Restore the TSS, RO GDT, LDT, and usermode-relevant MSRs. */
 	fix_processor_context();
 
 	/*
-	 * Restore segment registers.  This happens after restoring the GDT
-	 * and LDT, which happen in fix_processor_context().
+	 * Now that we have descriptor tables fully restored and working
+	 * exception handling, restore the usermode segments.
 	 */
-#ifdef CONFIG_X86_32
+#ifdef CONFIG_X86_64
+	loadsegment(ds, ctxt->es);
 	loadsegment(es, ctxt->es);
 	loadsegment(fs, ctxt->fs);
-	loadsegment(gs, ctxt->gs);
-	loadsegment(ss, ctxt->ss);
-#else
-/* CONFIG_X86_64 */
-	asm volatile ("movw %0, %%ds" :: "r" (ctxt->ds));
-	asm volatile ("movw %0, %%es" :: "r" (ctxt->es));
-	asm volatile ("movw %0, %%fs" :: "r" (ctxt->fs));
 	load_gs_index(ctxt->gs);
-	asm volatile ("movw %0, %%ss" :: "r" (ctxt->ss));
 
 	/*
-	 * Restore FSBASE and user GSBASE after reloading the respective
-	 * segment selectors.
+	 * Restore FSBASE and GSBASE after restoring the selectors, since
+	 * restoring the selectors clobbers the bases.  Keep in mind
+	 * that MSR_KERNEL_GS_BASE is horribly misnamed.
 	 */
 	wrmsrl(MSR_FS_BASE, ctxt->fs_base);
-	wrmsrl(MSR_KERNEL_GS_BASE, ctxt->gs_kernel_base);
+	wrmsrl(MSR_KERNEL_GS_BASE, ctxt->usermode_gs_base);
+#elif defined(CONFIG_X86_32_LAZY_GS)
+	loadsegment(gs, ctxt->gs);
 #endif
 
 	do_fpu_end();

commit 896c80bef4d3b357814a476663158aaf669d0fb3
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Dec 14 13:19:06 2017 -0800

    x86/power/32: Move SYSENTER MSR restoration to fix_processor_context()
    
    x86_64 restores system call MSRs in fix_processor_context(), and
    x86_32 restored them along with segment registers.  The 64-bit
    variant makes more sense, so move the 32-bit code to match the
    64-bit code.
    
    No side effects are expected to runtime behavior.
    
    Tested-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Zhang Rui <rui.zhang@intel.com>
    Link: http://lkml.kernel.org/r/65158f8d7ee64dd6bbc6c1c83b3b34aaa854e3ae.1513286253.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 472bc8c8212b..033c61e6891b 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -174,6 +174,9 @@ static void fix_processor_context(void)
 	write_gdt_entry(desc, GDT_ENTRY_TSS, &tss, DESC_TSS);
 
 	syscall_init();				/* This sets MSR_*STAR and related */
+#else
+	if (boot_cpu_has(X86_FEATURE_SEP))
+		enable_sep_cpu();
 #endif
 	load_TR_desc();				/* This does ltr */
 	load_mm_ldt(current->active_mm);	/* This does lldt */
@@ -237,12 +240,6 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	loadsegment(fs, ctxt->fs);
 	loadsegment(gs, ctxt->gs);
 	loadsegment(ss, ctxt->ss);
-
-	/*
-	 * sysenter MSRs
-	 */
-	if (boot_cpu_has(X86_FEATURE_SEP))
-		enable_sep_cpu();
 #else
 /* CONFIG_X86_64 */
 	asm volatile ("movw %0, %%ds" :: "r" (ctxt->ds));

commit 090edbe23ff57940fca7f57d9165ce57a826bd7a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Dec 14 13:19:05 2017 -0800

    x86/power/64: Use struct desc_ptr for the IDT in struct saved_context
    
    x86_64's saved_context nonsensically used separate idt_limit and
    idt_base fields and then cast &idt_limit to struct desc_ptr *.
    
    This was correct (with -fno-strict-aliasing), but it's confusing,
    served no purpose, and required #ifdeffery. Simplify this by
    using struct desc_ptr directly.
    
    No change in functionality.
    
    Tested-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Zhang Rui <rui.zhang@intel.com>
    Link: http://lkml.kernel.org/r/967909ce38d341b01d45eff53e278e2728a3a93a.1513286253.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 5191de14f4df..472bc8c8212b 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -82,12 +82,8 @@ static void __save_processor_state(struct saved_context *ctxt)
 	/*
 	 * descriptor tables
 	 */
-#ifdef CONFIG_X86_32
 	store_idt(&ctxt->idt);
-#else
-/* CONFIG_X86_64 */
-	store_idt((struct desc_ptr *)&ctxt->idt_limit);
-#endif
+
 	/*
 	 * We save it here, but restore it only in the hibernate case.
 	 * For ACPI S3 resume, this is loaded via 'early_gdt_desc' in 64-bit
@@ -219,12 +215,7 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	 * now restore the descriptor tables to their proper values
 	 * ltr is done i fix_processor_context().
 	 */
-#ifdef CONFIG_X86_32
 	load_idt(&ctxt->idt);
-#else
-/* CONFIG_X86_64 */
-	load_idt((const struct desc_ptr *)&ctxt->idt_limit);
-#endif
 
 #ifdef CONFIG_X86_64
 	/*

commit 5b06bbcfc2c621da3009da8decb7511500c293ed
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 30 07:57:57 2017 -0800

    x86/power: Fix some ordering bugs in __restore_processor_context()
    
    __restore_processor_context() had a couple of ordering bugs.  It
    restored GSBASE after calling load_gs_index(), and the latter can
    call into tracing code.  It also tried to restore segment registers
    before restoring the LDT, which is straight-up wrong.
    
    Reorder the code so that we restore GSBASE, then the descriptor
    tables, then the segments.
    
    This fixes two bugs.  First, it fixes a regression that broke resume
    under certain configurations due to irqflag tracing in
    native_load_gs_index().  Second, it fixes resume when the userspace
    process that initiated suspect had funny segments.  The latter can be
    reproduced by compiling this:
    
    // SPDX-License-Identifier: GPL-2.0
    /*
     * ldt_echo.c - Echo argv[1] while using an LDT segment
     */
    
    int main(int argc, char **argv)
    {
            int ret;
            size_t len;
            char *buf;
    
            const struct user_desc desc = {
                    .entry_number    = 0,
                    .base_addr       = 0,
                    .limit           = 0xfffff,
                    .seg_32bit       = 1,
                    .contents        = 0, /* Data, grow-up */
                    .read_exec_only  = 0,
                    .limit_in_pages  = 1,
                    .seg_not_present = 0,
                    .useable         = 0
            };
    
            if (argc != 2)
                    errx(1, "Usage: %s STRING", argv[0]);
    
            len = asprintf(&buf, "%s\n", argv[1]);
            if (len < 0)
                    errx(1, "Out of memory");
    
            ret = syscall(SYS_modify_ldt, 1, &desc, sizeof(desc));
            if (ret < -1)
                    errno = -ret;
            if (ret)
                    err(1, "modify_ldt");
    
            asm volatile ("movw %0, %%es" :: "rm" ((unsigned short)7));
            write(1, buf, len);
            return 0;
    }
    
    and running ldt_echo >/sys/power/mem
    
    Without the fix, the latter causes a triple fault on resume.
    
    Fixes: ca37e57bbe0c ("x86/entry/64: Add missing irqflags tracing to native_load_gs_index()")
    Reported-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: https://lkml.kernel.org/r/6b31721ea92f51ea839e79bd97ade4a75b1eeea2.1512057304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 84fcfde53f8f..5191de14f4df 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -226,8 +226,20 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	load_idt((const struct desc_ptr *)&ctxt->idt_limit);
 #endif
 
+#ifdef CONFIG_X86_64
 	/*
-	 * segment registers
+	 * We need GSBASE restored before percpu access can work.
+	 * percpu access can happen in exception handlers or in complicated
+	 * helpers like load_gs_index().
+	 */
+	wrmsrl(MSR_GS_BASE, ctxt->gs_base);
+#endif
+
+	fix_processor_context();
+
+	/*
+	 * Restore segment registers.  This happens after restoring the GDT
+	 * and LDT, which happen in fix_processor_context().
 	 */
 #ifdef CONFIG_X86_32
 	loadsegment(es, ctxt->es);
@@ -248,13 +260,14 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	load_gs_index(ctxt->gs);
 	asm volatile ("movw %0, %%ss" :: "r" (ctxt->ss));
 
+	/*
+	 * Restore FSBASE and user GSBASE after reloading the respective
+	 * segment selectors.
+	 */
 	wrmsrl(MSR_FS_BASE, ctxt->fs_base);
-	wrmsrl(MSR_GS_BASE, ctxt->gs_base);
 	wrmsrl(MSR_KERNEL_GS_BASE, ctxt->gs_kernel_base);
 #endif
 
-	fix_processor_context();
-
 	do_fpu_end();
 	tsc_verify_tsc_adjust(true);
 	x86_platform.restore_sched_clock_state();

commit 6faadbbb7f9da70ce484f98f72223c20125a1009
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Sep 14 11:59:30 2017 +0200

    dmi: Mark all struct dmi_system_id instances const
    
    ... and __initconst if applicable.
    
    Based on similar work for an older kernel in the Grsecurity patch.
    
    [JD: fix toshiba-wmi build]
    [JD: add htcpen]
    [JD: move __initconst where checkscript wants it]
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jean Delvare <jdelvare@suse.de>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 4d68d59f457d..84fcfde53f8f 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -428,7 +428,7 @@ static int msr_initialize_bdw(const struct dmi_system_id *d)
 	return msr_init_context(bdw_msr_id, ARRAY_SIZE(bdw_msr_id));
 }
 
-static struct dmi_system_id msr_save_dmi_table[] = {
+static const struct dmi_system_id msr_save_dmi_table[] = {
 	{
 	 .callback = msr_initialize_bdw,
 	 .ident = "BROADWELL BDX_EP",

commit 72c0098d92cedb11c7e0151e84918840a4e96b31
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Sep 6 19:54:53 2017 -0700

    x86/mm: Reinitialize TLB state on hotplug and resume
    
    When Linux brings a CPU down and back up, it switches to init_mm and then
    loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect
    of masking off the ASID bits in CR3.
    
    This can result in some confusion in the TLB handling code.  If we
    bring a CPU down and back up with any ASID other than 0, we end up
    with the wrong ASID active on the CPU after resume.  This could
    cause our internal state to become corrupt, although major
    corruption is unlikely because init_mm doesn't have any user pages.
    More obviously, if CONFIG_DEBUG_VM=y, we'll trip over an assertion
    in the next context switch.  The result of *that* is a failure to
    resume from suspend with probability 1 - 1/6^(cpus-1).
    
    Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Jiri Kosina <jikos@kernel.org>
    Fixes: 10af6235e0d3 ("x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID")
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 78459a6d455a..4d68d59f457d 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -181,6 +181,7 @@ static void fix_processor_context(void)
 #endif
 	load_TR_desc();				/* This does ltr */
 	load_mm_ldt(current->active_mm);	/* This does lldt */
+	initialize_tlbstate_and_flush();
 
 	fpu__resume_cpu();
 

commit 6c690ee1039b251e583fc65b28da30e97d6a7385
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Jun 12 10:26:14 2017 -0700

    x86/mm: Split read_cr3() into read_cr3_pa() and __read_cr3()
    
    The kernel has several code paths that read CR3.  Most of them assume that
    CR3 contains the PGD's physical address, whereas some of them awkwardly
    use PHYSICAL_PAGE_MASK to mask off low bits.
    
    Add explicit mask macros for CR3 and convert all of the CR3 readers.
    This will keep them from breaking when PCID is enabled.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/883f8fb121f4616c1c1427ad87350bb2f5ffeca1.1497288170.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 6b05a9219ea2..78459a6d455a 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -129,7 +129,7 @@ static void __save_processor_state(struct saved_context *ctxt)
 	 */
 	ctxt->cr0 = read_cr0();
 	ctxt->cr2 = read_cr2();
-	ctxt->cr3 = read_cr3();
+	ctxt->cr3 = __read_cr3();
 	ctxt->cr4 = __read_cr4();
 #ifdef CONFIG_X86_64
 	ctxt->cr8 = read_cr8();

commit 69218e47994da614e7af600bf06887750ab6657a
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Mar 14 10:05:07 2017 -0700

    x86: Remap GDT tables in the fixmap section
    
    Each processor holds a GDT in its per-cpu structure. The sgdt
    instruction gives the base address of the current GDT. This address can
    be used to bypass KASLR memory randomization. With another bug, an
    attacker could target other per-cpu structures or deduce the base of
    the main memory section (PAGE_OFFSET).
    
    This patch relocates the GDT table for each processor inside the
    fixmap section. The space is reserved based on number of supported
    processors.
    
    For consistency, the remapping is done by default on 32 and 64-bit.
    
    Each processor switches to its remapped GDT at the end of
    initialization. For hibernation, the main processor returns with the
    original GDT and switches back to the remapping at completion.
    
    This patch was tested on both architectures. Hibernation and KVM were
    both tested specially for their usage of the GDT.
    
    Thanks to Boris Ostrovsky <boris.ostrovsky@oracle.com> for testing and
    recommending changes for Xen support.
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lorenzo Stoakes <lstoakes@gmail.com>
    Cc: Luis R . Rodriguez <mcgrof@kernel.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kvm@vger.kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-pm@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Cc: zijun_hu <zijun_hu@htc.com>
    Link: http://lkml.kernel.org/r/20170314170508.100882-2-thgarnie@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 66ade16c7693..6b05a9219ea2 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -95,7 +95,7 @@ static void __save_processor_state(struct saved_context *ctxt)
 	 * 'pmode_gdt' in wakeup_start.
 	 */
 	ctxt->gdt_desc.size = GDT_SIZE - 1;
-	ctxt->gdt_desc.address = (unsigned long)get_cpu_gdt_table(smp_processor_id());
+	ctxt->gdt_desc.address = (unsigned long)get_cpu_gdt_rw(smp_processor_id());
 
 	store_tr(ctxt->tr);
 
@@ -162,7 +162,7 @@ static void fix_processor_context(void)
 	int cpu = smp_processor_id();
 	struct tss_struct *t = &per_cpu(cpu_tss, cpu);
 #ifdef CONFIG_X86_64
-	struct desc_struct *desc = get_cpu_gdt_table(cpu);
+	struct desc_struct *desc = get_cpu_gdt_rw(cpu);
 	tss_desc tss;
 #endif
 	set_tss_desc(cpu, t);	/*
@@ -183,6 +183,9 @@ static void fix_processor_context(void)
 	load_mm_ldt(current->active_mm);	/* This does lldt */
 
 	fpu__resume_cpu();
+
+	/* The processor is back on the direct GDT, load back the fixmap */
+	load_fixmap_gdt(cpu);
 }
 
 /**

commit 6a369583178d0b89c2c3919c4456ee22fee0f249
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 13:14:17 2016 +0000

    x86/tsc: Validate TSC_ADJUST after resume
    
    Some 'feature' BIOSes fiddle with the TSC_ADJUST register during
    suspend/resume which renders the TSC unusable.
    
    Add sanity checks into the resume path and restore the
    original value if it was adjusted.
    
    Reported-and-tested-by: Roland Scheidegger <rscheidegger_lists@hispeed.ch>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bruce Schlobohm <bruce.schlobohm@intel.com>
    Cc: Kevin Stanton <kevin.b.stanton@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Allen Hung <allen_hung@dell.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161213131211.317654500@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 53cace2ec0e2..66ade16c7693 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -252,6 +252,7 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	fix_processor_context();
 
 	do_fpu_end();
+	tsc_verify_tsc_adjust(true);
 	x86_platform.restore_sched_clock_state();
 	mtrr_bp_restore();
 	perf_restore_debug_store();

commit 1ef55be16ed69538f89e0a6508be5e62fdc9851c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 29 12:48:12 2016 -0700

    x86/asm: Get rid of __read_cr4_safe()
    
    We use __read_cr4() vs __read_cr4_safe() inconsistently.  On
    CR4-less CPUs, all CR4 bits are effectively clear, so we can make
    the code simpler and more robust by making __read_cr4() always fix
    up faults on 32-bit kernels.
    
    This may fix some bugs on old 486-like CPUs, but I don't have any
    easy way to test that.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: david@saggiorato.net
    Link: http://lkml.kernel.org/r/ea647033d357d9ce2ad2bbde5a631045f5052fb6.1475178370.git.luto@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index b12c26e2e309..53cace2ec0e2 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -130,7 +130,7 @@ static void __save_processor_state(struct saved_context *ctxt)
 	ctxt->cr0 = read_cr0();
 	ctxt->cr2 = read_cr2();
 	ctxt->cr3 = read_cr3();
-	ctxt->cr4 = __read_cr4_safe();
+	ctxt->cr4 = __read_cr4();
 #ifdef CONFIG_X86_64
 	ctxt->cr8 = read_cr8();
 #endif

commit 406f992e4a372dafbe3c2cff7efbb2002a5c8ebd
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Jul 14 03:55:23 2016 +0200

    x86 / hibernate: Use hlt_play_dead() when resuming from hibernation
    
    On Intel hardware, native_play_dead() uses mwait_play_dead() by
    default and only falls back to the other methods if that fails.
    That also happens during resume from hibernation, when the restore
    (boot) kernel runs disable_nonboot_cpus() to take all of the CPUs
    except for the boot one offline.
    
    However, that is problematic, because the address passed to
    __monitor() in mwait_play_dead() is likely to be written to in the
    last phase of hibernate image restoration and that causes the "dead"
    CPU to start executing instructions again.  Unfortunately, the page
    containing the address in that CPU's instruction pointer may not be
    valid any more at that point.
    
    First, that page may have been overwritten with image kernel memory
    contents already, so the instructions the CPU attempts to execute may
    simply be invalid.  Second, the page tables previously used by that
    CPU may have been overwritten by image kernel memory contents, so the
    address in its instruction pointer is impossible to resolve then.
    
    A report from Varun Koyyalagunta and investigation carried out by
    Chen Yu show that the latter sometimes happens in practice.
    
    To prevent it from happening, temporarily change the smp_ops.play_dead
    pointer during resume from hibernation so that it points to a special
    "play dead" routine which uses hlt_play_dead() and avoids the
    inadvertent "revivals" of "dead" CPUs this way.
    
    A slightly unpleasant consequence of this change is that if the
    system is hibernated with one or more CPUs offline, it will generally
    draw more power after resume than it did before hibernation, because
    the physical state entered by CPUs via hlt_play_dead() is higher-power
    than the mwait_play_dead() one in the majority of cases.  It is
    possible to work around this, but it is unclear how much of a problem
    that's going to be in practice, so the workaround will be implemented
    later if it turns out to be necessary.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=106371
    Reported-by: Varun Koyyalagunta <cpudebug@centtech.com>
    Original-by: Chen Yu <yu.c.chen@intel.com>
    Tested-by: Chen Yu <yu.c.chen@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index d5f64996394a..b12c26e2e309 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -12,6 +12,7 @@
 #include <linux/export.h>
 #include <linux/smp.h>
 #include <linux/perf_event.h>
+#include <linux/tboot.h>
 
 #include <asm/pgtable.h>
 #include <asm/proto.h>
@@ -266,6 +267,35 @@ void notrace restore_processor_state(void)
 EXPORT_SYMBOL(restore_processor_state);
 #endif
 
+#if defined(CONFIG_HIBERNATION) && defined(CONFIG_HOTPLUG_CPU)
+static void resume_play_dead(void)
+{
+	play_dead_common();
+	tboot_shutdown(TB_SHUTDOWN_WFS);
+	hlt_play_dead();
+}
+
+int hibernate_resume_nonboot_cpu_disable(void)
+{
+	void (*play_dead)(void) = smp_ops.play_dead;
+	int ret;
+
+	/*
+	 * Ensure that MONITOR/MWAIT will not be used in the "play dead" loop
+	 * during hibernate image restoration, because it is likely that the
+	 * monitored address will be actually written to at that time and then
+	 * the "dead" CPU will attempt to execute instructions again, but the
+	 * address in its instruction pointer may not be possible to resolve
+	 * any more at that point (the page tables used by it previously may
+	 * have been overwritten by hibernate image data).
+	 */
+	smp_ops.play_dead = resume_play_dead;
+	ret = disable_nonboot_cpus();
+	smp_ops.play_dead = play_dead;
+	return ret;
+}
+#endif
+
 /*
  * When bsp_check() is called in hibernate and suspend, cpu hotplug
  * is disabled already. So it's unnessary to handle race condition between

commit 7a9c2dd08eadd5c6943115dbbec040c38d2e0822
Author: Chen Yu <yu.c.chen@intel.com>
Date:   Wed Nov 25 01:03:41 2015 +0800

    x86/pm: Introduce quirk framework to save/restore extra MSR registers around suspend/resume
    
    A bug was reported that on certain Broadwell platforms, after
    resuming from S3, the CPU is running at an anomalously low
    speed.
    
    It turns out that the BIOS has modified the value of the
    THERM_CONTROL register during S3, and changed it from 0 to 0x10,
    thus enabled clock modulation(bit4), but with undefined CPU Duty
    Cycle(bit1:3) - which causes the problem.
    
    Here is a simple scenario to reproduce the issue:
    
     1. Boot up the system
     2. Get MSR 0x19a, it should be 0
     3. Put the system into sleep, then wake it up
     4. Get MSR 0x19a, it shows 0x10, while it should be 0
    
    Although some BIOSen want to change the CPU Duty Cycle during
    S3, in our case we don't want the BIOS to do any modification.
    
    Fix this issue by introducing a more generic x86 framework to
    save/restore specified MSR registers(THERM_CONTROL in this case)
    for suspend/resume. This allows us to fix similar bugs in a much
    simpler way in the future.
    
    When the kernel wants to protect certain MSRs during suspending,
    we simply add a quirk entry in msr_save_dmi_table, and customize
    the MSR registers inside the quirk callback, for example:
    
      u32 msr_id_need_to_save[] = {MSR_ID0, MSR_ID1, MSR_ID2...};
    
    and the quirk mechanism ensures that, once resumed from suspend,
    the MSRs indicated by these IDs will be restored to their
    original, pre-suspend values.
    
    Since both 64-bit and 32-bit kernels are affected, this patch
    covers the common 64/32-bit suspend/resume code path. And
    because the MSRs specified by the user might not be available or
    readable in any situation, we use rdmsrl_safe() to safely save
    these MSRs.
    
    Reported-and-tested-by: Marcin Kaszewski <marcin.kaszewski@intel.com>
    Signed-off-by: Chen Yu <yu.c.chen@intel.com>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@suse.de
    Cc: len.brown@intel.com
    Cc: linux@horizon.com
    Cc: luto@kernel.org
    Cc: rjw@rjwysocki.net
    Link: http://lkml.kernel.org/r/c9abdcbc173dd2f57e8990e304376f19287e92ba.1448382971.git.yu.c.chen@intel.com
    [ More edits to the naming of data structures. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 9ab52791fed5..d5f64996394a 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -23,6 +23,7 @@
 #include <asm/debugreg.h>
 #include <asm/cpu.h>
 #include <asm/mmu_context.h>
+#include <linux/dmi.h>
 
 #ifdef CONFIG_X86_32
 __visible unsigned long saved_context_ebx;
@@ -32,6 +33,29 @@ __visible unsigned long saved_context_eflags;
 #endif
 struct saved_context saved_context;
 
+static void msr_save_context(struct saved_context *ctxt)
+{
+	struct saved_msr *msr = ctxt->saved_msrs.array;
+	struct saved_msr *end = msr + ctxt->saved_msrs.num;
+
+	while (msr < end) {
+		msr->valid = !rdmsrl_safe(msr->info.msr_no, &msr->info.reg.q);
+		msr++;
+	}
+}
+
+static void msr_restore_context(struct saved_context *ctxt)
+{
+	struct saved_msr *msr = ctxt->saved_msrs.array;
+	struct saved_msr *end = msr + ctxt->saved_msrs.num;
+
+	while (msr < end) {
+		if (msr->valid)
+			wrmsrl(msr->info.msr_no, msr->info.reg.q);
+		msr++;
+	}
+}
+
 /**
  *	__save_processor_state - save CPU registers before creating a
  *		hibernation image and before restoring the memory state from it
@@ -111,6 +135,7 @@ static void __save_processor_state(struct saved_context *ctxt)
 #endif
 	ctxt->misc_enable_saved = !rdmsrl_safe(MSR_IA32_MISC_ENABLE,
 					       &ctxt->misc_enable);
+	msr_save_context(ctxt);
 }
 
 /* Needed by apm.c */
@@ -229,6 +254,7 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	x86_platform.restore_sched_clock_state();
 	mtrr_bp_restore();
 	perf_restore_debug_store();
+	msr_restore_context(ctxt);
 }
 
 /* Needed by apm.c */
@@ -320,3 +346,69 @@ static int __init bsp_pm_check_init(void)
 }
 
 core_initcall(bsp_pm_check_init);
+
+static int msr_init_context(const u32 *msr_id, const int total_num)
+{
+	int i = 0;
+	struct saved_msr *msr_array;
+
+	if (saved_context.saved_msrs.array || saved_context.saved_msrs.num > 0) {
+		pr_err("x86/pm: MSR quirk already applied, please check your DMI match table.\n");
+		return -EINVAL;
+	}
+
+	msr_array = kmalloc_array(total_num, sizeof(struct saved_msr), GFP_KERNEL);
+	if (!msr_array) {
+		pr_err("x86/pm: Can not allocate memory to save/restore MSRs during suspend.\n");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < total_num; i++) {
+		msr_array[i].info.msr_no	= msr_id[i];
+		msr_array[i].valid		= false;
+		msr_array[i].info.reg.q		= 0;
+	}
+	saved_context.saved_msrs.num	= total_num;
+	saved_context.saved_msrs.array	= msr_array;
+
+	return 0;
+}
+
+/*
+ * The following section is a quirk framework for problematic BIOSen:
+ * Sometimes MSRs are modified by the BIOSen after suspended to
+ * RAM, this might cause unexpected behavior after wakeup.
+ * Thus we save/restore these specified MSRs across suspend/resume
+ * in order to work around it.
+ *
+ * For any further problematic BIOSen/platforms,
+ * please add your own function similar to msr_initialize_bdw.
+ */
+static int msr_initialize_bdw(const struct dmi_system_id *d)
+{
+	/* Add any extra MSR ids into this array. */
+	u32 bdw_msr_id[] = { MSR_IA32_THERM_CONTROL };
+
+	pr_info("x86/pm: %s detected, MSR saving is needed during suspending.\n", d->ident);
+	return msr_init_context(bdw_msr_id, ARRAY_SIZE(bdw_msr_id));
+}
+
+static struct dmi_system_id msr_save_dmi_table[] = {
+	{
+	 .callback = msr_initialize_bdw,
+	 .ident = "BROADWELL BDX_EP",
+	 .matches = {
+		DMI_MATCH(DMI_PRODUCT_NAME, "GRANTLEY"),
+		DMI_MATCH(DMI_PRODUCT_VERSION, "E63448-400"),
+		},
+	},
+	{}
+};
+
+static int pm_check_save_msr(void)
+{
+	dmi_check_system(msr_save_dmi_table);
+	return 0;
+}
+
+device_initcall(pm_check_save_msr);

commit 37868fe113ff2ba814b3b4eb12df214df555f8dc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 30 14:31:32 2015 -0700

    x86/ldt: Make modify_ldt synchronous
    
    modify_ldt() has questionable locking and does not synchronize
    threads.  Improve it: redesign the locking and synchronize all
    threads' LDTs using an IPI on all modifications.
    
    This will dramatically slow down modify_ldt in multithreaded
    programs, but there shouldn't be any multithreaded programs that
    care about modify_ldt's performance in the first place.
    
    This fixes some fallout from the CVE-2015-5157 fixes.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: security@kernel.org <security@kernel.org>
    Cc: <stable@vger.kernel.org>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/4c6978476782160600471bd865b318db34c7b628.1438291540.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 0d7dd1f5ac36..9ab52791fed5 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -22,6 +22,7 @@
 #include <asm/fpu/internal.h>
 #include <asm/debugreg.h>
 #include <asm/cpu.h>
+#include <asm/mmu_context.h>
 
 #ifdef CONFIG_X86_32
 __visible unsigned long saved_context_ebx;
@@ -153,7 +154,7 @@ static void fix_processor_context(void)
 	syscall_init();				/* This sets MSR_*STAR and related */
 #endif
 	load_TR_desc();				/* This does ltr */
-	load_LDT(&current->active_mm->context);	/* This does lldt */
+	load_mm_ldt(current->active_mm);	/* This does lldt */
 
 	fpu__resume_cpu();
 }

commit 952f07ecbd4d9bac77c003ba136f8ee8ce631591
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 16:56:05 2015 +0200

    x86/fpu: Move various internal function prototypes to fpu/internal.h
    
    There are a number of FPU internal function prototypes and an inline function
    in fpu/api.h, mostly placed so historically as the code grew over the years.
    
    Move them over into fpu/internal.h where they belong. (Add sched.h include
    to stackprotector.h which incorrectly relied on getting it from fpu/api.h.)
    
    fpu/api.h is now a pure file that only contains FPU APIs intended for driver
    use.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index ad0ce6b70fac..0d7dd1f5ac36 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -19,6 +19,7 @@
 #include <asm/page.h>
 #include <asm/mce.h>
 #include <asm/suspend.h>
+#include <asm/fpu/internal.h>
 #include <asm/debugreg.h>
 #include <asm/cpu.h>
 

commit 9254aaa0fea4e8aa4e83539c379aecb41a975ca6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 10:02:32 2015 +0200

    x86/fpu: Move XCR0 manipulation to the FPU code proper
    
    The suspend code accesses FPU state internals, add a helper for
    it and isolate it.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 62054acbd0d8..ad0ce6b70fac 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -18,10 +18,8 @@
 #include <asm/mtrr.h>
 #include <asm/page.h>
 #include <asm/mce.h>
-#include <asm/xcr.h>
 #include <asm/suspend.h>
 #include <asm/debugreg.h>
-#include <asm/fpu/internal.h> /* xfeatures_mask */
 #include <asm/cpu.h>
 
 #ifdef CONFIG_X86_32
@@ -155,6 +153,8 @@ static void fix_processor_context(void)
 #endif
 	load_TR_desc();				/* This does ltr */
 	load_LDT(&current->active_mm->context);	/* This does lldt */
+
+	fpu__resume_cpu();
 }
 
 /**
@@ -221,12 +221,6 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	wrmsrl(MSR_KERNEL_GS_BASE, ctxt->gs_kernel_base);
 #endif
 
-	/*
-	 * restore XCR0 for xsave capable cpu's.
-	 */
-	if (cpu_has_xsave)
-		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
-
 	fix_processor_context();
 
 	do_fpu_end();

commit 614df7fb8a661b0881f9709206350b59de3f84ab
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 09:20:33 2015 +0200

    x86/fpu: Rename 'pcntxt_mask' to 'xfeatures_mask'
    
    So the 'pcntxt_mask' is a misnomer, it's essentially meaningless to anyone
    who doesn't know what it does exactly.
    
    Name it more descriptively as 'xfeatures_mask'.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index edaf934c749e..62054acbd0d8 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -21,7 +21,7 @@
 #include <asm/xcr.h>
 #include <asm/suspend.h>
 #include <asm/debugreg.h>
-#include <asm/fpu/internal.h> /* pcntxt_mask */
+#include <asm/fpu/internal.h> /* xfeatures_mask */
 #include <asm/cpu.h>
 
 #ifdef CONFIG_X86_32
@@ -225,7 +225,7 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	 * restore XCR0 for xsave capable cpu's.
 	 */
 	if (cpu_has_xsave)
-		xsetbv(XCR_XFEATURE_ENABLED_MASK, pcntxt_mask);
+		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
 
 	fix_processor_context();
 

commit 78f7f1e54bac032b98956862a5bcf8c28ed22d07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:54:44 2015 +0200

    x86/fpu: Rename fpu-internal.h to fpu/internal.h
    
    This unifies all the FPU related header files under a unified, hiearchical
    naming scheme:
    
     - asm/fpu/types.h:      FPU related data types, needed for 'struct task_struct',
                             widely included in almost all kernel code, and hence kept
                             as small as possible.
    
     - asm/fpu/api.h:        FPU related 'public' methods exported to other subsystems.
    
     - asm/fpu/internal.h:   FPU subsystem internal methods
    
     - asm/fpu/xsave.h:      XSAVE support internal methods
    
    (Also standardize the header guard in asm/fpu/internal.h.)
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 757678fb26e1..edaf934c749e 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -21,7 +21,7 @@
 #include <asm/xcr.h>
 #include <asm/suspend.h>
 #include <asm/debugreg.h>
-#include <asm/fpu-internal.h> /* pcntxt_mask */
+#include <asm/fpu/internal.h> /* pcntxt_mask */
 #include <asm/cpu.h>
 
 #ifdef CONFIG_X86_32

commit 24933b82c0d9a711475a5ef7904eb733f561e637
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:05 2015 -0800

    x86/asm/entry: Rename 'init_tss' to 'cpu_tss'
    
    It has nothing to do with init -- there's only one TSS per cpu.
    
    Other names considered include:
    
     - current_tss: Confusing because we never switch the tss.
     - singleton_tss: Too long.
    
    This patch was generated with 's/init_tss/cpu_tss/g'.  Followup
    patches will fix INIT_TSS and INIT_TSS_IST by hand.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/da29fb2a793e4f649d93ce2d1ed320ebe8516262.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 3e32ed5648a0..757678fb26e1 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -134,7 +134,7 @@ static void do_fpu_end(void)
 static void fix_processor_context(void)
 {
 	int cpu = smp_processor_id();
-	struct tss_struct *t = &per_cpu(init_tss, cpu);
+	struct tss_struct *t = &per_cpu(cpu_tss, cpu);
 #ifdef CONFIG_X86_64
 	struct desc_struct *desc = get_cpu_gdt_table(cpu);
 	tss_desc tss;

commit 1e02ce4cccdcb9688386e5b8d2c9fa4660b45389
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:08 2014 -0700

    x86: Store a per-cpu shadow copy of CR4
    
    Context switches and TLB flushes can change individual bits of CR4.
    CR4 reads take several cycles, so store a shadow copy of CR4 in a
    per-cpu variable.
    
    To avoid wasting a cache line, I added the CR4 shadow to
    cpu_tlbstate, which is already touched in switch_mm.  The heaviest
    users of the cr4 shadow will be switch_mm and __switch_to_xtra, and
    __switch_to_xtra is called shortly after switch_mm during context
    switch, so the cacheline is likely to be hot.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/3a54dd3353fffbf84804398e00dfdc5b7c1afd7d.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 6ec7910f59bf..3e32ed5648a0 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -105,11 +105,8 @@ static void __save_processor_state(struct saved_context *ctxt)
 	ctxt->cr0 = read_cr0();
 	ctxt->cr2 = read_cr2();
 	ctxt->cr3 = read_cr3();
-#ifdef CONFIG_X86_32
-	ctxt->cr4 = read_cr4_safe();
-#else
-/* CONFIG_X86_64 */
-	ctxt->cr4 = read_cr4();
+	ctxt->cr4 = __read_cr4_safe();
+#ifdef CONFIG_X86_64
 	ctxt->cr8 = read_cr8();
 #endif
 	ctxt->misc_enable_saved = !rdmsrl_safe(MSR_IA32_MISC_ENABLE,
@@ -175,12 +172,12 @@ static void notrace __restore_processor_state(struct saved_context *ctxt)
 	/* cr4 was introduced in the Pentium CPU */
 #ifdef CONFIG_X86_32
 	if (ctxt->cr4)
-		write_cr4(ctxt->cr4);
+		__write_cr4(ctxt->cr4);
 #else
 /* CONFIG X86_64 */
 	wrmsrl(MSR_EFER, ctxt->efer);
 	write_cr8(ctxt->cr8);
-	write_cr4(ctxt->cr4);
+	__write_cr4(ctxt->cr4);
 #endif
 	write_cr3(ctxt->cr3);
 	write_cr2(ctxt->cr2);

commit b8f99b3e0e066e7b2f3dbc348fe33d8277950727
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jun 24 20:58:26 2014 -0400

    x86, power, suspend: Annotate restore_processor_state() with notrace
    
    ftrace_stop() is used to stop function tracing during suspend and resume
    which removes a lot of possible debugging opportunities with tracing.
    The reason was that some function in the resume path was causing a triple
    fault if it were to be traced. The issue I found was that doing something
    as simple as calling smp_processor_id() would reboot the box!
    
    When function tracing was first created I didn't have a good way to figure
    out what function was having issues, or it looked to be multiple ones. To
    fix it, we just created a big hammer approach to the problem which was to
    add a flag in the mcount trampoline that could be checked and not call
    the traced functions.
    
    Lately I developed better ways to find problem functions and I can bisect
    down to see what function is causing the issue. I removed the flag that
    stopped tracing and proceeded to find the problem function and it ended
    up being restore_processor_state(). This function makes sense as when the
    CPU comes back online from a suspend it calls this function to set up
    registers, amongst them the GS register, which stores things such as
    what CPU the processor is (if you call smp_processor_id() without this
    set up properly, it would fault).
    
    By making restore_processor_state() notrace, the system can suspend and
    resume without the need of the big hammer tracing to stop.
    
    Link: http://lkml.kernel.org/r/3577662.BSnUZfboWb@vostro.rjw.lan
    
    Acked-by: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 424f4c97a44d..6ec7910f59bf 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -165,7 +165,7 @@ static void fix_processor_context(void)
  *		by __save_processor_state()
  *	@ctxt - structure to load the registers contents from
  */
-static void __restore_processor_state(struct saved_context *ctxt)
+static void notrace __restore_processor_state(struct saved_context *ctxt)
 {
 	if (ctxt->misc_enable_saved)
 		wrmsrl(MSR_IA32_MISC_ENABLE, ctxt->misc_enable);
@@ -239,7 +239,7 @@ static void __restore_processor_state(struct saved_context *ctxt)
 }
 
 /* Needed by apm.c */
-void restore_processor_state(void)
+void notrace restore_processor_state(void)
 {
 	__restore_processor_state(&saved_context);
 }

commit d6efc2f7240b4e55590df69d74f33fdb72ce934a
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:49 2013 -0700

    x86, asmlinkage, power: Make various symbols used by the suspend asm code visible
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-16-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 1cf5b300305e..424f4c97a44d 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -25,10 +25,10 @@
 #include <asm/cpu.h>
 
 #ifdef CONFIG_X86_32
-unsigned long saved_context_ebx;
-unsigned long saved_context_esp, saved_context_ebp;
-unsigned long saved_context_esi, saved_context_edi;
-unsigned long saved_context_eflags;
+__visible unsigned long saved_context_ebx;
+__visible unsigned long saved_context_esp, saved_context_ebp;
+__visible unsigned long saved_context_esi, saved_context_edi;
+__visible unsigned long saved_context_eflags;
 #endif
 struct saved_context saved_context;
 

commit cc456c4e7cac3837a86aaa7ca3cb9f488d44d196
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 1 21:53:30 2013 -0400

    x86, gdt, hibernate: Store/load GDT for hibernate path.
    
    The git commite7a5cd063c7b4c58417f674821d63f5eb6747e37
    ("x86-64, gdt: Store/load GDT for ACPI S3 or hibernate/resume path
    is not needed.") assumes that for the hibernate path the booting
    kernel and the resuming kernel MUST be the same. That is certainly
    the case for a 32-bit kernel (see check_image_kernel and
    CONFIG_ARCH_HIBERNATION_HEADER config option).
    
    However for 64-bit kernels it is OK to have a different kernel
    version (and size of the image) of the booting and resuming kernels.
    Hence the above mentioned git commit introduces an regression.
    
    This patch fixes it by introducing a 'struct desc_ptr gdt_desc'
    back in the 'struct saved_context'. However instead of having in the
    'save_processor_state' and 'restore_processor_state' the
    store/load_gdt calls, we are only saving the GDT in the
    save_processor_state.
    
    For the restore path the lgdt operation is done in
    hibernate_asm_[32|64].S in the 'restore_registers' path.
    
    The apt reader of this description will recognize that only 64-bit
    kernels need this treatment, not 32-bit. This patch adds the logic
    in the 32-bit path to be more similar to 64-bit so that in the future
    the unification process can take advantage of this.
    
    [ hpa: this also reverts an inadvertent on-disk format change ]
    
    Suggested-by: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Link: http://lkml.kernel.org/r/1367459610-9656-2-git-send-email-konrad.wilk@oracle.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 6d6e907cee46..1cf5b300305e 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -25,16 +25,12 @@
 #include <asm/cpu.h>
 
 #ifdef CONFIG_X86_32
-static struct saved_context saved_context;
-
 unsigned long saved_context_ebx;
 unsigned long saved_context_esp, saved_context_ebp;
 unsigned long saved_context_esi, saved_context_edi;
 unsigned long saved_context_eflags;
-#else
-/* CONFIG_X86_64 */
-struct saved_context saved_context;
 #endif
+struct saved_context saved_context;
 
 /**
  *	__save_processor_state - save CPU registers before creating a
@@ -67,6 +63,15 @@ static void __save_processor_state(struct saved_context *ctxt)
 /* CONFIG_X86_64 */
 	store_idt((struct desc_ptr *)&ctxt->idt_limit);
 #endif
+	/*
+	 * We save it here, but restore it only in the hibernate case.
+	 * For ACPI S3 resume, this is loaded via 'early_gdt_desc' in 64-bit
+	 * mode in "secondary_startup_64". In 32-bit mode it is done via
+	 * 'pmode_gdt' in wakeup_start.
+	 */
+	ctxt->gdt_desc.size = GDT_SIZE - 1;
+	ctxt->gdt_desc.address = (unsigned long)get_cpu_gdt_table(smp_processor_id());
+
 	store_tr(ctxt->tr);
 
 	/* XMM0..XMM15 should be handled by kernel_fpu_begin(). */

commit 1e2f5b598aa56c3978c2e623f72e9656a565c6c9
Merge: f9b3bcfbc43a 4d681be3c33d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:41:21 2013 -0700

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt update from Ingo Molnar:
     "Various paravirtualization related changes - the biggest one makes
      guest support optional via CONFIG_HYPERVISOR_GUEST"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, wakeup, sleep: Use pvops functions for changing GDT entries
      x86, xen, gdt: Remove the pvops variant of store_gdt.
      x86-32, gdt: Store/load GDT for ACPI S3 or hibernation/resume path is not needed
      x86-64, gdt: Store/load GDT for ACPI S3 or hibernate/resume path is not needed.
      x86: Make Linux guest support optional
      x86, Kconfig: Move PARAVIRT_DEBUG into the paravirt menu

commit 4d681be3c33dd74efffbe2a8f70634f7128602ec
Author: konrad@kernel.org <konrad@kernel.org>
Date:   Fri Apr 5 16:42:24 2013 -0400

    x86, wakeup, sleep: Use pvops functions for changing GDT entries
    
    We check the TSS descriptor before we try to dereference it.
    Also we document what the value '9' actually means using the
    AMD64 Architecture Programmer's Manual Volume 2, pg 90:
    "Hex value 9: Available 64-bit TSS" and pg 91:
    "The available 32-bit TSS (09h), which is redefined as the
    available 64-bit TSS."
    
    Without this, on Xen, where the GDT is available as R/O (to
    protect the hypervisor from the guest modifying it), we end up
    with a pagetable fault.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Link: http://lkml.kernel.org/r/1365194544-14648-5-git-send-email-konrad.wilk@oracle.com
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 82c39c532349..168da8429032 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -132,7 +132,10 @@ static void fix_processor_context(void)
 {
 	int cpu = smp_processor_id();
 	struct tss_struct *t = &per_cpu(init_tss, cpu);
-
+#ifdef CONFIG_X86_64
+	struct desc_struct *desc = get_cpu_gdt_table(cpu);
+	tss_desc tss;
+#endif
 	set_tss_desc(cpu, t);	/*
 				 * This just modifies memory; should not be
 				 * necessary. But... This is necessary, because
@@ -141,7 +144,9 @@ static void fix_processor_context(void)
 				 */
 
 #ifdef CONFIG_X86_64
-	get_cpu_gdt_table(cpu)[GDT_ENTRY_TSS].type = 9;
+	memcpy(&tss, &desc[GDT_ENTRY_TSS], sizeof(tss_desc));
+	tss.type = 0x9; /* The available 64-bit TSS (see AMD vol 2, pg 91 */
+	write_gdt_entry(desc, GDT_ENTRY_TSS, &tss, DESC_TSS);
 
 	syscall_init();				/* This sets MSR_*STAR and related */
 #endif

commit 84e70971e67d97bc2db18a4e76d42846272a54bd
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Apr 5 16:42:22 2013 -0400

    x86-32, gdt: Store/load GDT for ACPI S3 or hibernation/resume path is not needed
    
    During the ACPI S3 suspend, we store the GDT in the wakup_header (see
    wakeup_asm.s) field called 'pmode_gdt'.
    
    Which is then used during the resume path and has the same exact
    value as what the store/load_gdt do with the saved_context
    (which is saved/restored via save/restore_processor_state()).
    
    The flow during resume from ACPI S3 is simpler than the 64-bit
    counterpart. We only use the early bootstrap once (wakeup_gdt) and
    do various checks in real mode.
    
    After the checks are completed, we load the saved GDT ('pmode_gdt') and
    continue on with the resume (by heading to startup_32 in trampoline_32.S) -
    which quickly jumps to what was saved in 'pmode_entry'
    aka 'wakeup_pmode_return'.
    
    The 'wakeup_pmode_return' restores the GDT (saved_gdt) again (which was
    saved in do_suspend_lowlevel initially). After that it ends up calling
    the 'ret_point' which calls 'restore_processor_state()'.
    
    We have two opportunities to remove code where we restore the same GDT
    twice.
    
    Here is the call chain:
     wakeup_start
           |- lgdtl wakeup_gdt [the work-around broken BIOSes]
           |
           | - lgdtl pmode_gdt [the real one]
           |
           \-- startup_32 (in trampoline_32.S)
                  \-- wakeup_pmode_return (in wakeup_32.S)
                           |- lgdtl saved_gdt [the real one]
                           \-- ret_point
                                 |..
                                 |- call restore_processor_state
    
    The hibernate path is much simpler. During the saving of the hibernation
    image we call save_processor_state() and save the contents of that
    along with the rest of the kernel in the hibernation image destination.
    We save the EIP of 'restore_registers' (restore_jump_address) and
    cr3 (restore_cr3).
    
    During hibernate resume, the 'restore_registers' (via the
    'restore_jump_address) in hibernate_asm_32.S is invoked which
    restores the contents of most registers. Naturally the resume path benefits
    from already being in 32-bit mode, so it does not have to reload the GDT.
    
    It only reloads the cr3 (from restore_cr3) and continues on. Note
    that the restoration of the restore image page-tables is done prior to
    this.
    
    After the 'restore_registers' it returns and we end up called
    restore_processor_state() - where we reload the GDT. The reload of
    the GDT is not needed as bootup kernel has already loaded the GDT
    which is at the same physical location as the the restored kernel.
    
    Note that the hibernation path assumes the GDT is correct during its
    'restore_registers'. The assumption in the code is that the restored
    image is the same as saved - meaning we are not trying to restore
    an different kernel in the virtual address space of a new kernel.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Link: http://lkml.kernel.org/r/1365194544-14648-3-git-send-email-konrad.wilk@oracle.com
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 6bd94233669c..82c39c532349 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -61,7 +61,6 @@ static void __save_processor_state(struct saved_context *ctxt)
 	 * descriptor tables
 	 */
 #ifdef CONFIG_X86_32
-	store_gdt(&ctxt->gdt);
 	store_idt(&ctxt->idt);
 #else
 /* CONFIG_X86_64 */
@@ -181,7 +180,6 @@ static void __restore_processor_state(struct saved_context *ctxt)
 	 * ltr is done i fix_processor_context().
 	 */
 #ifdef CONFIG_X86_32
-	load_gdt(&ctxt->gdt);
 	load_idt(&ctxt->idt);
 #else
 /* CONFIG_X86_64 */

commit e7a5cd063c7b4c58417f674821d63f5eb6747e37
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Apr 5 16:42:21 2013 -0400

    x86-64, gdt: Store/load GDT for ACPI S3 or hibernate/resume path is not needed.
    
    During the ACPI S3 resume path the trampoline code handles it already.
    
    During the ACPI S3 suspend phase (acpi_suspend_lowlevel) we set:
    early_gdt_descr.address = (..)get_cpu_gdt_table(smp_processor_id());
    
    which is then used during the resume path and has the same exact
    value as what the store/load_gdt do with the saved_context
    (which is saved/restored via save/restore_processor_state()).
    
    The flow during resume is complex and for 64-bit kernels we use three GDTs
    - one early bootstrap GDT (wakeup_igdt) that we load to workaround
    broken BIOSes, an early Protected Mode to Long Mode transition one
    (tr_gdt), and the final one - early_gdt_descr (which points to the real GDT).
    
    The early ('wakeup_gdt') is loaded in 'trampoline_start' for working
    around broken BIOSes, and then when we end up in Protected Mode in the
    startup_32 (in trampoline_64.s, not head_32.s) we use the 'tr_gdt'
    (still in trampoline_64.s). This 'tr_gdt' has a a 32-bit code segment,
    64-bit code segment with L=1, and a 32-bit data segment.
    
    Once we have transitioned from Protected Mode to Long Mode we then
    set the GDT to 'early_gdt_desc' and then via an iretq emerge in
    wakeup_long64 (set via 'initial_code' variable in acpi_suspend_lowlevel).
    
    In the wakeup_long64 we end up restoring the %rip (which is set to
    'resume_point') and jump there.
    
    In 'resume_point' we call 'restore_processor_state' which does
    the load_gdt on the saved context. This load_gdt is redundant as the
    GDT loaded via early_gdt_desc is the same.
    
    Here is the call-chain:
     wakeup_start
       |- lgdtl wakeup_gdt [the work-around broken BIOSes]
       |
       \-- trampoline_start (trampoline_64.S)
             |- lgdtl tr_gdt
             |
             \-- startup_32 (trampoline_64.S)
                   |
                   \-- startup_64 (trampoline_64.S)
                          |
                          \-- secondary_startup_64
                                   |- lgdtl early_gdt_desc
                                   | ...
                                   |- movq initial_code(%rip), %eax
                                   |-.. lretq
                                   \-- wakeup_64
                                         |-- other registers are reloaded
                                         |-- call restore_processor_state
    
    The hibernate path is much simpler. During the saving of the hibernation
    image we call save_processor_state() and save the contents of that along
    with the rest of the kernel in the hibernation image destination.
    We save the EIP of 'restore_registers' (restore_jump_address) and cr3
    (restore_cr3).
    
    During hibernate resume, the 'restore_registers' (via the
    'restore_jump_address) in hibernate_asm_64.S is invoked which restores
    the contents of most registers. Naturally the resume path benefits from
    already being in 64-bit mode, so it does not have to load the GDT.
    
    It only reloads the cr3 (from restore_cr3) and continues on. Note that
    the restoration of the restore image page-tables is done prior to this.
    
    After the 'restore_registers' it returns and we end up called
    restore_processor_state() - where we reload the GDT. The reload of
    the GDT is not needed as bootup kernel has already loaded the GDT which
    is at the same physical location as the the restored kernel.
    
    Note that the hibernation path assumes the GDT is correct during its
    'restore_registers'. The assumption in the code is that the restored
    image is the same as saved - meaning we are not trying to restore
    an different kernel in the virtual address space of a new kernel.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Link: http://lkml.kernel.org/r/1365194544-14648-2-git-send-email-konrad.wilk@oracle.com
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 120cee1c3f8d..6bd94233669c 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -65,7 +65,6 @@ static void __save_processor_state(struct saved_context *ctxt)
 	store_idt(&ctxt->idt);
 #else
 /* CONFIG_X86_64 */
-	store_gdt((struct desc_ptr *)&ctxt->gdt_limit);
 	store_idt((struct desc_ptr *)&ctxt->idt_limit);
 #endif
 	store_tr(ctxt->tr);
@@ -186,7 +185,6 @@ static void __restore_processor_state(struct saved_context *ctxt)
 	load_idt(&ctxt->idt);
 #else
 /* CONFIG_X86_64 */
-	load_gdt((const struct desc_ptr *)&ctxt->gdt_limit);
 	load_idt((const struct desc_ptr *)&ctxt->idt_limit);
 #endif
 

commit 1d9d8639c063caf6efc2447f5f26aa637f844ff6
Author: Stephane Eranian <eranian@google.com>
Date:   Fri Mar 15 14:26:07 2013 +0100

    perf,x86: fix kernel crash with PEBS/BTS after suspend/resume
    
    This patch fixes a kernel crash when using precise sampling (PEBS)
    after a suspend/resume. Turns out the CPU notifier code is not invoked
    on CPU0 (BP). Therefore, the DS_AREA (used by PEBS) is not restored properly
    by the kernel and keeps it power-on/resume value of 0 causing any PEBS
    measurement to crash when running on CPU0.
    
    The workaround is to add a hook in the actual resume code to restore
    the DS Area MSR value. It is invoked for all CPUS. So for all but CPU0,
    the DS_AREA will be restored twice but this is harmless.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 120cee1c3f8d..3c68768d7a75 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -11,6 +11,7 @@
 #include <linux/suspend.h>
 #include <linux/export.h>
 #include <linux/smp.h>
+#include <linux/perf_event.h>
 
 #include <asm/pgtable.h>
 #include <asm/proto.h>
@@ -228,6 +229,7 @@ static void __restore_processor_state(struct saved_context *ctxt)
 	do_fpu_end();
 	x86_platform.restore_sched_clock_state();
 	mtrr_bp_restore();
+	perf_restore_debug_store();
 }
 
 /* Needed by apm.c */

commit a71c8bc5dfefbbf80ef90739791554ef7ea4401b
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue Nov 13 11:32:51 2012 -0800

    x86, topology: Debug CPU0 hotplug
    
    CONFIG_DEBUG_HOTPLUG_CPU0 is for debugging the CPU0 hotplug feature. The switch
    offlines CPU0 as soon as possible and boots userspace up with CPU0 offlined.
    User can online CPU0 back after boot time. The default value of the switch is
    off.
    
    To debug CPU0 hotplug, you need to enable CPU0 offline/online feature by either
    turning on CONFIG_BOOTPARAM_HOTPLUG_CPU0 during compilation or giving
    cpu0_hotplug kernel parameter at boot.
    
    It's safe and early place to take down CPU0 after all hotplug notifiers
    are installed and SMP is booted.
    
    Please note that some applications or drivers, e.g. some versions of udevd,
    during boot time may put CPU0 online again in this CPU0 hotplug debug mode.
    
    In this debug mode, setup_local_APIC() may report a warning on max_loops<=0
    when CPU0 is onlined back after boot time. This is because pending interrupt in
    IRR can not move to ISR. The warning is not CPU0 specfic and it can happen on
    other CPUs as well. It is harmless except the first CPU0 online takes a bit
    longer time. And so this debug mode is useful to expose this issue. I'll send
    a seperate patch to fix this generic warning issue.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352835171-3958-15-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index adde77588e25..120cee1c3f8d 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -21,6 +21,7 @@
 #include <asm/suspend.h>
 #include <asm/debugreg.h>
 #include <asm/fpu-internal.h> /* pcntxt_mask */
+#include <asm/cpu.h>
 
 #ifdef CONFIG_X86_32
 static struct saved_context saved_context;
@@ -263,6 +264,43 @@ static int bsp_pm_callback(struct notifier_block *nb, unsigned long action,
 	case PM_HIBERNATION_PREPARE:
 		ret = bsp_check();
 		break;
+#ifdef CONFIG_DEBUG_HOTPLUG_CPU0
+	case PM_RESTORE_PREPARE:
+		/*
+		 * When system resumes from hibernation, online CPU0 because
+		 * 1. it's required for resume and
+		 * 2. the CPU was online before hibernation
+		 */
+		if (!cpu_online(0))
+			_debug_hotplug_cpu(0, 1);
+		break;
+	case PM_POST_RESTORE:
+		/*
+		 * When a resume really happens, this code won't be called.
+		 *
+		 * This code is called only when user space hibernation software
+		 * prepares for snapshot device during boot time. So we just
+		 * call _debug_hotplug_cpu() to restore to CPU0's state prior to
+		 * preparing the snapshot device.
+		 *
+		 * This works for normal boot case in our CPU0 hotplug debug
+		 * mode, i.e. CPU0 is offline and user mode hibernation
+		 * software initializes during boot time.
+		 *
+		 * If CPU0 is online and user application accesses snapshot
+		 * device after boot time, this will offline CPU0 and user may
+		 * see different CPU0 state before and after accessing
+		 * the snapshot device. But hopefully this is not a case when
+		 * user debugging CPU0 hotplug. Even if users hit this case,
+		 * they can easily online CPU0 back.
+		 *
+		 * To simplify this debug code, we only consider normal boot
+		 * case. Otherwise we need to remember CPU0's state and restore
+		 * to that state and resolve racy conditions etc.
+		 */
+		_debug_hotplug_cpu(0, 0);
+		break;
+#endif
 	default:
 		break;
 	}

commit 209efae12981f3d2d694499b761def10895c078c
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue Nov 13 11:32:42 2012 -0800

    x86, hotplug, suspend: Online CPU0 for suspend or hibernate
    
    Because x86 BIOS requires CPU0 to resume from sleep, suspend or hibernate can't
    be executed if CPU0 is detected offline. To make suspend or hibernate and
    further resume succeed, CPU0 must be online.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352835171-3958-6-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 218cdb16163c..adde77588e25 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -237,3 +237,47 @@ void restore_processor_state(void)
 #ifdef CONFIG_X86_32
 EXPORT_SYMBOL(restore_processor_state);
 #endif
+
+/*
+ * When bsp_check() is called in hibernate and suspend, cpu hotplug
+ * is disabled already. So it's unnessary to handle race condition between
+ * cpumask query and cpu hotplug.
+ */
+static int bsp_check(void)
+{
+	if (cpumask_first(cpu_online_mask) != 0) {
+		pr_warn("CPU0 is offline.\n");
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+static int bsp_pm_callback(struct notifier_block *nb, unsigned long action,
+			   void *ptr)
+{
+	int ret = 0;
+
+	switch (action) {
+	case PM_SUSPEND_PREPARE:
+	case PM_HIBERNATION_PREPARE:
+		ret = bsp_check();
+		break;
+	default:
+		break;
+	}
+	return notifier_from_errno(ret);
+}
+
+static int __init bsp_pm_check_init(void)
+{
+	/*
+	 * Set this bsp_pm_callback as lower priority than
+	 * cpu_hotplug_pm_callback. So cpu_hotplug_pm_callback will be called
+	 * earlier to disable cpu hotplug before bsp online check.
+	 */
+	pm_notifier(bsp_pm_callback, -INT_MAX);
+	return 0;
+}
+
+core_initcall(bsp_pm_check_init);

commit dba69d1092e291e257fb5673a3ad0e4c87878ebc
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Sun Apr 1 13:53:36 2012 -0300

    x86, kvm: Call restore_sched_clock_state() only after %gs is initialized
    
    s2ram broke due to this KVM commit:
    
      b74f05d61b73 x86: kvmclock: abstract save/restore sched_clock_state
    
    restore_sched_clock_state() methods use percpu data, therefore
    they must run after %gs is initialized, but before mtrr_bp_restore()
    (due to lockstat using sched_clock).
    
    Move it to the correct place.
    
    Reported-and-tested-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Avi Kivity <avi@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 47936830968c..218cdb16163c 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -225,13 +225,13 @@ static void __restore_processor_state(struct saved_context *ctxt)
 	fix_processor_context();
 
 	do_fpu_end();
+	x86_platform.restore_sched_clock_state();
 	mtrr_bp_restore();
 }
 
 /* Needed by apm.c */
 void restore_processor_state(void)
 {
-	x86_platform.restore_sched_clock_state();
 	__restore_processor_state(&saved_context);
 }
 #ifdef CONFIG_X86_32

commit 2e7580b0e75d771d93e24e681031a165b1d31071
Merge: d25413efa953 cf9eeac46350
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 14:35:31 2012 -0700

    Merge branch 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Avi Kivity:
     "Changes include timekeeping improvements, support for assigning host
      PCI devices that share interrupt lines, s390 user-controlled guests, a
      large ppc update, and random fixes."
    
    This is with the sign-off's fixed, hopefully next merge window we won't
    have rebased commits.
    
    * 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: Convert intx_mask_lock to spin lock
      KVM: x86: fix kvm_write_tsc() TSC matching thinko
      x86: kvmclock: abstract save/restore sched_clock_state
      KVM: nVMX: Fix erroneous exception bitmap check
      KVM: Ignore the writes to MSR_K7_HWCR(3)
      KVM: MMU: make use of ->root_level in reset_rsvds_bits_mask
      KVM: PMU: add proper support for fixed counter 2
      KVM: PMU: Fix raw event check
      KVM: PMU: warn when pin control is set in eventsel msr
      KVM: VMX: Fix delayed load of shared MSRs
      KVM: use correct tlbs dirty type in cmpxchg
      KVM: Allow host IRQ sharing for assigned PCI 2.3 devices
      KVM: Ensure all vcpus are consistent with in-kernel irqchip settings
      KVM: x86 emulator: Allow PM/VM86 switch during task switch
      KVM: SVM: Fix CPL updates
      KVM: x86 emulator: VM86 segments must have DPL 3
      KVM: x86 emulator: Fix task switch privilege checks
      arch/powerpc/kvm/book3s_hv.c: included linux/sched.h twice
      KVM: x86 emulator: correctly mask pmc index bits in RDPMC instruction emulation
      KVM: mmu_notifier: Flush TLBs before releasing mmu_lock
      ...

commit b74f05d61b73af584d0c39121980171389ecfaaa
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Feb 13 11:07:27 2012 -0200

    x86: kvmclock: abstract save/restore sched_clock_state
    
    Upon resume from hibernation, CPU 0's hvclock area contains the old
    values for system_time and tsc_timestamp. It is necessary for the
    hypervisor to update these values with uptodate ones before the CPU uses
    them.
    
    Abstract TSC's save/restore sched_clock_state functions and use
    restore_state to write to KVM_SYSTEM_TIME MSR, forcing an update.
    
    Also move restore_sched_clock_state before __restore_processor_state,
    since the later calls CONFIG_LOCK_STAT's lockstat_clock (also for TSC).
    Thanks to Igor Mammedov for tracking it down.
    
    Fixes suspend-to-disk with kvmclock.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index f10c0afa1cb4..0e76a2814127 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -114,7 +114,7 @@ static void __save_processor_state(struct saved_context *ctxt)
 void save_processor_state(void)
 {
 	__save_processor_state(&saved_context);
-	save_sched_clock_state();
+	x86_platform.save_sched_clock_state();
 }
 #ifdef CONFIG_X86_32
 EXPORT_SYMBOL(save_processor_state);
@@ -230,8 +230,8 @@ static void __restore_processor_state(struct saved_context *ctxt)
 /* Needed by apm.c */
 void restore_processor_state(void)
 {
+	x86_platform.restore_sched_clock_state();
 	__restore_processor_state(&saved_context);
-	restore_sched_clock_state();
 }
 #ifdef CONFIG_X86_32
 EXPORT_SYMBOL(restore_processor_state);

commit 1361b83a13d4d92e53fbb6c877528713e118b821
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 13:19:22 2012 -0800

    i387: Split up <asm/i387.h> into exported and internal interfaces
    
    While various modules include <asm/i387.h> to get access to things we
    actually *intend* for them to use, most of that header file was really
    pretty low-level internal stuff that we really don't want to expose to
    others.
    
    So split the header file into two: the small exported interfaces remain
    in <asm/i387.h>, while the internal definitions that are only used by
    core architecture code are now in <asm/fpu-internal.h>.
    
    The guiding principle for this was to expose functions that we export to
    modules, and leave them in <asm/i387.h>, while stuff that is used by
    task switching or was marked GPL-only is in <asm/fpu-internal.h>.
    
    The fpu-internal.h file could be further split up too, especially since
    arch/x86/kvm/ uses some of the remaining stuff for its module.  But that
    kvm usage should probably be abstracted out a bit, and at least now the
    internal FPU accessor functions are much more contained.  Even if it
    isn't perhaps as contained as it _could_ be.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1202211340330.5354@i5.linux-foundation.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index f10c0afa1cb4..4889655ba784 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -20,6 +20,7 @@
 #include <asm/xcr.h>
 #include <asm/suspend.h>
 #include <asm/debugreg.h>
+#include <asm/fpu-internal.h> /* pcntxt_mask */
 
 #ifdef CONFIG_X86_32
 static struct saved_context saved_context;

commit 69c60c88eeb364ebf58432f9bc38033522d58767
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu May 26 12:22:53 2011 -0400

    x86: Fix files explicitly requiring export.h for EXPORT_SYMBOL/THIS_MODULE
    
    These files were implicitly getting EXPORT_SYMBOL via device.h
    which was including module.h, but that will be fixed up shortly.
    
    By fixing these now, we can avoid seeing things like:
    
    arch/x86/kernel/rtc.c:29: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL’
    arch/x86/kernel/pci-dma.c:20: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL’
    arch/x86/kernel/e820.c:69: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL_GPL’
    
    [ with input from Randy Dunlap <rdunlap@xenotime.net> and also
      from Stephen Rothwell <sfr@canb.auug.org.au> ]
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 87bb35e34ef1..f10c0afa1cb4 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -9,6 +9,7 @@
  */
 
 #include <linux/suspend.h>
+#include <linux/export.h>
 #include <linux/smp.h>
 
 #include <asm/pgtable.h>

commit cd7240c0b900eb6d690ccee088a6c9b46dae815a
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Aug 19 17:03:38 2010 -0700

    x86, tsc, sched: Recompute cyc2ns_offset's during resume from sleep states
    
    TSC's get reset after suspend/resume (even on cpu's with invariant TSC
    which runs at a constant rate across ACPI P-, C- and T-states). And in
    some systems BIOS seem to reinit TSC to arbitrary large value (still
    sync'd across cpu's) during resume.
    
    This leads to a scenario of scheduler rq->clock (sched_clock_cpu()) less
    than rq->age_stamp (introduced in 2.6.32). This leads to a big value
    returned by scale_rt_power() and the resulting big group power set by the
    update_group_power() is causing improper load balancing between busy and
    idle cpu's after suspend/resume.
    
    This resulted in multi-threaded workloads (like kernel-compilation) go
    slower after suspend/resume cycle on core i5 laptops.
    
    Fix this by recomputing cyc2ns_offset's during resume, so that
    sched_clock() continues from the point where it was left off during
    suspend.
    
    Reported-by: Florian Pritz <flo@xssn.at>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: <stable@kernel.org> # [v2.6.32+]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1282262618.2675.24.camel@sbsiddha-MOBL3.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index e7e8c5f54956..87bb35e34ef1 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -113,6 +113,7 @@ static void __save_processor_state(struct saved_context *ctxt)
 void save_processor_state(void)
 {
 	__save_processor_state(&saved_context);
+	save_sched_clock_state();
 }
 #ifdef CONFIG_X86_32
 EXPORT_SYMBOL(save_processor_state);
@@ -229,6 +230,7 @@ static void __restore_processor_state(struct saved_context *ctxt)
 void restore_processor_state(void)
 {
 	__restore_processor_state(&saved_context);
+	restore_sched_clock_state();
 }
 #ifdef CONFIG_X86_32
 EXPORT_SYMBOL(restore_processor_state);

commit a2531293dbb7608fa672ff28efe3ab4027917a2f
Author: Pavel Machek <pavel@ucw.cz>
Date:   Sun Jul 18 14:27:13 2010 +0200

    update email address
    
    pavel@suse.cz no longer works, replace it with working address.
    
    Signed-off-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 1290ba54b350..e7e8c5f54956 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -4,7 +4,7 @@
  * Distribute under GPLv2
  *
  * Copyright (c) 2007 Rafael J. Wysocki <rjw@sisk.pl>
- * Copyright (c) 2002 Pavel Machek <pavel@suse.cz>
+ * Copyright (c) 2002 Pavel Machek <pavel@ucw.cz>
  * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>
  */
 

commit 85a0e7539781dad4bfcffd98e72fa9f130f4e40d
Author: Ondrej Zary <linux@rainbow-software.org>
Date:   Tue Jun 8 00:32:49 2010 +0200

    PM / x86: Save/restore MISC_ENABLE register
    
    Save/restore MISC_ENABLE register on suspend/resume.
    This fixes OOPS (invalid opcode) on resume from STR on Asus P4P800-VM,
    which wakes up with MWAIT disabled.
    
    Fixes https://bugzilla.kernel.org/show_bug.cgi?id=15385
    
    Signed-off-by: Ondrej Zary <linux@rainbow-software.org>
    Tested-by: Alan Stern <stern@rowland.harvard.edu>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 0a979f3e5b8a..1290ba54b350 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -105,6 +105,8 @@ static void __save_processor_state(struct saved_context *ctxt)
 	ctxt->cr4 = read_cr4();
 	ctxt->cr8 = read_cr8();
 #endif
+	ctxt->misc_enable_saved = !rdmsrl_safe(MSR_IA32_MISC_ENABLE,
+					       &ctxt->misc_enable);
 }
 
 /* Needed by apm.c */
@@ -152,6 +154,8 @@ static void fix_processor_context(void)
  */
 static void __restore_processor_state(struct saved_context *ctxt)
 {
+	if (ctxt->misc_enable_saved)
+		wrmsrl(MSR_IA32_MISC_ENABLE, ctxt->misc_enable);
 	/*
 	 * control registers
 	 */

commit 24f1e32c60c45c89a997c73395b69c8af6f0a84e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 9 19:22:48 2009 +0200

    hw-breakpoints: Rewrite the hw-breakpoints layer on top of perf events
    
    This patch rebase the implementation of the breakpoints API on top of
    perf events instances.
    
    Each breakpoints are now perf events that handle the
    register scheduling, thread/cpu attachment, etc..
    
    The new layering is now made as follows:
    
           ptrace       kgdb      ftrace   perf syscall
              \          |          /         /
               \         |         /         /
                                            /
                Core breakpoint API        /
                                          /
                         |               /
                         |              /
    
                  Breakpoints perf events
    
                         |
                         |
    
                   Breakpoints PMU ---- Debug Register constraints handling
                                        (Part of core breakpoint API)
                         |
                         |
    
                 Hardware debug registers
    
    Reasons of this rewrite:
    
    - Use the centralized/optimized pmu registers scheduling,
      implying an easier arch integration
    - More powerful register handling: perf attributes (pinned/flexible
      events, exclusive/non-exclusive, tunable period, etc...)
    
    Impact:
    
    - New perf ABI: the hardware breakpoints counters
    - Ptrace breakpoints setting remains tricky and still needs some per
      thread breakpoints references.
    
    Todo (in the order):
    
    - Support breakpoints perf counter events for perf tools (ie: implement
      perf_bpcounter_event())
    - Support from perf tools
    
    Changes in v2:
    
    - Follow the perf "event " rename
    - The ptrace regression have been fixed (ptrace breakpoint perf events
      weren't released when a task ended)
    - Drop the struct hw_breakpoint and store generic fields in
      perf_event_attr.
    - Separate core and arch specific headers, drop
      asm-generic/hw_breakpoint.h and create linux/hw_breakpoint.h
    - Use new generic len/type for breakpoint
    - Handle off case: when breakpoints api is not supported by an arch
    
    Changes in v3:
    
    - Fix broken CONFIG_KVM, we need to propagate the breakpoint api
      changes to kvm when we exit the guest and restore the bp registers
      to the host.
    
    Changes in v4:
    
    - Drop the hw_breakpoint_restore() stub as it is only used by KVM
    - EXPORT_SYMBOL_GPL hw_breakpoint_restore() as KVM can be built as a
      module
    - Restore the breakpoints unconditionally on kvm guest exit:
      TIF_DEBUG_THREAD doesn't anymore cover every cases of running
      breakpoints and vcpu->arch.switch_db_regs might not always be
      set when the guest used debug registers.
      (Waiting for a reliable optimization)
    
    Changes in v5:
    
    - Split-up the asm-generic/hw-breakpoint.h moving to
      linux/hw_breakpoint.h into a separate patch
    - Optimize the breakpoints restoring while switching from kvm guest
      to host. We only want to restore the state if we have active
      breakpoints to the host, otherwise we don't care about messed-up
      address registers.
    - Add asm/hw_breakpoint.h to Kbuild
    - Fix bad breakpoint type in trace_selftest.c
    
    Changes in v6:
    
    - Fix wrong header inclusion in trace.h (triggered a build
      error with CONFIG_FTRACE_SELFTEST
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index e09a44fc4664..0a979f3e5b8a 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -105,7 +105,6 @@ static void __save_processor_state(struct saved_context *ctxt)
 	ctxt->cr4 = read_cr4();
 	ctxt->cr8 = read_cr8();
 #endif
-	hw_breakpoint_disable();
 }
 
 /* Needed by apm.c */
@@ -144,11 +143,6 @@ static void fix_processor_context(void)
 #endif
 	load_TR_desc();				/* This does ltr */
 	load_LDT(&current->active_mm->context);	/* This does lldt */
-
-	/*
-	 * Now maybe reload the debug registers
-	 */
-	load_debug_registers();
 }
 
 /**

commit 0f8f86c7bdd1c954fbe153af437a0d91a6c5721a
Merge: dca2d6ac09d9 f39cdf25bf77
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Oct 18 01:09:09 2009 +0200

    Merge commit 'perf/core' into perf/hw-breakpoint
    
    Conflicts:
            kernel/Makefile
            kernel/trace/Makefile
            kernel/trace/trace.h
            samples/Makefile
    
    Merge reason: We need to be uptodate with the perf events development
    branch because we plan to rewrite the breakpoints API on top of
    perf events.

commit bc3eb7076b2b808f79abd4bd33b6a7feebb6f5d5
Author: Andi Kleen <andi@firstfloor.org>
Date:   Fri Sep 18 07:49:25 2009 +0200

    x86: Remove final bits of CONFIG_X86_OLD_MCE
    
    Caught by Linus.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    [ fixed up context conflict manually. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 417c9f5b4afa..8aa85f17667e 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -243,10 +243,6 @@ static void __restore_processor_state(struct saved_context *ctxt)
 
 	do_fpu_end();
 	mtrr_bp_restore();
-
-#ifdef CONFIG_X86_OLD_MCE
-	mcheck_init(&boot_cpu_data);
-#endif
 }
 
 /* Needed by apm.c */

commit a1922ed661ab2c1637d0b10cde933bd9cd33d965
Merge: 75e33751ca8b d28daf923ac5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 7 08:19:51 2009 +0200

    Merge branch 'tracing/core' into tracing/hw-breakpoints
    
    Conflicts:
            arch/Kconfig
            kernel/trace/trace.h
    
    Merge reason: resolve the conflicts, plus adopt to the new
                  ring-buffer APIs.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit d0af9eed5aa91b6b7b5049cae69e5ea956fd85c3
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed Aug 19 18:05:36 2009 -0700

    x86, pat/mtrr: Rendezvous all the cpus for MTRR/PAT init
    
    SDM Vol 3a section titled "MTRR considerations in MP systems" specifies
    the need for synchronizing the logical cpu's while initializing/updating
    MTRR.
    
    Currently Linux kernel does the synchronization of all cpu's only when
    a single MTRR register is programmed/updated. During an AP online
    (during boot/cpu-online/resume)  where we initialize all the MTRR/PAT registers,
    we don't follow this synchronization algorithm.
    
    This can lead to scenarios where during a dynamic cpu online, that logical cpu
    is initializing MTRR/PAT with cache disabled (cr0.cd=1) etc while other logical
    HT sibling continue to run (also with cache disabled because of cr0.cd=1
    on its sibling).
    
    Starting from Westmere, VMX transitions with cr0.cd=1 don't work properly
    (because of some VMX performance optimizations) and the above scenario
    (with one logical cpu doing VMX activity and another logical cpu coming online)
    can result in system crash.
    
    Fix the MTRR initialization by doing rendezvous of all the cpus. During
    boot and resume, we delay the MTRR/PAT init for APs till all the
    logical cpu's come online and the rendezvous process at the end of AP's bringup,
    will initialize the MTRR/PAT for all AP's.
    
    For dynamic single cpu online, we synchronize all the logical cpus and
    do the MTRR/PAT init on the AP that is coming online.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index b3d20b9cac63..417c9f5b4afa 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -242,7 +242,7 @@ static void __restore_processor_state(struct saved_context *ctxt)
 	fix_processor_context();
 
 	do_fpu_end();
-	mtrr_ap_init();
+	mtrr_bp_restore();
 
 #ifdef CONFIG_X86_OLD_MCE
 	mcheck_init(&boot_cpu_data);

commit 7262b6e4a4cc18d0f67df145d032c843e4bc382b
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Tue Jun 23 12:40:54 2009 +0900

    x86, mce: Fix mce resume on 32bit
    
    Calling mcheck_init() on resume is required only with
    CONFIG_X86_OLD_MCE=y.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index d277ef1eea51..b3d20b9cac63 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -244,7 +244,7 @@ static void __restore_processor_state(struct saved_context *ctxt)
 	do_fpu_end();
 	mtrr_ap_init();
 
-#ifdef CONFIG_X86_32
+#ifdef CONFIG_X86_OLD_MCE
 	mcheck_init(&boot_cpu_data);
 #endif
 }

commit eadb8a091b27a840de7450f84ecff5ef13476424
Merge: 73874005cd88 65795efbd380
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 17 12:52:15 2009 +0200

    Merge branch 'linus' into tracing/hw-breakpoints
    
    Conflicts:
            arch/x86/Kconfig
            arch/x86/kernel/traps.c
            arch/x86/power/cpu.c
            arch/x86/power/cpu_32.c
            kernel/Makefile
    
    Semantic conflict:
            arch/x86/kernel/hw_breakpoint.c
    
    Merge reason: Resolve the conflicts, move from put_cpu_no_sched() to
                  put_cpu() in arch/x86/kernel/hw_breakpoint.c.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 08687aec71bc9134fe336e561f6877bacf74fc0a
Author: Sergio Luis <sergio@larces.uece.br>
Date:   Tue Apr 28 00:27:22 2009 +0200

    x86: unify power/cpu_(32|64).c
    
    This is the last unification step. Here we do remove one of the files
    and rename the left one as cpu.c, as both are now the same.
    Also update power/Makefile, telling it to build cpu.o, instead of
    cpu_(32|64).o
    
    Signed-off-by: Sergio Luis <sergio@larces.uece.br>
    Signed-off-by: Lauro Salmito <laurosalmito@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
new file mode 100644
index 000000000000..d277ef1eea51
--- /dev/null
+++ b/arch/x86/power/cpu.c
@@ -0,0 +1,259 @@
+/*
+ * Suspend support specific for i386/x86-64.
+ *
+ * Distribute under GPLv2
+ *
+ * Copyright (c) 2007 Rafael J. Wysocki <rjw@sisk.pl>
+ * Copyright (c) 2002 Pavel Machek <pavel@suse.cz>
+ * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>
+ */
+
+#include <linux/suspend.h>
+#include <linux/smp.h>
+
+#include <asm/pgtable.h>
+#include <asm/proto.h>
+#include <asm/mtrr.h>
+#include <asm/page.h>
+#include <asm/mce.h>
+#include <asm/xcr.h>
+#include <asm/suspend.h>
+
+#ifdef CONFIG_X86_32
+static struct saved_context saved_context;
+
+unsigned long saved_context_ebx;
+unsigned long saved_context_esp, saved_context_ebp;
+unsigned long saved_context_esi, saved_context_edi;
+unsigned long saved_context_eflags;
+#else
+/* CONFIG_X86_64 */
+struct saved_context saved_context;
+#endif
+
+/**
+ *	__save_processor_state - save CPU registers before creating a
+ *		hibernation image and before restoring the memory state from it
+ *	@ctxt - structure to store the registers contents in
+ *
+ *	NOTE: If there is a CPU register the modification of which by the
+ *	boot kernel (ie. the kernel used for loading the hibernation image)
+ *	might affect the operations of the restored target kernel (ie. the one
+ *	saved in the hibernation image), then its contents must be saved by this
+ *	function.  In other words, if kernel A is hibernated and different
+ *	kernel B is used for loading the hibernation image into memory, the
+ *	kernel A's __save_processor_state() function must save all registers
+ *	needed by kernel A, so that it can operate correctly after the resume
+ *	regardless of what kernel B does in the meantime.
+ */
+static void __save_processor_state(struct saved_context *ctxt)
+{
+#ifdef CONFIG_X86_32
+	mtrr_save_fixed_ranges(NULL);
+#endif
+	kernel_fpu_begin();
+
+	/*
+	 * descriptor tables
+	 */
+#ifdef CONFIG_X86_32
+	store_gdt(&ctxt->gdt);
+	store_idt(&ctxt->idt);
+#else
+/* CONFIG_X86_64 */
+	store_gdt((struct desc_ptr *)&ctxt->gdt_limit);
+	store_idt((struct desc_ptr *)&ctxt->idt_limit);
+#endif
+	store_tr(ctxt->tr);
+
+	/* XMM0..XMM15 should be handled by kernel_fpu_begin(). */
+	/*
+	 * segment registers
+	 */
+#ifdef CONFIG_X86_32
+	savesegment(es, ctxt->es);
+	savesegment(fs, ctxt->fs);
+	savesegment(gs, ctxt->gs);
+	savesegment(ss, ctxt->ss);
+#else
+/* CONFIG_X86_64 */
+	asm volatile ("movw %%ds, %0" : "=m" (ctxt->ds));
+	asm volatile ("movw %%es, %0" : "=m" (ctxt->es));
+	asm volatile ("movw %%fs, %0" : "=m" (ctxt->fs));
+	asm volatile ("movw %%gs, %0" : "=m" (ctxt->gs));
+	asm volatile ("movw %%ss, %0" : "=m" (ctxt->ss));
+
+	rdmsrl(MSR_FS_BASE, ctxt->fs_base);
+	rdmsrl(MSR_GS_BASE, ctxt->gs_base);
+	rdmsrl(MSR_KERNEL_GS_BASE, ctxt->gs_kernel_base);
+	mtrr_save_fixed_ranges(NULL);
+
+	rdmsrl(MSR_EFER, ctxt->efer);
+#endif
+
+	/*
+	 * control registers
+	 */
+	ctxt->cr0 = read_cr0();
+	ctxt->cr2 = read_cr2();
+	ctxt->cr3 = read_cr3();
+#ifdef CONFIG_X86_32
+	ctxt->cr4 = read_cr4_safe();
+#else
+/* CONFIG_X86_64 */
+	ctxt->cr4 = read_cr4();
+	ctxt->cr8 = read_cr8();
+#endif
+}
+
+/* Needed by apm.c */
+void save_processor_state(void)
+{
+	__save_processor_state(&saved_context);
+}
+#ifdef CONFIG_X86_32
+EXPORT_SYMBOL(save_processor_state);
+#endif
+
+static void do_fpu_end(void)
+{
+	/*
+	 * Restore FPU regs if necessary.
+	 */
+	kernel_fpu_end();
+}
+
+static void fix_processor_context(void)
+{
+	int cpu = smp_processor_id();
+	struct tss_struct *t = &per_cpu(init_tss, cpu);
+
+	set_tss_desc(cpu, t);	/*
+				 * This just modifies memory; should not be
+				 * necessary. But... This is necessary, because
+				 * 386 hardware has concept of busy TSS or some
+				 * similar stupidity.
+				 */
+
+#ifdef CONFIG_X86_64
+	get_cpu_gdt_table(cpu)[GDT_ENTRY_TSS].type = 9;
+
+	syscall_init();				/* This sets MSR_*STAR and related */
+#endif
+	load_TR_desc();				/* This does ltr */
+	load_LDT(&current->active_mm->context);	/* This does lldt */
+
+	/*
+	 * Now maybe reload the debug registers
+	 */
+	if (current->thread.debugreg7) {
+#ifdef CONFIG_X86_32
+		set_debugreg(current->thread.debugreg0, 0);
+		set_debugreg(current->thread.debugreg1, 1);
+		set_debugreg(current->thread.debugreg2, 2);
+		set_debugreg(current->thread.debugreg3, 3);
+		/* no 4 and 5 */
+		set_debugreg(current->thread.debugreg6, 6);
+		set_debugreg(current->thread.debugreg7, 7);
+#else
+		/* CONFIG_X86_64 */
+		loaddebug(&current->thread, 0);
+		loaddebug(&current->thread, 1);
+		loaddebug(&current->thread, 2);
+		loaddebug(&current->thread, 3);
+		/* no 4 and 5 */
+		loaddebug(&current->thread, 6);
+		loaddebug(&current->thread, 7);
+#endif
+	}
+
+}
+
+/**
+ *	__restore_processor_state - restore the contents of CPU registers saved
+ *		by __save_processor_state()
+ *	@ctxt - structure to load the registers contents from
+ */
+static void __restore_processor_state(struct saved_context *ctxt)
+{
+	/*
+	 * control registers
+	 */
+	/* cr4 was introduced in the Pentium CPU */
+#ifdef CONFIG_X86_32
+	if (ctxt->cr4)
+		write_cr4(ctxt->cr4);
+#else
+/* CONFIG X86_64 */
+	wrmsrl(MSR_EFER, ctxt->efer);
+	write_cr8(ctxt->cr8);
+	write_cr4(ctxt->cr4);
+#endif
+	write_cr3(ctxt->cr3);
+	write_cr2(ctxt->cr2);
+	write_cr0(ctxt->cr0);
+
+	/*
+	 * now restore the descriptor tables to their proper values
+	 * ltr is done i fix_processor_context().
+	 */
+#ifdef CONFIG_X86_32
+	load_gdt(&ctxt->gdt);
+	load_idt(&ctxt->idt);
+#else
+/* CONFIG_X86_64 */
+	load_gdt((const struct desc_ptr *)&ctxt->gdt_limit);
+	load_idt((const struct desc_ptr *)&ctxt->idt_limit);
+#endif
+
+	/*
+	 * segment registers
+	 */
+#ifdef CONFIG_X86_32
+	loadsegment(es, ctxt->es);
+	loadsegment(fs, ctxt->fs);
+	loadsegment(gs, ctxt->gs);
+	loadsegment(ss, ctxt->ss);
+
+	/*
+	 * sysenter MSRs
+	 */
+	if (boot_cpu_has(X86_FEATURE_SEP))
+		enable_sep_cpu();
+#else
+/* CONFIG_X86_64 */
+	asm volatile ("movw %0, %%ds" :: "r" (ctxt->ds));
+	asm volatile ("movw %0, %%es" :: "r" (ctxt->es));
+	asm volatile ("movw %0, %%fs" :: "r" (ctxt->fs));
+	load_gs_index(ctxt->gs);
+	asm volatile ("movw %0, %%ss" :: "r" (ctxt->ss));
+
+	wrmsrl(MSR_FS_BASE, ctxt->fs_base);
+	wrmsrl(MSR_GS_BASE, ctxt->gs_base);
+	wrmsrl(MSR_KERNEL_GS_BASE, ctxt->gs_kernel_base);
+#endif
+
+	/*
+	 * restore XCR0 for xsave capable cpu's.
+	 */
+	if (cpu_has_xsave)
+		xsetbv(XCR_XFEATURE_ENABLED_MASK, pcntxt_mask);
+
+	fix_processor_context();
+
+	do_fpu_end();
+	mtrr_ap_init();
+
+#ifdef CONFIG_X86_32
+	mcheck_init(&boot_cpu_data);
+#endif
+}
+
+/* Needed by apm.c */
+void restore_processor_state(void)
+{
+	__restore_processor_state(&saved_context);
+}
+#ifdef CONFIG_X86_32
+EXPORT_SYMBOL(restore_processor_state);
+#endif

commit c57591244a08bb441c83472f5c110151bb7c2cc6
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sat Feb 9 23:24:09 2008 +0100

    x86 PM: rename 32-bit files in arch/x86/power
    
    Rename cpu.c, suspend.c and swsusp.S in arch/x86/power to cpu_32.c,
    hibernate_32.c and hibernate_asm_32.S, respectively, and update the
    purpose and copyright information in these files.
    
    Update the Makefile in arch/x86/power to reflect the above changes.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
deleted file mode 100644
index efcf620d1439..000000000000
--- a/arch/x86/power/cpu.c
+++ /dev/null
@@ -1,133 +0,0 @@
-/*
- * Suspend support specific for i386.
- *
- * Distribute under GPLv2
- *
- * Copyright (c) 2002 Pavel Machek <pavel@suse.cz>
- * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>
- */
-
-#include <linux/module.h>
-#include <linux/suspend.h>
-#include <asm/mtrr.h>
-#include <asm/mce.h>
-
-static struct saved_context saved_context;
-
-unsigned long saved_context_ebx;
-unsigned long saved_context_esp, saved_context_ebp;
-unsigned long saved_context_esi, saved_context_edi;
-unsigned long saved_context_eflags;
-
-static void __save_processor_state(struct saved_context *ctxt)
-{
-	mtrr_save_fixed_ranges(NULL);
-	kernel_fpu_begin();
-
-	/*
-	 * descriptor tables
-	 */
- 	store_gdt(&ctxt->gdt);
- 	store_idt(&ctxt->idt);
- 	store_tr(ctxt->tr);
-
-	/*
-	 * segment registers
-	 */
- 	savesegment(es, ctxt->es);
- 	savesegment(fs, ctxt->fs);
- 	savesegment(gs, ctxt->gs);
- 	savesegment(ss, ctxt->ss);
-
-	/*
-	 * control registers 
-	 */
-	ctxt->cr0 = read_cr0();
-	ctxt->cr2 = read_cr2();
-	ctxt->cr3 = read_cr3();
-	ctxt->cr4 = read_cr4();
-}
-
-void save_processor_state(void)
-{
-	__save_processor_state(&saved_context);
-}
-
-static void do_fpu_end(void)
-{
-	/*
-	 * Restore FPU regs if necessary.
-	 */
-	kernel_fpu_end();
-}
-
-static void fix_processor_context(void)
-{
-	int cpu = smp_processor_id();
-	struct tss_struct * t = &per_cpu(init_tss, cpu);
-
-	set_tss_desc(cpu,t);	/* This just modifies memory; should not be necessary. But... This is necessary, because 386 hardware has concept of busy TSS or some similar stupidity. */
-
-	load_TR_desc();				/* This does ltr */
-	load_LDT(&current->active_mm->context);	/* This does lldt */
-
-	/*
-	 * Now maybe reload the debug registers
-	 */
-	if (current->thread.debugreg7) {
-		set_debugreg(current->thread.debugreg0, 0);
-		set_debugreg(current->thread.debugreg1, 1);
-		set_debugreg(current->thread.debugreg2, 2);
-		set_debugreg(current->thread.debugreg3, 3);
-		/* no 4 and 5 */
-		set_debugreg(current->thread.debugreg6, 6);
-		set_debugreg(current->thread.debugreg7, 7);
-	}
-
-}
-
-static void __restore_processor_state(struct saved_context *ctxt)
-{
-	/*
-	 * control registers
-	 */
-	write_cr4(ctxt->cr4);
-	write_cr3(ctxt->cr3);
-	write_cr2(ctxt->cr2);
-	write_cr0(ctxt->cr0);
-
-	/*
-	 * now restore the descriptor tables to their proper values
-	 * ltr is done i fix_processor_context().
-	 */
- 	load_gdt(&ctxt->gdt);
- 	load_idt(&ctxt->idt);
-
-	/*
-	 * segment registers
-	 */
- 	loadsegment(es, ctxt->es);
- 	loadsegment(fs, ctxt->fs);
- 	loadsegment(gs, ctxt->gs);
- 	loadsegment(ss, ctxt->ss);
-
-	/*
-	 * sysenter MSRs
-	 */
-	if (boot_cpu_has(X86_FEATURE_SEP))
-		enable_sep_cpu();
-
-	fix_processor_context();
-	do_fpu_end();
-	mtrr_ap_init();
-	mcheck_init(&boot_cpu_data);
-}
-
-void restore_processor_state(void)
-{
-	__restore_processor_state(&saved_context);
-}
-
-/* Needed by apm.c */
-EXPORT_SYMBOL(save_processor_state);
-EXPORT_SYMBOL(restore_processor_state);

commit cae4595764cb3b08f6517e99bac1e3862854b1a1
Author: Jan Beulich <jbeulich@novell.com>
Date:   Wed Jan 30 13:31:23 2008 +0100

    x86: make __{save,restore}_processor_state static
    
    .. allowing to remove their declarations from a global include file
    (the symbols don't exist for anything but x86).
    
    Likewise for 64-bits' fix_processor_context(), just that that one was
    properly declared in an arch-specific header.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 5a98dc35addf..efcf620d1439 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -19,7 +19,7 @@ unsigned long saved_context_esp, saved_context_ebp;
 unsigned long saved_context_esi, saved_context_edi;
 unsigned long saved_context_eflags;
 
-void __save_processor_state(struct saved_context *ctxt)
+static void __save_processor_state(struct saved_context *ctxt)
 {
 	mtrr_save_fixed_ranges(NULL);
 	kernel_fpu_begin();
@@ -86,7 +86,7 @@ static void fix_processor_context(void)
 
 }
 
-void __restore_processor_state(struct saved_context *ctxt)
+static void __restore_processor_state(struct saved_context *ctxt)
 {
 	/*
 	 * control registers

commit 0f5340933f9bacb403f49baaf8073320e3984841
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:30:59 2008 +0100

    x86: x86-32 thread_struct.debugreg
    
    This replaces the debugreg[7] member of thread_struct with individual
    members debugreg0, etc.  This saves two words for the dummies 4 and 5,
    and harmonizes the code between 32 and 64.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
index 998fd3ec0d68..5a98dc35addf 100644
--- a/arch/x86/power/cpu.c
+++ b/arch/x86/power/cpu.c
@@ -74,14 +74,14 @@ static void fix_processor_context(void)
 	/*
 	 * Now maybe reload the debug registers
 	 */
-	if (current->thread.debugreg[7]){
-		set_debugreg(current->thread.debugreg[0], 0);
-		set_debugreg(current->thread.debugreg[1], 1);
-		set_debugreg(current->thread.debugreg[2], 2);
-		set_debugreg(current->thread.debugreg[3], 3);
+	if (current->thread.debugreg7) {
+		set_debugreg(current->thread.debugreg0, 0);
+		set_debugreg(current->thread.debugreg1, 1);
+		set_debugreg(current->thread.debugreg2, 2);
+		set_debugreg(current->thread.debugreg3, 3);
 		/* no 4 and 5 */
-		set_debugreg(current->thread.debugreg[6], 6);
-		set_debugreg(current->thread.debugreg[7], 7);
+		set_debugreg(current->thread.debugreg6, 6);
+		set_debugreg(current->thread.debugreg7, 7);
 	}
 
 }

commit 4b60eb8380a0b588a03b6052d7ac93e1964c75b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:34 2007 +0200

    i386: move power
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c
new file mode 100644
index 000000000000..998fd3ec0d68
--- /dev/null
+++ b/arch/x86/power/cpu.c
@@ -0,0 +1,133 @@
+/*
+ * Suspend support specific for i386.
+ *
+ * Distribute under GPLv2
+ *
+ * Copyright (c) 2002 Pavel Machek <pavel@suse.cz>
+ * Copyright (c) 2001 Patrick Mochel <mochel@osdl.org>
+ */
+
+#include <linux/module.h>
+#include <linux/suspend.h>
+#include <asm/mtrr.h>
+#include <asm/mce.h>
+
+static struct saved_context saved_context;
+
+unsigned long saved_context_ebx;
+unsigned long saved_context_esp, saved_context_ebp;
+unsigned long saved_context_esi, saved_context_edi;
+unsigned long saved_context_eflags;
+
+void __save_processor_state(struct saved_context *ctxt)
+{
+	mtrr_save_fixed_ranges(NULL);
+	kernel_fpu_begin();
+
+	/*
+	 * descriptor tables
+	 */
+ 	store_gdt(&ctxt->gdt);
+ 	store_idt(&ctxt->idt);
+ 	store_tr(ctxt->tr);
+
+	/*
+	 * segment registers
+	 */
+ 	savesegment(es, ctxt->es);
+ 	savesegment(fs, ctxt->fs);
+ 	savesegment(gs, ctxt->gs);
+ 	savesegment(ss, ctxt->ss);
+
+	/*
+	 * control registers 
+	 */
+	ctxt->cr0 = read_cr0();
+	ctxt->cr2 = read_cr2();
+	ctxt->cr3 = read_cr3();
+	ctxt->cr4 = read_cr4();
+}
+
+void save_processor_state(void)
+{
+	__save_processor_state(&saved_context);
+}
+
+static void do_fpu_end(void)
+{
+	/*
+	 * Restore FPU regs if necessary.
+	 */
+	kernel_fpu_end();
+}
+
+static void fix_processor_context(void)
+{
+	int cpu = smp_processor_id();
+	struct tss_struct * t = &per_cpu(init_tss, cpu);
+
+	set_tss_desc(cpu,t);	/* This just modifies memory; should not be necessary. But... This is necessary, because 386 hardware has concept of busy TSS or some similar stupidity. */
+
+	load_TR_desc();				/* This does ltr */
+	load_LDT(&current->active_mm->context);	/* This does lldt */
+
+	/*
+	 * Now maybe reload the debug registers
+	 */
+	if (current->thread.debugreg[7]){
+		set_debugreg(current->thread.debugreg[0], 0);
+		set_debugreg(current->thread.debugreg[1], 1);
+		set_debugreg(current->thread.debugreg[2], 2);
+		set_debugreg(current->thread.debugreg[3], 3);
+		/* no 4 and 5 */
+		set_debugreg(current->thread.debugreg[6], 6);
+		set_debugreg(current->thread.debugreg[7], 7);
+	}
+
+}
+
+void __restore_processor_state(struct saved_context *ctxt)
+{
+	/*
+	 * control registers
+	 */
+	write_cr4(ctxt->cr4);
+	write_cr3(ctxt->cr3);
+	write_cr2(ctxt->cr2);
+	write_cr0(ctxt->cr0);
+
+	/*
+	 * now restore the descriptor tables to their proper values
+	 * ltr is done i fix_processor_context().
+	 */
+ 	load_gdt(&ctxt->gdt);
+ 	load_idt(&ctxt->idt);
+
+	/*
+	 * segment registers
+	 */
+ 	loadsegment(es, ctxt->es);
+ 	loadsegment(fs, ctxt->fs);
+ 	loadsegment(gs, ctxt->gs);
+ 	loadsegment(ss, ctxt->ss);
+
+	/*
+	 * sysenter MSRs
+	 */
+	if (boot_cpu_has(X86_FEATURE_SEP))
+		enable_sep_cpu();
+
+	fix_processor_context();
+	do_fpu_end();
+	mtrr_ap_init();
+	mcheck_init(&boot_cpu_data);
+}
+
+void restore_processor_state(void)
+{
+	__restore_processor_state(&saved_context);
+}
+
+/* Needed by apm.c */
+EXPORT_SYMBOL(save_processor_state);
+EXPORT_SYMBOL(restore_processor_state);
