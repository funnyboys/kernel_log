commit e4553b4976d1178c13da295cb5c7b21f55baf8f9
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Jun 16 20:41:23 2020 -0700

    KVM: VMX: Remove vcpu_vmx's defunct copy of host_pkru
    
    Remove vcpu_vmx.host_pkru, which got left behind when PKRU support was
    moved to common x86 code.
    
    No functional change intended.
    
    Fixes: 37486135d3a7b ("KVM: x86: Fix pkru save/restore when guest CR4.PKE=0, move it to x86.c")
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200617034123.25647-1-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 8a83b5edc820..639798e4a6ca 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -288,8 +288,6 @@ struct vcpu_vmx {
 
 	u64 current_tsc_ratio;
 
-	u32 host_pkru;
-
 	unsigned long host_debugctlmsr;
 
 	/*

commit 7a35e515a7055f483f87d12041c41db11b36c9ee
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Fri Jun 5 13:59:05 2020 +0200

    KVM: VMX: Properly handle kvm_read/write_guest_virt*() result
    
    Syzbot reports the following issue:
    
    WARNING: CPU: 0 PID: 6819 at arch/x86/kvm/x86.c:618
     kvm_inject_emulated_page_fault+0x210/0x290 arch/x86/kvm/x86.c:618
    ...
    Call Trace:
    ...
    RIP: 0010:kvm_inject_emulated_page_fault+0x210/0x290 arch/x86/kvm/x86.c:618
    ...
     nested_vmx_get_vmptr+0x1f9/0x2a0 arch/x86/kvm/vmx/nested.c:4638
     handle_vmon arch/x86/kvm/vmx/nested.c:4767 [inline]
     handle_vmon+0x168/0x3a0 arch/x86/kvm/vmx/nested.c:4728
     vmx_handle_exit+0x29c/0x1260 arch/x86/kvm/vmx/vmx.c:6067
    
    'exception' we're trying to inject with kvm_inject_emulated_page_fault()
    comes from:
    
      nested_vmx_get_vmptr()
       kvm_read_guest_virt()
         kvm_read_guest_virt_helper()
           vcpu->arch.walk_mmu->gva_to_gpa()
    
    but it is only set when GVA to GPA conversion fails. In case it doesn't but
    we still fail kvm_vcpu_read_guest_page(), X86EMUL_IO_NEEDED is returned and
    nested_vmx_get_vmptr() calls kvm_inject_emulated_page_fault() with zeroed
    'exception'. This happen when the argument is MMIO.
    
    Paolo also noticed that nested_vmx_get_vmptr() is not the only place in
    KVM code where kvm_read/write_guest_virt*() return result is mishandled.
    VMX instructions along with INVPCID have the same issue. This was already
    noticed before, e.g. see commit 541ab2aeb282 ("KVM: x86: work around
    leak of uninitialized stack contents") but was never fully fixed.
    
    KVM could've handled the request correctly by going to userspace and
    performing I/O but there doesn't seem to be a good need for such requests
    in the first place.
    
    Introduce vmx_handle_memory_failure() as an interim solution.
    
    Note, nested_vmx_get_vmptr() now has three possible outcomes: OK, PF,
    KVM_EXIT_INTERNAL_ERROR and callers need to know if userspace exit is
    needed (for KVM_EXIT_INTERNAL_ERROR) in case of failure. We don't seem
    to have a good enum describing this tristate, just add "int *ret" to
    nested_vmx_get_vmptr() interface to pass the information.
    
    Reported-by: syzbot+2a7156e11dc199bdbd8a@syzkaller.appspotmail.com
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200605115906.532682-1-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 672c28f17e49..8a83b5edc820 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -355,6 +355,8 @@ struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr);
 void pt_update_intercept_for_msr(struct vcpu_vmx *vmx);
 void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp);
 int vmx_find_msr_index(struct vmx_msrs *m, u32 msr);
+int vmx_handle_memory_failure(struct kvm_vcpu *vcpu, int r,
+			      struct x86_exception *e);
 
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1

commit 850448f35aaf45215276468d63c91ab1e230cf06
Author: Peter Shier <pshier@google.com>
Date:   Tue May 26 14:51:06 2020 -0700

    KVM: nVMX: Fix VMX preemption timer migration
    
    Add new field to hold preemption timer expiration deadline
    appended to struct kvm_vmx_nested_state_hdr. This is to prevent
    the first VM-Enter after migration from incorrectly restarting the timer
    with the full timer value instead of partially decayed timer value.
    KVM_SET_NESTED_STATE restarts timer using migrated state regardless
    of whether L1 sets VM_EXIT_SAVE_VMX_PREEMPTION_TIMER.
    
    Fixes: cf8b84f48a593 ("kvm: nVMX: Prepare for checkpointing L2 state")
    
    Signed-off-by: Peter Shier <pshier@google.com>
    Signed-off-by: Makarand Sonare <makarandsonare@google.com>
    Message-Id: <20200526215107.205814-2-makarandsonare@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 298ddef79d00..672c28f17e49 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -169,6 +169,8 @@ struct nested_vmx {
 	u16 posted_intr_nv;
 
 	struct hrtimer preemption_timer;
+	u64 preemption_timer_deadline;
+	bool has_preemption_timer_deadline;
 	bool preemption_timer_expired;
 
 	/* to migrate it to L2 if VM_ENTRY_LOAD_DEBUG_CONTROLS is off */

commit bd31fe495d0d1a67fe6f44f06dfef637f202241d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri May 1 21:32:31 2020 -0700

    KVM: VMX: Add proper cache tracking for CR0
    
    Move CR0 caching into the standard register caching mechanism in order
    to take advantage of the availability checks provided by regs_avail.
    This avoids multiple VMREADs in the (uncommon) case where kvm_read_cr0()
    is called multiple times in a single VM-Exit, and more importantly
    eliminates a kvm_x86_ops hook, saves a retpoline on SVM when reading
    CR0, and squashes the confusing naming discrepancy of "cache_reg" vs.
    "decache_cr0_guest_bits".
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200502043234.12481-8-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 04bb557acdd2..298ddef79d00 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -452,6 +452,7 @@ static inline void vmx_register_cache_reset(struct kvm_vcpu *vcpu)
 				  | (1 << VCPU_EXREG_RFLAGS)
 				  | (1 << VCPU_EXREG_PDPTR)
 				  | (1 << VCPU_EXREG_SEGMENTS)
+				  | (1 << VCPU_EXREG_CR0)
 				  | (1 << VCPU_EXREG_CR3)
 				  | (1 << VCPU_EXREG_CR4)
 				  | (1 << VCPU_EXREG_EXIT_INFO_1)

commit f98c1e77127de7d9ff558570c25d02ef077df50f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri May 1 21:32:30 2020 -0700

    KVM: VMX: Add proper cache tracking for CR4
    
    Move CR4 caching into the standard register caching mechanism in order
    to take advantage of the availability checks provided by regs_avail.
    This avoids multiple VMREADs and retpolines (when configured) during
    nested VMX transitions as kvm_read_cr4_bits() is invoked multiple times
    on each transition, e.g. when stuffing CR0 and CR3.
    
    As an added bonus, this eliminates a kvm_x86_ops hook, saves a retpoline
    on SVM when reading CR4, and squashes the confusing naming discrepancy
    of "cache_reg" vs. "decache_cr4_guest_bits".
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200502043234.12481-7-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 373674d455e1..04bb557acdd2 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -453,6 +453,7 @@ static inline void vmx_register_cache_reset(struct kvm_vcpu *vcpu)
 				  | (1 << VCPU_EXREG_PDPTR)
 				  | (1 << VCPU_EXREG_SEGMENTS)
 				  | (1 << VCPU_EXREG_CR3)
+				  | (1 << VCPU_EXREG_CR4)
 				  | (1 << VCPU_EXREG_EXIT_INFO_1)
 				  | (1 << VCPU_EXREG_EXIT_INFO_2));
 	vcpu->arch.regs_dirty = 0;

commit 1af1bb05625bcdd09522f416b62bcc72cc2fdd3b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed May 6 16:58:50 2020 -0700

    KVM: nVMX: Skip IBPB when temporarily switching between vmcs01 and vmcs02
    
    Skip the Indirect Branch Prediction Barrier that is triggered on a VMCS
    switch when temporarily loading vmcs02 to synchronize it to vmcs12, i.e.
    give copy_vmcs02_to_vmcs12_rare() the same treatment as
    vmx_switch_vmcs().
    
    Make vmx_vcpu_load() static now that it's only referenced within vmx.c.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200506235850.22600-3-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index d3d48acc6bd9..373674d455e1 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -322,7 +322,6 @@ struct kvm_vmx {
 bool nested_vmx_allowed(struct kvm_vcpu *vcpu);
 void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 			struct loaded_vmcs *buddy);
-void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 int allocate_vpid(void);
 void free_vpid(int vpid);
 void vmx_set_constant_host_state(struct vcpu_vmx *vmx);

commit 5c911beff20aa8639e7a1f28988736c13e03ed54
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri May 1 09:31:17 2020 -0700

    KVM: nVMX: Skip IBPB when switching between vmcs01 and vmcs02
    
    Skip the Indirect Branch Prediction Barrier that is triggered on a VMCS
    switch when running with spectre_v2_user=on/auto if the switch is
    between two VMCSes in the same guest, i.e. between vmcs01 and vmcs02.
    The IBPB is intended to prevent one guest from attacking another, which
    is unnecessary in the nested case as it's the same guest from KVM's
    perspective.
    
    This all but eliminates the overhead observed for nested VMX transitions
    when running with CONFIG_RETPOLINE=y and spectre_v2_user=on/auto, which
    can be significant, e.g. roughly 3x on current systems.
    
    Reported-by: Alexander Graf <graf@amazon.com>
    Cc: KarimAllah Raslan <karahmed@amazon.de>
    Cc: stable@vger.kernel.org
    Fixes: 15d45071523d ("KVM/x86: Add IBPB support")
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200501163117.4655-1-sean.j.christopherson@intel.com>
    [Invert direction of bool argument. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index b5e773267abe..d3d48acc6bd9 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -320,7 +320,8 @@ struct kvm_vmx {
 };
 
 bool nested_vmx_allowed(struct kvm_vcpu *vcpu);
-void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu);
+void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
+			struct loaded_vmcs *buddy);
 void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 int allocate_vpid(void);
 void free_vpid(int vpid);

commit 1b660b6baaafd8b9056740b83decd7fc74023627
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 22 19:25:44 2020 -0700

    KVM: VMX: Split out architectural interrupt/NMI blocking checks
    
    Move the architectural (non-KVM specific) interrupt/NMI blocking checks
    to a separate helper so that they can be used in a future patch by
    vmx_check_nested_events().
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200423022550.15113-8-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index edfb739e5907..b5e773267abe 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -344,6 +344,8 @@ void vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);
 void update_exception_bitmap(struct kvm_vcpu *vcpu);
 void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu);
+bool vmx_nmi_blocked(struct kvm_vcpu *vcpu);
+bool vmx_interrupt_blocked(struct kvm_vcpu *vcpu);
 bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu);
 void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked);
 void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu);

commit 8791585837f659943936b8e1cce9d039436ad1ca
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 15 13:34:54 2020 -0700

    KVM: VMX: Cache vmcs.EXIT_INTR_INFO using arch avail_reg flags
    
    Introduce a new "extended register" type, EXIT_INFO_2 (to pair with the
    nomenclature in .get_exit_info()), and use it to cache VMX's
    vmcs.EXIT_INTR_INFO.  Drop a comment in vmx_recover_nmi_blocking() that
    is obsoleted by the generic caching mechanism.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200415203454.8296-6-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index a13eafec67fc..edfb739e5907 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -451,7 +451,8 @@ static inline void vmx_register_cache_reset(struct kvm_vcpu *vcpu)
 				  | (1 << VCPU_EXREG_PDPTR)
 				  | (1 << VCPU_EXREG_SEGMENTS)
 				  | (1 << VCPU_EXREG_CR3)
-				  | (1 << VCPU_EXREG_EXIT_INFO_1));
+				  | (1 << VCPU_EXREG_EXIT_INFO_1)
+				  | (1 << VCPU_EXREG_EXIT_INFO_2));
 	vcpu->arch.regs_dirty = 0;
 }
 
@@ -506,6 +507,17 @@ static inline unsigned long vmx_get_exit_qual(struct kvm_vcpu *vcpu)
 	return vmx->exit_qualification;
 }
 
+static inline u32 vmx_get_intr_info(struct kvm_vcpu *vcpu)
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	if (!kvm_register_is_available(vcpu, VCPU_EXREG_EXIT_INFO_2)) {
+		kvm_register_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_2);
+		vmx->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+	}
+	return vmx->exit_intr_info;
+}
+
 struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu, gfp_t flags);
 void free_vmcs(struct vmcs *vmcs);
 int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);

commit 5addc235199f15ae964e7ac6b20cf43f4a661573
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 15 13:34:53 2020 -0700

    KVM: VMX: Cache vmcs.EXIT_QUALIFICATION using arch avail_reg flags
    
    Introduce a new "extended register" type, EXIT_INFO_1 (to pair with the
    nomenclature in .get_exit_info()), and use it to cache VMX's
    vmcs.EXIT_QUALIFICATION.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200415203454.8296-5-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 78c99a60fa49..a13eafec67fc 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -210,6 +210,7 @@ struct vcpu_vmx {
 	 */
 	bool		      guest_state_loaded;
 
+	unsigned long         exit_qualification;
 	u32                   exit_intr_info;
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
@@ -449,7 +450,8 @@ static inline void vmx_register_cache_reset(struct kvm_vcpu *vcpu)
 				  | (1 << VCPU_EXREG_RFLAGS)
 				  | (1 << VCPU_EXREG_PDPTR)
 				  | (1 << VCPU_EXREG_SEGMENTS)
-				  | (1 << VCPU_EXREG_CR3));
+				  | (1 << VCPU_EXREG_CR3)
+				  | (1 << VCPU_EXREG_EXIT_INFO_1));
 	vcpu->arch.regs_dirty = 0;
 }
 
@@ -493,6 +495,17 @@ static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 	return &(to_vmx(vcpu)->pi_desc);
 }
 
+static inline unsigned long vmx_get_exit_qual(struct kvm_vcpu *vcpu)
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	if (!kvm_register_is_available(vcpu, VCPU_EXREG_EXIT_INFO_1)) {
+		kvm_register_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_1);
+		vmx->exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
+	}
+	return vmx->exit_qualification;
+}
+
 struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu, gfp_t flags);
 void free_vmcs(struct vmcs *vmcs);
 int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);

commit ec0241f3bbe155a58455ce4a6057be5db6529b0f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 15 13:34:52 2020 -0700

    KVM: nVMX: Drop manual clearing of segment cache on nested VMCS switch
    
    Drop the call to vmx_segment_cache_clear() in vmx_switch_vmcs() now that
    the entire register cache is reset when switching the active VMCS, e.g.
    vmx_segment_cache_test_set() will reset the segment cache due to
    VCPU_EXREG_SEGMENTS being unavailable.
    
    Move vmx_segment_cache_clear() to vmx.c now that it's no longer invoked
    by the nested code.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200415203454.8296-4-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 6b668b604898..78c99a60fa49 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -443,11 +443,6 @@ BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL)
 BUILD_CONTROLS_SHADOW(exec, CPU_BASED_VM_EXEC_CONTROL)
 BUILD_CONTROLS_SHADOW(secondary_exec, SECONDARY_VM_EXEC_CONTROL)
 
-static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
-{
-	vmx->segment_cache.bitmask = 0;
-}
-
 static inline void vmx_register_cache_reset(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)

commit e5d03de5937e915c8e41b6357c337529dfae6797
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 15 13:34:51 2020 -0700

    KVM: nVMX: Reset register cache (available and dirty masks) on VMCS switch
    
    Reset the per-vCPU available and dirty register masks when switching
    between vmcs01 and vmcs02, as the masks track state relative to the
    current VMCS.  The stale masks don't cause problems in the current code
    base because the registers are either unconditionally written on nested
    transitions or, in the case of segment registers, have an additional
    tracker that is manually reset.
    
    Note, by dropping (previously implicitly, now explicitly) the dirty mask
    when switching the active VMCS, KVM is technically losing writes to the
    associated fields.  But, the only regs that can be dirtied (RIP, RSP and
    PDPTRs) are unconditionally written on nested transitions, e.g. explicit
    writeback is a waste of cycles, and a WARN_ON would be rather pointless.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200415203454.8296-3-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 31d7252df163..6b668b604898 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -8,6 +8,7 @@
 #include <asm/intel_pt.h>
 
 #include "capabilities.h"
+#include "kvm_cache_regs.h"
 #include "ops.h"
 #include "vmcs.h"
 
@@ -447,6 +448,16 @@ static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 	vmx->segment_cache.bitmask = 0;
 }
 
+static inline void vmx_register_cache_reset(struct kvm_vcpu *vcpu)
+{
+	vcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)
+				  | (1 << VCPU_EXREG_RFLAGS)
+				  | (1 << VCPU_EXREG_PDPTR)
+				  | (1 << VCPU_EXREG_SEGMENTS)
+				  | (1 << VCPU_EXREG_CR3));
+	vcpu->arch.regs_dirty = 0;
+}
+
 static inline u32 vmx_vmentry_ctrl(void)
 {
 	u32 vmentry_ctrl = vmcs_config.vmentry_ctrl;

commit 1196cb970b996be69a2fcd9756117b394f8e7526
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:23 2020 -0700

    KVM: nVMX: Reload APIC access page on nested VM-Exit only if necessary
    
    Defer reloading L1's APIC page by logging the need for a reload and
    processing it during nested VM-Exit instead of unconditionally reloading
    the APIC page on nested VM-Exit.  This eliminates a TLB flush on the
    majority of VM-Exits as the APIC page rarely needs to be reloaded.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-28-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 4c7b0713b438..31d7252df163 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -136,6 +136,7 @@ struct nested_vmx {
 	bool vmcs02_initialized;
 
 	bool change_vmcs01_virtual_apic_mode;
+	bool reload_vmcs01_apic_access_page;
 
 	/*
 	 * Enlightened VMCS has been enabled. It does not mean that L1 has to

commit 5058b692c69997f9736b94df786509366c32f34d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:14 2020 -0700

    KVM: VMX: Move vmx_flush_tlb() to vmx.c
    
    Move vmx_flush_tlb() to vmx.c and make it non-inline static now that all
    its callers live in vmx.c.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-19-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 45978552524e..4c7b0713b438 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -500,31 +500,6 @@ static inline struct vmcs *alloc_vmcs(bool shadow)
 
 u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);
 
-static inline void vmx_flush_tlb(struct kvm_vcpu *vcpu)
-{
-	struct vcpu_vmx *vmx = to_vmx(vcpu);
-
-	/*
-	 * Flush all EPTP/VPID contexts, as the TLB flush _may_ have been
-	 * invoked via kvm_flush_remote_tlbs().  Flushing remote TLBs requires
-	 * all contexts to be flushed, not just the active context.
-	 *
-	 * Note, this also ensures a deferred TLB flush with VPID enabled and
-	 * EPT disabled invalidates the "correct" VPID, by nuking both L1 and
-	 * L2's VPIDs.
-	 */
-	if (enable_ept) {
-		ept_sync_global();
-	} else if (enable_vpid) {
-		if (cpu_has_vmx_invvpid_global()) {
-			vpid_sync_vcpu_global();
-		} else {
-			vpid_sync_vcpu_single(vmx->vpid);
-			vpid_sync_vcpu_single(vmx->nested.vpid02);
-		}
-	}
-}
-
 static inline void decache_tsc_multiplier(struct vcpu_vmx *vmx)
 {
 	vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;

commit f55ac304ca47039368a5971fa61ebc8160c90659
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:12 2020 -0700

    KVM: x86: Drop @invalidate_gpa param from kvm_x86_ops' tlb_flush()
    
    Drop @invalidate_gpa from ->tlb_flush() and kvm_vcpu_flush_tlb() now
    that all callers pass %true for said param, or ignore the param (SVM has
    an internal call to svm_flush_tlb() in svm_flush_tlb_guest that somewhat
    arbitrarily passes %false).
    
    Remove __vmx_flush_tlb() as it is no longer used.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-17-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 1560296dde25..45978552524e 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -500,46 +500,28 @@ static inline struct vmcs *alloc_vmcs(bool shadow)
 
 u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);
 
-static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
-				bool invalidate_gpa)
-{
-	if (enable_ept && (invalidate_gpa || !enable_vpid)) {
-		if (!VALID_PAGE(vcpu->arch.mmu->root_hpa))
-			return;
-		ept_sync_context(construct_eptp(vcpu,
-						vcpu->arch.mmu->root_hpa));
-	} else {
-		vpid_sync_context(vpid);
-	}
-}
-
-static inline void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
+static inline void vmx_flush_tlb(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
 	/*
-	 * Flush all EPTP/VPID contexts if the TLB flush _may_ have been
-	 * invoked via kvm_flush_remote_tlbs(), which always passes %true for
-	 * @invalidate_gpa.  Flushing remote TLBs requires all contexts to be
-	 * flushed, not just the active context.
+	 * Flush all EPTP/VPID contexts, as the TLB flush _may_ have been
+	 * invoked via kvm_flush_remote_tlbs().  Flushing remote TLBs requires
+	 * all contexts to be flushed, not just the active context.
 	 *
 	 * Note, this also ensures a deferred TLB flush with VPID enabled and
 	 * EPT disabled invalidates the "correct" VPID, by nuking both L1 and
 	 * L2's VPIDs.
 	 */
-	if (invalidate_gpa) {
-		if (enable_ept) {
-			ept_sync_global();
-		} else if (enable_vpid) {
-			if (cpu_has_vmx_invvpid_global()) {
-				vpid_sync_vcpu_global();
-			} else {
-				vpid_sync_vcpu_single(vmx->vpid);
-				vpid_sync_vcpu_single(vmx->nested.vpid02);
-			}
+	if (enable_ept) {
+		ept_sync_global();
+	} else if (enable_vpid) {
+		if (cpu_has_vmx_invvpid_global()) {
+			vpid_sync_vcpu_global();
+		} else {
+			vpid_sync_vcpu_single(vmx->vpid);
+			vpid_sync_vcpu_single(vmx->nested.vpid02);
 		}
-	} else {
-		__vmx_flush_tlb(vcpu, vmx->vpid, false);
 	}
 }
 

commit ca431c0cc3317e36c7011df2d2319385465db442
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:08 2020 -0700

    KVM: VMX: Drop redundant capability checks in low level INVVPID helpers
    
    Remove the INVVPID capabilities checks from vpid_sync_vcpu_single() and
    vpid_sync_vcpu_global() now that all callers ensure the INVVPID variant
    is supported.  Note, in some cases the guarantee is provided in concert
    with hardware_setup(), which enables VPID if and only if at least of
    invvpid_single() or invvpid_global() is supported.
    
    Drop the WARN_ON_ONCE() from vmx_flush_tlb() as vpid_sync_vcpu_single()
    will trigger a WARN() on INVVPID failure, i.e. if SINGLE_CONTEXT isn't
    supported.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-13-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index fa40319f1698..1560296dde25 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -534,7 +534,6 @@ static inline void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 			if (cpu_has_vmx_invvpid_global()) {
 				vpid_sync_vcpu_global();
 			} else {
-				WARN_ON_ONCE(!cpu_has_vmx_invvpid_single());
 				vpid_sync_vcpu_single(vmx->vpid);
 				vpid_sync_vcpu_single(vmx->nested.vpid02);
 			}

commit e8eff282154fc392dadf6a779009c7ecaa7e169b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:27:57 2020 -0700

    KVM: VMX: Flush all EPTP/VPID contexts on remote TLB flush
    
    Flush all EPTP/VPID contexts if a TLB flush _may_ have been triggered by
    a remote or deferred TLB flush, i.e. by KVM_REQ_TLB_FLUSH.  Remote TLB
    flushes require all contexts to be invalidated, not just the active
    contexts, e.g. all mappings in all contexts for a given HVA need to be
    invalidated on a mmu_notifier invalidation.  Similarly, the instigator
    of the deferred TLB flush may be expecting all contexts to be flushed,
    e.g. vmx_vcpu_load_vmcs().
    
    Without nested VMX, flushing only the current EPTP/VPID context isn't
    problematic because KVM uses a constant VPID for each vCPU, and
    mmu_alloc_direct_roots() all but guarantees KVM will use a single EPTP
    for L1.  In the rare case where a different EPTP is created or reused,
    KVM (currently) unconditionally flushes the new EPTP context prior to
    entering the guest.
    
    With nested VMX, KVM conditionally uses a different VPID for L2, and
    unconditionally uses a different EPTP for L2.  Because KVM doesn't
    _intentionally_ guarantee L2's EPTP/VPID context is flushed on nested
    VM-Enter, it'd be possible for a malicious L1 to attack the host and/or
    different VMs by exploiting the lack of flushing for L2.
    
      1) Launch nested guest from malicious L1.
    
      2) Nested VM-Enter to L2.
    
      3) Access target GPA 'g'.  CPU inserts TLB entry tagged with L2's ASID
         mapping 'g' to host PFN 'x'.
    
      2) Nested VM-Exit to L1.
    
      3) L1 triggers kernel same-page merging (ksm) by duplicating/zeroing
         the page for PFN 'x'.
    
      4) Host kernel merges PFN 'x' with PFN 'y', i.e. unmaps PFN 'x' and
         remaps the page to PFN 'y'.  mmu_notifier sends invalidate command,
         KVM flushes TLB only for L1's ASID.
    
      4) Host kernel reallocates PFN 'x' to some other task/guest.
    
      5) Nested VM-Enter to L2.  KVM does not invalidate L2's EPTP or VPID.
    
      6) L2 accesses GPA 'g' and gains read/write access to PFN 'x' via its
         stale TLB entry.
    
    However, current KVM unconditionally flushes L1's EPTP/VPID context on
    nested VM-Exit.  But, that behavior is mostly unintentional, KVM doesn't
    go out of its way to flush EPTP/VPID on nested VM-Enter/VM-Exit, rather
    a TLB flush is guaranteed to occur prior to re-entering L1 due to
    __kvm_mmu_new_cr3() always being called with skip_tlb_flush=false.  On
    nested VM-Enter, this happens via kvm_init_shadow_ept_mmu() (nested EPT
    enabled) or in nested_vmx_load_cr3() (nested EPT disabled).  On nested
    VM-Exit it occurs via nested_vmx_load_cr3().
    
    This also fixes a bug where a deferred TLB flush in the context of L2,
    with EPT disabled, would flush L1's VPID instead of L2's VPID, as
    vmx_flush_tlb() flushes L1's VPID regardless of is_guest_mode().
    
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Ben Gardon <bgardon@google.com>
    Cc: Jim Mattson <jmattson@google.com>
    Cc: Junaid Shahid <junaids@google.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: John Haxby <john.haxby@oracle.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Fixes: efebf0aaec3d ("KVM: nVMX: Do not flush TLB on L1<->L2 transitions if L1 uses VPID and EPT")
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-2-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index aab9df55336e..fa40319f1698 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -515,7 +515,33 @@ static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
 
 static inline void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 {
-	__vmx_flush_tlb(vcpu, to_vmx(vcpu)->vpid, invalidate_gpa);
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	/*
+	 * Flush all EPTP/VPID contexts if the TLB flush _may_ have been
+	 * invoked via kvm_flush_remote_tlbs(), which always passes %true for
+	 * @invalidate_gpa.  Flushing remote TLBs requires all contexts to be
+	 * flushed, not just the active context.
+	 *
+	 * Note, this also ensures a deferred TLB flush with VPID enabled and
+	 * EPT disabled invalidates the "correct" VPID, by nuking both L1 and
+	 * L2's VPIDs.
+	 */
+	if (invalidate_gpa) {
+		if (enable_ept) {
+			ept_sync_global();
+		} else if (enable_vpid) {
+			if (cpu_has_vmx_invvpid_global()) {
+				vpid_sync_vcpu_global();
+			} else {
+				WARN_ON_ONCE(!cpu_has_vmx_invvpid_single());
+				vpid_sync_vcpu_single(vmx->vpid);
+				vpid_sync_vcpu_single(vmx->nested.vpid02);
+			}
+		}
+	} else {
+		__vmx_flush_tlb(vcpu, vmx->vpid, false);
+	}
 }
 
 static inline void decache_tsc_multiplier(struct vcpu_vmx *vmx)

commit 8c1b724ddb218f221612d4c649bc9c7819d8d7a6
Merge: f14a9532ee30 514ccc194971
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 15:13:15 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - GICv4.1 support
    
       - 32bit host removal
    
      PPC:
       - secure (encrypted) using under the Protected Execution Framework
         ultravisor
    
      s390:
       - allow disabling GISA (hardware interrupt injection) and protected
         VMs/ultravisor support.
    
      x86:
       - New dirty bitmap flag that sets all bits in the bitmap when dirty
         page logging is enabled; this is faster because it doesn't require
         bulk modification of the page tables.
    
       - Initial work on making nested SVM event injection more similar to
         VMX, and less buggy.
    
       - Various cleanups to MMU code (though the big ones and related
         optimizations were delayed to 5.8). Instead of using cr3 in
         function names which occasionally means eptp, KVM too has
         standardized on "pgd".
    
       - A large refactoring of CPUID features, which now use an array that
         parallels the core x86_features.
    
       - Some removal of pointer chasing from kvm_x86_ops, which will also
         be switched to static calls as soon as they are available.
    
       - New Tigerlake CPUID features.
    
       - More bugfixes, optimizations and cleanups.
    
      Generic:
       - selftests: cleanups, new MMU notifier stress test, steal-time test
    
       - CSV output for kvm_stat"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (277 commits)
      x86/kvm: fix a missing-prototypes "vmread_error"
      KVM: x86: Fix BUILD_BUG() in __cpuid_entry_get_reg() w/ CONFIG_UBSAN=y
      KVM: VMX: Add a trampoline to fix VMREAD error handling
      KVM: SVM: Annotate svm_x86_ops as __initdata
      KVM: VMX: Annotate vmx_x86_ops as __initdata
      KVM: x86: Drop __exit from kvm_x86_ops' hardware_unsetup()
      KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection
      KVM: x86: Set kvm_x86_ops only after ->hardware_setup() completes
      KVM: VMX: Configure runtime hooks using vmx_x86_ops
      KVM: VMX: Move hardware_setup() definition below vmx_x86_ops
      KVM: x86: Move init-only kvm_x86_ops to separate struct
      KVM: Pass kvm_init()'s opaque param to additional arch funcs
      s390/gmap: return proper error code on ksm unsharing
      KVM: selftests: Fix cosmetic copy-paste error in vm_mem_region_move()
      KVM: Fix out of range accesses to memslots
      KVM: X86: Micro-optimize IPI fastpath delay
      KVM: X86: Delay read msr data iff writes ICR MSR
      KVM: PPC: Book3S HV: Add a capability for enabling secure guests
      KVM: arm64: GICv4.1: Expose HW-based SGIs in debugfs
      KVM: arm64: GICv4.1: Allow non-trapping WFI when using HW SGIs
      ...

commit fdf5563a720004199324371c08071b8ea27bd994
Merge: 97cddfc34549 a2150327250e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 11:04:05 2020 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "This topic tree contains more commits than usual:
    
       - most of it are uaccess cleanups/reorganization by Al
    
       - there's a bunch of prototype declaration (--Wmissing-prototypes)
         cleanups
    
       - misc other cleanups all around the map"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      x86/mm/set_memory: Fix -Wmissing-prototypes warnings
      x86/efi: Add a prototype for efi_arch_mem_reserve()
      x86/mm: Mark setup_emu2phys_nid() static
      x86/jump_label: Move 'inline' keyword placement
      x86/platform/uv: Add a missing prototype for uv_bau_message_interrupt()
      kill uaccess_try()
      x86: unsafe_put-style macro for sigmask
      x86: x32_setup_rt_frame(): consolidate uaccess areas
      x86: __setup_rt_frame(): consolidate uaccess areas
      x86: __setup_frame(): consolidate uaccess areas
      x86: setup_sigcontext(): list user_access_{begin,end}() into callers
      x86: get rid of put_user_try in __setup_rt_frame() (both 32bit and 64bit)
      x86: ia32_setup_rt_frame(): consolidate uaccess areas
      x86: ia32_setup_frame(): consolidate uaccess areas
      x86: ia32_setup_sigcontext(): lift user_access_{begin,end}() into the callers
      x86/alternatives: Mark text_poke_loc_init() static
      x86/cpu: Fix a -Wmissing-prototypes warning for init_ia32_feat_ctl()
      x86/mm: Drop pud_mknotpresent()
      x86: Replace setup_irq() by request_irq()
      x86/configs: Slightly reduce defconfigs
      ...

commit d260f9ef50c76c5587353fa71719be8cf5525f06
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 12:37:50 2020 -0700

    KVM: VMX: Fold loaded_vmcs_init() into alloc_loaded_vmcs()
    
    Subsume loaded_vmcs_init() into alloc_loaded_vmcs(), its only remaining
    caller, and drop the VMCLEAR on the shadow VMCS, which is guaranteed to
    be NULL.  loaded_vmcs_init() was previously used by loaded_vmcs_clear(),
    but loaded_vmcs_clear() also subsumed loaded_vmcs_init() to properly
    handle smp_wmb() with respect to VMCLEAR.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321193751.24985-3-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index be93d597306c..79d38f41ef7a 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -492,7 +492,6 @@ struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu, gfp_t flags);
 void free_vmcs(struct vmcs *vmcs);
 int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
 void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
-void loaded_vmcs_init(struct loaded_vmcs *loaded_vmcs);
 void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs);
 
 static inline struct vmcs *alloc_vmcs(bool shadow)

commit 727a7e27cf88a261c5a0f14f4f9ee4d767352766
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 5 03:52:50 2020 -0500

    KVM: x86: rename set_cr3 callback and related flags to load_mmu_pgd
    
    The set_cr3 callback is not setting the guest CR3, it is setting the
    root of the guest page tables, either shadow or two-dimensional.
    To make this clearer as well as to indicate that the MMU calls it
    via kvm_mmu_load_cr3, rename it to load_mmu_pgd.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index fc45bdb5a62f..be93d597306c 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -334,9 +334,9 @@ u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu);
 void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask);
 void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer);
 void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
-void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
 int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
 void set_cr4_guest_host_mask(struct vcpu_vmx *vmx);
+void vmx_load_mmu_pgd(struct kvm_vcpu *vcpu, unsigned long cr3);
 void ept_save_pdptrs(struct kvm_vcpu *vcpu);
 void vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 void vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);

commit 91661989d17ccec17bca199e7cb1f463ba4e5b78
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:57:06 2020 -0800

    KVM: x86: Move VMX's host_efer to common x86 code
    
    Move host_efer to common x86 code and use it for CPUID's is_efer_nx() to
    avoid constantly re-reading the MSR.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 9a51a3a77233..fc45bdb5a62f 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -12,7 +12,6 @@
 #include "vmcs.h"
 
 extern const u32 vmx_msr_index[];
-extern u64 host_efer;
 
 extern u32 get_umwait_control_msr(void);
 

commit 2ef7619d43731b6eaa7cc2e03d000e4bbc1bf612
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:22 2020 -0800

    KVM: VMX: Add helpers to query Intel PT mode
    
    Add helpers to query which of the (two) supported PT modes is active.
    The primary motivation is to help document that there is a third PT mode
    (host-only) that's currently not supported by KVM.  As is, it's not
    obvious that PT_MODE_SYSTEM != !PT_MODE_HOST_GUEST and vice versa, e.g.
    that "pt_mode == PT_MODE_SYSTEM" and "pt_mode != PT_MODE_HOST_GUEST" are
    two distinct checks.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index e64da06c7009..9a51a3a77233 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -452,7 +452,7 @@ static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 static inline u32 vmx_vmentry_ctrl(void)
 {
 	u32 vmentry_ctrl = vmcs_config.vmentry_ctrl;
-	if (pt_mode == PT_MODE_SYSTEM)
+	if (vmx_pt_mode_is_system())
 		vmentry_ctrl &= ~(VM_ENTRY_PT_CONCEAL_PIP |
 				  VM_ENTRY_LOAD_IA32_RTIT_CTL);
 	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
@@ -463,7 +463,7 @@ static inline u32 vmx_vmentry_ctrl(void)
 static inline u32 vmx_vmexit_ctrl(void)
 {
 	u32 vmexit_ctrl = vmcs_config.vmexit_ctrl;
-	if (pt_mode == PT_MODE_SYSTEM)
+	if (vmx_pt_mode_is_system())
 		vmexit_ctrl &= ~(VM_EXIT_PT_CONCEAL_PIP |
 				 VM_EXIT_CLEAR_IA32_RTIT_CTL);
 	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */

commit 5ef8acbdd687c9d72582e2c05c0b9756efb37863
Author: Oliver Upton <oupton@google.com>
Date:   Fri Feb 7 02:36:07 2020 -0800

    KVM: nVMX: Emulate MTF when performing instruction emulation
    
    Since commit 5f3d45e7f282 ("kvm/x86: add support for
    MONITOR_TRAP_FLAG"), KVM has allowed an L1 guest to use the monitor trap
    flag processor-based execution control for its L2 guest. KVM simply
    forwards any MTF VM-exits to the L1 guest, which works for normal
    instruction execution.
    
    However, when KVM needs to emulate an instruction on the behalf of an L2
    guest, the monitor trap flag is not emulated. Add the necessary logic to
    kvm_skip_emulated_instruction() to synthesize an MTF VM-exit to L1 upon
    instruction emulation for L2.
    
    Fixes: 5f3d45e7f282 ("kvm/x86: add support for MONITOR_TRAP_FLAG")
    Signed-off-by: Oliver Upton <oupton@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 7f42cf3dcd70..e64da06c7009 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -150,6 +150,9 @@ struct nested_vmx {
 	/* L2 must run next, and mustn't decide to exit to L1. */
 	bool nested_run_pending;
 
+	/* Pending MTF VM-exit into L1.  */
+	bool mtf_pending;
+
 	struct loaded_vmcs vmcs02;
 
 	/*

commit b10c307f6f314c068814d0e23c86f06d5d57004b
Author: Benjamin Thiel <b.thiel@posteo.de>
Date:   Thu Jan 23 18:29:45 2020 +0100

    x86/cpu: Move prototype for get_umwait_control_msr() to a global location
    
    .. in order to fix a -Wmissing-prototypes warning.
    
    No functional change.
    
    Signed-off-by: Benjamin Thiel <b.thiel@posteo.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: kvm@vger.kernel.org
    Link: https://lkml.kernel.org/r/20200123172945.7235-1-b.thiel@posteo.de

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 7f42cf3dcd70..b4e14ed66ca9 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -14,8 +14,6 @@
 extern const u32 vmx_msr_index[];
 extern u64 host_efer;
 
-extern u32 get_umwait_control_msr(void);
-
 #define MSR_TYPE_R	1
 #define MSR_TYPE_W	2
 #define MSR_TYPE_RW	3

commit 32ad73db7fc5fe7eebafdab3b528f99ab8498e3f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 20 20:44:55 2019 -0800

    x86/msr-index: Clean up bit defines for IA32_FEATURE_CONTROL MSR
    
    As pointed out by Boris, the defines for bits in IA32_FEATURE_CONTROL
    are quite a mouthful, especially the VMX bits which must differentiate
    between enabling VMX inside and outside SMX (TXT) operation.  Rename the
    MSR and its bit defines to abbreviate FEATURE_CONTROL as FEAT_CTL to
    make them a little friendlier on the eyes.
    
    Arguably, the MSR itself should keep the full IA32_FEATURE_CONTROL name
    to match Intel's SDM, but a future patch will add a dedicated Kconfig,
    file and functions for the MSR. Using the full name for those assets is
    rather unwieldy, so bite the bullet and use IA32_FEAT_CTL so that its
    nomenclature is consistent throughout the kernel.
    
    Opportunistically, fix a few other annoyances with the defines:
    
      - Relocate the bit defines so that they immediately follow the MSR
        define, e.g. aren't mistaken as belonging to MISC_FEATURE_CONTROL.
      - Add whitespace around the block of feature control defines to make
        it clear they're all related.
      - Use BIT() instead of manually encoding the bit shift.
      - Use "VMX" instead of "VMXON" to match the SDM.
      - Append "_ENABLED" to the LMCE (Local Machine Check Exception) bit to
        be consistent with the kernel's verbiage used for all other feature
        control bits.  Note, the SDM refers to the LMCE bit as LMCE_ON,
        likely to differentiate it from IA32_MCG_EXT_CTL.LMCE_EN.  Ignore
        the (literal) one-off usage of _ON, the SDM is simply "wrong".
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20191221044513.21680-2-sean.j.christopherson@intel.com

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index a4f7f737c5d4..7f42cf3dcd70 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -289,7 +289,7 @@ struct vcpu_vmx {
 
 	/*
 	 * Only bits masked by msr_ia32_feature_control_valid_bits can be set in
-	 * msr_ia32_feature_control. FEATURE_CONTROL_LOCKED is always included
+	 * msr_ia32_feature_control. FEAT_CTL_LOCKED is always included
 	 * in msr_ia32_feature_control_valid_bits.
 	 */
 	u64 msr_ia32_feature_control;

commit 7d73710d9ca2564f29d291d0b3badc09efdf25e9
Author: Jim Mattson <jmattson@google.com>
Date:   Tue Dec 3 16:24:42 2019 -0800

    kvm: vmx: Stop wasting a page for guest_msrs
    
    We will never need more guest_msrs than there are indices in
    vmx_msr_index. Thus, at present, the guest_msrs array will not exceed
    168 bytes.
    
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 7c1b978b2df4..a4f7f737c5d4 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -22,6 +22,12 @@ extern u32 get_umwait_control_msr(void);
 
 #define X2APIC_MSR(r) (APIC_BASE_MSR + ((r) >> 4))
 
+#ifdef CONFIG_X86_64
+#define NR_SHARED_MSRS	7
+#else
+#define NR_SHARED_MSRS	4
+#endif
+
 #define NR_LOADSTORE_MSRS 8
 
 struct vmx_msrs {
@@ -206,7 +212,7 @@ struct vcpu_vmx {
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
-	struct shared_msr_entry *guest_msrs;
+	struct shared_msr_entry guest_msrs[NR_SHARED_MSRS];
 	int                   nmsrs;
 	int                   save_nmsrs;
 	bool                  guest_msrs_ready;

commit 46f4f0aabc61bfd365e1eb3c8a6d766d1a49cf32
Merge: 14edff88315a b07a5c53d42a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 21 10:01:51 2019 +0100

    Merge branch 'kvm-tsx-ctrl' into HEAD
    
    Conflicts:
            arch/x86/kvm/vmx/vmx.c

commit 662f1d1d19317e792ccfc53dee625c02dcefac58
Author: Aaron Lewis <aaronlewis@google.com>
Date:   Thu Nov 7 21:14:39 2019 -0800

    KVM: nVMX: Add support for capturing highest observable L2 TSC
    
    The L1 hypervisor may include the IA32_TIME_STAMP_COUNTER MSR in the
    vmcs12 MSR VM-exit MSR-store area as a way of determining the highest
    TSC value that might have been observed by L2 prior to VM-exit. The
    current implementation does not capture a very tight bound on this
    value.  To tighten the bound, add the IA32_TIME_STAMP_COUNTER MSR to the
    vmcs02 VM-exit MSR-store area whenever it appears in the vmcs12 VM-exit
    MSR-store area.  When L0 processes the vmcs12 VM-exit MSR-store area
    during the emulation of an L2->L1 VM-exit, special-case the
    IA32_TIME_STAMP_COUNTER MSR, using the value stored in the vmcs02
    VM-exit MSR-store area to derive the value to be stored in the vmcs12
    VM-exit MSR-store area.
    
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Aaron Lewis <aaronlewis@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 73ff03091d29..90b97d9d4f7d 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -233,6 +233,10 @@ struct vcpu_vmx {
 		struct vmx_msrs host;
 	} msr_autoload;
 
+	struct msr_autostore {
+		struct vmx_msrs guest;
+	} msr_autostore;
+
 	struct {
 		int vm86_active;
 		ulong save_rflags;
@@ -337,6 +341,7 @@ void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu);
 struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr);
 void pt_update_intercept_for_msr(struct vcpu_vmx *vmx);
 void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp);
+int vmx_find_msr_index(struct vmx_msrs *m, u32 msr);
 
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1

commit 7cfe0526fd379e4ff9c3dcf933c1966a3a635013
Author: Aaron Lewis <aaronlewis@google.com>
Date:   Thu Nov 7 21:14:37 2019 -0800

    kvm: vmx: Rename NR_AUTOLOAD_MSRS to NR_LOADSTORE_MSRS
    
    Rename NR_AUTOLOAD_MSRS to NR_LOADSTORE_MSRS.  This needs to be done
    due to the addition of the MSR-autostore area that will be added in a
    future patch.  After that the name AUTOLOAD will no longer make sense.
    
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Aaron Lewis <aaronlewis@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 43331dfafffe..73ff03091d29 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -22,11 +22,11 @@ extern u32 get_umwait_control_msr(void);
 
 #define X2APIC_MSR(r) (APIC_BASE_MSR + ((r) >> 4))
 
-#define NR_AUTOLOAD_MSRS 8
+#define NR_LOADSTORE_MSRS 8
 
 struct vmx_msrs {
 	unsigned int		nr;
-	struct vmx_msr_entry	val[NR_AUTOLOAD_MSRS];
+	struct vmx_msr_entry	val[NR_LOADSTORE_MSRS];
 };
 
 struct shared_msr_entry {

commit 02d496cfb88a4856b9d67ade32317077c510aebc
Author: Liran Alon <liran.alon@oracle.com>
Date:   Mon Nov 11 14:30:55 2019 +0200

    KVM: nVMX: Update vmcs01 TPR_THRESHOLD if L2 changed L1 TPR
    
    When L1 don't use TPR-Shadow to run L2, L0 configures vmcs02 without
    TPR-Shadow and install intercepts on CR8 access (load and store).
    
    If L1 do not intercept L2 CR8 access, L0 intercepts on those accesses
    will emulate load/store on L1's LAPIC TPR. If in this case L2 lowers
    TPR such that there is now an injectable interrupt to L1,
    apic_update_ppr() will request a KVM_REQ_EVENT which will trigger a call
    to update_cr8_intercept() to update TPR-Threshold to highest pending IRR
    priority.
    
    However, this update to TPR-Threshold is done while active vmcs is
    vmcs02 instead of vmcs01. Thus, when later at some point L0 will
    emulate an exit from L2 to L1, L1 will still run with high
    TPR-Threshold. This will result in every VMEntry to L1 to immediately
    exit on TPR_BELOW_THRESHOLD and continue to do so infinitely until
    some condition will cause KVM_REQ_EVENT to be set.
    (Note that TPR_BELOW_THRESHOLD exit handler do not set KVM_REQ_EVENT
    until apic_update_ppr() will notice a new injectable interrupt for PPR)
    
    To fix this issue, change update_cr8_intercept() such that if L2 lowers
    L1's TPR in a way that requires to lower L1's TPR-Threshold, save update
    to TPR-Threshold and apply it to vmcs01 when L0 emulates an exit from
    L2 to L1.
    
    Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index bee16687dc0b..43331dfafffe 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -167,6 +167,9 @@ struct nested_vmx {
 	u64 vmcs01_debugctl;
 	u64 vmcs01_guest_bndcfgs;
 
+	/* to migrate it to L1 if L2 writes to L1's CR8 directly */
+	int l1_tpr_threshold;
+
 	u16 vpid02;
 	u16 last_vpid;
 

commit 29881b6ec6e453ff8df37ad8f44e17bf0d4e1e12
Author: Joao Martins <joao.m.martins@oracle.com>
Date:   Mon Nov 11 17:20:12 2019 +0000

    KVM: VMX: Introduce pi_is_pir_empty() helper
    
    Streamline the PID.PIR check and change its call sites to use
    the newly added helper.
    
    Suggested-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Joao Martins <joao.m.martins@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 1e32ab54fc2d..5a0f34b1e226 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -355,6 +355,11 @@ static inline int pi_test_and_set_pir(int vector, struct pi_desc *pi_desc)
 	return test_and_set_bit(vector, (unsigned long *)pi_desc->pir);
 }
 
+static inline bool pi_is_pir_empty(struct pi_desc *pi_desc)
+{
+	return bitmap_empty((unsigned long *)pi_desc->pir, NR_VECTORS);
+}
+
 static inline void pi_set_sn(struct pi_desc *pi_desc)
 {
 	set_bit(POSTED_INTR_SN,

commit 132194ffa138863eac620abb3b6f983278e61b4a
Author: Joao Martins <joao.m.martins@oracle.com>
Date:   Mon Nov 11 17:20:11 2019 +0000

    KVM: VMX: Do not change PID.NDST when loading a blocked vCPU
    
    When vCPU enters block phase, pi_pre_block() inserts vCPU to a per pCPU
    linked list of all vCPUs that are blocked on this pCPU. Afterwards, it
    changes PID.NV to POSTED_INTR_WAKEUP_VECTOR which its handler
    (wakeup_handler()) is responsible to kick (unblock) any vCPU on that
    linked list that now has pending posted interrupts.
    
    While vCPU is blocked (in kvm_vcpu_block()), it may be preempted which
    will cause vmx_vcpu_pi_put() to set PID.SN.  If later the vCPU will be
    scheduled to run on a different pCPU, vmx_vcpu_pi_load() will clear
    PID.SN but will also *overwrite PID.NDST to this different pCPU*.
    Instead of keeping it with original pCPU which vCPU had entered block
    phase on.
    
    This results in an issue because when a posted interrupt is delivered, as
    the wakeup_handler() will be executed and fail to find blocked vCPU on
    its per pCPU linked list of all vCPUs that are blocked on this pCPU.
    Which is due to the vCPU being placed on a *different* per pCPU
    linked list i.e. the original pCPU in which it entered block phase.
    
    The regression is introduced by commit c112b5f50232 ("KVM: x86:
    Recompute PID.ON when clearing PID.SN"). Therefore, partially revert
    it and reintroduce the condition in vmx_vcpu_pi_load() responsible for
    avoiding changing PID.NDST when loading a blocked vCPU.
    
    Fixes: c112b5f50232 ("KVM: x86: Recompute PID.ON when clearing PID.SN")
    Tested-by: Nathan Ni <nathan.ni@oracle.com>
    Co-developed-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Joao Martins <joao.m.martins@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index bee16687dc0b..1e32ab54fc2d 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -373,6 +373,12 @@ static inline void pi_clear_on(struct pi_desc *pi_desc)
 		(unsigned long *)&pi_desc->control);
 }
 
+static inline void pi_clear_sn(struct pi_desc *pi_desc)
+{
+	clear_bit(POSTED_INTR_SN,
+		(unsigned long *)&pi_desc->control);
+}
+
 static inline int pi_test_on(struct pi_desc *pi_desc)
 {
 	return test_bit(POSTED_INTR_ON,

commit 6e3ba4abcea5681eebbfc10f1b56c9fbe80b6685
Author: Tao Xu <tao3.xu@intel.com>
Date:   Tue Jul 16 14:55:50 2019 +0800

    KVM: vmx: Emulate MSR IA32_UMWAIT_CONTROL
    
    UMWAIT and TPAUSE instructions use 32bit IA32_UMWAIT_CONTROL at MSR index
    E1H to determines the maximum time in TSC-quanta that the processor can
    reside in either C0.1 or C0.2.
    
    This patch emulates MSR IA32_UMWAIT_CONTROL in guest and differentiate
    IA32_UMWAIT_CONTROL between host and guest. The variable
    mwait_control_cached in arch/x86/kernel/cpu/umwait.c caches the MSR value,
    so this patch uses it to avoid frequently rdmsr of IA32_UMWAIT_CONTROL.
    
    Co-developed-by: Jingqi Liu <jingqi.liu@intel.com>
    Signed-off-by: Jingqi Liu <jingqi.liu@intel.com>
    Signed-off-by: Tao Xu <tao3.xu@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 64d5a4890aa9..bee16687dc0b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -14,6 +14,8 @@
 extern const u32 vmx_msr_index[];
 extern u64 host_efer;
 
+extern u32 get_umwait_control_msr(void);
+
 #define MSR_TYPE_R	1
 #define MSR_TYPE_W	2
 #define MSR_TYPE_RW	3
@@ -211,6 +213,7 @@ struct vcpu_vmx {
 #endif
 
 	u64		      spec_ctrl;
+	u32		      msr_ia32_umwait_control;
 
 	u32 secondary_exec_control;
 
@@ -497,6 +500,12 @@ static inline void decache_tsc_multiplier(struct vcpu_vmx *vmx)
 	vmcs_write64(TSC_MULTIPLIER, vmx->current_tsc_ratio);
 }
 
+static inline bool vmx_has_waitpkg(struct vcpu_vmx *vmx)
+{
+	return vmx->secondary_exec_control &
+		SECONDARY_EXEC_ENABLE_USR_WAIT_PAUSE;
+}
+
 void dump_vmcs(void);
 
 #endif /* __KVM_X86_VMX_H */

commit c5c5d6fae001c653a4e831325e062816a60c5e38
Author: Peter Xu <peterx@redhat.com>
Date:   Fri Sep 6 10:17:21 2019 +0800

    KVM: VMX: Change ple_window type to unsigned int
    
    The VMX ple_window is 32 bits wide, so logically it can overflow with
    an int.  The module parameter is declared as unsigned int which is
    good, however the dynamic variable is not.  Switching all the
    ple_window references to use unsigned int.
    
    The tracepoint changes will also affect SVM, but SVM is using an even
    smaller width (16 bits) so it's always fine.
    
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 82d0bc3a4d52..64d5a4890aa9 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -253,7 +253,7 @@ struct vcpu_vmx {
 	struct nested_vmx nested;
 
 	/* Dynamic PLE window. */
-	int ple_window;
+	unsigned int ple_window;
 	bool ple_window_dirty;
 
 	bool req_immediate_exit;

commit 3af80fec6e7fe2e89aa131a0ebdb90be780668f8
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:18:00 2019 -0700

    KVM: VMX: Explicitly initialize controls shadow at VMCS allocation
    
    Or: Don't re-initialize vmcs02's controls on every nested VM-Entry.
    
    VMWRITEs to the major VMCS controls are deceptively expensive.  Intel
    CPUs with VMCS caching (Westmere and later) also optimize away
    consistency checks on VM-Entry, i.e. skip consistency checks if the
    relevant fields have not changed since the last successful VM-Entry (of
    the cached VMCS).  Because uops are a precious commodity, uCode's dirty
    VMCS field tracking isn't as precise as software would prefer.  Notably,
    writing any of the major VMCS fields effectively marks the entire VMCS
    dirty, i.e. causes the next VM-Entry to perform all consistency checks,
    which consumes several hundred cycles.
    
    Zero out the controls' shadow copies during VMCS allocation and use the
    optimized setter when "initializing" controls.  While this technically
    affects both non-nested and nested virtualization, nested virtualization
    is the primary beneficiary as avoid VMWRITEs when prepare vmcs02 allows
    hardware to optimizie away consistency checks.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 52d7bc90d9ef..82d0bc3a4d52 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -388,11 +388,6 @@ static inline u8 vmx_get_rvi(void)
 }
 
 #define BUILD_CONTROLS_SHADOW(lname, uname)				    \
-static inline void lname##_controls_init(struct vcpu_vmx *vmx, u32 val)	    \
-{									    \
-	vmcs_write32(uname, val);					    \
-	vmx->loaded_vmcs->controls_shadow.lname = val;			    \
-}									    \
 static inline void lname##_controls_set(struct vcpu_vmx *vmx, u32 val)	    \
 {									    \
 	if (vmx->loaded_vmcs->controls_shadow.lname != val) {		    \

commit ae81d08993cbc515e3181ee6bebce5cd878133f2
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:17:59 2019 -0700

    KVM: nVMX: Don't reset VMCS controls shadow on VMCS switch
    
    ... now that the shadow copies are per-VMCS.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index ec11ecf6f040..52d7bc90d9ef 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -388,10 +388,6 @@ static inline u8 vmx_get_rvi(void)
 }
 
 #define BUILD_CONTROLS_SHADOW(lname, uname)				    \
-static inline void lname##_controls_reset_shadow(struct vcpu_vmx *vmx)	    \
-{									    \
-	vmx->loaded_vmcs->controls_shadow.lname = vmcs_read32(uname);	    \
-}									    \
 static inline void lname##_controls_init(struct vcpu_vmx *vmx, u32 val)	    \
 {									    \
 	vmcs_write32(uname, val);					    \

commit 09e226cf07e6bf85d885afa43fabc02b88dc1652
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:17:58 2019 -0700

    KVM: nVMX: Shadow VMCS controls on a per-VMCS basis
    
    ... to pave the way for not preserving the shadow copies across switches
    between vmcs01 and vmcs02, and eventually to avoid VMWRITEs to vmcs02
    when the desired value is unchanged across nested VM-Enters.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index bde6c43eea16..ec11ecf6f040 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -85,14 +85,6 @@ struct pt_desc {
 	struct pt_ctx guest;
 };
 
-struct vmx_controls_shadow {
-	u32 vm_entry;
-	u32 vm_exit;
-	u32 pin;
-	u32 exec;
-	u32 secondary_exec;
-};
-
 /*
  * The nested_vmx structure is part of vcpu_vmx, and holds information we need
  * for correct emulation of VMX (i.e., nested VMX) on this vcpu.
@@ -209,8 +201,6 @@ struct vcpu_vmx {
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
-	struct vmx_controls_shadow	controls_shadow;
-
 	struct shared_msr_entry *guest_msrs;
 	int                   nmsrs;
 	int                   save_nmsrs;
@@ -400,21 +390,23 @@ static inline u8 vmx_get_rvi(void)
 #define BUILD_CONTROLS_SHADOW(lname, uname)				    \
 static inline void lname##_controls_reset_shadow(struct vcpu_vmx *vmx)	    \
 {									    \
-	vmx->controls_shadow.lname = vmcs_read32(uname);		    \
+	vmx->loaded_vmcs->controls_shadow.lname = vmcs_read32(uname);	    \
 }									    \
 static inline void lname##_controls_init(struct vcpu_vmx *vmx, u32 val)	    \
 {									    \
 	vmcs_write32(uname, val);					    \
-	vmx->controls_shadow.lname = val;				    \
+	vmx->loaded_vmcs->controls_shadow.lname = val;			    \
 }									    \
 static inline void lname##_controls_set(struct vcpu_vmx *vmx, u32 val)	    \
 {									    \
-	if (vmx->controls_shadow.lname != val)				    \
-		lname##_controls_init(vmx, val);			    \
+	if (vmx->loaded_vmcs->controls_shadow.lname != val) {		    \
+		vmcs_write32(uname, val);				    \
+		vmx->loaded_vmcs->controls_shadow.lname = val;		    \
+	}								    \
 }									    \
 static inline u32 lname##_controls_get(struct vcpu_vmx *vmx)		    \
 {									    \
-	return vmx->controls_shadow.lname;				    \
+	return vmx->loaded_vmcs->controls_shadow.lname;			    \
 }									    \
 static inline void lname##_controls_setbit(struct vcpu_vmx *vmx, u32 val)   \
 {									    \

commit fe7f895dae4fa5725c0459b316b3e9ee814d2583
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:17:57 2019 -0700

    KVM: VMX: Shadow VMCS secondary execution controls
    
    Prepare to shadow all major control fields on a per-VMCS basis, which
    allows KVM to avoid costly VMWRITEs when switching between vmcs01 and
    vmcs02.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 754cc52f2640..bde6c43eea16 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -90,6 +90,7 @@ struct vmx_controls_shadow {
 	u32 vm_exit;
 	u32 pin;
 	u32 exec;
+	u32 secondary_exec;
 };
 
 /*
@@ -427,6 +428,7 @@ BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS)
 BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS)
 BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL)
 BUILD_CONTROLS_SHADOW(exec, CPU_BASED_VM_EXEC_CONTROL)
+BUILD_CONTROLS_SHADOW(secondary_exec, SECONDARY_VM_EXEC_CONTROL)
 
 static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 {

commit 2183f5645ae7e074ed1777f3de9a782dd23db248
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:17:56 2019 -0700

    KVM: VMX: Shadow VMCS primary execution controls
    
    Prepare to shadow all major control fields on a per-VMCS basis, which
    allows KVM to avoid VMREADs when switching between vmcs01 and vmcs02,
    and more importantly can eliminate costly VMWRITEs to controls when
    preparing vmcs02.
    
    Shadowing exec controls also saves a VMREAD when opening virtual
    INTR/NMI windows, yay...
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 1cdcaf8a6d97..754cc52f2640 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -89,6 +89,7 @@ struct vmx_controls_shadow {
 	u32 vm_entry;
 	u32 vm_exit;
 	u32 pin;
+	u32 exec;
 };
 
 /*
@@ -425,6 +426,7 @@ static inline void lname##_controls_clearbit(struct vcpu_vmx *vmx, u32 val) \
 BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS)
 BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS)
 BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL)
+BUILD_CONTROLS_SHADOW(exec, CPU_BASED_VM_EXEC_CONTROL)
 
 static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 {

commit c5f2c76643b612ffa47e4660c8f44deba619b068
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:17:55 2019 -0700

    KVM: VMX: Shadow VMCS pin controls
    
    Prepare to shadow all major control fields on a per-VMCS basis, which
    allows KVM to avoid costly VMWRITEs when switching between vmcs01 and
    vmcs02.
    
    Shadowing pin controls also allows a future patch to remove the per-VMCS
    'hv_timer_armed' flag, as the shadow copy is a superset of said flag.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index db4f9289d5da..1cdcaf8a6d97 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -88,6 +88,7 @@ struct pt_desc {
 struct vmx_controls_shadow {
 	u32 vm_entry;
 	u32 vm_exit;
+	u32 pin;
 };
 
 /*
@@ -423,6 +424,7 @@ static inline void lname##_controls_clearbit(struct vcpu_vmx *vmx, u32 val) \
 }
 BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS)
 BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS)
+BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL)
 
 static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 {

commit 70f932ecdfe6b593ef6784d55d2c096aafac1510
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:17:54 2019 -0700

    KVM: VMX: Add builder macros for shadowing controls
    
    ... to pave the way for shadowing all (five) major VMCS control fields
    without massive amounts of error prone copy+paste+modify.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 7f67f327204a..db4f9289d5da 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -85,6 +85,11 @@ struct pt_desc {
 	struct pt_ctx guest;
 };
 
+struct vmx_controls_shadow {
+	u32 vm_entry;
+	u32 vm_exit;
+};
+
 /*
  * The nested_vmx structure is part of vcpu_vmx, and holds information we need
  * for correct emulation of VMX (i.e., nested VMX) on this vcpu.
@@ -200,6 +205,9 @@ struct vcpu_vmx {
 	u32                   exit_intr_info;
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
+
+	struct vmx_controls_shadow	controls_shadow;
+
 	struct shared_msr_entry *guest_msrs;
 	int                   nmsrs;
 	int                   save_nmsrs;
@@ -211,8 +219,6 @@ struct vcpu_vmx {
 
 	u64		      spec_ctrl;
 
-	u32 vm_entry_controls_shadow;
-	u32 vm_exit_controls_shadow;
 	u32 secondary_exec_control;
 
 	/*
@@ -388,69 +394,35 @@ static inline u8 vmx_get_rvi(void)
 	return vmcs_read16(GUEST_INTR_STATUS) & 0xff;
 }
 
-static inline void vm_entry_controls_reset_shadow(struct vcpu_vmx *vmx)
-{
-	vmx->vm_entry_controls_shadow = vmcs_read32(VM_ENTRY_CONTROLS);
-}
-
-static inline void vm_entry_controls_init(struct vcpu_vmx *vmx, u32 val)
-{
-	vmcs_write32(VM_ENTRY_CONTROLS, val);
-	vmx->vm_entry_controls_shadow = val;
-}
-
-static inline void vm_entry_controls_set(struct vcpu_vmx *vmx, u32 val)
-{
-	if (vmx->vm_entry_controls_shadow != val)
-		vm_entry_controls_init(vmx, val);
-}
-
-static inline u32 vm_entry_controls_get(struct vcpu_vmx *vmx)
-{
-	return vmx->vm_entry_controls_shadow;
-}
-
-static inline void vm_entry_controls_setbit(struct vcpu_vmx *vmx, u32 val)
-{
-	vm_entry_controls_set(vmx, vm_entry_controls_get(vmx) | val);
-}
-
-static inline void vm_entry_controls_clearbit(struct vcpu_vmx *vmx, u32 val)
-{
-	vm_entry_controls_set(vmx, vm_entry_controls_get(vmx) & ~val);
-}
-
-static inline void vm_exit_controls_reset_shadow(struct vcpu_vmx *vmx)
-{
-	vmx->vm_exit_controls_shadow = vmcs_read32(VM_EXIT_CONTROLS);
-}
-
-static inline void vm_exit_controls_init(struct vcpu_vmx *vmx, u32 val)
-{
-	vmcs_write32(VM_EXIT_CONTROLS, val);
-	vmx->vm_exit_controls_shadow = val;
-}
-
-static inline void vm_exit_controls_set(struct vcpu_vmx *vmx, u32 val)
-{
-	if (vmx->vm_exit_controls_shadow != val)
-		vm_exit_controls_init(vmx, val);
-}
-
-static inline u32 vm_exit_controls_get(struct vcpu_vmx *vmx)
-{
-	return vmx->vm_exit_controls_shadow;
-}
-
-static inline void vm_exit_controls_setbit(struct vcpu_vmx *vmx, u32 val)
-{
-	vm_exit_controls_set(vmx, vm_exit_controls_get(vmx) | val);
-}
-
-static inline void vm_exit_controls_clearbit(struct vcpu_vmx *vmx, u32 val)
-{
-	vm_exit_controls_set(vmx, vm_exit_controls_get(vmx) & ~val);
+#define BUILD_CONTROLS_SHADOW(lname, uname)				    \
+static inline void lname##_controls_reset_shadow(struct vcpu_vmx *vmx)	    \
+{									    \
+	vmx->controls_shadow.lname = vmcs_read32(uname);		    \
+}									    \
+static inline void lname##_controls_init(struct vcpu_vmx *vmx, u32 val)	    \
+{									    \
+	vmcs_write32(uname, val);					    \
+	vmx->controls_shadow.lname = val;				    \
+}									    \
+static inline void lname##_controls_set(struct vcpu_vmx *vmx, u32 val)	    \
+{									    \
+	if (vmx->controls_shadow.lname != val)				    \
+		lname##_controls_init(vmx, val);			    \
+}									    \
+static inline u32 lname##_controls_get(struct vcpu_vmx *vmx)		    \
+{									    \
+	return vmx->controls_shadow.lname;				    \
+}									    \
+static inline void lname##_controls_setbit(struct vcpu_vmx *vmx, u32 val)   \
+{									    \
+	lname##_controls_set(vmx, lname##_controls_get(vmx) | val);	    \
+}									    \
+static inline void lname##_controls_clearbit(struct vcpu_vmx *vmx, u32 val) \
+{									    \
+	lname##_controls_set(vmx, lname##_controls_get(vmx) & ~val);	    \
 }
+BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS)
+BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS)
 
 static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 {

commit c075c3e49d7ae3599106f1af53352268030469db
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 12:17:53 2019 -0700

    KVM: nVMX: Use adjusted pin controls for vmcs02
    
    KVM provides a module parameter to allow disabling virtual NMI support
    to simplify testing (hardware *without* virtual NMI support is hard to
    come by but it does have users).  When preparing vmcs02, use the accessor
    for pin controls to ensure that the module param is respected for nested
    guests.
    
    Opportunistically swap the order of applying L0's and L1's pin controls
    to better align with other controls and to prepare for a future patche
    that will ignore L1's, but not L0's, preemption timer flag.
    
    Fixes: d02fcf50779ec ("kvm: vmx: Allow disabling virtual NMI support")
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 613f272cdcf8..7f67f327204a 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -480,6 +480,7 @@ static inline u32 vmx_vmexit_ctrl(void)
 }
 
 u32 vmx_exec_control(struct vcpu_vmx *vmx);
+u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx);
 
 static inline struct kvm_vmx *to_kvm_vmx(struct kvm *kvm)
 {

commit 8ef863e67a89c72b5af893a1214f6c35a5f456f8
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 09:06:32 2019 -0700

    KVM: nVMX: Don't reread VMCS-agnostic state when switching VMCS
    
    When switching between vmcs01 and vmcs02, there is no need to update
    state tracking for values that aren't tied to any particular VMCS as
    the per-vCPU values are already up-to-date (vmx_switch_vmcs() can only
    be called when the vCPU is loaded).
    
    Avoiding the update eliminates a RDMSR, and potentially a RDPKRU and
    posted-interrupt update (cmpxchg64() and more).
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 0bd598c47aec..613f272cdcf8 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -302,6 +302,7 @@ struct kvm_vmx {
 };
 
 bool nested_vmx_allowed(struct kvm_vcpu *vcpu);
+void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu);
 void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 int allocate_vpid(void);
 void free_vpid(int vpid);

commit 13b964a29d66333c1957dbd51f2c9e138c546f7a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 09:06:31 2019 -0700

    KVM: nVMX: Don't "put" vCPU or host state when switching VMCS
    
    When switching between vmcs01 and vmcs02, KVM isn't actually switching
    between guest and host.  If guest state is already loaded (the likely,
    if not guaranteed, case), keep the guest state loaded and manually swap
    the loaded_cpu_state pointer after propagating saved host state to the
    new vmcs0{1,2}.
    
    Avoiding the switch between guest and host reduces the latency of
    switching between vmcs01 and vmcs02 by several hundred cycles, and
    reduces the roundtrip time of a nested VM by upwards of 1000 cycles.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 4c5c24f37c5f..0bd598c47aec 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -303,11 +303,12 @@ struct kvm_vmx {
 
 bool nested_vmx_allowed(struct kvm_vcpu *vcpu);
 void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
-void vmx_vcpu_put(struct kvm_vcpu *vcpu);
 int allocate_vpid(void);
 void free_vpid(int vpid);
 void vmx_set_constant_host_state(struct vcpu_vmx *vmx);
 void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu);
+void vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,
+			unsigned long fs_base, unsigned long gs_base);
 int vmx_get_cpl(struct kvm_vcpu *vcpu);
 unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu);
 void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);

commit b464f57e133d8c751ca1fb4af039c808b873876b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jun 7 19:00:14 2019 +0200

    KVM: VMX: simplify vmx_prepare_switch_to_{guest,host}
    
    vmx->loaded_cpu_state can only be NULL or equal to vmx->loaded_vmcs,
    so change it to a bool.  Because the direction of the bool is
    now the opposite of vmx->guest_msrs_dirty, change the direction of
    vmx->guest_msrs_dirty so that they match.
    
    Finally, do not imply that MSRs have to be reloaded when
    vmx->guest_state_loaded is false; instead, set vmx->guest_msrs_ready
    to false explicitly in vmx_prepare_switch_to_host.
    
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index f03af64e9934..4c5c24f37c5f 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -187,13 +187,23 @@ struct vcpu_vmx {
 	struct kvm_vcpu       vcpu;
 	u8                    fail;
 	u8		      msr_bitmap_mode;
+
+	/*
+	 * If true, host state has been stored in vmx->loaded_vmcs for
+	 * the CPU registers that only need to be switched when transitioning
+	 * to/from the kernel, and the registers have been loaded with guest
+	 * values.  If false, host state is loaded in the CPU registers
+	 * and vmx->loaded_vmcs->host_state is invalid.
+	 */
+	bool		      guest_state_loaded;
+
 	u32                   exit_intr_info;
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 	struct shared_msr_entry *guest_msrs;
 	int                   nmsrs;
 	int                   save_nmsrs;
-	bool                  guest_msrs_dirty;
+	bool                  guest_msrs_ready;
 #ifdef CONFIG_X86_64
 	u64		      msr_host_kernel_gs_base;
 	u64		      msr_guest_kernel_gs_base;
@@ -208,14 +218,10 @@ struct vcpu_vmx {
 	/*
 	 * loaded_vmcs points to the VMCS currently used in this vcpu. For a
 	 * non-nested (L1) guest, it always points to vmcs01. For a nested
-	 * guest (L2), it points to a different VMCS.  loaded_cpu_state points
-	 * to the VMCS whose state is loaded into the CPU registers that only
-	 * need to be switched when transitioning to/from the kernel; a NULL
-	 * value indicates that host state is loaded.
+	 * guest (L2), it points to a different VMCS.
 	 */
 	struct loaded_vmcs    vmcs01;
 	struct loaded_vmcs   *loaded_vmcs;
-	struct loaded_vmcs   *loaded_cpu_state;
 
 	struct msr_autoload {
 		struct vmx_msrs guest;

commit 7952d769c29caac4fb51c1f0f0c12434c805b127
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 08:36:29 2019 -0700

    KVM: nVMX: Sync rarely accessed guest fields only when needed
    
    Many guest fields are rarely read (or written) by VMMs, i.e. likely
    aren't accessed between runs of a nested VMCS.  Delay pulling rarely
    accessed guest fields from vmcs02 until they are VMREAD or until vmcs12
    is dirtied.  The latter case is necessary because nested VM-Entry will
    consume all manner of fields when vmcs12 is dirty, e.g. for consistency
    checks.
    
    Note, an alternative to synchronizing all guest fields on VMREAD would
    be to read *only* the field being accessed, but switching VMCS pointers
    is expensive and odds are good if one guest field is being accessed then
    others will soon follow, or that vmcs12 will be dirtied due to a VMWRITE
    (see above).  And the full synchronization results in slightly cleaner
    code.
    
    Note, although GUEST_PDPTRs are relevant only for a 32-bit PAE guest,
    they are accessed quite frequently for said guests, and a separate patch
    is in flight to optimize away GUEST_PDTPR synchronziation for non-PAE
    guests.
    
    Skipping rarely accessed guest fields reduces the latency of a nested
    VM-Exit by ~200 cycles.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index f4448292df0f..f03af64e9934 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -109,6 +109,7 @@ struct nested_vmx {
 	 * to guest memory during VM exit.
 	 */
 	struct vmcs12 *cached_shadow_vmcs12;
+
 	/*
 	 * Indicates if the shadow vmcs or enlightened vmcs must be updated
 	 * with the data held by struct vmcs12.
@@ -116,6 +117,12 @@ struct nested_vmx {
 	bool need_vmcs12_to_shadow_sync;
 	bool dirty_vmcs12;
 
+	/*
+	 * Indicates lazily loaded guest state has not yet been decached from
+	 * vmcs02.
+	 */
+	bool need_sync_vmcs02_to_vmcs12_rare;
+
 	/*
 	 * vmcs02 has been initialized, i.e. state that is constant for
 	 * vmcs02 has been written to the backing VMCS.  Initialization

commit 3731905ef28fc1a9240d1532b2a9efbaea205dc0
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue May 7 08:36:27 2019 -0700

    KVM: nVMX: Use descriptive names for VMCS sync functions and flags
    
    Nested virtualization involves copying data between many different types
    of VMCSes, e.g. vmcs02, vmcs12, shadow VMCS and eVMCS.  Rename a variety
    of functions and flags to document both the source and destination of
    each sync.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index decd31055da8..f4448292df0f 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -113,7 +113,7 @@ struct nested_vmx {
 	 * Indicates if the shadow vmcs or enlightened vmcs must be updated
 	 * with the data held by struct vmcs12.
 	 */
-	bool need_vmcs12_sync;
+	bool need_vmcs12_to_shadow_sync;
 	bool dirty_vmcs12;
 
 	/*

commit 2342080cd6752fd40958f5a2aee0fb496ae92dce
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Apr 19 22:50:57 2019 -0700

    KVM: VMX: Store the host kernel's IDT base in a global variable
    
    Although the kernel may use multiple IDTs, KVM should only ever see the
    "real" IDT, e.g. the early init IDT is long gone by the time KVM runs
    and the debug stack IDT is only used for small windows of time in very
    specific flows.
    
    Before commit a547c6db4d2f1 ("KVM: VMX: Enable acknowledge interupt on
    vmexit"), the kernel's IDT base was consumed by KVM only when setting
    constant VMCS state, i.e. to set VMCS.HOST_IDTR_BASE.  Because constant
    host state is done once per vCPU, there was ostensibly no need to cache
    the kernel's IDT base.
    
    When support for "ack interrupt on exit" was introduced, KVM added a
    second consumer of the IDT base as handling already-acked interrupts
    requires directly calling the interrupt handler, i.e. KVM uses the IDT
    base to find the address of the handler.  Because interrupts are a fast
    path, KVM cached the IDT base to avoid having to VMREAD HOST_IDTR_BASE.
    Presumably, the IDT base was cached on a per-vCPU basis simply because
    the existing code grabbed the IDT base on a per-vCPU (VMCS) basis.
    
    Note, all post-boot IDTs use the same handlers for external interrupts,
    i.e. the "ack interrupt on exit" use of the IDT base would be unaffected
    even if the cached IDT somehow did not match the current IDT.  And as
    for the original use case of setting VMCS.HOST_IDTR_BASE, if any of the
    above analysis is wrong then KVM has had a bug since the beginning of
    time since KVM has effectively been caching the IDT at vCPU creation
    since commit a8b732ca01c ("[PATCH] kvm: userspace interface").
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 1cdaa5af8245..decd31055da8 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -187,7 +187,6 @@ struct vcpu_vmx {
 	int                   nmsrs;
 	int                   save_nmsrs;
 	bool                  guest_msrs_dirty;
-	unsigned long	      host_idt_base;
 #ifdef CONFIG_X86_64
 	u64		      msr_host_kernel_gs_base;
 	u64		      msr_guest_kernel_gs_base;

commit 73f624f47c495d7129abef4b7031ed371cc7abb6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 6 14:32:59 2019 +0200

    KVM: x86: move MSR_IA32_POWER_CTL handling to common code
    
    Make it available to AMD hosts as well, just in case someone is trying
    to use an Intel processor's CPUID setup.
    
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 61128b48c503..1cdaa5af8245 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -260,8 +260,6 @@ struct vcpu_vmx {
 
 	unsigned long host_debugctlmsr;
 
-	u64 msr_ia32_power_ctl;
-
 	/*
 	 * Only bits masked by msr_ia32_feature_control_valid_bits can be set in
 	 * msr_ia32_feature_control. FEATURE_CONTROL_LOCKED is always included

commit 4d259965655c92053f3255aca14d81aab1e21219
Author: Yi Wang <wang.yi59@zte.com.cn>
Date:   Mon May 20 12:27:47 2019 +0800

    kvm: vmx: Fix -Wmissing-prototypes warnings
    
    We get a warning when build kernel W=1:
    arch/x86/kvm/vmx/vmx.c:6365:6: warning: no previous prototype for ‘vmx_update_host_rsp’ [-Wmissing-prototypes]
     void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
    
    Add the missing declaration to fix this.
    
    Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 63d37ccce3dc..61128b48c503 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -319,6 +319,7 @@ void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked);
 void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu);
 struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr);
 void pt_update_intercept_for_msr(struct vcpu_vmx *vmx);
+void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp);
 
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1

commit dee9c0493108b36e89d289c8fd6f4c90321d0d5e
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Thu Jan 31 21:24:42 2019 +0100

    KVM/nVMX: Use kvm_vcpu_map for accessing the enlightened VMCS
    
    Use kvm_vcpu_map for accessing the enlightened VMCS since using
    kvm_vcpu_gpa_to_page() and kmap() will only work for guest memory that has
    a "struct page".
    
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index c0ff305d59f7..63d37ccce3dc 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -172,7 +172,7 @@ struct nested_vmx {
 	} smm;
 
 	gpa_t hv_evmcs_vmptr;
-	struct page *hv_evmcs_page;
+	struct kvm_host_map hv_evmcs_map;
 	struct hv_enlightened_vmcs *hv_evmcs;
 };
 

commit 3278e0492554895509530d493fbfa9a9f1b27a41
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Thu Jan 31 21:24:38 2019 +0100

    KVM/nVMX: Use kvm_vcpu_map when mapping the posted interrupt descriptor table
    
    Use kvm_vcpu_map when mapping the posted interrupt descriptor table since
    using kvm_vcpu_gpa_to_page() and kmap() will only work for guest memory
    that has a "struct page".
    
    One additional semantic change is that the virtual host mapping lifecycle
    has changed a bit. It now has the same lifetime of the pinning of the
    interrupt descriptor table page on the host side.
    
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index b03b18cf9b6b..c0ff305d59f7 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -143,7 +143,7 @@ struct nested_vmx {
 	 */
 	struct page *apic_access_page;
 	struct kvm_host_map virtual_apic_map;
-	struct page *pi_desc_page;
+	struct kvm_host_map pi_desc_map;
 
 	struct kvm_host_map msr_bitmap_map;
 

commit 96c66e87deeeb3cc78a3b82a1de8e365eec206c1
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Thu Jan 31 21:24:37 2019 +0100

    KVM/nVMX: Use kvm_vcpu_map when mapping the virtual APIC page
    
    Use kvm_vcpu_map when mapping the virtual APIC page since using
    kvm_vcpu_gpa_to_page() and kmap() will only work for guest memory that has
    a "struct page".
    
    One additional semantic change is that the virtual host mapping lifecycle
    has changed a bit. It now has the same lifetime of the pinning of the
    virtual APIC page on the host side.
    
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index b07d4b1d63cf..b03b18cf9b6b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -142,7 +142,7 @@ struct nested_vmx {
 	 * pointers, so we must keep them pinned while L2 runs.
 	 */
 	struct page *apic_access_page;
-	struct page *virtual_apic_page;
+	struct kvm_host_map virtual_apic_map;
 	struct page *pi_desc_page;
 
 	struct kvm_host_map msr_bitmap_map;

commit 31f0b6c4ba7da19192492b988f06f27bbe259082
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Thu Jan 31 21:24:36 2019 +0100

    KVM/nVMX: Use kvm_vcpu_map when mapping the L1 MSR bitmap
    
    Use kvm_vcpu_map when mapping the L1 MSR bitmap since using
    kvm_vcpu_gpa_to_page() and kmap() will only work for guest memory that has
    a "struct page".
    
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 1e42f983e0f1..b07d4b1d63cf 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -144,6 +144,9 @@ struct nested_vmx {
 	struct page *apic_access_page;
 	struct page *virtual_apic_page;
 	struct page *pi_desc_page;
+
+	struct kvm_host_map msr_bitmap_map;
+
 	struct pi_desc *pi_desc;
 	bool pi_pending;
 	u16 posted_intr_nv;

commit 6c6a2ab962af8f197984c45d585814f9839e86d5
Author: Liran Alon <liran.alon@oracle.com>
Date:   Mon Apr 15 18:45:26 2019 +0300

    KVM: VMX: Nop emulation of MSR_IA32_POWER_CTL
    
    Since commits 668fffa3f838 ("kvm: better MWAIT emulation for guestsâ€)
    and 4d5422cea3b6 ("KVM: X86: Provide a capability to disable MWAIT interceptsâ€),
    KVM was modified to allow an admin to configure certain guests to execute
    MONITOR/MWAIT inside guest without being intercepted by host.
    
    This is useful in case admin wishes to allocate a dedicated logical
    processor for each vCPU thread. Thus, making it safe for guest to
    completely control the power-state of the logical processor.
    
    The ability to use this new KVM capability was introduced to QEMU by
    commits 6f131f13e68d ("kvm: support -overcommit cpu-pm=on|offâ€) and
    2266d4431132 ("i386/cpu: make -cpu host support monitor/mwaitâ€).
    
    However, exposing MONITOR/MWAIT to a Linux guest may cause it's intel_idle
    kernel module to execute c1e_promotion_disable() which will attempt to
    RDMSR/WRMSR from/to MSR_IA32_POWER_CTL to manipulate the "C1E Enable"
    bit. This behaviour was introduced by commit
    32e9518005c8 ("intel_idle: export both C1 and C1Eâ€).
    
    Becuase KVM doesn't emulate this MSR, running KVM with ignore_msrs=0
    will cause the above guest behaviour to raise a #GP which will cause
    guest to kernel panic.
    
    Therefore, add support for nop emulation of MSR_IA32_POWER_CTL to
    avoid #GP in guest in this scenario.
    
    Future commits can optimise emulation further by reflecting guest
    MSR changes to host MSR to provide guest with the ability to
    fine-tune the dedicated logical processor power-state.
    
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index f879529906b4..1e42f983e0f1 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -257,6 +257,8 @@ struct vcpu_vmx {
 
 	unsigned long host_debugctlmsr;
 
+	u64 msr_ia32_power_ctl;
+
 	/*
 	 * Only bits masked by msr_ia32_feature_control_valid_bits can be set in
 	 * msr_ia32_feature_control. FEATURE_CONTROL_LOCKED is always included

commit 690908104e39d37947f89d76388c876ce4ec5fda
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Apr 15 15:16:17 2019 +0200

    KVM: nVMX: allow tests to use bad virtual-APIC page address
    
    As mentioned in the comment, there are some special cases where we can simply
    clear the TPR shadow bit from the CPU-based execution controls in the vmcs02.
    Handle them so that we can remove some XFAILs from vmx.flat.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index a1e00d0a2482..f879529906b4 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -517,4 +517,6 @@ static inline void decache_tsc_multiplier(struct vcpu_vmx *vmx)
 	vmcs_write64(TSC_MULTIPLIER, vmx->current_tsc_ratio);
 }
 
+void dump_vmcs(void);
+
 #endif /* __KVM_X86_VMX_H */

commit 0cf9135b773bf32fba9dd8e6699c1b331ee4b749
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Mar 7 15:43:02 2019 -0800

    KVM: x86: Emulate MSR_IA32_ARCH_CAPABILITIES on AMD hosts
    
    The CPUID flag ARCH_CAPABILITIES is unconditioinally exposed to host
    userspace for all x86 hosts, i.e. KVM advertises ARCH_CAPABILITIES
    regardless of hardware support under the pretense that KVM fully
    emulates MSR_IA32_ARCH_CAPABILITIES.  Unfortunately, only VMX hosts
    handle accesses to MSR_IA32_ARCH_CAPABILITIES (despite KVM_GET_MSRS
    also reporting MSR_IA32_ARCH_CAPABILITIES for all hosts).
    
    Move the MSR_IA32_ARCH_CAPABILITIES handling to common x86 code so
    that it's emulated on AMD hosts.
    
    Fixes: 1eaafe91a0df4 ("kvm: x86: IA32_ARCH_CAPABILITIES is always supported")
    Cc: stable@vger.kernel.org
    Reported-by: Xiaoyao Li <xiaoyao.li@linux.intel.com>
    Cc: Jim Mattson <jmattson@google.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 1554cb45b393..a1e00d0a2482 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -190,7 +190,6 @@ struct vcpu_vmx {
 	u64		      msr_guest_kernel_gs_base;
 #endif
 
-	u64		      arch_capabilities;
 	u64		      spec_ctrl;
 
 	u32 vm_entry_controls_shadow;

commit 4183683918efc3549b5ebddde4ed5edfdac45c17
Author: Ben Gardon <bgardon@google.com>
Date:   Mon Feb 11 11:02:52 2019 -0800

    kvm: vmx: Add memcg accounting to KVM allocations
    
    There are many KVM kernel memory allocations which are tied to the life of
    the VM process and should be charged to the VM process's cgroup. If the
    allocations aren't tied to the process, the OOM killer will not know
    that killing the process will free the associated kernel memory.
    Add __GFP_ACCOUNT flags to many of the allocations which are not yet being
    charged to the VM process's cgroup.
    
    Tested:
            Ran all kvm-unit-tests on a 64 bit Haswell machine, the patch
            introduced no new failures.
            Ran a kernel memory accounting test which creates a VM to touch
            memory and then checks that the kernel memory allocated for the
            process is within certain bounds.
            With this patch we account for much more of the vmalloc and slab memory
            allocated for the VM.
    
    Signed-off-by: Ben Gardon <bgardon@google.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index d7d1048d221b..1554cb45b393 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -479,7 +479,7 @@ static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 	return &(to_vmx(vcpu)->pi_desc);
 }
 
-struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu);
+struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu, gfp_t flags);
 void free_vmcs(struct vmcs *vmcs);
 int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
 void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
@@ -488,7 +488,8 @@ void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs);
 
 static inline struct vmcs *alloc_vmcs(bool shadow)
 {
-	return alloc_vmcs_cpu(shadow, raw_smp_processor_id());
+	return alloc_vmcs_cpu(shadow, raw_smp_processor_id(),
+			      GFP_KERNEL_ACCOUNT);
 }
 
 u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);

commit d92935979adba274b1099e67b7f713f6d8413121
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Thu Jan 31 11:26:39 2019 +0800

    kvm: vmx: Fix typos in vmentry/vmexit control setting
    
    Previously, 'commit f99e3daf94ff ("KVM: x86: Add Intel PT
    virtualization work mode")' work mode' offered framework
    to support Intel PT virtualization. However, the patch has
    some typos in vmx_vmentry_ctrl() and vmx_vmexit_ctrl(), e.g.
    used wrong flags and wrong variable, which will cause the
    VM entry failure later.
    
    Fixes: 'commit f99e3daf94ff ("KVM: x86: Add Intel PT virtualization work mode")'
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index ec23b4d65fb7..d7d1048d221b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -444,7 +444,8 @@ static inline u32 vmx_vmentry_ctrl(void)
 {
 	u32 vmentry_ctrl = vmcs_config.vmentry_ctrl;
 	if (pt_mode == PT_MODE_SYSTEM)
-		vmentry_ctrl &= ~(VM_EXIT_PT_CONCEAL_PIP | VM_EXIT_CLEAR_IA32_RTIT_CTL);
+		vmentry_ctrl &= ~(VM_ENTRY_PT_CONCEAL_PIP |
+				  VM_ENTRY_LOAD_IA32_RTIT_CTL);
 	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
 	return vmentry_ctrl &
 		~(VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL | VM_ENTRY_LOAD_IA32_EFER);
@@ -454,9 +455,10 @@ static inline u32 vmx_vmexit_ctrl(void)
 {
 	u32 vmexit_ctrl = vmcs_config.vmexit_ctrl;
 	if (pt_mode == PT_MODE_SYSTEM)
-		vmexit_ctrl &= ~(VM_ENTRY_PT_CONCEAL_PIP | VM_ENTRY_LOAD_IA32_RTIT_CTL);
+		vmexit_ctrl &= ~(VM_EXIT_PT_CONCEAL_PIP |
+				 VM_EXIT_CLEAR_IA32_RTIT_CTL);
 	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
-	return vmcs_config.vmexit_ctrl &
+	return vmexit_ctrl &
 		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);
 }
 

commit 81b016676e1c8f58027bd4d2b1d8a981776b36fe
Author: Luwei Kang <luwei.kang@intel.com>
Date:   Thu Jan 31 16:52:02 2019 +0800

    KVM: x86: Sync the pending Posted-Interrupts
    
    Some Posted-Interrupts from passthrough devices may be lost or
    overwritten when the vCPU is in runnable state.
    
    The SN (Suppress Notification) of PID (Posted Interrupt Descriptor) will
    be set when the vCPU is preempted (vCPU in KVM_MP_STATE_RUNNABLE state
    but not running on physical CPU). If a posted interrupt coming at this
    time, the irq remmaping facility will set the bit of PIR (Posted
    Interrupt Requests) without ON (Outstanding Notification).
    So this interrupt can't be sync to APIC virtualization register and
    will not be handled by Guest because ON is zero.
    
    Signed-off-by: Luwei Kang <luwei.kang@intel.com>
    [Eliminate the pi_clear_sn fast path. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 6ee6a492efaf..ec23b4d65fb7 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -336,16 +336,16 @@ static inline int pi_test_and_set_pir(int vector, struct pi_desc *pi_desc)
 	return test_and_set_bit(vector, (unsigned long *)pi_desc->pir);
 }
 
-static inline void pi_clear_sn(struct pi_desc *pi_desc)
+static inline void pi_set_sn(struct pi_desc *pi_desc)
 {
-	return clear_bit(POSTED_INTR_SN,
-			(unsigned long *)&pi_desc->control);
+	set_bit(POSTED_INTR_SN,
+		(unsigned long *)&pi_desc->control);
 }
 
-static inline void pi_set_sn(struct pi_desc *pi_desc)
+static inline void pi_set_on(struct pi_desc *pi_desc)
 {
-	return set_bit(POSTED_INTR_SN,
-			(unsigned long *)&pi_desc->control);
+	set_bit(POSTED_INTR_ON,
+		(unsigned long *)&pi_desc->control);
 }
 
 static inline void pi_clear_on(struct pi_desc *pi_desc)

commit c9afc58cc368623aaa34a62e321eea2a59240f6f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jan 25 07:41:05 2019 -0800

    KVM: VMX: Pass "launched" directly to the vCPU-run asm blob
    
    ...and remove struct vcpu_vmx's temporary __launched variable.
    
    Eliminating __launched is a bonus, the real motivation is to get to the
    point where the only reference to struct vcpu_vmx in the asm code is
    to vcpu.arch.regs, which will simplify moving the blob to a proper asm
    file.  Note that also means this approach is deliberately different than
    what is used in nested_vmx_check_vmentry_hw().
    
    Use BL as it is a callee-save register in both 32-bit and 64-bit ABIs,
    i.e. it can't be modified by vmx_update_host_rsp(), to avoid having to
    temporarily save/restore the launched flag.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 8e203b725928..6ee6a492efaf 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -208,7 +208,7 @@ struct vcpu_vmx {
 	struct loaded_vmcs    vmcs01;
 	struct loaded_vmcs   *loaded_vmcs;
 	struct loaded_vmcs   *loaded_cpu_state;
-	bool                  __launched; /* temporary, used in vmx_vcpu_run */
+
 	struct msr_autoload {
 		struct vmx_msrs guest;
 		struct vmx_msrs host;

commit 5a8781607e677eda60b20e0a4c91d2a5f12f9244
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jan 25 07:41:02 2019 -0800

    KVM: nVMX: Cache host_rsp on a per-VMCS basis
    
    Currently, host_rsp is cached on a per-vCPU basis, i.e. it's stored in
    struct vcpu_vmx.  In non-nested usage the caching is for all intents
    and purposes 100% effective, e.g. only the first VMLAUNCH needs to
    synchronize VMCS.HOST_RSP since the call stack to vmx_vcpu_run() is
    identical each and every time.  But when running a nested guest, KVM
    must invalidate the cache when switching the current VMCS as it can't
    guarantee the new VMCS has the same HOST_RSP as the previous VMCS.  In
    other words, the cache loses almost all of its efficacy when running a
    nested VM.
    
    Move host_rsp to struct vmcs_host_state, which is per-VMCS, so that it
    is cached on a per-VMCS basis and restores its 100% hit rate when
    nested VMs are in play.
    
    Note that the host_rsp cache for vmcs02 essentially "breaks" when
    nested early checks are enabled as nested_vmx_check_vmentry_hw() will
    see a different RSP at the time of its VM-Enter.  While it's possible
    to avoid even that VMCS.HOST_RSP synchronization, e.g. by employing a
    dedicated VM-Exit stack, there is little motivation for doing so as
    the overhead of two VMWRITEs (~55 cycles) is dwarfed by the overhead
    of the extra VMX transition (600+ cycles) and is a proverbial drop in
    the ocean relative to the total cost of a nested transtion (10s of
    thousands of cycles).
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 99328954c2fc..8e203b725928 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -175,7 +175,6 @@ struct nested_vmx {
 
 struct vcpu_vmx {
 	struct kvm_vcpu       vcpu;
-	unsigned long         host_rsp;
 	u8                    fail;
 	u8		      msr_bitmap_mode;
 	u32                   exit_intr_info;

commit 453eafbe65f72c04fc7c74c5c95c04e78e907dfb
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Dec 20 12:25:17 2018 -0800

    KVM: VMX: Move VM-Enter + VM-Exit handling to non-inline sub-routines
    
    Transitioning to/from a VMX guest requires KVM to manually save/load
    the bulk of CPU state that the guest is allowed to direclty access,
    e.g. XSAVE state, CR2, GPRs, etc...  For obvious reasons, loading the
    guest's GPR snapshot prior to VM-Enter and saving the snapshot after
    VM-Exit is done via handcoded assembly.  The assembly blob is written
    as inline asm so that it can easily access KVM-defined structs that
    are used to hold guest state, e.g. moving the blob to a standalone
    assembly file would require generating defines for struct offsets.
    
    The other relevant aspect of VMX transitions in KVM is the handling of
    VM-Exits.  KVM doesn't employ a separate VM-Exit handler per se, but
    rather treats the VMX transition as a mega instruction (with many side
    effects), i.e. sets the VMCS.HOST_RIP to a label immediately following
    VMLAUNCH/VMRESUME.  The label is then exposed to C code via a global
    variable definition in the inline assembly.
    
    Because of the global variable, KVM takes steps to (attempt to) ensure
    only a single instance of the owning C function, e.g. vmx_vcpu_run, is
    generated by the compiler.  The earliest approach placed the inline
    assembly in a separate noinline function[1].  Later, the assembly was
    folded back into vmx_vcpu_run() and tagged with __noclone[2][3], which
    is still used today.
    
    After moving to __noclone, an edge case was encountered where GCC's
    -ftracer optimization resulted in the inline assembly blob being
    duplicated.  This was "fixed" by explicitly disabling -ftracer in the
    __noclone definition[4].
    
    Recently, it was found that disabling -ftracer causes build warnings
    for unsuspecting users of __noclone[5], and more importantly for KVM,
    prevents the compiler for properly optimizing vmx_vcpu_run()[6].  And
    perhaps most importantly of all, it was pointed out that there is no
    way to prevent duplication of a function with 100% reliability[7],
    i.e. more edge cases may be encountered in the future.
    
    So to summarize, the only way to prevent the compiler from duplicating
    the global variable definition is to move the variable out of inline
    assembly, which has been suggested several times over[1][7][8].
    
    Resolve the aforementioned issues by moving the VMLAUNCH+VRESUME and
    VM-Exit "handler" to standalone assembly sub-routines.  Moving only
    the core VMX transition codes allows the struct indexing to remain as
    inline assembly and also allows the sub-routines to be used by
    nested_vmx_check_vmentry_hw().  Reusing the sub-routines has a happy
    side-effect of eliminating two VMWRITEs in the nested_early_check path
    as there is no longer a need to dynamically change VMCS.HOST_RIP.
    
    Note that callers to vmx_vmenter() must account for the CALL modifying
    RSP, e.g. must subtract op-size from RSP when synchronizing RSP with
    VMCS.HOST_RSP and "restore" RSP prior to the CALL.  There are no great
    alternatives to fudging RSP.  Saving RSP in vmx_enter() is difficult
    because doing so requires a second register (VMWRITE does not provide
    an immediate encoding for the VMCS field and KVM supports Hyper-V's
    memory-based eVMCS ABI).  The other more drastic alternative would be
    to use eschew VMCS.HOST_RSP and manually save/load RSP using a per-cpu
    variable (which can be encoded as e.g. gs:[imm]).  But because a valid
    stack is needed at the time of VM-Exit (NMIs aren't blocked and a user
    could theoretically insert INT3/INT1ICEBRK at the VM-Exit handler), a
    dedicated per-cpu VM-Exit stack would be required.  A dedicated stack
    isn't difficult to implement, but it would require at least one page
    per CPU and knowledge of the stack in the dumpstack routines.  And in
    most cases there is essentially zero overhead in dynamically updating
    VMCS.HOST_RSP, e.g. the VMWRITE can be avoided for all but the first
    VMLAUNCH unless nested_early_check=1, which is not a fast path.  In
    other words, avoiding the VMCS.HOST_RSP by using a dedicated stack
    would only make the code marginally less ugly while requiring at least
    one page per CPU and forcing the kernel to be aware (and approve) of
    the VM-Exit stack shenanigans.
    
    [1] cea15c24ca39 ("KVM: Move KVM context switch into own function")
    [2] a3b5ba49a8c5 ("KVM: VMX: add the __noclone attribute to vmx_vcpu_run")
    [3] 104f226bfd0a ("KVM: VMX: Fold __vmx_vcpu_run() into vmx_vcpu_run()")
    [4] 95272c29378e ("compiler-gcc: disable -ftracer for __noclone functions")
    [5] https://lkml.kernel.org/r/20181218140105.ajuiglkpvstt3qxs@treble
    [6] https://patchwork.kernel.org/patch/8707981/#21817015
    [7] https://lkml.kernel.org/r/ri6y38lo23g.fsf@suse.cz
    [8] https://lkml.kernel.org/r/20181218212042.GE25620@tassilo.jf.intel.com
    
    Suggested-by: Andi Kleen <ak@linux.intel.com>
    Suggested-by: Martin Jambor <mjambor@suse.cz>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Nadav Amit <namit@vmware.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Martin Jambor <mjambor@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 20172c11d5f8..99328954c2fc 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -12,7 +12,6 @@
 #include "vmcs.h"
 
 extern const u32 vmx_msr_index[];
-extern const ulong vmx_return;
 extern u64 host_efer;
 
 #define MSR_TYPE_R	1

commit b08c28960f254bd246af8e30a468dfc7dd56e03b
Author: Chao Peng <chao.p.peng@linux.intel.com>
Date:   Wed Oct 24 16:05:15 2018 +0800

    KVM: x86: Set intercept for Intel PT MSRs read/write
    
    To save performance overhead, disable intercept Intel PT MSRs
    read/write when Intel PT is enabled in guest.
    MSR_IA32_RTIT_CTL is an exception that will always be intercepted.
    
    Signed-off-by: Chao Peng <chao.p.peng@linux.intel.com>
    Signed-off-by: Luwei Kang <luwei.kang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index dd3b9ab90556..20172c11d5f8 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -316,6 +316,7 @@ bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu);
 void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked);
 void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu);
 struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr);
+void pt_update_intercept_for_msr(struct vcpu_vmx *vmx);
 
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1

commit 2ef444f1600bfc2d8522df0f537aafef79befa7e
Author: Chao Peng <chao.p.peng@linux.intel.com>
Date:   Wed Oct 24 16:05:12 2018 +0800

    KVM: x86: Add Intel PT context switch for each vcpu
    
    Load/Store Intel Processor Trace register in context switch.
    MSR IA32_RTIT_CTL is loaded/stored automatically from VMCS.
    In Host-Guest mode, we need load/resore PT MSRs only when PT
    is enabled in guest.
    
    Signed-off-by: Chao Peng <chao.p.peng@linux.intel.com>
    Signed-off-by: Luwei Kang <luwei.kang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 86eb9c887386..dd3b9ab90556 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -66,6 +66,25 @@ struct pi_desc {
 	u32 rsvd[6];
 } __aligned(64);
 
+#define RTIT_ADDR_RANGE		4
+
+struct pt_ctx {
+	u64 ctl;
+	u64 status;
+	u64 output_base;
+	u64 output_mask;
+	u64 cr3_match;
+	u64 addr_a[RTIT_ADDR_RANGE];
+	u64 addr_b[RTIT_ADDR_RANGE];
+};
+
+struct pt_desc {
+	u64 ctl_bitmask;
+	u32 addr_range;
+	u32 caps[PT_CPUID_REGS_NUM * PT_CPUID_LEAVES];
+	struct pt_ctx host;
+	struct pt_ctx guest;
+};
 
 /*
  * The nested_vmx structure is part of vcpu_vmx, and holds information we need
@@ -249,6 +268,8 @@ struct vcpu_vmx {
 	u64 msr_ia32_feature_control;
 	u64 msr_ia32_feature_control_valid_bits;
 	u64 ept_pointer;
+
+	struct pt_desc pt_desc;
 };
 
 enum ept_pointers_status {

commit f99e3daf94ff35dd4a878d32ff66e1fd35223ad6
Author: Chao Peng <chao.p.peng@linux.intel.com>
Date:   Wed Oct 24 16:05:10 2018 +0800

    KVM: x86: Add Intel PT virtualization work mode
    
    Intel Processor Trace virtualization can be work in one
    of 2 possible modes:
    
    a. System-Wide mode (default):
       When the host configures Intel PT to collect trace packets
       of the entire system, it can leave the relevant VMX controls
       clear to allow VMX-specific packets to provide information
       across VMX transitions.
       KVM guest will not aware this feature in this mode and both
       host and KVM guest trace will output to host buffer.
    
    b. Host-Guest mode:
       Host can configure trace-packet generation while in
       VMX non-root operation for guests and root operation
       for native executing normally.
       Intel PT will be exposed to KVM guest in this mode, and
       the trace output to respective buffer of host and guest.
       In this mode, tht status of PT will be saved and disabled
       before VM-entry and restored after VM-exit if trace
       a virtual machine.
    
    Signed-off-by: Chao Peng <chao.p.peng@linux.intel.com>
    Signed-off-by: Luwei Kang <luwei.kang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index f932d7c971e9..86eb9c887386 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -5,6 +5,7 @@
 #include <linux/kvm_host.h>
 
 #include <asm/kvm.h>
+#include <asm/intel_pt.h>
 
 #include "capabilities.h"
 #include "ops.h"
@@ -421,13 +422,19 @@ static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 
 static inline u32 vmx_vmentry_ctrl(void)
 {
+	u32 vmentry_ctrl = vmcs_config.vmentry_ctrl;
+	if (pt_mode == PT_MODE_SYSTEM)
+		vmentry_ctrl &= ~(VM_EXIT_PT_CONCEAL_PIP | VM_EXIT_CLEAR_IA32_RTIT_CTL);
 	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
-	return vmcs_config.vmentry_ctrl &
+	return vmentry_ctrl &
 		~(VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL | VM_ENTRY_LOAD_IA32_EFER);
 }
 
 static inline u32 vmx_vmexit_ctrl(void)
 {
+	u32 vmexit_ctrl = vmcs_config.vmexit_ctrl;
+	if (pt_mode == PT_MODE_SYSTEM)
+		vmexit_ctrl &= ~(VM_ENTRY_PT_CONCEAL_PIP | VM_ENTRY_LOAD_IA32_RTIT_CTL);
 	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
 	return vmcs_config.vmexit_ctrl &
 		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);

commit 7c97fcb3b68cd4d48a071bc1929c753d255dea47
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Dec 3 13:53:17 2018 -0800

    KVM: VMX: Expose nested_vmx_allowed() to nested VMX as a non-inline
    
    Exposing only the function allows @nested, i.e. the module param, to be
    statically defined in vmx.c, ensuring we aren't unnecessarily checking
    said variable in the nested code.  nested_vmx_allowed() is exposed due
    to the need to verify nested support in vmx_{get,set}_nested_state().
    The downside is that nested_vmx_allowed() likely won't be inlined in
    vmx_{get,set}_nested_state(), but that should be a non-issue as they're
    not a hot path.  Keeping vmx_{get,set}_nested_state() in vmx.c isn't a
    viable option as they need access to several nested-only functions.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index ee2cbe64813d..f932d7c971e9 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -267,6 +267,7 @@ struct kvm_vmx {
 	spinlock_t ept_pointer_lock;
 };
 
+bool nested_vmx_allowed(struct kvm_vcpu *vcpu);
 void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void vmx_vcpu_put(struct kvm_vcpu *vcpu);
 int allocate_vpid(void);

commit 97b7ead392637247569818b6603e54b0a6277dd0
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Dec 3 13:53:16 2018 -0800

    KVM: VMX: Expose various getters and setters to nested VMX
    
    ...as they're used directly by the nested code.  This will allow
    moving the bulk of the nested code out of vmx.c without concurrent
    changes to vmx.h.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 5c105aa4dc35..ee2cbe64813d 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -267,6 +267,33 @@ struct kvm_vmx {
 	spinlock_t ept_pointer_lock;
 };
 
+void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
+void vmx_vcpu_put(struct kvm_vcpu *vcpu);
+int allocate_vpid(void);
+void free_vpid(int vpid);
+void vmx_set_constant_host_state(struct vcpu_vmx *vmx);
+void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu);
+int vmx_get_cpl(struct kvm_vcpu *vcpu);
+unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu);
+void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);
+u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu);
+void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask);
+void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer);
+void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
+void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
+int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
+void set_cr4_guest_host_mask(struct vcpu_vmx *vmx);
+void ept_save_pdptrs(struct kvm_vcpu *vcpu);
+void vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
+void vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
+u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);
+void update_exception_bitmap(struct kvm_vcpu *vcpu);
+void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu);
+bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu);
+void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked);
+void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu);
+struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr);
+
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1
 

commit cf3646eb3adfd9149e3c29fc765a59e8bd6ff82d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Dec 3 13:53:15 2018 -0800

    KVM: VMX: Expose misc variables needed for nested VMX
    
    Exposed vmx_msr_index, vmx_return and host_efer via vmx.h so that the
    nested code can be moved out of vmx.c.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 4c7fcf1571c3..5c105aa4dc35 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -10,6 +10,10 @@
 #include "ops.h"
 #include "vmcs.h"
 
+extern const u32 vmx_msr_index[];
+extern const ulong vmx_return;
+extern u64 host_efer;
+
 #define MSR_TYPE_R	1
 #define MSR_TYPE_W	2
 #define MSR_TYPE_RW	3

commit 89b0c9f58350f6820f062ea12000e8a171177f3b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Dec 3 13:53:07 2018 -0800

    KVM: VMX: Move VMX instruction wrappers to a dedicated header file
    
    VMX has a few hundred lines of code just to wrap various VMX specific
    instructions, e.g. VMWREAD, INVVPID, etc...  Move them to a dedicated
    header so it's easier to find/isolate the boilerplate.
    
    With this change, more inlines can be moved from vmx.c to vmx.h.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index c9aa150aa014..4c7fcf1571c3 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -7,6 +7,7 @@
 #include <asm/kvm.h>
 
 #include "capabilities.h"
+#include "ops.h"
 #include "vmcs.h"
 
 #define MSR_TYPE_R	1
@@ -312,6 +313,75 @@ static inline int pi_test_sn(struct pi_desc *pi_desc)
 			(unsigned long *)&pi_desc->control);
 }
 
+static inline u8 vmx_get_rvi(void)
+{
+	return vmcs_read16(GUEST_INTR_STATUS) & 0xff;
+}
+
+static inline void vm_entry_controls_reset_shadow(struct vcpu_vmx *vmx)
+{
+	vmx->vm_entry_controls_shadow = vmcs_read32(VM_ENTRY_CONTROLS);
+}
+
+static inline void vm_entry_controls_init(struct vcpu_vmx *vmx, u32 val)
+{
+	vmcs_write32(VM_ENTRY_CONTROLS, val);
+	vmx->vm_entry_controls_shadow = val;
+}
+
+static inline void vm_entry_controls_set(struct vcpu_vmx *vmx, u32 val)
+{
+	if (vmx->vm_entry_controls_shadow != val)
+		vm_entry_controls_init(vmx, val);
+}
+
+static inline u32 vm_entry_controls_get(struct vcpu_vmx *vmx)
+{
+	return vmx->vm_entry_controls_shadow;
+}
+
+static inline void vm_entry_controls_setbit(struct vcpu_vmx *vmx, u32 val)
+{
+	vm_entry_controls_set(vmx, vm_entry_controls_get(vmx) | val);
+}
+
+static inline void vm_entry_controls_clearbit(struct vcpu_vmx *vmx, u32 val)
+{
+	vm_entry_controls_set(vmx, vm_entry_controls_get(vmx) & ~val);
+}
+
+static inline void vm_exit_controls_reset_shadow(struct vcpu_vmx *vmx)
+{
+	vmx->vm_exit_controls_shadow = vmcs_read32(VM_EXIT_CONTROLS);
+}
+
+static inline void vm_exit_controls_init(struct vcpu_vmx *vmx, u32 val)
+{
+	vmcs_write32(VM_EXIT_CONTROLS, val);
+	vmx->vm_exit_controls_shadow = val;
+}
+
+static inline void vm_exit_controls_set(struct vcpu_vmx *vmx, u32 val)
+{
+	if (vmx->vm_exit_controls_shadow != val)
+		vm_exit_controls_init(vmx, val);
+}
+
+static inline u32 vm_exit_controls_get(struct vcpu_vmx *vmx)
+{
+	return vmx->vm_exit_controls_shadow;
+}
+
+static inline void vm_exit_controls_setbit(struct vcpu_vmx *vmx, u32 val)
+{
+	vm_exit_controls_set(vmx, vm_exit_controls_get(vmx) | val);
+}
+
+static inline void vm_exit_controls_clearbit(struct vcpu_vmx *vmx, u32 val)
+{
+	vm_exit_controls_set(vmx, vm_exit_controls_get(vmx) & ~val);
+}
+
 static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 {
 	vmx->segment_cache.bitmask = 0;
@@ -348,4 +418,42 @@ static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 	return &(to_vmx(vcpu)->pi_desc);
 }
 
+struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu);
+void free_vmcs(struct vmcs *vmcs);
+int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
+void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
+void loaded_vmcs_init(struct loaded_vmcs *loaded_vmcs);
+void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs);
+
+static inline struct vmcs *alloc_vmcs(bool shadow)
+{
+	return alloc_vmcs_cpu(shadow, raw_smp_processor_id());
+}
+
+u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);
+
+static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
+				bool invalidate_gpa)
+{
+	if (enable_ept && (invalidate_gpa || !enable_vpid)) {
+		if (!VALID_PAGE(vcpu->arch.mmu->root_hpa))
+			return;
+		ept_sync_context(construct_eptp(vcpu,
+						vcpu->arch.mmu->root_hpa));
+	} else {
+		vpid_sync_context(vpid);
+	}
+}
+
+static inline void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
+{
+	__vmx_flush_tlb(vcpu, to_vmx(vcpu)->vpid, invalidate_gpa);
+}
+
+static inline void decache_tsc_multiplier(struct vcpu_vmx *vmx)
+{
+	vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+	vmcs_write64(TSC_MULTIPLIER, vmx->current_tsc_ratio);
+}
+
 #endif /* __KVM_X86_VMX_H */

commit 8373d25d25d14fe644feae007c15a5a10cf8e888
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Dec 3 13:53:08 2018 -0800

    KVM: VMX: Add vmx.h to hold VMX definitions
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
new file mode 100644
index 000000000000..c9aa150aa014
--- /dev/null
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -0,0 +1,351 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __KVM_X86_VMX_H
+#define __KVM_X86_VMX_H
+
+#include <linux/kvm_host.h>
+
+#include <asm/kvm.h>
+
+#include "capabilities.h"
+#include "vmcs.h"
+
+#define MSR_TYPE_R	1
+#define MSR_TYPE_W	2
+#define MSR_TYPE_RW	3
+
+#define X2APIC_MSR(r) (APIC_BASE_MSR + ((r) >> 4))
+
+#define NR_AUTOLOAD_MSRS 8
+
+struct vmx_msrs {
+	unsigned int		nr;
+	struct vmx_msr_entry	val[NR_AUTOLOAD_MSRS];
+};
+
+struct shared_msr_entry {
+	unsigned index;
+	u64 data;
+	u64 mask;
+};
+
+enum segment_cache_field {
+	SEG_FIELD_SEL = 0,
+	SEG_FIELD_BASE = 1,
+	SEG_FIELD_LIMIT = 2,
+	SEG_FIELD_AR = 3,
+
+	SEG_FIELD_NR = 4
+};
+
+/* Posted-Interrupt Descriptor */
+struct pi_desc {
+	u32 pir[8];     /* Posted interrupt requested */
+	union {
+		struct {
+				/* bit 256 - Outstanding Notification */
+			u16	on	: 1,
+				/* bit 257 - Suppress Notification */
+				sn	: 1,
+				/* bit 271:258 - Reserved */
+				rsvd_1	: 14;
+				/* bit 279:272 - Notification Vector */
+			u8	nv;
+				/* bit 287:280 - Reserved */
+			u8	rsvd_2;
+				/* bit 319:288 - Notification Destination */
+			u32	ndst;
+		};
+		u64 control;
+	};
+	u32 rsvd[6];
+} __aligned(64);
+
+
+/*
+ * The nested_vmx structure is part of vcpu_vmx, and holds information we need
+ * for correct emulation of VMX (i.e., nested VMX) on this vcpu.
+ */
+struct nested_vmx {
+	/* Has the level1 guest done vmxon? */
+	bool vmxon;
+	gpa_t vmxon_ptr;
+	bool pml_full;
+
+	/* The guest-physical address of the current VMCS L1 keeps for L2 */
+	gpa_t current_vmptr;
+	/*
+	 * Cache of the guest's VMCS, existing outside of guest memory.
+	 * Loaded from guest memory during VMPTRLD. Flushed to guest
+	 * memory during VMCLEAR and VMPTRLD.
+	 */
+	struct vmcs12 *cached_vmcs12;
+	/*
+	 * Cache of the guest's shadow VMCS, existing outside of guest
+	 * memory. Loaded from guest memory during VM entry. Flushed
+	 * to guest memory during VM exit.
+	 */
+	struct vmcs12 *cached_shadow_vmcs12;
+	/*
+	 * Indicates if the shadow vmcs or enlightened vmcs must be updated
+	 * with the data held by struct vmcs12.
+	 */
+	bool need_vmcs12_sync;
+	bool dirty_vmcs12;
+
+	/*
+	 * vmcs02 has been initialized, i.e. state that is constant for
+	 * vmcs02 has been written to the backing VMCS.  Initialization
+	 * is delayed until L1 actually attempts to run a nested VM.
+	 */
+	bool vmcs02_initialized;
+
+	bool change_vmcs01_virtual_apic_mode;
+
+	/*
+	 * Enlightened VMCS has been enabled. It does not mean that L1 has to
+	 * use it. However, VMX features available to L1 will be limited based
+	 * on what the enlightened VMCS supports.
+	 */
+	bool enlightened_vmcs_enabled;
+
+	/* L2 must run next, and mustn't decide to exit to L1. */
+	bool nested_run_pending;
+
+	struct loaded_vmcs vmcs02;
+
+	/*
+	 * Guest pages referred to in the vmcs02 with host-physical
+	 * pointers, so we must keep them pinned while L2 runs.
+	 */
+	struct page *apic_access_page;
+	struct page *virtual_apic_page;
+	struct page *pi_desc_page;
+	struct pi_desc *pi_desc;
+	bool pi_pending;
+	u16 posted_intr_nv;
+
+	struct hrtimer preemption_timer;
+	bool preemption_timer_expired;
+
+	/* to migrate it to L2 if VM_ENTRY_LOAD_DEBUG_CONTROLS is off */
+	u64 vmcs01_debugctl;
+	u64 vmcs01_guest_bndcfgs;
+
+	u16 vpid02;
+	u16 last_vpid;
+
+	struct nested_vmx_msrs msrs;
+
+	/* SMM related state */
+	struct {
+		/* in VMX operation on SMM entry? */
+		bool vmxon;
+		/* in guest mode on SMM entry? */
+		bool guest_mode;
+	} smm;
+
+	gpa_t hv_evmcs_vmptr;
+	struct page *hv_evmcs_page;
+	struct hv_enlightened_vmcs *hv_evmcs;
+};
+
+struct vcpu_vmx {
+	struct kvm_vcpu       vcpu;
+	unsigned long         host_rsp;
+	u8                    fail;
+	u8		      msr_bitmap_mode;
+	u32                   exit_intr_info;
+	u32                   idt_vectoring_info;
+	ulong                 rflags;
+	struct shared_msr_entry *guest_msrs;
+	int                   nmsrs;
+	int                   save_nmsrs;
+	bool                  guest_msrs_dirty;
+	unsigned long	      host_idt_base;
+#ifdef CONFIG_X86_64
+	u64		      msr_host_kernel_gs_base;
+	u64		      msr_guest_kernel_gs_base;
+#endif
+
+	u64		      arch_capabilities;
+	u64		      spec_ctrl;
+
+	u32 vm_entry_controls_shadow;
+	u32 vm_exit_controls_shadow;
+	u32 secondary_exec_control;
+
+	/*
+	 * loaded_vmcs points to the VMCS currently used in this vcpu. For a
+	 * non-nested (L1) guest, it always points to vmcs01. For a nested
+	 * guest (L2), it points to a different VMCS.  loaded_cpu_state points
+	 * to the VMCS whose state is loaded into the CPU registers that only
+	 * need to be switched when transitioning to/from the kernel; a NULL
+	 * value indicates that host state is loaded.
+	 */
+	struct loaded_vmcs    vmcs01;
+	struct loaded_vmcs   *loaded_vmcs;
+	struct loaded_vmcs   *loaded_cpu_state;
+	bool                  __launched; /* temporary, used in vmx_vcpu_run */
+	struct msr_autoload {
+		struct vmx_msrs guest;
+		struct vmx_msrs host;
+	} msr_autoload;
+
+	struct {
+		int vm86_active;
+		ulong save_rflags;
+		struct kvm_segment segs[8];
+	} rmode;
+	struct {
+		u32 bitmask; /* 4 bits per segment (1 bit per field) */
+		struct kvm_save_segment {
+			u16 selector;
+			unsigned long base;
+			u32 limit;
+			u32 ar;
+		} seg[8];
+	} segment_cache;
+	int vpid;
+	bool emulation_required;
+
+	u32 exit_reason;
+
+	/* Posted interrupt descriptor */
+	struct pi_desc pi_desc;
+
+	/* Support for a guest hypervisor (nested VMX) */
+	struct nested_vmx nested;
+
+	/* Dynamic PLE window. */
+	int ple_window;
+	bool ple_window_dirty;
+
+	bool req_immediate_exit;
+
+	/* Support for PML */
+#define PML_ENTITY_NUM		512
+	struct page *pml_pg;
+
+	/* apic deadline value in host tsc */
+	u64 hv_deadline_tsc;
+
+	u64 current_tsc_ratio;
+
+	u32 host_pkru;
+
+	unsigned long host_debugctlmsr;
+
+	/*
+	 * Only bits masked by msr_ia32_feature_control_valid_bits can be set in
+	 * msr_ia32_feature_control. FEATURE_CONTROL_LOCKED is always included
+	 * in msr_ia32_feature_control_valid_bits.
+	 */
+	u64 msr_ia32_feature_control;
+	u64 msr_ia32_feature_control_valid_bits;
+	u64 ept_pointer;
+};
+
+enum ept_pointers_status {
+	EPT_POINTERS_CHECK = 0,
+	EPT_POINTERS_MATCH = 1,
+	EPT_POINTERS_MISMATCH = 2
+};
+
+struct kvm_vmx {
+	struct kvm kvm;
+
+	unsigned int tss_addr;
+	bool ept_identity_pagetable_done;
+	gpa_t ept_identity_map_addr;
+
+	enum ept_pointers_status ept_pointers_match;
+	spinlock_t ept_pointer_lock;
+};
+
+#define POSTED_INTR_ON  0
+#define POSTED_INTR_SN  1
+
+static inline bool pi_test_and_set_on(struct pi_desc *pi_desc)
+{
+	return test_and_set_bit(POSTED_INTR_ON,
+			(unsigned long *)&pi_desc->control);
+}
+
+static inline bool pi_test_and_clear_on(struct pi_desc *pi_desc)
+{
+	return test_and_clear_bit(POSTED_INTR_ON,
+			(unsigned long *)&pi_desc->control);
+}
+
+static inline int pi_test_and_set_pir(int vector, struct pi_desc *pi_desc)
+{
+	return test_and_set_bit(vector, (unsigned long *)pi_desc->pir);
+}
+
+static inline void pi_clear_sn(struct pi_desc *pi_desc)
+{
+	return clear_bit(POSTED_INTR_SN,
+			(unsigned long *)&pi_desc->control);
+}
+
+static inline void pi_set_sn(struct pi_desc *pi_desc)
+{
+	return set_bit(POSTED_INTR_SN,
+			(unsigned long *)&pi_desc->control);
+}
+
+static inline void pi_clear_on(struct pi_desc *pi_desc)
+{
+	clear_bit(POSTED_INTR_ON,
+		(unsigned long *)&pi_desc->control);
+}
+
+static inline int pi_test_on(struct pi_desc *pi_desc)
+{
+	return test_bit(POSTED_INTR_ON,
+			(unsigned long *)&pi_desc->control);
+}
+
+static inline int pi_test_sn(struct pi_desc *pi_desc)
+{
+	return test_bit(POSTED_INTR_SN,
+			(unsigned long *)&pi_desc->control);
+}
+
+static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
+{
+	vmx->segment_cache.bitmask = 0;
+}
+
+static inline u32 vmx_vmentry_ctrl(void)
+{
+	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
+	return vmcs_config.vmentry_ctrl &
+		~(VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL | VM_ENTRY_LOAD_IA32_EFER);
+}
+
+static inline u32 vmx_vmexit_ctrl(void)
+{
+	/* Loading of EFER and PERF_GLOBAL_CTRL are toggled dynamically */
+	return vmcs_config.vmexit_ctrl &
+		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);
+}
+
+u32 vmx_exec_control(struct vcpu_vmx *vmx);
+
+static inline struct kvm_vmx *to_kvm_vmx(struct kvm *kvm)
+{
+	return container_of(kvm, struct kvm_vmx, kvm);
+}
+
+static inline struct vcpu_vmx *to_vmx(struct kvm_vcpu *vcpu)
+{
+	return container_of(vcpu, struct vcpu_vmx, vcpu);
+}
+
+static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
+{
+	return &(to_vmx(vcpu)->pi_desc);
+}
+
+#endif /* __KVM_X86_VMX_H */
