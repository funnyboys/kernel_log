commit 404d5d7bff0d419fe11c7eaebca9ec8f25258f95
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue Apr 28 14:23:25 2020 +0800

    KVM: X86: Introduce more exit_fastpath_completion enum values
    
    Adds a fastpath_t typedef since enum lines are a bit long, and replace
    EXIT_FASTPATH_SKIP_EMUL_INS with two new exit_fastpath_completion enum values.
    
    - EXIT_FASTPATH_EXIT_HANDLED  kvm will still go through it's full run loop,
                                  but it would skip invoking the exit handler.
    
    - EXIT_FASTPATH_REENTER_GUEST complete fastpath, guest can be re-entered
                                  without invoking the exit handler or going
                                  back to vcpu_run
    
    Tested-by: Haiwei Li <lihaiwei@tencent.com>
    Cc: Haiwei Li <lihaiwei@tencent.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Message-Id: <1588055009-12677-4-git-send-email-wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index e02fe28254b6..6eb62e97e59f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -274,7 +274,7 @@ bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 bool kvm_vector_hashing_enabled(void);
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len);
-enum exit_fastpath_completion handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu);
+fastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu);
 
 extern u64 host_xcr0;
 extern u64 supported_xcr0;

commit 5a9f54435a488f8a1153efd36cccee3e7e0fc28b
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue Apr 28 14:23:26 2020 +0800

    KVM: X86: Introduce kvm_vcpu_exit_request() helper
    
    Introduce kvm_vcpu_exit_request() helper, we need to check some conditions
    before enter guest again immediately, we skip invoking the exit handler and
    go through full run loop if complete fastpath but there is stuff preventing
    we enter guest again immediately.
    
    Tested-by: Haiwei Li <lihaiwei@tencent.com>
    Cc: Haiwei Li <lihaiwei@tencent.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Message-Id: <1588055009-12677-5-git-send-email-wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 7b5ed8ed628e..e02fe28254b6 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -364,5 +364,6 @@ static inline bool kvm_dr7_valid(u64 data)
 void kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu);
 void kvm_load_host_xsave_state(struct kvm_vcpu *vcpu);
 u64 kvm_spec_ctrl_valid_bits(struct kvm_vcpu *vcpu);
+bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu);
 
 #endif

commit eeeb4f67a6cd437da1f5d1a20596cdc2d7b50551
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 14:28:20 2020 -0700

    KVM: x86: Introduce KVM_REQ_TLB_FLUSH_CURRENT to flush current ASID
    
    Add KVM_REQ_TLB_FLUSH_CURRENT to allow optimized TLB flushing of VMX's
    EPTP/VPID contexts[*] from the KVM MMU and/or in a deferred manner, e.g.
    to flush L2's context during nested VM-Enter.
    
    Convert KVM_REQ_TLB_FLUSH to KVM_REQ_TLB_FLUSH_CURRENT in flows where
    the flush is directly associated with vCPU-scoped instruction emulation,
    i.e. MOV CR3 and INVPCID.
    
    Add a comment in vmx_vcpu_load_vmcs() above its KVM_REQ_TLB_FLUSH to
    make it clear that it deliberately requests a flush of all contexts.
    
    Service any pending flush request on nested VM-Exit as it's possible a
    nested VM-Exit could occur after requesting a flush for L2.  Add the
    same logic for nested VM-Enter even though it's _extremely_ unlikely
    for flush to be pending on nested VM-Enter, but theoretically possible
    (in the future) due to RSM (SMM) emulation.
    
    [*] Intel also has an Address Space Identifier (ASID) concept, e.g.
        EPTP+VPID+PCID == ASID, it's just not documented in the SDM because
        the rules of invalidation are different based on which piece of the
        ASID is being changed, i.e. whether the EPTP, VPID, or PCID context
        must be invalidated.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320212833.3507-25-sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index b968acc0516f..7b5ed8ed628e 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -125,6 +125,12 @@ static inline bool mmu_is_nested(struct kvm_vcpu *vcpu)
 	return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
 }
 
+static inline void kvm_vcpu_flush_tlb_current(struct kvm_vcpu *vcpu)
+{
+	++vcpu->stat.tlb_flush;
+	kvm_x86_ops.tlb_flush_current(vcpu);
+}
+
 static inline int is_pae(struct kvm_vcpu *vcpu)
 {
 	return kvm_read_cr4_bits(vcpu, X86_CR4_PAE);

commit afaf0b2f9b801c6eb2278b52d49e6a7d7b659cf1
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 13:26:00 2020 -0700

    KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection
    
    Replace the kvm_x86_ops pointer in common x86 with an instance of the
    struct to save one pointer dereference when invoking functions.  Copy the
    struct by value to set the ops during kvm_init().
    
    Arbitrarily use kvm_x86_ops.hardware_enable to track whether or not the
    ops have been initialized, i.e. a vendor KVM module has been loaded.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321202603.19355-7-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index c1954e216b41..b968acc0516f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -97,7 +97,7 @@ static inline bool is_64_bit_mode(struct kvm_vcpu *vcpu)
 
 	if (!is_long_mode(vcpu))
 		return false;
-	kvm_x86_ops->get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
+	kvm_x86_ops.get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
 	return cs_l;
 }
 
@@ -237,7 +237,7 @@ static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
 
 static inline bool kvm_vcpu_latch_init(struct kvm_vcpu *vcpu)
 {
-	return is_smm(vcpu) || kvm_x86_ops->apic_init_signal_blocked(vcpu);
+	return is_smm(vcpu) || kvm_x86_ops.apic_init_signal_blocked(vcpu);
 }
 
 void kvm_set_pending_timer(struct kvm_vcpu *vcpu);

commit 408e9a318f57ba8be82ba01e98cc271b97392187
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 5 16:11:56 2020 +0100

    KVM: CPUID: add support for supervisor states
    
    Current CPUID 0xd enumeration code does not support supervisor
    states, because KVM only supports setting IA32_XSS to zero.
    Change it instead to use a new variable supported_xss, to be
    set from the hardware_setup callback which is in charge of CPU
    capabilities.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 4d890bf827e8..c1954e216b41 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -272,6 +272,7 @@ enum exit_fastpath_completion handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vc
 
 extern u64 host_xcr0;
 extern u64 supported_xcr0;
+extern u64 supported_xss;
 
 static inline bool kvm_mpx_supported(void)
 {

commit 615a4ae1c74c2997f19189412fae0326baaf1bff
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:25 2020 -0800

    KVM: x86: Make kvm_mpx_supported() an inline function
    
    Expose kvm_mpx_supported() as a static inline so that it can be inlined
    in kvm_intel.ko.
    
    No functional change intended.
    
    Reviewed-by: Xiaoyao Li <xiaoyao.li@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 7a7dd5aa8586..4d890bf827e8 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -273,6 +273,12 @@ enum exit_fastpath_completion handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vc
 extern u64 host_xcr0;
 extern u64 supported_xcr0;
 
+static inline bool kvm_mpx_supported(void)
+{
+	return (supported_xcr0 & (XFEATURE_MASK_BNDREGS | XFEATURE_MASK_BNDCSR))
+		== (XFEATURE_MASK_BNDREGS | XFEATURE_MASK_BNDCSR);
+}
+
 extern unsigned int min_timer_period_us;
 
 extern bool enable_vmware_backdoor;

commit cfc481810c903a5f74e5c7bf50ca8e28318dbc44
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:56:23 2020 -0800

    KVM: x86: Calculate the supported xcr0 mask at load time
    
    Add a new global variable, supported_xcr0, to track which xcr0 bits can
    be exposed to the guest instead of calculating the mask on every call.
    The supported bits are constant for a given instance of KVM.
    
    This paves the way toward eliminating the ->mpx_supported() call in
    kvm_mpx_supported(), e.g. eliminates multiple retpolines in VMX's nested
    VM-Enter path, and eventually toward eliminating ->mpx_supported()
    altogether.
    
    No functional change intended.
    
    Reviewed-by: Xiaoyao Li <xiaoyao.li@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index f3c6e55eb5d9..7a7dd5aa8586 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -270,13 +270,8 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len);
 enum exit_fastpath_completion handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu);
 
-#define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \
-				| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \
-				| XFEATURE_MASK_BNDCSR | XFEATURE_MASK_AVX512 \
-				| XFEATURE_MASK_PKRU)
 extern u64 host_xcr0;
-
-extern u64 kvm_supported_xcr0(void);
+extern u64 supported_xcr0;
 
 extern unsigned int min_timer_period_us;
 

commit 2f728d66e8a7d89d7cb141bf0acb30c61ae7ded5
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:29:49 2020 -0800

    KVM: x86: Move kvm_emulate.h into KVM's private directory
    
    Now that the emulation context is dynamically allocated and not embedded
    in struct kvm_vcpu, move its header, kvm_emulate.h, out of the public
    asm directory and into KVM's private x86 directory.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 8409842a25d9..f3c6e55eb5d9 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -5,6 +5,7 @@
 #include <linux/kvm_host.h>
 #include <asm/pvclock.h>
 #include "kvm_cache_regs.h"
+#include "kvm_emulate.h"
 
 #define KVM_DEFAULT_PLE_GAP		128
 #define KVM_VMX_DEFAULT_PLE_WINDOW	4096

commit f0ed4760ed216fa0de52347289ded52be9a2c725
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 15:29:43 2020 -0800

    KVM: x86: Move emulation-only helpers to emulate.c
    
    Move ctxt_virt_addr_bits() and emul_is_noncanonical_address() from x86.h
    to emulate.c.  This eliminates all references to struct x86_emulate_ctxt
    from x86.h, and sets the stage for a future patch to stop including
    kvm_emulate.h in asm/kvm_host.h.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 3624665acee4..8409842a25d9 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -149,11 +149,6 @@ static inline u8 vcpu_virt_addr_bits(struct kvm_vcpu *vcpu)
 	return kvm_read_cr4_bits(vcpu, X86_CR4_LA57) ? 57 : 48;
 }
 
-static inline u8 ctxt_virt_addr_bits(struct x86_emulate_ctxt *ctxt)
-{
-	return (ctxt->ops->get_cr(ctxt, 4) & X86_CR4_LA57) ? 57 : 48;
-}
-
 static inline u64 get_canonical(u64 la, u8 vaddr_bits)
 {
 	return ((int64_t)la << (64 - vaddr_bits)) >> (64 - vaddr_bits);
@@ -164,12 +159,6 @@ static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
 	return get_canonical(la, vcpu_virt_addr_bits(vcpu)) != la;
 }
 
-static inline bool emul_is_noncanonical_address(u64 la,
-						struct x86_emulate_ctxt *ctxt)
-{
-	return get_canonical(la, ctxt_virt_addr_bits(ctxt)) != la;
-}
-
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {

commit 9b5e85320fcc3af20ce0397b2c6363b6ee5815b6
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jan 24 15:07:22 2020 -0800

    KVM: x86: Take a u64 when checking for a valid dr7 value
    
    Take a u64 instead of an unsigned long in kvm_dr7_valid() to fix a build
    warning on i386 due to right-shifting a 32-bit value by 32 when checking
    for bits being set in dr7[63:32].
    
    Alternatively, the warning could be resolved by rewriting the check to
    use an i386-friendly method, but taking a u64 fixes another oddity on
    32-bit KVM.  Beause KVM implements natural width VMCS fields as u64s to
    avoid layout issues between 32-bit and 64-bit, a devious guest can stuff
    vmcs12->guest_dr7 with a 64-bit value even when both the guest and host
    are 32-bit kernels.  KVM eventually drops vmcs12->guest_dr7[63:32] when
    propagating vmcs12->guest_dr7 to vmcs02, but ideally KVM would not rely
    on that behavior for correctness.
    
    Cc: Jim Mattson <jmattson@google.com>
    Cc: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Fixes: ecb697d10f70 ("KVM: nVMX: Check GUEST_DR7 on vmentry of nested guests")
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2d2ff855773b..3624665acee4 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -357,7 +357,7 @@ static inline bool kvm_pat_valid(u64 data)
 	return (data | ((data & 0x0202020202020202ull) << 1)) == data;
 }
 
-static inline bool kvm_dr7_valid(unsigned long data)
+static inline bool kvm_dr7_valid(u64 data)
 {
 	/* Bits [63:32] are reserved */
 	return !(data >> 32);

commit b91991bf6b707482953c094dbd9615f6382ba2cb
Author: Krish Sadhukhan <krish.sadhukhan@oracle.com>
Date:   Wed Jan 15 19:54:32 2020 -0500

    KVM: nVMX: Check GUEST_DR7 on vmentry of nested guests
    
    According to section "Checks on Guest Control Registers, Debug Registers, and
    and MSRs" in Intel SDM vol 3C, the following checks are performed on vmentry
    of nested guests:
    
        If the "load debug controls" VM-entry control is 1, bits 63:32 in the DR7
        field must be 0.
    
    In KVM, GUEST_DR7 is set prior to the vmcs02 VM-entry by kvm_set_dr() and the
    latter synthesizes a #GP if any bit in the high dword in the former is set.
    Hence this field needs to be checked in software.
    
    Signed-off-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Reviewed-by: Karl Heubaum <karl.heubaum@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index e007b61b932a..2d2ff855773b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -357,6 +357,12 @@ static inline bool kvm_pat_valid(u64 data)
 	return (data | ((data & 0x0202020202020202ull) << 1)) == data;
 }
 
+static inline bool kvm_dr7_valid(unsigned long data)
+{
+	/* Bits [63:32] are reserved */
+	return !(data >> 32);
+}
+
 void kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu);
 void kvm_load_host_xsave_state(struct kvm_vcpu *vcpu);
 u64 kvm_spec_ctrl_valid_bits(struct kvm_vcpu *vcpu);

commit de761ea792c8344b7170624dbbcba4d730e23897
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Jan 15 10:36:05 2020 -0800

    KVM: x86: Perform non-canonical checks in 32-bit KVM
    
    Remove the CONFIG_X86_64 condition from the low level non-canonical
    helpers to effectively enable non-canonical checks on 32-bit KVM.
    Non-canonical checks are performed by hardware if the CPU *supports*
    64-bit mode, whether or not the CPU is actually in 64-bit mode is
    irrelevant.
    
    For the most part, skipping non-canonical checks on 32-bit KVM is ok-ish
    because 32-bit KVM always (hopefully) drops bits 63:32 of whatever value
    it's checking before propagating it to hardware, and architecturally,
    the expected behavior for the guest is a bit of a grey area since the
    vCPU itself doesn't support 64-bit mode.  I.e. a 32-bit KVM guest can
    observe the missed checks in several paths, e.g. INVVPID and VM-Enter,
    but it's debatable whether or not the missed checks constitute a bug
    because technically the vCPU doesn't support 64-bit mode.
    
    The primary motivation for enabling the non-canonical checks is defense
    in depth.  As mentioned above, a guest can trigger a missed check via
    INVVPID or VM-Enter.  INVVPID is straightforward as it takes a 64-bit
    virtual address as part of its 128-bit INVVPID descriptor and fails if
    the address is non-canonical, even if INVVPID is executed in 32-bit PM.
    Nested VM-Enter is a bit more convoluted as it requires the guest to
    write natural width VMCS fields via memory accesses and then VMPTRLD the
    VMCS, but it's still possible.  In both cases, KVM is saved from a true
    bug only because its flows that propagate values to hardware (correctly)
    take "unsigned long" parameters and so drop bits 63:32 of the bad value.
    
    Explicitly performing the non-canonical checks makes it less likely that
    a bad value will be propagated to hardware, e.g. in the INVVPID case,
    if __invvpid() didn't implicitly drop bits 63:32 then KVM would BUG() on
    the resulting unexpected INVVPID failure due to hardware rejecting the
    non-canonical address.
    
    The only downside to enabling the non-canonical checks is that it adds a
    relatively small amount of overhead, but the affected flows are not hot
    paths, i.e. the overhead is negligible.
    
    Note, KVM technically could gate the non-canonical checks on 32-bit KVM
    with static_cpu_has(X86_FEATURE_LM), but on bare metal that's an even
    bigger waste of code for everyone except the 0.00000000000001% of the
    population running on Yonah, and nested 32-bit on 64-bit already fudges
    things with respect to 64-bit CPU behavior.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    [Also do so in nested_vmx_check_host_state as reported by Krish. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index dd6e34d0a881..e007b61b932a 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -161,21 +161,13 @@ static inline u64 get_canonical(u64 la, u8 vaddr_bits)
 
 static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
 {
-#ifdef CONFIG_X86_64
 	return get_canonical(la, vcpu_virt_addr_bits(vcpu)) != la;
-#else
-	return false;
-#endif
 }
 
 static inline bool emul_is_noncanonical_address(u64 la,
 						struct x86_emulate_ctxt *ctxt)
 {
-#ifdef CONFIG_X86_64
 	return get_canonical(la, ctxt_virt_addr_bits(ctxt)) != la;
-#else
-	return false;
-#endif
 }
 
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,

commit 6441fa6178f5456d1d4b512c08798888f99db185
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jan 20 16:33:06 2020 +0100

    KVM: x86: avoid incorrect writes to host MSR_IA32_SPEC_CTRL
    
    If the guest is configured to have SPEC_CTRL but the host does not
    (which is a nonsensical configuration but these are not explicitly
    forbidden) then a host-initiated MSR write can write vmx->spec_ctrl
    (respectively svm->spec_ctrl) and trigger a #GP when KVM tries to
    restore the host value of the MSR.  Add a more comprehensive check
    for valid bits of SPEC_CTRL, covering host CPUID flags and,
    since we are at it and it is more correct that way, guest CPUID
    flags too.
    
    For AMD, remove the unnecessary is_guest_mode check around setting
    the MSR interception bitmap, so that the code looks the same as
    for Intel.
    
    Cc: Jim Mattson <jmattson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index ab715cee3653..dd6e34d0a881 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -367,5 +367,6 @@ static inline bool kvm_pat_valid(u64 data)
 
 void kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu);
 void kvm_load_host_xsave_state(struct kvm_vcpu *vcpu);
+u64 kvm_spec_ctrl_valid_bits(struct kvm_vcpu *vcpu);
 
 #endif

commit a0a2260c12d8658e522f21ed8ece72bbdede58fd
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Dec 17 13:32:39 2019 -0800

    KVM: x86: Move bit() helper to cpuid.h
    
    Move bit() to cpuid.h in preparation for incorporating the reverse_cpuid
    array in bit() build-time assertions.  Opportunistically use the BIT()
    macro instead of open-coding the shift.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 9805cf2c6b35..ab715cee3653 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -144,11 +144,6 @@ static inline bool is_pae_paging(struct kvm_vcpu *vcpu)
 	return !is_long_mode(vcpu) && is_pae(vcpu) && is_paging(vcpu);
 }
 
-static inline u32 bit(int bitno)
-{
-	return 1 << (bitno & 31);
-}
-
 static inline u8 vcpu_virt_addr_bits(struct kvm_vcpu *vcpu)
 {
 	return kvm_read_cr4_bits(vcpu, X86_CR4_LA57) ? 57 : 48;

commit 1e9e2622a149e88bd636c9f8fb346a6e6aefeae0
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Thu Nov 21 11:17:11 2019 +0800

    KVM: VMX: FIXED+PHYSICAL mode single target IPI fastpath
    
    ICR and TSCDEADLINE MSRs write cause the main MSRs write vmexits in our
    product observation, multicast IPIs are not as common as unicast IPI like
    RESCHEDULE_VECTOR and CALL_FUNCTION_SINGLE_VECTOR etc.
    
    This patch introduce a mechanism to handle certain performance-critical
    WRMSRs in a very early stage of KVM VMExit handler.
    
    This mechanism is specifically used for accelerating writes to x2APIC ICR
    that attempt to send a virtual IPI with physical destination-mode, fixed
    delivery-mode and single target. Which was found as one of the main causes
    of VMExits for Linux workloads.
    
    The reason this mechanism significantly reduce the latency of such virtual
    IPIs is by sending the physical IPI to the target vCPU in a very early stage
    of KVM VMExit handler, before host interrupts are enabled and before expensive
    operations such as reacquiring KVM’s SRCU lock.
    Latency is reduced even more when KVM is able to use APICv posted-interrupt
    mechanism (which allows to deliver the virtual IPI directly to target vCPU
    without the need to kick it to host).
    
    Testing on Xeon Skylake server:
    
    The virtual IPI latency from sender send to receiver receive reduces
    more than 200+ cpu cycles.
    
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index cab5e71f0f0f..9805cf2c6b35 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -291,6 +291,7 @@ bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 bool kvm_vector_hashing_enabled(void);
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len);
+enum exit_fastpath_completion handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu);
 
 #define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \
 				| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \

commit 736c291c9f36b07f8889c61764c28edce20e715d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 6 15:57:14 2019 -0800

    KVM: x86: Use gpa_t for cr2/gpa to fix TDP support on 32-bit KVM
    
    Convert a plethora of parameters and variables in the MMU and page fault
    flows from type gva_t to gpa_t to properly handle TDP on 32-bit KVM.
    
    Thanks to PSE and PAE paging, 32-bit kernels can access 64-bit physical
    addresses.  When TDP is enabled, the fault address is a guest physical
    address and thus can be a 64-bit value, even when both KVM and its guest
    are using 32-bit virtual addressing, e.g. VMX's VMCS.GUEST_PHYSICAL is a
    64-bit field, not a natural width field.
    
    Using a gva_t for the fault address means KVM will incorrectly drop the
    upper 32-bits of the GPA.  Ditto for gva_to_gpa() when it is used to
    translate L2 GPAs to L1 GPAs.
    
    Opportunistically rename variables and parameters to better reflect the
    dual address modes, e.g. use "cr2_or_gpa" for fault addresses and plain
    "addr" instead of "vaddr" when the address may be either a GVA or an L2
    GPA.  Similarly, use "gpa" in the nonpaging_page_fault() flows to avoid
    a confusing "gpa_t gva" declaration; this also sets the stage for a
    future patch to combing nonpaging_page_fault() and tdp_page_fault() with
    minimal churn.
    
    Sprinkle in a few comments to document flows where an address is known
    to be a GVA and thus can be safely truncated to a 32-bit value.  Add
    WARNs in kvm_handle_page_fault() and FNAME(gva_to_gpa_nested)() to help
    document such cases and detect bugs.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 29391af8871d..cab5e71f0f0f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -289,7 +289,7 @@ int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
 bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 					  int page_num);
 bool kvm_vector_hashing_enabled(void);
-int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
+int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len);
 
 #define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \

commit 27cbe7d61898a1d1d39be32e5acff7d4be6e9d87
Author: Liran Alon <liran.alon@oracle.com>
Date:   Mon Nov 11 11:16:40 2019 +0200

    KVM: x86: Prevent set vCPU into INIT/SIPI_RECEIVED state when INIT are latched
    
    Commit 4b9852f4f389 ("KVM: x86: Fix INIT signal handling in various CPU states")
    fixed KVM to also latch pending LAPIC INIT event when vCPU is in VMX
    operation.
    
    However, current API of KVM_SET_MP_STATE allows userspace to put vCPU
    into KVM_MP_STATE_SIPI_RECEIVED or KVM_MP_STATE_INIT_RECEIVED even when
    vCPU is in VMX operation.
    
    Fix this by introducing a util method to check if vCPU state latch INIT
    signals and use it in KVM_SET_MP_STATE handler.
    
    Fixes: 4b9852f4f389 ("KVM: x86: Fix INIT signal handling in various CPU states")
    Reported-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Mihai Carabas <mihai.carabas@oracle.com>
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2b0805012e3c..29391af8871d 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -258,6 +258,11 @@ static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
 	return !(kvm->arch.disabled_quirks & quirk);
 }
 
+static inline bool kvm_vcpu_latch_init(struct kvm_vcpu *vcpu)
+{
+	return is_smm(vcpu) || kvm_x86_ops->apic_init_signal_blocked(vcpu);
+}
+
 void kvm_set_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 

commit 139a12cfe1a040fd881338a7cc042bd37159ea9a
Author: Aaron Lewis <aaronlewis@google.com>
Date:   Mon Oct 21 16:30:25 2019 -0700

    KVM: x86: Move IA32_XSS-swapping on VM-entry/VM-exit to common x86 code
    
    Hoist the vendor-specific code related to loading the hardware IA32_XSS
    MSR with guest/host values on VM-entry/VM-exit to common x86 code.
    
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Aaron Lewis <aaronlewis@google.com>
    Change-Id: Ic6e3430833955b98eb9b79ae6715cf2a3fdd6d82
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 45d82b8277e5..2b0805012e3c 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -364,7 +364,7 @@ static inline bool kvm_pat_valid(u64 data)
 	return (data | ((data & 0x0202020202020202ull) << 1)) == data;
 }
 
-void kvm_load_guest_xcr0(struct kvm_vcpu *vcpu);
-void kvm_put_guest_xcr0(struct kvm_vcpu *vcpu);
+void kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu);
+void kvm_load_host_xsave_state(struct kvm_vcpu *vcpu);
 
 #endif

commit 489cbcf01d1c9e1bf09b7e371d0f312b3a1f3ef2
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Sep 27 14:45:20 2019 -0700

    KVM: x86: Add WARNs to detect out-of-bounds register indices
    
    Add WARN_ON_ONCE() checks in kvm_register_{read,write}() to detect reg
    values that would cause KVM to overflow vcpu->arch.regs.  Change the reg
    param to an 'int' to make it clear that the reg index is unverified.
    
    Regarding the overhead of WARN_ON_ONCE(), now that all fixed GPR reads
    and writes use dedicated accessors, e.g. kvm_rax_read(), the overhead
    is limited to flows where the reg index is generated at runtime.  And
    there is at least one historical bug where KVM has generated an out-of-
    bounds access to arch.regs (see commit b68f3cc7d9789, "KVM: x86: Always
    use 32-bit SMRAM save state for 32-bit kernels").
    
    Adding the WARN_ON_ONCE() protection paves the way for additional
    cleanup related to kvm_reg and kvm_reg_ex.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index dbf7442a822b..45d82b8277e5 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -238,8 +238,7 @@ static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 	return false;
 }
 
-static inline unsigned long kvm_register_readl(struct kvm_vcpu *vcpu,
-					       enum kvm_reg reg)
+static inline unsigned long kvm_register_readl(struct kvm_vcpu *vcpu, int reg)
 {
 	unsigned long val = kvm_register_read(vcpu, reg);
 
@@ -247,8 +246,7 @@ static inline unsigned long kvm_register_readl(struct kvm_vcpu *vcpu,
 }
 
 static inline void kvm_register_writel(struct kvm_vcpu *vcpu,
-				       enum kvm_reg reg,
-				       unsigned long val)
+				       int reg, unsigned long val)
 {
 	if (!is_64_bit_mode(vcpu))
 		val = (u32)val;

commit 9497e1f2ec93f0c8a7832c3c1eb3af8159800cd8
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Aug 27 14:40:36 2019 -0700

    KVM: x86: Move triple fault request into RM int injection
    
    Request triple fault in kvm_inject_realmode_interrupt() instead of
    returning EMULATE_FAIL and deferring to the caller.  All existing
    callers request triple fault and it's highly unlikely Real Mode is
    going to acquire new features.  While this consolidates a small amount
    of code, the real goal is to remove the last reference to EMULATE_FAIL.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index b5274e2a53cf..dbf7442a822b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -261,7 +261,7 @@ static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
 }
 
 void kvm_set_pending_timer(struct kvm_vcpu *vcpu);
-int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
+void kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 
 void kvm_write_tsc(struct kvm_vcpu *vcpu, struct msr_data *msr);
 u64 get_kvmclock_ns(struct kvm *kvm);

commit 871bd0346018df53055141f09754cb5ffb334c7b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 1 13:35:21 2019 -0700

    KVM: x86: Rename access permissions cache member in struct kvm_vcpu_arch
    
    Rename "access" to "mmio_access" to match the other MMIO cache members
    and to make it more obvious that it's tracking the access permissions
    for the MMIO cache.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 6594020c0691..b5274e2a53cf 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -196,7 +196,7 @@ static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 	 * actually a nGPA.
 	 */
 	vcpu->arch.mmio_gva = mmu_is_nested(vcpu) ? 0 : gva & PAGE_MASK;
-	vcpu->arch.access = access;
+	vcpu->arch.mmio_access = access;
 	vcpu->arch.mmio_gfn = gfn;
 	vcpu->arch.mmio_gen = gen;
 }

commit 0c5f81dad46c90792e6c3c4797131323c9e96dcd
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Sat Jul 6 09:26:51 2019 +0800

    KVM: LAPIC: Inject timer interrupt via posted interrupt
    
    Dedicated instances are currently disturbed by unnecessary jitter due
    to the emulated lapic timers firing on the same pCPUs where the
    vCPUs reside.  There is no hardware virtual timer on Intel for guest
    like ARM, so both programming timer in guest and the emulated timer fires
    incur vmexits.  This patch tries to avoid vmexit when the emulated timer
    fires, at least in dedicated instance scenario when nohz_full is enabled.
    
    In that case, the emulated timers can be offload to the nearest busy
    housekeeping cpus since APICv has been found for several years in server
    processors. The guest timer interrupt can then be injected via posted interrupts,
    which are delivered by the housekeeping cpu once the emulated timer fires.
    
    The host should tuned so that vCPUs are placed on isolated physical
    processors, and with several pCPUs surplus for busy housekeeping.
    If disabled mwait/hlt/pause vmexits keep the vCPUs in non-root mode,
    ~3% redis performance benefit can be observed on Skylake server, and the
    number of external interrupt vmexits drops substantially.  Without patch
    
                VM-EXIT  Samples  Samples%  Time%   Min Time  Max Time   Avg time
    EXTERNAL_INTERRUPT    42916    49.43%   39.30%   0.47us   106.09us   0.71us ( +-   1.09% )
    
    While with patch:
    
                VM-EXIT  Samples  Samples%  Time%   Min Time  Max Time         Avg time
    EXTERNAL_INTERRUPT    6871     9.29%     2.96%   0.44us    57.88us   0.72us ( +-   4.02% )
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index e08a12892e8b..6594020c0691 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -301,6 +301,8 @@ extern unsigned int min_timer_period_us;
 
 extern bool enable_vmware_backdoor;
 
+extern int pi_inject_timer;
+
 extern struct static_key kvm_no_apic_vcpu;
 
 static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)

commit bf03d4f9334728bf7c8ffc7de787df48abd6340e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 6 18:52:44 2019 +0200

    KVM: x86: introduce is_pae_paging
    
    Checking for 32-bit PAE is quite common around code that fiddles with
    the PDPTRs.  Add a function to compress all checks into a single
    invocation.
    
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 275b3b646023..e08a12892e8b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -139,6 +139,11 @@ static inline int is_paging(struct kvm_vcpu *vcpu)
 	return likely(kvm_read_cr0_bits(vcpu, X86_CR0_PG));
 }
 
+static inline bool is_pae_paging(struct kvm_vcpu *vcpu)
+{
+	return !is_long_mode(vcpu) && is_pae(vcpu) && is_paging(vcpu);
+}
+
 static inline u32 bit(int bitno)
 {
 	return 1 << (bitno & 31);

commit b51700632e0e53254733ff706e5bdca22d19dbe5
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue May 21 14:06:53 2019 +0800

    KVM: X86: Provide a capability to disable cstate msr read intercepts
    
    Allow guest reads CORE cstate when exposing host CPU power management capabilities
    to the guest. PKG cstate is restricted to avoid a guest to get the whole package
    information in multi-tenant scenario.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index a470ff0868c5..275b3b646023 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -333,6 +333,11 @@ static inline bool kvm_pause_in_guest(struct kvm *kvm)
 	return kvm->arch.pause_in_guest;
 }
 
+static inline bool kvm_cstate_in_guest(struct kvm *kvm)
+{
+	return kvm->arch.cstate_in_guest;
+}
+
 DECLARE_PER_CPU(struct kvm_vcpu *, current_vcpu);
 
 static inline void kvm_before_interrupt(struct kvm_vcpu *vcpu)

commit 0ef0fd351550130129bbdb77362488befd7b69d2
Merge: 4489da718309 c011d23ba046
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 17 10:33:30 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for SVE and Pointer Authentication in guests
       - PMU improvements
    
      POWER:
       - support for direct access to the POWER9 XIVE interrupt controller
       - memory and performance optimizations
    
      x86:
       - support for accessing memory not backed by struct page
       - fixes and refactoring
    
      Generic:
       - dirty page tracking improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (155 commits)
      kvm: fix compilation on aarch64
      Revert "KVM: nVMX: Expose RDPMC-exiting only when guest supports PMU"
      kvm: x86: Fix L1TF mitigation for shadow MMU
      KVM: nVMX: Disable intercept for FS/GS base MSRs in vmcs02 when possible
      KVM: PPC: Book3S: Remove useless checks in 'release' method of KVM device
      KVM: PPC: Book3S HV: XIVE: Fix spelling mistake "acessing" -> "accessing"
      KVM: PPC: Book3S HV: Make sure to load LPID for radix VCPUs
      kvm: nVMX: Set nested_run_pending in vmx_set_nested_state after checks complete
      tests: kvm: Add tests for KVM_SET_NESTED_STATE
      KVM: nVMX: KVM_SET_NESTED_STATE - Tear down old EVMCS state before setting new state
      tests: kvm: Add tests for KVM_CAP_MAX_VCPUS and KVM_CAP_MAX_CPU_ID
      tests: kvm: Add tests to .gitignore
      KVM: Introduce KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2
      KVM: Fix kvm_clear_dirty_log_protect off-by-(minus-)one
      KVM: Fix the bitmap range to copy during clear dirty
      KVM: arm64: Fix ptrauth ID register masking logic
      KVM: x86: use direct accessors for RIP and RSP
      KVM: VMX: Use accessors for GPRs outside of dedicated caching logic
      KVM: x86: Omit caching logic for always-available GPRs
      kvm, x86: Properly check whether a pfn is an MMIO or not
      ...

commit 39497d7660d9866a47a2dc9055672358da57ad3d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Apr 17 10:15:32 2019 -0700

    KVM: lapic: Track lapic timer advance per vCPU
    
    Automatically adjusting the globally-shared timer advancement could
    corrupt the timer, e.g. if multiple vCPUs are concurrently adjusting
    the advancement value.  That could be partially fixed by using a local
    variable for the arithmetic, but it would still be susceptible to a
    race when setting timer_advance_adjust_done.
    
    And because virtual_tsc_khz and tsc_scaling_ratio are per-vCPU, the
    correct calibration for a given vCPU may not apply to all vCPUs.
    
    Furthermore, lapic_timer_advance_ns is marked __read_mostly, which is
    effectively violated when finding a stable advancement takes an extended
    amount of timer.
    
    Opportunistically change the definition of lapic_timer_advance_ns to
    a u32 so that it matches the style of struct kvm_timer.  Explicitly
    pass the param to kvm_create_lapic() so that it doesn't have to be
    exposed to lapic.c, thus reducing the probability of unintentionally
    using the global value instead of the per-vCPU value.
    
    Cc: Liran Alon <liran.alon@oracle.com>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Cc: stable@vger.kernel.org
    Fixes: 3b8a5df6c4dc6 ("KVM: LAPIC: Tune lapic_timer_advance_ns automatically")
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index aedc5d0d4989..534d3f28bb01 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -294,8 +294,6 @@ extern u64 kvm_supported_xcr0(void);
 
 extern unsigned int min_timer_period_us;
 
-extern unsigned int lapic_timer_advance_ns;
-
 extern bool enable_vmware_backdoor;
 
 extern struct static_key kvm_no_apic_vcpu;

commit 674ea351cdeb01d2740edce31db7f2d79ce6095d
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Apr 10 11:41:40 2019 +0200

    KVM: x86: optimize check for valid PAT value
    
    This check will soon be done on every nested vmentry and vmexit,
    "parallelize" it using bitwise operations.
    
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index aedc5d0d4989..eda954441213 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -347,6 +347,16 @@ static inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 	__this_cpu_write(current_vcpu, NULL);
 }
 
+
+static inline bool kvm_pat_valid(u64 data)
+{
+	if (data & 0xF8F8F8F8F8F8F8F8ull)
+		return false;
+	/* 0, 1, 4, 5, 6, 7 are valid values.  */
+	return (data | ((data & 0x0202020202020202ull) << 1)) == data;
+}
+
 void kvm_load_guest_xcr0(struct kvm_vcpu *vcpu);
 void kvm_put_guest_xcr0(struct kvm_vcpu *vcpu);
+
 #endif

commit 1811d979c71621aafc7b879477202d286f7e863b
Author: WANG Chao <chao.wang@ucloud.cn>
Date:   Fri Apr 12 15:55:39 2019 +0800

    x86/kvm: move kvm_load/put_guest_xcr0 into atomic context
    
    guest xcr0 could leak into host when MCE happens in guest mode. Because
    do_machine_check() could schedule out at a few places.
    
    For example:
    
    kvm_load_guest_xcr0
    ...
    kvm_x86_ops->run(vcpu) {
      vmx_vcpu_run
        vmx_complete_atomic_exit
          kvm_machine_check
            do_machine_check
              do_memory_failure
                memory_failure
                  lock_page
    
    In this case, host_xcr0 is 0x2ff, guest vcpu xcr0 is 0xff. After schedule
    out, host cpu has guest xcr0 loaded (0xff).
    
    In __switch_to {
         switch_fpu_finish
           copy_kernel_to_fpregs
             XRSTORS
    
    If any bit i in XSTATE_BV[i] == 1 and xcr0[i] == 0, XRSTORS will
    generate #GP (In this case, bit 9). Then ex_handler_fprestore kicks in
    and tries to reinitialize fpu by restoring init fpu state. Same story as
    last #GP, except we get DOUBLE FAULT this time.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: WANG Chao <chao.wang@ucloud.cn>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 28406aa1136d..aedc5d0d4989 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -347,4 +347,6 @@ static inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 	__this_cpu_write(current_vcpu, NULL);
 }
 
+void kvm_load_guest_xcr0(struct kvm_vcpu *vcpu);
+void kvm_put_guest_xcr0(struct kvm_vcpu *vcpu);
 #endif

commit 361209e054a2c9f34da090ee1ee4c1e8bfe76a64
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:14 2019 -0800

    KVM: Explicitly define the "memslot update in-progress" bit
    
    KVM uses bit 0 of the memslots generation as an "update in-progress"
    flag, which is used by x86 to prevent caching MMIO access while the
    memslots are changing.  Although the intended behavior is flag-like,
    e.g. MMIO sptes intentionally drop the in-progress bit so as to avoid
    caching data from in-flux memslots, the implementation oftentimes treats
    the bit as part of the generation number itself, e.g. incrementing the
    generation increments twice, once to set the flag and once to clear it.
    
    Prior to commit 4bd518f1598d ("KVM: use separate generations for
    each address space"), incorporating the "update in-progress" bit into
    the generation number largely made sense, e.g. "real" generations are
    even, "bogus" generations are odd, most code doesn't need to be aware of
    the bit, etc...
    
    Now that unique memslots generation numbers are assigned to each address
    space, stealthing the in-progress status into the generation number
    results in a wide variety of subtle code, e.g. kvm_create_vm() jumps
    over bit 0 when initializing the memslots generation without any hint as
    to why.
    
    Explicitly define the flag and convert as much code as possible (which
    isn't much) to actually treat it like a flag.  This paves the way for
    eventually using a different bit for "update in-progress" so that it can
    be a flag in truth instead of a awkward extension to the generation
    number.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 20ede17202bf..28406aa1136d 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -183,7 +183,7 @@ static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 {
 	u64 gen = kvm_memslots(vcpu->kvm)->generation;
 
-	if (unlikely(gen & 1))
+	if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
 		return;
 
 	/*

commit ddfd1730fd829743e41213e32ccc8b4aa6dc8325
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:13 2019 -0800

    KVM: x86/mmu: Do not cache MMIO accesses while memslots are in flux
    
    When installing new memslots, KVM sets bit 0 of the generation number to
    indicate that an update is in-progress.  Until the update is complete,
    there are no guarantees as to whether a vCPU will see the old or the new
    memslots.  Explicity prevent caching MMIO accesses so as to avoid using
    an access cached from the old memslots after the new memslots have been
    installed.
    
    Note that it is unclear whether or not disabling caching during the
    update window is strictly necessary as there is no definitive
    documentation as to what ordering guarantees KVM provides with respect
    to updating memslots.  That being said, the MMIO spte code does not
    allow reusing sptes created while an update is in-progress, and the
    associated documentation explicitly states:
    
        We do not want to use an MMIO sptes created with an odd generation
        number, ...  If KVM is unlucky and creates an MMIO spte while the
        low bit is 1, the next access to the spte will always be a cache miss.
    
    At the very least, disabling the per-vCPU MMIO cache during updates will
    make its behavior consistent with the MMIO spte behavior and
    documentation.
    
    Fixes: 56f17dd3fbc4 ("kvm: x86: fix stale mmio cache bug")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 224cd0a47568..20ede17202bf 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -181,6 +181,11 @@ static inline bool emul_is_noncanonical_address(u64 la,
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {
+	u64 gen = kvm_memslots(vcpu->kvm)->generation;
+
+	if (unlikely(gen & 1))
+		return;
+
 	/*
 	 * If this is a shadow nested page table, the "GVA" is
 	 * actually a nGPA.
@@ -188,7 +193,7 @@ static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 	vcpu->arch.mmio_gva = mmu_is_nested(vcpu) ? 0 : gva & PAGE_MASK;
 	vcpu->arch.access = access;
 	vcpu->arch.mmio_gfn = gfn;
-	vcpu->arch.mmio_gen = kvm_memslots(vcpu->kvm)->generation;
+	vcpu->arch.mmio_gen = gen;
 }
 
 static inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)

commit da998b46d244767505e41d050dcac5e4d03ba96f
Author: Jim Mattson <jmattson@google.com>
Date:   Tue Oct 16 14:29:22 2018 -0700

    kvm: x86: Defer setting of CR2 until #PF delivery
    
    When exception payloads are enabled by userspace (which is not yet
    possible) and a #PF is raised in L2, defer the setting of CR2 until
    the #PF is delivered. This allows the L1 hypervisor to intercept the
    fault before CR2 is modified.
    
    For backwards compatibility, when exception payloads are not enabled
    by userspace, kvm_multiple_exception modifies CR2 when the #PF
    exception is raised.
    
    Reported-by: Jim Mattson <jmattson@google.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 67b9568613f3..224cd0a47568 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -266,6 +266,8 @@ int kvm_write_guest_virt_system(struct kvm_vcpu *vcpu,
 
 int handle_ud(struct kvm_vcpu *vcpu);
 
+void kvm_deliver_exception_payload(struct kvm_vcpu *vcpu);
+
 void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu);
 u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);

commit c60658d1d983641fcdbb16f86bc2f3806d88bab4
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Thu Aug 23 13:56:53 2018 -0700

    KVM: x86: Unexport x86_emulate_instruction()
    
    Allowing x86_emulate_instruction() to be called directly has led to
    subtle bugs being introduced, e.g. not setting EMULTYPE_NO_REEXECUTE
    in the emulation type.  While most of the blame lies on re-execute
    being opt-out, exporting x86_emulate_instruction() also exposes its
    cr2 parameter, which may have contributed to commit d391f1207067
    ("x86/kvm/vmx: do not use vm-exit instruction length for fast MMIO
    when running nested") using x86_emulate_instruction() instead of
    emulate_instruction() because "hey, I have a cr2!", which in turn
    introduced its EMULTYPE_NO_REEXECUTE bug.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 257f27620bc2..67b9568613f3 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -274,6 +274,8 @@ int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
 bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 					  int page_num);
 bool kvm_vector_hashing_enabled(void);
+int x86_emulate_instruction(struct kvm_vcpu *vcpu, unsigned long cr2,
+			    int emulation_type, void *insn, int insn_len);
 
 #define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \
 				| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \

commit 0447378a4a793da008451fad50bc0f93e9675ae6
Author: Marc Orr <marcorr@google.com>
Date:   Wed Jun 20 17:21:29 2018 -0700

    kvm: vmx: Nested VM-entry prereqs for event inj.
    
    This patch extends the checks done prior to a nested VM entry.
    Specifically, it extends the check_vmentry_prereqs function with checks
    for fields relevant to the VM-entry event injection information, as
    described in the Intel SDM, volume 3.
    
    This patch is motivated by a syzkaller bug, where a bad VM-entry
    interruption information field is generated in the VMCS02, which causes
    the nested VM launch to fail. Then, KVM fails to resume L1.
    
    While KVM should be improved to correctly resume L1 execution after a
    failed nested launch, this change is justified because the existing code
    to resume L1 is flaky/ad-hoc and the test coverage for resuming L1 is
    sparse.
    
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: Marc Orr <marcorr@google.com>
    [Removed comment whose parts were describing previous revisions and the
     rest was obvious from function/variable naming. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 331993c49dae..257f27620bc2 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -110,6 +110,15 @@ static inline bool is_la57_mode(struct kvm_vcpu *vcpu)
 #endif
 }
 
+static inline bool x86_exception_has_error_code(unsigned int vector)
+{
+	static u32 exception_has_error_code = BIT(DF_VECTOR) | BIT(TS_VECTOR) |
+			BIT(NP_VECTOR) | BIT(SS_VECTOR) | BIT(GP_VECTOR) |
+			BIT(PF_VECTOR) | BIT(AC_VECTOR);
+
+	return (1U << vector) & exception_has_error_code;
+}
+
 static inline bool mmu_is_nested(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;

commit ce14e868a54edeb2e30cb7a7b104a2fc4b9d76ca
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jun 6 17:37:49 2018 +0200

    KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
    
    Int the next patch the emulator's .read_std and .write_std callbacks will
    grow another argument, which is not needed in kvm_read_guest_virt and
    kvm_write_guest_virt_system's callers.  Since we have to make separate
    functions, let's give the currently existing names a nicer interface, too.
    
    Fixes: 129a72a0d3c8 ("KVM: x86: Introduce segmented_write_std", 2017-01-12)
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index c9492f764902..331993c49dae 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -247,11 +247,11 @@ int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 void kvm_write_tsc(struct kvm_vcpu *vcpu, struct msr_data *msr);
 u64 get_kvmclock_ns(struct kvm *kvm);
 
-int kvm_read_guest_virt(struct x86_emulate_ctxt *ctxt,
+int kvm_read_guest_virt(struct kvm_vcpu *vcpu,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
-int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
+int kvm_write_guest_virt_system(struct kvm_vcpu *vcpu,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 

commit 5e62493f1a70e7f13059544daaee05e40e8548e2
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Tue Apr 17 06:43:58 2018 +0200

    x86/headers/UAPI: Move DISABLE_EXITS KVM capability bits to the UAPI
    
    Move DISABLE_EXITS KVM capability bits to the UAPI just like the rest of
    capabilities.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 7d35ce672989..c9492f764902 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -302,13 +302,6 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 	    __rem;						\
 	 })
 
-#define KVM_X86_DISABLE_EXITS_MWAIT          (1 << 0)
-#define KVM_X86_DISABLE_EXITS_HTL            (1 << 1)
-#define KVM_X86_DISABLE_EXITS_PAUSE          (1 << 2)
-#define KVM_X86_DISABLE_VALID_EXITS          (KVM_X86_DISABLE_EXITS_MWAIT | \
-                                              KVM_X86_DISABLE_EXITS_HTL | \
-                                              KVM_X86_DISABLE_EXITS_PAUSE)
-
 static inline bool kvm_mwait_in_guest(struct kvm *kvm)
 {
 	return kvm->arch.mwait_in_guest;

commit 082d06edab49f302eb96b7a9d029f52713156354
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue Apr 3 16:28:48 2018 -0700

    KVM: X86: Introduce handle_ud()
    
    Introduce handle_ud() to handle invalid opcode, this function will be
    used by later patches.
    
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 1e8617414ee4..7d35ce672989 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -255,6 +255,8 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
+int handle_ud(struct kvm_vcpu *vcpu);
+
 void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu);
 u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);

commit 04140b4144cd888c080cddbb2be2ec603f00d081
Author: Liran Alon <liran.alon@oracle.com>
Date:   Fri Mar 23 03:01:31 2018 +0300

    KVM: x86: Rename interrupt.pending to interrupt.injected
    
    For exceptions & NMIs events, KVM code use the following
    coding convention:
    *) "pending" represents an event that should be injected to guest at
    some point but it's side-effects have not yet occurred.
    *) "injected" represents an event that it's side-effects have already
    occurred.
    
    However, interrupts don't conform to this coding convention.
    All current code flows mark interrupt.pending when it's side-effects
    have already taken place (For example, bit moved from LAPIC IRR to
    ISR). Therefore, it makes sense to just rename
    interrupt.pending to interrupt.injected.
    
    This change follows logic of previous commit 664f8e26b00c ("KVM: X86:
    Fix loss of exception which has not yet been injected") which changed
    exception to follow this coding convention as well.
    
    It is important to note that in case !lapic_in_kernel(vcpu),
    interrupt.pending usage was and still incorrect.
    In this case, interrrupt.pending can only be set using one of the
    following ioctls: KVM_INTERRUPT, KVM_SET_VCPU_EVENTS and
    KVM_SET_SREGS. Looking at how QEMU uses these ioctls, one can see that
    QEMU uses them either to re-set an "interrupt.pending" state it has
    received from KVM (via KVM_GET_VCPU_EVENTS interrupt.pending or
    via KVM_GET_SREGS interrupt_bitmap) or by dispatching a new interrupt
    from QEMU's emulated LAPIC which reset bit in IRR and set bit in ISR
    before sending ioctl to KVM. So it seems that indeed "interrupt.pending"
    in this case is also suppose to represent "interrupt.injected".
    However, kvm_cpu_has_interrupt() & kvm_cpu_has_injectable_intr()
    is misusing (now named) interrupt.injected in order to return if
    there is a pending interrupt.
    This leads to nVMX/nSVM not be able to distinguish if it should exit
    from L2 to L1 on EXTERNAL_INTERRUPT on pending interrupt or should
    re-inject an injected interrupt.
    Therefore, add a FIXME at these functions for handling this issue.
    
    This patch introduce no semantics change.
    
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 42350391b62f..1e8617414ee4 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -55,19 +55,19 @@ static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 	bool soft)
 {
-	vcpu->arch.interrupt.pending = true;
+	vcpu->arch.interrupt.injected = true;
 	vcpu->arch.interrupt.soft = soft;
 	vcpu->arch.interrupt.nr = vector;
 }
 
 static inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)
 {
-	vcpu->arch.interrupt.pending = false;
+	vcpu->arch.interrupt.injected = false;
 }
 
 static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)
 {
-	return vcpu->arch.exception.injected || vcpu->arch.interrupt.pending ||
+	return vcpu->arch.exception.injected || vcpu->arch.interrupt.injected ||
 		vcpu->arch.nmi_injected;
 }
 

commit 8566ac8b8e7cac5814fb744ff5159d1797a1a6bd
Author: Babu Moger <babu.moger@amd.com>
Date:   Fri Mar 16 16:37:26 2018 -0400

    KVM: SVM: Implement pause loop exit logic in SVM
    
    Bring the PLE(pause loop exit) logic to AMD svm driver.
    
    While testing, we found this helping in situations where numerous
    pauses are generated. Without these patches we could see continuos
    VMEXITS due to pause interceptions. Tested it on AMD EPYC server with
    boot parameter idle=poll on a VM with 32 vcpus to simulate extensive
    pause behaviour. Here are VMEXITS in 10 seconds interval.
    
    Pauses                  810199                  504
    Total                   882184                  325415
    
    Signed-off-by: Babu Moger <babu.moger@amd.com>
    [Prevented the window from dropping below the initial value. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 0804722b809e..42350391b62f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -11,6 +11,8 @@
 #define KVM_DEFAULT_PLE_WINDOW_GROW	2
 #define KVM_DEFAULT_PLE_WINDOW_SHRINK	0
 #define KVM_VMX_DEFAULT_PLE_WINDOW_MAX	UINT_MAX
+#define KVM_SVM_DEFAULT_PLE_WINDOW_MAX	USHRT_MAX
+#define KVM_SVM_DEFAULT_PLE_WINDOW	3000
 
 static inline unsigned int __grow_ple_window(unsigned int val,
 		unsigned int base, unsigned int modifier, unsigned int max)

commit c8e88717cfc6b36bedea22368d97667446318291
Author: Babu Moger <babu.moger@amd.com>
Date:   Fri Mar 16 16:37:24 2018 -0400

    KVM: VMX: Bring the common code to header file
    
    This patch brings some of the code from vmx to x86.h header file. Now, we
    can share this code between vmx and svm. Modified couple functions to make
    it common.
    
    Signed-off-by: Babu Moger <babu.moger@amd.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 35efd567a676..0804722b809e 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -6,6 +6,42 @@
 #include <asm/pvclock.h>
 #include "kvm_cache_regs.h"
 
+#define KVM_DEFAULT_PLE_GAP		128
+#define KVM_VMX_DEFAULT_PLE_WINDOW	4096
+#define KVM_DEFAULT_PLE_WINDOW_GROW	2
+#define KVM_DEFAULT_PLE_WINDOW_SHRINK	0
+#define KVM_VMX_DEFAULT_PLE_WINDOW_MAX	UINT_MAX
+
+static inline unsigned int __grow_ple_window(unsigned int val,
+		unsigned int base, unsigned int modifier, unsigned int max)
+{
+	u64 ret = val;
+
+	if (modifier < 1)
+		return base;
+
+	if (modifier < base)
+		ret *= modifier;
+	else
+		ret += modifier;
+
+	return min(ret, (u64)max);
+}
+
+static inline unsigned int __shrink_ple_window(unsigned int val,
+		unsigned int base, unsigned int modifier, unsigned int min)
+{
+	if (modifier < 1)
+		return base;
+
+	if (modifier < base)
+		val /= modifier;
+	else
+		val -= modifier;
+
+	return max(val, min);
+}
+
 #define MSR_IA32_CR_PAT_DEFAULT  0x0007040600070406ULL
 
 static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)

commit dd60d217062f4527f4a94af8b3a2e9666c26f903
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Jul 25 17:20:32 2017 -0700

    KVM: x86: Fix perf timer mode IP reporting
    
    KVM and perf have a special backdoor mechanism to report the IP for interrupts
    re-executed after vm exit. This works for the NMIs that perf normally uses.
    
    However when perf is in timer mode it doesn't work because the timer interrupt
    doesn't get this special treatment. This is common when KVM is running
    nested in another hypervisor which may not implement the PMU, so only
    timer mode is available.
    
    Call the functions to set up the backdoor IP also for non NMI interrupts.
    
    I renamed the functions to set up the backdoor IP reporting to be more
    appropiate for their new use.  The SVM change is only compile tested.
    
    v2: Moved the functions inline.
    For the normal interrupt case the before/after functions are now
    called from x86.c, not arch specific code.
    For the NMI case we still need to call it in the architecture
    specific code, because it's already needed in the low level *_run
    functions.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    [Removed unnecessary calls from arch handle_external_intr. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index b620cfa8e8d5..35efd567a676 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -203,8 +203,6 @@ static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
 	return !(kvm->arch.disabled_quirks & quirk);
 }
 
-void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
-void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_set_pending_timer(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 
@@ -286,4 +284,16 @@ static inline bool kvm_pause_in_guest(struct kvm *kvm)
 	return kvm->arch.pause_in_guest;
 }
 
+DECLARE_PER_CPU(struct kvm_vcpu *, current_vcpu);
+
+static inline void kvm_before_interrupt(struct kvm_vcpu *vcpu)
+{
+	__this_cpu_write(current_vcpu, vcpu);
+}
+
+static inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
+{
+	__this_cpu_write(current_vcpu, NULL);
+}
+
 #endif

commit b31c114b82b2b55913d2cf744e6a665c2ca090ac
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Mar 12 04:53:04 2018 -0700

    KVM: X86: Provide a capability to disable PAUSE intercepts
    
    Allow to disable pause loop exit/pause filtering on a per VM basis.
    
    If some VMs have dedicated host CPUs, they won't be negatively affected
    due to needlessly intercepted PAUSE instructions.
    
    Thanks to Jan H. Schönherr's initial patch.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 6d14589c1a91..b620cfa8e8d5 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -266,8 +266,10 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 
 #define KVM_X86_DISABLE_EXITS_MWAIT          (1 << 0)
 #define KVM_X86_DISABLE_EXITS_HTL            (1 << 1)
+#define KVM_X86_DISABLE_EXITS_PAUSE          (1 << 2)
 #define KVM_X86_DISABLE_VALID_EXITS          (KVM_X86_DISABLE_EXITS_MWAIT | \
-                                              KVM_X86_DISABLE_EXITS_HTL)
+                                              KVM_X86_DISABLE_EXITS_HTL | \
+                                              KVM_X86_DISABLE_EXITS_PAUSE)
 
 static inline bool kvm_mwait_in_guest(struct kvm *kvm)
 {
@@ -279,4 +281,9 @@ static inline bool kvm_hlt_in_guest(struct kvm *kvm)
 	return kvm->arch.hlt_in_guest;
 }
 
+static inline bool kvm_pause_in_guest(struct kvm *kvm)
+{
+	return kvm->arch.pause_in_guest;
+}
+
 #endif

commit caa057a2cad647fb368a12c8e6c410ac4c28e063
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Mar 12 04:53:03 2018 -0700

    KVM: X86: Provide a capability to disable HLT intercepts
    
    If host CPUs are dedicated to a VM, we can avoid VM exits on HLT.
    This patch adds the per-VM capability to disable them.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 026b239bf058..6d14589c1a91 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -265,11 +265,18 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 	 })
 
 #define KVM_X86_DISABLE_EXITS_MWAIT          (1 << 0)
-#define KVM_X86_DISABLE_VALID_EXITS          (KVM_X86_DISABLE_EXITS_MWAIT)
+#define KVM_X86_DISABLE_EXITS_HTL            (1 << 1)
+#define KVM_X86_DISABLE_VALID_EXITS          (KVM_X86_DISABLE_EXITS_MWAIT | \
+                                              KVM_X86_DISABLE_EXITS_HTL)
 
 static inline bool kvm_mwait_in_guest(struct kvm *kvm)
 {
 	return kvm->arch.mwait_in_guest;
 }
 
+static inline bool kvm_hlt_in_guest(struct kvm *kvm)
+{
+	return kvm->arch.hlt_in_guest;
+}
+
 #endif

commit 4d5422cea3b61f158d58924cbb43feada456ba5c
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Mar 12 04:53:02 2018 -0700

    KVM: X86: Provide a capability to disable MWAIT intercepts
    
    Allowing a guest to execute MWAIT without interception enables a guest
    to put a (physical) CPU into a power saving state, where it takes
    longer to return from than what may be desired by the host.
    
    Don't give a guest that power over a host by default. (Especially,
    since nothing prevents a guest from using MWAIT even when it is not
    advertised via CPUID.)
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Jan H. Schönherr <jschoenh@amazon.de>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 18e2e0a91edc..026b239bf058 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -2,8 +2,6 @@
 #ifndef ARCH_X86_KVM_X86_H
 #define ARCH_X86_KVM_X86_H
 
-#include <asm/processor.h>
-#include <asm/mwait.h>
 #include <linux/kvm_host.h>
 #include <asm/pvclock.h>
 #include "kvm_cache_regs.h"
@@ -266,10 +264,12 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 	    __rem;						\
 	 })
 
-static inline bool kvm_mwait_in_guest(void)
+#define KVM_X86_DISABLE_EXITS_MWAIT          (1 << 0)
+#define KVM_X86_DISABLE_VALID_EXITS          (KVM_X86_DISABLE_EXITS_MWAIT)
+
+static inline bool kvm_mwait_in_guest(struct kvm *kvm)
 {
-	return boot_cpu_has(X86_FEATURE_MWAIT) &&
-		!boot_cpu_has_bug(X86_BUG_MONITOR);
+	return kvm->arch.mwait_in_guest;
 }
 
 #endif

commit c4ae60e4bbf8f5fd4929d2c98543d9c163cb336b
Author: Liran Alon <liran.alon@oracle.com>
Date:   Mon Mar 12 13:12:47 2018 +0200

    KVM: x86: Add module parameter for supporting VMware backdoor
    
    Support access to VMware backdoor requires KVM to intercept #GP
    exceptions from guest which introduce slight performance hit.
    Therefore, control this support by module parameter.
    
    Note that module parameter is exported as it should be consumed by
    kvm_intel & kvm_amd to determine if they should intercept #GP or not.
    
    This commit doesn't change semantics.
    It is done as a preparation for future commits.
    
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index b91215d1fd80..18e2e0a91edc 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -242,6 +242,8 @@ extern unsigned int min_timer_period_us;
 
 extern unsigned int lapic_timer_advance_ns;
 
+extern bool enable_vmware_backdoor;
+
 extern struct static_key kvm_no_apic_vcpu;
 
 static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)

commit 5c7d4f9ad39d980728b39752304ce10bb2960cbf
Author: Liran Alon <liran.alon@oracle.com>
Date:   Sun Nov 19 18:25:43 2017 +0200

    KVM: nVMX: Fix bug of injecting L2 exception into L1
    
    kvm_clear_exception_queue() should clear pending exception.
    This also includes exceptions which were only marked pending but not
    yet injected. This is because exception.pending is used for both L1
    and L2 to determine if an exception should be raised to guest.
    Note that an exception which is pending but not yet injected will
    be raised again once the guest will be resumed.
    
    Consider the following scenario:
    1) L0 KVM with ignore_msrs=false.
    2) L1 prepare vmcs12 with the following:
        a) No intercepts on MSR (MSR_BITMAP exist and is filled with 0).
        b) No intercept for #GP.
        c) vmx-preemption-timer is configured.
    3) L1 enters into L2.
    4) L2 reads an unhandled MSR that exists in MSR_BITMAP
    (such as 0x1fff).
    
    L2 RDMSR could be handled as described below:
    1) L2 exits to L0 on RDMSR and calls handle_rdmsr().
    2) handle_rdmsr() calls kvm_inject_gp() which sets
    KVM_REQ_EVENT, exception.pending=true and exception.injected=false.
    3) vcpu_enter_guest() consumes KVM_REQ_EVENT and calls
    inject_pending_event() which calls vmx_check_nested_events()
    which sees that exception.pending=true but
    nested_vmx_check_exception() returns 0 and therefore does nothing at
    this point. However let's assume it later sees vmx-preemption-timer
    expired and therefore exits from L2 to L1 by calling
    nested_vmx_vmexit().
    4) nested_vmx_vmexit() calls prepare_vmcs12()
    which calls vmcs12_save_pending_event() but it does nothing as
    exception.injected is false. Also prepare_vmcs12() calls
    kvm_clear_exception_queue() which does nothing as
    exception.injected is already false.
    5) We now return from vmx_check_nested_events() with 0 while still
    having exception.pending=true!
    6) Therefore inject_pending_event() continues
    and we inject L2 exception to L1!...
    
    This commit will fix above issue by changing step (4) to
    clear exception.pending in kvm_clear_exception_queue().
    
    Fixes: 664f8e26b00c ("KVM: X86: Fix loss of exception which has not yet been injected")
    Signed-off-by: Liran Alon <liran.alon@oracle.com>
    Reviewed-by: Nikita Leshenko <nikita.leshchenko@oracle.com>
    Reviewed-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Signed-off-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index c69f973111cb..b91215d1fd80 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -12,6 +12,7 @@
 
 static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 {
+	vcpu->arch.exception.pending = false;
 	vcpu->arch.exception.injected = false;
 }
 

commit 431f5d4443964bcc58ac2e4c6fd8ace6dcd3a8d1
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Nov 29 22:23:43 2017 +0100

    KVM: x86: simplify kvm_mwait_in_guest()
    
    If Intel/AMD implements MWAIT, we expect that it works well and only
    reject known bugs;  no reason to do it the other way around for minor
    vendors.  (Not that they are relevant ATM.)
    
    This allows further simplification of kvm_mwait_in_guest().
    And use boot_cpu_has() instead of "cpu_has(&boot_cpu_data," while at it.
    
    Reviewed-by: Alexander Graf <agraf@suse.de>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index d15859ec5e92..c69f973111cb 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -265,18 +265,8 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 
 static inline bool kvm_mwait_in_guest(void)
 {
-	if (!cpu_has(&boot_cpu_data, X86_FEATURE_MWAIT))
-		return false;
-
-	switch (boot_cpu_data.x86_vendor) {
-	case X86_VENDOR_AMD:
-		/* All AMD CPUs have a working MWAIT implementation */
-		return true;
-	case X86_VENDOR_INTEL:
-		return !boot_cpu_has_bug(X86_BUG_MONITOR);
-	default:
-		return false;
-	}
+	return boot_cpu_has(X86_FEATURE_MWAIT) &&
+		!boot_cpu_has_bug(X86_BUG_MONITOR);
 }
 
 #endif

commit 346f48fa311ceee7478b5b810ba050a4a22e0b2e
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Nov 29 22:23:42 2017 +0100

    KVM: x86: drop bogus MWAIT check
    
    The check was added in some iteration while trying to fix a reported OS
    X on Core 2 bug, but that bug is elsewhere.
    
    The comment is misleading because the guest can call MWAIT with ECX = 0
    even if we enforce CPUID5_ECX_INTERRUPT_BREAK;  the call would have the
    exactly the same effect as if the host didn't have the feature.
    
    A problem is that a QEMU feature exposes CPUID5_ECX_INTERRUPT_BREAK on
    CPUs that do not support it.  Removing the check changes behavior on
    last Pentium 4 lines (Presler, Dempsey, and Tulsa, which had VMX and
    MONITOR while missing INTERRUPT_BREAK) when running a guest OS that uses
    MWAIT without checking for its presence (QEMU doesn't expose MONITOR).
    
    The only known OS that ignores the MONITOR flag is old Mac OS X and we
    allowed it to bug on Core 2 (MWAIT used to throw #UD and only that OS
    noticed), so we can save another 20 lines letting it bug on even older
    CPUs.  Alternatively, we can return MWAIT exiting by default and let
    userspace toggle it.
    
    Reviewed-by: Alexander Graf <agraf@suse.de>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 81f5f50794f6..d15859ec5e92 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -265,8 +265,6 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 
 static inline bool kvm_mwait_in_guest(void)
 {
-	unsigned int eax, ebx, ecx, edx;
-
 	if (!cpu_has(&boot_cpu_data, X86_FEATURE_MWAIT))
 		return false;
 
@@ -275,29 +273,10 @@ static inline bool kvm_mwait_in_guest(void)
 		/* All AMD CPUs have a working MWAIT implementation */
 		return true;
 	case X86_VENDOR_INTEL:
-		/* Handle Intel below */
-		break;
+		return !boot_cpu_has_bug(X86_BUG_MONITOR);
 	default:
 		return false;
 	}
-
-	if (boot_cpu_has_bug(X86_BUG_MONITOR))
-		return false;
-
-	/*
-	 * Intel CPUs without CPUID5_ECX_INTERRUPT_BREAK are problematic as
-	 * they would allow guest to stop the CPU completely by disabling
-	 * interrupts then invoking MWAIT.
-	 */
-	if (boot_cpu_data.cpuid_level < CPUID_MWAIT_LEAF)
-		return false;
-
-	cpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &edx);
-
-	if (!(ecx & CPUID5_ECX_INTERRUPT_BREAK))
-		return false;
-
-	return true;
 }
 
 #endif

commit 2a140f3b6e23a309453b6f68709a50ece543f0f4
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Nov 29 22:23:41 2017 +0100

    KVM: x86: prevent MWAIT in guest with buggy MONITOR
    
    The bug prevents MWAIT from waking up after a write to the monitored
    cache line.
    KVM might emulate a CPU model that shouldn't have the bug, so the guest
    would not employ a workaround and possibly miss wakeups.
    Better to avoid the situation.
    
    Reviewed-by: Alexander Graf <agraf@suse.de>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index d0b95b7a90b4..81f5f50794f6 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -281,6 +281,9 @@ static inline bool kvm_mwait_in_guest(void)
 		return false;
 	}
 
+	if (boot_cpu_has_bug(X86_BUG_MONITOR))
+		return false;
+
 	/*
 	 * Intel CPUs without CPUID5_ECX_INTERRUPT_BREAK are problematic as
 	 * they would allow guest to stop the CPU completely by disabling

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 51e349cf5f45..d0b95b7a90b4 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef ARCH_X86_KVM_X86_H
 #define ARCH_X86_KVM_X86_H
 

commit 664f8e26b00c7673a8303b0d40853a0c24ca93e1
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Aug 24 03:35:09 2017 -0700

    KVM: X86: Fix loss of exception which has not yet been injected
    
    vmx_complete_interrupts() assumes that the exception is always injected,
    so it can be dropped by kvm_clear_exception_queue().  However,
    an exception cannot be injected immediately if it is: 1) originally
    destined to a nested guest; 2) trapped to cause a vmexit; 3) happening
    right after VMLAUNCH/VMRESUME, i.e. when nested_run_pending is true.
    
    This patch applies to exceptions the same algorithm that is used for
    NMIs, replacing exception.reinject with "exception.injected" (equivalent
    to nmi_injected).
    
    exception.pending now represents an exception that is queued and whose
    side effects (e.g., update RFLAGS.RF or DR7) have not been applied yet.
    If exception.pending is true, the exception might result in a nested
    vmexit instead, too (in which case the side effects must not be applied).
    
    exception.injected instead represents an exception that is going to be
    injected into the guest at the next vmentry.
    
    Reported-by: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index de1abf4d7925..51e349cf5f45 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -11,7 +11,7 @@
 
 static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 {
-	vcpu->arch.exception.pending = false;
+	vcpu->arch.exception.injected = false;
 }
 
 static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
@@ -29,7 +29,7 @@ static inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)
 {
-	return vcpu->arch.exception.pending || vcpu->arch.interrupt.pending ||
+	return vcpu->arch.exception.injected || vcpu->arch.interrupt.pending ||
 		vcpu->arch.nmi_injected;
 }
 

commit fd8cb433734eeb870156a67f5d56b6564cd2ea94
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Thu Aug 24 20:27:56 2017 +0800

    KVM: MMU: Expose the LA57 feature to VM.
    
    This patch exposes 5 level page table feature to the VM.
    At the same time, the canonical virtual address checking is
    extended to support both 48-bits and 57-bits address width.
    
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 69de8bf48178..de1abf4d7925 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -97,6 +97,40 @@ static inline u32 bit(int bitno)
 	return 1 << (bitno & 31);
 }
 
+static inline u8 vcpu_virt_addr_bits(struct kvm_vcpu *vcpu)
+{
+	return kvm_read_cr4_bits(vcpu, X86_CR4_LA57) ? 57 : 48;
+}
+
+static inline u8 ctxt_virt_addr_bits(struct x86_emulate_ctxt *ctxt)
+{
+	return (ctxt->ops->get_cr(ctxt, 4) & X86_CR4_LA57) ? 57 : 48;
+}
+
+static inline u64 get_canonical(u64 la, u8 vaddr_bits)
+{
+	return ((int64_t)la << (64 - vaddr_bits)) >> (64 - vaddr_bits);
+}
+
+static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
+{
+#ifdef CONFIG_X86_64
+	return get_canonical(la, vcpu_virt_addr_bits(vcpu)) != la;
+#else
+	return false;
+#endif
+}
+
+static inline bool emul_is_noncanonical_address(u64 la,
+						struct x86_emulate_ctxt *ctxt)
+{
+#ifdef CONFIG_X86_64
+	return get_canonical(la, ctxt_virt_addr_bits(ctxt)) != la;
+#else
+	return false;
+#endif
+}
+
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {

commit 855feb6736403f398dd43764254c5f0522bfc130
Author: Yu Zhang <yu.c.zhang@linux.intel.com>
Date:   Thu Aug 24 20:27:55 2017 +0800

    KVM: MMU: Add 5 level EPT & Shadow page table support.
    
    Extends the shadow paging code, so that 5 level shadow page
    table can be constructed if VM is running in 5 level paging
    mode.
    
    Also extends the ept code, so that 5 level ept table can be
    constructed if maxphysaddr of VM exceeds 48 bits. Unlike the
    shadow logic, KVM should still use 4 level ept table for a VM
    whose physical address width is less than 48 bits, even when
    the VM is running in 5 level paging mode.
    
    Signed-off-by: Yu Zhang <yu.c.zhang@linux.intel.com>
    [Unconditionally reset the MMU context in kvm_cpuid_update.
     Changing MAXPHYADDR invalidates the reserved bit bitmasks.
     - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 113460370a7f..69de8bf48178 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -62,6 +62,16 @@ static inline bool is_64_bit_mode(struct kvm_vcpu *vcpu)
 	return cs_l;
 }
 
+static inline bool is_la57_mode(struct kvm_vcpu *vcpu)
+{
+#ifdef CONFIG_X86_64
+	return (vcpu->arch.efer & EFER_LMA) &&
+		 kvm_read_cr4_bits(vcpu, X86_CR4_LA57);
+#else
+	return 0;
+#endif
+}
+
 static inline bool mmu_is_nested(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;

commit 9034e6e89526627094ff80df05db54f76a6f8fa8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Aug 17 18:36:58 2017 +0200

    KVM: x86: fix use of L1 MMIO areas in nested guests
    
    There is currently some confusion between nested and L1 GPAs.  The
    assignment to "direct" in kvm_mmu_page_fault tries to fix that, but
    it is not enough.  What this patch does is fence off the MMIO cache
    completely when using shadow nested page tables, since we have neither
    a GVA nor an L1 GPA to put in the cache.  This also allows some
    simplifications in kvm_mmu_page_fault and FNAME(page_fault).
    
    The EPT misconfig likewise does not have an L1 GPA to pass to
    kvm_io_bus_write, so that must be skipped for guest mode.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [Changed comment to say "GPAs" instead of "L1's physical addresses", as
     per David's review. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 612067074905..113460370a7f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -90,7 +90,11 @@ static inline u32 bit(int bitno)
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {
-	vcpu->arch.mmio_gva = gva & PAGE_MASK;
+	/*
+	 * If this is a shadow nested page table, the "GVA" is
+	 * actually a nGPA.
+	 */
+	vcpu->arch.mmio_gva = mmu_is_nested(vcpu) ? 0 : gva & PAGE_MASK;
 	vcpu->arch.access = access;
 	vcpu->arch.mmio_gfn = gfn;
 	vcpu->arch.mmio_gen = kvm_memslots(vcpu->kvm)->generation;

commit 668fffa3f838edfcb1679f842f7ef1afa61c3e9a
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Fri Apr 21 12:27:17 2017 +0200

    kvm: better MWAIT emulation for guests
    
    Guests that are heavy on futexes end up IPI'ing each other a lot. That
    can lead to significant slowdowns and latency increase for those guests
    when running within KVM.
    
    If only a single guest is needed on a host, we have a lot of spare host
    CPU time we can throw at the problem. Modern CPUs implement a feature
    called "MWAIT" which allows guests to wake up sleeping remote CPUs without
    an IPI - thus without an exit - at the expense of never going out of guest
    context.
    
    The decision whether this is something sensible to use should be up to the
    VM admin, so to user space. We can however allow MWAIT execution on systems
    that support it properly hardware wise.
    
    This patch adds a CAP to user space and a KVM cpuid leaf to indicate
    availability of native MWAIT execution. With that enabled, the worst a
    guest can do is waste as many cycles as a "jmp ." would do, so it's not
    a privilege problem.
    
    We consciously do *not* expose the feature in our CPUID bitmap, as most
    people will want to benefit from sleeping vCPUs to allow for over commit.
    
    Reported-by: "Gabriel L. Somlo" <gsomlo@gmail.com>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    [agraf: fix amd, change commit message]
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index e8ff3e4ce38a..612067074905 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -1,6 +1,8 @@
 #ifndef ARCH_X86_KVM_X86_H
 #define ARCH_X86_KVM_X86_H
 
+#include <asm/processor.h>
+#include <asm/mwait.h>
 #include <linux/kvm_host.h>
 #include <asm/pvclock.h>
 #include "kvm_cache_regs.h"
@@ -212,4 +214,38 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 	    __rem;						\
 	 })
 
+static inline bool kvm_mwait_in_guest(void)
+{
+	unsigned int eax, ebx, ecx, edx;
+
+	if (!cpu_has(&boot_cpu_data, X86_FEATURE_MWAIT))
+		return false;
+
+	switch (boot_cpu_data.x86_vendor) {
+	case X86_VENDOR_AMD:
+		/* All AMD CPUs have a working MWAIT implementation */
+		return true;
+	case X86_VENDOR_INTEL:
+		/* Handle Intel below */
+		break;
+	default:
+		return false;
+	}
+
+	/*
+	 * Intel CPUs without CPUID5_ECX_INTERRUPT_BREAK are problematic as
+	 * they would allow guest to stop the CPU completely by disabling
+	 * interrupts then invoking MWAIT.
+	 */
+	if (boot_cpu_data.cpuid_level < CPUID_MWAIT_LEAF)
+		return false;
+
+	cpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &edx);
+
+	if (!(ecx & CPUID5_ECX_INTERRUPT_BREAK))
+		return false;
+
+	return true;
+}
+
 #endif

commit 108b249c453dd7132599ab6dc7e435a7036c193f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Sep 1 14:21:03 2016 +0200

    KVM: x86: introduce get_kvmclock_ns
    
    Introduce a function that reads the exact nanoseconds value that is
    provided to the guest in kvmclock.  This crystallizes the notion of
    kvmclock as a thin veneer over a stable TSC, that the guest will
    (hopefully) convert with NTP.  In other words, kvmclock is *not* a
    paravirtualized host-to-guest NTP.
    
    Drop the get_kernel_ns() function, that was used both to get the base
    value of the master clock and to get the current value of kvmclock.
    The former use is replaced by ktime_get_boot_ns(), the latter is
    the purpose of get_kernel_ns().
    
    This also allows KVM to provide a Hyper-V time reference counter that
    is synchronized with the time that is computed from the TSC page.
    
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index a82ca466b62e..e8ff3e4ce38a 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -148,11 +148,6 @@ static inline void kvm_register_writel(struct kvm_vcpu *vcpu,
 	return kvm_register_write(vcpu, reg, val);
 }
 
-static inline u64 get_kernel_ns(void)
-{
-	return ktime_get_boot_ns();
-}
-
 static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
 {
 	return !(kvm->arch.disabled_quirks & quirk);
@@ -164,6 +159,7 @@ void kvm_set_pending_timer(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 
 void kvm_write_tsc(struct kvm_vcpu *vcpu, struct msr_data *msr);
+u64 get_kvmclock_ns(struct kvm *kvm);
 
 int kvm_read_guest_virt(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,

commit 8d93c874ac899bfdf0ad3787baef684a0c878c2c
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Jun 20 22:28:02 2016 -0300

    KVM: x86: move nsec_to_cycles from x86.c to x86.h
    
    Move the inline function nsec_to_cycles from x86.c to x86.h, as
    the next patch uses it from lapic.c.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 7ce3634ab5fe..a82ca466b62e 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -2,6 +2,7 @@
 #define ARCH_X86_KVM_X86_H
 
 #include <linux/kvm_host.h>
+#include <asm/pvclock.h>
 #include "kvm_cache_regs.h"
 
 #define MSR_IA32_CR_PAT_DEFAULT  0x0007040600070406ULL
@@ -195,6 +196,12 @@ extern unsigned int lapic_timer_advance_ns;
 
 extern struct static_key kvm_no_apic_vcpu;
 
+static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
+{
+	return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+				   vcpu->arch.virtual_tsc_shift);
+}
+
 /* Same "calling convention" as do_div:
  * - divide (n << 32) by base
  * - put result in n

commit 17a511f878505d31f298de443269b3070c134163
Author: Huaitong Han <huaitong.han@intel.com>
Date:   Tue Mar 22 16:51:16 2016 +0800

    KVM, pkeys: add pkeys support for xsave state
    
    This patch adds pkeys support for xsave state.
    
    Signed-off-by: Huaitong Han <huaitong.han@intel.com>
    Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 007940faa5c6..7ce3634ab5fe 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -183,7 +183,8 @@ bool kvm_vector_hashing_enabled(void);
 
 #define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \
 				| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \
-				| XFEATURE_MASK_BNDCSR | XFEATURE_MASK_AVX512)
+				| XFEATURE_MASK_BNDCSR | XFEATURE_MASK_AVX512 \
+				| XFEATURE_MASK_PKRU)
 extern u64 host_xcr0;
 
 extern u64 kvm_supported_xcr0(void);

commit 520040146a0af36f7875ec06b58f44b19a0edf53
Author: Feng Wu <feng.wu@intel.com>
Date:   Mon Jan 25 16:53:33 2016 +0800

    KVM: x86: Use vector-hashing to deliver lowest-priority interrupts
    
    Use vector-hashing to deliver lowest-priority interrupts, As an
    example, modern Intel CPUs in server platform use this method to
    handle lowest-priority interrupts.
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 34f416427143..007940faa5c6 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -179,6 +179,7 @@ int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);
 int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
 bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 					  int page_num);
+bool kvm_vector_hashing_enabled(void);
 
 #define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \
 				| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \

commit b51012deb390528d89d426f328d84618683f5d73
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jan 22 11:39:22 2016 +0100

    KVM: x86: introduce do_shl32_div32
    
    This is similar to the existing div_frac function, but it returns the
    remainder too.  Unlike div_frac, it can be used to implement long
    division, e.g. (a << 64) / b for 32-bit a and b.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index f2afa5fe48a6..34f416427143 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -192,4 +192,19 @@ extern unsigned int min_timer_period_us;
 extern unsigned int lapic_timer_advance_ns;
 
 extern struct static_key kvm_no_apic_vcpu;
+
+/* Same "calling convention" as do_div:
+ * - divide (n << 32) by base
+ * - put result in n
+ * - return remainder
+ */
+#define do_shl32_div32(n, base)					\
+	({							\
+	    u32 __quot, __rem;					\
+	    asm("divl %2" : "=a" (__quot), "=d" (__rem)		\
+			: "rm" (base), "0" (0), "1" ((u32) n));	\
+	    n = __quot;						\
+	    __rem;						\
+	 })
+
 #endif

commit d91cab78133d33b1dfd3d3fa7167fcbf74fb5f99
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Wed Sep 2 16:31:26 2015 -0700

    x86/fpu: Rename XSAVE macros
    
    There are two concepts that have some confusing naming:
     1. Extended State Component numbers (currently called
        XFEATURE_BIT_*)
     2. Extended State Component masks (currently called XSTATE_*)
    
    The numbers are (currently) from 0-9.  State component 3 is the
    bounds registers for MPX, for instance.
    
    But when we want to enable "state component 3", we go set a bit
    in XCR0.  The bit we set is 1<<3.  We can check to see if a
    state component feature is enabled by looking at its bit.
    
    The current 'xfeature_bit's are at best xfeature bit _numbers_.
    Calling them bits is at best inconsistent with ending the enum
    list with 'XFEATURES_NR_MAX'.
    
    This patch renames the enum to be 'xfeature'.  These also
    happen to be what the Intel documentation calls a "state
    component".
    
    We also want to differentiate these from the "XSTATE_*" macros.
    The "XSTATE_*" macros are a mask, and we rename them to match.
    
    These macros are reasonably widely used so this patch is a
    wee bit big, but this really is just a rename.
    
    The only non-mechanical part of this is the
    
            s/XSTATE_EXTEND_MASK/XFEATURE_MASK_EXTEND/
    
    We need a better name for it, but that's another patch.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: dave@sr71.net
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20150902233126.38653250@viggo.jf.intel.com
    [ Ported to v4.3-rc1. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2f822cd886c2..f2afa5fe48a6 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -180,9 +180,9 @@ int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
 bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 					  int page_num);
 
-#define KVM_SUPPORTED_XCR0     (XSTATE_FP | XSTATE_SSE | XSTATE_YMM \
-				| XSTATE_BNDREGS | XSTATE_BNDCSR \
-				| XSTATE_AVX512)
+#define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \
+				| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \
+				| XFEATURE_MASK_BNDCSR | XFEATURE_MASK_AVX512)
 extern u64 host_xcr0;
 
 extern u64 kvm_supported_xcr0(void);

commit e83d58874ba1de74c13d3c6b05f95a023c860d25
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Fri Jul 3 15:01:34 2015 +0300

    kvm/x86: move Hyper-V MSR's/hypercall code into hyperv.c file
    
    This patch introduce Hyper-V related source code file - hyperv.c and
    per vm and per vcpu hyperv context structures.
    All Hyper-V MSR's and hypercall code moved into hyperv.c.
    All Hyper-V kvm/vcpu fields moved into appropriate hyperv context
    structures. Copyrights and authors information copied from x86.c
    to hyperv.c.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Reviewed-by: Peter Hornyack <peterhornyack@google.com>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 0ca2f3e4803c..2f822cd886c2 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -147,6 +147,11 @@ static inline void kvm_register_writel(struct kvm_vcpu *vcpu,
 	return kvm_register_write(vcpu, reg, val);
 }
 
+static inline u64 get_kernel_ns(void)
+{
+	return ktime_get_boot_ns();
+}
+
 static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
 {
 	return !(kvm->arch.disabled_quirks & quirk);

commit 41dbc6bcd9722bba8f09256655d060af30c63d34
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jul 23 08:22:45 2015 +0200

    KVM: x86: introduce kvm_check_has_quirk
    
    The logic of the disabled_quirks field usually results in a double
    negation.  Wrap it in a simple function that checks the bit and
    negates it.
    
    Based on a patch from Xiao Guangrong.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index edc8cdcd786b..0ca2f3e4803c 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -147,6 +147,11 @@ static inline void kvm_register_writel(struct kvm_vcpu *vcpu,
 	return kvm_register_write(vcpu, reg, val);
 }
 
+static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
+{
+	return !(kvm->arch.disabled_quirks & quirk);
+}
+
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_set_pending_timer(struct kvm_vcpu *vcpu);

commit 6a39bbc5da27c3b2520876b71e4f7b50f5313503
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:35 2015 +0800

    KVM: MTRR: do not map huge page for non-consistent range
    
    Based on Intel's SDM, mapping huge page which do not have consistent
    memory cache for each 4k page will cause undefined behavior
    
    In order to avoiding this kind of undefined behavior, we force to use
    4k pages under this case
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 0e4727c49279..edc8cdcd786b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -167,6 +167,8 @@ u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);
 int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);
 int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
+bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
+					  int page_num);
 
 #define KVM_SUPPORTED_XCR0     (XSTATE_FP | XSTATE_SSE | XSTATE_YMM \
 				| XSTATE_BNDREGS | XSTATE_BNDCSR \

commit 19efffa244071ccd0385b240d03adb38feaab04e
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:31 2015 +0800

    KVM: MTRR: sort variable MTRRs
    
    Sort all valid variable MTRRs based on its base address, it will help us to
    check a range to see if it's fully contained in variable MTRRs
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    [Fix list insertion sort, simplify var_mtrr_range_is_valid to just
     test the V bit. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index aeb0bb2f1df4..0e4727c49279 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -162,6 +162,7 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
+void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu);
 u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);
 int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);

commit ff53604b40b439cbb235f89bda99839ca81d3b9d
Author: Xiao Guangrong <guangrong.xiao@linux.intel.com>
Date:   Mon Jun 15 16:55:22 2015 +0800

    KVM: x86: move MTRR related code to a separate file
    
    MTRR code locates in x86.c and mmu.c so that move them to a separate file to
    make the organization more clearer and it will be the place where we fully
    implement vMTRR
    
    Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 01a1d011e073..aeb0bb2f1df4 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -162,7 +162,10 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
+u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
 bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
 
 #define KVM_SUPPORTED_XCR0     (XSTATE_FP | XSTATE_SSE | XSTATE_YMM \
 				| XSTATE_BNDREGS | XSTATE_BNDCSR \

commit 74545705cb2e398efc3cd751628c58f021323434
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Mon Apr 27 15:11:25 2015 +0200

    KVM: x86: fix initial PAT value
    
    PAT should be 0007_0406_0007_0406h on RESET and not modified on INIT.
    VMX used a wrong value (host's PAT) and while SVM used the right one,
    it never got to arch.pat.
    
    This is not an issue with QEMU as it will force the correct value.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index f5fef1868096..01a1d011e073 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -4,6 +4,8 @@
 #include <linux/kvm_host.h>
 #include "kvm_cache_regs.h"
 
+#define MSR_IA32_CR_PAT_DEFAULT  0x0007040600070406ULL
+
 static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.exception.pending = false;

commit bab5bb398273bb37547a185f7b344b37c700d0b9
Author: Nicholas Krause <xerofoify@gmail.com>
Date:   Thu Jan 1 22:05:18 2015 -0500

    kvm: x86: Remove kvm_make_request from lapic.c
    
    Adds a function kvm_vcpu_set_pending_timer instead of calling
    kvm_make_request in lapic.c.
    
    Signed-off-by: Nicholas Krause <xerofoify@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 07994f38dacf..f5fef1868096 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -147,6 +147,7 @@ static inline void kvm_register_writel(struct kvm_vcpu *vcpu,
 
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
+void kvm_set_pending_timer(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 
 void kvm_write_tsc(struct kvm_vcpu *vcpu, struct msr_data *msr);

commit d0659d946be05e098883b6955d2764595997f6a4
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Dec 16 09:08:15 2014 -0500

    KVM: x86: add option to advance tscdeadline hrtimer expiration
    
    For the hrtimer which emulates the tscdeadline timer in the guest,
    add an option to advance expiration, and busy spin on VM-entry waiting
    for the actual expiration time to elapse.
    
    This allows achieving low latencies in cyclictest (or any scenario
    which requires strict timing regarding timer expiration).
    
    Reduces average cyclictest latency from 12us to 8us
    on Core i5 desktop.
    
    Note: this option requires tuning to find the appropriate value
    for a particular hardware/guest combination. One method is to measure the
    average delay between apic_timer_fn and VM-entry.
    Another method is to start with 1000ns, and increase the value
    in say 500ns increments until avg cyclictest numbers stop decreasing.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index cc1d61af6140..07994f38dacf 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -170,5 +170,7 @@ extern u64 kvm_supported_xcr0(void);
 
 extern unsigned int min_timer_period_us;
 
+extern unsigned int lapic_timer_advance_ns;
+
 extern struct static_key kvm_no_apic_vcpu;
 #endif

commit 612263b30c900b534fa76609d314ed55c255a94e
Author: Chao Peng <chao.p.peng@linux.intel.com>
Date:   Wed Oct 22 17:35:24 2014 +0800

    KVM: x86: Enable Intel AVX-512 for guest
    
    Expose Intel AVX-512 feature bits to guest. Also add checks for
    xcr0 AVX512 related bits according to spec:
    http://download-software.intel.com/sites/default/files/managed/71/2e/319433-017.pdf
    
    Signed-off-by: Chao Peng <chao.p.peng@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 7cb9c45a5fe0..cc1d61af6140 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -162,7 +162,8 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);
 
 #define KVM_SUPPORTED_XCR0     (XSTATE_FP | XSTATE_SSE | XSTATE_YMM \
-				| XSTATE_BNDREGS | XSTATE_BNDCSR)
+				| XSTATE_BNDREGS | XSTATE_BNDCSR \
+				| XSTATE_AVX512)
 extern u64 host_xcr0;
 
 extern u64 kvm_supported_xcr0(void);

commit 4566654bb9be9e8864df417bb72ceee5136b6a6a
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Thu Sep 18 22:39:44 2014 +0300

    KVM: vmx: Inject #GP on invalid PAT CR
    
    Guest which sets the PAT CR to invalid value should get a #GP.  Currently, if
    vmx supports loading PAT CR during entry, then the value is not checked.  This
    patch makes the required check in that case.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 985fb2c006fa..7cb9c45a5fe0 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -159,6 +159,8 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
+bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+
 #define KVM_SUPPORTED_XCR0     (XSTATE_FP | XSTATE_SSE | XSTATE_YMM \
 				| XSTATE_BNDREGS | XSTATE_BNDCSR)
 extern u64 host_xcr0;

commit 56f17dd3fbc44adcdbc3340fe3988ddb833a47a7
Author: David Matlack <dmatlack@google.com>
Date:   Mon Aug 18 15:46:07 2014 -0700

    kvm: x86: fix stale mmio cache bug
    
    The following events can lead to an incorrect KVM_EXIT_MMIO bubbling
    up to userspace:
    
    (1) Guest accesses gpa X without a memory slot. The gfn is cached in
    struct kvm_vcpu_arch (mmio_gfn). On Intel EPT-enabled hosts, KVM sets
    the SPTE write-execute-noread so that future accesses cause
    EPT_MISCONFIGs.
    
    (2) Host userspace creates a memory slot via KVM_SET_USER_MEMORY_REGION
    covering the page just accessed.
    
    (3) Guest attempts to read or write to gpa X again. On Intel, this
    generates an EPT_MISCONFIG. The memory slot generation number that
    was incremented in (2) would normally take care of this but we fast
    path mmio faults through quickly_check_mmio_pf(), which only checks
    the per-vcpu mmio cache. Since we hit the cache, KVM passes a
    KVM_EXIT_MMIO up to userspace.
    
    This patch fixes the issue by using the memslot generation number
    to validate the mmio cache.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: David Matlack <dmatlack@google.com>
    [xiaoguangrong: adjust the code to make it simpler for stable-tree fix.]
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Reviewed-by: David Matlack <dmatlack@google.com>
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Tested-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 306a1b77581f..985fb2c006fa 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -88,15 +88,23 @@ static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 	vcpu->arch.mmio_gva = gva & PAGE_MASK;
 	vcpu->arch.access = access;
 	vcpu->arch.mmio_gfn = gfn;
+	vcpu->arch.mmio_gen = kvm_memslots(vcpu->kvm)->generation;
+}
+
+static inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.mmio_gen == kvm_memslots(vcpu->kvm)->generation;
 }
 
 /*
- * Clear the mmio cache info for the given gva,
- * specially, if gva is ~0ul, we clear all mmio cache info.
+ * Clear the mmio cache info for the given gva. If gva is MMIO_GVA_ANY, we
+ * clear all mmio cache info.
  */
+#define MMIO_GVA_ANY (~(gva_t)0)
+
 static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
 {
-	if (gva != (~0ul) && vcpu->arch.mmio_gva != (gva & PAGE_MASK))
+	if (gva != MMIO_GVA_ANY && vcpu->arch.mmio_gva != (gva & PAGE_MASK))
 		return;
 
 	vcpu->arch.mmio_gva = 0;
@@ -104,7 +112,8 @@ static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
 
 static inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)
 {
-	if (vcpu->arch.mmio_gva && vcpu->arch.mmio_gva == (gva & PAGE_MASK))
+	if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gva &&
+	      vcpu->arch.mmio_gva == (gva & PAGE_MASK))
 		return true;
 
 	return false;
@@ -112,7 +121,8 @@ static inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)
 
 static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
-	if (vcpu->arch.mmio_gfn && vcpu->arch.mmio_gfn == gpa >> PAGE_SHIFT)
+	if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gfn &&
+	      vcpu->arch.mmio_gfn == gpa >> PAGE_SHIFT)
 		return true;
 
 	return false;

commit 27e6fb5dae2819d17f38dc9224692b771e989981
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Wed Jun 18 17:19:26 2014 +0300

    KVM: vmx: vmx instructions handling does not consider cs.l
    
    VMX instructions use 32-bit operands in 32-bit mode, and 64-bit operands in
    64-bit mode.  The current implementation is broken since it does not use the
    register operands correctly, and always uses 64-bit for reads and writes.
    Moreover, write to memory in vmwrite only considers long-mode, so it ignores
    cs.l. This patch fixes this behavior.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index c5b61a7eb144..306a1b77581f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -126,6 +126,15 @@ static inline unsigned long kvm_register_readl(struct kvm_vcpu *vcpu,
 	return is_64_bit_mode(vcpu) ? val : (u32)val;
 }
 
+static inline void kvm_register_writel(struct kvm_vcpu *vcpu,
+				       enum kvm_reg reg,
+				       unsigned long val)
+{
+	if (!is_64_bit_mode(vcpu))
+		val = (u32)val;
+	return kvm_register_write(vcpu, reg, val);
+}
+
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);

commit 5777392e83c96e3a0799dd2985598e0fc76cf4aa
Author: Nadav Amit <nadav.amit@gmail.com>
Date:   Wed Jun 18 17:19:23 2014 +0300

    KVM: x86: check DR6/7 high-bits are clear only on long-mode
    
    When the guest sets DR6 and DR7, KVM asserts the high 32-bits are clear, and
    otherwise injects a #GP exception. This exception should only be injected only
    if running in long-mode.
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 8c97bac9a895..c5b61a7eb144 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -47,6 +47,16 @@ static inline int is_long_mode(struct kvm_vcpu *vcpu)
 #endif
 }
 
+static inline bool is_64_bit_mode(struct kvm_vcpu *vcpu)
+{
+	int cs_db, cs_l;
+
+	if (!is_long_mode(vcpu))
+		return false;
+	kvm_x86_ops->get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
+	return cs_l;
+}
+
 static inline bool mmu_is_nested(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
@@ -108,6 +118,14 @@ static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 	return false;
 }
 
+static inline unsigned long kvm_register_readl(struct kvm_vcpu *vcpu,
+					       enum kvm_reg reg)
+{
+	unsigned long val = kvm_register_read(vcpu, reg);
+
+	return is_64_bit_mode(vcpu) ? val : (u32)val;
+}
+
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);

commit 4ff417320c2dfc984ec1939a7da888976441a881
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Feb 24 12:15:16 2014 +0100

    KVM: x86: introduce kvm_supported_xcr0()
    
    XSAVE support for KVM is already using host_xcr0 & KVM_SUPPORTED_XCR0 as
    a "dynamic" version of KVM_SUPPORTED_XCR0.
    
    However, this is not enough because the MPX bits should not be presented
    to the guest unless kvm_x86_ops confirms the support.  So, replace all
    instances of host_xcr0 & KVM_SUPPORTED_XCR0 with a new function
    kvm_supported_xcr0() that also has this check.
    
    Note that here:
    
                    if (xstate_bv & ~KVM_SUPPORTED_XCR0)
                            return -EINVAL;
                    if (xstate_bv & ~host_cr0)
                            return -EINVAL;
    
    the code is equivalent to
    
                    if ((xstate_bv & ~KVM_SUPPORTED_XCR0) ||
                        (xstate_bv & ~host_cr0)
                            return -EINVAL;
    
    i.e. "xstate_bv & (~KVM_SUPPORTED_XCR0 | ~host_cr0)" which is in turn
    equal to "xstate_bv & ~(KVM_SUPPORTED_XCR0 & host_cr0)".  So we should
    also use the new function there.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 392ecbff0030..8c97bac9a895 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -126,6 +126,8 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 				| XSTATE_BNDREGS | XSTATE_BNDCSR)
 extern u64 host_xcr0;
 
+extern u64 kvm_supported_xcr0(void);
+
 extern unsigned int min_timer_period_us;
 
 extern struct static_key kvm_no_apic_vcpu;

commit 390bd528ae1c14d0b7f5db8225984f98617b3357
Author: Liu, Jinsong <jinsong.liu@intel.com>
Date:   Mon Feb 24 10:58:09 2014 +0000

    KVM: x86: Enable Intel MPX for guest
    
    From 44c2abca2c2eadc6f2f752b66de4acc8131880c4 Mon Sep 17 00:00:00 2001
    From: Liu Jinsong <jinsong.liu@intel.com>
    Date: Mon, 24 Feb 2014 18:12:31 +0800
    Subject: [PATCH v5 3/3] KVM: x86: Enable Intel MPX for guest
    
    This patch enable Intel MPX feature to guest.
    
    Signed-off-by: Xudong Hao <xudong.hao@intel.com>
    Signed-off-by: Liu Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 8da5823bcde6..392ecbff0030 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -122,7 +122,8 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
-#define KVM_SUPPORTED_XCR0	(XSTATE_FP | XSTATE_SSE | XSTATE_YMM)
+#define KVM_SUPPORTED_XCR0     (XSTATE_FP | XSTATE_SSE | XSTATE_YMM \
+				| XSTATE_BNDREGS | XSTATE_BNDCSR)
 extern u64 host_xcr0;
 
 extern unsigned int min_timer_period_us;

commit 9ed96e87c5748de4c2807ef17e81287c7304186c
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Jan 6 12:00:02 2014 -0200

    KVM: x86: limit PIT timer frequency
    
    Limit PIT timer frequency similarly to the limit applied by
    LAPIC timer.
    
    Cc: stable@kernel.org
    Reviewed-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 587fb9ede436..8da5823bcde6 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -125,5 +125,7 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 #define KVM_SUPPORTED_XCR0	(XSTATE_FP | XSTATE_SSE | XSTATE_YMM)
 extern u64 host_xcr0;
 
+extern unsigned int min_timer_period_us;
+
 extern struct static_key kvm_no_apic_vcpu;
 #endif

commit 647e23bb333ff196e7be8ae08c842d855fb850f6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 2 16:06:14 2013 +0200

    KVM: x86: mask unsupported XSAVE entries from leaf 0Dh index 0
    
    XSAVE entries that KVM does not support are reported by
    KVM_GET_SUPPORTED_CPUID for leaf 0Dh index 0 if the host supports them;
    they should be left out unless there is also hypervisor support for them.
    
    Sub-leafs are correctly handled in supported_xcr0_bit, fix index 0
    to match.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index e224f7a671b6..587fb9ede436 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -122,6 +122,7 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
+#define KVM_SUPPORTED_XCR0	(XSTATE_FP | XSTATE_SSE | XSTATE_YMM)
 extern u64 host_xcr0;
 
 extern struct static_key kvm_no_apic_vcpu;

commit 8fe8ab46be06fcd9abfe6fe9928fd95b54ab079a
Author: Will Auld <will.auld.intel@gmail.com>
Date:   Thu Nov 29 12:42:12 2012 -0800

    KVM: x86: Add code to track call origin for msr assignment
    
    In order to track who initiated the call (host or guest) to modify an msr
    value I have changed function call parameters along the call path. The
    specific change is to add a struct pointer parameter that points to (index,
    data, caller) information rather than having this information passed as
    individual parameters.
    
    The initial use for this capability is for updating the IA32_TSC_ADJUST msr
    while setting the tsc value. It is anticipated that this capability is
    useful for other tasks.
    
    Signed-off-by: Will Auld <will.auld@intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2b5219c12ac8..e224f7a671b6 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -112,7 +112,7 @@ void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 
-void kvm_write_tsc(struct kvm_vcpu *vcpu, u64 data);
+void kvm_write_tsc(struct kvm_vcpu *vcpu, struct msr_data *msr);
 
 int kvm_read_guest_virt(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,

commit 54e9818f3903902a4ea3046035739b8770880092
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Aug 5 15:58:32 2012 +0300

    KVM: use jump label to optimize checking for in kernel local apic presence
    
    Usually all vcpus have local apic pointer initialized, so the check may
    be completely skipped.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 3d1134ddb885..2b5219c12ac8 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -124,4 +124,5 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 
 extern u64 host_xcr0;
 
+extern struct static_key kvm_no_apic_vcpu;
 #endif

commit c36fc04ef558c95cff46a8c89d2f804f217335f5
Author: Davidlohr Bueso <dave@gnu.org>
Date:   Thu Mar 8 12:45:54 2012 +0100

    KVM: x86: add paging gcc optimization
    
    Since most guests will have paging enabled for memory management, add likely() optimization
    around CR0.PG checks.
    
    Signed-off-by: Davidlohr Bueso <dave@gnu.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index cb80c293cdd8..3d1134ddb885 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -64,7 +64,7 @@ static inline int is_pse(struct kvm_vcpu *vcpu)
 
 static inline int is_paging(struct kvm_vcpu *vcpu)
 {
-	return kvm_read_cr0_bits(vcpu, X86_CR0_PG);
+	return likely(kvm_read_cr0_bits(vcpu, X86_CR0_PG));
 }
 
 static inline u32 bit(int bitno)

commit 00b27a3efb116062ca5a276ad5cb01ea1b80b5f6
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Nov 23 16:30:32 2011 +0200

    KVM: Move cpuid code to new file
    
    The cpuid code has grown; put it into a separate file.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index d36fe237c665..cb80c293cdd8 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -33,9 +33,6 @@ static inline bool kvm_exception_is_soft(unsigned int nr)
 	return (nr == BP_VECTOR) || (nr == OF_VECTOR);
 }
 
-struct kvm_cpuid_entry2 *kvm_find_cpuid_entry(struct kvm_vcpu *vcpu,
-                                             u32 function, u32 index);
-
 static inline bool is_protmode(struct kvm_vcpu *vcpu)
 {
 	return kvm_read_cr0_bits(vcpu, X86_CR0_PE);
@@ -125,4 +122,6 @@ int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
+extern u64 host_xcr0;
+
 #endif

commit bebb106a5afa32efdf5332ed4a40bf4d6d06b56e
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Jul 12 03:23:20 2011 +0800

    KVM: MMU: cache mmio info on page fault path
    
    If the page fault is caused by mmio, we can cache the mmio info, later, we do
    not need to walk guest page table and quickly know it is a mmio fault while we
    emulate the mmio instruction
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 256da82856bd..d36fe237c665 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -75,6 +75,42 @@ static inline u32 bit(int bitno)
 	return 1 << (bitno & 31);
 }
 
+static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
+					gva_t gva, gfn_t gfn, unsigned access)
+{
+	vcpu->arch.mmio_gva = gva & PAGE_MASK;
+	vcpu->arch.access = access;
+	vcpu->arch.mmio_gfn = gfn;
+}
+
+/*
+ * Clear the mmio cache info for the given gva,
+ * specially, if gva is ~0ul, we clear all mmio cache info.
+ */
+static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
+{
+	if (gva != (~0ul) && vcpu->arch.mmio_gva != (gva & PAGE_MASK))
+		return;
+
+	vcpu->arch.mmio_gva = 0;
+}
+
+static inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)
+{
+	if (vcpu->arch.mmio_gva && vcpu->arch.mmio_gva == (gva & PAGE_MASK))
+		return true;
+
+	return false;
+}
+
+static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
+{
+	if (vcpu->arch.mmio_gfn && vcpu->arch.mmio_gfn == gpa >> PAGE_SHIFT)
+		return true;
+
+	return false;
+}
+
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);

commit 6a4d7550601b5b17df227959bdbec208384f729c
Author: Nadav Har'El <nyh@il.ibm.com>
Date:   Wed May 25 23:08:00 2011 +0300

    KVM: nVMX: Implement VMPTRST
    
    This patch implements the VMPTRST instruction.
    
    Signed-off-by: Nadav Har'El <nyh@il.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 6f150539b262..256da82856bd 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -85,4 +85,8 @@ int kvm_read_guest_virt(struct x86_emulate_ctxt *ctxt,
 	gva_t addr, void *val, unsigned int bytes,
 	struct x86_exception *exception);
 
+int kvm_write_guest_virt_system(struct x86_emulate_ctxt *ctxt,
+	gva_t addr, void *val, unsigned int bytes,
+	struct x86_exception *exception);
+
 #endif

commit 064aea774768749c6fd308b37818ea3a9600583d
Author: Nadav Har'El <nyh@il.ibm.com>
Date:   Wed May 25 23:04:56 2011 +0300

    KVM: nVMX: Decoding memory operands of VMX instructions
    
    This patch includes a utility function for decoding pointer operands of VMX
    instructions issued by L1 (a guest hypervisor)
    
    Signed-off-by: Nadav Har'El <nyh@il.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index e407ed3df817..6f150539b262 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -81,4 +81,8 @@ int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 
 void kvm_write_tsc(struct kvm_vcpu *vcpu, u64 data);
 
+int kvm_read_guest_virt(struct x86_emulate_ctxt *ctxt,
+	gva_t addr, void *val, unsigned int bytes,
+	struct x86_exception *exception);
+
 #endif

commit 71f9833bb1cba9939245f3e57388d87d69f8f399
Author: Serge E. Hallyn <serge@hallyn.com>
Date:   Wed Apr 13 09:12:54 2011 -0500

    KVM: fix push of wrong eip when doing softint
    
    When doing a soft int, we need to bump eip before pushing it to
    the stack.  Otherwise we'll do the int a second time.
    
    [apw@canonical.com: merged eip update as per Jan's recommendation.]
    Signed-off-by: Serge E. Hallyn <serge.hallyn@ubuntu.com>
    Signed-off-by: Andy Whitcroft <apw@canonical.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index c600da830ce0..e407ed3df817 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -77,7 +77,7 @@ static inline u32 bit(int bitno)
 
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
-int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq);
+int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);
 
 void kvm_write_tsc(struct kvm_vcpu *vcpu, u64 data);
 

commit 24d1b15f72abe3465e871d11cfc9dc34d1aab8b2
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Tue Dec 7 17:15:05 2010 +0100

    KVM: SVM: Do not report xsave in supported cpuid
    
    To support xsave properly for the guest the SVM module need
    software support for it. As long as this is not present do
    not report the xsave as supported feature in cpuid.
    As a side-effect this patch moves the bit() helper function
    into the x86.h file so that it can be used in svm.c too.
    
    KVM-Stable-Tag.
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2cea414489f3..c600da830ce0 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -70,6 +70,11 @@ static inline int is_paging(struct kvm_vcpu *vcpu)
 	return kvm_read_cr0_bits(vcpu, X86_CR0_PG);
 }
 
+static inline u32 bit(int bitno)
+{
+	return 1 << (bitno & 31);
+}
+
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq);

commit 63995653ade16deacaea5b49ceaf6376314593ac
Author: Mohammed Gamal <m.gamal005@gmail.com>
Date:   Sun Sep 19 14:34:06 2010 +0200

    KVM: Add kvm_inject_realmode_interrupt() wrapper
    
    This adds a wrapper function kvm_inject_realmode_interrupt() around the
    emulator function emulate_int_real() to allow real mode interrupt injection.
    
    [avi: initialize operand and address sizes before emulating interrupts]
    [avi: initialize rip for real mode interrupt injection]
    [avi: clear interrupt pending flag after emulating interrupt injection]
    
    Signed-off-by: Mohammed Gamal <m.gamal005@gmail.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index bf4dc2f40d7f..2cea414489f3 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -72,6 +72,7 @@ static inline int is_paging(struct kvm_vcpu *vcpu)
 
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
+int kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq);
 
 void kvm_write_tsc(struct kvm_vcpu *vcpu, u64 data);
 

commit 6539e738f65a8f1fc7806295d5d701fba4008343
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:50 2010 +0200

    KVM: MMU: Implement nested gva_to_gpa functions
    
    This patch adds the functions to do a nested l2_gva to
    l1_gpa page table walk.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2d6385e44ccf..bf4dc2f40d7f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -50,6 +50,11 @@ static inline int is_long_mode(struct kvm_vcpu *vcpu)
 #endif
 }
 
+static inline bool mmu_is_nested(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
+}
+
 static inline int is_pae(struct kvm_vcpu *vcpu)
 {
 	return kvm_read_cr4_bits(vcpu, X86_CR4_PAE);

commit 99e3e30aee1a326a98bf3a5f47b8622219c685f3
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Thu Aug 19 22:07:17 2010 -1000

    KVM: x86: Move TSC offset writes to common code
    
    Also, ensure that the storing of the offset and the reading of the TSC
    are never preempted by taking a spinlock.  While the lock is overkill
    now, it is useful later in this patch series.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index b7a404722d2b..2d6385e44ccf 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -68,4 +68,6 @@ static inline int is_paging(struct kvm_vcpu *vcpu)
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 
+void kvm_write_tsc(struct kvm_vcpu *vcpu, u64 data);
+
 #endif

commit a1f4d39500ad8ed61825eff061debff42386ab5b
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Jun 21 11:44:20 2010 +0300

    KVM: Remove memory alias support
    
    As advertised in feature-removal-schedule.txt.  Equivalent support is provided
    by overlapping memory regions.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index f4b54458285b..b7a404722d2b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -65,13 +65,6 @@ static inline int is_paging(struct kvm_vcpu *vcpu)
 	return kvm_read_cr0_bits(vcpu, X86_CR0_PG);
 }
 
-static inline struct kvm_mem_aliases *kvm_aliases(struct kvm *kvm)
-{
-	return rcu_dereference_check(kvm->arch.aliases,
-			srcu_read_lock_held(&kvm->srcu)
-			|| lockdep_is_held(&kvm->slots_lock));
-}
-
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 

commit 90d83dc3d49f5101addae962ccc1b4aff66b68d8
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Mon Apr 19 17:41:23 2010 +0800

    KVM: use the correct RCU API for PROVE_RCU=y
    
    The RCU/SRCU API have already changed for proving RCU usage.
    
    I got the following dmesg when PROVE_RCU=y because we used incorrect API.
    This patch coverts rcu_deference() to srcu_dereference() or family API.
    
    ===================================================
    [ INFO: suspicious rcu_dereference_check() usage. ]
    ---------------------------------------------------
    arch/x86/kvm/mmu.c:3020 invoked rcu_dereference_check() without protection!
    
    other info that might help us debug this:
    
    rcu_scheduler_active = 1, debug_locks = 0
    2 locks held by qemu-system-x86/8550:
     #0:  (&kvm->slots_lock){+.+.+.}, at: [<ffffffffa011a6ac>] kvm_set_memory_region+0x29/0x50 [kvm]
     #1:  (&(&kvm->mmu_lock)->rlock){+.+...}, at: [<ffffffffa012262d>] kvm_arch_commit_memory_region+0xa6/0xe2 [kvm]
    
    stack backtrace:
    Pid: 8550, comm: qemu-system-x86 Not tainted 2.6.34-rc4-tip-01028-g939eab1 #27
    Call Trace:
     [<ffffffff8106c59e>] lockdep_rcu_dereference+0xaa/0xb3
     [<ffffffffa012f6c1>] kvm_mmu_calculate_mmu_pages+0x44/0x7d [kvm]
     [<ffffffffa012263e>] kvm_arch_commit_memory_region+0xb7/0xe2 [kvm]
     [<ffffffffa011a5d7>] __kvm_set_memory_region+0x636/0x6e2 [kvm]
     [<ffffffffa011a6ba>] kvm_set_memory_region+0x37/0x50 [kvm]
     [<ffffffffa015e956>] vmx_set_tss_addr+0x46/0x5a [kvm_intel]
     [<ffffffffa0126592>] kvm_arch_vm_ioctl+0x17a/0xcf8 [kvm]
     [<ffffffff810a8692>] ? unlock_page+0x27/0x2c
     [<ffffffff810bf879>] ? __do_fault+0x3a9/0x3e1
     [<ffffffffa011b12f>] kvm_vm_ioctl+0x364/0x38d [kvm]
     [<ffffffff81060cfa>] ? up_read+0x23/0x3d
     [<ffffffff810f3587>] vfs_ioctl+0x32/0xa6
     [<ffffffff810f3b19>] do_vfs_ioctl+0x495/0x4db
     [<ffffffff810e6b2f>] ? fget_light+0xc2/0x241
     [<ffffffff810e416c>] ? do_sys_open+0x104/0x116
     [<ffffffff81382d6d>] ? retint_swapgs+0xe/0x13
     [<ffffffff810f3ba6>] sys_ioctl+0x47/0x6a
     [<ffffffff810021db>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index b7a404722d2b..f4b54458285b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -65,6 +65,13 @@ static inline int is_paging(struct kvm_vcpu *vcpu)
 	return kvm_read_cr0_bits(vcpu, X86_CR0_PG);
 }
 
+static inline struct kvm_mem_aliases *kvm_aliases(struct kvm *kvm)
+{
+	return rcu_dereference_check(kvm->arch.aliases,
+			srcu_read_lock_held(&kvm->srcu)
+			|| lockdep_is_held(&kvm->slots_lock));
+}
+
 void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
 void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
 

commit ff9d07a0e7ce756a183e7c2e483aec452ee6b574
Author: Zhang, Yanmin <yanmin_zhang@linux.intel.com>
Date:   Mon Apr 19 13:32:45 2010 +0800

    KVM: Implement perf callbacks for guest sampling
    
    Below patch implements the perf_guest_info_callbacks on kvm.
    
    Signed-off-by: Zhang Yanmin <yanmin_zhang@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2d101639bd8d..b7a404722d2b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -65,4 +65,7 @@ static inline int is_paging(struct kvm_vcpu *vcpu)
 	return kvm_read_cr0_bits(vcpu, X86_CR0_PG);
 }
 
+void kvm_before_handle_nmi(struct kvm_vcpu *vcpu);
+void kvm_after_handle_nmi(struct kvm_vcpu *vcpu);
+
 #endif

commit f6801dff23bd1902473902194667f4ac1eb6ea26
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jan 21 15:31:50 2010 +0200

    KVM: Rename vcpu->shadow_efer to efer
    
    None of the other registers have the shadow_ prefix.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2dc24a755b6d..2d101639bd8d 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -44,7 +44,7 @@ static inline bool is_protmode(struct kvm_vcpu *vcpu)
 static inline int is_long_mode(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
-	return vcpu->arch.shadow_efer & EFER_LMA;
+	return vcpu->arch.efer & EFER_LMA;
 #else
 	return 0;
 #endif

commit 836a1b3c3456042704c86aaa3d837b976de9343b
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jan 21 15:31:49 2010 +0200

    KVM: Move cr0/cr4/efer related helpers to x86.h
    
    They have more general scope than the mmu.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index f783d8fe0d1d..2dc24a755b6d 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -41,4 +41,28 @@ static inline bool is_protmode(struct kvm_vcpu *vcpu)
 	return kvm_read_cr0_bits(vcpu, X86_CR0_PE);
 }
 
+static inline int is_long_mode(struct kvm_vcpu *vcpu)
+{
+#ifdef CONFIG_X86_64
+	return vcpu->arch.shadow_efer & EFER_LMA;
+#else
+	return 0;
+#endif
+}
+
+static inline int is_pae(struct kvm_vcpu *vcpu)
+{
+	return kvm_read_cr4_bits(vcpu, X86_CR4_PAE);
+}
+
+static inline int is_pse(struct kvm_vcpu *vcpu)
+{
+	return kvm_read_cr4_bits(vcpu, X86_CR4_PSE);
+}
+
+static inline int is_paging(struct kvm_vcpu *vcpu)
+{
+	return kvm_read_cr0_bits(vcpu, X86_CR0_PG);
+}
+
 #endif

commit 3eeb3288bcbf64da90afc26389b8844df7c34912
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jan 21 15:31:48 2010 +0200

    KVM: Add a helper for checking if the guest is in protected mode
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 5eadea585d2a..f783d8fe0d1d 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -2,6 +2,7 @@
 #define ARCH_X86_KVM_X86_H
 
 #include <linux/kvm_host.h>
+#include "kvm_cache_regs.h"
 
 static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 {
@@ -35,4 +36,9 @@ static inline bool kvm_exception_is_soft(unsigned int nr)
 struct kvm_cpuid_entry2 *kvm_find_cpuid_entry(struct kvm_vcpu *vcpu,
                                              u32 function, u32 index);
 
+static inline bool is_protmode(struct kvm_vcpu *vcpu)
+{
+	return kvm_read_cr0_bits(vcpu, X86_CR0_PE);
+}
+
 #endif

commit fc61b800f9506ca47bf1439342a79847f2353562
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Jul 5 17:39:35 2009 +0300

    KVM: Add Directed EOI support to APIC emulation
    
    Directed EOI is specified by x2APIC, but is available even when lapic is
    in xAPIC mode.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 4c8e10af78e8..5eadea585d2a 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -31,4 +31,8 @@ static inline bool kvm_exception_is_soft(unsigned int nr)
 {
 	return (nr == BP_VECTOR) || (nr == OF_VECTOR);
 }
+
+struct kvm_cpuid_entry2 *kvm_find_cpuid_entry(struct kvm_vcpu *vcpu,
+                                             u32 function, u32 index);
+
 #endif

commit 66fd3f7f901f29a557a473af595bf11b270b9ac2
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon May 11 13:35:50 2009 +0300

    KVM: Do not re-execute INTn instruction.
    
    Re-inject event instead. This is what Intel suggest. Also use correct
    instruction length when re-injecting soft fault/interrupt.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index c1f1a8ceba64..4c8e10af78e8 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -8,9 +8,11 @@ static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.exception.pending = false;
 }
 
-static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector)
+static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
+	bool soft)
 {
 	vcpu->arch.interrupt.pending = true;
+	vcpu->arch.interrupt.soft = soft;
 	vcpu->arch.interrupt.nr = vector;
 }
 
@@ -24,4 +26,9 @@ static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)
 	return vcpu->arch.exception.pending || vcpu->arch.interrupt.pending ||
 		vcpu->arch.nmi_injected;
 }
+
+static inline bool kvm_exception_is_soft(unsigned int nr)
+{
+	return (nr == BP_VECTOR) || (nr == OF_VECTOR);
+}
 #endif

commit 923c61bbc6413e87e5f6b0bae663d202a8de0537
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon May 11 13:35:48 2009 +0300

    KVM: Remove irq_pending bitmap
    
    Only one interrupt vector can be injected from userspace irqchip at
    any given time so no need to store it in a bitmap. Put it into interrupt
    queue directly.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 21203d421276..c1f1a8ceba64 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -19,18 +19,6 @@ static inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.interrupt.pending = false;
 }
 
-static inline u8 kvm_pop_irq(struct kvm_vcpu *vcpu)
-{
-	int word_index = __ffs(vcpu->arch.irq_summary);
-	int bit_index = __ffs(vcpu->arch.irq_pending[word_index]);
-	int irq = word_index * BITS_PER_LONG + bit_index;
-
-	clear_bit(bit_index, &vcpu->arch.irq_pending[word_index]);
-	if (!vcpu->arch.irq_pending[word_index])
-		clear_bit(word_index, &vcpu->arch.irq_summary);
-	return irq;
-}
-
 static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.exception.pending || vcpu->arch.interrupt.pending ||

commit 3298b75c880d6f0fd70750233c0f3e71a72a5bfb
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon May 11 13:35:46 2009 +0300

    KVM: Unprotect a page if #PF happens during NMI injection.
    
    It is done for exception and interrupt already.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 39350b252725..21203d421276 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -30,4 +30,10 @@ static inline u8 kvm_pop_irq(struct kvm_vcpu *vcpu)
 		clear_bit(word_index, &vcpu->arch.irq_summary);
 	return irq;
 }
+
+static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)
+{
+	return vcpu->arch.exception.pending || vcpu->arch.interrupt.pending ||
+		vcpu->arch.nmi_injected;
+}
 #endif

commit 115666dfc733a6749d99bcf3b2fe3fa253218b36
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Apr 21 17:45:04 2009 +0300

    KVM: Remove kvm_push_irq()
    
    No longer used.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2ab679102dcd..39350b252725 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -30,11 +30,4 @@ static inline u8 kvm_pop_irq(struct kvm_vcpu *vcpu)
 		clear_bit(word_index, &vcpu->arch.irq_summary);
 	return irq;
 }
-
-static inline void kvm_push_irq(struct kvm_vcpu *vcpu, u8 irq)
-{
-        set_bit(irq, vcpu->arch.irq_pending);
-        set_bit(irq / BITS_PER_LONG, &vcpu->arch.irq_summary);
-}
-
 #endif

commit fe4c7b1914ac46af751d256f5a20c2e12dcbaaae
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Mar 23 11:23:18 2009 +0200

    KVM: reuse (pop|push)_irq from svm.c in vmx.c
    
    The prioritized bit vector manipulation functions are useful in both vmx and
    svm.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 6a4be78a7384..2ab679102dcd 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -19,4 +19,22 @@ static inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.interrupt.pending = false;
 }
 
+static inline u8 kvm_pop_irq(struct kvm_vcpu *vcpu)
+{
+	int word_index = __ffs(vcpu->arch.irq_summary);
+	int bit_index = __ffs(vcpu->arch.irq_pending[word_index]);
+	int irq = word_index * BITS_PER_LONG + bit_index;
+
+	clear_bit(bit_index, &vcpu->arch.irq_pending[word_index]);
+	if (!vcpu->arch.irq_pending[word_index])
+		clear_bit(word_index, &vcpu->arch.irq_summary);
+	return irq;
+}
+
+static inline void kvm_push_irq(struct kvm_vcpu *vcpu, u8 irq)
+{
+        set_bit(irq, vcpu->arch.irq_pending);
+        set_bit(irq / BITS_PER_LONG, &vcpu->arch.irq_summary);
+}
+
 #endif

commit 937a7eaef9f08342958d17055a350982b7bd92cb
Author: Avi Kivity <avi@qumranet.com>
Date:   Thu Jul 3 15:17:01 2008 +0300

    KVM: Add a pending interrupt queue
    
    Similar to the exception queue, this hold interrupts that have been
    accepted by the virtual processor core but not yet injected.
    
    Not yet used.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index c666649c4bb2..6a4be78a7384 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -8,4 +8,15 @@ static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.exception.pending = false;
 }
 
+static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector)
+{
+	vcpu->arch.interrupt.pending = true;
+	vcpu->arch.interrupt.nr = vector;
+}
+
+static inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)
+{
+	vcpu->arch.interrupt.pending = false;
+}
+
 #endif

commit 26eef70c3e8c76e73dff2579c792fc7355f8a291
Author: Avi Kivity <avi@qumranet.com>
Date:   Thu Jul 3 14:59:22 2008 +0300

    KVM: Clear exception queue before emulating an instruction
    
    If we're emulating an instruction, either it will succeed, in which case
    any previously queued exception will be spurious, or we will requeue the
    same exception.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
new file mode 100644
index 000000000000..c666649c4bb2
--- /dev/null
+++ b/arch/x86/kvm/x86.h
@@ -0,0 +1,11 @@
+#ifndef ARCH_X86_KVM_X86_H
+#define ARCH_X86_KVM_X86_H
+
+#include <linux/kvm_host.h>
+
+static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
+{
+	vcpu->arch.exception.pending = false;
+}
+
+#endif
