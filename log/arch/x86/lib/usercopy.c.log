commit 4012e77a903d114f915fc607d6d2ed54a3d6c9b1
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Aug 29 08:47:18 2018 -0700

    x86/nmi: Fix NMI uaccess race against CR3 switching
    
    
    A NMI can hit in the middle of context switching or in the middle of
    switch_mm_irqs_off().  In either case, CR3 might not match current->mm,
    which could cause copy_from_user_nmi() and friends to read the wrong
    memory.
    
    Fix it by adding a new nmi_uaccess_okay() helper and checking it in
    copy_from_user_nmi() and in __copy_from_user_nmi()'s callers.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jann Horn <jannh@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/dd956eba16646fd0b15c3c0741269dfd84452dac.1535557289.git.luto@kernel.org

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index c8c6ad0d58b8..3f435d7fca5e 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -7,6 +7,8 @@
 #include <linux/uaccess.h>
 #include <linux/export.h>
 
+#include <asm/tlbflush.h>
+
 /*
  * We rely on the nested NMI work to allow atomic faults from the NMI path; the
  * nested NMI paths are careful to preserve CR2.
@@ -19,6 +21,9 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	if (__range_not_ok(from, n, TASK_SIZE))
 		return n;
 
+	if (!nmi_uaccess_okay())
+		return n;
+
 	/*
 	 * Even though this function is typically called from NMI/IRQ context
 	 * disable pagefaults so that its behaviour is consistent even when

commit beba3a20bf90ce1b93e24592c3ebf0d0bb581bbe
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Mar 25 19:33:21 2017 -0400

    x86: switch to RAW_COPY_USER
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index a851f3d199c2..c8c6ad0d58b8 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -4,12 +4,9 @@
  *  For licencing details see kernel-base/COPYING
  */
 
-#include <linux/highmem.h>
+#include <linux/uaccess.h>
 #include <linux/export.h>
 
-#include <asm/word-at-a-time.h>
-#include <linux/sched.h>
-
 /*
  * We rely on the nested NMI work to allow atomic faults from the NMI path; the
  * nested NMI paths are careful to preserve CR2.
@@ -34,53 +31,3 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	return ret;
 }
 EXPORT_SYMBOL_GPL(copy_from_user_nmi);
-
-/**
- * copy_to_user: - Copy a block of data into user space.
- * @to:   Destination address, in user space.
- * @from: Source address, in kernel space.
- * @n:    Number of bytes to copy.
- *
- * Context: User context only. This function may sleep if pagefaults are
- *          enabled.
- *
- * Copy data from kernel space to user space.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- */
-unsigned long _copy_to_user(void __user *to, const void *from, unsigned n)
-{
-	if (access_ok(VERIFY_WRITE, to, n))
-		n = __copy_to_user(to, from, n);
-	return n;
-}
-EXPORT_SYMBOL(_copy_to_user);
-
-/**
- * copy_from_user: - Copy a block of data from user space.
- * @to:   Destination address, in kernel space.
- * @from: Source address, in user space.
- * @n:    Number of bytes to copy.
- *
- * Context: User context only. This function may sleep if pagefaults are
- *          enabled.
- *
- * Copy data from user space to kernel space.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- *
- * If some data could not be copied, this function will pad the copied
- * data to the requested size using zero bytes.
- */
-unsigned long _copy_from_user(void *to, const void __user *from, unsigned n)
-{
-	unsigned long res = n;
-	if (access_ok(VERIFY_READ, from, n))
-		res = __copy_from_user_inatomic(to, from, n);
-	if (unlikely(res))
-		memset(to + n - res, 0, res);
-	return res;
-}
-EXPORT_SYMBOL(_copy_from_user);

commit 122b05ddf506e637336dcf64b5a129825f7bf6d4
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Mar 25 18:36:22 2017 -0400

    amd64: get rid of zeroing
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index c074799bddae..a851f3d199c2 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -76,10 +76,11 @@ EXPORT_SYMBOL(_copy_to_user);
  */
 unsigned long _copy_from_user(void *to, const void __user *from, unsigned n)
 {
+	unsigned long res = n;
 	if (access_ok(VERIFY_READ, from, n))
-		n = __copy_from_user(to, from, n);
-	else
-		memset(to, 0, n);
-	return n;
+		res = __copy_from_user_inatomic(to, from, n);
+	if (unlikely(res))
+		memset(to + n - res, 0, res);
+	return res;
 }
 EXPORT_SYMBOL(_copy_from_user);

commit adb402cd1461eef6e1a21db4532a3b9e6a6be853
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Oct 31 16:10:15 2016 +0100

    x86/copy_user: Unify the code by removing the 64-bit asm _copy_*_user() variants
    
    We already have the same functionality in usercopy_32.c. Share it with
    64-bit and get rid of some more asm glue which is not needed anymore.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161031151015.22087-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index b4908789484e..c074799bddae 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -34,3 +34,52 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	return ret;
 }
 EXPORT_SYMBOL_GPL(copy_from_user_nmi);
+
+/**
+ * copy_to_user: - Copy a block of data into user space.
+ * @to:   Destination address, in user space.
+ * @from: Source address, in kernel space.
+ * @n:    Number of bytes to copy.
+ *
+ * Context: User context only. This function may sleep if pagefaults are
+ *          enabled.
+ *
+ * Copy data from kernel space to user space.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ */
+unsigned long _copy_to_user(void __user *to, const void *from, unsigned n)
+{
+	if (access_ok(VERIFY_WRITE, to, n))
+		n = __copy_to_user(to, from, n);
+	return n;
+}
+EXPORT_SYMBOL(_copy_to_user);
+
+/**
+ * copy_from_user: - Copy a block of data from user space.
+ * @to:   Destination address, in kernel space.
+ * @from: Source address, in user space.
+ * @n:    Number of bytes to copy.
+ *
+ * Context: User context only. This function may sleep if pagefaults are
+ *          enabled.
+ *
+ * Copy data from user space to kernel space.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ *
+ * If some data could not be copied, this function will pad the copied
+ * data to the requested size using zero bytes.
+ */
+unsigned long _copy_from_user(void *to, const void __user *from, unsigned n)
+{
+	if (access_ok(VERIFY_READ, from, n))
+		n = __copy_from_user(to, from, n);
+	else
+		memset(to, 0, n);
+	return n;
+}
+EXPORT_SYMBOL(_copy_from_user);

commit e683014c2113d374b8716a7c938803c2f578efd5
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:57 2016 -0400

    x86/lib: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed a couple implicit header usage issues that were fixed.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-5-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index e342586db6e4..b4908789484e 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -5,7 +5,7 @@
  */
 
 #include <linux/highmem.h>
-#include <linux/module.h>
+#include <linux/export.h>
 
 #include <asm/word-at-a-time.h>
 #include <linux/sched.h>

commit ebf2d2689de551d90965090bb991fc640a0c0d41
Author: Yann Droneaud <ydroneaud@opteya.com>
Date:   Mon Jun 22 21:38:43 2015 +0200

    perf/x86: Fix copy_from_user_nmi() return if range is not ok
    
    Commit 0a196848ca36 ("perf: Fix arch_perf_out_copy_user default"),
    changes copy_from_user_nmi() to return the number of
    remaining bytes so that it behave like copy_from_user().
    
    Unfortunately, when the range is outside of the process
    memory, the return value  is still the number of byte
    copied, eg. 0, instead of the remaining bytes.
    
    As all users of copy_from_user_nmi() were modified as
    part of commit 0a196848ca36, the function should be
    fixed to return the total number of bytes if range is
    not correct.
    
    Signed-off-by: Yann Droneaud <ydroneaud@opteya.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1435001923-30986-1-git-send-email-ydroneaud@opteya.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index ddf9ecb53cc3..e342586db6e4 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -20,7 +20,7 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	unsigned long ret;
 
 	if (__range_not_ok(from, n, TASK_SIZE))
-		return 0;
+		return n;
 
 	/*
 	 * Even though this function is typically called from NMI/IRQ context

commit 0a196848ca365ec582c6d86659be456be6d4ed96
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 30 21:16:22 2013 +0100

    perf: Fix arch_perf_out_copy_user default
    
    The arch_perf_output_copy_user() default of
    __copy_from_user_inatomic() returns bytes not copied, while all other
    argument functions given DEFINE_OUTPUT_COPY() return bytes copied.
    
    Since copy_from_user_nmi() is the odd duck out by returning bytes
    copied where all other *copy_{to,from}* functions return bytes not
    copied, change it over and ammend DEFINE_OUTPUT_COPY() to expect bytes
    not copied.
    
    Oddly enough DEFINE_OUTPUT_COPY() already returned bytes not copied
    while expecting its worker functions to return bytes copied.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: will.deacon@arm.com
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20131030201622.GR16117@laptop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index 5465b8613944..ddf9ecb53cc3 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -31,6 +31,6 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	ret = __copy_from_user_inatomic(to, from, n);
 	pagefault_enable();
 
-	return n - ret;
+	return ret;
 }
 EXPORT_SYMBOL_GPL(copy_from_user_nmi);

commit e00b12e64be9a34ef071de7b6052ca9ea29dd460
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 24 12:52:06 2013 +0200

    perf/x86: Further optimize copy_from_user_nmi()
    
    Now that we can deal with nested NMI due to IRET re-enabling NMIs and
    can deal with faults from NMI by making sure we preserve CR2 over NMIs
    we can in fact simply access user-space memory from NMI context.
    
    So rewrite copy_from_user_nmi() to use __copy_from_user_inatomic() and
    rework the fault path to do the minimal required work before taking
    the in_atomic() fault handler.
    
    In particular avoid perf_sw_event() which would make perf recurse on
    itself (it should be harmless as our recursion protections should be
    able to deal with this -- but why tempt fate).
    
    Also rename notify_page_fault() to kprobes_fault() as that is a much
    better name; there is no notifier in it and its specific to kprobes.
    
    Don measured that his worst case NMI path shrunk from ~300K cycles to
    ~150K cycles.
    
    Cc: Stephane Eranian <eranian@google.com>
    Cc: jmario@redhat.com
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: dave.hansen@linux.intel.com
    Tested-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20131024105206.GM2490@laptop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index 4f74d94c8d97..5465b8613944 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -11,39 +11,26 @@
 #include <linux/sched.h>
 
 /*
- * best effort, GUP based copy_from_user() that is NMI-safe
+ * We rely on the nested NMI work to allow atomic faults from the NMI path; the
+ * nested NMI paths are careful to preserve CR2.
  */
 unsigned long
 copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 {
-	unsigned long offset, addr = (unsigned long)from;
-	unsigned long size, len = 0;
-	struct page *page;
-	void *map;
-	int ret;
+	unsigned long ret;
 
 	if (__range_not_ok(from, n, TASK_SIZE))
-		return len;
-
-	do {
-		ret = __get_user_pages_fast(addr, 1, 0, &page);
-		if (!ret)
-			break;
-
-		offset = addr & (PAGE_SIZE - 1);
-		size = min(PAGE_SIZE - offset, n - len);
-
-		map = kmap_atomic(page);
-		memcpy(to, map+offset, size);
-		kunmap_atomic(map);
-		put_page(page);
-
-		len  += size;
-		to   += size;
-		addr += size;
-
-	} while (len < n);
-
-	return len;
+		return 0;
+
+	/*
+	 * Even though this function is typically called from NMI/IRQ context
+	 * disable pagefaults so that its behaviour is consistent even when
+	 * called form other contexts.
+	 */
+	pagefault_disable();
+	ret = __copy_from_user_inatomic(to, from, n);
+	pagefault_enable();
+
+	return n - ret;
 }
 EXPORT_SYMBOL_GPL(copy_from_user_nmi);

commit 25f42985825dd93f0593efe454e54c2aa13f7830
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Jun 11 15:44:26 2012 +0200

    perf/x86: Fix broken LBR fixup code
    
    I noticed that the LBR fixups were not working anymore
    on programs where they used to. I tracked this down to
    a recent change to copy_from_user_nmi():
    
     db0dc75d640 ("perf/x86: Check user address explicitly in copy_from_user_nmi()")
    
    This commit added a call to __range_not_ok() to the
    copy_from_user_nmi() routine. The problem is that the logic
    of the test must be reversed. __range_not_ok() returns 0 if the
    range is VALID. We want to return early from copy_from_user_nmi()
    if the range is NOT valid.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Arun Sharma <asharma@fb.com>
    Link: http://lkml.kernel.org/r/20120611134426.GA7542@quad
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index 677b1ed184c9..4f74d94c8d97 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -22,7 +22,7 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	void *map;
 	int ret;
 
-	if (__range_not_ok(from, n, TASK_SIZE) == 0)
+	if (__range_not_ok(from, n, TASK_SIZE))
 		return len;
 
 	do {

commit db0dc75d6403b6663c0eab4c6ccb672eb9b2ed72
Author: Arun Sharma <asharma@fb.com>
Date:   Fri Apr 20 15:41:36 2012 -0700

    perf/x86: Check user address explicitly in copy_from_user_nmi()
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1334961696-19580-5-git-send-email-asharma@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index f61ee67ec00f..677b1ed184c9 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -8,6 +8,7 @@
 #include <linux/module.h>
 
 #include <asm/word-at-a-time.h>
+#include <linux/sched.h>
 
 /*
  * best effort, GUP based copy_from_user() that is NMI-safe
@@ -21,6 +22,9 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	void *map;
 	int ret;
 
+	if (__range_not_ok(from, n, TASK_SIZE) == 0)
+		return len;
+
 	do {
 		ret = __get_user_pages_fast(addr, 1, 0, &page);
 		if (!ret)

commit 4ae73f2d53255c388d50bf83c1681112a6f9cba1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 26 10:14:39 2012 -0700

    x86: use generic strncpy_from_user routine
    
    The generic strncpy_from_user() is not really optimal, since it is
    designed to work on both little-endian and big-endian.  And on
    little-endian you can simplify much of the logic to find the first zero
    byte, since little-endian arithmetic doesn't have to worry about the
    carry bit propagating into earlier bytes (only later bytes, which we
    don't care about).
    
    But I have patches to make the generic routines use the architecture-
    specific <asm/word-at-a-time.h> infrastructure, so that we can regain
    the little-endian optimizations.  But before we do that, switch over to
    the generic routines to make the patches each do just one well-defined
    thing.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index 2e4e4b02c37a..f61ee67ec00f 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -43,100 +43,3 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	return len;
 }
 EXPORT_SYMBOL_GPL(copy_from_user_nmi);
-
-/*
- * Do a strncpy, return length of string without final '\0'.
- * 'count' is the user-supplied count (return 'count' if we
- * hit it), 'max' is the address space maximum (and we return
- * -EFAULT if we hit it).
- */
-static inline long do_strncpy_from_user(char *dst, const char __user *src, long count, unsigned long max)
-{
-	long res = 0;
-
-	/*
-	 * Truncate 'max' to the user-specified limit, so that
-	 * we only have one limit we need to check in the loop
-	 */
-	if (max > count)
-		max = count;
-
-	while (max >= sizeof(unsigned long)) {
-		unsigned long c, mask;
-
-		/* Fall back to byte-at-a-time if we get a page fault */
-		if (unlikely(__get_user(c,(unsigned long __user *)(src+res))))
-			break;
-		mask = has_zero(c);
-		if (mask) {
-			mask = (mask - 1) & ~mask;
-			mask >>= 7;
-			*(unsigned long *)(dst+res) = c & mask;
-			return res + count_masked_bytes(mask);
-		}
-		*(unsigned long *)(dst+res) = c;
-		res += sizeof(unsigned long);
-		max -= sizeof(unsigned long);
-	}
-
-	while (max) {
-		char c;
-
-		if (unlikely(__get_user(c,src+res)))
-			return -EFAULT;
-		dst[res] = c;
-		if (!c)
-			return res;
-		res++;
-		max--;
-	}
-
-	/*
-	 * Uhhuh. We hit 'max'. But was that the user-specified maximum
-	 * too? If so, that's ok - we got as much as the user asked for.
-	 */
-	if (res >= count)
-		return res;
-
-	/*
-	 * Nope: we hit the address space limit, and we still had more
-	 * characters the caller would have wanted. That's an EFAULT.
-	 */
-	return -EFAULT;
-}
-
-/**
- * strncpy_from_user: - Copy a NUL terminated string from userspace.
- * @dst:   Destination address, in kernel space.  This buffer must be at
- *         least @count bytes long.
- * @src:   Source address, in user space.
- * @count: Maximum number of bytes to copy, including the trailing NUL.
- *
- * Copies a NUL-terminated string from userspace to kernel space.
- *
- * On success, returns the length of the string (not including the trailing
- * NUL).
- *
- * If access to userspace fails, returns -EFAULT (some data may have been
- * copied).
- *
- * If @count is smaller than the length of the string, copies @count bytes
- * and returns @count.
- */
-long
-strncpy_from_user(char *dst, const char __user *src, long count)
-{
-	unsigned long max_addr, src_addr;
-
-	if (unlikely(count <= 0))
-		return 0;
-
-	max_addr = current_thread_info()->addr_limit.seg;
-	src_addr = (unsigned long)src;
-	if (likely(src_addr < max_addr)) {
-		unsigned long max = max_addr - src_addr;
-		return do_strncpy_from_user(dst, src, count, max);
-	}
-	return -EFAULT;
-}
-EXPORT_SYMBOL(strncpy_from_user);

commit 0749708352fddbe0fa81fc25f96e3b1f77c655f4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 28 14:27:38 2012 -0700

    x86: make word-at-a-time strncpy_from_user clear bytes at the end
    
    This makes the newly optimized x86 strncpy_from_user clear the final
    bytes in the word past the final NUL character, rather than copy them as
    the word they were in the source.
    
    NOTE! Unlike the silly semantics of the libc 'strncpy()' function, the
    kernel strncpy_from_user() has never cleared all of the end of the
    destination buffer.  And neither does it do so now: it only clears the
    bytes at the end of the last word it copied.
    
    So why make this change at all? It doesn't really cost us anything extra
    (we have to calculate the mask to get the length anyway), and it means
    that *if* any user actually cares about zeroing the whole buffer, they
    can do a "memset()" before the strncpy_from_user(), and we will no
    longer write random bytes after the NUL character.
    
    In particular, the buffer contents will now at no point contain random
    source data from beyond the end of the string.
    
    In other words, it makes behavior a bit more repeatable at no new cost,
    so it's a small cleanup.  I've been carrying this as a patch for the
    last few weeks or so in my tree (done at the same time the sign error
    was fixed in commit 12e993b89464), I might as well commit it.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index d6ae30bbd7bb..2e4e4b02c37a 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -44,13 +44,6 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 }
 EXPORT_SYMBOL_GPL(copy_from_user_nmi);
 
-static inline unsigned long count_bytes(unsigned long mask)
-{
-	mask = (mask - 1) & ~mask;
-	mask >>= 7;
-	return count_masked_bytes(mask);
-}
-
 /*
  * Do a strncpy, return length of string without final '\0'.
  * 'count' is the user-supplied count (return 'count' if we
@@ -69,16 +62,19 @@ static inline long do_strncpy_from_user(char *dst, const char __user *src, long
 		max = count;
 
 	while (max >= sizeof(unsigned long)) {
-		unsigned long c;
+		unsigned long c, mask;
 
 		/* Fall back to byte-at-a-time if we get a page fault */
 		if (unlikely(__get_user(c,(unsigned long __user *)(src+res))))
 			break;
-		/* This can write a few bytes past the NUL character, but that's ok */
+		mask = has_zero(c);
+		if (mask) {
+			mask = (mask - 1) & ~mask;
+			mask >>= 7;
+			*(unsigned long *)(dst+res) = c & mask;
+			return res + count_masked_bytes(mask);
+		}
 		*(unsigned long *)(dst+res) = c;
-		c = has_zero(c);
-		if (c)
-			return res + count_bytes(c);
 		res += sizeof(unsigned long);
 		max -= sizeof(unsigned long);
 	}

commit 12e993b89464707398e4209bd99983e376454985
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 15 17:23:00 2012 -0700

    x86-32: fix up strncpy_from_user() sign error
    
    The 'max' range needs to be unsigned, since the size of the user address
    space is bigger than 2GB.
    
    We know that 'count' is positive in 'long' (that is checked in the
    caller), so we will truncate 'max' down to something that fits in a
    signed long, but before we actually do that, that comparison needs to be
    done in unsigned.
    
    Bug introduced in commit 92ae03f2ef99 ("x86: merge 32/64-bit versions of
    'strncpy_from_user()' and speed it up").  On x86-64 you can't trigger
    this, since the user address space is much smaller than 63 bits, and on
    x86-32 it works in practice, since you would seldom hit the strncpy
    limits anyway.
    
    I had actually tested the corner-cases, I had only tested them on
    x86-64.  Besides, I had only worried about the case of a pointer *close*
    to the end of the address space, rather than really far away from it ;)
    
    This also changes the "we hit the user-specified maximum" to return
    'res', for the trivial reason that gcc seems to generate better code
    that way.  'res' and 'count' are the same in that case, so it really
    doesn't matter which one we return.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index 57252c928f56..d6ae30bbd7bb 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -57,7 +57,7 @@ static inline unsigned long count_bytes(unsigned long mask)
  * hit it), 'max' is the address space maximum (and we return
  * -EFAULT if we hit it).
  */
-static inline long do_strncpy_from_user(char *dst, const char __user *src, long count, long max)
+static inline long do_strncpy_from_user(char *dst, const char __user *src, long count, unsigned long max)
 {
 	long res = 0;
 
@@ -100,7 +100,7 @@ static inline long do_strncpy_from_user(char *dst, const char __user *src, long
 	 * too? If so, that's ok - we got as much as the user asked for.
 	 */
 	if (res >= count)
-		return count;
+		return res;
 
 	/*
 	 * Nope: we hit the address space limit, and we still had more

commit 92ae03f2ef99fbc23bfa9080d6b58f25227bd7ef
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 6 14:32:32 2012 -0700

    x86: merge 32/64-bit versions of 'strncpy_from_user()' and speed it up
    
    This merges the 32- and 64-bit versions of the x86 strncpy_from_user()
    by just rewriting it in C rather than the ancient inline asm versions
    that used lodsb/stosb and had been duplicated for (trivial) differences
    between the 32-bit and 64-bit versions.
    
    While doing that, it also speeds them up by doing the accesses a word at
    a time.  Finally, the new routines also properly handle the case of
    hitting the end of the address space, which we have never done correctly
    before (fs/namei.c has a hack around it for that reason).
    
    Despite all these improvements, it actually removes more lines than it
    adds, due to the de-duplication.  Also, we no longer export (or define)
    the legacy __strncpy_from_user() function (that was defined to not do
    the user permission checks), since it's not actually used anywhere, and
    the user address space checks are built in to the new code.
    
    Other architecture maintainers have been notified that the old hack in
    fs/namei.c will be going away in the 3.5 merge window, in case they
    copied the x86 approach of being a bit cavalier about the end of the
    address space.
    
    Cc: linux-arch@vger.kernel.org
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index 97be9cb54483..57252c928f56 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -7,6 +7,8 @@
 #include <linux/highmem.h>
 #include <linux/module.h>
 
+#include <asm/word-at-a-time.h>
+
 /*
  * best effort, GUP based copy_from_user() that is NMI-safe
  */
@@ -41,3 +43,104 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 	return len;
 }
 EXPORT_SYMBOL_GPL(copy_from_user_nmi);
+
+static inline unsigned long count_bytes(unsigned long mask)
+{
+	mask = (mask - 1) & ~mask;
+	mask >>= 7;
+	return count_masked_bytes(mask);
+}
+
+/*
+ * Do a strncpy, return length of string without final '\0'.
+ * 'count' is the user-supplied count (return 'count' if we
+ * hit it), 'max' is the address space maximum (and we return
+ * -EFAULT if we hit it).
+ */
+static inline long do_strncpy_from_user(char *dst, const char __user *src, long count, long max)
+{
+	long res = 0;
+
+	/*
+	 * Truncate 'max' to the user-specified limit, so that
+	 * we only have one limit we need to check in the loop
+	 */
+	if (max > count)
+		max = count;
+
+	while (max >= sizeof(unsigned long)) {
+		unsigned long c;
+
+		/* Fall back to byte-at-a-time if we get a page fault */
+		if (unlikely(__get_user(c,(unsigned long __user *)(src+res))))
+			break;
+		/* This can write a few bytes past the NUL character, but that's ok */
+		*(unsigned long *)(dst+res) = c;
+		c = has_zero(c);
+		if (c)
+			return res + count_bytes(c);
+		res += sizeof(unsigned long);
+		max -= sizeof(unsigned long);
+	}
+
+	while (max) {
+		char c;
+
+		if (unlikely(__get_user(c,src+res)))
+			return -EFAULT;
+		dst[res] = c;
+		if (!c)
+			return res;
+		res++;
+		max--;
+	}
+
+	/*
+	 * Uhhuh. We hit 'max'. But was that the user-specified maximum
+	 * too? If so, that's ok - we got as much as the user asked for.
+	 */
+	if (res >= count)
+		return count;
+
+	/*
+	 * Nope: we hit the address space limit, and we still had more
+	 * characters the caller would have wanted. That's an EFAULT.
+	 */
+	return -EFAULT;
+}
+
+/**
+ * strncpy_from_user: - Copy a NUL terminated string from userspace.
+ * @dst:   Destination address, in kernel space.  This buffer must be at
+ *         least @count bytes long.
+ * @src:   Source address, in user space.
+ * @count: Maximum number of bytes to copy, including the trailing NUL.
+ *
+ * Copies a NUL-terminated string from userspace to kernel space.
+ *
+ * On success, returns the length of the string (not including the trailing
+ * NUL).
+ *
+ * If access to userspace fails, returns -EFAULT (some data may have been
+ * copied).
+ *
+ * If @count is smaller than the length of the string, copies @count bytes
+ * and returns @count.
+ */
+long
+strncpy_from_user(char *dst, const char __user *src, long count)
+{
+	unsigned long max_addr, src_addr;
+
+	if (unlikely(count <= 0))
+		return 0;
+
+	max_addr = current_thread_info()->addr_limit.seg;
+	src_addr = (unsigned long)src;
+	if (likely(src_addr < max_addr)) {
+		unsigned long max = max_addr - src_addr;
+		return do_strncpy_from_user(dst, src, count, max);
+	}
+	return -EFAULT;
+}
+EXPORT_SYMBOL(strncpy_from_user);

commit 1ac2e6ca44e13a087eb7438d284f887097ee7e84
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jun 7 11:49:55 2011 +0200

    x86, perf: Make copy_from_user_nmi() a library function
    
    copy_from_user_nmi() is used in oprofile and perf. Moving it to other
    library functions like copy_from_user(). As this is x86 code for 32
    and 64 bits, create a new file usercopy.c for unified code.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110607172413.GJ20052@erda.amd.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
new file mode 100644
index 000000000000..97be9cb54483
--- /dev/null
+++ b/arch/x86/lib/usercopy.c
@@ -0,0 +1,43 @@
+/*
+ * User address space access functions.
+ *
+ *  For licencing details see kernel-base/COPYING
+ */
+
+#include <linux/highmem.h>
+#include <linux/module.h>
+
+/*
+ * best effort, GUP based copy_from_user() that is NMI-safe
+ */
+unsigned long
+copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
+{
+	unsigned long offset, addr = (unsigned long)from;
+	unsigned long size, len = 0;
+	struct page *page;
+	void *map;
+	int ret;
+
+	do {
+		ret = __get_user_pages_fast(addr, 1, 0, &page);
+		if (!ret)
+			break;
+
+		offset = addr & (PAGE_SIZE - 1);
+		size = min(PAGE_SIZE - offset, n - len);
+
+		map = kmap_atomic(page);
+		memcpy(to, map+offset, size);
+		kunmap_atomic(map);
+		put_page(page);
+
+		len  += size;
+		to   += size;
+		addr += size;
+
+	} while (len < n);
+
+	return len;
+}
+EXPORT_SYMBOL_GPL(copy_from_user_nmi);
