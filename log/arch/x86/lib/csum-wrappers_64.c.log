commit 0a5ea224b2fdf9dca9291ef7b5a12fd846a5dc34
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Feb 16 16:50:00 2020 -0500

    x86: switch both 32bit and 64bit to providing csum_and_copy_from_user()
    
    ... rather than messing with the wrapper.  As a side effect,
    32bit variant gets access_ok() into it and can be switched to
    user_access_begin()/user_access_end()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 7028d1dc5c6b..ee63d7576fd2 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -10,7 +10,7 @@
 #include <asm/smap.h>
 
 /**
- * csum_partial_copy_from_user - Copy and checksum from user space.
+ * csum_and_copy_from_user - Copy and checksum from user space.
  * @src: source address (user space)
  * @dst: destination address
  * @len: number of bytes to be copied.
@@ -21,7 +21,7 @@
  * src and dst are best aligned to 64bits.
  */
 __wsum
-csum_partial_copy_from_user(const void __user *src, void *dst,
+csum_and_copy_from_user(const void __user *src, void *dst,
 			    int len, __wsum isum, int *errp)
 {
 	might_sleep();
@@ -68,7 +68,7 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 
 	return isum;
 }
-EXPORT_SYMBOL(csum_partial_copy_from_user);
+EXPORT_SYMBOL(csum_and_copy_from_user);
 
 /**
  * csum_and_copy_to_user - Copy and checksum to user space.

commit 73e800ecb67bf24c0508c63350c1dc509f00b1ce
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Feb 16 14:56:36 2020 -0500

    x86_64: csum_..._copy_..._user(): switch to unsafe_..._user()
    
    We already have stac/clac pair around the calls of csum_partial_copy_generic().
    Stretch that area back, so that it covers the preceding loop (and convert
    the loop body from __{get,put}_user() to unsafe_{get,put}_user()).
    That brings the beginning of the areas to the earlier access_ok(),
    which allows to convert them into user_access_{begin,end}() ones.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 875c2f5968a0..7028d1dc5c6b 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -27,7 +27,7 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 	might_sleep();
 	*errp = 0;
 
-	if (!likely(access_ok(src, len)))
+	if (!user_access_begin(src, len))
 		goto out_err;
 
 	/*
@@ -42,8 +42,7 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 		while (((unsigned long)src & 6) && len >= 2) {
 			__u16 val16;
 
-			if (__get_user(val16, (const __u16 __user *)src))
-				goto out_err;
+			unsafe_get_user(val16, (const __u16 __user *)src, out);
 
 			*(__u16 *)dst = val16;
 			isum = (__force __wsum)add32_with_carry(
@@ -53,15 +52,16 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 			len -= 2;
 		}
 	}
-	stac();
 	isum = csum_partial_copy_generic((__force const void *)src,
 				dst, len, isum, errp, NULL);
-	clac();
+	user_access_end();
 	if (unlikely(*errp))
 		goto out_err;
 
 	return isum;
 
+out:
+	user_access_end();
 out_err:
 	*errp = -EFAULT;
 	memset(dst, 0, len);
@@ -89,7 +89,7 @@ csum_and_copy_to_user(const void *src, void __user *dst,
 
 	might_sleep();
 
-	if (unlikely(!access_ok(dst, len))) {
+	if (!user_access_begin(dst, len)) {
 		*errp = -EFAULT;
 		return 0;
 	}
@@ -100,9 +100,7 @@ csum_and_copy_to_user(const void *src, void __user *dst,
 
 			isum = (__force __wsum)add32_with_carry(
 					(__force unsigned)isum, val16);
-			*errp = __put_user(val16, (__u16 __user *)dst);
-			if (*errp)
-				return isum;
+			unsafe_put_user(val16, (__u16 __user *)dst, out);
 			src += 2;
 			dst += 2;
 			len -= 2;
@@ -110,11 +108,14 @@ csum_and_copy_to_user(const void *src, void __user *dst,
 	}
 
 	*errp = 0;
-	stac();
 	ret = csum_partial_copy_generic(src, (void __force *)dst,
 					len, isum, NULL, errp);
-	clac();
+	user_access_end();
 	return ret;
+out:
+	user_access_end();
+	*errp = -EFAULT;
+	return isum;
 }
 EXPORT_SYMBOL(csum_and_copy_to_user);
 

commit 2a89b674fd6834dacf2a6edfbdf5607c163dd36e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Mar 19 13:19:34 2020 -0400

    get rid of csum_partial_copy_to_user()
    
    For historical reasons some architectures call their csum_and_copy_to_user()
    csum_partial_copy_to_user() instead (and supply a macro defining the
    former as the latter).  That's the last remnants of old experiment that
    went nowhere; time to bury them.  Rename those to csum_and_copy_to_user()
    and get rid of the macros.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index c66c8b00f236..875c2f5968a0 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -71,7 +71,7 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 EXPORT_SYMBOL(csum_partial_copy_from_user);
 
 /**
- * csum_partial_copy_to_user - Copy and checksum to user space.
+ * csum_and_copy_to_user - Copy and checksum to user space.
  * @src: source address
  * @dst: destination address (user space)
  * @len: number of bytes to be copied.
@@ -82,7 +82,7 @@ EXPORT_SYMBOL(csum_partial_copy_from_user);
  * src and dst are best aligned to 64bits.
  */
 __wsum
-csum_partial_copy_to_user(const void *src, void __user *dst,
+csum_and_copy_to_user(const void *src, void __user *dst,
 			  int len, __wsum isum, int *errp)
 {
 	__wsum ret;
@@ -116,7 +116,7 @@ csum_partial_copy_to_user(const void *src, void __user *dst,
 	clac();
 	return ret;
 }
-EXPORT_SYMBOL(csum_partial_copy_to_user);
+EXPORT_SYMBOL(csum_and_copy_to_user);
 
 /**
  * csum_partial_copy_nocheck - Copy and checksum.

commit 7e300dabb7e74097137b4ed28a1f9887a7f2ac5b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:25 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 223
    
    Based on 1 normalized pattern(s):
    
      subject to the gnu public license v 2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 9 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171440.130801526@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index a6a2b7dccbff..c66c8b00f236 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -1,6 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright 2002, 2003 Andi Kleen, SuSE Labs.
- * Subject to the GNU Public License v.2
  *
  * Wrappers of assembly checksum functions for x86-64.
  */

commit 96d4f267e40f9509e8a66e2b39e8b95655617693
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 3 18:57:57 2019 -0800

    Remove 'type' argument from access_ok() function
    
    Nobody has actually used the type (VERIFY_READ vs VERIFY_WRITE) argument
    of the user address range verification function since we got rid of the
    old racy i386-only code to walk page tables by hand.
    
    It existed because the original 80386 would not honor the write protect
    bit when in kernel mode, so you had to do COW by hand before doing any
    user access.  But we haven't supported that in a long time, and these
    days the 'type' argument is a purely historical artifact.
    
    A discussion about extending 'user_access_begin()' to do the range
    checking resulted this patch, because there is no way we're going to
    move the old VERIFY_xyz interface to that model.  And it's best done at
    the end of the merge window when I've done most of my merges, so let's
    just get this done once and for all.
    
    This patch was mostly done with a sed-script, with manual fix-ups for
    the cases that weren't of the trivial 'access_ok(VERIFY_xyz' form.
    
    There were a couple of notable cases:
    
     - csky still had the old "verify_area()" name as an alias.
    
     - the iter_iov code had magical hardcoded knowledge of the actual
       values of VERIFY_{READ,WRITE} (not that they mattered, since nothing
       really used it)
    
     - microblaze used the type argument for a debug printout
    
    but other than those oddities this should be a total no-op patch.
    
    I tried to fix up all architectures, did fairly extensive grepping for
    access_ok() uses, and the changes are trivial, but I may have missed
    something.  Any missed conversion should be trivially fixable, though.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 8bd53589ecfb..a6a2b7dccbff 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -27,7 +27,7 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 	might_sleep();
 	*errp = 0;
 
-	if (!likely(access_ok(VERIFY_READ, src, len)))
+	if (!likely(access_ok(src, len)))
 		goto out_err;
 
 	/*
@@ -89,7 +89,7 @@ csum_partial_copy_to_user(const void *src, void __user *dst,
 
 	might_sleep();
 
-	if (unlikely(!access_ok(VERIFY_WRITE, dst, len))) {
+	if (unlikely(!access_ok(dst, len))) {
 		*errp = -EFAULT;
 		return 0;
 	}

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit 13d4ea097d18b419ad2a2b696063d44bf59acec0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 14 13:22:57 2016 -0700

    x86/uaccess: Move thread_info::addr_limit to thread_struct
    
    struct thread_info is a legacy mess.  To prepare for its partial removal,
    move thread_info::addr_limit out.
    
    As an added benefit, this way is simpler.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/15bee834d09402b47ac86f2feccdf6529f9bc5b0.1468527351.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 28a6654f0d08..b6fcb9a9ddbc 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -6,6 +6,7 @@
  */
 #include <asm/checksum.h>
 #include <linux/module.h>
+#include <linux/uaccess.h>
 #include <asm/smap.h>
 
 /**

commit e683014c2113d374b8716a7c938803c2f578efd5
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:57 2016 -0400

    x86/lib: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed a couple implicit header usage issues that were fixed.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-5-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 28a6654f0d08..f8dc05d0881e 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -5,7 +5,7 @@
  * Wrappers of assembly checksum functions for x86-64.
  */
 #include <asm/checksum.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <asm/smap.h>
 
 /**

commit 1e94082963747b551b129528714827f76a090e93
Author: Alexander Duyck <aduyck@mirantis.com>
Date:   Fri Mar 11 14:05:41 2016 -0800

    ipv6: Pass proto to csum_ipv6_magic as __u8 instead of unsigned short
    
    This patch updates csum_ipv6_magic so that it correctly recognizes that
    protocol is a unsigned 8 bit value.
    
    This will allow us to better understand what limitations may or may not be
    present in how we handle the data.  For example there are a number of
    places that call htonl on the protocol value.  This is likely not necessary
    and can be replaced with a multiplication by ntohl(1) which will be
    converted to a shift by the compiler.
    
    Signed-off-by: Alexander Duyck <aduyck@mirantis.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 1318f75d56e4..28a6654f0d08 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -135,7 +135,7 @@ EXPORT_SYMBOL(csum_partial_copy_nocheck);
 
 __sum16 csum_ipv6_magic(const struct in6_addr *saddr,
 			const struct in6_addr *daddr,
-			__u32 len, unsigned short proto, __wsum sum)
+			__u32 len, __u8 proto, __wsum sum)
 {
 	__u64 rest, sum64;
 

commit 3b91270a0a2fe00b7aa1ce16b3ecf055315c64e3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 16 11:00:42 2014 -0800

    x86-64: make csum_partial_copy_from_user() error handling consistent
    
    Al Viro pointed out that the x86-64 csum_partial_copy_from_user() is
    somewhat confused about what it should do on errors, notably it mostly
    clears the uncopied end result buffer, but misses that for the initial
    alignment case.
    
    All users should check for errors, so it's dubious whether the clearing
    is even necessary, and Al also points out that we should probably clean
    up the calling conventions, but regardless of any future changes to this
    function, the fact that it is inconsistent is just annoying.
    
    So make the __get_user() failure path use the same error exit as all the
    other errors do.
    
    Reported-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: David Miller <davem@davemloft.net>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 7609e0e421ec..1318f75d56e4 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -41,9 +41,8 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 		while (((unsigned long)src & 6) && len >= 2) {
 			__u16 val16;
 
-			*errp = __get_user(val16, (const __u16 __user *)src);
-			if (*errp)
-				return isum;
+			if (__get_user(val16, (const __u16 __user *)src))
+				goto out_err;
 
 			*(__u16 *)dst = val16;
 			isum = (__force __wsum)add32_with_carry(

commit 7263dda41b5a28ae6566fd126d9b06ada73dd721
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Aug 30 15:43:03 2013 -0700

    x86, smap: Handle csum_partial_copy_*_user()
    
    Add SMAP annotations to csum_partial_copy_to/from_user().  These
    functions legitimately access user space and thus need to set the AC
    flag.
    
    TODO: add explicit checks that the side with the kernel space pointer
    really points into kernel space.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/n/tip-2aps0u00eer658fd5xyanan7@git.kernel.org
    Cc: <stable@vger.kernel.org> # v3.7+

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 25b7ae8d058a..7609e0e421ec 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -6,6 +6,7 @@
  */
 #include <asm/checksum.h>
 #include <linux/module.h>
+#include <asm/smap.h>
 
 /**
  * csum_partial_copy_from_user - Copy and checksum from user space.
@@ -52,8 +53,10 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 			len -= 2;
 		}
 	}
+	stac();
 	isum = csum_partial_copy_generic((__force const void *)src,
 				dst, len, isum, errp, NULL);
+	clac();
 	if (unlikely(*errp))
 		goto out_err;
 
@@ -82,6 +85,8 @@ __wsum
 csum_partial_copy_to_user(const void *src, void __user *dst,
 			  int len, __wsum isum, int *errp)
 {
+	__wsum ret;
+
 	might_sleep();
 
 	if (unlikely(!access_ok(VERIFY_WRITE, dst, len))) {
@@ -105,8 +110,11 @@ csum_partial_copy_to_user(const void *src, void __user *dst,
 	}
 
 	*errp = 0;
-	return csum_partial_copy_generic(src, (void __force *)dst,
-					 len, isum, NULL, errp);
+	stac();
+	ret = csum_partial_copy_generic(src, (void __force *)dst,
+					len, isum, NULL, errp);
+	clac();
+	return ret;
 }
 EXPORT_SYMBOL(csum_partial_copy_to_user);
 

commit c15acff337ca5c2f101fee99f36c89d47839d387
Author: Wanpeng Li <liwp@linux.vnet.ibm.com>
Date:   Tue Jun 12 12:53:21 2012 +0800

    x86: Fix kernel-doc warnings
    
    Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jason Wessel <jason.wessel@windriver.com>
    Cc: Jan Kiszka <jan.kiszka@siemens.com>
    Cc: Gavin Shan <shangw@linux.vnet.ibm.com>
    Cc: Wanpeng Li <liwp.linux@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 459b58a8a15c..25b7ae8d058a 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -115,7 +115,7 @@ EXPORT_SYMBOL(csum_partial_copy_to_user);
  * @src: source address
  * @dst: destination address
  * @len: number of bytes to be copied.
- * @isum: initial sum that is added into the result (32bit unfolded)
+ * @sum: initial sum that is added into the result (32bit unfolded)
  *
  * Returns an 32bit unfolded checksum of the buffer.
  */

commit d76c1ae4d1f4f322d47e7c6e47a277384ba9d9cb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Feb 17 16:48:25 2008 +0100

    x86: clean up csum-wrappers_64.c some more
    
    no code changed:
    
    arch/x86/lib/csum-wrappers_64.o:
       text    data     bss     dec     hex filename
        839       0       0     839     347 csum-wrappers_64.o.before
        839       0       0     839     347 csum-wrappers_64.o.after
    md5:
    b31994226c33e0b52bef5a0e110b84b0  csum-wrappers_64.o.before.asm
    b31994226c33e0b52bef5a0e110b84b0  csum-wrappers_64.o.after.asm
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index 95e45dcc5a29..459b58a8a15c 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -1,9 +1,9 @@
-/* Copyright 2002,2003 Andi Kleen, SuSE Labs.
+/*
+ * Copyright 2002, 2003 Andi Kleen, SuSE Labs.
  * Subject to the GNU Public License v.2
  *
  * Wrappers of assembly checksum functions for x86-64.
  */
-
 #include <asm/checksum.h>
 #include <linux/module.h>
 
@@ -24,37 +24,47 @@ csum_partial_copy_from_user(const void __user *src, void *dst,
 {
 	might_sleep();
 	*errp = 0;
-	if (likely(access_ok(VERIFY_READ, src, len))) {
-		/* Why 6, not 7? To handle odd addresses aligned we
-		   would need to do considerable complications to fix the
-		   checksum which is defined as an 16bit accumulator. The
-		   fix alignment code is primarily for performance
-		   compatibility with 32bit and that will handle odd
-		   addresses slowly too. */
-		if (unlikely((unsigned long)src & 6)) {
-			while (((unsigned long)src & 6) && len >= 2) {
-				__u16 val16;
-				*errp = __get_user(val16, (const __u16 __user *)src);
-				if (*errp)
-					return isum;
-				*(__u16 *)dst = val16;
-				isum = (__force __wsum)add32_with_carry(
-						(__force unsigned)isum, val16);
-				src += 2;
-				dst += 2;
-				len -= 2;
-			}
+
+	if (!likely(access_ok(VERIFY_READ, src, len)))
+		goto out_err;
+
+	/*
+	 * Why 6, not 7? To handle odd addresses aligned we
+	 * would need to do considerable complications to fix the
+	 * checksum which is defined as an 16bit accumulator. The
+	 * fix alignment code is primarily for performance
+	 * compatibility with 32bit and that will handle odd
+	 * addresses slowly too.
+	 */
+	if (unlikely((unsigned long)src & 6)) {
+		while (((unsigned long)src & 6) && len >= 2) {
+			__u16 val16;
+
+			*errp = __get_user(val16, (const __u16 __user *)src);
+			if (*errp)
+				return isum;
+
+			*(__u16 *)dst = val16;
+			isum = (__force __wsum)add32_with_carry(
+					(__force unsigned)isum, val16);
+			src += 2;
+			dst += 2;
+			len -= 2;
 		}
-		isum = csum_partial_copy_generic((__force const void *)src,
-					dst, len, isum, errp, NULL);
-		if (likely(*errp == 0))
-			return isum;
 	}
+	isum = csum_partial_copy_generic((__force const void *)src,
+				dst, len, isum, errp, NULL);
+	if (unlikely(*errp))
+		goto out_err;
+
+	return isum;
+
+out_err:
 	*errp = -EFAULT;
 	memset(dst, 0, len);
+
 	return isum;
 }
-
 EXPORT_SYMBOL(csum_partial_copy_from_user);
 
 /**
@@ -73,6 +83,7 @@ csum_partial_copy_to_user(const void *src, void __user *dst,
 			  int len, __wsum isum, int *errp)
 {
 	might_sleep();
+
 	if (unlikely(!access_ok(VERIFY_WRITE, dst, len))) {
 		*errp = -EFAULT;
 		return 0;
@@ -81,6 +92,7 @@ csum_partial_copy_to_user(const void *src, void __user *dst,
 	if (unlikely((unsigned long)dst & 6)) {
 		while (((unsigned long)dst & 6) && len >= 2) {
 			__u16 val16 = *(__u16 *)src;
+
 			isum = (__force __wsum)add32_with_carry(
 					(__force unsigned)isum, val16);
 			*errp = __put_user(val16, (__u16 __user *)dst);
@@ -93,9 +105,9 @@ csum_partial_copy_to_user(const void *src, void __user *dst,
 	}
 
 	*errp = 0;
-	return csum_partial_copy_generic(src, (void __force *)dst, len, isum, NULL, errp);
+	return csum_partial_copy_generic(src, (void __force *)dst,
+					 len, isum, NULL, errp);
 }
-
 EXPORT_SYMBOL(csum_partial_copy_to_user);
 
 /**
@@ -122,14 +134,17 @@ __sum16 csum_ipv6_magic(const struct in6_addr *saddr,
 
 	rest = (__force __u64)htonl(len) + (__force __u64)htons(proto) +
 		(__force __u64)sum;
-	asm("  addq (%[saddr]),%[sum]\n"
-	    "  adcq 8(%[saddr]),%[sum]\n"
-	    "  adcq (%[daddr]),%[sum]\n"
-	    "  adcq 8(%[daddr]),%[sum]\n"
-	    "  adcq $0,%[sum]\n"
+
+	asm("	addq (%[saddr]),%[sum]\n"
+	    "	adcq 8(%[saddr]),%[sum]\n"
+	    "	adcq (%[daddr]),%[sum]\n"
+	    "	adcq 8(%[daddr]),%[sum]\n"
+	    "	adcq $0,%[sum]\n"
+
 	    : [sum] "=r" (sum64)
 	    : "[sum]" (rest), [saddr] "r" (saddr), [daddr] "r" (daddr));
-	return csum_fold((__force __wsum)add32_with_carry(sum64 & 0xffffffff, sum64>>32));
-}
 
+	return csum_fold(
+	       (__force __wsum)add32_with_carry(sum64 & 0xffffffff, sum64>>32));
+}
 EXPORT_SYMBOL(csum_ipv6_magic);

commit 0df025b709ae09081e21545761a249ec2d969689
Author: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
Date:   Sun Feb 17 14:56:50 2008 +0100

    x86: coding style fixes in arch/x86/lib/csum-wrappers_64.c
    
    no code changed:
    
    arch/x86/lib/csum-wrappers_64.o:
       text    data     bss     dec     hex filename
        839       0       0     839     347 csum-wrappers_64.o.before
        839       0       0     839     347 csum-wrappers_64.o.after
    md5:
    b31994226c33e0b52bef5a0e110b84b0  csum-wrappers_64.o.before.asm
    b31994226c33e0b52bef5a0e110b84b0  csum-wrappers_64.o.after.asm
    
    Signed-off-by: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
index fd42a4a095fc..95e45dcc5a29 100644
--- a/arch/x86/lib/csum-wrappers_64.c
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -1,117 +1,117 @@
 /* Copyright 2002,2003 Andi Kleen, SuSE Labs.
  * Subject to the GNU Public License v.2
- * 
+ *
  * Wrappers of assembly checksum functions for x86-64.
  */
 
 #include <asm/checksum.h>
 #include <linux/module.h>
 
-/** 
- * csum_partial_copy_from_user - Copy and checksum from user space. 
- * @src: source address (user space) 
+/**
+ * csum_partial_copy_from_user - Copy and checksum from user space.
+ * @src: source address (user space)
  * @dst: destination address
  * @len: number of bytes to be copied.
  * @isum: initial sum that is added into the result (32bit unfolded)
  * @errp: set to -EFAULT for an bad source address.
- * 
+ *
  * Returns an 32bit unfolded checksum of the buffer.
- * src and dst are best aligned to 64bits. 
- */ 
+ * src and dst are best aligned to 64bits.
+ */
 __wsum
 csum_partial_copy_from_user(const void __user *src, void *dst,
 			    int len, __wsum isum, int *errp)
-{ 
+{
 	might_sleep();
 	*errp = 0;
-	if (likely(access_ok(VERIFY_READ,src, len))) { 
+	if (likely(access_ok(VERIFY_READ, src, len))) {
 		/* Why 6, not 7? To handle odd addresses aligned we
 		   would need to do considerable complications to fix the
 		   checksum which is defined as an 16bit accumulator. The
 		   fix alignment code is primarily for performance
 		   compatibility with 32bit and that will handle odd
 		   addresses slowly too. */
-		if (unlikely((unsigned long)src & 6)) {			
-			while (((unsigned long)src & 6) && len >= 2) { 
-				__u16 val16;			
+		if (unlikely((unsigned long)src & 6)) {
+			while (((unsigned long)src & 6) && len >= 2) {
+				__u16 val16;
 				*errp = __get_user(val16, (const __u16 __user *)src);
 				if (*errp)
 					return isum;
 				*(__u16 *)dst = val16;
 				isum = (__force __wsum)add32_with_carry(
 						(__force unsigned)isum, val16);
-				src += 2; 
-				dst += 2; 
+				src += 2;
+				dst += 2;
 				len -= 2;
 			}
 		}
 		isum = csum_partial_copy_generic((__force const void *)src,
 					dst, len, isum, errp, NULL);
-		if (likely(*errp == 0)) 
+		if (likely(*errp == 0))
 			return isum;
-	} 
+	}
 	*errp = -EFAULT;
-	memset(dst,0,len); 
-	return isum;		
-} 
+	memset(dst, 0, len);
+	return isum;
+}
 
 EXPORT_SYMBOL(csum_partial_copy_from_user);
 
-/** 
- * csum_partial_copy_to_user - Copy and checksum to user space. 
+/**
+ * csum_partial_copy_to_user - Copy and checksum to user space.
  * @src: source address
  * @dst: destination address (user space)
  * @len: number of bytes to be copied.
  * @isum: initial sum that is added into the result (32bit unfolded)
  * @errp: set to -EFAULT for an bad destination address.
- * 
+ *
  * Returns an 32bit unfolded checksum of the buffer.
  * src and dst are best aligned to 64bits.
- */ 
+ */
 __wsum
 csum_partial_copy_to_user(const void *src, void __user *dst,
 			  int len, __wsum isum, int *errp)
-{ 
+{
 	might_sleep();
 	if (unlikely(!access_ok(VERIFY_WRITE, dst, len))) {
 		*errp = -EFAULT;
-		return 0; 
+		return 0;
 	}
 
 	if (unlikely((unsigned long)dst & 6)) {
-		while (((unsigned long)dst & 6) && len >= 2) { 
+		while (((unsigned long)dst & 6) && len >= 2) {
 			__u16 val16 = *(__u16 *)src;
 			isum = (__force __wsum)add32_with_carry(
 					(__force unsigned)isum, val16);
 			*errp = __put_user(val16, (__u16 __user *)dst);
 			if (*errp)
 				return isum;
-			src += 2; 
-			dst += 2; 
+			src += 2;
+			dst += 2;
 			len -= 2;
 		}
 	}
 
 	*errp = 0;
-	return csum_partial_copy_generic(src, (void __force *)dst,len,isum,NULL,errp); 
-} 
+	return csum_partial_copy_generic(src, (void __force *)dst, len, isum, NULL, errp);
+}
 
 EXPORT_SYMBOL(csum_partial_copy_to_user);
 
-/** 
+/**
  * csum_partial_copy_nocheck - Copy and checksum.
  * @src: source address
  * @dst: destination address
  * @len: number of bytes to be copied.
  * @isum: initial sum that is added into the result (32bit unfolded)
- * 
+ *
  * Returns an 32bit unfolded checksum of the buffer.
- */ 
+ */
 __wsum
 csum_partial_copy_nocheck(const void *src, void *dst, int len, __wsum sum)
-{ 
-	return csum_partial_copy_generic(src,dst,len,sum,NULL,NULL);
-} 
+{
+	return csum_partial_copy_generic(src, dst, len, sum, NULL, NULL);
+}
 EXPORT_SYMBOL(csum_partial_copy_nocheck);
 
 __sum16 csum_ipv6_magic(const struct in6_addr *saddr,
@@ -119,16 +119,16 @@ __sum16 csum_ipv6_magic(const struct in6_addr *saddr,
 			__u32 len, unsigned short proto, __wsum sum)
 {
 	__u64 rest, sum64;
-     
+
 	rest = (__force __u64)htonl(len) + (__force __u64)htons(proto) +
 		(__force __u64)sum;
 	asm("  addq (%[saddr]),%[sum]\n"
 	    "  adcq 8(%[saddr]),%[sum]\n"
-	    "  adcq (%[daddr]),%[sum]\n" 
+	    "  adcq (%[daddr]),%[sum]\n"
 	    "  adcq 8(%[daddr]),%[sum]\n"
 	    "  adcq $0,%[sum]\n"
-	    : [sum] "=r" (sum64) 
-	    : "[sum]" (rest),[saddr] "r" (saddr), [daddr] "r" (daddr));
+	    : [sum] "=r" (sum64)
+	    : "[sum]" (rest), [saddr] "r" (saddr), [daddr] "r" (daddr));
 	return csum_fold((__force __wsum)add32_with_carry(sum64 & 0xffffffff, sum64>>32));
 }
 

commit 185f3d38900f750a4566f87cde6a178f3595a115
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:17:08 2007 +0200

    x86_64: move lib
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c
new file mode 100644
index 000000000000..fd42a4a095fc
--- /dev/null
+++ b/arch/x86/lib/csum-wrappers_64.c
@@ -0,0 +1,135 @@
+/* Copyright 2002,2003 Andi Kleen, SuSE Labs.
+ * Subject to the GNU Public License v.2
+ * 
+ * Wrappers of assembly checksum functions for x86-64.
+ */
+
+#include <asm/checksum.h>
+#include <linux/module.h>
+
+/** 
+ * csum_partial_copy_from_user - Copy and checksum from user space. 
+ * @src: source address (user space) 
+ * @dst: destination address
+ * @len: number of bytes to be copied.
+ * @isum: initial sum that is added into the result (32bit unfolded)
+ * @errp: set to -EFAULT for an bad source address.
+ * 
+ * Returns an 32bit unfolded checksum of the buffer.
+ * src and dst are best aligned to 64bits. 
+ */ 
+__wsum
+csum_partial_copy_from_user(const void __user *src, void *dst,
+			    int len, __wsum isum, int *errp)
+{ 
+	might_sleep();
+	*errp = 0;
+	if (likely(access_ok(VERIFY_READ,src, len))) { 
+		/* Why 6, not 7? To handle odd addresses aligned we
+		   would need to do considerable complications to fix the
+		   checksum which is defined as an 16bit accumulator. The
+		   fix alignment code is primarily for performance
+		   compatibility with 32bit and that will handle odd
+		   addresses slowly too. */
+		if (unlikely((unsigned long)src & 6)) {			
+			while (((unsigned long)src & 6) && len >= 2) { 
+				__u16 val16;			
+				*errp = __get_user(val16, (const __u16 __user *)src);
+				if (*errp)
+					return isum;
+				*(__u16 *)dst = val16;
+				isum = (__force __wsum)add32_with_carry(
+						(__force unsigned)isum, val16);
+				src += 2; 
+				dst += 2; 
+				len -= 2;
+			}
+		}
+		isum = csum_partial_copy_generic((__force const void *)src,
+					dst, len, isum, errp, NULL);
+		if (likely(*errp == 0)) 
+			return isum;
+	} 
+	*errp = -EFAULT;
+	memset(dst,0,len); 
+	return isum;		
+} 
+
+EXPORT_SYMBOL(csum_partial_copy_from_user);
+
+/** 
+ * csum_partial_copy_to_user - Copy and checksum to user space. 
+ * @src: source address
+ * @dst: destination address (user space)
+ * @len: number of bytes to be copied.
+ * @isum: initial sum that is added into the result (32bit unfolded)
+ * @errp: set to -EFAULT for an bad destination address.
+ * 
+ * Returns an 32bit unfolded checksum of the buffer.
+ * src and dst are best aligned to 64bits.
+ */ 
+__wsum
+csum_partial_copy_to_user(const void *src, void __user *dst,
+			  int len, __wsum isum, int *errp)
+{ 
+	might_sleep();
+	if (unlikely(!access_ok(VERIFY_WRITE, dst, len))) {
+		*errp = -EFAULT;
+		return 0; 
+	}
+
+	if (unlikely((unsigned long)dst & 6)) {
+		while (((unsigned long)dst & 6) && len >= 2) { 
+			__u16 val16 = *(__u16 *)src;
+			isum = (__force __wsum)add32_with_carry(
+					(__force unsigned)isum, val16);
+			*errp = __put_user(val16, (__u16 __user *)dst);
+			if (*errp)
+				return isum;
+			src += 2; 
+			dst += 2; 
+			len -= 2;
+		}
+	}
+
+	*errp = 0;
+	return csum_partial_copy_generic(src, (void __force *)dst,len,isum,NULL,errp); 
+} 
+
+EXPORT_SYMBOL(csum_partial_copy_to_user);
+
+/** 
+ * csum_partial_copy_nocheck - Copy and checksum.
+ * @src: source address
+ * @dst: destination address
+ * @len: number of bytes to be copied.
+ * @isum: initial sum that is added into the result (32bit unfolded)
+ * 
+ * Returns an 32bit unfolded checksum of the buffer.
+ */ 
+__wsum
+csum_partial_copy_nocheck(const void *src, void *dst, int len, __wsum sum)
+{ 
+	return csum_partial_copy_generic(src,dst,len,sum,NULL,NULL);
+} 
+EXPORT_SYMBOL(csum_partial_copy_nocheck);
+
+__sum16 csum_ipv6_magic(const struct in6_addr *saddr,
+			const struct in6_addr *daddr,
+			__u32 len, unsigned short proto, __wsum sum)
+{
+	__u64 rest, sum64;
+     
+	rest = (__force __u64)htonl(len) + (__force __u64)htons(proto) +
+		(__force __u64)sum;
+	asm("  addq (%[saddr]),%[sum]\n"
+	    "  adcq 8(%[saddr]),%[sum]\n"
+	    "  adcq (%[daddr]),%[sum]\n" 
+	    "  adcq 8(%[daddr]),%[sum]\n"
+	    "  adcq $0,%[sum]\n"
+	    : [sum] "=r" (sum64) 
+	    : "[sum]" (rest),[saddr] "r" (saddr), [daddr] "r" (daddr));
+	return csum_fold((__force __wsum)add32_with_carry(sum64 & 0xffffffff, sum64>>32));
+}
+
+EXPORT_SYMBOL(csum_ipv6_magic);
