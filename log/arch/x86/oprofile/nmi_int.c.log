commit 85c615eb52222bc5fab6c7190d146bc59fac289e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Feb 20 21:58:21 2018 +0100

    x86/oprofile: Fix bogus GCC-8 warning in nmi_setup()
    
    GCC-8 shows a warning for the x86 oprofile code that copies per-CPU
    data from CPU 0 to all other CPUs, which when building a non-SMP
    kernel turns into a memcpy() with identical source and destination
    pointers:
    
     arch/x86/oprofile/nmi_int.c: In function 'mux_clone':
     arch/x86/oprofile/nmi_int.c:285:2: error: 'memcpy' source argument is the same as destination [-Werror=restrict]
       memcpy(per_cpu(cpu_msrs, cpu).multiplex,
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
              per_cpu(cpu_msrs, 0).multiplex,
              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
              sizeof(struct op_msr) * model->num_virt_counters);
              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     arch/x86/oprofile/nmi_int.c: In function 'nmi_setup':
     arch/x86/oprofile/nmi_int.c:466:3: error: 'memcpy' source argument is the same as destination [-Werror=restrict]
     arch/x86/oprofile/nmi_int.c:470:3: error: 'memcpy' source argument is the same as destination [-Werror=restrict]
    
    I have analyzed a number of such warnings now: some are valid and the
    GCC warning is welcome. Others turned out to be false-positives, and
    GCC was changed to not warn about those any more. This is a corner case
    that is a false-positive but the GCC developers feel it's better to keep
    warning about it.
    
    In this case, it seems best to work around it by telling GCC
    a little more clearly that this code path is never hit with
    an IS_ENABLED() configuration check.
    
    Cc:stable as we also want old kernels to build cleanly with GCC-8.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Jessica Yu <jeyu@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Sebor <msebor@gcc.gnu.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: oprofile-list@lists.sf.net
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180220205826.2008875-1-arnd@arndb.de
    Link: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84095
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 174c59774cc9..a7a7677265b6 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -460,7 +460,7 @@ static int nmi_setup(void)
 		goto fail;
 
 	for_each_possible_cpu(cpu) {
-		if (!cpu)
+		if (!IS_ENABLED(CONFIG_SMP) || !cpu)
 			continue;
 
 		memcpy(per_cpu(cpu_msrs, cpu).counters,

commit e4dca7b7aa08b22893c45485d222b5807c1375ae
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 17 19:04:42 2017 -0700

    treewide: Fix function prototypes for module_param_call()
    
    Several function prototypes for the set/get functions defined by
    module_param_call() have a slightly wrong argument types. This fixes
    those in an effort to clean up the calls when running under type-enforced
    compiler instrumentation for CFI. This is the result of running the
    following semantic patch:
    
    @match_module_param_call_function@
    declarer name module_param_call;
    identifier _name, _set_func, _get_func;
    expression _arg, _mode;
    @@
    
     module_param_call(_name, _set_func, _get_func, _arg, _mode);
    
    @fix_set_prototype
     depends on match_module_param_call_function@
    identifier match_module_param_call_function._set_func;
    identifier _val, _param;
    type _val_type, _param_type;
    @@
    
     int _set_func(
    -_val_type _val
    +const char * _val
     ,
    -_param_type _param
    +const struct kernel_param * _param
     ) { ... }
    
    @fix_get_prototype
     depends on match_module_param_call_function@
    identifier match_module_param_call_function._get_func;
    identifier _val, _param;
    type _val_type, _param_type;
    @@
    
     int _get_func(
    -_val_type _val
    +char * _val
     ,
    -_param_type _param
    +const struct kernel_param * _param
     ) { ... }
    
    Two additional by-hand changes are included for places where the above
    Coccinelle script didn't notice them:
    
            drivers/platform/x86/thinkpad_acpi.c
            fs/lockd/svc.c
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Jessica Yu <jeyu@kernel.org>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index ffdbc4836b4f..174c59774cc9 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -592,7 +592,7 @@ enum __force_cpu_type {
 
 static int force_cpu_type;
 
-static int set_cpu_type(const char *str, struct kernel_param *kp)
+static int set_cpu_type(const char *str, const struct kernel_param *kp)
 {
 	if (!strcmp(str, "timer")) {
 		force_cpu_type = timer;

commit 89666c50472263fba97b7edbfd2a642d1d9d6f74
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Nov 17 19:35:40 2016 +0100

    x86/oprofile/nmi: Convert to hotplug state machine
    
    Install the callbacks via the state machine and let the core invoke
    the callbacks on the already online CPUs.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Robert Richter <rric@kernel.org>
    Cc: oprofile-list@lists.sf.net
    Cc: rt@linuxtronix.de
    Link: http://lkml.kernel.org/r/20161117183541.8588-20-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index c39172cd6c87..ffdbc4836b4f 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -339,10 +339,11 @@ static int allocate_msrs(void)
 	return 0;
 }
 
-static void nmi_cpu_setup(void *dummy)
+static void nmi_cpu_setup(void)
 {
 	int cpu = smp_processor_id();
 	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
+
 	nmi_cpu_save_registers(msrs);
 	raw_spin_lock(&oprofilefs_lock);
 	model->setup_ctrs(model, msrs);
@@ -369,7 +370,7 @@ static void nmi_cpu_restore_registers(struct op_msrs *msrs)
 	}
 }
 
-static void nmi_cpu_shutdown(void *dummy)
+static void nmi_cpu_shutdown(void)
 {
 	unsigned int v;
 	int cpu = smp_processor_id();
@@ -387,24 +388,26 @@ static void nmi_cpu_shutdown(void *dummy)
 	nmi_cpu_restore_registers(msrs);
 }
 
-static void nmi_cpu_up(void)
+static int nmi_cpu_online(unsigned int cpu)
 {
 	local_irq_disable();
 	if (nmi_enabled)
-		nmi_cpu_setup(NULL);
+		nmi_cpu_setup();
 	if (ctr_running)
 		nmi_cpu_start(NULL);
 	local_irq_enable();
+	return 0;
 }
 
-static void nmi_cpu_down(void)
+static int nmi_cpu_down_prep(unsigned int cpu)
 {
 	local_irq_disable();
 	if (ctr_running)
 		nmi_cpu_stop(NULL);
 	if (nmi_enabled)
-		nmi_cpu_shutdown(NULL);
+		nmi_cpu_shutdown();
 	local_irq_enable();
+	return 0;
 }
 
 static int nmi_create_files(struct dentry *root)
@@ -437,24 +440,7 @@ static int nmi_create_files(struct dentry *root)
 	return 0;
 }
 
-static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
-				 void *data)
-{
-	switch (action & ~CPU_TASKS_FROZEN) {
-	case CPU_DOWN_FAILED:
-	case CPU_ONLINE:
-		nmi_cpu_up();
-		break;
-	case CPU_DOWN_PREPARE:
-		nmi_cpu_down();
-		break;
-	}
-	return NOTIFY_DONE;
-}
-
-static struct notifier_block oprofile_cpu_nb = {
-	.notifier_call = oprofile_cpu_notifier
-};
+static enum cpuhp_state cpuhp_nmi_online;
 
 static int nmi_setup(void)
 {
@@ -497,20 +483,17 @@ static int nmi_setup(void)
 	if (err)
 		goto fail;
 
-	cpu_notifier_register_begin();
-
-	/* Use get/put_online_cpus() to protect 'nmi_enabled' */
-	get_online_cpus();
 	nmi_enabled = 1;
 	/* make nmi_enabled visible to the nmi handler: */
 	smp_mb();
-	on_each_cpu(nmi_cpu_setup, NULL, 1);
-	__register_cpu_notifier(&oprofile_cpu_nb);
-	put_online_cpus();
-
-	cpu_notifier_register_done();
-
+	err = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, "x86/oprofile:online",
+				nmi_cpu_online, nmi_cpu_down_prep);
+	if (err < 0)
+		goto fail_nmi;
+	cpuhp_nmi_online = err;
 	return 0;
+fail_nmi:
+	unregister_nmi_handler(NMI_LOCAL, "oprofile");
 fail:
 	free_msrs();
 	return err;
@@ -520,17 +503,9 @@ static void nmi_shutdown(void)
 {
 	struct op_msrs *msrs;
 
-	cpu_notifier_register_begin();
-
-	/* Use get/put_online_cpus() to protect 'nmi_enabled' & 'ctr_running' */
-	get_online_cpus();
-	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
+	cpuhp_remove_state(cpuhp_nmi_online);
 	nmi_enabled = 0;
 	ctr_running = 0;
-	__unregister_cpu_notifier(&oprofile_cpu_nb);
-	put_online_cpus();
-
-	cpu_notifier_register_done();
 
 	/* make variables visible to the nmi handler: */
 	smp_mb();

commit 08ed487c819d285c3f6ceafd156094102acdfec2
Author: Anna-Maria Gleixner <anna-maria@linutronix.de>
Date:   Thu Nov 17 19:35:39 2016 +0100

    x86/oprofile/nmi: Remove superfluous smp_function_call_single()
    
    Since commit 1cf4f629d9d2 ("cpu/hotplug: Move online calls to
    hotplugged cpu") the CPU_ONLINE and CPU_DOWN_PREPARE notifiers are
    always run on the hot plugged CPU, and as of commit 3b9d6da67e11
    ("cpu/hotplug: Fix rollback during error-out in __cpu_disable()")
    the CPU_DOWN_FAILED notifier also runs on the hot plugged CPU.
    This patch converts the SMP functional calls into direct calls.
    
    smp_call_function_single() executes the function with interrupts
    disabled. This calling convention is preserved.
    
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Robert Richter <rric@kernel.org>
    Cc: rt@linuxtronix.de
    Cc: oprofile-list@lists.sf.net
    Link: http://lkml.kernel.org/r/20161117183541.8588-19-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 28c04123b6dd..c39172cd6c87 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -387,20 +387,24 @@ static void nmi_cpu_shutdown(void *dummy)
 	nmi_cpu_restore_registers(msrs);
 }
 
-static void nmi_cpu_up(void *dummy)
+static void nmi_cpu_up(void)
 {
+	local_irq_disable();
 	if (nmi_enabled)
-		nmi_cpu_setup(dummy);
+		nmi_cpu_setup(NULL);
 	if (ctr_running)
-		nmi_cpu_start(dummy);
+		nmi_cpu_start(NULL);
+	local_irq_enable();
 }
 
-static void nmi_cpu_down(void *dummy)
+static void nmi_cpu_down(void)
 {
+	local_irq_disable();
 	if (ctr_running)
-		nmi_cpu_stop(dummy);
+		nmi_cpu_stop(NULL);
 	if (nmi_enabled)
-		nmi_cpu_shutdown(dummy);
+		nmi_cpu_shutdown(NULL);
+	local_irq_enable();
 }
 
 static int nmi_create_files(struct dentry *root)
@@ -436,15 +440,13 @@ static int nmi_create_files(struct dentry *root)
 static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
 				 void *data)
 {
-	int cpu = (unsigned long)data;
-
 	switch (action & ~CPU_TASKS_FROZEN) {
 	case CPU_DOWN_FAILED:
 	case CPU_ONLINE:
-		smp_call_function_single(cpu, nmi_cpu_up, NULL, 0);
+		nmi_cpu_up();
 		break;
 	case CPU_DOWN_PREPARE:
-		smp_call_function_single(cpu, nmi_cpu_down, NULL, 1);
+		nmi_cpu_down();
 		break;
 	}
 	return NOTIFY_DONE;

commit 93984fbd4e33cc861d5b49caed02a02cbfb01340
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 4 22:25:00 2016 +0200

    x86/cpufeature: Replace cpu_has_apic with boot_cpu_has() usage
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: linux-pm@vger.kernel.org
    Cc: oprofile-list@lists.sf.net
    Link: http://lkml.kernel.org/r/1459801503-15600-8-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 25171e9595f7..28c04123b6dd 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -700,7 +700,7 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	char *cpu_type = NULL;
 	int ret = 0;
 
-	if (!cpu_has_apic)
+	if (!boot_cpu_has(X86_FEATURE_APIC))
 		return -ENODEV;
 
 	if (force_cpu_type == timer)

commit 7b5e74e637e4a977c7cf40fd7de332f60b68180e
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:41:54 2016 +0200

    x86/cpufeature: Remove cpu_has_arch_perfmon
    
    Use boot_cpu_has() instead.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: oprofile-list@lists.sf.net
    Link: http://lkml.kernel.org/r/1459266123-21878-2-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 0e07e0968c3a..25171e9595f7 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -636,7 +636,7 @@ static int __init ppro_init(char **cpu_type)
 	__u8 cpu_model = boot_cpu_data.x86_model;
 	struct op_x86_model_spec *spec = &op_ppro_spec;	/* default */
 
-	if (force_cpu_type == arch_perfmon && cpu_has_arch_perfmon)
+	if (force_cpu_type == arch_perfmon && boot_cpu_has(X86_FEATURE_ARCH_PERFMON))
 		return 0;
 
 	/*
@@ -761,7 +761,7 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		if (cpu_type)
 			break;
 
-		if (!cpu_has_arch_perfmon)
+		if (!boot_cpu_has(X86_FEATURE_ARCH_PERFMON))
 			return -ENODEV;
 
 		/* use arch perfmon as fallback */

commit 57d335ce88d055eb212e2531dd7a8b4240404a57
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 19 12:28:38 2016 +0100

    x86/oprofile/nmi: Add missing hotplug FROZEN handling
    
    We really do not want to keep that nmi enabled across suspend/resume.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 1d2e6392f5fa..0e07e0968c3a 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -437,7 +437,8 @@ static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
 				 void *data)
 {
 	int cpu = (unsigned long)data;
-	switch (action) {
+
+	switch (action & ~CPU_TASKS_FROZEN) {
 	case CPU_DOWN_FAILED:
 	case CPU_ONLINE:
 		smp_call_function_single(cpu, nmi_cpu_up, NULL, 0);

commit 89cbc76768c2fa4ed95545bf961f3a14ddfeed21
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:40 2014 -0500

    x86: Replace __get_cpu_var uses
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    #define __get_cpu_var(var) (*this_cpu_ptr(&(var)))
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 379e8bd0deea..1d2e6392f5fa 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -64,11 +64,11 @@ u64 op_x86_get_ctrl(struct op_x86_model_spec const *model,
 static int profile_exceptions_notify(unsigned int val, struct pt_regs *regs)
 {
 	if (ctr_running)
-		model->check_ctrs(regs, &__get_cpu_var(cpu_msrs));
+		model->check_ctrs(regs, this_cpu_ptr(&cpu_msrs));
 	else if (!nmi_enabled)
 		return NMI_DONE;
 	else
-		model->stop(&__get_cpu_var(cpu_msrs));
+		model->stop(this_cpu_ptr(&cpu_msrs));
 	return NMI_HANDLED;
 }
 
@@ -91,7 +91,7 @@ static void nmi_cpu_save_registers(struct op_msrs *msrs)
 
 static void nmi_cpu_start(void *dummy)
 {
-	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
+	struct op_msrs const *msrs = this_cpu_ptr(&cpu_msrs);
 	if (!msrs->controls)
 		WARN_ON_ONCE(1);
 	else
@@ -111,7 +111,7 @@ static int nmi_start(void)
 
 static void nmi_cpu_stop(void *dummy)
 {
-	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
+	struct op_msrs const *msrs = this_cpu_ptr(&cpu_msrs);
 	if (!msrs->controls)
 		WARN_ON_ONCE(1);
 	else

commit 76902e3d9ce88ebd70e362e4d3ff173afa5ce575
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:08:49 2014 +0530

    x86, oprofile, nmi: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the oprofile code in x86 by using this latter form of callback
    registration. But retain the calls to get/put_online_cpus(), since they are
    used in other places as well, to protect the variables 'nmi_enabled' and
    'ctr_running'. Strictly speaking, this is not necessary since
    cpu_notifier_register_begin/done() provide a stronger synchronization
    with CPU hotplug than get/put_online_cpus(). However, let's retain the
    calls to get/put_online_cpus() to be consistent with the other call-sites.
    
    By nesting get/put_online_cpus() *inside* cpu_notifier_register_begin/done(),
    we avoid the ABBA deadlock possibility mentioned above.
    
    Cc: Robert Richter <rric@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 6890d8498e0b..379e8bd0deea 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -494,14 +494,19 @@ static int nmi_setup(void)
 	if (err)
 		goto fail;
 
+	cpu_notifier_register_begin();
+
+	/* Use get/put_online_cpus() to protect 'nmi_enabled' */
 	get_online_cpus();
-	register_cpu_notifier(&oprofile_cpu_nb);
 	nmi_enabled = 1;
 	/* make nmi_enabled visible to the nmi handler: */
 	smp_mb();
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
+	__register_cpu_notifier(&oprofile_cpu_nb);
 	put_online_cpus();
 
+	cpu_notifier_register_done();
+
 	return 0;
 fail:
 	free_msrs();
@@ -512,12 +517,18 @@ static void nmi_shutdown(void)
 {
 	struct op_msrs *msrs;
 
+	cpu_notifier_register_begin();
+
+	/* Use get/put_online_cpus() to protect 'nmi_enabled' & 'ctr_running' */
 	get_online_cpus();
-	unregister_cpu_notifier(&oprofile_cpu_nb);
 	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
 	nmi_enabled = 0;
 	ctr_running = 0;
+	__unregister_cpu_notifier(&oprofile_cpu_nb);
 	put_online_cpus();
+
+	cpu_notifier_register_done();
+
 	/* make variables visible to the nmi handler: */
 	smp_mb();
 	unregister_nmi_handler(NMI_LOCAL, "oprofile");

commit 6af4ea0ba708172be8caf1ba5047b2b8a9d2fea3
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Jul 19 16:10:36 2013 +0400

    oprofilefs_create_...() do not need superblock argument
    
    same story as with oprofilefs_mkdir()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 8bb2de6e103c..6890d8498e0b 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -421,13 +421,13 @@ static int nmi_create_files(struct dentry *root)
 
 		snprintf(buf,  sizeof(buf), "%d", i);
 		dir = oprofilefs_mkdir(root, buf);
-		oprofilefs_create_ulong(root->d_sb, dir, "enabled", &counter_config[i].enabled);
-		oprofilefs_create_ulong(root->d_sb, dir, "event", &counter_config[i].event);
-		oprofilefs_create_ulong(root->d_sb, dir, "count", &counter_config[i].count);
-		oprofilefs_create_ulong(root->d_sb, dir, "unit_mask", &counter_config[i].unit_mask);
-		oprofilefs_create_ulong(root->d_sb, dir, "kernel", &counter_config[i].kernel);
-		oprofilefs_create_ulong(root->d_sb, dir, "user", &counter_config[i].user);
-		oprofilefs_create_ulong(root->d_sb, dir, "extra", &counter_config[i].extra);
+		oprofilefs_create_ulong(dir, "enabled", &counter_config[i].enabled);
+		oprofilefs_create_ulong(dir, "event", &counter_config[i].event);
+		oprofilefs_create_ulong(dir, "count", &counter_config[i].count);
+		oprofilefs_create_ulong(dir, "unit_mask", &counter_config[i].unit_mask);
+		oprofilefs_create_ulong(dir, "kernel", &counter_config[i].kernel);
+		oprofilefs_create_ulong(dir, "user", &counter_config[i].user);
+		oprofilefs_create_ulong(dir, "extra", &counter_config[i].extra);
 	}
 
 	return 0;

commit ecde28237e10de3750a97579f42bc2ec65b8a0e1
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Jul 19 15:58:27 2013 +0400

    oprofilefs_mkdir() doesn't need superblock argument
    
    it's always equal to ->d_sb of the second argument (parent dentry),
    due to either being literally that, or ->d_sb of parent's parent.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 483f02b8c1a8..8bb2de6e103c 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -420,7 +420,7 @@ static int nmi_create_files(struct dentry *root)
 			continue;
 
 		snprintf(buf,  sizeof(buf), "%d", i);
-		dir = oprofilefs_mkdir(root->d_sb, root, buf);
+		dir = oprofilefs_mkdir(root, buf);
 		oprofilefs_create_ulong(root->d_sb, dir, "enabled", &counter_config[i].enabled);
 		oprofilefs_create_ulong(root->d_sb, dir, "event", &counter_config[i].event);
 		oprofilefs_create_ulong(root->d_sb, dir, "count", &counter_config[i].count);

commit ef7bca1456e7f65e66b9466c3b149601fe32eec0
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Jul 19 15:52:42 2013 +0400

    oprofile: don't bother with passing superblock to ->create_files()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 48768df2471a..483f02b8c1a8 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -403,7 +403,7 @@ static void nmi_cpu_down(void *dummy)
 		nmi_cpu_shutdown(dummy);
 }
 
-static int nmi_create_files(struct super_block *sb, struct dentry *root)
+static int nmi_create_files(struct dentry *root)
 {
 	unsigned int i;
 
@@ -420,14 +420,14 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 			continue;
 
 		snprintf(buf,  sizeof(buf), "%d", i);
-		dir = oprofilefs_mkdir(sb, root, buf);
-		oprofilefs_create_ulong(sb, dir, "enabled", &counter_config[i].enabled);
-		oprofilefs_create_ulong(sb, dir, "event", &counter_config[i].event);
-		oprofilefs_create_ulong(sb, dir, "count", &counter_config[i].count);
-		oprofilefs_create_ulong(sb, dir, "unit_mask", &counter_config[i].unit_mask);
-		oprofilefs_create_ulong(sb, dir, "kernel", &counter_config[i].kernel);
-		oprofilefs_create_ulong(sb, dir, "user", &counter_config[i].user);
-		oprofilefs_create_ulong(sb, dir, "extra", &counter_config[i].extra);
+		dir = oprofilefs_mkdir(root->d_sb, root, buf);
+		oprofilefs_create_ulong(root->d_sb, dir, "enabled", &counter_config[i].enabled);
+		oprofilefs_create_ulong(root->d_sb, dir, "event", &counter_config[i].event);
+		oprofilefs_create_ulong(root->d_sb, dir, "count", &counter_config[i].count);
+		oprofilefs_create_ulong(root->d_sb, dir, "unit_mask", &counter_config[i].unit_mask);
+		oprofilefs_create_ulong(root->d_sb, dir, "kernel", &counter_config[i].kernel);
+		oprofilefs_create_ulong(root->d_sb, dir, "user", &counter_config[i].user);
+		oprofilefs_create_ulong(root->d_sb, dir, "extra", &counter_config[i].extra);
 	}
 
 	return 0;

commit 44009105081b51417f311f4c3be0061870b6b8ed
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Wed Oct 10 10:18:35 2012 +0300

    oprofile, x86: Fix wrapping bug in op_x86_get_ctrl()
    
    The "event" variable is a u16 so the shift will always wrap to zero
    making the line a no-op.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: <stable@vger.kernel.org> v2.6.32..
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 26b8a8514ee5..48768df2471a 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -55,7 +55,7 @@ u64 op_x86_get_ctrl(struct op_x86_model_spec const *model,
 	val |= counter_config->extra;
 	event &= model->event_mask ? model->event_mask : 0xFF;
 	val |= event & 0xFF;
-	val |= (event & 0x0F00) << 24;
+	val |= (u64)(event & 0x0F00) << 24;
 
 	return val;
 }

commit c23205c8488f11cb9ebe7a7b5851a1d8a0171011
Merge: 5d81e5cfb37a de346b694906
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Nov 15 11:05:18 2011 +0100

    Merge branch 'core' of git://amd64.org/linux/rric into perf/core

commit de346b6949063aa040ef607943b072835294f4b3
Merge: dcfce4a09593 9c48f1c629ec
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Nov 8 15:52:15 2011 +0100

    Merge branch 'perf/core' into oprofile/master
    
    Merge reason: Resolve conflicts with Don's NMI rework:
    
        commit 9c48f1c629ecfa114850c03f875c6691003214de
        Author: Don Zickus <dzickus@redhat.com>
        Date:   Fri Sep 30 15:06:21 2011 -0400
        x86, nmi: Wire up NMI handlers to new routines
    
    Conflicts:
            arch/x86/oprofile/nmi_timer_int.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

commit 159a80b2142df709416ab369113de7d511c48331
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Oct 11 19:39:16 2011 +0200

    oprofile, x86: Add kernel parameter oprofile.cpu_type=timer
    
    We need this to better test x86 NMI timer mode. Otherwise it is very
    hard to setup systems with NMI timer enabled, but we have this e.g. in
    virtual machine environments.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 68894fdc034b..7ca4d43e8988 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -613,24 +613,36 @@ static int __init p4_init(char **cpu_type)
 	return 0;
 }
 
-static int force_arch_perfmon;
-static int force_cpu_type(const char *str, struct kernel_param *kp)
+enum __force_cpu_type {
+	reserved = 0,		/* do not force */
+	timer,
+	arch_perfmon,
+};
+
+static int force_cpu_type;
+
+static int set_cpu_type(const char *str, struct kernel_param *kp)
 {
-	if (!strcmp(str, "arch_perfmon")) {
-		force_arch_perfmon = 1;
+	if (!strcmp(str, "timer")) {
+		force_cpu_type = timer;
+		printk(KERN_INFO "oprofile: forcing NMI timer mode\n");
+	} else if (!strcmp(str, "arch_perfmon")) {
+		force_cpu_type = arch_perfmon;
 		printk(KERN_INFO "oprofile: forcing architectural perfmon\n");
+	} else {
+		force_cpu_type = 0;
 	}
 
 	return 0;
 }
-module_param_call(cpu_type, force_cpu_type, NULL, NULL, 0);
+module_param_call(cpu_type, set_cpu_type, NULL, NULL, 0);
 
 static int __init ppro_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
 	struct op_x86_model_spec *spec = &op_ppro_spec;	/* default */
 
-	if (force_arch_perfmon && cpu_has_arch_perfmon)
+	if (force_cpu_type == arch_perfmon && cpu_has_arch_perfmon)
 		return 0;
 
 	/*
@@ -697,6 +709,9 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	if (!cpu_has_apic)
 		return -ENODEV;
 
+	if (force_cpu_type == timer)
+		return -ENODEV;
+
 	switch (vendor) {
 	case X86_VENDOR_AMD:
 		/* Needs to be at least an Athlon (or hammer in 32bit mode) */

commit 7115e3fcf45514db7525a05365b10454ff7f345e
Merge: 1f6e05171bb5 c752d04066a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 26 17:03:38 2011 +0200

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (121 commits)
      perf symbols: Increase symbol KSYM_NAME_LEN size
      perf hists browser: Refuse 'a' hotkey on non symbolic views
      perf ui browser: Use libslang to read keys
      perf tools: Fix tracing info recording
      perf hists browser: Elide DSO column when it is set to just one DSO, ditto for threads
      perf hists: Don't consider filtered entries when calculating column widths
      perf hists: Don't decay total_period for filtered entries
      perf hists browser: Honour symbol_conf.show_{nr_samples,total_period}
      perf hists browser: Do not exit on tab key with single event
      perf annotate browser: Don't change selection line when returning from callq
      perf tools: handle endianness of feature bitmap
      perf tools: Add prelink suggestion to dso update message
      perf script: Fix unknown feature comment
      perf hists browser: Apply the dso and thread filters when merging new batches
      perf hists: Move the dso and thread filters from hist_browser
      perf ui browser: Honour the xterm colors
      perf top tui: Give color hints just on the percentage, like on --stdio
      perf ui browser: Make the colors configurable and change the defaults
      perf tui: Remove unneeded call to newtCls on startup
      perf hists: Don't format the percentage on hist_entry__snprintf
      ...
    
    Fix up conflicts in arch/x86/kernel/kprobes.c manually.
    
    Ingo's tree did the insane "add volatile to const array", which just
    doesn't make sense ("volatile const"?).  But we could remove the const
    *and* make the array volatile to make doubly sure that gcc doesn't
    optimize it away..
    
    Also fix up kernel/trace/ring_buffer.c non-data-conflicts manually: the
    reader_lock has been turned into a raw lock by the core locking merge,
    and there was a new user of it introduced in this perf core merge.  Make
    sure that new use also uses the raw accessor functions.

commit b716916679e72054d436afadce2f94dcad71cfad
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Sep 21 11:30:18 2011 +0200

    perf, x86: Implement IBS initialization
    
    This patch implements IBS feature detection and initialzation. The
    code is shared between perf and oprofile. If IBS is available on the
    system for perf, a pmu is setup.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1316597423-25723-3-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index adf8fb31aa7b..c04dc145a4b7 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -385,8 +385,6 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
 	nmi_cpu_restore_registers(msrs);
-	if (model->cpu_down)
-		model->cpu_down();
 }
 
 static void nmi_cpu_up(void *dummy)

commit 9c48f1c629ecfa114850c03f875c6691003214de
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Sep 30 15:06:21 2011 -0400

    x86, nmi: Wire up NMI handlers to new routines
    
    Just convert all the files that have an nmi handler to the new routines.
    Most of it is straight forward conversion.  A couple of places needed some
    tweaking like kgdb which separates the debug notifier from the nmi handler
    and mce removes a call to notify_die.
    
    [Thanks to Ying for finding out the history behind that mce call
    
    https://lkml.org/lkml/2010/5/27/114
    
    And Boris responding that he would like to remove that call because of it
    
    https://lkml.org/lkml/2011/9/21/163]
    
    The things that get converted are the registeration/unregistration routines
    and the nmi handler itself has its args changed along with code removal
    to check which list it is on (most are on one NMI list except for kgdb
    which has both an NMI routine and an NMI Unknown routine).
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Corey Minyard <minyard@acm.org>
    Cc: Jason Wessel <jason.wessel@windriver.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Corey Minyard <minyard@acm.org>
    Cc: Jack Steiner <steiner@sgi.com>
    Link: http://lkml.kernel.org/r/1317409584-23662-4-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 68894fdc034b..adf8fb31aa7b 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -61,26 +61,15 @@ u64 op_x86_get_ctrl(struct op_x86_model_spec const *model,
 }
 
 
-static int profile_exceptions_notify(struct notifier_block *self,
-				     unsigned long val, void *data)
+static int profile_exceptions_notify(unsigned int val, struct pt_regs *regs)
 {
-	struct die_args *args = (struct die_args *)data;
-	int ret = NOTIFY_DONE;
-
-	switch (val) {
-	case DIE_NMI:
-		if (ctr_running)
-			model->check_ctrs(args->regs, &__get_cpu_var(cpu_msrs));
-		else if (!nmi_enabled)
-			break;
-		else
-			model->stop(&__get_cpu_var(cpu_msrs));
-		ret = NOTIFY_STOP;
-		break;
-	default:
-		break;
-	}
-	return ret;
+	if (ctr_running)
+		model->check_ctrs(regs, &__get_cpu_var(cpu_msrs));
+	else if (!nmi_enabled)
+		return NMI_DONE;
+	else
+		model->stop(&__get_cpu_var(cpu_msrs));
+	return NMI_HANDLED;
 }
 
 static void nmi_cpu_save_registers(struct op_msrs *msrs)
@@ -363,12 +352,6 @@ static void nmi_cpu_setup(void *dummy)
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 }
 
-static struct notifier_block profile_exceptions_nb = {
-	.notifier_call = profile_exceptions_notify,
-	.next = NULL,
-	.priority = NMI_LOCAL_LOW_PRIOR,
-};
-
 static void nmi_cpu_restore_registers(struct op_msrs *msrs)
 {
 	struct op_msr *counters = msrs->counters;
@@ -508,7 +491,8 @@ static int nmi_setup(void)
 	ctr_running = 0;
 	/* make variables visible to the nmi handler: */
 	smp_mb();
-	err = register_die_notifier(&profile_exceptions_nb);
+	err = register_nmi_handler(NMI_LOCAL, profile_exceptions_notify,
+					0, "oprofile");
 	if (err)
 		goto fail;
 
@@ -538,7 +522,7 @@ static void nmi_shutdown(void)
 	put_online_cpus();
 	/* make variables visible to the nmi handler: */
 	smp_mb();
-	unregister_die_notifier(&profile_exceptions_nb);
+	unregister_nmi_handler(NMI_LOCAL, "oprofile");
 	msrs = &get_cpu_var(cpu_msrs);
 	model->shutdown(msrs);
 	free_msrs();

commit 2d21a29fb62f142b8a62496700d8d82a6a8fd783
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 25 16:18:34 2009 +0200

    locking, oprofile: Annotate oprofilefs lock as raw
    
    The oprofilefs_lock can be taken in atomic context (in profiling
    interrupts) and therefore cannot cannot be preempted on -rt -
    annotate it.
    
    In mainline this change documents the low level nature of
    the lock - otherwise there's no functional difference. Lockdep
    and Sparse checking will work as usual.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 68894fdc034b..96646b3aeca8 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -355,10 +355,10 @@ static void nmi_cpu_setup(void *dummy)
 	int cpu = smp_processor_id();
 	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
 	nmi_cpu_save_registers(msrs);
-	spin_lock(&oprofilefs_lock);
+	raw_spin_lock(&oprofilefs_lock);
 	model->setup_ctrs(model, msrs);
 	nmi_cpu_setup_mux(cpu, msrs);
-	spin_unlock(&oprofilefs_lock);
+	raw_spin_unlock(&oprofilefs_lock);
 	per_cpu(saved_lvtpc, cpu) = apic_read(APIC_LVTPC);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 }

commit 8fe7e94eb71430cf63a742f3c19739d82a662758
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jun 1 15:31:44 2011 +0200

    oprofile, x86: Fix race in nmi handler while starting counters
    
    In some rare cases, nmis are generated immediately after the nmi
    handler of the cpu was started. This causes the counter not to be
    enabled. Before enabling the nmi handlers we need to set variable
    ctr_running first and make sure its value is written to memory.
    
    Also, the patch makes all existing barriers a memory barrier instead
    of a compiler barrier only.
    
    Reported-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: <stable@kernel.org> # .35+
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index cf9750004a08..68894fdc034b 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -112,8 +112,10 @@ static void nmi_cpu_start(void *dummy)
 static int nmi_start(void)
 {
 	get_online_cpus();
-	on_each_cpu(nmi_cpu_start, NULL, 1);
 	ctr_running = 1;
+	/* make ctr_running visible to the nmi handler: */
+	smp_mb();
+	on_each_cpu(nmi_cpu_start, NULL, 1);
 	put_online_cpus();
 	return 0;
 }
@@ -504,15 +506,18 @@ static int nmi_setup(void)
 
 	nmi_enabled = 0;
 	ctr_running = 0;
-	barrier();
+	/* make variables visible to the nmi handler: */
+	smp_mb();
 	err = register_die_notifier(&profile_exceptions_nb);
 	if (err)
 		goto fail;
 
 	get_online_cpus();
 	register_cpu_notifier(&oprofile_cpu_nb);
-	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;
+	/* make nmi_enabled visible to the nmi handler: */
+	smp_mb();
+	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	put_online_cpus();
 
 	return 0;
@@ -531,7 +536,8 @@ static void nmi_shutdown(void)
 	nmi_enabled = 0;
 	ctr_running = 0;
 	put_online_cpus();
-	barrier();
+	/* make variables visible to the nmi handler: */
+	smp_mb();
 	unregister_die_notifier(&profile_exceptions_nb);
 	msrs = &get_cpu_var(cpu_msrs);
 	model->shutdown(msrs);

commit 1dfd7b494b3d8fb1e8a7383a8095f77eb058cd83
Merge: 6c6804fb2cef 914a76ca5eed
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 29 09:32:28 2011 +0200

    Merge branch 'core' of git://git.kernel.org/pub/scm/linux/kernel/git/rric/oprofile into perf/urgent

commit 914a76ca5eedc9f286a36f61c4eaa95b451ba3e6
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Mar 16 15:44:33 2011 -0400

    oprofile, x86: Allow setting EDGE/INV/CMASK for counter events
    
    For some performance events it's useful to set the EDGE and INV
    bits and the CMASK mask in the counter control register. The list
    of predefined events Intel releases for each CPU has some events which
    require these settings to get more "natural" to use higher level events.
    
    oprofile currently doesn't allow this.
    
    This patch adds new extra configuration fields for them, so that
    they can be specified in oprofilefs.
    
    An updated oprofile daemon can then make use of this to set them.
    
    v2: Write back masked extra value to variable.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index e2b7b0c06cdf..ee165f1a1f37 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -49,6 +49,10 @@ u64 op_x86_get_ctrl(struct op_x86_model_spec const *model,
 	val |= counter_config->user ? ARCH_PERFMON_EVENTSEL_USR : 0;
 	val |= counter_config->kernel ? ARCH_PERFMON_EVENTSEL_OS : 0;
 	val |= (counter_config->unit_mask & 0xFF) << 8;
+	counter_config->extra &= (ARCH_PERFMON_EVENTSEL_INV |
+				  ARCH_PERFMON_EVENTSEL_EDGE |
+				  ARCH_PERFMON_EVENTSEL_CMASK);
+	val |= counter_config->extra;
 	event &= model->event_mask ? model->event_mask : 0xFF;
 	val |= event & 0xFF;
 	val |= (event & 0x0F00) << 24;
@@ -440,6 +444,7 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 		oprofilefs_create_ulong(sb, dir, "unit_mask", &counter_config[i].unit_mask);
 		oprofilefs_create_ulong(sb, dir, "kernel", &counter_config[i].kernel);
 		oprofilefs_create_ulong(sb, dir, "user", &counter_config[i].user);
+		oprofilefs_create_ulong(sb, dir, "extra", &counter_config[i].extra);
 	}
 
 	return 0;

commit f3c6ea1b06c71b43f751b36bd99345369fe911af
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Mar 23 22:15:54 2011 +0100

    x86: Use syscore_ops instead of sysdev classes and sysdevs
    
    Some subsystems in the x86 tree need to carry out suspend/resume and
    shutdown operations with one CPU on-line and interrupts disabled and
    they define sysdev classes and sysdevs or sysdev drivers for this
    purpose.  This leads to unnecessarily complicated code and excessive
    memory usage, so switch them to using struct syscore_ops objects for
    this purpose instead.
    
    Generally, there are three categories of subsystems that use
    sysdevs for implementing PM operations: (1) subsystems whose
    suspend/resume callbacks ignore their arguments entirely (the
    majority), (2) subsystems whose suspend/resume callbacks use their
    struct sys_device argument, but don't really need to do that,
    because they can be implemented differently in an arguably simpler
    way (io_apic.c), and (3) subsystems whose suspend/resume callbacks
    use their struct sys_device argument, but the value of that argument
    is always the same and could be ignored (microcode_core.c).  In all
    of these cases the subsystems in question may be readily converted to
    using struct syscore_ops objects for power management and shutdown.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index e2b7b0c06cdf..8dace181c88e 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -15,7 +15,7 @@
 #include <linux/notifier.h>
 #include <linux/smp.h>
 #include <linux/oprofile.h>
-#include <linux/sysdev.h>
+#include <linux/syscore_ops.h>
 #include <linux/slab.h>
 #include <linux/moduleparam.h>
 #include <linux/kdebug.h>
@@ -536,7 +536,7 @@ static void nmi_shutdown(void)
 
 #ifdef CONFIG_PM
 
-static int nmi_suspend(struct sys_device *dev, pm_message_t state)
+static int nmi_suspend(void)
 {
 	/* Only one CPU left, just stop that one */
 	if (nmi_enabled == 1)
@@ -544,49 +544,31 @@ static int nmi_suspend(struct sys_device *dev, pm_message_t state)
 	return 0;
 }
 
-static int nmi_resume(struct sys_device *dev)
+static void nmi_resume(void)
 {
 	if (nmi_enabled == 1)
 		nmi_cpu_start(NULL);
-	return 0;
 }
 
-static struct sysdev_class oprofile_sysclass = {
-	.name		= "oprofile",
+static struct syscore_ops oprofile_syscore_ops = {
 	.resume		= nmi_resume,
 	.suspend	= nmi_suspend,
 };
 
-static struct sys_device device_oprofile = {
-	.id	= 0,
-	.cls	= &oprofile_sysclass,
-};
-
-static int __init init_sysfs(void)
+static void __init init_suspend_resume(void)
 {
-	int error;
-
-	error = sysdev_class_register(&oprofile_sysclass);
-	if (error)
-		return error;
-
-	error = sysdev_register(&device_oprofile);
-	if (error)
-		sysdev_class_unregister(&oprofile_sysclass);
-
-	return error;
+	register_syscore_ops(&oprofile_syscore_ops);
 }
 
-static void exit_sysfs(void)
+static void exit_suspend_resume(void)
 {
-	sysdev_unregister(&device_oprofile);
-	sysdev_class_unregister(&oprofile_sysclass);
+	unregister_syscore_ops(&oprofile_syscore_ops);
 }
 
 #else
 
-static inline int  init_sysfs(void) { return 0; }
-static inline void exit_sysfs(void) { }
+static inline void init_suspend_resume(void) { }
+static inline void exit_suspend_resume(void) { }
 
 #endif /* CONFIG_PM */
 
@@ -789,9 +771,7 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 
 	mux_init(ops);
 
-	ret = init_sysfs();
-	if (ret)
-		return ret;
+	init_suspend_resume();
 
 	printk(KERN_INFO "oprofile: using NMI interrupt.\n");
 	return 0;
@@ -799,5 +779,5 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 
 void op_nmi_exit(void)
 {
-	exit_sysfs();
+	exit_suspend_resume();
 }

commit 42776163e13a56ea3096edff7a5df95408e80eb4
Merge: edb2877f4a62 3d03e2ea7410
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 11 11:02:13 2011 -0800

    Merge branch 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (28 commits)
      perf session: Fix infinite loop in __perf_session__process_events
      perf evsel: Support perf_evsel__open(cpus > 1 && threads > 1)
      perf sched: Use PTHREAD_STACK_MIN to avoid pthread_attr_setstacksize() fail
      perf tools: Emit clearer message for sys_perf_event_open ENOENT return
      perf stat: better error message for unsupported events
      perf sched: Fix allocation result check
      perf, x86: P4 PMU - Fix unflagged overflows handling
      dynamic debug: Fix build issue with older gcc
      tracing: Fix TRACE_EVENT power tracepoint creation
      tracing: Fix preempt count leak
      tracepoint: Add __rcu annotation
      tracing: remove duplicate null-pointer check in skb tracepoint
      tracing/trivial: Add missing comma in TRACE_EVENT comment
      tracing: Include module.h in define_trace.h
      x86: Save rbp in pt_regs on irq entry
      x86, dumpstack: Fix unused variable warning
      x86, NMI: Clean-up default_do_nmi()
      x86, NMI: Allow NMI reason io port (0x61) to be processed on any CPU
      x86, NMI: Remove DIE_NMI_IPI
      x86, NMI: Add priorities to handlers
      ...

commit 72eb6a791459c87a0340318840bb3bd9252b627b
Merge: 23d69b09b78c 55ee4ef30241
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 17:02:58 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (30 commits)
      gameport: use this_cpu_read instead of lookup
      x86: udelay: Use this_cpu_read to avoid address calculation
      x86: Use this_cpu_inc_return for nmi counter
      x86: Replace uses of current_cpu_data with this_cpu ops
      x86: Use this_cpu_ops to optimize code
      vmstat: User per cpu atomics to avoid interrupt disable / enable
      irq_work: Use per cpu atomics instead of regular atomics
      cpuops: Use cmpxchg for xchg to avoid lock semantics
      x86: this_cpu_cmpxchg and this_cpu_xchg operations
      percpu: Generic this_cpu_cmpxchg() and this_cpu_xchg support
      percpu,x86: relocate this_cpu_add_return() and friends
      connector: Use this_cpu operations
      xen: Use this_cpu_inc_return
      taskstats: Use this_cpu_ops
      random: Use this_cpu_inc_return
      fs: Use this_cpu_inc_return in buffer.c
      highmem: Use this_cpu_xx_return() operations
      vmstat: Use this_cpu_inc_return for vm statistics
      x86: Support for this_cpu_add, sub, dec, inc_return
      percpu: Generic support for this_cpu_add, sub, dec, inc_return
      ...
    
    Fixed up conflicts: in arch/x86/kernel/{apic/nmi.c, apic/x2apic_uv_x.c, process.c}
    as per Tejun.

commit c410b8307702c1e1f35be3fd868ad18e4ba0410f
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Jan 6 16:18:50 2011 -0500

    x86, NMI: Remove DIE_NMI_IPI
    
    With priorities in place and no one really understanding the difference between
    DIE_NMI and DIE_NMI_IPI, just remove DIE_NMI_IPI and convert everyone to DIE_NMI.
    
    This also simplifies default_do_nmi() a little bit.  Instead of calling the
    die_notifier in both the if and else part, just pull it out and call it before
    the if-statement.  This has the side benefit of avoiding a call to the ioport
    to see if there is an external NMI sitting around until after the (more frequent)
    internal NMIs are dealt with.
    
    Patch-Inspired-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1294348732-15030-5-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 6e84ea42085a..e77ea0b566e0 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -65,7 +65,6 @@ static int profile_exceptions_notify(struct notifier_block *self,
 
 	switch (val) {
 	case DIE_NMI:
-	case DIE_NMI_IPI:
 		if (ctr_running)
 			model->check_ctrs(args->regs, &__get_cpu_var(cpu_msrs));
 		else if (!nmi_enabled)

commit 166d751479c6d4e5b17dfc1f204a9c4397c9b3f1
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Jan 6 16:18:49 2011 -0500

    x86, NMI: Add priorities to handlers
    
    In order to consolidate the NMI die_chain events, we need to setup the priorities
    for the die notifiers.
    
    I started by defining a bunch of common priorities that can be used by the
    notifier blocks.  Then I modified the notifier blocks to use the newly created
    priorities.
    
    Now that the priorities are straightened out, it should be easier to remove the
    event DIE_NMI_IPI.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1294348732-15030-4-git-send-email-dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 358c8b9c96a7..6e84ea42085a 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -361,7 +361,7 @@ static void nmi_cpu_setup(void *dummy)
 static struct notifier_block profile_exceptions_nb = {
 	.notifier_call = profile_exceptions_notify,
 	.next = NULL,
-	.priority = 2
+	.priority = NMI_LOCAL_LOW_PRIOR,
 };
 
 static void nmi_cpu_restore_registers(struct op_msrs *msrs)

commit 0a3aee0da4402aa19b66e458038533c896fb80c6
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Dec 18 16:28:55 2010 +0100

    x86: Use this_cpu_ops to optimize code
    
    Go through x86 code and replace __get_cpu_var and get_cpu_var
    instances that refer to a scalar and are not used for address
    determinations.
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 4e8baad36d37..a0cae67a657a 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -143,7 +143,7 @@ static inline int has_mux(void)
 
 inline int op_x86_phys_to_virt(int phys)
 {
-	return __get_cpu_var(switch_index) + phys;
+	return __this_cpu_read(switch_index) + phys;
 }
 
 inline int op_x86_virt_to_phys(int virt)

commit 30570bced107243d5227527dd5317b22883dcf0c
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Aug 31 10:44:38 2010 +0200

    oprofile, x86: Add support for AMD family 15h
    
    This patch adds support for AMD family 15h (Interlagos/Valencia/
    Zambezi) cpus.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 4e8baad36d37..358c8b9c96a7 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -732,6 +732,9 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		case 0x14:
 			cpu_type = "x86-64/family14h";
 			break;
+		case 0x15:
+			cpu_type = "x86-64/family15h";
+			break;
 		default:
 			return -ENODEV;
 		}

commit e63414740e15b4e2dc54c63fb9ea501b257fb0b5
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Aug 26 12:30:17 2010 +0200

    oprofile, x86: Add support for AMD family 14h
    
    This patch adds support for AMD family 14h (Ontario/Zacate) cpus.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 0b0d1d628207..4e8baad36d37 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -729,6 +729,9 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		case 0x12:
 			cpu_type = "x86-64/family12h";
 			break;
+		case 0x14:
+			cpu_type = "x86-64/family14h";
+			break;
 		default:
 			return -ENODEV;
 		}

commit 3acbf0849bcbb639fde53dc627e3b55a4c6429d2
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Aug 31 10:44:17 2010 +0200

    oprofile, x86: Add support for AMD family 12h
    
    This patch adds support for AMD family 12h (Llano) cpus.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index bd1489c3ce09..0b0d1d628207 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -726,6 +726,9 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		case 0x11:
 			cpu_type = "x86-64/family11h";
 			break;
+		case 0x12:
+			cpu_type = "x86-64/family12h";
+			break;
 		default:
 			return -ENODEV;
 		}

commit 5140434d5f82f2e2119926272ada2e9731ec04f1
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Sep 30 18:55:47 2010 +0200

    oprofile, x86: Simplify init/exit functions
    
    Now, that we only call the exit function if init succeeds with commit:
    
     979048e oprofile: don't call arch exit code from init code on failure
    
    we can simplify the x86 init/exit functions too. Variable using_nmi
    becomes obsolete.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index f1575c9a2572..bd1489c3ce09 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -695,9 +695,6 @@ static int __init ppro_init(char **cpu_type)
 	return 1;
 }
 
-/* in order to get sysfs right */
-static int using_nmi;
-
 int __init op_nmi_init(struct oprofile_operations *ops)
 {
 	__u8 vendor = boot_cpu_data.x86_vendor;
@@ -705,8 +702,6 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	char *cpu_type = NULL;
 	int ret = 0;
 
-	using_nmi = 0;
-
 	if (!cpu_has_apic)
 		return -ENODEV;
 
@@ -790,13 +785,11 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	if (ret)
 		return ret;
 
-	using_nmi = 1;
 	printk(KERN_INFO "oprofile: using NMI interrupt.\n");
 	return 0;
 }
 
 void op_nmi_exit(void)
 {
-	if (using_nmi)
-		exit_sysfs();
+	exit_sysfs();
 }

commit bb7ab785ad05a97a2c9ffb3a06547ed39f3133e8
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Tue Sep 21 03:26:35 2010 -0400

    oprofile: Add Support for Intel CPU Family 6 / Model 29
    
    This patch adds CPU type detection for dunnington processor (Family 6
    / Model 29) to be identified as core 2 family cpu type (wikipedia
    source).
    
    I tested oprofile on Intel(R) Xeon(R) CPU E7440 reporting itself as
    model 29, and it runs without an issue.
    
    Spec:
    
     http://www.intel.com/Assets/en_US/PDF/specupdate/320336.pdf
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 009b819f48d0..f1575c9a2572 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -674,6 +674,7 @@ static int __init ppro_init(char **cpu_type)
 	case 0x0f:
 	case 0x16:
 	case 0x17:
+	case 0x1d:
 		*cpu_type = "i386/core_2";
 		break;
 	case 0x1a:

commit c33f543d320843e1732534c3931da4bbd18e6c14
Author: Patrick Simmons <linuxrocks123@netscape.net>
Date:   Wed Sep 8 10:34:28 2010 -0400

    oprofile: Add Support for Intel CPU Family 6 / Model 22 (Intel Celeron 540)
    
    This patch adds CPU type detection for the Intel Celeron 540, which is
    part of the Core 2 family according to Wikipedia; the family and ID pair
    is absent from the Volume 3B table referenced in the source code
    comments.  I have tested this patch on an Intel Celeron 540 machine
    reporting itself as Family 6 Model 22, and OProfile runs on the machine
    without issue.
    
    Spec:
    
     http://download.intel.com/design/mobile/SPECUPDT/317667.pdf
    
    Signed-off-by: Patrick Simmons <linuxrocks123@netscape.net>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index cfe4faabb0f6..009b819f48d0 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -671,7 +671,9 @@ static int __init ppro_init(char **cpu_type)
 	case 14:
 		*cpu_type = "i386/core";
 		break;
-	case 15: case 23:
+	case 0x0f:
+	case 0x16:
+	case 0x17:
 		*cpu_type = "i386/core_2";
 		break;
 	case 0x1a:

commit 269f45c25028c75fe10e6d9be86e7202ab461fbc
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Sep 1 14:50:50 2010 +0200

    oprofile, x86: fix init_sysfs() function stub
    
    The use of the return value of init_sysfs() with commit
    
     10f0412 oprofile, x86: fix init_sysfs error handling
    
    discovered the following build error for !CONFIG_PM:
    
     .../linux/arch/x86/oprofile/nmi_int.c: In function ‘op_nmi_init’:
     .../linux/arch/x86/oprofile/nmi_int.c:784: error: expected expression before ‘do’
     make[2]: *** [arch/x86/oprofile/nmi_int.o] Error 1
     make[1]: *** [arch/x86/oprofile] Error 2
    
    This patch fixes this.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 73a41d3c6c09..cfe4faabb0f6 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -585,8 +585,10 @@ static void exit_sysfs(void)
 }
 
 #else
-#define init_sysfs() do { } while (0)
-#define exit_sysfs() do { } while (0)
+
+static inline int  init_sysfs(void) { return 0; }
+static inline void exit_sysfs(void) { }
+
 #endif /* CONFIG_PM */
 
 static int __init p4_init(char **cpu_type)

commit 10f0412f57f2a76a90eff4376f59cbb0a39e4e18
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Aug 30 10:56:18 2010 +0200

    oprofile, x86: fix init_sysfs error handling
    
    On failure init_sysfs() might not properly free resources. The error
    code of the function is not checked. And, when reinitializing the exit
    function might be called twice. This patch fixes all this.
    
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index f6b48f6c5951..73a41d3c6c09 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -568,8 +568,13 @@ static int __init init_sysfs(void)
 	int error;
 
 	error = sysdev_class_register(&oprofile_sysclass);
-	if (!error)
-		error = sysdev_register(&device_oprofile);
+	if (error)
+		return error;
+
+	error = sysdev_register(&device_oprofile);
+	if (error)
+		sysdev_class_unregister(&oprofile_sysclass);
+
 	return error;
 }
 
@@ -695,6 +700,8 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	char *cpu_type = NULL;
 	int ret = 0;
 
+	using_nmi = 0;
+
 	if (!cpu_has_apic)
 		return -ENODEV;
 
@@ -774,7 +781,10 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 
 	mux_init(ops);
 
-	init_sysfs();
+	ret = init_sysfs();
+	if (ret)
+		return ret;
+
 	using_nmi = 1;
 	printk(KERN_INFO "oprofile: using NMI interrupt.\n");
 	return 0;

commit a7c55cbee0c1bae9bf5a15a08300e91d88706e45
Author: Josh Hunt <johunt@akamai.com>
Date:   Wed Aug 4 20:27:05 2010 -0400

    oprofile: add support for Intel processor model 30
    
    Newer Intel processors identifying themselves as model 30 are not recognized by
    oprofile.
    
    <cpuinfo snippet>
    model           : 30
    model name      : Intel(R) Xeon(R) CPU           X3470  @ 2.93GHz
    </cpuinfo snippet>
    
    Running oprofile on these machines gives the following:
    + opcontrol --init
    + opcontrol --list-events
    oprofile: available events for CPU type "Intel Architectural Perfmon"
    
    See Intel 64 and IA-32 Architectures Software Developer's Manual
    Volume 3B (Document 253669) Chapter 18 for architectural perfmon events
    This is a limited set of fallback events because oprofile doesn't know your CPU
    CPU_CLK_UNHALTED: (counter: all)
            Clock cycles when not halted (min count: 6000)
    INST_RETIRED: (counter: all)
            number of instructions retired (min count: 6000)
    LLC_MISSES: (counter: all)
            Last level cache demand requests from this core that missed the LLC
    (min count: 6000)
            Unit masks (default 0x41)
            ----------
            0x41: No unit mask
    LLC_REFS: (counter: all)
            Last level cache demand requests from this core (min count: 6000)
            Unit masks (default 0x4f)
            ----------
            0x4f: No unit mask
    BR_MISS_PRED_RETIRED: (counter: all)
            number of mispredicted branches retired (precise) (min count: 500)
    + opcontrol --shutdown
    
    Tested using oprofile 0.9.6.
    
    Signed-off-by: Josh Hunt <johunt@akamai.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 1ba67dc8006a..f6b48f6c5951 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -668,6 +668,7 @@ static int __init ppro_init(char **cpu_type)
 		*cpu_type = "i386/core_2";
 		break;
 	case 0x1a:
+	case 0x1e:
 	case 0x2e:
 		spec = &op_arch_perfmon_spec;
 		*cpu_type = "i386/core_i7";

commit 45c34e05c4e3d36e7c44e790241ea11a1d90d54e
Author: John Villalovos <sodarock@gmail.com>
Date:   Fri May 7 12:41:40 2010 -0400

    Oprofile: Change CPUIDS from decimal to hex, and add some comments
    
    Back when the patch was submitted for "Add Xeon 7500 series support to
    oprofile", Robert Richter had asked for a followon patch that
    converted all the CPU ID values to hex.
    
    I have done that here for the "i386/core_i7" and "i386/atom" class
    processors in the ppro_init() function and also added some comments on
    where to find documentation on the Intel processors.
    
    Signed-off-by: John L. Villalovos <john.l.villalovos@intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index b28d2f1253bb..1ba67dc8006a 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -634,6 +634,18 @@ static int __init ppro_init(char **cpu_type)
 	if (force_arch_perfmon && cpu_has_arch_perfmon)
 		return 0;
 
+	/*
+	 * Documentation on identifying Intel processors by CPU family
+	 * and model can be found in the Intel Software Developer's
+	 * Manuals (SDM):
+	 *
+	 *  http://www.intel.com/products/processor/manuals/
+	 *
+	 * As of May 2010 the documentation for this was in the:
+	 * "Intel 64 and IA-32 Architectures Software Developer's
+	 * Manual Volume 3B: System Programming Guide", "Table B-1
+	 * CPUID Signature Values of DisplayFamily_DisplayModel".
+	 */
 	switch (cpu_model) {
 	case 0 ... 2:
 		*cpu_type = "i386/ppro";
@@ -655,12 +667,12 @@ static int __init ppro_init(char **cpu_type)
 	case 15: case 23:
 		*cpu_type = "i386/core_2";
 		break;
+	case 0x1a:
 	case 0x2e:
-	case 26:
 		spec = &op_arch_perfmon_spec;
 		*cpu_type = "i386/core_i7";
 		break;
-	case 28:
+	case 0x1c:
 		*cpu_type = "i386/atom";
 		break;
 	default:

commit bae663bc635e2726c7c5228dbf0f2051e16d1c81
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed May 5 17:47:17 2010 +0200

    oprofile/x86: make AMD IBS hotplug capable
    
    Current IBS code is not hotplug capable. An offline cpu might not be
    initialized or deinitialized properly. This patch fixes this by
    removing on_each_cpu() functions. The IBS init/deinit code is executed
    in the per-cpu functions model->setup_ctrs() and model->cpu_down()
    which are also called by hotplug notifiers. model->cpu_down() replaces
    model->exit() that became obsolete.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 2a086726cad1..b28d2f1253bb 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -397,6 +397,8 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
 	nmi_cpu_restore_registers(msrs);
+	if (model->cpu_down)
+		model->cpu_down();
 }
 
 static void nmi_cpu_up(void *dummy)
@@ -769,6 +771,4 @@ void op_nmi_exit(void)
 {
 	if (using_nmi)
 		exit_sysfs();
-	if (model->exit)
-		model->exit();
 }

commit 3de668ee8d5b1e08da3200f926ff5a28aeb99bc2
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon May 3 15:00:25 2010 +0200

    oprofile/x86: notify cpus only when daemon is running
    
    This patch moves the cpu notifier registration from nmi_init() to
    nmi_setup(). The corresponding unregistration function is now in
    nmi_shutdown(). Thus, the hotplug code is only active, if the oprofile
    daemon is running.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 7de0572b0a5e..2a086726cad1 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -504,6 +504,7 @@ static int nmi_setup(void)
 		goto fail;
 
 	get_online_cpus();
+	register_cpu_notifier(&oprofile_cpu_nb);
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;
 	put_online_cpus();
@@ -519,6 +520,7 @@ static void nmi_shutdown(void)
 	struct op_msrs *msrs;
 
 	get_online_cpus();
+	unregister_cpu_notifier(&oprofile_cpu_nb);
 	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
 	nmi_enabled = 0;
 	ctr_running = 0;
@@ -739,12 +741,6 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		return -ENODEV;
 	}
 
-	get_online_cpus();
-	register_cpu_notifier(&oprofile_cpu_nb);
-	nmi_enabled = 0;
-	ctr_running = 0;
-	put_online_cpus();
-
 	/* default values, can be overwritten by model */
 	ops->create_files	= nmi_create_files;
 	ops->setup		= nmi_setup;
@@ -771,14 +767,8 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 
 void op_nmi_exit(void)
 {
-	if (using_nmi) {
+	if (using_nmi)
 		exit_sysfs();
-		get_online_cpus();
-		unregister_cpu_notifier(&oprofile_cpu_nb);
-		nmi_enabled = 0;
-		ctr_running = 0;
-		put_online_cpus();
-	}
 	if (model->exit)
 		model->exit();
 }

commit d30d64c6da3ec7a0708bfffa7e05752d5b9a1093
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon May 3 15:52:26 2010 +0200

    oprofile/x86: reordering some functions
    
    Reordering some functions. Necessary for the next patch. No functional
    changes.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 94b5481bb6c6..7de0572b0a5e 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -364,56 +364,6 @@ static struct notifier_block profile_exceptions_nb = {
 	.priority = 2
 };
 
-static int nmi_setup(void)
-{
-	int err = 0;
-	int cpu;
-
-	if (!allocate_msrs())
-		return -ENOMEM;
-
-	/* We need to serialize save and setup for HT because the subset
-	 * of msrs are distinct for save and setup operations
-	 */
-
-	/* Assume saved/restored counters are the same on all CPUs */
-	err = model->fill_in_addresses(&per_cpu(cpu_msrs, 0));
-	if (err)
-		goto fail;
-
-	for_each_possible_cpu(cpu) {
-		if (!cpu)
-			continue;
-
-		memcpy(per_cpu(cpu_msrs, cpu).counters,
-		       per_cpu(cpu_msrs, 0).counters,
-		       sizeof(struct op_msr) * model->num_counters);
-
-		memcpy(per_cpu(cpu_msrs, cpu).controls,
-		       per_cpu(cpu_msrs, 0).controls,
-		       sizeof(struct op_msr) * model->num_controls);
-
-		mux_clone(cpu);
-	}
-
-	nmi_enabled = 0;
-	ctr_running = 0;
-	barrier();
-	err = register_die_notifier(&profile_exceptions_nb);
-	if (err)
-		goto fail;
-
-	get_online_cpus();
-	on_each_cpu(nmi_cpu_setup, NULL, 1);
-	nmi_enabled = 1;
-	put_online_cpus();
-
-	return 0;
-fail:
-	free_msrs();
-	return err;
-}
-
 static void nmi_cpu_restore_registers(struct op_msrs *msrs)
 {
 	struct op_msr *counters = msrs->counters;
@@ -449,23 +399,6 @@ static void nmi_cpu_shutdown(void *dummy)
 	nmi_cpu_restore_registers(msrs);
 }
 
-static void nmi_shutdown(void)
-{
-	struct op_msrs *msrs;
-
-	get_online_cpus();
-	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
-	nmi_enabled = 0;
-	ctr_running = 0;
-	put_online_cpus();
-	barrier();
-	unregister_die_notifier(&profile_exceptions_nb);
-	msrs = &get_cpu_var(cpu_msrs);
-	model->shutdown(msrs);
-	free_msrs();
-	put_cpu_var(cpu_msrs);
-}
-
 static void nmi_cpu_up(void *dummy)
 {
 	if (nmi_enabled)
@@ -531,6 +464,73 @@ static struct notifier_block oprofile_cpu_nb = {
 	.notifier_call = oprofile_cpu_notifier
 };
 
+static int nmi_setup(void)
+{
+	int err = 0;
+	int cpu;
+
+	if (!allocate_msrs())
+		return -ENOMEM;
+
+	/* We need to serialize save and setup for HT because the subset
+	 * of msrs are distinct for save and setup operations
+	 */
+
+	/* Assume saved/restored counters are the same on all CPUs */
+	err = model->fill_in_addresses(&per_cpu(cpu_msrs, 0));
+	if (err)
+		goto fail;
+
+	for_each_possible_cpu(cpu) {
+		if (!cpu)
+			continue;
+
+		memcpy(per_cpu(cpu_msrs, cpu).counters,
+		       per_cpu(cpu_msrs, 0).counters,
+		       sizeof(struct op_msr) * model->num_counters);
+
+		memcpy(per_cpu(cpu_msrs, cpu).controls,
+		       per_cpu(cpu_msrs, 0).controls,
+		       sizeof(struct op_msr) * model->num_controls);
+
+		mux_clone(cpu);
+	}
+
+	nmi_enabled = 0;
+	ctr_running = 0;
+	barrier();
+	err = register_die_notifier(&profile_exceptions_nb);
+	if (err)
+		goto fail;
+
+	get_online_cpus();
+	on_each_cpu(nmi_cpu_setup, NULL, 1);
+	nmi_enabled = 1;
+	put_online_cpus();
+
+	return 0;
+fail:
+	free_msrs();
+	return err;
+}
+
+static void nmi_shutdown(void)
+{
+	struct op_msrs *msrs;
+
+	get_online_cpus();
+	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
+	nmi_enabled = 0;
+	ctr_running = 0;
+	put_online_cpus();
+	barrier();
+	unregister_die_notifier(&profile_exceptions_nb);
+	msrs = &get_cpu_var(cpu_msrs);
+	model->shutdown(msrs);
+	free_msrs();
+	put_cpu_var(cpu_msrs);
+}
+
 #ifdef CONFIG_PM
 
 static int nmi_suspend(struct sys_device *dev, pm_message_t state)

commit de654649737696ecf32873c341b305e30f3dc777
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon May 3 14:41:22 2010 +0200

    oprofile/x86: stop disabled counters in nmi handler
    
    This patch adds checks to the nmi handler. Now samples are only
    generated and counters reenabled, if the counters are running.
    Otherwise the counters are stopped, if oprofile is using the nmi. In
    other cases it will ignore the nmi notification.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index b56601eaf29d..94b5481bb6c6 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -62,12 +62,16 @@ static int profile_exceptions_notify(struct notifier_block *self,
 {
 	struct die_args *args = (struct die_args *)data;
 	int ret = NOTIFY_DONE;
-	int cpu = smp_processor_id();
 
 	switch (val) {
 	case DIE_NMI:
 	case DIE_NMI_IPI:
-		model->check_ctrs(args->regs, &per_cpu(cpu_msrs, cpu));
+		if (ctr_running)
+			model->check_ctrs(args->regs, &__get_cpu_var(cpu_msrs));
+		else if (!nmi_enabled)
+			break;
+		else
+			model->stop(&__get_cpu_var(cpu_msrs));
 		ret = NOTIFY_STOP;
 		break;
 	default:
@@ -392,6 +396,9 @@ static int nmi_setup(void)
 		mux_clone(cpu);
 	}
 
+	nmi_enabled = 0;
+	ctr_running = 0;
+	barrier();
 	err = register_die_notifier(&profile_exceptions_nb);
 	if (err)
 		goto fail;
@@ -451,6 +458,7 @@ static void nmi_shutdown(void)
 	nmi_enabled = 0;
 	ctr_running = 0;
 	put_online_cpus();
+	barrier();
 	unregister_die_notifier(&profile_exceptions_nb);
 	msrs = &get_cpu_var(cpu_msrs);
 	model->shutdown(msrs);

commit 6ae56b55bc364bc2f2342f599b46581627ba22da
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Apr 29 14:55:55 2010 +0200

    oprofile/x86: protect cpu hotplug sections
    
    This patch reworks oprofile cpu hotplug code as follows:
    
    Introduce ctr_running variable to check, if counters are running or
    not. The state must be known for taking a cpu on or offline and when
    switching counters during counter multiplexing.
    
    Protect on_each_cpu() sections with get_online_cpus()/put_online_cpu()
    functions. This is necessary if notifiers or states are
    modified. Within these sections the cpu mask may not change.
    
    Switch only between counters in nmi_cpu_switch(), if counters are
    running. Otherwise the switch may restart a counter though they are
    disabled.
    
    Add nmi_cpu_setup() and nmi_cpu_shutdown() to cpu hotplug code. The
    function must also be called to avoid uninitialzed counter usage.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index c5df8ee76ee4..b56601eaf29d 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -31,8 +31,9 @@ static struct op_x86_model_spec *model;
 static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
 static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 
-/* 0 == registered but off, 1 == registered and on */
-static int nmi_enabled = 0;
+/* must be protected with get_online_cpus()/put_online_cpus(): */
+static int nmi_enabled;
+static int ctr_running;
 
 struct op_counter_config counter_config[OP_MAX_COUNTER];
 
@@ -103,7 +104,10 @@ static void nmi_cpu_start(void *dummy)
 
 static int nmi_start(void)
 {
+	get_online_cpus();
 	on_each_cpu(nmi_cpu_start, NULL, 1);
+	ctr_running = 1;
+	put_online_cpus();
 	return 0;
 }
 
@@ -118,7 +122,10 @@ static void nmi_cpu_stop(void *dummy)
 
 static void nmi_stop(void)
 {
+	get_online_cpus();
 	on_each_cpu(nmi_cpu_stop, NULL, 1);
+	ctr_running = 0;
+	put_online_cpus();
 }
 
 #ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
@@ -258,7 +265,10 @@ static int nmi_switch_event(void)
 	if (nmi_multiplex_on() < 0)
 		return -EINVAL;		/* not necessary */
 
-	on_each_cpu(nmi_cpu_switch, NULL, 1);
+	get_online_cpus();
+	if (ctr_running)
+		on_each_cpu(nmi_cpu_switch, NULL, 1);
+	put_online_cpus();
 
 	return 0;
 }
@@ -386,8 +396,11 @@ static int nmi_setup(void)
 	if (err)
 		goto fail;
 
+	get_online_cpus();
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;
+	put_online_cpus();
+
 	return 0;
 fail:
 	free_msrs();
@@ -433,8 +446,11 @@ static void nmi_shutdown(void)
 {
 	struct op_msrs *msrs;
 
-	nmi_enabled = 0;
+	get_online_cpus();
 	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
+	nmi_enabled = 0;
+	ctr_running = 0;
+	put_online_cpus();
 	unregister_die_notifier(&profile_exceptions_nb);
 	msrs = &get_cpu_var(cpu_msrs);
 	model->shutdown(msrs);
@@ -442,6 +458,22 @@ static void nmi_shutdown(void)
 	put_cpu_var(cpu_msrs);
 }
 
+static void nmi_cpu_up(void *dummy)
+{
+	if (nmi_enabled)
+		nmi_cpu_setup(dummy);
+	if (ctr_running)
+		nmi_cpu_start(dummy);
+}
+
+static void nmi_cpu_down(void *dummy)
+{
+	if (ctr_running)
+		nmi_cpu_stop(dummy);
+	if (nmi_enabled)
+		nmi_cpu_shutdown(dummy);
+}
+
 static int nmi_create_files(struct super_block *sb, struct dentry *root)
 {
 	unsigned int i;
@@ -478,10 +510,10 @@ static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
 	switch (action) {
 	case CPU_DOWN_FAILED:
 	case CPU_ONLINE:
-		smp_call_function_single(cpu, nmi_cpu_start, NULL, 0);
+		smp_call_function_single(cpu, nmi_cpu_up, NULL, 0);
 		break;
 	case CPU_DOWN_PREPARE:
-		smp_call_function_single(cpu, nmi_cpu_stop, NULL, 1);
+		smp_call_function_single(cpu, nmi_cpu_down, NULL, 1);
 		break;
 	}
 	return NOTIFY_DONE;
@@ -699,7 +731,11 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		return -ENODEV;
 	}
 
+	get_online_cpus();
 	register_cpu_notifier(&oprofile_cpu_nb);
+	nmi_enabled = 0;
+	ctr_running = 0;
+	put_online_cpus();
 
 	/* default values, can be overwritten by model */
 	ops->create_files	= nmi_create_files;
@@ -729,7 +765,11 @@ void op_nmi_exit(void)
 {
 	if (using_nmi) {
 		exit_sysfs();
+		get_online_cpus();
 		unregister_cpu_notifier(&oprofile_cpu_nb);
+		nmi_enabled = 0;
+		ctr_running = 0;
+		put_online_cpus();
 	}
 	if (model->exit)
 		model->exit();

commit 216f3d9b4e5121feea4b13fae9d4c83e8d7e1c8a
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon May 3 11:58:46 2010 +0200

    oprofile/x86: remove CONFIG_SMP macros
    
    CPU notifier register functions also exist if CONFIG_SMP is
    disabled. This change is part of hotplug code rework and also
    necessary for later patches.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 24582040b718..c5df8ee76ee4 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -471,7 +471,6 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 	return 0;
 }
 
-#ifdef CONFIG_SMP
 static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
 				 void *data)
 {
@@ -491,7 +490,6 @@ static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
 static struct notifier_block oprofile_cpu_nb = {
 	.notifier_call = oprofile_cpu_notifier
 };
-#endif
 
 #ifdef CONFIG_PM
 
@@ -701,9 +699,8 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		return -ENODEV;
 	}
 
-#ifdef CONFIG_SMP
 	register_cpu_notifier(&oprofile_cpu_nb);
-#endif
+
 	/* default values, can be overwritten by model */
 	ops->create_files	= nmi_create_files;
 	ops->setup		= nmi_setup;
@@ -732,9 +729,7 @@ void op_nmi_exit(void)
 {
 	if (using_nmi) {
 		exit_sysfs();
-#ifdef CONFIG_SMP
 		unregister_cpu_notifier(&oprofile_cpu_nb);
-#endif
 	}
 	if (model->exit)
 		model->exit();

commit 2623a1d55a6260c855e1f6d1895900b50b40a896
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon May 3 19:44:32 2010 +0200

    oprofile/x86: fix uninitialized counter usage during cpu hotplug
    
    This fixes a NULL pointer dereference that is triggered when taking a
    cpu offline after oprofile was initialized, e.g.:
    
     $ opcontrol --init
     $ opcontrol --start-daemon
     $ opcontrol --shutdown
     $ opcontrol --deinit
     $ echo 0 > /sys/devices/system/cpu/cpu1/online
    
    See the crash dump below. Though the counter has been disabled the cpu
    notifier is still active and trying to use already freed counter data.
    
    This fix is for linux-stable. To proper fix this, the hotplug code
    must be rewritten. Thus I will leave a WARN_ON_ONCE() message with
    this patch.
    
    BUG: unable to handle kernel NULL pointer dereference at (null)
    IP: [<ffffffff8132ad57>] op_amd_stop+0x2d/0x8e
    PGD 0
    Oops: 0000 [#1] SMP
    last sysfs file: /sys/devices/system/cpu/cpu1/online
    CPU 1
    Modules linked in:
    
    Pid: 0, comm: swapper Not tainted 2.6.34-rc5-oprofile-x86_64-standard-00210-g8c00f06 #16 Anaheim/Anaheim
    RIP: 0010:[<ffffffff8132ad57>]  [<ffffffff8132ad57>] op_amd_stop+0x2d/0x8e
    RSP: 0018:ffff880001843f28  EFLAGS: 00010006
    RAX: 0000000000000000 RBX: 0000000000000000 RCX: dead000000200200
    RDX: ffff880001843f68 RSI: dead000000100100 RDI: 0000000000000000
    RBP: ffff880001843f48 R08: 0000000000000000 R09: ffff880001843f08
    R10: ffffffff8102c9a5 R11: ffff88000184ea80 R12: 0000000000000000
    R13: ffff88000184f6c0 R14: 0000000000000000 R15: 0000000000000000
    FS:  00007fec6a92e6f0(0000) GS:ffff880001840000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    CR2: 0000000000000000 CR3: 000000000163b000 CR4: 00000000000006e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process swapper (pid: 0, threadinfo ffff88042fcd8000, task ffff88042fcd51d0)
    Stack:
     ffff880001843f48 0000000000000001 ffff88042e9f7d38 ffff880001843f68
    <0> ffff880001843f58 ffffffff8132a602 ffff880001843f98 ffffffff810521b3
    <0> ffff880001843f68 ffff880001843f68 ffff880001843f88 ffff88042fcd9fd8
    Call Trace:
     <IRQ>
     [<ffffffff8132a602>] nmi_cpu_stop+0x21/0x23
     [<ffffffff810521b3>] generic_smp_call_function_single_interrupt+0xdf/0x11b
     [<ffffffff8101804f>] smp_call_function_single_interrupt+0x22/0x31
     [<ffffffff810029f3>] call_function_single_interrupt+0x13/0x20
     <EOI>
     [<ffffffff8102c9a5>] ? wake_up_process+0x10/0x12
     [<ffffffff81008701>] ? default_idle+0x22/0x37
     [<ffffffff8100896d>] c1e_idle+0xdf/0xe6
     [<ffffffff813f1170>] ? atomic_notifier_call_chain+0x13/0x15
     [<ffffffff810012fb>] cpu_idle+0x4b/0x7e
     [<ffffffff813e8a4e>] start_secondary+0x1ae/0x1b2
    Code: 89 e5 41 55 49 89 fd 41 54 45 31 e4 53 31 db 48 83 ec 08 89 df e8 be f8 ff ff 48 98 48 83 3c c5 10 67 7a 81 00 74 1f 49 8b 45 08 <42> 8b 0c 20 0f 32 48 c1 e2 20 25 ff ff bf ff 48 09 d0 48 89 c2
    RIP  [<ffffffff8132ad57>] op_amd_stop+0x2d/0x8e
     RSP <ffff880001843f28>
    CR2: 0000000000000000
    ---[ end trace 679ac372d674b757 ]---
    Kernel panic - not syncing: Fatal exception in interrupt
    Pid: 0, comm: swapper Tainted: G      D    2.6.34-rc5-oprofile-x86_64-standard-00210-g8c00f06 #16
    Call Trace:
     <IRQ>  [<ffffffff813ebd6a>] panic+0x9e/0x10c
     [<ffffffff810474b0>] ? up+0x34/0x39
     [<ffffffff81031ccc>] ? kmsg_dump+0x112/0x12c
     [<ffffffff813eeff1>] oops_end+0x81/0x8e
     [<ffffffff8101efee>] no_context+0x1f3/0x202
     [<ffffffff8101f1b7>] __bad_area_nosemaphore+0x1ba/0x1e0
     [<ffffffff81028d24>] ? enqueue_task_fair+0x16d/0x17a
     [<ffffffff810264dc>] ? activate_task+0x42/0x53
     [<ffffffff8102c967>] ? try_to_wake_up+0x272/0x284
     [<ffffffff8101f1eb>] bad_area_nosemaphore+0xe/0x10
     [<ffffffff813f0f3f>] do_page_fault+0x1c8/0x37c
     [<ffffffff81028d24>] ? enqueue_task_fair+0x16d/0x17a
     [<ffffffff813ee55f>] page_fault+0x1f/0x30
     [<ffffffff8102c9a5>] ? wake_up_process+0x10/0x12
     [<ffffffff8132ad57>] ? op_amd_stop+0x2d/0x8e
     [<ffffffff8132ad46>] ? op_amd_stop+0x1c/0x8e
     [<ffffffff8132a602>] nmi_cpu_stop+0x21/0x23
     [<ffffffff810521b3>] generic_smp_call_function_single_interrupt+0xdf/0x11b
     [<ffffffff8101804f>] smp_call_function_single_interrupt+0x22/0x31
     [<ffffffff810029f3>] call_function_single_interrupt+0x13/0x20
     <EOI>  [<ffffffff8102c9a5>] ? wake_up_process+0x10/0x12
     [<ffffffff81008701>] ? default_idle+0x22/0x37
     [<ffffffff8100896d>] c1e_idle+0xdf/0xe6
     [<ffffffff813f1170>] ? atomic_notifier_call_chain+0x13/0x15
     [<ffffffff810012fb>] cpu_idle+0x4b/0x7e
     [<ffffffff813e8a4e>] start_secondary+0x1ae/0x1b2
    ------------[ cut here ]------------
    WARNING: at /local/rrichter/.source/linux/arch/x86/kernel/smp.c:118 native_smp_send_reschedule+0x27/0x53()
    Hardware name: Anaheim
    Modules linked in:
    Pid: 0, comm: swapper Tainted: G      D    2.6.34-rc5-oprofile-x86_64-standard-00210-g8c00f06 #16
    Call Trace:
     <IRQ>  [<ffffffff81017f32>] ? native_smp_send_reschedule+0x27/0x53
     [<ffffffff81030ee2>] warn_slowpath_common+0x77/0xa4
     [<ffffffff81030f1e>] warn_slowpath_null+0xf/0x11
     [<ffffffff81017f32>] native_smp_send_reschedule+0x27/0x53
     [<ffffffff8102634b>] resched_task+0x60/0x62
     [<ffffffff8102653a>] check_preempt_curr_idle+0x10/0x12
     [<ffffffff8102c8ea>] try_to_wake_up+0x1f5/0x284
     [<ffffffff8102c986>] default_wake_function+0xd/0xf
     [<ffffffff810a110d>] pollwake+0x57/0x5a
     [<ffffffff8102c979>] ? default_wake_function+0x0/0xf
     [<ffffffff81026be5>] __wake_up_common+0x46/0x75
     [<ffffffff81026ed0>] __wake_up+0x38/0x50
     [<ffffffff81031694>] printk_tick+0x39/0x3b
     [<ffffffff8103ac37>] update_process_times+0x3f/0x5c
     [<ffffffff8104dc63>] tick_periodic+0x5d/0x69
     [<ffffffff8104dc90>] tick_handle_periodic+0x21/0x71
     [<ffffffff81018fd0>] smp_apic_timer_interrupt+0x82/0x95
     [<ffffffff81002853>] apic_timer_interrupt+0x13/0x20
     [<ffffffff81030cb5>] ? panic_blink_one_second+0x0/0x7b
     [<ffffffff813ebdd6>] ? panic+0x10a/0x10c
     [<ffffffff810474b0>] ? up+0x34/0x39
     [<ffffffff81031ccc>] ? kmsg_dump+0x112/0x12c
     [<ffffffff813eeff1>] ? oops_end+0x81/0x8e
     [<ffffffff8101efee>] ? no_context+0x1f3/0x202
     [<ffffffff8101f1b7>] ? __bad_area_nosemaphore+0x1ba/0x1e0
     [<ffffffff81028d24>] ? enqueue_task_fair+0x16d/0x17a
     [<ffffffff810264dc>] ? activate_task+0x42/0x53
     [<ffffffff8102c967>] ? try_to_wake_up+0x272/0x284
     [<ffffffff8101f1eb>] ? bad_area_nosemaphore+0xe/0x10
     [<ffffffff813f0f3f>] ? do_page_fault+0x1c8/0x37c
     [<ffffffff81028d24>] ? enqueue_task_fair+0x16d/0x17a
     [<ffffffff813ee55f>] ? page_fault+0x1f/0x30
     [<ffffffff8102c9a5>] ? wake_up_process+0x10/0x12
     [<ffffffff8132ad57>] ? op_amd_stop+0x2d/0x8e
     [<ffffffff8132ad46>] ? op_amd_stop+0x1c/0x8e
     [<ffffffff8132a602>] ? nmi_cpu_stop+0x21/0x23
     [<ffffffff810521b3>] ? generic_smp_call_function_single_interrupt+0xdf/0x11b
     [<ffffffff8101804f>] ? smp_call_function_single_interrupt+0x22/0x31
     [<ffffffff810029f3>] ? call_function_single_interrupt+0x13/0x20
     <EOI>  [<ffffffff8102c9a5>] ? wake_up_process+0x10/0x12
     [<ffffffff81008701>] ? default_idle+0x22/0x37
     [<ffffffff8100896d>] ? c1e_idle+0xdf/0xe6
     [<ffffffff813f1170>] ? atomic_notifier_call_chain+0x13/0x15
     [<ffffffff810012fb>] ? cpu_idle+0x4b/0x7e
     [<ffffffff813e8a4e>] ? start_secondary+0x1ae/0x1b2
    ---[ end trace 679ac372d674b758 ]---
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: stable <stable@kernel.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 9f001d904599..24582040b718 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -95,7 +95,10 @@ static void nmi_cpu_save_registers(struct op_msrs *msrs)
 static void nmi_cpu_start(void *dummy)
 {
 	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
-	model->start(msrs);
+	if (!msrs->controls)
+		WARN_ON_ONCE(1);
+	else
+		model->start(msrs);
 }
 
 static int nmi_start(void)
@@ -107,7 +110,10 @@ static int nmi_start(void)
 static void nmi_cpu_stop(void *dummy)
 {
 	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
-	model->stop(msrs);
+	if (!msrs->controls)
+		WARN_ON_ONCE(1);
+	else
+		model->stop(msrs);
 }
 
 static void nmi_stop(void)

commit 8617f98c001d00b176422d707e6a67b88bcd7e0d
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Feb 26 17:20:55 2010 +0100

    oprofile/x86: return -EBUSY if counters are already reserved
    
    In case a counter is already reserved by the watchdog or perf_event
    subsystem, oprofile ignored this counters silently. This case is
    handled now and oprofile_setup() now reports an error.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index c0c21f200faf..9f001d904599 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -357,7 +357,10 @@ static int nmi_setup(void)
 	 */
 
 	/* Assume saved/restored counters are the same on all CPUs */
-	model->fill_in_addresses(&per_cpu(cpu_msrs, 0));
+	err = model->fill_in_addresses(&per_cpu(cpu_msrs, 0));
+	if (err)
+		goto fail;
+
 	for_each_possible_cpu(cpu) {
 		if (!cpu)
 			continue;

commit 8f5a2dd83a1f8e89fdc17eb0f2f07c2e713e635a
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Mar 23 19:09:51 2010 +0100

    oprofile/x86: rework error handler in nmi_setup()
    
    This patch improves the error handler in nmi_setup(). Most parts of
    the code are moved to allocate_msrs(). In case of an error
    allocate_msrs() also frees already allocated memory. nmi_setup()
    becomes easier and better extendable.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 2c505ee71014..c0c21f200faf 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -295,6 +295,7 @@ static void free_msrs(void)
 		kfree(per_cpu(cpu_msrs, i).controls);
 		per_cpu(cpu_msrs, i).controls = NULL;
 	}
+	nmi_shutdown_mux();
 }
 
 static int allocate_msrs(void)
@@ -307,14 +308,21 @@ static int allocate_msrs(void)
 		per_cpu(cpu_msrs, i).counters = kzalloc(counters_size,
 							GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).counters)
-			return 0;
+			goto fail;
 		per_cpu(cpu_msrs, i).controls = kzalloc(controls_size,
 							GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).controls)
-			return 0;
+			goto fail;
 	}
 
+	if (!nmi_setup_mux())
+		goto fail;
+
 	return 1;
+
+fail:
+	free_msrs();
+	return 0;
 }
 
 static void nmi_cpu_setup(void *dummy)
@@ -342,17 +350,7 @@ static int nmi_setup(void)
 	int cpu;
 
 	if (!allocate_msrs())
-		err = -ENOMEM;
-	else if (!nmi_setup_mux())
-		err = -ENOMEM;
-	else
-		err = register_die_notifier(&profile_exceptions_nb);
-
-	if (err) {
-		free_msrs();
-		nmi_shutdown_mux();
-		return err;
-	}
+		return -ENOMEM;
 
 	/* We need to serialize save and setup for HT because the subset
 	 * of msrs are distinct for save and setup operations
@@ -374,9 +372,17 @@ static int nmi_setup(void)
 
 		mux_clone(cpu);
 	}
+
+	err = register_die_notifier(&profile_exceptions_nb);
+	if (err)
+		goto fail;
+
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;
 	return 0;
+fail:
+	free_msrs();
+	return err;
 }
 
 static void nmi_cpu_restore_registers(struct op_msrs *msrs)
@@ -421,7 +427,6 @@ static void nmi_shutdown(void)
 	nmi_enabled = 0;
 	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
 	unregister_die_notifier(&profile_exceptions_nb);
-	nmi_shutdown_mux();
 	msrs = &get_cpu_var(cpu_msrs);
 	model->shutdown(msrs);
 	free_msrs();

commit c17c8fbf349482e89b57d1b800e83e9f4cf40c47
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Feb 25 20:20:25 2010 +0100

    oprofile/x86: use kzalloc() instead of kmalloc()
    
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 7170d1e29896..2c505ee71014 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -159,7 +159,7 @@ static int nmi_setup_mux(void)
 
 	for_each_possible_cpu(i) {
 		per_cpu(cpu_msrs, i).multiplex =
-			kmalloc(multiplex_size, GFP_KERNEL);
+			kzalloc(multiplex_size, GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).multiplex)
 			return 0;
 	}
@@ -304,11 +304,11 @@ static int allocate_msrs(void)
 
 	int i;
 	for_each_possible_cpu(i) {
-		per_cpu(cpu_msrs, i).counters = kmalloc(counters_size,
+		per_cpu(cpu_msrs, i).counters = kzalloc(counters_size,
 							GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).counters)
 			return 0;
-		per_cpu(cpu_msrs, i).controls = kmalloc(controls_size,
+		per_cpu(cpu_msrs, i).controls = kzalloc(controls_size,
 							GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).controls)
 			return 0;

commit 68dc819ce829f7e7977a56524e710473bdb55115
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Feb 25 19:16:46 2010 +0100

    oprofile/x86: fix perfctr nmi reservation for mulitplexing
    
    Multiple virtual counters share one physical counter. The reservation
    of virtual counters fails due to duplicate allocation of the same
    counter. The counters are already reserved. Thus, virtual counter
    reservation may removed at all. This also makes the code easier.
    
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 3347f696edc7..7170d1e29896 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -179,7 +179,6 @@ static void nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs)
 		if (counter_config[i].enabled) {
 			multiplex[i].saved = -(u64)counter_config[i].count;
 		} else {
-			multiplex[i].addr  = 0;
 			multiplex[i].saved = 0;
 		}
 	}
@@ -189,25 +188,27 @@ static void nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs)
 
 static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs)
 {
+	struct op_msr *counters = msrs->counters;
 	struct op_msr *multiplex = msrs->multiplex;
 	int i;
 
 	for (i = 0; i < model->num_counters; ++i) {
 		int virt = op_x86_phys_to_virt(i);
-		if (multiplex[virt].addr)
-			rdmsrl(multiplex[virt].addr, multiplex[virt].saved);
+		if (counters[i].addr)
+			rdmsrl(counters[i].addr, multiplex[virt].saved);
 	}
 }
 
 static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
 {
+	struct op_msr *counters = msrs->counters;
 	struct op_msr *multiplex = msrs->multiplex;
 	int i;
 
 	for (i = 0; i < model->num_counters; ++i) {
 		int virt = op_x86_phys_to_virt(i);
-		if (multiplex[virt].addr)
-			wrmsrl(multiplex[virt].addr, multiplex[virt].saved);
+		if (counters[i].addr)
+			wrmsrl(counters[i].addr, multiplex[virt].saved);
 	}
 }
 

commit e83e452b0692c9c13372540deb88a77d4ae2553d
Author: Andi Kleen <andi@firstfloor.org>
Date:   Thu Jan 21 23:26:27 2010 +0100

    oprofile/x86: add Xeon 7500 series support
    
    Add Xeon 7500 series support to oprofile.
    
    Straight forward: it's the same as Core i7, so just detect
    the model number. No user space changes needed.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 76d4f566adee..3347f696edc7 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -598,6 +598,7 @@ static int __init ppro_init(char **cpu_type)
 	case 15: case 23:
 		*cpu_type = "i386/core_2";
 		break;
+	case 0x2e:
 	case 26:
 		spec = &op_arch_perfmon_spec;
 		*cpu_type = "i386/core_i7";

commit d8cc108f4fab42b380c6b3f3356f99e8dd5372e2
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Mon Jan 18 11:25:36 2010 -0600

    oprofile/x86: fix crash when profiling more than 28 events
    
    With multiplexing enabled oprofile crashs when profiling more than 28
    events. This patch fixes this.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index cb88b1a0bd5f..76d4f566adee 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -222,7 +222,7 @@ static void nmi_cpu_switch(void *dummy)
 
 	/* move to next set */
 	si += model->num_counters;
-	if ((si > model->num_virt_counters) || (counter_config[si].count == 0))
+	if ((si >= model->num_virt_counters) || (counter_config[si].count == 0))
 		per_cpu(switch_index, cpu) = 0;
 	else
 		per_cpu(switch_index, cpu) = si;

commit 11be1a7b54283021777f409aa983ce125945e67c
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Jul 10 18:15:21 2009 +0200

    x86/oprofile: Add counter reservation check for virtual counters
    
    This patch adds a check for the availability of a counter. A virtual
    counter is used only if its physical counter is not reserved.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 5856e61cb098..cb88b1a0bd5f 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -435,15 +435,13 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 		struct dentry *dir;
 		char buf[4];
 
-#ifndef CONFIG_OPROFILE_EVENT_MULTIPLEX
 		/* quick little hack to _not_ expose a counter if it is not
 		 * available for use.  This should protect userspace app.
 		 * NOTE:  assumes 1:1 mapping here (that counters are organized
 		 *        sequentially in their struct assignment).
 		 */
-		if (unlikely(!avail_to_resrv_perfctr_nmi_bit(i)))
+		if (!avail_to_resrv_perfctr_nmi_bit(op_x86_virt_to_phys(i)))
 			continue;
-#endif /* CONFIG_OPROFILE_EVENT_MULTIPLEX */
 
 		snprintf(buf,  sizeof(buf), "%d", i);
 		dir = oprofilefs_mkdir(sb, root, buf);

commit 61d149d5248ad7428801cdede0f5fcc2b90cd61c
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Jul 10 15:47:17 2009 +0200

    x86/oprofile: Implement op_x86_virt_to_phys()
    
    This patch implements a common x86 function to convert virtual counter
    numbers to physical.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 7b3362f9abdb..5856e61cb098 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -129,6 +129,11 @@ inline int op_x86_phys_to_virt(int phys)
 	return __get_cpu_var(switch_index) + phys;
 }
 
+inline int op_x86_virt_to_phys(int virt)
+{
+	return virt % model->num_counters;
+}
+
 static void nmi_shutdown_mux(void)
 {
 	int i;
@@ -270,6 +275,7 @@ static void mux_clone(int cpu)
 #else
 
 inline int op_x86_phys_to_virt(int phys) { return phys; }
+inline int op_x86_virt_to_phys(int virt) { return virt; }
 static inline void nmi_shutdown_mux(void) { }
 static inline int nmi_setup_mux(void) { return 1; }
 static inline void

commit 1b294f5960cd89e49eeb3e797860c552b03f2272
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 14:56:25 2009 +0200

    oprofile: Adding switch counter to oprofile statistic variables
    
    This patch moves the multiplexing switch counter from x86 code to
    common oprofile statistic variables. Now the value will be available
    and usable for all architectures. The initialization and
    incrementation also moved to common code.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index da6d2ab31c6c..7b3362f9abdb 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -34,11 +34,6 @@ static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 /* 0 == registered but off, 1 == registered and on */
 static int nmi_enabled = 0;
 
-
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-extern atomic_t multiplex_counter;
-#endif
-
 struct op_counter_config counter_config[OP_MAX_COUNTER];
 
 /* common functions */
@@ -253,8 +248,6 @@ static int nmi_switch_event(void)
 
 	on_each_cpu(nmi_cpu_switch, NULL, 1);
 
-	atomic_inc(&multiplex_counter);
-
 	return 0;
 }
 

commit 4d015f79e972cea1761cfee8872b1c0992ccd8b2
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 21:42:51 2009 +0200

    x86/oprofile: Implement mux_clone()
    
    To setup a counter for all cpus, its structure is cloned from cpu
    0. This patch implements mux_clone() to do this part for multiplexing
    data.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index f0fb44725d80..da6d2ab31c6c 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -264,6 +264,16 @@ static inline void mux_init(struct oprofile_operations *ops)
 		ops->switch_events = nmi_switch_event;
 }
 
+static void mux_clone(int cpu)
+{
+	if (!has_mux())
+		return;
+
+	memcpy(per_cpu(cpu_msrs, cpu).multiplex,
+	       per_cpu(cpu_msrs, 0).multiplex,
+	       sizeof(struct op_msr) * model->num_virt_counters);
+}
+
 #else
 
 inline int op_x86_phys_to_virt(int phys) { return phys; }
@@ -272,6 +282,7 @@ static inline int nmi_setup_mux(void) { return 1; }
 static inline void
 nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs) { }
 static inline void mux_init(struct oprofile_operations *ops) { }
+static void mux_clone(int cpu) { }
 
 #endif
 
@@ -350,20 +361,18 @@ static int nmi_setup(void)
 	/* Assume saved/restored counters are the same on all CPUs */
 	model->fill_in_addresses(&per_cpu(cpu_msrs, 0));
 	for_each_possible_cpu(cpu) {
-		if (cpu != 0) {
-			memcpy(per_cpu(cpu_msrs, cpu).counters,
-				per_cpu(cpu_msrs, 0).counters,
-				sizeof(struct op_msr) * model->num_counters);
-
-			memcpy(per_cpu(cpu_msrs, cpu).controls,
-				per_cpu(cpu_msrs, 0).controls,
-				sizeof(struct op_msr) * model->num_controls);
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-			memcpy(per_cpu(cpu_msrs, cpu).multiplex,
-				per_cpu(cpu_msrs, 0).multiplex,
-				sizeof(struct op_msr) * model->num_virt_counters);
-#endif
-		}
+		if (!cpu)
+			continue;
+
+		memcpy(per_cpu(cpu_msrs, cpu).counters,
+		       per_cpu(cpu_msrs, 0).counters,
+		       sizeof(struct op_msr) * model->num_counters);
+
+		memcpy(per_cpu(cpu_msrs, cpu).controls,
+		       per_cpu(cpu_msrs, 0).controls,
+		       sizeof(struct op_msr) * model->num_controls);
+
+		mux_clone(cpu);
 	}
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;

commit 5280514471c2803776701c43c027038decac1103
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 16:02:44 2009 +0200

    x86/oprofile: Enable multiplexing only if the model supports it
    
    This patch checks if the model supports multiplexing. Only then
    multiplexing will be enabled. The code is added to the common x86
    initialization.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index dca7240aeb26..f0fb44725d80 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -258,6 +258,12 @@ static int nmi_switch_event(void)
 	return 0;
 }
 
+static inline void mux_init(struct oprofile_operations *ops)
+{
+	if (has_mux())
+		ops->switch_events = nmi_switch_event;
+}
+
 #else
 
 inline int op_x86_phys_to_virt(int phys) { return phys; }
@@ -265,6 +271,7 @@ static inline void nmi_shutdown_mux(void) { }
 static inline int nmi_setup_mux(void) { return 1; }
 static inline void
 nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs) { }
+static inline void mux_init(struct oprofile_operations *ops) { }
 
 #endif
 
@@ -682,9 +689,6 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	ops->start		= nmi_start;
 	ops->stop		= nmi_stop;
 	ops->cpu_type		= cpu_type;
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-	ops->switch_events	= nmi_switch_event;
-#endif
 
 	if (model->init)
 		ret = model->init(ops);
@@ -694,6 +698,8 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	if (!model->num_virt_counters)
 		model->num_virt_counters = model->num_counters;
 
+	mux_init(ops);
+
 	init_sysfs();
 	using_nmi = 1;
 	printk(KERN_INFO "oprofile: using NMI interrupt.\n");

commit 39e97f40c3a5e71de0532368deaa683e09b74ba2
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 15:11:45 2009 +0200

    x86/oprofile: Add function has_mux() to check multiplexing support
    
    The check is used to prevent running multiplexing code for models not
    supporting multiplexing. Before, the code was running but without
    effect.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 82ee29517f16..dca7240aeb26 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -124,6 +124,11 @@ static void nmi_stop(void)
 
 static DEFINE_PER_CPU(int, switch_index);
 
+static inline int has_mux(void)
+{
+	return !!model->switch_ctrl;
+}
+
 inline int op_x86_phys_to_virt(int phys)
 {
 	return __get_cpu_var(switch_index) + phys;
@@ -132,6 +137,10 @@ inline int op_x86_phys_to_virt(int phys)
 static void nmi_shutdown_mux(void)
 {
 	int i;
+
+	if (!has_mux())
+		return;
+
 	for_each_possible_cpu(i) {
 		kfree(per_cpu(cpu_msrs, i).multiplex);
 		per_cpu(cpu_msrs, i).multiplex = NULL;
@@ -144,12 +153,17 @@ static int nmi_setup_mux(void)
 	size_t multiplex_size =
 		sizeof(struct op_msr) * model->num_virt_counters;
 	int i;
+
+	if (!has_mux())
+		return 1;
+
 	for_each_possible_cpu(i) {
 		per_cpu(cpu_msrs, i).multiplex =
 			kmalloc(multiplex_size, GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).multiplex)
 			return 0;
 	}
+
 	return 1;
 }
 
@@ -158,6 +172,9 @@ static void nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs)
 	int i;
 	struct op_msr *multiplex = msrs->multiplex;
 
+	if (!has_mux())
+		return;
+
 	for (i = 0; i < model->num_virt_counters; ++i) {
 		if (counter_config[i].enabled) {
 			multiplex[i].saved = -(u64)counter_config[i].count;
@@ -229,7 +246,7 @@ static int nmi_multiplex_on(void)
 
 static int nmi_switch_event(void)
 {
-	if (!model->switch_ctrl)
+	if (!has_mux())
 		return -ENOSYS;		/* not implemented */
 	if (nmi_multiplex_on() < 0)
 		return -EINVAL;		/* not necessary */

commit 52471c67ee2fa5ed6f700ef57bf27833c63b2192
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Jul 6 14:43:55 2009 +0200

    x86/oprofile: Modify initialization of num_virt_counters
    
    Models that do not yet support counter multiplexing have to setup
    num_virt_counters. This patch implements the setup from num_counters
    if num_virt_counters is not set. Thus, num_virt_counters must be setup
    only for multiplexing support.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 826f391b4229..82ee29517f16 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -674,6 +674,9 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	if (ret)
 		return ret;
 
+	if (!model->num_virt_counters)
+		model->num_virt_counters = model->num_counters;
+
 	init_sysfs();
 	using_nmi = 1;
 	printk(KERN_INFO "oprofile: using NMI interrupt.\n");

commit 259a83a8abdb9d2664819ec80ad12ebaeb251e32
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 15:12:35 2009 +0200

    x86/oprofile: Remove const qualifier from struct op_x86_model_spec
    
    This patch removes the const qualifier from struct
    op_x86_model_spec to make model parameters changable.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 998c7dca31e7..826f391b4229 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -27,7 +27,7 @@
 #include "op_counter.h"
 #include "op_x86_model.h"
 
-static struct op_x86_model_spec const *model;
+static struct op_x86_model_spec *model;
 static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
 static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 
@@ -542,7 +542,7 @@ module_param_call(cpu_type, force_cpu_type, NULL, NULL, 0);
 static int __init ppro_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
-	struct op_x86_model_spec const *spec = &op_ppro_spec;	/* default */
+	struct op_x86_model_spec *spec = &op_ppro_spec;	/* default */
 
 	if (force_arch_perfmon && cpu_has_arch_perfmon)
 		return 0;

commit b28d1b923ab52d535c0719155dccf3b3d98bab9f
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 14:38:49 2009 +0200

    x86/oprofile: Moving nmi_cpu_switch() in nmi_int.c
    
    This patch moves some code in nmi_int.c to get a single separate
    multiplexing code section.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index f38c5cf0fdbb..998c7dca31e7 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -97,6 +97,29 @@ static void nmi_cpu_save_registers(struct op_msrs *msrs)
 	}
 }
 
+static void nmi_cpu_start(void *dummy)
+{
+	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
+	model->start(msrs);
+}
+
+static int nmi_start(void)
+{
+	on_each_cpu(nmi_cpu_start, NULL, 1);
+	return 0;
+}
+
+static void nmi_cpu_stop(void *dummy)
+{
+	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
+	model->stop(msrs);
+}
+
+static void nmi_stop(void)
+{
+	on_each_cpu(nmi_cpu_stop, NULL, 1);
+}
+
 #ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
 
 static DEFINE_PER_CPU(int, switch_index);
@@ -171,6 +194,53 @@ static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
 	}
 }
 
+static void nmi_cpu_switch(void *dummy)
+{
+	int cpu = smp_processor_id();
+	int si = per_cpu(switch_index, cpu);
+	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
+
+	nmi_cpu_stop(NULL);
+	nmi_cpu_save_mpx_registers(msrs);
+
+	/* move to next set */
+	si += model->num_counters;
+	if ((si > model->num_virt_counters) || (counter_config[si].count == 0))
+		per_cpu(switch_index, cpu) = 0;
+	else
+		per_cpu(switch_index, cpu) = si;
+
+	model->switch_ctrl(model, msrs);
+	nmi_cpu_restore_mpx_registers(msrs);
+
+	nmi_cpu_start(NULL);
+}
+
+
+/*
+ * Quick check to see if multiplexing is necessary.
+ * The check should be sufficient since counters are used
+ * in ordre.
+ */
+static int nmi_multiplex_on(void)
+{
+	return counter_config[model->num_counters].count ? 0 : -EINVAL;
+}
+
+static int nmi_switch_event(void)
+{
+	if (!model->switch_ctrl)
+		return -ENOSYS;		/* not implemented */
+	if (nmi_multiplex_on() < 0)
+		return -EINVAL;		/* not necessary */
+
+	on_each_cpu(nmi_cpu_switch, NULL, 1);
+
+	atomic_inc(&multiplex_counter);
+
+	return 0;
+}
+
 #else
 
 inline int op_x86_phys_to_virt(int phys) { return phys; }
@@ -325,29 +395,6 @@ static void nmi_shutdown(void)
 	put_cpu_var(cpu_msrs);
 }
 
-static void nmi_cpu_start(void *dummy)
-{
-	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
-	model->start(msrs);
-}
-
-static int nmi_start(void)
-{
-	on_each_cpu(nmi_cpu_start, NULL, 1);
-	return 0;
-}
-
-static void nmi_cpu_stop(void *dummy)
-{
-	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
-	model->stop(msrs);
-}
-
-static void nmi_stop(void)
-{
-	on_each_cpu(nmi_cpu_stop, NULL, 1);
-}
-
 static int nmi_create_files(struct super_block *sb, struct dentry *root)
 {
 	unsigned int i;
@@ -379,57 +426,6 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 	return 0;
 }
 
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-
-static void nmi_cpu_switch(void *dummy)
-{
-	int cpu = smp_processor_id();
-	int si = per_cpu(switch_index, cpu);
-	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
-
-	nmi_cpu_stop(NULL);
-	nmi_cpu_save_mpx_registers(msrs);
-
-	/* move to next set */
-	si += model->num_counters;
-	if ((si > model->num_virt_counters) || (counter_config[si].count == 0))
-		per_cpu(switch_index, cpu) = 0;
-	else
-		per_cpu(switch_index, cpu) = si;
-
-	model->switch_ctrl(model, msrs);
-	nmi_cpu_restore_mpx_registers(msrs);
-
-	nmi_cpu_start(NULL);
-}
-
-
-/*
- * Quick check to see if multiplexing is necessary.
- * The check should be sufficient since counters are used
- * in ordre.
- */
-static int nmi_multiplex_on(void)
-{
-	return counter_config[model->num_counters].count ? 0 : -EINVAL;
-}
-
-static int nmi_switch_event(void)
-{
-	if (!model->switch_ctrl)
-		return -ENOSYS;		/* not implemented */
-	if (nmi_multiplex_on() < 0)
-		return -EINVAL;		/* not necessary */
-
-	on_each_cpu(nmi_cpu_switch, NULL, 1);
-
-	atomic_inc(&multiplex_counter);
-
-	return 0;
-}
-
-#endif
-
 #ifdef CONFIG_SMP
 static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
 				 void *data)

commit d0f585dd20010f8479e56b5c6f391ef18e26877e
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 14:38:49 2009 +0200

    x86/oprofile: Moving nmi_cpu_save/restore_mpx_registers() in nmi_int.c
    
    This patch moves some code in nmi_int.c to get a single separate
    multiplexing code section.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index b1edfc922e7f..f38c5cf0fdbb 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -147,6 +147,30 @@ static void nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs)
 	per_cpu(switch_index, cpu) = 0;
 }
 
+static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs)
+{
+	struct op_msr *multiplex = msrs->multiplex;
+	int i;
+
+	for (i = 0; i < model->num_counters; ++i) {
+		int virt = op_x86_phys_to_virt(i);
+		if (multiplex[virt].addr)
+			rdmsrl(multiplex[virt].addr, multiplex[virt].saved);
+	}
+}
+
+static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
+{
+	struct op_msr *multiplex = msrs->multiplex;
+	int i;
+
+	for (i = 0; i < model->num_counters; ++i) {
+		int virt = op_x86_phys_to_virt(i);
+		if (multiplex[virt].addr)
+			wrmsrl(multiplex[virt].addr, multiplex[virt].saved);
+	}
+}
+
 #else
 
 inline int op_x86_phys_to_virt(int phys) { return phys; }
@@ -252,34 +276,6 @@ static int nmi_setup(void)
 	return 0;
 }
 
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-
-static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs)
-{
-	struct op_msr *multiplex = msrs->multiplex;
-	int i;
-
-	for (i = 0; i < model->num_counters; ++i) {
-		int virt = op_x86_phys_to_virt(i);
-		if (multiplex[virt].addr)
-			rdmsrl(multiplex[virt].addr, multiplex[virt].saved);
-	}
-}
-
-static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
-{
-	struct op_msr *multiplex = msrs->multiplex;
-	int i;
-
-	for (i = 0; i < model->num_counters; ++i) {
-		int virt = op_x86_phys_to_virt(i);
-		if (multiplex[virt].addr)
-			wrmsrl(multiplex[virt].addr, multiplex[virt].saved);
-	}
-}
-
-#endif
-
 static void nmi_cpu_restore_registers(struct op_msrs *msrs)
 {
 	struct op_msr *counters = msrs->counters;

commit 48fb4b46712c7d3e8adc79826311abd9ccbf7f1d
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 14:38:49 2009 +0200

    x86/oprofile: Moving nmi_setup_cpu_mux() in nmi_int.c
    
    This patch moves some code in nmi_int.c to get a single separate
    multiplexing code section.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 674fa37d1502..b1edfc922e7f 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -130,11 +130,30 @@ static int nmi_setup_mux(void)
 	return 1;
 }
 
+static void nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs)
+{
+	int i;
+	struct op_msr *multiplex = msrs->multiplex;
+
+	for (i = 0; i < model->num_virt_counters; ++i) {
+		if (counter_config[i].enabled) {
+			multiplex[i].saved = -(u64)counter_config[i].count;
+		} else {
+			multiplex[i].addr  = 0;
+			multiplex[i].saved = 0;
+		}
+	}
+
+	per_cpu(switch_index, cpu) = 0;
+}
+
 #else
 
 inline int op_x86_phys_to_virt(int phys) { return phys; }
 static inline void nmi_shutdown_mux(void) { }
 static inline int nmi_setup_mux(void) { return 1; }
+static inline void
+nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs) { }
 
 #endif
 
@@ -169,32 +188,6 @@ static int allocate_msrs(void)
 	return 1;
 }
 
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-
-static void nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs)
-{
-	int i;
-	struct op_msr *multiplex = msrs->multiplex;
-
-	for (i = 0; i < model->num_virt_counters; ++i) {
-		if (counter_config[i].enabled) {
-			multiplex[i].saved = -(u64)counter_config[i].count;
-		} else {
-			multiplex[i].addr  = 0;
-			multiplex[i].saved = 0;
-		}
-	}
-
-	per_cpu(switch_index, cpu) = 0;
-}
-
-#else
-
-static inline void
-nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs) { }
-
-#endif
-
 static void nmi_cpu_setup(void *dummy)
 {
 	int cpu = smp_processor_id();

commit 6ab82f958a5dca591a6ea17a3ca6f2aca06f4f2f
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 14:40:04 2009 +0200

    x86/oprofile: Implement multiplexing setup/shutdown functions
    
    This patch implements nmi_setup_mux() and nmi_shutdown_mux() functions
    to setup/shutdown multiplexing. Multiplexing code in nmi_int.c is now
    much more separated.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 02b57b8d0e61..674fa37d1502 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -106,9 +106,35 @@ inline int op_x86_phys_to_virt(int phys)
 	return __get_cpu_var(switch_index) + phys;
 }
 
+static void nmi_shutdown_mux(void)
+{
+	int i;
+	for_each_possible_cpu(i) {
+		kfree(per_cpu(cpu_msrs, i).multiplex);
+		per_cpu(cpu_msrs, i).multiplex = NULL;
+		per_cpu(switch_index, i) = 0;
+	}
+}
+
+static int nmi_setup_mux(void)
+{
+	size_t multiplex_size =
+		sizeof(struct op_msr) * model->num_virt_counters;
+	int i;
+	for_each_possible_cpu(i) {
+		per_cpu(cpu_msrs, i).multiplex =
+			kmalloc(multiplex_size, GFP_KERNEL);
+		if (!per_cpu(cpu_msrs, i).multiplex)
+			return 0;
+	}
+	return 1;
+}
+
 #else
 
 inline int op_x86_phys_to_virt(int phys) { return phys; }
+static inline void nmi_shutdown_mux(void) { }
+static inline int nmi_setup_mux(void) { return 1; }
 
 #endif
 
@@ -120,51 +146,27 @@ static void free_msrs(void)
 		per_cpu(cpu_msrs, i).counters = NULL;
 		kfree(per_cpu(cpu_msrs, i).controls);
 		per_cpu(cpu_msrs, i).controls = NULL;
-
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-		kfree(per_cpu(cpu_msrs, i).multiplex);
-		per_cpu(cpu_msrs, i).multiplex = NULL;
-#endif
 	}
 }
 
 static int allocate_msrs(void)
 {
-	int success = 1;
 	size_t controls_size = sizeof(struct op_msr) * model->num_controls;
 	size_t counters_size = sizeof(struct op_msr) * model->num_counters;
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-	size_t multiplex_size = sizeof(struct op_msr) * model->num_virt_counters;
-#endif
 
 	int i;
 	for_each_possible_cpu(i) {
 		per_cpu(cpu_msrs, i).counters = kmalloc(counters_size,
-								GFP_KERNEL);
-		if (!per_cpu(cpu_msrs, i).counters) {
-			success = 0;
-			break;
-		}
+							GFP_KERNEL);
+		if (!per_cpu(cpu_msrs, i).counters)
+			return 0;
 		per_cpu(cpu_msrs, i).controls = kmalloc(controls_size,
-								GFP_KERNEL);
-		if (!per_cpu(cpu_msrs, i).controls) {
-			success = 0;
-			break;
-		}
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-		per_cpu(cpu_msrs, i).multiplex =
-				kmalloc(multiplex_size, GFP_KERNEL);
-		if (!per_cpu(cpu_msrs, i).multiplex) {
-			success = 0;
-			break;
-		}
-#endif
+							GFP_KERNEL);
+		if (!per_cpu(cpu_msrs, i).controls)
+			return 0;
 	}
 
-	if (!success)
-		free_msrs();
-
-	return success;
+	return 1;
 }
 
 #ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
@@ -218,11 +220,15 @@ static int nmi_setup(void)
 	int cpu;
 
 	if (!allocate_msrs())
-		return -ENOMEM;
+		err = -ENOMEM;
+	else if (!nmi_setup_mux())
+		err = -ENOMEM;
+	else
+		err = register_die_notifier(&profile_exceptions_nb);
 
-	err = register_die_notifier(&profile_exceptions_nb);
 	if (err) {
 		free_msrs();
+		nmi_shutdown_mux();
 		return err;
 	}
 
@@ -314,9 +320,6 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
 	nmi_cpu_restore_registers(msrs);
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-	per_cpu(switch_index, cpu) = 0;
-#endif
 }
 
 static void nmi_shutdown(void)
@@ -326,6 +329,7 @@ static void nmi_shutdown(void)
 	nmi_enabled = 0;
 	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
 	unregister_die_notifier(&profile_exceptions_nb);
+	nmi_shutdown_mux();
 	msrs = &get_cpu_var(cpu_msrs);
 	model->shutdown(msrs);
 	free_msrs();

commit d8471ad3ab613a1ba7abd3aad46659de39a2871c
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 16 13:04:43 2009 +0200

    oprofile: Introduce op_x86_phys_to_virt()
    
    This new function translates physical to virtual counter numbers.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index b211d335e075..02b57b8d0e61 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -27,12 +27,6 @@
 #include "op_counter.h"
 #include "op_x86_model.h"
 
-
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-DEFINE_PER_CPU(int, switch_index);
-#endif
-
-
 static struct op_x86_model_spec const *model;
 static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
 static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
@@ -103,6 +97,21 @@ static void nmi_cpu_save_registers(struct op_msrs *msrs)
 	}
 }
 
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+
+static DEFINE_PER_CPU(int, switch_index);
+
+inline int op_x86_phys_to_virt(int phys)
+{
+	return __get_cpu_var(switch_index) + phys;
+}
+
+#else
+
+inline int op_x86_phys_to_virt(int phys) { return phys; }
+
+#endif
+
 static void free_msrs(void)
 {
 	int i;
@@ -248,31 +257,25 @@ static int nmi_setup(void)
 
 static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs)
 {
-	unsigned int si = __get_cpu_var(switch_index);
 	struct op_msr *multiplex = msrs->multiplex;
-	unsigned int i;
+	int i;
 
 	for (i = 0; i < model->num_counters; ++i) {
-		int offset = i + si;
-		if (multiplex[offset].addr) {
-			rdmsrl(multiplex[offset].addr,
-			       multiplex[offset].saved);
-		}
+		int virt = op_x86_phys_to_virt(i);
+		if (multiplex[virt].addr)
+			rdmsrl(multiplex[virt].addr, multiplex[virt].saved);
 	}
 }
 
 static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
 {
-	unsigned int si = __get_cpu_var(switch_index);
 	struct op_msr *multiplex = msrs->multiplex;
-	unsigned int i;
+	int i;
 
 	for (i = 0; i < model->num_counters; ++i) {
-		int offset = i + si;
-		if (multiplex[offset].addr) {
-			wrmsrl(multiplex[offset].addr,
-			       multiplex[offset].saved);
-		}
+		int virt = op_x86_phys_to_virt(i);
+		if (multiplex[virt].addr)
+			wrmsrl(multiplex[virt].addr, multiplex[virt].saved);
 	}
 }
 

commit 6bfccd099c2841e1c42530f1b6d2553bfa13be3a
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 19:23:50 2009 +0200

    x86/oprofile: Fix initialization of switch_index
    
    Variable switch_index must be initialized for each cpu. This patch
    fixes the initialization by moving it to the per-cpu init function
    nmi_cpu_setup().
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 8cd4658370be..b211d335e075 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -160,7 +160,7 @@ static int allocate_msrs(void)
 
 #ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
 
-static void nmi_setup_cpu_mux(struct op_msrs const * const msrs)
+static void nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs)
 {
 	int i;
 	struct op_msr *multiplex = msrs->multiplex;
@@ -173,8 +173,15 @@ static void nmi_setup_cpu_mux(struct op_msrs const * const msrs)
 			multiplex[i].saved = 0;
 		}
 	}
+
+	per_cpu(switch_index, cpu) = 0;
 }
 
+#else
+
+static inline void
+nmi_cpu_setup_mux(int cpu, struct op_msrs const * const msrs) { }
+
 #endif
 
 static void nmi_cpu_setup(void *dummy)
@@ -184,9 +191,7 @@ static void nmi_cpu_setup(void *dummy)
 	nmi_cpu_save_registers(msrs);
 	spin_lock(&oprofilefs_lock);
 	model->setup_ctrs(model, msrs);
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-	nmi_setup_cpu_mux(msrs);
-#endif
+	nmi_cpu_setup_mux(cpu, msrs);
 	spin_unlock(&oprofilefs_lock);
 	per_cpu(saved_lvtpc, cpu) = apic_read(APIC_LVTPC);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
@@ -662,9 +667,6 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	register_cpu_notifier(&oprofile_cpu_nb);
 #endif
 	/* default values, can be overwritten by model */
-#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-	__raw_get_cpu_var(switch_index) = 0;
-#endif
 	ops->create_files	= nmi_create_files;
 	ops->setup		= nmi_setup;
 	ops->shutdown		= nmi_shutdown;

commit 82a225283fb0d9438549595d9e6f3ecc42b42ad6
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 16:29:34 2009 +0200

    x86/oprofile: Use per_cpu() instead of __get_cpu_var()
    
    __get_cpu_var() calls smp_processor_id(). When the cpu id is already
    known, instead use per_cpu() to avoid generating the id again.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index e54f6a0b35ac..8cd4658370be 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -294,7 +294,7 @@ static void nmi_cpu_shutdown(void *dummy)
 {
 	unsigned int v;
 	int cpu = smp_processor_id();
-	struct op_msrs *msrs = &__get_cpu_var(cpu_msrs);
+	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
 
 	/* restoring APIC_LVTPC can trigger an apic error because the delivery
 	 * mode and vector nr combination can be illegal. That's by design: on
@@ -307,7 +307,7 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTERR, v);
 	nmi_cpu_restore_registers(msrs);
 #ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
-	__get_cpu_var(switch_index) = 0;
+	per_cpu(switch_index, cpu) = 0;
 #endif
 }
 

commit 4d4036e0e7299c6cbb2d2421b4b30b7a409ce61a
Author: Jason Yeh <jason.yeh@amd.com>
Date:   Wed Jul 8 13:49:38 2009 +0200

    oprofile: Implement performance counter multiplexing
    
    The number of hardware counters is limited. The multiplexing feature
    enables OProfile to gather more events than counters are provided by
    the hardware. This is realized by switching between events at an user
    specified time interval.
    
    A new file (/dev/oprofile/time_slice) is added for the user to specify
    the timer interval in ms. If the number of events to profile is higher
    than the number of hardware counters available, the patch will
    schedule a work queue that switches the event counter and re-writes
    the different sets of values into it. The switching mechanism needs to
    be implemented for each architecture to support multiplexing. This
    patch only implements AMD CPU support, but multiplexing can be easily
    extended for other models and architectures.
    
    There are follow-on patches that rework parts of this patch.
    
    Signed-off-by: Jason Yeh <jason.yeh@amd.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index fca8dc94531e..e54f6a0b35ac 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -1,11 +1,14 @@
 /**
  * @file nmi_int.c
  *
- * @remark Copyright 2002-2008 OProfile authors
+ * @remark Copyright 2002-2009 OProfile authors
  * @remark Read the file COPYING
  *
  * @author John Levon <levon@movementarian.org>
  * @author Robert Richter <robert.richter@amd.com>
+ * @author Barry Kasindorf <barry.kasindorf@amd.com>
+ * @author Jason Yeh <jason.yeh@amd.com>
+ * @author Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
  */
 
 #include <linux/init.h>
@@ -24,6 +27,12 @@
 #include "op_counter.h"
 #include "op_x86_model.h"
 
+
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+DEFINE_PER_CPU(int, switch_index);
+#endif
+
+
 static struct op_x86_model_spec const *model;
 static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
 static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
@@ -31,6 +40,13 @@ static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 /* 0 == registered but off, 1 == registered and on */
 static int nmi_enabled = 0;
 
+
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+extern atomic_t multiplex_counter;
+#endif
+
+struct op_counter_config counter_config[OP_MAX_COUNTER];
+
 /* common functions */
 
 u64 op_x86_get_ctrl(struct op_x86_model_spec const *model,
@@ -95,6 +111,11 @@ static void free_msrs(void)
 		per_cpu(cpu_msrs, i).counters = NULL;
 		kfree(per_cpu(cpu_msrs, i).controls);
 		per_cpu(cpu_msrs, i).controls = NULL;
+
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+		kfree(per_cpu(cpu_msrs, i).multiplex);
+		per_cpu(cpu_msrs, i).multiplex = NULL;
+#endif
 	}
 }
 
@@ -103,6 +124,9 @@ static int allocate_msrs(void)
 	int success = 1;
 	size_t controls_size = sizeof(struct op_msr) * model->num_controls;
 	size_t counters_size = sizeof(struct op_msr) * model->num_counters;
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+	size_t multiplex_size = sizeof(struct op_msr) * model->num_virt_counters;
+#endif
 
 	int i;
 	for_each_possible_cpu(i) {
@@ -118,6 +142,14 @@ static int allocate_msrs(void)
 			success = 0;
 			break;
 		}
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+		per_cpu(cpu_msrs, i).multiplex =
+				kmalloc(multiplex_size, GFP_KERNEL);
+		if (!per_cpu(cpu_msrs, i).multiplex) {
+			success = 0;
+			break;
+		}
+#endif
 	}
 
 	if (!success)
@@ -126,6 +158,25 @@ static int allocate_msrs(void)
 	return success;
 }
 
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+
+static void nmi_setup_cpu_mux(struct op_msrs const * const msrs)
+{
+	int i;
+	struct op_msr *multiplex = msrs->multiplex;
+
+	for (i = 0; i < model->num_virt_counters; ++i) {
+		if (counter_config[i].enabled) {
+			multiplex[i].saved = -(u64)counter_config[i].count;
+		} else {
+			multiplex[i].addr  = 0;
+			multiplex[i].saved = 0;
+		}
+	}
+}
+
+#endif
+
 static void nmi_cpu_setup(void *dummy)
 {
 	int cpu = smp_processor_id();
@@ -133,6 +184,9 @@ static void nmi_cpu_setup(void *dummy)
 	nmi_cpu_save_registers(msrs);
 	spin_lock(&oprofilefs_lock);
 	model->setup_ctrs(model, msrs);
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+	nmi_setup_cpu_mux(msrs);
+#endif
 	spin_unlock(&oprofilefs_lock);
 	per_cpu(saved_lvtpc, cpu) = apic_read(APIC_LVTPC);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
@@ -173,14 +227,52 @@ static int nmi_setup(void)
 			memcpy(per_cpu(cpu_msrs, cpu).controls,
 				per_cpu(cpu_msrs, 0).controls,
 				sizeof(struct op_msr) * model->num_controls);
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+			memcpy(per_cpu(cpu_msrs, cpu).multiplex,
+				per_cpu(cpu_msrs, 0).multiplex,
+				sizeof(struct op_msr) * model->num_virt_counters);
+#endif
 		}
-
 	}
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;
 	return 0;
 }
 
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+
+static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs)
+{
+	unsigned int si = __get_cpu_var(switch_index);
+	struct op_msr *multiplex = msrs->multiplex;
+	unsigned int i;
+
+	for (i = 0; i < model->num_counters; ++i) {
+		int offset = i + si;
+		if (multiplex[offset].addr) {
+			rdmsrl(multiplex[offset].addr,
+			       multiplex[offset].saved);
+		}
+	}
+}
+
+static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
+{
+	unsigned int si = __get_cpu_var(switch_index);
+	struct op_msr *multiplex = msrs->multiplex;
+	unsigned int i;
+
+	for (i = 0; i < model->num_counters; ++i) {
+		int offset = i + si;
+		if (multiplex[offset].addr) {
+			wrmsrl(multiplex[offset].addr,
+			       multiplex[offset].saved);
+		}
+	}
+}
+
+#endif
+
 static void nmi_cpu_restore_registers(struct op_msrs *msrs)
 {
 	struct op_msr *counters = msrs->counters;
@@ -214,6 +306,9 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
 	nmi_cpu_restore_registers(msrs);
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+	__get_cpu_var(switch_index) = 0;
+#endif
 }
 
 static void nmi_shutdown(void)
@@ -252,16 +347,15 @@ static void nmi_stop(void)
 	on_each_cpu(nmi_cpu_stop, NULL, 1);
 }
 
-struct op_counter_config counter_config[OP_MAX_COUNTER];
-
 static int nmi_create_files(struct super_block *sb, struct dentry *root)
 {
 	unsigned int i;
 
-	for (i = 0; i < model->num_counters; ++i) {
+	for (i = 0; i < model->num_virt_counters; ++i) {
 		struct dentry *dir;
 		char buf[4];
 
+#ifndef CONFIG_OPROFILE_EVENT_MULTIPLEX
 		/* quick little hack to _not_ expose a counter if it is not
 		 * available for use.  This should protect userspace app.
 		 * NOTE:  assumes 1:1 mapping here (that counters are organized
@@ -269,6 +363,7 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 		 */
 		if (unlikely(!avail_to_resrv_perfctr_nmi_bit(i)))
 			continue;
+#endif /* CONFIG_OPROFILE_EVENT_MULTIPLEX */
 
 		snprintf(buf,  sizeof(buf), "%d", i);
 		dir = oprofilefs_mkdir(sb, root, buf);
@@ -283,6 +378,57 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 	return 0;
 }
 
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+
+static void nmi_cpu_switch(void *dummy)
+{
+	int cpu = smp_processor_id();
+	int si = per_cpu(switch_index, cpu);
+	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
+
+	nmi_cpu_stop(NULL);
+	nmi_cpu_save_mpx_registers(msrs);
+
+	/* move to next set */
+	si += model->num_counters;
+	if ((si > model->num_virt_counters) || (counter_config[si].count == 0))
+		per_cpu(switch_index, cpu) = 0;
+	else
+		per_cpu(switch_index, cpu) = si;
+
+	model->switch_ctrl(model, msrs);
+	nmi_cpu_restore_mpx_registers(msrs);
+
+	nmi_cpu_start(NULL);
+}
+
+
+/*
+ * Quick check to see if multiplexing is necessary.
+ * The check should be sufficient since counters are used
+ * in ordre.
+ */
+static int nmi_multiplex_on(void)
+{
+	return counter_config[model->num_counters].count ? 0 : -EINVAL;
+}
+
+static int nmi_switch_event(void)
+{
+	if (!model->switch_ctrl)
+		return -ENOSYS;		/* not implemented */
+	if (nmi_multiplex_on() < 0)
+		return -EINVAL;		/* not necessary */
+
+	on_each_cpu(nmi_cpu_switch, NULL, 1);
+
+	atomic_inc(&multiplex_counter);
+
+	return 0;
+}
+
+#endif
+
 #ifdef CONFIG_SMP
 static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
 				 void *data)
@@ -516,12 +662,18 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	register_cpu_notifier(&oprofile_cpu_nb);
 #endif
 	/* default values, can be overwritten by model */
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+	__raw_get_cpu_var(switch_index) = 0;
+#endif
 	ops->create_files	= nmi_create_files;
 	ops->setup		= nmi_setup;
 	ops->shutdown		= nmi_shutdown;
 	ops->start		= nmi_start;
 	ops->stop		= nmi_stop;
 	ops->cpu_type		= cpu_type;
+#ifdef CONFIG_OPROFILE_EVENT_MULTIPLEX
+	ops->switch_events	= nmi_switch_event;
+#endif
 
 	if (model->init)
 		ret = model->init(ops);

commit 6e63ea4b0b14ff5fb8a3ca704fcda7d28b95f079
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 7 19:25:39 2009 +0200

    x86/oprofile: Whitespaces changes only
    
    This patch fixes whitespace changes of code that will be touched in
    follow-on patches.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 25da1e17815d..fca8dc94531e 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -516,12 +516,12 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	register_cpu_notifier(&oprofile_cpu_nb);
 #endif
 	/* default values, can be overwritten by model */
-	ops->create_files = nmi_create_files;
-	ops->setup = nmi_setup;
-	ops->shutdown = nmi_shutdown;
-	ops->start = nmi_start;
-	ops->stop = nmi_stop;
-	ops->cpu_type = cpu_type;
+	ops->create_files	= nmi_create_files;
+	ops->setup		= nmi_setup;
+	ops->shutdown		= nmi_shutdown;
+	ops->start		= nmi_start;
+	ops->stop		= nmi_stop;
+	ops->cpu_type		= cpu_type;
 
 	if (model->init)
 		ret = model->init(ops);

commit 44ab9a6b0e909145d42615493952fe986b1ce5c2
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 9 18:33:02 2009 +0200

    x86/oprofile: Rework and simplify nmi_cpu_setup()
    
    This patch removes the function nmi_save_registers(). Per-cpu code is
    now executed only in the function nmi_cpu_setup().  Also, it renames
    the per-cpu function nmi_restore_registers() to
    nmi_cpu_restore_registers().
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 93df76dd60f4..25da1e17815d 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -87,13 +87,6 @@ static void nmi_cpu_save_registers(struct op_msrs *msrs)
 	}
 }
 
-static void nmi_save_registers(void *dummy)
-{
-	int cpu = smp_processor_id();
-	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
-	nmi_cpu_save_registers(msrs);
-}
-
 static void free_msrs(void)
 {
 	int i;
@@ -137,6 +130,7 @@ static void nmi_cpu_setup(void *dummy)
 {
 	int cpu = smp_processor_id();
 	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
+	nmi_cpu_save_registers(msrs);
 	spin_lock(&oprofilefs_lock);
 	model->setup_ctrs(model, msrs);
 	spin_unlock(&oprofilefs_lock);
@@ -182,13 +176,12 @@ static int nmi_setup(void)
 		}
 
 	}
-	on_each_cpu(nmi_save_registers, NULL, 1);
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;
 	return 0;
 }
 
-static void nmi_restore_registers(struct op_msrs *msrs)
+static void nmi_cpu_restore_registers(struct op_msrs *msrs)
 {
 	struct op_msr *counters = msrs->counters;
 	struct op_msr *controls = msrs->controls;
@@ -220,7 +213,7 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTERR, v | APIC_LVT_MASKED);
 	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
-	nmi_restore_registers(msrs);
+	nmi_cpu_restore_registers(msrs);
 }
 
 static void nmi_shutdown(void)

commit debc6a6927dcd833a30750b07a4c2b456b71f1be
Merge: 21e70878215f 6847e154e3cd 4f6e1fe1d8ba
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 14 15:20:44 2009 +0200

    Merge commit 'v2.6.31-rc3'; commit 'tip/oprofile' into oprofile/core
    
    Conflicts:
            drivers/oprofile/oprofile_stats.c
            drivers/usb/otg/Kconfig
            drivers/usb/otg/Makefile
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

commit 8d7ff4f2a0b22b7d6d7bc3982257d1dadea22824
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jun 23 11:48:14 2009 +0200

    x86/oprofile: rename kernel parameter for architectural perfmon to arch_perfmon
    
    The short name of the achitecture is 'arch_perfmon'. This patch
    changes the kernel parameter to use this name.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index b07dd8d0b321..89b9a5cd63da 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -390,7 +390,7 @@ static int __init p4_init(char **cpu_type)
 static int force_arch_perfmon;
 static int force_cpu_type(const char *str, struct kernel_param *kp)
 {
-	if (!strcmp(str, "archperfmon")) {
+	if (!strcmp(str, "arch_perfmon")) {
 		force_arch_perfmon = 1;
 		printk(KERN_INFO "oprofile: forcing architectural perfmon\n");
 	}

commit 802070f5474af1a49435a9528aede47bb18abd47
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Jun 12 18:32:07 2009 +0200

    x86/oprofile: fix initialization of arch_perfmon for core_i7
    
    Commit:
    
     e419294 x86/oprofile: moving arch_perfmon counter setup to op_x86_model_spec.init
    
    introduced a bug in the initialization of core_i7 leading to the
    incorrect model setup to &op_ppro_spec. This patch fixes this.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 7826dfcc8428..28ee490c1b80 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -406,6 +406,7 @@ module_param_call(cpu_type, force_cpu_type, NULL, NULL, 0);
 static int __init ppro_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
+	struct op_x86_model_spec const *spec = &op_ppro_spec;	/* default */
 
 	if (force_arch_perfmon && cpu_has_arch_perfmon)
 		return 0;
@@ -432,7 +433,7 @@ static int __init ppro_init(char **cpu_type)
 		*cpu_type = "i386/core_2";
 		break;
 	case 26:
-		model = &op_arch_perfmon_spec;
+		spec = &op_arch_perfmon_spec;
 		*cpu_type = "i386/core_i7";
 		break;
 	case 28:
@@ -443,7 +444,7 @@ static int __init ppro_init(char **cpu_type)
 		return 0;
 	}
 
-	model = &op_ppro_spec;
+	model = spec;
 	return 1;
 }
 

commit 1241eb8f136bf3ea409f61590e7663465906d158
Merge: 51563a0e5650 940010c5a314
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Jun 12 17:58:48 2009 +0200

    Merge commit 'tip/perfcounters-for-linus' into oprofile/master
    
    Conflicts:
            arch/x86/oprofile/op_model_ppro.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

commit 1a245c45343651a87ff63afc5ddeb8e24d731835
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Jun 5 15:54:24 2009 +0200

    x86/oprofile: remove some local variables in MSR save/restore functions
    
    The patch removes some local variables in these functions.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 3b84b789de0b..80b63d5db509 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -71,18 +71,16 @@ static int profile_exceptions_notify(struct notifier_block *self,
 
 static void nmi_cpu_save_registers(struct op_msrs *msrs)
 {
-	unsigned int const nr_ctrs = model->num_counters;
-	unsigned int const nr_ctrls = model->num_controls;
 	struct op_msr *counters = msrs->counters;
 	struct op_msr *controls = msrs->controls;
 	unsigned int i;
 
-	for (i = 0; i < nr_ctrs; ++i) {
+	for (i = 0; i < model->num_counters; ++i) {
 		if (counters[i].addr)
 			rdmsrl(counters[i].addr, counters[i].saved);
 	}
 
-	for (i = 0; i < nr_ctrls; ++i) {
+	for (i = 0; i < model->num_controls; ++i) {
 		if (controls[i].addr)
 			rdmsrl(controls[i].addr, controls[i].saved);
 	}
@@ -191,18 +189,16 @@ static int nmi_setup(void)
 
 static void nmi_restore_registers(struct op_msrs *msrs)
 {
-	unsigned int const nr_ctrs = model->num_counters;
-	unsigned int const nr_ctrls = model->num_controls;
 	struct op_msr *counters = msrs->counters;
 	struct op_msr *controls = msrs->controls;
 	unsigned int i;
 
-	for (i = 0; i < nr_ctrls; ++i) {
+	for (i = 0; i < model->num_controls; ++i) {
 		if (controls[i].addr)
 			wrmsrl(controls[i].addr, controls[i].saved);
 	}
 
-	for (i = 0; i < nr_ctrs; ++i) {
+	for (i = 0; i < model->num_counters; ++i) {
 		if (counters[i].addr)
 			wrmsrl(counters[i].addr, counters[i].saved);
 	}

commit 95e74e62c1540b1115fe8cec5b592f22960f2bb2
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jun 3 19:09:27 2009 +0200

    x86/oprofile: use 64 bit values to save MSR states
    
    This patch removes struct op_saved_msr and replaces it by an u64
    variable. This makes code easier and it is possible to use 64 bit MSR
    functions.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 388ee15e0e42..3b84b789de0b 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -78,19 +78,13 @@ static void nmi_cpu_save_registers(struct op_msrs *msrs)
 	unsigned int i;
 
 	for (i = 0; i < nr_ctrs; ++i) {
-		if (counters[i].addr) {
-			rdmsr(counters[i].addr,
-				counters[i].saved.low,
-				counters[i].saved.high);
-		}
+		if (counters[i].addr)
+			rdmsrl(counters[i].addr, counters[i].saved);
 	}
 
 	for (i = 0; i < nr_ctrls; ++i) {
-		if (controls[i].addr) {
-			rdmsr(controls[i].addr,
-				controls[i].saved.low,
-				controls[i].saved.high);
-		}
+		if (controls[i].addr)
+			rdmsrl(controls[i].addr, controls[i].saved);
 	}
 }
 
@@ -204,19 +198,13 @@ static void nmi_restore_registers(struct op_msrs *msrs)
 	unsigned int i;
 
 	for (i = 0; i < nr_ctrls; ++i) {
-		if (controls[i].addr) {
-			wrmsr(controls[i].addr,
-				controls[i].saved.low,
-				controls[i].saved.high);
-		}
+		if (controls[i].addr)
+			wrmsrl(controls[i].addr, controls[i].saved);
 	}
 
 	for (i = 0; i < nr_ctrs; ++i) {
-		if (counters[i].addr) {
-			wrmsr(counters[i].addr,
-				counters[i].saved.low,
-				counters[i].saved.high);
-		}
+		if (counters[i].addr)
+			wrmsrl(counters[i].addr, counters[i].saved);
 	}
 }
 

commit 3370d358569755625aba4d9a846a040ce691d9ed
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon May 25 15:10:32 2009 +0200

    x86/oprofile: replace macros to calculate control register
    
    This patch introduces op_x86_get_ctrl() to calculate the value of the
    performance control register. This is generic code usable for all
    models. The event and reserved masks are model specific and stored in
    struct op_x86_model_spec. 64 bit MSR functions are used now. The patch
    removes many hard to read macros used for ctrl calculation.
    
    The function op_x86_get_ctrl() is common code and the first step to
    further merge performance counter implementations for x86 models.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index c31f87bbf436..388ee15e0e42 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -31,6 +31,26 @@ static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 /* 0 == registered but off, 1 == registered and on */
 static int nmi_enabled = 0;
 
+/* common functions */
+
+u64 op_x86_get_ctrl(struct op_x86_model_spec const *model,
+		    struct op_counter_config *counter_config)
+{
+	u64 val = 0;
+	u16 event = (u16)counter_config->event;
+
+	val |= ARCH_PERFMON_EVENTSEL_INT;
+	val |= counter_config->user ? ARCH_PERFMON_EVENTSEL_USR : 0;
+	val |= counter_config->kernel ? ARCH_PERFMON_EVENTSEL_OS : 0;
+	val |= (counter_config->unit_mask & 0xFF) << 8;
+	event &= model->event_mask ? model->event_mask : 0xFF;
+	val |= event & 0xFF;
+	val |= (event & 0x0F00) << 24;
+
+	return val;
+}
+
+
 static int profile_exceptions_notify(struct notifier_block *self,
 				     unsigned long val, void *data)
 {

commit ef8828ddf828174785421af67c281144d4b8e796
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon May 25 19:31:44 2009 +0200

    x86/oprofile: pass the model to setup_ctrs() functions
    
    In follow-on patches the setup_ctrs() functions will need data that
    describes the model. This patch extends the function argument list to
    pass a pointer of the model to these function.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index ae0ab03959b4..c31f87bbf436 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -125,7 +125,7 @@ static void nmi_cpu_setup(void *dummy)
 	int cpu = smp_processor_id();
 	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
 	spin_lock(&oprofilefs_lock);
-	model->setup_ctrs(msrs);
+	model->setup_ctrs(model, msrs);
 	spin_unlock(&oprofilefs_lock);
 	per_cpu(saved_lvtpc, cpu) = apic_read(APIC_LVTPC);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);

commit d20f24c66011f8a397bca6c5d1a6a7c7e612d2d7
Author: Robert Richter <robert.richter@amd.com>
Date:   Sun Jan 11 13:01:16 2009 +0100

    x86/oprofile: simplify AMD cpu init code
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index dd8515301fbf..ae0ab03959b4 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -460,27 +460,26 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		/* Needs to be at least an Athlon (or hammer in 32bit mode) */
 
 		switch (family) {
-		default:
-			return -ENODEV;
 		case 6:
-			model = &op_amd_spec;
 			cpu_type = "i386/athlon";
 			break;
 		case 0xf:
-			model = &op_amd_spec;
-			/* Actually it could be i386/hammer too, but give
-			 user space an consistent name. */
+			/*
+			 * Actually it could be i386/hammer too, but
+			 * give user space an consistent name.
+			 */
 			cpu_type = "x86-64/hammer";
 			break;
 		case 0x10:
-			model = &op_amd_spec;
 			cpu_type = "x86-64/family10";
 			break;
 		case 0x11:
-			model = &op_amd_spec;
 			cpu_type = "x86-64/family11h";
 			break;
+		default:
+			return -ENODEV;
 		}
+		model = &op_amd_spec;
 		break;
 
 	case X86_VENDOR_INTEL:

commit 940010c5a314a7bd9b498593bc6ba1718ac5aec5
Merge: 8dc8e5e8bc0c 991ec02cdca3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 17:55:42 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/irqinit.c
            arch/x86/kernel/irqinit_64.c
            arch/x86/kernel/traps.c
            arch/x86/mm/fault.c
            include/linux/sched.h
            kernel/exit.c

commit e419294ed3c98cccc145202e4fe165bfd8099d63
Author: Robert Richter <robert.richter@amd.com>
Date:   Sun Oct 12 15:12:34 2008 -0400

    x86/oprofile: moving arch_perfmon counter setup to op_x86_model_spec.init
    
    The function arch_perfmon_init() in nmi_int.c is model specific. This
    patch moves it to op_model_ppro.c by using the init function pointer
    in struct op_x86_model_spec.
    
    Cc: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 3b285e656e27..dd8515301fbf 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -427,7 +427,7 @@ static int __init ppro_init(char **cpu_type)
 		*cpu_type = "i386/core_2";
 		break;
 	case 26:
-		arch_perfmon_setup_counters();
+		model = &op_arch_perfmon_spec;
 		*cpu_type = "i386/core_i7";
 		break;
 	case 28:
@@ -442,16 +442,6 @@ static int __init ppro_init(char **cpu_type)
 	return 1;
 }
 
-static int __init arch_perfmon_init(char **cpu_type)
-{
-	if (!cpu_has_arch_perfmon)
-		return 0;
-	*cpu_type = "i386/arch_perfmon";
-	model = &op_arch_perfmon_spec;
-	arch_perfmon_setup_counters();
-	return 1;
-}
-
 /* in order to get sysfs right */
 static int using_nmi;
 
@@ -509,8 +499,15 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 			break;
 		}
 
-		if (!cpu_type && !arch_perfmon_init(&cpu_type))
+		if (cpu_type)
+			break;
+
+		if (!cpu_has_arch_perfmon)
 			return -ENODEV;
+
+		/* use arch perfmon as fallback */
+		cpu_type = "i386/arch_perfmon";
+		model = &op_arch_perfmon_spec;
 		break;
 
 	default:

commit 7e4e0bd50e80df2fe5501f48f872448376cdd997
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed May 6 12:10:23 2009 +0200

    oprofile: introduce module_param oprofile.cpu_type
    
    This patch removes module_param oprofile.force_arch_perfmon and
    introduces oprofile.cpu_type=archperfmon instead. This new parameter
    can be reused for other models and architectures.
    
    Currently only archperfmon is supported.
    
    Cc: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 3308147182ae..3b285e656e27 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -386,8 +386,17 @@ static int __init p4_init(char **cpu_type)
 	return 0;
 }
 
-int force_arch_perfmon;
-module_param(force_arch_perfmon, int, 0);
+static int force_arch_perfmon;
+static int force_cpu_type(const char *str, struct kernel_param *kp)
+{
+	if (!strcmp(str, "archperfmon")) {
+		force_arch_perfmon = 1;
+		printk(KERN_INFO "oprofile: forcing architectural perfmon\n");
+	}
+
+	return 0;
+}
+module_param_call(cpu_type, force_cpu_type, NULL, NULL, 0);
 
 static int __init ppro_init(char **cpu_type)
 {

commit 6adf406f0a0eaf37251018d15f51e93f5b538ee6
Author: Andi Kleen <andi@firstfloor.org>
Date:   Mon Apr 27 17:44:13 2009 +0200

    oprofile: add support for Core i7 and Atom
    
    The registers are about the same as other Family 6 CPUs
    so we only need to add detection.
    
    I'm not completely happy with calling Nehalem Core i7 because
    there will be undoubtedly other Nehalem based CPUs
    in the future with different marketing names, but it's
    the best we got for now.
    
    Requires updated oprofile userland for the new event files.
    
    If you don't want to update right now you can also use
    oprofile.force_arch_perfmon=1 (added in the next patch) with 0.9.4
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index f472c0c48a3e..3308147182ae 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -417,6 +417,13 @@ static int __init ppro_init(char **cpu_type)
 	case 15: case 23:
 		*cpu_type = "i386/core_2";
 		break;
+	case 26:
+		arch_perfmon_setup_counters();
+		*cpu_type = "i386/core_i7";
+		break;
+	case 28:
+		*cpu_type = "i386/atom";
+		break;
 	default:
 		/* Unknown */
 		return 0;

commit 1f3d7b60691993d8d368d8dd7d5d85871d41e8f5
Author: Andi Kleen <andi@firstfloor.org>
Date:   Mon Apr 27 17:44:12 2009 +0200

    oprofile: remove undocumented oprofile.p4force option
    
    There are no new P4s and the oprofile code knows about all existing
    ones, so we don't really need the p4force option anymore.
    
    Remove it.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index e5171c99e157..f472c0c48a3e 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -356,14 +356,11 @@ static void exit_sysfs(void)
 #define exit_sysfs() do { } while (0)
 #endif /* CONFIG_PM */
 
-static int p4force;
-module_param(p4force, int, 0);
-
 static int __init p4_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
 
-	if (!p4force && (cpu_model > 6 || cpu_model == 5))
+	if (cpu_model > 6 || cpu_model == 5)
 		return 0;
 
 #ifndef CONFIG_SMP

commit 1dcdb5a9e7c235e6e80f1f4d5b8247b3e5347e48
Author: Andi Kleen <andi@firstfloor.org>
Date:   Mon Apr 27 17:44:11 2009 +0200

    oprofile: re-add force_arch_perfmon option
    
    This re-adds the force_arch_perfmon option that was in the original
    arch perfmon patchkit. Originally this was rejected in favour
    of a generalized perfmon=name option, but it turned out implementing
    the later in a reliable way is hard (and it would have been easy
    to crash the kernel if a user gets it wrong)
    
    But now Atom and Core i7 support being readded a user would
    need to update their oprofile userland to beyond 0.9.4 to use oprofile again
    on Atom or Core i7.
    
    To avoid this problem readd the force_arch_perfmon option.
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 202864ad49a7..e5171c99e157 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -389,10 +389,16 @@ static int __init p4_init(char **cpu_type)
 	return 0;
 }
 
+int force_arch_perfmon;
+module_param(force_arch_perfmon, int, 0);
+
 static int __init ppro_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
 
+	if (force_arch_perfmon && cpu_has_arch_perfmon)
+		return 0;
+
 	switch (cpu_model) {
 	case 0 ... 2:
 		*cpu_type = "i386/ppro";

commit 5b75af0a02fcf3b8899f38ff6f22164c5d8e2fdd
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Feb 4 17:11:34 2009 +0100

    perfcounters: fix "perf counters kill oprofile" bug
    
    With oprofile as a module, and unloaded by profiling script,
    both oprofile and kerneltop work fine.. unless you leave kerneltop
    running when you start profiling, then you may see badness.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 202864ad49a7..c638685136e1 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -40,8 +40,9 @@ static int profile_exceptions_notify(struct notifier_block *self,
 
 	switch (val) {
 	case DIE_NMI:
-		if (model->check_ctrs(args->regs, &per_cpu(cpu_msrs, cpu)))
-			ret = NOTIFY_STOP;
+	case DIE_NMI_IPI:
+		model->check_ctrs(args->regs, &per_cpu(cpu_msrs, cpu));
+		ret = NOTIFY_STOP;
 		break;
 	default:
 		break;
@@ -134,7 +135,7 @@ static void nmi_cpu_setup(void *dummy)
 static struct notifier_block profile_exceptions_nb = {
 	.notifier_call = profile_exceptions_notify,
 	.next = NULL,
-	.priority = 0
+	.priority = 2
 };
 
 static int nmi_setup(void)

commit 3d337c653c94be50f11a45fb14a2afa8a8a1a618
Author: William Cohen <wcohen@redhat.com>
Date:   Sun Nov 30 15:39:10 2008 -0500

    x86/oprofile: fix Intel cpu family 6 detection
    
    Alan Jenkins wrote:
    > This is on an EeePC 701, /proc/cpuinfo as attached.
    >
    > Is this expected?  Will the next release work?
    >
    > Thanks, Alan
    >
    > # opcontrol --setup --no-vmlinux
    > cpu_type 'unset' is not valid
    > you should upgrade oprofile or force the use of timer mode
    >
    > # opcontrol -v
    > opcontrol: oprofile 0.9.4 compiled on Nov 29 2008 22:44:10
    >
    > # cat /dev/oprofile/cpu_type
    > i386/p6
    > # uname -r
    > 2.6.28-rc6eeepc
    
    Hi Alan,
    
    Looking at the kernel driver code for oprofile it can return the "i386/p6" for
    the cpu_type. However, looking at the user-space oprofile code there isn't the
    matching entry in libop/op_cpu_type.c or the events/unit_mask files in
    events/i386 directory.
    
    The Intel AP-485 says this is a "Intel Pentium M processor model D". Seems like
    the oprofile kernel driver should be identifying the processor as "i386/p6_mobile"
    
    The driver identification code doesn't look quite right in nmi_init.c
    
    http://git.kernel.org/?p=linux/kernel/git/sfr/linux-next.git;a=blob;f=arch/x86/oprofile/nmi_int.c;h=022cd41ea9b4106e5884277096e80e9088a7c7a9;hb=HEAD
    
    has:
    
    409         case 10 ... 13:
    410                 *cpu_type = "i386/p6";
    411                 break;
    
    Referring to the Intel AP-485:
    case 10 and 11 should produce "i386/piii"
    case 13 should produce "i386/p6_mobile"
    
    I didn't see anything for case 12.
    
    Something like the attached patch. I don't have a celeron machine to verify that
    changes in this area of the kernel fix thing.
    
    -Will
    
    Signed-off-by: William Cohen <wcohen@redhat.com>
    Tested-by: Alan Jenkins <alan-jenkins@tuffmail.co.uk>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 022cd41ea9b4..202864ad49a7 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -401,14 +401,13 @@ static int __init ppro_init(char **cpu_type)
 		*cpu_type = "i386/pii";
 		break;
 	case 6 ... 8:
+	case 10 ... 11:
 		*cpu_type = "i386/piii";
 		break;
 	case 9:
+	case 13:
 		*cpu_type = "i386/p6_mobile";
 		break;
-	case 10 ... 13:
-		*cpu_type = "i386/p6";
-		break;
 	case 14:
 		*cpu_type = "i386/core";
 		break;

commit 5a289395bf753f8a318d3a5fa335a757c16c0183
Merge: 5f87dfb79f82 59512900baab
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Oct 15 22:19:41 2008 +0200

    Merge branch 'oprofile/x86-oprofile-for-tip' into oprofile/oprofile-for-tip
    
    Conflicts:
            arch/x86/oprofile/op_model_ppro.c

commit 69046d430417c30f48867fc52e892c9050b3a29b
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Sep 5 12:17:40 2008 +0200

    x86/oprofile: reordering functions in nmi_int.c
    
    No functional changes. The intension is to remove static function
    declarations.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 57f6c9088081..370d832f398d 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -28,85 +28,9 @@ static struct op_x86_model_spec const *model;
 static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
 static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 
-static int nmi_start(void);
-static void nmi_stop(void);
-static void nmi_cpu_start(void *dummy);
-static void nmi_cpu_stop(void *dummy);
-
 /* 0 == registered but off, 1 == registered and on */
 static int nmi_enabled = 0;
 
-#ifdef CONFIG_SMP
-static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
-				 void *data)
-{
-	int cpu = (unsigned long)data;
-	switch (action) {
-	case CPU_DOWN_FAILED:
-	case CPU_ONLINE:
-		smp_call_function_single(cpu, nmi_cpu_start, NULL, 0);
-		break;
-	case CPU_DOWN_PREPARE:
-		smp_call_function_single(cpu, nmi_cpu_stop, NULL, 1);
-		break;
-	}
-	return NOTIFY_DONE;
-}
-
-static struct notifier_block oprofile_cpu_nb = {
-	.notifier_call = oprofile_cpu_notifier
-};
-#endif
-
-#ifdef CONFIG_PM
-
-static int nmi_suspend(struct sys_device *dev, pm_message_t state)
-{
-	/* Only one CPU left, just stop that one */
-	if (nmi_enabled == 1)
-		nmi_cpu_stop(NULL);
-	return 0;
-}
-
-static int nmi_resume(struct sys_device *dev)
-{
-	if (nmi_enabled == 1)
-		nmi_cpu_start(NULL);
-	return 0;
-}
-
-static struct sysdev_class oprofile_sysclass = {
-	.name		= "oprofile",
-	.resume		= nmi_resume,
-	.suspend	= nmi_suspend,
-};
-
-static struct sys_device device_oprofile = {
-	.id	= 0,
-	.cls	= &oprofile_sysclass,
-};
-
-static int __init init_sysfs(void)
-{
-	int error;
-
-	error = sysdev_class_register(&oprofile_sysclass);
-	if (!error)
-		error = sysdev_register(&device_oprofile);
-	return error;
-}
-
-static void exit_sysfs(void)
-{
-	sysdev_unregister(&device_oprofile);
-	sysdev_class_unregister(&oprofile_sysclass);
-}
-
-#else
-#define init_sysfs() do { } while (0)
-#define exit_sysfs() do { } while (0)
-#endif /* CONFIG_PM */
-
 static int profile_exceptions_notify(struct notifier_block *self,
 				     unsigned long val, void *data)
 {
@@ -361,6 +285,77 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 	return 0;
 }
 
+#ifdef CONFIG_SMP
+static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
+				 void *data)
+{
+	int cpu = (unsigned long)data;
+	switch (action) {
+	case CPU_DOWN_FAILED:
+	case CPU_ONLINE:
+		smp_call_function_single(cpu, nmi_cpu_start, NULL, 0);
+		break;
+	case CPU_DOWN_PREPARE:
+		smp_call_function_single(cpu, nmi_cpu_stop, NULL, 1);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block oprofile_cpu_nb = {
+	.notifier_call = oprofile_cpu_notifier
+};
+#endif
+
+#ifdef CONFIG_PM
+
+static int nmi_suspend(struct sys_device *dev, pm_message_t state)
+{
+	/* Only one CPU left, just stop that one */
+	if (nmi_enabled == 1)
+		nmi_cpu_stop(NULL);
+	return 0;
+}
+
+static int nmi_resume(struct sys_device *dev)
+{
+	if (nmi_enabled == 1)
+		nmi_cpu_start(NULL);
+	return 0;
+}
+
+static struct sysdev_class oprofile_sysclass = {
+	.name		= "oprofile",
+	.resume		= nmi_resume,
+	.suspend	= nmi_suspend,
+};
+
+static struct sys_device device_oprofile = {
+	.id	= 0,
+	.cls	= &oprofile_sysclass,
+};
+
+static int __init init_sysfs(void)
+{
+	int error;
+
+	error = sysdev_class_register(&oprofile_sysclass);
+	if (!error)
+		error = sysdev_register(&device_oprofile);
+	return error;
+}
+
+static void exit_sysfs(void)
+{
+	sysdev_unregister(&device_oprofile);
+	sysdev_class_unregister(&oprofile_sysclass);
+}
+
+#else
+#define init_sysfs() do { } while (0)
+#define exit_sysfs() do { } while (0)
+#endif /* CONFIG_PM */
+
 static int p4force;
 module_param(p4force, int, 0);
 

commit b99170288421c79f0c2efa8b33e26e65f4bb7fb8
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 18 14:50:31 2008 +0200

    oprofile: Implement Intel architectural perfmon support
    
    Newer Intel CPUs (Core1+) have support for architectural
    events described in CPUID 0xA. See the IA32 SDM Vol3b.18 for details.
    
    The advantage of this is that it can be done without knowing about
    the specific CPU, because the CPU describes by itself what
    performance events are supported. This is only a fallback
    because only a limited set of 6 events are supported.
    This allows to do profiling on Nehalem and on Atom systems
    (later not tested)
    
    This patch implements support for that in oprofile's Intel
    Family 6 profiling module. It also has the advantage of supporting
    an arbitary number of events now as reported by the CPU.
    Also allow arbitary counter widths >32bit while we're at it.
    
    Requires a patched oprofile userland to support the new
    architecture.
    
    v2: update for latest oprofile tree
        remove force_arch_perfmon
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 1059f3fe6b1d..12d6f85084f1 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -429,6 +429,16 @@ static int __init ppro_init(char **cpu_type)
 	return 1;
 }
 
+static int __init arch_perfmon_init(char **cpu_type)
+{
+	if (!cpu_has_arch_perfmon)
+		return 0;
+	*cpu_type = "i386/arch_perfmon";
+	model = &op_arch_perfmon_spec;
+	arch_perfmon_setup_counters();
+	return 1;
+}
+
 /* in order to get sysfs right */
 static int using_nmi;
 
@@ -436,7 +446,7 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 {
 	__u8 vendor = boot_cpu_data.x86_vendor;
 	__u8 family = boot_cpu_data.x86;
-	char *cpu_type;
+	char *cpu_type = NULL;
 	int ret = 0;
 
 	if (!cpu_has_apic)
@@ -474,19 +484,20 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		switch (family) {
 			/* Pentium IV */
 		case 0xf:
-			if (!p4_init(&cpu_type))
-				return -ENODEV;
+			p4_init(&cpu_type);
 			break;
 
 			/* A P6-class processor */
 		case 6:
-			if (!ppro_init(&cpu_type))
-				return -ENODEV;
+			ppro_init(&cpu_type);
 			break;
 
 		default:
-			return -ENODEV;
+			break;
 		}
+
+		if (!cpu_type && !arch_perfmon_init(&cpu_type))
+			return -ENODEV;
 		break;
 
 	default:

commit f645f6406463a01869c50844befc76d528971690
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Aug 20 17:22:02 2008 +0200

    oprofile: Don't report Nehalem as core_2
    
    This essentially reverts Linus' earlier 4b9f12a3779c548b68bc9af7d94030868ad3aa1b
    commit. Nehalem is not core_2, so it shouldn't be reported as such.
    However with the earlier arch perfmon patch it will fall back to
    arch perfmon mode now, so there is no need to fake it as core_2.
    The only drawback is that Linus will need to patch the arch perfmon
    support into his oprofile binary now, but I think he can do that.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 57f6c9088081..1059f3fe6b1d 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -420,9 +420,6 @@ static int __init ppro_init(char **cpu_type)
 	case 15: case 23:
 		*cpu_type = "i386/core_2";
 		break;
-	case 26:
-		*cpu_type = "i386/core_2";
-		break;
 	default:
 		/* Unknown */
 		return 0;

commit 0d15504f16f68725e4635aa85411015d1c573b0a
Merge: 59293c8ad547 f78e80209cf1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 30 12:19:41 2008 +0200

    Merge branch 'oprofile-for-tip' of git://git.kernel.org/pub/scm/linux/kernel/git/rric/oprofile into oprofile
    
    Conflicts:
            arch/x86/oprofile/nmi_int.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 59293c8ad54726150cf6178164311b004d615ce4
Merge: 45f197ade73b 94aca1dac6f6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 30 12:16:26 2008 +0200

    Merge commit 'v2.6.27-rc8' into oprofile
    
    Conflicts:
            arch/x86/oprofile/nmi_int.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f78e80209cf143be49f268c340431ae9fa3abb74
Merge: 4c168eaf7ea3 24342c34a022
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Sep 24 11:25:31 2008 +0200

    Merge commit 'v2.6.27-rc5' into tip/oprofile
    
    Conflicts:
            arch/x86/oprofile/nmi_int.c

commit 4c168eaf7ea39f25a45a3d8c7eebc3fedb633a1d
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Sep 24 11:08:52 2008 +0200

    Revert "Oprofile Multiplexing Patch"
    
    Reverting commit 1a960b402a51d80abf54e3f8e4972374ffe5f22d for the main
    branch. Multiplexing will be tracked on a separate feature branch.
    
    Conflicts:
    
        arch/x86/oprofile/nmi_int.c

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 4108d02c5292..287513a09819 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -23,18 +23,12 @@
 #include "op_counter.h"
 #include "op_x86_model.h"
 
-DEFINE_PER_CPU(int, switch_index);
-
 static struct op_x86_model_spec const *model;
 static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
 static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 
 static int nmi_start(void);
 static void nmi_stop(void);
-static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs);
-static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs);
-static void nmi_cpu_stop(void *dummy);
-static void nmi_cpu_start(void *dummy);
 
 /* 0 == registered but off, 1 == registered and on */
 static int nmi_enabled = 0;
@@ -87,47 +81,6 @@ static void exit_sysfs(void)
 #define exit_sysfs() do { } while (0)
 #endif /* CONFIG_PM */
 
-static void nmi_cpu_switch(void *dummy)
-{
-	int cpu = smp_processor_id();
-	int si = per_cpu(switch_index, cpu);
-	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
-
-	nmi_cpu_stop(NULL);
-	nmi_cpu_save_mpx_registers(msrs);
-
-	/* move to next set */
-	si += model->num_hardware_counters;
-	if ((si > model->num_counters) || (counter_config[si].count == 0))
-		per_cpu(switch_index, smp_processor_id()) = 0;
-	else
-		per_cpu(switch_index, smp_processor_id()) = si;
-
-	nmi_cpu_restore_mpx_registers(msrs);
-	model->setup_ctrs(msrs);
-	nmi_cpu_start(NULL);
-}
-
-/*
- * Quick check to see if multiplexing is necessary.
- * The check should be sufficient since counters are used
- * in ordre.
- */
-static int nmi_multiplex_on(void)
-{
-	return counter_config[model->num_hardware_counters].count ? 0 : -EINVAL;
-}
-
-static int nmi_switch_event(void)
-{
-	if (nmi_multiplex_on() < 0)
-		return -EINVAL;
-
-	on_each_cpu(nmi_cpu_switch, NULL, 1);
-
-	return 0;
-}
-
 static int profile_exceptions_notify(struct notifier_block *self,
 				     unsigned long val, void *data)
 {
@@ -191,10 +144,11 @@ static void free_msrs(void)
 
 static int allocate_msrs(void)
 {
-	int i, success = 1;
+	int success = 1;
 	size_t controls_size = sizeof(struct op_msr) * model->num_controls;
 	size_t counters_size = sizeof(struct op_msr) * model->num_counters;
 
+	int i;
 	for_each_possible_cpu(i) {
 		per_cpu(cpu_msrs, i).counters = kmalloc(counters_size,
 								GFP_KERNEL);
@@ -202,8 +156,8 @@ static int allocate_msrs(void)
 			success = 0;
 			break;
 		}
-		per_cpu(cpu_msrs, i).controls =
-				kmalloc(controls_size, GFP_KERNEL);
+		per_cpu(cpu_msrs, i).controls = kmalloc(controls_size,
+								GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).controls) {
 			success = 0;
 			break;
@@ -247,8 +201,7 @@ static int nmi_setup(void)
 		return err;
 	}
 
-	/*
-	 * We need to serialize save and setup for HT because the subset
+	/* We need to serialize save and setup for HT because the subset
 	 * of msrs are distinct for save and setup operations
 	 */
 
@@ -264,6 +217,7 @@ static int nmi_setup(void)
 				per_cpu(cpu_msrs, 0).controls,
 				sizeof(struct op_msr) * model->num_controls);
 		}
+
 	}
 	on_each_cpu(nmi_save_registers, NULL, 1);
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
@@ -271,41 +225,7 @@ static int nmi_setup(void)
 	return 0;
 }
 
-static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs)
-{
-	unsigned int si = __get_cpu_var(switch_index);
-	unsigned int const nr_ctrs = model->num_hardware_counters;
-	struct op_msr *counters = &msrs->counters[si];
-	unsigned int i;
-
-	for (i = 0; i < nr_ctrs; ++i) {
-		int offset = i + si;
-		if (counters[offset].addr) {
-			rdmsr(counters[offset].addr,
-				counters[offset].multiplex.low,
-				counters[offset].multiplex.high);
-		}
-	}
-}
-
-static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
-{
-	unsigned int si = __get_cpu_var(switch_index);
-	unsigned int const nr_ctrs = model->num_hardware_counters;
-	struct op_msr *counters = &msrs->counters[si];
-	unsigned int i;
-
-	for (i = 0; i < nr_ctrs; ++i) {
-		int offset = i + si;
-		if (counters[offset].addr) {
-			wrmsr(counters[offset].addr,
-				counters[offset].multiplex.low,
-				counters[offset].multiplex.high);
-		}
-	}
-}
-
-static void nmi_cpu_restore_registers(struct op_msrs *msrs)
+static void nmi_restore_registers(struct op_msrs *msrs)
 {
 	unsigned int const nr_ctrs = model->num_counters;
 	unsigned int const nr_ctrls = model->num_controls;
@@ -345,8 +265,7 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTERR, v | APIC_LVT_MASKED);
 	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
-	nmi_cpu_restore_registers(msrs);
-	__get_cpu_var(switch_index) = 0;
+	nmi_restore_registers(msrs);
 }
 
 static void nmi_shutdown(void)
@@ -409,7 +328,6 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 		oprofilefs_create_ulong(sb, dir, "unit_mask", &counter_config[i].unit_mask);
 		oprofilefs_create_ulong(sb, dir, "kernel", &counter_config[i].kernel);
 		oprofilefs_create_ulong(sb, dir, "user", &counter_config[i].user);
-		counter_config[i].save_count_low = 0;
 	}
 
 	return 0;
@@ -551,14 +469,12 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	}
 
 	/* default values, can be overwritten by model */
-	__raw_get_cpu_var(switch_index) = 0;
 	ops->create_files = nmi_create_files;
 	ops->setup = nmi_setup;
 	ops->shutdown = nmi_shutdown;
 	ops->start = nmi_start;
 	ops->stop = nmi_stop;
 	ops->cpu_type = cpu_type;
-	ops->switch_events = nmi_switch_event;
 
 	if (model->init)
 		ret = model->init(ops);

commit 45f197ade73ba95681b9803680c75352fc0a1c0a
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Sat Sep 20 12:58:40 2008 +0200

    x86, oprofile: BUG: using smp_processor_id() in preemptible code
    
    Add __raw access before setting per cpu variable switch_index, to avoid
    the following BUG:
    
    [  449.166827] BUG: using smp_processor_id() in preemptible [00000000] code: modprobe/6998
    [  449.166848] caller is op_nmi_init+0xf0/0x2b0 [oprofile]
    [  449.166855] Pid: 6998, comm: modprobe Not tainted 2.6.27-rc5-mm1 #29
    [  449.166860] Call Trace:
    [  449.166872]  [<ffffffff80362d67>] debug_smp_processor_id+0xd7/0xe0
    [  449.166887]  [<ffffffffa00181c0>] op_nmi_init+0xf0/0x2b0 [oprofile]
    [  449.166902]  [<ffffffffa0018000>] oprofile_init+0x0/0x60 [oprofile]
    [  449.166915]  [<ffffffffa00180a9>] oprofile_arch_init+0x9/0x30 [oprofile]
    [  449.166928]  [<ffffffffa001801e>] oprofile_init+0x1e/0x60 [oprofile]
    [  449.166937]  [<ffffffff8020903b>] _stext+0x3b/0x160
    [  449.166946]  [<ffffffff80477985>] __mutex_unlock_slowpath+0xe5/0x190
    [  449.166955]  [<ffffffff80262c1a>] trace_hardirqs_on_caller+0xca/0x140
    [  449.166965]  [<ffffffff8026f7fc>] sys_init_module+0xdc/0x210
    [  449.166972]  [<ffffffff8020b7cb>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Acked-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index fb4902bc6f14..4108d02c5292 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -551,7 +551,7 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	}
 
 	/* default values, can be overwritten by model */
-	__get_cpu_var(switch_index) = 0;
+	__raw_get_cpu_var(switch_index) = 0;
 	ops->create_files = nmi_create_files;
 	ops->setup = nmi_setup;
 	ops->shutdown = nmi_shutdown;

commit b61e06f258e50b25c38a73bea782bdb6876f0f70
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Sat Sep 20 18:02:27 2008 +0200

    x86, oprofile: BUG scheduling while atomic
    
    nmi_shutdown() calls unregister_die_notifier() from an atomic context
    after setting preempt_disable() via get_cpu_var():
    
    [ 1049.404154] BUG: scheduling while atomic: oprofiled/7796/0x00000002
    [ 1049.404171] INFO: lockdep is turned off.
    [ 1049.404176] Modules linked in: oprofile af_packet rfcomm l2cap kvm_intel kvm i915 drm acpi_cpufreq cpufreq_userspace cpufreq_conservative cpufreq_ondemand cpufreq_powersave freq_table container sbs sbshc dm_mod arc4 ecb cryptomgr aead snd_hda_intel crypto_blkcipher snd_pcm_oss crypto_algapi snd_pcm iwlagn iwlcore snd_timer iTCO_wdt led_class btusb iTCO_vendor_support snd psmouse bluetooth mac80211 soundcore cfg80211 snd_page_alloc intel_agp video output button battery ac dcdbas evdev ext3 jbd mbcache sg sd_mod piix ata_piix libata scsi_mod dock tg3 libphy ehci_hcd uhci_hcd usbcore thermal processor fan fuse
    [ 1049.404362] Pid: 7796, comm: oprofiled Not tainted 2.6.27-rc5-mm1 #30
    [ 1049.404368] Call Trace:
    [ 1049.404384]  [<ffffffff804769fd>] thread_return+0x4a0/0x7d3
    [ 1049.404396]  [<ffffffff8026ad92>] generic_exec_single+0x52/0xe0
    [ 1049.404405]  [<ffffffff8026ae1a>] generic_exec_single+0xda/0xe0
    [ 1049.404414]  [<ffffffff8026aee3>] smp_call_function_single+0x73/0x150
    [ 1049.404423]  [<ffffffff804770c5>] schedule_timeout+0x95/0xd0
    [ 1049.404430]  [<ffffffff80476083>] wait_for_common+0x43/0x180
    [ 1049.404438]  [<ffffffff80476154>] wait_for_common+0x114/0x180
    [ 1049.404448]  [<ffffffff80236980>] default_wake_function+0x0/0x10
    [ 1049.404457]  [<ffffffff8024f810>] synchronize_rcu+0x30/0x40
    [ 1049.404463]  [<ffffffff8024f890>] wakeme_after_rcu+0x0/0x10
    [ 1049.404472]  [<ffffffff80479ca0>] _spin_unlock_irqrestore+0x40/0x80
    [ 1049.404482]  [<ffffffff80256def>] atomic_notifier_chain_unregister+0x3f/0x60
    [ 1049.404501]  [<ffffffffa03d8801>] nmi_shutdown+0x51/0x90 [oprofile]
    [ 1049.404517]  [<ffffffffa03d6134>] oprofile_shutdown+0x34/0x70 [oprofile]
    [ 1049.404532]  [<ffffffffa03d721e>] event_buffer_release+0xe/0x40 [oprofile]
    [ 1049.404543]  [<ffffffff802bdcdd>] __fput+0xcd/0x240
    [ 1049.404551]  [<ffffffff802baa74>] filp_close+0x54/0x90
    [ 1049.404560]  [<ffffffff8023e1d1>] put_files_struct+0xb1/0xd0
    [ 1049.404568]  [<ffffffff8023f82f>] do_exit+0x18f/0x930
    [ 1049.404576]  [<ffffffff8020be03>] restore_args+0x0/0x30
    [ 1049.404584]  [<ffffffff80240006>] do_group_exit+0x36/0xa0
    [ 1049.404592]  [<ffffffff8020b7cb>] system_call_fastpath+0x16/0x1b
    
    This can be easily triggered with 'opcontrol --shutdown'.
    
    Simply move get_cpu_var() above unregister_die_notifier().
    
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Acked-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 0227694f7dab..8a5f1614a3d5 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -295,10 +295,12 @@ static void nmi_cpu_shutdown(void *dummy)
 
 static void nmi_shutdown(void)
 {
-	struct op_msrs *msrs = &get_cpu_var(cpu_msrs);
+	struct op_msrs *msrs;
+
 	nmi_enabled = 0;
 	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
 	unregister_die_notifier(&profile_exceptions_nb);
+	msrs = &get_cpu_var(cpu_msrs);
 	model->shutdown(msrs);
 	free_msrs();
 	put_cpu_var(cpu_msrs);

commit 80a8c9fffa78f57d7d4351af2f15a56386805ceb
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Aug 19 03:13:38 2008 +0200

    x86: fix oprofile + hibernation badness
    
    Vegard Nossum reported oprofile + hibernation problems:
    
    > Now some warnings:
    >
    > ------------[ cut here ]------------
    > WARNING: at /uio/arkimedes/s29/vegardno/git-working/linux-2.6/kernel/smp.c:328 s
    > mp_call_function_mask+0x194/0x1a0()
    
    The usual problem: the suspend function when interrupts are
    already disabled calls smp_call_function which is not allowed with
    interrupt off. But at this point all the other CPUs should be already
    down anyways, so it should be enough to just drop that.
    
    This patch should fix that problem at least by fixing cpu hotplug&
    suspend support.
    
    [ mingo@elte.hu: fixed 5 coding style errors. ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Tested-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 3f90289410e6..0227694f7dab 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -15,6 +15,7 @@
 #include <linux/slab.h>
 #include <linux/moduleparam.h>
 #include <linux/kdebug.h>
+#include <linux/cpu.h>
 #include <asm/nmi.h>
 #include <asm/msr.h>
 #include <asm/apic.h>
@@ -28,23 +29,48 @@ static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 
 static int nmi_start(void);
 static void nmi_stop(void);
+static void nmi_cpu_start(void *dummy);
+static void nmi_cpu_stop(void *dummy);
 
 /* 0 == registered but off, 1 == registered and on */
 static int nmi_enabled = 0;
 
+#ifdef CONFIG_SMP
+static int oprofile_cpu_notifier(struct notifier_block *b, unsigned long action,
+				 void *data)
+{
+	int cpu = (unsigned long)data;
+	switch (action) {
+	case CPU_DOWN_FAILED:
+	case CPU_ONLINE:
+		smp_call_function_single(cpu, nmi_cpu_start, NULL, 0);
+		break;
+	case CPU_DOWN_PREPARE:
+		smp_call_function_single(cpu, nmi_cpu_stop, NULL, 1);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block oprofile_cpu_nb = {
+	.notifier_call = oprofile_cpu_notifier
+};
+#endif
+
 #ifdef CONFIG_PM
 
 static int nmi_suspend(struct sys_device *dev, pm_message_t state)
 {
+	/* Only one CPU left, just stop that one */
 	if (nmi_enabled == 1)
-		nmi_stop();
+		nmi_cpu_stop(NULL);
 	return 0;
 }
 
 static int nmi_resume(struct sys_device *dev)
 {
 	if (nmi_enabled == 1)
-		nmi_start();
+		nmi_cpu_start(NULL);
 	return 0;
 }
 
@@ -463,6 +489,9 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	}
 
 	init_sysfs();
+#ifdef CONFIG_SMP
+	register_cpu_notifier(&oprofile_cpu_nb);
+#endif
 	using_nmi = 1;
 	ops->create_files = nmi_create_files;
 	ops->setup = nmi_setup;
@@ -476,6 +505,10 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 
 void op_nmi_exit(void)
 {
-	if (using_nmi)
+	if (using_nmi) {
 		exit_sysfs();
+#ifdef CONFIG_SMP
+		unregister_cpu_notifier(&oprofile_cpu_nb);
+#endif
+	}
 }

commit 7e7b43892b87b6be259479ef4de14029dcb4012f
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jul 24 16:00:16 2008 +0200

    x86/oprofile: fix on_each_cpu build error
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Cc: Jason Yeh <jason.yeh@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 2a65fe7680ab..fb4902bc6f14 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -123,7 +123,7 @@ static int nmi_switch_event(void)
 	if (nmi_multiplex_on() < 0)
 		return -EINVAL;
 
-	on_each_cpu(nmi_cpu_switch, NULL, 0, 1);
+	on_each_cpu(nmi_cpu_switch, NULL, 1);
 
 	return 0;
 }

commit 1a960b402a51d80abf54e3f8e4972374ffe5f22d
Author: Jason Yeh <jason.yeh@amd.com>
Date:   Wed Jul 23 23:05:53 2008 +0200

    Oprofile Multiplexing Patch
    
    This patch introduces multiplexing support for the Oprofile kernel
    module. It basically adds a new function pointer in oprofile_operator
    allowing each architecture to supply its callback to switch between
    different sets of event when the timer expires. Userspace tools can
    modify the time slice through /dev/oprofile/time_slice.
    
    It also modifies the number of counters exposed to the userspace through
    /dev/oprofile. For example, the number of counters for AMD CPUs are
    changed to 32 and multiplexed in the sets of 4.
    
    Signed-off-by: Jason Yeh <jason.yeh@amd.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 287513a09819..2a65fe7680ab 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -23,12 +23,18 @@
 #include "op_counter.h"
 #include "op_x86_model.h"
 
+DEFINE_PER_CPU(int, switch_index);
+
 static struct op_x86_model_spec const *model;
 static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
 static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 
 static int nmi_start(void);
 static void nmi_stop(void);
+static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs);
+static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs);
+static void nmi_cpu_stop(void *dummy);
+static void nmi_cpu_start(void *dummy);
 
 /* 0 == registered but off, 1 == registered and on */
 static int nmi_enabled = 0;
@@ -81,6 +87,47 @@ static void exit_sysfs(void)
 #define exit_sysfs() do { } while (0)
 #endif /* CONFIG_PM */
 
+static void nmi_cpu_switch(void *dummy)
+{
+	int cpu = smp_processor_id();
+	int si = per_cpu(switch_index, cpu);
+	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
+
+	nmi_cpu_stop(NULL);
+	nmi_cpu_save_mpx_registers(msrs);
+
+	/* move to next set */
+	si += model->num_hardware_counters;
+	if ((si > model->num_counters) || (counter_config[si].count == 0))
+		per_cpu(switch_index, smp_processor_id()) = 0;
+	else
+		per_cpu(switch_index, smp_processor_id()) = si;
+
+	nmi_cpu_restore_mpx_registers(msrs);
+	model->setup_ctrs(msrs);
+	nmi_cpu_start(NULL);
+}
+
+/*
+ * Quick check to see if multiplexing is necessary.
+ * The check should be sufficient since counters are used
+ * in ordre.
+ */
+static int nmi_multiplex_on(void)
+{
+	return counter_config[model->num_hardware_counters].count ? 0 : -EINVAL;
+}
+
+static int nmi_switch_event(void)
+{
+	if (nmi_multiplex_on() < 0)
+		return -EINVAL;
+
+	on_each_cpu(nmi_cpu_switch, NULL, 0, 1);
+
+	return 0;
+}
+
 static int profile_exceptions_notify(struct notifier_block *self,
 				     unsigned long val, void *data)
 {
@@ -144,11 +191,10 @@ static void free_msrs(void)
 
 static int allocate_msrs(void)
 {
-	int success = 1;
+	int i, success = 1;
 	size_t controls_size = sizeof(struct op_msr) * model->num_controls;
 	size_t counters_size = sizeof(struct op_msr) * model->num_counters;
 
-	int i;
 	for_each_possible_cpu(i) {
 		per_cpu(cpu_msrs, i).counters = kmalloc(counters_size,
 								GFP_KERNEL);
@@ -156,8 +202,8 @@ static int allocate_msrs(void)
 			success = 0;
 			break;
 		}
-		per_cpu(cpu_msrs, i).controls = kmalloc(controls_size,
-								GFP_KERNEL);
+		per_cpu(cpu_msrs, i).controls =
+				kmalloc(controls_size, GFP_KERNEL);
 		if (!per_cpu(cpu_msrs, i).controls) {
 			success = 0;
 			break;
@@ -201,7 +247,8 @@ static int nmi_setup(void)
 		return err;
 	}
 
-	/* We need to serialize save and setup for HT because the subset
+	/*
+	 * We need to serialize save and setup for HT because the subset
 	 * of msrs are distinct for save and setup operations
 	 */
 
@@ -217,7 +264,6 @@ static int nmi_setup(void)
 				per_cpu(cpu_msrs, 0).controls,
 				sizeof(struct op_msr) * model->num_controls);
 		}
-
 	}
 	on_each_cpu(nmi_save_registers, NULL, 1);
 	on_each_cpu(nmi_cpu_setup, NULL, 1);
@@ -225,7 +271,41 @@ static int nmi_setup(void)
 	return 0;
 }
 
-static void nmi_restore_registers(struct op_msrs *msrs)
+static void nmi_cpu_save_mpx_registers(struct op_msrs *msrs)
+{
+	unsigned int si = __get_cpu_var(switch_index);
+	unsigned int const nr_ctrs = model->num_hardware_counters;
+	struct op_msr *counters = &msrs->counters[si];
+	unsigned int i;
+
+	for (i = 0; i < nr_ctrs; ++i) {
+		int offset = i + si;
+		if (counters[offset].addr) {
+			rdmsr(counters[offset].addr,
+				counters[offset].multiplex.low,
+				counters[offset].multiplex.high);
+		}
+	}
+}
+
+static void nmi_cpu_restore_mpx_registers(struct op_msrs *msrs)
+{
+	unsigned int si = __get_cpu_var(switch_index);
+	unsigned int const nr_ctrs = model->num_hardware_counters;
+	struct op_msr *counters = &msrs->counters[si];
+	unsigned int i;
+
+	for (i = 0; i < nr_ctrs; ++i) {
+		int offset = i + si;
+		if (counters[offset].addr) {
+			wrmsr(counters[offset].addr,
+				counters[offset].multiplex.low,
+				counters[offset].multiplex.high);
+		}
+	}
+}
+
+static void nmi_cpu_restore_registers(struct op_msrs *msrs)
 {
 	unsigned int const nr_ctrs = model->num_counters;
 	unsigned int const nr_ctrls = model->num_controls;
@@ -265,7 +345,8 @@ static void nmi_cpu_shutdown(void *dummy)
 	apic_write(APIC_LVTERR, v | APIC_LVT_MASKED);
 	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
-	nmi_restore_registers(msrs);
+	nmi_cpu_restore_registers(msrs);
+	__get_cpu_var(switch_index) = 0;
 }
 
 static void nmi_shutdown(void)
@@ -328,6 +409,7 @@ static int nmi_create_files(struct super_block *sb, struct dentry *root)
 		oprofilefs_create_ulong(sb, dir, "unit_mask", &counter_config[i].unit_mask);
 		oprofilefs_create_ulong(sb, dir, "kernel", &counter_config[i].kernel);
 		oprofilefs_create_ulong(sb, dir, "user", &counter_config[i].user);
+		counter_config[i].save_count_low = 0;
 	}
 
 	return 0;
@@ -469,12 +551,14 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	}
 
 	/* default values, can be overwritten by model */
+	__get_cpu_var(switch_index) = 0;
 	ops->create_files = nmi_create_files;
 	ops->setup = nmi_setup;
 	ops->shutdown = nmi_shutdown;
 	ops->start = nmi_start;
 	ops->stop = nmi_stop;
 	ops->cpu_type = cpu_type;
+	ops->switch_events = nmi_switch_event;
 
 	if (model->init)
 		ret = model->init(ops);

commit 270d3e1a10e6ef85d5a085377e01d91dbcbe3726
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 22 21:09:01 2008 +0200

    OProfile: enable IBS for AMD CPUs
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Cc: Barry Kasindorf <barry.kasindorf@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index b29819313f2b..287513a09819 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -468,6 +468,14 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		return -ENODEV;
 	}
 
+	/* default values, can be overwritten by model */
+	ops->create_files = nmi_create_files;
+	ops->setup = nmi_setup;
+	ops->shutdown = nmi_shutdown;
+	ops->start = nmi_start;
+	ops->stop = nmi_stop;
+	ops->cpu_type = cpu_type;
+
 	if (model->init)
 		ret = model->init(ops);
 	if (ret)
@@ -475,12 +483,6 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 
 	init_sysfs();
 	using_nmi = 1;
-	ops->create_files = nmi_create_files;
-	ops->setup = nmi_setup;
-	ops->shutdown = nmi_shutdown;
-	ops->start = nmi_start;
-	ops->stop = nmi_stop;
-	ops->cpu_type = cpu_type;
 	printk(KERN_INFO "oprofile: using NMI interrupt.\n");
 	return 0;
 }

commit 6657fe4f5650ff7174d418d4bfa50c4640e81a2b
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 22 21:08:50 2008 +0200

    x86/oprofile: renaming athlon_*() into op_amd_*()
    
    These functions contain code for all AMD CPUs. The new names fit
    better.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Cc: Barry Kasindorf <barry.kasindorf@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 75e889156f23..b29819313f2b 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -425,21 +425,21 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		default:
 			return -ENODEV;
 		case 6:
-			model = &op_athlon_spec;
+			model = &op_amd_spec;
 			cpu_type = "i386/athlon";
 			break;
 		case 0xf:
-			model = &op_athlon_spec;
+			model = &op_amd_spec;
 			/* Actually it could be i386/hammer too, but give
 			 user space an consistent name. */
 			cpu_type = "x86-64/hammer";
 			break;
 		case 0x10:
-			model = &op_athlon_spec;
+			model = &op_amd_spec;
 			cpu_type = "x86-64/family10";
 			break;
 		case 0x11:
-			model = &op_athlon_spec;
+			model = &op_amd_spec;
 			cpu_type = "x86-64/family11h";
 			break;
 		}

commit adf5ec0bca553b763a6b9baed2677a4c7470025b
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 22 21:08:48 2008 +0200

    x86/oprofile: introduce model specific init/exit functions
    
    This patch implements model specific OProfile init/exit functions for
    x86 CPUs. Though there is more rework needed at the initialization
    code, this new introduced functions allow it to keep model specific
    code in the corresponding op_model_*.c files.
    
    The function interface is the same as for oprofile_arch_init/exit().
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 33db99ab90c0..75e889156f23 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -1,10 +1,11 @@
 /**
  * @file nmi_int.c
  *
- * @remark Copyright 2002 OProfile authors
+ * @remark Copyright 2002-2008 OProfile authors
  * @remark Read the file COPYING
  *
  * @author John Levon <levon@movementarian.org>
+ * @author Robert Richter <robert.richter@amd.com>
  */
 
 #include <linux/init.h>
@@ -411,6 +412,7 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	__u8 vendor = boot_cpu_data.x86_vendor;
 	__u8 family = boot_cpu_data.x86;
 	char *cpu_type;
+	int ret = 0;
 
 	if (!cpu_has_apic)
 		return -ENODEV;
@@ -466,6 +468,11 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 		return -ENODEV;
 	}
 
+	if (model->init)
+		ret = model->init(ops);
+	if (ret)
+		return ret;
+
 	init_sysfs();
 	using_nmi = 1;
 	ops->create_files = nmi_create_files;
@@ -482,4 +489,6 @@ void op_nmi_exit(void)
 {
 	if (using_nmi)
 		exit_sysfs();
+	if (model->exit)
+		model->exit();
 }

commit 12f2b2610e812627acf338aaf043fef20bb726ca
Author: Barry Kasindorf <barry.kasindorf@amd.com>
Date:   Tue Jul 22 21:08:47 2008 +0200

    oprofile: Add support for AMD Family 11h
    
    This patch add support for AMD Family 11h CPUs.
    
    Signed-off-by: Barry Kasindorf <barry.kasindorf@amd.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 3f90289410e6..33db99ab90c0 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -436,6 +436,10 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 			model = &op_athlon_spec;
 			cpu_type = "x86-64/family10";
 			break;
+		case 0x11:
+			model = &op_athlon_spec;
+			cpu_type = "x86-64/family11h";
+			break;
 		}
 		break;
 

commit 4b9f12a3779c548b68bc9af7d94030868ad3aa1b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 24 17:29:00 2008 -0700

    x86/oprofile/nmi_int: add Nehalem to list of ppro cores
    
    ..otherwise oprofile will fall back on that poor timer interrupt.
    
    Also replace the unreadable chain of if-statements with a "switch()"
    statement instead. It generates better code, and is a lot clearer.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 7f3329b55d2e..3f90289410e6 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -369,20 +369,34 @@ static int __init ppro_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
 
-	if (cpu_model == 14)
+	switch (cpu_model) {
+	case 0 ... 2:
+		*cpu_type = "i386/ppro";
+		break;
+	case 3 ... 5:
+		*cpu_type = "i386/pii";
+		break;
+	case 6 ... 8:
+		*cpu_type = "i386/piii";
+		break;
+	case 9:
+		*cpu_type = "i386/p6_mobile";
+		break;
+	case 10 ... 13:
+		*cpu_type = "i386/p6";
+		break;
+	case 14:
 		*cpu_type = "i386/core";
-	else if (cpu_model == 15 || cpu_model == 23)
+		break;
+	case 15: case 23:
+		*cpu_type = "i386/core_2";
+		break;
+	case 26:
 		*cpu_type = "i386/core_2";
-	else if (cpu_model > 0xd)
+		break;
+	default:
+		/* Unknown */
 		return 0;
-	else if (cpu_model == 9) {
-		*cpu_type = "i386/p6_mobile";
-	} else if (cpu_model > 5) {
-		*cpu_type = "i386/piii";
-	} else if (cpu_model > 2) {
-		*cpu_type = "i386/pii";
-	} else {
-		*cpu_type = "i386/ppro";
 	}
 
 	model = &op_ppro_spec;

commit 1a781a777b2f6ac46523fe92396215762ced624d
Merge: b9d2252c1e44 42a2f217a5e3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 15 21:55:59 2008 +0200

    Merge branch 'generic-ipi' into generic-ipi-for-linus
    
    Conflicts:
    
            arch/powerpc/Kconfig
            arch/s390/kernel/time.c
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/cpu/perfctr-watchdog.c
            arch/x86/kernel/i8259_64.c
            arch/x86/kernel/ldt.c
            arch/x86/kernel/nmi_64.c
            arch/x86/kernel/smpboot.c
            arch/x86/xen/smp.c
            include/asm-x86/hw_irq_32.h
            include/asm-x86/hw_irq_64.h
            include/asm-x86/mach-default/irq_vectors.h
            include/asm-x86/mach-voyager/irq_vectors.h
            include/asm-x86/smp.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 15c8b6c1aaaf1c4edd67e2f02e4d8e1bd1a51c0d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 9 09:39:44 2008 +0200

    on_each_cpu(): kill unused 'retry' parameter
    
    It's not even passed on to smp_call_function() anymore, since that
    was removed. So kill it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index cc48d3fde545..3238ad32ffd8 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -218,8 +218,8 @@ static int nmi_setup(void)
 		}
 
 	}
-	on_each_cpu(nmi_save_registers, NULL, 0, 1);
-	on_each_cpu(nmi_cpu_setup, NULL, 0, 1);
+	on_each_cpu(nmi_save_registers, NULL, 1);
+	on_each_cpu(nmi_cpu_setup, NULL, 1);
 	nmi_enabled = 1;
 	return 0;
 }
@@ -271,7 +271,7 @@ static void nmi_shutdown(void)
 {
 	struct op_msrs *msrs = &__get_cpu_var(cpu_msrs);
 	nmi_enabled = 0;
-	on_each_cpu(nmi_cpu_shutdown, NULL, 0, 1);
+	on_each_cpu(nmi_cpu_shutdown, NULL, 1);
 	unregister_die_notifier(&profile_exceptions_nb);
 	model->shutdown(msrs);
 	free_msrs();
@@ -285,7 +285,7 @@ static void nmi_cpu_start(void *dummy)
 
 static int nmi_start(void)
 {
-	on_each_cpu(nmi_cpu_start, NULL, 0, 1);
+	on_each_cpu(nmi_cpu_start, NULL, 1);
 	return 0;
 }
 
@@ -297,7 +297,7 @@ static void nmi_cpu_stop(void *dummy)
 
 static void nmi_stop(void)
 {
-	on_each_cpu(nmi_cpu_stop, NULL, 0, 1);
+	on_each_cpu(nmi_cpu_stop, NULL, 1);
 }
 
 struct op_counter_config counter_config[OP_MAX_COUNTER];

commit 93e1ade5382206d597e9d6de2d1383e69f54d064
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Sun Jun 22 09:40:18 2008 +0200

    x86/oprofile: disable preemption in nmi_shutdown
    
    fix:
    
        BUG: using smp_processor_id() in preemptible [00000000] code: oprofiled/27301
        caller is nmi_shutdown+0x11/0x60
        Pid: 27301, comm: oprofiled Not tainted 2.6.26-rc7 #25
         [<c028a90d>] debug_smp_processor_id+0xbd/0xc0
         [<c045fba1>] nmi_shutdown+0x11/0x60
         [<c045dd4a>] oprofile_shutdown+0x2a/0x60
    
    Note that we don't need this for the other functions, since they are all
    called with on_each_cpu() (which disables preemption for us anyway).
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Philippe Elie <phil.el@wanadoo.fr>
    Cc: oprofile-list@lists.sf.net
    Cc: Johannes Weiner <hannes@saeurebad.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index cc48d3fde545..2b6ad5b9f9d5 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -269,12 +269,13 @@ static void nmi_cpu_shutdown(void *dummy)
 
 static void nmi_shutdown(void)
 {
-	struct op_msrs *msrs = &__get_cpu_var(cpu_msrs);
+	struct op_msrs *msrs = &get_cpu_var(cpu_msrs);
 	nmi_enabled = 0;
 	on_each_cpu(nmi_cpu_shutdown, NULL, 0, 1);
 	unregister_die_notifier(&profile_exceptions_nb);
 	model->shutdown(msrs);
 	free_msrs();
+	put_cpu_var(cpu_msrs);
 }
 
 static void nmi_cpu_start(void *dummy)

commit d18d00f5dbcd1a95811617e9812cf0560bd465ee
Author: Mike Travis <travis@sgi.com>
Date:   Tue Mar 25 15:06:59 2008 -0700

    x86: oprofile: remove NR_CPUS arrays in arch/x86/oprofile/nmi_int.c
    
    Change the following arrays sized by NR_CPUS to be PERCPU variables:
    
            static struct op_msrs cpu_msrs[NR_CPUS];
            static unsigned long saved_lvtpc[NR_CPUS];
    
    Also some minor complaints from checkpatch.pl fixed.
    
    Based on:
            git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6.git
            git://git.kernel.org/pub/scm/linux/kernel/git/x86/linux-2.6-x86.git
    
    All changes were transparent except for:
    
     static void nmi_shutdown(void)
     {
    +       struct op_msrs *msrs = &__get_cpu_var(cpu_msrs);
            nmi_enabled = 0;
            on_each_cpu(nmi_cpu_shutdown, NULL, 0, 1);
            unregister_die_notifier(&profile_exceptions_nb);
    -       model->shutdown(cpu_msrs);
    +       model->shutdown(msrs);
            free_msrs();
     }
    
    The existing code passed a reference to cpu 0's instance of struct op_msrs
    to model->shutdown, whilst the other functions are passed a reference to
    <this cpu's> instance of a struct op_msrs.  This seemed to be a bug to me
    even though as long as cpu 0 and <this cpu> are of the same type it would
    have the same effect...?
    
    Cc: Philippe Elie <phil.el@wanadoo.fr>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 1f11cf0a307f..cc48d3fde545 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -23,8 +23,8 @@
 #include "op_x86_model.h"
 
 static struct op_x86_model_spec const *model;
-static struct op_msrs cpu_msrs[NR_CPUS];
-static unsigned long saved_lvtpc[NR_CPUS];
+static DEFINE_PER_CPU(struct op_msrs, cpu_msrs);
+static DEFINE_PER_CPU(unsigned long, saved_lvtpc);
 
 static int nmi_start(void);
 static void nmi_stop(void);
@@ -89,7 +89,7 @@ static int profile_exceptions_notify(struct notifier_block *self,
 
 	switch (val) {
 	case DIE_NMI:
-		if (model->check_ctrs(args->regs, &cpu_msrs[cpu]))
+		if (model->check_ctrs(args->regs, &per_cpu(cpu_msrs, cpu)))
 			ret = NOTIFY_STOP;
 		break;
 	default:
@@ -126,7 +126,7 @@ static void nmi_cpu_save_registers(struct op_msrs *msrs)
 static void nmi_save_registers(void *dummy)
 {
 	int cpu = smp_processor_id();
-	struct op_msrs *msrs = &cpu_msrs[cpu];
+	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
 	nmi_cpu_save_registers(msrs);
 }
 
@@ -134,10 +134,10 @@ static void free_msrs(void)
 {
 	int i;
 	for_each_possible_cpu(i) {
-		kfree(cpu_msrs[i].counters);
-		cpu_msrs[i].counters = NULL;
-		kfree(cpu_msrs[i].controls);
-		cpu_msrs[i].controls = NULL;
+		kfree(per_cpu(cpu_msrs, i).counters);
+		per_cpu(cpu_msrs, i).counters = NULL;
+		kfree(per_cpu(cpu_msrs, i).controls);
+		per_cpu(cpu_msrs, i).controls = NULL;
 	}
 }
 
@@ -149,13 +149,15 @@ static int allocate_msrs(void)
 
 	int i;
 	for_each_possible_cpu(i) {
-		cpu_msrs[i].counters = kmalloc(counters_size, GFP_KERNEL);
-		if (!cpu_msrs[i].counters) {
+		per_cpu(cpu_msrs, i).counters = kmalloc(counters_size,
+								GFP_KERNEL);
+		if (!per_cpu(cpu_msrs, i).counters) {
 			success = 0;
 			break;
 		}
-		cpu_msrs[i].controls = kmalloc(controls_size, GFP_KERNEL);
-		if (!cpu_msrs[i].controls) {
+		per_cpu(cpu_msrs, i).controls = kmalloc(controls_size,
+								GFP_KERNEL);
+		if (!per_cpu(cpu_msrs, i).controls) {
 			success = 0;
 			break;
 		}
@@ -170,11 +172,11 @@ static int allocate_msrs(void)
 static void nmi_cpu_setup(void *dummy)
 {
 	int cpu = smp_processor_id();
-	struct op_msrs *msrs = &cpu_msrs[cpu];
+	struct op_msrs *msrs = &per_cpu(cpu_msrs, cpu);
 	spin_lock(&oprofilefs_lock);
 	model->setup_ctrs(msrs);
 	spin_unlock(&oprofilefs_lock);
-	saved_lvtpc[cpu] = apic_read(APIC_LVTPC);
+	per_cpu(saved_lvtpc, cpu) = apic_read(APIC_LVTPC);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 }
 
@@ -203,13 +205,15 @@ static int nmi_setup(void)
 	 */
 
 	/* Assume saved/restored counters are the same on all CPUs */
-	model->fill_in_addresses(&cpu_msrs[0]);
+	model->fill_in_addresses(&per_cpu(cpu_msrs, 0));
 	for_each_possible_cpu(cpu) {
 		if (cpu != 0) {
-			memcpy(cpu_msrs[cpu].counters, cpu_msrs[0].counters,
+			memcpy(per_cpu(cpu_msrs, cpu).counters,
+				per_cpu(cpu_msrs, 0).counters,
 				sizeof(struct op_msr) * model->num_counters);
 
-			memcpy(cpu_msrs[cpu].controls, cpu_msrs[0].controls,
+			memcpy(per_cpu(cpu_msrs, cpu).controls,
+				per_cpu(cpu_msrs, 0).controls,
 				sizeof(struct op_msr) * model->num_controls);
 		}
 
@@ -249,7 +253,7 @@ static void nmi_cpu_shutdown(void *dummy)
 {
 	unsigned int v;
 	int cpu = smp_processor_id();
-	struct op_msrs *msrs = &cpu_msrs[cpu];
+	struct op_msrs *msrs = &__get_cpu_var(cpu_msrs);
 
 	/* restoring APIC_LVTPC can trigger an apic error because the delivery
 	 * mode and vector nr combination can be illegal. That's by design: on
@@ -258,23 +262,24 @@ static void nmi_cpu_shutdown(void *dummy)
 	 */
 	v = apic_read(APIC_LVTERR);
 	apic_write(APIC_LVTERR, v | APIC_LVT_MASKED);
-	apic_write(APIC_LVTPC, saved_lvtpc[cpu]);
+	apic_write(APIC_LVTPC, per_cpu(saved_lvtpc, cpu));
 	apic_write(APIC_LVTERR, v);
 	nmi_restore_registers(msrs);
 }
 
 static void nmi_shutdown(void)
 {
+	struct op_msrs *msrs = &__get_cpu_var(cpu_msrs);
 	nmi_enabled = 0;
 	on_each_cpu(nmi_cpu_shutdown, NULL, 0, 1);
 	unregister_die_notifier(&profile_exceptions_nb);
-	model->shutdown(cpu_msrs);
+	model->shutdown(msrs);
 	free_msrs();
 }
 
 static void nmi_cpu_start(void *dummy)
 {
-	struct op_msrs const *msrs = &cpu_msrs[smp_processor_id()];
+	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
 	model->start(msrs);
 }
 
@@ -286,7 +291,7 @@ static int nmi_start(void)
 
 static void nmi_cpu_stop(void *dummy)
 {
-	struct op_msrs const *msrs = &cpu_msrs[smp_processor_id()];
+	struct op_msrs const *msrs = &__get_cpu_var(cpu_msrs);
 	model->stop(msrs);
 }
 

commit b75f53dba8a4a61fda1ff7e0fb0fe3b0d80e0c64
Author: Carlos R. Mafra <crmafra@gmail.com>
Date:   Wed Jan 30 13:32:33 2008 +0100

    x86: fix style errors in nmi_int.c
    
    This patch fixes most errors detected by checkpatch.pl.
    
                                         errors   lines of code   errors/KLOC
    arch/x86/oprofile/nmi_int.c (after)       1             461           2.1
    arch/x86/oprofile/nmi_int.c (before)     60             477         125.7
    
    No code changed.
    
    size:
       text    data     bss     dec     hex filename
       2675     264     472    3411     d53 nmi_int.o.after
       2675     264     472    3411     d53 nmi_int.o.before
    
    md5sum:
      847aea0cc68fe1a2b5e7019439f3b4dd  nmi_int.o.after
      847aea0cc68fe1a2b5e7019439f3b4dd  nmi_int.o.before
    
    Signed-off-by: Carlos R. Mafra <crmafra@gmail.com>
    Reviewed-by: Jesper Juhl <jesper.juhl@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index c8ab79ef4276..1f11cf0a307f 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -18,11 +18,11 @@
 #include <asm/nmi.h>
 #include <asm/msr.h>
 #include <asm/apic.h>
- 
+
 #include "op_counter.h"
 #include "op_x86_model.h"
 
-static struct op_x86_model_spec const * model;
+static struct op_x86_model_spec const *model;
 static struct op_msrs cpu_msrs[NR_CPUS];
 static unsigned long saved_lvtpc[NR_CPUS];
 
@@ -41,7 +41,6 @@ static int nmi_suspend(struct sys_device *dev, pm_message_t state)
 	return 0;
 }
 
-
 static int nmi_resume(struct sys_device *dev)
 {
 	if (nmi_enabled == 1)
@@ -49,29 +48,27 @@ static int nmi_resume(struct sys_device *dev)
 	return 0;
 }
 
-
 static struct sysdev_class oprofile_sysclass = {
 	.name		= "oprofile",
 	.resume		= nmi_resume,
 	.suspend	= nmi_suspend,
 };
 
-
 static struct sys_device device_oprofile = {
 	.id	= 0,
 	.cls	= &oprofile_sysclass,
 };
 
-
 static int __init init_sysfs(void)
 {
 	int error;
-	if (!(error = sysdev_class_register(&oprofile_sysclass)))
+
+	error = sysdev_class_register(&oprofile_sysclass);
+	if (!error)
 		error = sysdev_register(&device_oprofile);
 	return error;
 }
 
-
 static void exit_sysfs(void)
 {
 	sysdev_unregister(&device_oprofile);
@@ -90,7 +87,7 @@ static int profile_exceptions_notify(struct notifier_block *self,
 	int ret = NOTIFY_DONE;
 	int cpu = smp_processor_id();
 
-	switch(val) {
+	switch (val) {
 	case DIE_NMI:
 		if (model->check_ctrs(args->regs, &cpu_msrs[cpu]))
 			ret = NOTIFY_STOP;
@@ -101,24 +98,24 @@ static int profile_exceptions_notify(struct notifier_block *self,
 	return ret;
 }
 
-static void nmi_cpu_save_registers(struct op_msrs * msrs)
+static void nmi_cpu_save_registers(struct op_msrs *msrs)
 {
 	unsigned int const nr_ctrs = model->num_counters;
-	unsigned int const nr_ctrls = model->num_controls; 
-	struct op_msr * counters = msrs->counters;
-	struct op_msr * controls = msrs->controls;
+	unsigned int const nr_ctrls = model->num_controls;
+	struct op_msr *counters = msrs->counters;
+	struct op_msr *controls = msrs->controls;
 	unsigned int i;
 
 	for (i = 0; i < nr_ctrs; ++i) {
-		if (counters[i].addr){
+		if (counters[i].addr) {
 			rdmsr(counters[i].addr,
 				counters[i].saved.low,
 				counters[i].saved.high);
 		}
 	}
- 
+
 	for (i = 0; i < nr_ctrls; ++i) {
-		if (controls[i].addr){
+		if (controls[i].addr) {
 			rdmsr(controls[i].addr,
 				controls[i].saved.low,
 				controls[i].saved.high);
@@ -126,15 +123,13 @@ static void nmi_cpu_save_registers(struct op_msrs * msrs)
 	}
 }
 
-
-static void nmi_save_registers(void * dummy)
+static void nmi_save_registers(void *dummy)
 {
 	int cpu = smp_processor_id();
-	struct op_msrs * msrs = &cpu_msrs[cpu];
+	struct op_msrs *msrs = &cpu_msrs[cpu];
 	nmi_cpu_save_registers(msrs);
 }
 
-
 static void free_msrs(void)
 {
 	int i;
@@ -146,7 +141,6 @@ static void free_msrs(void)
 	}
 }
 
-
 static int allocate_msrs(void)
 {
 	int success = 1;
@@ -173,11 +167,10 @@ static int allocate_msrs(void)
 	return success;
 }
 
-
-static void nmi_cpu_setup(void * dummy)
+static void nmi_cpu_setup(void *dummy)
 {
 	int cpu = smp_processor_id();
-	struct op_msrs * msrs = &cpu_msrs[cpu];
+	struct op_msrs *msrs = &cpu_msrs[cpu];
 	spin_lock(&oprofilefs_lock);
 	model->setup_ctrs(msrs);
 	spin_unlock(&oprofilefs_lock);
@@ -193,13 +186,14 @@ static struct notifier_block profile_exceptions_nb = {
 
 static int nmi_setup(void)
 {
-	int err=0;
+	int err = 0;
 	int cpu;
 
 	if (!allocate_msrs())
 		return -ENOMEM;
 
-	if ((err = register_die_notifier(&profile_exceptions_nb))){
+	err = register_die_notifier(&profile_exceptions_nb);
+	if (err) {
 		free_msrs();
 		return err;
 	}
@@ -210,7 +204,7 @@ static int nmi_setup(void)
 
 	/* Assume saved/restored counters are the same on all CPUs */
 	model->fill_in_addresses(&cpu_msrs[0]);
-	for_each_possible_cpu (cpu) {
+	for_each_possible_cpu(cpu) {
 		if (cpu != 0) {
 			memcpy(cpu_msrs[cpu].counters, cpu_msrs[0].counters,
 				sizeof(struct op_msr) * model->num_counters);
@@ -226,39 +220,37 @@ static int nmi_setup(void)
 	return 0;
 }
 
-
-static void nmi_restore_registers(struct op_msrs * msrs)
+static void nmi_restore_registers(struct op_msrs *msrs)
 {
 	unsigned int const nr_ctrs = model->num_counters;
-	unsigned int const nr_ctrls = model->num_controls; 
-	struct op_msr * counters = msrs->counters;
-	struct op_msr * controls = msrs->controls;
+	unsigned int const nr_ctrls = model->num_controls;
+	struct op_msr *counters = msrs->counters;
+	struct op_msr *controls = msrs->controls;
 	unsigned int i;
 
 	for (i = 0; i < nr_ctrls; ++i) {
-		if (controls[i].addr){
+		if (controls[i].addr) {
 			wrmsr(controls[i].addr,
 				controls[i].saved.low,
 				controls[i].saved.high);
 		}
 	}
- 
+
 	for (i = 0; i < nr_ctrs; ++i) {
-		if (counters[i].addr){
+		if (counters[i].addr) {
 			wrmsr(counters[i].addr,
 				counters[i].saved.low,
 				counters[i].saved.high);
 		}
 	}
 }
- 
 
-static void nmi_cpu_shutdown(void * dummy)
+static void nmi_cpu_shutdown(void *dummy)
 {
 	unsigned int v;
 	int cpu = smp_processor_id();
-	struct op_msrs * msrs = &cpu_msrs[cpu];
- 
+	struct op_msrs *msrs = &cpu_msrs[cpu];
+
 	/* restoring APIC_LVTPC can trigger an apic error because the delivery
 	 * mode and vector nr combination can be illegal. That's by design: on
 	 * power on apic lvt contain a zero vector nr which are legal only for
@@ -271,7 +263,6 @@ static void nmi_cpu_shutdown(void * dummy)
 	nmi_restore_registers(msrs);
 }
 
- 
 static void nmi_shutdown(void)
 {
 	nmi_enabled = 0;
@@ -281,45 +272,40 @@ static void nmi_shutdown(void)
 	free_msrs();
 }
 
- 
-static void nmi_cpu_start(void * dummy)
+static void nmi_cpu_start(void *dummy)
 {
-	struct op_msrs const * msrs = &cpu_msrs[smp_processor_id()];
+	struct op_msrs const *msrs = &cpu_msrs[smp_processor_id()];
 	model->start(msrs);
 }
- 
 
 static int nmi_start(void)
 {
 	on_each_cpu(nmi_cpu_start, NULL, 0, 1);
 	return 0;
 }
- 
- 
-static void nmi_cpu_stop(void * dummy)
+
+static void nmi_cpu_stop(void *dummy)
 {
-	struct op_msrs const * msrs = &cpu_msrs[smp_processor_id()];
+	struct op_msrs const *msrs = &cpu_msrs[smp_processor_id()];
 	model->stop(msrs);
 }
- 
- 
+
 static void nmi_stop(void)
 {
 	on_each_cpu(nmi_cpu_stop, NULL, 0, 1);
 }
 
-
 struct op_counter_config counter_config[OP_MAX_COUNTER];
 
-static int nmi_create_files(struct super_block * sb, struct dentry * root)
+static int nmi_create_files(struct super_block *sb, struct dentry *root)
 {
 	unsigned int i;
 
 	for (i = 0; i < model->num_counters; ++i) {
-		struct dentry * dir;
+		struct dentry *dir;
 		char buf[4];
- 
- 		/* quick little hack to _not_ expose a counter if it is not
+
+		/* quick little hack to _not_ expose a counter if it is not
 		 * available for use.  This should protect userspace app.
 		 * NOTE:  assumes 1:1 mapping here (that counters are organized
 		 *        sequentially in their struct assignment).
@@ -329,21 +315,21 @@ static int nmi_create_files(struct super_block * sb, struct dentry * root)
 
 		snprintf(buf,  sizeof(buf), "%d", i);
 		dir = oprofilefs_mkdir(sb, root, buf);
-		oprofilefs_create_ulong(sb, dir, "enabled", &counter_config[i].enabled); 
-		oprofilefs_create_ulong(sb, dir, "event", &counter_config[i].event); 
-		oprofilefs_create_ulong(sb, dir, "count", &counter_config[i].count); 
-		oprofilefs_create_ulong(sb, dir, "unit_mask", &counter_config[i].unit_mask); 
-		oprofilefs_create_ulong(sb, dir, "kernel", &counter_config[i].kernel); 
-		oprofilefs_create_ulong(sb, dir, "user", &counter_config[i].user); 
+		oprofilefs_create_ulong(sb, dir, "enabled", &counter_config[i].enabled);
+		oprofilefs_create_ulong(sb, dir, "event", &counter_config[i].event);
+		oprofilefs_create_ulong(sb, dir, "count", &counter_config[i].count);
+		oprofilefs_create_ulong(sb, dir, "unit_mask", &counter_config[i].unit_mask);
+		oprofilefs_create_ulong(sb, dir, "kernel", &counter_config[i].kernel);
+		oprofilefs_create_ulong(sb, dir, "user", &counter_config[i].user);
 	}
 
 	return 0;
 }
- 
+
 static int p4force;
 module_param(p4force, int, 0);
- 
-static int __init p4_init(char ** cpu_type)
+
+static int __init p4_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
 
@@ -356,15 +342,15 @@ static int __init p4_init(char ** cpu_type)
 	return 1;
 #else
 	switch (smp_num_siblings) {
-		case 1:
-			*cpu_type = "i386/p4";
-			model = &op_p4_spec;
-			return 1;
-
-		case 2:
-			*cpu_type = "i386/p4-ht";
-			model = &op_p4_ht2_spec;
-			return 1;
+	case 1:
+		*cpu_type = "i386/p4";
+		model = &op_p4_spec;
+		return 1;
+
+	case 2:
+		*cpu_type = "i386/p4-ht";
+		model = &op_p4_ht2_spec;
+		return 1;
 	}
 #endif
 
@@ -373,8 +359,7 @@ static int __init p4_init(char ** cpu_type)
 	return 0;
 }
 
-
-static int __init ppro_init(char ** cpu_type)
+static int __init ppro_init(char **cpu_type)
 {
 	__u8 cpu_model = boot_cpu_data.x86_model;
 
@@ -409,52 +394,52 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 
 	if (!cpu_has_apic)
 		return -ENODEV;
- 
+
 	switch (vendor) {
-		case X86_VENDOR_AMD:
-			/* Needs to be at least an Athlon (or hammer in 32bit mode) */
+	case X86_VENDOR_AMD:
+		/* Needs to be at least an Athlon (or hammer in 32bit mode) */
 
-			switch (family) {
-			default:
+		switch (family) {
+		default:
+			return -ENODEV;
+		case 6:
+			model = &op_athlon_spec;
+			cpu_type = "i386/athlon";
+			break;
+		case 0xf:
+			model = &op_athlon_spec;
+			/* Actually it could be i386/hammer too, but give
+			 user space an consistent name. */
+			cpu_type = "x86-64/hammer";
+			break;
+		case 0x10:
+			model = &op_athlon_spec;
+			cpu_type = "x86-64/family10";
+			break;
+		}
+		break;
+
+	case X86_VENDOR_INTEL:
+		switch (family) {
+			/* Pentium IV */
+		case 0xf:
+			if (!p4_init(&cpu_type))
 				return -ENODEV;
-			case 6:
-				model = &op_athlon_spec;
-				cpu_type = "i386/athlon";
-				break;
-			case 0xf:
-				model = &op_athlon_spec;
-				/* Actually it could be i386/hammer too, but give
-				   user space an consistent name. */
-				cpu_type = "x86-64/hammer";
-				break;
-			case 0x10:
-				model = &op_athlon_spec;
-				cpu_type = "x86-64/family10";
-				break;
-			}
 			break;
- 
-		case X86_VENDOR_INTEL:
-			switch (family) {
-				/* Pentium IV */
-				case 0xf:
-					if (!p4_init(&cpu_type))
-						return -ENODEV;
-					break;
-
-				/* A P6-class processor */
-				case 6:
-					if (!ppro_init(&cpu_type))
-						return -ENODEV;
-					break;
-
-				default:
-					return -ENODEV;
-			}
+
+			/* A P6-class processor */
+		case 6:
+			if (!ppro_init(&cpu_type))
+				return -ENODEV;
 			break;
 
 		default:
 			return -ENODEV;
+		}
+		break;
+
+	default:
+		return -ENODEV;
 	}
 
 	init_sysfs();
@@ -469,7 +454,6 @@ int __init op_nmi_init(struct oprofile_operations *ops)
 	return 0;
 }
 
-
 void op_nmi_exit(void)
 {
 	if (using_nmi)

commit af5ca3f4ec5cc4432a42a73b050dd8898ce8fd00
Author: Kay Sievers <kay.sievers@vrfy.org>
Date:   Thu Dec 20 02:09:39 2007 +0100

    Driver core: change sysdev classes to use dynamic kobject names
    
    All kobjects require a dynamically allocated name now. We no longer
    need to keep track if the name is statically assigned, we can just
    unconditionally free() all kobject names on cleanup.
    
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 944bbcdd2b8d..c8ab79ef4276 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -51,7 +51,7 @@ static int nmi_resume(struct sys_device *dev)
 
 
 static struct sysdev_class oprofile_sysclass = {
-	set_kset_name("oprofile"),
+	.name		= "oprofile",
 	.resume		= nmi_resume,
 	.suspend	= nmi_suspend,
 };

commit e107ebe0e4a11b821d76ad2c3010c6a6244bd930
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Fri Jan 18 22:49:33 2008 +0100

    x86: add support for the latest Intel processors to Oprofile
    
    The latest Intel processors (the 45nm ones) have a model number of 23
    (old ones had 15); they're otherwise compatible on the oprofile side.
    This patch adds the new model number to the oprofile code.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 2d0eeac7251f..944bbcdd2b8d 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -380,7 +380,7 @@ static int __init ppro_init(char ** cpu_type)
 
 	if (cpu_model == 14)
 		*cpu_type = "i386/core";
-	else if (cpu_model == 15)
+	else if (cpu_model == 15 || cpu_model == 23)
 		*cpu_type = "i386/core_2";
 	else if (cpu_model > 0xd)
 		return 0;

commit 0f8e45a288991ff24951b83fe83cf3eb011bbaed
Author: Stephane Eranian <eranian@hpl.hp.com>
Date:   Wed Oct 17 18:04:32 2007 +0200

    i386: make Oprofile call shutdown() only once per session
    
    Oprofile: call model->shutdown() only once to avoid calling release_ev*()
    multiple times
    
    [ tglx: arch/x86 adaptation ]
    
    Signed-off-by: Stephane Eranian <eranian@hpl.hp.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
index 11b7a51566a8..2d0eeac7251f 100644
--- a/arch/x86/oprofile/nmi_int.c
+++ b/arch/x86/oprofile/nmi_int.c
@@ -269,7 +269,6 @@ static void nmi_cpu_shutdown(void * dummy)
 	apic_write(APIC_LVTPC, saved_lvtpc[cpu]);
 	apic_write(APIC_LVTERR, v);
 	nmi_restore_registers(msrs);
-	model->shutdown(msrs);
 }
 
  
@@ -278,6 +277,7 @@ static void nmi_shutdown(void)
 	nmi_enabled = 0;
 	on_each_cpu(nmi_cpu_shutdown, NULL, 0, 1);
 	unregister_die_notifier(&profile_exceptions_nb);
+	model->shutdown(cpu_msrs);
 	free_msrs();
 }
 

commit ff4395654dc6a3a5e35611940047114d4f3d0a7a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:55 2007 +0200

    i386: move oprofile
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c
new file mode 100644
index 000000000000..11b7a51566a8
--- /dev/null
+++ b/arch/x86/oprofile/nmi_int.c
@@ -0,0 +1,477 @@
+/**
+ * @file nmi_int.c
+ *
+ * @remark Copyright 2002 OProfile authors
+ * @remark Read the file COPYING
+ *
+ * @author John Levon <levon@movementarian.org>
+ */
+
+#include <linux/init.h>
+#include <linux/notifier.h>
+#include <linux/smp.h>
+#include <linux/oprofile.h>
+#include <linux/sysdev.h>
+#include <linux/slab.h>
+#include <linux/moduleparam.h>
+#include <linux/kdebug.h>
+#include <asm/nmi.h>
+#include <asm/msr.h>
+#include <asm/apic.h>
+ 
+#include "op_counter.h"
+#include "op_x86_model.h"
+
+static struct op_x86_model_spec const * model;
+static struct op_msrs cpu_msrs[NR_CPUS];
+static unsigned long saved_lvtpc[NR_CPUS];
+
+static int nmi_start(void);
+static void nmi_stop(void);
+
+/* 0 == registered but off, 1 == registered and on */
+static int nmi_enabled = 0;
+
+#ifdef CONFIG_PM
+
+static int nmi_suspend(struct sys_device *dev, pm_message_t state)
+{
+	if (nmi_enabled == 1)
+		nmi_stop();
+	return 0;
+}
+
+
+static int nmi_resume(struct sys_device *dev)
+{
+	if (nmi_enabled == 1)
+		nmi_start();
+	return 0;
+}
+
+
+static struct sysdev_class oprofile_sysclass = {
+	set_kset_name("oprofile"),
+	.resume		= nmi_resume,
+	.suspend	= nmi_suspend,
+};
+
+
+static struct sys_device device_oprofile = {
+	.id	= 0,
+	.cls	= &oprofile_sysclass,
+};
+
+
+static int __init init_sysfs(void)
+{
+	int error;
+	if (!(error = sysdev_class_register(&oprofile_sysclass)))
+		error = sysdev_register(&device_oprofile);
+	return error;
+}
+
+
+static void exit_sysfs(void)
+{
+	sysdev_unregister(&device_oprofile);
+	sysdev_class_unregister(&oprofile_sysclass);
+}
+
+#else
+#define init_sysfs() do { } while (0)
+#define exit_sysfs() do { } while (0)
+#endif /* CONFIG_PM */
+
+static int profile_exceptions_notify(struct notifier_block *self,
+				     unsigned long val, void *data)
+{
+	struct die_args *args = (struct die_args *)data;
+	int ret = NOTIFY_DONE;
+	int cpu = smp_processor_id();
+
+	switch(val) {
+	case DIE_NMI:
+		if (model->check_ctrs(args->regs, &cpu_msrs[cpu]))
+			ret = NOTIFY_STOP;
+		break;
+	default:
+		break;
+	}
+	return ret;
+}
+
+static void nmi_cpu_save_registers(struct op_msrs * msrs)
+{
+	unsigned int const nr_ctrs = model->num_counters;
+	unsigned int const nr_ctrls = model->num_controls; 
+	struct op_msr * counters = msrs->counters;
+	struct op_msr * controls = msrs->controls;
+	unsigned int i;
+
+	for (i = 0; i < nr_ctrs; ++i) {
+		if (counters[i].addr){
+			rdmsr(counters[i].addr,
+				counters[i].saved.low,
+				counters[i].saved.high);
+		}
+	}
+ 
+	for (i = 0; i < nr_ctrls; ++i) {
+		if (controls[i].addr){
+			rdmsr(controls[i].addr,
+				controls[i].saved.low,
+				controls[i].saved.high);
+		}
+	}
+}
+
+
+static void nmi_save_registers(void * dummy)
+{
+	int cpu = smp_processor_id();
+	struct op_msrs * msrs = &cpu_msrs[cpu];
+	nmi_cpu_save_registers(msrs);
+}
+
+
+static void free_msrs(void)
+{
+	int i;
+	for_each_possible_cpu(i) {
+		kfree(cpu_msrs[i].counters);
+		cpu_msrs[i].counters = NULL;
+		kfree(cpu_msrs[i].controls);
+		cpu_msrs[i].controls = NULL;
+	}
+}
+
+
+static int allocate_msrs(void)
+{
+	int success = 1;
+	size_t controls_size = sizeof(struct op_msr) * model->num_controls;
+	size_t counters_size = sizeof(struct op_msr) * model->num_counters;
+
+	int i;
+	for_each_possible_cpu(i) {
+		cpu_msrs[i].counters = kmalloc(counters_size, GFP_KERNEL);
+		if (!cpu_msrs[i].counters) {
+			success = 0;
+			break;
+		}
+		cpu_msrs[i].controls = kmalloc(controls_size, GFP_KERNEL);
+		if (!cpu_msrs[i].controls) {
+			success = 0;
+			break;
+		}
+	}
+
+	if (!success)
+		free_msrs();
+
+	return success;
+}
+
+
+static void nmi_cpu_setup(void * dummy)
+{
+	int cpu = smp_processor_id();
+	struct op_msrs * msrs = &cpu_msrs[cpu];
+	spin_lock(&oprofilefs_lock);
+	model->setup_ctrs(msrs);
+	spin_unlock(&oprofilefs_lock);
+	saved_lvtpc[cpu] = apic_read(APIC_LVTPC);
+	apic_write(APIC_LVTPC, APIC_DM_NMI);
+}
+
+static struct notifier_block profile_exceptions_nb = {
+	.notifier_call = profile_exceptions_notify,
+	.next = NULL,
+	.priority = 0
+};
+
+static int nmi_setup(void)
+{
+	int err=0;
+	int cpu;
+
+	if (!allocate_msrs())
+		return -ENOMEM;
+
+	if ((err = register_die_notifier(&profile_exceptions_nb))){
+		free_msrs();
+		return err;
+	}
+
+	/* We need to serialize save and setup for HT because the subset
+	 * of msrs are distinct for save and setup operations
+	 */
+
+	/* Assume saved/restored counters are the same on all CPUs */
+	model->fill_in_addresses(&cpu_msrs[0]);
+	for_each_possible_cpu (cpu) {
+		if (cpu != 0) {
+			memcpy(cpu_msrs[cpu].counters, cpu_msrs[0].counters,
+				sizeof(struct op_msr) * model->num_counters);
+
+			memcpy(cpu_msrs[cpu].controls, cpu_msrs[0].controls,
+				sizeof(struct op_msr) * model->num_controls);
+		}
+
+	}
+	on_each_cpu(nmi_save_registers, NULL, 0, 1);
+	on_each_cpu(nmi_cpu_setup, NULL, 0, 1);
+	nmi_enabled = 1;
+	return 0;
+}
+
+
+static void nmi_restore_registers(struct op_msrs * msrs)
+{
+	unsigned int const nr_ctrs = model->num_counters;
+	unsigned int const nr_ctrls = model->num_controls; 
+	struct op_msr * counters = msrs->counters;
+	struct op_msr * controls = msrs->controls;
+	unsigned int i;
+
+	for (i = 0; i < nr_ctrls; ++i) {
+		if (controls[i].addr){
+			wrmsr(controls[i].addr,
+				controls[i].saved.low,
+				controls[i].saved.high);
+		}
+	}
+ 
+	for (i = 0; i < nr_ctrs; ++i) {
+		if (counters[i].addr){
+			wrmsr(counters[i].addr,
+				counters[i].saved.low,
+				counters[i].saved.high);
+		}
+	}
+}
+ 
+
+static void nmi_cpu_shutdown(void * dummy)
+{
+	unsigned int v;
+	int cpu = smp_processor_id();
+	struct op_msrs * msrs = &cpu_msrs[cpu];
+ 
+	/* restoring APIC_LVTPC can trigger an apic error because the delivery
+	 * mode and vector nr combination can be illegal. That's by design: on
+	 * power on apic lvt contain a zero vector nr which are legal only for
+	 * NMI delivery mode. So inhibit apic err before restoring lvtpc
+	 */
+	v = apic_read(APIC_LVTERR);
+	apic_write(APIC_LVTERR, v | APIC_LVT_MASKED);
+	apic_write(APIC_LVTPC, saved_lvtpc[cpu]);
+	apic_write(APIC_LVTERR, v);
+	nmi_restore_registers(msrs);
+	model->shutdown(msrs);
+}
+
+ 
+static void nmi_shutdown(void)
+{
+	nmi_enabled = 0;
+	on_each_cpu(nmi_cpu_shutdown, NULL, 0, 1);
+	unregister_die_notifier(&profile_exceptions_nb);
+	free_msrs();
+}
+
+ 
+static void nmi_cpu_start(void * dummy)
+{
+	struct op_msrs const * msrs = &cpu_msrs[smp_processor_id()];
+	model->start(msrs);
+}
+ 
+
+static int nmi_start(void)
+{
+	on_each_cpu(nmi_cpu_start, NULL, 0, 1);
+	return 0;
+}
+ 
+ 
+static void nmi_cpu_stop(void * dummy)
+{
+	struct op_msrs const * msrs = &cpu_msrs[smp_processor_id()];
+	model->stop(msrs);
+}
+ 
+ 
+static void nmi_stop(void)
+{
+	on_each_cpu(nmi_cpu_stop, NULL, 0, 1);
+}
+
+
+struct op_counter_config counter_config[OP_MAX_COUNTER];
+
+static int nmi_create_files(struct super_block * sb, struct dentry * root)
+{
+	unsigned int i;
+
+	for (i = 0; i < model->num_counters; ++i) {
+		struct dentry * dir;
+		char buf[4];
+ 
+ 		/* quick little hack to _not_ expose a counter if it is not
+		 * available for use.  This should protect userspace app.
+		 * NOTE:  assumes 1:1 mapping here (that counters are organized
+		 *        sequentially in their struct assignment).
+		 */
+		if (unlikely(!avail_to_resrv_perfctr_nmi_bit(i)))
+			continue;
+
+		snprintf(buf,  sizeof(buf), "%d", i);
+		dir = oprofilefs_mkdir(sb, root, buf);
+		oprofilefs_create_ulong(sb, dir, "enabled", &counter_config[i].enabled); 
+		oprofilefs_create_ulong(sb, dir, "event", &counter_config[i].event); 
+		oprofilefs_create_ulong(sb, dir, "count", &counter_config[i].count); 
+		oprofilefs_create_ulong(sb, dir, "unit_mask", &counter_config[i].unit_mask); 
+		oprofilefs_create_ulong(sb, dir, "kernel", &counter_config[i].kernel); 
+		oprofilefs_create_ulong(sb, dir, "user", &counter_config[i].user); 
+	}
+
+	return 0;
+}
+ 
+static int p4force;
+module_param(p4force, int, 0);
+ 
+static int __init p4_init(char ** cpu_type)
+{
+	__u8 cpu_model = boot_cpu_data.x86_model;
+
+	if (!p4force && (cpu_model > 6 || cpu_model == 5))
+		return 0;
+
+#ifndef CONFIG_SMP
+	*cpu_type = "i386/p4";
+	model = &op_p4_spec;
+	return 1;
+#else
+	switch (smp_num_siblings) {
+		case 1:
+			*cpu_type = "i386/p4";
+			model = &op_p4_spec;
+			return 1;
+
+		case 2:
+			*cpu_type = "i386/p4-ht";
+			model = &op_p4_ht2_spec;
+			return 1;
+	}
+#endif
+
+	printk(KERN_INFO "oprofile: P4 HyperThreading detected with > 2 threads\n");
+	printk(KERN_INFO "oprofile: Reverting to timer mode.\n");
+	return 0;
+}
+
+
+static int __init ppro_init(char ** cpu_type)
+{
+	__u8 cpu_model = boot_cpu_data.x86_model;
+
+	if (cpu_model == 14)
+		*cpu_type = "i386/core";
+	else if (cpu_model == 15)
+		*cpu_type = "i386/core_2";
+	else if (cpu_model > 0xd)
+		return 0;
+	else if (cpu_model == 9) {
+		*cpu_type = "i386/p6_mobile";
+	} else if (cpu_model > 5) {
+		*cpu_type = "i386/piii";
+	} else if (cpu_model > 2) {
+		*cpu_type = "i386/pii";
+	} else {
+		*cpu_type = "i386/ppro";
+	}
+
+	model = &op_ppro_spec;
+	return 1;
+}
+
+/* in order to get sysfs right */
+static int using_nmi;
+
+int __init op_nmi_init(struct oprofile_operations *ops)
+{
+	__u8 vendor = boot_cpu_data.x86_vendor;
+	__u8 family = boot_cpu_data.x86;
+	char *cpu_type;
+
+	if (!cpu_has_apic)
+		return -ENODEV;
+ 
+	switch (vendor) {
+		case X86_VENDOR_AMD:
+			/* Needs to be at least an Athlon (or hammer in 32bit mode) */
+
+			switch (family) {
+			default:
+				return -ENODEV;
+			case 6:
+				model = &op_athlon_spec;
+				cpu_type = "i386/athlon";
+				break;
+			case 0xf:
+				model = &op_athlon_spec;
+				/* Actually it could be i386/hammer too, but give
+				   user space an consistent name. */
+				cpu_type = "x86-64/hammer";
+				break;
+			case 0x10:
+				model = &op_athlon_spec;
+				cpu_type = "x86-64/family10";
+				break;
+			}
+			break;
+ 
+		case X86_VENDOR_INTEL:
+			switch (family) {
+				/* Pentium IV */
+				case 0xf:
+					if (!p4_init(&cpu_type))
+						return -ENODEV;
+					break;
+
+				/* A P6-class processor */
+				case 6:
+					if (!ppro_init(&cpu_type))
+						return -ENODEV;
+					break;
+
+				default:
+					return -ENODEV;
+			}
+			break;
+
+		default:
+			return -ENODEV;
+	}
+
+	init_sysfs();
+	using_nmi = 1;
+	ops->create_files = nmi_create_files;
+	ops->setup = nmi_setup;
+	ops->shutdown = nmi_shutdown;
+	ops->start = nmi_start;
+	ops->stop = nmi_stop;
+	ops->cpu_type = cpu_type;
+	printk(KERN_INFO "oprofile: using NMI interrupt.\n");
+	return 0;
+}
+
+
+void op_nmi_exit(void)
+{
+	if (using_nmi)
+		exit_sysfs();
+}
