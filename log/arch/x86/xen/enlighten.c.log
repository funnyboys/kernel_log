commit c6875f3aacf2a5a913205accddabf0bfb75cac76
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Sep 30 16:44:41 2019 -0400

    x86/xen: Return from panic notifier
    
    Currently execution of panic() continues until Xen's panic notifier
    (xen_panic_event()) is called at which point we make a hypercall that
    never returns.
    
    This means that any notifier that is supposed to be called later as
    well as significant part of panic() code (such as pstore writes from
    kmsg_dump()) is never executed.
    
    There is no reason for xen_panic_event() to be this last point in
    execution since panic()'s emergency_restart() will call into
    xen_emergency_restart() from where we can perform our hypercall.
    
    Nevertheless, we will provide xen_legacy_crash boot option that will
    preserve original behavior during crash. This option could be used,
    for example, if running kernel dumper (which happens after panic
    notifiers) is undesirable.
    
    Reported-by: James Dingwall <james@dingwall.me.uk>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 750f46ad018a..205b1176084f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -269,19 +269,41 @@ void xen_reboot(int reason)
 		BUG();
 }
 
+static int reboot_reason = SHUTDOWN_reboot;
+static bool xen_legacy_crash;
 void xen_emergency_restart(void)
 {
-	xen_reboot(SHUTDOWN_reboot);
+	xen_reboot(reboot_reason);
 }
 
 static int
 xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
 {
-	if (!kexec_crash_loaded())
-		xen_reboot(SHUTDOWN_crash);
+	if (!kexec_crash_loaded()) {
+		if (xen_legacy_crash)
+			xen_reboot(SHUTDOWN_crash);
+
+		reboot_reason = SHUTDOWN_crash;
+
+		/*
+		 * If panic_timeout==0 then we are supposed to wait forever.
+		 * However, to preserve original dom0 behavior we have to drop
+		 * into hypervisor. (domU behavior is controlled by its
+		 * config file)
+		 */
+		if (panic_timeout == 0)
+			panic_timeout = -1;
+	}
 	return NOTIFY_DONE;
 }
 
+static int __init parse_xen_legacy_crash(char *arg)
+{
+	xen_legacy_crash = true;
+	return 0;
+}
+early_param("xen_legacy_crash", parse_xen_legacy_crash);
+
 static struct notifier_block xen_panic_block = {
 	.notifier_call = xen_panic_event,
 	.priority = INT_MIN

commit 292974c5acae330186cbf5a833385f666aeb12c0
Merge: a234c7371fd8 975ef94a0284
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 2 12:15:55 2018 -0800

    Merge tag 'for-linus-4.20a-rc5-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen fixes from Juergen Gross:
    
     - A revert of a previous commit as it is no longer necessary and has
       shown to cause problems in some memory hotplug cases.
    
     - Some small fixes and a minor cleanup.
    
     - A patch for adding better diagnostic data in a very rare failure
       case.
    
    * tag 'for-linus-4.20a-rc5-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      pvcalls-front: fixes incorrect error handling
      Revert "xen/balloon: Mark unallocated host memory as UNUSABLE"
      xen: xlate_mmu: add missing header to fix 'W=1' warning
      xen/x86: add diagnostic printout to xen_mc_flush() in case of error
      x86/xen: cleanup includes in arch/x86/xen/spinlock.c

commit 123664101aa2156d05251704fc63f9bcbf77741a
Author: Igor Druzhinin <igor.druzhinin@citrix.com>
Date:   Tue Nov 27 20:58:21 2018 +0000

    Revert "xen/balloon: Mark unallocated host memory as UNUSABLE"
    
    This reverts commit b3cf8528bb21febb650a7ecbf080d0647be40b9f.
    
    That commit unintentionally broke Xen balloon memory hotplug with
    "hotplug_unpopulated" set to 1. As long as "System RAM" resource
    got assigned under a new "Unusable memory" resource in IO/Mem tree
    any attempt to online this memory would fail due to general kernel
    restrictions on having "System RAM" resources as 1st level only.
    
    The original issue that commit has tried to workaround fa564ad96366
    ("x86/PCI: Enable a 64bit BAR on AMD Family 15h (Models 00-1f, 30-3f,
    60-7f)") also got amended by the following 03a551734 ("x86/PCI: Move
    and shrink AMD 64-bit window to avoid conflict") which made the
    original fix to Xen ballooning unnecessary.
    
    Signed-off-by: Igor Druzhinin <igor.druzhinin@citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 67b2f31a1265..aa1cc483bd2a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -10,7 +10,6 @@
 #include <xen/xen.h>
 #include <xen/features.h>
 #include <xen/page.h>
-#include <xen/interface/memory.h>
 
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>
@@ -346,80 +345,3 @@ void xen_arch_unregister_cpu(int num)
 }
 EXPORT_SYMBOL(xen_arch_unregister_cpu);
 #endif
-
-#ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
-void __init arch_xen_balloon_init(struct resource *hostmem_resource)
-{
-	struct xen_memory_map memmap;
-	int rc;
-	unsigned int i, last_guest_ram;
-	phys_addr_t max_addr = PFN_PHYS(max_pfn);
-	struct e820_table *xen_e820_table;
-	const struct e820_entry *entry;
-	struct resource *res;
-
-	if (!xen_initial_domain())
-		return;
-
-	xen_e820_table = kmalloc(sizeof(*xen_e820_table), GFP_KERNEL);
-	if (!xen_e820_table)
-		return;
-
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_table->entries);
-	set_xen_guest_handle(memmap.buffer, xen_e820_table->entries);
-	rc = HYPERVISOR_memory_op(XENMEM_machine_memory_map, &memmap);
-	if (rc) {
-		pr_warn("%s: Can't read host e820 (%d)\n", __func__, rc);
-		goto out;
-	}
-
-	last_guest_ram = 0;
-	for (i = 0; i < memmap.nr_entries; i++) {
-		if (xen_e820_table->entries[i].addr >= max_addr)
-			break;
-		if (xen_e820_table->entries[i].type == E820_TYPE_RAM)
-			last_guest_ram = i;
-	}
-
-	entry = &xen_e820_table->entries[last_guest_ram];
-	if (max_addr >= entry->addr + entry->size)
-		goto out; /* No unallocated host RAM. */
-
-	hostmem_resource->start = max_addr;
-	hostmem_resource->end = entry->addr + entry->size;
-
-	/*
-	 * Mark non-RAM regions between the end of dom0 RAM and end of host RAM
-	 * as unavailable. The rest of that region can be used for hotplug-based
-	 * ballooning.
-	 */
-	for (; i < memmap.nr_entries; i++) {
-		entry = &xen_e820_table->entries[i];
-
-		if (entry->type == E820_TYPE_RAM)
-			continue;
-
-		if (entry->addr >= hostmem_resource->end)
-			break;
-
-		res = kzalloc(sizeof(*res), GFP_KERNEL);
-		if (!res)
-			goto out;
-
-		res->name = "Unavailable host RAM";
-		res->start = entry->addr;
-		res->end = (entry->addr + entry->size < hostmem_resource->end) ?
-			    entry->addr + entry->size : hostmem_resource->end;
-		rc = insert_resource(hostmem_resource, res);
-		if (rc) {
-			pr_warn("%s: Can't insert [%llx - %llx) (%d)\n",
-				__func__, res->start, res->end, rc);
-			kfree(res);
-			goto  out;
-		}
-	}
-
- out:
-	kfree(xen_e820_table);
-}
-#endif /* CONFIG_XEN_BALLOON_MEMORY_HOTPLUG */

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 67b2f31a1265..e996e8e744cb 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0
 
 #ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #endif
 #include <linux/cpu.h>
 #include <linux/kexec.h>

commit f682a7920baf7b721d01dd317f3b532265357cbb
Merge: 99792e0cea1e 3a025de64bf8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:54:58 2018 +0100

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt updates from Ingo Molnar:
     "Two main changes:
    
       - Remove no longer used parts of the paravirt infrastructure and put
         large quantities of paravirt ops under a new config option
         PARAVIRT_XXL=y, which is selected by XEN_PV only. (Joergen Gross)
    
       - Enable PV spinlocks on Hyperv (Yi Sun)"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/hyperv: Enable PV qspinlock for Hyper-V
      x86/hyperv: Add GUEST_IDLE_MSR support
      x86/paravirt: Clean up native_patch()
      x86/paravirt: Prevent redefinition of SAVE_FLAGS macro
      x86/xen: Make xen_reservation_lock static
      x86/paravirt: Remove unneeded mmu related paravirt ops bits
      x86/paravirt: Move the Xen-only pv_mmu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the pv_irq_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move items in pv_info under PARAVIRT_XXL umbrella
      x86/paravirt: Introduce new config option PARAVIRT_XXL
      x86/paravirt: Remove unused paravirt bits
      x86/paravirt: Use a single ops structure
      x86/paravirt: Remove clobbers from struct paravirt_patch_site
      x86/paravirt: Remove clobbers parameter from paravirt patch functions
      x86/paravirt: Make paravirt_patch_call() and paravirt_patch_jmp() static
      x86/xen: Add SPDX identifier in arch/x86/xen files
      x86/xen: Link platform-pci-unplug.o only if CONFIG_XEN_PVHVM
      x86/xen: Move pv specific parts of arch/x86/xen/mmu.c to mmu_pv.c
      x86/xen: Move pv irq related functions under CONFIG_XEN_PV umbrella

commit 3cfa210bf3fe0803cca17f3775d8cf2360d5f443
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 25 13:30:09 2018 -0700

    xen: don't include <xen/xen.h> from <asm/io.h> and <asm/dma-mapping.h>
    
    Nothing Xen specific in these headers, which get included from a lot
    of code in the kernel.  So prune the includes and move them to the
    Xen-specific files that actually use them instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2eeddd814653..0ca46e03b830 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -5,6 +5,7 @@
 #include <linux/kexec.h>
 #include <linux/slab.h>
 
+#include <xen/xen.h>
 #include <xen/features.h>
 #include <xen/page.h>
 #include <xen/interface/memory.h>

commit 901d209a8b0e448dadbc830bc7d7951d1c9621f9
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:15 2018 +0200

    x86/xen: Add SPDX identifier in arch/x86/xen files
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-5-jgross@suse.com

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2eeddd814653..749fb4b73eda 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1,3 +1,5 @@
+// SPDX-License-Identifier: GPL-2.0
+
 #ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
 #include <linux/bootmem.h>
 #endif

commit f2701b77bbd992f3df4631de8493f21db0830452
Merge: 18b57ce2eb8c acb1872577b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Aug 5 16:39:29 2018 +0200

    Merge 4.18-rc7 into master to pick up the KVM dependcy
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 447ae316670230d7d29430e2cbf1f5db4f49d14c
Author: Nicolai Stange <nstange@suse.de>
Date:   Sun Jul 29 12:15:33 2018 +0200

    x86: Don't include linux/irq.h from asm/hardirq.h
    
    The next patch in this series will have to make the definition of
    irq_cpustat_t available to entering_irq().
    
    Inclusion of asm/hardirq.h into asm/apic.h would cause circular header
    dependencies like
    
      asm/smp.h
        asm/apic.h
          asm/hardirq.h
            linux/irq.h
              linux/topology.h
                linux/smp.h
                  asm/smp.h
    
    or
    
      linux/gfp.h
        linux/mmzone.h
          asm/mmzone.h
            asm/mmzone_64.h
              asm/smp.h
                asm/apic.h
                  asm/hardirq.h
                    linux/irq.h
                      linux/irqdesc.h
                        linux/kobject.h
                          linux/sysfs.h
                            linux/kernfs.h
                              linux/idr.h
                                linux/gfp.h
    
    and others.
    
    This causes compilation errors because of the header guards becoming
    effective in the second inclusion: symbols/macros that had been defined
    before wouldn't be available to intermediate headers in the #include chain
    anymore.
    
    A possible workaround would be to move the definition of irq_cpustat_t
    into its own header and include that from both, asm/hardirq.h and
    asm/apic.h.
    
    However, this wouldn't solve the real problem, namely asm/harirq.h
    unnecessarily pulling in all the linux/irq.h cruft: nothing in
    asm/hardirq.h itself requires it. Also, note that there are some other
    archs, like e.g. arm64, which don't have that #include in their
    asm/hardirq.h.
    
    Remove the linux/irq.h #include from x86' asm/hardirq.h.
    
    Fix resulting compilation errors by adding appropriate #includes to *.c
    files as needed.
    
    Note that some of these *.c files could be cleaned up a bit wrt. to their
    set of #includes, but that should better be done from separate patches, if
    at all.
    
    Signed-off-by: Nicolai Stange <nstange@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c9081c6671f0..df208af3cd74 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -3,6 +3,7 @@
 #endif
 #include <linux/cpu.h>
 #include <linux/kexec.h>
+#include <linux/slab.h>
 
 #include <xen/features.h>
 #include <xen/page.h>

commit 1fe83888a2b776c204cb06629700adfb8e9cc123
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Fri Jun 8 10:40:38 2018 +0200

    xen: share start flags between PV and PVH
    
    Use a global variable to store the start flags for both PV and PVH.
    This allows the xen_initial_domain macro to work properly on PVH.
    
    Note that ARM is also switched to use the new variable.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c9081c6671f0..3b5318505c69 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -64,6 +64,13 @@ struct shared_info xen_dummy_shared_info;
 __read_mostly int xen_have_vector_callback;
 EXPORT_SYMBOL_GPL(xen_have_vector_callback);
 
+/*
+ * NB: needs to live in .data because it's used by xen_prepare_pvh which runs
+ * before clearing the bss.
+ */
+uint32_t xen_start_flags __attribute__((section(".data"))) = 0;
+EXPORT_SYMBOL(xen_start_flags);
+
 /*
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.

commit b3cf8528bb21febb650a7ecbf080d0647be40b9f
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Tue Dec 12 15:08:21 2017 -0500

    xen/balloon: Mark unallocated host memory as UNUSABLE
    
    Commit f5775e0b6116 ("x86/xen: discard RAM regions above the maximum
    reservation") left host memory not assigned to dom0 as available for
    memory hotplug.
    
    Unfortunately this also meant that those regions could be used by
    others. Specifically, commit fa564ad96366 ("x86/PCI: Enable a 64bit BAR
    on AMD Family 15h (Models 00-1f, 30-3f, 60-7f)") may try to map those
    addresses as MMIO.
    
    To prevent this mark unallocated host memory as E820_TYPE_UNUSABLE (thus
    effectively reverting f5775e0b6116) and keep track of that region as
    a hostmem resource that can be used for the hotplug.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d669e9d89001..c9081c6671f0 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1,8 +1,12 @@
+#ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
+#include <linux/bootmem.h>
+#endif
 #include <linux/cpu.h>
 #include <linux/kexec.h>
 
 #include <xen/features.h>
 #include <xen/page.h>
+#include <xen/interface/memory.h>
 
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>
@@ -331,3 +335,80 @@ void xen_arch_unregister_cpu(int num)
 }
 EXPORT_SYMBOL(xen_arch_unregister_cpu);
 #endif
+
+#ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
+void __init arch_xen_balloon_init(struct resource *hostmem_resource)
+{
+	struct xen_memory_map memmap;
+	int rc;
+	unsigned int i, last_guest_ram;
+	phys_addr_t max_addr = PFN_PHYS(max_pfn);
+	struct e820_table *xen_e820_table;
+	const struct e820_entry *entry;
+	struct resource *res;
+
+	if (!xen_initial_domain())
+		return;
+
+	xen_e820_table = kmalloc(sizeof(*xen_e820_table), GFP_KERNEL);
+	if (!xen_e820_table)
+		return;
+
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_table->entries);
+	set_xen_guest_handle(memmap.buffer, xen_e820_table->entries);
+	rc = HYPERVISOR_memory_op(XENMEM_machine_memory_map, &memmap);
+	if (rc) {
+		pr_warn("%s: Can't read host e820 (%d)\n", __func__, rc);
+		goto out;
+	}
+
+	last_guest_ram = 0;
+	for (i = 0; i < memmap.nr_entries; i++) {
+		if (xen_e820_table->entries[i].addr >= max_addr)
+			break;
+		if (xen_e820_table->entries[i].type == E820_TYPE_RAM)
+			last_guest_ram = i;
+	}
+
+	entry = &xen_e820_table->entries[last_guest_ram];
+	if (max_addr >= entry->addr + entry->size)
+		goto out; /* No unallocated host RAM. */
+
+	hostmem_resource->start = max_addr;
+	hostmem_resource->end = entry->addr + entry->size;
+
+	/*
+	 * Mark non-RAM regions between the end of dom0 RAM and end of host RAM
+	 * as unavailable. The rest of that region can be used for hotplug-based
+	 * ballooning.
+	 */
+	for (; i < memmap.nr_entries; i++) {
+		entry = &xen_e820_table->entries[i];
+
+		if (entry->type == E820_TYPE_RAM)
+			continue;
+
+		if (entry->addr >= hostmem_resource->end)
+			break;
+
+		res = kzalloc(sizeof(*res), GFP_KERNEL);
+		if (!res)
+			goto out;
+
+		res->name = "Unavailable host RAM";
+		res->start = entry->addr;
+		res->end = (entry->addr + entry->size < hostmem_resource->end) ?
+			    entry->addr + entry->size : hostmem_resource->end;
+		rc = insert_resource(hostmem_resource, res);
+		if (rc) {
+			pr_warn("%s: Can't insert [%llx - %llx) (%d)\n",
+				__func__, res->start, res->end, rc);
+			kfree(res);
+			goto  out;
+		}
+	}
+
+ out:
+	kfree(xen_e820_table);
+}
+#endif /* CONFIG_XEN_BALLOON_MEMORY_HOTPLUG */

commit eac779aa509d453a55da0ea4302bdb79c4e0854f
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Sun Oct 8 19:58:46 2017 -0700

    xen/vcpu: Use a unified name about cpu hotplug state for pv and pvhvm
    
    As xen_cpuhp_setup is called by PV and PVHVM, the name of "x86/xen/hvm_guest"
    is confusing.
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0e7ef69e8531..d669e9d89001 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -93,11 +93,11 @@ int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
 	int rc;
 
 	rc = cpuhp_setup_state_nocalls(CPUHP_XEN_PREPARE,
-				       "x86/xen/hvm_guest:prepare",
+				       "x86/xen/guest:prepare",
 				       cpu_up_prepare_cb, cpu_dead_cb);
 	if (rc >= 0) {
 		rc = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
-					       "x86/xen/hvm_guest:online",
+					       "x86/xen/guest:online",
 					       xen_cpu_up_online, NULL);
 		if (rc < 0)
 			cpuhp_remove_state_nocalls(CPUHP_XEN_PREPARE);

commit c9b5d98b25161a7ebee6ea59d6424dd9f33c1b99
Author: Ankur Arora <ankur.a.arora@oracle.com>
Date:   Fri Jun 2 17:06:01 2017 -0700

    xen/vcpu: Handle xen_vcpu_setup() failure in hotplug
    
    The hypercall VCPUOP_register_vcpu_info can fail. This failure is
    handled by making per_cpu(xen_vcpu, cpu) point to its shared_info
    slot and those without one (cpu >= MAX_VIRT_CPUS) be NULL.
    
    For PVH/PVHVM, this is not enough, because we also need to pull
    these VCPUs out of circulation.
    
    Fix for PVH/PVHVM: on registration failure in the cpuhp prepare
    callback (xen_cpu_up_prepare_hvm()), return an error to the cpuhp
    state-machine so it can fail the CPU init.
    
    Fix for PV: the registration happens before smp_init(), so, in the
    failure case we clamp setup_max_cpus and limit the number of VCPUs
    that smp_init() will bring-up to MAX_VIRT_CPUS.
    This is functionally correct but it makes the code a bit simpler
    if we get rid of this explicit clamping: for VCPUs that don't have
    valid xen_vcpu, fail the CPU init in the cpuhp prepare callback
    (xen_cpu_up_prepare_pv()).
    
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Ankur Arora <ankur.a.arora@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 276cc21619ec..0e7ef69e8531 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -106,8 +106,10 @@ int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
 	return rc >= 0 ? 0 : rc;
 }
 
-static void xen_vcpu_setup_restore(int cpu)
+static int xen_vcpu_setup_restore(int cpu)
 {
+	int rc = 0;
+
 	/* Any per_cpu(xen_vcpu) is stale, so reset it */
 	xen_vcpu_info_reset(cpu);
 
@@ -117,8 +119,10 @@ static void xen_vcpu_setup_restore(int cpu)
 	 */
 	if (xen_pv_domain() ||
 	    (xen_hvm_domain() && cpu_online(cpu))) {
-		xen_vcpu_setup(cpu);
+		rc = xen_vcpu_setup(cpu);
 	}
+
+	return rc;
 }
 
 /*
@@ -128,7 +132,7 @@ static void xen_vcpu_setup_restore(int cpu)
  */
 void xen_vcpu_restore(void)
 {
-	int cpu;
+	int cpu, rc;
 
 	for_each_possible_cpu(cpu) {
 		bool other_cpu = (cpu != smp_processor_id());
@@ -148,22 +152,25 @@ void xen_vcpu_restore(void)
 		if (xen_pv_domain() || xen_feature(XENFEAT_hvm_safe_pvclock))
 			xen_setup_runstate_info(cpu);
 
-		xen_vcpu_setup_restore(cpu);
-
-		if (other_cpu && is_up &&
+		rc = xen_vcpu_setup_restore(cpu);
+		if (rc)
+			pr_emerg_once("vcpu restore failed for cpu=%d err=%d. "
+					"System will hang.\n", cpu, rc);
+		/*
+		 * In case xen_vcpu_setup_restore() fails, do not bring up the
+		 * VCPU. This helps us avoid the resulting OOPS when the VCPU
+		 * accesses pvclock_vcpu_time via xen_vcpu (which is NULL.)
+		 * Note that this does not improve the situation much -- now the
+		 * VM hangs instead of OOPSing -- with the VCPUs that did not
+		 * fail, spinning in stop_machine(), waiting for the failed
+		 * VCPUs to come up.
+		 */
+		if (other_cpu && is_up && (rc == 0) &&
 		    HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL))
 			BUG();
 	}
 }
 
-static void clamp_max_cpus(void)
-{
-#ifdef CONFIG_SMP
-	if (setup_max_cpus > MAX_VIRT_CPUS)
-		setup_max_cpus = MAX_VIRT_CPUS;
-#endif
-}
-
 void xen_vcpu_info_reset(int cpu)
 {
 	if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS) {
@@ -175,7 +182,7 @@ void xen_vcpu_info_reset(int cpu)
 	}
 }
 
-void xen_vcpu_setup(int cpu)
+int xen_vcpu_setup(int cpu)
 {
 	struct vcpu_register_vcpu_info info;
 	int err;
@@ -196,7 +203,7 @@ void xen_vcpu_setup(int cpu)
 	 */
 	if (xen_hvm_domain()) {
 		if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
-			return;
+			return 0;
 	}
 
 	if (xen_have_vcpu_info_placement) {
@@ -230,11 +237,10 @@ void xen_vcpu_setup(int cpu)
 		}
 	}
 
-	if (!xen_have_vcpu_info_placement) {
-		if (cpu >= MAX_VIRT_CPUS)
-			clamp_max_cpus();
+	if (!xen_have_vcpu_info_placement)
 		xen_vcpu_info_reset(cpu);
-	}
+
+	return ((per_cpu(xen_vcpu, cpu) == NULL) ? -ENODEV : 0);
 }
 
 void xen_reboot(int reason)

commit 0b64ffb8db4e310f77a01079ca752d946a8526b5
Author: Ankur Arora <ankur.a.arora@oracle.com>
Date:   Fri Jun 2 17:05:59 2017 -0700

    xen/pvh*: Support > 32 VCPUs at domain restore
    
    When Xen restores a PVHVM or PVH guest, its shared_info only holds
    up to 32 CPUs. The hypercall VCPUOP_register_vcpu_info allows
    us to setup per-page areas for VCPUs. This means we can boot
    PVH* guests with more than 32 VCPUs. During restore the per-cpu
    structure is allocated freshly by the hypervisor (vcpu_info_mfn is
    set to INVALID_MFN) so that the newly restored guest can make a
    VCPUOP_register_vcpu_info hypercall.
    
    However, we end up triggering this condition in Xen:
    /* Run this command on yourself or on other offline VCPUS. */
     if ( (v != current) && !test_bit(_VPF_down, &v->pause_flags) )
    
    which means we are unable to setup the per-cpu VCPU structures
    for running VCPUS. The Linux PV code paths makes this work by
    iterating over cpu_possible in xen_vcpu_restore() with:
    
     1) is target CPU up (VCPUOP_is_up hypercall?)
     2) if yes, then VCPUOP_down to pause it
     3) VCPUOP_register_vcpu_info
     4) if it was down, then VCPUOP_up to bring it back up
    
    With Xen commit 192df6f9122d ("xen/x86: allow HVM guests to use
    hypercalls to bring up vCPUs") this is available for non-PV guests.
    As such first check if VCPUOP_is_up is actually possible before
    trying this dance.
    
    As most of this dance code is done already in xen_vcpu_restore()
    let's make it callable on PV, PVH and PVHVM.
    
    Based-on-patch-by: Konrad Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Ankur Arora <ankur.a.arora@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 96b745e3f56c..276cc21619ec 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -106,6 +106,21 @@ int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
 	return rc >= 0 ? 0 : rc;
 }
 
+static void xen_vcpu_setup_restore(int cpu)
+{
+	/* Any per_cpu(xen_vcpu) is stale, so reset it */
+	xen_vcpu_info_reset(cpu);
+
+	/*
+	 * For PVH and PVHVM, setup online VCPUs only. The rest will
+	 * be handled by hotplug.
+	 */
+	if (xen_pv_domain() ||
+	    (xen_hvm_domain() && cpu_online(cpu))) {
+		xen_vcpu_setup(cpu);
+	}
+}
+
 /*
  * On restore, set the vcpu placement up again.
  * If it fails, then we're in a bad state, since
@@ -117,17 +132,23 @@ void xen_vcpu_restore(void)
 
 	for_each_possible_cpu(cpu) {
 		bool other_cpu = (cpu != smp_processor_id());
-		bool is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up, xen_vcpu_nr(cpu),
-						NULL);
+		bool is_up;
+
+		if (xen_vcpu_nr(cpu) == XEN_VCPU_ID_INVALID)
+			continue;
+
+		/* Only Xen 4.5 and higher support this. */
+		is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up,
+					   xen_vcpu_nr(cpu), NULL) > 0;
 
 		if (other_cpu && is_up &&
 		    HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(cpu), NULL))
 			BUG();
 
-		xen_setup_runstate_info(cpu);
+		if (xen_pv_domain() || xen_feature(XENFEAT_hvm_safe_pvclock))
+			xen_setup_runstate_info(cpu);
 
-		if (xen_have_vcpu_info_placement)
-			xen_vcpu_setup(cpu);
+		xen_vcpu_setup_restore(cpu);
 
 		if (other_cpu && is_up &&
 		    HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL))
@@ -163,11 +184,11 @@ void xen_vcpu_setup(int cpu)
 	BUG_ON(HYPERVISOR_shared_info == &xen_dummy_shared_info);
 
 	/*
-	 * This path is called twice on PVHVM - first during bootup via
-	 * smp_init -> xen_hvm_cpu_notify, and then if the VCPU is being
-	 * hotplugged: cpu_up -> xen_hvm_cpu_notify.
-	 * As we can only do the VCPUOP_register_vcpu_info once lets
-	 * not over-write its result.
+	 * This path is called on PVHVM at bootup (xen_hvm_smp_prepare_boot_cpu)
+	 * and at restore (xen_vcpu_restore). Also called for hotplugged
+	 * VCPUs (cpu_init -> xen_hvm_cpu_prepare_hvm).
+	 * However, the hypercall can only be done once (see below) so if a VCPU
+	 * is offlined and comes back online then let's not redo the hypercall.
 	 *
 	 * For PV it is called during restore (xen_vcpu_restore) and bootup
 	 * (xen_setup_vcpu_info_placement). The hotplug mechanism does not
@@ -178,8 +199,6 @@ void xen_vcpu_setup(int cpu)
 			return;
 	}
 
-	xen_vcpu_info_reset(cpu);
-
 	if (xen_have_vcpu_info_placement) {
 		vcpup = &per_cpu(xen_vcpu_info, cpu);
 		info.mfn = arbitrary_virt_to_mfn(vcpup);
@@ -214,7 +233,7 @@ void xen_vcpu_setup(int cpu)
 	if (!xen_have_vcpu_info_placement) {
 		if (cpu >= MAX_VIRT_CPUS)
 			clamp_max_cpus();
-		return;
+		xen_vcpu_info_reset(cpu);
 	}
 }
 

commit ad73fd595c2ab168fdd01a266cbe6e4df95f8db0
Author: Ankur Arora <ankur.a.arora@oracle.com>
Date:   Fri Jun 2 17:05:58 2017 -0700

    xen/vcpu: Simplify xen_vcpu related code
    
    Largely mechanical changes to aid unification of xen_vcpu_restore()
    logic for PV, PVH and PVHVM.
    
    xen_vcpu_setup(): the only change in logic is that clamp_max_cpus()
    is now handled inside the "if (!xen_have_vcpu_info_placement)" block.
    
    xen_vcpu_restore(): code movement from enlighten_pv.c to enlighten.c.
    
    xen_vcpu_info_reset(): pulls together all the code where xen_vcpu
    is set to default.
    
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Ankur Arora <ankur.a.arora@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a5ffcbb20cc0..96b745e3f56c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -106,6 +106,35 @@ int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
 	return rc >= 0 ? 0 : rc;
 }
 
+/*
+ * On restore, set the vcpu placement up again.
+ * If it fails, then we're in a bad state, since
+ * we can't back out from using it...
+ */
+void xen_vcpu_restore(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		bool other_cpu = (cpu != smp_processor_id());
+		bool is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up, xen_vcpu_nr(cpu),
+						NULL);
+
+		if (other_cpu && is_up &&
+		    HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(cpu), NULL))
+			BUG();
+
+		xen_setup_runstate_info(cpu);
+
+		if (xen_have_vcpu_info_placement)
+			xen_vcpu_setup(cpu);
+
+		if (other_cpu && is_up &&
+		    HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL))
+			BUG();
+	}
+}
+
 static void clamp_max_cpus(void)
 {
 #ifdef CONFIG_SMP
@@ -114,6 +143,17 @@ static void clamp_max_cpus(void)
 #endif
 }
 
+void xen_vcpu_info_reset(int cpu)
+{
+	if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS) {
+		per_cpu(xen_vcpu, cpu) =
+			&HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
+	} else {
+		/* Set to NULL so that if somebody accesses it we get an OOPS */
+		per_cpu(xen_vcpu, cpu) = NULL;
+	}
+}
+
 void xen_vcpu_setup(int cpu)
 {
 	struct vcpu_register_vcpu_info info;
@@ -137,40 +177,45 @@ void xen_vcpu_setup(int cpu)
 		if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
 			return;
 	}
-	if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS)
-		per_cpu(xen_vcpu, cpu) =
-			&HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
+
+	xen_vcpu_info_reset(cpu);
+
+	if (xen_have_vcpu_info_placement) {
+		vcpup = &per_cpu(xen_vcpu_info, cpu);
+		info.mfn = arbitrary_virt_to_mfn(vcpup);
+		info.offset = offset_in_page(vcpup);
+
+		/*
+		 * Check to see if the hypervisor will put the vcpu_info
+		 * structure where we want it, which allows direct access via
+		 * a percpu-variable.
+		 * N.B. This hypercall can _only_ be called once per CPU.
+		 * Subsequent calls will error out with -EINVAL. This is due to
+		 * the fact that hypervisor has no unregister variant and this
+		 * hypercall does not allow to over-write info.mfn and
+		 * info.offset.
+		 */
+		err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
+					 xen_vcpu_nr(cpu), &info);
+
+		if (err) {
+			pr_warn_once("register_vcpu_info failed: cpu=%d err=%d\n",
+				     cpu, err);
+			xen_have_vcpu_info_placement = 0;
+		} else {
+			/*
+			 * This cpu is using the registered vcpu info, even if
+			 * later ones fail to.
+			 */
+			per_cpu(xen_vcpu, cpu) = vcpup;
+		}
+	}
 
 	if (!xen_have_vcpu_info_placement) {
 		if (cpu >= MAX_VIRT_CPUS)
 			clamp_max_cpus();
 		return;
 	}
-
-	vcpup = &per_cpu(xen_vcpu_info, cpu);
-	info.mfn = arbitrary_virt_to_mfn(vcpup);
-	info.offset = offset_in_page(vcpup);
-
-	/* Check to see if the hypervisor will put the vcpu_info
-	   structure where we want it, which allows direct access via
-	   a percpu-variable.
-	   N.B. This hypercall can _only_ be called once per CPU. Subsequent
-	   calls will error out with -EINVAL. This is due to the fact that
-	   hypervisor has no unregister variant and this hypercall does not
-	   allow to over-write info.mfn and info.offset.
-	 */
-	err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, xen_vcpu_nr(cpu),
-				 &info);
-
-	if (err) {
-		printk(KERN_DEBUG "register_vcpu_info failed: err=%d\n", err);
-		xen_have_vcpu_info_placement = 0;
-		clamp_max_cpus();
-	} else {
-		/* This cpu is using the registered vcpu info, even if
-		   later ones fail to. */
-		per_cpu(xen_vcpu, cpu) = vcpup;
-	}
 }
 
 void xen_reboot(int reason)

commit 3dbd8204af48d0da442f11ad39aa778a5fd462bf
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Tue May 2 13:23:06 2017 -0400

    xen: Move xen_have_vector_callback definition to enlighten.c
    
    Commit 84d582d236dc ("xen: Revert commits da72ff5bfcb0 and
    72a9b186292d") defined xen_have_vector_callback in enlighten_hvm.c.
    Since guest-type-neutral code refers to this variable this causes
    build failures when CONFIG_XEN_PVHVM is not defined.
    
    Moving xen_have_vector_callback definition to enlighten.c resolves
    this issue.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 41d324c7261d..a5ffcbb20cc0 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -57,6 +57,9 @@ EXPORT_SYMBOL_GPL(xen_start_info);
 
 struct shared_info xen_dummy_shared_info;
 
+__read_mostly int xen_have_vector_callback;
+EXPORT_SYMBOL_GPL(xen_have_vector_callback);
+
 /*
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.

commit e1dab14cf68d1e03950135969af3faf322f0db04
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 14 18:35:41 2017 +0100

    x86/xen: split off enlighten_pv.c
    
    Basically, enlighten.c is renamed to enlighten_pv.c and some code moved
    out to common enlighten.c.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5cd454cda2f6..41d324c7261d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1,93 +1,16 @@
-/*
- * Core of Xen paravirt_ops implementation.
- *
- * This file contains the xen_paravirt_ops structure itself, and the
- * implementations for:
- * - privileged instructions
- * - interrupt flags
- * - segment operations
- * - booting and setup
- *
- * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007
- */
-
 #include <linux/cpu.h>
-#include <linux/kernel.h>
-#include <linux/init.h>
-#include <linux/smp.h>
-#include <linux/preempt.h>
-#include <linux/hardirq.h>
-#include <linux/percpu.h>
-#include <linux/delay.h>
-#include <linux/start_kernel.h>
-#include <linux/sched.h>
-#include <linux/kprobes.h>
-#include <linux/bootmem.h>
-#include <linux/export.h>
-#include <linux/mm.h>
-#include <linux/page-flags.h>
-#include <linux/highmem.h>
-#include <linux/console.h>
-#include <linux/pci.h>
-#include <linux/gfp.h>
-#include <linux/memblock.h>
-#include <linux/edd.h>
-#include <linux/frame.h>
-
 #include <linux/kexec.h>
 
-#include <xen/xen.h>
-#include <xen/events.h>
-#include <xen/interface/xen.h>
-#include <xen/interface/version.h>
-#include <xen/interface/physdev.h>
-#include <xen/interface/vcpu.h>
-#include <xen/interface/memory.h>
-#include <xen/interface/nmi.h>
-#include <xen/interface/xen-mca.h>
 #include <xen/features.h>
 #include <xen/page.h>
-#include <xen/hvc-console.h>
-#include <xen/acpi.h>
 
-#include <asm/paravirt.h>
-#include <asm/apic.h>
-#include <asm/page.h>
-#include <asm/xen/pci.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>
-#include <asm/xen/cpuid.h>
-#include <asm/fixmap.h>
-#include <asm/processor.h>
-#include <asm/proto.h>
-#include <asm/msr-index.h>
-#include <asm/traps.h>
-#include <asm/setup.h>
-#include <asm/desc.h>
-#include <asm/pgalloc.h>
-#include <asm/pgtable.h>
-#include <asm/tlbflush.h>
-#include <asm/reboot.h>
-#include <asm/stackprotector.h>
-#include <asm/hypervisor.h>
-#include <asm/mach_traps.h>
-#include <asm/mwait.h>
-#include <asm/pci_x86.h>
 #include <asm/cpu.h>
 #include <asm/e820/api.h> 
 
-#ifdef CONFIG_ACPI
-#include <linux/acpi.h>
-#include <asm/acpi.h>
-#include <acpi/pdc_intel.h>
-#include <acpi/processor.h>
-#include <xen/interface/platform.h>
-#endif
-
 #include "xen-ops.h"
-#include "mmu.h"
 #include "smp.h"
-#include "multicalls.h"
 #include "pmu.h"
 
 EXPORT_SYMBOL_GPL(hypercall_page);
@@ -134,14 +57,6 @@ EXPORT_SYMBOL_GPL(xen_start_info);
 
 struct shared_info xen_dummy_shared_info;
 
-void *xen_initial_gdt;
-
-RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
-
-static int xen_cpu_up_prepare_pv(unsigned int cpu);
-static int xen_cpu_up_online(unsigned int cpu);
-static int xen_cpu_dead_pv(unsigned int cpu);
-
 /*
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.
@@ -163,18 +78,30 @@ struct shared_info *HYPERVISOR_shared_info = &xen_dummy_shared_info;
  */
 int xen_have_vcpu_info_placement = 1;
 
-struct tls_descs {
-	struct desc_struct desc[3];
-};
+static int xen_cpu_up_online(unsigned int cpu)
+{
+	xen_init_lock_cpu(cpu);
+	return 0;
+}
 
-/*
- * Updating the 3 TLS descriptors in the GDT on every task switch is
- * surprisingly expensive so we avoid updating them if they haven't
- * changed.  Since Xen writes different descriptors than the one
- * passed in the update_descriptor hypercall we keep shadow copies to
- * compare against.
- */
-static DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);
+int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
+		    int (*cpu_dead_cb)(unsigned int))
+{
+	int rc;
+
+	rc = cpuhp_setup_state_nocalls(CPUHP_XEN_PREPARE,
+				       "x86/xen/hvm_guest:prepare",
+				       cpu_up_prepare_cb, cpu_dead_cb);
+	if (rc >= 0) {
+		rc = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
+					       "x86/xen/hvm_guest:online",
+					       xen_cpu_up_online, NULL);
+		if (rc < 0)
+			cpuhp_remove_state_nocalls(CPUHP_XEN_PREPARE);
+	}
+
+	return rc >= 0 ? 0 : rc;
+}
 
 static void clamp_max_cpus(void)
 {
@@ -243,1510 +170,81 @@ void xen_vcpu_setup(int cpu)
 	}
 }
 
-/*
- * On restore, set the vcpu placement up again.
- * If it fails, then we're in a bad state, since
- * we can't back out from using it...
- */
-void xen_vcpu_restore(void)
+void xen_reboot(int reason)
 {
+	struct sched_shutdown r = { .reason = reason };
 	int cpu;
 
-	for_each_possible_cpu(cpu) {
-		bool other_cpu = (cpu != smp_processor_id());
-		bool is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up, xen_vcpu_nr(cpu),
-						NULL);
-
-		if (other_cpu && is_up &&
-		    HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(cpu), NULL))
-			BUG();
-
-		xen_setup_runstate_info(cpu);
-
-		if (xen_have_vcpu_info_placement)
-			xen_vcpu_setup(cpu);
-
-		if (other_cpu && is_up &&
-		    HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL))
-			BUG();
-	}
-}
-
-static void __init xen_banner(void)
-{
-	unsigned version = HYPERVISOR_xen_version(XENVER_version, NULL);
-	struct xen_extraversion extra;
-	HYPERVISOR_xen_version(XENVER_extraversion, &extra);
-
-	pr_info("Booting paravirtualized kernel %son %s\n",
-		xen_feature(XENFEAT_auto_translated_physmap) ?
-			"with PVH extensions " : "", pv_info.name);
-	printk(KERN_INFO "Xen version: %d.%d%s%s\n",
-	       version >> 16, version & 0xffff, extra.extraversion,
-	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
-}
-/* Check if running on Xen version (major, minor) or later */
-bool
-xen_running_on_version_or_later(unsigned int major, unsigned int minor)
-{
-	unsigned int version;
-
-	if (!xen_domain())
-		return false;
-
-	version = HYPERVISOR_xen_version(XENVER_version, NULL);
-	if ((((version >> 16) == major) && ((version & 0xffff) >= minor)) ||
-		((version >> 16) > major))
-		return true;
-	return false;
-}
-
-#define CPUID_THERM_POWER_LEAF 6
-#define APERFMPERF_PRESENT 0
-
-static __read_mostly unsigned int cpuid_leaf1_edx_mask = ~0;
-static __read_mostly unsigned int cpuid_leaf1_ecx_mask = ~0;
-
-static __read_mostly unsigned int cpuid_leaf1_ecx_set_mask;
-static __read_mostly unsigned int cpuid_leaf5_ecx_val;
-static __read_mostly unsigned int cpuid_leaf5_edx_val;
-
-static void xen_cpuid(unsigned int *ax, unsigned int *bx,
-		      unsigned int *cx, unsigned int *dx)
-{
-	unsigned maskebx = ~0;
-	unsigned maskecx = ~0;
-	unsigned maskedx = ~0;
-	unsigned setecx = 0;
-	/*
-	 * Mask out inconvenient features, to try and disable as many
-	 * unsupported kernel subsystems as possible.
-	 */
-	switch (*ax) {
-	case 1:
-		maskecx = cpuid_leaf1_ecx_mask;
-		setecx = cpuid_leaf1_ecx_set_mask;
-		maskedx = cpuid_leaf1_edx_mask;
-		break;
-
-	case CPUID_MWAIT_LEAF:
-		/* Synthesize the values.. */
-		*ax = 0;
-		*bx = 0;
-		*cx = cpuid_leaf5_ecx_val;
-		*dx = cpuid_leaf5_edx_val;
-		return;
-
-	case CPUID_THERM_POWER_LEAF:
-		/* Disabling APERFMPERF for kernel usage */
-		maskecx = ~(1 << APERFMPERF_PRESENT);
-		break;
-
-	case 0xb:
-		/* Suppress extended topology stuff */
-		maskebx = 0;
-		break;
-	}
-
-	asm(XEN_EMULATE_PREFIX "cpuid"
-		: "=a" (*ax),
-		  "=b" (*bx),
-		  "=c" (*cx),
-		  "=d" (*dx)
-		: "0" (*ax), "2" (*cx));
-
-	*bx &= maskebx;
-	*cx &= maskecx;
-	*cx |= setecx;
-	*dx &= maskedx;
-}
-STACK_FRAME_NON_STANDARD(xen_cpuid); /* XEN_EMULATE_PREFIX */
-
-static bool __init xen_check_mwait(void)
-{
-#ifdef CONFIG_ACPI
-	struct xen_platform_op op = {
-		.cmd			= XENPF_set_processor_pminfo,
-		.u.set_pminfo.id	= -1,
-		.u.set_pminfo.type	= XEN_PM_PDC,
-	};
-	uint32_t buf[3];
-	unsigned int ax, bx, cx, dx;
-	unsigned int mwait_mask;
-
-	/* We need to determine whether it is OK to expose the MWAIT
-	 * capability to the kernel to harvest deeper than C3 states from ACPI
-	 * _CST using the processor_harvest_xen.c module. For this to work, we
-	 * need to gather the MWAIT_LEAF values (which the cstate.c code
-	 * checks against). The hypervisor won't expose the MWAIT flag because
-	 * it would break backwards compatibility; so we will find out directly
-	 * from the hardware and hypercall.
-	 */
-	if (!xen_initial_domain())
-		return false;
-
-	/*
-	 * When running under platform earlier than Xen4.2, do not expose
-	 * mwait, to avoid the risk of loading native acpi pad driver
-	 */
-	if (!xen_running_on_version_or_later(4, 2))
-		return false;
-
-	ax = 1;
-	cx = 0;
-
-	native_cpuid(&ax, &bx, &cx, &dx);
-
-	mwait_mask = (1 << (X86_FEATURE_EST % 32)) |
-		     (1 << (X86_FEATURE_MWAIT % 32));
-
-	if ((cx & mwait_mask) != mwait_mask)
-		return false;
-
-	/* We need to emulate the MWAIT_LEAF and for that we need both
-	 * ecx and edx. The hypercall provides only partial information.
-	 */
-
-	ax = CPUID_MWAIT_LEAF;
-	bx = 0;
-	cx = 0;
-	dx = 0;
-
-	native_cpuid(&ax, &bx, &cx, &dx);
-
-	/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,
-	 * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.
-	 */
-	buf[0] = ACPI_PDC_REVISION_ID;
-	buf[1] = 1;
-	buf[2] = (ACPI_PDC_C_CAPABILITY_SMP | ACPI_PDC_EST_CAPABILITY_SWSMP);
-
-	set_xen_guest_handle(op.u.set_pminfo.pdc, buf);
-
-	if ((HYPERVISOR_platform_op(&op) == 0) &&
-	    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {
-		cpuid_leaf5_ecx_val = cx;
-		cpuid_leaf5_edx_val = dx;
-	}
-	return true;
-#else
-	return false;
-#endif
-}
-static void __init xen_init_cpuid_mask(void)
-{
-	unsigned int ax, bx, cx, dx;
-	unsigned int xsave_mask;
-
-	cpuid_leaf1_edx_mask =
-		~((1 << X86_FEATURE_MTRR) |  /* disable MTRR */
-		  (1 << X86_FEATURE_ACC));   /* thermal monitoring */
-
-	if (!xen_initial_domain())
-		cpuid_leaf1_edx_mask &=
-			~((1 << X86_FEATURE_ACPI));  /* disable ACPI */
-
-	cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_X2APIC % 32));
-
-	ax = 1;
-	cx = 0;
-	cpuid(1, &ax, &bx, &cx, &dx);
-
-	xsave_mask =
-		(1 << (X86_FEATURE_XSAVE % 32)) |
-		(1 << (X86_FEATURE_OSXSAVE % 32));
-
-	/* Xen will set CR4.OSXSAVE if supported and not disabled by force */
-	if ((cx & xsave_mask) != xsave_mask)
-		cpuid_leaf1_ecx_mask &= ~xsave_mask; /* disable XSAVE & OSXSAVE */
-	if (xen_check_mwait())
-		cpuid_leaf1_ecx_set_mask = (1 << (X86_FEATURE_MWAIT % 32));
-}
-
-static void xen_set_debugreg(int reg, unsigned long val)
-{
-	HYPERVISOR_set_debugreg(reg, val);
-}
-
-static unsigned long xen_get_debugreg(int reg)
-{
-	return HYPERVISOR_get_debugreg(reg);
-}
-
-static void xen_end_context_switch(struct task_struct *next)
-{
-	xen_mc_flush();
-	paravirt_end_context_switch(next);
-}
-
-static unsigned long xen_store_tr(void)
-{
-	return 0;
-}
-
-/*
- * Set the page permissions for a particular virtual address.  If the
- * address is a vmalloc mapping (or other non-linear mapping), then
- * find the linear mapping of the page and also set its protections to
- * match.
- */
-static void set_aliased_prot(void *v, pgprot_t prot)
-{
-	int level;
-	pte_t *ptep;
-	pte_t pte;
-	unsigned long pfn;
-	struct page *page;
-	unsigned char dummy;
-
-	ptep = lookup_address((unsigned long)v, &level);
-	BUG_ON(ptep == NULL);
-
-	pfn = pte_pfn(*ptep);
-	page = pfn_to_page(pfn);
-
-	pte = pfn_pte(pfn, prot);
-
-	/*
-	 * Careful: update_va_mapping() will fail if the virtual address
-	 * we're poking isn't populated in the page tables.  We don't
-	 * need to worry about the direct map (that's always in the page
-	 * tables), but we need to be careful about vmap space.  In
-	 * particular, the top level page table can lazily propagate
-	 * entries between processes, so if we've switched mms since we
-	 * vmapped the target in the first place, we might not have the
-	 * top-level page table entry populated.
-	 *
-	 * We disable preemption because we want the same mm active when
-	 * we probe the target and when we issue the hypercall.  We'll
-	 * have the same nominal mm, but if we're a kernel thread, lazy
-	 * mm dropping could change our pgd.
-	 *
-	 * Out of an abundance of caution, this uses __get_user() to fault
-	 * in the target address just in case there's some obscure case
-	 * in which the target address isn't readable.
-	 */
-
-	preempt_disable();
-
-	probe_kernel_read(&dummy, v, 1);
+	for_each_online_cpu(cpu)
+		xen_pmu_finish(cpu);
 
-	if (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))
+	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
 		BUG();
-
-	if (!PageHighMem(page)) {
-		void *av = __va(PFN_PHYS(pfn));
-
-		if (av != v)
-			if (HYPERVISOR_update_va_mapping((unsigned long)av, pte, 0))
-				BUG();
-	} else
-		kmap_flush_unused();
-
-	preempt_enable();
-}
-
-static void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)
-{
-	const unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;
-	int i;
-
-	/*
-	 * We need to mark the all aliases of the LDT pages RO.  We
-	 * don't need to call vm_flush_aliases(), though, since that's
-	 * only responsible for flushing aliases out the TLBs, not the
-	 * page tables, and Xen will flush the TLB for us if needed.
-	 *
-	 * To avoid confusing future readers: none of this is necessary
-	 * to load the LDT.  The hypervisor only checks this when the
-	 * LDT is faulted in due to subsequent descriptor access.
-	 */
-
-	for(i = 0; i < entries; i += entries_per_page)
-		set_aliased_prot(ldt + i, PAGE_KERNEL_RO);
 }
 
-static void xen_free_ldt(struct desc_struct *ldt, unsigned entries)
-{
-	const unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;
-	int i;
-
-	for(i = 0; i < entries; i += entries_per_page)
-		set_aliased_prot(ldt + i, PAGE_KERNEL);
-}
-
-static void xen_set_ldt(const void *addr, unsigned entries)
+void xen_emergency_restart(void)
 {
-	struct mmuext_op *op;
-	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
-
-	trace_xen_cpu_set_ldt(addr, entries);
-
-	op = mcs.args;
-	op->cmd = MMUEXT_SET_LDT;
-	op->arg1.linear_addr = (unsigned long)addr;
-	op->arg2.nr_ents = entries;
-
-	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
+	xen_reboot(SHUTDOWN_reboot);
 }
 
-static void xen_load_gdt(const struct desc_ptr *dtr)
+static int
+xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
 {
-	unsigned long va = dtr->address;
-	unsigned int size = dtr->size + 1;
-	unsigned pages = DIV_ROUND_UP(size, PAGE_SIZE);
-	unsigned long frames[pages];
-	int f;
-
-	/*
-	 * A GDT can be up to 64k in size, which corresponds to 8192
-	 * 8-byte entries, or 16 4k pages..
-	 */
-
-	BUG_ON(size > 65536);
-	BUG_ON(va & ~PAGE_MASK);
-
-	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
-		int level;
-		pte_t *ptep;
-		unsigned long pfn, mfn;
-		void *virt;
-
-		/*
-		 * The GDT is per-cpu and is in the percpu data area.
-		 * That can be virtually mapped, so we need to do a
-		 * page-walk to get the underlying MFN for the
-		 * hypercall.  The page can also be in the kernel's
-		 * linear range, so we need to RO that mapping too.
-		 */
-		ptep = lookup_address(va, &level);
-		BUG_ON(ptep == NULL);
-
-		pfn = pte_pfn(*ptep);
-		mfn = pfn_to_mfn(pfn);
-		virt = __va(PFN_PHYS(pfn));
-
-		frames[f] = mfn;
-
-		make_lowmem_page_readonly((void *)va);
-		make_lowmem_page_readonly(virt);
-	}
-
-	if (HYPERVISOR_set_gdt(frames, size / sizeof(struct desc_struct)))
-		BUG();
+	if (!kexec_crash_loaded())
+		xen_reboot(SHUTDOWN_crash);
+	return NOTIFY_DONE;
 }
 
-/*
- * load_gdt for early boot, when the gdt is only mapped once
- */
-static void __init xen_load_gdt_boot(const struct desc_ptr *dtr)
-{
-	unsigned long va = dtr->address;
-	unsigned int size = dtr->size + 1;
-	unsigned pages = DIV_ROUND_UP(size, PAGE_SIZE);
-	unsigned long frames[pages];
-	int f;
-
-	/*
-	 * A GDT can be up to 64k in size, which corresponds to 8192
-	 * 8-byte entries, or 16 4k pages..
-	 */
-
-	BUG_ON(size > 65536);
-	BUG_ON(va & ~PAGE_MASK);
-
-	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
-		pte_t pte;
-		unsigned long pfn, mfn;
-
-		pfn = virt_to_pfn(va);
-		mfn = pfn_to_mfn(pfn);
-
-		pte = pfn_pte(pfn, PAGE_KERNEL_RO);
-
-		if (HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))
-			BUG();
-
-		frames[f] = mfn;
-	}
-
-	if (HYPERVISOR_set_gdt(frames, size / sizeof(struct desc_struct)))
-		BUG();
-}
+static struct notifier_block xen_panic_block = {
+	.notifier_call = xen_panic_event,
+	.priority = INT_MIN
+};
 
-static inline bool desc_equal(const struct desc_struct *d1,
-			      const struct desc_struct *d2)
+int xen_panic_handler_init(void)
 {
-	return d1->a == d2->a && d1->b == d2->b;
+	atomic_notifier_chain_register(&panic_notifier_list, &xen_panic_block);
+	return 0;
 }
 
-static void load_TLS_descriptor(struct thread_struct *t,
-				unsigned int cpu, unsigned int i)
+void xen_pin_vcpu(int cpu)
 {
-	struct desc_struct *shadow = &per_cpu(shadow_tls_desc, cpu).desc[i];
-	struct desc_struct *gdt;
-	xmaddr_t maddr;
-	struct multicall_space mc;
+	static bool disable_pinning;
+	struct sched_pin_override pin_override;
+	int ret;
 
-	if (desc_equal(shadow, &t->tls_array[i]))
+	if (disable_pinning)
 		return;
 
-	*shadow = t->tls_array[i];
-
-	gdt = get_cpu_gdt_rw(cpu);
-	maddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);
-	mc = __xen_mc_entry(0);
+	pin_override.pcpu = cpu;
+	ret = HYPERVISOR_sched_op(SCHEDOP_pin_override, &pin_override);
 
-	MULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);
-}
+	/* Ignore errors when removing override. */
+	if (cpu < 0)
+		return;
 
-static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
-{
-	/*
-	 * XXX sleazy hack: If we're being called in a lazy-cpu zone
-	 * and lazy gs handling is enabled, it means we're in a
-	 * context switch, and %gs has just been saved.  This means we
-	 * can zero it out to prevent faults on exit from the
-	 * hypervisor if the next process has no %gs.  Either way, it
-	 * has been saved, and the new value will get loaded properly.
-	 * This will go away as soon as Xen has been modified to not
-	 * save/restore %gs for normal hypercalls.
-	 *
-	 * On x86_64, this hack is not used for %gs, because gs points
-	 * to KERNEL_GS_BASE (and uses it for PDA references), so we
-	 * must not zero %gs on x86_64
-	 *
-	 * For x86_64, we need to zero %fs, otherwise we may get an
-	 * exception between the new %fs descriptor being loaded and
-	 * %fs being effectively cleared at __switch_to().
-	 */
-	if (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU) {
-#ifdef CONFIG_X86_32
-		lazy_load_gs(0);
-#else
-		loadsegment(fs, 0);
-#endif
+	switch (ret) {
+	case -ENOSYS:
+		pr_warn("Unable to pin on physical cpu %d. In case of problems consider vcpu pinning.\n",
+			cpu);
+		disable_pinning = true;
+		break;
+	case -EPERM:
+		WARN(1, "Trying to pin vcpu without having privilege to do so\n");
+		disable_pinning = true;
+		break;
+	case -EINVAL:
+	case -EBUSY:
+		pr_warn("Physical cpu %d not available for pinning. Check Xen cpu configuration.\n",
+			cpu);
+		break;
+	case 0:
+		break;
+	default:
+		WARN(1, "rc %d while trying to pin vcpu\n", ret);
+		disable_pinning = true;
 	}
-
-	xen_mc_batch();
-
-	load_TLS_descriptor(t, cpu, 0);
-	load_TLS_descriptor(t, cpu, 1);
-	load_TLS_descriptor(t, cpu, 2);
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
-}
-
-#ifdef CONFIG_X86_64
-static void xen_load_gs_index(unsigned int idx)
-{
-	if (HYPERVISOR_set_segment_base(SEGBASE_GS_USER_SEL, idx))
-		BUG();
-}
-#endif
-
-static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
-				const void *ptr)
-{
-	xmaddr_t mach_lp = arbitrary_virt_to_machine(&dt[entrynum]);
-	u64 entry = *(u64 *)ptr;
-
-	trace_xen_cpu_write_ldt_entry(dt, entrynum, entry);
-
-	preempt_disable();
-
-	xen_mc_flush();
-	if (HYPERVISOR_update_descriptor(mach_lp.maddr, entry))
-		BUG();
-
-	preempt_enable();
 }
 
-static int cvt_gate_to_trap(int vector, const gate_desc *val,
-			    struct trap_info *info)
-{
-	unsigned long addr;
-
-	if (val->type != GATE_TRAP && val->type != GATE_INTERRUPT)
-		return 0;
-
-	info->vector = vector;
-
-	addr = gate_offset(*val);
-#ifdef CONFIG_X86_64
-	/*
-	 * Look for known traps using IST, and substitute them
-	 * appropriately.  The debugger ones are the only ones we care
-	 * about.  Xen will handle faults like double_fault,
-	 * so we should never see them.  Warn if
-	 * there's an unexpected IST-using fault handler.
-	 */
-	if (addr == (unsigned long)debug)
-		addr = (unsigned long)xen_debug;
-	else if (addr == (unsigned long)int3)
-		addr = (unsigned long)xen_int3;
-	else if (addr == (unsigned long)stack_segment)
-		addr = (unsigned long)xen_stack_segment;
-	else if (addr == (unsigned long)double_fault) {
-		/* Don't need to handle these */
-		return 0;
-#ifdef CONFIG_X86_MCE
-	} else if (addr == (unsigned long)machine_check) {
-		/*
-		 * when xen hypervisor inject vMCE to guest,
-		 * use native mce handler to handle it
-		 */
-		;
-#endif
-	} else if (addr == (unsigned long)nmi)
-		/*
-		 * Use the native version as well.
-		 */
-		;
-	else {
-		/* Some other trap using IST? */
-		if (WARN_ON(val->ist != 0))
-			return 0;
-	}
-#endif	/* CONFIG_X86_64 */
-	info->address = addr;
-
-	info->cs = gate_segment(*val);
-	info->flags = val->dpl;
-	/* interrupt gates clear IF */
-	if (val->type == GATE_INTERRUPT)
-		info->flags |= 1 << 2;
-
-	return 1;
-}
-
-/* Locations of each CPU's IDT */
-static DEFINE_PER_CPU(struct desc_ptr, idt_desc);
-
-/* Set an IDT entry.  If the entry is part of the current IDT, then
-   also update Xen. */
-static void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)
-{
-	unsigned long p = (unsigned long)&dt[entrynum];
-	unsigned long start, end;
-
-	trace_xen_cpu_write_idt_entry(dt, entrynum, g);
-
-	preempt_disable();
-
-	start = __this_cpu_read(idt_desc.address);
-	end = start + __this_cpu_read(idt_desc.size) + 1;
-
-	xen_mc_flush();
-
-	native_write_idt_entry(dt, entrynum, g);
-
-	if (p >= start && (p + 8) <= end) {
-		struct trap_info info[2];
-
-		info[1].address = 0;
-
-		if (cvt_gate_to_trap(entrynum, g, &info[0]))
-			if (HYPERVISOR_set_trap_table(info))
-				BUG();
-	}
-
-	preempt_enable();
-}
-
-static void xen_convert_trap_info(const struct desc_ptr *desc,
-				  struct trap_info *traps)
-{
-	unsigned in, out, count;
-
-	count = (desc->size+1) / sizeof(gate_desc);
-	BUG_ON(count > 256);
-
-	for (in = out = 0; in < count; in++) {
-		gate_desc *entry = (gate_desc*)(desc->address) + in;
-
-		if (cvt_gate_to_trap(in, entry, &traps[out]))
-			out++;
-	}
-	traps[out].address = 0;
-}
-
-void xen_copy_trap_info(struct trap_info *traps)
-{
-	const struct desc_ptr *desc = this_cpu_ptr(&idt_desc);
-
-	xen_convert_trap_info(desc, traps);
-}
-
-/* Load a new IDT into Xen.  In principle this can be per-CPU, so we
-   hold a spinlock to protect the static traps[] array (static because
-   it avoids allocation, and saves stack space). */
-static void xen_load_idt(const struct desc_ptr *desc)
-{
-	static DEFINE_SPINLOCK(lock);
-	static struct trap_info traps[257];
-
-	trace_xen_cpu_load_idt(desc);
-
-	spin_lock(&lock);
-
-	memcpy(this_cpu_ptr(&idt_desc), desc, sizeof(idt_desc));
-
-	xen_convert_trap_info(desc, traps);
-
-	xen_mc_flush();
-	if (HYPERVISOR_set_trap_table(traps))
-		BUG();
-
-	spin_unlock(&lock);
-}
-
-/* Write a GDT descriptor entry.  Ignore LDT descriptors, since
-   they're handled differently. */
-static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
-				const void *desc, int type)
-{
-	trace_xen_cpu_write_gdt_entry(dt, entry, desc, type);
-
-	preempt_disable();
-
-	switch (type) {
-	case DESC_LDT:
-	case DESC_TSS:
-		/* ignore */
-		break;
-
-	default: {
-		xmaddr_t maddr = arbitrary_virt_to_machine(&dt[entry]);
-
-		xen_mc_flush();
-		if (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))
-			BUG();
-	}
-
-	}
-
-	preempt_enable();
-}
-
-/*
- * Version of write_gdt_entry for use at early boot-time needed to
- * update an entry as simply as possible.
- */
-static void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,
-					    const void *desc, int type)
-{
-	trace_xen_cpu_write_gdt_entry(dt, entry, desc, type);
-
-	switch (type) {
-	case DESC_LDT:
-	case DESC_TSS:
-		/* ignore */
-		break;
-
-	default: {
-		xmaddr_t maddr = virt_to_machine(&dt[entry]);
-
-		if (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))
-			dt[entry] = *(struct desc_struct *)desc;
-	}
-
-	}
-}
-
-static void xen_load_sp0(struct tss_struct *tss,
-			 struct thread_struct *thread)
-{
-	struct multicall_space mcs;
-
-	mcs = xen_mc_entry(0);
-	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread->sp0);
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
-	tss->x86_tss.sp0 = thread->sp0;
-}
-
-void xen_set_iopl_mask(unsigned mask)
-{
-	struct physdev_set_iopl set_iopl;
-
-	/* Force the change at ring 0. */
-	set_iopl.iopl = (mask == 0) ? 1 : (mask >> 12) & 3;
-	HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
-}
-
-static void xen_io_delay(void)
-{
-}
-
-static DEFINE_PER_CPU(unsigned long, xen_cr0_value);
-
-static unsigned long xen_read_cr0(void)
-{
-	unsigned long cr0 = this_cpu_read(xen_cr0_value);
-
-	if (unlikely(cr0 == 0)) {
-		cr0 = native_read_cr0();
-		this_cpu_write(xen_cr0_value, cr0);
-	}
-
-	return cr0;
-}
-
-static void xen_write_cr0(unsigned long cr0)
-{
-	struct multicall_space mcs;
-
-	this_cpu_write(xen_cr0_value, cr0);
-
-	/* Only pay attention to cr0.TS; everything else is
-	   ignored. */
-	mcs = xen_mc_entry(0);
-
-	MULTI_fpu_taskswitch(mcs.mc, (cr0 & X86_CR0_TS) != 0);
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
-}
-
-static void xen_write_cr4(unsigned long cr4)
-{
-	cr4 &= ~(X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PCE);
-
-	native_write_cr4(cr4);
-}
-#ifdef CONFIG_X86_64
-static inline unsigned long xen_read_cr8(void)
-{
-	return 0;
-}
-static inline void xen_write_cr8(unsigned long val)
-{
-	BUG_ON(val);
-}
-#endif
-
-static u64 xen_read_msr_safe(unsigned int msr, int *err)
-{
-	u64 val;
-
-	if (pmu_msr_read(msr, &val, err))
-		return val;
-
-	val = native_read_msr_safe(msr, err);
-	switch (msr) {
-	case MSR_IA32_APICBASE:
-#ifdef CONFIG_X86_X2APIC
-		if (!(cpuid_ecx(1) & (1 << (X86_FEATURE_X2APIC & 31))))
-#endif
-			val &= ~X2APIC_ENABLE;
-		break;
-	}
-	return val;
-}
-
-static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
-{
-	int ret;
-
-	ret = 0;
-
-	switch (msr) {
-#ifdef CONFIG_X86_64
-		unsigned which;
-		u64 base;
-
-	case MSR_FS_BASE:		which = SEGBASE_FS; goto set;
-	case MSR_KERNEL_GS_BASE:	which = SEGBASE_GS_USER; goto set;
-	case MSR_GS_BASE:		which = SEGBASE_GS_KERNEL; goto set;
-
-	set:
-		base = ((u64)high << 32) | low;
-		if (HYPERVISOR_set_segment_base(which, base) != 0)
-			ret = -EIO;
-		break;
-#endif
-
-	case MSR_STAR:
-	case MSR_CSTAR:
-	case MSR_LSTAR:
-	case MSR_SYSCALL_MASK:
-	case MSR_IA32_SYSENTER_CS:
-	case MSR_IA32_SYSENTER_ESP:
-	case MSR_IA32_SYSENTER_EIP:
-		/* Fast syscall setup is all done in hypercalls, so
-		   these are all ignored.  Stub them out here to stop
-		   Xen console noise. */
-		break;
-
-	default:
-		if (!pmu_msr_write(msr, low, high, &ret))
-			ret = native_write_msr_safe(msr, low, high);
-	}
-
-	return ret;
-}
-
-static u64 xen_read_msr(unsigned int msr)
-{
-	/*
-	 * This will silently swallow a #GP from RDMSR.  It may be worth
-	 * changing that.
-	 */
-	int err;
-
-	return xen_read_msr_safe(msr, &err);
-}
-
-static void xen_write_msr(unsigned int msr, unsigned low, unsigned high)
-{
-	/*
-	 * This will silently swallow a #GP from WRMSR.  It may be worth
-	 * changing that.
-	 */
-	xen_write_msr_safe(msr, low, high);
-}
-
-void xen_setup_shared_info(void)
-{
-	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
-		set_fixmap(FIX_PARAVIRT_BOOTMAP,
-			   xen_start_info->shared_info);
-
-		HYPERVISOR_shared_info =
-			(struct shared_info *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);
-	} else
-		HYPERVISOR_shared_info =
-			(struct shared_info *)__va(xen_start_info->shared_info);
-
-#ifndef CONFIG_SMP
-	/* In UP this is as good a place as any to set up shared info */
-	xen_setup_vcpu_info_placement();
-#endif
-
-	xen_setup_mfn_list_list();
-}
-
-/* This is called once we have the cpu_possible_mask */
-void xen_setup_vcpu_info_placement(void)
-{
-	int cpu;
-
-	for_each_possible_cpu(cpu) {
-		/* Set up direct vCPU id mapping for PV guests. */
-		per_cpu(xen_vcpu_id, cpu) = cpu;
-		xen_vcpu_setup(cpu);
-	}
-
-	/*
-	 * xen_vcpu_setup managed to place the vcpu_info within the
-	 * percpu area for all cpus, so make use of it.
-	 */
-	if (xen_have_vcpu_info_placement) {
-		pv_irq_ops.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);
-		pv_irq_ops.restore_fl = __PV_IS_CALLEE_SAVE(xen_restore_fl_direct);
-		pv_irq_ops.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);
-		pv_irq_ops.irq_enable = __PV_IS_CALLEE_SAVE(xen_irq_enable_direct);
-		pv_mmu_ops.read_cr2 = xen_read_cr2_direct;
-	}
-}
-
-static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
-			  unsigned long addr, unsigned len)
-{
-	char *start, *end, *reloc;
-	unsigned ret;
-
-	start = end = reloc = NULL;
-
-#define SITE(op, x)							\
-	case PARAVIRT_PATCH(op.x):					\
-	if (xen_have_vcpu_info_placement) {				\
-		start = (char *)xen_##x##_direct;			\
-		end = xen_##x##_direct_end;				\
-		reloc = xen_##x##_direct_reloc;				\
-	}								\
-	goto patch_site
-
-	switch (type) {
-		SITE(pv_irq_ops, irq_enable);
-		SITE(pv_irq_ops, irq_disable);
-		SITE(pv_irq_ops, save_fl);
-		SITE(pv_irq_ops, restore_fl);
-#undef SITE
-
-	patch_site:
-		if (start == NULL || (end-start) > len)
-			goto default_patch;
-
-		ret = paravirt_patch_insns(insnbuf, len, start, end);
-
-		/* Note: because reloc is assigned from something that
-		   appears to be an array, gcc assumes it's non-null,
-		   but doesn't know its relationship with start and
-		   end. */
-		if (reloc > start && reloc < end) {
-			int reloc_off = reloc - start;
-			long *relocp = (long *)(insnbuf + reloc_off);
-			long delta = start - (char *)addr;
-
-			*relocp += delta;
-		}
-		break;
-
-	default_patch:
-	default:
-		ret = paravirt_patch_default(type, clobbers, insnbuf,
-					     addr, len);
-		break;
-	}
-
-	return ret;
-}
-
-static const struct pv_info xen_info __initconst = {
-	.shared_kernel_pmd = 0,
-
-#ifdef CONFIG_X86_64
-	.extra_user_64bit_cs = FLAT_USER_CS64,
-#endif
-	.name = "Xen",
-};
-
-static const struct pv_init_ops xen_init_ops __initconst = {
-	.patch = xen_patch,
-};
-
-static const struct pv_cpu_ops xen_cpu_ops __initconst = {
-	.cpuid = xen_cpuid,
-
-	.set_debugreg = xen_set_debugreg,
-	.get_debugreg = xen_get_debugreg,
-
-	.read_cr0 = xen_read_cr0,
-	.write_cr0 = xen_write_cr0,
-
-	.read_cr4 = native_read_cr4,
-	.write_cr4 = xen_write_cr4,
-
-#ifdef CONFIG_X86_64
-	.read_cr8 = xen_read_cr8,
-	.write_cr8 = xen_write_cr8,
-#endif
-
-	.wbinvd = native_wbinvd,
-
-	.read_msr = xen_read_msr,
-	.write_msr = xen_write_msr,
-
-	.read_msr_safe = xen_read_msr_safe,
-	.write_msr_safe = xen_write_msr_safe,
-
-	.read_pmc = xen_read_pmc,
-
-	.iret = xen_iret,
-#ifdef CONFIG_X86_64
-	.usergs_sysret64 = xen_sysret64,
-#endif
-
-	.load_tr_desc = paravirt_nop,
-	.set_ldt = xen_set_ldt,
-	.load_gdt = xen_load_gdt,
-	.load_idt = xen_load_idt,
-	.load_tls = xen_load_tls,
-#ifdef CONFIG_X86_64
-	.load_gs_index = xen_load_gs_index,
-#endif
-
-	.alloc_ldt = xen_alloc_ldt,
-	.free_ldt = xen_free_ldt,
-
-	.store_idt = native_store_idt,
-	.store_tr = xen_store_tr,
-
-	.write_ldt_entry = xen_write_ldt_entry,
-	.write_gdt_entry = xen_write_gdt_entry,
-	.write_idt_entry = xen_write_idt_entry,
-	.load_sp0 = xen_load_sp0,
-
-	.set_iopl_mask = xen_set_iopl_mask,
-	.io_delay = xen_io_delay,
-
-	/* Xen takes care of %gs when switching to usermode for us */
-	.swapgs = paravirt_nop,
-
-	.start_context_switch = paravirt_start_context_switch,
-	.end_context_switch = xen_end_context_switch,
-};
-
-void xen_reboot(int reason)
-{
-	struct sched_shutdown r = { .reason = reason };
-	int cpu;
-
-	for_each_online_cpu(cpu)
-		xen_pmu_finish(cpu);
-
-	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
-		BUG();
-}
-
-static void xen_restart(char *msg)
-{
-	xen_reboot(SHUTDOWN_reboot);
-}
-
-void xen_emergency_restart(void)
-{
-	xen_reboot(SHUTDOWN_reboot);
-}
-
-static void xen_machine_halt(void)
-{
-	xen_reboot(SHUTDOWN_poweroff);
-}
-
-static void xen_machine_power_off(void)
-{
-	if (pm_power_off)
-		pm_power_off();
-	xen_reboot(SHUTDOWN_poweroff);
-}
-
-static void xen_crash_shutdown(struct pt_regs *regs)
-{
-	xen_reboot(SHUTDOWN_crash);
-}
-
-static int
-xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
-{
-	if (!kexec_crash_loaded())
-		xen_reboot(SHUTDOWN_crash);
-	return NOTIFY_DONE;
-}
-
-static struct notifier_block xen_panic_block = {
-	.notifier_call= xen_panic_event,
-	.priority = INT_MIN
-};
-
-int xen_panic_handler_init(void)
-{
-	atomic_notifier_chain_register(&panic_notifier_list, &xen_panic_block);
-	return 0;
-}
-
-static const struct machine_ops xen_machine_ops __initconst = {
-	.restart = xen_restart,
-	.halt = xen_machine_halt,
-	.power_off = xen_machine_power_off,
-	.shutdown = xen_machine_halt,
-	.crash_shutdown = xen_crash_shutdown,
-	.emergency_restart = xen_emergency_restart,
-};
-
-static unsigned char xen_get_nmi_reason(void)
-{
-	unsigned char reason = 0;
-
-	/* Construct a value which looks like it came from port 0x61. */
-	if (test_bit(_XEN_NMIREASON_io_error,
-		     &HYPERVISOR_shared_info->arch.nmi_reason))
-		reason |= NMI_REASON_IOCHK;
-	if (test_bit(_XEN_NMIREASON_pci_serr,
-		     &HYPERVISOR_shared_info->arch.nmi_reason))
-		reason |= NMI_REASON_SERR;
-
-	return reason;
-}
-
-static void __init xen_boot_params_init_edd(void)
-{
-#if IS_ENABLED(CONFIG_EDD)
-	struct xen_platform_op op;
-	struct edd_info *edd_info;
-	u32 *mbr_signature;
-	unsigned nr;
-	int ret;
-
-	edd_info = boot_params.eddbuf;
-	mbr_signature = boot_params.edd_mbr_sig_buffer;
-
-	op.cmd = XENPF_firmware_info;
-
-	op.u.firmware_info.type = XEN_FW_DISK_INFO;
-	for (nr = 0; nr < EDDMAXNR; nr++) {
-		struct edd_info *info = edd_info + nr;
-
-		op.u.firmware_info.index = nr;
-		info->params.length = sizeof(info->params);
-		set_xen_guest_handle(op.u.firmware_info.u.disk_info.edd_params,
-				     &info->params);
-		ret = HYPERVISOR_platform_op(&op);
-		if (ret)
-			break;
-
-#define C(x) info->x = op.u.firmware_info.u.disk_info.x
-		C(device);
-		C(version);
-		C(interface_support);
-		C(legacy_max_cylinder);
-		C(legacy_max_head);
-		C(legacy_sectors_per_track);
-#undef C
-	}
-	boot_params.eddbuf_entries = nr;
-
-	op.u.firmware_info.type = XEN_FW_DISK_MBR_SIGNATURE;
-	for (nr = 0; nr < EDD_MBR_SIG_MAX; nr++) {
-		op.u.firmware_info.index = nr;
-		ret = HYPERVISOR_platform_op(&op);
-		if (ret)
-			break;
-		mbr_signature[nr] = op.u.firmware_info.u.disk_mbr_signature.mbr_signature;
-	}
-	boot_params.edd_mbr_sig_buf_entries = nr;
-#endif
-}
-
-/*
- * Set up the GDT and segment registers for -fstack-protector.  Until
- * we do this, we have to be careful not to call any stack-protected
- * function, which is most of the kernel.
- */
-static void xen_setup_gdt(int cpu)
-{
-	pv_cpu_ops.write_gdt_entry = xen_write_gdt_entry_boot;
-	pv_cpu_ops.load_gdt = xen_load_gdt_boot;
-
-	setup_stack_canary_segment(0);
-	switch_to_new_gdt(0);
-
-	pv_cpu_ops.write_gdt_entry = xen_write_gdt_entry;
-	pv_cpu_ops.load_gdt = xen_load_gdt;
-}
-
-static void __init xen_dom0_set_legacy_features(void)
-{
-	x86_platform.legacy.rtc = 1;
-}
-
-int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
-		    int (*cpu_dead_cb)(unsigned int))
-{
-	int rc;
-
-	rc = cpuhp_setup_state_nocalls(CPUHP_XEN_PREPARE,
-				       "x86/xen/hvm_guest:prepare",
-				       cpu_up_prepare_cb, cpu_dead_cb);
-	if (rc >= 0) {
-		rc = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
-					       "x86/xen/hvm_guest:online",
-					       xen_cpu_up_online, NULL);
-		if (rc < 0)
-			cpuhp_remove_state_nocalls(CPUHP_XEN_PREPARE);
-	}
-
-	return rc >= 0 ? 0 : rc;
-}
-
-/* First C function to be called on Xen boot */
-asmlinkage __visible void __init xen_start_kernel(void)
-{
-	struct physdev_set_iopl set_iopl;
-	unsigned long initrd_start = 0;
-	int rc;
-
-	if (!xen_start_info)
-		return;
-
-	xen_domain_type = XEN_PV_DOMAIN;
-
-	xen_setup_features();
-
-	xen_setup_machphys_mapping();
-
-	/* Install Xen paravirt ops */
-	pv_info = xen_info;
-	pv_init_ops = xen_init_ops;
-	pv_cpu_ops = xen_cpu_ops;
-
-	x86_platform.get_nmi_reason = xen_get_nmi_reason;
-
-	x86_init.resources.memory_setup = xen_memory_setup;
-	x86_init.oem.arch_setup = xen_arch_setup;
-	x86_init.oem.banner = xen_banner;
-
-	xen_init_time_ops();
-
-	/*
-	 * Set up some pagetable state before starting to set any ptes.
-	 */
-
-	xen_init_mmu_ops();
-
-	/* Prevent unwanted bits from being set in PTEs. */
-	__supported_pte_mask &= ~_PAGE_GLOBAL;
-
-	/*
-	 * Prevent page tables from being allocated in highmem, even
-	 * if CONFIG_HIGHPTE is enabled.
-	 */
-	__userpte_alloc_gfp &= ~__GFP_HIGHMEM;
-
-	/* Work out if we support NX */
-	x86_configure_nx();
-
-	/* Get mfn list */
-	xen_build_dynamic_phys_to_machine();
-
-	/*
-	 * Set up kernel GDT and segment registers, mainly so that
-	 * -fstack-protector code can be executed.
-	 */
-	xen_setup_gdt(0);
-
-	xen_init_irq_ops();
-	xen_init_cpuid_mask();
-
-#ifdef CONFIG_X86_LOCAL_APIC
-	/*
-	 * set up the basic apic ops.
-	 */
-	xen_init_apic();
-#endif
-
-	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {
-		pv_mmu_ops.ptep_modify_prot_start = xen_ptep_modify_prot_start;
-		pv_mmu_ops.ptep_modify_prot_commit = xen_ptep_modify_prot_commit;
-	}
-
-	machine_ops = xen_machine_ops;
-
-	/*
-	 * The only reliable way to retain the initial address of the
-	 * percpu gdt_page is to remember it here, so we can go and
-	 * mark it RW later, when the initial percpu area is freed.
-	 */
-	xen_initial_gdt = &per_cpu(gdt_page, 0);
-
-	xen_smp_init();
-
-#ifdef CONFIG_ACPI_NUMA
-	/*
-	 * The pages we from Xen are not related to machine pages, so
-	 * any NUMA information the kernel tries to get from ACPI will
-	 * be meaningless.  Prevent it from trying.
-	 */
-	acpi_numa = -1;
-#endif
-	/* Don't do the full vcpu_info placement stuff until we have a
-	   possible map and a non-dummy shared_info. */
-	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
-
-	WARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_pv, xen_cpu_dead_pv));
-
-	local_irq_disable();
-	early_boot_irqs_disabled = true;
-
-	xen_raw_console_write("mapping kernel into physical memory\n");
-	xen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base,
-				   xen_start_info->nr_pages);
-	xen_reserve_special_pages();
-
-	/* keep using Xen gdt for now; no urgent need to change it */
-
-#ifdef CONFIG_X86_32
-	pv_info.kernel_rpl = 1;
-	if (xen_feature(XENFEAT_supervisor_mode_kernel))
-		pv_info.kernel_rpl = 0;
-#else
-	pv_info.kernel_rpl = 0;
-#endif
-	/* set the limit of our address space */
-	xen_reserve_top();
-
-	/*
-	 * We used to do this in xen_arch_setup, but that is too late
-	 * on AMD were early_cpu_init (run before ->arch_setup()) calls
-	 * early_amd_init which pokes 0xcf8 port.
-	 */
-	set_iopl.iopl = 1;
-	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
-	if (rc != 0)
-		xen_raw_printk("physdev_op failed %d\n", rc);
-
-#ifdef CONFIG_X86_32
-	/* set up basic CPUID stuff */
-	cpu_detect(&new_cpu_data);
-	set_cpu_cap(&new_cpu_data, X86_FEATURE_FPU);
-	new_cpu_data.x86_capability[CPUID_1_EDX] = cpuid_edx(1);
-#endif
-
-	if (xen_start_info->mod_start) {
-	    if (xen_start_info->flags & SIF_MOD_START_PFN)
-		initrd_start = PFN_PHYS(xen_start_info->mod_start);
-	    else
-		initrd_start = __pa(xen_start_info->mod_start);
-	}
-
-	/* Poke various useful things into boot_params */
-	boot_params.hdr.type_of_loader = (9 << 4) | 0;
-	boot_params.hdr.ramdisk_image = initrd_start;
-	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
-	boot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);
-	boot_params.hdr.hardware_subarch = X86_SUBARCH_XEN;
-
-	if (!xen_initial_domain()) {
-		add_preferred_console("xenboot", 0, NULL);
-		add_preferred_console("tty", 0, NULL);
-		add_preferred_console("hvc", 0, NULL);
-		if (pci_xen)
-			x86_init.pci.arch_init = pci_xen_init;
-	} else {
-		const struct dom0_vga_console_info *info =
-			(void *)((char *)xen_start_info +
-				 xen_start_info->console.dom0.info_off);
-		struct xen_platform_op op = {
-			.cmd = XENPF_firmware_info,
-			.interface_version = XENPF_INTERFACE_VERSION,
-			.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS,
-		};
-
-		x86_platform.set_legacy_features =
-				xen_dom0_set_legacy_features;
-		xen_init_vga(info, xen_start_info->console.dom0.info_size);
-		xen_start_info->console.domU.mfn = 0;
-		xen_start_info->console.domU.evtchn = 0;
-
-		if (HYPERVISOR_platform_op(&op) == 0)
-			boot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;
-
-		/* Make sure ACS will be enabled */
-		pci_request_acs();
-
-		xen_acpi_sleep_register();
-
-		/* Avoid searching for BIOS MP tables */
-		x86_init.mpparse.find_smp_config = x86_init_noop;
-		x86_init.mpparse.get_smp_config = x86_init_uint_noop;
-
-		xen_boot_params_init_edd();
-	}
-#ifdef CONFIG_PCI
-	/* PCI BIOS service won't work from a PV guest. */
-	pci_probe &= ~PCI_PROBE_BIOS;
-#endif
-	xen_raw_console_write("about to get started...\n");
-
-	/* Let's presume PV guests always boot on vCPU with id 0. */
-	per_cpu(xen_vcpu_id, 0) = 0;
-
-	xen_setup_runstate_info(0);
-
-	xen_efi_init();
-
-	/* Start the world */
-#ifdef CONFIG_X86_32
-	i386_start_kernel();
-#else
-	cr4_init_shadow(); /* 32b kernel does this in i386_start_kernel() */
-	x86_64_start_reservations((char *)__pa_symbol(&boot_params));
-#endif
-}
-
-static int xen_cpu_up_prepare_pv(unsigned int cpu)
-{
-	int rc;
-
-	xen_setup_timer(cpu);
-
-	rc = xen_smp_intr_init(cpu);
-	if (rc) {
-		WARN(1, "xen_smp_intr_init() for CPU %d failed: %d\n",
-		     cpu, rc);
-		return rc;
-	}
-	return 0;
-}
-
-static int xen_cpu_dead_pv(unsigned int cpu)
-{
-	xen_smp_intr_free(cpu);
-
-	xen_teardown_timer(cpu);
-
-	return 0;
-}
-
-static int xen_cpu_up_online(unsigned int cpu)
-{
-	xen_init_lock_cpu(cpu);
-	return 0;
-}
-
-static uint32_t __init xen_platform_pv(void)
-{
-	if (xen_pv_domain())
-		return xen_cpuid_base();
-
-	return 0;
-}
-
-static void xen_set_cpu_features(struct cpuinfo_x86 *c)
-{
-	clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
-	set_cpu_cap(c, X86_FEATURE_XENPV);
-}
-
-void xen_pin_vcpu(int cpu)
-{
-	static bool disable_pinning;
-	struct sched_pin_override pin_override;
-	int ret;
-
-	if (disable_pinning)
-		return;
-
-	pin_override.pcpu = cpu;
-	ret = HYPERVISOR_sched_op(SCHEDOP_pin_override, &pin_override);
-
-	/* Ignore errors when removing override. */
-	if (cpu < 0)
-		return;
-
-	switch (ret) {
-	case -ENOSYS:
-		pr_warn("Unable to pin on physical cpu %d. In case of problems consider vcpu pinning.\n",
-			cpu);
-		disable_pinning = true;
-		break;
-	case -EPERM:
-		WARN(1, "Trying to pin vcpu without having privilege to do so\n");
-		disable_pinning = true;
-		break;
-	case -EINVAL:
-	case -EBUSY:
-		pr_warn("Physical cpu %d not available for pinning. Check Xen cpu configuration.\n",
-			cpu);
-		break;
-	case 0:
-		break;
-	default:
-		WARN(1, "rc %d while trying to pin vcpu\n", ret);
-		disable_pinning = true;
-	}
-}
-
-const struct hypervisor_x86 x86_hyper_xen_pv = {
-	.name                   = "Xen PV",
-	.detect                 = xen_platform_pv,
-	.set_cpu_features       = xen_set_cpu_features,
-	.pin_vcpu               = xen_pin_vcpu,
-};
-EXPORT_SYMBOL(x86_hyper_xen_pv);
-
 #ifdef CONFIG_HOTPLUG_CPU
 void xen_arch_register_cpu(int num)
 {

commit 98f2a47a002139ce677d3062d334af8b6e36e25c
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 14 18:35:40 2017 +0100

    x86/xen: split off enlighten_hvm.c
    
    Move PVHVM related code to enlighten_hvm.c. Three functions:
    xen_cpuhp_setup(), xen_reboot(), xen_emergency_restart() are shared, drop
    static qualifier from them. These functions will go to common code once
    it is split from enlighten.c.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2e392e9d83cb..5cd454cda2f6 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -45,10 +45,8 @@
 #include <xen/interface/memory.h>
 #include <xen/interface/nmi.h>
 #include <xen/interface/xen-mca.h>
-#include <xen/interface/hvm/start_info.h>
 #include <xen/features.h>
 #include <xen/page.h>
-#include <xen/hvm.h>
 #include <xen/hvc-console.h>
 #include <xen/acpi.h>
 
@@ -141,10 +139,8 @@ void *xen_initial_gdt;
 RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
 
 static int xen_cpu_up_prepare_pv(unsigned int cpu);
-static int xen_cpu_up_prepare_hvm(unsigned int cpu);
 static int xen_cpu_up_online(unsigned int cpu);
 static int xen_cpu_dead_pv(unsigned int cpu);
-static int xen_cpu_dead_hvm(unsigned int cpu);
 
 /*
  * Point at some empty memory to start with. We map the real shared_info
@@ -1283,7 +1279,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.end_context_switch = xen_end_context_switch,
 };
 
-static void xen_reboot(int reason)
+void xen_reboot(int reason)
 {
 	struct sched_shutdown r = { .reason = reason };
 	int cpu;
@@ -1300,7 +1296,7 @@ static void xen_restart(char *msg)
 	xen_reboot(SHUTDOWN_reboot);
 }
 
-static void xen_emergency_restart(void)
+void xen_emergency_restart(void)
 {
 	xen_reboot(SHUTDOWN_reboot);
 }
@@ -1436,8 +1432,8 @@ static void __init xen_dom0_set_legacy_features(void)
 	x86_platform.legacy.rtc = 1;
 }
 
-static int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
-			   int (*cpu_dead_cb)(unsigned int))
+int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
+		    int (*cpu_dead_cb)(unsigned int))
 {
 	int rc;
 
@@ -1660,79 +1656,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 #endif
 }
 
-void __ref xen_hvm_init_shared_info(void)
-{
-	int cpu;
-	struct xen_add_to_physmap xatp;
-	static struct shared_info *shared_info_page = 0;
-
-	if (!shared_info_page)
-		shared_info_page = (struct shared_info *)
-			extend_brk(PAGE_SIZE, PAGE_SIZE);
-	xatp.domid = DOMID_SELF;
-	xatp.idx = 0;
-	xatp.space = XENMAPSPACE_shared_info;
-	xatp.gpfn = __pa(shared_info_page) >> PAGE_SHIFT;
-	if (HYPERVISOR_memory_op(XENMEM_add_to_physmap, &xatp))
-		BUG();
-
-	HYPERVISOR_shared_info = (struct shared_info *)shared_info_page;
-
-	/* xen_vcpu is a pointer to the vcpu_info struct in the shared_info
-	 * page, we use it in the event channel upcall and in some pvclock
-	 * related functions. We don't need the vcpu_info placement
-	 * optimizations because we don't use any pv_mmu or pv_irq op on
-	 * HVM.
-	 * When xen_hvm_init_shared_info is run at boot time only vcpu 0 is
-	 * online but xen_hvm_init_shared_info is run at resume time too and
-	 * in that case multiple vcpus might be online. */
-	for_each_online_cpu(cpu) {
-		/* Leave it to be NULL. */
-		if (xen_vcpu_nr(cpu) >= MAX_VIRT_CPUS)
-			continue;
-		per_cpu(xen_vcpu, cpu) =
-			&HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
-	}
-}
-
-#ifdef CONFIG_XEN_PVHVM
-static void __init init_hvm_pv_info(void)
-{
-	int major, minor;
-	uint32_t eax, ebx, ecx, edx, base;
-
-	base = xen_cpuid_base();
-	eax = cpuid_eax(base + 1);
-
-	major = eax >> 16;
-	minor = eax & 0xffff;
-	printk(KERN_INFO "Xen version %d.%d.\n", major, minor);
-
-	xen_domain_type = XEN_HVM_DOMAIN;
-
-	/* PVH set up hypercall page in xen_prepare_pvh(). */
-	if (xen_pvh_domain())
-		pv_info.name = "Xen PVH";
-	else {
-		u64 pfn;
-		uint32_t msr;
-
-		pv_info.name = "Xen HVM";
-		msr = cpuid_ebx(base + 2);
-		pfn = __pa(hypercall_page);
-		wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
-	}
-
-	xen_setup_features();
-
-	cpuid(base + 4, &eax, &ebx, &ecx, &edx);
-	if (eax & XEN_HVM_CPUID_VCPU_ID_PRESENT)
-		this_cpu_write(xen_vcpu_id, ebx);
-	else
-		this_cpu_write(xen_vcpu_id, smp_processor_id());
-}
-#endif
-
 static int xen_cpu_up_prepare_pv(unsigned int cpu)
 {
 	int rc;
@@ -1748,37 +1671,6 @@ static int xen_cpu_up_prepare_pv(unsigned int cpu)
 	return 0;
 }
 
-static int xen_cpu_up_prepare_hvm(unsigned int cpu)
-{
-	int rc;
-
-	/*
-	 * This can happen if CPU was offlined earlier and
-	 * offlining timed out in common_cpu_die().
-	 */
-	if (cpu_report_state(cpu) == CPU_DEAD_FROZEN) {
-		xen_smp_intr_free(cpu);
-		xen_uninit_lock_cpu(cpu);
-	}
-
-	if (cpu_acpi_id(cpu) != U32_MAX)
-		per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
-	else
-		per_cpu(xen_vcpu_id, cpu) = cpu;
-	xen_vcpu_setup(cpu);
-
-	if (xen_feature(XENFEAT_hvm_safe_pvclock))
-		xen_setup_timer(cpu);
-
-	rc = xen_smp_intr_init(cpu);
-	if (rc) {
-		WARN(1, "xen_smp_intr_init() for CPU %d failed: %d\n",
-		     cpu, rc);
-		return rc;
-	}
-	return 0;
-}
-
 static int xen_cpu_dead_pv(unsigned int cpu)
 {
 	xen_smp_intr_free(cpu);
@@ -1788,75 +1680,12 @@ static int xen_cpu_dead_pv(unsigned int cpu)
 	return 0;
 }
 
-static int xen_cpu_dead_hvm(unsigned int cpu)
-{
-	xen_smp_intr_free(cpu);
-
-	if (xen_feature(XENFEAT_hvm_safe_pvclock))
-		xen_teardown_timer(cpu);
-
-       return 0;
-}
-
 static int xen_cpu_up_online(unsigned int cpu)
 {
 	xen_init_lock_cpu(cpu);
 	return 0;
 }
 
-#ifdef CONFIG_XEN_PVHVM
-#ifdef CONFIG_KEXEC_CORE
-static void xen_hvm_shutdown(void)
-{
-	native_machine_shutdown();
-	if (kexec_in_progress)
-		xen_reboot(SHUTDOWN_soft_reset);
-}
-
-static void xen_hvm_crash_shutdown(struct pt_regs *regs)
-{
-	native_machine_crash_shutdown(regs);
-	xen_reboot(SHUTDOWN_soft_reset);
-}
-#endif
-
-static void __init xen_hvm_guest_init(void)
-{
-	if (xen_pv_domain())
-		return;
-
-	init_hvm_pv_info();
-
-	xen_hvm_init_shared_info();
-
-	xen_panic_handler_init();
-
-	BUG_ON(!xen_feature(XENFEAT_hvm_callback_vector));
-
-	xen_hvm_smp_init();
-	WARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_hvm, xen_cpu_dead_hvm));
-	xen_unplug_emulated_devices();
-	x86_init.irqs.intr_init = xen_init_IRQ;
-	xen_hvm_init_time_ops();
-	xen_hvm_init_mmu_ops();
-
-	if (xen_pvh_domain())
-		machine_ops.emergency_restart = xen_emergency_restart;
-#ifdef CONFIG_KEXEC_CORE
-	machine_ops.shutdown = xen_hvm_shutdown;
-	machine_ops.crash_shutdown = xen_hvm_crash_shutdown;
-#endif
-}
-#endif
-
-static bool xen_nopv = false;
-static __init int xen_parse_nopv(char *arg)
-{
-       xen_nopv = true;
-       return 0;
-}
-early_param("xen_nopv", xen_parse_nopv);
-
 static uint32_t __init xen_platform_pv(void)
 {
 	if (xen_pv_domain())
@@ -1865,35 +1694,13 @@ static uint32_t __init xen_platform_pv(void)
 	return 0;
 }
 
-static uint32_t __init xen_platform_hvm(void)
-{
-	if (xen_pv_domain() || xen_nopv)
-		return 0;
-
-	return xen_cpuid_base();
-}
-
-bool xen_hvm_need_lapic(void)
-{
-	if (xen_nopv)
-		return false;
-	if (xen_pv_domain())
-		return false;
-	if (!xen_hvm_domain())
-		return false;
-	if (xen_feature(XENFEAT_hvm_pirqs))
-		return false;
-	return true;
-}
-EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);
-
 static void xen_set_cpu_features(struct cpuinfo_x86 *c)
 {
 	clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
 	set_cpu_cap(c, X86_FEATURE_XENPV);
 }
 
-static void xen_pin_vcpu(int cpu)
+void xen_pin_vcpu(int cpu)
 {
 	static bool disable_pinning;
 	struct sched_pin_override pin_override;
@@ -1940,15 +1747,6 @@ const struct hypervisor_x86 x86_hyper_xen_pv = {
 };
 EXPORT_SYMBOL(x86_hyper_xen_pv);
 
-const struct hypervisor_x86 x86_hyper_xen_hvm = {
-	.name                   = "Xen HVM",
-	.detect                 = xen_platform_hvm,
-	.init_platform          = xen_hvm_guest_init,
-	.pin_vcpu               = xen_pin_vcpu,
-	.x2apic_available       = xen_x2apic_para_available,
-};
-EXPORT_SYMBOL(x86_hyper_xen_hvm);
-
 #ifdef CONFIG_HOTPLUG_CPU
 void xen_arch_register_cpu(int num)
 {

commit 481d66325dfc2a6b4f5e729b64ad144383b63fad
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 14 18:35:39 2017 +0100

    x86/xen: split off enlighten_pvh.c
    
    Create enlighten_pvh.c by splitting off PVH related code from enlighten.c,
    put it under CONFIG_XEN_PVH.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7b55408ff0ab..2e392e9d83cb 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -180,20 +180,6 @@ struct tls_descs {
  */
 static DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);
 
-#ifdef CONFIG_XEN_PVH
-/*
- * PVH variables.
- *
- * xen_pvh and pvh_bootparams need to live in data segment since they
- * are used after startup_{32|64}, which clear .bss, are invoked.
- */
-bool xen_pvh __attribute__((section(".data"))) = 0;
-struct boot_params pvh_bootparams __attribute__((section(".data")));
-
-struct hvm_start_info pvh_start_info;
-unsigned int pvh_start_info_sz = sizeof(pvh_start_info);
-#endif
-
 static void clamp_max_cpus(void)
 {
 #ifdef CONFIG_SMP
@@ -1674,100 +1660,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 #endif
 }
 
-#ifdef CONFIG_XEN_PVH
-
-static void xen_pvh_arch_setup(void)
-{
-#ifdef CONFIG_ACPI
-	/* Make sure we don't fall back to (default) ACPI_IRQ_MODEL_PIC. */
-	if (nr_ioapics == 0)
-		acpi_irq_model = ACPI_IRQ_MODEL_PLATFORM;
-#endif
-}
-
-static void __init init_pvh_bootparams(void)
-{
-	struct xen_memory_map memmap;
-	unsigned int i;
-	int rc;
-
-	memset(&pvh_bootparams, 0, sizeof(pvh_bootparams));
-
-	memmap.nr_entries = ARRAY_SIZE(pvh_bootparams.e820_table);
-	set_xen_guest_handle(memmap.buffer, pvh_bootparams.e820_table);
-	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
-	if (rc) {
-		xen_raw_printk("XENMEM_memory_map failed (%d)\n", rc);
-		BUG();
-	}
-
-	if (memmap.nr_entries < E820_MAX_ENTRIES_ZEROPAGE - 1) {
-		pvh_bootparams.e820_table[memmap.nr_entries].addr =
-			ISA_START_ADDRESS;
-		pvh_bootparams.e820_table[memmap.nr_entries].size =
-			ISA_END_ADDRESS - ISA_START_ADDRESS;
-		pvh_bootparams.e820_table[memmap.nr_entries].type =
-			E820_TYPE_RESERVED;
-		memmap.nr_entries++;
-	} else
-		xen_raw_printk("Warning: Can fit ISA range into e820\n");
-
-	pvh_bootparams.e820_entries = memmap.nr_entries;
-	for (i = 0; i < pvh_bootparams.e820_entries; i++)
-		e820__range_add(pvh_bootparams.e820_table[i].addr,
-				pvh_bootparams.e820_table[i].size,
-				pvh_bootparams.e820_table[i].type);
-
-	e820__update_table(e820_table);
-
-	pvh_bootparams.hdr.cmd_line_ptr =
-		pvh_start_info.cmdline_paddr;
-
-	/* The first module is always ramdisk. */
-	if (pvh_start_info.nr_modules) {
-		struct hvm_modlist_entry *modaddr =
-			__va(pvh_start_info.modlist_paddr);
-		pvh_bootparams.hdr.ramdisk_image = modaddr->paddr;
-		pvh_bootparams.hdr.ramdisk_size = modaddr->size;
-	}
-
-	/*
-	 * See Documentation/x86/boot.txt.
-	 *
-	 * Version 2.12 supports Xen entry point but we will use default x86/PC
-	 * environment (i.e. hardware_subarch 0).
-	 */
-	pvh_bootparams.hdr.version = 0x212;
-	pvh_bootparams.hdr.type_of_loader = (9 << 4) | 0; /* Xen loader */
-}
-
-/*
- * This routine (and those that it might call) should not use
- * anything that lives in .bss since that segment will be cleared later.
- */
-void __init xen_prepare_pvh(void)
-{
-	u32 msr;
-	u64 pfn;
-
-	if (pvh_start_info.magic != XEN_HVM_START_MAGIC_VALUE) {
-		xen_raw_printk("Error: Unexpected magic value (0x%08x)\n",
-				pvh_start_info.magic);
-		BUG();
-	}
-
-	xen_pvh = 1;
-
-	msr = cpuid_ebx(xen_cpuid_base() + 2);
-	pfn = __pa(hypercall_page);
-	wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
-
-	init_pvh_bootparams();
-
-	x86_init.oem.arch_setup = xen_pvh_arch_setup;
-}
-#endif
-
 void __ref xen_hvm_init_shared_info(void)
 {
 	int cpu;

commit 52519f2af020b6b53b4e0cbb8cff71058ed434cd
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 14 18:35:37 2017 +0100

    x86/xen: globalize have_vcpu_info_placement
    
    have_vcpu_info_placement applies to both PV and HVM and as we're going
    to split the code we need to make it global.
    
    Rename to xen_have_vcpu_info_placement.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 52cd7189b5d3..7b55408ff0ab 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -165,7 +165,7 @@ struct shared_info *HYPERVISOR_shared_info = &xen_dummy_shared_info;
  *
  * 0: not available, 1: available
  */
-static int have_vcpu_info_placement = 1;
+int xen_have_vcpu_info_placement = 1;
 
 struct tls_descs {
 	struct desc_struct desc[3];
@@ -229,7 +229,7 @@ void xen_vcpu_setup(int cpu)
 		per_cpu(xen_vcpu, cpu) =
 			&HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
 
-	if (!have_vcpu_info_placement) {
+	if (!xen_have_vcpu_info_placement) {
 		if (cpu >= MAX_VIRT_CPUS)
 			clamp_max_cpus();
 		return;
@@ -252,7 +252,7 @@ void xen_vcpu_setup(int cpu)
 
 	if (err) {
 		printk(KERN_DEBUG "register_vcpu_info failed: err=%d\n", err);
-		have_vcpu_info_placement = 0;
+		xen_have_vcpu_info_placement = 0;
 		clamp_max_cpus();
 	} else {
 		/* This cpu is using the registered vcpu info, even if
@@ -281,7 +281,7 @@ void xen_vcpu_restore(void)
 
 		xen_setup_runstate_info(cpu);
 
-		if (have_vcpu_info_placement)
+		if (xen_have_vcpu_info_placement)
 			xen_vcpu_setup(cpu);
 
 		if (other_cpu && is_up &&
@@ -1160,7 +1160,7 @@ void xen_setup_vcpu_info_placement(void)
 	 * xen_vcpu_setup managed to place the vcpu_info within the
 	 * percpu area for all cpus, so make use of it.
 	 */
-	if (have_vcpu_info_placement) {
+	if (xen_have_vcpu_info_placement) {
 		pv_irq_ops.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);
 		pv_irq_ops.restore_fl = __PV_IS_CALLEE_SAVE(xen_restore_fl_direct);
 		pv_irq_ops.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);
@@ -1179,7 +1179,7 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 
 #define SITE(op, x)							\
 	case PARAVIRT_PATCH(op.x):					\
-	if (have_vcpu_info_placement) {					\
+	if (xen_have_vcpu_info_placement) {				\
 		start = (char *)xen_##x##_direct;			\
 		end = xen_##x##_direct_end;				\
 		reloc = xen_##x##_direct_reloc;				\

commit 0991d22d5e7fad3ea4f504a1eb0ccb4707135f64
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 14 18:35:36 2017 +0100

    x86/xen: separate PV and HVM hypervisors
    
    As a preparation to splitting the code we need to untangle it:
    
    x86_hyper_xen -> x86_hyper_xen_hvm and x86_hyper_xen_pv
    xen_platform() -> xen_platform_hvm() and xen_platform_pv()
    xen_cpu_up_prepare() -> xen_cpu_up_prepare_pv() and xen_cpu_up_prepare_hvm()
    xen_cpu_dead() -> xen_cpu_dead_pv() and xen_cpu_dead_pv_hvm()
    
    Add two parameters to xen_cpuhp_setup() to pass proper cpu_up_prepare and
    cpu_dead hooks. xen_set_cpu_features() is now PV-only so the redundant
    xen_pv_domain() check can be dropped.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 30822e8e64ac..52cd7189b5d3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -140,9 +140,11 @@ void *xen_initial_gdt;
 
 RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
 
-static int xen_cpu_up_prepare(unsigned int cpu);
+static int xen_cpu_up_prepare_pv(unsigned int cpu);
+static int xen_cpu_up_prepare_hvm(unsigned int cpu);
 static int xen_cpu_up_online(unsigned int cpu);
-static int xen_cpu_dead(unsigned int cpu);
+static int xen_cpu_dead_pv(unsigned int cpu);
+static int xen_cpu_dead_hvm(unsigned int cpu);
 
 /*
  * Point at some empty memory to start with. We map the real shared_info
@@ -1448,13 +1450,14 @@ static void __init xen_dom0_set_legacy_features(void)
 	x86_platform.legacy.rtc = 1;
 }
 
-static int xen_cpuhp_setup(void)
+static int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
+			   int (*cpu_dead_cb)(unsigned int))
 {
 	int rc;
 
 	rc = cpuhp_setup_state_nocalls(CPUHP_XEN_PREPARE,
 				       "x86/xen/hvm_guest:prepare",
-				       xen_cpu_up_prepare, xen_cpu_dead);
+				       cpu_up_prepare_cb, cpu_dead_cb);
 	if (rc >= 0) {
 		rc = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
 					       "x86/xen/hvm_guest:online",
@@ -1560,7 +1563,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
 
-	WARN_ON(xen_cpuhp_setup());
+	WARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_pv, xen_cpu_dead_pv));
 
 	local_irq_disable();
 	early_boot_irqs_disabled = true;
@@ -1838,28 +1841,41 @@ static void __init init_hvm_pv_info(void)
 }
 #endif
 
-static int xen_cpu_up_prepare(unsigned int cpu)
+static int xen_cpu_up_prepare_pv(unsigned int cpu)
 {
 	int rc;
 
-	if (xen_hvm_domain()) {
-		/*
-		 * This can happen if CPU was offlined earlier and
-		 * offlining timed out in common_cpu_die().
-		 */
-		if (cpu_report_state(cpu) == CPU_DEAD_FROZEN) {
-			xen_smp_intr_free(cpu);
-			xen_uninit_lock_cpu(cpu);
-		}
+	xen_setup_timer(cpu);
 
-		if (cpu_acpi_id(cpu) != U32_MAX)
-			per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
-		else
-			per_cpu(xen_vcpu_id, cpu) = cpu;
-		xen_vcpu_setup(cpu);
+	rc = xen_smp_intr_init(cpu);
+	if (rc) {
+		WARN(1, "xen_smp_intr_init() for CPU %d failed: %d\n",
+		     cpu, rc);
+		return rc;
+	}
+	return 0;
+}
+
+static int xen_cpu_up_prepare_hvm(unsigned int cpu)
+{
+	int rc;
+
+	/*
+	 * This can happen if CPU was offlined earlier and
+	 * offlining timed out in common_cpu_die().
+	 */
+	if (cpu_report_state(cpu) == CPU_DEAD_FROZEN) {
+		xen_smp_intr_free(cpu);
+		xen_uninit_lock_cpu(cpu);
 	}
 
-	if (xen_pv_domain() || xen_feature(XENFEAT_hvm_safe_pvclock))
+	if (cpu_acpi_id(cpu) != U32_MAX)
+		per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
+	else
+		per_cpu(xen_vcpu_id, cpu) = cpu;
+	xen_vcpu_setup(cpu);
+
+	if (xen_feature(XENFEAT_hvm_safe_pvclock))
 		xen_setup_timer(cpu);
 
 	rc = xen_smp_intr_init(cpu);
@@ -1871,16 +1887,25 @@ static int xen_cpu_up_prepare(unsigned int cpu)
 	return 0;
 }
 
-static int xen_cpu_dead(unsigned int cpu)
+static int xen_cpu_dead_pv(unsigned int cpu)
 {
 	xen_smp_intr_free(cpu);
 
-	if (xen_pv_domain() || xen_feature(XENFEAT_hvm_safe_pvclock))
-		xen_teardown_timer(cpu);
+	xen_teardown_timer(cpu);
 
 	return 0;
 }
 
+static int xen_cpu_dead_hvm(unsigned int cpu)
+{
+	xen_smp_intr_free(cpu);
+
+	if (xen_feature(XENFEAT_hvm_safe_pvclock))
+		xen_teardown_timer(cpu);
+
+       return 0;
+}
+
 static int xen_cpu_up_online(unsigned int cpu)
 {
 	xen_init_lock_cpu(cpu);
@@ -1917,7 +1942,7 @@ static void __init xen_hvm_guest_init(void)
 	BUG_ON(!xen_feature(XENFEAT_hvm_callback_vector));
 
 	xen_hvm_smp_init();
-	WARN_ON(xen_cpuhp_setup());
+	WARN_ON(xen_cpuhp_setup(xen_cpu_up_prepare_hvm, xen_cpu_dead_hvm));
 	xen_unplug_emulated_devices();
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();
@@ -1940,9 +1965,17 @@ static __init int xen_parse_nopv(char *arg)
 }
 early_param("xen_nopv", xen_parse_nopv);
 
-static uint32_t __init xen_platform(void)
+static uint32_t __init xen_platform_pv(void)
 {
-	if (xen_nopv)
+	if (xen_pv_domain())
+		return xen_cpuid_base();
+
+	return 0;
+}
+
+static uint32_t __init xen_platform_hvm(void)
+{
+	if (xen_pv_domain() || xen_nopv)
 		return 0;
 
 	return xen_cpuid_base();
@@ -1964,10 +1997,8 @@ EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);
 
 static void xen_set_cpu_features(struct cpuinfo_x86 *c)
 {
-	if (xen_pv_domain()) {
-		clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
-		set_cpu_cap(c, X86_FEATURE_XENPV);
-	}
+	clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
+	set_cpu_cap(c, X86_FEATURE_XENPV);
 }
 
 static void xen_pin_vcpu(int cpu)
@@ -2009,17 +2040,22 @@ static void xen_pin_vcpu(int cpu)
 	}
 }
 
-const struct hypervisor_x86 x86_hyper_xen = {
-	.name			= "Xen",
-	.detect			= xen_platform,
-#ifdef CONFIG_XEN_PVHVM
-	.init_platform		= xen_hvm_guest_init,
-#endif
-	.x2apic_available	= xen_x2apic_para_available,
+const struct hypervisor_x86 x86_hyper_xen_pv = {
+	.name                   = "Xen PV",
+	.detect                 = xen_platform_pv,
 	.set_cpu_features       = xen_set_cpu_features,
 	.pin_vcpu               = xen_pin_vcpu,
 };
-EXPORT_SYMBOL(x86_hyper_xen);
+EXPORT_SYMBOL(x86_hyper_xen_pv);
+
+const struct hypervisor_x86 x86_hyper_xen_hvm = {
+	.name                   = "Xen HVM",
+	.detect                 = xen_platform_hvm,
+	.init_platform          = xen_hvm_guest_init,
+	.pin_vcpu               = xen_pin_vcpu,
+	.x2apic_available       = xen_x2apic_para_available,
+};
+EXPORT_SYMBOL(x86_hyper_xen_hvm);
 
 #ifdef CONFIG_HOTPLUG_CPU
 void xen_arch_register_cpu(int num)

commit e5185a76a23b2d56fb2327ad8bd58fb1bcaa52b1
Merge: b678c91aefa7 4729277156cf
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 11 08:56:05 2017 +0200

    Merge branch 'x86/boot' into x86/mm, to avoid conflict
    
    There's a conflict between ongoing level-5 paging support and
    the E820 rewrite. Since the E820 rewrite is essentially ready,
    merge it into x86/mm to reduce tree conflicts.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 73fa1362a7f337d149f76d26b6c6845cb38c1af9
Merge: fdd3d8ce0ea6 6415813bae75
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Mar 30 09:07:54 2017 +0200

    Merge branch 'x86/cpu' into x86/mm, before applying dependent patch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b23adb7d3f7d1d7cce03db9704de67a99ceeda38
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 22 14:32:34 2017 -0700

    x86/xen/gdt: Use X86_FEATURE_XENPV instead of globals for the GDT fixup
    
    Xen imposes special requirements on the GDT.  Rather than using a
    global variable for the pgprot, just use an explicit special case
    for Xen -- this makes it clearer what's going on.  It also debloats
    64-bit kernels very slightly.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/e9ea96abbfd6a8c87753849171bb5987ecfeb523.1490218061.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 08faa61de5f7..4951fcf95143 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1545,9 +1545,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	 */
 	xen_initial_gdt = &per_cpu(gdt_page, 0);
 
-	/* GDT can only be remapped RO */
-	pg_fixmap_gdt_flags = PAGE_KERNEL_RO;
-
 	xen_smp_init();
 
 #ifdef CONFIG_ACPI_NUMA

commit 69218e47994da614e7af600bf06887750ab6657a
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Mar 14 10:05:07 2017 -0700

    x86: Remap GDT tables in the fixmap section
    
    Each processor holds a GDT in its per-cpu structure. The sgdt
    instruction gives the base address of the current GDT. This address can
    be used to bypass KASLR memory randomization. With another bug, an
    attacker could target other per-cpu structures or deduce the base of
    the main memory section (PAGE_OFFSET).
    
    This patch relocates the GDT table for each processor inside the
    fixmap section. The space is reserved based on number of supported
    processors.
    
    For consistency, the remapping is done by default on 32 and 64-bit.
    
    Each processor switches to its remapped GDT at the end of
    initialization. For hibernation, the main processor returns with the
    original GDT and switches back to the remapping at completion.
    
    This patch was tested on both architectures. Hibernation and KVM were
    both tested specially for their usage of the GDT.
    
    Thanks to Boris Ostrovsky <boris.ostrovsky@oracle.com> for testing and
    recommending changes for Xen support.
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lorenzo Stoakes <lstoakes@gmail.com>
    Cc: Luis R . Rodriguez <mcgrof@kernel.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kvm@vger.kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-pm@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Cc: zijun_hu <zijun_hu@htc.com>
    Link: http://lkml.kernel.org/r/20170314170508.100882-2-thgarnie@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ec1d5c46e58f..08faa61de5f7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -710,7 +710,7 @@ static void load_TLS_descriptor(struct thread_struct *t,
 
 	*shadow = t->tls_array[i];
 
-	gdt = get_cpu_gdt_table(cpu);
+	gdt = get_cpu_gdt_rw(cpu);
 	maddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);
 	mc = __xen_mc_entry(0);
 
@@ -1545,6 +1545,9 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	 */
 	xen_initial_gdt = &per_cpu(gdt_page, 0);
 
+	/* GDT can only be remapped RO */
+	pg_fixmap_gdt_flags = PAGE_KERNEL_RO;
+
 	xen_smp_init();
 
 #ifdef CONFIG_ACPI_NUMA

commit 6415813bae75feba10b8ca3ed6634a72c2a4d313
Author: Mathias Krause <minipli@googlemail.com>
Date:   Sun Feb 12 22:12:08 2017 +0100

    x86/cpu: Drop wp_works_ok member of struct cpuinfo_x86
    
    Remove the wp_works_ok member of struct cpuinfo_x86. It's an
    optimization back from Linux v0.99 times where we had no fixup support
    yet and did the CR0.WP test via special code in the page fault handler.
    The < 0 test was an optimization to not do the special casing for each
    NULL ptr access violation but just for the first one doing the WP test.
    Today it serves no real purpose as the test no longer needs special code
    in the page fault handler and the only call side -- mem_init() -- calls
    it just once, anyway. However, Xen pre-initializes it to 1, to skip the
    test.
    
    Doing the test again for Xen should be no issue at all, as even the
    commit introducing skipping the test (commit d560bc61575e ("x86, xen:
    Suppress WP test on Xen")) mentioned it being ban aid only. And, in
    fact, testing the patch on Xen showed nothing breaks.
    
    The pre-fixup times are long gone and with the removal of the fallback
    handling code in commit a5c2a893dbd4 ("x86, 386 removal: Remove
    CONFIG_X86_WP_WORKS_OK") the kernel requires a working CR0.WP anyway.
    So just get rid of the "optimization" and do the test unconditionally.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Arnd Hannemann <hannemann@nets.rwth-aachen.de>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Link: http://lkml.kernel.org/r/1486933932-585-3-git-send-email-minipli@googlemail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ec1d5c46e58f..bc3dab5d47ca 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1595,7 +1595,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	/* set up basic CPUID stuff */
 	cpu_detect(&new_cpu_data);
 	set_cpu_cap(&new_cpu_data, X86_FEATURE_FPU);
-	new_cpu_data.wp_works_ok = 1;
 	new_cpu_data.x86_capability[CPUID_1_EDX] = cpuid_edx(1);
 #endif
 

commit 687d77a5f7b2aae4ea0507888648823f7c24e974
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Mar 1 08:49:47 2017 +0100

    x86/xen: Update e820 table handling to the new core x86 E820 code
    
    Note that I restructured the Xen E820 logic a bit: instead of trying
    to sort the boot parameters, only the kernel's E820 table is sorted.
    
    This is how the x86 code does it and it reduces coupling between
    the in-kernel E820 code and the (unchanged) boot parameters.
    
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: <stefano.stabellini@eu.citrix.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ec1d5c46e58f..78243454f5e6 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -76,6 +76,7 @@
 #include <asm/mwait.h>
 #include <asm/pci_x86.h>
 #include <asm/cpu.h>
+#include <asm/e820/api.h> 
 
 #ifdef CONFIG_ACPI
 #include <linux/acpi.h>
@@ -1690,34 +1691,32 @@ static void __init init_pvh_bootparams(void)
 
 	memset(&pvh_bootparams, 0, sizeof(pvh_bootparams));
 
-	memmap.nr_entries = ARRAY_SIZE(pvh_bootparams.e820_map);
-	set_xen_guest_handle(memmap.buffer, pvh_bootparams.e820_map);
+	memmap.nr_entries = ARRAY_SIZE(pvh_bootparams.e820_table);
+	set_xen_guest_handle(memmap.buffer, pvh_bootparams.e820_table);
 	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
 	if (rc) {
 		xen_raw_printk("XENMEM_memory_map failed (%d)\n", rc);
 		BUG();
 	}
 
-	if (memmap.nr_entries < E820MAX - 1) {
-		pvh_bootparams.e820_map[memmap.nr_entries].addr =
+	if (memmap.nr_entries < E820_MAX_ENTRIES_ZEROPAGE - 1) {
+		pvh_bootparams.e820_table[memmap.nr_entries].addr =
 			ISA_START_ADDRESS;
-		pvh_bootparams.e820_map[memmap.nr_entries].size =
+		pvh_bootparams.e820_table[memmap.nr_entries].size =
 			ISA_END_ADDRESS - ISA_START_ADDRESS;
-		pvh_bootparams.e820_map[memmap.nr_entries].type =
-			E820_RESERVED;
+		pvh_bootparams.e820_table[memmap.nr_entries].type =
+			E820_TYPE_RESERVED;
 		memmap.nr_entries++;
 	} else
 		xen_raw_printk("Warning: Can fit ISA range into e820\n");
 
-	sanitize_e820_map(pvh_bootparams.e820_map,
-			  ARRAY_SIZE(pvh_bootparams.e820_map),
-			  &memmap.nr_entries);
-
 	pvh_bootparams.e820_entries = memmap.nr_entries;
 	for (i = 0; i < pvh_bootparams.e820_entries; i++)
-		e820_add_region(pvh_bootparams.e820_map[i].addr,
-				pvh_bootparams.e820_map[i].size,
-				pvh_bootparams.e820_map[i].type);
+		e820__range_add(pvh_bootparams.e820_table[i].addr,
+				pvh_bootparams.e820_table[i].size,
+				pvh_bootparams.e820_table[i].type);
+
+	e820__update_table(e820_table);
 
 	pvh_bootparams.hdr.cmd_line_ptr =
 		pvh_start_info.cmdline_paddr;

commit 7a1c44ebc5ac2e2c28d95b0da6060728c334e7e4
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Feb 6 10:58:06 2017 -0500

    xen/pvh: Use Xen's emergency_restart op for PVH guests
    
    Using native_machine_emergency_restart (called during reboot) will
    lead PVH guests to machine_real_restart()  where we try to use
    real_mode_header which is not initialized.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 6d406f3465bc..ec1d5c46e58f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1924,6 +1924,9 @@ static void __init xen_hvm_guest_init(void)
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();
 	xen_hvm_init_mmu_ops();
+
+	if (xen_pvh_domain())
+		machine_ops.emergency_restart = xen_emergency_restart;
 #ifdef CONFIG_KEXEC_CORE
 	machine_ops.shutdown = xen_hvm_shutdown;
 	machine_ops.crash_shutdown = xen_hvm_crash_shutdown;

commit 5adad168e586cb381633f45d181bb729b04393a5
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Sun Feb 5 19:50:58 2017 -0500

    xen/pvh: Make sure we don't use ACPI_IRQ_MODEL_PIC for SCI
    
    Since we are not using PIC and (at least currently) don't have IOAPIC
    we want to make sure that acpi_irq_model doesn't stay set to
    ACPI_IRQ_MODEL_PIC (which is the default value). If we allowed it to
    stay then acpi_os_install_interrupt_handler() would try (and fail) to
    request_irq() for PIC.
    
    Instead we set the model to ACPI_IRQ_MODEL_PLATFORM which will prevent
    this from happening.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d2144f7c8fab..6d406f3465bc 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1672,6 +1672,16 @@ asmlinkage __visible void __init xen_start_kernel(void)
 }
 
 #ifdef CONFIG_XEN_PVH
+
+static void xen_pvh_arch_setup(void)
+{
+#ifdef CONFIG_ACPI
+	/* Make sure we don't fall back to (default) ACPI_IRQ_MODEL_PIC. */
+	if (nr_ioapics == 0)
+		acpi_irq_model = ACPI_IRQ_MODEL_PLATFORM;
+#endif
+}
+
 static void __init init_pvh_bootparams(void)
 {
 	struct xen_memory_map memmap;
@@ -1752,6 +1762,8 @@ void __init xen_prepare_pvh(void)
 	wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
 
 	init_pvh_bootparams();
+
+	x86_init.oem.arch_setup = xen_pvh_arch_setup;
 }
 #endif
 

commit 7243b93345f7f8de260e8f5b4670803e64fcbb00
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Sun Feb 5 19:50:52 2017 -0500

    xen/pvh: Bootstrap PVH guest
    
    Start PVH guest at XEN_ELFNOTE_PHYS32_ENTRY address. Setup hypercall
    page, initialize boot_params, enable early page tables.
    
    Since this stub is executed before kernel entry point we cannot use
    variables in .bss which is cleared by kernel. We explicitly place
    variables that are initialized here into .data.
    
    While adjusting xen_hvm_init_shared_info() make it use cpuid_e?x()
    instead of cpuid() (wherever possible).
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 828f1b226f56..d2144f7c8fab 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -45,6 +45,7 @@
 #include <xen/interface/memory.h>
 #include <xen/interface/nmi.h>
 #include <xen/interface/xen-mca.h>
+#include <xen/interface/hvm/start_info.h>
 #include <xen/features.h>
 #include <xen/page.h>
 #include <xen/hvm.h>
@@ -176,6 +177,20 @@ struct tls_descs {
  */
 static DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);
 
+#ifdef CONFIG_XEN_PVH
+/*
+ * PVH variables.
+ *
+ * xen_pvh and pvh_bootparams need to live in data segment since they
+ * are used after startup_{32|64}, which clear .bss, are invoked.
+ */
+bool xen_pvh __attribute__((section(".data"))) = 0;
+struct boot_params pvh_bootparams __attribute__((section(".data")));
+
+struct hvm_start_info pvh_start_info;
+unsigned int pvh_start_info_sz = sizeof(pvh_start_info);
+#endif
+
 static void clamp_max_cpus(void)
 {
 #ifdef CONFIG_SMP
@@ -1656,6 +1671,90 @@ asmlinkage __visible void __init xen_start_kernel(void)
 #endif
 }
 
+#ifdef CONFIG_XEN_PVH
+static void __init init_pvh_bootparams(void)
+{
+	struct xen_memory_map memmap;
+	unsigned int i;
+	int rc;
+
+	memset(&pvh_bootparams, 0, sizeof(pvh_bootparams));
+
+	memmap.nr_entries = ARRAY_SIZE(pvh_bootparams.e820_map);
+	set_xen_guest_handle(memmap.buffer, pvh_bootparams.e820_map);
+	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
+	if (rc) {
+		xen_raw_printk("XENMEM_memory_map failed (%d)\n", rc);
+		BUG();
+	}
+
+	if (memmap.nr_entries < E820MAX - 1) {
+		pvh_bootparams.e820_map[memmap.nr_entries].addr =
+			ISA_START_ADDRESS;
+		pvh_bootparams.e820_map[memmap.nr_entries].size =
+			ISA_END_ADDRESS - ISA_START_ADDRESS;
+		pvh_bootparams.e820_map[memmap.nr_entries].type =
+			E820_RESERVED;
+		memmap.nr_entries++;
+	} else
+		xen_raw_printk("Warning: Can fit ISA range into e820\n");
+
+	sanitize_e820_map(pvh_bootparams.e820_map,
+			  ARRAY_SIZE(pvh_bootparams.e820_map),
+			  &memmap.nr_entries);
+
+	pvh_bootparams.e820_entries = memmap.nr_entries;
+	for (i = 0; i < pvh_bootparams.e820_entries; i++)
+		e820_add_region(pvh_bootparams.e820_map[i].addr,
+				pvh_bootparams.e820_map[i].size,
+				pvh_bootparams.e820_map[i].type);
+
+	pvh_bootparams.hdr.cmd_line_ptr =
+		pvh_start_info.cmdline_paddr;
+
+	/* The first module is always ramdisk. */
+	if (pvh_start_info.nr_modules) {
+		struct hvm_modlist_entry *modaddr =
+			__va(pvh_start_info.modlist_paddr);
+		pvh_bootparams.hdr.ramdisk_image = modaddr->paddr;
+		pvh_bootparams.hdr.ramdisk_size = modaddr->size;
+	}
+
+	/*
+	 * See Documentation/x86/boot.txt.
+	 *
+	 * Version 2.12 supports Xen entry point but we will use default x86/PC
+	 * environment (i.e. hardware_subarch 0).
+	 */
+	pvh_bootparams.hdr.version = 0x212;
+	pvh_bootparams.hdr.type_of_loader = (9 << 4) | 0; /* Xen loader */
+}
+
+/*
+ * This routine (and those that it might call) should not use
+ * anything that lives in .bss since that segment will be cleared later.
+ */
+void __init xen_prepare_pvh(void)
+{
+	u32 msr;
+	u64 pfn;
+
+	if (pvh_start_info.magic != XEN_HVM_START_MAGIC_VALUE) {
+		xen_raw_printk("Error: Unexpected magic value (0x%08x)\n",
+				pvh_start_info.magic);
+		BUG();
+	}
+
+	xen_pvh = 1;
+
+	msr = cpuid_ebx(xen_cpuid_base() + 2);
+	pfn = __pa(hypercall_page);
+	wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
+
+	init_pvh_bootparams();
+}
+#endif
+
 void __ref xen_hvm_init_shared_info(void)
 {
 	int cpu;
@@ -1695,20 +1794,29 @@ void __ref xen_hvm_init_shared_info(void)
 static void __init init_hvm_pv_info(void)
 {
 	int major, minor;
-	uint32_t eax, ebx, ecx, edx, pages, msr, base;
-	u64 pfn;
+	uint32_t eax, ebx, ecx, edx, base;
 
 	base = xen_cpuid_base();
-	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
+	eax = cpuid_eax(base + 1);
 
 	major = eax >> 16;
 	minor = eax & 0xffff;
 	printk(KERN_INFO "Xen version %d.%d.\n", major, minor);
 
-	cpuid(base + 2, &pages, &msr, &ecx, &edx);
+	xen_domain_type = XEN_HVM_DOMAIN;
 
-	pfn = __pa(hypercall_page);
-	wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
+	/* PVH set up hypercall page in xen_prepare_pvh(). */
+	if (xen_pvh_domain())
+		pv_info.name = "Xen PVH";
+	else {
+		u64 pfn;
+		uint32_t msr;
+
+		pv_info.name = "Xen HVM";
+		msr = cpuid_ebx(base + 2);
+		pfn = __pa(hypercall_page);
+		wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
+	}
 
 	xen_setup_features();
 
@@ -1717,10 +1825,6 @@ static void __init init_hvm_pv_info(void)
 		this_cpu_write(xen_vcpu_id, ebx);
 	else
 		this_cpu_write(xen_vcpu_id, smp_processor_id());
-
-	pv_info.name = "Xen HVM";
-
-	xen_domain_type = XEN_HVM_DOMAIN;
 }
 #endif
 

commit 063334f30543597430f172bd7690d21e3590e148
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Fri Feb 3 16:57:22 2017 -0500

    xen/x86: Remove PVH support
    
    We are replacing existing PVH guests with new implementation.
    
    We are keeping xen_pvh_domain() macro (for now set to zero) because
    when we introduce new PVH implementation later in this series we will
    reuse current PVH-specific code (xen_pvh_gnttab_setup()), and that
    code is conditioned by 'if (xen_pvh_domain())'. (We will also need
    a noop xen_pvh_domain() for !CONFIG_XEN_PVH).
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 51ef95232725..828f1b226f56 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1138,10 +1138,11 @@ void xen_setup_vcpu_info_placement(void)
 		xen_vcpu_setup(cpu);
 	}
 
-	/* xen_vcpu_setup managed to place the vcpu_info within the
-	 * percpu area for all cpus, so make use of it. Note that for
-	 * PVH we want to use native IRQ mechanism. */
-	if (have_vcpu_info_placement && !xen_pvh_domain()) {
+	/*
+	 * xen_vcpu_setup managed to place the vcpu_info within the
+	 * percpu area for all cpus, so make use of it.
+	 */
+	if (have_vcpu_info_placement) {
 		pv_irq_ops.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);
 		pv_irq_ops.restore_fl = __PV_IS_CALLEE_SAVE(xen_restore_fl_direct);
 		pv_irq_ops.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);
@@ -1413,49 +1414,9 @@ static void __init xen_boot_params_init_edd(void)
  * Set up the GDT and segment registers for -fstack-protector.  Until
  * we do this, we have to be careful not to call any stack-protected
  * function, which is most of the kernel.
- *
- * Note, that it is __ref because the only caller of this after init
- * is PVH which is not going to use xen_load_gdt_boot or other
- * __init functions.
  */
-static void __ref xen_setup_gdt(int cpu)
+static void xen_setup_gdt(int cpu)
 {
-	if (xen_feature(XENFEAT_auto_translated_physmap)) {
-#ifdef CONFIG_X86_64
-		unsigned long dummy;
-
-		load_percpu_segment(cpu); /* We need to access per-cpu area */
-		switch_to_new_gdt(cpu); /* GDT and GS set */
-
-		/* We are switching of the Xen provided GDT to our HVM mode
-		 * GDT. The new GDT has  __KERNEL_CS with CS.L = 1
-		 * and we are jumping to reload it.
-		 */
-		asm volatile ("pushq %0\n"
-			      "leaq 1f(%%rip),%0\n"
-			      "pushq %0\n"
-			      "lretq\n"
-			      "1:\n"
-			      : "=&r" (dummy) : "0" (__KERNEL_CS));
-
-		/*
-		 * While not needed, we also set the %es, %ds, and %fs
-		 * to zero. We don't care about %ss as it is NULL.
-		 * Strictly speaking this is not needed as Xen zeros those
-		 * out (and also MSR_FS_BASE, MSR_GS_BASE, MSR_KERNEL_GS_BASE)
-		 *
-		 * Linux zeros them in cpu_init() and in secondary_startup_64
-		 * (for BSP).
-		 */
-		loadsegment(es, 0);
-		loadsegment(ds, 0);
-		loadsegment(fs, 0);
-#else
-		/* PVH: TODO Implement. */
-		BUG();
-#endif
-		return; /* PVH does not need any PV GDT ops. */
-	}
 	pv_cpu_ops.write_gdt_entry = xen_write_gdt_entry_boot;
 	pv_cpu_ops.load_gdt = xen_load_gdt_boot;
 
@@ -1466,59 +1427,6 @@ static void __ref xen_setup_gdt(int cpu)
 	pv_cpu_ops.load_gdt = xen_load_gdt;
 }
 
-#ifdef CONFIG_XEN_PVH
-/*
- * A PV guest starts with default flags that are not set for PVH, set them
- * here asap.
- */
-static void xen_pvh_set_cr_flags(int cpu)
-{
-
-	/* Some of these are setup in 'secondary_startup_64'. The others:
-	 * X86_CR0_TS, X86_CR0_PE, X86_CR0_ET are set by Xen for HVM guests
-	 * (which PVH shared codepaths), while X86_CR0_PG is for PVH. */
-	write_cr0(read_cr0() | X86_CR0_MP | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM);
-
-	if (!cpu)
-		return;
-	/*
-	 * For BSP, PSE PGE are set in probe_page_size_mask(), for APs
-	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu__init_cpu().
-	*/
-	if (boot_cpu_has(X86_FEATURE_PSE))
-		cr4_set_bits_and_update_boot(X86_CR4_PSE);
-
-	if (boot_cpu_has(X86_FEATURE_PGE))
-		cr4_set_bits_and_update_boot(X86_CR4_PGE);
-}
-
-/*
- * Note, that it is ref - because the only caller of this after init
- * is PVH which is not going to use xen_load_gdt_boot or other
- * __init functions.
- */
-void __ref xen_pvh_secondary_vcpu_init(int cpu)
-{
-	xen_setup_gdt(cpu);
-	xen_pvh_set_cr_flags(cpu);
-}
-
-static void __init xen_pvh_early_guest_init(void)
-{
-	if (!xen_feature(XENFEAT_auto_translated_physmap))
-		return;
-
-	BUG_ON(!xen_feature(XENFEAT_hvm_callback_vector));
-
-	xen_pvh_early_cpu_init(0, false);
-	xen_pvh_set_cr_flags(0);
-
-#ifdef CONFIG_X86_32
-	BUG(); /* PVH: Implement proper support. */
-#endif
-}
-#endif    /* CONFIG_XEN_PVH */
-
 static void __init xen_dom0_set_legacy_features(void)
 {
 	x86_platform.legacy.rtc = 1;
@@ -1555,24 +1463,17 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	xen_domain_type = XEN_PV_DOMAIN;
 
 	xen_setup_features();
-#ifdef CONFIG_XEN_PVH
-	xen_pvh_early_guest_init();
-#endif
+
 	xen_setup_machphys_mapping();
 
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;
-	if (!xen_pvh_domain()) {
-		pv_cpu_ops = xen_cpu_ops;
+	pv_cpu_ops = xen_cpu_ops;
 
-		x86_platform.get_nmi_reason = xen_get_nmi_reason;
-	}
+	x86_platform.get_nmi_reason = xen_get_nmi_reason;
 
-	if (xen_feature(XENFEAT_auto_translated_physmap))
-		x86_init.resources.memory_setup = xen_auto_xlated_memory_setup;
-	else
-		x86_init.resources.memory_setup = xen_memory_setup;
+	x86_init.resources.memory_setup = xen_memory_setup;
 	x86_init.oem.arch_setup = xen_arch_setup;
 	x86_init.oem.banner = xen_banner;
 
@@ -1665,18 +1566,15 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	/* set the limit of our address space */
 	xen_reserve_top();
 
-	/* PVH: runs at default kernel iopl of 0 */
-	if (!xen_pvh_domain()) {
-		/*
-		 * We used to do this in xen_arch_setup, but that is too late
-		 * on AMD were early_cpu_init (run before ->arch_setup()) calls
-		 * early_amd_init which pokes 0xcf8 port.
-		 */
-		set_iopl.iopl = 1;
-		rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
-		if (rc != 0)
-			xen_raw_printk("physdev_op failed %d\n", rc);
-	}
+	/*
+	 * We used to do this in xen_arch_setup, but that is too late
+	 * on AMD were early_cpu_init (run before ->arch_setup()) calls
+	 * early_amd_init which pokes 0xcf8 port.
+	 */
+	set_iopl.iopl = 1;
+	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
+	if (rc != 0)
+		xen_raw_printk("physdev_op failed %d\n", rc);
 
 #ifdef CONFIG_X86_32
 	/* set up basic CPUID stuff */

commit 73c1b41e63f040e92669e61a02c7893933bfe743
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:19:54 2016 +0100

    cpu/hotplug: Cleanup state names
    
    When the state names got added a script was used to add the extra argument
    to the calls. The script basically converted the state constant to a
    string, but the cleanup to convert these strings into meaningful ones did
    not happen.
    
    Replace all the useless strings with 'subsys/xxx/yyy:state' strings which
    are used in all the other places already.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Link: http://lkml.kernel.org/r/20161221192112.085444152@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ced7027b3fbc..51ef95232725 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1529,11 +1529,11 @@ static int xen_cpuhp_setup(void)
 	int rc;
 
 	rc = cpuhp_setup_state_nocalls(CPUHP_XEN_PREPARE,
-				       "XEN_HVM_GUEST_PREPARE",
+				       "x86/xen/hvm_guest:prepare",
 				       xen_cpu_up_prepare, xen_cpu_dead);
 	if (rc >= 0) {
 		rc = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
-					       "XEN_HVM_GUEST_ONLINE",
+					       "x86/xen/hvm_guest:online",
 					       xen_cpu_up_online, NULL);
 		if (rc < 0)
 			cpuhp_remove_state_nocalls(CPUHP_XEN_PREPARE);

commit af25ed59b5616b389d90877f7085dc5d457a3d49
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Oct 31 15:18:49 2016 -0700

    x86/fpu: Remove clts()
    
    The kernel doesn't use clts() any more.  Remove it and all of its
    paravirt infrastructure.
    
    A careful reader may notice that xen_clts() appears to have been
    buggy -- it didn't update xen_cr0_value.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm list <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/3d3c8ca62f17579b9849a013d71e59a4d5d1b079.1477951965.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bdd855685403..ced7027b3fbc 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -980,17 +980,6 @@ static void xen_io_delay(void)
 {
 }
 
-static void xen_clts(void)
-{
-	struct multicall_space mcs;
-
-	mcs = xen_mc_entry(0);
-
-	MULTI_fpu_taskswitch(mcs.mc, 0);
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
-}
-
 static DEFINE_PER_CPU(unsigned long, xen_cr0_value);
 
 static unsigned long xen_read_cr0(void)
@@ -1233,8 +1222,6 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.set_debugreg = xen_set_debugreg,
 	.get_debugreg = xen_get_debugreg,
 
-	.clts = xen_clts,
-
 	.read_cr0 = xen_read_cr0,
 	.write_cr0 = xen_write_cr0,
 

commit aa34e07e457ed13b44d680b5b605e3e5a585f611
Merge: 0d7317598214 c251f15c7dbf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 24 19:52:24 2016 -0700

    Merge tag 'for-linus-4.9-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen fixes from David Vrabel:
    
     - advertise control feature flags in xenstore
    
     - fix x86 build when XEN_PVHVM is disabled
    
    * tag 'for-linus-4.9-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xenbus: check return value of xenbus_scanf()
      xenbus: prefer list_for_each()
      x86: xen: move cpu_up functions out of ifdef
      xenbus: advertise control feature flags

commit cb5f7e7c1ded5ff91b18116669c0f43c82bea3db
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Oct 12 17:20:38 2016 +0200

    x86: xen: move cpu_up functions out of ifdef
    
    Three newly introduced functions are not defined when CONFIG_XEN_PVHVM is
    disabled, but are still being used:
    
    arch/x86/xen/enlighten.c:141:12: warning: ‘xen_cpu_up_prepare’ used but never defined
    arch/x86/xen/enlighten.c:142:12: warning: ‘xen_cpu_up_online’ used but never defined
    arch/x86/xen/enlighten.c:143:12: warning: ‘xen_cpu_dead’ used but never defined
    
    Fixes: 4d737042d6c4 ("xen/x86: Convert to hotplug state machine")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 96c2dea798a1..a637f902f59e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1838,6 +1838,7 @@ static void __init init_hvm_pv_info(void)
 
 	xen_domain_type = XEN_HVM_DOMAIN;
 }
+#endif
 
 static int xen_cpu_up_prepare(unsigned int cpu)
 {
@@ -1888,6 +1889,7 @@ static int xen_cpu_up_online(unsigned int cpu)
 	return 0;
 }
 
+#ifdef CONFIG_XEN_PVHVM
 #ifdef CONFIG_KEXEC_CORE
 static void xen_hvm_shutdown(void)
 {

commit 541efb7632642cab55361178d73d544f025b593c
Merge: 6218590bcb45 a6a198bc60e6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 6 11:19:10 2016 -0700

    Merge tag 'for-linus-4.9-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
     "xen features and fixes for 4.9:
    
       - switch to new CPU hotplug mechanism
    
       - support driver_override in pciback
    
       - require vector callback for HVM guests (the alternate mechanism via
         the platform device has been broken for ages)"
    
    * tag 'for-linus-4.9-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/x86: Update topology map for PV VCPUs
      xen/x86: Initialize per_cpu(xen_vcpu, 0) a little earlier
      xen/pciback: support driver_override
      xen/pciback: avoid multiple entries in slot list
      xen/pciback: simplify pcistub device handling
      xen: Remove event channel notification through Xen PCI platform device
      xen/events: Convert to hotplug state machine
      xen/x86: Convert to hotplug state machine
      x86/xen: add missing \n at end of printk warning message
      xen/grant-table: Use kmalloc_array() in arch_gnttab_valloc()
      xen: Make VPMU init message look less scary
      xen: rename xen_pmu_init() in sys-hypervisor.c
      hotplug: Prevent alloc/free of irq descriptors during cpu up/down (again)
      xen/x86: Move irq allocation from Xen smp_op.cpu_up()

commit 565fdc6a2afeb14e654a8d9ebc792afb9d2c0019
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Sun Oct 2 18:45:19 2016 -0400

    xen/x86: Initialize per_cpu(xen_vcpu, 0) a little earlier
    
    xen_cpuhp_setup() calls mutex_lock() which, when CONFIG_DEBUG_MUTEXES
    is defined, ends up calling xen_save_fl(). That routine expects
    per_cpu(xen_vcpu, 0) to be already initialized.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reported-by: Sander Eikelenboom <linux@eikelenboom.it>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 366b6aed4a8e..96c2dea798a1 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1644,7 +1644,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	xen_initial_gdt = &per_cpu(gdt_page, 0);
 
 	xen_smp_init();
-	WARN_ON(xen_cpuhp_setup());
 
 #ifdef CONFIG_ACPI_NUMA
 	/*
@@ -1658,6 +1657,8 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
 
+	WARN_ON(xen_cpuhp_setup());
+
 	local_irq_disable();
 	early_boot_irqs_disabled = true;
 

commit 1a4a2bc460721bc8f91e4c1294d39b38e5af132f
Merge: 110a9e42b687 1ef55be16ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 16:13:28 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull low-level x86 updates from Ingo Molnar:
     "In this cycle this topic tree has become one of those 'super topics'
      that accumulated a lot of changes:
    
       - Add CONFIG_VMAP_STACK=y support to the core kernel and enable it on
         x86 - preceded by an array of changes. v4.8 saw preparatory changes
         in this area already - this is the rest of the work. Includes the
         thread stack caching performance optimization. (Andy Lutomirski)
    
       - switch_to() cleanups and all around enhancements. (Brian Gerst)
    
       - A large number of dumpstack infrastructure enhancements and an
         unwinder abstraction. The secret long term plan is safe(r) live
         patching plus maybe another attempt at debuginfo based unwinding -
         but all these current bits are standalone enhancements in a frame
         pointer based debug environment as well. (Josh Poimboeuf)
    
       - More __ro_after_init and const annotations. (Kees Cook)
    
       - Enable KASLR for the vmemmap memory region. (Thomas Garnier)"
    
    [ The virtually mapped stack changes are pretty fundamental, and not
      x86-specific per se, even if they are only used on x86 right now. ]
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/asm: Get rid of __read_cr4_safe()
      thread_info: Use unsigned long for flags
      x86/alternatives: Add stack frame dependency to alternative_call_2()
      x86/dumpstack: Fix show_stack() task pointer regression
      x86/dumpstack: Remove dump_trace() and related callbacks
      x86/dumpstack: Convert show_trace_log_lvl() to use the new unwinder
      oprofile/x86: Convert x86_backtrace() to use the new unwinder
      x86/stacktrace: Convert save_stack_trace_*() to use the new unwinder
      perf/x86: Convert perf_callchain_kernel() to use the new unwinder
      x86/unwind: Add new unwind interface and implementations
      x86/dumpstack: Remove NULL task pointer convention
      fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y
      sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
      lib/syscall: Pin the task stack in collect_syscall()
      x86/process: Pin the target stack in get_wchan()
      x86/dumpstack: Pin the target stack when dumping it
      kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function
      sched/core: Add try_get_task_stack() and put_task_stack()
      x86/entry/64: Fix a minor comment rebase error
      iommu/amd: Don't put completion-wait semaphore on stack
      ...

commit 72a9b186292d98494f222226cfd24a1621796209
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Fri Aug 26 23:55:36 2016 +0200

    xen: Remove event channel notification through Xen PCI platform device
    
    Ever since commit 254d1a3f02eb ("xen/pv-on-hvm kexec: shutdown watches
    from old kernel") using the INTx interrupt from Xen PCI platform
    device for event channel notification would just lockup the guest
    during bootup.  postcore_initcall now calls xs_reset_watches which
    will eventually try to read a value from XenStore and will get stuck
    on read_reply at XenBus forever since the platform driver is not
    probed yet and its INTx interrupt handler is not registered yet. That
    means that the guest can not be notified at this moment of any pending
    event channels and none of the per-event handlers will ever be invoked
    (including the XenStore one) and the reply will never be picked up by
    the kernel.
    
    The exact stack where things get stuck during xenbus_init:
    
    -xenbus_init
     -xs_init
      -xs_reset_watches
       -xenbus_scanf
        -xenbus_read
         -xs_single
          -xs_single
           -xs_talkv
    
    Vector callbacks have always been the favourite event notification
    mechanism since their introduction in commit 38e20b07efd5 ("x86/xen:
    event channels delivery on HVM.") and the vector callback feature has
    always been advertised for quite some time by Xen that's why INTx was
    broken for several years now without impacting anyone.
    
    Luckily this also means that event channel notification through INTx
    is basically dead-code which can be safely removed without impacting
    anybody since it has been effectively disabled for more than 4 years
    with nobody complaining about it (at least as far as I'm aware of).
    
    This commit removes event channel notification through Xen PCI
    platform device.
    
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Cc: Julien Grall <julien.grall@citrix.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Ross Lagerwall <ross.lagerwall@citrix.com>
    Cc: xen-devel@lists.xenproject.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-pci@vger.kernel.org
    Cc: Anthony Liguori <aliguori@amazon.com>
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a9ba14ac3aea..366b6aed4a8e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -137,8 +137,6 @@ struct shared_info xen_dummy_shared_info;
 void *xen_initial_gdt;
 
 RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
-__read_mostly int xen_have_vector_callback;
-EXPORT_SYMBOL_GPL(xen_have_vector_callback);
 
 static int xen_cpu_up_prepare(unsigned int cpu);
 static int xen_cpu_up_online(unsigned int cpu);
@@ -1524,10 +1522,7 @@ static void __init xen_pvh_early_guest_init(void)
 	if (!xen_feature(XENFEAT_auto_translated_physmap))
 		return;
 
-	if (!xen_feature(XENFEAT_hvm_callback_vector))
-		return;
-
-	xen_have_vector_callback = 1;
+	BUG_ON(!xen_feature(XENFEAT_hvm_callback_vector));
 
 	xen_pvh_early_cpu_init(0, false);
 	xen_pvh_set_cr_flags(0);
@@ -1864,9 +1859,7 @@ static int xen_cpu_up_prepare(unsigned int cpu)
 		xen_vcpu_setup(cpu);
 	}
 
-	if (xen_pv_domain() ||
-	    (xen_have_vector_callback &&
-	     xen_feature(XENFEAT_hvm_safe_pvclock)))
+	if (xen_pv_domain() || xen_feature(XENFEAT_hvm_safe_pvclock))
 		xen_setup_timer(cpu);
 
 	rc = xen_smp_intr_init(cpu);
@@ -1882,9 +1875,7 @@ static int xen_cpu_dead(unsigned int cpu)
 {
 	xen_smp_intr_free(cpu);
 
-	if (xen_pv_domain() ||
-	    (xen_have_vector_callback &&
-	     xen_feature(XENFEAT_hvm_safe_pvclock)))
+	if (xen_pv_domain() || xen_feature(XENFEAT_hvm_safe_pvclock))
 		xen_teardown_timer(cpu);
 
 	return 0;
@@ -1922,8 +1913,8 @@ static void __init xen_hvm_guest_init(void)
 
 	xen_panic_handler_init();
 
-	if (xen_feature(XENFEAT_hvm_callback_vector))
-		xen_have_vector_callback = 1;
+	BUG_ON(!xen_feature(XENFEAT_hvm_callback_vector));
+
 	xen_hvm_smp_init();
 	WARN_ON(xen_cpuhp_setup());
 	xen_unplug_emulated_devices();
@@ -1961,7 +1952,7 @@ bool xen_hvm_need_lapic(void)
 		return false;
 	if (!xen_hvm_domain())
 		return false;
-	if (xen_feature(XENFEAT_hvm_pirqs) && xen_have_vector_callback)
+	if (xen_feature(XENFEAT_hvm_pirqs))
 		return false;
 	return true;
 }

commit 1ef55be16ed69538f89e0a6508be5e62fdc9851c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 29 12:48:12 2016 -0700

    x86/asm: Get rid of __read_cr4_safe()
    
    We use __read_cr4() vs __read_cr4_safe() inconsistently.  On
    CR4-less CPUs, all CR4 bits are effectively clear, so we can make
    the code simpler and more robust by making __read_cr4() always fix
    up faults on 32-bit kernels.
    
    This may fix some bugs on old 486-like CPUs, but I don't have any
    easy way to test that.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: david@saggiorato.net
    Link: http://lkml.kernel.org/r/ea647033d357d9ce2ad2bbde5a631045f5052fb6.1475178370.git.luto@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b86ebb1a9a7f..e2cf8fcea6bb 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1237,7 +1237,6 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.write_cr0 = xen_write_cr0,
 
 	.read_cr4 = native_read_cr4,
-	.read_cr4_safe = native_read_cr4_safe,
 	.write_cr4 = xen_write_cr4,
 
 #ifdef CONFIG_X86_64

commit 4d737042d6c4ee10a632cf94b953169d13955a40
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Wed Sep 7 13:19:00 2016 -0400

    xen/x86: Convert to hotplug state machine
    
    Switch to new CPU hotplug infrastructure.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Suggested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c7f6b1f90efa..a9ba14ac3aea 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -140,7 +140,9 @@ RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
 __read_mostly int xen_have_vector_callback;
 EXPORT_SYMBOL_GPL(xen_have_vector_callback);
 
-static struct notifier_block xen_cpu_notifier;
+static int xen_cpu_up_prepare(unsigned int cpu);
+static int xen_cpu_up_online(unsigned int cpu);
+static int xen_cpu_dead(unsigned int cpu);
 
 /*
  * Point at some empty memory to start with. We map the real shared_info
@@ -1541,6 +1543,24 @@ static void __init xen_dom0_set_legacy_features(void)
 	x86_platform.legacy.rtc = 1;
 }
 
+static int xen_cpuhp_setup(void)
+{
+	int rc;
+
+	rc = cpuhp_setup_state_nocalls(CPUHP_XEN_PREPARE,
+				       "XEN_HVM_GUEST_PREPARE",
+				       xen_cpu_up_prepare, xen_cpu_dead);
+	if (rc >= 0) {
+		rc = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
+					       "XEN_HVM_GUEST_ONLINE",
+					       xen_cpu_up_online, NULL);
+		if (rc < 0)
+			cpuhp_remove_state_nocalls(CPUHP_XEN_PREPARE);
+	}
+
+	return rc >= 0 ? 0 : rc;
+}
+
 /* First C function to be called on Xen boot */
 asmlinkage __visible void __init xen_start_kernel(void)
 {
@@ -1629,7 +1649,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	xen_initial_gdt = &per_cpu(gdt_page, 0);
 
 	xen_smp_init();
-	register_cpu_notifier(&xen_cpu_notifier);
+	WARN_ON(xen_cpuhp_setup());
 
 #ifdef CONFIG_ACPI_NUMA
 	/*
@@ -1823,63 +1843,58 @@ static void __init init_hvm_pv_info(void)
 	xen_domain_type = XEN_HVM_DOMAIN;
 }
 
-static int xen_cpu_notify(struct notifier_block *self, unsigned long action,
-			 void *hcpu)
+static int xen_cpu_up_prepare(unsigned int cpu)
 {
-	int cpu = (long)hcpu;
 	int rc;
 
-	switch (action) {
-	case CPU_UP_PREPARE:
-		if (xen_hvm_domain()) {
-			/*
-			 * This can happen if CPU was offlined earlier and
-			 * offlining timed out in common_cpu_die().
-			 */
-			if (cpu_report_state(cpu) == CPU_DEAD_FROZEN) {
-				xen_smp_intr_free(cpu);
-				xen_uninit_lock_cpu(cpu);
-			}
-
-			if (cpu_acpi_id(cpu) != U32_MAX)
-				per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
-			else
-				per_cpu(xen_vcpu_id, cpu) = cpu;
-			xen_vcpu_setup(cpu);
+	if (xen_hvm_domain()) {
+		/*
+		 * This can happen if CPU was offlined earlier and
+		 * offlining timed out in common_cpu_die().
+		 */
+		if (cpu_report_state(cpu) == CPU_DEAD_FROZEN) {
+			xen_smp_intr_free(cpu);
+			xen_uninit_lock_cpu(cpu);
 		}
 
-		if (xen_pv_domain() ||
-		    (xen_have_vector_callback &&
-		     xen_feature(XENFEAT_hvm_safe_pvclock)))
-			xen_setup_timer(cpu);
+		if (cpu_acpi_id(cpu) != U32_MAX)
+			per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
+		else
+			per_cpu(xen_vcpu_id, cpu) = cpu;
+		xen_vcpu_setup(cpu);
+	}
 
-		rc = xen_smp_intr_init(cpu);
-		if (rc) {
-			WARN(1, "xen_smp_intr_init() for CPU %d failed: %d\n",
-			     cpu, rc);
-			return NOTIFY_BAD;
-		}
+	if (xen_pv_domain() ||
+	    (xen_have_vector_callback &&
+	     xen_feature(XENFEAT_hvm_safe_pvclock)))
+		xen_setup_timer(cpu);
 
-		break;
-	case CPU_ONLINE:
-		xen_init_lock_cpu(cpu);
-		break;
-	case CPU_UP_CANCELED:
-		xen_smp_intr_free(cpu);
-		if (xen_pv_domain() ||
-		    (xen_have_vector_callback &&
-		     xen_feature(XENFEAT_hvm_safe_pvclock)))
-			xen_teardown_timer(cpu);
-		break;
-	default:
-		break;
+	rc = xen_smp_intr_init(cpu);
+	if (rc) {
+		WARN(1, "xen_smp_intr_init() for CPU %d failed: %d\n",
+		     cpu, rc);
+		return rc;
 	}
-	return NOTIFY_OK;
+	return 0;
 }
 
-static struct notifier_block xen_cpu_notifier = {
-	.notifier_call	= xen_cpu_notify,
-};
+static int xen_cpu_dead(unsigned int cpu)
+{
+	xen_smp_intr_free(cpu);
+
+	if (xen_pv_domain() ||
+	    (xen_have_vector_callback &&
+	     xen_feature(XENFEAT_hvm_safe_pvclock)))
+		xen_teardown_timer(cpu);
+
+	return 0;
+}
+
+static int xen_cpu_up_online(unsigned int cpu)
+{
+	xen_init_lock_cpu(cpu);
+	return 0;
+}
 
 #ifdef CONFIG_KEXEC_CORE
 static void xen_hvm_shutdown(void)
@@ -1910,7 +1925,7 @@ static void __init xen_hvm_guest_init(void)
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;
 	xen_hvm_smp_init();
-	register_cpu_notifier(&xen_cpu_notifier);
+	WARN_ON(xen_cpuhp_setup());
 	xen_unplug_emulated_devices();
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();

commit 99bc67536d04bcb9133546284acfb15d033b8bfe
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Aug 29 08:48:45 2016 +0200

    xen: Add xen_pin_vcpu() to support calling functions on a dedicated pCPU
    
    Some hardware models (e.g. Dell Studio 1555 laptops) require calls to
    the firmware to be issued on CPU 0 only. As Dom0 might have to use
    these calls, add xen_pin_vcpu() to achieve this functionality.
    
    In case either the domain doesn't have the privilege to make the
    related hypercall or the hypervisor isn't supporting it, issue a
    warning once and disable further pinning attempts.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas_Warzecha@dell.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: boris.ostrovsky@oracle.com
    Cc: chrisw@sous-sol.org
    Cc: hpa@zytor.com
    Cc: jdelvare@suse.com
    Cc: jeremy@goop.org
    Cc: linux@roeck-us.net
    Cc: pali.rohar@gmail.com
    Cc: rusty@rustcorp.com.au
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1472453327-19050-5-git-send-email-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b86ebb1a9a7f..bc9aaba01a22 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1925,6 +1925,45 @@ static void xen_set_cpu_features(struct cpuinfo_x86 *c)
 	}
 }
 
+static void xen_pin_vcpu(int cpu)
+{
+	static bool disable_pinning;
+	struct sched_pin_override pin_override;
+	int ret;
+
+	if (disable_pinning)
+		return;
+
+	pin_override.pcpu = cpu;
+	ret = HYPERVISOR_sched_op(SCHEDOP_pin_override, &pin_override);
+
+	/* Ignore errors when removing override. */
+	if (cpu < 0)
+		return;
+
+	switch (ret) {
+	case -ENOSYS:
+		pr_warn("Unable to pin on physical cpu %d. In case of problems consider vcpu pinning.\n",
+			cpu);
+		disable_pinning = true;
+		break;
+	case -EPERM:
+		WARN(1, "Trying to pin vcpu without having privilege to do so\n");
+		disable_pinning = true;
+		break;
+	case -EINVAL:
+	case -EBUSY:
+		pr_warn("Physical cpu %d not available for pinning. Check Xen cpu configuration.\n",
+			cpu);
+		break;
+	case 0:
+		break;
+	default:
+		WARN(1, "rc %d while trying to pin vcpu\n", ret);
+		disable_pinning = true;
+	}
+}
+
 const struct hypervisor_x86 x86_hyper_xen = {
 	.name			= "Xen",
 	.detect			= xen_platform,
@@ -1933,6 +1972,7 @@ const struct hypervisor_x86 x86_hyper_xen = {
 #endif
 	.x2apic_available	= xen_x2apic_para_available,
 	.set_cpu_features       = xen_set_cpu_features,
+	.pin_vcpu               = xen_pin_vcpu,
 };
 EXPORT_SYMBOL(x86_hyper_xen);
 

commit 5fc509bc2bd6dddd4107eaf90680cd76cfc2ffed
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Wed Aug 3 13:22:27 2016 -0400

    xen/x86: Move irq allocation from Xen smp_op.cpu_up()
    
    Commit ce0d3c0a6fb1 ("genirq: Revert sparse irq locking around
    __cpu_up() and move it to x86 for now") reverted irq locking
    introduced by commit a89941816726 ("hotplug: Prevent alloc/free
    of irq descriptors during cpu up/down") because of Xen allocating
    irqs in both of its cpu_up ops.
    
    We can move those allocations into CPU notifiers so that original
    patch can be reinstated.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8ffb089b19a5..c7f6b1f90efa 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -140,6 +140,8 @@ RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
 __read_mostly int xen_have_vector_callback;
 EXPORT_SYMBOL_GPL(xen_have_vector_callback);
 
+static struct notifier_block xen_cpu_notifier;
+
 /*
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.
@@ -1627,6 +1629,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	xen_initial_gdt = &per_cpu(gdt_page, 0);
 
 	xen_smp_init();
+	register_cpu_notifier(&xen_cpu_notifier);
 
 #ifdef CONFIG_ACPI_NUMA
 	/*
@@ -1820,21 +1823,53 @@ static void __init init_hvm_pv_info(void)
 	xen_domain_type = XEN_HVM_DOMAIN;
 }
 
-static int xen_hvm_cpu_notify(struct notifier_block *self, unsigned long action,
-			      void *hcpu)
+static int xen_cpu_notify(struct notifier_block *self, unsigned long action,
+			 void *hcpu)
 {
 	int cpu = (long)hcpu;
+	int rc;
+
 	switch (action) {
 	case CPU_UP_PREPARE:
-		if (cpu_acpi_id(cpu) != U32_MAX)
-			per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
-		else
-			per_cpu(xen_vcpu_id, cpu) = cpu;
-		xen_vcpu_setup(cpu);
-		if (xen_have_vector_callback) {
-			if (xen_feature(XENFEAT_hvm_safe_pvclock))
-				xen_setup_timer(cpu);
+		if (xen_hvm_domain()) {
+			/*
+			 * This can happen if CPU was offlined earlier and
+			 * offlining timed out in common_cpu_die().
+			 */
+			if (cpu_report_state(cpu) == CPU_DEAD_FROZEN) {
+				xen_smp_intr_free(cpu);
+				xen_uninit_lock_cpu(cpu);
+			}
+
+			if (cpu_acpi_id(cpu) != U32_MAX)
+				per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
+			else
+				per_cpu(xen_vcpu_id, cpu) = cpu;
+			xen_vcpu_setup(cpu);
 		}
+
+		if (xen_pv_domain() ||
+		    (xen_have_vector_callback &&
+		     xen_feature(XENFEAT_hvm_safe_pvclock)))
+			xen_setup_timer(cpu);
+
+		rc = xen_smp_intr_init(cpu);
+		if (rc) {
+			WARN(1, "xen_smp_intr_init() for CPU %d failed: %d\n",
+			     cpu, rc);
+			return NOTIFY_BAD;
+		}
+
+		break;
+	case CPU_ONLINE:
+		xen_init_lock_cpu(cpu);
+		break;
+	case CPU_UP_CANCELED:
+		xen_smp_intr_free(cpu);
+		if (xen_pv_domain() ||
+		    (xen_have_vector_callback &&
+		     xen_feature(XENFEAT_hvm_safe_pvclock)))
+			xen_teardown_timer(cpu);
 		break;
 	default:
 		break;
@@ -1842,8 +1877,8 @@ static int xen_hvm_cpu_notify(struct notifier_block *self, unsigned long action,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block xen_hvm_cpu_notifier = {
-	.notifier_call	= xen_hvm_cpu_notify,
+static struct notifier_block xen_cpu_notifier = {
+	.notifier_call	= xen_cpu_notify,
 };
 
 #ifdef CONFIG_KEXEC_CORE
@@ -1875,7 +1910,7 @@ static void __init xen_hvm_guest_init(void)
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;
 	xen_hvm_smp_init();
-	register_cpu_notifier(&xen_hvm_cpu_notifier);
+	register_cpu_notifier(&xen_cpu_notifier);
 	xen_unplug_emulated_devices();
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();

commit 55467dea2967259f21f4f854fc99d39cc5fea60e
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Fri Jul 29 11:06:48 2016 +0200

    xen: change the type of xen_vcpu_id to uint32_t
    
    We pass xen_vcpu_id mapping information to hypercalls which require
    uint32_t type so it would be cleaner to have it as uint32_t. The
    initializer to -1 can be dropped as we always do the mapping before using
    it and we never check the 'not set' value anyway.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8ffb089b19a5..b86ebb1a9a7f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -118,7 +118,7 @@ DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 
 /* Linux <-> Xen vCPU id mapping */
-DEFINE_PER_CPU(int, xen_vcpu_id) = -1;
+DEFINE_PER_CPU(uint32_t, xen_vcpu_id);
 EXPORT_PER_CPU_SYMBOL(xen_vcpu_id);
 
 enum xen_domain_type xen_domain_type = XEN_NATIVE;

commit c0253115968c35f3e1ee497282efb75ccf29fb98
Author: Petr Tesarik <ptesarik@suse.com>
Date:   Tue Aug 2 14:06:19 2016 -0700

    kexec: allow kdump with crash_kexec_post_notifiers
    
    If a crash kernel is loaded, do not crash the running domain.  This is
    needed if the kernel is loaded with crash_kexec_post_notifiers, because
    panic notifiers are run before __crash_kexec() in that case, and this
    Xen hook prevents its being called later.
    
    [akpm@linux-foundation.org: build fix: unconditionally include kexec.h]
    Link: http://lkml.kernel.org/r/20160713122000.14969.99963.stgit@hananiah.suse.cz
    Signed-off-by: Petr Tesarik <ptesarik@suse.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index cd993051aed7..8ffb089b19a5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -34,9 +34,7 @@
 #include <linux/edd.h>
 #include <linux/frame.h>
 
-#ifdef CONFIG_KEXEC_CORE
 #include <linux/kexec.h>
-#endif
 
 #include <xen/xen.h>
 #include <xen/events.h>
@@ -1334,7 +1332,8 @@ static void xen_crash_shutdown(struct pt_regs *regs)
 static int
 xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
 {
-	xen_reboot(SHUTDOWN_crash);
+	if (!kexec_crash_loaded())
+		xen_reboot(SHUTDOWN_crash);
 	return NOTIFY_DONE;
 }
 

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit 08fd8c17686c6b09fa410a26d516548dd80ff147
Merge: e831101a73fb d34c30cc1fa8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 11:35:37 2016 -0700

    Merge tag 'for-linus-4.8-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
     "Features and fixes for 4.8-rc0:
    
       - ACPI support for guests on ARM platforms.
       - Generic steal time support for arm and x86.
       - Support cases where kernel cpu is not Xen VCPU number (e.g., if
         in-guest kexec is used).
       - Use the system workqueue instead of a custom workqueue in various
         places"
    
    * tag 'for-linus-4.8-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (47 commits)
      xen: add static initialization of steal_clock op to xen_time_ops
      xen/pvhvm: run xen_vcpu_setup() for the boot CPU
      xen/evtchn: use xen_vcpu_id mapping
      xen/events: fifo: use xen_vcpu_id mapping
      xen/events: use xen_vcpu_id mapping in events_base
      x86/xen: use xen_vcpu_id mapping when pointing vcpu_info to shared_info
      x86/xen: use xen_vcpu_id mapping for HYPERVISOR_vcpu_op
      xen: introduce xen_vcpu_id mapping
      x86/acpi: store ACPI ids from MADT for future usage
      x86/xen: update cpuid.h from Xen-4.7
      xen/evtchn: add IOCTL_EVTCHN_RESTRICT
      xen-blkback: really don't leak mode property
      xen-blkback: constify instance of "struct attribute_group"
      xen-blkfront: prefer xenbus_scanf() over xenbus_gather()
      xen-blkback: prefer xenbus_scanf() over xenbus_gather()
      xen: support runqueue steal time on xen
      arm/xen: add support for vm_assist hypercall
      xen: update xen headers
      xen-pciback: drop superfluous variables
      xen-pciback: short-circuit read path used for merging write values
      ...

commit ee42d665d3f5db975caf87baf101a57235ddb566
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Jun 30 17:56:43 2016 +0200

    xen/pvhvm: run xen_vcpu_setup() for the boot CPU
    
    Historically we didn't call VCPUOP_register_vcpu_info for CPU0 for
    PVHVM guests (while we had it for PV and ARM guests). This is usually
    fine as we can use vcpu info in the shared_info page but when we try
    booting on a vCPU with Xen's vCPU id > 31 (e.g. when we try to kdump
    after crashing on this CPU) we're not able to boot.
    
    Switch to always doing VCPUOP_register_vcpu_info for the boot CPU.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5ca92e600a69..85ef4c0442e0 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -184,7 +184,7 @@ static void clamp_max_cpus(void)
 #endif
 }
 
-static void xen_vcpu_setup(int cpu)
+void xen_vcpu_setup(int cpu)
 {
 	struct vcpu_register_vcpu_info info;
 	int err;

commit e15a8621935cac527b4e0ed4078d24c3e5ef73a6
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Jun 30 17:56:39 2016 +0200

    x86/xen: use xen_vcpu_id mapping when pointing vcpu_info to shared_info
    
    shared_info page has space for 32 vcpu info slots for first 32 vCPUs
    but these are the first 32 vCPUs from Xen's perspective and we should
    map them accordingly with the newly introduced xen_vcpu_id mapping.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 46f3399aee2c..5ca92e600a69 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -207,8 +207,9 @@ static void xen_vcpu_setup(int cpu)
 		if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
 			return;
 	}
-	if (cpu < MAX_VIRT_CPUS)
-		per_cpu(xen_vcpu,cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+	if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS)
+		per_cpu(xen_vcpu, cpu) =
+			&HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
 
 	if (!have_vcpu_info_placement) {
 		if (cpu >= MAX_VIRT_CPUS)
@@ -1783,9 +1784,10 @@ void __ref xen_hvm_init_shared_info(void)
 	 * in that case multiple vcpus might be online. */
 	for_each_online_cpu(cpu) {
 		/* Leave it to be NULL. */
-		if (cpu >= MAX_VIRT_CPUS)
+		if (xen_vcpu_nr(cpu) >= MAX_VIRT_CPUS)
 			continue;
-		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+		per_cpu(xen_vcpu, cpu) =
+			&HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
 	}
 }
 

commit ad5475f9faf5186b7f59de2c6481ee3e211f1ed7
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Jun 30 17:56:38 2016 +0200

    x86/xen: use xen_vcpu_id mapping for HYPERVISOR_vcpu_op
    
    HYPERVISOR_vcpu_op() passes Linux's idea of vCPU id as a parameter
    while Xen's idea is expected. In some cases these ideas diverge so we
    need to do remapping.
    
    Convert all callers of HYPERVISOR_vcpu_op() to use xen_vcpu_nr().
    
    Leave xen_fill_possible_map() and xen_filter_cpu_maps() intact as
    they're only being called by PV guests before perpu areas are
    initialized. While the issue could be solved by switching to
    early_percpu for xen_vcpu_id I think it's not worth it: PV guests will
    probably never get to the point where their idea of vCPU id diverges
    from Xen's.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d093949c94e0..46f3399aee2c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -228,7 +228,8 @@ static void xen_vcpu_setup(int cpu)
 	   hypervisor has no unregister variant and this hypercall does not
 	   allow to over-write info.mfn and info.offset.
 	 */
-	err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, cpu, &info);
+	err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, xen_vcpu_nr(cpu),
+				 &info);
 
 	if (err) {
 		printk(KERN_DEBUG "register_vcpu_info failed: err=%d\n", err);
@@ -252,10 +253,11 @@ void xen_vcpu_restore(void)
 
 	for_each_possible_cpu(cpu) {
 		bool other_cpu = (cpu != smp_processor_id());
-		bool is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up, cpu, NULL);
+		bool is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up, xen_vcpu_nr(cpu),
+						NULL);
 
 		if (other_cpu && is_up &&
-		    HYPERVISOR_vcpu_op(VCPUOP_down, cpu, NULL))
+		    HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(cpu), NULL))
 			BUG();
 
 		xen_setup_runstate_info(cpu);
@@ -264,7 +266,7 @@ void xen_vcpu_restore(void)
 			xen_vcpu_setup(cpu);
 
 		if (other_cpu && is_up &&
-		    HYPERVISOR_vcpu_op(VCPUOP_up, cpu, NULL))
+		    HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL))
 			BUG();
 	}
 }

commit 88e957d6e47f1232ad15b21e54a44f1147ea8c1b
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Jun 30 17:56:37 2016 +0200

    xen: introduce xen_vcpu_id mapping
    
    It may happen that Xen's and Linux's ideas of vCPU id diverge. In
    particular, when we crash on a secondary vCPU we may want to do kdump
    and unlike plain kexec where we do migrate_to_reboot_cpu() we try
    booting on the vCPU which crashed. This doesn't work very well for
    PVHVM guests as we have a number of hypercalls where we pass vCPU id
    as a parameter. These hypercalls either fail or do something
    unexpected.
    
    To solve the issue introduce percpu xen_vcpu_id mapping. ARM and PV
    guests get direct mapping for now. Boot CPU for PVHVM guest gets its
    id from CPUID. With secondary CPUs it is a bit more
    trickier. Currently, we initialize IPI vectors before these CPUs boot
    so we can't use CPUID. Use ACPI ids from MADT instead.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ee7d71ec762f..d093949c94e0 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -59,6 +59,7 @@
 #include <asm/xen/pci.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>
+#include <asm/xen/cpuid.h>
 #include <asm/fixmap.h>
 #include <asm/processor.h>
 #include <asm/proto.h>
@@ -118,6 +119,10 @@ DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
  */
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 
+/* Linux <-> Xen vCPU id mapping */
+DEFINE_PER_CPU(int, xen_vcpu_id) = -1;
+EXPORT_PER_CPU_SYMBOL(xen_vcpu_id);
+
 enum xen_domain_type xen_domain_type = XEN_NATIVE;
 EXPORT_SYMBOL_GPL(xen_domain_type);
 
@@ -1137,8 +1142,11 @@ void xen_setup_vcpu_info_placement(void)
 {
 	int cpu;
 
-	for_each_possible_cpu(cpu)
+	for_each_possible_cpu(cpu) {
+		/* Set up direct vCPU id mapping for PV guests. */
+		per_cpu(xen_vcpu_id, cpu) = cpu;
 		xen_vcpu_setup(cpu);
+	}
 
 	/* xen_vcpu_setup managed to place the vcpu_info within the
 	 * percpu area for all cpus, so make use of it. Note that for
@@ -1729,6 +1737,9 @@ asmlinkage __visible void __init xen_start_kernel(void)
 #endif
 	xen_raw_console_write("about to get started...\n");
 
+	/* Let's presume PV guests always boot on vCPU with id 0. */
+	per_cpu(xen_vcpu_id, 0) = 0;
+
 	xen_setup_runstate_info(0);
 
 	xen_efi_init();
@@ -1797,6 +1808,12 @@ static void __init init_hvm_pv_info(void)
 
 	xen_setup_features();
 
+	cpuid(base + 4, &eax, &ebx, &ecx, &edx);
+	if (eax & XEN_HVM_CPUID_VCPU_ID_PRESENT)
+		this_cpu_write(xen_vcpu_id, ebx);
+	else
+		this_cpu_write(xen_vcpu_id, smp_processor_id());
+
 	pv_info.name = "Xen HVM";
 
 	xen_domain_type = XEN_HVM_DOMAIN;
@@ -1808,6 +1825,10 @@ static int xen_hvm_cpu_notify(struct notifier_block *self, unsigned long action,
 	int cpu = (long)hcpu;
 	switch (action) {
 	case CPU_UP_PREPARE:
+		if (cpu_acpi_id(cpu) != U32_MAX)
+			per_cpu(xen_vcpu_id, cpu) = cpu_acpi_id(cpu);
+		else
+			per_cpu(xen_vcpu_id, cpu) = cpu;
 		xen_vcpu_setup(cpu);
 		if (xen_have_vector_callback) {
 			if (xen_feature(XENFEAT_hvm_safe_pvclock))

commit 7a2463dcacee3f2f36c78418c201756372eeea6b
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:59 2016 -0400

    x86/xen: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20160714001901.31603-7-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 760789ae8562..1f984ab87c4a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -23,7 +23,7 @@
 #include <linux/sched.h>
 #include <linux/kprobes.h>
 #include <linux/bootmem.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/mm.h>
 #include <linux/page-flags.h>
 #include <linux/highmem.h>

commit 585423c8c4d2f39a2c299bc6dd16433e6141fba5
Author: Amitoj Kaur Chawla <amitoj1606@gmail.com>
Date:   Wed Jun 29 20:30:38 2016 +0530

    x86/xen: Use DIV_ROUND_UP
    
    The kernel.h macro DIV_ROUND_UP performs the computation
    (((n) + (d) - 1) /(d)) but is perhaps more readable.
    
    The Coccinelle script used to make this change is as follows:
    @haskernel@
    @@
    
    #include <linux/kernel.h>
    
    @depends on haskernel@
    expression n,d;
    @@
    
    (
    - (n + d - 1) / d
    + DIV_ROUND_UP(n,d)
    |
    - (n + (d - 1)) / d
    + DIV_ROUND_UP(n,d)
    )
    
    Signed-off-by: Amitoj Kaur Chawla <amitoj1606@gmail.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 760789ae8562..ee7d71ec762f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -590,7 +590,7 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 {
 	unsigned long va = dtr->address;
 	unsigned int size = dtr->size + 1;
-	unsigned pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	unsigned pages = DIV_ROUND_UP(size, PAGE_SIZE);
 	unsigned long frames[pages];
 	int f;
 
@@ -639,7 +639,7 @@ static void __init xen_load_gdt_boot(const struct desc_ptr *dtr)
 {
 	unsigned long va = dtr->address;
 	unsigned int size = dtr->size + 1;
-	unsigned pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	unsigned pages = DIV_ROUND_UP(size, PAGE_SIZE);
 	unsigned long frames[pages];
 	int f;
 

commit 99158f10e91768d34c5004c40c42f802b719bcae
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue May 24 15:48:38 2016 -0700

    x86/xen: Simplify set_aliased_prot()
    
    A year ago, via the following commit:
    
      aa1acff356bb ("x86/xen: Probe target addresses in set_aliased_prot() before the hypercall")
    
    I added an explicit probe to work around a hypercall issue.  The code can
    be simplified by using probe_kernel_read().
    
    No change in functionality.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Vrabel <dvrabel@cantab.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/0706f1a2538e481194514197298cca6b5e3f2638.1464129798.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 760789ae8562..0f87db2cc6a8 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -521,9 +521,7 @@ static void set_aliased_prot(void *v, pgprot_t prot)
 
 	preempt_disable();
 
-	pagefault_disable();	/* Avoid warnings due to being atomic. */
-	__get_user(dummy, (unsigned char __user __force *)v);
-	pagefault_enable();
+	probe_kernel_read(&dummy, v, 1);
 
 	if (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))
 		BUG();

commit 9a45f036af363aec1efec08827c825d69c115a9a
Merge: 168f1a7163b3 d2d3462f9f08
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 15:54:01 2016 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - prepare for more KASLR related changes, by restructuring, cleaning
         up and fixing the existing boot code.  (Kees Cook, Baoquan He,
         Yinghai Lu)
    
       - simplifly/concentrate subarch handling code, eliminate
         paravirt_enabled() usage.  (Luis R Rodriguez)"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (50 commits)
      x86/KASLR: Clarify purpose of each get_random_long()
      x86/KASLR: Add virtual address choosing function
      x86/KASLR: Return earliest overlap when avoiding regions
      x86/KASLR: Add 'struct slot_area' to manage random_addr slots
      x86/boot: Add missing file header comments
      x86/KASLR: Initialize mapping_info every time
      x86/boot: Comment what finalize_identity_maps() does
      x86/KASLR: Build identity mappings on demand
      x86/boot: Split out kernel_ident_mapping_init()
      x86/boot: Clean up indenting for asm/boot.h
      x86/KASLR: Improve comments around the mem_avoid[] logic
      x86/boot: Simplify pointer casting in choose_random_location()
      x86/KASLR: Consolidate mem_avoid[] entries
      x86/boot: Clean up pointer casting
      x86/boot: Warn on future overlapping memcpy() use
      x86/boot: Extract error reporting functions
      x86/boot: Correctly bounds-check relocations
      x86/KASLR: Clean up unused code from old 'run_size' and rename it to 'kernel_total_size'
      x86/boot: Fix "run_size" calculation
      x86/boot: Calculate decompression size during boot not build
      ...

commit 867fe800b4c423bce46e66ccb2ce91bebbd5afc6
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Wed Apr 13 17:04:44 2016 -0700

    x86/paravirt: Remove paravirt_enabled()
    
    Now that all previous paravirt_enabled() uses were replaced with proper
    x86 semantics by the previous patches we can remove the unused
    paravirt_enabled() mechanism.
    
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrew.cooper3@citrix.com
    Cc: andriy.shevchenko@linux.intel.com
    Cc: bigeasy@linutronix.de
    Cc: boris.ostrovsky@oracle.com
    Cc: david.vrabel@citrix.com
    Cc: ffainelli@freebox.fr
    Cc: george.dunlap@citrix.com
    Cc: glin@suse.com
    Cc: jlee@suse.com
    Cc: josh@joshtriplett.org
    Cc: julien.grall@linaro.org
    Cc: konrad.wilk@oracle.com
    Cc: kozerkov@parallels.com
    Cc: lenb@kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-acpi@vger.kernel.org
    Cc: lv.zheng@intel.com
    Cc: matt@codeblueprint.co.uk
    Cc: mbizon@freebox.fr
    Cc: rjw@rjwysocki.net
    Cc: robert.moore@intel.com
    Cc: rusty@rustcorp.com.au
    Cc: tiwai@suse.de
    Cc: toshi.kani@hp.com
    Cc: xen-devel@lists.xensource.com
    Link: http://lkml.kernel.org/r/1460592286-300-15-git-send-email-mcgrof@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 752029d571bf..5fc20a1108c7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1187,7 +1187,6 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 }
 
 static const struct pv_info xen_info __initconst = {
-	.paravirt_enabled = 1,
 	.shared_kernel_pmd = 0,
 
 #ifdef CONFIG_X86_64

commit 8d152e7a5c7537b18b4e9e0eb96f549b016636dc
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Wed Apr 13 17:04:34 2016 -0700

    x86/rtc: Replace paravirt rtc check with platform legacy quirk
    
    We have 4 types of x86 platforms that disable RTC:
    
      * Intel MID
      * Lguest - uses paravirt
      * Xen dom-U - uses paravirt
      * x86 on legacy systems annotated with an ACPI legacy flag
    
    We can consolidate all of these into a platform specific legacy
    quirk set early in boot through i386_start_kernel() and through
    x86_64_start_reservations(). This deals with the RTC quirks which
    we can rely on through the hardware subarch, the ACPI check can
    be dealt with separately.
    
    For Xen things are bit more complex given that the @X86_SUBARCH_XEN
    x86_hardware_subarch is shared on for Xen which uses the PV path for
    both domU and dom0. Since the semantics for differentiating between
    the two are Xen specific we provide a platform helper to help override
    default legacy features -- x86_platform.set_legacy_features(). Use
    of this helper is highly discouraged, its only purpose should be
    to account for the lack of semantics available within your given
    x86_hardware_subarch.
    
    As per 0-day, this bumps the vmlinux size using i386-tinyconfig as
    follows:
    
    TOTAL   TEXT   init.text    x86_early_init_platform_quirks()
    +70     +62    +62          +43
    
    Only 8 bytes overhead total, as the main increase in size is
    all removed via __init.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrew.cooper3@citrix.com
    Cc: andriy.shevchenko@linux.intel.com
    Cc: bigeasy@linutronix.de
    Cc: boris.ostrovsky@oracle.com
    Cc: david.vrabel@citrix.com
    Cc: ffainelli@freebox.fr
    Cc: george.dunlap@citrix.com
    Cc: glin@suse.com
    Cc: jlee@suse.com
    Cc: josh@joshtriplett.org
    Cc: julien.grall@linaro.org
    Cc: konrad.wilk@oracle.com
    Cc: kozerkov@parallels.com
    Cc: lenb@kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-acpi@vger.kernel.org
    Cc: lv.zheng@intel.com
    Cc: matt@codeblueprint.co.uk
    Cc: mbizon@freebox.fr
    Cc: rjw@rjwysocki.net
    Cc: robert.moore@intel.com
    Cc: rusty@rustcorp.com.au
    Cc: tiwai@suse.de
    Cc: toshi.kani@hp.com
    Cc: xen-devel@lists.xensource.com
    Link: http://lkml.kernel.org/r/1460592286-300-5-git-send-email-mcgrof@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 61f4d9f67f60..752029d571bf 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1193,7 +1193,6 @@ static const struct pv_info xen_info __initconst = {
 #ifdef CONFIG_X86_64
 	.extra_user_64bit_cs = FLAT_USER_CS64,
 #endif
-	.features = 0,
 	.name = "Xen",
 };
 
@@ -1506,6 +1505,11 @@ static void __init xen_pvh_early_guest_init(void)
 }
 #endif    /* CONFIG_XEN_PVH */
 
+static void __init xen_dom0_set_legacy_features(void)
+{
+	x86_platform.legacy.rtc = 1;
+}
+
 /* First C function to be called on Xen boot */
 asmlinkage __visible void __init xen_start_kernel(void)
 {
@@ -1527,8 +1531,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
-	if (xen_initial_domain())
-		pv_info.features |= PV_SUPPORTED_RTC;
 	pv_init_ops = xen_init_ops;
 	if (!xen_pvh_domain()) {
 		pv_cpu_ops = xen_cpu_ops;
@@ -1688,6 +1690,8 @@ asmlinkage __visible void __init xen_start_kernel(void)
 			.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS,
 		};
 
+		x86_platform.set_legacy_features =
+				xen_dom0_set_legacy_features;
 		xen_init_vga(info, xen_start_info->console.dom0.info_size);
 		xen_start_info->console.domU.mfn = 0;
 		xen_start_info->console.domU.evtchn = 0;

commit ea1794812410e92c537c839bedeb2d2b2f87c80d
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Wed Apr 13 17:04:32 2016 -0700

    x86/xen: Use X86_SUBARCH_XEN for PV guest boots
    
    The use of subarch should have no current effect on Xen
    PV guests, as such this should have no current functional
    effects.
    
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrew.cooper3@citrix.com
    Cc: andriy.shevchenko@linux.intel.com
    Cc: bigeasy@linutronix.de
    Cc: boris.ostrovsky@oracle.com
    Cc: ffainelli@freebox.fr
    Cc: george.dunlap@citrix.com
    Cc: glin@suse.com
    Cc: jgross@suse.com
    Cc: jlee@suse.com
    Cc: josh@joshtriplett.org
    Cc: julien.grall@linaro.org
    Cc: konrad.wilk@oracle.com
    Cc: kozerkov@parallels.com
    Cc: lenb@kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-acpi@vger.kernel.org
    Cc: lv.zheng@intel.com
    Cc: matt@codeblueprint.co.uk
    Cc: mbizon@freebox.fr
    Cc: rjw@rjwysocki.net
    Cc: robert.moore@intel.com
    Cc: rusty@rustcorp.com.au
    Cc: tiwai@suse.de
    Cc: toshi.kani@hp.com
    Cc: xen-devel@lists.xensource.com
    Link: http://lkml.kernel.org/r/1460592286-300-3-git-send-email-mcgrof@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 880862c7d9dd..61f4d9f67f60 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1670,6 +1670,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	boot_params.hdr.ramdisk_image = initrd_start;
 	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
 	boot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);
+	boot_params.hdr.hardware_subarch = X86_SUBARCH_XEN;
 
 	if (!xen_initial_domain()) {
 		add_preferred_console("xenboot", 0, NULL);

commit dd2f4a004b016bbfb64f1de49cb45e66232e40a6
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sat Apr 2 07:01:38 2016 -0700

    x86/paravirt: Add paravirt_{read,write}_msr()
    
    This adds paravirt callbacks for unsafe MSR access.  On native, they
    call native_{read,write}_msr().  On Xen, they use xen_{read,write}_msr_safe().
    
    Nothing uses them yet for ease of bisection.  The next patch will
    use them in rdmsrl(), wrmsrl(), etc.
    
    I intentionally didn't make them warn on #GP on Xen.  I think that
    should be done separately by the Xen maintainers.
    
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: KVM list <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel <Xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/880eebc5dcd2ad9f310d41345f82061ea500e9fa.1459605520.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 13f756fdcb33..6ab672233ac9 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1092,6 +1092,26 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 	return ret;
 }
 
+static u64 xen_read_msr(unsigned int msr)
+{
+	/*
+	 * This will silently swallow a #GP from RDMSR.  It may be worth
+	 * changing that.
+	 */
+	int err;
+
+	return xen_read_msr_safe(msr, &err);
+}
+
+static void xen_write_msr(unsigned int msr, unsigned low, unsigned high)
+{
+	/*
+	 * This will silently swallow a #GP from WRMSR.  It may be worth
+	 * changing that.
+	 */
+	xen_write_msr_safe(msr, low, high);
+}
+
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
@@ -1222,6 +1242,9 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 
 	.wbinvd = native_wbinvd,
 
+	.read_msr = xen_read_msr,
+	.write_msr = xen_write_msr,
+
 	.read_msr_safe = xen_read_msr_safe,
 	.write_msr_safe = xen_write_msr_safe,
 

commit c2ee03b2a94d7ba692cf6206bbe069d5bfcc20ed
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sat Apr 2 07:01:36 2016 -0700

    x86/paravirt: Add _safe to the read_ms()r and write_msr() PV callbacks
    
    These callbacks match the _safe variants, so name them accordingly.
    This will make room for unsafe PV callbacks.
    
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: KVM list <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel <Xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/9ee3fb6a196a514c93325bdfa15594beecf04876.1459605520.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9b8f1eacc110..13f756fdcb33 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1222,8 +1222,8 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 
 	.wbinvd = native_wbinvd,
 
-	.read_msr = xen_read_msr_safe,
-	.write_msr = xen_write_msr_safe,
+	.read_msr_safe = xen_read_msr_safe,
+	.write_msr_safe = xen_write_msr_safe,
 
 	.read_pmc = xen_read_pmc,
 

commit d8d1c35139481ee8e292d91cd3fd35a6b3a316eb
Merge: cb44d0cfc296 1886297ce0c8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 13 11:36:19 2016 +0200

    Merge branch 'x86/mm' into x86/asm to resolve conflict and to create common base
    
     Conflicts:
            arch/x86/include/asm/cpufeature.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 16bf92261b1b6cb1a1c0671b445a2fcb5a1ecc96
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:42:03 2016 +0200

    x86/cpufeature: Remove cpu_has_pse
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1459266123-21878-11-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 055f48ddb03c..ff2a2e6ef7af 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1469,7 +1469,7 @@ static void xen_pvh_set_cr_flags(int cpu)
 	 * For BSP, PSE PGE are set in probe_page_size_mask(), for APs
 	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu__init_cpu().
 	*/
-	if (cpu_has_pse)
+	if (boot_cpu_has(X86_FEATURE_PSE))
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
 	if (boot_cpu_has(X86_FEATURE_PGE))

commit c109bf95992b391bb40bc37c5d309d13fead99b5
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:42:02 2016 +0200

    x86/cpufeature: Remove cpu_has_pge
    
    Use static_cpu_has() in __flush_tlb_all() due to the time-sensitivity of
    this one.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1459266123-21878-10-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 880862c7d9dd..055f48ddb03c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1472,7 +1472,7 @@ static void xen_pvh_set_cr_flags(int cpu)
 	if (cpu_has_pse)
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
-	if (cpu_has_pge)
+	if (boot_cpu_has(X86_FEATURE_PGE))
 		cr4_set_bits_and_update_boot(X86_CR4_PGE);
 }
 

commit 88ba281108ed0c25c9d292b48bd3f272fcb90dd0
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Wed Mar 23 15:42:02 2016 -0600

    x86/xen, pat: Remove PAT table init code from Xen
    
    Xen supports PAT without MTRRs for its guests.  In order to
    enable WC attribute, it was necessary for xen_start_kernel()
    to call pat_init_cache_modes() to update PAT table before
    starting guest kernel.
    
    Now that the kernel initializes PAT table to the BIOS handoff
    state when MTRR is disabled, this Xen-specific PAT init code
    is no longer necessary.  Delete it from xen_start_kernel().
    
    Also change __init_cache_modes() to a static function since
    PAT table should not be tweaked by other modules.
    
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: elliott@hpe.com
    Cc: paul.gortmaker@windriver.com
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1458769323-24491-7-git-send-email-toshi.kani@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c469a7c7c309..d8cca75e3b3e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -75,7 +75,6 @@
 #include <asm/mach_traps.h>
 #include <asm/mwait.h>
 #include <asm/pci_x86.h>
-#include <asm/pat.h>
 #include <asm/cpu.h>
 
 #ifdef CONFIG_ACPI
@@ -1511,7 +1510,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 {
 	struct physdev_set_iopl set_iopl;
 	unsigned long initrd_start = 0;
-	u64 pat;
 	int rc;
 
 	if (!xen_start_info)
@@ -1618,13 +1616,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 				   xen_start_info->nr_pages);
 	xen_reserve_special_pages();
 
-	/*
-	 * Modify the cache mode translation tables to match Xen's PAT
-	 * configuration.
-	 */
-	rdmsrl(MSR_IA32_CR_PAT, pat);
-	__init_cache_modes(pat);
-
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 #ifdef CONFIG_X86_32

commit 02f037d641dc6672be5cfe7875a48ab99b95b154
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Wed Mar 23 15:41:57 2016 -0600

    x86/mm/pat: Add support of non-default PAT MSR setting
    
    In preparation for fixing a regression caused by:
    
      9cd25aac1f44 ("x86/mm/pat: Emulate PAT when it is disabled")'
    
    ... PAT needs to support a case that PAT MSR is initialized with a
    non-default value.
    
    When pat_init() is called and PAT is disabled, it initializes the
    PAT table with the BIOS default value. Xen, however, sets PAT MSR
    with a non-default value to enable WC. This causes inconsistency
    between the PAT table and PAT MSR when PAT is set to disable on Xen.
    
    Change pat_init() to handle the PAT disable cases properly.  Add
    init_cache_modes() to handle two cases when PAT is set to disable.
    
     1. CPU supports PAT: Set PAT table to be consistent with PAT MSR.
     2. CPU does not support PAT: Set PAT table to be consistent with
        PWT and PCD bits in a PTE.
    
    Note, __init_cache_modes(), renamed from pat_init_cache_modes(),
    will be changed to a static function in a later patch.
    
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: elliott@hpe.com
    Cc: konrad.wilk@oracle.com
    Cc: paul.gortmaker@windriver.com
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1458769323-24491-2-git-send-email-toshi.kani@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 880862c7d9dd..c469a7c7c309 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1623,7 +1623,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	 * configuration.
 	 */
 	rdmsrl(MSR_IA32_CR_PAT, pat);
-	pat_init_cache_modes(pat);
+	__init_cache_modes(pat);
 
 	/* keep using Xen gdt for now; no urgent need to change it */
 

commit d88f48e12821ab4b2244124d50ac094568f48db5
Merge: be53f58fa0fc 9da77666d697
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 09:47:32 2016 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes:
    
       - fix hotplug bugs
       - fix irq live lock
       - fix various topology handling bugs
       - fix APIC ACK ordering
       - fix PV iopl handling
       - fix speling
       - fix/tweak memcpy_mcsafe() return value
       - fix fbcon bug
       - remove stray prototypes"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/msr: Remove unused native_read_tscp()
      x86/apic: Remove declaration of unused hw_nmi_is_cpu_stuck
      x86/oprofile/nmi: Add missing hotplug FROZEN handling
      x86/hpet: Use proper mask to modify hotplug action
      x86/apic/uv: Fix the hotplug notifier
      x86/apb/timer: Use proper mask to modify hotplug action
      x86/topology: Use total_cpus not nr_cpu_ids for logical packages
      x86/topology: Fix Intel HT disable
      x86/topology: Fix logical package mapping
      x86/irq: Cure live lock in fixup_irqs()
      x86/tsc: Prevent NULL pointer deref in calibrate_delay_is_known()
      x86/apic: Fix suspicious RCU usage in smp_trace_call_function_interrupt()
      x86/iopl: Fix iopl capability check on Xen PV
      x86/iopl/64: Properly context-switch IOPL on Xen PV
      selftests/x86: Add an iopl test
      x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe()
      x86/video: Don't assume all FB devices are PCI devices
      arch/x86/irq: Purge useless handler declarations from hw_irq.h
      x86: Fix misspellings in comments

commit 26660a4046b171a752e72a1dd32153230234fe3a
Merge: 46e595a17dcf 1bcb58a09993
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 20 18:23:21 2016 -0700

    Merge branch 'core-objtool-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull 'objtool' stack frame validation from Ingo Molnar:
     "This tree adds a new kernel build-time object file validation feature
      (ONFIG_STACK_VALIDATION=y): kernel stack frame correctness validation.
      It was written by and is maintained by Josh Poimboeuf.
    
      The motivation: there's a category of hard to find kernel bugs, most
      of them in assembly code (but also occasionally in C code), that
      degrades the quality of kernel stack dumps/backtraces.  These bugs are
      hard to detect at the source code level.  Such bugs result in
      incorrect/incomplete backtraces most of time - but can also in some
      rare cases result in crashes or other undefined behavior.
    
      The build time correctness checking is done via the new 'objtool'
      user-space utility that was written for this purpose and which is
      hosted in the kernel repository in tools/objtool/.  The tool's (very
      simple) UI and source code design is shaped after Git and perf and
      shares quite a bit of infrastructure with tools/perf (which tooling
      infrastructure sharing effort got merged via perf and is already
      upstream).  Objtool follows the well-known kernel coding style.
    
      Objtool does not try to check .c or .S files, it instead analyzes the
      resulting .o generated machine code from first principles: it decodes
      the instruction stream and interprets it.  (Right now objtool supports
      the x86-64 architecture.)
    
      From tools/objtool/Documentation/stack-validation.txt:
    
       "The kernel CONFIG_STACK_VALIDATION option enables a host tool named
        objtool which runs at compile time.  It has a "check" subcommand
        which analyzes every .o file and ensures the validity of its stack
        metadata.  It enforces a set of rules on asm code and C inline
        assembly code so that stack traces can be reliable.
    
        Currently it only checks frame pointer usage, but there are plans to
        add CFI validation for C files and CFI generation for asm files.
    
        For each function, it recursively follows all possible code paths
        and validates the correct frame pointer state at each instruction.
    
        It also follows code paths involving special sections, like
        .altinstructions, __jump_table, and __ex_table, which can add
        alternative execution paths to a given instruction (or set of
        instructions).  Similarly, it knows how to follow switch statements,
        for which gcc sometimes uses jump tables."
    
      When this new kernel option is enabled (it's disabled by default), the
      tool, if it finds any suspicious assembly code pattern, outputs
      warnings in compiler warning format:
    
        warning: objtool: rtlwifi_rate_mapping()+0x2e7: frame pointer state mismatch
        warning: objtool: cik_tiling_mode_table_init()+0x6ce: call without frame pointer save/setup
        warning: objtool:__schedule()+0x3c0: duplicate frame pointer save
        warning: objtool:__schedule()+0x3fd: sibling call from callable instruction with changed frame pointer
    
      ... so that scripts that pick up compiler warnings will notice them.
      All known warnings triggered by the tool are fixed by the tree, most
      of the commits in fact prepare the kernel to be warning-free.  Most of
      them are bugfixes or cleanups that stand on their own, but there are
      also some annotations of 'special' stack frames for justified cases
      such entries to JIT-ed code (BPF) or really special boot time code.
    
      There are two other long-term motivations behind this tool as well:
    
       - To improve the quality and reliability of kernel stack frames, so
         that they can be used for optimized live patching.
    
       - To create independent infrastructure to check the correctness of
         CFI stack frames at build time.  CFI debuginfo is notoriously
         unreliable and we cannot use it in the kernel as-is without extra
         checking done both on the kernel side and on the build side.
    
      The quality of kernel stack frames matters to debuggability as well,
      so IMO we can merge this without having to consider the live patching
      or CFI debuginfo angle"
    
    * 'core-objtool-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (52 commits)
      objtool: Only print one warning per function
      objtool: Add several performance improvements
      tools: Copy hashtable.h into tools directory
      objtool: Fix false positive warnings for functions with multiple switch statements
      objtool: Rename some variables and functions
      objtool: Remove superflous INIT_LIST_HEAD
      objtool: Add helper macros for traversing instructions
      objtool: Fix false positive warnings related to sibling calls
      objtool: Compile with debugging symbols
      objtool: Detect infinite recursion
      objtool: Prevent infinite recursion in noreturn detection
      objtool: Detect and warn if libelf is missing and don't break the build
      tools: Support relative directory path for 'O='
      objtool: Support CROSS_COMPILE
      x86/asm/decoder: Use explicitly signed chars
      objtool: Enable stack metadata validation on 64-bit x86
      objtool: Add CONFIG_STACK_VALIDATION option
      objtool: Add tool to perform compile-time stack metadata validation
      x86/kprobes: Mark kretprobe_trampoline() stack frame as non-standard
      sched: Always inline context_switch()
      ...

commit b7a584598aea7ca73140cb87b40319944dd3393f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 16 14:14:21 2016 -0700

    x86/iopl/64: Properly context-switch IOPL on Xen PV
    
    On Xen PV, regs->flags doesn't reliably reflect IOPL and the
    exit-to-userspace code doesn't change IOPL.  We need to context
    switch it manually.
    
    I'm doing this without going through paravirt because this is
    specific to Xen PV.  After the dust settles, we can merge this with
    the 32-bit code, tidy up the iopl syscall implementation, and remove
    the set_iopl pvop entirely.
    
    Fixes XSA-171.
    
    Reviewewd-by: Jan Beulich <JBeulich@suse.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/693c3bd7aeb4d3c27c92c622b7d0f554a458173c.1458162709.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2c261082eadf..8381fb990c7f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -961,7 +961,7 @@ static void xen_load_sp0(struct tss_struct *tss,
 	tss->x86_tss.sp0 = thread->sp0;
 }
 
-static void xen_set_iopl_mask(unsigned mask)
+void xen_set_iopl_mask(unsigned mask)
 {
 	struct physdev_set_iopl set_iopl;
 

commit 983bb6d254c77aaec581473e11c2eb859294f1f2
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Sun Feb 28 22:22:36 2016 -0600

    x86/xen: Mark xen_cpuid() stack frame as non-standard
    
    objtool reports the following false positive warning:
    
      arch/x86/xen/enlighten.o: warning: objtool: xen_cpuid()+0x41: can't find jump dest instruction at .text+0x108
    
    The warning is due to xen_cpuid()'s use of XEN_EMULATE_PREFIX to insert
    some fake instructions which objtool doesn't know how to decode.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Bernd Petrovitsch <bernd@petrovitsch.priv.at>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris J Arges <chris.j.arges@canonical.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Marek <mmarek@suse.cz>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Pedro Alves <palves@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: live-patching@vger.kernel.org
    Link: http://lkml.kernel.org/r/bb88399840406629e3417831dc371ecd2842e2a6.1456719558.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d09e4c9d7cc5..5c45a69ffe84 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -32,6 +32,7 @@
 #include <linux/gfp.h>
 #include <linux/memblock.h>
 #include <linux/edd.h>
+#include <linux/frame.h>
 
 #ifdef CONFIG_KEXEC_CORE
 #include <linux/kexec.h>
@@ -351,8 +352,8 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 	*cx &= maskecx;
 	*cx |= setecx;
 	*dx &= maskedx;
-
 }
+STACK_FRAME_NON_STANDARD(xen_cpuid); /* XEN_EMULATE_PREFIX */
 
 static bool __init xen_check_mwait(void)
 {

commit 16aaa53756501914a863ae7a15fcb070dc27c3d7
Author: Huaitong Han <huaitong.han@intel.com>
Date:   Mon Jan 25 20:41:47 2016 +0100

    x86/cpufeature: Use enum cpuid_leafs instead of magic numbers
    
    Most of the magic numbers in x86_capability[] have been converted to
    'enum cpuid_leafs', and this patch updates the remaining part.
    
    Signed-off-by: Huaitong Han <huaitong.han@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Kuleshov <kuleshovmail@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hector Marco-Gisbert <hecmargi@upv.es>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: lguest@lists.ozlabs.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1453750913-4781-3-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d09e4c9d7cc5..2c261082eadf 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1654,7 +1654,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	cpu_detect(&new_cpu_data);
 	set_cpu_cap(&new_cpu_data, X86_FEATURE_FPU);
 	new_cpu_data.wp_works_ok = 1;
-	new_cpu_data.x86_capability[0] = cpuid_edx(1);
+	new_cpu_data.x86_capability[CPUID_1_EDX] = cpuid_edx(1);
 #endif
 
 	if (xen_start_info->mod_start) {

commit c9bed1cf51011c815d88288b774865d013ca78a8
Merge: 75777c1855e1 a4cdb556cae0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 13:05:36 2016 -0800

    Merge tag 'for-linus-4.5-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
     "Xen features and fixes for 4.5-rc0:
    
       - Stolen ticks and PV wallclock support for arm/arm64
    
       - Add grant copy ioctl to gntdev device"
    
    * tag 'for-linus-4.5-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/gntdev: add ioctl for grant copy
      x86/xen: don't reset vcpu_info on a cancelled suspend
      xen/gntdev: constify mmu_notifier_ops structures
      xen/grant-table: constify gnttab_ops structure
      xen/time: use READ_ONCE
      xen/x86: convert remaining timespec to timespec64 in xen_pvclock_gtod_notify
      xen/x86: support XENPF_settime64
      xen/arm: set the system time in Xen via the XENPF_settime64 hypercall
      xen/arm: introduce xen_read_wallclock
      arm: extend pvclock_wall_clock with sec_hi
      xen: introduce XENPF_settime64
      xen/arm: introduce HYPERVISOR_platform_op on arm and arm64
      xen: rename dom0_op to platform_op
      xen/arm: account for stolen ticks
      arm64: introduce CONFIG_PARAVIRT, PARAVIRT_TIME_ACCOUNTING and pv_time_ops
      arm: introduce CONFIG_PARAVIRT, PARAVIRT_TIME_ACCOUNTING and pv_time_ops
      missing include asm/paravirt.h in cputime.c
      xen: move xen_setup_runstate_info and get_runstate_snapshot to drivers/xen/time.c

commit 67c707e451e12f59e57bca6cf33b5803cb74b022
Merge: 463eb8ac337b 0105c8d8334f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 11 16:26:03 2016 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "The main changes in this cycle were:
    
       - code patching and cpu_has cleanups (Borislav Petkov)
    
       - paravirt cleanups (Juergen Gross)
    
       - TSC cleanup (Thomas Gleixner)
    
       - ptrace cleanup (Chen Gang)"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      arch/x86/kernel/ptrace.c: Remove unused arg_offs_table
      x86/mm: Align macro defines
      x86/cpu: Provide a config option to disable static_cpu_has
      x86/cpufeature: Remove unused and seldomly used cpu_has_xx macros
      x86/cpufeature: Cleanup get_cpu_cap()
      x86/cpufeature: Move some of the scattered feature bits to x86_capability
      x86/paravirt: Remove paravirt ops pmd_update[_defer] and pte_update_defer
      x86/paravirt: Remove unused pv_apic_ops structure
      x86/tsc: Remove unused tsc_pre_init() hook
      x86: Remove unused function cpu_has_ht_siblings()
      x86/paravirt: Kill some unused patching functions

commit 88cbfd07119e394b9cbb1a4a764056c4b37e8378
Merge: 4f19b8803bdd 8705d603edd4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 11 15:58:16 2016 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - vDSO and asm entry improvements (Andy Lutomirski)
    
       - Xen paravirt entry enhancements (Boris Ostrovsky)
    
       - asm entry labels enhancement (Borislav Petkov)
    
       - and other misc changes (Thomas Gleixner, me)"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vsdo: Fix build on PARAVIRT_CLOCK=y, KVM_GUEST=n
      Revert "x86/kvm: On KVM re-enable (e.g. after suspend), update clocks"
      x86/entry/64_compat: Make labels local
      x86/platform/uv: Include clocksource.h for clocksource_touch_watchdog()
      x86/vdso: Enable vdso pvclock access on all vdso variants
      x86/vdso: Remove pvclock fixmap machinery
      x86/vdso: Get pvclock data from the vvar VMA instead of the fixmap
      x86, vdso, pvclock: Simplify and speed up the vdso pvclock reader
      x86/kvm: On KVM re-enable (e.g. after suspend), update clocks
      x86/entry/64: Bypass enter_from_user_mode on non-context-tracking boots
      x86/asm: Add asm macros for static keys/jump labels
      x86/asm: Error out if asm/jump_label.h is included inappropriately
      context_tracking: Switch to new static_branch API
      x86/entry, x86/paravirt: Remove the unused usergs_sysret32 PV op
      x86/paravirt: Remove the unused irq_enable_sysexit pv op
      x86/xen: Avoid fast syscall path for Xen PV guests

commit cfafae940381207d48b11a73a211142dba5947d3
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Mon Nov 23 10:36:12 2015 +0000

    xen: rename dom0_op to platform_op
    
    The dom0_op hypercall has been renamed to platform_op since Xen 3.2,
    which is ancient, and modern upstream Linux kernels cannot run as dom0
    and it anymore anyway.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5774800ff583..f963c40fae01 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -415,7 +415,7 @@ static bool __init xen_check_mwait(void)
 
 	set_xen_guest_handle(op.u.set_pminfo.pdc, buf);
 
-	if ((HYPERVISOR_dom0_op(&op) == 0) &&
+	if ((HYPERVISOR_platform_op(&op) == 0) &&
 	    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {
 		cpuid_leaf5_ecx_val = cx;
 		cpuid_leaf5_edx_val = dx;
@@ -1374,7 +1374,7 @@ static void __init xen_boot_params_init_edd(void)
 		info->params.length = sizeof(info->params);
 		set_xen_guest_handle(op.u.firmware_info.u.disk_info.edd_params,
 				     &info->params);
-		ret = HYPERVISOR_dom0_op(&op);
+		ret = HYPERVISOR_platform_op(&op);
 		if (ret)
 			break;
 
@@ -1392,7 +1392,7 @@ static void __init xen_boot_params_init_edd(void)
 	op.u.firmware_info.type = XEN_FW_DISK_MBR_SIGNATURE;
 	for (nr = 0; nr < EDD_MBR_SIG_MAX; nr++) {
 		op.u.firmware_info.index = nr;
-		ret = HYPERVISOR_dom0_op(&op);
+		ret = HYPERVISOR_platform_op(&op);
 		if (ret)
 			break;
 		mbr_signature[nr] = op.u.firmware_info.u.disk_mbr_signature.mbr_signature;
@@ -1698,7 +1698,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 		xen_start_info->console.domU.mfn = 0;
 		xen_start_info->console.domU.evtchn = 0;
 
-		if (HYPERVISOR_dom0_op(&op) == 0)
+		if (HYPERVISOR_platform_op(&op) == 0)
 			boot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;
 
 		/* Make sure ACS will be enabled */

commit d8c98a1d1488747625ad6044d423406e17e99b7a
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Dec 11 09:07:53 2015 -0500

    x86/paravirt: Prevent rtc_cmos platform device init on PV guests
    
    Adding the rtc platform device in non-privileged Xen PV guests causes
    an IRQ conflict because these guests do not have legacy PIC and may
    allocate irqs in the legacy range.
    
    In a single VCPU Xen PV guest we should have:
    
    /proc/interrupts:
               CPU0
      0:       4934  xen-percpu-virq      timer0
      1:          0  xen-percpu-ipi       spinlock0
      2:          0  xen-percpu-ipi       resched0
      3:          0  xen-percpu-ipi       callfunc0
      4:          0  xen-percpu-virq      debug0
      5:          0  xen-percpu-ipi       callfuncsingle0
      6:          0  xen-percpu-ipi       irqwork0
      7:        321   xen-dyn-event     xenbus
      8:         90   xen-dyn-event     hvc_console
      ...
    
    But hvc_console cannot get its interrupt because it is already in use
    by rtc0 and the console does not work.
    
      genirq: Flags mismatch irq 8. 00000000 (hvc_console) vs. 00000000 (rtc0)
    
    We can avoid this problem by realizing that unprivileged PV guests (both
    Xen and lguests) are not supposed to have rtc_cmos device and so
    adding it is not necessary.
    
    Privileged guests (i.e. Xen's dom0) do use it but they should not have
    irq conflicts since they allocate irqs above legacy range (above
    gsi_top, in fact).
    
    Instead of explicitly testing whether the guest is privileged we can
    extend pv_info structure to include information about guest's RTC
    support.
    
    Reported-and-tested-by: Sander Eikelenboom <linux@eikelenboom.it>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: vkuznets@redhat.com
    Cc: xen-devel@lists.xenproject.org
    Cc: konrad.wilk@oracle.com
    Cc: stable@vger.kernel.org # 4.2+
    Link: http://lkml.kernel.org/r/1449842873-2613-1-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d315151411e5..b7de78bdc09c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1192,7 +1192,7 @@ static const struct pv_info xen_info __initconst = {
 #ifdef CONFIG_X86_64
 	.extra_user_64bit_cs = FLAT_USER_CS64,
 #endif
-
+	.features = 0,
 	.name = "Xen",
 };
 
@@ -1535,6 +1535,8 @@ asmlinkage __visible void __init xen_start_kernel(void)
 
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
+	if (xen_initial_domain())
+		pv_info.features |= PV_SUPPORTED_RTC;
 	pv_init_ops = xen_init_ops;
 	pv_apic_ops = xen_apic_ops;
 	if (!xen_pvh_domain()) {

commit 0fa85119cd480c1ded7a81ed64f723fe16a15355
Merge: d6ccc3ec9525 1eab0e42450c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Dec 19 11:49:13 2015 +0100

    Merge branch 'linus' into x86/cleanups
    
    Pull in upstream changes so we can apply depending patches.

commit 91e2eea98f94a2ebb143d4c4cdeaa4573d62dc17
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Thu Nov 19 16:55:45 2015 -0500

    x86/xen: Avoid fast syscall path for Xen PV guests
    
    After 32-bit syscall rewrite, and specifically after commit:
    
      5f310f739b4c ("x86/entry/32: Re-implement SYSENTER using the new C path")
    
    ... the stack frame that is passed to xen_sysexit is no longer a
    "standard" one (i.e. it's not pt_regs).
    
    Since we end up calling xen_iret from xen_sysexit we don't need
    to fix up the stack and instead follow entry_SYSENTER_32's IRET
    path directly to xen_iret.
    
    We can do the same thing for compat mode even though stack does
    not need to be fixed. This will allow us to drop usergs_sysret32
    paravirt op (in the subsequent patch)
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: david.vrabel@citrix.com
    Cc: konrad.wilk@oracle.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1447970147-1733-2-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5774800ff583..d315151411e5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1886,8 +1886,10 @@ EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);
 
 static void xen_set_cpu_features(struct cpuinfo_x86 *c)
 {
-	if (xen_pv_domain())
+	if (xen_pv_domain()) {
 		clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
+		set_cpu_cap(c, X86_FEATURE_XENPV);
+	}
 }
 
 const struct hypervisor_x86 x86_hyper_xen = {

commit 88c15ec90ff16880efab92b519436ee17b198477
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Thu Nov 19 16:55:46 2015 -0500

    x86/paravirt: Remove the unused irq_enable_sysexit pv op
    
    As result of commit "x86/xen: Avoid fast syscall path for Xen PV
    guests", the irq_enable_sysexit pv op is not called by Xen PV guests
    anymore and since they were the only ones who used it we can
    safely remove it.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: david.vrabel@citrix.com
    Cc: konrad.wilk@oracle.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1447970147-1733-3-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d315151411e5..a068e36382b7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1229,10 +1229,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 
 	.iret = xen_iret,
 #ifdef CONFIG_X86_64
-	.usergs_sysret32 = xen_sysret32,
 	.usergs_sysret64 = xen_sysret64,
-#else
-	.irq_enable_sysexit = xen_sysexit,
 #endif
 
 	.load_tr_desc = paravirt_nop,

commit 5fdf5d37f40a3b18c0d613463867f71c017b75ef
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Thu Nov 19 16:55:45 2015 -0500

    x86/xen: Avoid fast syscall path for Xen PV guests
    
    After 32-bit syscall rewrite, and specifically after commit:
    
      5f310f739b4c ("x86/entry/32: Re-implement SYSENTER using the new C path")
    
    ... the stack frame that is passed to xen_sysexit is no longer a
    "standard" one (i.e. it's not pt_regs).
    
    Since we end up calling xen_iret from xen_sysexit we don't need
    to fix up the stack and instead follow entry_SYSENTER_32's IRET
    path directly to xen_iret.
    
    We can do the same thing for compat mode even though stack does
    not need to be fixed. This will allow us to drop usergs_sysret32
    paravirt op (in the subsequent patch)
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: david.vrabel@citrix.com
    Cc: konrad.wilk@oracle.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1447970147-1733-2-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5774800ff583..d315151411e5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1886,8 +1886,10 @@ EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);
 
 static void xen_set_cpu_features(struct cpuinfo_x86 *c)
 {
-	if (xen_pv_domain())
+	if (xen_pv_domain()) {
 		clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
+		set_cpu_cap(c, X86_FEATURE_XENPV);
+	}
 }
 
 const struct hypervisor_x86 x86_hyper_xen = {

commit 460958659270b7d750d4ccfe052171cb6f655cbb
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Nov 17 14:44:32 2015 +0100

    x86/paravirt: Remove unused pv_apic_ops structure
    
    The only member of that structure is startup_ipi_hook which is always
    set to paravirt_nop.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: jeremy@goop.org
    Cc: chrisw@sous-sol.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xen.org
    Cc: konrad.wilk@oracle.com
    Cc: boris.ostrovsky@oracle.com
    Link: http://lkml.kernel.org/r/1447767872-16730-1-git-send-email-jgross@suse.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 993b7a71386d..2745e8ae93f3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1264,12 +1264,6 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.end_context_switch = xen_end_context_switch,
 };
 
-static const struct pv_apic_ops xen_apic_ops __initconst = {
-#ifdef CONFIG_X86_LOCAL_APIC
-	.startup_ipi_hook = paravirt_nop,
-#endif
-};
-
 static void xen_reboot(int reason)
 {
 	struct sched_shutdown r = { .reason = reason };
@@ -1535,7 +1529,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;
-	pv_apic_ops = xen_apic_ops;
 	if (!xen_pvh_domain()) {
 		pv_cpu_ops = xen_cpu_ops;
 

commit a314e3eb845389b8f68130c79a63832229dea87b
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Oct 22 16:20:46 2015 +0000

    xen/arm: Enable cpu_hotplug.c
    
    Build cpu_hotplug for ARM and ARM64 guests.
    
    Rename arch_(un)register_cpu to xen_(un)register_cpu and provide an
    empty implementation on ARM and ARM64. On x86 just call
    arch_(un)register_cpu as we are already doing.
    
    Initialize cpu_hotplug on ARM.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 993b7a71386d..5774800ff583 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -75,6 +75,7 @@
 #include <asm/mwait.h>
 #include <asm/pci_x86.h>
 #include <asm/pat.h>
+#include <asm/cpu.h>
 
 #ifdef CONFIG_ACPI
 #include <linux/acpi.h>
@@ -1899,3 +1900,17 @@ const struct hypervisor_x86 x86_hyper_xen = {
 	.set_cpu_features       = xen_set_cpu_features,
 };
 EXPORT_SYMBOL(x86_hyper_xen);
+
+#ifdef CONFIG_HOTPLUG_CPU
+void xen_arch_register_cpu(int num)
+{
+	arch_register_cpu(num);
+}
+EXPORT_SYMBOL(xen_arch_register_cpu);
+
+void xen_arch_unregister_cpu(int num)
+{
+	arch_unregister_cpu(num);
+}
+EXPORT_SYMBOL(xen_arch_unregister_cpu);
+#endif

commit 0b34a166f291d255755be46e43ed5497cdd194f2
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Fri Sep 25 11:59:52 2015 +0200

    x86/xen: Support kexec/kdump in HVM guests by doing a soft reset
    
    Currently there is a number of issues preventing PVHVM Xen guests from
    doing successful kexec/kdump:
    
      - Bound event channels.
      - Registered vcpu_info.
      - PIRQ/emuirq mappings.
      - shared_info frame after XENMAPSPACE_shared_info operation.
      - Active grant mappings.
    
    Basically, newly booted kernel stumbles upon already set up Xen
    interfaces and there is no way to reestablish them. In Xen-4.7 a new
    feature called 'soft reset' is coming. A guest performing kexec/kdump
    operation is supposed to call SCHEDOP_shutdown hypercall with
    SHUTDOWN_soft_reset reason before jumping to new kernel. Hypervisor
    (with some help from toolstack) will do full domain cleanup (but
    keeping its memory and vCPU contexts intact) returning the guest to
    the state it had when it was first booted and thus allowing it to
    start over.
    
    Doing SHUTDOWN_soft_reset on Xen hypervisors which don't support it is
    probably OK as by default all unknown shutdown reasons cause domain
    destroy with a message in toolstack log: 'Unknown shutdown reason code
    5. Destroying domain.'  which gives a clue to what the problem is and
    eliminates false expectations.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1f1dbd2949a4..993b7a71386d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -33,6 +33,10 @@
 #include <linux/memblock.h>
 #include <linux/edd.h>
 
+#ifdef CONFIG_KEXEC_CORE
+#include <linux/kexec.h>
+#endif
+
 #include <xen/xen.h>
 #include <xen/events.h>
 #include <xen/interface/xen.h>
@@ -1808,6 +1812,21 @@ static struct notifier_block xen_hvm_cpu_notifier = {
 	.notifier_call	= xen_hvm_cpu_notify,
 };
 
+#ifdef CONFIG_KEXEC_CORE
+static void xen_hvm_shutdown(void)
+{
+	native_machine_shutdown();
+	if (kexec_in_progress)
+		xen_reboot(SHUTDOWN_soft_reset);
+}
+
+static void xen_hvm_crash_shutdown(struct pt_regs *regs)
+{
+	native_machine_crash_shutdown(regs);
+	xen_reboot(SHUTDOWN_soft_reset);
+}
+#endif
+
 static void __init xen_hvm_guest_init(void)
 {
 	if (xen_pv_domain())
@@ -1827,6 +1846,10 @@ static void __init xen_hvm_guest_init(void)
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();
 	xen_hvm_init_mmu_ops();
+#ifdef CONFIG_KEXEC_CORE
+	machine_ops.shutdown = xen_hvm_shutdown;
+	machine_ops.crash_shutdown = xen_hvm_crash_shutdown;
+#endif
 }
 #endif
 

commit 2ecf91b6d8b0ee8ef38aa7ea2a0fe0cd57b6ca50
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Sep 21 09:09:28 2015 -0400

    xen/x86: Don't try to write syscall-related MSRs for PV guests
    
    For PV guests these registers are set up by hypervisor and thus
    should not be written by the guest. The comment in xen_write_msr_safe()
    says so but we still write the MSRs, causing the hypervisor to
    print a warning.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 30d12afe52ed..1f1dbd2949a4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1077,6 +1077,7 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 		/* Fast syscall setup is all done in hypercalls, so
 		   these are all ignored.  Stub them out here to stop
 		   Xen console noise. */
+		break;
 
 	default:
 		if (!pmu_msr_write(msr, low, high, &ret))

commit 752240e74d650faa24425adc523f1308973ea51c
Merge: b8cb642af982 626d7508664c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 8 11:46:48 2015 -0700

    Merge tag 'for-linus-4.3-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
     "Xen features and fixes for 4.3:
    
       - Convert xen-blkfront to the multiqueue API
       - [arm] Support binding event channels to different VCPUs.
       - [x86] Support > 512 GiB in a PV guests (off by default as such a
         guest cannot be migrated with the current toolstack).
       - [x86] PMU support for PV dom0 (limited support for using perf with
         Xen and other guests)"
    
    * tag 'for-linus-4.3-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (33 commits)
      xen: switch extra memory accounting to use pfns
      xen: limit memory to architectural maximum
      xen: avoid another early crash of memory limited dom0
      xen: avoid early crash of memory limited dom0
      arm/xen: Remove helpers which are PV specific
      xen/x86: Don't try to set PCE bit in CR4
      xen/PMU: PMU emulation code
      xen/PMU: Intercept PMU-related MSR and APIC accesses
      xen/PMU: Describe vendor-specific PMU registers
      xen/PMU: Initialization code for Xen PMU
      xen/PMU: Sysfs interface for setting Xen PMU mode
      xen: xensyms support
      xen: remove no longer needed p2m.h
      xen: allow more than 512 GB of RAM for 64 bit pv-domains
      xen: move p2m list if conflicting with e820 map
      xen: add explicit memblock_reserve() calls for special pages
      mm: provide early_memremap_ro to establish read-only mapping
      xen: check for initrd conflicting with e820 map
      xen: check pre-allocated page tables for conflict with memory map
      xen: check for kernel memory conflicting with memory layout
      ...

commit 3375d8284dfb7866f261ec008d15d30999ff273b
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Aug 10 16:34:38 2015 -0400

    xen/x86: Don't try to set PCE bit in CR4
    
    Since VPMU code emulates RDPMC instruction with RDMSR and because hypervisor
    does not emulate it there is no reason to try setting CR4's PCE bit (and the
    hypervisor will warn on seeing it set).
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index fdaba49f6759..25309c168311 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1011,8 +1011,7 @@ static void xen_write_cr0(unsigned long cr0)
 
 static void xen_write_cr4(unsigned long cr4)
 {
-	cr4 &= ~X86_CR4_PGE;
-	cr4 &= ~X86_CR4_PSE;
+	cr4 &= ~(X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PCE);
 
 	native_write_cr4(cr4);
 }

commit 6b08cd6328c58a2ae190c5ee03a2ffcab5ef828e
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Aug 10 16:34:36 2015 -0400

    xen/PMU: Intercept PMU-related MSR and APIC accesses
    
    Provide interfaces for recognizing accesses to PMU-related MSRs and
    LVTPC APIC and process these accesses in Xen PMU code.
    
    (The interrupt handler performs XENPMU_flush right away in the beginning
    since no PMU emulation is available. It will be added with a later patch).
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 19072f91a8e2..fdaba49f6759 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1031,6 +1031,9 @@ static u64 xen_read_msr_safe(unsigned int msr, int *err)
 {
 	u64 val;
 
+	if (pmu_msr_read(msr, &val, err))
+		return val;
+
 	val = native_read_msr_safe(msr, err);
 	switch (msr) {
 	case MSR_IA32_APICBASE:
@@ -1077,17 +1080,13 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 		   Xen console noise. */
 
 	default:
-		ret = native_write_msr_safe(msr, low, high);
+		if (!pmu_msr_write(msr, low, high, &ret))
+			ret = native_write_msr_safe(msr, low, high);
 	}
 
 	return ret;
 }
 
-unsigned long long xen_read_pmc(int counter)
-{
-	return 0;
-}
-
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {

commit 65d0cf0be79feebeb19e7626fd3ed41ae73f642d
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Aug 10 16:34:34 2015 -0400

    xen/PMU: Initialization code for Xen PMU
    
    Map shared data structure that will hold CPU registers, VPMU context,
    V/PCPU IDs of the CPU interrupted by PMU interrupt. Hypervisor fills
    this information in its handler and passes it to the guest for further
    processing.
    
    Set up PMU VIRQ.
    
    Now that perf infrastructure will assume that PMU is available on a PV
    guest we need to be careful and make sure that accesses via RDPMC
    instruction don't cause fatal traps by the hypervisor. Provide a nop
    RDPMC handler.
    
    For the same reason avoid issuing a warning on a write to APIC's LVTPC.
    
    Both of these will be made functional in later patches.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 373dbc9810d1..19072f91a8e2 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -84,6 +84,7 @@
 #include "mmu.h"
 #include "smp.h"
 #include "multicalls.h"
+#include "pmu.h"
 
 EXPORT_SYMBOL_GPL(hypercall_page);
 
@@ -1082,6 +1083,11 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 	return ret;
 }
 
+unsigned long long xen_read_pmc(int counter)
+{
+	return 0;
+}
+
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
@@ -1216,7 +1222,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.write_msr = xen_write_msr_safe,
 
 	.read_tsc = native_read_tsc,
-	.read_pmc = native_read_pmc,
+	.read_pmc = xen_read_pmc,
 
 	.read_tscp = native_read_tscp,
 
@@ -1267,6 +1273,10 @@ static const struct pv_apic_ops xen_apic_ops __initconst = {
 static void xen_reboot(int reason)
 {
 	struct sched_shutdown r = { .reason = reason };
+	int cpu;
+
+	for_each_online_cpu(cpu)
+		xen_pmu_finish(cpu);
 
 	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
 		BUG();

commit 6c2681c863b24360098d1ba60f2af060a13a0561
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:34 2015 +0200

    xen: add explicit memblock_reserve() calls for special pages
    
    Some special pages containing interfaces to xen are being reserved
    implicitly only today. The memblock_reserve() call to reserve them is
    meant to reserve the p2m list supplied by xen. It is just reserving
    not only the p2m list itself, but some more pages up to the start of
    the xen built page tables.
    
    To be able to move the p2m list to another pfn range, which is needed
    for support of huge RAM, this memblock_reserve() must be split up to
    cover all affected reserved pages explicitly.
    
    The affected pages are:
    - start_info page
    - xenstore ring (might be missing, mfn is 0 in this case)
    - console ring (not for initial domain)
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 11d6fb4e8483..373dbc9810d1 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1610,7 +1610,9 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	early_boot_irqs_disabled = true;
 
 	xen_raw_console_write("mapping kernel into physical memory\n");
-	xen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base, xen_start_info->nr_pages);
+	xen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base,
+				   xen_start_info->nr_pages);
+	xen_reserve_special_pages();
 
 	/*
 	 * Modify the cache mode translation tables to match Xen's PAT

commit 5b929bd11df23922daf1be5d52731cc3900c1d79
Merge: b2c51106c758 37868fe113ff
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 31 10:23:35 2015 +0200

    Merge branch 'x86/urgent' into x86/asm, before applying dependent patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit aa1acff356bbedfd03b544051f5b371746735d89
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 30 14:31:31 2015 -0700

    x86/xen: Probe target addresses in set_aliased_prot() before the hypercall
    
    The update_va_mapping hypercall can fail if the VA isn't present
    in the guest's page tables.  Under certain loads, this can
    result in an OOPS when the target address is in unpopulated vmap
    space.
    
    While we're at it, add comments to help explain what's going on.
    
    This isn't a great long-term fix.  This code should probably be
    changed to use something like set_memory_ro.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Vrabel <dvrabel@cantab.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: security@kernel.org <security@kernel.org>
    Cc: <stable@vger.kernel.org>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/0b0e55b995cda11e7829f140b833ef932fcabe3a.1438291540.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0b95c9b8283f..11d6fb4e8483 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -483,6 +483,7 @@ static void set_aliased_prot(void *v, pgprot_t prot)
 	pte_t pte;
 	unsigned long pfn;
 	struct page *page;
+	unsigned char dummy;
 
 	ptep = lookup_address((unsigned long)v, &level);
 	BUG_ON(ptep == NULL);
@@ -492,6 +493,32 @@ static void set_aliased_prot(void *v, pgprot_t prot)
 
 	pte = pfn_pte(pfn, prot);
 
+	/*
+	 * Careful: update_va_mapping() will fail if the virtual address
+	 * we're poking isn't populated in the page tables.  We don't
+	 * need to worry about the direct map (that's always in the page
+	 * tables), but we need to be careful about vmap space.  In
+	 * particular, the top level page table can lazily propagate
+	 * entries between processes, so if we've switched mms since we
+	 * vmapped the target in the first place, we might not have the
+	 * top-level page table entry populated.
+	 *
+	 * We disable preemption because we want the same mm active when
+	 * we probe the target and when we issue the hypercall.  We'll
+	 * have the same nominal mm, but if we're a kernel thread, lazy
+	 * mm dropping could change our pgd.
+	 *
+	 * Out of an abundance of caution, this uses __get_user() to fault
+	 * in the target address just in case there's some obscure case
+	 * in which the target address isn't readable.
+	 */
+
+	preempt_disable();
+
+	pagefault_disable();	/* Avoid warnings due to being atomic. */
+	__get_user(dummy, (unsigned char __user __force *)v);
+	pagefault_enable();
+
 	if (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))
 		BUG();
 
@@ -503,6 +530,8 @@ static void set_aliased_prot(void *v, pgprot_t prot)
 				BUG();
 	} else
 		kmap_flush_unused();
+
+	preempt_enable();
 }
 
 static void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)
@@ -510,6 +539,17 @@ static void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)
 	const unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;
 	int i;
 
+	/*
+	 * We need to mark the all aliases of the LDT pages RO.  We
+	 * don't need to call vm_flush_aliases(), though, since that's
+	 * only responsible for flushing aliases out the TLBs, not the
+	 * page tables, and Xen will flush the TLB for us if needed.
+	 *
+	 * To avoid confusing future readers: none of this is necessary
+	 * to load the LDT.  The hypervisor only checks this when the
+	 * LDT is faulted in due to subsequent descriptor access.
+	 */
+
 	for(i = 0; i < entries; i += entries_per_page)
 		set_aliased_prot(ldt + i, PAGE_KERNEL_RO);
 }

commit 9261e050b686c9fe229cd9918d997b3caaf20e34
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:43:57 2015 +0200

    x86/asm/tsc, x86/paravirt: Remove read_tsc() and read_tscp() paravirt hooks
    
    We've had ->read_tsc() and ->read_tscp() paravirt hooks since
    the very beginning of paravirt, i.e.,
    
      d3561b7fa0fb ("[PATCH] paravirt: header and stubs for paravirtualisation").
    
    AFAICT, the only paravirt guest implementation that ever
    replaced these calls was vmware, and it's gone. Arguably even
    vmware shouldn't have hooked RDTSC -- we fully support systems
    that don't have a TSC at all, so there's no point for a paravirt
    implementation to pretend that we have a TSC but to replace it.
    
    I also doubt that these hooks actually worked. Calls to rdtscl()
    and rdtscll(), which respected the hooks, were used seemingly
    interchangeably with native_read_tsc(), which did not.
    
    Just remove them. If anyone ever needs them again, they can try
    to make a case for why they need them.
    
    Before, on a paravirt config:
      text          data     bss     dec     hex filename
      12618257      1816384 1093632 15528273 ecf151 vmlinux
    
    After:
      text          data     bss     dec     hex filename
      12617207      1816384 1093632 15527223 eced37 vmlinux
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/d08a2600fb298af163681e5efd8e599d889a5b97.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0b95c9b8283f..32136bfca43f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1175,11 +1175,8 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.read_msr = xen_read_msr_safe,
 	.write_msr = xen_write_msr_safe,
 
-	.read_tsc = native_read_tsc,
 	.read_pmc = native_read_pmc,
 
-	.read_tscp = native_read_tscp,
-
 	.iret = xen_iret,
 #ifdef CONFIG_X86_64
 	.usergs_sysret32 = xen_sysret32,

commit d70b3ef54ceaf1c7c92209f5a662a670d04cbed9
Merge: 650ec5a6bd5d 7ef3d7d58d9d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:59:09 2015 -0700

    Merge branch 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "There were so many changes in the x86/asm, x86/apic and x86/mm topics
      in this cycle that the topical separation of -tip broke down somewhat -
      so the result is a more traditional architecture pull request,
      collected into the 'x86/core' topic.
    
      The topics were still maintained separately as far as possible, so
      bisectability and conceptual separation should still be pretty good -
      but there were a handful of merge points to avoid excessive
      dependencies (and conflicts) that would have been poorly tested in the
      end.
    
      The next cycle will hopefully be much more quiet (or at least will
      have fewer dependencies).
    
      The main changes in this cycle were:
    
       * x86/apic changes, with related IRQ core changes: (Jiang Liu, Thomas
         Gleixner)
    
         - This is the second and most intrusive part of changes to the x86
           interrupt handling - full conversion to hierarchical interrupt
           domains:
    
              [IOAPIC domain]   -----
                                     |
              [MSI domain]      --------[Remapping domain] ----- [ Vector domain ]
                                     |   (optional)          |
              [HPET MSI domain] -----                        |
                                                             |
              [DMAR domain]     -----------------------------
                                                             |
              [Legacy domain]   -----------------------------
    
           This now reflects the actual hardware and allowed us to distangle
           the domain specific code from the underlying parent domain, which
           can be optional in the case of interrupt remapping.  It's a clear
           separation of functionality and removes quite some duct tape
           constructs which plugged the remap code between ioapic/msi/hpet
           and the vector management.
    
         - Intel IOMMU IRQ remapping enhancements, to allow direct interrupt
           injection into guests (Feng Wu)
    
       * x86/asm changes:
    
         - Tons of cleanups and small speedups, micro-optimizations.  This
           is in preparation to move a good chunk of the low level entry
           code from assembly to C code (Denys Vlasenko, Andy Lutomirski,
           Brian Gerst)
    
         - Moved all system entry related code to a new home under
           arch/x86/entry/ (Ingo Molnar)
    
         - Removal of the fragile and ugly CFI dwarf debuginfo annotations.
           Conversion to C will reintroduce many of them - but meanwhile
           they are only getting in the way, and the upstream kernel does
           not rely on them (Ingo Molnar)
    
         - NOP handling refinements. (Borislav Petkov)
    
       * x86/mm changes:
    
         - Big PAT and MTRR rework: making the code more robust and
           preparing to phase out exposing direct MTRR interfaces to drivers -
           in favor of using PAT driven interfaces (Toshi Kani, Luis R
           Rodriguez, Borislav Petkov)
    
         - New ioremap_wt()/set_memory_wt() interfaces to support
           Write-Through cached memory mappings.  This is especially
           important for good performance on NVDIMM hardware (Toshi Kani)
    
       * x86/ras changes:
    
         - Add support for deferred errors on AMD (Aravind Gopalakrishnan)
    
           This is an important RAS feature which adds hardware support for
           poisoned data.  That means roughly that the hardware marks data
           which it has detected as corrupted but wasn't able to correct, as
           poisoned data and raises an APIC interrupt to signal that in the
           form of a deferred error.  It is the OS's responsibility then to
           take proper recovery action and thus prolonge system lifetime as
           far as possible.
    
         - Add support for Intel "Local MCE"s: upcoming CPUs will support
           CPU-local MCE interrupts, as opposed to the traditional system-
           wide broadcasted MCE interrupts (Ashok Raj)
    
         - Misc cleanups (Borislav Petkov)
    
       * x86/platform changes:
    
         - Intel Atom SoC updates
    
      ... and lots of other cleanups, fixlets and other changes - see the
      shortlog and the Git log for details"
    
    * 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (222 commits)
      x86/hpet: Use proper hpet device number for MSI allocation
      x86/hpet: Check for irq==0 when allocating hpet MSI interrupts
      x86/mm/pat, drivers/infiniband/ipath: Use arch_phys_wc_add() and require PAT disabled
      x86/mm/pat, drivers/media/ivtv: Use arch_phys_wc_add() and require PAT disabled
      x86/platform/intel/baytrail: Add comments about why we disabled HPET on Baytrail
      genirq: Prevent crash in irq_move_irq()
      genirq: Enhance irq_data_to_desc() to support hierarchy irqdomain
      iommu, x86: Properly handle posted interrupts for IOMMU hotplug
      iommu, x86: Provide irq_remapping_cap() interface
      iommu, x86: Setup Posted-Interrupts capability for Intel iommu
      iommu, x86: Add cap_pi_support() to detect VT-d PI capability
      iommu, x86: Avoid migrating VT-d posted interrupts
      iommu, x86: Save the mode (posted or remapped) of an IRTE
      iommu, x86: Implement irq_set_vcpu_affinity for intel_ir_chip
      iommu: dmar: Provide helper to copy shared irte fields
      iommu: dmar: Extend struct irte for VT-d Posted-Interrupts
      iommu: Add new member capability to struct irq_remap_ops
      x86/asm/entry/64: Disentangle error_entry/exit gsbase/ebx/usermode code
      x86/asm/entry/32: Shorten __audit_syscall_entry() args preparation
      x86/asm/entry/32: Explain reloading of registers after __audit_syscall_entry()
      ...

commit 7ef3d7d58d9dc73ee3d4f8f56d0024c8cca8163f
Merge: cb17b2a674f2 539f51136500 7ea402d01cb6 b58d93075013
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 22 09:15:03 2015 +0200

    Merge branches 'x86/apic', 'x86/asm', 'x86/mm' and 'x86/platform' into x86/core, to merge last updates
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9cd25aac1f44f269de5ecea11f7d927f37f1d01c
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jun 4 18:55:10 2015 +0200

    x86/mm/pat: Emulate PAT when it is disabled
    
    In the case when PAT is disabled on the command line with
    "nopat" or when virtualization doesn't support PAT (correctly) -
    see
    
      9d34cfdf4796 ("x86: Don't rely on VMWare emulating PAT MSR correctly").
    
    we emulate it using the PWT and PCD cache attribute bits. Get
    rid of boot_pat_state while at it.
    
    Based on a conglomerate patch from Toshi Kani.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Elliott@hp.com
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arnd@arndb.de
    Cc: hch@lst.de
    Cc: hmh@hmh.eng.br
    Cc: konrad.wilk@oracle.com
    Cc: linux-mm <linux-mm@kvack.org>
    Cc: linux-nvdimm@lists.01.org
    Cc: stefan.bader@canonical.com
    Cc: yigal@plexistor.com
    Link: http://lkml.kernel.org/r/1433436928-31903-3-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 46957ead3060..53233a9beea9 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1467,6 +1467,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 {
 	struct physdev_set_iopl set_iopl;
 	unsigned long initrd_start = 0;
+	u64 pat;
 	int rc;
 
 	if (!xen_start_info)
@@ -1574,8 +1575,8 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	 * Modify the cache mode translation tables to match Xen's PAT
 	 * configuration.
 	 */
-
-	pat_init_cache_modes();
+	rdmsrl(MSR_IA32_CR_PAT, pat);
+	pat_init_cache_modes(pat);
 
 	/* keep using Xen gdt for now; no urgent need to change it */
 

commit 21c4cd108a1b144ad645355bfee1f8be937f03a2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 14:27:17 2015 +0200

    x86/fpu: Simplify fpu__cpu_init()
    
    After the latest round of cleanups, fpu__cpu_init() has become
    a simple call to fpu__init_cpu().
    
    Rename fpu__init_cpu() to fpu__cpu_init() and remove the
    extra layer.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 43f8704b7289..98088bf5906a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1423,7 +1423,7 @@ static void xen_pvh_set_cr_flags(int cpu)
 		return;
 	/*
 	 * For BSP, PSE PGE are set in probe_page_size_mask(), for APs
-	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu__cpu_init().
+	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu__init_cpu().
 	*/
 	if (cpu_has_pse)
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);

commit 3a9c4b0d7e1a693beeac24d749f6938444148e86
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 13:16:51 2015 +0200

    x86/fpu: Rename fpu_init() to fpu__cpu_init()
    
    fpu_init() is a bit of a misnomer in that it (falsely) creates the
    impression that it's related to the (old) fpu_finit() function,
    which initializes FPU ctx state.
    
    Rename it to fpu__cpu_init() to make its boot time initialization
    clear, and to move it to the fpu__*() namespace.
    
    Also fix and extend its comment block to point out that it's
    called not only on the boot CPU, but on secondary CPUs as well.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 46957ead3060..43f8704b7289 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1423,7 +1423,7 @@ static void xen_pvh_set_cr_flags(int cpu)
 		return;
 	/*
 	 * For BSP, PSE PGE are set in probe_page_size_mask(), for APs
-	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu_init.
+	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu__cpu_init().
 	*/
 	if (cpu_has_pse)
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);

commit 7ae383be81781c5e1347f71c3eb0d53ce5188200
Merge: 2a4e90b18c25 3e0283a53f7d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri May 8 13:33:33 2015 +0200

    Merge branch 'linus' into x86/asm, before applying dependent patch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a71dbdaa8ca2933391b08e0ae5567083e3af0892
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon May 4 11:02:15 2015 -0400

    hypervisor/x86/xen: Unset X86_BUG_SYSRET_SS_ATTRS on Xen PV guests
    
    Commit 61f01dd941ba ("x86_64, asm: Work around AMD SYSRET SS descriptor
    attribute issue") makes AMD processors set SS to __KERNEL_DS in
    __switch_to() to deal with cases when SS is NULL.
    
    This breaks Xen PV guests who do not want to load SS with__KERNEL_DS.
    
    Since the problem that the commit is trying to address would have to be
    fixed in the hypervisor (if it in fact exists under Xen) there is no
    reason to set X86_BUG_SYSRET_SS_ATTRS flag for PV VPCUs here.
    
    This can be easily achieved by adding x86_hyper_xen_hvm.set_cpu_features
    op which will clear this flag. (And since this structure is no longer
    HVM-specific we should do some renaming).
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reported-by: Sander Eikelenboom <linux@eikelenboom.it>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 94578efd3067..46957ead3060 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1760,6 +1760,9 @@ static struct notifier_block xen_hvm_cpu_notifier = {
 
 static void __init xen_hvm_guest_init(void)
 {
+	if (xen_pv_domain())
+		return;
+
 	init_hvm_pv_info();
 
 	xen_hvm_init_shared_info();
@@ -1775,6 +1778,7 @@ static void __init xen_hvm_guest_init(void)
 	xen_hvm_init_time_ops();
 	xen_hvm_init_mmu_ops();
 }
+#endif
 
 static bool xen_nopv = false;
 static __init int xen_parse_nopv(char *arg)
@@ -1784,14 +1788,11 @@ static __init int xen_parse_nopv(char *arg)
 }
 early_param("xen_nopv", xen_parse_nopv);
 
-static uint32_t __init xen_hvm_platform(void)
+static uint32_t __init xen_platform(void)
 {
 	if (xen_nopv)
 		return 0;
 
-	if (xen_pv_domain())
-		return 0;
-
 	return xen_cpuid_base();
 }
 
@@ -1809,11 +1810,19 @@ bool xen_hvm_need_lapic(void)
 }
 EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);
 
-const struct hypervisor_x86 x86_hyper_xen_hvm __refconst = {
-	.name			= "Xen HVM",
-	.detect			= xen_hvm_platform,
+static void xen_set_cpu_features(struct cpuinfo_x86 *c)
+{
+	if (xen_pv_domain())
+		clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
+}
+
+const struct hypervisor_x86 x86_hyper_xen = {
+	.name			= "Xen",
+	.detect			= xen_platform,
+#ifdef CONFIG_XEN_PVHVM
 	.init_platform		= xen_hvm_guest_init,
+#endif
 	.x2apic_available	= xen_x2apic_para_available,
+	.set_cpu_features       = xen_set_cpu_features,
 };
-EXPORT_SYMBOL(x86_hyper_xen_hvm);
-#endif
+EXPORT_SYMBOL(x86_hyper_xen);

commit aac82d319148c6a84e1bf90b86d3e0ec8bf0ee38
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Apr 3 15:51:54 2015 -0700

    x86, paravirt, xen: Remove the 64-bit ->irq_enable_sysexit() pvop
    
    We don't use irq_enable_sysexit on 64-bit kernels any more.
    Remove all the paravirt and Xen machinery to support it on
    64-bit kernels.
    
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Denys Vlasenko <vda.linux@googlemail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8a03355698fe5b94194e9e7360f19f91c1b2cf1f.1428100853.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 81665c9f2132..3797b6b31f95 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1267,10 +1267,11 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.read_tscp = native_read_tscp,
 
 	.iret = xen_iret,
-	.irq_enable_sysexit = xen_sysexit,
 #ifdef CONFIG_X86_64
 	.usergs_sysret32 = xen_sysret32,
 	.usergs_sysret64 = xen_sysret64,
+#else
+	.irq_enable_sysexit = xen_sysexit,
 #endif
 
 	.load_tr_desc = paravirt_nop,

commit 497a5df7bf6ffd136ae21c49d1a01292930d7ca2
Merge: 714d8e7e2719 0b97b03d88b4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 16 14:01:03 2015 -0500

    Merge tag 'stable/for-linus-4.1-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen features and fixes from David Vrabel:
    
     - use a single source list of hypercalls, generating other tables etc.
       at build time.
    
     - add a "Xen PV" APIC driver to support >255 VCPUs in PV guests.
    
     - significant performance improve to guest save/restore/migration.
    
     - scsiback/front save/restore support.
    
     - infrastructure for multi-page xenbus rings.
    
     - misc fixes.
    
    * tag 'stable/for-linus-4.1-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/pci: Try harder to get PXM information for Xen
      xenbus_client: Extend interface to support multi-page ring
      xen-pciback: also support disabling of bus-mastering and memory-write-invalidate
      xen: support suspend/resume in pvscsi frontend
      xen: scsiback: add LUN of restored domain
      xen-scsiback: define a pr_fmt macro with xen-pvscsi
      xen/mce: fix up xen_late_init_mcelog() error handling
      xen/privcmd: improve performance of MMAPBATCH_V2
      xen: unify foreign GFN map/unmap for auto-xlated physmap guests
      x86/xen/apic: WARN with details.
      x86/xen: Provide a "Xen PV" APIC driver to support >255 VCPUs
      xen/pciback: Don't print scary messages when unsupported by hypervisor.
      xen: use generated hypercall symbols in arch/x86/xen/xen-head.S
      xen: use generated hypervisor symbols in arch/x86/xen/trace.c
      xen: synchronize include/xen/interface/xen.h with xen
      xen: build infrastructure for generating hypercall depending symbols
      xen: balloon: Use static attribute groups for sysfs entries
      xen: pcpu: Use static attribute groups for sysfs entry

commit feb44f1f7a4ac299d1ab1c3606860e70b9b89d69
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Mar 2 12:06:23 2015 -0500

    x86/xen: Provide a "Xen PV" APIC driver to support >255 VCPUs
    
    Instead of mangling the default APIC driver, provide a Xen PV guest
    specific one that explicitly provides appropriate methods.
    
    This allows use to report that all APIC IDs are valid, allowing dom0
    to boot with more than 255 VCPUs.
    
    Since the probe order of APIC drivers is link dependent, we add in an
    late probe function to change to the Xen PV if it hadn't been done
    during bootup.
    
    Suggested-by: David Vrabel <david.vrabel@citrix.com>
    Reported-by: Cathy Avery <cathy.avery@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5240f563076d..b9a227284149 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -927,92 +927,6 @@ static void xen_io_delay(void)
 {
 }
 
-#ifdef CONFIG_X86_LOCAL_APIC
-static unsigned long xen_set_apic_id(unsigned int x)
-{
-	WARN_ON(1);
-	return x;
-}
-static unsigned int xen_get_apic_id(unsigned long x)
-{
-	return ((x)>>24) & 0xFFu;
-}
-static u32 xen_apic_read(u32 reg)
-{
-	struct xen_platform_op op = {
-		.cmd = XENPF_get_cpuinfo,
-		.interface_version = XENPF_INTERFACE_VERSION,
-		.u.pcpu_info.xen_cpuid = 0,
-	};
-	int ret = 0;
-
-	/* Shouldn't need this as APIC is turned off for PV, and we only
-	 * get called on the bootup processor. But just in case. */
-	if (!xen_initial_domain() || smp_processor_id())
-		return 0;
-
-	if (reg == APIC_LVR)
-		return 0x10;
-
-	if (reg != APIC_ID)
-		return 0;
-
-	ret = HYPERVISOR_dom0_op(&op);
-	if (ret)
-		return 0;
-
-	return op.u.pcpu_info.apic_id << 24;
-}
-
-static void xen_apic_write(u32 reg, u32 val)
-{
-	/* Warn to see if there's any stray references */
-	WARN_ON(1);
-}
-
-static u64 xen_apic_icr_read(void)
-{
-	return 0;
-}
-
-static void xen_apic_icr_write(u32 low, u32 id)
-{
-	/* Warn to see if there's any stray references */
-	WARN_ON(1);
-}
-
-static void xen_apic_wait_icr_idle(void)
-{
-        return;
-}
-
-static u32 xen_safe_apic_wait_icr_idle(void)
-{
-        return 0;
-}
-
-static void set_xen_basic_apic_ops(void)
-{
-	apic->read = xen_apic_read;
-	apic->write = xen_apic_write;
-	apic->icr_read = xen_apic_icr_read;
-	apic->icr_write = xen_apic_icr_write;
-	apic->wait_icr_idle = xen_apic_wait_icr_idle;
-	apic->safe_wait_icr_idle = xen_safe_apic_wait_icr_idle;
-	apic->set_apic_id = xen_set_apic_id;
-	apic->get_apic_id = xen_get_apic_id;
-
-#ifdef CONFIG_SMP
-	apic->send_IPI_allbutself = xen_send_IPI_allbutself;
-	apic->send_IPI_mask_allbutself = xen_send_IPI_mask_allbutself;
-	apic->send_IPI_mask = xen_send_IPI_mask;
-	apic->send_IPI_all = xen_send_IPI_all;
-	apic->send_IPI_self = xen_send_IPI_self;
-#endif
-}
-
-#endif
-
 static void xen_clts(void)
 {
 	struct multicall_space mcs;
@@ -1618,7 +1532,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	/*
 	 * set up the basic apic ops.
 	 */
-	set_xen_basic_apic_ops();
+	xen_init_apic();
 #endif
 
 	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {
@@ -1731,8 +1645,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 		if (HYPERVISOR_dom0_op(&op) == 0)
 			boot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;
 
-		xen_init_apic();
-
 		/* Make sure ACS will be enabled */
 		pci_request_acs();
 

commit 8ef46a672a7d852709561d10672b6eaa8a4acd82
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:02 2015 -0800

    x86/asm/entry: Add this_cpu_sp0() to read sp0 for the current cpu
    
    We currently store references to the top of the kernel stack in
    multiple places: kernel_stack (with an offset) and
    init_tss.x86_tss.sp0 (no offset).  The latter is defined by
    hardware and is a clean canonical way to find the top of the
    stack.  Add an accessor so we can start using it.
    
    This needs minor paravirt tweaks.  On native, sp0 defines the
    top of the kernel stack and is therefore always correct.  On Xen
    and lguest, the hypervisor tracks the top of the stack, but we
    want to start reading sp0 in the kernel.  Fixing this is simple:
    just update our local copy of sp0 as well as the hypervisor's
    copy on task switches.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8d675581859712bee09a055ed8f785d80dac1eca.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5240f563076d..81665c9f2132 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -912,6 +912,7 @@ static void xen_load_sp0(struct tss_struct *tss,
 	mcs = xen_mc_entry(0);
 	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread->sp0);
 	xen_mc_issue(PARAVIRT_LAZY_CPU);
+	tss->x86_tss.sp0 = thread->sp0;
 }
 
 static void xen_set_iopl_mask(unsigned mask)

commit 5054daa285beaf706f051fbd395dc36c9f0f907f
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Feb 23 11:01:00 2015 -0500

    x86/xen: Initialize cr4 shadow for 64-bit PV(H) guests
    
    Commit 1e02ce4cccdc ("x86: Store a per-cpu shadow copy of CR4")
    introduced CR4 shadows.
    
    These shadows are initialized in early boot code. The commit missed
    initialization for 64-bit PV(H) guests that this patch adds.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index efee14db009b..5240f563076d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1758,6 +1758,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 #ifdef CONFIG_X86_32
 	i386_start_kernel();
 #else
+	cr4_init_shadow(); /* 32b kernel does this in i386_start_kernel() */
 	x86_64_start_reservations((char *)__pa_symbol(&boot_params));
 #endif
 }

commit 31795b470b0872b66f7fa26f791b695c79821220
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Wed Feb 11 14:39:18 2015 -0500

    x86/xen: Make sure X2APIC_ENABLE bit of MSR_IA32_APICBASE is not set
    
    Commit d524165cb8db ("x86/apic: Check x2apic early") tests X2APIC_ENABLE
    bit of MSR_IA32_APICBASE when CONFIG_X86_X2APIC is off and panics
    the kernel when this bit is set.
    
    Xen's PV guests will pass this MSR read to the hypervisor which will
    return its version of the MSR, where this bit might be set. Make sure
    we clear it before returning MSR value to the caller.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bd8b8459c3d0..efee14db009b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1070,6 +1070,23 @@ static inline void xen_write_cr8(unsigned long val)
 	BUG_ON(val);
 }
 #endif
+
+static u64 xen_read_msr_safe(unsigned int msr, int *err)
+{
+	u64 val;
+
+	val = native_read_msr_safe(msr, err);
+	switch (msr) {
+	case MSR_IA32_APICBASE:
+#ifdef CONFIG_X86_X2APIC
+		if (!(cpuid_ecx(1) & (1 << (X86_FEATURE_X2APIC & 31))))
+#endif
+			val &= ~X2APIC_ENABLE;
+		break;
+	}
+	return val;
+}
+
 static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 {
 	int ret;
@@ -1240,7 +1257,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 
 	.wbinvd = native_wbinvd,
 
-	.read_msr = native_read_msr_safe,
+	.read_msr = xen_read_msr_safe,
 	.write_msr = xen_write_msr_safe,
 
 	.read_tsc = native_read_tsc,

commit 375074cc736ab1d89a708c0a8d7baa4a70d5d476
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:07 2014 -0700

    x86: Clean up cr4 manipulation
    
    CR4 manipulation was split, seemingly at random, between direct
    (write_cr4) and using a helper (set/clear_in_cr4).  Unfortunately,
    the set_in_cr4 and clear_in_cr4 helpers also poke at the boot code,
    which only a small subset of users actually wanted.
    
    This patch replaces all cr4 access in functions that don't leave cr4
    exactly the way they found it with new helpers cr4_set_bits,
    cr4_clear_bits, and cr4_set_bits_and_update_boot.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/495a10bdc9e67016b8fd3945700d46cfd5c12c2f.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 78a881b7fc41..bd8b8459c3d0 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1494,10 +1494,10 @@ static void xen_pvh_set_cr_flags(int cpu)
 	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu_init.
 	*/
 	if (cpu_has_pse)
-		set_in_cr4(X86_CR4_PSE);
+		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
 	if (cpu_has_pge)
-		set_in_cr4(X86_CR4_PGE);
+		cr4_set_bits_and_update_boot(X86_CR4_PGE);
 }
 
 /*

commit 613d4cefbbb39d3c37f3f22150263900ba11a3e3
Merge: 4f7a42deff8b f221b04fe07e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 14 08:07:42 2015 +1300

    Merge tag 'stable/for-linus-3.19-rc4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen bug fixes from David Vrabel:
     "Several critical linear p2m fixes that prevented some hosts from
      booting"
    
    * tag 'stable/for-linus-3.19-rc4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      x86/xen: properly retrieve NMI reason
      xen: check for zero sized area when invalidating memory
      xen: use correct type for physical addresses
      xen: correct race in alloc_p2m_pmd()
      xen: correct error for building p2m list on 32 bits
      x86/xen: avoid freeing static 'name' when kasprintf() fails
      x86/xen: add extra memory for remapped frames during setup
      x86/xen: don't count how many PFNs are identity mapped
      x86/xen: Free bootmem in free_p2m_page() during early boot
      x86/xen: Remove unnecessary BUG_ON(preemptible()) in xen_setup_timer()

commit f221b04fe07eb56c39935e31bb8e9ddacc00612f
Author: Jan Beulich <JBeulich@suse.com>
Date:   Tue Jan 13 07:40:05 2015 +0000

    x86/xen: properly retrieve NMI reason
    
    Using the native code here can't work properly, as the hypervisor would
    normally have cleared the two reason bits by the time Dom0 gets to see
    the NMI (if passed to it at all). There's a shared info field for this,
    and there's an existing hook to use - just fit the two together. This
    is particularly relevant so that NMIs intended to be handled by APEI /
    GHES actually make it to the respective handler.
    
    Note that the hook can (and should) be used irrespective of whether
    being in Dom0, as accessing port 0x61 in a DomU would be even worse,
    while the shared info field would just hold zero all the time. Note
    further that hardware NMI handling for PVH doesn't currently work
    anyway due to missing code in the hypervisor (but it is expected to
    work the native rather than the PV way).
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index fac5e4f9607c..115016347806 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -40,6 +40,7 @@
 #include <xen/interface/physdev.h>
 #include <xen/interface/vcpu.h>
 #include <xen/interface/memory.h>
+#include <xen/interface/nmi.h>
 #include <xen/interface/xen-mca.h>
 #include <xen/features.h>
 #include <xen/page.h>
@@ -66,6 +67,7 @@
 #include <asm/reboot.h>
 #include <asm/stackprotector.h>
 #include <asm/hypervisor.h>
+#include <asm/mach_traps.h>
 #include <asm/mwait.h>
 #include <asm/pci_x86.h>
 #include <asm/pat.h>
@@ -1357,6 +1359,21 @@ static const struct machine_ops xen_machine_ops __initconst = {
 	.emergency_restart = xen_emergency_restart,
 };
 
+static unsigned char xen_get_nmi_reason(void)
+{
+	unsigned char reason = 0;
+
+	/* Construct a value which looks like it came from port 0x61. */
+	if (test_bit(_XEN_NMIREASON_io_error,
+		     &HYPERVISOR_shared_info->arch.nmi_reason))
+		reason |= NMI_REASON_IOCHK;
+	if (test_bit(_XEN_NMIREASON_pci_serr,
+		     &HYPERVISOR_shared_info->arch.nmi_reason))
+		reason |= NMI_REASON_SERR;
+
+	return reason;
+}
+
 static void __init xen_boot_params_init_edd(void)
 {
 #if IS_ENABLED(CONFIG_EDD)
@@ -1541,9 +1558,12 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;
 	pv_apic_ops = xen_apic_ops;
-	if (!xen_pvh_domain())
+	if (!xen_pvh_domain()) {
 		pv_cpu_ops = xen_cpu_ops;
 
+		x86_platform.get_nmi_reason = xen_get_nmi_reason;
+	}
+
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		x86_init.resources.memory_setup = xen_auto_xlated_memory_setup;
 	else

commit 47591df505129c9774af6cca2debf283a6e56ed7
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Nov 3 14:02:04 2014 +0100

    xen: Support Xen pv-domains using PAT
    
    With the dynamical mapping between cache modes and pgprot values it is
    now possible to use all cache modes via the Xen hypervisor PAT settings
    in a pv domain.
    
    All to be done is to read the PAT configuration MSR and set up the
    translation tables accordingly.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stefan.bader@canonical.com
    Cc: xen-devel@lists.xensource.com
    Cc: ville.syrjala@linux.intel.com
    Cc: jbeulich@suse.com
    Cc: toshi.kani@hp.com
    Cc: plagnioj@jcrosoft.com
    Cc: tomi.valkeinen@ti.com
    Cc: bhelgaas@google.com
    Link: http://lkml.kernel.org/r/1415019724-4317-19-git-send-email-jgross@suse.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index fac5e4f9607c..6bf3a13e3e0f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1100,12 +1100,6 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 		/* Fast syscall setup is all done in hypercalls, so
 		   these are all ignored.  Stub them out here to stop
 		   Xen console noise. */
-		break;
-
-	case MSR_IA32_CR_PAT:
-		if (smp_processor_id() == 0)
-			xen_set_pat(((u64)high << 32) | low);
-		break;
 
 	default:
 		ret = native_write_msr_safe(msr, low, high);
@@ -1561,10 +1555,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 
 	/* Prevent unwanted bits from being set in PTEs. */
 	__supported_pte_mask &= ~_PAGE_GLOBAL;
-#if 0
-	if (!xen_initial_domain())
-#endif
-		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
 
 	/*
 	 * Prevent page tables from being allocated in highmem, even
@@ -1617,14 +1607,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	 * be meaningless.  Prevent it from trying.
 	 */
 	acpi_numa = -1;
-#endif
-#ifdef CONFIG_X86_PAT
-	/*
-	 * For right now disable the PAT. We should remove this once
-	 * git commit 8eaffa67b43e99ae581622c5133e20b0f48bcef1
-	 * (xen/pat: Disable PAT support for now) is reverted.
-	 */
-	pat_enabled = 0;
 #endif
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
@@ -1636,6 +1618,13 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	xen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base, xen_start_info->nr_pages);
 
+	/*
+	 * Modify the cache mode translation tables to match Xen's PAT
+	 * configuration.
+	 */
+
+	pat_init_cache_modes();
+
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 #ifdef CONFIG_X86_32

commit 2c185687ab016954557aac80074f5d7f7f5d275c
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Oct 14 13:33:46 2014 +0200

    x86/xen: delay construction of mfn_list_list
    
    The 3 level p2m tree for the Xen tools is constructed very early at
    boot by calling xen_build_mfn_list_list(). Memory needed for this tree
    is allocated via extend_brk().
    
    As this tree (other than the kernel internal p2m tree) is only needed
    for domain save/restore, live migration and crash dump analysis it
    doesn't matter whether it is constructed very early or just some
    milliseconds later when memory allocation is possible by other means.
    
    This patch moves the call of xen_build_mfn_list_list() just after
    calling xen_pagetable_p2m_copy() simplifying this function, too, as it
    doesn't have to bother with two parallel trees now. The same applies
    for some other internal functions.
    
    While simplifying code, make early_can_reuse_p2m_middle() static and
    drop the unused second parameter. p2m_mid_identity_mfn can be removed
    as well, it isn't used either.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1a3f0445432a..fac5e4f9607c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1636,9 +1636,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	xen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base, xen_start_info->nr_pages);
 
-	/* Allocate and initialize top and mid mfn levels for p2m structure */
-	xen_build_mfn_list_list();
-
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 #ifdef CONFIG_X86_32

commit 0429fbc0bdc297d64188483ba029a23773ae07b0
Merge: 6929c358972f 513d1a2884a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 15 07:48:18 2014 +0200

    Merge branch 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    Pull percpu consistent-ops changes from Tejun Heo:
     "Way back, before the current percpu allocator was implemented, static
      and dynamic percpu memory areas were allocated and handled separately
      and had their own accessors.  The distinction has been gone for many
      years now; however, the now duplicate two sets of accessors remained
      with the pointer based ones - this_cpu_*() - evolving various other
      operations over time.  During the process, we also accumulated other
      inconsistent operations.
    
      This pull request contains Christoph's patches to clean up the
      duplicate accessor situation.  __get_cpu_var() uses are replaced with
      with this_cpu_ptr() and __this_cpu_ptr() with raw_cpu_ptr().
    
      Unfortunately, the former sometimes is tricky thanks to C being a bit
      messy with the distinction between lvalues and pointers, which led to
      a rather ugly solution for cpumask_var_t involving the introduction of
      this_cpu_cpumask_var_ptr().
    
      This converts most of the uses but not all.  Christoph will follow up
      with the remaining conversions in this merge window and hopefully
      remove the obsolete accessors"
    
    * 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (38 commits)
      irqchip: Properly fetch the per cpu offset
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t -fix
      ia64: sn_nodepda cannot be assigned to after this_cpu conversion. Use __this_cpu_write.
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t
      Revert "powerpc: Replace __get_cpu_var uses"
      percpu: Remove __this_cpu_ptr
      clocksource: Replace __this_cpu_ptr with raw_cpu_ptr
      sparc: Replace __get_cpu_var uses
      avr32: Replace __get_cpu_var with __this_cpu_write
      blackfin: Replace __get_cpu_var uses
      tile: Use this_cpu_ptr() for hardware counters
      tile: Replace __get_cpu_var uses
      powerpc: Replace __get_cpu_var uses
      alpha: Replace __get_cpu_var
      ia64: Replace __get_cpu_var uses
      s390: cio driver &__get_cpu_var replacements
      s390: Replace __get_cpu_var uses
      mips: Replace __get_cpu_var uses
      MIPS: Replace __get_cpu_var uses in FPU emulator.
      arm: Replace __this_cpu_ptr with raw_cpu_ptr
      ...

commit a2ef5dc2c7cbedbeb4c847039845afaea5e63745
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Wed Sep 10 16:36:06 2014 -0700

    x86/xen: Set EFER.NX and EFER.SCE in PVH guests
    
    This fixes two bugs in PVH guests:
    
      - Not setting EFER.NX means the NX bit in page table entries is
        ignored on Intel processors and causes reserved bit page faults on
        AMD processors.
    
      - After the Xen commit 7645640d6ff1 ("x86/PVH: don't set EFER_SCE for
        pvh guest") PVH guests are required to set EFER.SCE to enable the
        SYSCALL instruction.
    
    Secondary VCPUs are started with pagetables with the NX bit set so
    EFER.NX must be set before using any stack or data segment.
    xen_pvh_cpu_early_init() is the new secondary VCPU entry point that
    sets EFER before jumping to cpu_bringup_and_idle().
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f7e01fa1545f..acb0effd8077 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1463,6 +1463,7 @@ static void __ref xen_setup_gdt(int cpu)
 	pv_cpu_ops.load_gdt = xen_load_gdt;
 }
 
+#ifdef CONFIG_XEN_PVH
 /*
  * A PV guest starts with default flags that are not set for PVH, set them
  * here asap.
@@ -1508,12 +1509,15 @@ static void __init xen_pvh_early_guest_init(void)
 		return;
 
 	xen_have_vector_callback = 1;
+
+	xen_pvh_early_cpu_init(0, false);
 	xen_pvh_set_cr_flags(0);
 
 #ifdef CONFIG_X86_32
 	BUG(); /* PVH: Implement proper support. */
 #endif
 }
+#endif    /* CONFIG_XEN_PVH */
 
 /* First C function to be called on Xen boot */
 asmlinkage __visible void __init xen_start_kernel(void)
@@ -1528,7 +1532,9 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	xen_domain_type = XEN_PV_DOMAIN;
 
 	xen_setup_features();
+#ifdef CONFIG_XEN_PVH
 	xen_pvh_early_guest_init();
+#endif
 	xen_setup_machphys_mapping();
 
 	/* Install Xen paravirt ops */

commit d1e9abd630fe66046087f6501a4e4b8de55f7ab9
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Sep 17 06:12:36 2014 +0200

    xen: eliminate scalability issues from initrd handling
    
    Size restrictions native kernels wouldn't have resulted from the initrd
    getting mapped into the initial mapping. The kernel doesn't really need
    the initrd to be mapped, so use infrastructure available in Xen to avoid
    the mapping and hence the restriction.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index be1cff572ccf..f7e01fa1545f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1519,6 +1519,7 @@ static void __init xen_pvh_early_guest_init(void)
 asmlinkage __visible void __init xen_start_kernel(void)
 {
 	struct physdev_set_iopl set_iopl;
+	unsigned long initrd_start = 0;
 	int rc;
 
 	if (!xen_start_info)
@@ -1665,10 +1666,16 @@ asmlinkage __visible void __init xen_start_kernel(void)
 	new_cpu_data.x86_capability[0] = cpuid_edx(1);
 #endif
 
+	if (xen_start_info->mod_start) {
+	    if (xen_start_info->flags & SIF_MOD_START_PFN)
+		initrd_start = PFN_PHYS(xen_start_info->mod_start);
+	    else
+		initrd_start = __pa(xen_start_info->mod_start);
+	}
+
 	/* Poke various useful things into boot_params */
 	boot_params.hdr.type_of_loader = (9 << 4) | 0;
-	boot_params.hdr.ramdisk_image = xen_start_info->mod_start
-		? __pa(xen_start_info->mod_start) : 0;
+	boot_params.hdr.ramdisk_image = initrd_start;
 	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
 	boot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);
 

commit f955371ca9d3986bca100666041fcfa9b6d21962
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Tue Jan 7 17:03:06 2014 +0000

    x86: remove the Xen-specific _PAGE_IOMAP PTE flag
    
    The _PAGE_IO_MAP PTE flag was only used by Xen PV guests to mark PTEs
    that were used to map I/O regions that are 1:1 in the p2m.  This
    allowed Xen to obtain the correct PFN when converting the MFNs read
    from a PTE back to their PFN.
    
    Xen guests no longer use _PAGE_IOMAP for this. Instead mfn_to_pfn()
    returns the correct PFN by using a combination of the m2p and p2m to
    determine if an MFN corresponds to a 1:1 mapping in the the p2m.
    
    Remove _PAGE_IOMAP, replacing it with _PAGE_UNUSED2 to allow for
    future uses of the PTE flag.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c0cb11fb5008..be1cff572ccf 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1559,8 +1559,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 #endif
 		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
 
-	__supported_pte_mask |= _PAGE_IOMAP;
-
 	/*
 	 * Prevent page tables from being allocated in highmem, even
 	 * if CONFIG_HIGHPTE is enabled.

commit 89cbc76768c2fa4ed95545bf961f3a14ddfeed21
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:40 2014 -0500

    x86: Replace __get_cpu_var uses
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    #define __get_cpu_var(var) (*this_cpu_ptr(&(var)))
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c0cb11fb5008..2628ee556756 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -821,7 +821,7 @@ static void xen_convert_trap_info(const struct desc_ptr *desc,
 
 void xen_copy_trap_info(struct trap_info *traps)
 {
-	const struct desc_ptr *desc = &__get_cpu_var(idt_desc);
+	const struct desc_ptr *desc = this_cpu_ptr(&idt_desc);
 
 	xen_convert_trap_info(desc, traps);
 }
@@ -838,7 +838,7 @@ static void xen_load_idt(const struct desc_ptr *desc)
 
 	spin_lock(&lock);
 
-	__get_cpu_var(idt_desc) = *desc;
+	memcpy(this_cpu_ptr(&idt_desc), desc, sizeof(idt_desc));
 
 	xen_convert_trap_info(desc, traps);
 

commit e306e3be1cbe5b11d0f8a53a557c205cf27e4979
Merge: 7f0d32e0c1a7 8b63c998b31c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 7 11:33:15 2014 -0700

    Merge tag 'stable/for-linus-3.17-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen updates from David Vrabel:
     - remove unused V2 grant table support
     - note that Konrad is xen-blkkback/front maintainer
     - add 'xen_nopv' option to disable PV extentions for x86 HVM guests
     - misc minor cleanups
    
    * tag 'stable/for-linus-3.17-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen-pciback: Document the 'quirks' sysfs file
      xen/pciback: Fix error return code in xen_pcibk_attach()
      xen/events: drop negativity check of unsigned parameter
      xen/setup: Remove Identity Map Debug Message
      xen/events/fifo: remove a unecessary use of BM()
      xen/events/fifo: ensure all bitops are properly aligned even on x86
      xen/events/fifo: reset control block and local HEADs on resume
      xen/arm: use BUG_ON
      xen/grant-table: remove support for V2 tables
      x86/xen: safely map and unmap grant frames when in atomic context
      MAINTAINERS: Make me the Xen block subsystem (front and back) maintainer
      xen: Introduce 'xen_nopv' to disable PV extensions for HVM guests.

commit 76f09aa464a1913efd596dd0edbf88f932fde08c
Merge: e9c9eecabaa8 99a5603e2a1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 17:13:50 2014 -0700

    Merge branch 'x86-efi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull EFI changes from Ingo Molnar:
     "Main changes in this cycle are:
    
       - arm64 efi stub fixes, preservation of FP/SIMD registers across
         firmware calls, and conversion of the EFI stub code into a static
         library - Ard Biesheuvel
    
       - Xen EFI support - Daniel Kiper
    
       - Support for autoloading the efivars driver - Lee, Chun-Yi
    
       - Use the PE/COFF headers in the x86 EFI boot stub to request that
         the stub be loaded with CONFIG_PHYSICAL_ALIGN alignment - Michael
         Brown
    
       - Consolidate all the x86 EFI quirks into one file - Saurabh Tangri
    
       - Additional error logging in x86 EFI boot stub - Ulf Winkelvos
    
       - Support loading initrd above 4G in EFI boot stub - Yinghai Lu
    
       - EFI reboot patches for ACPI hardware reduced platforms"
    
    * 'x86-efi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (31 commits)
      efi/arm64: Handle missing virtual mapping for UEFI System Table
      arch/x86/xen: Silence compiler warnings
      xen: Silence compiler warnings
      x86/efi: Request desired alignment via the PE/COFF headers
      x86/efi: Add better error logging to EFI boot stub
      efi: Autoload efivars
      efi: Update stale locking comment for struct efivars
      arch/x86: Remove efi_set_rtc_mmss()
      arch/x86: Replace plain strings with constants
      xen: Put EFI machinery in place
      xen: Define EFI related stuff
      arch/x86: Remove redundant set_bit(EFI_MEMMAP) call
      arch/x86: Remove redundant set_bit(EFI_SYSTEM_TABLES) call
      efi: Introduce EFI_PARAVIRT flag
      arch/x86: Do not access EFI memory map if it is not available
      efi: Use early_mem*() instead of early_io*()
      arch/ia64: Define early_memunmap()
      x86/reboot: Add EFI reboot quirk for ACPI Hardware Reduced flag
      efi/reboot: Allow powering off machines using EFI
      efi/reboot: Add generic wrapper around EfiResetSystem()
      ...

commit c7341d6a61c3526631a4e4565cda72cb1f51d146
Author: Daniel Kiper <daniel.kiper@oracle.com>
Date:   Sat Jul 12 23:09:48 2014 +0200

    arch/x86/xen: Silence compiler warnings
    
    Compiler complains in the following way when x86 32-bit kernel
    with Xen support is build:
    
      CC      arch/x86/xen/enlighten.o
    arch/x86/xen/enlighten.c: In function ‘xen_start_kernel’:
    arch/x86/xen/enlighten.c:1726:3: warning: right shift count >= width of type [enabled by default]
    
    Such line contains following EFI initialization code:
    
    boot_params.efi_info.efi_systab_hi = (__u32)(__pa(efi_systab_xen) >> 32);
    
    There is no issue if x86 64-bit kernel is build. However, 32-bit case
    generate warning (even if that code will not be executed because Xen
    does not work on 32-bit EFI platforms) due to __pa() returning unsigned long
    type which has 32-bits width. So move whole EFI initialization stuff
    to separate function and build it conditionally to avoid above mentioned
    warning on x86 32-bit architecture.
    
    Signed-off-by: Daniel Kiper <daniel.kiper@oracle.com>
    Reviewed-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bc89647f0325..2cd0463dd262 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -32,7 +32,6 @@
 #include <linux/gfp.h>
 #include <linux/memblock.h>
 #include <linux/edd.h>
-#include <linux/efi.h>
 
 #include <xen/xen.h>
 #include <xen/events.h>
@@ -1521,7 +1520,6 @@ asmlinkage __visible void __init xen_start_kernel(void)
 {
 	struct physdev_set_iopl set_iopl;
 	int rc;
-	efi_system_table_t *efi_systab_xen;
 
 	if (!xen_start_info)
 		return;
@@ -1717,18 +1715,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 
 	xen_setup_runstate_info(0);
 
-	efi_systab_xen = xen_efi_probe();
-
-	if (efi_systab_xen) {
-		strncpy((char *)&boot_params.efi_info.efi_loader_signature, "Xen",
-				sizeof(boot_params.efi_info.efi_loader_signature));
-		boot_params.efi_info.efi_systab = (__u32)__pa(efi_systab_xen);
-		boot_params.efi_info.efi_systab_hi = (__u32)(__pa(efi_systab_xen) >> 32);
-
-		set_bit(EFI_BOOT, &efi.flags);
-		set_bit(EFI_PARAVIRT, &efi.flags);
-		set_bit(EFI_64BIT, &efi.flags);
-	}
+	xen_efi_init();
 
 	/* Start the world */
 #ifdef CONFIG_X86_32

commit be81c8a1da24288b0231be50130a64f5cdffdcd4
Author: Daniel Kiper <daniel.kiper@oracle.com>
Date:   Mon Jun 30 19:53:02 2014 +0200

    xen: Put EFI machinery in place
    
    This patch enables EFI usage under Xen dom0. Standard EFI Linux
    Kernel infrastructure cannot be used because it requires direct
    access to EFI data and code. However, in dom0 case it is not possible
    because above mentioned EFI stuff is fully owned and controlled
    by Xen hypervisor. In this case all calls from dom0 to EFI must
    be requested via special hypercall which in turn executes relevant
    EFI code in behalf of dom0.
    
    When dom0 kernel boots it checks for EFI availability on a machine.
    If it is detected then artificial EFI system table is filled.
    Native EFI callas are replaced by functions which mimics them
    by calling relevant hypercall. Later pointer to EFI system table
    is passed to standard EFI machinery and it continues EFI subsystem
    initialization taking into account that there is no direct access
    to EFI boot services, runtime, tables, structures, etc. After that
    system runs as usual.
    
    This patch is based on Jan Beulich and Tang Liang work.
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Tang Liang <liang.tang@oracle.com>
    Signed-off-by: Daniel Kiper <daniel.kiper@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f17b29210ac4..bc89647f0325 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -32,6 +32,7 @@
 #include <linux/gfp.h>
 #include <linux/memblock.h>
 #include <linux/edd.h>
+#include <linux/efi.h>
 
 #include <xen/xen.h>
 #include <xen/events.h>
@@ -1520,6 +1521,7 @@ asmlinkage __visible void __init xen_start_kernel(void)
 {
 	struct physdev_set_iopl set_iopl;
 	int rc;
+	efi_system_table_t *efi_systab_xen;
 
 	if (!xen_start_info)
 		return;
@@ -1715,6 +1717,19 @@ asmlinkage __visible void __init xen_start_kernel(void)
 
 	xen_setup_runstate_info(0);
 
+	efi_systab_xen = xen_efi_probe();
+
+	if (efi_systab_xen) {
+		strncpy((char *)&boot_params.efi_info.efi_loader_signature, "Xen",
+				sizeof(boot_params.efi_info.efi_loader_signature));
+		boot_params.efi_info.efi_systab = (__u32)__pa(efi_systab_xen);
+		boot_params.efi_info.efi_systab_hi = (__u32)(__pa(efi_systab_xen) >> 32);
+
+		set_bit(EFI_BOOT, &efi.flags);
+		set_bit(EFI_PARAVIRT, &efi.flags);
+		set_bit(EFI_64BIT, &efi.flags);
+	}
+
 	/* Start the world */
 #ifdef CONFIG_X86_32
 	i386_start_kernel();

commit 8d693b911bb9c57009c24cb1772d205b84c7985c
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Jul 11 11:51:35 2014 -0400

    xen: Introduce 'xen_nopv' to disable PV extensions for HVM guests.
    
    By default when CONFIG_XEN and CONFIG_XEN_PVHVM kernels are
    run, they will enable the PV extensions (drivers, interrupts, timers,
    etc) - which is the best option for the majority of use cases.
    
    However, in some cases (kexec not fully working, benchmarking)
    we want to disable Xen PV extensions. As such introduce the
    'xen_nopv' parameter that will do it.
    
    This parameter is intended only for HVM guests as the Xen PV
    guests MUST boot with PV extensions. However, even if you use
    'xen_nopv' on Xen PV guests it will be ignored.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    ---
    [v2: s/off/xen_nopv/ per Boris Ostrovsky recommendation.]
    [v3: Add Reviewed-by]
    [v4: Clarify that this is only for HVM guests]

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ffb101e45731..eb822391b84c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1826,8 +1826,19 @@ static void __init xen_hvm_guest_init(void)
 	xen_hvm_init_mmu_ops();
 }
 
+static bool xen_nopv = false;
+static __init int xen_parse_nopv(char *arg)
+{
+       xen_nopv = true;
+       return 0;
+}
+early_param("xen_nopv", xen_parse_nopv);
+
 static uint32_t __init xen_hvm_platform(void)
 {
+	if (xen_nopv)
+		return 0;
+
 	if (xen_pv_domain())
 		return 0;
 
@@ -1836,6 +1847,8 @@ static uint32_t __init xen_hvm_platform(void)
 
 bool xen_hvm_need_lapic(void)
 {
+	if (xen_nopv)
+		return false;
 	if (xen_pv_domain())
 		return false;
 	if (!xen_hvm_domain())

commit 3d09c623947a855ab9a21267c31f926cafa2c5bb
Merge: 92b944170d67 13cd36a37a06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 19 07:53:27 2014 -1000

    Merge tag 'stable/for-linus-3.16-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen fixes from David Vrabel:
     "Xen regression and PVH fixes for 3.16-rc1
    
       - fix dom0 PVH memory setup on latest unstable Xen releases
       - fix 64-bit x86 PV guest boot failure on Xen 3.1 and earlier
       - fix resume regression on non-PV (auto-translated physmap) guests"
    
    * tag 'stable/for-linus-3.16-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/grant-table: fix suspend for non-PV guests
      x86/xen: no need to explicitly register an NMI callback
      Revert "xen/pvh: Update E820 to work with PVH (v2)"
      x86/xen: fix memory setup for PVH dom0

commit abacaadc4144a8849782cc0917a624a7114ffbb1
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jun 2 17:58:01 2014 +0100

    x86/xen: fix memory setup for PVH dom0
    
    Since af06d66ee32b (x86: fix setup of PVH Dom0 memory map) in Xen, PVH
    dom0 need only use the memory memory provided by Xen which has already
    setup all the correct holes.
    
    xen_memory_setup() then ends up being trivial for a PVH guest so
    introduce a new function (xen_auto_xlated_memory_setup()).
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>
    Tested-by: Roger Pau Monné <roger.pau@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 662bdcbbbd18..3123d1f74415 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1537,7 +1537,10 @@ asmlinkage void __init xen_start_kernel(void)
 	if (!xen_pvh_domain())
 		pv_cpu_ops = xen_cpu_ops;
 
-	x86_init.resources.memory_setup = xen_memory_setup;
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		x86_init.resources.memory_setup = xen_auto_xlated_memory_setup;
+	else
+		x86_init.resources.memory_setup = xen_memory_setup;
 	x86_init.oem.arch_setup = xen_arch_setup;
 	x86_init.oem.banner = xen_banner;
 

commit 9f888b3a10782de27864659d4ab48eb6ef2c0bf3
Merge: 5d70dacd4e7c 77945ca73e9a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 2 08:24:12 2014 -0700

    Merge tag 'stable/for-linus-3.16-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip into next
    
    Pull Xen updates from David Vrabel:
     "xen: features and fixes for 3.16-rc0
       - support foreign mappings in PVH domains (needed when dom0 is PVH)
    
       - fix mapping high MMIO regions in x86 PV guests (this is also the
         first half of removing the PAGE_IOMAP PTE flag).
    
       - ARM suspend/resume support.
    
       - ARM multicall support"
    
    * tag 'stable/for-linus-3.16-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      x86/xen: map foreign pfns for autotranslated guests
      xen-acpi-processor: Don't display errors when we get -ENOSYS
      xen/pciback: Document the entry points for 'pcistub_put_pci_dev'
      xen/pciback: Document when the 'unbind' and 'bind' functions are called.
      xen-pciback: Document when we FLR an PCI device.
      xen-pciback: First reset, then free.
      xen-pciback: Cleanup up pcistub_put_pci_dev
      x86/xen: do not use _PAGE_IOMAP in xen_remap_domain_mfn_range()
      x86/xen: set regions above the end of RAM as 1:1
      x86/xen: only warn once if bad MFNs are found during setup
      x86/xen: compactly store large identity ranges in the p2m
      x86/xen: fix set_phys_range_identity() if pfn_e > MAX_P2M_PFN
      x86/xen: rename early_p2m_alloc() and early_p2m_alloc_middle()
      xen/x86: set panic notifier priority to minimum
      arm,arm64/xen: introduce HYPERVISOR_suspend()
      xen: refactor suspend pre/post hooks
      arm: xen: export HYPERVISOR_multicall to modules.
      arm64: introduce virt_to_pfn
      arm/xen: Remove definiition of virt_to_pfn in asm/xen/page.h
      arm: xen: implement multicall hypercall support.

commit bc5eb20161ad742a38b7b4deda393e577ade669b
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue May 13 18:56:25 2014 +0200

    xen/x86: set panic notifier priority to minimum
    
    Execution is not going to continue after telling Xen about the crash.
    Let other panic notifiers run by postponing the final hypercall as much
    as possible.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 201d09a7c46b..662bdcbbbd18 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1339,6 +1339,7 @@ xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
 
 static struct notifier_block xen_panic_block = {
 	.notifier_call= xen_panic_event,
+	.priority = INT_MIN
 };
 
 int xen_panic_handler_init(void)

commit 2605fc216fa492f9e7c488bdc7f687cd6dcc703b
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri May 2 00:44:37 2014 +0200

    asmlinkage, x86: Add explicit __visible to arch/x86/*
    
    As requested by Linus add explicit __visible to the asmlinkage users.
    This marks all functions visible to assembler.
    
    Tree sweep for arch/x86/*
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398984278-29319-3-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 201d09a7c46b..c34bfc4bbe7f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1515,7 +1515,7 @@ static void __init xen_pvh_early_guest_init(void)
 }
 
 /* First C function to be called on Xen boot */
-asmlinkage void __init xen_start_kernel(void)
+asmlinkage __visible void __init xen_start_kernel(void)
 {
 	struct physdev_set_iopl set_iopl;
 	int rc;

commit afca50132cfa7bfc5646676d8c67dc18454f38f8
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Wed Jan 29 16:15:18 2014 -0800

    xen/pvh: set CR4 flags for APs
    
    During bootup in the 'probe_page_size_mask' these CR4 flags are
    set in there. But for AP processors they are not set as we do not
    use 'secondary_startup_64' which the baremetal kernels uses.
    Instead do it in this function which we use in Xen PVH during our
    startup for AP processors.
    
    As such fix it up to make sure we have that flag set.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a4d7b647867f..201d09a7c46b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1473,6 +1473,18 @@ static void xen_pvh_set_cr_flags(int cpu)
 	 * X86_CR0_TS, X86_CR0_PE, X86_CR0_ET are set by Xen for HVM guests
 	 * (which PVH shared codepaths), while X86_CR0_PG is for PVH. */
 	write_cr0(read_cr0() | X86_CR0_MP | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM);
+
+	if (!cpu)
+		return;
+	/*
+	 * For BSP, PSE PGE are set in probe_page_size_mask(), for APs
+	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu_init.
+	*/
+	if (cpu_has_pse)
+		set_in_cr4(X86_CR4_PSE);
+
+	if (cpu_has_pge)
+		set_in_cr4(X86_CR4_PGE);
 }
 
 /*

commit c9f6e9977e38de15da96b732a8dec0ef56cbf977
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Mon Jan 20 09:20:07 2014 -0500

    xen/pvh: Set X86_CR0_WP and others in CR0 (v2)
    
    otherwise we will get for some user-space applications
    that use 'clone' with CLONE_CHILD_SETTID | CLONE_CHILD_CLEARTID
    end up hitting an assert in glibc manifested by:
    
    general protection ip:7f80720d364c sp:7fff98fd8a80 error:0 in
    libc-2.13.so[7f807209e000+180000]
    
    This is due to the nature of said operations which sets and clears
    the PID.  "In the successful one I can see that the page table of
    the parent process has been updated successfully to use a
    different physical page, so the write of the tid on
    that page only affects the child...
    
    On the other hand, in the failed case, the write seems to happen before
    the copy of the original page is done, so both the parent and the child
    end up with the same value (because the parent copies the page after
    the write of the child tid has already happened)."
    (Roger's analysis). The nature of this is due to the Xen's commit
    of 51e2cac257ec8b4080d89f0855c498cbbd76a5e5
    "x86/pvh: set only minimal cr0 and cr4 flags in order to use paging"
    the CR0_WP was removed so COW features of the Linux kernel were not
    operating properly.
    
    While doing that also update the rest of the CR0 flags to be inline
    with what a baremetal Linux kernel would set them to.
    
    In 'secondary_startup_64' (baremetal Linux) sets:
    
    X86_CR0_PE | X86_CR0_MP | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP |
    X86_CR0_AM | X86_CR0_PG
    
    The hypervisor for HVM type guests (which PVH is a bit) sets:
    X86_CR0_PE | X86_CR0_ET | X86_CR0_TS
    For PVH it specifically sets:
    X86_CR0_PG
    
    Which means we need to set the rest: X86_CR0_MP | X86_CR0_NE  |
    X86_CR0_WP | X86_CR0_AM to have full parity.
    
    Signed-off-by: Roger Pau Monne <roger.pau@citrix.com>
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    [v1: Took out the cr4 writes to be a seperate patch]
    [v2: 0-DAY kernel found xen_setup_gdt to be missing a static]

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b6d61c353fe5..a4d7b647867f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1414,7 +1414,7 @@ static void __init xen_boot_params_init_edd(void)
  * is PVH which is not going to use xen_load_gdt_boot or other
  * __init functions.
  */
-void __ref xen_setup_gdt(int cpu)
+static void __ref xen_setup_gdt(int cpu)
 {
 	if (xen_feature(XENFEAT_auto_translated_physmap)) {
 #ifdef CONFIG_X86_64
@@ -1462,13 +1462,40 @@ void __ref xen_setup_gdt(int cpu)
 	pv_cpu_ops.load_gdt = xen_load_gdt;
 }
 
+/*
+ * A PV guest starts with default flags that are not set for PVH, set them
+ * here asap.
+ */
+static void xen_pvh_set_cr_flags(int cpu)
+{
+
+	/* Some of these are setup in 'secondary_startup_64'. The others:
+	 * X86_CR0_TS, X86_CR0_PE, X86_CR0_ET are set by Xen for HVM guests
+	 * (which PVH shared codepaths), while X86_CR0_PG is for PVH. */
+	write_cr0(read_cr0() | X86_CR0_MP | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM);
+}
+
+/*
+ * Note, that it is ref - because the only caller of this after init
+ * is PVH which is not going to use xen_load_gdt_boot or other
+ * __init functions.
+ */
+void __ref xen_pvh_secondary_vcpu_init(int cpu)
+{
+	xen_setup_gdt(cpu);
+	xen_pvh_set_cr_flags(cpu);
+}
+
 static void __init xen_pvh_early_guest_init(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap))
 		return;
 
-	if (xen_feature(XENFEAT_hvm_callback_vector))
-		xen_have_vector_callback = 1;
+	if (!xen_feature(XENFEAT_hvm_callback_vector))
+		return;
+
+	xen_have_vector_callback = 1;
+	xen_pvh_set_cr_flags(0);
 
 #ifdef CONFIG_X86_32
 	BUG(); /* PVH: Implement proper support. */

commit 5602aba808aa5440e4b8fa4340079cb2047651ee
Author: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
Date:   Tue Jan 7 21:37:09 2014 +0800

    xen/pvh: remove duplicated include from enlighten.c
    
    Remove duplicated include.
    
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a4e2f30917a3..b6d61c353fe5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -46,7 +46,6 @@
 #include <xen/hvm.h>
 #include <xen/hvc-console.h>
 #include <xen/acpi.h>
-#include <xen/features.h>
 
 #include <asm/paravirt.h>
 #include <asm/apic.h>

commit 2771374d47220c7ec271281437625e9519505bb2
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Wed Dec 11 15:36:51 2013 -0500

    xen/pvh: Piggyback on PVHVM for event channels (v2)
    
    PVH is a PV guest with a twist - there are certain things
    that work in it like HVM and some like PV. There is
    a similar mode - PVHVM where we run in HVM mode with
    PV code enabled - and this patch explores that.
    
    The most notable PV interfaces are the XenBus and event channels.
    
    We will piggyback on how the event channel mechanism is
    used in PVHVM - that is we want the normal native IRQ mechanism
    and we will install a vector (hvm callback) for which we
    will call the event channel mechanism.
    
    This means that from a pvops perspective, we can use
    native_irq_ops instead of the Xen PV specific. Albeit in the
    future we could support pirq_eoi_map. But that is
    a feature request that can be shared with PVHVM.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2eca6187fc92..a4e2f30917a3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1144,8 +1144,9 @@ void xen_setup_vcpu_info_placement(void)
 		xen_vcpu_setup(cpu);
 
 	/* xen_vcpu_setup managed to place the vcpu_info within the
-	   percpu area for all cpus, so make use of it */
-	if (have_vcpu_info_placement) {
+	 * percpu area for all cpus, so make use of it. Note that for
+	 * PVH we want to use native IRQ mechanism. */
+	if (have_vcpu_info_placement && !xen_pvh_domain()) {
 		pv_irq_ops.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);
 		pv_irq_ops.restore_fl = __PV_IS_CALLEE_SAVE(xen_restore_fl_direct);
 		pv_irq_ops.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);

commit 5840c84b16aad223d5305d8a569ea55de4120d67
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Fri Dec 13 11:48:08 2013 -0500

    xen/pvh: Secondary VCPU bringup (non-bootup CPUs)
    
    The VCPU bringup protocol follows the PV with certain twists.
    From xen/include/public/arch-x86/xen.h:
    
    Also note that when calling DOMCTL_setvcpucontext and VCPU_initialise
    for HVM and PVH guests, not all information in this structure is updated:
    
     - For HVM guests, the structures read include: fpu_ctxt (if
     VGCT_I387_VALID is set), flags, user_regs, debugreg[*]
    
     - PVH guests are the same as HVM guests, but additionally use ctrlreg[3] to
     set cr3. All other fields not used should be set to 0.
    
    This is what we do. We piggyback on the 'xen_setup_gdt' - but modify
    a bit - we need to call 'load_percpu_segment' so that 'switch_to_new_gdt'
    can load per-cpu data-structures. It has no effect on the VCPU0.
    
    We also piggyback on the %rdi register to pass in the CPU number - so
    that when we bootup a new CPU, the cpu_bringup_and_idle will have
    passed as the first parameter the CPU number (via %rdi for 64-bit).
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1170d00879d5..2eca6187fc92 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1409,14 +1409,19 @@ static void __init xen_boot_params_init_edd(void)
  * Set up the GDT and segment registers for -fstack-protector.  Until
  * we do this, we have to be careful not to call any stack-protected
  * function, which is most of the kernel.
+ *
+ * Note, that it is __ref because the only caller of this after init
+ * is PVH which is not going to use xen_load_gdt_boot or other
+ * __init functions.
  */
-static void __init xen_setup_gdt(void)
+void __ref xen_setup_gdt(int cpu)
 {
 	if (xen_feature(XENFEAT_auto_translated_physmap)) {
 #ifdef CONFIG_X86_64
 		unsigned long dummy;
 
-		switch_to_new_gdt(0); /* GDT and GS set */
+		load_percpu_segment(cpu); /* We need to access per-cpu area */
+		switch_to_new_gdt(cpu); /* GDT and GS set */
 
 		/* We are switching of the Xen provided GDT to our HVM mode
 		 * GDT. The new GDT has  __KERNEL_CS with CS.L = 1
@@ -1529,7 +1534,7 @@ asmlinkage void __init xen_start_kernel(void)
 	 * Set up kernel GDT and segment registers, mainly so that
 	 * -fstack-protector code can be executed.
 	 */
-	xen_setup_gdt();
+	xen_setup_gdt(0);
 
 	xen_init_irq_ops();
 	xen_init_cpuid_mask();

commit 8d656bbe43aee6d1be6b49fcf8acbc04588472bc
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Fri Dec 13 13:03:37 2013 -0500

    xen/pvh: Load GDT/GS in early PV bootup code for BSP.
    
    During early bootup we start life using the Xen provided
    GDT, which means that we are running with %cs segment set
    to FLAT_KERNEL_CS (FLAT_RING3_CS64 0xe033, GDT index 261).
    
    But for PVH we want to be use HVM type mechanism for
    segment operations. As such we need to switch to the HVM
    one and also reload ourselves with the __KERNEL_CS:eip
    to run in the proper GDT and segment.
    
    For HVM this is usually done in 'secondary_startup_64' in
    (head_64.S) but since we are not taking that bootup
    path (we start in PV - xen_start_kernel) we need to do
    that in the early PV bootup paths.
    
    For good measure we also zero out the %fs, %ds, and %es
    (not strictly needed as Xen has already cleared them
    for us). The %gs is loaded by 'switch_to_new_gdt'.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 23ead298edbd..1170d00879d5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1410,8 +1410,43 @@ static void __init xen_boot_params_init_edd(void)
  * we do this, we have to be careful not to call any stack-protected
  * function, which is most of the kernel.
  */
-static void __init xen_setup_stackprotector(void)
+static void __init xen_setup_gdt(void)
 {
+	if (xen_feature(XENFEAT_auto_translated_physmap)) {
+#ifdef CONFIG_X86_64
+		unsigned long dummy;
+
+		switch_to_new_gdt(0); /* GDT and GS set */
+
+		/* We are switching of the Xen provided GDT to our HVM mode
+		 * GDT. The new GDT has  __KERNEL_CS with CS.L = 1
+		 * and we are jumping to reload it.
+		 */
+		asm volatile ("pushq %0\n"
+			      "leaq 1f(%%rip),%0\n"
+			      "pushq %0\n"
+			      "lretq\n"
+			      "1:\n"
+			      : "=&r" (dummy) : "0" (__KERNEL_CS));
+
+		/*
+		 * While not needed, we also set the %es, %ds, and %fs
+		 * to zero. We don't care about %ss as it is NULL.
+		 * Strictly speaking this is not needed as Xen zeros those
+		 * out (and also MSR_FS_BASE, MSR_GS_BASE, MSR_KERNEL_GS_BASE)
+		 *
+		 * Linux zeros them in cpu_init() and in secondary_startup_64
+		 * (for BSP).
+		 */
+		loadsegment(es, 0);
+		loadsegment(ds, 0);
+		loadsegment(fs, 0);
+#else
+		/* PVH: TODO Implement. */
+		BUG();
+#endif
+		return; /* PVH does not need any PV GDT ops. */
+	}
 	pv_cpu_ops.write_gdt_entry = xen_write_gdt_entry_boot;
 	pv_cpu_ops.load_gdt = xen_load_gdt_boot;
 
@@ -1494,7 +1529,7 @@ asmlinkage void __init xen_start_kernel(void)
 	 * Set up kernel GDT and segment registers, mainly so that
 	 * -fstack-protector code can be executed.
 	 */
-	xen_setup_stackprotector();
+	xen_setup_gdt();
 
 	xen_init_irq_ops();
 	xen_init_cpuid_mask();

commit 696fd7c5b2ecb31b339019ced4fe15a3f9e7419a
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun Dec 15 12:37:46 2013 -0500

    xen/pvh: Don't setup P2M tree.
    
    P2M is not available for PVH. Fortunatly for us the
    P2M code already has mostly the support for auto-xlat guest thanks to
    commit 3d24bbd7dddbea54358a9795abaf051b0f18973c
    "grant-table: call set_phys_to_machine after mapping grant refs"
    which: "
    introduces set_phys_to_machine calls for auto_translated guests
    (even on x86) in gnttab_map_refs and gnttab_unmap_refs.
    translated by swiotlb-xen... " so we don't need to muck much.
    
    with above mentioned "commit you'll get set_phys_to_machine calls
    from gnttab_map_refs and gnttab_unmap_refs but PVH guests won't do
    anything with them " (Stefano Stabellini) which is OK - we want
    them to be NOPs.
    
    This is because we assume that an "IOMMU is always present on the
    plaform and Xen is going to make the appropriate IOMMU pagetable
    changes in the hypercall implementation of GNTTABOP_map_grant_ref
    and GNTTABOP_unmap_grant_ref, then eveything should be transparent
    from PVH priviligied point of view and DMA transfers involving
    foreign pages keep working with no issues[sp]
    
    Otherwise we would need a P2M (and an M2P) for PVH priviligied to
    track these foreign pages .. (see arch/arm/xen/p2m.c)."
    (Stefano Stabellini).
    
    We still have to inhibit the building of the P2M tree.
    That had been done in the past by not calling
    xen_build_dynamic_phys_to_machine (which setups the P2M tree
    and gives us virtual address to access them). But we are missing
    a check for xen_build_mfn_list_list - which was continuing to setup
    the P2M tree and would blow up at trying to get the virtual
    address of p2m_missing (which would have been setup by
    xen_build_dynamic_phys_to_machine).
    
    Hence a check is needed to not call xen_build_mfn_list_list when
    running in auto-xlat mode.
    
    Instead of replicating the check for auto-xlat in enlighten.c
    do it in the p2m.c code. The reason is that the xen_build_mfn_list_list
    is called also in xen_arch_post_suspend without any checks for
    auto-xlat. So for PVH or PV with auto-xlat - we would needlessly
    allocate space for an P2M tree.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index eb0efc2f9d3c..23ead298edbd 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1488,8 +1488,7 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_configure_nx();
 
 	/* Get mfn list */
-	if (!xen_feature(XENFEAT_auto_translated_physmap))
-		xen_build_dynamic_phys_to_machine();
+	xen_build_dynamic_phys_to_machine();
 
 	/*
 	 * Set up kernel GDT and segment registers, mainly so that

commit d285d68314af49c4456b71d248e355dd33ae375c
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Fri Dec 13 12:45:31 2013 -0500

    xen/pvh: Early bootup changes in PV code (v4).
    
    We don't use the filtering that 'xen_cpuid' is doing
    because the hypervisor treats 'XEN_EMULATE_PREFIX' as
    an invalid instruction. This means that all of the filtering
    will have to be done in the hypervisor/toolstack.
    
    Without the filtering we expose to the guest the:
    
     - cpu topology (sockets, cores, etc);
     - the APERF (which the generic scheduler likes to
        use), see  5e626254206a709c6e937f3dda69bf26c7344f6f
        "xen/setup: filter APERFMPERF cpuid feature out"
     - and the inability to figure out whether MWAIT_LEAF
       should be exposed or not. See
       df88b2d96e36d9a9e325bfcd12eb45671cbbc937
       "xen/enlighten: Disable MWAIT_LEAF so that acpi-pad won't be loaded."
     - x2apic, see  4ea9b9aca90cfc71e6872ed3522356755162932c
       "xen: mask x2APIC feature in PV"
    
    We also check for vector callback early on, as it is a required
    feature. PVH also runs at default kernel IOPL.
    
    Finally, pure PV settings are moved to a separate function that are
    only called for pure PV, ie, pv with pvmmu. They are also #ifdef
    with CONFIG_XEN_PVMMU.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index fa6ade76ef3f..eb0efc2f9d3c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -46,6 +46,7 @@
 #include <xen/hvm.h>
 #include <xen/hvc-console.h>
 #include <xen/acpi.h>
+#include <xen/features.h>
 
 #include <asm/paravirt.h>
 #include <asm/apic.h>
@@ -262,8 +263,9 @@ static void __init xen_banner(void)
 	struct xen_extraversion extra;
 	HYPERVISOR_xen_version(XENVER_extraversion, &extra);
 
-	printk(KERN_INFO "Booting paravirtualized kernel on %s\n",
-	       pv_info.name);
+	pr_info("Booting paravirtualized kernel %son %s\n",
+		xen_feature(XENFEAT_auto_translated_physmap) ?
+			"with PVH extensions " : "", pv_info.name);
 	printk(KERN_INFO "Xen version: %d.%d%s%s\n",
 	       version >> 16, version & 0xffff, extra.extraversion,
 	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
@@ -433,7 +435,7 @@ static void __init xen_init_cpuid_mask(void)
 
 	ax = 1;
 	cx = 0;
-	xen_cpuid(&ax, &bx, &cx, &dx);
+	cpuid(1, &ax, &bx, &cx, &dx);
 
 	xsave_mask =
 		(1 << (X86_FEATURE_XSAVE % 32)) |
@@ -1420,6 +1422,19 @@ static void __init xen_setup_stackprotector(void)
 	pv_cpu_ops.load_gdt = xen_load_gdt;
 }
 
+static void __init xen_pvh_early_guest_init(void)
+{
+	if (!xen_feature(XENFEAT_auto_translated_physmap))
+		return;
+
+	if (xen_feature(XENFEAT_hvm_callback_vector))
+		xen_have_vector_callback = 1;
+
+#ifdef CONFIG_X86_32
+	BUG(); /* PVH: Implement proper support. */
+#endif
+}
+
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {
@@ -1431,13 +1446,16 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_domain_type = XEN_PV_DOMAIN;
 
+	xen_setup_features();
+	xen_pvh_early_guest_init();
 	xen_setup_machphys_mapping();
 
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;
-	pv_cpu_ops = xen_cpu_ops;
 	pv_apic_ops = xen_apic_ops;
+	if (!xen_pvh_domain())
+		pv_cpu_ops = xen_cpu_ops;
 
 	x86_init.resources.memory_setup = xen_memory_setup;
 	x86_init.oem.arch_setup = xen_arch_setup;
@@ -1469,8 +1487,6 @@ asmlinkage void __init xen_start_kernel(void)
 	/* Work out if we support NX */
 	x86_configure_nx();
 
-	xen_setup_features();
-
 	/* Get mfn list */
 	if (!xen_feature(XENFEAT_auto_translated_physmap))
 		xen_build_dynamic_phys_to_machine();
@@ -1548,14 +1564,18 @@ asmlinkage void __init xen_start_kernel(void)
 	/* set the limit of our address space */
 	xen_reserve_top();
 
-	/* We used to do this in xen_arch_setup, but that is too late on AMD
-	 * were early_cpu_init (run before ->arch_setup()) calls early_amd_init
-	 * which pokes 0xcf8 port.
-	 */
-	set_iopl.iopl = 1;
-	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
-	if (rc != 0)
-		xen_raw_printk("physdev_op failed %d\n", rc);
+	/* PVH: runs at default kernel iopl of 0 */
+	if (!xen_pvh_domain()) {
+		/*
+		 * We used to do this in xen_arch_setup, but that is too late
+		 * on AMD were early_cpu_init (run before ->arch_setup()) calls
+		 * early_amd_init which pokes 0xcf8 port.
+		 */
+		set_iopl.iopl = 1;
+		rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
+		if (rc != 0)
+			xen_raw_printk("physdev_op failed %d\n", rc);
+	}
 
 #ifdef CONFIG_X86_32
 	/* set up basic CPUID stuff */

commit a60d4b9874dc62cf0fd0d42b247baaaef75d30f8
Merge: fa1586a7e437 c3b7cb1fd8c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 10 20:07:04 2013 -0700

    Merge tag 'stable/for-linus-3.12-rc0-tag-two' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen bug-fixes from Konrad Rzeszutek Wilk:
     "This pull I usually do after rc1 is out but because we have a nice
      amount of fixes, some bootup related fixes for ARM, and it is early in
      the cycle we figured to do it now to help with tracking of potential
      regressions.
    
      The simple ones are the ARM ones - one of the patches fell through the
      cracks, other fixes a bootup issue (unconditionally using Xen
      functions).  Then a fix for a regression causing preempt count being
      off (patch causing this went in v3.12).
    
      Lastly are the fixes to make Xen PVHVM guests use PV ticketlocks (Xen
      PV already does).
    
      The enablement of that was supposed to be part of the x86 spinlock
      merge in commit 816434ec4a67 ("The biggest change here are
      paravirtualized ticket spinlocks (PV spinlocks), which bring a nice
      speedup on various benchmarks...") but unfortunatly it would cause
      hang when booting Xen PVHVM guests.  Yours truly got all of the bugs
      fixed last week and they (six of them) are included in this pull.
    
      Bug-fixes:
       - Boot on ARM without using Xen unconditionally
       - On Xen ARM don't run cpuidle/cpufreq
       - Fix regression in balloon driver, preempt count warnings
       - Fixes to make PVHVM able to use pv ticketlock.
       - Revert Xen PVHVM disabling pv ticketlock (aka, re-enable pv ticketlocks)"
    
    * tag 'stable/for-linus-3.12-rc0-tag-two' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/spinlock: Don't use __initdate for xen_pv_spin
      Revert "xen/spinlock: Disable IRQ spinlock (PV) allocation on PVHVM"
      xen/spinlock: Don't setup xen spinlock IPI kicker if disabled.
      xen/smp: Update pv_lock_ops functions before alternative code starts under PVHVM
      xen/spinlock: We don't need the old structure anymore
      xen/spinlock: Fix locking path engaging too soon under PVHVM.
      xen/arm: disable cpuidle and cpufreq when linux is running as dom0
      xen/p2m: Don't call get_balloon_scratch_page() twice, keep interrupts disabled for multicalls
      ARM: xen: only set pm function ptrs for Xen guests

commit 1fb3a8b2cfb278f139d9ff7ca5fe06a65de64494
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Aug 13 11:47:21 2013 -0400

    xen/spinlock: Fix locking path engaging too soon under PVHVM.
    
    The xen_lock_spinning has a check for the kicker interrupts
    and if it is not initialized it will spin normally (not enter
    the slowpath).
    
    But for PVHVM case we would initialize the kicker interrupt
    before the CPU came online. This meant that if the booting
    CPU used a spinlock and went in the slowpath - it would
    enter the slowpath and block forever. The forever part because
    during bootup: the spinlock would be taken _before_ the CPU
    sets itself to be online (more on this further), and we enter
    to poll on the event channel forever.
    
    The bootup CPU (see commit fc78d343fa74514f6fd117b5ef4cd27e4ac30236
    "xen/smp: initialize IPI vectors before marking CPU online"
    for details) and the CPU that started the bootup consult
    the cpu_online_mask to determine whether the booting CPU should
    get an IPI. The booting CPU has to set itself in this mask via:
    
      set_cpu_online(smp_processor_id(), true);
    
    However, if the spinlock is taken before this (and it is) and
    it polls on an event channel - it will never be woken up as
    the kernel will never send an IPI to an offline CPU.
    
    Note that the PVHVM logic in sending IPIs is using the HVM
    path which has numerous checks using the cpu_online_mask
    and cpu_active_mask. See above mention git commit for details.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 15939e872db2..f091c80974c4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1692,7 +1692,6 @@ static int xen_hvm_cpu_notify(struct notifier_block *self, unsigned long action,
 	case CPU_UP_PREPARE:
 		xen_vcpu_setup(cpu);
 		if (xen_have_vector_callback) {
-			xen_init_lock_cpu(cpu);
 			if (xen_feature(XENFEAT_hvm_safe_pvclock))
 				xen_setup_timer(cpu);
 		}

commit cf39c8e5352b4fb9efedfe7e9acb566a85ed847c
Merge: 3398d252a4da 23b7eaf82207
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 17:45:39 2013 -0700

    Merge tag 'stable/for-linus-3.12-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen updates from Konrad Rzeszutek Wilk:
     "A couple of features and a ton of bug-fixes.  There is also some
      maintership changes.  Jeremy is enjoying the full-time work at the
      startup and as much as he would love to help - he can't find the time.
      I have a bunch of other things that I promised to work on - paravirt
      diet, get SWIOTLB working everywhere, etc, but haven't been able to
      find the time.
    
      As such both David Vrabel and Boris Ostrovsky have graciously
      volunteered to help with the maintership role.  They will keep the lid
      on regressions, bug-fixes, etc.  I will be in the background to help -
      but eventually there will be less of me doing the Xen GIT pulls and
      more of them.  Stefano is still doing the ARM/ARM64 and will continue
      on doing so.
    
      Features:
       - Xen Trusted Platform Module (TPM) frontend driver - with the
         backend in MiniOS.
       - Scalability improvements in event channel.
       - Two extra Xen co-maintainers (David, Boris) and one going away (Jeremy)
    
      Bug-fixes:
       - Make the 1:1 mapping work during early bootup on selective regions.
       - Add scratch page to balloon driver to deal with unexpected code
         still holding on stale pages.
       - Allow NMIs on PV guests (64-bit only)
       - Remove unnecessary TLB flush in M2P code.
       - Fixes duplicate callbacks in Xen granttable code.
       - Fixes in PRIVCMD_MMAPBATCH ioctls to allow retries
       - Fix for events being lost due to rescheduling on different VCPUs.
       - More documentation"
    
    * tag 'stable/for-linus-3.12-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (23 commits)
      hvc_xen: Remove unnecessary __GFP_ZERO from kzalloc
      drivers/xen-tpmfront: Fix compile issue with missing option.
      xen/balloon: don't set P2M entry for auto translated guest
      xen/evtchn: double free on error
      Xen: Fix retry calls into PRIVCMD_MMAPBATCH*.
      xen/pvhvm: Initialize xen panic handler for PVHVM guests
      xen/m2p: use GNTTABOP_unmap_and_replace to reinstate the original mapping
      xen: fix ARM build after 6efa20e4
      MAINTAINERS: Remove Jeremy from the Xen subsystem.
      xen/events: document behaviour when scanning the start word for events
      x86/xen: during early setup, only 1:1 map the ISA region
      x86/xen: disable premption when enabling local irqs
      swiotlb-xen: replace dma_length with sg_dma_len() macro
      swiotlb: replace dma_length with sg_dma_len() macro
      xen/balloon: set a mapping for ballooned out pages
      xen/evtchn: improve scalability by using per-user locks
      xen/p2m: avoid unneccesary TLB flush in m2p_remove_override()
      MAINTAINERS: Add in two extra co-maintainers of the Xen tree.
      MAINTAINERS: Update the Xen subsystem's with proper mailing list.
      xen: replace strict_strtoul() with kstrtoul()
      ...

commit 669b0ae961e87c824233475e987b2d39996d4849
Author: Vaughan Cao <vaughan.cao@oracle.com>
Date:   Fri Aug 16 16:10:56 2013 +0800

    xen/pvhvm: Initialize xen panic handler for PVHVM guests
    
    kernel use callback linked in panic_notifier_list to notice others when panic
    happens.
    NORET_TYPE void panic(const char * fmt, ...){
        ...
        atomic_notifier_call_chain(&panic_notifier_list, 0, buf);
    }
    When Xen becomes aware of this, it will call xen_reboot(SHUTDOWN_crash) to
    send out an event with reason code - SHUTDOWN_crash.
    
    xen_panic_handler_init() is defined to register on panic_notifier_list but
    we only call it in xen_arch_setup which only be called by PV, this patch is
    necessary for PVHVM.
    
    Without this patch, setting 'on_crash=coredump-restart' in PVHVM guest config
    file won't lead a vmcore to be generate when the guest panics. It can be
    reproduced with 'echo c > /proc/sysrq-trigger'.
    
    Signed-off-by: Vaughan Cao <vaughan.cao@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Joe Jin <joe.jin@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b5a22fa7e249..15939e872db2 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1713,6 +1713,8 @@ static void __init xen_hvm_guest_init(void)
 
 	xen_hvm_init_shared_info();
 
+	xen_panic_handler_init();
+
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;
 	xen_hvm_smp_init();

commit 6efa20e49b9cb1db1ab66870cc37323474a75a13
Author: Konrad Rzeszutek Wilk <konrad@kernel.org>
Date:   Fri Jul 19 11:51:31 2013 -0400

    xen: Support 64-bit PV guest receiving NMIs
    
    This is based on a patch that Zhenzhong Duan had sent - which
    was missing some of the remaining pieces. The kernel has the
    logic to handle Xen-type-exceptions using the paravirt interface
    in the assembler code (see PARAVIRT_ADJUST_EXCEPTION_FRAME -
    pv_irq_ops.adjust_exception_frame and and INTERRUPT_RETURN -
    pv_cpu_ops.iret).
    
    That means the nmi handler (and other exception handlers) use
    the hypervisor iret.
    
    The other changes that would be neccessary for this would
    be to translate the NMI_VECTOR to one of the entries on the
    ipi_vector and make xen_send_IPI_mask_allbutself use different
    events.
    
    Fortunately for us commit 1db01b4903639fcfaec213701a494fe3fb2c490b
    (xen: Clean up apic ipi interface) implemented this and we piggyback
    on the cleanup such that the apic IPI interface will pass the right
    vector value for NMI.
    
    With this patch we can trigger NMIs within a PV guest (only tested
    x86_64).
    
    For this to work with normal PV guests (not initial domain)
    we need the domain to be able to use the APIC ops - they are
    already implemented to use the Xen event channels. For that
    to be turned on in a PV domU we need to remove the masking
    of X86_FEATURE_APIC.
    
    Incidentally that means kgdb will also now work within
    a PV guest without using the 'nokgdbroundup' workaround.
    
    Note that the 32-bit version is different and this patch
    does not enable that.
    
    CC: Lisa Nguyen <lisa@xenapiadmin.com>
    CC: Ben Guthro <benjamin.guthro@citrix.com>
    CC: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    [v1: Fixed up per David Vrabel comments]
    Reviewed-by: Ben Guthro <benjamin.guthro@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 193097ef3d7d..b5a22fa7e249 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -427,8 +427,7 @@ static void __init xen_init_cpuid_mask(void)
 
 	if (!xen_initial_domain())
 		cpuid_leaf1_edx_mask &=
-			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
-			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
+			~((1 << X86_FEATURE_ACPI));  /* disable ACPI */
 
 	cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_X2APIC % 32));
 
@@ -735,8 +734,7 @@ static int cvt_gate_to_trap(int vector, const gate_desc *val,
 		addr = (unsigned long)xen_int3;
 	else if (addr == (unsigned long)stack_segment)
 		addr = (unsigned long)xen_stack_segment;
-	else if (addr == (unsigned long)double_fault ||
-		 addr == (unsigned long)nmi) {
+	else if (addr == (unsigned long)double_fault) {
 		/* Don't need to handle these */
 		return 0;
 #ifdef CONFIG_X86_MCE
@@ -747,7 +745,12 @@ static int cvt_gate_to_trap(int vector, const gate_desc *val,
 		 */
 		;
 #endif
-	} else {
+	} else if (addr == (unsigned long)nmi)
+		/*
+		 * Use the native version as well.
+		 */
+		;
+	else {
 		/* Some other trap using IST? */
 		if (WARN_ON(val->ist != 0))
 			return 0;

commit 9df56f19a500bea90d160be1bf77e4fbcd204d3f
Author: Jason Wang <jasowang@redhat.com>
Date:   Thu Jul 25 16:54:35 2013 +0800

    x86: Correctly detect hypervisor
    
    We try to handle the hypervisor compatibility mode by detecting hypervisor
    through a specific order. This is not robust, since hypervisors may implement
    each others features.
    
    This patch tries to handle this situation by always choosing the last one in the
    CPUID leaves. This is done by letting .detect() return a priority instead of
    true/false and just re-using the CPUID leaf where the signature were found as
    the priority (or 1 if it was found by DMI). Then we can just pick hypervisor who
    has the highest priority. Other sophisticated detection method could also be
    implemented on top.
    
    Suggested by H. Peter Anvin and Paolo Bonzini.
    
    Acked-by: K. Y. Srinivasan <kys@microsoft.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Doug Covelli <dcovelli@vmware.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dan Hecht <dhecht@vmware.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jason Wang <jasowang@redhat.com>
    Link: http://lkml.kernel.org/r/1374742475-2485-4-git-send-email-jasowang@redhat.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 193097ef3d7d..2fcaedc0b739 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1720,15 +1720,12 @@ static void __init xen_hvm_guest_init(void)
 	xen_hvm_init_mmu_ops();
 }
 
-static bool __init xen_hvm_platform(void)
+static uint32_t __init xen_hvm_platform(void)
 {
 	if (xen_pv_domain())
-		return false;
-
-	if (!xen_cpuid_base())
-		return false;
+		return 0;
 
-	return true;
+	return xen_cpuid_base();
 }
 
 bool xen_hvm_need_lapic(void)

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2fa02bc50034..193097ef3d7d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1681,8 +1681,8 @@ static void __init init_hvm_pv_info(void)
 	xen_domain_type = XEN_HVM_DOMAIN;
 }
 
-static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
-				    unsigned long action, void *hcpu)
+static int xen_hvm_cpu_notify(struct notifier_block *self, unsigned long action,
+			      void *hcpu)
 {
 	int cpu = (long)hcpu;
 	switch (action) {
@@ -1700,7 +1700,7 @@ static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block xen_hvm_cpu_notifier __cpuinitdata = {
+static struct notifier_block xen_hvm_cpu_notifier = {
 	.notifier_call	= xen_hvm_cpu_notify,
 };
 

commit 60e019eb37a8d989031ad47ae9810453536f3127
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Apr 29 16:04:20 2013 +0200

    x86: Get rid of ->hard_math and all the FPU asm fu
    
    Reimplement FPU detection code in C and drop old, not-so-recommended
    detection method in asm. Move all the relevant stuff into i387.c where
    it conceptually belongs. Finally drop cpuinfo_x86.hard_math.
    
    [ hpa: huge thanks to Borislav for taking my original concept patch
      and productizing it ]
    
    [ Boris, note to self: do not use static_cpu_has before alternatives! ]
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1367244262-29511-2-git-send-email-bp@alien8.de
    Link: http://lkml.kernel.org/r/1365436666-9837-2-git-send-email-bp@alien8.de
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a492be2635ac..2fa02bc50034 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1557,7 +1557,7 @@ asmlinkage void __init xen_start_kernel(void)
 #ifdef CONFIG_X86_32
 	/* set up basic CPUID stuff */
 	cpu_detect(&new_cpu_data);
-	new_cpu_data.hard_math = 1;
+	set_cpu_cap(&new_cpu_data, X86_FEATURE_FPU);
 	new_cpu_data.wp_works_ok = 1;
 	new_cpu_data.x86_capability[0] = cpuid_edx(1);
 #endif

commit 607eeb0b836aa24a6972a460a213c4f87902d403
Merge: 4c4445013f79 7c86617dde00
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 11 16:19:30 2013 -0700

    Merge tag 'stable/for-linus-3.10-rc0-tag-two' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen bug-fixes from Konrad Rzeszutek Wilk:
     - More fixes in the vCPU PVHVM hotplug path.
     - Add more documentation.
     - Fix various ARM related issues in the Xen generic drivers.
     - Updates in the xen-pciback driver per Bjorn's updates.
     - Mask the x2APIC feature for PV guests.
    
    * tag 'stable/for-linus-3.10-rc0-tag-two' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/pci: Used cached MSI-X capability offset
      xen/pci: Use PCI_MSIX_TABLE_BIR, not PCI_MSIX_FLAGS_BIRMASK
      xen: clear IRQ_NOAUTOEN and IRQ_NOREQUEST
      xen: mask x2APIC feature in PV
      xen: SWIOTLB is only used on x86
      xen/spinlock: Fix check from greater than to be also be greater or equal to.
      xen/smp/pvhvm: Don't point per_cpu(xen_vpcu, 33 and larger) to shared_info
      xen/vcpu: Document the xen_vcpu_info and xen_vcpu
      xen/vcpu/pvhvm: Fix vcpu hotplugging hanging.

commit 4ea9b9aca90cfc71e6872ed3522356755162932c
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Fri Mar 29 09:14:00 2013 +0800

    xen: mask x2APIC feature in PV
    
    On x2apic enabled pvm, doing sysrq+l, got NULL pointer dereference as below.
    
        SysRq : Show backtrace of all active CPUs
        BUG: unable to handle kernel NULL pointer dereference at           (null)
        IP: [<ffffffff8125e3cb>] memcpy+0xb/0x120
        Call Trace:
         [<ffffffff81039633>] ? __x2apic_send_IPI_mask+0x73/0x160
         [<ffffffff8103973e>] x2apic_send_IPI_all+0x1e/0x20
         [<ffffffff8103498c>] arch_trigger_all_cpu_backtrace+0x6c/0xb0
         [<ffffffff81501be4>] ? _raw_spin_lock_irqsave+0x34/0x50
         [<ffffffff8131654e>] sysrq_handle_showallcpus+0xe/0x10
         [<ffffffff8131616d>] __handle_sysrq+0x7d/0x140
         [<ffffffff81316230>] ? __handle_sysrq+0x140/0x140
         [<ffffffff81316287>] write_sysrq_trigger+0x57/0x60
         [<ffffffff811ca996>] proc_reg_write+0x86/0xc0
         [<ffffffff8116dd8e>] vfs_write+0xce/0x190
         [<ffffffff8116e3e5>] sys_write+0x55/0x90
         [<ffffffff8150a242>] system_call_fastpath+0x16/0x1b
    
    That's because apic points to apic_x2apic_cluster or apic_x2apic_phys
    but the basic element like cpumask isn't initialized.
    
    Mask x2APIC feature in pvm to avoid overwrite of apic pointer,
    update commit message per Konrad's suggestion.
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Tested-by: Tamon Shiose <tamon.shiose@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 31e19f97fb5a..4bd0066a1ef0 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -429,6 +429,9 @@ static void __init xen_init_cpuid_mask(void)
 		cpuid_leaf1_edx_mask &=
 			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
 			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
+
+	cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_X2APIC % 32));
+
 	ax = 1;
 	cx = 0;
 	xen_cpuid(&ax, &bx, &cx, &dx);

commit d5b17dbff83d63fb6bf35daec21c8ebfb8d695b5
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 09:24:07 2013 -0400

    xen/smp/pvhvm: Don't point per_cpu(xen_vpcu, 33 and larger) to shared_info
    
    As it will point to some data, but not event channel data (the
    shared_info has an array limited to 32).
    
    This means that for PVHVM guests with more than 32 VCPUs without
    the usage of VCPUOP_register_info any interrupts to VCPUs
    larger than 32 would have gone unnoticed during early bootup.
    
    That is OK, as during early bootup, in smp_init we end up calling
    the hotplug mechanism (xen_hvm_cpu_notify) which makes the
    VCPUOP_register_vcpu_info call for all VCPUs and we can receive
    interrupts on VCPUs 33 and further.
    
    This is just a cleanup.
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a2babdb13a26..31e19f97fb5a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1646,6 +1646,9 @@ void __ref xen_hvm_init_shared_info(void)
 	 * online but xen_hvm_init_shared_info is run at resume time too and
 	 * in that case multiple vcpus might be online. */
 	for_each_online_cpu(cpu) {
+		/* Leave it to be NULL. */
+		if (cpu >= MAX_VIRT_CPUS)
+			continue;
 		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 	}
 }

commit a520996ae2e2792e1f90b74e67c974120c8a3b83
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 08:51:47 2013 -0400

    xen/vcpu: Document the xen_vcpu_info and xen_vcpu
    
    They are important structures and it is not clear at first
    look what they are for.
    
    The xen_vcpu is a pointer. By default it points to the shared_info
    structure (at the CPU offset location). However if the
    VCPUOP_register_vcpu_info hypercall is implemented we can make the
    xen_vcpu pointer point to a per-CPU location.
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    [v1: Added comments from Ian Campbell]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 94a81f41e8a2..a2babdb13a26 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -85,7 +85,29 @@
 
 EXPORT_SYMBOL_GPL(hypercall_page);
 
+/*
+ * Pointer to the xen_vcpu_info structure or
+ * &HYPERVISOR_shared_info->vcpu_info[cpu]. See xen_hvm_init_shared_info
+ * and xen_vcpu_setup for details. By default it points to share_info->vcpu_info
+ * but if the hypervisor supports VCPUOP_register_vcpu_info then it can point
+ * to xen_vcpu_info. The pointer is used in __xen_evtchn_do_upcall to
+ * acknowledge pending events.
+ * Also more subtly it is used by the patched version of irq enable/disable
+ * e.g. xen_irq_enable_direct and xen_iret in PV mode.
+ *
+ * The desire to be able to do those mask/unmask operations as a single
+ * instruction by using the per-cpu offset held in %gs is the real reason
+ * vcpu info is in a per-cpu pointer and the original reason for this
+ * hypercall.
+ *
+ */
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
+
+/*
+ * Per CPU pages used if hypervisor supports VCPUOP_register_vcpu_info
+ * hypercall. This can be used both in PV and PVHVM mode. The structure
+ * overrides the default per_cpu(xen_vcpu, cpu) value.
+ */
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 
 enum xen_domain_type xen_domain_type = XEN_NATIVE;
@@ -187,7 +209,12 @@ static void xen_vcpu_setup(int cpu)
 
 	/* Check to see if the hypervisor will put the vcpu_info
 	   structure where we want it, which allows direct access via
-	   a percpu-variable. */
+	   a percpu-variable.
+	   N.B. This hypercall can _only_ be called once per CPU. Subsequent
+	   calls will error out with -EINVAL. This is due to the fact that
+	   hypervisor has no unregister variant and this hypercall does not
+	   allow to over-write info.mfn and info.offset.
+	 */
 	err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, cpu, &info);
 
 	if (err) {

commit 7f1fc268c47491fd5e63548f6415fc8604e13003
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 09:30:09 2013 -0400

    xen/vcpu/pvhvm: Fix vcpu hotplugging hanging.
    
    If a user did:
    
            echo 0 > /sys/devices/system/cpu/cpu1/online
            echo 1 > /sys/devices/system/cpu/cpu1/online
    
    we would (this a build with DEBUG enabled) get to:
    smpboot: ++++++++++++++++++++=_---CPU UP  1
    .. snip..
    smpboot: Stack at about ffff880074c0ff44
    smpboot: CPU1: has booted.
    
    and hang. The RCU mechanism would kick in an try to IPI the CPU1
    but the IPIs (and all other interrupts) would never arrive at the
    CPU1. At first glance at least. A bit digging in the hypervisor
    trace shows that (using xenanalyze):
    
    [vla] d4v1 vec 243 injecting
       0.043163027 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043163639 --|x d4v1 vmentry cycles 1468
    ]  0.043164913 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043164913 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
       0.043164913 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043165526 --|x d4v1 vmentry cycles 1472
    ]  0.043166800 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043166800 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
    
    there is a pending event (subsequent debugging shows it is the IPI
    from the VCPU0 when smpboot.c on VCPU1 has done
    "set_cpu_online(smp_processor_id(), true)") and the guest VCPU1 is
    interrupted with the callback IPI (0xf3 aka 243) which ends up calling
    __xen_evtchn_do_upcall.
    
    The __xen_evtchn_do_upcall seems to do *something* but not acknowledge
    the pending events. And the moment the guest does a 'cli' (that is the
    ffffffff81673254 in the log above) the hypervisor is invoked again to
    inject the IPI (0xf3) to tell the guest it has pending interrupts.
    This repeats itself forever.
    
    The culprit was the per_cpu(xen_vcpu, cpu) pointer. At the bootup
    we set each per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] but later on use the VCPUOP_register_vcpu_info
    to register per-CPU  structures (xen_vcpu_setup).
    This is used to allow events for more than 32 VCPUs and for performance
    optimizations reasons.
    
    When the user performs the VCPU hotplug we end up calling the
    the xen_vcpu_setup once more. We make the hypercall which returns
    -EINVAL as it does not allow multiple registration calls (and
    already has re-assigned where the events are being set). We pick
    the fallback case and set per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] (which is a good fallback during bootup).
    However the hypervisor is still setting events in the register
    per-cpu structure (per_cpu(xen_vcpu_info, cpu)).
    
    As such when the events are set by the hypervisor (such as timer one),
    and when we iterate in __xen_evtchn_do_upcall we end up reading stale
    events from the shared_info->vcpu_info[vcpu] instead of the
    per_cpu(xen_vcpu_info, cpu) structures. Hence we never acknowledge the
    events that the hypervisor has set and the hypervisor keeps on reminding
    us to ack the events which we never do.
    
    The fix is simple. Don't on the second time when xen_vcpu_setup is
    called over-write the per_cpu(xen_vcpu, cpu) if it points to
    per_cpu(xen_vcpu_info).
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    CC: stable@vger.kernel.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ddbd54a9b845..94a81f41e8a2 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -157,6 +157,21 @@ static void xen_vcpu_setup(int cpu)
 
 	BUG_ON(HYPERVISOR_shared_info == &xen_dummy_shared_info);
 
+	/*
+	 * This path is called twice on PVHVM - first during bootup via
+	 * smp_init -> xen_hvm_cpu_notify, and then if the VCPU is being
+	 * hotplugged: cpu_up -> xen_hvm_cpu_notify.
+	 * As we can only do the VCPUOP_register_vcpu_info once lets
+	 * not over-write its result.
+	 *
+	 * For PV it is called during restore (xen_vcpu_restore) and bootup
+	 * (xen_setup_vcpu_info_placement). The hotplug mechanism does not
+	 * use this function.
+	 */
+	if (xen_hvm_domain()) {
+		if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
+			return;
+	}
 	if (cpu < MAX_VIRT_CPUS)
 		per_cpu(xen_vcpu,cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 

commit 1e2f5b598aa56c3978c2e623f72e9656a565c6c9
Merge: f9b3bcfbc43a 4d681be3c33d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:41:21 2013 -0700

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt update from Ingo Molnar:
     "Various paravirtualization related changes - the biggest one makes
      guest support optional via CONFIG_HYPERVISOR_GUEST"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, wakeup, sleep: Use pvops functions for changing GDT entries
      x86, xen, gdt: Remove the pvops variant of store_gdt.
      x86-32, gdt: Store/load GDT for ACPI S3 or hibernation/resume path is not needed
      x86-64, gdt: Store/load GDT for ACPI S3 or hibernate/resume path is not needed.
      x86: Make Linux guest support optional
      x86, Kconfig: Move PARAVIRT_DEBUG into the paravirt menu

commit 7918c92ae9638eb8a6ec18e2b4a0de84557cccc8
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 16 15:18:00 2013 -0400

    xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
    
    When we online the CPU, we get this splat:
    
    smpboot: Booting Node 0 Processor 1 APIC 0x2
    installing Xen timer for CPU 1
    BUG: sleeping function called from invalid context at /home/konrad/ssd/konrad/linux/mm/slab.c:3179
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/1
    Pid: 0, comm: swapper/1 Not tainted 3.9.0-rc6upstream-00001-g3884fad #1
    Call Trace:
     [<ffffffff810c1fea>] __might_sleep+0xda/0x100
     [<ffffffff81194617>] __kmalloc_track_caller+0x1e7/0x2c0
     [<ffffffff81303758>] ? kasprintf+0x38/0x40
     [<ffffffff813036eb>] kvasprintf+0x5b/0x90
     [<ffffffff81303758>] kasprintf+0x38/0x40
     [<ffffffff81044510>] xen_setup_timer+0x30/0xb0
     [<ffffffff810445af>] xen_hvm_setup_cpu_clockevents+0x1f/0x30
     [<ffffffff81666d0a>] start_secondary+0x19c/0x1a8
    
    The solution to that is use kasprintf in the CPU hotplug path
    that 'online's the CPU. That is, do it in in xen_hvm_cpu_notify,
    and remove the call to in xen_hvm_setup_cpu_clockevents.
    
    Unfortunatly the later is not a good idea as the bootup path
    does not use xen_hvm_cpu_notify so we would end up never allocating
    timer%d interrupt lines when booting. As such add the check for
    atomic() to continue.
    
    CC: stable@vger.kernel.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 47d32434d0b6..ddbd54a9b845 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1641,8 +1641,11 @@ static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
 	switch (action) {
 	case CPU_UP_PREPARE:
 		xen_vcpu_setup(cpu);
-		if (xen_have_vector_callback)
+		if (xen_have_vector_callback) {
 			xen_init_lock_cpu(cpu);
+			if (xen_feature(XENFEAT_hvm_safe_pvclock))
+				xen_setup_timer(cpu);
+		}
 		break;
 	default:
 		break;

commit 96f28bc66adb1414cfc9405ff80cfffdc44edd84
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Apr 3 17:31:50 2013 +0100

    x86/xen: populate boot_params with EDD data
    
    During early setup of a dom0 kernel, populate boot_params with the
    Enhanced Disk Drive (EDD) and MBR signature data.  This makes
    information on the BIOS boot device available in /sys/firmware/edd/.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c8e1c7b95c3b..47d32434d0b6 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -31,6 +31,7 @@
 #include <linux/pci.h>
 #include <linux/gfp.h>
 #include <linux/memblock.h>
+#include <linux/edd.h>
 
 #include <xen/xen.h>
 #include <xen/events.h>
@@ -1306,6 +1307,55 @@ static const struct machine_ops xen_machine_ops __initconst = {
 	.emergency_restart = xen_emergency_restart,
 };
 
+static void __init xen_boot_params_init_edd(void)
+{
+#if IS_ENABLED(CONFIG_EDD)
+	struct xen_platform_op op;
+	struct edd_info *edd_info;
+	u32 *mbr_signature;
+	unsigned nr;
+	int ret;
+
+	edd_info = boot_params.eddbuf;
+	mbr_signature = boot_params.edd_mbr_sig_buffer;
+
+	op.cmd = XENPF_firmware_info;
+
+	op.u.firmware_info.type = XEN_FW_DISK_INFO;
+	for (nr = 0; nr < EDDMAXNR; nr++) {
+		struct edd_info *info = edd_info + nr;
+
+		op.u.firmware_info.index = nr;
+		info->params.length = sizeof(info->params);
+		set_xen_guest_handle(op.u.firmware_info.u.disk_info.edd_params,
+				     &info->params);
+		ret = HYPERVISOR_dom0_op(&op);
+		if (ret)
+			break;
+
+#define C(x) info->x = op.u.firmware_info.u.disk_info.x
+		C(device);
+		C(version);
+		C(interface_support);
+		C(legacy_max_cylinder);
+		C(legacy_max_head);
+		C(legacy_sectors_per_track);
+#undef C
+	}
+	boot_params.eddbuf_entries = nr;
+
+	op.u.firmware_info.type = XEN_FW_DISK_MBR_SIGNATURE;
+	for (nr = 0; nr < EDD_MBR_SIG_MAX; nr++) {
+		op.u.firmware_info.index = nr;
+		ret = HYPERVISOR_dom0_op(&op);
+		if (ret)
+			break;
+		mbr_signature[nr] = op.u.firmware_info.u.disk_mbr_signature.mbr_signature;
+	}
+	boot_params.edd_mbr_sig_buf_entries = nr;
+#endif
+}
+
 /*
  * Set up the GDT and segment registers for -fstack-protector.  Until
  * we do this, we have to be careful not to call any stack-protected
@@ -1508,6 +1558,8 @@ asmlinkage void __init xen_start_kernel(void)
 		/* Avoid searching for BIOS MP tables */
 		x86_init.mpparse.find_smp_config = x86_init_noop;
 		x86_init.mpparse.get_smp_config = x86_init_uint_noop;
+
+		xen_boot_params_init_edd();
 	}
 #ifdef CONFIG_PCI
 	/* PCI BIOS service won't work from a PV guest. */

commit 357d122670937c35b33d99c46356ef2b63182a1f
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Apr 5 16:42:23 2013 -0400

    x86, xen, gdt: Remove the pvops variant of store_gdt.
    
    The two use-cases where we needed to store the GDT were during ACPI S3 suspend
    and resume. As the patches:
     x86/gdt/i386: store/load GDT for ACPI S3 or hibernation/resume path is not needed
     x86/gdt/64-bit: store/load GDT for ACPI S3 or hibernate/resume path is not needed.
    
    have demonstrated - there are other mechanism by which the GDT is
    saved and reloaded during early resume path.
    
    Hence we do not need to worry about the pvops call-chain for saving the
    GDT and can and can eliminate it. The other areas where the store_gdt is
    used are never going to be hit when running under the pvops platforms.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Link: http://lkml.kernel.org/r/1365194544-14648-4-git-send-email-konrad.wilk@oracle.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c8e1c7b95c3b..ddab5390d548 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1220,7 +1220,6 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.alloc_ldt = xen_alloc_ldt,
 	.free_ldt = xen_free_ldt,
 
-	.store_gdt = native_store_gdt,
 	.store_idt = native_store_idt,
 	.store_tr = xen_store_tr,
 

commit c79c49826270b8b0061b2fca840fc3f013c8a78a
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Feb 26 12:51:27 2013 -0500

    xen/pat: Disable PAT using pat_enabled value.
    
    The git commit 8eaffa67b43e99ae581622c5133e20b0f48bcef1
    (xen/pat: Disable PAT support for now) explains in details why
    we want to disable PAT for right now. However that
    change was not enough and we should have also disabled
    the pat_enabled value. Otherwise we end up with:
    
    mmap-example:3481 map pfn expected mapping type write-back for
    [mem 0x00010000-0x00010fff], got uncached-minus
     ------------[ cut here ]------------
    WARNING: at /build/buildd/linux-3.8.0/arch/x86/mm/pat.c:774 untrack_pfn+0xb8/0xd0()
    mem 0x00010000-0x00010fff], got uncached-minus
    ------------[ cut here ]------------
    WARNING: at /build/buildd/linux-3.8.0/arch/x86/mm/pat.c:774
    untrack_pfn+0xb8/0xd0()
    ...
    Pid: 3481, comm: mmap-example Tainted: GF 3.8.0-6-generic #13-Ubuntu
    Call Trace:
     [<ffffffff8105879f>] warn_slowpath_common+0x7f/0xc0
     [<ffffffff810587fa>] warn_slowpath_null+0x1a/0x20
     [<ffffffff8104bcc8>] untrack_pfn+0xb8/0xd0
     [<ffffffff81156c1c>] unmap_single_vma+0xac/0x100
     [<ffffffff81157459>] unmap_vmas+0x49/0x90
     [<ffffffff8115f808>] exit_mmap+0x98/0x170
     [<ffffffff810559a4>] mmput+0x64/0x100
     [<ffffffff810560f5>] dup_mm+0x445/0x660
     [<ffffffff81056d9f>] copy_process.part.22+0xa5f/0x1510
     [<ffffffff81057931>] do_fork+0x91/0x350
     [<ffffffff81057c76>] sys_clone+0x16/0x20
     [<ffffffff816ccbf9>] stub_clone+0x69/0x90
     [<ffffffff816cc89d>] ? system_call_fastpath+0x1a/0x1f
    ---[ end trace 4918cdd0a4c9fea4 ]---
    
    (a similar message shows up if you end up launching 'mcelog')
    
    The call chain is (as analyzed by Liu, Jinsong):
    do_fork
      --> copy_process
        --> dup_mm
          --> dup_mmap
            --> copy_page_range
              --> track_pfn_copy
                --> reserve_pfn_range
                  --> line 624: flags != want_flags
    It comes from different memory types of page table (_PAGE_CACHE_WB) and MTRR
    (_PAGE_CACHE_UC_MINUS).
    
    Stefan Bader dug in this deep and found out that:
    "That makes it clearer as this will do
    
    reserve_memtype(...)
    --> pat_x_mtrr_type
      --> mtrr_type_lookup
        --> __mtrr_type_lookup
    
    And that can return -1/0xff in case of MTRR not being enabled/initialized. Which
    is not the case (given there are no messages for it in dmesg). This is not equal
    to MTRR_TYPE_WRBACK and thus becomes _PAGE_CACHE_UC_MINUS.
    
    It looks like the problem starts early in reserve_memtype:
    
            if (!pat_enabled) {
                    /* This is identical to page table setting without PAT */
                    if (new_type) {
                            if (req_type == _PAGE_CACHE_WC)
                                    *new_type = _PAGE_CACHE_UC_MINUS;
                            else
                                    *new_type = req_type & _PAGE_CACHE_MASK;
                    }
                    return 0;
            }
    
    This would be what we want, that is clearing the PWT and PCD flags from the
    supported flags - if pat_enabled is disabled."
    
    This patch does that - disabling PAT.
    
    CC: stable@vger.kernel.org # 3.3 and further
    Reported-by: Sander Eikelenboom <linux@eikelenboom.it>
    Reported-and-Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reported-and-Tested-by: Stefan Bader <stefan.bader@canonical.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 39928d16be3b..c8e1c7b95c3b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -67,6 +67,7 @@
 #include <asm/hypervisor.h>
 #include <asm/mwait.h>
 #include <asm/pci_x86.h>
+#include <asm/pat.h>
 
 #ifdef CONFIG_ACPI
 #include <linux/acpi.h>
@@ -1417,7 +1418,14 @@ asmlinkage void __init xen_start_kernel(void)
 	 */
 	acpi_numa = -1;
 #endif
-
+#ifdef CONFIG_X86_PAT
+	/*
+	 * For right now disable the PAT. We should remove this once
+	 * git commit 8eaffa67b43e99ae581622c5133e20b0f48bcef1
+	 * (xen/pat: Disable PAT support for now) is reverted.
+	 */
+	pat_enabled = 0;
+#endif
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];

commit 5800700f66678ea5c85e7d62b138416070bf7f60
Merge: 266d7ad7f4fe af8d102f999a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 19 19:07:27 2013 -0800

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/apic changes from Ingo Molnar:
     "Main changes:
    
       - Multiple MSI support added to the APIC, PCI and AHCI code - acked
         by all relevant maintainers, by Alexander Gordeev.
    
         The advantage is that multiple AHCI ports can have multiple MSI
         irqs assigned, and can thus spread to multiple CPUs.
    
         [ Drivers can make use of this new facility via the
           pci_enable_msi_block_auto() method ]
    
       - x86 IOAPIC code from interrupt remapping cleanups from Joerg
         Roedel:
    
         These patches move all interrupt remapping specific checks out of
         the x86 core code and replaces the respective call-sites with
         function pointers.  As a result the interrupt remapping code is
         better abstraced from x86 core interrupt handling code.
    
       - Various smaller improvements, fixes and cleanups."
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      x86/intel/irq_remapping: Clean up x2apic opt-out security warning mess
      x86, kvm: Fix intialization warnings in kvm.c
      x86, irq: Move irq_remapped out of x86 core code
      x86, io_apic: Introduce eoi_ioapic_pin call-back
      x86, msi: Introduce x86_msi.compose_msi_msg call-back
      x86, irq: Introduce setup_remapped_irq()
      x86, irq: Move irq_remapped() check into free_remapped_irq
      x86, io-apic: Remove !irq_remapped() check from __target_IO_APIC_irq()
      x86, io-apic: Move CONFIG_IRQ_REMAP code out of x86 core
      x86, irq: Add data structure to keep AMD specific irq remapping information
      x86, irq: Move irq_remapping_enabled declaration to iommu code
      x86, io_apic: Remove irq_remapping_enabled check in setup_timer_IRQ0_pin
      x86, io_apic: Move irq_remapping_enabled checks out of check_timer()
      x86, io_apic: Convert setup_ioapic_entry to function pointer
      x86, io_apic: Introduce set_affinity function pointer
      x86, msi: Use IRQ remapping specific setup_msi_irqs routine
      x86, hpet: Introduce x86_msi_ops.setup_hpet_msi
      x86, io_apic: Introduce x86_io_apic_ops.print_entries for debugging
      x86, io_apic: Introduce x86_io_apic_ops.disable()
      x86, apic: Mask IO-APIC and PIC unconditionally on LAPIC resume
      ...

commit e9daff24a266307943457086533041bd971d0ef9
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Feb 14 21:29:31 2013 -0500

    Revert "xen PVonHVM: use E820_Reserved area for shared_info"
    
    This reverts commit 9d02b43dee0d7fb18dfb13a00915550b1a3daa9f.
    
    We are doing this b/c on 32-bit PVonHVM with older hypervisors
    (Xen 4.1) it ends up bothing up the start_info. This is bad b/c
    we use it for the time keeping, and the timekeeping code loops
    forever - as the version field never changes. Olaf says to
    revert it, so lets do that.
    
    Acked-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5fb3ec1e062e..e0140923062f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1517,72 +1517,51 @@ asmlinkage void __init xen_start_kernel(void)
 #endif
 }
 
-#ifdef CONFIG_XEN_PVHVM
-#define HVM_SHARED_INFO_ADDR 0xFE700000UL
-static struct shared_info *xen_hvm_shared_info;
-static unsigned long xen_hvm_sip_phys;
-static int xen_major, xen_minor;
-
-static void xen_hvm_connect_shared_info(unsigned long pfn)
+void __ref xen_hvm_init_shared_info(void)
 {
+	int cpu;
 	struct xen_add_to_physmap xatp;
+	static struct shared_info *shared_info_page = 0;
 
+	if (!shared_info_page)
+		shared_info_page = (struct shared_info *)
+			extend_brk(PAGE_SIZE, PAGE_SIZE);
 	xatp.domid = DOMID_SELF;
 	xatp.idx = 0;
 	xatp.space = XENMAPSPACE_shared_info;
-	xatp.gpfn = pfn;
+	xatp.gpfn = __pa(shared_info_page) >> PAGE_SHIFT;
 	if (HYPERVISOR_memory_op(XENMEM_add_to_physmap, &xatp))
 		BUG();
 
-}
-static void __init xen_hvm_set_shared_info(struct shared_info *sip)
-{
-	int cpu;
-
-	HYPERVISOR_shared_info = sip;
+	HYPERVISOR_shared_info = (struct shared_info *)shared_info_page;
 
 	/* xen_vcpu is a pointer to the vcpu_info struct in the shared_info
 	 * page, we use it in the event channel upcall and in some pvclock
 	 * related functions. We don't need the vcpu_info placement
 	 * optimizations because we don't use any pv_mmu or pv_irq op on
-	 * HVM. */
-	for_each_online_cpu(cpu)
+	 * HVM.
+	 * When xen_hvm_init_shared_info is run at boot time only vcpu 0 is
+	 * online but xen_hvm_init_shared_info is run at resume time too and
+	 * in that case multiple vcpus might be online. */
+	for_each_online_cpu(cpu) {
 		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
-}
-
-/* Reconnect the shared_info pfn to a (new) mfn */
-void xen_hvm_resume_shared_info(void)
-{
-	xen_hvm_connect_shared_info(xen_hvm_sip_phys >> PAGE_SHIFT);
-}
-
-/* Xen tools prior to Xen 4 do not provide a E820_Reserved area for guest usage.
- * On these old tools the shared info page will be placed in E820_Ram.
- * Xen 4 provides a E820_Reserved area at 0xFC000000, and this code expects
- * that nothing is mapped up to HVM_SHARED_INFO_ADDR.
- * Xen 4.3+ provides an explicit 1MB area at HVM_SHARED_INFO_ADDR which is used
- * here for the shared info page. */
-static void __init xen_hvm_init_shared_info(void)
-{
-	if (xen_major < 4) {
-		xen_hvm_shared_info = extend_brk(PAGE_SIZE, PAGE_SIZE);
-		xen_hvm_sip_phys = __pa(xen_hvm_shared_info);
-	} else {
-		xen_hvm_sip_phys = HVM_SHARED_INFO_ADDR;
-		set_fixmap(FIX_PARAVIRT_BOOTMAP, xen_hvm_sip_phys);
-		xen_hvm_shared_info =
-		(struct shared_info *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);
 	}
-	xen_hvm_connect_shared_info(xen_hvm_sip_phys >> PAGE_SHIFT);
-	xen_hvm_set_shared_info(xen_hvm_shared_info);
 }
 
+#ifdef CONFIG_XEN_PVHVM
 static void __init init_hvm_pv_info(void)
 {
+	int major, minor;
 	uint32_t eax, ebx, ecx, edx, pages, msr, base;
 	u64 pfn;
 
 	base = xen_cpuid_base();
+	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
+
+	major = eax >> 16;
+	minor = eax & 0xffff;
+	printk(KERN_INFO "Xen version %d.%d.\n", major, minor);
+
 	cpuid(base + 2, &pages, &msr, &ecx, &edx);
 
 	pfn = __pa(hypercall_page);
@@ -1633,22 +1612,12 @@ static void __init xen_hvm_guest_init(void)
 
 static bool __init xen_hvm_platform(void)
 {
-	uint32_t eax, ebx, ecx, edx, base;
-
 	if (xen_pv_domain())
 		return false;
 
-	base = xen_cpuid_base();
-	if (!base)
+	if (!xen_cpuid_base())
 		return false;
 
-	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
-
-	xen_major = eax >> 16;
-	xen_minor = eax & 0xffff;
-
-	printk(KERN_INFO "Xen version %d.%d.\n", xen_major, xen_minor);
-
 	return true;
 }
 

commit 5eb65be2d9a1f7c5e2b95aede16e7eab1cdb67e2
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Feb 14 21:29:27 2013 -0500

    Revert "xen/PVonHVM: fix compile warning in init_hvm_pv_info"
    
    This reverts commit a7be94ac8d69c037d08f0fd94b45a593f1d45176.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 138e5667409a..5fb3ec1e062e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1579,7 +1579,7 @@ static void __init xen_hvm_init_shared_info(void)
 
 static void __init init_hvm_pv_info(void)
 {
-	uint32_t ecx, edx, pages, msr, base;
+	uint32_t eax, ebx, ecx, edx, pages, msr, base;
 	u64 pfn;
 
 	base = xen_cpuid_base();

commit 4cca6ea04d31c22a7d0436949c072b27bde41f86
Author: Alok N Kataria <akataria@vmware.com>
Date:   Thu Jan 17 15:44:42 2013 -0800

    x86/apic: Allow x2apic without IR on VMware platform
    
    This patch updates x2apic initializaition code to allow x2apic
    on VMware platform even without interrupt remapping support.
    The hypervisor_x2apic_available hook was added in x2apic
    initialization code and used by KVM and XEN, before this.
    I have also cleaned up that code to export this hook through the
    hypervisor_x86 structure.
    
    Compile tested for KVM and XEN configs, this patch doesn't have
    any functional effect on those two platforms.
    
    On VMware platform, verified that x2apic is used in physical
    mode on products that support this.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Reviewed-by: Doug Covelli <dcovelli@vmware.com>
    Reviewed-by: Dan Hecht <dhecht@vmware.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Avi Kivity <avi@redhat.com>
    Link: http://lkml.kernel.org/r/1358466282.423.60.camel@akataria-dtop.eng.vmware.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 138e5667409a..8b4c56d85ca0 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1668,6 +1668,7 @@ const struct hypervisor_x86 x86_hyper_xen_hvm __refconst = {
 	.name			= "Xen HVM",
 	.detect			= xen_hvm_platform,
 	.init_platform		= xen_hvm_guest_init,
+	.x2apic_available	= xen_x2apic_para_available,
 };
 EXPORT_SYMBOL(x86_hyper_xen_hvm);
 #endif

commit 9d328a948f38ec240fc6d05db2c146e23ccd9b8b
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Thu Dec 13 15:33:05 2012 +0000

    xen/vcpu: Fix vcpu restore path.
    
    The runstate of vcpu should be restored for all possible cpus, as well as the
    vcpu info placement.
    
    Acked-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 3aeaa933b527..138e5667409a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -193,10 +193,11 @@ void xen_vcpu_restore(void)
 {
 	int cpu;
 
-	for_each_online_cpu(cpu) {
+	for_each_possible_cpu(cpu) {
 		bool other_cpu = (cpu != smp_processor_id());
+		bool is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up, cpu, NULL);
 
-		if (other_cpu &&
+		if (other_cpu && is_up &&
 		    HYPERVISOR_vcpu_op(VCPUOP_down, cpu, NULL))
 			BUG();
 
@@ -205,7 +206,7 @@ void xen_vcpu_restore(void)
 		if (have_vcpu_info_placement)
 			xen_vcpu_setup(cpu);
 
-		if (other_cpu &&
+		if (other_cpu && is_up &&
 		    HYPERVISOR_vcpu_op(VCPUOP_up, cpu, NULL))
 			BUG();
 	}

commit a7be94ac8d69c037d08f0fd94b45a593f1d45176
Author: Olaf Hering <olaf@aepfle.de>
Date:   Thu Nov 29 15:32:26 2012 +0100

    xen/PVonHVM: fix compile warning in init_hvm_pv_info
    
    After merging the xen-two tree, today's linux-next build (x86_64
    allmodconfig) produced this warning:
    
    arch/x86/xen/enlighten.c: In function 'init_hvm_pv_info':
    arch/x86/xen/enlighten.c:1617:16: warning: unused variable 'ebx' [-Wunused-variable]
    arch/x86/xen/enlighten.c:1617:11: warning: unused variable 'eax' [-Wunused-variable]
    
    Introduced by commit 9d02b43dee0d ("xen PVonHVM: use E820_Reserved area
    for shared_info").
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 3325cd9f7790..3aeaa933b527 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1578,7 +1578,7 @@ static void __init xen_hvm_init_shared_info(void)
 
 static void __init init_hvm_pv_info(void)
 {
-	uint32_t eax, ebx, ecx, edx, pages, msr, base;
+	uint32_t ecx, edx, pages, msr, base;
 	u64 pfn;
 
 	base = xen_cpuid_base();

commit 394b40f62d7ae18a1c48c13fc483b8193f8c3a98
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Nov 27 11:39:40 2012 -0500

    xen/acpi: Move the xen_running_on_version_or_later function.
    
    As on ia64 builds we get:
    include/xen/interface/version.h: In function 'xen_running_on_version_or_later':
    include/xen/interface/version.h:76: error: implicit declaration of function 'HYPERVISOR_xen_version'
    
    We can later on make this function exportable if there are
    modules using part of it. For right now the only two users are
    built-in.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index eb0edff5499b..3325cd9f7790 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -223,6 +223,21 @@ static void __init xen_banner(void)
 	       version >> 16, version & 0xffff, extra.extraversion,
 	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
 }
+/* Check if running on Xen version (major, minor) or later */
+bool
+xen_running_on_version_or_later(unsigned int major, unsigned int minor)
+{
+	unsigned int version;
+
+	if (!xen_domain())
+		return false;
+
+	version = HYPERVISOR_xen_version(XENVER_version, NULL);
+	if ((((version >> 16) == major) && ((version & 0xffff) >= minor)) ||
+		((version >> 16) > major))
+		return true;
+	return false;
+}
 
 #define CPUID_THERM_POWER_LEAF 6
 #define APERFMPERF_PRESENT 0

commit e3aa4e61b57da7574fdd1b4c9ca9bdee06a4d23e
Author: Liu, Jinsong <jinsong.liu@intel.com>
Date:   Thu Nov 8 05:44:28 2012 +0000

    xen/acpi: revert pad config check in xen_check_mwait
    
    With Xen acpi pad logic added into kernel, we can now revert xen mwait related
    patch df88b2d96e36d9a9e325bfcd12eb45671cbbc937 ("xen/enlighten: Disable
    MWAIT_LEAF so that acpi-pad won't be loaded. "). The reason is, when running under
    newer Xen platform, Xen pad driver would be early loaded, so native pad driver
    would fail to be loaded, and hence no mwait/monitor #UD risk again.
    
    Another point is, only Xen4.2 or later support Xen acpi pad, so we won't expose
    mwait cpuid capability when running under older Xen platform.
    
    Signed-off-by: Liu, Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a90c3bb58bed..eb0edff5499b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -287,8 +287,7 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 
 static bool __init xen_check_mwait(void)
 {
-#if defined(CONFIG_ACPI) && !defined(CONFIG_ACPI_PROCESSOR_AGGREGATOR) && \
-	!defined(CONFIG_ACPI_PROCESSOR_AGGREGATOR_MODULE)
+#ifdef CONFIG_ACPI
 	struct xen_platform_op op = {
 		.cmd			= XENPF_set_processor_pminfo,
 		.u.set_pminfo.id	= -1,
@@ -309,6 +308,13 @@ static bool __init xen_check_mwait(void)
 	if (!xen_initial_domain())
 		return false;
 
+	/*
+	 * When running under platform earlier than Xen4.2, do not expose
+	 * mwait, to avoid the risk of loading native acpi pad driver
+	 */
+	if (!xen_running_on_version_or_later(4, 2))
+		return false;
+
 	ax = 1;
 	cx = 0;
 

commit 9d02b43dee0d7fb18dfb13a00915550b1a3daa9f
Author: Olaf Hering <olaf@aepfle.de>
Date:   Thu Nov 1 22:02:30 2012 +0100

    xen PVonHVM: use E820_Reserved area for shared_info
    
    This is a respin of 00e37bdb0113a98408de42db85be002f21dbffd3
    ("xen PVonHVM: move shared_info to MMIO before kexec").
    
    Currently kexec in a PVonHVM guest fails with a triple fault because the
    new kernel overwrites the shared info page. The exact failure depends on
    the size of the kernel image. This patch moves the pfn from RAM into an
    E820 reserved memory area.
    
    The pfn containing the shared_info is located somewhere in RAM. This will
    cause trouble if the current kernel is doing a kexec boot into a new
    kernel. The new kernel (and its startup code) can not know where the pfn
    is, so it can not reserve the page. The hypervisor will continue to update
    the pfn, and as a result memory corruption occours in the new kernel.
    
    The toolstack marks the memory area FC000000-FFFFFFFF as reserved in the
    E820 map. Within that range newer toolstacks (4.3+) will keep 1MB
    starting from FE700000 as reserved for guest use. Older Xen4 toolstacks
    will usually not allocate areas up to FE700000, so FE700000 is expected
    to work also with older toolstacks.
    
    In Xen3 there is no reserved area at a fixed location. If the guest is
    started on such old hosts the shared_info page will be placed in RAM. As
    a result kexec can not be used.
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 586d83812b67..a90c3bb58bed 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1495,51 +1495,72 @@ asmlinkage void __init xen_start_kernel(void)
 #endif
 }
 
-void __ref xen_hvm_init_shared_info(void)
+#ifdef CONFIG_XEN_PVHVM
+#define HVM_SHARED_INFO_ADDR 0xFE700000UL
+static struct shared_info *xen_hvm_shared_info;
+static unsigned long xen_hvm_sip_phys;
+static int xen_major, xen_minor;
+
+static void xen_hvm_connect_shared_info(unsigned long pfn)
 {
-	int cpu;
 	struct xen_add_to_physmap xatp;
-	static struct shared_info *shared_info_page = 0;
 
-	if (!shared_info_page)
-		shared_info_page = (struct shared_info *)
-			extend_brk(PAGE_SIZE, PAGE_SIZE);
 	xatp.domid = DOMID_SELF;
 	xatp.idx = 0;
 	xatp.space = XENMAPSPACE_shared_info;
-	xatp.gpfn = __pa(shared_info_page) >> PAGE_SHIFT;
+	xatp.gpfn = pfn;
 	if (HYPERVISOR_memory_op(XENMEM_add_to_physmap, &xatp))
 		BUG();
 
-	HYPERVISOR_shared_info = (struct shared_info *)shared_info_page;
+}
+static void __init xen_hvm_set_shared_info(struct shared_info *sip)
+{
+	int cpu;
+
+	HYPERVISOR_shared_info = sip;
 
 	/* xen_vcpu is a pointer to the vcpu_info struct in the shared_info
 	 * page, we use it in the event channel upcall and in some pvclock
 	 * related functions. We don't need the vcpu_info placement
 	 * optimizations because we don't use any pv_mmu or pv_irq op on
-	 * HVM.
-	 * When xen_hvm_init_shared_info is run at boot time only vcpu 0 is
-	 * online but xen_hvm_init_shared_info is run at resume time too and
-	 * in that case multiple vcpus might be online. */
-	for_each_online_cpu(cpu) {
+	 * HVM. */
+	for_each_online_cpu(cpu)
 		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+}
+
+/* Reconnect the shared_info pfn to a (new) mfn */
+void xen_hvm_resume_shared_info(void)
+{
+	xen_hvm_connect_shared_info(xen_hvm_sip_phys >> PAGE_SHIFT);
+}
+
+/* Xen tools prior to Xen 4 do not provide a E820_Reserved area for guest usage.
+ * On these old tools the shared info page will be placed in E820_Ram.
+ * Xen 4 provides a E820_Reserved area at 0xFC000000, and this code expects
+ * that nothing is mapped up to HVM_SHARED_INFO_ADDR.
+ * Xen 4.3+ provides an explicit 1MB area at HVM_SHARED_INFO_ADDR which is used
+ * here for the shared info page. */
+static void __init xen_hvm_init_shared_info(void)
+{
+	if (xen_major < 4) {
+		xen_hvm_shared_info = extend_brk(PAGE_SIZE, PAGE_SIZE);
+		xen_hvm_sip_phys = __pa(xen_hvm_shared_info);
+	} else {
+		xen_hvm_sip_phys = HVM_SHARED_INFO_ADDR;
+		set_fixmap(FIX_PARAVIRT_BOOTMAP, xen_hvm_sip_phys);
+		xen_hvm_shared_info =
+		(struct shared_info *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);
 	}
+	xen_hvm_connect_shared_info(xen_hvm_sip_phys >> PAGE_SHIFT);
+	xen_hvm_set_shared_info(xen_hvm_shared_info);
 }
 
-#ifdef CONFIG_XEN_PVHVM
 static void __init init_hvm_pv_info(void)
 {
-	int major, minor;
 	uint32_t eax, ebx, ecx, edx, pages, msr, base;
 	u64 pfn;
 
 	base = xen_cpuid_base();
-	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
-
-	major = eax >> 16;
-	minor = eax & 0xffff;
-	printk(KERN_INFO "Xen version %d.%d.\n", major, minor);
-
 	cpuid(base + 2, &pages, &msr, &ecx, &edx);
 
 	pfn = __pa(hypercall_page);
@@ -1590,12 +1611,22 @@ static void __init xen_hvm_guest_init(void)
 
 static bool __init xen_hvm_platform(void)
 {
+	uint32_t eax, ebx, ecx, edx, base;
+
 	if (xen_pv_domain())
 		return false;
 
-	if (!xen_cpuid_base())
+	base = xen_cpuid_base();
+	if (!base)
 		return false;
 
+	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
+
+	xen_major = eax >> 16;
+	xen_minor = eax & 0xffff;
+
+	printk(KERN_INFO "Xen version %d.%d.\n", xen_major, xen_minor);
+
 	return true;
 }
 

commit e05dacd71db0a5da7c1a44bcaab2a8a240b9c233
Merge: 3ab0b83bf6a1 ddffeb8c4d03
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Oct 19 15:19:19 2012 -0400

    Merge commit 'v3.7-rc1' into stable/for-linus-3.7
    
    * commit 'v3.7-rc1': (10892 commits)
      Linux 3.7-rc1
      x86, boot: Explicitly include autoconf.h for hostprogs
      perf: Fix UAPI fallout
      ARM: config: make sure that platforms are ordered by option string
      ARM: config: sort select statements alphanumerically
      UAPI: (Scripted) Disintegrate include/linux/byteorder
      UAPI: (Scripted) Disintegrate include/linux
      UAPI: Unexport linux/blk_types.h
      UAPI: Unexport part of linux/ppp-comp.h
      perf: Handle new rbtree implementation
      procfs: don't need a PATH_MAX allocation to hold a string representation of an int
      vfs: embed struct filename inside of names_cache allocation if possible
      audit: make audit_inode take struct filename
      vfs: make path_openat take a struct filename pointer
      vfs: turn do_path_lookup into wrapper around struct filename variant
      audit: allow audit code to satisfy getname requests from its names_list
      vfs: define struct filename and have getname() return it
      btrfs: Fix compilation with user namespace support enabled
      userns: Fix posix_acl_file_xattr_userns gid conversion
      userns: Properly print bluetooth socket uids
      ...

commit c2103b7ef7ecb4d17138b6ced4aad1b576affc4e
Author: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
Date:   Sun Oct 7 22:06:47 2012 +0800

    xen/x86: remove duplicated include from enlighten.c
    
    Remove duplicated include.
    
    dpatch engine is used to auto generate this patch.
    (https://github.com/weiyj/dpatch)
    
    CC: stable@vger.kernel.org
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 4466feb4c69b..783522b66b0f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -81,8 +81,6 @@
 #include "smp.h"
 #include "multicalls.h"
 
-#include <xen/events.h>
-
 EXPORT_SYMBOL_GPL(hypercall_page);
 
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);

commit ccff9b1db693062b0a9c9070f4304deb47ef215c
Merge: 3dc329baa2ed cb6b6df111e4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 12 22:20:28 2012 +0900

    Merge tag 'stable/for-linus-3.7-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen fixes from Konrad Rzeszutek Wilk:
     "This has four bug-fixes and one tiny feature that I forgot to put
      initially in my tree due to oversight.
    
      The feature is for kdump kernels to speed up the /proc/vmcore reading.
      There is a ram_is_pfn helper function that the different platforms can
      register for.  We are now doing that.
    
      The bug-fixes cover some embarrassing struct pv_cpu_ops variables
      being set to NULL on Xen (but not baremetal).  We had a similar issue
      in the past with {write|read}_msr_safe and this fills the three
      missing ones.  The other bug-fix is to make the console output (hvc)
      be capable of dealing with misbehaving backends and not fall flat on
      its face.  Lastly, a quirk for older XenBus implementations that came
      with an ancient v3.4 hypervisor (so RHEL5 based) - reading of certain
      non-existent attributes just hangs the guest during bootup - so we
      take precaution of not doing that on such older installations.
    
      Feature:
       - Register a pfn_is_ram helper to speed up reading of /proc/vmcore.
      Bug-fixes:
       - Three pvops call for Xen were undefined causing BUG_ONs.
       - Add a quirk so that the shutdown watches (used by kdump) are not
         used with older Xen (3.4).
       - Fix ungraceful state transition for the HVC console."
    
    * tag 'stable/for-linus-3.7-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/pv-on-hvm kexec: add quirk for Xen 3.4 and shutdown watches.
      xen/bootup: allow {read|write}_cr8 pvops call.
      xen/bootup: allow read_tscp call for Xen PV guests.
      xen pv-on-hvm: add pfn_is_ram helper for kdump
      xen/hvc: handle backend CLOSED without CLOSING

commit 1a7bbda5b1ab0e02622761305a32dc38735b90b2
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Oct 10 13:25:48 2012 -0400

    xen/bootup: allow {read|write}_cr8 pvops call.
    
    We actually do not do anything about it. Just return a default
    value of zero and if the kernel tries to write anything but 0
    we BUG_ON.
    
    This fixes the case when an user tries to suspend the machine
    and it blows up in save_processor_state b/c 'read_cr8' is set
    to NULL and we get:
    
    kernel BUG at /home/konrad/ssd/linux/arch/x86/include/asm/paravirt.h:100!
    invalid opcode: 0000 [#1] SMP
    Pid: 2687, comm: init.late Tainted: G           O 3.6.0upstream-00002-gac264ac-dirty #4 Bochs Bochs
    RIP: e030:[<ffffffff814d5f42>]  [<ffffffff814d5f42>] save_processor_state+0x212/0x270
    
    .. snip..
    Call Trace:
     [<ffffffff810733bf>] do_suspend_lowlevel+0xf/0xac
     [<ffffffff8107330c>] ? x86_acpi_suspend_lowlevel+0x10c/0x150
     [<ffffffff81342ee2>] acpi_suspend_enter+0x57/0xd5
    
    CC: stable@vger.kernel.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 668bbfee0cce..4466feb4c69b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -987,7 +987,16 @@ static void xen_write_cr4(unsigned long cr4)
 
 	native_write_cr4(cr4);
 }
-
+#ifdef CONFIG_X86_64
+static inline unsigned long xen_read_cr8(void)
+{
+	return 0;
+}
+static inline void xen_write_cr8(unsigned long val)
+{
+	BUG_ON(val);
+}
+#endif
 static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 {
 	int ret;
@@ -1156,6 +1165,11 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.read_cr4_safe = native_read_cr4_safe,
 	.write_cr4 = xen_write_cr4,
 
+#ifdef CONFIG_X86_64
+	.read_cr8 = xen_read_cr8,
+	.write_cr8 = xen_write_cr8,
+#endif
+
 	.wbinvd = native_wbinvd,
 
 	.read_msr = native_read_msr_safe,

commit cd0608e71e9757f4dae35bcfb4e88f4d1a03a8ab
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Oct 10 13:30:47 2012 -0400

    xen/bootup: allow read_tscp call for Xen PV guests.
    
    The hypervisor will trap it. However without this patch,
    we would crash as the .read_tscp is set to NULL. This patch
    fixes it and sets it to the native_read_tscp call.
    
    CC: stable@vger.kernel.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 70f140447a28..668bbfee0cce 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1164,6 +1164,8 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.read_tsc = native_read_tsc,
 	.read_pmc = native_read_pmc,
 
+	.read_tscp = native_read_tscp,
+
 	.iret = xen_iret,
 	.irq_enable_sysexit = xen_sysexit,
 #ifdef CONFIG_X86_64

commit f1c6872e4980bc4078cfaead05f892b3d78dea64
Merge: 5f3d2f2e1a63 4ed5978bdd99
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 7 07:13:01 2012 +0900

    Merge tag 'stable/for-linus-3.7-arm-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull ADM Xen support from Konrad Rzeszutek Wilk:
    
      Features:
       * Allow a Linux guest to boot as initial domain and as normal guests
         on Xen on ARM (specifically ARMv7 with virtualized extensions).  PV
         console, block and network frontend/backends are working.
      Bug-fixes:
       * Fix compile linux-next fallout.
       * Fix PVHVM bootup crashing.
    
      The Xen-unstable hypervisor (so will be 4.3 in a ~6 months), supports
      ARMv7 platforms.
    
      The goal in implementing this architecture is to exploit the hardware
      as much as possible.  That means use as little as possible of PV
      operations (so no PV MMU) - and use existing PV drivers for I/Os
      (network, block, console, etc).  This is similar to how PVHVM guests
      operate in X86 platform nowadays - except that on ARM there is no need
      for QEMU.  The end result is that we share a lot of the generic Xen
      drivers and infrastructure.
    
      Details on how to compile/boot/etc are available at this Wiki:
    
        http://wiki.xen.org/wiki/Xen_ARMv7_with_Virtualization_Extensions
    
      and this blog has links to a technical discussion/presentations on the
      overall architecture:
    
        http://blog.xen.org/index.php/2012/09/21/xensummit-sessions-new-pvh-virtualisation-mode-for-arm-cortex-a15arm-servers-and-x86/
    
    * tag 'stable/for-linus-3.7-arm-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen: (21 commits)
      xen/xen_initial_domain: check that xen_start_info is initialized
      xen: mark xen_init_IRQ __init
      xen/Makefile: fix dom-y build
      arm: introduce a DTS for Xen unprivileged virtual machines
      MAINTAINERS: add myself as Xen ARM maintainer
      xen/arm: compile netback
      xen/arm: compile blkfront and blkback
      xen/arm: implement alloc/free_xenballooned_pages with alloc_pages/kfree
      xen/arm: receive Xen events on ARM
      xen/arm: initialize grant_table on ARM
      xen/arm: get privilege status
      xen/arm: introduce CONFIG_XEN on ARM
      xen: do not compile manage, balloon, pci, acpi, pcpu and cpu_hotplug on ARM
      xen/arm: Introduce xen_ulong_t for unsigned long
      xen/arm: Xen detection and shared_info page mapping
      docs: Xen ARM DT bindings
      xen/arm: empty implementation of grant_table arch specific functions
      xen/arm: sync_bitops
      xen/arm: page.h definitions
      xen/arm: hypercalls
      ...

commit 56d92aa5cf7c96c70f81d0350c94faf46a9fb76d
Merge: 33c2a174120b c341ca45ce56
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 22:09:10 2012 -0700

    Merge tag 'stable/for-linus-3.7-x86-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen update from Konrad Rzeszutek Wilk:
     "Features:
       - When hotplugging PCI devices in a PV guest we can allocate
         Xen-SWIOTLB later.
       - Cleanup Xen SWIOTLB.
       - Support pages out grants from HVM domains in the backends.
       - Support wild cards in xen-pciback.hide=(BDF) arguments.
       - Update grant status updates with upstream hypervisor.
       - Boot PV guests with more than 128GB.
       - Cleanup Xen MMU code/add comments.
       - Obtain XENVERS using a preferred method.
       - Lay out generic changes to support Xen ARM.
       - Allow privcmd ioctl for HVM (used to do only PV).
       - Do v2 of mmap_batch for privcmd ioctls.
       - If hypervisor saves the LED keyboard light - we will now instruct
         the kernel about its state.
      Fixes:
       - More fixes to Xen PCI backend for various calls/FLR/etc.
       - With more than 4GB in a 64-bit PV guest disable native SWIOTLB.
       - Fix up smatch warnings.
       - Fix up various return values in privmcmd and mm."
    
    * tag 'stable/for-linus-3.7-x86-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen: (48 commits)
      xen/pciback: Restore the PCI config space after an FLR.
      xen-pciback: properly clean up after calling pcistub_device_find()
      xen/vga: add the xen EFI video mode support
      xen/x86: retrieve keyboard shift status flags from hypervisor.
      xen/gndev: Xen backend support for paged out grant targets V4.
      xen-pciback: support wild cards in slot specifications
      xen/swiotlb: Fix compile warnings when using plain integer instead of NULL pointer.
      xen/swiotlb: Remove functions not needed anymore.
      xen/pcifront: Use Xen-SWIOTLB when initting if required.
      xen/swiotlb: For early initialization, return zero on success.
      xen/swiotlb: Use the swiotlb_late_init_with_tbl to init Xen-SWIOTLB late when PV PCI is used.
      xen/swiotlb: Move the error strings to its own function.
      xen/swiotlb: Move the nr_tbl determination in its own function.
      xen/arm: compile and run xenbus
      xen: resynchronise grant table status codes with upstream
      xen/privcmd: return -EFAULT on error
      xen/privcmd: Fix mmap batch ioctl error status copy back.
      xen/privcmd: add PRIVCMD_MMAPBATCH_V2 ioctl
      xen/mm: return more precise error from xen_remap_domain_range()
      xen/mmu: If the revector fails, don't attempt to revector anything else.
      ...

commit ae1659ee6b4bf5e12a141d04636a3de89543c237
Merge: c341ca45ce56 bbd6eb29214e
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Sep 26 16:42:14 2012 -0400

    Merge branch 'xenarm-for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm into stable/for-linus-3.7
    
    * 'xenarm-for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm:
      arm: introduce a DTS for Xen unprivileged virtual machines
      MAINTAINERS: add myself as Xen ARM maintainer
      xen/arm: compile netback
      xen/arm: compile blkfront and blkback
      xen/arm: implement alloc/free_xenballooned_pages with alloc_pages/kfree
      xen/arm: receive Xen events on ARM
      xen/arm: initialize grant_table on ARM
      xen/arm: get privilege status
      xen/arm: introduce CONFIG_XEN on ARM
      xen: do not compile manage, balloon, pci, acpi, pcpu and cpu_hotplug on ARM
      xen/arm: Introduce xen_ulong_t for unsigned long
      xen/arm: Xen detection and shared_info page mapping
      docs: Xen ARM DT bindings
      xen/arm: empty implementation of grant_table arch specific functions
      xen/arm: sync_bitops
      xen/arm: page.h definitions
      xen/arm: hypercalls
      arm: initial Xen support
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit ffb8b233c2261b7978dc3bd759aaa19bd1a7fadf
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Sep 21 12:30:35 2012 -0400

    xen/x86: retrieve keyboard shift status flags from hypervisor.
    
    The xen c/s 25873 allows the hypervisor to retrieve the NUMLOCK flag.
    With this patch, the Linux kernel can get the state according to the
    data in the BIOS.
    
    Acked-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 47b3acdc2ac5..67897152237c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1440,11 +1440,19 @@ asmlinkage void __init xen_start_kernel(void)
 		const struct dom0_vga_console_info *info =
 			(void *)((char *)xen_start_info +
 				 xen_start_info->console.dom0.info_off);
+		struct xen_platform_op op = {
+			.cmd = XENPF_firmware_info,
+			.interface_version = XENPF_INTERFACE_VERSION,
+			.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS,
+		};
 
 		xen_init_vga(info, xen_start_info->console.dom0.info_size);
 		xen_start_info->console.domU.mfn = 0;
 		xen_start_info->console.domU.evtchn = 0;
 
+		if (HYPERVISOR_dom0_op(&op) == 0)
+			boot_params.kbd_status = op.u.firmware_info.u.kbd_shift_flags;
+
 		xen_init_apic();
 
 		/* Make sure ACS will be enabled */

commit bd49940a35ec7d488ae63bd625639893b3385b97
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Sep 19 08:30:55 2012 -0400

    xen/boot: Disable BIOS SMP MP table search.
    
    As the initial domain we are able to search/map certain regions
    of memory to harvest configuration data. For all low-level we
    use ACPI tables - for interrupts we use exclusively ACPI _PRT
    (so DSDT) and MADT for INT_SRC_OVR.
    
    The SMP MP table is not used at all. As a matter of fact we do
    not even support machines that only have SMP MP but no ACPI tables.
    
    Lets follow how Moorestown does it and just disable searching
    for BIOS SMP tables.
    
    This also fixes an issue on HP Proliant BL680c G5 and DL380 G6:
    
    9f->100 for 1:1 PTE
    Freeing 9f-100 pfn range: 97 pages freed
    1-1 mapping on 9f->100
    .. snip..
    e820: BIOS-provided physical RAM map:
    Xen: [mem 0x0000000000000000-0x000000000009efff] usable
    Xen: [mem 0x000000000009f400-0x00000000000fffff] reserved
    Xen: [mem 0x0000000000100000-0x00000000cfd1dfff] usable
    .. snip..
    Scan for SMP in [mem 0x00000000-0x000003ff]
    Scan for SMP in [mem 0x0009fc00-0x0009ffff]
    Scan for SMP in [mem 0x000f0000-0x000fffff]
    found SMP MP-table at [mem 0x000f4fa0-0x000f4faf] mapped at [ffff8800000f4fa0]
    (XEN) mm.c:908:d0 Error getting mfn 100 (pfn 5555555555555555) from L1 entry 0000000000100461 for l1e_owner=0, pg_owner=0
    (XEN) mm.c:4995:d0 ptwr_emulate: could not get_page_from_l1e()
    BUG: unable to handle kernel NULL pointer dereference at           (null)
    IP: [<ffffffff81ac07e2>] xen_set_pte_init+0x66/0x71
    . snip..
    Pid: 0, comm: swapper Not tainted 3.6.0-rc6upstream-00188-gb6fb969-dirty #2 HP ProLiant BL680c G5
    .. snip..
    Call Trace:
     [<ffffffff81ad31c6>] __early_ioremap+0x18a/0x248
     [<ffffffff81624731>] ? printk+0x48/0x4a
     [<ffffffff81ad32ac>] early_ioremap+0x13/0x15
     [<ffffffff81acc140>] get_mpc_size+0x2f/0x67
     [<ffffffff81acc284>] smp_scan_config+0x10c/0x136
     [<ffffffff81acc2e4>] default_find_smp_config+0x36/0x5a
     [<ffffffff81ac3085>] setup_arch+0x5b3/0xb5b
     [<ffffffff81624731>] ? printk+0x48/0x4a
     [<ffffffff81abca7f>] start_kernel+0x90/0x390
     [<ffffffff81abc356>] x86_64_start_reservations+0x131/0x136
     [<ffffffff81abfa83>] xen_start_kernel+0x65f/0x661
    (XEN) Domain 0 crashed: 'noreboot' set - not rebooting.
    
    which is that ioremap would end up mapping 0xff using _PAGE_IOMAP
    (which is what early_ioremap sticks as a flag) - which meant
    we would get MFN 0xFF (pte ff461, which is OK), and then it would
    also map 0x100 (b/c ioremap tries to get page aligned request, and
    it was trying to map 0xf4fa0 + PAGE_SIZE - so it mapped the next page)
    as _PAGE_IOMAP. Since 0x100 is actually a RAM page, and the _PAGE_IOMAP
    bypasses the P2M lookup we would happily set the PTE to 1000461.
    Xen would deny the request since we do not have access to the
    Machine Frame Number (MFN) of 0x100. The P2M[0x100] is for example
    0x80140.
    
    CC: stable@vger.kernel.org
    Fixes-Oracle-Bugzilla: https://bugzilla.oracle.com/bugzilla/show_bug.cgi?id=13665
    Acked-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9642d4a38602..1fbe75a95f15 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1452,6 +1452,10 @@ asmlinkage void __init xen_start_kernel(void)
 		pci_request_acs();
 
 		xen_acpi_sleep_register();
+
+		/* Avoid searching for BIOS MP tables */
+		x86_init.mpparse.find_smp_config = x86_init_noop;
+		x86_init.mpparse.get_smp_config = x86_init_uint_noop;
 	}
 #ifdef CONFIG_PCI
 	/* PCI BIOS service won't work from a PV guest. */

commit 98104c3480e568d9c145adbc7dc56c9d4d170e30
Merge: 25a765b7f05c 328731876451
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Sep 12 11:16:27 2012 -0400

    Merge branch 'stable/128gb.v5.1' into stable/for-linus-3.7
    
    * stable/128gb.v5.1:
      xen/mmu: If the revector fails, don't attempt to revector anything else.
      xen/p2m: When revectoring deal with holes in the P2M array.
      xen/mmu: Release just the MFN list, not MFN list and part of pagetables.
      xen/mmu: Remove from __ka space PMD entries for pagetables.
      xen/mmu: Copy and revector the P2M tree.
      xen/p2m: Add logic to revector a P2M tree to use __va leafs.
      xen/mmu: Recycle the Xen provided L4, L3, and L2 pages
      xen/mmu: For 64-bit do not call xen_map_identity_early
      xen/mmu: use copy_page instead of memcpy.
      xen/mmu: Provide comments describing the _ka and _va aliasing issue
      xen/mmu: The xen_setup_kernel_pagetable doesn't need to return anything.
      Revert "xen/x86: Workaround 64-bit hypervisor and 32-bit initial domain." and "xen/x86: Use memblock_reserve for sensitive areas."
      xen/x86: Workaround 64-bit hypervisor and 32-bit initial domain.
      xen/x86: Use memblock_reserve for sensitive areas.
      xen/p2m: Fix the comment describing the P2M tree.
    
    Conflicts:
            arch/x86/xen/mmu.c
    
    The pagetable_init is the old xen_pagetable_setup_done and xen_pagetable_setup_start
    rolled in one.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 25a765b7f05cb8460fa01b54568894b20e184862
Merge: 9d2be9287107 64282278989d
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Sep 12 11:14:33 2012 -0400

    Merge branch 'x86/platform' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into stable/for-linus-3.7
    
    * 'x86/platform' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (9690 commits)
      x86: Document x86_init.paging.pagetable_init()
      x86: xen: Cleanup and remove x86_init.paging.pagetable_setup_done()
      x86: Move paging_init() call to x86_init.paging.pagetable_init()
      x86: Rename pagetable_setup_start() to pagetable_init()
      x86: Remove base argument from x86_init.paging.pagetable_setup_start
      Linux 3.6-rc5
      HID: tpkbd: work even if the new Lenovo Keyboard driver is not configured
      Remove user-triggerable BUG from mpol_to_str
      xen/pciback: Fix proper FLR steps.
      uml: fix compile error in deliver_alarm()
      dj: memory scribble in logi_dj
      Fix order of arguments to compat_put_time[spec|val]
      xen: Use correct masking in xen_swiotlb_alloc_coherent.
      xen: fix logical error in tlb flushing
      xen/p2m: Fix one-off error in checking the P2M tree directory.
      powerpc: Don't use __put_user() in patch_instruction
      powerpc: Make sure IPI handlers see data written by IPI senders
      powerpc: Restore correct DSCR in context switch
      powerpc: Fix DSCR inheritance in copy_thread()
      powerpc: Keep thread.dscr and thread.dscr_inherit in sync
      ...

commit 593d0a3e9f813db910dc50574532914db21d09ff
Merge: 50e900417b80 4cb38750d490
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Sep 5 10:22:45 2012 -0400

    Merge commit '4cb38750d49010ae72e718d46605ac9ba5a851b4' into stable/for-linus-3.6
    
    * commit '4cb38750d49010ae72e718d46605ac9ba5a851b4': (6849 commits)
      bcma: fix invalid PMU chip control masks
      [libata] pata_cmd64x: whitespace cleanup
      libata-acpi: fix up for acpi_pm_device_sleep_state API
      sata_dwc_460ex: device tree may specify dma_channel
      ahci, trivial: fixed coding style issues related to braces
      ahci_platform: add hibernation callbacks
      libata-eh.c: local functions should not be exposed globally
      libata-transport.c: local functions should not be exposed globally
      sata_dwc_460ex: support hardreset
      ata: use module_pci_driver
      drivers/ata/pata_pcmcia.c: adjust suspicious bit operation
      pata_imx: Convert to clk_prepare_enable/clk_disable_unprepare
      ahci: Enable SB600 64bit DMA on MSI K9AGM2 (MS-7327) v2
      [libata] Prevent interface errors with Seagate FreeAgent GoFlex
      drivers/acpi/glue: revert accidental license-related 6b66d95895c bits
      libata-acpi: add missing inlines in libata.h
      i2c-omap: Add support for I2C_M_STOP message flag
      i2c: Fall back to emulated SMBus if the operation isn't supported natively
      i2c: Add SCCB support
      i2c-tiny-usb: Add support for the Robofuzz OSIF USB/I2C converter
      ...

commit 267560874cb0189f28e7ae6dfbc8e98b8848be98
Merge: 4ae4614712e0 c96aae1f7f39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Aug 25 17:31:59 2012 -0700

    Merge tag 'stable/for-linus-3.6-rc3-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull three xen bug-fixes from Konrad Rzeszutek Wilk:
     - Revert the kexec fix which caused on non-kexec shutdowns a race.
     - Reuse existing P2M leafs - instead of requiring to allocate a large
       area of bootup virtual address estate.
     - Fix a one-off error when adding PFNs for balloon pages.
    
    * tag 'stable/for-linus-3.6-rc3-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/setup: Fix one-off error when adding for-balloon PFNs to the P2M.
      xen/p2m: Reuse existing P2M leafs if they are filled with 1:1 PFNs or INVALID.
      Revert "xen PVonHVM: move shared_info to MMIO before kexec"

commit 3699aad047e16a5775b1d051425f422a9384270d
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Jun 28 22:47:35 2012 -0400

    xen/mmu: The xen_setup_kernel_pagetable doesn't need to return anything.
    
    We don't need to return the new PGD - as we do not use it.
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ff962d4b821e..d87a038a0484 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1258,7 +1258,6 @@ asmlinkage void __init xen_start_kernel(void)
 {
 	struct physdev_set_iopl set_iopl;
 	int rc;
-	pgd_t *pgd;
 
 	if (!xen_start_info)
 		return;
@@ -1350,8 +1349,6 @@ asmlinkage void __init xen_start_kernel(void)
 	acpi_numa = -1;
 #endif
 
-	pgd = (pgd_t *)xen_start_info->pt_base;
-
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
@@ -1360,7 +1357,7 @@ asmlinkage void __init xen_start_kernel(void)
 	early_boot_irqs_disabled = true;
 
 	xen_raw_console_write("mapping kernel into physical memory\n");
-	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
+	xen_setup_kernel_pagetable((pgd_t *)xen_start_info->pt_base, xen_start_info->nr_pages);
 
 	/* Allocate and initialize top and mid mfn levels for p2m structure */
 	xen_build_mfn_list_list();

commit 51faaf2b0d5c7f44d82964f0c70b1c4e44d4e633
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Aug 22 13:00:10 2012 -0400

    Revert "xen/x86: Workaround 64-bit hypervisor and 32-bit initial domain." and "xen/x86: Use memblock_reserve for sensitive areas."
    
    This reverts commit 806c312e50f122c47913145cf884f53dd09d9199 and
    commit 59b294403e9814e7c1154043567f0d71bac7a511.
    
    And also documents setup.c and why we want to do it that way, which
    is that we tried to make the the memblock_reserve more selective so
    that it would be clear what region is reserved. Sadly we ran
    in the problem wherein on a 64-bit hypervisor with a 32-bit
    initial domain, the pt_base has the cr3 value which is not
    neccessarily where the pagetable starts! As Jan put it: "
    Actually, the adjustment turns out to be correct: The page
    tables for a 32-on-64 dom0 get allocated in the order "first L1",
    "first L2", "first L3", so the offset to the page table base is
    indeed 2. When reading xen/include/public/xen.h's comment
    very strictly, this is not a violation (since there nothing is said
    that the first thing in the page table space is pointed to by
    pt_base; I admit that this seems to be implied though, namely
    do I think that it is implied that the page table space is the
    range [pt_base, pt_base + nt_pt_frames), whereas that
    range here indeed is [pt_base - 2, pt_base - 2 + nt_pt_frames),
    which - without a priori knowledge - the kernel would have
    difficulty to figure out)." - so lets just fall back to the
    easy way and reserve the whole region.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 511f92d79e4a..ff962d4b821e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -998,66 +998,7 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 
 	return ret;
 }
-/*
- * If the MFN is not in the m2p (provided to us by the hypervisor) this
- * function won't do anything. In practice this means that the XenBus
- * MFN won't be available for the initial domain. */
-static unsigned long __init xen_reserve_mfn(unsigned long mfn)
-{
-	unsigned long pfn, end_pfn = 0;
-
-	if (!mfn)
-		return end_pfn;
-
-	pfn = mfn_to_pfn(mfn);
-	if (phys_to_machine_mapping_valid(pfn)) {
-		end_pfn = PFN_PHYS(pfn) + PAGE_SIZE;
-		memblock_reserve(PFN_PHYS(pfn), end_pfn);
-	}
-	return end_pfn;
-}
-static void __init xen_reserve_internals(void)
-{
-	unsigned long size;
-	unsigned long last_phys = 0;
-
-	if (!xen_pv_domain())
-		return;
-
-	/* xen_start_info does not exist in the M2P, hence can't use
-	 * xen_reserve_mfn. */
-	memblock_reserve(__pa(xen_start_info), PAGE_SIZE);
-	last_phys = __pa(xen_start_info) + PAGE_SIZE;
-
-	last_phys = max(xen_reserve_mfn(PFN_DOWN(xen_start_info->shared_info)), last_phys);
-	last_phys = max(xen_reserve_mfn(xen_start_info->store_mfn), last_phys);
 
-	if (!xen_initial_domain())
-		last_phys = max(xen_reserve_mfn(xen_start_info->console.domU.mfn), last_phys);
-
-	if (xen_feature(XENFEAT_auto_translated_physmap))
-		return;
-
-	/*
-	 * ALIGN up to compensate for the p2m_page pointing to an array that
-	 * can partially filled (look in xen_build_dynamic_phys_to_machine).
-	 */
-
-	size = PAGE_ALIGN(xen_start_info->nr_pages * sizeof(unsigned long));
-
-	/* We could use xen_reserve_mfn here, but would end up looping quite
-	 * a lot (and call memblock_reserve for each PAGE), so lets just use
-	 * the easy way and reserve it wholesale. */
-	memblock_reserve(__pa(xen_start_info->mfn_list), size);
-	last_phys = max(__pa(xen_start_info->mfn_list) + size, last_phys);
-	/* The pagetables are reserved in mmu.c */
-
-	/* Under 64-bit hypervisor with a 32-bit domain, the hypervisor
-	 * offsets the pt_base by two pages. Hence the reservation that is done
-	 * in mmu.c misses two pages. We correct it here if we detect this. */
-	if (last_phys < __pa(xen_start_info->pt_base))
-		memblock_reserve(last_phys, __pa(xen_start_info->pt_base) - last_phys);
-}
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
@@ -1421,7 +1362,6 @@ asmlinkage void __init xen_start_kernel(void)
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
 
-	xen_reserve_internals();
 	/* Allocate and initialize top and mid mfn levels for p2m structure */
 	xen_build_mfn_list_list();
 

commit b8b0f559c7b1dcf5503817e518c81c9a18ee45e0
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Aug 21 14:49:34 2012 -0400

    xen/apic/xenbus/swiotlb/pcifront/grant/tmem: Make functions or variables static.
    
    There is no need for those functions/variables to be visible. Make them
    static and also fix the compile warnings of this sort:
    
    drivers/xen/<some file>.c: warning: symbol '<blah>' was not declared. Should it be static?
    
    Some of them just require including the header file that
    declares the functions.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ff962d4b821e..cb1b1914dbd3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -79,6 +79,8 @@
 #include "smp.h"
 #include "multicalls.h"
 
+#include <xen/events.h>
+
 EXPORT_SYMBOL_GPL(hypercall_page);
 
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);

commit 806c312e50f122c47913145cf884f53dd09d9199
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Aug 21 14:31:24 2012 -0400

    xen/x86: Workaround 64-bit hypervisor and 32-bit initial domain.
    
    If a 64-bit hypervisor is booted with a 32-bit initial domain,
    the hypervisor deals with the initial domain as "compat" and
    does some extra adjustments (like pagetables are 4 bytes instead
    of 8). It also adjusts the xen_start_info->pt_base incorrectly.
    
    When booted with a 32-bit hypervisor (32-bit initial domain):
    ..
    (XEN)  Start info:    cf831000->cf83147c
    (XEN)  Page tables:   cf832000->cf8b5000
    ..
    [    0.000000] PT: cf832000 (f832000)
    [    0.000000] Reserving PT: f832000->f8b5000
    
    And with a 64-bit hypervisor:
    (XEN)  Start info:    00000000cf831000->00000000cf8314b4
    (XEN)  Page tables:   00000000cf832000->00000000cf8b6000
    
    [    0.000000] PT: cf834000 (f834000)
    [    0.000000] Reserving PT: f834000->f8b8000
    
    To deal with this, we keep keep track of the highest physical
    address we have reserved via memblock_reserve. If that address
    does not overlap with pt_base, we have a gap which we reserve.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e532eb50e8d7..511f92d79e4a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1002,19 +1002,24 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
  * If the MFN is not in the m2p (provided to us by the hypervisor) this
  * function won't do anything. In practice this means that the XenBus
  * MFN won't be available for the initial domain. */
-static void __init xen_reserve_mfn(unsigned long mfn)
+static unsigned long __init xen_reserve_mfn(unsigned long mfn)
 {
-	unsigned long pfn;
+	unsigned long pfn, end_pfn = 0;
 
 	if (!mfn)
-		return;
+		return end_pfn;
+
 	pfn = mfn_to_pfn(mfn);
-	if (phys_to_machine_mapping_valid(pfn))
-		memblock_reserve(PFN_PHYS(pfn), PAGE_SIZE);
+	if (phys_to_machine_mapping_valid(pfn)) {
+		end_pfn = PFN_PHYS(pfn) + PAGE_SIZE;
+		memblock_reserve(PFN_PHYS(pfn), end_pfn);
+	}
+	return end_pfn;
 }
 static void __init xen_reserve_internals(void)
 {
 	unsigned long size;
+	unsigned long last_phys = 0;
 
 	if (!xen_pv_domain())
 		return;
@@ -1022,12 +1027,13 @@ static void __init xen_reserve_internals(void)
 	/* xen_start_info does not exist in the M2P, hence can't use
 	 * xen_reserve_mfn. */
 	memblock_reserve(__pa(xen_start_info), PAGE_SIZE);
+	last_phys = __pa(xen_start_info) + PAGE_SIZE;
 
-	xen_reserve_mfn(PFN_DOWN(xen_start_info->shared_info));
-	xen_reserve_mfn(xen_start_info->store_mfn);
+	last_phys = max(xen_reserve_mfn(PFN_DOWN(xen_start_info->shared_info)), last_phys);
+	last_phys = max(xen_reserve_mfn(xen_start_info->store_mfn), last_phys);
 
 	if (!xen_initial_domain())
-		xen_reserve_mfn(xen_start_info->console.domU.mfn);
+		last_phys = max(xen_reserve_mfn(xen_start_info->console.domU.mfn), last_phys);
 
 	if (xen_feature(XENFEAT_auto_translated_physmap))
 		return;
@@ -1043,8 +1049,14 @@ static void __init xen_reserve_internals(void)
 	 * a lot (and call memblock_reserve for each PAGE), so lets just use
 	 * the easy way and reserve it wholesale. */
 	memblock_reserve(__pa(xen_start_info->mfn_list), size);
-
+	last_phys = max(__pa(xen_start_info->mfn_list) + size, last_phys);
 	/* The pagetables are reserved in mmu.c */
+
+	/* Under 64-bit hypervisor with a 32-bit domain, the hypervisor
+	 * offsets the pt_base by two pages. Hence the reservation that is done
+	 * in mmu.c misses two pages. We correct it here if we detect this. */
+	if (last_phys < __pa(xen_start_info->pt_base))
+		memblock_reserve(last_phys, __pa(xen_start_info->pt_base) - last_phys);
 }
 void xen_setup_shared_info(void)
 {

commit 59b294403e9814e7c1154043567f0d71bac7a511
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Jul 19 10:23:47 2012 -0400

    xen/x86: Use memblock_reserve for sensitive areas.
    
    instead of a big memblock_reserve. This way we can be more
    selective in freeing regions (and it also makes it easier
    to understand where is what).
    
    [v1: Move the auto_translate_physmap to proper line]
    [v2: Per Stefano suggestion add more comments]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ff962d4b821e..e532eb50e8d7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -998,7 +998,54 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 
 	return ret;
 }
+/*
+ * If the MFN is not in the m2p (provided to us by the hypervisor) this
+ * function won't do anything. In practice this means that the XenBus
+ * MFN won't be available for the initial domain. */
+static void __init xen_reserve_mfn(unsigned long mfn)
+{
+	unsigned long pfn;
+
+	if (!mfn)
+		return;
+	pfn = mfn_to_pfn(mfn);
+	if (phys_to_machine_mapping_valid(pfn))
+		memblock_reserve(PFN_PHYS(pfn), PAGE_SIZE);
+}
+static void __init xen_reserve_internals(void)
+{
+	unsigned long size;
+
+	if (!xen_pv_domain())
+		return;
+
+	/* xen_start_info does not exist in the M2P, hence can't use
+	 * xen_reserve_mfn. */
+	memblock_reserve(__pa(xen_start_info), PAGE_SIZE);
+
+	xen_reserve_mfn(PFN_DOWN(xen_start_info->shared_info));
+	xen_reserve_mfn(xen_start_info->store_mfn);
 
+	if (!xen_initial_domain())
+		xen_reserve_mfn(xen_start_info->console.domU.mfn);
+
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return;
+
+	/*
+	 * ALIGN up to compensate for the p2m_page pointing to an array that
+	 * can partially filled (look in xen_build_dynamic_phys_to_machine).
+	 */
+
+	size = PAGE_ALIGN(xen_start_info->nr_pages * sizeof(unsigned long));
+
+	/* We could use xen_reserve_mfn here, but would end up looping quite
+	 * a lot (and call memblock_reserve for each PAGE), so lets just use
+	 * the easy way and reserve it wholesale. */
+	memblock_reserve(__pa(xen_start_info->mfn_list), size);
+
+	/* The pagetables are reserved in mmu.c */
+}
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
@@ -1362,6 +1409,7 @@ asmlinkage void __init xen_start_kernel(void)
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
 
+	xen_reserve_internals();
 	/* Allocate and initialize top and mid mfn levels for p2m structure */
 	xen_build_mfn_list_list();
 

commit ca08649eb5dd30f11a5a8fe8659b48899b7ea6a1
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Aug 16 11:31:27 2012 -0400

    Revert "xen PVonHVM: move shared_info to MMIO before kexec"
    
    This reverts commit 00e37bdb0113a98408de42db85be002f21dbffd3.
    
    During shutdown of PVHVM guests with more than 2VCPUs on certain
    machines we can hit the race where the replaced shared_info is not
    replaced fast enough and the PV time clock retries reading the same
    area over and over without any any success and is stuck in an
    infinite loop.
    
    Acked-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a6f8acbdfc9a..f1814fc2cb77 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -31,7 +31,6 @@
 #include <linux/pci.h>
 #include <linux/gfp.h>
 #include <linux/memblock.h>
-#include <linux/syscore_ops.h>
 
 #include <xen/xen.h>
 #include <xen/interface/xen.h>
@@ -1472,130 +1471,38 @@ asmlinkage void __init xen_start_kernel(void)
 #endif
 }
 
-#ifdef CONFIG_XEN_PVHVM
-/*
- * The pfn containing the shared_info is located somewhere in RAM. This
- * will cause trouble if the current kernel is doing a kexec boot into a
- * new kernel. The new kernel (and its startup code) can not know where
- * the pfn is, so it can not reserve the page. The hypervisor will
- * continue to update the pfn, and as a result memory corruption occours
- * in the new kernel.
- *
- * One way to work around this issue is to allocate a page in the
- * xen-platform pci device's BAR memory range. But pci init is done very
- * late and the shared_info page is already in use very early to read
- * the pvclock. So moving the pfn from RAM to MMIO is racy because some
- * code paths on other vcpus could access the pfn during the small
- * window when the old pfn is moved to the new pfn. There is even a
- * small window were the old pfn is not backed by a mfn, and during that
- * time all reads return -1.
- *
- * Because it is not known upfront where the MMIO region is located it
- * can not be used right from the start in xen_hvm_init_shared_info.
- *
- * To minimise trouble the move of the pfn is done shortly before kexec.
- * This does not eliminate the race because all vcpus are still online
- * when the syscore_ops will be called. But hopefully there is no work
- * pending at this point in time. Also the syscore_op is run last which
- * reduces the risk further.
- */
-
-static struct shared_info *xen_hvm_shared_info;
-
-static void xen_hvm_connect_shared_info(unsigned long pfn)
+void __ref xen_hvm_init_shared_info(void)
 {
+	int cpu;
 	struct xen_add_to_physmap xatp;
+	static struct shared_info *shared_info_page = 0;
 
+	if (!shared_info_page)
+		shared_info_page = (struct shared_info *)
+			extend_brk(PAGE_SIZE, PAGE_SIZE);
 	xatp.domid = DOMID_SELF;
 	xatp.idx = 0;
 	xatp.space = XENMAPSPACE_shared_info;
-	xatp.gpfn = pfn;
+	xatp.gpfn = __pa(shared_info_page) >> PAGE_SHIFT;
 	if (HYPERVISOR_memory_op(XENMEM_add_to_physmap, &xatp))
 		BUG();
 
-}
-static void xen_hvm_set_shared_info(struct shared_info *sip)
-{
-	int cpu;
-
-	HYPERVISOR_shared_info = sip;
+	HYPERVISOR_shared_info = (struct shared_info *)shared_info_page;
 
 	/* xen_vcpu is a pointer to the vcpu_info struct in the shared_info
 	 * page, we use it in the event channel upcall and in some pvclock
 	 * related functions. We don't need the vcpu_info placement
 	 * optimizations because we don't use any pv_mmu or pv_irq op on
 	 * HVM.
-	 * When xen_hvm_set_shared_info is run at boot time only vcpu 0 is
-	 * online but xen_hvm_set_shared_info is run at resume time too and
+	 * When xen_hvm_init_shared_info is run at boot time only vcpu 0 is
+	 * online but xen_hvm_init_shared_info is run at resume time too and
 	 * in that case multiple vcpus might be online. */
 	for_each_online_cpu(cpu) {
 		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 	}
 }
 
-/* Reconnect the shared_info pfn to a mfn */
-void xen_hvm_resume_shared_info(void)
-{
-	xen_hvm_connect_shared_info(__pa(xen_hvm_shared_info) >> PAGE_SHIFT);
-}
-
-#ifdef CONFIG_KEXEC
-static struct shared_info *xen_hvm_shared_info_kexec;
-static unsigned long xen_hvm_shared_info_pfn_kexec;
-
-/* Remember a pfn in MMIO space for kexec reboot */
-void __devinit xen_hvm_prepare_kexec(struct shared_info *sip, unsigned long pfn)
-{
-	xen_hvm_shared_info_kexec = sip;
-	xen_hvm_shared_info_pfn_kexec = pfn;
-}
-
-static void xen_hvm_syscore_shutdown(void)
-{
-	struct xen_memory_reservation reservation = {
-		.domid = DOMID_SELF,
-		.nr_extents = 1,
-	};
-	unsigned long prev_pfn;
-	int rc;
-
-	if (!xen_hvm_shared_info_kexec)
-		return;
-
-	prev_pfn = __pa(xen_hvm_shared_info) >> PAGE_SHIFT;
-	set_xen_guest_handle(reservation.extent_start, &prev_pfn);
-
-	/* Move pfn to MMIO, disconnects previous pfn from mfn */
-	xen_hvm_connect_shared_info(xen_hvm_shared_info_pfn_kexec);
-
-	/* Update pointers, following hypercall is also a memory barrier */
-	xen_hvm_set_shared_info(xen_hvm_shared_info_kexec);
-
-	/* Allocate new mfn for previous pfn */
-	do {
-		rc = HYPERVISOR_memory_op(XENMEM_populate_physmap, &reservation);
-		if (rc == 0)
-			msleep(123);
-	} while (rc == 0);
-
-	/* Make sure the previous pfn is really connected to a (new) mfn */
-	BUG_ON(rc != 1);
-}
-
-static struct syscore_ops xen_hvm_syscore_ops = {
-	.shutdown = xen_hvm_syscore_shutdown,
-};
-#endif
-
-/* Use a pfn in RAM, may move to MMIO before kexec. */
-static void __init xen_hvm_init_shared_info(void)
-{
-	/* Remember pointer for resume */
-	xen_hvm_shared_info = extend_brk(PAGE_SIZE, PAGE_SIZE);
-	xen_hvm_connect_shared_info(__pa(xen_hvm_shared_info) >> PAGE_SHIFT);
-	xen_hvm_set_shared_info(xen_hvm_shared_info);
-}
-
+#ifdef CONFIG_XEN_PVHVM
 static void __init init_hvm_pv_info(void)
 {
 	int major, minor;
@@ -1646,9 +1553,6 @@ static void __init xen_hvm_guest_init(void)
 	init_hvm_pv_info();
 
 	xen_hvm_init_shared_info();
-#ifdef CONFIG_KEXEC
-	register_syscore_ops(&xen_hvm_syscore_ops);
-#endif
 
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;

commit 0ec53ecf38bcbf95b4b057328a8fbba4d22ef28b
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Sep 14 13:37:32 2012 +0000

    xen/arm: receive Xen events on ARM
    
    Compile events.c on ARM.
    Parse, map and enable the IRQ to get event notifications from the device
    tree (node "/xen").
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 47b3acdc2ac5..689a4c9da866 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -33,6 +33,7 @@
 #include <linux/memblock.h>
 
 #include <xen/xen.h>
+#include <xen/events.h>
 #include <xen/interface/xen.h>
 #include <xen/interface/version.h>
 #include <xen/interface/physdev.h>

commit 62c4d9afa4bcf5315e2745a17a0228bf65b9ba40
Merge: 5fecc9d8f59e c3d93f880197
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 24 13:14:03 2012 -0700

    Merge tag 'stable/for-linus-3.6-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen update from Konrad Rzeszutek Wilk:
     "Features:
       * Performance improvement to lower the amount of traps the hypervisor
         has to do 32-bit guests.  Mainly for setting PTE entries and
         updating TLS descriptors.
       * MCE polling driver to collect hypervisor MCE buffer and present
         them to /dev/mcelog.
       * Physical CPU online/offline support.  When an privileged guest is
         booted it is present with virtual CPUs, which might have an 1:1 to
         physical CPUs but usually don't.  This provides mechanism to
         offline/online physical CPUs.
      Bug-fixes for:
       * Coverity found fixes in the console and ACPI processor driver.
       * PVonHVM kexec fixes along with some cleanups.
       * Pages that fall within E820 gaps and non-RAM regions (and had been
         released to hypervisor) would be populated back, but potentially in
         non-RAM regions."
    
    * tag 'stable/for-linus-3.6-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: populate correct number of pages when across mem boundary (v2)
      xen PVonHVM: move shared_info to MMIO before kexec
      xen: simplify init_hvm_pv_info
      xen: remove cast from HYPERVISOR_shared_info assignment
      xen: enable platform-pci only in a Xen guest
      xen/pv-on-hvm kexec: shutdown watches from old kernel
      xen/x86: avoid updating TLS descriptors if they haven't changed
      xen/x86: add desc_equal() to compare GDT descriptors
      xen/mm: zero PTEs for non-present MFNs in the initial page table
      xen/mm: do direct hypercall in xen_set_pte() if batching is unavailable
      xen/hvc: Fix up checks when the info is allocated.
      xen/acpi: Fix potential memory leak.
      xen/mce: add .poll method for mcelog device driver
      xen/mce: schedule a workqueue to avoid sleep in atomic context
      xen/pcpu: Xen physical cpus online/offline sys interface
      xen/mce: Register native mce handler as vMCE bounce back point
      x86, MCE, AMD: Adjust initcall sequence for xen
      xen/mce: Add mcelog support for Xen platform

commit 00e37bdb0113a98408de42db85be002f21dbffd3
Author: Olaf Hering <olaf@aepfle.de>
Date:   Tue Jul 17 17:43:35 2012 +0200

    xen PVonHVM: move shared_info to MMIO before kexec
    
    Currently kexec in a PVonHVM guest fails with a triple fault because the
    new kernel overwrites the shared info page. The exact failure depends on
    the size of the kernel image. This patch moves the pfn from RAM into
    MMIO space before the kexec boot.
    
    The pfn containing the shared_info is located somewhere in RAM. This
    will cause trouble if the current kernel is doing a kexec boot into a
    new kernel. The new kernel (and its startup code) can not know where the
    pfn is, so it can not reserve the page. The hypervisor will continue to
    update the pfn, and as a result memory corruption occours in the new
    kernel.
    
    One way to work around this issue is to allocate a page in the
    xen-platform pci device's BAR memory range. But pci init is done very
    late and the shared_info page is already in use very early to read the
    pvclock. So moving the pfn from RAM to MMIO is racy because some code
    paths on other vcpus could access the pfn during the small   window when
    the old pfn is moved to the new pfn. There is even a  small window were
    the old pfn is not backed by a mfn, and during that time all reads
    return -1.
    
    Because it is not known upfront where the MMIO region is located it can
    not be used right from the start in xen_hvm_init_shared_info.
    
    To minimise trouble the move of the pfn is done shortly before kexec.
    This does not eliminate the race because all vcpus are still online when
    the syscore_ops will be called. But hopefully there is no work pending
    at this point in time. Also the syscore_op is run last which reduces the
    risk further.
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f1814fc2cb77..a6f8acbdfc9a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -31,6 +31,7 @@
 #include <linux/pci.h>
 #include <linux/gfp.h>
 #include <linux/memblock.h>
+#include <linux/syscore_ops.h>
 
 #include <xen/xen.h>
 #include <xen/interface/xen.h>
@@ -1471,38 +1472,130 @@ asmlinkage void __init xen_start_kernel(void)
 #endif
 }
 
-void __ref xen_hvm_init_shared_info(void)
+#ifdef CONFIG_XEN_PVHVM
+/*
+ * The pfn containing the shared_info is located somewhere in RAM. This
+ * will cause trouble if the current kernel is doing a kexec boot into a
+ * new kernel. The new kernel (and its startup code) can not know where
+ * the pfn is, so it can not reserve the page. The hypervisor will
+ * continue to update the pfn, and as a result memory corruption occours
+ * in the new kernel.
+ *
+ * One way to work around this issue is to allocate a page in the
+ * xen-platform pci device's BAR memory range. But pci init is done very
+ * late and the shared_info page is already in use very early to read
+ * the pvclock. So moving the pfn from RAM to MMIO is racy because some
+ * code paths on other vcpus could access the pfn during the small
+ * window when the old pfn is moved to the new pfn. There is even a
+ * small window were the old pfn is not backed by a mfn, and during that
+ * time all reads return -1.
+ *
+ * Because it is not known upfront where the MMIO region is located it
+ * can not be used right from the start in xen_hvm_init_shared_info.
+ *
+ * To minimise trouble the move of the pfn is done shortly before kexec.
+ * This does not eliminate the race because all vcpus are still online
+ * when the syscore_ops will be called. But hopefully there is no work
+ * pending at this point in time. Also the syscore_op is run last which
+ * reduces the risk further.
+ */
+
+static struct shared_info *xen_hvm_shared_info;
+
+static void xen_hvm_connect_shared_info(unsigned long pfn)
 {
-	int cpu;
 	struct xen_add_to_physmap xatp;
-	static struct shared_info *shared_info_page = 0;
 
-	if (!shared_info_page)
-		shared_info_page = (struct shared_info *)
-			extend_brk(PAGE_SIZE, PAGE_SIZE);
 	xatp.domid = DOMID_SELF;
 	xatp.idx = 0;
 	xatp.space = XENMAPSPACE_shared_info;
-	xatp.gpfn = __pa(shared_info_page) >> PAGE_SHIFT;
+	xatp.gpfn = pfn;
 	if (HYPERVISOR_memory_op(XENMEM_add_to_physmap, &xatp))
 		BUG();
 
-	HYPERVISOR_shared_info = (struct shared_info *)shared_info_page;
+}
+static void xen_hvm_set_shared_info(struct shared_info *sip)
+{
+	int cpu;
+
+	HYPERVISOR_shared_info = sip;
 
 	/* xen_vcpu is a pointer to the vcpu_info struct in the shared_info
 	 * page, we use it in the event channel upcall and in some pvclock
 	 * related functions. We don't need the vcpu_info placement
 	 * optimizations because we don't use any pv_mmu or pv_irq op on
 	 * HVM.
-	 * When xen_hvm_init_shared_info is run at boot time only vcpu 0 is
-	 * online but xen_hvm_init_shared_info is run at resume time too and
+	 * When xen_hvm_set_shared_info is run at boot time only vcpu 0 is
+	 * online but xen_hvm_set_shared_info is run at resume time too and
 	 * in that case multiple vcpus might be online. */
 	for_each_online_cpu(cpu) {
 		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 	}
 }
 
-#ifdef CONFIG_XEN_PVHVM
+/* Reconnect the shared_info pfn to a mfn */
+void xen_hvm_resume_shared_info(void)
+{
+	xen_hvm_connect_shared_info(__pa(xen_hvm_shared_info) >> PAGE_SHIFT);
+}
+
+#ifdef CONFIG_KEXEC
+static struct shared_info *xen_hvm_shared_info_kexec;
+static unsigned long xen_hvm_shared_info_pfn_kexec;
+
+/* Remember a pfn in MMIO space for kexec reboot */
+void __devinit xen_hvm_prepare_kexec(struct shared_info *sip, unsigned long pfn)
+{
+	xen_hvm_shared_info_kexec = sip;
+	xen_hvm_shared_info_pfn_kexec = pfn;
+}
+
+static void xen_hvm_syscore_shutdown(void)
+{
+	struct xen_memory_reservation reservation = {
+		.domid = DOMID_SELF,
+		.nr_extents = 1,
+	};
+	unsigned long prev_pfn;
+	int rc;
+
+	if (!xen_hvm_shared_info_kexec)
+		return;
+
+	prev_pfn = __pa(xen_hvm_shared_info) >> PAGE_SHIFT;
+	set_xen_guest_handle(reservation.extent_start, &prev_pfn);
+
+	/* Move pfn to MMIO, disconnects previous pfn from mfn */
+	xen_hvm_connect_shared_info(xen_hvm_shared_info_pfn_kexec);
+
+	/* Update pointers, following hypercall is also a memory barrier */
+	xen_hvm_set_shared_info(xen_hvm_shared_info_kexec);
+
+	/* Allocate new mfn for previous pfn */
+	do {
+		rc = HYPERVISOR_memory_op(XENMEM_populate_physmap, &reservation);
+		if (rc == 0)
+			msleep(123);
+	} while (rc == 0);
+
+	/* Make sure the previous pfn is really connected to a (new) mfn */
+	BUG_ON(rc != 1);
+}
+
+static struct syscore_ops xen_hvm_syscore_ops = {
+	.shutdown = xen_hvm_syscore_shutdown,
+};
+#endif
+
+/* Use a pfn in RAM, may move to MMIO before kexec. */
+static void __init xen_hvm_init_shared_info(void)
+{
+	/* Remember pointer for resume */
+	xen_hvm_shared_info = extend_brk(PAGE_SIZE, PAGE_SIZE);
+	xen_hvm_connect_shared_info(__pa(xen_hvm_shared_info) >> PAGE_SHIFT);
+	xen_hvm_set_shared_info(xen_hvm_shared_info);
+}
+
 static void __init init_hvm_pv_info(void)
 {
 	int major, minor;
@@ -1553,6 +1646,9 @@ static void __init xen_hvm_guest_init(void)
 	init_hvm_pv_info();
 
 	xen_hvm_init_shared_info();
+#ifdef CONFIG_KEXEC
+	register_syscore_ops(&xen_hvm_syscore_ops);
+#endif
 
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;

commit 4ff2d06255461390ad685843d0d7364aaa6642d2
Author: Olaf Hering <olaf@aepfle.de>
Date:   Tue Jul 17 11:59:15 2012 +0200

    xen: simplify init_hvm_pv_info
    
    init_hvm_pv_info is called only in PVonHVM context, move it into ifdef.
    init_hvm_pv_info does not fail, make it a void function.
    remove arguments from init_hvm_pv_info because they are not used by the
    caller.
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index affa94b29d08..f1814fc2cb77 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1471,32 +1471,6 @@ asmlinkage void __init xen_start_kernel(void)
 #endif
 }
 
-static int init_hvm_pv_info(int *major, int *minor)
-{
-	uint32_t eax, ebx, ecx, edx, pages, msr, base;
-	u64 pfn;
-
-	base = xen_cpuid_base();
-	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
-
-	*major = eax >> 16;
-	*minor = eax & 0xffff;
-	printk(KERN_INFO "Xen version %d.%d.\n", *major, *minor);
-
-	cpuid(base + 2, &pages, &msr, &ecx, &edx);
-
-	pfn = __pa(hypercall_page);
-	wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
-
-	xen_setup_features();
-
-	pv_info.name = "Xen HVM";
-
-	xen_domain_type = XEN_HVM_DOMAIN;
-
-	return 0;
-}
-
 void __ref xen_hvm_init_shared_info(void)
 {
 	int cpu;
@@ -1529,6 +1503,31 @@ void __ref xen_hvm_init_shared_info(void)
 }
 
 #ifdef CONFIG_XEN_PVHVM
+static void __init init_hvm_pv_info(void)
+{
+	int major, minor;
+	uint32_t eax, ebx, ecx, edx, pages, msr, base;
+	u64 pfn;
+
+	base = xen_cpuid_base();
+	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
+
+	major = eax >> 16;
+	minor = eax & 0xffff;
+	printk(KERN_INFO "Xen version %d.%d.\n", major, minor);
+
+	cpuid(base + 2, &pages, &msr, &ecx, &edx);
+
+	pfn = __pa(hypercall_page);
+	wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
+
+	xen_setup_features();
+
+	pv_info.name = "Xen HVM";
+
+	xen_domain_type = XEN_HVM_DOMAIN;
+}
+
 static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
 				    unsigned long action, void *hcpu)
 {
@@ -1551,12 +1550,7 @@ static struct notifier_block xen_hvm_cpu_notifier __cpuinitdata = {
 
 static void __init xen_hvm_guest_init(void)
 {
-	int r;
-	int major, minor;
-
-	r = init_hvm_pv_info(&major, &minor);
-	if (r < 0)
-		return;
+	init_hvm_pv_info();
 
 	xen_hvm_init_shared_info();
 

commit 4648da7cb4079e263eaf4dcd3b10fdb2409d4ad6
Author: Olaf Hering <olaf@aepfle.de>
Date:   Tue Jul 17 11:57:41 2012 +0200

    xen: remove cast from HYPERVISOR_shared_info assignment
    
    Both have type struct shared_info so no cast is needed.
    
    Signed-off-by: Olaf Hering <olaf@aepfle.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9c808693afad..affa94b29d08 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -108,7 +108,7 @@ EXPORT_SYMBOL_GPL(xen_have_vector_callback);
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.
  */
-struct shared_info *HYPERVISOR_shared_info = (void *)&xen_dummy_shared_info;
+struct shared_info *HYPERVISOR_shared_info = &xen_dummy_shared_info;
 
 /*
  * Flag to determine whether vcpu info placement is available on all

commit 1c32cdc633c96a14cca35de9db19338354c25111
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jul 9 11:39:08 2012 +0100

    xen/x86: avoid updating TLS descriptors if they haven't changed
    
    When switching tasks in a Xen PV guest, avoid updating the TLS
    descriptors if they haven't changed.  This improves the speed of
    context switches by almost 10% as much of the time the descriptors are
    the same or only one is different.
    
    The descriptors written into the GDT by Xen are modified from the
    values passed in the update_descriptor hypercall so we keep shadow
    copies of the three TLS descriptors to compare against.
    
    lmbench3 test     Before  After  Improvement
    --------------------------------------------
    lat_ctx -s 32 24   7.19    6.52  9%
    lat_pipe          12.56   11.66  7%
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 225ffdc3c4ca..9c808693afad 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -125,6 +125,19 @@ struct shared_info *HYPERVISOR_shared_info = (void *)&xen_dummy_shared_info;
  */
 static int have_vcpu_info_placement = 1;
 
+struct tls_descs {
+	struct desc_struct desc[3];
+};
+
+/*
+ * Updating the 3 TLS descriptors in the GDT on every task switch is
+ * surprisingly expensive so we avoid updating them if they haven't
+ * changed.  Since Xen writes different descriptors than the one
+ * passed in the update_descriptor hypercall we keep shadow copies to
+ * compare against.
+ */
+static DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);
+
 static void clamp_max_cpus(void)
 {
 #ifdef CONFIG_SMP
@@ -548,9 +561,19 @@ static inline bool desc_equal(const struct desc_struct *d1,
 static void load_TLS_descriptor(struct thread_struct *t,
 				unsigned int cpu, unsigned int i)
 {
-	struct desc_struct *gdt = get_cpu_gdt_table(cpu);
-	xmaddr_t maddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);
-	struct multicall_space mc = __xen_mc_entry(0);
+	struct desc_struct *shadow = &per_cpu(shadow_tls_desc, cpu).desc[i];
+	struct desc_struct *gdt;
+	xmaddr_t maddr;
+	struct multicall_space mc;
+
+	if (desc_equal(shadow, &t->tls_array[i]))
+		return;
+
+	*shadow = t->tls_array[i];
+
+	gdt = get_cpu_gdt_table(cpu);
+	maddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);
+	mc = __xen_mc_entry(0);
 
 	MULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);
 }

commit 59290362da587ce2e6d2f4a7f85e362fa2d7fd39
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jul 9 11:39:07 2012 +0100

    xen/x86: add desc_equal() to compare GDT descriptors
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    [v1: Moving it to the Xen file]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0c1ab43f6672..225ffdc3c4ca 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -539,6 +539,12 @@ static void __init xen_load_gdt_boot(const struct desc_ptr *dtr)
 		BUG();
 }
 
+static inline bool desc_equal(const struct desc_struct *d1,
+			      const struct desc_struct *d2)
+{
+	return d1->a == d2->a && d1->b == d2->b;
+}
+
 static void load_TLS_descriptor(struct thread_struct *t,
 				unsigned int cpu, unsigned int i)
 {

commit 05e36006adc3046f86f2be8652a22d5f77ebd6ea
Author: Liu, Jinsong <jinsong.liu@intel.com>
Date:   Thu Jun 7 20:10:37 2012 +0800

    xen/mce: Register native mce handler as vMCE bounce back point
    
    When Xen hypervisor inject vMCE to guest, use native mce handler
    to handle it
    
    Signed-off-by: Ke, Liping <liping.ke@intel.com>
    Signed-off-by: Jiang, Yunhong <yunhong.jiang@intel.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Liu, Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9a6346865c49..0c1ab43f6672 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -626,8 +626,8 @@ static int cvt_gate_to_trap(int vector, const gate_desc *val,
 	/*
 	 * Look for known traps using IST, and substitute them
 	 * appropriately.  The debugger ones are the only ones we care
-	 * about.  Xen will handle faults like double_fault and
-	 * machine_check, so we should never see them.  Warn if
+	 * about.  Xen will handle faults like double_fault,
+	 * so we should never see them.  Warn if
 	 * there's an unexpected IST-using fault handler.
 	 */
 	if (addr == (unsigned long)debug)
@@ -642,7 +642,11 @@ static int cvt_gate_to_trap(int vector, const gate_desc *val,
 		return 0;
 #ifdef CONFIG_X86_MCE
 	} else if (addr == (unsigned long)machine_check) {
-		return 0;
+		/*
+		 * when xen hypervisor inject vMCE to guest,
+		 * use native mce handler to handle it
+		 */
+		;
 #endif
 	} else {
 		/* Some other trap using IST? */

commit cef12ee52b054282461a6d5fe7742755fa6e3bd3
Author: Liu, Jinsong <jinsong.liu@intel.com>
Date:   Thu Jun 7 19:56:51 2012 +0800

    xen/mce: Add mcelog support for Xen platform
    
    When MCA error occurs, it would be handled by Xen hypervisor first,
    and then the error information would be sent to initial domain for logging.
    
    This patch gets error information from Xen hypervisor and convert
    Xen format error into Linux format mcelog. This logic is basically
    self-contained, not touching other kernel components.
    
    By using tools like mcelog tool users could read specific error information,
    like what they did under native Linux.
    
    To test follow directions outlined in Documentation/acpi/apei/einj.txt
    
    Acked-and-tested-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Ke, Liping <liping.ke@intel.com>
    Signed-off-by: Jiang, Yunhong <yunhong.jiang@intel.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Liu, Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ff962d4b821e..9a6346865c49 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -38,6 +38,7 @@
 #include <xen/interface/physdev.h>
 #include <xen/interface/vcpu.h>
 #include <xen/interface/memory.h>
+#include <xen/interface/xen-mca.h>
 #include <xen/features.h>
 #include <xen/page.h>
 #include <xen/hvm.h>
@@ -341,9 +342,7 @@ static void __init xen_init_cpuid_mask(void)
 	unsigned int xsave_mask;
 
 	cpuid_leaf1_edx_mask =
-		~((1 << X86_FEATURE_MCE)  |  /* disable MCE */
-		  (1 << X86_FEATURE_MCA)  |  /* disable MCA */
-		  (1 << X86_FEATURE_MTRR) |  /* disable MTRR */
+		~((1 << X86_FEATURE_MTRR) |  /* disable MTRR */
 		  (1 << X86_FEATURE_ACC));   /* thermal monitoring */
 
 	if (!xen_initial_domain())

commit b0338e99b2a775c157e3e795f49fdcfb6c257f7a
Merge: 90574ebb7e6e 715c85b1fc82
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Jul 5 21:12:11 2012 +0200

    Merge branch 'x86/cpu' into perf/core
    
    Merge this branch because we changed the wrmsr*_safe() API and there's
    a conflict.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 069915b94642b05c9b45fee36862157026b36614
Merge: 41c8c535237e b9e0d95c041c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 15 17:17:15 2012 -0700

    Merge tag 'stable/for-linus-3.5-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull five Xen bug-fixes from Konrad Rzeszutek Wilk:
    
     - When booting as PVHVM we would try to use PV console - but would not validate
       the parameters causing us to crash during restore b/c we re-use the wrong event
       channel.
     - When booting on machines with SR-IOV PCI bridge we didn't check for the bridge
       and tried to use it.
     - Under AMD machines would advertise the APERFMPERF resulting in needless amount
       of MSRs from the guest.
     - A global value (xen_released_pages) was not subtracted at bootup when pages
       were added back in. This resulted in the balloon worker having the wrong
       account of how many pages were truly released.
     - Fix dead-lock when xen-blkfront is run in the same domain as xen-blkback.
    
    * tag 'stable/for-linus-3.5-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: mark local pages as FOREIGN in the m2p_override
      xen/setup: filter APERFMPERF cpuid feature out
      xen/balloon: Subtract from xen_released_pages the count that is populated.
      xen/pci: Check for PCI bridge before using it.
      xen/events: Add WARN_ON when quick lookup found invalid type.
      xen/hvc: Check HVM_PARAM_CONSOLE_[EVTCHN|PFN] for correctness.
      xen/hvc: Fix error cases around HVM_PARAM_CONSOLE_PFN
      xen/hvc: Collapse error logic.

commit 1f975f78c84c852e09463a2dfa57e3174e5c719e
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Fri Jun 1 16:52:35 2012 +0200

    x86, pvops: Remove hooks for {rd,wr}msr_safe_regs
    
    There were paravirt_ops hooks for the full register set variant of
    {rd,wr}msr_safe which are actually not used by anyone anymore. Remove
    them to make the code cleaner and avoid silent breakages when the pvops
    members were uninitialized. This has been boot-tested natively and under
    Xen with PVOPS enabled and disabled on one machine.
    
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Link: http://lkml.kernel.org/r/1338562358-28182-2-git-send-email-bp@amd64.org
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e74df9548a02..60f1131eb94f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1116,9 +1116,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.wbinvd = native_wbinvd,
 
 	.read_msr = native_read_msr_safe,
-	.rdmsr_regs = native_rdmsr_safe_regs,
 	.write_msr = xen_write_msr_safe,
-	.wrmsr_regs = native_wrmsr_safe_regs,
 
 	.read_tsc = native_read_tsc,
 	.read_pmc = native_read_pmc,

commit 5e626254206a709c6e937f3dda69bf26c7344f6f
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Tue May 29 13:07:31 2012 +0200

    xen/setup: filter APERFMPERF cpuid feature out
    
    Xen PV kernels allow access to the APERF/MPERF registers to read the
    effective frequency. Access to the MSRs is however redirected to the
    currently scheduled physical CPU, making consecutive read and
    compares unreliable. In addition each rdmsr traps into the hypervisor.
    So to avoid bogus readouts and expensive traps, disable the kernel
    internal feature flag for APERF/MPERF if running under Xen.
    This will
    a) remove the aperfmperf flag from /proc/cpuinfo
    b) not mislead the power scheduler (arch/x86/kernel/cpu/sched.c) to
       use the feature to improve scheduling (by default disabled)
    c) not mislead the cpufreq driver to use the MSRs
    
    This does not cover userland programs which access the MSRs via the
    device file interface, but this will be addressed separately.
    
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Cc: stable@vger.kernel.org # v3.0+
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d1f9a0472d44..272ebd0ce326 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -208,6 +208,9 @@ static void __init xen_banner(void)
 	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
 }
 
+#define CPUID_THERM_POWER_LEAF 6
+#define APERFMPERF_PRESENT 0
+
 static __read_mostly unsigned int cpuid_leaf1_edx_mask = ~0;
 static __read_mostly unsigned int cpuid_leaf1_ecx_mask = ~0;
 
@@ -241,6 +244,11 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		*dx = cpuid_leaf5_edx_val;
 		return;
 
+	case CPUID_THERM_POWER_LEAF:
+		/* Disabling APERFMPERF for kernel usage */
+		maskecx = ~(1 << APERFMPERF_PRESENT);
+		break;
+
 	case 0xb:
 		/* Suppress extended topology stuff */
 		maskebx = 0;

commit 1ab46fd319bcf1fcd9fb6311727d532b580e4eba
Author: Konrad Rzeszutek Wilk <konrad@darnok.org>
Date:   Wed May 30 18:23:56 2012 -0400

    x86, amd, xen: Avoid NULL pointer paravirt references
    
    Stub out MSR methods that aren't actually needed.  This fixes a crash
    as Xen Dom0 on AMD Trinity systems.  A bigger patch should be added to
    remove the paravirt machinery completely for the methods which
    apparently have no users!
    
    Reported-by: Andre Przywara <andre.przywara@amd.com>
    Link: http://lkml.kernel.org/r/20120530222356.GA28417@andromeda.dapyr.net
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 75f33b2a5933..e74df9548a02 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1116,7 +1116,10 @@ static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.wbinvd = native_wbinvd,
 
 	.read_msr = native_read_msr_safe,
+	.rdmsr_regs = native_rdmsr_safe_regs,
 	.write_msr = xen_write_msr_safe,
+	.wrmsr_regs = native_wrmsr_safe_regs,
+
 	.read_tsc = native_read_tsc,
 	.read_pmc = native_read_pmc,
 

commit b5f4035adfffbcc6b478de5b8c44b618b3124aff
Merge: ce004178be1b 68c2c39a76b0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 16:02:08 2012 -0700

    Merge tag 'stable/for-linus-3.5-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen updates from Konrad Rzeszutek Wilk:
     "Features:
       * Extend the APIC ops implementation and add IRQ_WORKER vector
         support so that 'perf' can work properly.
       * Fix self-ballooning code, and balloon logic when booting as initial
         domain.
       * Move array printing code to generic debugfs
       * Support XenBus domains.
       * Lazily free grants when a domain is dead/non-existent.
       * In M2P code use batching calls
      Bug-fixes:
       * Fix NULL dereference in allocation failure path (hvc_xen)
       * Fix unbinding of IRQ_WORKER vector during vCPU hot-unplug
       * Fix HVM guest resume - we would leak an PIRQ value instead of
         reusing the existing one."
    
    Fix up add-add onflicts in arch/x86/xen/enlighten.c due to addition of
    apic ipi interface next to the new apic_id functions.
    
    * tag 'stable/for-linus-3.5-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: do not map the same GSI twice in PVHVM guests.
      hvc_xen: NULL dereference on allocation failure
      xen: Add selfballoning memory reservation tunable.
      xenbus: Add support for xenbus backend in stub domain
      xen/smp: unbind irqworkX when unplugging vCPUs.
      xen: enter/exit lazy_mmu_mode around m2p_override calls
      xen/acpi/sleep: Enable ACPI sleep via the __acpi_os_prepare_sleep
      xen: implement IRQ_WORK_VECTOR handler
      xen: implement apic ipi interface
      xen/setup: update VA mapping when releasing memory during setup
      xen/setup: Combine the two hypercall functions - since they are quite similar.
      xen/setup: Populate freed MFNs from non-RAM E820 entries and gaps to E820 RAM
      xen/setup: Only print "Freeing XXX-YYY pfn range: Z pages freed" if Z > 0
      xen/gnttab: add deferred freeing logic
      debugfs: Add support to print u32 array in debugfs
      xen/p2m: An early bootup variant of set_phys_to_machine
      xen/p2m: Collapse early_alloc_p2m_middle redundant checks.
      xen/p2m: Allow alloc_p2m_middle to call reserve_brk depending on argument
      xen/p2m: Move code around to allow for better re-usage.

commit f08b9c2f8af0d61faa1170aeae4fbca1eff6a504
Merge: d79ee93de909 0ab711ae6ab0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 18:38:11 2012 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/apic changes from Ingo Molnar:
     "Most of the changes are about helping virtualized guest kernels
      achieve better performance."
    
    Fix up trivial conflicts with the iommu updates to arch/x86/kernel/apic/io_apic.c
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic: Implement EIO micro-optimization
      x86/apic: Add apic->eoi_write() callback
      x86/apic: Use symbolic APIC_EOI_ACK
      x86/apic: Fix typo EIO_ACK -> EOI_ACK and document it
      x86/xen/apic: Add missing #include <xen/xen.h>
      x86/apic: Only compile local function if used with !CONFIG_GENERIC_PENDING_IRQ
      x86/apic: Fix UP boot crash
      x86: Conditionally update time when ack-ing pending irqs
      xen/apic: implement io apic read with hypercall
      Revert "xen/x86: Workaround 'x86/ioapic: Add register level checks to detect bogus io-apic entries'"
      xen/x86: Implement x86_apic_ops
      x86/apic: Replace io_apic_ops with x86_io_apic_ops.

commit 4b3451ad1313134821ad13b6fe821d0b4409aa18
Merge: f62805f1f30a 83d51ab473dd
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon May 7 15:33:27 2012 -0400

    Merge branch 'stable/autoballoon.v5.2' into stable/for-linus-3.5
    
    * stable/autoballoon.v5.2:
      xen/setup: update VA mapping when releasing memory during setup
      xen/setup: Combine the two hypercall functions - since they are quite similar.
      xen/setup: Populate freed MFNs from non-RAM E820 entries and gaps to E820 RAM
      xen/setup: Only print "Freeing XXX-YYY pfn range: Z pages freed" if Z > 0
      xen/p2m: An early bootup variant of set_phys_to_machine
      xen/p2m: Collapse early_alloc_p2m_middle redundant checks.
      xen/p2m: Allow alloc_p2m_middle to call reserve_brk depending on argument
      xen/p2m: Move code around to allow for better re-usage.

commit 211063dc159695bd6072c5393e9bc729481c6ede
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Dec 8 17:32:23 2011 +0800

    xen/acpi/sleep: Enable ACPI sleep via the __acpi_os_prepare_sleep
    
    Provide the registration callback to call in the Xen's
    ACPI sleep functionality. This means that during S3/S5
    we make a hypercall XENPF_enter_acpi_sleep with the
    proper PM1A/PM1B registers.
    
    Based of Ke Yu's <ke.yu@intel.com> initial idea.
    [ From http://xenbits.xensource.com/linux-2.6.18-xen.hg
    change c68699484a65 ]
    
    [v1: Added Copyright and license]
    [v2: Added check if PM1A/B the 16-bits MSB contain something. The spec
         only uses 16-bits but might have more in future]
    Signed-off-by: Liang Tang <liang.tang@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1ed61c2bf633..eca90e5be1e7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -42,6 +42,7 @@
 #include <xen/page.h>
 #include <xen/hvm.h>
 #include <xen/hvc-console.h>
+#include <xen/acpi.h>
 
 #include <asm/paravirt.h>
 #include <asm/apic.h>
@@ -1373,6 +1374,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 		/* Make sure ACS will be enabled */
 		pci_request_acs();
+
+		xen_acpi_sleep_register();
 	}
 		
 

commit f447d56d36af18c5104ff29dcb1327c0c0ac3634
Author: Ben Guthro <ben@guthro.net>
Date:   Sat Apr 21 00:11:04 2012 +0800

    xen: implement apic ipi interface
    
    Map native ipi vector to xen vector.
    Implement apic ipi interface with xen_send_IPI_one.
    
    Tested-by: Steven Noonan <steven@uplinklabs.net>
    Signed-off-by: Ben Guthro <ben@guthro.net>
    Signed-off-by: Lin Ming <mlin@ss.pku.edu.cn>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 4f51bebac02c..1ed61c2bf633 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -74,6 +74,7 @@
 
 #include "xen-ops.h"
 #include "mmu.h"
+#include "smp.h"
 #include "multicalls.h"
 
 EXPORT_SYMBOL_GPL(hypercall_page);
@@ -849,6 +850,14 @@ static void set_xen_basic_apic_ops(void)
 	apic->icr_write = xen_apic_icr_write;
 	apic->wait_icr_idle = xen_apic_wait_icr_idle;
 	apic->safe_wait_icr_idle = xen_safe_apic_wait_icr_idle;
+
+#ifdef CONFIG_SMP
+	apic->send_IPI_allbutself = xen_send_IPI_allbutself;
+	apic->send_IPI_mask_allbutself = xen_send_IPI_mask_allbutself;
+	apic->send_IPI_mask = xen_send_IPI_mask;
+	apic->send_IPI_all = xen_send_IPI_all;
+	apic->send_IPI_self = xen_send_IPI_self;
+#endif
 }
 
 #endif

commit 83d51ab473dddde7df858015070ed22b84ebe9a9
Author: David Vrabel <dvrabel@cantab.net>
Date:   Thu May 3 16:15:42 2012 +0100

    xen/setup: update VA mapping when releasing memory during setup
    
    In xen_memory_setup(), if a page that is being released has a VA
    mapping this must also be updated.  Otherwise, the page will be not
    released completely -- it will still be referenced in Xen and won't be
    freed util the mapping is removed and this prevents it from being
    reallocated at a different PFN.
    
    This was already being done for the ISA memory region in
    xen_ident_map_ISA() but on many systems this was omitting a few pages
    as many systems marked a few pages below the ISA memory region as
    reserved in the e820 map.
    
    This fixes errors such as:
    
    (XEN) page_alloc.c:1148:d0 Over-allocation for domain 0: 2097153 > 2097152
    (XEN) memory.c:133:d0 Could not allocate order=0 extent: id=0 memflags=0 (0 of 17)
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index fe06bf4ef0e3..ac90e5629508 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1308,7 +1308,6 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
-	xen_ident_map_ISA();
 
 	/* Allocate and initialize top and mid mfn levels for p2m structure */
 	xen_build_mfn_list_list();

commit 76a8df7b49168509df02461f83fab117a4a86e08
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri May 4 14:29:46 2012 +0100

    xen/pci: don't use PCI BIOS service for configuration space accesses
    
    The accessing PCI configuration space with the PCI BIOS32 service does
    not work in PV guests.
    
    On systems without MMCONFIG or where the BIOS hasn't marked the
    MMCONFIG region as reserved in the e820 map, the BIOS service is
    probed (even though direct access is preferred) and this hangs.
    
    CC: stable@kernel.org
    Acked-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    [v1: Fixed compile error when CONFIG_PCI is not set]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 4f437dedbdf5..95dccce8e979 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -63,6 +63,7 @@
 #include <asm/stackprotector.h>
 #include <asm/hypervisor.h>
 #include <asm/mwait.h>
+#include <asm/pci_x86.h>
 
 #ifdef CONFIG_ACPI
 #include <linux/acpi.h>
@@ -1398,8 +1399,10 @@ asmlinkage void __init xen_start_kernel(void)
 		/* Make sure ACS will be enabled */
 		pci_request_acs();
 	}
-		
-
+#ifdef CONFIG_PCI
+	/* PCI BIOS service won't work from a PV guest. */
+	pci_probe &= ~PCI_PROBE_BIOS;
+#endif
 	xen_raw_console_write("about to get started...\n");
 
 	xen_setup_runstate_info(0);

commit 558daa289a402dbcce0c065c6ff3cc2e00ffeac8
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 2 15:04:51 2012 -0400

    xen/apic: Return the APIC ID (and version) for CPU 0.
    
    On x86_64 on AMD machines where the first APIC_ID is not zero, we get:
    
    ACPI: LAPIC (acpi_id[0x01] lapic_id[0x10] enabled)
    BIOS bug: APIC version is 0 for CPU 1/0x10, fixing up to 0x10
    BIOS bug: APIC version mismatch, boot CPU: 0, CPU 1: version 10
    
    which means that when the ACPI processor driver loads and
    tries to parse the _Pxx states it fails to do as, as it
    ends up calling acpi_get_cpuid which does this:
    
    for_each_possible_cpu(i) {
            if (cpu_physical_id(i) == apic_id)
                    return i;
    }
    
    And the bootup CPU, has not been found so it fails and returns -1
    for the first CPU - which then subsequently in the loop that
    "acpi_processor_get_info" does results in returning an error, which
    means that "acpi_processor_add" failing and per_cpu(processor)
    is never set (and is NULL).
    
    That means that when xen-acpi-processor tries to load (much much
    later on) and parse the P-states it gets -ENODEV from
    acpi_processor_register_performance() (which tries to read
    the per_cpu(processor)) and fails to parse the data.
    
    Reported-by-and-Tested-by:  Stefan Bader <stefan.bader@canonical.com>
    Suggested-by:  Boris Ostrovsky <boris.ostrovsky@amd.com>
    [v2: Bit-shift APIC ID by 24 bits]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a8f8844b8d32..4f437dedbdf5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -809,9 +809,40 @@ static void xen_io_delay(void)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
+static unsigned long xen_set_apic_id(unsigned int x)
+{
+	WARN_ON(1);
+	return x;
+}
+static unsigned int xen_get_apic_id(unsigned long x)
+{
+	return ((x)>>24) & 0xFFu;
+}
 static u32 xen_apic_read(u32 reg)
 {
-	return 0;
+	struct xen_platform_op op = {
+		.cmd = XENPF_get_cpuinfo,
+		.interface_version = XENPF_INTERFACE_VERSION,
+		.u.pcpu_info.xen_cpuid = 0,
+	};
+	int ret = 0;
+
+	/* Shouldn't need this as APIC is turned off for PV, and we only
+	 * get called on the bootup processor. But just in case. */
+	if (!xen_initial_domain() || smp_processor_id())
+		return 0;
+
+	if (reg == APIC_LVR)
+		return 0x10;
+
+	if (reg != APIC_ID)
+		return 0;
+
+	ret = HYPERVISOR_dom0_op(&op);
+	if (ret)
+		return 0;
+
+	return op.u.pcpu_info.apic_id << 24;
 }
 
 static void xen_apic_write(u32 reg, u32 val)
@@ -849,6 +880,8 @@ static void set_xen_basic_apic_ops(void)
 	apic->icr_write = xen_apic_icr_write;
 	apic->wait_icr_idle = xen_apic_wait_icr_idle;
 	apic->safe_wait_icr_idle = xen_safe_apic_wait_icr_idle;
+	apic->set_apic_id = xen_set_apic_id;
+	apic->get_apic_id = xen_get_apic_id;
 }
 
 #endif

commit 31b3c9d723407b395564d1fff3624cc0083ae520
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Mar 20 18:53:10 2012 -0400

    xen/x86: Implement x86_apic_ops
    
    Or rather just implement one different function as opposed
    to the native one : the read function.
    
    We synthesize the values.
    
    Acked-by:  Suresh Siddha <suresh.b.siddha@intel.com>
    [v1: Rebased on top of tip/x86/urgent]
    [v2: Return 0xfd instead of 0xff in the default case]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a8f8844b8d32..c2ea9e9f420d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1362,6 +1362,8 @@ asmlinkage void __init xen_start_kernel(void)
 		xen_start_info->console.domU.mfn = 0;
 		xen_start_info->console.domU.evtchn = 0;
 
+		xen_init_apic();
+
 		/* Make sure ACS will be enabled */
 		pci_request_acs();
 	}

commit df88b2d96e36d9a9e325bfcd12eb45671cbbc937
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Apr 26 13:13:21 2012 -0400

    xen/enlighten: Disable MWAIT_LEAF so that acpi-pad won't be loaded.
    
    There are exactly four users of __monitor and __mwait:
    
     - cstate.c (which allows acpi_processor_ffh_cstate_enter to be called
       when the cpuidle API drivers are used. However patch
       "cpuidle: replace xen access to x86 pm_idle and default_idle"
       provides a mechanism to disable the cpuidle and use safe_halt.
     - smpboot (which allows mwait_play_dead to be called). However
       safe_halt is always used so we skip that.
     - intel_idle (same deal as above).
     - acpi_pad.c. This the one that we do not want to run as we
       will hit the below crash.
    
    Why do we want to expose MWAIT_LEAF in the first place?
    We want it for the xen-acpi-processor driver - which uploads
    C-states to the hypervisor. If MWAIT_LEAF is set, the cstate.c
    sets the proper address in the C-states so that the hypervisor
    can benefit from using the MWAIT functionality. And that is
    the sole reason for using it.
    
    Without this patch, if a module performs mwait or monitor we
    get this:
    
    invalid opcode: 0000 [#1] SMP
    CPU 2
    .. snip..
    Pid: 5036, comm: insmod Tainted: G           O 3.4.0-rc2upstream-dirty #2 Intel Corporation S2600CP/S2600CP
    RIP: e030:[<ffffffffa000a017>]  [<ffffffffa000a017>] mwait_check_init+0x17/0x1000 [mwait_check]
    RSP: e02b:ffff8801c298bf18  EFLAGS: 00010282
    RAX: ffff8801c298a010 RBX: ffffffffa03b2000 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: ffff8801c29800d8 RDI: ffff8801ff097200
    RBP: ffff8801c298bf18 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000001 R12: 0000000000000000
    R13: ffffffffa000a000 R14: 0000005148db7294 R15: 0000000000000003
    FS:  00007fbb364f2700(0000) GS:ffff8801ff08c000(0000) knlGS:0000000000000000
    CS:  e033 DS: 0000 ES: 0000 CR0: 000000008005003b
    CR2: 000000000179f038 CR3: 00000001c9469000 CR4: 0000000000002660
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process insmod (pid: 5036, threadinfo ffff8801c298a000, task ffff8801c29cd7e0)
    Stack:
     ffff8801c298bf48 ffffffff81002124 ffffffffa03b2000 00000000000081fd
     000000000178f010 000000000178f030 ffff8801c298bf78 ffffffff810c41e6
     00007fff3fb30db9 00007fff3fb30db9 00000000000081fd 0000000000010000
    Call Trace:
     [<ffffffff81002124>] do_one_initcall+0x124/0x170
     [<ffffffff810c41e6>] sys_init_module+0xc6/0x220
     [<ffffffff815b15b9>] system_call_fastpath+0x16/0x1b
    Code: <0f> 01 c8 31 c0 0f 01 c9 c9 c3 00 00 00 00 00 00 00 00 00 00 00 00
    RIP  [<ffffffffa000a017>] mwait_check_init+0x17/0x1000 [mwait_check]
     RSP <ffff8801c298bf18>
    ---[ end trace 16582fc8a3d1e29a ]---
    Kernel panic - not syncing: Fatal exception
    
    With this module (which is what acpi_pad.c would hit):
    
    MODULE_AUTHOR("Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>");
    MODULE_DESCRIPTION("mwait_check_and_back");
    MODULE_LICENSE("GPL");
    MODULE_VERSION();
    
    static int __init mwait_check_init(void)
    {
            __monitor((void *)&current_thread_info()->flags, 0, 0);
            __mwait(0, 0);
            return 0;
    }
    static void __exit mwait_check_exit(void)
    {
    }
    module_init(mwait_check_init);
    module_exit(mwait_check_exit);
    
    Reported-by: Liu, Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 4f51bebac02c..a8f8844b8d32 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -261,7 +261,8 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 
 static bool __init xen_check_mwait(void)
 {
-#ifdef CONFIG_ACPI
+#if defined(CONFIG_ACPI) && !defined(CONFIG_ACPI_PROCESSOR_AGGREGATOR) && \
+	!defined(CONFIG_ACPI_PROCESSOR_AGGREGATOR_MODULE)
 	struct xen_platform_op op = {
 		.cmd			= XENPF_set_processor_pminfo,
 		.u.set_pminfo.id	= -1,
@@ -349,7 +350,6 @@ static void __init xen_init_cpuid_mask(void)
 	/* Xen will set CR4.OSXSAVE if supported and not disabled by force */
 	if ((cx & xsave_mask) != xsave_mask)
 		cpuid_leaf1_ecx_mask &= ~xsave_mask; /* disable XSAVE & OSXSAVE */
-
 	if (xen_check_mwait())
 		cpuid_leaf1_ecx_set_mask = (1 << (X86_FEATURE_MWAIT % 32));
 }

commit 5f054e31c63be774bf1ce252f20d56012a00f8a5
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Mar 29 15:38:31 2012 +1030

    documentation: remove references to cpu_*_map.
    
    This has been obsolescent for a while, fix documentation and
    misc comments.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b132ade26f77..4f51bebac02c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -967,7 +967,7 @@ void xen_setup_shared_info(void)
 	xen_setup_mfn_list_list();
 }
 
-/* This is called once we have the cpu_possible_map */
+/* This is called once we have the cpu_possible_mask */
 void xen_setup_vcpu_info_placement(void)
 {
 	int cpu;

commit d4c6fa73fe984e504d52f3d6bba291fd76fe49f7
Merge: aab008db8063 4bc25af79ec5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 19:59:19 2012 -0700

    Merge tag 'stable/for-linus-3.4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull xen updates from Konrad Rzeszutek Wilk:
     "which has three neat features:
    
       - PV multiconsole support, so that there can be hvc1, hvc2, etc; This
         can be used in HVM and in PV mode.
    
       - P-state and C-state power management driver that uploads said power
         management data to the hypervisor.  It also inhibits cpufreq
         scaling drivers to load so that only the hypervisor can make power
         management decisions - fixing a weird perf bug.
    
         There is one thing in the Kconfig that you won't like: "default y
         if (X86_ACPI_CPUFREQ = y || X86_POWERNOW_K8 = y)" (note, that it
         all depends on CONFIG_XEN which depends on CONFIG_PARAVIRT which by
         default is off).  I've a fix to convert that boolean expression
         into "default m" which I am going to post after the cpufreq git
         pull - as the two patches to make this work depend on a fix in Dave
         Jones's tree.
    
       - Function Level Reset (FLR) support in the Xen PCI backend.
    
      Fixes:
    
       - Kconfig dependencies for Xen PV keyboard and video
       - Compile warnings and constify fixes
       - Change over to use percpu_xxx instead of this_cpu_xxx"
    
    Fix up trivial conflicts in drivers/tty/hvc/hvc_xen.c due to changes to
    a removed commit.
    
    * tag 'stable/for-linus-3.4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen kconfig: relax INPUT_XEN_KBDDEV_FRONTEND deps
      xen/acpi-processor: C and P-state driver that uploads said data to hypervisor.
      xen: constify all instances of "struct attribute_group"
      xen/xenbus: ignore console/0
      hvc_xen: introduce HVC_XEN_FRONTEND
      hvc_xen: implement multiconsole support
      hvc_xen: support PV on HVM consoles
      xenbus: don't free other end details too early
      xen/enlighten: Expose MWAIT and MWAIT_LEAF if hypervisor OKs it.
      xen/setup/pm/acpi: Remove the call to boot_option_idle_override.
      xenbus: address compiler warnings
      xen: use this_cpu_xxx replace percpu_xxx funcs
      xen/pciback: Support pci_reset_function, aka FLR or D3 support.
      pci: Introduce __pci_reset_function_locked to be used when holding device_lock.
      xen: Utilize the restore_msi_irqs hook.

commit 73c154c60be106b47f15d1111fc2d75cc7a436f2
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Feb 13 22:26:32 2012 -0500

    xen/enlighten: Expose MWAIT and MWAIT_LEAF if hypervisor OKs it.
    
    For the hypervisor to take advantage of the MWAIT support it needs
    to extract from the ACPI _CST the register address. But the
    hypervisor does not have the support to parse DSDT so it relies on
    the initial domain (dom0) to parse the ACPI Power Management information
    and push it up to the hypervisor. The pushing of the data is done
    by the processor_harveset_xen module which parses the information that
    the ACPI parser has graciously exposed in 'struct acpi_processor'.
    
    For the ACPI parser to also expose the Cx states for MWAIT, we need
    to expose the MWAIT capability (leaf 1). Furthermore we also need to
    expose the MWAIT_LEAF capability (leaf 5) for cstate.c to properly
    function.
    
    The hypervisor could expose these flags when it traps the XEN_EMULATE_PREFIX
    operations, but it can't do it since it needs to be backwards compatible.
    Instead we choose to use the native CPUID to figure out if the MWAIT
    capability exists and use the XEN_SET_PDC query hypercall to figure out
    if the hypervisor wants us to expose the MWAIT_LEAF capability or not.
    
    Note: The XEN_SET_PDC query was implemented in c/s 23783:
    "ACPI: add _PDC input override mechanism".
    
    With this in place, instead of
     C3 ACPI IOPORT 415
    we get now
     C3:ACPI FFH INTEL MWAIT 0x20
    
    Note: The cpu_idle which would be calling the mwait variants for idling
    never gets set b/c we set the default pm_idle to be the hypercall variant.
    
    Acked-by: Jan Beulich <JBeulich@suse.com>
    [v2: Fix missing header file include and #ifdef]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 312c9e3cb635..fe06bf4ef0e3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -62,6 +62,15 @@
 #include <asm/reboot.h>
 #include <asm/stackprotector.h>
 #include <asm/hypervisor.h>
+#include <asm/mwait.h>
+
+#ifdef CONFIG_ACPI
+#include <linux/acpi.h>
+#include <asm/acpi.h>
+#include <acpi/pdc_intel.h>
+#include <acpi/processor.h>
+#include <xen/interface/platform.h>
+#endif
 
 #include "xen-ops.h"
 #include "mmu.h"
@@ -200,13 +209,17 @@ static void __init xen_banner(void)
 static __read_mostly unsigned int cpuid_leaf1_edx_mask = ~0;
 static __read_mostly unsigned int cpuid_leaf1_ecx_mask = ~0;
 
+static __read_mostly unsigned int cpuid_leaf1_ecx_set_mask;
+static __read_mostly unsigned int cpuid_leaf5_ecx_val;
+static __read_mostly unsigned int cpuid_leaf5_edx_val;
+
 static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		      unsigned int *cx, unsigned int *dx)
 {
 	unsigned maskebx = ~0;
 	unsigned maskecx = ~0;
 	unsigned maskedx = ~0;
-
+	unsigned setecx = 0;
 	/*
 	 * Mask out inconvenient features, to try and disable as many
 	 * unsupported kernel subsystems as possible.
@@ -214,9 +227,18 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 	switch (*ax) {
 	case 1:
 		maskecx = cpuid_leaf1_ecx_mask;
+		setecx = cpuid_leaf1_ecx_set_mask;
 		maskedx = cpuid_leaf1_edx_mask;
 		break;
 
+	case CPUID_MWAIT_LEAF:
+		/* Synthesize the values.. */
+		*ax = 0;
+		*bx = 0;
+		*cx = cpuid_leaf5_ecx_val;
+		*dx = cpuid_leaf5_edx_val;
+		return;
+
 	case 0xb:
 		/* Suppress extended topology stuff */
 		maskebx = 0;
@@ -232,9 +254,75 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 
 	*bx &= maskebx;
 	*cx &= maskecx;
+	*cx |= setecx;
 	*dx &= maskedx;
+
 }
 
+static bool __init xen_check_mwait(void)
+{
+#ifdef CONFIG_ACPI
+	struct xen_platform_op op = {
+		.cmd			= XENPF_set_processor_pminfo,
+		.u.set_pminfo.id	= -1,
+		.u.set_pminfo.type	= XEN_PM_PDC,
+	};
+	uint32_t buf[3];
+	unsigned int ax, bx, cx, dx;
+	unsigned int mwait_mask;
+
+	/* We need to determine whether it is OK to expose the MWAIT
+	 * capability to the kernel to harvest deeper than C3 states from ACPI
+	 * _CST using the processor_harvest_xen.c module. For this to work, we
+	 * need to gather the MWAIT_LEAF values (which the cstate.c code
+	 * checks against). The hypervisor won't expose the MWAIT flag because
+	 * it would break backwards compatibility; so we will find out directly
+	 * from the hardware and hypercall.
+	 */
+	if (!xen_initial_domain())
+		return false;
+
+	ax = 1;
+	cx = 0;
+
+	native_cpuid(&ax, &bx, &cx, &dx);
+
+	mwait_mask = (1 << (X86_FEATURE_EST % 32)) |
+		     (1 << (X86_FEATURE_MWAIT % 32));
+
+	if ((cx & mwait_mask) != mwait_mask)
+		return false;
+
+	/* We need to emulate the MWAIT_LEAF and for that we need both
+	 * ecx and edx. The hypercall provides only partial information.
+	 */
+
+	ax = CPUID_MWAIT_LEAF;
+	bx = 0;
+	cx = 0;
+	dx = 0;
+
+	native_cpuid(&ax, &bx, &cx, &dx);
+
+	/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,
+	 * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.
+	 */
+	buf[0] = ACPI_PDC_REVISION_ID;
+	buf[1] = 1;
+	buf[2] = (ACPI_PDC_C_CAPABILITY_SMP | ACPI_PDC_EST_CAPABILITY_SWSMP);
+
+	set_xen_guest_handle(op.u.set_pminfo.pdc, buf);
+
+	if ((HYPERVISOR_dom0_op(&op) == 0) &&
+	    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {
+		cpuid_leaf5_ecx_val = cx;
+		cpuid_leaf5_edx_val = dx;
+	}
+	return true;
+#else
+	return false;
+#endif
+}
 static void __init xen_init_cpuid_mask(void)
 {
 	unsigned int ax, bx, cx, dx;
@@ -261,6 +349,9 @@ static void __init xen_init_cpuid_mask(void)
 	/* Xen will set CR4.OSXSAVE if supported and not disabled by force */
 	if ((cx & xsave_mask) != xsave_mask)
 		cpuid_leaf1_ecx_mask &= ~xsave_mask; /* disable XSAVE & OSXSAVE */
+
+	if (xen_check_mwait())
+		cpuid_leaf1_ecx_set_mask = (1 << (X86_FEATURE_MWAIT % 32));
 }
 
 static void xen_set_debugreg(int reg, unsigned long val)

commit 8eaffa67b43e99ae581622c5133e20b0f48bcef1
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Feb 10 09:16:27 2012 -0500

    xen/pat: Disable PAT support for now.
    
    [Pls also look at https://lkml.org/lkml/2012/2/10/228]
    
    Using of PAT to change pages from WB to WC works quite nicely.
    Changing it back to WB - not so much. The crux of the matter is
    that the code that does this (__page_change_att_set_clr) has only
    limited information so when it tries to the change it gets
    the "raw" unfiltered information instead of the properly filtered one -
    and the "raw" one tell it that PSE bit is on (while infact it
    is not).  As a result when the PTE is set to be WB from WC, we get
    tons of:
    
    :WARNING: at arch/x86/xen/mmu.c:475 xen_make_pte+0x67/0xa0()
    :Hardware name: HP xw4400 Workstation
    .. snip..
    :Pid: 27, comm: kswapd0 Tainted: G        W    3.2.2-1.fc16.x86_64 #1
    :Call Trace:
    : [<ffffffff8106dd1f>] warn_slowpath_common+0x7f/0xc0
    : [<ffffffff8106dd7a>] warn_slowpath_null+0x1a/0x20
    : [<ffffffff81005a17>] xen_make_pte+0x67/0xa0
    : [<ffffffff810051bd>] __raw_callee_save_xen_make_pte+0x11/0x1e
    : [<ffffffff81040e15>] ? __change_page_attr_set_clr+0x9d5/0xc00
    : [<ffffffff8114c2e8>] ? __purge_vmap_area_lazy+0x158/0x1d0
    : [<ffffffff8114cca5>] ? vm_unmap_aliases+0x175/0x190
    : [<ffffffff81041168>] change_page_attr_set_clr+0x128/0x4c0
    : [<ffffffff81041542>] set_pages_array_wb+0x42/0xa0
    : [<ffffffff8100a9b2>] ? check_events+0x12/0x20
    : [<ffffffffa0074d4c>] ttm_pages_put+0x1c/0x70 [ttm]
    : [<ffffffffa0074e98>] ttm_page_pool_free+0xf8/0x180 [ttm]
    : [<ffffffffa0074f78>] ttm_pool_mm_shrink+0x58/0x90 [ttm]
    : [<ffffffff8112ba04>] shrink_slab+0x154/0x310
    : [<ffffffff8112f17a>] balance_pgdat+0x4fa/0x6c0
    : [<ffffffff8112f4b8>] kswapd+0x178/0x3d0
    : [<ffffffff815df134>] ? __schedule+0x3d4/0x8c0
    : [<ffffffff81090410>] ? remove_wait_queue+0x50/0x50
    : [<ffffffff8112f340>] ? balance_pgdat+0x6c0/0x6c0
    : [<ffffffff8108fb6c>] kthread+0x8c/0xa0
    
    for every page. The proper fix for this is has been posted
    and is https://lkml.org/lkml/2012/2/10/228
    "x86/cpa: Use pte_attrs instead of pte_flags on CPA/set_p.._wb/wc operations."
    along with a detailed description of the problem and solution.
    
    But since that posting has gone nowhere I am proposing
    this band-aid solution so that at least users don't get
    the page corruption (the pages that are WC don't get changed to WB
    and end up being recycled for filesystem or other things causing
    mysterious crashes).
    
    The negative impact of this patch is that users of WC flag
    (which are InfiniBand, radeon, nouveau drivers) won't be able
    to set that flag - so they are going to see performance degradation.
    But stability is more important here.
    
    Fixes RH BZ# 742032, 787403, and 745574
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7c44e1bf981e..4172af8ceeb3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1141,7 +1141,9 @@ asmlinkage void __init xen_start_kernel(void)
 
 	/* Prevent unwanted bits from being set in PTEs. */
 	__supported_pte_mask &= ~_PAGE_GLOBAL;
+#if 0
 	if (!xen_initial_domain())
+#endif
 		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
 
 	__supported_pte_mask |= _PAGE_IOMAP;

commit 416d7214741daba3acd6d328289858390bef37bc
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Feb 10 09:24:08 2012 -0500

    xen/setup: Remove redundant filtering of PTE masks.
    
    commit 7347b4082e55ac4a673f06a0a0ce25c37273c9ec "xen: Allow
    unprivileged Xen domains to create iomap pages" added a redundant
    line in the early bootup code to filter out the PTE. That
    filtering is already done a bit earlier so this extra processing
    is not required.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 12eb07bfb267..7c44e1bf981e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1204,10 +1204,6 @@ asmlinkage void __init xen_start_kernel(void)
 
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
-	if (!xen_initial_domain())
-		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
-
-	__supported_pte_mask |= _PAGE_IOMAP;
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];

commit 2113f4691663f033189bf43d7501c6d29cd685a5
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri Jan 13 23:53:35 2012 +0800

    xen: use this_cpu_xxx replace percpu_xxx funcs
    
    percpu_xxx funcs are duplicated with this_cpu_xxx funcs, so replace them
    for further code clean up.
    
    I don't know much of xen code. But, since the code is in x86 architecture,
    the percpu_xxx is exactly same as this_cpu_xxx serials functions. So, the
    change is safe.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Christoph Lameter <cl@gentwo.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 12eb07bfb267..312c9e3cb635 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -777,11 +777,11 @@ static DEFINE_PER_CPU(unsigned long, xen_cr0_value);
 
 static unsigned long xen_read_cr0(void)
 {
-	unsigned long cr0 = percpu_read(xen_cr0_value);
+	unsigned long cr0 = this_cpu_read(xen_cr0_value);
 
 	if (unlikely(cr0 == 0)) {
 		cr0 = native_read_cr0();
-		percpu_write(xen_cr0_value, cr0);
+		this_cpu_write(xen_cr0_value, cr0);
 	}
 
 	return cr0;
@@ -791,7 +791,7 @@ static void xen_write_cr0(unsigned long cr0)
 {
 	struct multicall_space mcs;
 
-	percpu_write(xen_cr0_value, cr0);
+	this_cpu_write(xen_cr0_value, cr0);
 
 	/* Only pay attention to cr0.TS; everything else is
 	   ignored. */

commit fe091c208a40299fba40e62292a610fb91e44b4e
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Dec 8 10:22:07 2011 -0800

    memblock: Kill memblock_init()
    
    memblock_init() initializes arrays for regions and memblock itself;
    however, all these can be done with struct initializers and
    memblock_init() can be removed.  This patch kills memblock_init() and
    initializes memblock with struct initializer.
    
    The only difference is that the first dummy entries don't have .nid
    set to MAX_NUMNODES initially.  This doesn't cause any behavior
    difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1f928659c338..12eb07bfb267 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1215,8 +1215,6 @@ asmlinkage void __init xen_start_kernel(void)
 	local_irq_disable();
 	early_boot_irqs_disabled = true;
 
-	memblock_init();
-
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
 	xen_ident_map_ISA();

commit 90d4f5534d14815bd94c10e8ceccc57287657ecc
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Thu Oct 27 22:28:59 2011 -0700

    xen:pvhvm: enable PVHVM VCPU placement when using more than 32 CPUs.
    
    PVHVM running with more than 32 vcpus and pv_irq/pv_time enabled
    need VCPU placement to work, or else it will softlockup.
    
    CC: stable@kernel.org
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index da8afd576a6b..1f928659c338 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1356,7 +1356,7 @@ static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
 	int cpu = (long)hcpu;
 	switch (action) {
 	case CPU_UP_PREPARE:
-		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+		xen_vcpu_setup(cpu);
 		if (xen_have_vector_callback)
 			xen_init_lock_cpu(cpu);
 		break;
@@ -1386,7 +1386,6 @@ static void __init xen_hvm_guest_init(void)
 	xen_hvm_smp_init();
 	register_cpu_notifier(&xen_hvm_cpu_notifier);
 	xen_unplug_emulated_devices();
-	have_vcpu_info_placement = 0;
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();
 	xen_hvm_init_mmu_ops();

commit 5e287830136a8edb76e9f9c432b264d99833172f
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Sep 29 13:06:42 2011 -0400

    xen/enlighten: Fix compile warnings and set cx to known value.
    
    We get:
    linux/arch/x86/xen/enlighten.c: In function ‘xen_start_kernel’:
    linux/arch/x86/xen/enlighten.c:226: warning: ‘cx’ may be used uninitialized in this function
    linux/arch/x86/xen/enlighten.c:240: note: ‘cx’ was declared here
    
    and the cx is really not set but passed in the xen_cpuid instruction
    which masks the value with returned masked_ecx from cpuid. This
    can potentially lead to invalid data being stored in cx.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2d69617950f7..da8afd576a6b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -251,6 +251,7 @@ static void __init xen_init_cpuid_mask(void)
 			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
 			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
 	ax = 1;
+	cx = 0;
 	xen_cpuid(&ax, &bx, &cx, &dx);
 
 	xsave_mask =

commit 4762e252f4afc6909edb0babe9c25f126afedcaa
Merge: 3210d190dcb7 60c5f08e154f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 22 11:25:44 2011 -0700

    Merge branch 'stable/bug.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/bug.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/tracing: Fix tracing config option properly
      xen: Do not enable PV IPIs when vector callback not present
      xen/x86: replace order-based range checking of M2P table by linear one
      xen: xen-selfballoon.c needs more header files

commit ccbcdf7cf1b5f6c6db30d84095b9c6c53043af55
Author: Jan Beulich <JBeulich@novell.com>
Date:   Tue Aug 16 15:07:41 2011 +0100

    xen/x86: replace order-based range checking of M2P table by linear one
    
    The order-based approach is not only less efficient (requiring a shift
    and a compare, typical generated code looking like this
    
            mov     eax, [machine_to_phys_order]
            mov     ecx, eax
            shr     ebx, cl
            test    ebx, ebx
            jnz     ...
    
    whereas a direct check requires just a compare, like in
    
            cmp     ebx, [machine_to_phys_nr]
            jae     ...
    
    ), but also slightly dangerous in the 32-on-64 case - the element
    address calculation can wrap if the next power of two boundary is
    sufficiently far away from the actual upper limit of the table, and
    hence can result in user space addresses being accessed (with it being
    unknown what may actually be mapped there).
    
    Additionally, the elimination of the mistaken use of fls() here (should
    have been __fls()) fixes a latent issue on x86-64 that would trigger
    if the code was run on a system with memory extending beyond the 44-bit
    boundary.
    
    CC: stable@kernel.org
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    [v1: Based on Jeremy's feedback]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 974a528458a0..b960429d5b65 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -77,8 +77,8 @@ EXPORT_SYMBOL_GPL(xen_domain_type);
 
 unsigned long *machine_to_phys_mapping = (void *)MACH2PHYS_VIRT_START;
 EXPORT_SYMBOL(machine_to_phys_mapping);
-unsigned int   machine_to_phys_order;
-EXPORT_SYMBOL(machine_to_phys_order);
+unsigned long  machine_to_phys_nr;
+EXPORT_SYMBOL(machine_to_phys_nr);
 
 struct start_info *xen_start_info;
 EXPORT_SYMBOL_GPL(xen_start_info);

commit 06e727d2a5d9d889fabad35223ad77205a9bebb9
Merge: e68ff9cd1555 3ae36655b97a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 12 20:46:24 2011 -0700

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-tip
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-tip:
      x86-64: Rework vsyscall emulation and add vsyscall= parameter
      x86-64: Wire up getcpu syscall
      x86: Remove unnecessary compile flag tweaks for vsyscall code
      x86-64: Add vsyscall:emulate_vsyscall trace event
      x86-64: Add user_64bit_mode paravirt op
      x86-64, xen: Enable the vvar mapping
      x86-64: Work around gold bug 13023
      x86-64: Move the "user" vsyscall segment out of the data segment.
      x86-64: Pad vDSO to a page boundary

commit 318f5a2a672152328c9fb4dead504b89ec738a43
Author: Andy Lutomirski <luto@MIT.EDU>
Date:   Wed Aug 3 09:31:53 2011 -0400

    x86-64: Add user_64bit_mode paravirt op
    
    Three places in the kernel assume that the only long mode CPL 3
    selector is __USER_CS.  This is not true on Xen -- Xen's sysretq
    changes cs to the magic value 0xe033.
    
    Two of the places are corner cases, but as of "x86-64: Improve
    vsyscall emulation CS and RIP handling"
    (c9712944b2a12373cb6ff8059afcfb7e826a6c54), vsyscalls will segfault
    if called with Xen's extra CS selector.  This causes a panic when
    older init builds die.
    
    It seems impossible to make Xen use __USER_CS reliably without
    taking a performance hit on every system call, so this fixes the
    tests instead with a new paravirt op.  It's a little ugly because
    ptrace.h can't include paravirt.h.
    
    Signed-off-by: Andy Lutomirski <luto@mit.edu>
    Link: http://lkml.kernel.org/r/f4fcb3947340d9e96ce1054a432f183f9da9db83.1312378163.git.luto@mit.edu
    Reported-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5525163a0398..78fe33d03565 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -937,6 +937,10 @@ static const struct pv_info xen_info __initconst = {
 	.paravirt_enabled = 1,
 	.shared_kernel_pmd = 0,
 
+#ifdef CONFIG_X86_64
+	.extra_user_64bit_cs = FLAT_USER_CS64,
+#endif
+
 	.name = "Xen",
 };
 

commit c61264f98c1a974ee6f545f61a4ab33b141d6bda
Merge: a23a334bd547 bd9ddc875b66
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 24 09:06:47 2011 -0700

    Merge branch 'upstream/xen-tracing2' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'upstream/xen-tracing2' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen/trace: use class for multicall trace
      xen/trace: convert mmu events to use DECLARE_EVENT_CLASS()/DEFINE_EVENT()
      xen/multicall: move *idx fields to start of mc_buffer
      xen/multicall: special-case singleton hypercalls
      xen/multicalls: add unlikely around slowpath in __xen_mc_entry()
      xen/multicalls: disable MC_DEBUG
      xen/mmu: tune pgtable alloc/release
      xen/mmu: use extend_args for more mmuext updates
      xen/trace: add tlb flush tracepoints
      xen/trace: add segment desc tracing
      xen/trace: add xen_pgd_(un)pin tracepoints
      xen/trace: add ptpage alloc/release tracepoints
      xen/trace: add mmu tracepoints
      xen/trace: add multicall tracing
      xen/trace: set up tracepoint skeleton
      xen/multicalls: remove debugfs stats
      trace/xen: add skeleton for Xen trace events

commit ab78f7ad2c78fb1b724b278479adec998933be36
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Dec 17 15:54:28 2010 -0800

    xen/trace: add segment desc tracing
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5525163a0398..520325c2ceba 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -341,6 +341,8 @@ static void xen_set_ldt(const void *addr, unsigned entries)
 	struct mmuext_op *op;
 	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
 
+	trace_xen_cpu_set_ldt(addr, entries);
+
 	op = mcs.args;
 	op->cmd = MMUEXT_SET_LDT;
 	op->arg1.linear_addr = (unsigned long)addr;
@@ -496,6 +498,8 @@ static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 	xmaddr_t mach_lp = arbitrary_virt_to_machine(&dt[entrynum]);
 	u64 entry = *(u64 *)ptr;
 
+	trace_xen_cpu_write_ldt_entry(dt, entrynum, entry);
+
 	preempt_disable();
 
 	xen_mc_flush();
@@ -565,6 +569,8 @@ static void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)
 	unsigned long p = (unsigned long)&dt[entrynum];
 	unsigned long start, end;
 
+	trace_xen_cpu_write_idt_entry(dt, entrynum, g);
+
 	preempt_disable();
 
 	start = __this_cpu_read(idt_desc.address);
@@ -619,6 +625,8 @@ static void xen_load_idt(const struct desc_ptr *desc)
 	static DEFINE_SPINLOCK(lock);
 	static struct trap_info traps[257];
 
+	trace_xen_cpu_load_idt(desc);
+
 	spin_lock(&lock);
 
 	__get_cpu_var(idt_desc) = *desc;
@@ -637,6 +645,8 @@ static void xen_load_idt(const struct desc_ptr *desc)
 static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
 				const void *desc, int type)
 {
+	trace_xen_cpu_write_gdt_entry(dt, entry, desc, type);
+
 	preempt_disable();
 
 	switch (type) {
@@ -665,6 +675,8 @@ static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
 static void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,
 					    const void *desc, int type)
 {
+	trace_xen_cpu_write_gdt_entry(dt, entry, desc, type);
+
 	switch (type) {
 	case DESC_LDT:
 	case DESC_TSS:
@@ -684,7 +696,9 @@ static void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,
 static void xen_load_sp0(struct tss_struct *tss,
 			 struct thread_struct *thread)
 {
-	struct multicall_space mcs = xen_mc_entry(0);
+	struct multicall_space mcs;
+
+	mcs = xen_mc_entry(0);
 	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread->sp0);
 	xen_mc_issue(PARAVIRT_LAZY_CPU);
 }

commit f7fdd84e04c8fdc9196abe3bfd27535bccb52ee5
Merge: 03dc6107ff48 c2419b4a4727
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Jun 21 09:25:41 2011 -0400

    Merge branch 'stable/vga.support' into stable/drivers
    
    * stable/vga.support:
      xen: allow enable use of VGA console on dom0

commit b2abe50688dcb470e2e46109da7e7e02245ed59b
Author: Tom Goetz <tom.goetz@virtualcomputer.com>
Date:   Mon May 16 15:06:26 2011 -0400

    xen: When calling power_off, don't call the halt function.
    
    .. As it won't actually power off the machine.
    
    Reported-by: Sven Köhler <sven.koehler@gmail.com>
    Tested-by: Sven Köhler <sven.koehler@gmail.com>
    Signed-off-by: Tom Goetz <tom.goetz@virtualcomputer.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index dd7b88f2ec7a..5525163a0398 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1033,6 +1033,13 @@ static void xen_machine_halt(void)
 	xen_reboot(SHUTDOWN_poweroff);
 }
 
+static void xen_machine_power_off(void)
+{
+	if (pm_power_off)
+		pm_power_off();
+	xen_reboot(SHUTDOWN_poweroff);
+}
+
 static void xen_crash_shutdown(struct pt_regs *regs)
 {
 	xen_reboot(SHUTDOWN_crash);
@@ -1058,7 +1065,7 @@ int xen_panic_handler_init(void)
 static const struct machine_ops xen_machine_ops __initconst = {
 	.restart = xen_restart,
 	.halt = xen_machine_halt,
-	.power_off = xen_machine_halt,
+	.power_off = xen_machine_power_off,
 	.shutdown = xen_machine_halt,
 	.crash_shutdown = xen_crash_shutdown,
 	.emergency_restart = xen_emergency_restart,

commit c2419b4a4727f67af2fc2cd68b0d878b75e781bb
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue May 31 10:50:10 2011 -0400

    xen: allow enable use of VGA console on dom0
    
    Get the information about the VGA console hardware from Xen, and put
    it into the form the bootloader normally generates, so that the rest
    of the kernel can deal with VGA as usual.
    
    [ Impact: make VGA console work in dom0 ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    [v1: Rebased on 2.6.39]
    [v2: Removed incorrect comments and fixed compile warnings]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e3c6a06cf725..4abd2d5d04f7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1241,6 +1241,14 @@ asmlinkage void __init xen_start_kernel(void)
 		if (pci_xen)
 			x86_init.pci.arch_init = pci_xen_init;
 	} else {
+		const struct dom0_vga_console_info *info =
+			(void *)((char *)xen_start_info +
+				 xen_start_info->console.dom0.info_off);
+
+		xen_init_vga(info, xen_start_info->console.dom0.info_size);
+		xen_start_info->console.domU.mfn = 0;
+		xen_start_info->console.domU.evtchn = 0;
+
 		/* Make sure ACS will be enabled */
 		pci_request_acs();
 	}

commit ad3062a0f438a5f436dae267f795c0a9686f11d2
Author: Daniel Kiper <dkiper@net-space.pl>
Date:   Wed May 4 20:15:18 2011 +0200

    arch/x86/xen/enlighten: Cleanup code/data sections definitions
    
    Cleanup code/data sections definitions
    accordingly to include/linux/init.h.
    
    Signed-off-by: Daniel Kiper <dkiper@net-space.pl>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e3c6a06cf725..dd7b88f2ec7a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -235,7 +235,7 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 	*dx &= maskedx;
 }
 
-static __init void xen_init_cpuid_mask(void)
+static void __init xen_init_cpuid_mask(void)
 {
 	unsigned int ax, bx, cx, dx;
 	unsigned int xsave_mask;
@@ -400,7 +400,7 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 /*
  * load_gdt for early boot, when the gdt is only mapped once
  */
-static __init void xen_load_gdt_boot(const struct desc_ptr *dtr)
+static void __init xen_load_gdt_boot(const struct desc_ptr *dtr)
 {
 	unsigned long va = dtr->address;
 	unsigned int size = dtr->size + 1;
@@ -662,7 +662,7 @@ static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
  * Version of write_gdt_entry for use at early boot-time needed to
  * update an entry as simply as possible.
  */
-static __init void xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,
+static void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,
 					    const void *desc, int type)
 {
 	switch (type) {
@@ -933,18 +933,18 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 	return ret;
 }
 
-static const struct pv_info xen_info __initdata = {
+static const struct pv_info xen_info __initconst = {
 	.paravirt_enabled = 1,
 	.shared_kernel_pmd = 0,
 
 	.name = "Xen",
 };
 
-static const struct pv_init_ops xen_init_ops __initdata = {
+static const struct pv_init_ops xen_init_ops __initconst = {
 	.patch = xen_patch,
 };
 
-static const struct pv_cpu_ops xen_cpu_ops __initdata = {
+static const struct pv_cpu_ops xen_cpu_ops __initconst = {
 	.cpuid = xen_cpuid,
 
 	.set_debugreg = xen_set_debugreg,
@@ -1004,7 +1004,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.end_context_switch = xen_end_context_switch,
 };
 
-static const struct pv_apic_ops xen_apic_ops __initdata = {
+static const struct pv_apic_ops xen_apic_ops __initconst = {
 #ifdef CONFIG_X86_LOCAL_APIC
 	.startup_ipi_hook = paravirt_nop,
 #endif
@@ -1055,7 +1055,7 @@ int xen_panic_handler_init(void)
 	return 0;
 }
 
-static const struct machine_ops __initdata xen_machine_ops = {
+static const struct machine_ops xen_machine_ops __initconst = {
 	.restart = xen_restart,
 	.halt = xen_machine_halt,
 	.power_off = xen_machine_halt,
@@ -1332,7 +1332,7 @@ static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block __cpuinitdata xen_hvm_cpu_notifier = {
+static struct notifier_block xen_hvm_cpu_notifier __cpuinitdata = {
 	.notifier_call	= xen_hvm_cpu_notify,
 };
 
@@ -1381,7 +1381,7 @@ bool xen_hvm_need_lapic(void)
 }
 EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);
 
-const __refconst struct hypervisor_x86 x86_hyper_xen_hvm = {
+const struct hypervisor_x86 x86_hyper_xen_hvm __refconst = {
 	.name			= "Xen HVM",
 	.detect			= xen_hvm_platform,
 	.init_platform		= xen_hvm_guest_init,

commit 947ccf9c3c30307b774af3666ee74fcd9f47f646
Author: Shan Haitao <haitao.shan@intel.com>
Date:   Tue Nov 9 11:43:36 2010 -0800

    xen: Allow PV-OPS kernel to detect whether XSAVE is supported
    
    Xen fails to mask XSAVE from the cpuid feature, despite not historically
    supporting guest use of XSAVE.  However, now that XSAVE support has been
    added to Xen, we need to reliably detect its presence.
    
    The most reliable way to do this is to look at the OSXSAVE feature in
    cpuid which is set iff the OS (Xen, in this case), has set
    CR4.OSXSAVE.
    
    [ Cleaned up conditional a bit. - Jeremy ]
    
    Signed-off-by: Shan Haitao <haitao.shan@intel.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 66272a237622..e3c6a06cf725 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -238,6 +238,7 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 static __init void xen_init_cpuid_mask(void)
 {
 	unsigned int ax, bx, cx, dx;
+	unsigned int xsave_mask;
 
 	cpuid_leaf1_edx_mask =
 		~((1 << X86_FEATURE_MCE)  |  /* disable MCE */
@@ -249,8 +250,16 @@ static __init void xen_init_cpuid_mask(void)
 		cpuid_leaf1_edx_mask &=
 			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
 			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
+	ax = 1;
+	xen_cpuid(&ax, &bx, &cx, &dx);
 
-	cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_XSAVE % 32)); /* disable XSAVE */
+	xsave_mask =
+		(1 << (X86_FEATURE_XSAVE % 32)) |
+		(1 << (X86_FEATURE_OSXSAVE % 32));
+
+	/* Xen will set CR4.OSXSAVE if supported and not disabled by force */
+	if ((cx & xsave_mask) != xsave_mask)
+		cpuid_leaf1_ecx_mask &= ~xsave_mask; /* disable XSAVE & OSXSAVE */
 }
 
 static void xen_set_debugreg(int reg, unsigned long val)

commit 61f4237d5b005767a76f4f3694e68e6f78f392d9
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Sat Sep 18 22:25:30 2010 -0700

    xen: just completely disable XSAVE
    
    Some (old) versions of Xen just kill the domain if it tries to set any
    unknown bits in CR4, so we can't reliably probe for OSXSAVE in
    CR4.
    
    Since Xen doesn't support XSAVE for guests at the moment, and no such
    support is being worked on, there's no downside in just unconditionally
    masking XSAVE support.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 49dbd78ec3cb..66272a237622 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -250,23 +250,7 @@ static __init void xen_init_cpuid_mask(void)
 			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
 			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
 
-	ax = 1;
-	cx = 0;
-	xen_cpuid(&ax, &bx, &cx, &dx);
-
-	/* cpuid claims we support xsave; try enabling it to see what happens */
-	if (cx & (1 << (X86_FEATURE_XSAVE % 32))) {
-		unsigned long cr4;
-
-		set_in_cr4(X86_CR4_OSXSAVE);
-		
-		cr4 = read_cr4();
-
-		if ((cr4 & X86_CR4_OSXSAVE) == 0)
-			cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_XSAVE % 32));
-
-		clear_in_cr4(X86_CR4_OSXSAVE);
-	}
+	cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_XSAVE % 32)); /* disable XSAVE */
 }
 
 static void xen_set_debugreg(int reg, unsigned long val)

commit 76ca07832842100b14a31ad8996dab7b0c28aa42
Merge: 27d2a8b97ebc b056b6a0144d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 10:59:09 2011 -0700

    Merge branch 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm
    
    * 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm:
      xen: suspend: remove xen_hvm_suspend
      xen: suspend: pull pre/post suspend hooks out into suspend_info
      xen: suspend: move arch specific pre/post suspend hooks into generic hooks
      xen: suspend: refactor non-arch specific pre/post suspend hooks
      xen: suspend: add "arch" to pre/post suspend hooks
      xen: suspend: pass extra hypercall argument via suspend_info struct
      xen: suspend: refactor cancellation flag into a structure
      xen: suspend: use HYPERVISOR_suspend for PVHVM case instead of open coding
      xen: switch to new schedop hypercall by default.
      xen: use new schedop interface for suspend
      xen: do not respond to unknown xenstore control requests
      xen: fix compile issue if XEN is enabled but XEN_PVHVM is disabled
      xen: PV on HVM: support PV spinlocks and IPIs
      xen: make the ballon driver work for hvm domains
      xen-blkfront: handle Xen major numbers other than XENVBD
      xen: do not use xen_info on HVM, set pv_info name to "Xen HVM"
      xen: no need to delay xen_setup_shutdown_event for hvm guests anymore

commit 99bbb3a84a99cd04ab16b998b20f01a72cfa9f4f
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Dec 2 17:55:10 2010 +0000

    xen: PV on HVM: support PV spinlocks and IPIs
    
    Initialize PV spinlocks on boot CPU right after native_smp_prepare_cpus
    (that switch to APIC mode and initialize APIC routing); on secondary
    CPUs on CPU_UP_PREPARE.
    
    Enable the usage of event channels to send and receive IPIs when
    running as a PV on HVM guest.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2f67e2ea222e..fe02574789c5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1330,6 +1330,8 @@ static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
 	switch (action) {
 	case CPU_UP_PREPARE:
 		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+		if (xen_have_vector_callback)
+			xen_init_lock_cpu(cpu);
 		break;
 	default:
 		break;
@@ -1354,6 +1356,7 @@ static void __init xen_hvm_guest_init(void)
 
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;
+	xen_hvm_smp_init();
 	register_cpu_notifier(&xen_hvm_cpu_notifier);
 	xen_unplug_emulated_devices();
 	have_vcpu_info_placement = 0;

commit cff520b9c2ee1486ea9ff1dbc774510c62e5ecb9
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Dec 2 17:54:54 2010 +0000

    xen: do not use xen_info on HVM, set pv_info name to "Xen HVM"
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 50542efe45fb..2f67e2ea222e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1284,8 +1284,7 @@ static int init_hvm_pv_info(int *major, int *minor)
 
 	xen_setup_features();
 
-	pv_info = xen_info;
-	pv_info.kernel_rpl = 0;
+	pv_info.name = "Xen HVM";
 
 	xen_domain_type = XEN_HVM_DOMAIN;
 

commit 44b46c3ef805793ab3a7730dc71c72d0f258ea8e
Author: Ian Campbell <Ian.Campbell@eu.citrix.com>
Date:   Fri Feb 11 16:37:41 2011 +0000

    xen: annotate functions which only call into __init at start of day
    
    Both xen_hvm_init_shared_info and xen_build_mfn_list_list can be
    called at resume time as well as at start of day but only reference
    __init functions (extend_brk) at start of day. Hence annotate with
    __ref.
    
        WARNING: arch/x86/built-in.o(.text+0x4f1): Section mismatch in reference
            from the function xen_hvm_init_shared_info() to the function
            .init.text:extend_brk()
        The function xen_hvm_init_shared_info() references
        the function __init extend_brk().
        This is often because xen_hvm_init_shared_info lacks a __init
        annotation or the annotation of extend_brk is wrong.
    
    xen_hvm_init_shared_info calls extend_brk() iff !shared_info_page and
    initialises shared_info_page with the result. This happens at start of
    day only.
    
        WARNING: arch/x86/built-in.o(.text+0x599b): Section mismatch in reference
            from the function xen_build_mfn_list_list() to the function
            .init.text:extend_brk()
        The function xen_build_mfn_list_list() references
        the function __init extend_brk().
        This is often because xen_build_mfn_list_list lacks a __init
        annotation or the annotation of extend_brk is wrong.
    
    (this warning occurs multiple times)
    
    xen_build_mfn_list_list only calls extend_brk() at boot time, while
    building the initial mfn list list
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 50542efe45fb..28e6d42ce2b8 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1292,7 +1292,7 @@ static int init_hvm_pv_info(int *major, int *minor)
 	return 0;
 }
 
-void xen_hvm_init_shared_info(void)
+void __ref xen_hvm_init_shared_info(void)
 {
 	int cpu;
 	struct xen_add_to_physmap xatp;

commit 2ce802f62ba32a7d95748ac92bf351f76affb6ff
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 20 12:06:35 2011 +0100

    lockdep: Move early boot local IRQ enable/disable status to init/main.c
    
    During early boot, local IRQ is disabled until IRQ subsystem is
    properly initialized.  During this time, no one should enable
    local IRQ and some operations which usually are not allowed with
    IRQ disabled, e.g. operations which might sleep or require
    communications with other processors, are allowed.
    
    lockdep tracked this with early_boot_irqs_off/on() callbacks.
    As other subsystems need this information too, move it to
    init/main.c and make it generally available.  While at it,
    toggle the boolean to early_boot_irqs_disabled instead of
    enabled so that it can be initialized with %false and %true
    indicates the exceptional condition.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <20110120110635.GB6036@htj.dyndns.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7e8d3bc80af6..50542efe45fb 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1194,7 +1194,7 @@ asmlinkage void __init xen_start_kernel(void)
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
 
 	local_irq_disable();
-	early_boot_irqs_off();
+	early_boot_irqs_disabled = true;
 
 	memblock_init();
 

commit 9f99a2f0e44663517b99b69a3e4a499d0ba877df
Merge: 8c8ae4e8cd5a 110e7c7e4f8a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 10 08:48:46 2011 -0800

    Merge branch 'stable/bug-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/bug-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/event: validate irq before get evtchn by irq
      xen/fb: fix potential memory leak
      xen/fb: fix xenfb suspend/resume race.
      xen: disable ACPI NUMA for PV guests
      xen/irq: Cleanup the find_unbound_irq

commit 8c8ae4e8cd5a67467192f3361eeec463694f8ed8
Merge: a1e8fad5900f d9b8ca8474fd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 10 08:48:29 2011 -0800

    Merge branch 'stable/generic' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/generic' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: HVM X2APIC support
      apic: Move hypervisor detection of x2apic to hypervisor.h

commit c1f5db1a53a7286f6c818f71016ae6aced814d27
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Fri Dec 3 09:54:03 2010 +0000

    xen: disable ACPI NUMA for PV guests
    
    Xen does not currently expose PV-NUMA information to PV
    guests. Therefore disable NUMA for the time being to prevent the
    kernel picking up on an host-level NUMA information which it might
    come across in the firmware.
    
    [ Added comment - Jeremy ]
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 44dcad43989d..ed47d12fdecb 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1174,6 +1174,15 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_smp_init();
 
+#ifdef CONFIG_ACPI_NUMA
+	/*
+	 * The pages we from Xen are not related to machine pages, so
+	 * any NUMA information the kernel tries to get from ACPI will
+	 * be meaningless.  Prevent it from trying.
+	 */
+	acpi_numa = -1;
+#endif
+
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
 	if (!xen_initial_domain())

commit d9b8ca8474fd4fdd43ba6d97a4fee8b49b978067
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Dec 21 14:18:49 2010 +0800

    xen: HVM X2APIC support
    
    This patch is similiar to Gleb Natapov's patch for KVM, which enable the
    hypervisor to emulate x2apic feature for the guest. By this way, the emulation
    of lapic would be simpler with x2apic interface(MSR), and faster.
    
    [v2: Re-organized 'xen_hvm_need_lapic' per Ian Campbell suggestion]
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 44dcad43989d..5b8986fe3371 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1256,25 +1256,6 @@ asmlinkage void __init xen_start_kernel(void)
 #endif
 }
 
-static uint32_t xen_cpuid_base(void)
-{
-	uint32_t base, eax, ebx, ecx, edx;
-	char signature[13];
-
-	for (base = 0x40000000; base < 0x40010000; base += 0x100) {
-		cpuid(base, &eax, &ebx, &ecx, &edx);
-		*(uint32_t *)(signature + 0) = ebx;
-		*(uint32_t *)(signature + 4) = ecx;
-		*(uint32_t *)(signature + 8) = edx;
-		signature[12] = 0;
-
-		if (!strcmp("XenVMMXenVMM", signature) && ((eax - base) >= 2))
-			return base;
-	}
-
-	return 0;
-}
-
 static int init_hvm_pv_info(int *major, int *minor)
 {
 	uint32_t eax, ebx, ecx, edx, pages, msr, base;
@@ -1384,6 +1365,18 @@ static bool __init xen_hvm_platform(void)
 	return true;
 }
 
+bool xen_hvm_need_lapic(void)
+{
+	if (xen_pv_domain())
+		return false;
+	if (!xen_hvm_domain())
+		return false;
+	if (xen_feature(XENFEAT_hvm_pirqs) && xen_have_vector_callback)
+		return false;
+	return true;
+}
+EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);
+
 const __refconst struct hypervisor_x86 x86_hyper_xen_hvm = {
 	.name			= "Xen HVM",
 	.detect			= xen_hvm_platform,

commit 780f36d8b3fa9572f731d4fb85067b2e45e6f993
Author: Christoph Lameter <cl@linux.com>
Date:   Mon Dec 6 11:16:29 2010 -0600

    xen: Use this_cpu_ops
    
    Use this_cpu_ops to reduce code size and simplify things in various places.
    
    V3->V4:
            Move instance of this_cpu_inc_return to a later patchset so that
            this patch can be applied without infrastructure changes.
    
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 44dcad43989d..aa8c89ae54cf 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -574,8 +574,8 @@ static void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)
 
 	preempt_disable();
 
-	start = __get_cpu_var(idt_desc).address;
-	end = start + __get_cpu_var(idt_desc).size + 1;
+	start = __this_cpu_read(idt_desc.address);
+	end = start + __this_cpu_read(idt_desc.size) + 1;
 
 	xen_mc_flush();
 

commit 8338fded137681bc3c1e99a69ac937a4fb016fe4
Merge: 9cd6315357ec 29dcbc5c25d6 805e3f495057
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 3 10:08:52 2010 -0800

    Merge branches 'upstream/core' and 'upstream/bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen: allocate irq descs on any NUMA node
      xen: prevent crashes with non-HIGHMEM 32-bit kernels with largeish memory
      xen: use default_idle
      xen: clean up "extra" memory handling some more
    
    * 'upstream/bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen: x86/32: perform initial startup on initial_page_table
      xen: don't bother to stop other cpus on shutdown/reboot

commit 805e3f495057aa5307ad4e3d6dc7073d4733c691
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Wed Nov 3 15:32:21 2010 +0000

    xen: x86/32: perform initial startup on initial_page_table
    
    Only make swapper_pg_dir readonly and pinned when generic x86 architecture code
    (which also starts on initial_page_table) switches to it.  This helps ensure
    that the generic setup paths work on Xen unmodified. In particular
    clone_pgd_range writes directly to the destination pgd entries and is used to
    initialise swapper_pg_dir so we need to ensure that it remains writeable until
    the last possible moment during bring up.
    
    This is complicated slightly by the need to avoid sharing kernel PMD entries
    when running under Xen, therefore the Xen implementation must make a copy of
    the kernel PMD (which is otherwise referred to by both intial_page_table and
    swapper_pg_dir) before switching to swapper_pg_dir.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 4a5973abefc8..0db7303bccb6 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1187,8 +1187,6 @@ asmlinkage void __init xen_start_kernel(void)
 	/* Allocate and initialize top and mid mfn levels for p2m structure */
 	xen_build_mfn_list_list();
 
-	init_mm.pgd = pgd;
-
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 #ifdef CONFIG_X86_32

commit 31e323cca9d5c8afd372976c35a5d46192f540d1
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Nov 29 14:16:53 2010 -0800

    xen: don't bother to stop other cpus on shutdown/reboot
    
    Xen will shoot all the VCPUs when we do a shutdown hypercall, so there's
    no need to do it manually.
    
    In any case it will fail because all the IPI irqs have been pulled
    down by this point, so the cross-CPU calls will simply hang forever.
    
    Until change 76fac077db6b34e2c6383a7b4f3f4f7b7d06d8ce the function calls
    were not synchronously waited for, so this wasn't apparent.  However after
    that change the calls became synchronous leading to a hang on shutdown
    on multi-VCPU guests.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>
    Cc: Alok Kataria <akataria@vmware.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 235c0f4d3861..4a5973abefc8 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1016,10 +1016,6 @@ static void xen_reboot(int reason)
 {
 	struct sched_shutdown r = { .reason = reason };
 
-#ifdef CONFIG_SMP
-	stop_other_cpus();
-#endif
-
 	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
 		BUG();
 }

commit 8a3fbc9fdb4fd8f0970ab1a98f694ff25b7d5b47
Merge: c12ae95ccc2d e6d4a76dbf2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 25 08:35:53 2010 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: remove duplicated #include
      xen: x86/32: perform initial startup on initial_page_table

commit 5b5c1af104ab5adec1be9dcb4c787492d83d8d83
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Wed Nov 24 12:09:41 2010 +0000

    xen: x86/32: perform initial startup on initial_page_table
    
    Only make swapper_pg_dir readonly and pinned when generic x86 architecture code
    (which also starts on initial_page_table) switches to it.  This helps ensure
    that the generic setup paths work on Xen unmodified. In particular
    clone_pgd_range writes directly to the destination pgd entries and is used to
    initialise swapper_pg_dir so we need to ensure that it remains writeable until
    the last possible moment during bring up.
    
    This is complicated slightly by the need to avoid sharing kernel PMD entries
    when running under Xen, therefore the Xen implementation must make a copy of
    the kernel PMD (which is otherwise referred to by both intial_page_table and
    swapper_pg_dir) before switching to swapper_pg_dir.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 235c0f4d3861..ff82909801b6 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1191,8 +1191,6 @@ asmlinkage void __init xen_start_kernel(void)
 	/* Allocate and initialize top and mid mfn levels for p2m structure */
 	xen_build_mfn_list_list();
 
-	init_mm.pgd = pgd;
-
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 #ifdef CONFIG_X86_32

commit ec35a69c467026437519bafcf325a7362e422db9
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Nov 16 12:09:59 2010 -0500

    xen: set IO permission early (before early_cpu_init())
    
    This patch is based off "xen dom0: Set up basic IO permissions for dom0."
    by Juan Quintela <quintela@redhat.com>.
    
    On AMD machines when we boot the kernel as Domain 0 we get this nasty:
    
    mapping kernel into physical memory
    Xen: setup ISA identity maps
    about to get started...
    (XEN) traps.c:475:d0 Unhandled general protection fault fault/trap [#13] on VCPU 0 [ec=0000]
    (XEN) domain_crash_sync called from entry.S
    (XEN) Domain 0 (vcpu#0) crashed on cpu#0:
    (XEN) ----[ Xen-4.1-101116  x86_64  debug=y  Not tainted ]----
    (XEN) CPU:    0
    (XEN) RIP:    e033:[<ffffffff8130271b>]
    (XEN) RFLAGS: 0000000000000282   EM: 1   CONTEXT: pv guest
    (XEN) rax: 000000008000c068   rbx: ffffffff8186c680   rcx: 0000000000000068
    (XEN) rdx: 0000000000000cf8   rsi: 000000000000c000   rdi: 0000000000000000
    (XEN) rbp: ffffffff81801e98   rsp: ffffffff81801e50   r8:  ffffffff81801eac
    (XEN) r9:  ffffffff81801ea8   r10: ffffffff81801eb4   r11: 00000000ffffffff
    (XEN) r12: ffffffff8186c694   r13: ffffffff81801f90   r14: ffffffffffffffff
    (XEN) r15: 0000000000000000   cr0: 000000008005003b   cr4: 00000000000006f0
    (XEN) cr3: 0000000221803000   cr2: 0000000000000000
    (XEN) ds: 0000   es: 0000   fs: 0000   gs: 0000   ss: e02b   cs: e033
    (XEN) Guest stack trace from rsp=ffffffff81801e50:
    
    RIP points to read_pci_config() function.
    
    The issue is that we don't set IO permissions for the Linux kernel early enough.
    
    The call sequence used to be:
    
        xen_start_kernel()
            x86_init.oem.arch_setup = xen_setup_arch;
            setup_arch:
               - early_cpu_init
                   - early_init_amd
                      - read_pci_config
               - x86_init.oem.arch_setup [ xen_arch_setup ]
                   - set IO permissions.
    
    We need to set the IO permissions earlier on, which this patch does.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bd3554934613..7250bef7f49e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1095,6 +1095,8 @@ static void __init xen_setup_stackprotector(void)
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {
+	struct physdev_set_iopl set_iopl;
+	int rc;
 	pgd_t *pgd;
 
 	if (!xen_start_info)
@@ -1209,10 +1211,18 @@ asmlinkage void __init xen_start_kernel(void)
 #else
 	pv_info.kernel_rpl = 0;
 #endif
-
 	/* set the limit of our address space */
 	xen_reserve_top();
 
+	/* We used to do this in xen_arch_setup, but that is too late on AMD
+	 * were early_cpu_init (run before ->arch_setup()) calls early_amd_init
+	 * which pokes 0xcf8 port.
+	 */
+	set_iopl.iopl = 1;
+	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
+	if (rc != 0)
+		xen_raw_printk("physdev_op failed %d\n", rc);
+
 #ifdef CONFIG_X86_32
 	/* set up basic CPUID stuff */
 	cpu_detect(&new_cpu_data);

commit 7e77506a5918d82cafa2ffa783ab57c23f9e9817
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Thu Sep 30 12:37:26 2010 +0100

    xen: implement XENMEM_machphys_mapping
    
    This hypercall allows Xen to specify a non-default location for the
    machine to physical mapping. This capability is used when running a 32
    bit domain 0 on a 64 bit hypervisor to shrink the hypervisor hole to
    exactly the size required.
    
    [ Impact: add Xen hypercall definitions ]
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 235c0f4d3861..bd3554934613 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -75,6 +75,11 @@ DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 enum xen_domain_type xen_domain_type = XEN_NATIVE;
 EXPORT_SYMBOL_GPL(xen_domain_type);
 
+unsigned long *machine_to_phys_mapping = (void *)MACH2PHYS_VIRT_START;
+EXPORT_SYMBOL(machine_to_phys_mapping);
+unsigned int   machine_to_phys_order;
+EXPORT_SYMBOL(machine_to_phys_order);
+
 struct start_info *xen_start_info;
 EXPORT_SYMBOL_GPL(xen_start_info);
 
@@ -1097,6 +1102,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_domain_type = XEN_PV_DOMAIN;
 
+	xen_setup_machphys_mapping();
+
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;

commit 18cb657ca1bafe635f368346a1676fb04c512edf
Merge: 2301b65b86df e28c31a96b15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 28 17:11:17 2010 -0700

    Merge branch 'stable/xen-pcifront-0.8.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
      and branch 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm
    
    * 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm:
      xen: register xen pci notifier
      xen: initialize cpu masks for pv guests in xen_smp_init
      xen: add a missing #include to arch/x86/pci/xen.c
      xen: mask the MTRR feature from the cpuid
      xen: make hvc_xen console work for dom0.
      xen: add the direct mapping area for ISA bus access
      xen: Initialize xenbus for dom0.
      xen: use vcpu_ops to setup cpu masks
      xen: map a dummy page for local apic and ioapic in xen_set_fixmap
      xen: remap MSIs into pirqs when running as initial domain
      xen: remap GSIs as pirqs when running as initial domain
      xen: introduce XEN_DOM0 as a silent option
      xen: map MSIs into pirqs
      xen: support GSI -> pirq remapping in PV on HVM guests
      xen: add xen hvm acpi_register_gsi variant
      acpi: use indirect call to register gsi in different modes
      xen: implement xen_hvm_register_pirq
      xen: get the maximum number of pirqs from xen
      xen: support pirq != irq
    
    * 'stable/xen-pcifront-0.8.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen: (27 commits)
      X86/PCI: Remove the dependency on isapnp_disable.
      xen: Update Makefile with CONFIG_BLOCK dependency for biomerge.c
      MAINTAINERS: Add myself to the Xen Hypervisor Interface and remove Chris Wright.
      x86: xen: Sanitse irq handling (part two)
      swiotlb-xen: On x86-32 builts, select SWIOTLB instead of depending on it.
      MAINTAINERS: Add myself for Xen PCI and Xen SWIOTLB maintainer.
      xen/pci: Request ACS when Xen-SWIOTLB is activated.
      xen-pcifront: Xen PCI frontend driver.
      xenbus: prevent warnings on unhandled enumeration values
      xenbus: Xen paravirtualised PCI hotplug support.
      xen/x86/PCI: Add support for the Xen PCI subsystem
      x86: Introduce x86_msi_ops
      msi: Introduce default_[teardown|setup]_msi_irqs with fallback.
      x86/PCI: Export pci_walk_bus function.
      x86/PCI: make sure _PAGE_IOMAP it set on pci mappings
      x86/PCI: Clean up pci_cache_line_size
      xen: fix shared irq device passthrough
      xen: Provide a variant of xen_poll_irq with timeout.
      xen: Find an unbound irq number in reverse order (high to low).
      xen: statically initialize cpu_evtchn_mask_p
      ...
    
    Fix up trivial conflicts in drivers/pci/Makefile

commit 17bb51d56cdc8cbf252031db3107de034cfeb44c
Merge: 0671b7674f42 95aac7b1cd22
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 27 18:42:52 2010 -0700

    Merge branch 'akpm-incoming-2'
    
    * akpm-incoming-2: (139 commits)
      epoll: make epoll_wait() use the hrtimer range feature
      select: rename estimate_accuracy() to select_estimate_accuracy()
      Remove duplicate includes from many files
      ramoops: use the platform data structure instead of module params
      kernel/resource.c: handle reinsertion of an already-inserted resource
      kfifo: fix kfifo_alloc() to return a signed int value
      w1: don't allow arbitrary users to remove w1 devices
      alpha: remove dma64_addr_t usage
      mips: remove dma64_addr_t usage
      sparc: remove dma64_addr_t usage
      fuse: use release_pages()
      taskstats: use real microsecond granularity for CPU times
      taskstats: split fill_pid function
      taskstats: separate taskstats commands
      delayacct: align to 8 byte boundary on 64-bit systems
      delay-accounting: reimplement -c for getdelays.c to report information on a target command
      namespaces Kconfig: move namespace menu location after the cgroup
      namespaces Kconfig: remove the cgroup device whitelist experimental tag
      namespaces Kconfig: remove pointless cgroup dependency
      namespaces Kconfig: make namespace a submenu
      ...

commit 0671b7674f42ab3a200401ea0e48d6f47d34acae
Merge: 0b2d8d9e562d 47f19a0814e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 27 18:38:55 2010 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      percpu: Remove the multi-page alignment facility
      x86-32: Allocate irq stacks seperate from percpu area
      x86-32, mm: Remove duplicated #include
      x86, printk: Get rid of <0> from stack output
      x86, kexec: Make sure to stop all CPUs before exiting the kernel
      x86/vsmp: Eliminate kconfig dependency warning

commit 61d8e11e519ee7912ab59610fba1aaf08e3c1d84
Author: Zimny Lech <napohybelskurwysynom2010@gmail.com>
Date:   Wed Oct 27 15:34:53 2010 -0700

    Remove duplicate includes from many files
    
    Signed-off-by: Zimny Lech <napohybelskurwysynom2010@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 44ab12dc2a12..0cd12db0b142 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -59,7 +59,6 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/reboot.h>
-#include <asm/setup.h>
 #include <asm/stackprotector.h>
 #include <asm/hypervisor.h>
 

commit 520045db940a381d2bee1c1b2179f7921b40fb10
Merge: 426e1f5cec48 9387377eb79a 45263cb0993d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 26 18:20:19 2010 -0700

    Merge branches 'upstream/xenfs' and 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'upstream/xenfs' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen/privcmd: make privcmd visible in domU
      xen/privcmd: move remap_domain_mfn_range() to core xen code and export.
      privcmd: MMAPBATCH: Fix error handling/reporting
      xenbus: export xen_store_interface for xenfs
      xen/privcmd: make sure vma is ours before doing anything to it
      xen/privcmd: print SIGBUS faults
      xen/xenfs: set_page_dirty is supposed to return true if it dirties
      xen/privcmd: create address space to allow writable mmaps
      xen: add privcmd driver
      xen: add variable hypercall caller
      xen: add xen_set_domain_pte()
      xen: add /proc/xen/xsd_{kva,port} to xenfs
    
    * 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen: (29 commits)
      xen: include xen/xen.h for definition of xen_initial_domain()
      xen: use host E820 map for dom0
      xen: correctly rebuild mfn list list after migration.
      xen: improvements to VIRQ_DEBUG output
      xen: set up IRQ before binding virq to evtchn
      xen: ensure that all event channels start off bound to VCPU 0
      xen/hvc: only notify if we actually sent something
      xen: don't add extra_pages for RAM after mem_end
      xen: add support for PAT
      xen: make sure xen_max_p2m_pfn is up to date
      xen: limit extra memory to a certain ratio of base
      xen: add extra pages for E820 RAM regions, even if beyond mem_end
      xen: make sure xen_extra_mem_start is beyond all non-RAM e820
      xen: implement "extra" memory to reserve space for pages not present at boot
      xen: Use host-provided E820 map
      xen: don't map missing memory
      xen: defer building p2m mfn structures until kernel is mapped
      xen: add return value to set_phys_to_machine()
      xen: convert p2m to a 3 level tree
      xen: make install_p2mtop_page() static
      ...
    
    Fix up trivial conflict in arch/x86/xen/mmu.c, and fix the use of
    'reserve_early()' - in the new memblock world order it is now
    'memblock_x86_reserve_range()' instead. Pointed out by Jeremy.

commit 7d7a48b76012eeee25871e1d69104dec4b6d740f
Merge: e4072a9a9d18 229aebb873e2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 25 19:38:43 2010 +0200

    Merge branch 'linus' into x86/urgent
    
    Merge reason: We want to queue up a dependent fix.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ff12849a7a187e17fcbd888b39850d22103395c6
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Tue Sep 28 16:45:51 2010 +0100

    xen: mask the MTRR feature from the cpuid
    
    We don't want Linux to think that the cpu supports MTRRs when running
    under Xen because MTRR operations could only be performed through
    hypercalls.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9efb00446250..d48a32b10a3c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -243,6 +243,7 @@ static __init void xen_init_cpuid_mask(void)
 	cpuid_leaf1_edx_mask =
 		~((1 << X86_FEATURE_MCE)  |  /* disable MCE */
 		  (1 << X86_FEATURE_MCA)  |  /* disable MCA */
+		  (1 << X86_FEATURE_MTRR) |  /* disable MTRR */
 		  (1 << X86_FEATURE_ACC));   /* thermal monitoring */
 
 	if (!xen_initial_domain())

commit 4ec5387cc36c6472a2ff2c82e9865abe8cab96c2
Author: Juan Quintela <quintela@redhat.com>
Date:   Thu Sep 2 15:45:43 2010 +0100

    xen: add the direct mapping area for ISA bus access
    
    add the direct mapping area for ISA bus access when running as initial
    domain
    
    Signed-off-by: Juan Quintela <quintela@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1ccfa1bf0f89..9efb00446250 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1186,6 +1186,7 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
+	xen_ident_map_ISA();
 
 	init_mm.pgd = pgd;
 

commit 41f2e4771a4f1ba26c35438daf32917b9ef7858d
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Mar 30 11:47:40 2010 -0700

    xen: add support for PAT
    
    Convert Linux PAT entries into Xen ones when constructing ptes.  Linux
    doesn't use _PAGE_PAT for ptes, so the only difference in the first 4
    entries is that Linux uses _PAGE_PWT for WC, whereas Xen (and default)
    use it for WT.
    
    xen_pte_val does the inverse conversion.
    
    We hard-code assumptions about Linux's current PAT layout, but a
    warning on the wrmsr to MSR_IA32_CR_PAT should point out any problems.
    If necessary we could go to a more general table-based conversion between
    Linux and Xen PAT entries.
    
    hugetlbfs poses a problem at the moment, the x86 architecture uses the
    same flag for _PAGE_PAT and _PAGE_PSE, which changes meaning depending
    on which pagetable level we're using.  At the moment this should be OK
    so long as nobody tries to do a pte_val on a hugetlbfs pte.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d8873014b5ed..b860e576ed0c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -829,6 +829,11 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 		   Xen console noise. */
 		break;
 
+	case MSR_IA32_CR_PAT:
+		if (smp_processor_id() == 0)
+			xen_set_pat(((u64)high << 32) | low);
+		break;
+
 	default:
 		ret = native_write_msr_safe(msr, low, high);
 	}

commit 33a847502b0338351cebd8fc0c68ac796cfadbbd
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Aug 27 15:18:19 2010 -0700

    xen: defer building p2m mfn structures until kernel is mapped
    
    When building mfn parts of p2m structure, we rely on being able to
    use mfn_to_virt, which in turn requires kernel to be mapped into
    the linear area (which is distinct from the kernel image mapping
    on 64-bit).  Defer calling xen_build_mfn_list_list() until after
    xen_setup_kernel_pagetable();
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ee304b52d8b7..d8873014b5ed 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1178,6 +1178,9 @@ asmlinkage void __init xen_start_kernel(void)
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
 
+	/* Allocate and initialize top and mid mfn levels for p2m structure */
+	xen_build_mfn_list_list();
+
 	init_mm.pgd = pgd;
 
 	/* keep using Xen gdt for now; no urgent need to change it */

commit 1e17fc7eff56d23a835d5d33e71d813aa9eb8ecc
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Sep 3 15:04:08 2010 -0700

    xen: remove noise about registering vcpu info
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7d46c8441418..ee304b52d8b7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -135,9 +135,6 @@ static void xen_vcpu_setup(int cpu)
 	info.mfn = arbitrary_virt_to_mfn(vcpup);
 	info.offset = offset_in_page(vcpup);
 
-	printk(KERN_DEBUG "trying to map vcpu_info %d at %p, mfn %llx, offset %d\n",
-	       cpu, vcpup, info.mfn, info.offset);
-
 	/* Check to see if the hypervisor will put the vcpu_info
 	   structure where we want it, which allows direct access via
 	   a percpu-variable. */
@@ -151,9 +148,6 @@ static void xen_vcpu_setup(int cpu)
 		/* This cpu is using the registered vcpu info, even if
 		   later ones fail to. */
 		per_cpu(xen_vcpu, cpu) = vcpup;
-
-		printk(KERN_DEBUG "cpu %d using vcpu_info at %p\n",
-		       cpu, vcpup);
 	}
 }
 
@@ -873,8 +867,6 @@ void xen_setup_vcpu_info_placement(void)
 	/* xen_vcpu_setup managed to place the vcpu_info within the
 	   percpu area for all cpus, so make use of it */
 	if (have_vcpu_info_placement) {
-		printk(KERN_INFO "Xen: using vcpu_info placement\n");
-
 		pv_irq_ops.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);
 		pv_irq_ops.restore_fl = __PV_IS_CALLEE_SAVE(xen_restore_fl_direct);
 		pv_irq_ops.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);

commit 76fac077db6b34e2c6383a7b4f3f4f7b7d06d8ce
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Oct 11 14:37:08 2010 -0700

    x86, kexec: Make sure to stop all CPUs before exiting the kernel
    
    x86 smp_ops now has a new op, stop_other_cpus which takes a parameter
    "wait" this allows the caller to specify if it wants to stop until all
    the cpus have processed the stop IPI.  This is required specifically
    for the kexec case where we should wait for all the cpus to be stopped
    before starting the new kernel.  We now wait for the cpus to stop in
    all cases except for panic/kdump where we expect things to be broken
    and we are doing our best to make things work anyway.
    
    This patch fixes a legitimate regression, which was introduced during
    2.6.30, by commit id 4ef702c10b5df18ab04921fc252c26421d4d6c75.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    LKML-Reference: <1286833028.1372.20.camel@ank32.eng.vmware.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: <stable@kernel.org> v2.6.30-36
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7d46c8441418..44f80861382f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1018,7 +1018,7 @@ static void xen_reboot(int reason)
 	struct sched_shutdown r = { .reason = reason };
 
 #ifdef CONFIG_SMP
-	smp_send_stop();
+	stop_other_cpus();
 #endif
 
 	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))

commit b5401a96b59475c1c878439caecb8c521bdfd4ad
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Thu Mar 18 16:31:34 2010 -0400

    xen/x86/PCI: Add support for the Xen PCI subsystem
    
    The frontend stub lives in arch/x86/pci/xen.c, alongside other
    sub-arch PCI init code (e.g. olpc.c).
    
    It provides a mechanism for Xen PCI frontend to setup/destroy
    legacy interrupts, MSI/MSI-X, and PCI configuration operations.
    
    [ Impact: add core of Xen PCI support ]
    [ v2: Removed the IOMMU code and only focusing on PCI.]
    [ v3: removed usage of pci_scan_all_fns as that does not exist]
    [ v4: introduced pci_xen value to fix compile warnings]
    [ v5: squished fixes+features in one patch, changed Reviewed-by to Ccs]
    [ v7: added Acked-by]
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Acked-by: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Cc: Qing He <qing.he@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7d46c8441418..1ccfa1bf0f89 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -45,6 +45,7 @@
 #include <asm/paravirt.h>
 #include <asm/apic.h>
 #include <asm/page.h>
+#include <asm/xen/pci.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/fixmap.h>
@@ -1220,6 +1221,8 @@ asmlinkage void __init xen_start_kernel(void)
 		add_preferred_console("xenboot", 0, NULL);
 		add_preferred_console("tty", 0, NULL);
 		add_preferred_console("hvc", 0, NULL);
+		if (pci_xen)
+			x86_init.pci.arch_init = pci_xen_init;
 	} else {
 		/* Make sure ACS will be enabled */
 		pci_request_acs();

commit 236260b90dd94516982ad67aa6f5449c4c37db7b
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Oct 6 15:52:29 2010 -0700

    memblock: Allow memblock_init to be called early
    
    The Xen setup code needs to call memblock_x86_reserve_range() very early,
    so allow it to initialize the memblock subsystem before doing so.  The
    second memblock_init() is ignored.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    LKML-Reference: <4CACFDAD.3090900@goop.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7d46c8441418..63b83ceebd1a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -30,6 +30,7 @@
 #include <linux/console.h>
 #include <linux/pci.h>
 #include <linux/gfp.h>
+#include <linux/memblock.h>
 
 #include <xen/xen.h>
 #include <xen/interface/xen.h>
@@ -1183,6 +1184,8 @@ asmlinkage void __init xen_start_kernel(void)
 	local_irq_disable();
 	early_boot_irqs_off();
 
+	memblock_init();
+
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
 

commit 26f0cf91813bdc8e61595f8ad6660251e2ee9cf6
Merge: d862b13bc8cb fe96eb404e33
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 12 09:09:41 2010 -0700

    Merge branch 'stable/xen-swiotlb-0.8.6' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/xen-swiotlb-0.8.6' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      x86: Detect whether we should use Xen SWIOTLB.
      pci-swiotlb-xen: Add glue code to setup dma_ops utilizing xen_swiotlb_* functions.
      swiotlb-xen: SWIOTLB library for Xen PV guest with PCI passthrough.
      xen/mmu: inhibit vmap aliases rather than trying to clear them out
      vmap: add flag to allow lazy unmap to be disabled at runtime
      xen: Add xen_create_contiguous_region
      xen: Rename the balloon lock
      xen: Allow unprivileged Xen domains to create iomap pages
      xen: use _PAGE_IOMAP in ioremap to do machine mappings
    
    Fix up trivial conflicts (adding both xen swiotlb and xen pci platform
    driver setup close to each other) in drivers/xen/{Kconfig,Makefile} and
    include/xen/xen-ops.h

commit ca50a5f39041497253c6362f2ba4da1b56d3e6cb
Merge: a70ce4b6064b ca65f9fc0c44
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Aug 4 14:49:16 2010 -0700

    Merge branch 'upstream/pvhvm' into upstream/xen
    
    * upstream/pvhvm:
      Introduce CONFIG_XEN_PVHVM compile option
      blkfront: do not create a PV cdrom device if xen_hvm_guest
      support multiple .discard.* sections to avoid section type conflicts
      xen/pvhvm: fix build problem when !CONFIG_XEN
      xenfs: enable for HVM domains too
      x86: Call HVMOP_pagetable_dying on exit_mmap.
      x86: Unplug emulated disks and nics.
      x86: Use xen_vcpuop_clockevent, xen_clocksource and xen wallclock.
      xen: Fix find_unbound_irq in presence of ioapic irqs.
      xen: Add suspend/resume support for PV on HVM guests.
      xen: Xen PCI platform device driver.
      x86/xen: event channels delivery on HVM.
      x86: early PV on HVM features initialization.
      xen: Add support for HVM hypercalls.
    
    Conflicts:
            arch/x86/xen/enlighten.c
            arch/x86/xen/time.c

commit 086748e52fb072ff0935ba4512e29c421bd5b716
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Tue Aug 3 14:55:14 2010 -0700

    xen/panic: use xen_reboot and fix smp_send_stop
    
    Offline vcpu when using stop_self.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d99522e8f033..3c4da8bee06f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1043,10 +1043,7 @@ static void xen_crash_shutdown(struct pt_regs *regs)
 static int
 xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
 {
-	struct sched_shutdown r = { .reason = SHUTDOWN_crash};
-
-	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
-		BUG();
+	xen_reboot(SHUTDOWN_crash);
 	return NOTIFY_DONE;
 }
 

commit f09f6d194d85043e0eb105a577e7ad6d8170ab66
Author: Donald Dutile <ddutile@redhat.com>
Date:   Thu Jul 15 14:56:49 2010 -0400

    Xen: register panic notifier to take crashes of xen guests on panic
    
    Register a panic notifier so that when the guest crashes it can shut
    down the domain and indicate it was a crash to the host.
    
    Signed-off-by: Donald Dutile <ddutile@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 90a3e8026767..d99522e8f033 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1040,6 +1040,26 @@ static void xen_crash_shutdown(struct pt_regs *regs)
 	xen_reboot(SHUTDOWN_crash);
 }
 
+static int
+xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
+{
+	struct sched_shutdown r = { .reason = SHUTDOWN_crash};
+
+	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
+		BUG();
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block xen_panic_block = {
+	.notifier_call= xen_panic_event,
+};
+
+int xen_panic_handler_init(void)
+{
+	atomic_notifier_chain_register(&panic_notifier_list, &xen_panic_block);
+	return 0;
+}
+
 static const struct machine_ops __initdata xen_machine_ops = {
 	.restart = xen_restart,
 	.halt = xen_machine_halt,

commit c06ee78d73fd24e8d8a65f16380f6a0551107e1b
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Mon Jul 19 10:25:08 2010 -0700

    xen: support large numbers of CPUs with vcpu info placement
    
    When vcpu info placement is supported, we're not limited to MAX_VIRT_CPUS
    vcpus.  However, if it isn't supported, then ignore any excess vcpus.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index fef034a04c24..90a3e8026767 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -97,6 +97,14 @@ struct shared_info *HYPERVISOR_shared_info = (void *)&xen_dummy_shared_info;
  */
 static int have_vcpu_info_placement = 1;
 
+static void clamp_max_cpus(void)
+{
+#ifdef CONFIG_SMP
+	if (setup_max_cpus > MAX_VIRT_CPUS)
+		setup_max_cpus = MAX_VIRT_CPUS;
+#endif
+}
+
 static void xen_vcpu_setup(int cpu)
 {
 	struct vcpu_register_vcpu_info info;
@@ -104,13 +112,17 @@ static void xen_vcpu_setup(int cpu)
 	struct vcpu_info *vcpup;
 
 	BUG_ON(HYPERVISOR_shared_info == &xen_dummy_shared_info);
-	per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 
-	if (!have_vcpu_info_placement)
-		return;		/* already tested, not available */
+	if (cpu < MAX_VIRT_CPUS)
+		per_cpu(xen_vcpu,cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 
-	vcpup = &per_cpu(xen_vcpu_info, cpu);
+	if (!have_vcpu_info_placement) {
+		if (cpu >= MAX_VIRT_CPUS)
+			clamp_max_cpus();
+		return;
+	}
 
+	vcpup = &per_cpu(xen_vcpu_info, cpu);
 	info.mfn = arbitrary_virt_to_mfn(vcpup);
 	info.offset = offset_in_page(vcpup);
 
@@ -125,6 +137,7 @@ static void xen_vcpu_setup(int cpu)
 	if (err) {
 		printk(KERN_DEBUG "register_vcpu_info failed: err=%d\n", err);
 		have_vcpu_info_placement = 0;
+		clamp_max_cpus();
 	} else {
 		/* This cpu is using the registered vcpu info, even if
 		   later ones fail to. */

commit 8a22b9996b001c88f2bfb54c6de6a05fc39e177a
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Jul 12 11:49:59 2010 -0700

    xen: drop xen_sched_clock in favour of using plain wallclock time
    
    xen_sched_clock only counts unstolen time.  In principle this should
    be useful to the Linux scheduler so that it knows how much time a process
    actually consumed.  But in practice this doesn't work very well as the
    scheduler expects the sched_clock time to be synchronized between
    cpus.  It also uses sched_clock to measure the time a task spends
    sleeping, in which case "unstolen time" isn't meaningful.
    
    So just use plain xen_clocksource_read to return wallclock nanoseconds
    for sched_clock.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 399bed2de881..fef034a04c24 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -926,7 +926,7 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 };
 
 static const struct pv_time_ops xen_time_ops __initdata = {
-	.sched_clock = xen_sched_clock,
+	.sched_clock = xen_clocksource_read,
 };
 
 static const struct pv_cpu_ops xen_cpu_ops __initdata = {

commit ca65f9fc0c447da5b270b05c41c21b19c88617c3
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Jul 29 14:37:48 2010 +0100

    Introduce CONFIG_XEN_PVHVM compile option
    
    This patch introduce a CONFIG_XEN_PVHVM compile time option to
    enable/disable Xen PV on HVM support.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 75b479a684fd..6f5345378abc 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1282,6 +1282,7 @@ void xen_hvm_init_shared_info(void)
 	}
 }
 
+#ifdef CONFIG_XEN_PVHVM
 static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
 				    unsigned long action, void *hcpu)
 {
@@ -1338,3 +1339,4 @@ const __refconst struct hypervisor_x86 x86_hyper_xen_hvm = {
 	.init_platform		= xen_hvm_guest_init,
 };
 EXPORT_SYMBOL(x86_hyper_xen_hvm);
+#endif

commit 5915100106b8f14a38053ad6c03a664d208aeaa2
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Jun 17 14:22:52 2010 +0100

    x86: Call HVMOP_pagetable_dying on exit_mmap.
    
    When a pagetable is about to be destroyed, we notify Xen so that the
    hypervisor can clear the related shadow pagetable.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 157c93b62dd4..75b479a684fd 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1318,6 +1318,7 @@ static void __init xen_hvm_guest_init(void)
 	have_vcpu_info_placement = 0;
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();
+	xen_hvm_init_mmu_ops();
 }
 
 static bool __init xen_hvm_platform(void)

commit c1c5413ad58cb73267d328e6020268aa2e50d8ca
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri May 14 12:44:30 2010 +0100

    x86: Unplug emulated disks and nics.
    
    Add a xen_emul_unplug command line option to the kernel to unplug
    xen emulated disks and nics.
    
    Set the default value of xen_emul_unplug depending on whether or
    not the Xen PV frontends and the Xen platform PCI driver have
    been compiled for this kernel (modules or built-in are both OK).
    
    The user can specify xen_emul_unplug=ignore to enable PV drivers on HVM
    even if the host platform doesn't support unplug.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a90172963884..157c93b62dd4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1314,6 +1314,7 @@ static void __init xen_hvm_guest_init(void)
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;
 	register_cpu_notifier(&xen_hvm_cpu_notifier);
+	xen_unplug_emulated_devices();
 	have_vcpu_info_placement = 0;
 	x86_init.irqs.intr_init = xen_init_IRQ;
 	xen_hvm_init_time_ops();

commit 409771d258e9dd71c30f3c9520fd2b796ffc40f0
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri May 14 12:48:19 2010 +0100

    x86: Use xen_vcpuop_clockevent, xen_clocksource and xen wallclock.
    
    Use xen_vcpuop_clockevent instead of hpet and APIC timers as main
    clockevent device on all vcpus, use the xen wallclock time as wallclock
    instead of rtc and use xen_clocksource as clocksource.
    The pv clock algorithm needs to work correctly for the xen_clocksource
    and xen wallclock to be usable, only modern Xen versions offer a
    reliable pv clock in HVM guests (XENFEAT_hvm_safe_pvclock).
    
    Using the hpet as clocksource means a VMEXIT every time we read/write to
    the hpet mmio addresses, pvclock give us a better rating without
    VMEXITs. Same goes for the xen wallclock and xen_vcpuop_clockevent
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Don Dutile <ddutile@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 127c95c8d15e..a90172963884 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -935,10 +935,6 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 	.patch = xen_patch,
 };
 
-static const struct pv_time_ops xen_time_ops __initdata = {
-	.sched_clock = xen_sched_clock,
-};
-
 static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.cpuid = xen_cpuid,
 
@@ -1076,7 +1072,6 @@ asmlinkage void __init xen_start_kernel(void)
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;
-	pv_time_ops = xen_time_ops;
 	pv_cpu_ops = xen_cpu_ops;
 	pv_apic_ops = xen_apic_ops;
 
@@ -1084,13 +1079,7 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_init.oem.arch_setup = xen_arch_setup;
 	x86_init.oem.banner = xen_banner;
 
-	x86_init.timers.timer_init = xen_time_init;
-	x86_init.timers.setup_percpu_clockev = x86_init_noop;
-	x86_cpuinit.setup_percpu_clockev = x86_init_noop;
-
-	x86_platform.calibrate_tsc = xen_tsc_khz;
-	x86_platform.get_wallclock = xen_get_wallclock;
-	x86_platform.set_wallclock = xen_set_wallclock;
+	xen_init_time_ops();
 
 	/*
 	 * Set up some pagetable state before starting to set any ptes.
@@ -1327,6 +1316,7 @@ static void __init xen_hvm_guest_init(void)
 	register_cpu_notifier(&xen_hvm_cpu_notifier);
 	have_vcpu_info_placement = 0;
 	x86_init.irqs.intr_init = xen_init_IRQ;
+	xen_hvm_init_time_ops();
 }
 
 static bool __init xen_hvm_platform(void)

commit 016b6f5fe8398b0291cece60b749d7c930a2e09c
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri May 14 12:45:07 2010 +0100

    xen: Add suspend/resume support for PV on HVM guests.
    
    Suspend/resume requires few different things on HVM: the suspend
    hypercall is different; we don't need to save/restore memory related
    settings; except the shared info page and the callback mechanism.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b211a04c4b2c..127c95c8d15e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1262,13 +1262,15 @@ static int init_hvm_pv_info(int *major, int *minor)
 	return 0;
 }
 
-static void __init init_shared_info(void)
+void xen_hvm_init_shared_info(void)
 {
+	int cpu;
 	struct xen_add_to_physmap xatp;
-	struct shared_info *shared_info_page;
+	static struct shared_info *shared_info_page = 0;
 
-	shared_info_page = (struct shared_info *)
-		extend_brk(PAGE_SIZE, PAGE_SIZE);
+	if (!shared_info_page)
+		shared_info_page = (struct shared_info *)
+			extend_brk(PAGE_SIZE, PAGE_SIZE);
 	xatp.domid = DOMID_SELF;
 	xatp.idx = 0;
 	xatp.space = XENMAPSPACE_shared_info;
@@ -1278,7 +1280,17 @@ static void __init init_shared_info(void)
 
 	HYPERVISOR_shared_info = (struct shared_info *)shared_info_page;
 
-	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
+	/* xen_vcpu is a pointer to the vcpu_info struct in the shared_info
+	 * page, we use it in the event channel upcall and in some pvclock
+	 * related functions. We don't need the vcpu_info placement
+	 * optimizations because we don't use any pv_mmu or pv_irq op on
+	 * HVM.
+	 * When xen_hvm_init_shared_info is run at boot time only vcpu 0 is
+	 * online but xen_hvm_init_shared_info is run at resume time too and
+	 * in that case multiple vcpus might be online. */
+	for_each_online_cpu(cpu) {
+		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+	}
 }
 
 static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
@@ -1308,7 +1320,7 @@ static void __init xen_hvm_guest_init(void)
 	if (r < 0)
 		return;
 
-	init_shared_info();
+	xen_hvm_init_shared_info();
 
 	if (xen_feature(XENFEAT_hvm_callback_vector))
 		xen_have_vector_callback = 1;

commit 38e20b07efd541a959de367dc90a17f92ce2e8a6
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Fri May 14 12:40:51 2010 +0100

    x86/xen: event channels delivery on HVM.
    
    Set the callback to receive evtchns from Xen, using the
    callback vector delivery mechanism.
    
    The traditional way for receiving event channel notifications from Xen
    is via the interrupts from the platform PCI device.
    The callback vector is a newer alternative that allow us to receive
    notifications on any vcpu and doesn't need any PCI support: we allocate
    a vector exclusively to receive events, in the vector handler we don't
    need to interact with the vlapic, therefore we avoid a VMEXIT.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 09b36e9d507a..b211a04c4b2c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -11,6 +11,7 @@
  * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007
  */
 
+#include <linux/cpu.h>
 #include <linux/kernel.h>
 #include <linux/init.h>
 #include <linux/smp.h>
@@ -38,6 +39,7 @@
 #include <xen/interface/memory.h>
 #include <xen/features.h>
 #include <xen/page.h>
+#include <xen/hvm.h>
 #include <xen/hvc-console.h>
 
 #include <asm/paravirt.h>
@@ -80,6 +82,8 @@ struct shared_info xen_dummy_shared_info;
 void *xen_initial_gdt;
 
 RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
+__read_mostly int xen_have_vector_callback;
+EXPORT_SYMBOL_GPL(xen_have_vector_callback);
 
 /*
  * Point at some empty memory to start with. We map the real shared_info
@@ -1277,6 +1281,24 @@ static void __init init_shared_info(void)
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
 }
 
+static int __cpuinit xen_hvm_cpu_notify(struct notifier_block *self,
+				    unsigned long action, void *hcpu)
+{
+	int cpu = (long)hcpu;
+	switch (action) {
+	case CPU_UP_PREPARE:
+		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block __cpuinitdata xen_hvm_cpu_notifier = {
+	.notifier_call	= xen_hvm_cpu_notify,
+};
+
 static void __init xen_hvm_guest_init(void)
 {
 	int r;
@@ -1287,6 +1309,12 @@ static void __init xen_hvm_guest_init(void)
 		return;
 
 	init_shared_info();
+
+	if (xen_feature(XENFEAT_hvm_callback_vector))
+		xen_have_vector_callback = 1;
+	register_cpu_notifier(&xen_hvm_cpu_notifier);
+	have_vcpu_info_placement = 0;
+	x86_init.irqs.intr_init = xen_init_IRQ;
 }
 
 static bool __init xen_hvm_platform(void)

commit bee6ab53e652a414af20392899879b58cd80d033
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Fri May 14 12:39:33 2010 +0100

    x86: early PV on HVM features initialization.
    
    Initialize basic pv on hvm features adding a new Xen HVM specific
    hypervisor_x86 structure.
    
    Don't try to initialize xen-kbdfront and xen-fbfront when running on HVM
    because the backends are not available.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Yaozu (Eddie) Dong <eddie.dong@intel.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 65d8d79b46a8..09b36e9d507a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -35,6 +35,7 @@
 #include <xen/interface/version.h>
 #include <xen/interface/physdev.h>
 #include <xen/interface/vcpu.h>
+#include <xen/interface/memory.h>
 #include <xen/features.h>
 #include <xen/page.h>
 #include <xen/hvc-console.h>
@@ -55,7 +56,9 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/reboot.h>
+#include <asm/setup.h>
 #include <asm/stackprotector.h>
+#include <asm/hypervisor.h>
 
 #include "xen-ops.h"
 #include "mmu.h"
@@ -76,6 +79,8 @@ struct shared_info xen_dummy_shared_info;
 
 void *xen_initial_gdt;
 
+RESERVE_BRK(shared_info_page_brk, PAGE_SIZE);
+
 /*
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.
@@ -1206,3 +1211,98 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_64_start_reservations((char *)__pa_symbol(&boot_params));
 #endif
 }
+
+static uint32_t xen_cpuid_base(void)
+{
+	uint32_t base, eax, ebx, ecx, edx;
+	char signature[13];
+
+	for (base = 0x40000000; base < 0x40010000; base += 0x100) {
+		cpuid(base, &eax, &ebx, &ecx, &edx);
+		*(uint32_t *)(signature + 0) = ebx;
+		*(uint32_t *)(signature + 4) = ecx;
+		*(uint32_t *)(signature + 8) = edx;
+		signature[12] = 0;
+
+		if (!strcmp("XenVMMXenVMM", signature) && ((eax - base) >= 2))
+			return base;
+	}
+
+	return 0;
+}
+
+static int init_hvm_pv_info(int *major, int *minor)
+{
+	uint32_t eax, ebx, ecx, edx, pages, msr, base;
+	u64 pfn;
+
+	base = xen_cpuid_base();
+	cpuid(base + 1, &eax, &ebx, &ecx, &edx);
+
+	*major = eax >> 16;
+	*minor = eax & 0xffff;
+	printk(KERN_INFO "Xen version %d.%d.\n", *major, *minor);
+
+	cpuid(base + 2, &pages, &msr, &ecx, &edx);
+
+	pfn = __pa(hypercall_page);
+	wrmsr_safe(msr, (u32)pfn, (u32)(pfn >> 32));
+
+	xen_setup_features();
+
+	pv_info = xen_info;
+	pv_info.kernel_rpl = 0;
+
+	xen_domain_type = XEN_HVM_DOMAIN;
+
+	return 0;
+}
+
+static void __init init_shared_info(void)
+{
+	struct xen_add_to_physmap xatp;
+	struct shared_info *shared_info_page;
+
+	shared_info_page = (struct shared_info *)
+		extend_brk(PAGE_SIZE, PAGE_SIZE);
+	xatp.domid = DOMID_SELF;
+	xatp.idx = 0;
+	xatp.space = XENMAPSPACE_shared_info;
+	xatp.gpfn = __pa(shared_info_page) >> PAGE_SHIFT;
+	if (HYPERVISOR_memory_op(XENMEM_add_to_physmap, &xatp))
+		BUG();
+
+	HYPERVISOR_shared_info = (struct shared_info *)shared_info_page;
+
+	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
+}
+
+static void __init xen_hvm_guest_init(void)
+{
+	int r;
+	int major, minor;
+
+	r = init_hvm_pv_info(&major, &minor);
+	if (r < 0)
+		return;
+
+	init_shared_info();
+}
+
+static bool __init xen_hvm_platform(void)
+{
+	if (xen_pv_domain())
+		return false;
+
+	if (!xen_cpuid_base())
+		return false;
+
+	return true;
+}
+
+const __refconst struct hypervisor_x86 x86_hyper_xen_hvm = {
+	.name			= "Xen HVM",
+	.detect			= xen_hvm_platform,
+	.init_platform		= xen_hvm_guest_init,
+};
+EXPORT_SYMBOL(x86_hyper_xen_hvm);

commit 093d7b4639951ea3021a6f70d376c3ff31f4740c
Author: Miroslav Rezanina <mrezanin@redhat.com>
Date:   Wed Sep 16 03:56:17 2009 -0400

    xen: release unused free memory
    
    Scan an e820 table and release any memory which lies between e820 entries,
    as it won't be used and would just be wasted.  At present this is just to
    release any memory beyond the end of the e820 map, but it will also deal
    with holes being punched in the map.
    
    Derived from patch by Miroslav Rezanina <mrezanin@redhat.com>
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 65d8d79b46a8..399bed2de881 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -731,7 +731,6 @@ static void set_xen_basic_apic_ops(void)
 
 #endif
 
-
 static void xen_clts(void)
 {
 	struct multicall_space mcs;

commit 7347b4082e55ac4a673f06a0a0ce25c37273c9ec
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Feb 19 13:31:06 2010 -0500

    xen: Allow unprivileged Xen domains to create iomap pages
    
    PV DomU domains are allowed to map hardware MFNs for PCI passthrough,
    but are not generally allowed to map raw machine pages.  In particular,
    various pieces of code try to map DMI and ACPI tables in the ISA ROM
    range.  We disallow _PAGE_IOMAP for those mappings, so that they are
    redirected to a set of local zeroed pages we reserve for that purpose.
    
    [ Impact: prevent passthrough of ISA space, as we only allow PCI ]
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 65d8d79b46a8..3254e8bc4cd7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1145,6 +1145,10 @@ asmlinkage void __init xen_start_kernel(void)
 
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
+	if (!xen_initial_domain())
+		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
+
+	__supported_pte_mask |= _PAGE_IOMAP;
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b607239c1ba8..65d8d79b46a8 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -28,6 +28,7 @@
 #include <linux/highmem.h>
 #include <linux/console.h>
 #include <linux/pci.h>
+#include <linux/gfp.h>
 
 #include <xen/xen.h>
 #include <xen/interface/xen.h>

commit 817a824b75b1475f1b067c8cee318c7b4d66fcde
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Fri Feb 26 17:16:00 2010 +0000

    x86, xen: Disable highmem PTE allocation even when CONFIG_HIGHPTE=y
    
    There's a path in the pagefault code where the kernel deliberately
    breaks its own locking rules by kmapping a high pte page without
    holding the pagetable lock (in at least page_check_address). This
    breaks Xen's ability to track the pinned/unpinned state of the
    page. There does not appear to be a viable workaround for this
    behaviour so simply disable HIGHPTE for all Xen guests.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    LKML-Reference: <1267204562-11844-1-git-send-email-ian.campbell@citrix.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Pasi Kärkkäinen <pasik@iki.fi>
    Cc: <stable@kernel.org> # .32.x: 14315592: Allow highmem user page tables to be disabled at boot time
    Cc: <stable@kernel.org> # .32.x
    Cc: <xen-devel@lists.xensource.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 36daccb68642..b607239c1ba8 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -50,6 +50,7 @@
 #include <asm/traps.h>
 #include <asm/setup.h>
 #include <asm/desc.h>
+#include <asm/pgalloc.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/reboot.h>
@@ -1094,6 +1095,12 @@ asmlinkage void __init xen_start_kernel(void)
 
 	__supported_pte_mask |= _PAGE_IOMAP;
 
+	/*
+	 * Prevent page tables from being allocated in highmem, even
+	 * if CONFIG_HIGHPTE is enabled.
+	 */
+	__userpte_alloc_gfp &= ~__GFP_HIGHMEM;
+
 	/* Work out if we support NX */
 	x86_configure_nx();
 

commit e68266b7001a4e29af083716f0c36c0d6dbb1b39
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Wed Jan 13 10:16:08 2010 +0000

    x86: xen: 64-bit kernel RPL should be 0
    
    Under Xen 64 bit guests actually run their kernel in ring 3,
    however the hypervisor takes care of squashing descriptor the
    RPLs transparently (in order to allow them to continue to
    differentiate between user and kernel space CS using the RPL).
    Therefore the Xen paravirt backend should use RPL==0 instead of
    1 (or 3). Using RPL==1 causes generic arch code to take
    incorrect code paths because it uses "testl $3, <CS>, je foo"
    type tests for a userspace CS and this considers 1==userspace.
    
    This issue was previously masked because get_kernel_rpl() was
    omitted when setting CS in kernel_thread(). This was fixed when
    kernel_thread() was unified with 32 bit in
    f443ff4201dd25cd4dec183f9919ecba90c8edc2.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Cc: Christian Kujau <lists@nerdbynature.de>
    Cc: Jeremy Fitzhardinge <Jeremy.Fitzhardinge@citrix.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    LKML-Reference: <1263377768-19600-2-git-send-email-ian.campbell@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2b26dd5930c6..36daccb68642 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1151,9 +1151,13 @@ asmlinkage void __init xen_start_kernel(void)
 
 	/* keep using Xen gdt for now; no urgent need to change it */
 
+#ifdef CONFIG_X86_32
 	pv_info.kernel_rpl = 1;
 	if (xen_feature(XENFEAT_supervisor_mode_kernel))
 		pv_info.kernel_rpl = 0;
+#else
+	pv_info.kernel_rpl = 0;
+#endif
 
 	/* set the limit of our address space */
 	xen_reserve_top();

commit 11bd04f6f35621193311c32e0721142b073a7794
Merge: 4e2ccdb04091 9e0b5b2c447a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 11 12:18:16 2009 -0800

    Merge branch 'linux-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jbarnes/pci-2.6
    
    * 'linux-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jbarnes/pci-2.6: (109 commits)
      PCI: fix coding style issue in pci_save_state()
      PCI: add pci_request_acs
      PCI: fix BUG_ON triggered by logical PCIe root port removal
      PCI: remove ifdefed pci_cleanup_aer_correct_error_status
      PCI: unconditionally clear AER uncorr status register during cleanup
      x86/PCI: claim SR-IOV BARs in pcibios_allocate_resource
      PCI: portdrv: remove redundant definitions
      PCI: portdrv: remove unnecessary struct pcie_port_data
      PCI: portdrv: minor cleanup for pcie_port_device_register
      PCI: portdrv: add missing irq cleanup
      PCI: portdrv: enable device before irq initialization
      PCI: portdrv: cleanup service irqs initialization
      PCI: portdrv: check capabilities first
      PCI: portdrv: move PME capability check
      PCI: portdrv: remove redundant pcie type calculation
      PCI: portdrv: cleanup pcie_device registration
      PCI: portdrv: remove redundant pcie_port_device_probe
      PCI: Always set prefetchable base/limit upper32 registers
      PCI: read-modify-write the pcie device control register when initiating pcie flr
      PCI: show dma_mask bits in /sys
      ...
    
    Fixed up conflicts in:
            arch/x86/kernel/amd_iommu_init.c
            drivers/pci/dmar.c
            drivers/pci/hotplug/acpiphp_glue.c

commit ab1831b0b87851c874a75e4b3a8538e3d76b37d7
Merge: eae6fa9b0c3e bc2c0303226e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 10 09:35:02 2009 -0800

    Merge branch 'bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen: try harder to balloon up under memory pressure.
      Xen balloon: fix totalram_pages counting.
      xen: explicitly create/destroy stop_machine workqueues outside suspend/resume region.
      xen: improve error handling in do_suspend.
      xen: don't leak IRQs over suspend/resume.
      xen: call clock resume notifier on all CPUs
      xen: use iret for return from 64b kernel to 32b usermode
      xen: don't call dpm_resume_noirq() with interrupts disabled.
      xen: register runstate info for boot CPU early
      xen: register runstate on secondary CPUs
      xen: register timer interrupt with IRQF_TIMER
      xen: correctly restore pfn_to_mfn_list_list after resume
      xen: restore runstate_info even if !have_vcpu_info_placement
      xen: re-register runstate area earlier on resume.
      xen: wait up to 5 minutes for device connetion
      xen: improvement to wait_for_devices()
      xen: fix is_disconnected_device/exists_disconnected_device
      xen/xenbus: make DEVICE_ATTR()s static

commit e33c01972239fee4696679ae5f7d1f340f424999
Merge: 343036cea285 ccef086454d4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 13:27:33 2009 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (36 commits)
      x86, mm: Correct the implementation of is_untracked_pat_range()
      x86/pat: Trivial: don't create debugfs for memtype if pat is disabled
      x86, mtrr: Fix sorting of mtrr after subtracting
      x86: Move find_smp_config() earlier and avoid bootmem usage
      x86, platform: Change is_untracked_pat_range() to bool; cleanup init
      x86: Change is_ISA_range() into an inline function
      x86, mm: is_untracked_pat_range() takes a normal semiclosed range
      x86, mm: Call is_untracked_pat_range() rather than is_ISA_range()
      x86: UV SGI: Don't track GRU space in PAT
      x86: SGI UV: Fix BAU initialization
      x86, numa: Use near(er) online node instead of roundrobin for NUMA
      x86, numa, bootmem: Only free bootmem on NUMA failure path
      x86: Change crash kernel to reserve via reserve_early()
      x86: Eliminate redundant/contradicting cache line size config options
      x86: When cleaning MTRRs, do not fold WP into UC
      x86: remove "extern" from function prototypes in <asm/proto.h>
      x86, mm: Report state of NX protections during boot
      x86, mm: Clean up and simplify NX enablement
      x86, pageattr: Make set_memory_(x|nx) aware of NX support
      x86, sleep: Always save the value of EFER
      ...
    
    Fix up conflicts (added both iommu_shutdown and is_untracked_pat_range)
    to 'struct x86_platform_ops') in
            arch/x86/include/asm/x86_init.h
            arch/x86/kernel/x86_init.c

commit 5d990b627537e59a3a2f039ff588a4750e9c1a6a
Author: Chris Wright <chrisw@sous-sol.org>
Date:   Fri Dec 4 12:15:21 2009 -0800

    PCI: add pci_request_acs
    
    Commit ae21ee65e8bc228416bbcc8a1da01c56a847a60c "PCI: acs p2p upsteram
    forwarding enabling" doesn't actually enable ACS.
    
    Add a function to pci core to allow an IOMMU to request that ACS
    be enabled.  The existing mechanism of using iommu_found() in the pci
    core to know when ACS should be enabled doesn't actually work due to
    initialization order;  iommu has only been detected not initialized.
    
    Have Intel and AMD IOMMUs request ACS, and Xen does as well during early
    init of dom0.
    
    Cc: Allen Kay <allen.m.kay@intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Chris Wright <chrisw@sous-sol.org>
    Signed-off-by: Jesse Barnes <jbarnes@virtuousgeek.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5bccd706232c..e2511bccbc8d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -27,6 +27,7 @@
 #include <linux/page-flags.h>
 #include <linux/highmem.h>
 #include <linux/console.h>
+#include <linux/pci.h>
 
 #include <xen/xen.h>
 #include <xen/interface/xen.h>
@@ -1170,7 +1171,11 @@ asmlinkage void __init xen_start_kernel(void)
 		add_preferred_console("xenboot", 0, NULL);
 		add_preferred_console("tty", 0, NULL);
 		add_preferred_console("hvc", 0, NULL);
+	} else {
+		/* Make sure ACS will be enabled */
+		pci_request_acs();
 	}
+		
 
 	xen_raw_console_write("about to get started...\n");
 

commit 499d19b82b586aef18727b9ae1437f8f37b66e91
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Nov 24 09:38:25 2009 -0800

    xen: register runstate info for boot CPU early
    
    printk timestamping uses sched_clock, which in turn relies on runstate
    info under Xen.  So make sure we set it up before any printks can
    be called.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a7b49f99a130..79f97383cde3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1181,6 +1181,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_raw_console_write("about to get started...\n");
 
+	xen_setup_runstate_info(0);
+
 	/* Start the world */
 #ifdef CONFIG_X86_32
 	i386_start_kernel();

commit 3905bb2aa7bb801b31946b37a4635ebac4009051
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Sat Nov 21 08:46:29 2009 +0800

    xen: restore runstate_info even if !have_vcpu_info_placement
    
    Even if have_vcpu_info_placement is not set, we still need to set up
    the runstate area on each resumed vcpu.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index cb61f77e4496..a7b49f99a130 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -138,26 +138,23 @@ static void xen_vcpu_setup(int cpu)
  */
 void xen_vcpu_restore(void)
 {
-	if (have_vcpu_info_placement) {
-		int cpu;
+	int cpu;
 
-		for_each_online_cpu(cpu) {
-			bool other_cpu = (cpu != smp_processor_id());
+	for_each_online_cpu(cpu) {
+		bool other_cpu = (cpu != smp_processor_id());
 
-			if (other_cpu &&
-			    HYPERVISOR_vcpu_op(VCPUOP_down, cpu, NULL))
-				BUG();
+		if (other_cpu &&
+		    HYPERVISOR_vcpu_op(VCPUOP_down, cpu, NULL))
+			BUG();
 
-			xen_setup_runstate_info(cpu);
+		xen_setup_runstate_info(cpu);
 
+		if (have_vcpu_info_placement)
 			xen_vcpu_setup(cpu);
 
-			if (other_cpu &&
-			    HYPERVISOR_vcpu_op(VCPUOP_up, cpu, NULL))
-				BUG();
-		}
-
-		BUG_ON(!have_vcpu_info_placement);
+		if (other_cpu &&
+		    HYPERVISOR_vcpu_op(VCPUOP_up, cpu, NULL))
+			BUG();
 	}
 }
 

commit be012920ecba161ad20303a3f6d9e96c58cf97c7
Author: Ian Campbell <Ian.Campbell@citrix.com>
Date:   Sat Nov 21 08:35:55 2009 +0800

    xen: re-register runstate area earlier on resume.
    
    This is necessary to ensure the runstate area is available to
    xen_sched_clock before any calls to printk which will require it in
    order to provide a timestamp.
    
    I chose to pull the xen_setup_runstate_info out of xen_time_init into
    the caller in order to maintain parity with calling
    xen_setup_runstate_info separately from calling xen_time_resume.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index dfbf70e65860..cb61f77e4496 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -148,6 +148,8 @@ void xen_vcpu_restore(void)
 			    HYPERVISOR_vcpu_op(VCPUOP_down, cpu, NULL))
 				BUG();
 
+			xen_setup_runstate_info(cpu);
+
 			xen_vcpu_setup(cpu);
 
 			if (other_cpu &&

commit 4763ed4d45522b876c97e1f7f4b659d211f75571
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri Nov 13 15:28:16 2009 -0800

    x86, mm: Clean up and simplify NX enablement
    
    The 32- and 64-bit code used very different mechanisms for enabling
    NX, but even the 32-bit code was enabling NX in head_32.S if it is
    available.  Furthermore, we had a bewildering collection of tests for
    the available of NX.
    
    This patch:
    
    a) merges the 32-bit set_nx() and the 64-bit check_efer() function
       into a single x86_configure_nx() function.  EFER control is left
       to the head code.
    
    b) eliminates the nx_enabled variable entirely.  Things that need to
       test for NX enablement can verify __supported_pte_mask directly,
       and cpu_has_nx gives the supported status of NX.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    LKML-Reference: <1258154897-6770-5-git-send-email-hpa@zytor.com>
    Acked-by: Kees Cook <kees.cook@canonical.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 3439616d69f1..c5e805d4a788 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1082,10 +1082,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 	__supported_pte_mask |= _PAGE_IOMAP;
 
-#ifdef CONFIG_X86_64
 	/* Work out if we support NX */
-	check_efer();
-#endif
+	x86_configure_nx();
 
 	xen_setup_features();
 

commit 1ccbf5344c3daef046d2323190cc6807c44f1917
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Oct 6 15:11:14 2009 -0700

    xen: move Xen-testing predicates to common header
    
    Move xen_domain and related tests out of asm-x86 to xen/xen.h so they
    can be included whenever they are necessary.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jesse Barnes <jbarnes@virtuousgeek.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 23a4d80fb39e..5bccd706232c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -28,6 +28,7 @@
 #include <linux/highmem.h>
 #include <linux/console.h>
 
+#include <xen/xen.h>
 #include <xen/interface/xen.h>
 #include <xen/interface/version.h>
 #include <xen/interface/physdev.h>

commit 82d6469916c6fcfa345636a49004c9d1753905d1
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Thu Oct 22 16:41:15 2009 -0700

    xen: mask extended topology info in cpuid
    
    A Xen guest never needs to know about extended topology, and knowing
    would just confuse it.
    
    This patch just zeros ebx in leaf 0xb which indicates no topology info,
    preventing a crash under Xen on cpus which support this leaf.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 23a4d80fb39e..dfbf70e65860 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -178,6 +178,7 @@ static __read_mostly unsigned int cpuid_leaf1_ecx_mask = ~0;
 static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		      unsigned int *cx, unsigned int *dx)
 {
+	unsigned maskebx = ~0;
 	unsigned maskecx = ~0;
 	unsigned maskedx = ~0;
 
@@ -185,9 +186,16 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 	 * Mask out inconvenient features, to try and disable as many
 	 * unsupported kernel subsystems as possible.
 	 */
-	if (*ax == 1) {
+	switch (*ax) {
+	case 1:
 		maskecx = cpuid_leaf1_ecx_mask;
 		maskedx = cpuid_leaf1_edx_mask;
+		break;
+
+	case 0xb:
+		/* Suppress extended topology stuff */
+		maskebx = 0;
+		break;
 	}
 
 	asm(XEN_EMULATE_PREFIX "cpuid"
@@ -197,6 +205,7 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		  "=d" (*dx)
 		: "0" (*ax), "2" (*cx));
 
+	*bx &= maskebx;
 	*cx &= maskecx;
 	*dx &= maskedx;
 }

commit 973df35ed9ff7806403e793a2ad7e9bd4c2fd2a9
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Oct 27 16:54:19 2009 -0700

    xen: set up mmu_ops before trying to set any ptes
    
    xen_setup_stackprotector() ends up trying to set page protections,
    so we need to have vm_mmu_ops set up before trying to do so.
    Failing to do so causes an early boot crash.
    
    [ Impact: Fix early crash under Xen. ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 3439616d69f1..23a4d80fb39e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1075,6 +1075,8 @@ asmlinkage void __init xen_start_kernel(void)
 	 * Set up some pagetable state before starting to set any ptes.
 	 */
 
+	xen_init_mmu_ops();
+
 	/* Prevent unwanted bits from being set in PTEs. */
 	__supported_pte_mask &= ~_PAGE_GLOBAL;
 	if (!xen_initial_domain())
@@ -1099,7 +1101,6 @@ asmlinkage void __init xen_start_kernel(void)
 	 */
 	xen_setup_stackprotector();
 
-	xen_init_mmu_ops();
 	xen_init_irq_ops();
 	xen_init_cpuid_mask();
 

commit 14c93e8eba70c3c85d8f8acc6cfdc728aef92076
Merge: 14c0abf14a5e c44c9ec0f38b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 23 14:35:10 2009 +0200

    Merge branch 'bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen into x86/urgent

commit b75fe4e5b869f8dbebd36df64a7fcda0c5b318ed
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Sep 21 13:34:06 2009 -0700

    xen: check EFER for NX before setting up GDT mapping
    
    x86-64 assumes NX is available by default, so we need to
    explicitly check for it before using NX.  Some first-generation
    Intel x86-64 processors didn't support NX, and even recent systems
    allow it to be disabled in BIOS.
    
    [ Impact: prevent Xen crash on NX-less 64-bit machines ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0dd0c2c6cae0..5d701bf66a23 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1083,6 +1083,11 @@ asmlinkage void __init xen_start_kernel(void)
 
 	__supported_pte_mask |= _PAGE_IOMAP;
 
+#ifdef CONFIG_X86_64
+	/* Work out if we support NX */
+	check_efer();
+#endif
+
 	xen_setup_features();
 
 	/* Get mfn list */
@@ -1123,11 +1128,6 @@ asmlinkage void __init xen_start_kernel(void)
 
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
-#ifdef CONFIG_X86_64
-	/* Work out if we support NX */
-	check_efer();
-#endif
-
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];

commit 78f28b7c555359c67c2a0d23f7436e915329421e
Merge: 3240a77b515f 7bd867dfb4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 18 14:05:47 2009 -0700

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (38 commits)
      x86: Move get/set_wallclock to x86_platform_ops
      x86: platform: Fix section annotations
      x86: apic namespace cleanup
      x86: Distangle ioapic and i8259
      x86: Add Moorestown early detection
      x86: Add hardware_subarch ID for Moorestown
      x86: Add early platform detection
      x86: Move tsc_init to late_time_init
      x86: Move tsc_calibration to x86_init_ops
      x86: Replace the now identical time_32/64.c by time.c
      x86: time_32/64.c unify profile_pc
      x86: Move calibrate_cpu to tsc.c
      x86: Make timer setup and global variables the same in time_32/64.c
      x86: Remove mca bus ifdef from timer interrupt
      x86: Simplify timer_ack magic in time_32.c
      x86: Prepare unification of time_32/64.c
      x86: Remove do_timer hook
      x86: Add timer_init to x86_init_ops
      x86: Move percpu clockevents setup to x86_init_ops
      x86: Move xen_post_allocator_init into xen_pagetable_setup_done
      ...
    
    Fix up conflicts in arch/x86/include/asm/io_apic.h

commit 7bd867dfb4e0357e06a3211ab2bd0e714110def3
Author: Feng Tang <feng.tang@intel.com>
Date:   Thu Sep 10 10:48:56 2009 +0800

    x86: Move get/set_wallclock to x86_platform_ops
    
    get/set_wallclock() have already a set of platform dependent
    implementations (default, EFI, paravirt). MRST will add another
    variant.
    
    Moving them to platform ops simplifies the existing code and minimizes
    the effort to integrate new variants.
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ee8cac77c8a4..b5bf8b9119a3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -842,8 +842,6 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 };
 
 static const struct pv_time_ops xen_time_ops __initdata = {
-	.set_wallclock = xen_set_wallclock,
-	.get_wallclock = xen_get_wallclock,
 	.sched_clock = xen_sched_clock,
 };
 
@@ -980,6 +978,8 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_cpuinit.setup_percpu_clockev = x86_init_noop;
 
 	x86_platform.calibrate_tsc = xen_tsc_khz;
+	x86_platform.get_wallclock = xen_get_wallclock;
+	x86_platform.set_wallclock = xen_set_wallclock;
 
 #ifdef CONFIG_X86_64
 	/*

commit b8cb48aae1b8c50b37dcb7710363aa69a7a0d9ca
Merge: 0cc6d77e55ec 78c86e5e5691
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 10:23:49 2009 -0700

    Merge branch 'x86-xen-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-xen-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: split __phys_addr out into separate file
      xen: use stronger barrier after unlocking lock
      xen: only enable interrupts while actually blocking for spinlock
      xen: make -fstack-protector work under Xen

commit c7208de304ac335d5d58db346bb773a471fc636b
Merge: 15b0404272e1 5367b6887e7d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 07:57:32 2009 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (22 commits)
      x86: Fix code patching for paravirt-alternatives on 486
      x86, msr: change msr-reg.o to obj-y, and export its symbols
      x86: Use hard_smp_processor_id() to get apic id for AMD K8 cpus
      x86, sched: Workaround broken sched domain creation for AMD Magny-Cours
      x86, mcheck: Use correct cpumask for shared bank4
      x86, cacheinfo: Fixup L3 cache information for AMD multi-node processors
      x86: Fix CPU llc_shared_map information for AMD Magny-Cours
      x86, msr: Fix msr-reg.S compilation with gas 2.16.1, on 32-bit too
      x86: Move kernel_fpu_using to irq_fpu_usable in asm/i387.h
      x86, msr: fix msr-reg.S compilation with gas 2.16.1
      x86, msr: Export the register-setting MSR functions via /dev/*/msr
      x86, msr: Create _on_cpu helpers for {rw,wr}msr_safe_regs()
      x86, msr: Have the _safe MSR functions return -EIO, not -EFAULT
      x86, msr: CFI annotations, cleanups for msr-reg.S
      x86, asm: Make _ASM_EXTABLE() usable from assembly code
      x86, asm: Add 32-bit versions of the combined CFI macros
      x86, AMD: Disable wrongly set X86_FEATURE_LAHF_LM CPUID bit
      x86, msr: Rewrite AMD rd/wrmsr variants
      x86, msr: Add rd/wrmsr interfaces with preset registers
      x86: add specific support for Intel Atom architecture
      ...

commit 577eebeae34d340685d8985dfdb7dfe337c511e8
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Thu Aug 27 12:46:35 2009 -0700

    xen: make -fstack-protector work under Xen
    
    -fstack-protector uses a special per-cpu "stack canary" value.
    gcc generates special code in each function to test the canary to make
    sure that the function's stack hasn't been overrun.
    
    On x86-64, this is simply an offset of %gs, which is the usual per-cpu
    base segment register, so setting it up simply requires loading %gs's
    base as normal.
    
    On i386, the stack protector segment is %gs (rather than the usual kernel
    percpu %fs segment register).  This requires setting up the full kernel
    GDT and then loading %gs accordingly.  We also need to make sure %gs is
    initialized when bringing up secondary cpus too.
    
    To keep things consistent, we do the full GDT/segment register setup on
    both architectures.
    
    Because we need to avoid -fstack-protected code before setting up the GDT
    and because there's no way to disable it on a per-function basis, several
    files need to have stack-protector inhibited.
    
    [ Impact: allow Xen booting with stack-protector enabled ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index eb33aaa8415d..76143136dead 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -51,6 +51,7 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/reboot.h>
+#include <asm/stackprotector.h>
 
 #include "xen-ops.h"
 #include "mmu.h"
@@ -330,18 +331,28 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 	unsigned long frames[pages];
 	int f;
 
-	/* A GDT can be up to 64k in size, which corresponds to 8192
-	   8-byte entries, or 16 4k pages.. */
+	/*
+	 * A GDT can be up to 64k in size, which corresponds to 8192
+	 * 8-byte entries, or 16 4k pages..
+	 */
 
 	BUG_ON(size > 65536);
 	BUG_ON(va & ~PAGE_MASK);
 
 	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
 		int level;
-		pte_t *ptep = lookup_address(va, &level);
+		pte_t *ptep;
 		unsigned long pfn, mfn;
 		void *virt;
 
+		/*
+		 * The GDT is per-cpu and is in the percpu data area.
+		 * That can be virtually mapped, so we need to do a
+		 * page-walk to get the underlying MFN for the
+		 * hypercall.  The page can also be in the kernel's
+		 * linear range, so we need to RO that mapping too.
+		 */
+		ptep = lookup_address(va, &level);
 		BUG_ON(ptep == NULL);
 
 		pfn = pte_pfn(*ptep);
@@ -358,6 +369,44 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 		BUG();
 }
 
+/*
+ * load_gdt for early boot, when the gdt is only mapped once
+ */
+static __init void xen_load_gdt_boot(const struct desc_ptr *dtr)
+{
+	unsigned long va = dtr->address;
+	unsigned int size = dtr->size + 1;
+	unsigned pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	unsigned long frames[pages];
+	int f;
+
+	/*
+	 * A GDT can be up to 64k in size, which corresponds to 8192
+	 * 8-byte entries, or 16 4k pages..
+	 */
+
+	BUG_ON(size > 65536);
+	BUG_ON(va & ~PAGE_MASK);
+
+	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
+		pte_t pte;
+		unsigned long pfn, mfn;
+
+		pfn = virt_to_pfn(va);
+		mfn = pfn_to_mfn(pfn);
+
+		pte = pfn_pte(pfn, PAGE_KERNEL_RO);
+
+		if (HYPERVISOR_update_va_mapping((unsigned long)va, pte, 0))
+			BUG();
+
+		frames[f] = mfn;
+	}
+
+	if (HYPERVISOR_set_gdt(frames, size / sizeof(struct desc_struct)))
+		BUG();
+}
+
 static void load_TLS_descriptor(struct thread_struct *t,
 				unsigned int cpu, unsigned int i)
 {
@@ -581,6 +630,29 @@ static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
 	preempt_enable();
 }
 
+/*
+ * Version of write_gdt_entry for use at early boot-time needed to
+ * update an entry as simply as possible.
+ */
+static __init void xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,
+					    const void *desc, int type)
+{
+	switch (type) {
+	case DESC_LDT:
+	case DESC_TSS:
+		/* ignore */
+		break;
+
+	default: {
+		xmaddr_t maddr = virt_to_machine(&dt[entry]);
+
+		if (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))
+			dt[entry] = *(struct desc_struct *)desc;
+	}
+
+	}
+}
+
 static void xen_load_sp0(struct tss_struct *tss,
 			 struct thread_struct *thread)
 {
@@ -965,6 +1037,23 @@ static const struct machine_ops __initdata xen_machine_ops = {
 	.emergency_restart = xen_emergency_restart,
 };
 
+/*
+ * Set up the GDT and segment registers for -fstack-protector.  Until
+ * we do this, we have to be careful not to call any stack-protected
+ * function, which is most of the kernel.
+ */
+static void __init xen_setup_stackprotector(void)
+{
+	pv_cpu_ops.write_gdt_entry = xen_write_gdt_entry_boot;
+	pv_cpu_ops.load_gdt = xen_load_gdt_boot;
+
+	setup_stack_canary_segment(0);
+	switch_to_new_gdt(0);
+
+	pv_cpu_ops.write_gdt_entry = xen_write_gdt_entry;
+	pv_cpu_ops.load_gdt = xen_load_gdt;
+}
+
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {
@@ -983,13 +1072,28 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
 
-#ifdef CONFIG_X86_64
 	/*
-	 * Setup percpu state.  We only need to do this for 64-bit
-	 * because 32-bit already has %fs set properly.
+	 * Set up some pagetable state before starting to set any ptes.
 	 */
-	load_percpu_segment(0);
-#endif
+
+	/* Prevent unwanted bits from being set in PTEs. */
+	__supported_pte_mask &= ~_PAGE_GLOBAL;
+	if (!xen_initial_domain())
+		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
+
+	__supported_pte_mask |= _PAGE_IOMAP;
+
+	xen_setup_features();
+
+	/* Get mfn list */
+	if (!xen_feature(XENFEAT_auto_translated_physmap))
+		xen_build_dynamic_phys_to_machine();
+
+	/*
+	 * Set up kernel GDT and segment registers, mainly so that
+	 * -fstack-protector code can be executed.
+	 */
+	xen_setup_stackprotector();
 
 	xen_init_irq_ops();
 	xen_init_cpuid_mask();
@@ -1001,8 +1105,6 @@ asmlinkage void __init xen_start_kernel(void)
 	set_xen_basic_apic_ops();
 #endif
 
-	xen_setup_features();
-
 	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {
 		pv_mmu_ops.ptep_modify_prot_start = xen_ptep_modify_prot_start;
 		pv_mmu_ops.ptep_modify_prot_commit = xen_ptep_modify_prot_commit;
@@ -1019,17 +1121,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_smp_init();
 
-	/* Get mfn list */
-	if (!xen_feature(XENFEAT_auto_translated_physmap))
-		xen_build_dynamic_phys_to_machine();
-
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
-	/* Prevent unwanted bits from being set in PTEs. */
-	__supported_pte_mask &= ~_PAGE_GLOBAL;
-	if (!xen_initial_domain())
-		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
-
 #ifdef CONFIG_X86_64
 	/* Work out if we support NX */
 	check_efer();

commit 0cc0213e73af5963eca259c84876937c20689dbd
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Aug 31 14:23:29 2009 -0700

    x86, msr: Have the _safe MSR functions return -EIO, not -EFAULT
    
    For some reason, the _safe MSR functions returned -EFAULT, not -EIO.
    However, the only user which cares about the return code as anything
    other than a boolean is the MSR driver, which wants -EIO.  Change it
    to -EIO across the board.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0a1700a2be9c..a8432d816903 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -713,7 +713,7 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 	set:
 		base = ((u64)high << 32) | low;
 		if (HYPERVISOR_set_segment_base(which, base) != 0)
-			ret = -EFAULT;
+			ret = -EIO;
 		break;
 #endif
 

commit 2d826404f0bdcac2a4dd7e3c446b70d6a3b63b78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 17:06:25 2009 +0200

    x86: Move tsc_calibration to x86_init_ops
    
    TSC calibration is modified by the vmware hypervisor and paravirt by
    separate means. Moorestown wants to add its own calibration routine as
    well. So make calibrate_tsc a proper x86_init_ops function and
    override it by paravirt or by the early setup of the vmware
    hypervisor.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 84826b842b54..ee8cac77c8a4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -844,7 +844,6 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 static const struct pv_time_ops xen_time_ops __initdata = {
 	.set_wallclock = xen_set_wallclock,
 	.get_wallclock = xen_get_wallclock,
-	.get_tsc_khz = xen_tsc_khz,
 	.sched_clock = xen_sched_clock,
 };
 
@@ -980,6 +979,8 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_init.timers.setup_percpu_clockev = x86_init_noop;
 	x86_cpuinit.setup_percpu_clockev = x86_init_noop;
 
+	x86_platform.calibrate_tsc = xen_tsc_khz;
+
 #ifdef CONFIG_X86_64
 	/*
 	 * Setup percpu state.  We only need to do this for 64-bit

commit 845b3944bbdf9e9247849bf037f27ff3a3f26d87
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 19 15:37:03 2009 +0200

    x86: Add timer_init to x86_init_ops
    
    The timer init code is convoluted with several quirks and the paravirt
    timer chooser. Figuring out which code path is actually taken is not
    for the faint hearted.
    
    Move the numaq TSC quirk to tsc_pre_init x86_init_ops function and
    replace the paravirt time chooser and the remaining x86 quirk with a
    simple x86_init_ops function.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 14e597e0c160..84826b842b54 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -842,8 +842,6 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 };
 
 static const struct pv_time_ops xen_time_ops __initdata = {
-	.time_init = xen_time_init,
-
 	.set_wallclock = xen_set_wallclock,
 	.get_wallclock = xen_get_wallclock,
 	.get_tsc_khz = xen_tsc_khz,
@@ -977,6 +975,8 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_init.resources.memory_setup = xen_memory_setup;
 	x86_init.oem.arch_setup = xen_arch_setup;
 	x86_init.oem.banner = xen_banner;
+
+	x86_init.timers.timer_init = xen_time_init;
 	x86_init.timers.setup_percpu_clockev = x86_init_noop;
 	x86_cpuinit.setup_percpu_clockev = x86_init_noop;
 

commit 736decac643e8982655e22ac7f0e5e61c5b7f9bd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 19 12:35:53 2009 +0200

    x86: Move percpu clockevents setup to x86_init_ops
    
    paravirt overrides the setup of the default apic timers as per cpu
    timers. Moorestown needs to override that as well.
    
    Move it to x86_init_ops setup and create a separate x86_cpuinit struct
    which holds the function for the secondary evtl. hotplugabble CPUs.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a924caa168dd..14e597e0c160 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -912,8 +912,6 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 
 static const struct pv_apic_ops xen_apic_ops __initdata = {
 #ifdef CONFIG_X86_LOCAL_APIC
-	.setup_boot_clock = paravirt_nop,
-	.setup_secondary_clock = paravirt_nop,
 	.startup_ipi_hook = paravirt_nop,
 #endif
 };
@@ -979,6 +977,8 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_init.resources.memory_setup = xen_memory_setup;
 	x86_init.oem.arch_setup = xen_arch_setup;
 	x86_init.oem.banner = xen_banner;
+	x86_init.timers.setup_percpu_clockev = x86_init_noop;
+	x86_cpuinit.setup_percpu_clockev = x86_init_noop;
 
 #ifdef CONFIG_X86_64
 	/*

commit f1d7062a235d057e5d85ed2860bef609e0160cde
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 13:13:52 2009 +0200

    x86: Move xen_post_allocator_init into xen_pagetable_setup_done
    
    We really do not need two paravirt/x86_init_ops functions which are
    called in two consecutive source lines. Move the only user of
    post_allocator_init into the already existing pagetable_setup_done
    function.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 12ea09ec39b5..a924caa168dd 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -839,8 +839,6 @@ static const struct pv_info xen_info __initdata = {
 
 static const struct pv_init_ops xen_init_ops __initdata = {
 	.patch = xen_patch,
-
-	.post_allocator_init = xen_post_allocator_init,
 };
 
 static const struct pv_time_ops xen_time_ops __initdata = {

commit 030cb6c00d242c20e92a3327d0cac17ce02d0cc3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 14:30:02 2009 +0200

    x86: Move paravirt pagetable_setup to x86_init_ops
    
    Replace more paravirt hackery by proper x86_init_ops.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 46e23cde143a..12ea09ec39b5 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -977,7 +977,6 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_time_ops = xen_time_ops;
 	pv_cpu_ops = xen_cpu_ops;
 	pv_apic_ops = xen_apic_ops;
-	pv_mmu_ops = xen_mmu_ops;
 
 	x86_init.resources.memory_setup = xen_memory_setup;
 	x86_init.oem.arch_setup = xen_arch_setup;
@@ -991,6 +990,7 @@ asmlinkage void __init xen_start_kernel(void)
 	load_percpu_segment(0);
 #endif
 
+	xen_init_mmu_ops();
 	xen_init_irq_ops();
 	xen_init_cpuid_mask();
 

commit 6f30c1ac3fcf11e08f00670f293546a112cdf4e3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 13:19:57 2009 +0200

    x86: Move paravirt banner printout to x86_init_ops
    
    Replace another obscure paravirt magic and move it to
    x86_init_ops. Such a hook is also useful for embedded and special
    hardware.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 73c7b1d610f3..46e23cde143a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -840,7 +840,6 @@ static const struct pv_info xen_info __initdata = {
 static const struct pv_init_ops xen_init_ops __initdata = {
 	.patch = xen_patch,
 
-	.banner = xen_banner,
 	.post_allocator_init = xen_post_allocator_init,
 };
 
@@ -982,6 +981,7 @@ asmlinkage void __init xen_start_kernel(void)
 
 	x86_init.resources.memory_setup = xen_memory_setup;
 	x86_init.oem.arch_setup = xen_arch_setup;
+	x86_init.oem.banner = xen_banner;
 
 #ifdef CONFIG_X86_64
 	/*

commit 42bbdb43b16d233b2dacb4cd76e28f61c2a86dc6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 13:04:10 2009 +0200

    x86: Replace ARCH_SETUP by a proper x86_init_ops
    
    ARCH_SETUP is a horrible leftover from the old arch/i386 mach support
    code. It still has a lonely user in xen. Move it to x86_init_ops.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 50b20c64f0bd..73c7b1d610f3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -841,7 +841,6 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 	.patch = xen_patch,
 
 	.banner = xen_banner,
-	.arch_setup = xen_arch_setup,
 	.post_allocator_init = xen_post_allocator_init,
 };
 
@@ -982,6 +981,7 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_mmu_ops = xen_mmu_ops;
 
 	x86_init.resources.memory_setup = xen_memory_setup;
+	x86_init.oem.arch_setup = xen_arch_setup;
 
 #ifdef CONFIG_X86_64
 	/*

commit 6b18ae3e2ff62daa9f181401759161dd8de0aadf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 10:19:54 2009 +0200

    x86: Move memory_setup to x86_init_ops
    
    memory_setup is overridden by x86_quirks and by paravirts with weak
    functions and quirks. Unify the whole mess and make it an
    unconditional x86_init_ops function which defaults to the standard
    function and can be overridden by the early platform code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e90540a46a0b..50b20c64f0bd 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -841,7 +841,6 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 	.patch = xen_patch,
 
 	.banner = xen_banner,
-	.memory_setup = xen_memory_setup,
 	.arch_setup = xen_arch_setup,
 	.post_allocator_init = xen_post_allocator_init,
 };
@@ -982,6 +981,8 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
 
+	x86_init.resources.memory_setup = xen_memory_setup;
+
 #ifdef CONFIG_X86_64
 	/*
 	 * Setup percpu state.  We only need to do this for 64-bit

commit 7adb4df410966dfe43e4815256e3215110648fb8
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Tue Aug 25 21:06:03 2009 -0700

    x86, xen: Initialize cx to suppress warning
    
    Initialize cx before calling xen_cpuid(), in order to suppress the
    "may be used uninitialized in this function" warning.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0b755cd7686d..eb33aaa8415d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -215,6 +215,7 @@ static __init void xen_init_cpuid_mask(void)
 			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
 
 	ax = 1;
+	cx = 0;
 	xen_cpuid(&ax, &bx, &cx, &dx);
 
 	/* cpuid claims we support xsave; try enabling it to see what happens */

commit d560bc61575efae43595cbcb56d0ba3b9450139c
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Aug 25 12:53:02 2009 -0700

    x86, xen: Suppress WP test on Xen
    
    Xen always runs on CPUs which properly support WP enforcement in
    privileged mode, so there's no need to test for it.
    
    This also works around a crash reported by Arnd Hannemann, though I
    think its just a band-aid for that case.
    
    Reported-by: Arnd Hannemann <hannemann@nets.rwth-aachen.de>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e90540a46a0b..0b755cd7686d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1059,6 +1059,7 @@ asmlinkage void __init xen_start_kernel(void)
 	/* set up basic CPUID stuff */
 	cpu_detect(&new_cpu_data);
 	new_cpu_data.hard_math = 1;
+	new_cpu_data.wp_works_ok = 1;
 	new_cpu_data.x86_capability[0] = cpuid_edx(1);
 #endif
 

commit cbcb340cb6a6f9f32724c90493f509dd41105e20
Merge: 78b89ecd7317 ce2eef33d35c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 20 12:05:24 2009 +0200

    Merge branch 'bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen into x86/urgent

commit ce2eef33d35cd7b932492b5a81fb0febd2b323cd
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Aug 17 12:26:53 2009 -0700

    xen: rearrange things to fix stackprotector
    
    Make sure the stack-protector segment registers are properly set up
    before calling any functions which may have stack-protection compiled
    into them.
    
    [ Impact: prevent Xen early-boot crash when stack-protector is enabled ]
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f09e8c36ee80..edcf72a3c29c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -925,10 +925,6 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_domain_type = XEN_PV_DOMAIN;
 
-	BUG_ON(memcmp(xen_start_info->magic, "xen-3", 5) != 0);
-
-	xen_setup_features();
-
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;
@@ -937,8 +933,15 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
 
-	xen_init_irq_ops();
+#ifdef CONFIG_X86_64
+	/*
+	 * Setup percpu state.  We only need to do this for 64-bit
+	 * because 32-bit already has %fs set properly.
+	 */
+	load_percpu_segment(0);
+#endif
 
+	xen_init_irq_ops();
 	xen_init_cpuid_mask();
 
 #ifdef CONFIG_X86_LOCAL_APIC
@@ -948,6 +951,8 @@ asmlinkage void __init xen_start_kernel(void)
 	set_xen_basic_apic_ops();
 #endif
 
+	xen_setup_features();
+
 	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {
 		pv_mmu_ops.ptep_modify_prot_start = xen_ptep_modify_prot_start;
 		pv_mmu_ops.ptep_modify_prot_commit = xen_ptep_modify_prot_commit;
@@ -955,13 +960,6 @@ asmlinkage void __init xen_start_kernel(void)
 
 	machine_ops = xen_machine_ops;
 
-#ifdef CONFIG_X86_64
-	/*
-	 * Setup percpu state.  We only need to do this for 64-bit
-	 * because 32-bit already has %fs set properly.
-	 */
-	load_percpu_segment(0);
-#endif
 	/*
 	 * The only reliable way to retain the initial address of the
 	 * percpu gdt_page is to remember it here, so we can go and

commit a789ed5fb6d0256c4177c2cc27e06520ddbe4d4c
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Apr 24 00:26:50 2009 -0700

    xen: cache cr0 value to avoid trap'n'emulate for read_cr0
    
    stts() is implemented in terms of read_cr0/write_cr0 to update the
    state of the TS bit.  This happens during context switch, and so
    is fairly performance critical.  Rather than falling back to
    a trap-and-emulate native read_cr0, implement our own by caching
    the last-written value from write_cr0 (the TS bit is the only one
    we really care about).
    
    Impact: optimise Xen context switches
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e9df942aa143..0a1700a2be9c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -658,10 +658,26 @@ static void xen_clts(void)
 	xen_mc_issue(PARAVIRT_LAZY_CPU);
 }
 
+static DEFINE_PER_CPU(unsigned long, xen_cr0_value);
+
+static unsigned long xen_read_cr0(void)
+{
+	unsigned long cr0 = percpu_read(xen_cr0_value);
+
+	if (unlikely(cr0 == 0)) {
+		cr0 = native_read_cr0();
+		percpu_write(xen_cr0_value, cr0);
+	}
+
+	return cr0;
+}
+
 static void xen_write_cr0(unsigned long cr0)
 {
 	struct multicall_space mcs;
 
+	percpu_write(xen_cr0_value, cr0);
+
 	/* Only pay attention to cr0.TS; everything else is
 	   ignored. */
 	mcs = xen_mc_entry(0);
@@ -847,7 +863,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 
 	.clts = xen_clts,
 
-	.read_cr0 = native_read_cr0,
+	.read_cr0 = xen_read_cr0,
 	.write_cr0 = xen_write_cr0,
 
 	.read_cr4 = native_read_cr4,

commit b80119bb35a49a4e8dbfb9708872adfd5cf38dee
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Apr 24 00:22:08 2009 -0700

    xen/x86-64: clean up warnings about IST-using traps
    
    Ignore known IST-using traps.  Aside from the debugger traps, they're
    low-level faults which Xen will handle for us, so the kernel needn't
    worry about them.  Keep warning in case unknown trap starts using IST.
    
    Impact: suppress spurious warnings
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7566e13c0cac..e9df942aa143 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -439,14 +439,32 @@ static int cvt_gate_to_trap(int vector, const gate_desc *val,
 
 	addr = gate_offset(*val);
 #ifdef CONFIG_X86_64
+	/*
+	 * Look for known traps using IST, and substitute them
+	 * appropriately.  The debugger ones are the only ones we care
+	 * about.  Xen will handle faults like double_fault and
+	 * machine_check, so we should never see them.  Warn if
+	 * there's an unexpected IST-using fault handler.
+	 */
 	if (addr == (unsigned long)debug)
 		addr = (unsigned long)xen_debug;
 	else if (addr == (unsigned long)int3)
 		addr = (unsigned long)xen_int3;
 	else if (addr == (unsigned long)stack_segment)
 		addr = (unsigned long)xen_stack_segment;
-	else
-		WARN_ON(val->ist != 0);
+	else if (addr == (unsigned long)double_fault ||
+		 addr == (unsigned long)nmi) {
+		/* Don't need to handle these */
+		return 0;
+#ifdef CONFIG_X86_MCE
+	} else if (addr == (unsigned long)machine_check) {
+		return 0;
+#endif
+	} else {
+		/* Some other trap using IST? */
+		if (WARN_ON(val->ist != 0))
+			return 0;
+	}
 #endif	/* CONFIG_X86_64 */
 	info->address = addr;
 

commit 6cac5a924668a56c7ccefc345805f1fe0536a90e
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Sun Mar 29 19:56:29 2009 -0700

    xen/x86-64: fix breakpoints and hardware watchpoints
    
    Native x86-64 uses the IST mechanism to run int3 and debug traps on
    an alternative stack.  Xen does not do this, and so the frames were
    being misinterpreted by the ptrace code.  This change special-cases
    these two exceptions by using Xen variants which run on the normal
    kernel stack properly.
    
    Impact: avoid crash or bad data when IST trap is invoked under Xen
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 12a3159333bc..7566e13c0cac 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -20,6 +20,7 @@
 #include <linux/delay.h>
 #include <linux/start_kernel.h>
 #include <linux/sched.h>
+#include <linux/kprobes.h>
 #include <linux/bootmem.h>
 #include <linux/module.h>
 #include <linux/mm.h>
@@ -44,6 +45,7 @@
 #include <asm/processor.h>
 #include <asm/proto.h>
 #include <asm/msr-index.h>
+#include <asm/traps.h>
 #include <asm/setup.h>
 #include <asm/desc.h>
 #include <asm/pgtable.h>
@@ -428,11 +430,26 @@ static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 static int cvt_gate_to_trap(int vector, const gate_desc *val,
 			    struct trap_info *info)
 {
+	unsigned long addr;
+
 	if (val->type != GATE_TRAP && val->type != GATE_INTERRUPT)
 		return 0;
 
 	info->vector = vector;
-	info->address = gate_offset(*val);
+
+	addr = gate_offset(*val);
+#ifdef CONFIG_X86_64
+	if (addr == (unsigned long)debug)
+		addr = (unsigned long)xen_debug;
+	else if (addr == (unsigned long)int3)
+		addr = (unsigned long)xen_int3;
+	else if (addr == (unsigned long)stack_segment)
+		addr = (unsigned long)xen_stack_segment;
+	else
+		WARN_ON(val->ist != 0);
+#endif	/* CONFIG_X86_64 */
+	info->address = addr;
+
 	info->cs = gate_segment(*val);
 	info->flags = val->dpl;
 	/* interrupt gates clear IF */

commit 2b2a733447b2bce5fef053df38412e4c0634ec22
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Sun Mar 29 22:57:15 2009 -0700

    xen: clean up gate trap/interrupt constants
    
    Use GATE_INTERRUPT/TRAP rather than 0xe/f.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f7d0fd7ff8e1..f09e8c36ee80 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -428,7 +428,7 @@ static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 static int cvt_gate_to_trap(int vector, const gate_desc *val,
 			    struct trap_info *info)
 {
-	if (val->type != 0xf && val->type != 0xe)
+	if (val->type != GATE_TRAP && val->type != GATE_INTERRUPT)
 		return 0;
 
 	info->vector = vector;
@@ -436,8 +436,8 @@ static int cvt_gate_to_trap(int vector, const gate_desc *val,
 	info->cs = gate_segment(*val);
 	info->flags = val->dpl;
 	/* interrupt gates clear IF */
-	if (val->type == 0xe)
-		info->flags |= 4;
+	if (val->type == GATE_INTERRUPT)
+		info->flags |= 1 << 2;
 
 	return 1;
 }

commit bc6081ff98eec627919e0c68415e46a78fe51dc4
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Mar 27 11:29:02 2009 -0700

    xen: set _PAGE_NX in __supported_pte_mask before pagetable construction
    
    Some 64-bit machines don't support the NX flag in ptes.
    Check for NX before constructing the kernel pagetables.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 69de19168a7e..f7d0fd7ff8e1 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -42,6 +42,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/fixmap.h>
 #include <asm/processor.h>
+#include <asm/proto.h>
 #include <asm/msr-index.h>
 #include <asm/setup.h>
 #include <asm/desc.h>
@@ -914,7 +915,6 @@ static const struct machine_ops __initdata xen_machine_ops = {
 	.emergency_restart = xen_emergency_restart,
 };
 
-
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {
@@ -982,6 +982,11 @@ asmlinkage void __init xen_start_kernel(void)
 	if (!xen_initial_domain())
 		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
 
+#ifdef CONFIG_X86_64
+	/* Work out if we support NX */
+	check_efer();
+#endif
+
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];

commit 191216b9289ed02256086e6bab4f668112109399
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Sat Mar 7 17:09:27 2009 -0800

    xen: mask XSAVE from cpuid
    
    Xen leaves XSAVE set in cpuid, but doesn't allow cr4.OSXSAVE
    to be set.  This confuses the kernel and it ends up crashing on
    an xsetbv instruction.
    
    At boot time, try to set cr4.OSXSAVE, and mask XSAVE out of
    cpuid it we can't.  This will produce a spurious error from Xen,
    but allows us to support XSAVE if/when Xen does.
    
    This also factors out the cpuid mask decisions to boot time.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5d9a1c37c515..69de19168a7e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -168,21 +168,23 @@ static void __init xen_banner(void)
 	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
 }
 
+static __read_mostly unsigned int cpuid_leaf1_edx_mask = ~0;
+static __read_mostly unsigned int cpuid_leaf1_ecx_mask = ~0;
+
 static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		      unsigned int *cx, unsigned int *dx)
 {
+	unsigned maskecx = ~0;
 	unsigned maskedx = ~0;
 
 	/*
 	 * Mask out inconvenient features, to try and disable as many
 	 * unsupported kernel subsystems as possible.
 	 */
-	if (*ax == 1)
-		maskedx = ~((1 << X86_FEATURE_APIC) |  /* disable APIC */
-			    (1 << X86_FEATURE_ACPI) |  /* disable ACPI */
-			    (1 << X86_FEATURE_MCE)  |  /* disable MCE */
-			    (1 << X86_FEATURE_MCA)  |  /* disable MCA */
-			    (1 << X86_FEATURE_ACC));   /* thermal monitoring */
+	if (*ax == 1) {
+		maskecx = cpuid_leaf1_ecx_mask;
+		maskedx = cpuid_leaf1_edx_mask;
+	}
 
 	asm(XEN_EMULATE_PREFIX "cpuid"
 		: "=a" (*ax),
@@ -190,9 +192,43 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		  "=c" (*cx),
 		  "=d" (*dx)
 		: "0" (*ax), "2" (*cx));
+
+	*cx &= maskecx;
 	*dx &= maskedx;
 }
 
+static __init void xen_init_cpuid_mask(void)
+{
+	unsigned int ax, bx, cx, dx;
+
+	cpuid_leaf1_edx_mask =
+		~((1 << X86_FEATURE_MCE)  |  /* disable MCE */
+		  (1 << X86_FEATURE_MCA)  |  /* disable MCA */
+		  (1 << X86_FEATURE_ACC));   /* thermal monitoring */
+
+	if (!xen_initial_domain())
+		cpuid_leaf1_edx_mask &=
+			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
+			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
+
+	ax = 1;
+	xen_cpuid(&ax, &bx, &cx, &dx);
+
+	/* cpuid claims we support xsave; try enabling it to see what happens */
+	if (cx & (1 << (X86_FEATURE_XSAVE % 32))) {
+		unsigned long cr4;
+
+		set_in_cr4(X86_CR4_OSXSAVE);
+		
+		cr4 = read_cr4();
+
+		if ((cr4 & X86_CR4_OSXSAVE) == 0)
+			cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_XSAVE % 32));
+
+		clear_in_cr4(X86_CR4_OSXSAVE);
+	}
+}
+
 static void xen_set_debugreg(int reg, unsigned long val)
 {
 	HYPERVISOR_set_debugreg(reg, val);
@@ -903,6 +939,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_init_irq_ops();
 
+	xen_init_cpuid_mask();
+
 #ifdef CONFIG_X86_LOCAL_APIC
 	/*
 	 * set up the basic apic ops.

commit c667d5d6a77a5f33f9181bcf92a04fdc69712a2b
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 16:34:27 2009 -0800

    xen: remove xen_load_gdt debug
    
    Don't need the noise.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 334d2ffc4e2a..5d9a1c37c515 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -310,9 +310,6 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 
 		frames[f] = mfn;
 
-		printk("xen_load_gdt: %d va=%p mfn=%lx pfn=%lx va'=%p\n",
-		       f, (void *)va, mfn, pfn, virt);
-
 		make_lowmem_page_readonly((void *)va);
 		make_lowmem_page_readonly(virt);
 	}

commit a957fac50023eac09368ab19d3a7ec725c2657c3
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 15:26:00 2009 -0800

    xen: make xen_load_gdt simpler
    
    Remove use of multicall machinery which is unused (gdt loading
    is never performance critical).  This removes the implicit use
    of percpu variables, which simplifies understanding how
    the percpu code's use of load_gdt interacts with this code.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8e024554a3e1..334d2ffc4e2a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -284,12 +284,11 @@ static void xen_set_ldt(const void *addr, unsigned entries)
 
 static void xen_load_gdt(const struct desc_ptr *dtr)
 {
-	unsigned long *frames;
 	unsigned long va = dtr->address;
 	unsigned int size = dtr->size + 1;
 	unsigned pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	unsigned long frames[pages];
 	int f;
-	struct multicall_space mcs;
 
 	/* A GDT can be up to 64k in size, which corresponds to 8192
 	   8-byte entries, or 16 4k pages.. */
@@ -297,9 +296,6 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 	BUG_ON(size > 65536);
 	BUG_ON(va & ~PAGE_MASK);
 
-	mcs = xen_mc_entry(sizeof(*frames) * pages);
-	frames = mcs.args;
-
 	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
 		int level;
 		pte_t *ptep = lookup_address(va, &level);
@@ -314,13 +310,15 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 
 		frames[f] = mfn;
 
+		printk("xen_load_gdt: %d va=%p mfn=%lx pfn=%lx va'=%p\n",
+		       f, (void *)va, mfn, pfn, virt);
+
 		make_lowmem_page_readonly((void *)va);
 		make_lowmem_page_readonly(virt);
 	}
 
-	MULTI_set_gdt(mcs.mc, frames, size / sizeof(struct desc_struct));
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
+	if (HYPERVISOR_set_gdt(frames, size / sizeof(struct desc_struct)))
+		BUG();
 }
 
 static void load_TLS_descriptor(struct thread_struct *t,

commit c7da8c829b3f919089ff021d6ddc376d38299729
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 13:02:18 2009 -0800

    xen: clean up xen_load_gdt
    
    Makes the logic a bit clearer.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 82cd39a6cbd3..8e024554a3e1 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -301,10 +301,21 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 	frames = mcs.args;
 
 	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
-		frames[f] = arbitrary_virt_to_mfn((void *)va);
+		int level;
+		pte_t *ptep = lookup_address(va, &level);
+		unsigned long pfn, mfn;
+		void *virt;
+
+		BUG_ON(ptep == NULL);
+
+		pfn = pte_pfn(*ptep);
+		mfn = pfn_to_mfn(pfn);
+		virt = __va(PFN_PHYS(pfn));
+
+		frames[f] = mfn;
 
 		make_lowmem_page_readonly((void *)va);
-		make_lowmem_page_readonly(mfn_to_virt(frames[f]));
+		make_lowmem_page_readonly(virt);
 	}
 
 	MULTI_set_gdt(mcs.mc, frames, size / sizeof(struct desc_struct));

commit 6d02c42698f99eccb290ac53d4f10ca883b9f90c
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Sun Mar 29 22:57:15 2009 -0700

    xen: clean up gate trap/interrupt constants
    
    Use GATE_INTERRUPT/TRAP rather than 0xe/f.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 80f4c5343495..12a3159333bc 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -428,7 +428,7 @@ static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 static int cvt_gate_to_trap(int vector, const gate_desc *val,
 			    struct trap_info *info)
 {
-	if (val->type != 0xf && val->type != 0xe)
+	if (val->type != GATE_TRAP && val->type != GATE_INTERRUPT)
 		return 0;
 
 	info->vector = vector;
@@ -436,8 +436,8 @@ static int cvt_gate_to_trap(int vector, const gate_desc *val,
 	info->cs = gate_segment(*val);
 	info->flags = val->dpl;
 	/* interrupt gates clear IF */
-	if (val->type == 0xe)
-		info->flags |= 4;
+	if (val->type == GATE_INTERRUPT)
+		info->flags |= 1 << 2;
 
 	return 1;
 }

commit 707ebbc81c61eb480d8a51ca61e355e240df1d32
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Mar 27 11:29:02 2009 -0700

    xen: set _PAGE_NX in __supported_pte_mask before pagetable construction
    
    Some 64-bit machines don't support the NX flag in ptes.
    Check for NX before constructing the kernel pagetables.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index da33e0c5870d..80f4c5343495 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -42,6 +42,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/fixmap.h>
 #include <asm/processor.h>
+#include <asm/proto.h>
 #include <asm/msr-index.h>
 #include <asm/setup.h>
 #include <asm/desc.h>
@@ -912,7 +913,6 @@ static const struct machine_ops __initdata xen_machine_ops = {
 	.emergency_restart = xen_emergency_restart,
 };
 
-
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {
@@ -980,6 +980,11 @@ asmlinkage void __init xen_start_kernel(void)
 	if (!xen_initial_domain())
 		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
 
+#ifdef CONFIG_X86_64
+	/* Work out if we support NX */
+	check_efer();
+#endif
+
 	/* Don't do the full vcpu_info placement stuff until we have a
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];

commit e826fe1ba1563a9272345da8e3279a930ac160a7
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Sat Mar 7 17:09:27 2009 -0800

    xen: mask XSAVE from cpuid
    
    Xen leaves XSAVE set in cpuid, but doesn't allow cr4.OSXSAVE
    to be set.  This confuses the kernel and it ends up crashing on
    an xsetbv instruction.
    
    At boot time, try to set cr4.OSXSAVE, and mask XSAVE out of
    cpuid it we can't.  This will produce a spurious error from Xen,
    but allows us to support XSAVE if/when Xen does.
    
    This also factors out the cpuid mask decisions to boot time.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 75b7a0f90380..da33e0c5870d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -168,21 +168,23 @@ static void __init xen_banner(void)
 	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
 }
 
+static __read_mostly unsigned int cpuid_leaf1_edx_mask = ~0;
+static __read_mostly unsigned int cpuid_leaf1_ecx_mask = ~0;
+
 static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		      unsigned int *cx, unsigned int *dx)
 {
+	unsigned maskecx = ~0;
 	unsigned maskedx = ~0;
 
 	/*
 	 * Mask out inconvenient features, to try and disable as many
 	 * unsupported kernel subsystems as possible.
 	 */
-	if (*ax == 1)
-		maskedx = ~((1 << X86_FEATURE_APIC) |  /* disable APIC */
-			    (1 << X86_FEATURE_ACPI) |  /* disable ACPI */
-			    (1 << X86_FEATURE_MCE)  |  /* disable MCE */
-			    (1 << X86_FEATURE_MCA)  |  /* disable MCA */
-			    (1 << X86_FEATURE_ACC));   /* thermal monitoring */
+	if (*ax == 1) {
+		maskecx = cpuid_leaf1_ecx_mask;
+		maskedx = cpuid_leaf1_edx_mask;
+	}
 
 	asm(XEN_EMULATE_PREFIX "cpuid"
 		: "=a" (*ax),
@@ -190,9 +192,43 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 		  "=c" (*cx),
 		  "=d" (*dx)
 		: "0" (*ax), "2" (*cx));
+
+	*cx &= maskecx;
 	*dx &= maskedx;
 }
 
+static __init void xen_init_cpuid_mask(void)
+{
+	unsigned int ax, bx, cx, dx;
+
+	cpuid_leaf1_edx_mask =
+		~((1 << X86_FEATURE_MCE)  |  /* disable MCE */
+		  (1 << X86_FEATURE_MCA)  |  /* disable MCA */
+		  (1 << X86_FEATURE_ACC));   /* thermal monitoring */
+
+	if (!xen_initial_domain())
+		cpuid_leaf1_edx_mask &=
+			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
+			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
+
+	ax = 1;
+	xen_cpuid(&ax, &bx, &cx, &dx);
+
+	/* cpuid claims we support xsave; try enabling it to see what happens */
+	if (cx & (1 << (X86_FEATURE_XSAVE % 32))) {
+		unsigned long cr4;
+
+		set_in_cr4(X86_CR4_OSXSAVE);
+		
+		cr4 = read_cr4();
+
+		if ((cr4 & X86_CR4_OSXSAVE) == 0)
+			cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_XSAVE % 32));
+
+		clear_in_cr4(X86_CR4_OSXSAVE);
+	}
+}
+
 static void xen_set_debugreg(int reg, unsigned long val)
 {
 	HYPERVISOR_set_debugreg(reg, val);
@@ -901,6 +937,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_init_irq_ops();
 
+	xen_init_cpuid_mask();
+
 #ifdef CONFIG_X86_LOCAL_APIC
 	/*
 	 * set up the basic apic ops.

commit b4b7e58590d0e94ed78bd6be1aa163caba7b6c74
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 16:34:27 2009 -0800

    xen: remove xen_load_gdt debug
    
    Don't need the noise.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 48b399bd6e0d..75b7a0f90380 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -310,9 +310,6 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 
 		frames[f] = mfn;
 
-		printk("xen_load_gdt: %d va=%p mfn=%lx pfn=%lx va'=%p\n",
-		       f, (void *)va, mfn, pfn, virt);
-
 		make_lowmem_page_readonly((void *)va);
 		make_lowmem_page_readonly(virt);
 	}

commit 3ce5fa7ebff74b6a4dc5fdcdc22e6979f5a4ff85
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 15:26:00 2009 -0800

    xen: make xen_load_gdt simpler
    
    Remove use of multicall machinery which is unused (gdt loading
    is never performance critical).  This removes the implicit use
    of percpu variables, which simplifies understanding how
    the percpu code's use of load_gdt interacts with this code.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5776dc270297..48b399bd6e0d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -284,12 +284,11 @@ static void xen_set_ldt(const void *addr, unsigned entries)
 
 static void xen_load_gdt(const struct desc_ptr *dtr)
 {
-	unsigned long *frames;
 	unsigned long va = dtr->address;
 	unsigned int size = dtr->size + 1;
 	unsigned pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	unsigned long frames[pages];
 	int f;
-	struct multicall_space mcs;
 
 	/* A GDT can be up to 64k in size, which corresponds to 8192
 	   8-byte entries, or 16 4k pages.. */
@@ -297,9 +296,6 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 	BUG_ON(size > 65536);
 	BUG_ON(va & ~PAGE_MASK);
 
-	mcs = xen_mc_entry(sizeof(*frames) * pages);
-	frames = mcs.args;
-
 	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
 		int level;
 		pte_t *ptep = lookup_address(va, &level);
@@ -314,13 +310,15 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 
 		frames[f] = mfn;
 
+		printk("xen_load_gdt: %d va=%p mfn=%lx pfn=%lx va'=%p\n",
+		       f, (void *)va, mfn, pfn, virt);
+
 		make_lowmem_page_readonly((void *)va);
 		make_lowmem_page_readonly(virt);
 	}
 
-	MULTI_set_gdt(mcs.mc, frames, size / sizeof(struct desc_struct));
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
+	if (HYPERVISOR_set_gdt(frames, size / sizeof(struct desc_struct)))
+		BUG();
 }
 
 static void load_TLS_descriptor(struct thread_struct *t,

commit 6ed6bf428aff64fe37cdc54b239d598fee6016f1
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Mar 4 13:02:18 2009 -0800

    xen: clean up xen_load_gdt
    
    Makes the logic a bit clearer.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 70b355d3a86c..5776dc270297 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -301,10 +301,21 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 	frames = mcs.args;
 
 	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
-		frames[f] = arbitrary_virt_to_mfn((void *)va);
+		int level;
+		pte_t *ptep = lookup_address(va, &level);
+		unsigned long pfn, mfn;
+		void *virt;
+
+		BUG_ON(ptep == NULL);
+
+		pfn = pte_pfn(*ptep);
+		mfn = pfn_to_mfn(pfn);
+		virt = __va(PFN_PHYS(pfn));
+
+		frames[f] = mfn;
 
 		make_lowmem_page_readonly((void *)va);
-		make_lowmem_page_readonly(mfn_to_virt(frames[f]));
+		make_lowmem_page_readonly(virt);
 	}
 
 	MULTI_set_gdt(mcs.mc, frames, size / sizeof(struct desc_struct));

commit 224101ed69d3fbb486868e0f6e0f9fa37302efb4
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Feb 18 11:18:57 2009 -0800

    x86/paravirt: finish change from lazy cpu to context switch start/end
    
    Impact: fix lazy context switch API
    
    Pass the previous and next tasks into the context switch start
    end calls, so that the called functions can properly access the
    task state (esp in end_context_switch, in which the next task
    is not yet completely current).
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f586e63b9a63..70b355d3a86c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -203,10 +203,10 @@ static unsigned long xen_get_debugreg(int reg)
 	return HYPERVISOR_get_debugreg(reg);
 }
 
-static void xen_leave_lazy_cpu(void)
+static void xen_end_context_switch(struct task_struct *next)
 {
 	xen_mc_flush();
-	paravirt_leave_lazy_cpu();
+	paravirt_end_context_switch(next);
 }
 
 static unsigned long xen_store_tr(void)
@@ -817,10 +817,8 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	/* Xen takes care of %gs when switching to usermode for us */
 	.swapgs = paravirt_nop,
 
-	.lazy_mode = {
-		.enter = paravirt_enter_lazy_cpu,
-		.leave = xen_leave_lazy_cpu,
-	},
+	.start_context_switch = paravirt_start_context_switch,
+	.end_context_switch = xen_end_context_switch,
 };
 
 static const struct pv_apic_ops xen_apic_ops __initdata = {

commit b407fc57b815b2016186220baabc76cc8264206e
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Feb 17 23:46:21 2009 -0800

    x86/paravirt: flush pending mmu updates on context switch
    
    Impact: allow preemption during lazy mmu updates
    
    If we're in lazy mmu mode when context switching, leave
    lazy mmu mode, but remember the task's state in
    TIF_LAZY_MMU_UPDATES.  When we resume the task, check this
    flag and re-enter lazy mmu mode if its set.
    
    This sets things up for allowing lazy mmu mode while preemptible,
    though that won't actually be active until the next change.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 82cd39a6cbd3..f586e63b9a63 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -203,10 +203,10 @@ static unsigned long xen_get_debugreg(int reg)
 	return HYPERVISOR_get_debugreg(reg);
 }
 
-void xen_leave_lazy(void)
+static void xen_leave_lazy_cpu(void)
 {
-	paravirt_leave_lazy(paravirt_get_lazy_mode());
 	xen_mc_flush();
+	paravirt_leave_lazy_cpu();
 }
 
 static unsigned long xen_store_tr(void)
@@ -819,7 +819,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 
 	.lazy_mode = {
 		.enter = paravirt_enter_lazy_cpu,
-		.leave = xen_leave_lazy,
+		.leave = xen_leave_lazy_cpu,
 	},
 };
 

commit 91d75e209bd59695f0708d66964d928d45b3b2f3
Merge: 9976b39b5031 8b0e5860cb09
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 4 02:29:19 2009 +0100

    Merge branch 'x86/core' into core/percpu

commit 9976b39b5031bbf76f715893cf080b6a17683881
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Feb 27 09:19:26 2009 -0800

    xen: deal with virtually mapped percpu data
    
    The virtually mapped percpu space causes us two problems:
    
     - for hypercalls which take an mfn, we need to do a full pagetable
       walk to convert the percpu va into an mfn, and
    
     - when a hypercall requires a page to be mapped RO via all its aliases,
       we need to make sure its RO in both the percpu mapping and in the
       linear mapping
    
    This primarily affects the gdt and the vcpu info structure.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Xen-devel <xen-devel@lists.xensource.com>
    Cc: Gerd Hoffmann <kraxel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 86497d5f44cd..352ea6830659 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -103,7 +103,7 @@ static void xen_vcpu_setup(int cpu)
 
 	vcpup = &per_cpu(xen_vcpu_info, cpu);
 
-	info.mfn = virt_to_mfn(vcpup);
+	info.mfn = arbitrary_virt_to_mfn(vcpup);
 	info.offset = offset_in_page(vcpup);
 
 	printk(KERN_DEBUG "trying to map vcpu_info %d at %p, mfn %llx, offset %d\n",
@@ -301,8 +301,10 @@ static void xen_load_gdt(const struct desc_ptr *dtr)
 	frames = mcs.args;
 
 	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
-		frames[f] = virt_to_mfn(va);
+		frames[f] = arbitrary_virt_to_mfn((void *)va);
+
 		make_lowmem_page_readonly((void *)va);
+		make_lowmem_page_readonly(mfn_to_virt(frames[f]));
 	}
 
 	MULTI_set_gdt(mcs.mc, frames, size / sizeof(struct desc_struct));
@@ -314,7 +316,7 @@ static void load_TLS_descriptor(struct thread_struct *t,
 				unsigned int cpu, unsigned int i)
 {
 	struct desc_struct *gdt = get_cpu_gdt_table(cpu);
-	xmaddr_t maddr = virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);
+	xmaddr_t maddr = arbitrary_virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);
 	struct multicall_space mc = __xen_mc_entry(0);
 
 	MULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);
@@ -488,7 +490,7 @@ static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
 		break;
 
 	default: {
-		xmaddr_t maddr = virt_to_machine(&dt[entry]);
+		xmaddr_t maddr = arbitrary_virt_to_machine(&dt[entry]);
 
 		xen_mc_flush();
 		if (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))

commit 13b2eda64d14d0a0c15c092664c7351ea58ea851
Merge: a852cbfaaf81 55d808567186
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 26 06:30:42 2009 +0100

    Merge branch 'x86/urgent' into x86/core
    
    Conflicts:
            arch/x86/mach-voyager/voyager_smp.c

commit 55d8085671863fe4ee6a17b7814bd38180a44e1d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Feb 25 09:42:25 2009 -0800

    xen: disable interrupts early, as start_kernel expects
    
    This avoids a lockdep warning from:
            if (DEBUG_LOCKS_WARN_ON(unlikely(!early_boot_irqs_enabled)))
                    return;
    in trace_hardirqs_on_caller();
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Cc: Xen-devel <xen-devel@lists.xensource.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bea215230b20..b58e96338149 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1672,6 +1672,9 @@ asmlinkage void __init xen_start_kernel(void)
 	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
 
+	local_irq_disable();
+	early_boot_irqs_off();
+
 	xen_raw_console_write("mapping kernel into physical memory\n");
 	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
 

commit 7b6aa335ca1a845c2262ec7a595b4521bca0f79d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 13:58:15 2009 +0100

    x86, apic: remove genapic.h
    
    Impact: cleanup
    
    Remove genapic.h and remove all references to it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e3dd3fb67290..86497d5f44cd 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -36,7 +36,7 @@
 #include <xen/hvc-console.h>
 
 #include <asm/paravirt.h>
-#include <asm/genapic.h>
+#include <asm/apic.h>
 #include <asm/page.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>

commit c1eeb2de41d7015678bdd412b48a5f071b84e29a
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Feb 16 23:02:14 2009 -0800

    x86: fold apic_ops into genapic
    
    Impact: cleanup
    
    make it simpler, don't need have one extra struct.
    
    v2: fix the sgi_uv build
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 95ff6a0e942a..e3dd3fb67290 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -36,7 +36,7 @@
 #include <xen/hvc-console.h>
 
 #include <asm/paravirt.h>
-#include <asm/apic.h>
+#include <asm/genapic.h>
 #include <asm/page.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>
@@ -554,14 +554,15 @@ static u32 xen_safe_apic_wait_icr_idle(void)
         return 0;
 }
 
-static struct apic_ops xen_basic_apic_ops = {
-	.read = xen_apic_read,
-	.write = xen_apic_write,
-	.icr_read = xen_apic_icr_read,
-	.icr_write = xen_apic_icr_write,
-	.wait_icr_idle = xen_apic_wait_icr_idle,
-	.safe_wait_icr_idle = xen_safe_apic_wait_icr_idle,
-};
+static void set_xen_basic_apic_ops(void)
+{
+	apic->read = xen_apic_read;
+	apic->write = xen_apic_write;
+	apic->icr_read = xen_apic_icr_read;
+	apic->icr_write = xen_apic_icr_write;
+	apic->wait_icr_idle = xen_apic_wait_icr_idle;
+	apic->safe_wait_icr_idle = xen_safe_apic_wait_icr_idle;
+}
 
 #endif
 
@@ -898,7 +899,7 @@ asmlinkage void __init xen_start_kernel(void)
 	/*
 	 * set up the basic apic ops.
 	 */
-	apic_ops = &xen_basic_apic_ops;
+	set_xen_basic_apic_ops();
 #endif
 
 	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {

commit ccbeed3a05908d201b47b6c3dd1a373138bba566
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 9 22:17:40 2009 +0900

    x86: make lazy %gs optional on x86_32
    
    Impact: pt_regs changed, lazy gs handling made optional, add slight
            overhead to SAVE_ALL, simplifies error_code path a bit
    
    On x86_32, %gs hasn't been used by kernel and handled lazily.  pt_regs
    doesn't have place for it and gs is saved/loaded only when necessary.
    In preparation for stack protector support, this patch makes lazy %gs
    handling optional by doing the followings.
    
    * Add CONFIG_X86_32_LAZY_GS and place for gs in pt_regs.
    
    * Save and restore %gs along with other registers in entry_32.S unless
      LAZY_GS.  Note that this unfortunately adds "pushl $0" on SAVE_ALL
      even when LAZY_GS.  However, it adds no overhead to common exit path
      and simplifies entry path with error code.
    
    * Define different user_gs accessors depending on LAZY_GS and add
      lazy_save_gs() and lazy_load_gs() which are noop if !LAZY_GS.  The
      lazy_*_gs() ops are used to save, load and clear %gs lazily.
    
    * Define ELF_CORE_COPY_KERNEL_REGS() which always read %gs directly.
    
    xen and lguest changes need to be verified.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 37230342c2c4..95ff6a0e942a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -323,13 +323,14 @@ static void load_TLS_descriptor(struct thread_struct *t,
 static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
 {
 	/*
-	 * XXX sleazy hack: If we're being called in a lazy-cpu zone,
-	 * it means we're in a context switch, and %gs has just been
-	 * saved.  This means we can zero it out to prevent faults on
-	 * exit from the hypervisor if the next process has no %gs.
-	 * Either way, it has been saved, and the new value will get
-	 * loaded properly.  This will go away as soon as Xen has been
-	 * modified to not save/restore %gs for normal hypercalls.
+	 * XXX sleazy hack: If we're being called in a lazy-cpu zone
+	 * and lazy gs handling is enabled, it means we're in a
+	 * context switch, and %gs has just been saved.  This means we
+	 * can zero it out to prevent faults on exit from the
+	 * hypervisor if the next process has no %gs.  Either way, it
+	 * has been saved, and the new value will get loaded properly.
+	 * This will go away as soon as Xen has been modified to not
+	 * save/restore %gs for normal hypercalls.
 	 *
 	 * On x86_64, this hack is not used for %gs, because gs points
 	 * to KERNEL_GS_BASE (and uses it for PDA references), so we
@@ -341,7 +342,7 @@ static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
 	 */
 	if (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU) {
 #ifdef CONFIG_X86_32
-		loadsegment(gs, 0);
+		lazy_load_gs(0);
 #else
 		loadsegment(fs, 0);
 #endif

commit e4d0407185cdbdcfd99fc23bde2e5454bbc46329
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Feb 2 13:55:54 2009 -0800

    xen: use direct ops on 64-bit
    
    Enable the use of the direct vcpu-access operations on 64-bit.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index aed7ceeb4b65..37230342c2c4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -87,14 +87,7 @@ struct shared_info *HYPERVISOR_shared_info = (void *)&xen_dummy_shared_info;
  *
  * 0: not available, 1: available
  */
-static int have_vcpu_info_placement =
-#ifdef CONFIG_X86_32
-	1
-#else
-	0
-#endif
-	;
-
+static int have_vcpu_info_placement = 1;
 
 static void xen_vcpu_setup(int cpu)
 {
@@ -914,11 +907,6 @@ asmlinkage void __init xen_start_kernel(void)
 
 	machine_ops = xen_machine_ops;
 
-#ifdef CONFIG_X86_64
-	/* Disable until direct per-cpu data access. */
-	have_vcpu_info_placement = 0;
-#endif
-
 #ifdef CONFIG_X86_64
 	/*
 	 * Setup percpu state.  We only need to do this for 64-bit

commit 383414322b3b3ced0cbc146801e0cc6c60a6c5f4
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Feb 2 13:55:31 2009 -0800

    xen: setup percpu data pointers
    
    We need to access percpu data fairly early, so set up the percpu
    registers as soon as possible.  We only need to load the appropriate
    segment register.  We already have a GDT, but its hard to change it
    early because we need to manipulate the pagetable to do so, and that
    hasn't been set up yet.
    
    Also, set the kernel stack when bringing up secondary CPUs.  If we
    don't they all end up sharing the same stack...
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index cd022c43dfbc..aed7ceeb4b65 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -66,6 +66,8 @@ EXPORT_SYMBOL_GPL(xen_start_info);
 
 struct shared_info xen_dummy_shared_info;
 
+void *xen_initial_gdt;
+
 /*
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.
@@ -917,8 +919,19 @@ asmlinkage void __init xen_start_kernel(void)
 	have_vcpu_info_placement = 0;
 #endif
 
-	/* setup percpu state */
+#ifdef CONFIG_X86_64
+	/*
+	 * Setup percpu state.  We only need to do this for 64-bit
+	 * because 32-bit already has %fs set properly.
+	 */
 	load_percpu_segment(0);
+#endif
+	/*
+	 * The only reliable way to retain the initial address of the
+	 * percpu gdt_page is to remember it here, so we can go and
+	 * mark it RW later, when the initial percpu area is freed.
+	 */
+	xen_initial_gdt = &per_cpu(gdt_page, 0);
 
 	xen_smp_init();
 

commit 327641da8e3e227f42690479182b896fd19486be
Merge: 1f4f931501e9 ef3892bd6342
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Feb 4 16:58:26 2009 -0800

    Merge branch 'core/percpu' into x86/paravirt

commit 795f99b61d20c34cb04d17d8906b32f745a635ec
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Jan 30 17:47:54 2009 +0900

    xen: setup percpu data pointers
    
    Impact: fix xen booting
    
    We need to access percpu data fairly early, so set up the percpu
    registers as soon as possible.  We only need to load the appropriate
    segment register.  We already have a GDT, but its hard to change it
    early because we need to manipulate the pagetable to do so, and that
    hasn't been set up yet.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bef941f61451..fe19c88a5029 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1647,6 +1647,9 @@ asmlinkage void __init xen_start_kernel(void)
 	have_vcpu_info_placement = 0;
 #endif
 
+	/* setup percpu state */
+	load_percpu_segment(0);
+
 	xen_smp_init();
 
 	/* Get mfn list */

commit ecb93d1ccd0aac63f03be2db3cac3fa974716f4c
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jan 28 14:35:05 2009 -0800

    x86/paravirt: add register-saving thunks to reduce caller register pressure
    
    Impact: Optimization
    
    One of the problems with inserting a pile of C calls where previously
    there were none is that the register pressure is greatly increased.
    The C calling convention says that the caller must expect a certain
    set of registers may be trashed by the callee, and that the callee can
    use those registers without restriction.  This includes the function
    argument registers, and several others.
    
    This patch seeks to alleviate this pressure by introducing wrapper
    thunks that will do the register saving/restoring, so that the
    callsite doesn't need to worry about it, but the callee function can
    be conventional compiler-generated code.  In many cases (particularly
    performance-sensitive cases) the callee will be in assembler anyway,
    and need not use the compiler's calling convention.
    
    Standard calling convention is:
             arguments          return      scratch
    x86-32   eax edx ecx        eax         ?
    x86-64   rdi rsi rdx rcx    rax         r8 r9 r10 r11
    
    The thunk preserves all argument and scratch registers.  The return
    register is not preserved, and is available as a scratch register for
    unwrapped callee code (and of course the return value).
    
    Wrapped function pointers are themselves wrapped in a struct
    paravirt_callee_save structure, in order to get some warning from the
    compiler when functions with mismatched calling conventions are used.
    
    The most common paravirt ops, both statically and dynamically, are
    interrupt enable/disable/save/restore, so handle them first.  This is
    particularly easy since their calls are handled specially anyway.
    
    XXX Deal with VMI.  What's their calling convention?
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0cd2a165f179..ff6d530ccc77 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -676,10 +676,10 @@ void xen_setup_vcpu_info_placement(void)
 	if (have_vcpu_info_placement) {
 		printk(KERN_INFO "Xen: using vcpu_info placement\n");
 
-		pv_irq_ops.save_fl = xen_save_fl_direct;
-		pv_irq_ops.restore_fl = xen_restore_fl_direct;
-		pv_irq_ops.irq_disable = xen_irq_disable_direct;
-		pv_irq_ops.irq_enable = xen_irq_enable_direct;
+		pv_irq_ops.save_fl = __PV_IS_CALLEE_SAVE(xen_save_fl_direct);
+		pv_irq_ops.restore_fl = __PV_IS_CALLEE_SAVE(xen_restore_fl_direct);
+		pv_irq_ops.irq_disable = __PV_IS_CALLEE_SAVE(xen_irq_disable_direct);
+		pv_irq_ops.irq_enable = __PV_IS_CALLEE_SAVE(xen_irq_enable_direct);
 		pv_mmu_ops.read_cr2 = xen_read_cr2_direct;
 	}
 }

commit 319f3ba52c71630865b10ac3b99dd020440d681d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jan 28 14:35:01 2009 -0800

    xen: move remaining mmu-related stuff into mmu.c
    
    Impact: Cleanup
    
    Move remaining mmu-related stuff into mmu.c.
    A general cleanup, and lay the groundwork for later patches.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 6b3f7eef57e3..0cd2a165f179 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -61,35 +61,6 @@ DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 enum xen_domain_type xen_domain_type = XEN_NATIVE;
 EXPORT_SYMBOL_GPL(xen_domain_type);
 
-/*
- * Identity map, in addition to plain kernel map.  This needs to be
- * large enough to allocate page table pages to allocate the rest.
- * Each page can map 2MB.
- */
-static pte_t level1_ident_pgt[PTRS_PER_PTE * 4] __page_aligned_bss;
-
-#ifdef CONFIG_X86_64
-/* l3 pud for userspace vsyscall mapping */
-static pud_t level3_user_vsyscall[PTRS_PER_PUD] __page_aligned_bss;
-#endif /* CONFIG_X86_64 */
-
-/*
- * Note about cr3 (pagetable base) values:
- *
- * xen_cr3 contains the current logical cr3 value; it contains the
- * last set cr3.  This may not be the current effective cr3, because
- * its update may be being lazily deferred.  However, a vcpu looking
- * at its own cr3 can use this value knowing that it everything will
- * be self-consistent.
- *
- * xen_current_cr3 contains the actual vcpu cr3; it is set once the
- * hypercall to set the vcpu cr3 is complete (so it may be a little
- * out of date, but it will never be set early).  If one vcpu is
- * looking at another vcpu's cr3 value, it should use this variable.
- */
-DEFINE_PER_CPU(unsigned long, xen_cr3);	 /* cr3 stored as physaddr */
-DEFINE_PER_CPU(unsigned long, xen_current_cr3);	 /* actual vcpu cr3 */
-
 struct start_info *xen_start_info;
 EXPORT_SYMBOL_GPL(xen_start_info);
 
@@ -237,7 +208,7 @@ static unsigned long xen_get_debugreg(int reg)
 	return HYPERVISOR_get_debugreg(reg);
 }
 
-static void xen_leave_lazy(void)
+void xen_leave_lazy(void)
 {
 	paravirt_leave_lazy(paravirt_get_lazy_mode());
 	xen_mc_flush();
@@ -598,76 +569,6 @@ static struct apic_ops xen_basic_apic_ops = {
 
 #endif
 
-static void xen_flush_tlb(void)
-{
-	struct mmuext_op *op;
-	struct multicall_space mcs;
-
-	preempt_disable();
-
-	mcs = xen_mc_entry(sizeof(*op));
-
-	op = mcs.args;
-	op->cmd = MMUEXT_TLB_FLUSH_LOCAL;
-	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
-
-	xen_mc_issue(PARAVIRT_LAZY_MMU);
-
-	preempt_enable();
-}
-
-static void xen_flush_tlb_single(unsigned long addr)
-{
-	struct mmuext_op *op;
-	struct multicall_space mcs;
-
-	preempt_disable();
-
-	mcs = xen_mc_entry(sizeof(*op));
-	op = mcs.args;
-	op->cmd = MMUEXT_INVLPG_LOCAL;
-	op->arg1.linear_addr = addr & PAGE_MASK;
-	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
-
-	xen_mc_issue(PARAVIRT_LAZY_MMU);
-
-	preempt_enable();
-}
-
-static void xen_flush_tlb_others(const struct cpumask *cpus,
-				 struct mm_struct *mm, unsigned long va)
-{
-	struct {
-		struct mmuext_op op;
-		DECLARE_BITMAP(mask, NR_CPUS);
-	} *args;
-	struct multicall_space mcs;
-
-	BUG_ON(cpumask_empty(cpus));
-	BUG_ON(!mm);
-
-	mcs = xen_mc_entry(sizeof(*args));
-	args = mcs.args;
-	args->op.arg2.vcpumask = to_cpumask(args->mask);
-
-	/* Remove us, and any offline CPUS. */
-	cpumask_and(to_cpumask(args->mask), cpus, cpu_online_mask);
-	cpumask_clear_cpu(smp_processor_id(), to_cpumask(args->mask));
-	if (unlikely(cpumask_empty(to_cpumask(args->mask))))
-		goto issue;
-
-	if (va == TLB_FLUSH_ALL) {
-		args->op.cmd = MMUEXT_TLB_FLUSH_MULTI;
-	} else {
-		args->op.cmd = MMUEXT_INVLPG_MULTI;
-		args->op.arg1.linear_addr = va;
-	}
-
-	MULTI_mmuext_op(mcs.mc, &args->op, 1, NULL, DOMID_SELF);
-
-issue:
-	xen_mc_issue(PARAVIRT_LAZY_MMU);
-}
 
 static void xen_clts(void)
 {
@@ -693,21 +594,6 @@ static void xen_write_cr0(unsigned long cr0)
 	xen_mc_issue(PARAVIRT_LAZY_CPU);
 }
 
-static void xen_write_cr2(unsigned long cr2)
-{
-	percpu_read(xen_vcpu)->arch.cr2 = cr2;
-}
-
-static unsigned long xen_read_cr2(void)
-{
-	return percpu_read(xen_vcpu)->arch.cr2;
-}
-
-static unsigned long xen_read_cr2_direct(void)
-{
-	return percpu_read(xen_vcpu_info.arch.cr2);
-}
-
 static void xen_write_cr4(unsigned long cr4)
 {
 	cr4 &= ~X86_CR4_PGE;
@@ -716,71 +602,6 @@ static void xen_write_cr4(unsigned long cr4)
 	native_write_cr4(cr4);
 }
 
-static unsigned long xen_read_cr3(void)
-{
-	return percpu_read(xen_cr3);
-}
-
-static void set_current_cr3(void *v)
-{
-	percpu_write(xen_current_cr3, (unsigned long)v);
-}
-
-static void __xen_write_cr3(bool kernel, unsigned long cr3)
-{
-	struct mmuext_op *op;
-	struct multicall_space mcs;
-	unsigned long mfn;
-
-	if (cr3)
-		mfn = pfn_to_mfn(PFN_DOWN(cr3));
-	else
-		mfn = 0;
-
-	WARN_ON(mfn == 0 && kernel);
-
-	mcs = __xen_mc_entry(sizeof(*op));
-
-	op = mcs.args;
-	op->cmd = kernel ? MMUEXT_NEW_BASEPTR : MMUEXT_NEW_USER_BASEPTR;
-	op->arg1.mfn = mfn;
-
-	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
-
-	if (kernel) {
-		percpu_write(xen_cr3, cr3);
-
-		/* Update xen_current_cr3 once the batch has actually
-		   been submitted. */
-		xen_mc_callback(set_current_cr3, (void *)cr3);
-	}
-}
-
-static void xen_write_cr3(unsigned long cr3)
-{
-	BUG_ON(preemptible());
-
-	xen_mc_batch();  /* disables interrupts */
-
-	/* Update while interrupts are disabled, so its atomic with
-	   respect to ipis */
-	percpu_write(xen_cr3, cr3);
-
-	__xen_write_cr3(true, cr3);
-
-#ifdef CONFIG_X86_64
-	{
-		pgd_t *user_pgd = xen_get_user_pgd(__va(cr3));
-		if (user_pgd)
-			__xen_write_cr3(false, __pa(user_pgd));
-		else
-			__xen_write_cr3(false, 0);
-	}
-#endif
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);  /* interrupts restored */
-}
-
 static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 {
 	int ret;
@@ -822,185 +643,6 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 	return ret;
 }
 
-/* Early in boot, while setting up the initial pagetable, assume
-   everything is pinned. */
-static __init void xen_alloc_pte_init(struct mm_struct *mm, unsigned long pfn)
-{
-#ifdef CONFIG_FLATMEM
-	BUG_ON(mem_map);	/* should only be used early */
-#endif
-	make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
-}
-
-/* Early release_pte assumes that all pts are pinned, since there's
-   only init_mm and anything attached to that is pinned. */
-static void xen_release_pte_init(unsigned long pfn)
-{
-	make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
-}
-
-static void pin_pagetable_pfn(unsigned cmd, unsigned long pfn)
-{
-	struct mmuext_op op;
-	op.cmd = cmd;
-	op.arg1.mfn = pfn_to_mfn(pfn);
-	if (HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF))
-		BUG();
-}
-
-/* This needs to make sure the new pte page is pinned iff its being
-   attached to a pinned pagetable. */
-static void xen_alloc_ptpage(struct mm_struct *mm, unsigned long pfn, unsigned level)
-{
-	struct page *page = pfn_to_page(pfn);
-
-	if (PagePinned(virt_to_page(mm->pgd))) {
-		SetPagePinned(page);
-
-		vm_unmap_aliases();
-		if (!PageHighMem(page)) {
-			make_lowmem_page_readonly(__va(PFN_PHYS((unsigned long)pfn)));
-			if (level == PT_PTE && USE_SPLIT_PTLOCKS)
-				pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
-		} else {
-			/* make sure there are no stray mappings of
-			   this page */
-			kmap_flush_unused();
-		}
-	}
-}
-
-static void xen_alloc_pte(struct mm_struct *mm, unsigned long pfn)
-{
-	xen_alloc_ptpage(mm, pfn, PT_PTE);
-}
-
-static void xen_alloc_pmd(struct mm_struct *mm, unsigned long pfn)
-{
-	xen_alloc_ptpage(mm, pfn, PT_PMD);
-}
-
-static int xen_pgd_alloc(struct mm_struct *mm)
-{
-	pgd_t *pgd = mm->pgd;
-	int ret = 0;
-
-	BUG_ON(PagePinned(virt_to_page(pgd)));
-
-#ifdef CONFIG_X86_64
-	{
-		struct page *page = virt_to_page(pgd);
-		pgd_t *user_pgd;
-
-		BUG_ON(page->private != 0);
-
-		ret = -ENOMEM;
-
-		user_pgd = (pgd_t *)__get_free_page(GFP_KERNEL | __GFP_ZERO);
-		page->private = (unsigned long)user_pgd;
-
-		if (user_pgd != NULL) {
-			user_pgd[pgd_index(VSYSCALL_START)] =
-				__pgd(__pa(level3_user_vsyscall) | _PAGE_TABLE);
-			ret = 0;
-		}
-
-		BUG_ON(PagePinned(virt_to_page(xen_get_user_pgd(pgd))));
-	}
-#endif
-
-	return ret;
-}
-
-static void xen_pgd_free(struct mm_struct *mm, pgd_t *pgd)
-{
-#ifdef CONFIG_X86_64
-	pgd_t *user_pgd = xen_get_user_pgd(pgd);
-
-	if (user_pgd)
-		free_page((unsigned long)user_pgd);
-#endif
-}
-
-/* This should never happen until we're OK to use struct page */
-static void xen_release_ptpage(unsigned long pfn, unsigned level)
-{
-	struct page *page = pfn_to_page(pfn);
-
-	if (PagePinned(page)) {
-		if (!PageHighMem(page)) {
-			if (level == PT_PTE && USE_SPLIT_PTLOCKS)
-				pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);
-			make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
-		}
-		ClearPagePinned(page);
-	}
-}
-
-static void xen_release_pte(unsigned long pfn)
-{
-	xen_release_ptpage(pfn, PT_PTE);
-}
-
-static void xen_release_pmd(unsigned long pfn)
-{
-	xen_release_ptpage(pfn, PT_PMD);
-}
-
-#if PAGETABLE_LEVELS == 4
-static void xen_alloc_pud(struct mm_struct *mm, unsigned long pfn)
-{
-	xen_alloc_ptpage(mm, pfn, PT_PUD);
-}
-
-static void xen_release_pud(unsigned long pfn)
-{
-	xen_release_ptpage(pfn, PT_PUD);
-}
-#endif
-
-#ifdef CONFIG_HIGHPTE
-static void *xen_kmap_atomic_pte(struct page *page, enum km_type type)
-{
-	pgprot_t prot = PAGE_KERNEL;
-
-	if (PagePinned(page))
-		prot = PAGE_KERNEL_RO;
-
-	if (0 && PageHighMem(page))
-		printk("mapping highpte %lx type %d prot %s\n",
-		       page_to_pfn(page), type,
-		       (unsigned long)pgprot_val(prot) & _PAGE_RW ? "WRITE" : "READ");
-
-	return kmap_atomic_prot(page, type, prot);
-}
-#endif
-
-#ifdef CONFIG_X86_32
-static __init pte_t mask_rw_pte(pte_t *ptep, pte_t pte)
-{
-	/* If there's an existing pte, then don't allow _PAGE_RW to be set */
-	if (pte_val_ma(*ptep) & _PAGE_PRESENT)
-		pte = __pte_ma(((pte_val_ma(*ptep) & _PAGE_RW) | ~_PAGE_RW) &
-			       pte_val_ma(pte));
-
-	return pte;
-}
-
-/* Init-time set_pte while constructing initial pagetables, which
-   doesn't allow RO pagetable pages to be remapped RW */
-static __init void xen_set_pte_init(pte_t *ptep, pte_t pte)
-{
-	pte = mask_rw_pte(ptep, pte);
-
-	xen_set_pte(ptep, pte);
-}
-#endif
-
-static __init void xen_pagetable_setup_start(pgd_t *base)
-{
-}
-
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
@@ -1021,37 +663,6 @@ void xen_setup_shared_info(void)
 	xen_setup_mfn_list_list();
 }
 
-static __init void xen_pagetable_setup_done(pgd_t *base)
-{
-	xen_setup_shared_info();
-}
-
-static __init void xen_post_allocator_init(void)
-{
-	pv_mmu_ops.set_pte = xen_set_pte;
-	pv_mmu_ops.set_pmd = xen_set_pmd;
-	pv_mmu_ops.set_pud = xen_set_pud;
-#if PAGETABLE_LEVELS == 4
-	pv_mmu_ops.set_pgd = xen_set_pgd;
-#endif
-
-	/* This will work as long as patching hasn't happened yet
-	   (which it hasn't) */
-	pv_mmu_ops.alloc_pte = xen_alloc_pte;
-	pv_mmu_ops.alloc_pmd = xen_alloc_pmd;
-	pv_mmu_ops.release_pte = xen_release_pte;
-	pv_mmu_ops.release_pmd = xen_release_pmd;
-#if PAGETABLE_LEVELS == 4
-	pv_mmu_ops.alloc_pud = xen_alloc_pud;
-	pv_mmu_ops.release_pud = xen_release_pud;
-#endif
-
-#ifdef CONFIG_X86_64
-	SetPagePinned(virt_to_page(level3_user_vsyscall));
-#endif
-	xen_mark_init_mm_pinned();
-}
-
 /* This is called once we have the cpu_possible_map */
 void xen_setup_vcpu_info_placement(void)
 {
@@ -1126,49 +737,6 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 	return ret;
 }
 
-static void xen_set_fixmap(unsigned idx, unsigned long phys, pgprot_t prot)
-{
-	pte_t pte;
-
-	phys >>= PAGE_SHIFT;
-
-	switch (idx) {
-	case FIX_BTMAP_END ... FIX_BTMAP_BEGIN:
-#ifdef CONFIG_X86_F00F_BUG
-	case FIX_F00F_IDT:
-#endif
-#ifdef CONFIG_X86_32
-	case FIX_WP_TEST:
-	case FIX_VDSO:
-# ifdef CONFIG_HIGHMEM
-	case FIX_KMAP_BEGIN ... FIX_KMAP_END:
-# endif
-#else
-	case VSYSCALL_LAST_PAGE ... VSYSCALL_FIRST_PAGE:
-#endif
-#ifdef CONFIG_X86_LOCAL_APIC
-	case FIX_APIC_BASE:	/* maps dummy local APIC */
-#endif
-		pte = pfn_pte(phys, prot);
-		break;
-
-	default:
-		pte = mfn_pte(phys, prot);
-		break;
-	}
-
-	__native_set_fixmap(idx, pte);
-
-#ifdef CONFIG_X86_64
-	/* Replicate changes to map the vsyscall page into the user
-	   pagetable vsyscall mapping. */
-	if (idx >= VSYSCALL_LAST_PAGE && idx <= VSYSCALL_FIRST_PAGE) {
-		unsigned long vaddr = __fix_to_virt(idx);
-		set_pte_vaddr_pud(level3_user_vsyscall, vaddr, pte);
-	}
-#endif
-}
-
 static const struct pv_info xen_info __initdata = {
 	.paravirt_enabled = 1,
 	.shared_kernel_pmd = 0,
@@ -1264,86 +832,6 @@ static const struct pv_apic_ops xen_apic_ops __initdata = {
 #endif
 };
 
-static const struct pv_mmu_ops xen_mmu_ops __initdata = {
-	.pagetable_setup_start = xen_pagetable_setup_start,
-	.pagetable_setup_done = xen_pagetable_setup_done,
-
-	.read_cr2 = xen_read_cr2,
-	.write_cr2 = xen_write_cr2,
-
-	.read_cr3 = xen_read_cr3,
-	.write_cr3 = xen_write_cr3,
-
-	.flush_tlb_user = xen_flush_tlb,
-	.flush_tlb_kernel = xen_flush_tlb,
-	.flush_tlb_single = xen_flush_tlb_single,
-	.flush_tlb_others = xen_flush_tlb_others,
-
-	.pte_update = paravirt_nop,
-	.pte_update_defer = paravirt_nop,
-
-	.pgd_alloc = xen_pgd_alloc,
-	.pgd_free = xen_pgd_free,
-
-	.alloc_pte = xen_alloc_pte_init,
-	.release_pte = xen_release_pte_init,
-	.alloc_pmd = xen_alloc_pte_init,
-	.alloc_pmd_clone = paravirt_nop,
-	.release_pmd = xen_release_pte_init,
-
-#ifdef CONFIG_HIGHPTE
-	.kmap_atomic_pte = xen_kmap_atomic_pte,
-#endif
-
-#ifdef CONFIG_X86_64
-	.set_pte = xen_set_pte,
-#else
-	.set_pte = xen_set_pte_init,
-#endif
-	.set_pte_at = xen_set_pte_at,
-	.set_pmd = xen_set_pmd_hyper,
-
-	.ptep_modify_prot_start = __ptep_modify_prot_start,
-	.ptep_modify_prot_commit = __ptep_modify_prot_commit,
-
-	.pte_val = xen_pte_val,
-	.pgd_val = xen_pgd_val,
-
-	.make_pte = xen_make_pte,
-	.make_pgd = xen_make_pgd,
-
-#ifdef CONFIG_X86_PAE
-	.set_pte_atomic = xen_set_pte_atomic,
-	.set_pte_present = xen_set_pte_at,
-	.pte_clear = xen_pte_clear,
-	.pmd_clear = xen_pmd_clear,
-#endif	/* CONFIG_X86_PAE */
-	.set_pud = xen_set_pud_hyper,
-
-	.make_pmd = xen_make_pmd,
-	.pmd_val = xen_pmd_val,
-
-#if PAGETABLE_LEVELS == 4
-	.pud_val = xen_pud_val,
-	.make_pud = xen_make_pud,
-	.set_pgd = xen_set_pgd_hyper,
-
-	.alloc_pud = xen_alloc_pte_init,
-	.release_pud = xen_release_pte_init,
-#endif	/* PAGETABLE_LEVELS == 4 */
-
-	.activate_mm = xen_activate_mm,
-	.dup_mmap = xen_dup_mmap,
-	.exit_mmap = xen_exit_mmap,
-
-	.lazy_mode = {
-		.enter = paravirt_enter_lazy_mmu,
-		.leave = xen_leave_lazy,
-	},
-
-	.set_fixmap = xen_set_fixmap,
-};
-
 static void xen_reboot(int reason)
 {
 	struct sched_shutdown r = { .reason = reason };
@@ -1386,223 +874,6 @@ static const struct machine_ops __initdata xen_machine_ops = {
 };
 
 
-static void __init xen_reserve_top(void)
-{
-#ifdef CONFIG_X86_32
-	unsigned long top = HYPERVISOR_VIRT_START;
-	struct xen_platform_parameters pp;
-
-	if (HYPERVISOR_xen_version(XENVER_platform_parameters, &pp) == 0)
-		top = pp.virt_start;
-
-	reserve_top_address(-top);
-#endif	/* CONFIG_X86_32 */
-}
-
-/*
- * Like __va(), but returns address in the kernel mapping (which is
- * all we have until the physical memory mapping has been set up.
- */
-static void *__ka(phys_addr_t paddr)
-{
-#ifdef CONFIG_X86_64
-	return (void *)(paddr + __START_KERNEL_map);
-#else
-	return __va(paddr);
-#endif
-}
-
-/* Convert a machine address to physical address */
-static unsigned long m2p(phys_addr_t maddr)
-{
-	phys_addr_t paddr;
-
-	maddr &= PTE_PFN_MASK;
-	paddr = mfn_to_pfn(maddr >> PAGE_SHIFT) << PAGE_SHIFT;
-
-	return paddr;
-}
-
-/* Convert a machine address to kernel virtual */
-static void *m2v(phys_addr_t maddr)
-{
-	return __ka(m2p(maddr));
-}
-
-static void set_page_prot(void *addr, pgprot_t prot)
-{
-	unsigned long pfn = __pa(addr) >> PAGE_SHIFT;
-	pte_t pte = pfn_pte(pfn, prot);
-
-	if (HYPERVISOR_update_va_mapping((unsigned long)addr, pte, 0))
-		BUG();
-}
-
-static __init void xen_map_identity_early(pmd_t *pmd, unsigned long max_pfn)
-{
-	unsigned pmdidx, pteidx;
-	unsigned ident_pte;
-	unsigned long pfn;
-
-	ident_pte = 0;
-	pfn = 0;
-	for (pmdidx = 0; pmdidx < PTRS_PER_PMD && pfn < max_pfn; pmdidx++) {
-		pte_t *pte_page;
-
-		/* Reuse or allocate a page of ptes */
-		if (pmd_present(pmd[pmdidx]))
-			pte_page = m2v(pmd[pmdidx].pmd);
-		else {
-			/* Check for free pte pages */
-			if (ident_pte == ARRAY_SIZE(level1_ident_pgt))
-				break;
-
-			pte_page = &level1_ident_pgt[ident_pte];
-			ident_pte += PTRS_PER_PTE;
-
-			pmd[pmdidx] = __pmd(__pa(pte_page) | _PAGE_TABLE);
-		}
-
-		/* Install mappings */
-		for (pteidx = 0; pteidx < PTRS_PER_PTE; pteidx++, pfn++) {
-			pte_t pte;
-
-			if (pfn > max_pfn_mapped)
-				max_pfn_mapped = pfn;
-
-			if (!pte_none(pte_page[pteidx]))
-				continue;
-
-			pte = pfn_pte(pfn, PAGE_KERNEL_EXEC);
-			pte_page[pteidx] = pte;
-		}
-	}
-
-	for (pteidx = 0; pteidx < ident_pte; pteidx += PTRS_PER_PTE)
-		set_page_prot(&level1_ident_pgt[pteidx], PAGE_KERNEL_RO);
-
-	set_page_prot(pmd, PAGE_KERNEL_RO);
-}
-
-#ifdef CONFIG_X86_64
-static void convert_pfn_mfn(void *v)
-{
-	pte_t *pte = v;
-	int i;
-
-	/* All levels are converted the same way, so just treat them
-	   as ptes. */
-	for (i = 0; i < PTRS_PER_PTE; i++)
-		pte[i] = xen_make_pte(pte[i].pte);
-}
-
-/*
- * Set up the inital kernel pagetable.
- *
- * We can construct this by grafting the Xen provided pagetable into
- * head_64.S's preconstructed pagetables.  We copy the Xen L2's into
- * level2_ident_pgt, level2_kernel_pgt and level2_fixmap_pgt.  This
- * means that only the kernel has a physical mapping to start with -
- * but that's enough to get __va working.  We need to fill in the rest
- * of the physical mapping once some sort of allocator has been set
- * up.
- */
-static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd,
-						unsigned long max_pfn)
-{
-	pud_t *l3;
-	pmd_t *l2;
-
-	/* Zap identity mapping */
-	init_level4_pgt[0] = __pgd(0);
-
-	/* Pre-constructed entries are in pfn, so convert to mfn */
-	convert_pfn_mfn(init_level4_pgt);
-	convert_pfn_mfn(level3_ident_pgt);
-	convert_pfn_mfn(level3_kernel_pgt);
-
-	l3 = m2v(pgd[pgd_index(__START_KERNEL_map)].pgd);
-	l2 = m2v(l3[pud_index(__START_KERNEL_map)].pud);
-
-	memcpy(level2_ident_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
-	memcpy(level2_kernel_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
-
-	l3 = m2v(pgd[pgd_index(__START_KERNEL_map + PMD_SIZE)].pgd);
-	l2 = m2v(l3[pud_index(__START_KERNEL_map + PMD_SIZE)].pud);
-	memcpy(level2_fixmap_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
-
-	/* Set up identity map */
-	xen_map_identity_early(level2_ident_pgt, max_pfn);
-
-	/* Make pagetable pieces RO */
-	set_page_prot(init_level4_pgt, PAGE_KERNEL_RO);
-	set_page_prot(level3_ident_pgt, PAGE_KERNEL_RO);
-	set_page_prot(level3_kernel_pgt, PAGE_KERNEL_RO);
-	set_page_prot(level3_user_vsyscall, PAGE_KERNEL_RO);
-	set_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);
-	set_page_prot(level2_fixmap_pgt, PAGE_KERNEL_RO);
-
-	/* Pin down new L4 */
-	pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE,
-			  PFN_DOWN(__pa_symbol(init_level4_pgt)));
-
-	/* Unpin Xen-provided one */
-	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));
-
-	/* Switch over */
-	pgd = init_level4_pgt;
-
-	/*
-	 * At this stage there can be no user pgd, and no page
-	 * structure to attach it to, so make sure we just set kernel
-	 * pgd.
-	 */
-	xen_mc_batch();
-	__xen_write_cr3(true, __pa(pgd));
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
-
-	reserve_early(__pa(xen_start_info->pt_base),
-		      __pa(xen_start_info->pt_base +
-			   xen_start_info->nr_pt_frames * PAGE_SIZE),
-		      "XEN PAGETABLES");
-
-	return pgd;
-}
-#else	/* !CONFIG_X86_64 */
-static pmd_t level2_kernel_pgt[PTRS_PER_PMD] __page_aligned_bss;
-
-static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd,
-						unsigned long max_pfn)
-{
-	pmd_t *kernel_pmd;
-
-	init_pg_tables_start = __pa(pgd);
-	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
-	max_pfn_mapped = PFN_DOWN(init_pg_tables_end + 512*1024);
-
-	kernel_pmd = m2v(pgd[KERNEL_PGD_BOUNDARY].pgd);
-	memcpy(level2_kernel_pgt, kernel_pmd, sizeof(pmd_t) * PTRS_PER_PMD);
-
-	xen_map_identity_early(level2_kernel_pgt, max_pfn);
-
-	memcpy(swapper_pg_dir, pgd, sizeof(pgd_t) * PTRS_PER_PGD);
-	set_pgd(&swapper_pg_dir[KERNEL_PGD_BOUNDARY],
-			__pgd(__pa(level2_kernel_pgt) | _PAGE_PRESENT));
-
-	set_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);
-	set_page_prot(swapper_pg_dir, PAGE_KERNEL_RO);
-	set_page_prot(empty_zero_page, PAGE_KERNEL_RO);
-
-	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));
-
-	xen_write_cr3(__pa(swapper_pg_dir));
-
-	pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(__pa(swapper_pg_dir)));
-
-	return swapper_pg_dir;
-}
-#endif	/* CONFIG_X86_64 */
-
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {

commit 9b7ed8faa034fc2d350e2eff5c68680eb5c43a07
Merge: 6522869c3466 c43e0e46adf7
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Jan 30 14:50:57 2009 -0800

    Merge branch 'core/percpu' into x86/paravirt

commit ab897d2013128f470240a541b31cf5e636984e71
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Jan 22 14:24:16 2009 -0800

    x86/pvops: remove pte_flags pvop
    
    pte_flags() was introduced as a new pvop in order to extract just the
    flags portion of a pte, which is a potentially cheaper operation than
    extracting the page number as well.  It turns out this operation is
    not needed, because simply using a mask to extract the flags from a
    pte is sufficient for all current users.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bea215230b20..6f1bb71aa13a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1314,7 +1314,6 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.ptep_modify_prot_commit = __ptep_modify_prot_commit,
 
 	.pte_val = xen_pte_val,
-	.pte_flags = native_pte_flags,
 	.pgd_val = xen_pgd_val,
 
 	.make_pte = xen_make_pte,

commit 8ce031972b40da58c268caba8c5ea3c0856d7131
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 12:21:27 2009 +0900

    x86: remove pda_init()
    
    Impact: cleanup
    
    Copy the code to cpu_init() to satisfy the requirement that the cpu
    be reinitialized.  Remove all other calls, since the segments are
    already initialized in head_64.S.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 75b94139e1f2..bef941f61451 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1645,7 +1645,6 @@ asmlinkage void __init xen_start_kernel(void)
 #ifdef CONFIG_X86_64
 	/* Disable until direct per-cpu data access. */
 	have_vcpu_info_placement = 0;
-	pda_init(0);
 #endif
 
 	xen_smp_init();

commit 6dbde3530850d4d8bfc1b6bd4006d92786a2787f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jan 15 22:15:53 2009 +0900

    percpu: add optimized generic percpu accessors
    
    It is an optimization and a cleanup, and adds the following new
    generic percpu methods:
    
      percpu_read()
      percpu_write()
      percpu_add()
      percpu_sub()
      percpu_and()
      percpu_or()
      percpu_xor()
    
    and implements support for them on x86. (other architectures will fall
    back to a default implementation)
    
    The advantage is that for example to read a local percpu variable,
    instead of this sequence:
    
     return __get_cpu_var(var);
    
     ffffffff8102ca2b:      48 8b 14 fd 80 09 74    mov    -0x7e8bf680(,%rdi,8),%rdx
     ffffffff8102ca32:      81
     ffffffff8102ca33:      48 c7 c0 d8 59 00 00    mov    $0x59d8,%rax
     ffffffff8102ca3a:      48 8b 04 10             mov    (%rax,%rdx,1),%rax
    
    We can get a single instruction by using the optimized variants:
    
     return percpu_read(var);
    
     ffffffff8102ca3f:      65 48 8b 05 91 8f fd    mov    %gs:0x7efd8f91(%rip),%rax
    
    I also cleaned up the x86-specific APIs and made the x86 code use
    these new generic percpu primitives.
    
    tj: * fixed generic percpu_sub() definition as Roel Kluin pointed out
        * added percpu_and() for completeness's sake
        * made generic percpu ops atomic against preemption
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 312414ef9365..75b94139e1f2 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -695,17 +695,17 @@ static void xen_write_cr0(unsigned long cr0)
 
 static void xen_write_cr2(unsigned long cr2)
 {
-	x86_read_percpu(xen_vcpu)->arch.cr2 = cr2;
+	percpu_read(xen_vcpu)->arch.cr2 = cr2;
 }
 
 static unsigned long xen_read_cr2(void)
 {
-	return x86_read_percpu(xen_vcpu)->arch.cr2;
+	return percpu_read(xen_vcpu)->arch.cr2;
 }
 
 static unsigned long xen_read_cr2_direct(void)
 {
-	return x86_read_percpu(xen_vcpu_info.arch.cr2);
+	return percpu_read(xen_vcpu_info.arch.cr2);
 }
 
 static void xen_write_cr4(unsigned long cr4)
@@ -718,12 +718,12 @@ static void xen_write_cr4(unsigned long cr4)
 
 static unsigned long xen_read_cr3(void)
 {
-	return x86_read_percpu(xen_cr3);
+	return percpu_read(xen_cr3);
 }
 
 static void set_current_cr3(void *v)
 {
-	x86_write_percpu(xen_current_cr3, (unsigned long)v);
+	percpu_write(xen_current_cr3, (unsigned long)v);
 }
 
 static void __xen_write_cr3(bool kernel, unsigned long cr3)
@@ -748,7 +748,7 @@ static void __xen_write_cr3(bool kernel, unsigned long cr3)
 	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
 
 	if (kernel) {
-		x86_write_percpu(xen_cr3, cr3);
+		percpu_write(xen_cr3, cr3);
 
 		/* Update xen_current_cr3 once the batch has actually
 		   been submitted. */
@@ -764,7 +764,7 @@ static void xen_write_cr3(unsigned long cr3)
 
 	/* Update while interrupts are disabled, so its atomic with
 	   respect to ipis */
-	x86_write_percpu(xen_cr3, cr3);
+	percpu_write(xen_cr3, cr3);
 
 	__xen_write_cr3(true, cr3);
 

commit 004aa322f855a765741d9437a98dd8fe2e4f32a6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: misc clean up after the percpu update
    
    Do the following cleanups:
    
    * kill x86_64_init_pda() which now is equivalent to pda_init()
    
    * use per_cpu_offset() instead of cpu_pda() when initializing
      initial_gs
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 965539ec425f..312414ef9365 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1645,7 +1645,7 @@ asmlinkage void __init xen_start_kernel(void)
 #ifdef CONFIG_X86_64
 	/* Disable until direct per-cpu data access. */
 	have_vcpu_info_placement = 0;
-	x86_64_init_pda();
+	pda_init(0);
 #endif
 
 	xen_smp_init();

commit 4595f9620cda8a1e973588e743cf5f8436dd20c6
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Jan 10 21:58:09 2009 -0800

    x86: change flush_tlb_others to take a const struct cpumask
    
    Impact: reduce stack usage, use new cpumask API.
    
    This is made a little more tricky by uv_flush_tlb_others which
    actually alters its argument, for an IPI to be sent to the remaining
    cpus in the mask.
    
    I solve this by allocating a cpumask_var_t for this case and falling back
    to IPI should this fail.
    
    To eliminate temporaries in the caller, all flush_tlb_others implementations
    now do the this-cpu-elimination step themselves.
    
    Note also the curious "cpus_or(f->flush_cpumask, cpumask, f->flush_cpumask)"
    which has been there since pre-git and yet f->flush_cpumask is always zero
    at this point.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bea215230b20..965539ec425f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -634,35 +634,27 @@ static void xen_flush_tlb_single(unsigned long addr)
 	preempt_enable();
 }
 
-static void xen_flush_tlb_others(const cpumask_t *cpus, struct mm_struct *mm,
-				 unsigned long va)
+static void xen_flush_tlb_others(const struct cpumask *cpus,
+				 struct mm_struct *mm, unsigned long va)
 {
 	struct {
 		struct mmuext_op op;
-		cpumask_t mask;
+		DECLARE_BITMAP(mask, NR_CPUS);
 	} *args;
-	cpumask_t cpumask = *cpus;
 	struct multicall_space mcs;
 
-	/*
-	 * A couple of (to be removed) sanity checks:
-	 *
-	 * - current CPU must not be in mask
-	 * - mask must exist :)
-	 */
-	BUG_ON(cpus_empty(cpumask));
-	BUG_ON(cpu_isset(smp_processor_id(), cpumask));
+	BUG_ON(cpumask_empty(cpus));
 	BUG_ON(!mm);
 
-	/* If a CPU which we ran on has gone down, OK. */
-	cpus_and(cpumask, cpumask, cpu_online_map);
-	if (cpus_empty(cpumask))
-		return;
-
 	mcs = xen_mc_entry(sizeof(*args));
 	args = mcs.args;
-	args->mask = cpumask;
-	args->op.arg2.vcpumask = &args->mask;
+	args->op.arg2.vcpumask = to_cpumask(args->mask);
+
+	/* Remove us, and any offline CPUS. */
+	cpumask_and(to_cpumask(args->mask), cpus, cpu_online_mask);
+	cpumask_clear_cpu(smp_processor_id(), to_cpumask(args->mask));
+	if (unlikely(cpumask_empty(to_cpumask(args->mask))))
+		goto issue;
 
 	if (va == TLB_FLUSH_ALL) {
 		args->op.cmd = MMUEXT_TLB_FLUSH_MULTI;
@@ -673,6 +665,7 @@ static void xen_flush_tlb_others(const cpumask_t *cpus, struct mm_struct *mm,
 
 	MULTI_mmuext_op(mcs.mc, &args->op, 1, NULL, DOMID_SELF);
 
+issue:
 	xen_mc_issue(PARAVIRT_LAZY_MMU);
 }
 

commit ecbf29cdb3990c83d90d0c4187c89fb2ce423367
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Dec 16 12:37:07 2008 -0800

    xen: clean up asm/xen/hypervisor.h
    
    Impact: cleanup
    
    hypervisor.h had accumulated a lot of crud, including lots of spurious
    #includes.  Clean it all up, and go around fixing up everything else
    accordingly.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 86cd2f829683..bea215230b20 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -28,6 +28,7 @@
 #include <linux/console.h>
 
 #include <xen/interface/xen.h>
+#include <xen/interface/version.h>
 #include <xen/interface/physdev.h>
 #include <xen/interface/vcpu.h>
 #include <xen/features.h>

commit f63c2f248959366cd11bfa476f866737047cf663
Author: Tej <bewith.tej@gmail.com>
Date:   Tue Dec 16 11:56:06 2008 -0800

    xen: whitespace/checkpatch cleanup
    
    Impact: cleanup
    
    Signed-off-by: Tej <bewith.tej@gmail.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5e4686d70f62..86cd2f829683 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -793,7 +793,7 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 
 	ret = 0;
 
-	switch(msr) {
+	switch (msr) {
 #ifdef CONFIG_X86_64
 		unsigned which;
 		u64 base;
@@ -1453,7 +1453,7 @@ static __init void xen_map_identity_early(pmd_t *pmd, unsigned long max_pfn)
 
 	ident_pte = 0;
 	pfn = 0;
-	for(pmdidx = 0; pmdidx < PTRS_PER_PMD && pfn < max_pfn; pmdidx++) {
+	for (pmdidx = 0; pmdidx < PTRS_PER_PMD && pfn < max_pfn; pmdidx++) {
 		pte_t *pte_page;
 
 		/* Reuse or allocate a page of ptes */
@@ -1471,7 +1471,7 @@ static __init void xen_map_identity_early(pmd_t *pmd, unsigned long max_pfn)
 		}
 
 		/* Install mappings */
-		for(pteidx = 0; pteidx < PTRS_PER_PTE; pteidx++, pfn++) {
+		for (pteidx = 0; pteidx < PTRS_PER_PTE; pteidx++, pfn++) {
 			pte_t pte;
 
 			if (pfn > max_pfn_mapped)
@@ -1485,7 +1485,7 @@ static __init void xen_map_identity_early(pmd_t *pmd, unsigned long max_pfn)
 		}
 	}
 
-	for(pteidx = 0; pteidx < ident_pte; pteidx += PTRS_PER_PTE)
+	for (pteidx = 0; pteidx < ident_pte; pteidx += PTRS_PER_PTE)
 		set_page_prot(&level1_ident_pgt[pteidx], PAGE_KERNEL_RO);
 
 	set_page_prot(pmd, PAGE_KERNEL_RO);
@@ -1499,7 +1499,7 @@ static void convert_pfn_mfn(void *v)
 
 	/* All levels are converted the same way, so just treat them
 	   as ptes. */
-	for(i = 0; i < PTRS_PER_PTE; i++)
+	for (i = 0; i < PTRS_PER_PTE; i++)
 		pte[i] = xen_make_pte(pte[i].pte);
 }
 
@@ -1514,7 +1514,8 @@ static void convert_pfn_mfn(void *v)
  * of the physical mapping once some sort of allocator has been set
  * up.
  */
-static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)
+static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd,
+						unsigned long max_pfn)
 {
 	pud_t *l3;
 	pmd_t *l2;
@@ -1577,7 +1578,8 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pf
 #else	/* !CONFIG_X86_64 */
 static pmd_t level2_kernel_pgt[PTRS_PER_PMD] __page_aligned_bss;
 
-static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)
+static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd,
+						unsigned long max_pfn)
 {
 	pmd_t *kernel_pmd;
 

commit d05fdf316067cd311d5e7add08da26ded8a58080
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Oct 28 19:23:06 2008 +1100

    xen: make sure stray alias mappings are gone before pinning
    
    Xen requires that all mappings of pagetable pages are read-only, so
    that they can't be updated illegally.  As a result, if a page is being
    turned into a pagetable page, we need to make sure all its mappings
    are RO.
    
    If the page had been used for ioremap or vmalloc, it may still have
    left over mappings as a result of not having been lazily unmapped.
    This change makes sure we explicitly mop them all up before pinning
    the page.
    
    Unlike aliases created by kmap, the there can be vmalloc aliases even
    for non-high pages, so we must do the flush unconditionally.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Linux Memory Management List <linux-mm@kvack.org>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b61534c7a4c4..5e4686d70f62 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -863,15 +863,16 @@ static void xen_alloc_ptpage(struct mm_struct *mm, unsigned long pfn, unsigned l
 	if (PagePinned(virt_to_page(mm->pgd))) {
 		SetPagePinned(page);
 
+		vm_unmap_aliases();
 		if (!PageHighMem(page)) {
 			make_lowmem_page_readonly(__va(PFN_PHYS((unsigned long)pfn)));
 			if (level == PT_PTE && USE_SPLIT_PTLOCKS)
 				pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
-		} else
+		} else {
 			/* make sure there are no stray mappings of
 			   this page */
 			kmap_flush_unused();
-			vm_unmap_aliases();
+		}
 	}
 }
 

commit db64fe02258f1507e13fe5212a989922323685ce
Author: Nick Piggin <npiggin@suse.de>
Date:   Sat Oct 18 20:27:03 2008 -0700

    mm: rewrite vmap layer
    
    Rewrite the vmap allocator to use rbtrees and lazy tlb flushing, and
    provide a fast, scalable percpu frontend for small vmaps (requires a
    slightly different API, though).
    
    The biggest problem with vmap is actually vunmap.  Presently this requires
    a global kernel TLB flush, which on most architectures is a broadcast IPI
    to all CPUs to flush the cache.  This is all done under a global lock.  As
    the number of CPUs increases, so will the number of vunmaps a scaled
    workload will want to perform, and so will the cost of a global TLB flush.
     This gives terrible quadratic scalability characteristics.
    
    Another problem is that the entire vmap subsystem works under a single
    lock.  It is a rwlock, but it is actually taken for write in all the fast
    paths, and the read locking would likely never be run concurrently anyway,
    so it's just pointless.
    
    This is a rewrite of vmap subsystem to solve those problems.  The existing
    vmalloc API is implemented on top of the rewritten subsystem.
    
    The TLB flushing problem is solved by using lazy TLB unmapping.  vmap
    addresses do not have to be flushed immediately when they are vunmapped,
    because the kernel will not reuse them again (would be a use-after-free)
    until they are reallocated.  So the addresses aren't allocated again until
    a subsequent TLB flush.  A single TLB flush then can flush multiple
    vunmaps from each CPU.
    
    XEN and PAT and such do not like deferred TLB flushing because they can't
    always handle multiple aliasing virtual addresses to a physical address.
    They now call vm_unmap_aliases() in order to flush any deferred mappings.
    That call is very expensive (well, actually not a lot more expensive than
    a single vunmap under the old scheme), however it should be OK if not
    called too often.
    
    The virtual memory extent information is stored in an rbtree rather than a
    linked list to improve the algorithmic scalability.
    
    There is a per-CPU allocator for small vmaps, which amortizes or avoids
    global locking.
    
    To use the per-CPU interface, the vm_map_ram / vm_unmap_ram interfaces
    must be used in place of vmap and vunmap.  Vmalloc does not use these
    interfaces at the moment, so it will not be quite so scalable (although it
    will use lazy TLB flushing).
    
    As a quick test of performance, I ran a test that loops in the kernel,
    linearly mapping then touching then unmapping 4 pages.  Different numbers
    of tests were run in parallel on an 4 core, 2 socket opteron.  Results are
    in nanoseconds per map+touch+unmap.
    
    threads           vanilla         vmap rewrite
    1                 14700           2900
    2                 33600           3000
    4                 49500           2800
    8                 70631           2900
    
    So with a 8 cores, the rewritten version is already 25x faster.
    
    In a slightly more realistic test (although with an older and less
    scalable version of the patch), I ripped the not-very-good vunmap batching
    code out of XFS, and implemented the large buffer mapping with vm_map_ram
    and vm_unmap_ram...  along with a couple of other tricks, I was able to
    speed up a large directory workload by 20x on a 64 CPU system.  I believe
    vmap/vunmap is actually sped up a lot more than 20x on such a system, but
    I'm running into other locks now.  vmap is pretty well blown off the
    profiles.
    
    Before:
    1352059 total                                      0.1401
    798784 _write_lock                              8320.6667 <- vmlist_lock
    529313 default_idle                             1181.5022
     15242 smp_call_function                         15.8771  <- vmap tlb flushing
      2472 __get_vm_area_node                         1.9312  <- vmap
      1762 remove_vm_area                             4.5885  <- vunmap
       316 map_vm_area                                0.2297  <- vmap
       312 kfree                                      0.1950
       300 _spin_lock                                 3.1250
       252 sn_send_IPI_phys                           0.4375  <- tlb flushing
       238 vmap                                       0.8264  <- vmap
       216 find_lock_page                             0.5192
       196 find_next_bit                              0.3603
       136 sn2_send_IPI                               0.2024
       130 pio_phys_write_mmr                         2.0312
       118 unmap_kernel_range                         0.1229
    
    After:
     78406 total                                      0.0081
     40053 default_idle                              89.4040
     33576 ia64_spinlock_contention                 349.7500
      1650 _spin_lock                                17.1875
       319 __reg_op                                   0.5538
       281 _atomic_dec_and_lock                       1.0977
       153 mutex_unlock                               1.5938
       123 iget_locked                                0.1671
       117 xfs_dir_lookup                             0.1662
       117 dput                                       0.1406
       114 xfs_iget_core                              0.0268
        92 xfs_da_hashname                            0.1917
        75 d_alloc                                    0.0670
        68 vmap_page_range                            0.0462 <- vmap
        58 kmem_cache_alloc                           0.0604
        57 memset                                     0.0540
        52 rb_next                                    0.1625
        50 __copy_user                                0.0208
        49 bitmap_find_free_region                    0.2188 <- vmap
        46 ia64_sn_udelay                             0.1106
        45 find_inode_fast                            0.1406
        42 memcmp                                     0.2188
        42 finish_task_switch                         0.1094
        42 __d_lookup                                 0.0410
        40 radix_tree_lookup_slot                     0.1250
        37 _spin_unlock_irqrestore                    0.3854
        36 xfs_bmapi                                  0.0050
        36 kmem_cache_free                            0.0256
        35 xfs_vn_getattr                             0.0322
        34 radix_tree_lookup                          0.1062
        33 __link_path_walk                           0.0035
        31 xfs_da_do_buf                              0.0091
        30 _xfs_buf_find                              0.0204
        28 find_get_page                              0.0875
        27 xfs_iread                                  0.0241
        27 __strncpy_from_user                        0.2812
        26 _xfs_buf_initialize                        0.0406
        24 _xfs_buf_lookup_pages                      0.0179
        24 vunmap_page_range                          0.0250 <- vunmap
        23 find_lock_page                             0.0799
        22 vm_map_ram                                 0.0087 <- vmap
        20 kfree                                      0.0125
        19 put_page                                   0.0330
        18 __kmalloc                                  0.0176
        17 xfs_da_node_lookup_int                     0.0086
        17 _read_lock                                 0.0885
        17 page_waitqueue                             0.0664
    
    vmap has gone from being the top 5 on the profiles and flushing the crap
    out of all TLBs, to using less than 1% of kernel time.
    
    [akpm@linux-foundation.org: cleanups, section fix]
    [akpm@linux-foundation.org: fix build on alpha]
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Krzysztof Helt <krzysztof.h1@poczta.fm>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0013a729b41d..b61534c7a4c4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -871,6 +871,7 @@ static void xen_alloc_ptpage(struct mm_struct *mm, unsigned long pfn, unsigned l
 			/* make sure there are no stray mappings of
 			   this page */
 			kmap_flush_unused();
+			vm_unmap_aliases();
 	}
 }
 

commit 365d46dc9be9b3c833990a06f3994b1987eda578
Merge: 5dc64a3442b9 fd0480883066
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 12 12:35:23 2008 +0200

    Merge branch 'linus' into x86/xen
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/process_64.c
            arch/x86/xen/enlighten.c

commit d84705969f898f294bc3fc32eca33580f14105bd
Merge: 725c25819e4a 11494547b175
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 10 19:50:00 2008 +0200

    Merge branch 'x86/apic' into x86-v28-for-linus-phase4-B
    
    Conflicts:
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/apic_64.c
            arch/x86/kernel/setup.c
            drivers/pci/intel-iommu.c
            include/asm-x86/cpufeature.h
            include/asm-x86/dma-mapping.h

commit 5dc64a3442b98eaa0e3730c35fcf00cf962a93e7
Author: Ian Campbell <Ian.Campbell@citrix.com>
Date:   Fri Oct 10 11:27:38 2008 +0100

    xen: do not reserve 2 pages of padding between hypervisor and fixmap.
    
    When reserving space for the hypervisor the Xen paravirt backend adds
    an extra two pages (this was carried forward from the 2.6.18-xen tree
    which had them "for safety"). Depending on various CONFIG options this
    can cause the boot time fixmaps to span multiple PMDs which is not
    supported and triggers a WARN in early_ioremap_init().
    
    This was exposed by 2216d199b1430d1c0affb1498a9ebdbd9c0de439 which
    moved the dmi table parsing earlier.
        x86: fix CONFIG_X86_RESERVE_LOW_64K=y
    
        The bad_bios_dmi_table() quirk never triggered because we do DMI setup
        too late. Move it a bit earlier.
    
    There is no real reason to reserve these two extra pages and the
    fixmap already incorporates FIX_HOLE which serves the same
    purpose. None of the other callers of reserve_top_address do this.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 85692c9f6496..977a54255fb4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1370,7 +1370,7 @@ static void __init xen_reserve_top(void)
 	if (HYPERVISOR_xen_version(XENVER_platform_parameters, &pp) == 0)
 		top = pp.virt_start;
 
-	reserve_top_address(-top + 2 * PAGE_SIZE);
+	reserve_top_address(-top);
 #endif	/* CONFIG_X86_32 */
 }
 

commit e496e3d645c93206faf61ff6005995ebd08cc39c
Merge: b159d7a989e5 5bbd4c372400 175e438f7a2d 516cbf3730c4 af2d237bf574 9b1568458a3e 5b7e41ff3726 1befdefcf476 a03352d2c1dc 7b22ff5344fd 2c7e9fd4c6cb 91030ca1e739 dd5523552c28 b3e15bdef689 20211e4d3447 efd327a2d412 c7ffa6c26277 e51a1ac2dfca 5df455155124 d99e90164e6c e621bd18958e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 6 18:17:07 2008 +0200

    Merge branches 'x86/alternatives', 'x86/cleanups', 'x86/commandline', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/doc', 'x86/exports', 'x86/fpu', 'x86/gart', 'x86/idle', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/oprofile', 'x86/paravirt', 'x86/reboot', 'x86/sparse-fixes', 'x86/tsc', 'x86/urgent' and 'x86/vmalloc' into x86-v28-for-linus-phase1

commit db053b86f4b1ec790da2dafe2acb93be76288bb9
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Oct 2 16:41:31 2008 -0700

    xen: clean up x86-64 warnings
    
    There are a couple of Xen features which rely on directly accessing
    per-cpu data via a segment register, which is not yet available on
    x86-64.  In the meantime, just disable direct access to the vcpu info
    structure; this leaves some of the code as dead, but it will come to
    life in time, and the warnings are suppressed.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8ca2f88bde1e..85692c9f6496 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -112,7 +112,14 @@ struct shared_info *HYPERVISOR_shared_info = (void *)&xen_dummy_shared_info;
  *
  * 0: not available, 1: available
  */
-static int have_vcpu_info_placement = 1;
+static int have_vcpu_info_placement =
+#ifdef CONFIG_X86_32
+	1
+#else
+	0
+#endif
+	;
+
 
 static void xen_vcpu_setup(int cpu)
 {
@@ -941,6 +948,7 @@ static void *xen_kmap_atomic_pte(struct page *page, enum km_type type)
 }
 #endif
 
+#ifdef CONFIG_X86_32
 static __init pte_t mask_rw_pte(pte_t *ptep, pte_t pte)
 {
 	/* If there's an existing pte, then don't allow _PAGE_RW to be set */
@@ -959,6 +967,7 @@ static __init void xen_set_pte_init(pte_t *ptep, pte_t pte)
 
 	xen_set_pte(ptep, pte);
 }
+#endif
 
 static __init void xen_pagetable_setup_start(pgd_t *base)
 {
@@ -1025,7 +1034,6 @@ void xen_setup_vcpu_info_placement(void)
 
 	/* xen_vcpu_setup managed to place the vcpu_info within the
 	   percpu area for all cpus, so make use of it */
-#ifdef CONFIG_X86_32
 	if (have_vcpu_info_placement) {
 		printk(KERN_INFO "Xen: using vcpu_info placement\n");
 
@@ -1035,7 +1043,6 @@ void xen_setup_vcpu_info_placement(void)
 		pv_irq_ops.irq_enable = xen_irq_enable_direct;
 		pv_mmu_ops.read_cr2 = xen_read_cr2_direct;
 	}
-#endif
 }
 
 static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
@@ -1056,12 +1063,10 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 	goto patch_site
 
 	switch (type) {
-#ifdef CONFIG_X86_32
 		SITE(pv_irq_ops, irq_enable);
 		SITE(pv_irq_ops, irq_disable);
 		SITE(pv_irq_ops, save_fl);
 		SITE(pv_irq_ops, restore_fl);
-#endif /* CONFIG_X86_32 */
 #undef SITE
 
 	patch_site:
@@ -1399,48 +1404,11 @@ static void *m2v(phys_addr_t maddr)
 	return __ka(m2p(maddr));
 }
 
-#ifdef CONFIG_X86_64
-static void walk(pgd_t *pgd, unsigned long addr)
-{
-	unsigned l4idx = pgd_index(addr);
-	unsigned l3idx = pud_index(addr);
-	unsigned l2idx = pmd_index(addr);
-	unsigned l1idx = pte_index(addr);
-	pgd_t l4;
-	pud_t l3;
-	pmd_t l2;
-	pte_t l1;
-
-	xen_raw_printk("walk %p, %lx -> %d %d %d %d\n",
-		       pgd, addr, l4idx, l3idx, l2idx, l1idx);
-
-	l4 = pgd[l4idx];
-	xen_raw_printk("  l4: %016lx\n", l4.pgd);
-	xen_raw_printk("      %016lx\n", pgd_val(l4));
-
-	l3 = ((pud_t *)(m2v(l4.pgd)))[l3idx];
-	xen_raw_printk("  l3: %016lx\n", l3.pud);
-	xen_raw_printk("      %016lx\n", pud_val(l3));
-
-	l2 = ((pmd_t *)(m2v(l3.pud)))[l2idx];
-	xen_raw_printk("  l2: %016lx\n", l2.pmd);
-	xen_raw_printk("      %016lx\n", pmd_val(l2));
-
-	l1 = ((pte_t *)(m2v(l2.pmd)))[l1idx];
-	xen_raw_printk("  l1: %016lx\n", l1.pte);
-	xen_raw_printk("      %016lx\n", pte_val(l1));
-}
-#endif
-
 static void set_page_prot(void *addr, pgprot_t prot)
 {
 	unsigned long pfn = __pa(addr) >> PAGE_SHIFT;
 	pte_t pte = pfn_pte(pfn, prot);
 
-	xen_raw_printk("addr=%p pfn=%lx mfn=%lx prot=%016llx pte=%016llx\n",
-		       addr, pfn, get_phys_to_machine(pfn),
-		       pgprot_val(prot), pte.pte);
-
 	if (HYPERVISOR_update_va_mapping((unsigned long)addr, pte, 0))
 		BUG();
 }
@@ -1698,15 +1666,6 @@ asmlinkage void __init xen_start_kernel(void)
 
 	xen_raw_console_write("about to get started...\n");
 
-#if 0
-	xen_raw_printk("&boot_params=%p __pa(&boot_params)=%lx __va(__pa(&boot_params))=%lx\n",
-		       &boot_params, __pa_symbol(&boot_params),
-		       __va(__pa_symbol(&boot_params)));
-
-	walk(pgd, &boot_params);
-	walk(pgd, __va(__pa(&boot_params)));
-#endif
-
 	/* Start the world */
 #ifdef CONFIG_X86_32
 	i386_start_kernel();

commit 6a9e91846bf52cc70a0417de19fdfac224c435c4
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Sep 9 15:43:25 2008 -0700

    xen: fix pinning when not using split pte locks
    
    We only pin PTE pages when using split PTE locks, so don't do the
    pin/unpin when attaching/detaching pte pages to a pinned pagetable.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b106e825d266..8ca2f88bde1e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -826,7 +826,7 @@ static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 
 		if (!PageHighMem(page)) {
 			make_lowmem_page_readonly(__va(PFN_PHYS((unsigned long)pfn)));
-			if (level == PT_PTE)
+			if (level == PT_PTE && USE_SPLIT_PTLOCKS)
 				pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
 		} else
 			/* make sure there are no stray mappings of
@@ -894,7 +894,7 @@ static void xen_release_ptpage(u32 pfn, unsigned level)
 
 	if (PagePinned(page)) {
 		if (!PageHighMem(page)) {
-			if (level == PT_PTE)
+			if (level == PT_PTE && USE_SPLIT_PTLOCKS)
 				pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);
 			make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
 		}

commit e4a6be4d2850da032a782b5296c07dfdf583af86
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Thu Jul 24 12:15:45 2008 -0300

    x86, xen: Use native_pte_flags instead of native_pte_val for .pte_flags
    
    Using native_pte_val triggers the BUG_ON() in the paravirt_ops
    version of pte_flags().
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9ff6e3cbf08f..a4e201b47f64 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1324,7 +1324,7 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.ptep_modify_prot_commit = __ptep_modify_prot_commit,
 
 	.pte_val = xen_pte_val,
-	.pte_flags = native_pte_val,
+	.pte_flags = native_pte_flags,
 	.pgd_val = xen_pgd_val,
 
 	.make_pte = xen_make_pte,

commit f86399396ce7a4f4069828b7dceac5aa5113dfb5
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Wed Jul 30 18:32:27 2008 -0300

    x86, paravirt_ops: use unsigned long instead of u32 for alloc_p*() pfn args
    
    This patch changes the pfn args from 'u32' to 'unsigned long'
    on alloc_p*() functions on paravirt_ops, and the corresponding
    implementations for Xen and VMI. The prototypes for CONFIG_PARAVIRT=n
    are already using unsigned long, so paravirt.h now matches the prototypes
    on asm-x86/pgalloc.h.
    
    It shouldn't result in any changes on generated code on 32-bit, with
    or without CONFIG_PARAVIRT. On both cases, 'codiff -f' didn't show any
    change after applying this patch.
    
    On 64-bit, there are (expected) binary changes only when CONFIG_PARAVIRT
    is enabled, as the patch is really supposed to change the size of the
    pfn args.
    
    [ v2: KVM_GUEST: use the right parameter type on kvm_release_pt() ]
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Acked-by: Zachary Amsden <zach@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9ff6e3cbf08f..db970bdc5e3d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -812,7 +812,7 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 
 /* Early in boot, while setting up the initial pagetable, assume
    everything is pinned. */
-static __init void xen_alloc_pte_init(struct mm_struct *mm, u32 pfn)
+static __init void xen_alloc_pte_init(struct mm_struct *mm, unsigned long pfn)
 {
 #ifdef CONFIG_FLATMEM
 	BUG_ON(mem_map);	/* should only be used early */
@@ -822,7 +822,7 @@ static __init void xen_alloc_pte_init(struct mm_struct *mm, u32 pfn)
 
 /* Early release_pte assumes that all pts are pinned, since there's
    only init_mm and anything attached to that is pinned. */
-static void xen_release_pte_init(u32 pfn)
+static void xen_release_pte_init(unsigned long pfn)
 {
 	make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
 }
@@ -838,7 +838,7 @@ static void pin_pagetable_pfn(unsigned cmd, unsigned long pfn)
 
 /* This needs to make sure the new pte page is pinned iff its being
    attached to a pinned pagetable. */
-static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
+static void xen_alloc_ptpage(struct mm_struct *mm, unsigned long pfn, unsigned level)
 {
 	struct page *page = pfn_to_page(pfn);
 
@@ -856,12 +856,12 @@ static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 	}
 }
 
-static void xen_alloc_pte(struct mm_struct *mm, u32 pfn)
+static void xen_alloc_pte(struct mm_struct *mm, unsigned long pfn)
 {
 	xen_alloc_ptpage(mm, pfn, PT_PTE);
 }
 
-static void xen_alloc_pmd(struct mm_struct *mm, u32 pfn)
+static void xen_alloc_pmd(struct mm_struct *mm, unsigned long pfn)
 {
 	xen_alloc_ptpage(mm, pfn, PT_PMD);
 }
@@ -909,7 +909,7 @@ static void xen_pgd_free(struct mm_struct *mm, pgd_t *pgd)
 }
 
 /* This should never happen until we're OK to use struct page */
-static void xen_release_ptpage(u32 pfn, unsigned level)
+static void xen_release_ptpage(unsigned long pfn, unsigned level)
 {
 	struct page *page = pfn_to_page(pfn);
 
@@ -923,23 +923,23 @@ static void xen_release_ptpage(u32 pfn, unsigned level)
 	}
 }
 
-static void xen_release_pte(u32 pfn)
+static void xen_release_pte(unsigned long pfn)
 {
 	xen_release_ptpage(pfn, PT_PTE);
 }
 
-static void xen_release_pmd(u32 pfn)
+static void xen_release_pmd(unsigned long pfn)
 {
 	xen_release_ptpage(pfn, PT_PMD);
 }
 
 #if PAGETABLE_LEVELS == 4
-static void xen_alloc_pud(struct mm_struct *mm, u32 pfn)
+static void xen_alloc_pud(struct mm_struct *mm, unsigned long pfn)
 {
 	xen_alloc_ptpage(mm, pfn, PT_PUD);
 }
 
-static void xen_release_pud(u32 pfn)
+static void xen_release_pud(unsigned long pfn)
 {
 	xen_release_ptpage(pfn, PT_PUD);
 }

commit 6e833587e11ed0dbf12e647127f2650e2f80b26d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Aug 19 13:16:17 2008 -0700

    xen: clean up domain mode predicates
    
    There are four operating modes Xen code may find itself running in:
     - native
     - hvm domain
     - pv dom0
     - pv domU
    
    Clean up predicates for testing for these states to make them more consistent.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Xen-devel <xen-devel@lists.xensource.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 53afa14eb314..b106e825d266 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -56,6 +56,9 @@ EXPORT_SYMBOL_GPL(hypercall_page);
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 
+enum xen_domain_type xen_domain_type = XEN_NATIVE;
+EXPORT_SYMBOL_GPL(xen_domain_type);
+
 /*
  * Identity map, in addition to plain kernel map.  This needs to be
  * large enough to allocate page table pages to allocate the rest.
@@ -1613,6 +1616,8 @@ asmlinkage void __init xen_start_kernel(void)
 	if (!xen_start_info)
 		return;
 
+	xen_domain_type = XEN_PV_DOMAIN;
+
 	BUG_ON(memcmp(xen_start_info->magic, "xen-3", 5) != 0);
 
 	xen_setup_features();
@@ -1650,7 +1655,7 @@ asmlinkage void __init xen_start_kernel(void)
 
 	/* Prevent unwanted bits from being set in PTEs. */
 	__supported_pte_mask &= ~_PAGE_GLOBAL;
-	if (!is_initial_xendomain())
+	if (!xen_initial_domain())
 		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
 
 	/* Don't do the full vcpu_info placement stuff until we have a
@@ -1685,7 +1690,7 @@ asmlinkage void __init xen_start_kernel(void)
 	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
 	boot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);
 
-	if (!is_initial_xendomain()) {
+	if (!xen_initial_domain()) {
 		add_preferred_console("xenboot", 0, NULL);
 		add_preferred_console("tty", 0, NULL);
 		add_preferred_console("hvc", 0, NULL);

commit 169ad16bb87c10a3f7c108bb7008ebc0270f617a
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Mon Jul 28 18:32:09 2008 -0300

    xen_alloc_ptpage: cast PFN_PHYS() argument to unsigned long
    
    Currently paravirt_ops alloc_p*() uses u32 for the pfn args. We should
    change that later, but while the pfn parameter is still u32, we need to
    cast the PFN_PHYS() argument at xen_alloc_ptpage() to unsigned long,
    otherwise it will lose bits on the shift.
    
    I think PFN_PHYS() should behave better when fed with smaller integers,
    but a cast to unsigned long won't be enough for all cases on 32-bit PAE,
    and a cast to u64 would be overkill for most users of PFN_PHYS().
    
    We could have two different flavors of PFN_PHYS: one for low pages
    only (unsigned long) and another that works for any page (u64)),
    but while we don't have it, we will need the cast to unsigned long on
    xen_alloc_ptpage().
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 04ec69e4d02e..53afa14eb314 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -822,7 +822,7 @@ static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 		SetPagePinned(page);
 
 		if (!PageHighMem(page)) {
-			make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
+			make_lowmem_page_readonly(__va(PFN_PHYS((unsigned long)pfn)));
 			if (level == PT_PTE)
 				pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
 		} else

commit cef43bf6b3afd819f7cdcba356af0e8220fb3789
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jul 28 13:33:44 2008 -0700

    xen: fix allocation and use of large ldts, cleanup
    
    Add a proper comment for set_aliased_prot() and fix an
    unsigned long/void * warning.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index cf8b3a93122b..04ec69e4d02e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -237,8 +237,10 @@ static unsigned long xen_store_tr(void)
 }
 
 /*
- * If 'v' is a vmalloc mapping, then find the linear mapping of the
- * page (if any) and also set its protections to match:
+ * Set the page permissions for a particular virtual address.  If the
+ * address is a vmalloc mapping (or other non-linear mapping), then
+ * find the linear mapping of the page and also set its protections to
+ * match.
  */
 static void set_aliased_prot(void *v, pgprot_t prot)
 {
@@ -387,8 +389,7 @@ static void xen_load_gs_index(unsigned int idx)
 static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 				const void *ptr)
 {
-	unsigned long lp = (unsigned long)&dt[entrynum];
-	xmaddr_t mach_lp = arbitrary_virt_to_machine(lp);
+	xmaddr_t mach_lp = arbitrary_virt_to_machine(&dt[entrynum]);
 	u64 entry = *(u64 *)ptr;
 
 	preempt_disable();

commit 0d1edf46ba229b46efacf75c0544b88c05a7b266
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jul 28 11:53:57 2008 -0700

    xen: compile irq functions without -pg for ftrace
    
    For some reason I managed to miss a bunch of irq-related functions
    which also need to be compiled without -pg when using ftrace.  This
    patch moves them into their own file, and starts a cleanup process
    I've been meaning to do anyway.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: "Alex Nixon (Intern)" <Alex.Nixon@eu.citrix.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b795470ec069..cf8b3a93122b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -30,7 +30,6 @@
 #include <xen/interface/xen.h>
 #include <xen/interface/physdev.h>
 #include <xen/interface/vcpu.h>
-#include <xen/interface/sched.h>
 #include <xen/features.h>
 #include <xen/page.h>
 #include <xen/hvc-console.h>
@@ -226,94 +225,6 @@ static unsigned long xen_get_debugreg(int reg)
 	return HYPERVISOR_get_debugreg(reg);
 }
 
-static unsigned long xen_save_fl(void)
-{
-	struct vcpu_info *vcpu;
-	unsigned long flags;
-
-	vcpu = x86_read_percpu(xen_vcpu);
-
-	/* flag has opposite sense of mask */
-	flags = !vcpu->evtchn_upcall_mask;
-
-	/* convert to IF type flag
-	   -0 -> 0x00000000
-	   -1 -> 0xffffffff
-	*/
-	return (-flags) & X86_EFLAGS_IF;
-}
-
-static void xen_restore_fl(unsigned long flags)
-{
-	struct vcpu_info *vcpu;
-
-	/* convert from IF type flag */
-	flags = !(flags & X86_EFLAGS_IF);
-
-	/* There's a one instruction preempt window here.  We need to
-	   make sure we're don't switch CPUs between getting the vcpu
-	   pointer and updating the mask. */
-	preempt_disable();
-	vcpu = x86_read_percpu(xen_vcpu);
-	vcpu->evtchn_upcall_mask = flags;
-	preempt_enable_no_resched();
-
-	/* Doesn't matter if we get preempted here, because any
-	   pending event will get dealt with anyway. */
-
-	if (flags == 0) {
-		preempt_check_resched();
-		barrier(); /* unmask then check (avoid races) */
-		if (unlikely(vcpu->evtchn_upcall_pending))
-			force_evtchn_callback();
-	}
-}
-
-static void xen_irq_disable(void)
-{
-	/* There's a one instruction preempt window here.  We need to
-	   make sure we're don't switch CPUs between getting the vcpu
-	   pointer and updating the mask. */
-	preempt_disable();
-	x86_read_percpu(xen_vcpu)->evtchn_upcall_mask = 1;
-	preempt_enable_no_resched();
-}
-
-static void xen_irq_enable(void)
-{
-	struct vcpu_info *vcpu;
-
-	/* We don't need to worry about being preempted here, since
-	   either a) interrupts are disabled, so no preemption, or b)
-	   the caller is confused and is trying to re-enable interrupts
-	   on an indeterminate processor. */
-
-	vcpu = x86_read_percpu(xen_vcpu);
-	vcpu->evtchn_upcall_mask = 0;
-
-	/* Doesn't matter if we get preempted here, because any
-	   pending event will get dealt with anyway. */
-
-	barrier(); /* unmask then check (avoid races) */
-	if (unlikely(vcpu->evtchn_upcall_pending))
-		force_evtchn_callback();
-}
-
-static void xen_safe_halt(void)
-{
-	/* Blocking includes an implicit local_irq_enable(). */
-	if (HYPERVISOR_sched_op(SCHEDOP_block, NULL) != 0)
-		BUG();
-}
-
-static void xen_halt(void)
-{
-	if (irqs_disabled())
-		HYPERVISOR_vcpu_op(VCPUOP_down, smp_processor_id(), NULL);
-	else
-		xen_safe_halt();
-}
-
 static void xen_leave_lazy(void)
 {
 	paravirt_leave_lazy(paravirt_get_lazy_mode());
@@ -1308,36 +1219,6 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	},
 };
 
-static void __init __xen_init_IRQ(void)
-{
-#ifdef CONFIG_X86_64
-	int i;
-
-	/* Create identity vector->irq map */
-	for(i = 0; i < NR_VECTORS; i++) {
-		int cpu;
-
-		for_each_possible_cpu(cpu)
-			per_cpu(vector_irq, cpu)[i] = i;
-	}
-#endif	/* CONFIG_X86_64 */
-
-	xen_init_IRQ();
-}
-
-static const struct pv_irq_ops xen_irq_ops __initdata = {
-	.init_IRQ = __xen_init_IRQ,
-	.save_fl = xen_save_fl,
-	.restore_fl = xen_restore_fl,
-	.irq_disable = xen_irq_disable,
-	.irq_enable = xen_irq_enable,
-	.safe_halt = xen_safe_halt,
-	.halt = xen_halt,
-#ifdef CONFIG_X86_64
-	.adjust_exception_frame = xen_adjust_exception_frame,
-#endif
-};
-
 static const struct pv_apic_ops xen_apic_ops __initdata = {
 #ifdef CONFIG_X86_LOCAL_APIC
 	.apic_write = xen_apic_write,
@@ -1740,10 +1621,11 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_init_ops = xen_init_ops;
 	pv_time_ops = xen_time_ops;
 	pv_cpu_ops = xen_cpu_ops;
-	pv_irq_ops = xen_irq_ops;
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
 
+	xen_init_irq_ops();
+
 	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {
 		pv_mmu_ops.ptep_modify_prot_start = xen_ptep_modify_prot_start;
 		pv_mmu_ops.ptep_modify_prot_commit = xen_ptep_modify_prot_commit;

commit d89961e2dc87b6e30b8e3f60bd2af5cd92cf4643
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Jul 24 13:48:58 2008 -0700

    xen: suppress known wrmsrs
    
    In general, Xen doesn't support wrmsr from an unprivileged domain; it
    just ends up ignoring the instruction and printing a message on the
    console.
    
    Given that there are sets of MSRs we know the kernel will try to write
    to, but we don't care, just eat them in xen_write_msr to cut down on
    console noise.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b011e4a5dbbe..b795470ec069 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -854,6 +854,19 @@ static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
 			ret = -EFAULT;
 		break;
 #endif
+
+	case MSR_STAR:
+	case MSR_CSTAR:
+	case MSR_LSTAR:
+	case MSR_SYSCALL_MASK:
+	case MSR_IA32_SYSENTER_CS:
+	case MSR_IA32_SYSENTER_ESP:
+	case MSR_IA32_SYSENTER_EIP:
+		/* Fast syscall setup is all done in hypercalls, so
+		   these are all ignored.  Stub them out here to stop
+		   Xen console noise. */
+		break;
+
 	default:
 		ret = native_write_msr_safe(msr, low, high);
 	}

commit a05d2ebab28011c2f3f520833f4bfdd2fd1b9c02
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sun Jul 27 08:45:02 2008 -0700

    xen: fix allocation and use of large ldts
    
    When the ldt gets to more than 1 page in size, the kernel uses vmalloc
    to allocate it.  This means that:
    
     - when making the ldt RO, we must update the pages in both the vmalloc
       mapping and the linear mapping to make sure there are no RW aliases.
    
     - we need to use arbitrary_virt_to_machine to compute the machine addr
       for each update
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e2767c28dac7..b011e4a5dbbe 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -325,24 +325,55 @@ static unsigned long xen_store_tr(void)
 	return 0;
 }
 
+/*
+ * If 'v' is a vmalloc mapping, then find the linear mapping of the
+ * page (if any) and also set its protections to match:
+ */
+static void set_aliased_prot(void *v, pgprot_t prot)
+{
+	int level;
+	pte_t *ptep;
+	pte_t pte;
+	unsigned long pfn;
+	struct page *page;
+
+	ptep = lookup_address((unsigned long)v, &level);
+	BUG_ON(ptep == NULL);
+
+	pfn = pte_pfn(*ptep);
+	page = pfn_to_page(pfn);
+
+	pte = pfn_pte(pfn, prot);
+
+	if (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))
+		BUG();
+
+	if (!PageHighMem(page)) {
+		void *av = __va(PFN_PHYS(pfn));
+
+		if (av != v)
+			if (HYPERVISOR_update_va_mapping((unsigned long)av, pte, 0))
+				BUG();
+	} else
+		kmap_flush_unused();
+}
+
 static void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)
 {
-	unsigned pages = roundup(entries * LDT_ENTRY_SIZE, PAGE_SIZE);
-	void *v = ldt;
+	const unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;
 	int i;
 
-	for(i = 0; i < pages; i += PAGE_SIZE)
-		make_lowmem_page_readonly(v + i);
+	for(i = 0; i < entries; i += entries_per_page)
+		set_aliased_prot(ldt + i, PAGE_KERNEL_RO);
 }
 
 static void xen_free_ldt(struct desc_struct *ldt, unsigned entries)
 {
-	unsigned pages = roundup(entries * LDT_ENTRY_SIZE, PAGE_SIZE);
-	void *v = ldt;
+	const unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;
 	int i;
 
-	for(i = 0; i < pages; i += PAGE_SIZE)
-		make_lowmem_page_readwrite(v + i);
+	for(i = 0; i < entries; i += entries_per_page)
+		set_aliased_prot(ldt + i, PAGE_KERNEL);
 }
 
 static void xen_set_ldt(const void *addr, unsigned entries)
@@ -446,7 +477,7 @@ static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 				const void *ptr)
 {
 	unsigned long lp = (unsigned long)&dt[entrynum];
-	xmaddr_t mach_lp = virt_to_machine(lp);
+	xmaddr_t mach_lp = arbitrary_virt_to_machine(lp);
 	u64 entry = *(u64 *)ptr;
 
 	preempt_disable();
@@ -579,7 +610,7 @@ static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
 }
 
 static void xen_load_sp0(struct tss_struct *tss,
-			  struct thread_struct *thread)
+			 struct thread_struct *thread)
 {
 	struct multicall_space mcs = xen_mc_entry(0);
 	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread->sp0);

commit b56afe1d41653fb07ab1b5af5ccc12001c4dd5a0
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Thu Jul 24 12:15:45 2008 -0300

    x86, xen: Use native_pte_flags instead of native_pte_val for .pte_flags
    
    Using native_pte_val triggers the BUG_ON() in the paravirt_ops
    version of pte_flags().
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 06219e60e9c8..e2767c28dac7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1347,7 +1347,7 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.ptep_modify_prot_commit = __ptep_modify_prot_commit,
 
 	.pte_val = xen_pte_val,
-	.pte_flags = native_pte_val,
+	.pte_flags = native_pte_flags,
 	.pgd_val = xen_pgd_val,
 
 	.make_pte = xen_make_pte,

commit 10a010f6953b5a14ba2f0be40a4fce1bea220875
Merge: 510b37258dfd fb2e405fc1fc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 25 13:08:16 2008 +0200

    Merge branch 'linus' into x86/x2apic
    
    Conflicts:
    
            drivers/pci/dmar.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 38ffbe66d59051fd9cfcfc8545f164700e2fa3bc
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jul 23 14:21:18 2008 -0700

    x86/paravirt/xen: properly fill out the ldt ops
    
    LTP testing showed that Xen does not properly implement
    sys_modify_ldt().  This patch does the final little bits needed to
    make the ldt work properly.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9ff6e3cbf08f..06219e60e9c8 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -325,6 +325,26 @@ static unsigned long xen_store_tr(void)
 	return 0;
 }
 
+static void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)
+{
+	unsigned pages = roundup(entries * LDT_ENTRY_SIZE, PAGE_SIZE);
+	void *v = ldt;
+	int i;
+
+	for(i = 0; i < pages; i += PAGE_SIZE)
+		make_lowmem_page_readonly(v + i);
+}
+
+static void xen_free_ldt(struct desc_struct *ldt, unsigned entries)
+{
+	unsigned pages = roundup(entries * LDT_ENTRY_SIZE, PAGE_SIZE);
+	void *v = ldt;
+	int i;
+
+	for(i = 0; i < pages; i += PAGE_SIZE)
+		make_lowmem_page_readwrite(v + i);
+}
+
 static void xen_set_ldt(const void *addr, unsigned entries)
 {
 	struct mmuext_op *op;
@@ -1220,6 +1240,9 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.load_gs_index = xen_load_gs_index,
 #endif
 
+	.alloc_ldt = xen_alloc_ldt,
+	.free_ldt = xen_free_ldt,
+
 	.store_gdt = native_store_gdt,
 	.store_idt = native_store_idt,
 	.store_tr = xen_store_tr,

commit 59438c9fc4f7a92c808c9049bc6b396f98bf954c
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jul 21 22:59:42 2008 -0700

    x86: rename PTE_MASK to PTE_PFN_MASK
    
    Rusty, in his peevish way, complained that macros defining constants
    should have a name which somewhat accurately reflects the actual
    purpose of the constant.
    
    Aside from the fact that PTE_MASK gives no clue as to what's actually
    being masked, and is misleadingly similar to the functionally entirely
    different PMD_MASK, PUD_MASK and PGD_MASK, I don't really see what the
    problem is.
    
    But if this patch silences the incessent noise, then it will have
    achieved its goal (TODO: write test-case).
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 194bbd6e3241..9ff6e3cbf08f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1435,7 +1435,7 @@ static unsigned long m2p(phys_addr_t maddr)
 {
 	phys_addr_t paddr;
 
-	maddr &= PTE_MASK;
+	maddr &= PTE_PFN_MASK;
 	paddr = mfn_to_pfn(maddr >> PAGE_SHIFT) << PAGE_SHIFT;
 
 	return paddr;

commit 76c3bb15d6786a0b8da0ad0090e0c9c3672fc08b
Merge: 7be42004065c 93ded9b8fd42
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 22 09:06:21 2008 +0200

    Merge branch 'linus' into x86/x2apic

commit acee709cab689ec7703770e8b8cb5cc3a4abcb31
Merge: 33a37eb411d1 5ff4789d045c 35b680557f95 c4dc59ae7af8 7edf8891ad7a 9781f39fd209 48fe4a76e27d be54f9d1c8df 77e442461c74 caadbdce240c 5e5a29bf2624 e3a61b0a8c0e fec0962e0bed fab3b58d3b24 f2ba93929fdb 48ae74443403 3cabf37f6167 7019cc2dd6fa 2ddf9b7b3e66 e66d90fb4abd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 21 16:37:17 2008 +0200

    Merge branches 'x86/urgent', 'x86/amd-iommu', 'x86/apic', 'x86/cleanups', 'x86/core', 'x86/cpu', 'x86/fixmap', 'x86/gart', 'x86/kprobes', 'x86/memtest', 'x86/modules', 'x86/nmi', 'x86/pat', 'x86/reboot', 'x86/setup', 'x86/step', 'x86/unify-pci', 'x86/uv', 'x86/xen' and 'xen-64bit' into x86/for-linus

commit caf43bf7c6a55e89b6df5179df434d67e24aa32e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jul 20 14:06:50 2008 +0200

    x86, xen: fix apic_ops build on UP
    
    fix:
    
     arch/x86/xen/enlighten.c:615: error: variable ‘xen_basic_apic_ops’ has initializer but incomplete type
     arch/x86/xen/enlighten.c:616: error: unknown field ‘read’ specified in initializer
     [...]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 008b7b69581e..e4d1459a63df 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -35,6 +35,7 @@
 #include <xen/page.h>
 
 #include <asm/paravirt.h>
+#include <asm/apic.h>
 #include <asm/page.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>

commit 453c1404c5273a30d715e5a83372a78cff70b6d9
Merge: a208f37a465e 35b680557f95
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 23:00:05 2008 +0200

    Merge branch 'x86/apic' into x86/x2apic
    
    Conflicts:
    
            arch/x86/kernel/paravirt.c
            arch/x86/kernel/smpboot.c
            arch/x86/kernel/vmi_32.c
            arch/x86/lguest/boot.c
            arch/x86/xen/enlighten.c
            include/asm-x86/apic.h
            include/asm-x86/paravirt.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a208f37a465e222218974ab20a31b42b7b4893b2
Merge: 511d9d341836 5b664cb235e9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 22:50:34 2008 +0200

    Merge branch 'linus' into x86/x2apic

commit 95c7c23b06bc92f1772b9c9460845f179ba8c39e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 15 13:42:34 2008 -0700

    xen: report hypervisor version
    
    Various versions of the hypervisor have differences in what ABIs and
    features they support.  Print some details into the boot log to help
    with remote debugging.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bb508456ef52..5328e46d9cf7 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -167,10 +167,14 @@ void xen_vcpu_restore(void)
 
 static void __init xen_banner(void)
 {
+	unsigned version = HYPERVISOR_xen_version(XENVER_version, NULL);
+	struct xen_extraversion extra;
+	HYPERVISOR_xen_version(XENVER_extraversion, &extra);
+
 	printk(KERN_INFO "Booting paravirtualized kernel on %s\n",
 	       pv_info.name);
-	printk(KERN_INFO "Hypervisor signature: %s%s\n",
-	       xen_start_info->magic,
+	printk(KERN_INFO "Xen version: %d.%d%s%s\n",
+	       version >> 16, version & 0xffff, extra.extraversion,
 	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
 }
 

commit 593f4a788e5d09e9f00182561437461b0b564de4
Author: Maciej W. Rozycki <macro@linux-mips.org>
Date:   Wed Jul 16 19:15:30 2008 +0100

    x86: APIC: remove apic_write_around(); use alternatives
    
    Use alternatives to select the workaround for the 11AP Pentium erratum
    for the affected steppings on the fly rather than build time.  Remove the
    X86_GOOD_APIC configuration option and replace all the calls to
    apic_write_around() with plain apic_write(), protecting accesses to the
    ESR as appropriate due to the 3AP Pentium erratum.  Remove
    apic_read_around() and all its invocations altogether as not needed.
    Remove apic_write_atomic() and all its implementing backends.  The use of
    ASM_OUTPUT2() is not strictly needed for input constraints, but I have
    used it for readability's sake.
    
    I had the feeling no one else was brave enough to do it, so I went ahead
    and here it is.  Verified by checking the generated assembly and tested
    with both a 32-bit and a 64-bit configuration, also with the 11AP
    "feature" forced on and verified with gdb on /proc/kcore to work as
    expected (as an 11AP machines are quite hard to get hands on these days).
    Some script complained about the use of "volatile", but apic_write() needs
    it for the same reason and is effectively a replacement for writel(), so I
    have disregarded it.
    
    I am not sure what the policy wrt defconfig files is, they are generated
    and there is risk of a conflict resulting from an unrelated change, so I
    have left changes to them out.  The option will get removed from them at
    the next run.
    
    Some testing with machines other than mine will be needed to avoid some
    stupid mistake, but despite its volume, the change is not really that
    intrusive, so I am fairly confident that because it works for me, it will
    everywhere.
    
    Signed-off-by: Maciej W. Rozycki <macro@linux-mips.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bb508456ef52..7f26c3718777 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1131,7 +1131,6 @@ static const struct pv_irq_ops xen_irq_ops __initdata = {
 static const struct pv_apic_ops xen_apic_ops __initdata = {
 #ifdef CONFIG_X86_LOCAL_APIC
 	.apic_write = xen_apic_write,
-	.apic_write_atomic = xen_apic_write,
 	.apic_read = xen_apic_read,
 	.setup_boot_clock = paravirt_nop,
 	.setup_secondary_clock = paravirt_nop,

commit b3fe124389f9dd97f0bbd954da2910e286648f0f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jul 9 13:45:33 2008 +0200

    xen64: fix build error on 32-bit + !HIGHMEM
    
    fix:
    
    arch/x86/xen/enlighten.c: In function 'xen_set_fixmap':
    arch/x86/xen/enlighten.c:1127: error: 'FIX_KMAP_BEGIN' undeclared (first use in this function)
    arch/x86/xen/enlighten.c:1127: error: (Each undeclared identifier is reported only once
    arch/x86/xen/enlighten.c:1127: error: for each function it appears in.)
    arch/x86/xen/enlighten.c:1127: error: 'FIX_KMAP_END' undeclared (first use in this function)
    make[1]: *** [arch/x86/xen/enlighten.o] Error 1
    make: *** [arch/x86/xen/enlighten.o] Error 2
    
    FIX_KMAP_BEGIN is only available on HIGHMEM.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 776c0fb77d69..3da6acb7eafc 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1124,7 +1124,9 @@ static void xen_set_fixmap(unsigned idx, unsigned long phys, pgprot_t prot)
 #ifdef CONFIG_X86_32
 	case FIX_WP_TEST:
 	case FIX_VDSO:
+# ifdef CONFIG_HIGHMEM
 	case FIX_KMAP_BEGIN ... FIX_KMAP_END:
+# endif
 #else
 	case VSYSCALL_LAST_PAGE ... VSYSCALL_FIRST_PAGE:
 #endif

commit 1153968a48e3ca3e2b7a437e8b82ec9e6f768e24
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:16 2008 -0700

    xen: implement Xen write_msr operation
    
    64-bit uses MSRs for important things like the base for fs and
    gs-prefixed addresses.  It's more efficient to use a hypercall to
    update these, rather than go via the trap and emulate path.
    
    Other MSR writes are just passed through; in an unprivileged domain
    they do nothing, but it might be useful later.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f64b8729cd07..776c0fb77d69 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -41,6 +41,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/fixmap.h>
 #include <asm/processor.h>
+#include <asm/msr-index.h>
 #include <asm/setup.h>
 #include <asm/desc.h>
 #include <asm/pgtable.h>
@@ -777,6 +778,34 @@ static void xen_write_cr3(unsigned long cr3)
 	xen_mc_issue(PARAVIRT_LAZY_CPU);  /* interrupts restored */
 }
 
+static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)
+{
+	int ret;
+
+	ret = 0;
+
+	switch(msr) {
+#ifdef CONFIG_X86_64
+		unsigned which;
+		u64 base;
+
+	case MSR_FS_BASE:		which = SEGBASE_FS; goto set;
+	case MSR_KERNEL_GS_BASE:	which = SEGBASE_GS_USER; goto set;
+	case MSR_GS_BASE:		which = SEGBASE_GS_KERNEL; goto set;
+
+	set:
+		base = ((u64)high << 32) | low;
+		if (HYPERVISOR_set_segment_base(which, base) != 0)
+			ret = -EFAULT;
+		break;
+#endif
+	default:
+		ret = native_write_msr_safe(msr, low, high);
+	}
+
+	return ret;
+}
+
 /* Early in boot, while setting up the initial pagetable, assume
    everything is pinned. */
 static __init void xen_alloc_pte_init(struct mm_struct *mm, u32 pfn)
@@ -1165,7 +1194,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.wbinvd = native_wbinvd,
 
 	.read_msr = native_read_msr_safe,
-	.write_msr = native_write_msr_safe,
+	.write_msr = xen_write_msr_safe,
 	.read_tsc = native_read_tsc,
 	.read_pmc = native_read_pmc,
 

commit bf18bf94dc72db998d0fbebc846c07c858a59c90
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:15 2008 -0700

    xen64: set up userspace syscall patch
    
    64-bit userspace expects the vdso to be mapped at a specific fixed
    address, which happens to be in the middle of the kernel address
    space.  Because we have split user and kernel pagetables, we need to
    make special arrangements for the vsyscall mapping to appear in the
    kernel part of the user pagetable.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 87d36044054d..f64b8729cd07 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -56,6 +56,18 @@ EXPORT_SYMBOL_GPL(hypercall_page);
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 
+/*
+ * Identity map, in addition to plain kernel map.  This needs to be
+ * large enough to allocate page table pages to allocate the rest.
+ * Each page can map 2MB.
+ */
+static pte_t level1_ident_pgt[PTRS_PER_PTE * 4] __page_aligned_bss;
+
+#ifdef CONFIG_X86_64
+/* l3 pud for userspace vsyscall mapping */
+static pud_t level3_user_vsyscall[PTRS_PER_PUD] __page_aligned_bss;
+#endif /* CONFIG_X86_64 */
+
 /*
  * Note about cr3 (pagetable base) values:
  *
@@ -831,12 +843,20 @@ static int xen_pgd_alloc(struct mm_struct *mm)
 #ifdef CONFIG_X86_64
 	{
 		struct page *page = virt_to_page(pgd);
+		pgd_t *user_pgd;
 
 		BUG_ON(page->private != 0);
 
-		page->private = __get_free_page(GFP_KERNEL | __GFP_ZERO);
-		if (page->private == 0)
-			ret = -ENOMEM;
+		ret = -ENOMEM;
+
+		user_pgd = (pgd_t *)__get_free_page(GFP_KERNEL | __GFP_ZERO);
+		page->private = (unsigned long)user_pgd;
+
+		if (user_pgd != NULL) {
+			user_pgd[pgd_index(VSYSCALL_START)] =
+				__pgd(__pa(level3_user_vsyscall) | _PAGE_TABLE);
+			ret = 0;
+		}
 
 		BUG_ON(PagePinned(virt_to_page(xen_get_user_pgd(pgd))));
 	}
@@ -977,6 +997,9 @@ static __init void xen_post_allocator_init(void)
 	pv_mmu_ops.release_pud = xen_release_pud;
 #endif
 
+#ifdef CONFIG_X86_64
+	SetPagePinned(virt_to_page(level3_user_vsyscall));
+#endif
 	xen_mark_init_mm_pinned();
 }
 
@@ -1088,6 +1111,15 @@ static void xen_set_fixmap(unsigned idx, unsigned long phys, pgprot_t prot)
 	}
 
 	__native_set_fixmap(idx, pte);
+
+#ifdef CONFIG_X86_64
+	/* Replicate changes to map the vsyscall page into the user
+	   pagetable vsyscall mapping. */
+	if (idx >= VSYSCALL_LAST_PAGE && idx <= VSYSCALL_FIRST_PAGE) {
+		unsigned long vaddr = __fix_to_virt(idx);
+		set_pte_vaddr_pud(level3_user_vsyscall, vaddr, pte);
+	}
+#endif
 }
 
 static const struct pv_info xen_info __initdata = {
@@ -1427,13 +1459,6 @@ static void set_page_prot(void *addr, pgprot_t prot)
 		BUG();
 }
 
-/*
- * Identity map, in addition to plain kernel map.  This needs to be
- * large enough to allocate page table pages to allocate the rest.
- * Each page can map 2MB.
- */
-static pte_t level1_ident_pgt[PTRS_PER_PTE * 4] __page_aligned_bss;
-
 static __init void xen_map_identity_early(pmd_t *pmd, unsigned long max_pfn)
 {
 	unsigned pmdidx, pteidx;
@@ -1533,6 +1558,7 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pf
 	set_page_prot(init_level4_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level3_ident_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level3_kernel_pgt, PAGE_KERNEL_RO);
+	set_page_prot(level3_user_vsyscall, PAGE_KERNEL_RO);
 	set_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level2_fixmap_pgt, PAGE_KERNEL_RO);
 

commit 6fcac6d305e8238939e169f4c52e8ec8a552a31f
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:14 2008 -0700

    xen64: set up syscall and sysenter entrypoints for 64-bit
    
    We set up entrypoints for syscall and sysenter.  sysenter is only used
    for 32-bit compat processes, whereas syscall can be used in by both 32
    and 64-bit processes.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 48f1a7eca8b9..87d36044054d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1139,6 +1139,10 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 
 	.iret = xen_iret,
 	.irq_enable_sysexit = xen_sysexit,
+#ifdef CONFIG_X86_64
+	.usergs_sysret32 = xen_sysret32,
+	.usergs_sysret64 = xen_sysret64,
+#endif
 
 	.load_tr_desc = paravirt_nop,
 	.set_ldt = xen_set_ldt,

commit d6182fbf04164016cb6540db02eef3d6bdc967c3
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:13 2008 -0700

    xen64: allocate and manage user pagetables
    
    Because the x86_64 architecture does not enforce segment limits, Xen
    cannot protect itself with them as it does in 32-bit mode.  Therefore,
    to protect itself, it runs the guest kernel in ring 3.  Since it also
    runs the guest userspace in ring3, the guest kernel must maintain a
    second pagetable for its userspace, which does not map kernel space.
    Naturally, the guest kernel pagetables map both kernel and userspace.
    
    The userspace pagetable is attached to the corresponding kernel
    pagetable via the pgd's page->private field.  It is allocated and
    freed at the same time as the kernel pgd via the
    paravirt_pgd_alloc/free hooks.
    
    Fortunately, the user pagetable is almost entirely shared with the
    kernel pagetable; the only difference is the pgd page itself.  set_pgd
    will populate all entries in the kernel pagetable, and also set the
    corresponding user pgd entry if the address is less than
    STACK_TOP_MAX.
    
    The user pagetable must be pinned and unpinned with the kernel one,
    but because the pagetables are aliased, pgd_walk() only needs to be
    called on the kernel pagetable.  The user pgd page is then
    pinned/unpinned along with the kernel pgd page.
    
    xen_write_cr3 must write both the kernel and user cr3s.
    
    The init_mm.pgd pagetable never has a user pagetable allocated for it,
    because it can never be used while running usermode.
    
    One awkward area is that early in boot the page structures are not
    available.  No user pagetable can exist at that point, but it
    complicates the logic to avoid looking at the page structure.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c13698faae54..48f1a7eca8b9 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -46,7 +46,6 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/reboot.h>
-#include <asm/pgalloc.h>
 
 #include "xen-ops.h"
 #include "mmu.h"
@@ -711,29 +710,57 @@ static void set_current_cr3(void *v)
 	x86_write_percpu(xen_current_cr3, (unsigned long)v);
 }
 
-static void xen_write_cr3(unsigned long cr3)
+static void __xen_write_cr3(bool kernel, unsigned long cr3)
 {
 	struct mmuext_op *op;
 	struct multicall_space mcs;
-	unsigned long mfn = pfn_to_mfn(PFN_DOWN(cr3));
+	unsigned long mfn;
 
-	BUG_ON(preemptible());
+	if (cr3)
+		mfn = pfn_to_mfn(PFN_DOWN(cr3));
+	else
+		mfn = 0;
 
-	mcs = xen_mc_entry(sizeof(*op));  /* disables interrupts */
+	WARN_ON(mfn == 0 && kernel);
 
-	/* Update while interrupts are disabled, so its atomic with
-	   respect to ipis */
-	x86_write_percpu(xen_cr3, cr3);
+	mcs = __xen_mc_entry(sizeof(*op));
 
 	op = mcs.args;
-	op->cmd = MMUEXT_NEW_BASEPTR;
+	op->cmd = kernel ? MMUEXT_NEW_BASEPTR : MMUEXT_NEW_USER_BASEPTR;
 	op->arg1.mfn = mfn;
 
 	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
 
-	/* Update xen_update_cr3 once the batch has actually
-	   been submitted. */
-	xen_mc_callback(set_current_cr3, (void *)cr3);
+	if (kernel) {
+		x86_write_percpu(xen_cr3, cr3);
+
+		/* Update xen_current_cr3 once the batch has actually
+		   been submitted. */
+		xen_mc_callback(set_current_cr3, (void *)cr3);
+	}
+}
+
+static void xen_write_cr3(unsigned long cr3)
+{
+	BUG_ON(preemptible());
+
+	xen_mc_batch();  /* disables interrupts */
+
+	/* Update while interrupts are disabled, so its atomic with
+	   respect to ipis */
+	x86_write_percpu(xen_cr3, cr3);
+
+	__xen_write_cr3(true, cr3);
+
+#ifdef CONFIG_X86_64
+	{
+		pgd_t *user_pgd = xen_get_user_pgd(__va(cr3));
+		if (user_pgd)
+			__xen_write_cr3(false, __pa(user_pgd));
+		else
+			__xen_write_cr3(false, 0);
+	}
+#endif
 
 	xen_mc_issue(PARAVIRT_LAZY_CPU);  /* interrupts restored */
 }
@@ -794,6 +821,40 @@ static void xen_alloc_pmd(struct mm_struct *mm, u32 pfn)
 	xen_alloc_ptpage(mm, pfn, PT_PMD);
 }
 
+static int xen_pgd_alloc(struct mm_struct *mm)
+{
+	pgd_t *pgd = mm->pgd;
+	int ret = 0;
+
+	BUG_ON(PagePinned(virt_to_page(pgd)));
+
+#ifdef CONFIG_X86_64
+	{
+		struct page *page = virt_to_page(pgd);
+
+		BUG_ON(page->private != 0);
+
+		page->private = __get_free_page(GFP_KERNEL | __GFP_ZERO);
+		if (page->private == 0)
+			ret = -ENOMEM;
+
+		BUG_ON(PagePinned(virt_to_page(xen_get_user_pgd(pgd))));
+	}
+#endif
+
+	return ret;
+}
+
+static void xen_pgd_free(struct mm_struct *mm, pgd_t *pgd)
+{
+#ifdef CONFIG_X86_64
+	pgd_t *user_pgd = xen_get_user_pgd(pgd);
+
+	if (user_pgd)
+		free_page((unsigned long)user_pgd);
+#endif
+}
+
 /* This should never happen until we're OK to use struct page */
 static void xen_release_ptpage(u32 pfn, unsigned level)
 {
@@ -1168,8 +1229,8 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.pte_update = paravirt_nop,
 	.pte_update_defer = paravirt_nop,
 
-	.pgd_alloc = __paravirt_pgd_alloc,
-	.pgd_free = paravirt_nop,
+	.pgd_alloc = xen_pgd_alloc,
+	.pgd_free = xen_pgd_free,
 
 	.alloc_pte = xen_alloc_pte_init,
 	.release_pte = xen_release_pte_init,
@@ -1480,7 +1541,15 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pf
 
 	/* Switch over */
 	pgd = init_level4_pgt;
-	xen_write_cr3(__pa(pgd));
+
+	/*
+	 * At this stage there can be no user pgd, and no page
+	 * structure to attach it to, so make sure we just set kernel
+	 * pgd.
+	 */
+	xen_mc_batch();
+	__xen_write_cr3(true, __pa(pgd));
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
 
 	reserve_early(__pa(xen_start_info->pt_base),
 		      __pa(xen_start_info->pt_base +

commit 8a95408e183b3e4aaf3b6a66fa34bff4db53011b
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Tue Jul 8 15:07:10 2008 -0700

    xen64: Clear %fs on xen_load_tls()
    
    We need to do this, otherwise we can get a GPF on hypercall return
    after TLS descriptor is cleared but %fs is still pointing to it.
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0172ba774523..c13698faae54 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -364,14 +364,6 @@ static void load_TLS_descriptor(struct thread_struct *t,
 
 static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
 {
-	xen_mc_batch();
-
-	load_TLS_descriptor(t, cpu, 0);
-	load_TLS_descriptor(t, cpu, 1);
-	load_TLS_descriptor(t, cpu, 2);
-
-	xen_mc_issue(PARAVIRT_LAZY_CPU);
-
 	/*
 	 * XXX sleazy hack: If we're being called in a lazy-cpu zone,
 	 * it means we're in a context switch, and %gs has just been
@@ -380,9 +372,30 @@ static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
 	 * Either way, it has been saved, and the new value will get
 	 * loaded properly.  This will go away as soon as Xen has been
 	 * modified to not save/restore %gs for normal hypercalls.
+	 *
+	 * On x86_64, this hack is not used for %gs, because gs points
+	 * to KERNEL_GS_BASE (and uses it for PDA references), so we
+	 * must not zero %gs on x86_64
+	 *
+	 * For x86_64, we need to zero %fs, otherwise we may get an
+	 * exception between the new %fs descriptor being loaded and
+	 * %fs being effectively cleared at __switch_to().
 	 */
-	if (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU)
+	if (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU) {
+#ifdef CONFIG_X86_32
 		loadsegment(gs, 0);
+#else
+		loadsegment(fs, 0);
+#endif
+	}
+
+	xen_mc_batch();
+
+	load_TLS_descriptor(t, cpu, 0);
+	load_TLS_descriptor(t, cpu, 1);
+	load_TLS_descriptor(t, cpu, 2);
+
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
 }
 
 #ifdef CONFIG_X86_64

commit b7c3c5c15936a40c79ef40af7b3bac801c7feb20
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:07 2008 -0700

    xen: make sure the kernel command line is right
    
    Point the boot params cmd_line_ptr to the domain-builder-provided
    command line.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 3b6b7fcf5b55..0172ba774523 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1587,6 +1587,7 @@ asmlinkage void __init xen_start_kernel(void)
 	boot_params.hdr.ramdisk_image = xen_start_info->mod_start
 		? __pa(xen_start_info->mod_start) : 0;
 	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
+	boot_params.hdr.cmd_line_ptr = __pa(xen_start_info->cmd_line);
 
 	if (!is_initial_xendomain()) {
 		add_preferred_console("xenboot", 0, NULL);

commit a8fc1089e49caa5dca346dfacb5c84abf9a22a0c
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Tue Jul 8 15:07:05 2008 -0700

    xen64: implement xen_load_gs_index()
    
    xen-64: implement xen_load_gs_index()
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 52f2292672c4..3b6b7fcf5b55 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -385,6 +385,14 @@ static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
 		loadsegment(gs, 0);
 }
 
+#ifdef CONFIG_X86_64
+static void xen_load_gs_index(unsigned int idx)
+{
+	if (HYPERVISOR_set_segment_base(SEGBASE_GS_USER_SEL, idx))
+		BUG();
+}
+#endif
+
 static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 				const void *ptr)
 {
@@ -1063,6 +1071,9 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.load_gdt = xen_load_gdt,
 	.load_idt = xen_load_idt,
 	.load_tls = xen_load_tls,
+#ifdef CONFIG_X86_64
+	.load_gs_index = xen_load_gs_index,
+#endif
 
 	.store_gdt = native_store_gdt,
 	.store_idt = native_store_idt,

commit 0725cbb97793d4e65bf148e4872959cdbb8c6ddd
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:03 2008 -0700

    xen64: add identity irq->vector map
    
    The x86_64 interrupt subsystem is oriented towards vectors, as opposed
    to a flat irq space as it is in x86-32.  This patch adds a simple
    identity irq->vector mapping so that we can continue to feed irqs into
    do_IRQ() and get a good result.
    
    Ideally x86_32 will unify with the 64-bit code and use vectors too.
    At that point we can move to mapping event channels to vectors, which
    will allow us to economise on irqs (so per-cpu event channels can
    share irqs, rather than having to allocte one per cpu, for example).
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8b60982e457a..52f2292672c4 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1085,8 +1085,25 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	},
 };
 
+static void __init __xen_init_IRQ(void)
+{
+#ifdef CONFIG_X86_64
+	int i;
+
+	/* Create identity vector->irq map */
+	for(i = 0; i < NR_VECTORS; i++) {
+		int cpu;
+
+		for_each_possible_cpu(cpu)
+			per_cpu(vector_irq, cpu)[i] = i;
+	}
+#endif	/* CONFIG_X86_64 */
+
+	xen_init_IRQ();
+}
+
 static const struct pv_irq_ops xen_irq_ops __initdata = {
-	.init_IRQ = xen_init_IRQ,
+	.init_IRQ = __xen_init_IRQ,
 	.save_fl = xen_save_fl,
 	.restore_fl = xen_restore_fl,
 	.irq_disable = xen_irq_disable,

commit 952d1d7055c8cbf95b4ad2f90be5ed37db8a48ee
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:01 2008 -0700

    xen64: add pvop for swapgs
    
    swapgs is a no-op under Xen, because the hypervisor makes sure the
    right version of %gs is current when switching between user and kernel
    modes.  This means that the swapgs "implementation" can be inlined and
    used when the stack is unsafe (usermode).  Unfortunately, it means
    that disabling patching will result in a non-booting kernel...
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 9d94483b3b5e..8b60982e457a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1076,6 +1076,9 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.set_iopl_mask = xen_set_iopl_mask,
 	.io_delay = xen_io_delay,
 
+	/* Xen takes care of %gs when switching to usermode for us */
+	.swapgs = paravirt_nop,
+
 	.lazy_mode = {
 		.enter = paravirt_enter_lazy_cpu,
 		.leave = xen_leave_lazy,

commit 997409d3d0bd6894f33e31ced251c0fdf523aa14
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:00 2008 -0700

    xen64: deal with extra words Xen pushes onto exception frames
    
    Xen pushes two extra words containing the values of rcx and r11.  This
    pvop hook copies the words back into their appropriate registers, and
    cleans them off the stack.  This leaves the stack in native form, so
    the normal handler can run unchanged.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f5e96f7a4c5c..9d94483b3b5e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1091,7 +1091,7 @@ static const struct pv_irq_ops xen_irq_ops __initdata = {
 	.safe_halt = xen_safe_halt,
 	.halt = xen_halt,
 #ifdef CONFIG_X86_64
-	.adjust_exception_frame = paravirt_nop,
+	.adjust_exception_frame = xen_adjust_exception_frame,
 #endif
 };
 

commit e176d367d0cc8b8efd2e0960c9edf5d2fe7cd9f1
Author: Eduardo Habkost <ehabkost@Rawhide-64.localdomain>
Date:   Tue Jul 8 15:06:59 2008 -0700

    xen64: xen_write_idt_entry() and cvt_gate_to_trap()
    
    Changed to use the (to-be-)unified descriptor structs.
    
    Signed-off-by: Eduardo Habkost <ehabkost@Rawhide-64.localdomain>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index da91404fc66c..f5e96f7a4c5c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -401,23 +401,18 @@ static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
 	preempt_enable();
 }
 
-static int cvt_gate_to_trap(int vector, u32 low, u32 high,
+static int cvt_gate_to_trap(int vector, const gate_desc *val,
 			    struct trap_info *info)
 {
-	u8 type, dpl;
-
-	type = (high >> 8) & 0x1f;
-	dpl = (high >> 13) & 3;
-
-	if (type != 0xf && type != 0xe)
+	if (val->type != 0xf && val->type != 0xe)
 		return 0;
 
 	info->vector = vector;
-	info->address = (high & 0xffff0000) | (low & 0x0000ffff);
-	info->cs = low >> 16;
-	info->flags = dpl;
+	info->address = gate_offset(*val);
+	info->cs = gate_segment(*val);
+	info->flags = val->dpl;
 	/* interrupt gates clear IF */
-	if (type == 0xe)
+	if (val->type == 0xe)
 		info->flags |= 4;
 
 	return 1;
@@ -444,11 +439,10 @@ static void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)
 
 	if (p >= start && (p + 8) <= end) {
 		struct trap_info info[2];
-		u32 *desc = (u32 *)g;
 
 		info[1].address = 0;
 
-		if (cvt_gate_to_trap(entrynum, desc[0], desc[1], &info[0]))
+		if (cvt_gate_to_trap(entrynum, g, &info[0]))
 			if (HYPERVISOR_set_trap_table(info))
 				BUG();
 	}
@@ -461,13 +455,13 @@ static void xen_convert_trap_info(const struct desc_ptr *desc,
 {
 	unsigned in, out, count;
 
-	count = (desc->size+1) / 8;
+	count = (desc->size+1) / sizeof(gate_desc);
 	BUG_ON(count > 256);
 
 	for (in = out = 0; in < count; in++) {
-		const u32 *entry = (u32 *)(desc->address + in * 8);
+		gate_desc *entry = (gate_desc*)(desc->address) + in;
 
-		if (cvt_gate_to_trap(in, entry[0], entry[1], &traps[out]))
+		if (cvt_gate_to_trap(in, entry, &traps[out]))
 			out++;
 	}
 	traps[out].address = 0;

commit 8745f8b0b914cf1d617ecc49726c24011858c74e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:57 2008 -0700

    xen64: defer setting pagetable alloc/release ops
    
    We need to wait until the page structure is available to use the
    proper pagetable page alloc/release operations, since they use struct
    page to determine if a pagetable is pinned.
    
    This happened to work in 32bit because nobody allocated new pagetable
    pages in the interim between xen_pagetable_setup_done and
    xen_post_allocator_init, but the 64-bit kenrel needs to allocate more
    pagetable levels.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 19c12a6c7311..da91404fc66c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -878,30 +878,29 @@ void xen_setup_shared_info(void)
 
 static __init void xen_pagetable_setup_done(pgd_t *base)
 {
-	/* This will work as long as patching hasn't happened yet
-	   (which it hasn't) */
-	pv_mmu_ops.alloc_pte = xen_alloc_pte;
-	pv_mmu_ops.alloc_pmd = xen_alloc_pmd;
-	pv_mmu_ops.release_pte = xen_release_pte;
-	pv_mmu_ops.release_pmd = xen_release_pmd;
-#if PAGETABLE_LEVELS == 4
-	pv_mmu_ops.alloc_pud = xen_alloc_pud;
-	pv_mmu_ops.release_pud = xen_release_pud;
-#endif
-
-	pv_mmu_ops.set_pte = xen_set_pte;
-
 	xen_setup_shared_info();
 }
 
 static __init void xen_post_allocator_init(void)
 {
+	pv_mmu_ops.set_pte = xen_set_pte;
 	pv_mmu_ops.set_pmd = xen_set_pmd;
 	pv_mmu_ops.set_pud = xen_set_pud;
 #if PAGETABLE_LEVELS == 4
 	pv_mmu_ops.set_pgd = xen_set_pgd;
 #endif
 
+	/* This will work as long as patching hasn't happened yet
+	   (which it hasn't) */
+	pv_mmu_ops.alloc_pte = xen_alloc_pte;
+	pv_mmu_ops.alloc_pmd = xen_alloc_pmd;
+	pv_mmu_ops.release_pte = xen_release_pte;
+	pv_mmu_ops.release_pmd = xen_release_pmd;
+#if PAGETABLE_LEVELS == 4
+	pv_mmu_ops.alloc_pud = xen_alloc_pud;
+	pv_mmu_ops.release_pud = xen_release_pud;
+#endif
+
 	xen_mark_init_mm_pinned();
 }
 

commit 39dbc5bd345ebf93e066dde7f8e29467eb61b42e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:53 2008 -0700

    xen32: create initial mappings like 64-bit
    
    Rearrange the pagetable initialization to share code with the 64-bit
    kernel.  Rather than deferring anything to pagetable_setup_start, just
    set up an initial pagetable in swapper_pg_dir early at startup, and
    create an additional 8MB of physical memory mappings.  This matches
    the native head_32.S mappings to a large degree, and allows the rest
    of the pagetable setup to continue without much Xen vs. native
    difference.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e9e3bafe48cf..19c12a6c7311 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -854,50 +854,6 @@ static __init void xen_set_pte_init(pte_t *ptep, pte_t pte)
 
 static __init void xen_pagetable_setup_start(pgd_t *base)
 {
-#ifdef CONFIG_X86_32
-	pgd_t *xen_pgd = (pgd_t *)xen_start_info->pt_base;
-	int i;
-
-	init_mm.pgd = base;
-	/*
-	 * copy top-level of Xen-supplied pagetable into place.  This
-	 * is a stand-in while we copy the pmd pages.
-	 */
-	memcpy(base, xen_pgd, PTRS_PER_PGD * sizeof(pgd_t));
-
-	/*
-	 * For PAE, need to allocate new pmds, rather than
-	 * share Xen's, since Xen doesn't like pmd's being
-	 * shared between address spaces.
-	 */
-	for (i = 0; i < PTRS_PER_PGD; i++) {
-		if (pgd_val_ma(xen_pgd[i]) & _PAGE_PRESENT) {
-			pmd_t *pmd = (pmd_t *)alloc_bootmem_low_pages(PAGE_SIZE);
-
-			memcpy(pmd, (void *)pgd_page_vaddr(xen_pgd[i]),
-			       PAGE_SIZE);
-
-			make_lowmem_page_readonly(pmd);
-
-			set_pgd(&base[i], __pgd(1 + __pa(pmd)));
-		} else
-			pgd_clear(&base[i]);
-	}
-
-	/* make sure zero_page is mapped RO so we can use it in pagetables */
-	make_lowmem_page_readonly(empty_zero_page);
-	make_lowmem_page_readonly(base);
-	/*
-	 * Switch to new pagetable.  This is done before
-	 * pagetable_init has done anything so that the new pages
-	 * added to the table can be prepared properly for Xen.
-	 */
-	xen_write_cr3(__pa(base));
-
-	/* Unpin initial Xen pagetable */
-	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE,
-			  PFN_DOWN(__pa(xen_start_info->pt_base)));
-#endif	/* CONFIG_X86_32 */
 }
 
 void xen_setup_shared_info(void)
@@ -936,12 +892,6 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 	pv_mmu_ops.set_pte = xen_set_pte;
 
 	xen_setup_shared_info();
-
-#ifdef CONFIG_X86_32
-	/* Actually pin the pagetable down, but we can't set PG_pinned
-	   yet because the page structures don't exist yet. */
-	pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(__pa(base)));
-#endif
 }
 
 static __init void xen_post_allocator_init(void)
@@ -1299,14 +1249,17 @@ static void __init xen_reserve_top(void)
 #endif	/* CONFIG_X86_32 */
 }
 
-#ifdef CONFIG_X86_64
 /*
  * Like __va(), but returns address in the kernel mapping (which is
  * all we have until the physical memory mapping has been set up.
  */
 static void *__ka(phys_addr_t paddr)
 {
+#ifdef CONFIG_X86_64
 	return (void *)(paddr + __START_KERNEL_map);
+#else
+	return __va(paddr);
+#endif
 }
 
 /* Convert a machine address to physical address */
@@ -1326,6 +1279,7 @@ static void *m2v(phys_addr_t maddr)
 	return __ka(m2p(maddr));
 }
 
+#ifdef CONFIG_X86_64
 static void walk(pgd_t *pgd, unsigned long addr)
 {
 	unsigned l4idx = pgd_index(addr);
@@ -1356,13 +1310,14 @@ static void walk(pgd_t *pgd, unsigned long addr)
 	xen_raw_printk("  l1: %016lx\n", l1.pte);
 	xen_raw_printk("      %016lx\n", pte_val(l1));
 }
+#endif
 
 static void set_page_prot(void *addr, pgprot_t prot)
 {
 	unsigned long pfn = __pa(addr) >> PAGE_SHIFT;
 	pte_t pte = pfn_pte(pfn, prot);
 
-	xen_raw_printk("addr=%p pfn=%lx mfn=%lx prot=%016x pte=%016x\n",
+	xen_raw_printk("addr=%p pfn=%lx mfn=%lx prot=%016llx pte=%016llx\n",
 		       addr, pfn, get_phys_to_machine(pfn),
 		       pgprot_val(prot), pte.pte);
 
@@ -1370,17 +1325,6 @@ static void set_page_prot(void *addr, pgprot_t prot)
 		BUG();
 }
 
-static void convert_pfn_mfn(void *v)
-{
-	pte_t *pte = v;
-	int i;
-
-	/* All levels are converted the same way, so just treat them
-	   as ptes. */
-	for(i = 0; i < PTRS_PER_PTE; i++)
-		pte[i] = xen_make_pte(pte[i].pte);
-}
-
 /*
  * Identity map, in addition to plain kernel map.  This needs to be
  * large enough to allocate page table pages to allocate the rest.
@@ -1388,7 +1332,7 @@ static void convert_pfn_mfn(void *v)
  */
 static pte_t level1_ident_pgt[PTRS_PER_PTE * 4] __page_aligned_bss;
 
-static __init void xen_map_identity_early(unsigned long max_pfn)
+static __init void xen_map_identity_early(pmd_t *pmd, unsigned long max_pfn)
 {
 	unsigned pmdidx, pteidx;
 	unsigned ident_pte;
@@ -1399,11 +1343,9 @@ static __init void xen_map_identity_early(unsigned long max_pfn)
 	for(pmdidx = 0; pmdidx < PTRS_PER_PMD && pfn < max_pfn; pmdidx++) {
 		pte_t *pte_page;
 
-		BUG_ON(level2_ident_pgt[pmdidx].pmd != level2_kernel_pgt[pmdidx].pmd);
-
 		/* Reuse or allocate a page of ptes */
-		if (pmd_present(level2_ident_pgt[pmdidx]))
-			pte_page = m2v(level2_ident_pgt[pmdidx].pmd);
+		if (pmd_present(pmd[pmdidx]))
+			pte_page = m2v(pmd[pmdidx].pmd);
 		else {
 			/* Check for free pte pages */
 			if (ident_pte == ARRAY_SIZE(level1_ident_pgt))
@@ -1412,9 +1354,7 @@ static __init void xen_map_identity_early(unsigned long max_pfn)
 			pte_page = &level1_ident_pgt[ident_pte];
 			ident_pte += PTRS_PER_PTE;
 
-			/* Install new l1 in l2(s) */
-			level2_ident_pgt[pmdidx] = __pmd(__pa(pte_page) | _PAGE_TABLE);
-			level2_kernel_pgt[pmdidx] = level2_ident_pgt[pmdidx];
+			pmd[pmdidx] = __pmd(__pa(pte_page) | _PAGE_TABLE);
 		}
 
 		/* Install mappings */
@@ -1434,6 +1374,20 @@ static __init void xen_map_identity_early(unsigned long max_pfn)
 
 	for(pteidx = 0; pteidx < ident_pte; pteidx += PTRS_PER_PTE)
 		set_page_prot(&level1_ident_pgt[pteidx], PAGE_KERNEL_RO);
+
+	set_page_prot(pmd, PAGE_KERNEL_RO);
+}
+
+#ifdef CONFIG_X86_64
+static void convert_pfn_mfn(void *v)
+{
+	pte_t *pte = v;
+	int i;
+
+	/* All levels are converted the same way, so just treat them
+	   as ptes. */
+	for(i = 0; i < PTRS_PER_PTE; i++)
+		pte[i] = xen_make_pte(pte[i].pte);
 }
 
 /*
@@ -1471,18 +1425,18 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pf
 	memcpy(level2_fixmap_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
 
 	/* Set up identity map */
-	xen_map_identity_early(max_pfn);
+	xen_map_identity_early(level2_ident_pgt, max_pfn);
 
 	/* Make pagetable pieces RO */
 	set_page_prot(init_level4_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level3_ident_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level3_kernel_pgt, PAGE_KERNEL_RO);
-	set_page_prot(level2_ident_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level2_fixmap_pgt, PAGE_KERNEL_RO);
 
 	/* Pin down new L4 */
-	pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE, PFN_DOWN(__pa_symbol(init_level4_pgt)));
+	pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE,
+			  PFN_DOWN(__pa_symbol(init_level4_pgt)));
 
 	/* Unpin Xen-provided one */
 	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));
@@ -1498,17 +1452,37 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pf
 
 	return pgd;
 }
-#else
+#else	/* !CONFIG_X86_64 */
+static pmd_t level2_kernel_pgt[PTRS_PER_PMD] __page_aligned_bss;
+
 static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)
 {
+	pmd_t *kernel_pmd;
+
 	init_pg_tables_start = __pa(pgd);
 	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
 	max_pfn_mapped = PFN_DOWN(init_pg_tables_end + 512*1024);
 
-	x86_write_percpu(xen_cr3, __pa(pgd));
-	x86_write_percpu(xen_current_cr3, __pa(pgd));
+	kernel_pmd = m2v(pgd[KERNEL_PGD_BOUNDARY].pgd);
+	memcpy(level2_kernel_pgt, kernel_pmd, sizeof(pmd_t) * PTRS_PER_PMD);
 
-	return pgd;
+	xen_map_identity_early(level2_kernel_pgt, max_pfn);
+
+	memcpy(swapper_pg_dir, pgd, sizeof(pgd_t) * PTRS_PER_PGD);
+	set_pgd(&swapper_pg_dir[KERNEL_PGD_BOUNDARY],
+			__pgd(__pa(level2_kernel_pgt) | _PAGE_PRESENT));
+
+	set_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);
+	set_page_prot(swapper_pg_dir, PAGE_KERNEL_RO);
+	set_page_prot(empty_zero_page, PAGE_KERNEL_RO);
+
+	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));
+
+	xen_write_cr3(__pa(swapper_pg_dir));
+
+	pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(__pa(swapper_pg_dir)));
+
+	return swapper_pg_dir;
 }
 #endif	/* CONFIG_X86_64 */
 

commit d114e1981cc1a51131230993a082c27c79ab370a
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:52 2008 -0700

    xen64: map an initial chunk of physical memory
    
    Early in boot, map a chunk of extra physical memory for use later on.
    We need a pool of mapped pages to allocate further pages to construct
    pagetables mapping all physical memory.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 392450787aa9..e9e3bafe48cf 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1381,6 +1381,61 @@ static void convert_pfn_mfn(void *v)
 		pte[i] = xen_make_pte(pte[i].pte);
 }
 
+/*
+ * Identity map, in addition to plain kernel map.  This needs to be
+ * large enough to allocate page table pages to allocate the rest.
+ * Each page can map 2MB.
+ */
+static pte_t level1_ident_pgt[PTRS_PER_PTE * 4] __page_aligned_bss;
+
+static __init void xen_map_identity_early(unsigned long max_pfn)
+{
+	unsigned pmdidx, pteidx;
+	unsigned ident_pte;
+	unsigned long pfn;
+
+	ident_pte = 0;
+	pfn = 0;
+	for(pmdidx = 0; pmdidx < PTRS_PER_PMD && pfn < max_pfn; pmdidx++) {
+		pte_t *pte_page;
+
+		BUG_ON(level2_ident_pgt[pmdidx].pmd != level2_kernel_pgt[pmdidx].pmd);
+
+		/* Reuse or allocate a page of ptes */
+		if (pmd_present(level2_ident_pgt[pmdidx]))
+			pte_page = m2v(level2_ident_pgt[pmdidx].pmd);
+		else {
+			/* Check for free pte pages */
+			if (ident_pte == ARRAY_SIZE(level1_ident_pgt))
+				break;
+
+			pte_page = &level1_ident_pgt[ident_pte];
+			ident_pte += PTRS_PER_PTE;
+
+			/* Install new l1 in l2(s) */
+			level2_ident_pgt[pmdidx] = __pmd(__pa(pte_page) | _PAGE_TABLE);
+			level2_kernel_pgt[pmdidx] = level2_ident_pgt[pmdidx];
+		}
+
+		/* Install mappings */
+		for(pteidx = 0; pteidx < PTRS_PER_PTE; pteidx++, pfn++) {
+			pte_t pte;
+
+			if (pfn > max_pfn_mapped)
+				max_pfn_mapped = pfn;
+
+			if (!pte_none(pte_page[pteidx]))
+				continue;
+
+			pte = pfn_pte(pfn, PAGE_KERNEL_EXEC);
+			pte_page[pteidx] = pte;
+		}
+	}
+
+	for(pteidx = 0; pteidx < ident_pte; pteidx += PTRS_PER_PTE)
+		set_page_prot(&level1_ident_pgt[pteidx], PAGE_KERNEL_RO);
+}
+
 /*
  * Set up the inital kernel pagetable.
  *
@@ -1392,7 +1447,7 @@ static void convert_pfn_mfn(void *v)
  * of the physical mapping once some sort of allocator has been set
  * up.
  */
-static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd)
+static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)
 {
 	pud_t *l3;
 	pmd_t *l2;
@@ -1415,6 +1470,9 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd)
 	l2 = m2v(l3[pud_index(__START_KERNEL_map + PMD_SIZE)].pud);
 	memcpy(level2_fixmap_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
 
+	/* Set up identity map */
+	xen_map_identity_early(max_pfn);
+
 	/* Make pagetable pieces RO */
 	set_page_prot(init_level4_pgt, PAGE_KERNEL_RO);
 	set_page_prot(level3_ident_pgt, PAGE_KERNEL_RO);
@@ -1424,7 +1482,7 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd)
 	set_page_prot(level2_fixmap_pgt, PAGE_KERNEL_RO);
 
 	/* Pin down new L4 */
-	pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE, PFN_DOWN(__pa(init_level4_pgt)));
+	pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE, PFN_DOWN(__pa_symbol(init_level4_pgt)));
 
 	/* Unpin Xen-provided one */
 	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));
@@ -1433,19 +1491,23 @@ static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd)
 	pgd = init_level4_pgt;
 	xen_write_cr3(__pa(pgd));
 
-	max_pfn_mapped = PFN_DOWN(__pa(pgd) +
-				  xen_start_info->nr_pt_frames*PAGE_SIZE +
-				  512*1024);
+	reserve_early(__pa(xen_start_info->pt_base),
+		      __pa(xen_start_info->pt_base +
+			   xen_start_info->nr_pt_frames * PAGE_SIZE),
+		      "XEN PAGETABLES");
 
 	return pgd;
 }
 #else
-static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd)
+static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)
 {
 	init_pg_tables_start = __pa(pgd);
 	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
 	max_pfn_mapped = PFN_DOWN(init_pg_tables_end + 512*1024);
 
+	x86_write_percpu(xen_cr3, __pa(pgd));
+	x86_write_percpu(xen_current_cr3, __pa(pgd));
+
 	return pgd;
 }
 #endif	/* CONFIG_X86_64 */
@@ -1502,15 +1564,12 @@ asmlinkage void __init xen_start_kernel(void)
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
 
 	xen_raw_console_write("mapping kernel into physical memory\n");
-	pgd = xen_setup_kernel_pagetable(pgd);
+	pgd = xen_setup_kernel_pagetable(pgd, xen_start_info->nr_pages);
 
 	init_mm.pgd = pgd;
 
 	/* keep using Xen gdt for now; no urgent need to change it */
 
-	x86_write_percpu(xen_cr3, __pa(pgd));
-	x86_write_percpu(xen_current_cr3, __pa(pgd));
-
 	pv_info.kernel_rpl = 1;
 	if (xen_feature(XENFEAT_supervisor_mode_kernel))
 		pv_info.kernel_rpl = 0;

commit 22911b3f1cf5431058e56b1727e8ef77be5e0ac9
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:51 2008 -0700

    xen64: 64-bit starts using set_pte from very early
    
    It also doesn't need the 32-bit hack version of set_pte for initial
    pagetable construction, so just make it use the real thing.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a991ee7ade9e..392450787aa9 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1194,7 +1194,11 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.kmap_atomic_pte = xen_kmap_atomic_pte,
 #endif
 
+#ifdef CONFIG_X86_64
+	.set_pte = xen_set_pte,
+#else
 	.set_pte = xen_set_pte_init,
+#endif
 	.set_pte_at = xen_set_pte_at,
 	.set_pmd = xen_set_pmd_hyper,
 

commit 084a2a4e7656209ea93aac9778defa03213ca31d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:50 2008 -0700

    xen64: early mapping setup
    
    Set up the initial pagetables to map the kernel mapping into the
    physical mapping space.  This makes __va() usable, since it requires
    physical mappings.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 2b7bea3bb6f3..a991ee7ade9e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -33,6 +33,7 @@
 #include <xen/interface/sched.h>
 #include <xen/features.h>
 #include <xen/page.h>
+#include <xen/hvc-console.h>
 
 #include <asm/paravirt.h>
 #include <asm/page.h>
@@ -1294,6 +1295,157 @@ static void __init xen_reserve_top(void)
 #endif	/* CONFIG_X86_32 */
 }
 
+#ifdef CONFIG_X86_64
+/*
+ * Like __va(), but returns address in the kernel mapping (which is
+ * all we have until the physical memory mapping has been set up.
+ */
+static void *__ka(phys_addr_t paddr)
+{
+	return (void *)(paddr + __START_KERNEL_map);
+}
+
+/* Convert a machine address to physical address */
+static unsigned long m2p(phys_addr_t maddr)
+{
+	phys_addr_t paddr;
+
+	maddr &= PTE_MASK;
+	paddr = mfn_to_pfn(maddr >> PAGE_SHIFT) << PAGE_SHIFT;
+
+	return paddr;
+}
+
+/* Convert a machine address to kernel virtual */
+static void *m2v(phys_addr_t maddr)
+{
+	return __ka(m2p(maddr));
+}
+
+static void walk(pgd_t *pgd, unsigned long addr)
+{
+	unsigned l4idx = pgd_index(addr);
+	unsigned l3idx = pud_index(addr);
+	unsigned l2idx = pmd_index(addr);
+	unsigned l1idx = pte_index(addr);
+	pgd_t l4;
+	pud_t l3;
+	pmd_t l2;
+	pte_t l1;
+
+	xen_raw_printk("walk %p, %lx -> %d %d %d %d\n",
+		       pgd, addr, l4idx, l3idx, l2idx, l1idx);
+
+	l4 = pgd[l4idx];
+	xen_raw_printk("  l4: %016lx\n", l4.pgd);
+	xen_raw_printk("      %016lx\n", pgd_val(l4));
+
+	l3 = ((pud_t *)(m2v(l4.pgd)))[l3idx];
+	xen_raw_printk("  l3: %016lx\n", l3.pud);
+	xen_raw_printk("      %016lx\n", pud_val(l3));
+
+	l2 = ((pmd_t *)(m2v(l3.pud)))[l2idx];
+	xen_raw_printk("  l2: %016lx\n", l2.pmd);
+	xen_raw_printk("      %016lx\n", pmd_val(l2));
+
+	l1 = ((pte_t *)(m2v(l2.pmd)))[l1idx];
+	xen_raw_printk("  l1: %016lx\n", l1.pte);
+	xen_raw_printk("      %016lx\n", pte_val(l1));
+}
+
+static void set_page_prot(void *addr, pgprot_t prot)
+{
+	unsigned long pfn = __pa(addr) >> PAGE_SHIFT;
+	pte_t pte = pfn_pte(pfn, prot);
+
+	xen_raw_printk("addr=%p pfn=%lx mfn=%lx prot=%016x pte=%016x\n",
+		       addr, pfn, get_phys_to_machine(pfn),
+		       pgprot_val(prot), pte.pte);
+
+	if (HYPERVISOR_update_va_mapping((unsigned long)addr, pte, 0))
+		BUG();
+}
+
+static void convert_pfn_mfn(void *v)
+{
+	pte_t *pte = v;
+	int i;
+
+	/* All levels are converted the same way, so just treat them
+	   as ptes. */
+	for(i = 0; i < PTRS_PER_PTE; i++)
+		pte[i] = xen_make_pte(pte[i].pte);
+}
+
+/*
+ * Set up the inital kernel pagetable.
+ *
+ * We can construct this by grafting the Xen provided pagetable into
+ * head_64.S's preconstructed pagetables.  We copy the Xen L2's into
+ * level2_ident_pgt, level2_kernel_pgt and level2_fixmap_pgt.  This
+ * means that only the kernel has a physical mapping to start with -
+ * but that's enough to get __va working.  We need to fill in the rest
+ * of the physical mapping once some sort of allocator has been set
+ * up.
+ */
+static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd)
+{
+	pud_t *l3;
+	pmd_t *l2;
+
+	/* Zap identity mapping */
+	init_level4_pgt[0] = __pgd(0);
+
+	/* Pre-constructed entries are in pfn, so convert to mfn */
+	convert_pfn_mfn(init_level4_pgt);
+	convert_pfn_mfn(level3_ident_pgt);
+	convert_pfn_mfn(level3_kernel_pgt);
+
+	l3 = m2v(pgd[pgd_index(__START_KERNEL_map)].pgd);
+	l2 = m2v(l3[pud_index(__START_KERNEL_map)].pud);
+
+	memcpy(level2_ident_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
+	memcpy(level2_kernel_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
+
+	l3 = m2v(pgd[pgd_index(__START_KERNEL_map + PMD_SIZE)].pgd);
+	l2 = m2v(l3[pud_index(__START_KERNEL_map + PMD_SIZE)].pud);
+	memcpy(level2_fixmap_pgt, l2, sizeof(pmd_t) * PTRS_PER_PMD);
+
+	/* Make pagetable pieces RO */
+	set_page_prot(init_level4_pgt, PAGE_KERNEL_RO);
+	set_page_prot(level3_ident_pgt, PAGE_KERNEL_RO);
+	set_page_prot(level3_kernel_pgt, PAGE_KERNEL_RO);
+	set_page_prot(level2_ident_pgt, PAGE_KERNEL_RO);
+	set_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);
+	set_page_prot(level2_fixmap_pgt, PAGE_KERNEL_RO);
+
+	/* Pin down new L4 */
+	pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE, PFN_DOWN(__pa(init_level4_pgt)));
+
+	/* Unpin Xen-provided one */
+	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));
+
+	/* Switch over */
+	pgd = init_level4_pgt;
+	xen_write_cr3(__pa(pgd));
+
+	max_pfn_mapped = PFN_DOWN(__pa(pgd) +
+				  xen_start_info->nr_pt_frames*PAGE_SIZE +
+				  512*1024);
+
+	return pgd;
+}
+#else
+static __init pgd_t *xen_setup_kernel_pagetable(pgd_t *pgd)
+{
+	init_pg_tables_start = __pa(pgd);
+	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
+	max_pfn_mapped = PFN_DOWN(init_pg_tables_end + 512*1024);
+
+	return pgd;
+}
+#endif	/* CONFIG_X86_64 */
+
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {
@@ -1336,32 +1488,29 @@ asmlinkage void __init xen_start_kernel(void)
 
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
-#ifdef CONFIG_X86_32
-	init_pg_tables_start = __pa(pgd);
-	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
-	max_pfn_mapped = (init_pg_tables_end + 512*1024) >> PAGE_SHIFT;
-#endif
+	/* Prevent unwanted bits from being set in PTEs. */
+	__supported_pte_mask &= ~_PAGE_GLOBAL;
+	if (!is_initial_xendomain())
+		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
+
+	/* Don't do the full vcpu_info placement stuff until we have a
+	   possible map and a non-dummy shared_info. */
+	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
+
+	xen_raw_console_write("mapping kernel into physical memory\n");
+	pgd = xen_setup_kernel_pagetable(pgd);
 
-	init_mm.pgd = pgd; /* use the Xen pagetables to start */
+	init_mm.pgd = pgd;
 
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 	x86_write_percpu(xen_cr3, __pa(pgd));
 	x86_write_percpu(xen_current_cr3, __pa(pgd));
 
-	/* Don't do the full vcpu_info placement stuff until we have a
-	   possible map and a non-dummy shared_info. */
-	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
-
 	pv_info.kernel_rpl = 1;
 	if (xen_feature(XENFEAT_supervisor_mode_kernel))
 		pv_info.kernel_rpl = 0;
 
-	/* Prevent unwanted bits from being set in PTEs. */
-	__supported_pte_mask &= ~_PAGE_GLOBAL;
-	if (!is_initial_xendomain())
-		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
-
 	/* set the limit of our address space */
 	xen_reserve_top();
 
@@ -1384,10 +1533,21 @@ asmlinkage void __init xen_start_kernel(void)
 		add_preferred_console("hvc", 0, NULL);
 	}
 
+	xen_raw_console_write("about to get started...\n");
+
+#if 0
+	xen_raw_printk("&boot_params=%p __pa(&boot_params)=%lx __va(__pa(&boot_params))=%lx\n",
+		       &boot_params, __pa_symbol(&boot_params),
+		       __va(__pa_symbol(&boot_params)));
+
+	walk(pgd, &boot_params);
+	walk(pgd, __va(__pa(&boot_params)));
+#endif
+
 	/* Start the world */
 #ifdef CONFIG_X86_32
 	i386_start_kernel();
 #else
-	x86_64_start_kernel((char *)&boot_params);
+	x86_64_start_reservations((char *)__pa_symbol(&boot_params));
 #endif
 }

commit 7d087b68d6ddb2398fb7f6e45990b7248de640ef
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:48 2008 -0700

    xen: cpu_detect is 32-bit only
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index dbe3549fad40..2b7bea3bb6f3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1365,12 +1365,12 @@ asmlinkage void __init xen_start_kernel(void)
 	/* set the limit of our address space */
 	xen_reserve_top();
 
+#ifdef CONFIG_X86_32
 	/* set up basic CPUID stuff */
 	cpu_detect(&new_cpu_data);
-#ifdef CONFIG_X86_32
 	new_cpu_data.hard_math = 1;
-#endif
 	new_cpu_data.x86_capability[0] = cpuid_edx(1);
+#endif
 
 	/* Poke various useful things into boot_params */
 	boot_params.hdr.type_of_loader = (9 << 4) | 0;

commit 15664f968a95d8fbf4a0d7b462fcc20f88906bb3
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:47 2008 -0700

    xen64: use set_fixmap for shared_info structure
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f3f11acf7856..dbe3549fad40 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -902,18 +902,11 @@ static __init void xen_pagetable_setup_start(pgd_t *base)
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
-		unsigned long addr = fix_to_virt(FIX_PARAVIRT_BOOTMAP);
-
-		/*
-		 * Create a mapping for the shared info page.
-		 * Should be set_fixmap(), but shared_info is a machine
-		 * address with no corresponding pseudo-phys address.
-		 */
-		set_pte_mfn(addr,
-			    PFN_DOWN(xen_start_info->shared_info),
-			    PAGE_KERNEL);
-
-		HYPERVISOR_shared_info = (struct shared_info *)addr;
+		set_fixmap(FIX_PARAVIRT_BOOTMAP,
+			   xen_start_info->shared_info);
+
+		HYPERVISOR_shared_info =
+			(struct shared_info *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);
 	} else
 		HYPERVISOR_shared_info =
 			(struct shared_info *)__va(xen_start_info->shared_info);
@@ -1050,8 +1043,13 @@ static void xen_set_fixmap(unsigned idx, unsigned long phys, pgprot_t prot)
 #ifdef CONFIG_X86_F00F_BUG
 	case FIX_F00F_IDT:
 #endif
+#ifdef CONFIG_X86_32
 	case FIX_WP_TEST:
 	case FIX_VDSO:
+	case FIX_KMAP_BEGIN ... FIX_KMAP_END:
+#else
+	case VSYSCALL_LAST_PAGE ... VSYSCALL_FIRST_PAGE:
+#endif
 #ifdef CONFIG_X86_LOCAL_APIC
 	case FIX_APIC_BASE:	/* maps dummy local APIC */
 #endif

commit 5b09b2876ed1a8e34a0da8f069575fc6174e2077
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:42 2008 -0700

    x86_64: add workaround for no %gs-based percpu
    
    As a stopgap until Mike Travis's x86-64 gs-based percpu patches are
    ready, provide workaround functions for x86_read/write_percpu for
    Xen's use.
    
    Specifically, this means that we can't really make use of vcpu
    placement, because we can't use a single gs-based memory access to get
    to vcpu fields.  So disable all that for now.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a85f447b8d00..f3f11acf7856 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -971,6 +971,7 @@ void xen_setup_vcpu_info_placement(void)
 
 	/* xen_vcpu_setup managed to place the vcpu_info within the
 	   percpu area for all cpus, so make use of it */
+#ifdef CONFIG_X86_32
 	if (have_vcpu_info_placement) {
 		printk(KERN_INFO "Xen: using vcpu_info placement\n");
 
@@ -980,6 +981,7 @@ void xen_setup_vcpu_info_placement(void)
 		pv_irq_ops.irq_enable = xen_irq_enable_direct;
 		pv_mmu_ops.read_cr2 = xen_read_cr2_direct;
 	}
+#endif
 }
 
 static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
@@ -1000,10 +1002,12 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 	goto patch_site
 
 	switch (type) {
+#ifdef CONFIG_X86_32
 		SITE(pv_irq_ops, irq_enable);
 		SITE(pv_irq_ops, irq_disable);
 		SITE(pv_irq_ops, save_fl);
 		SITE(pv_irq_ops, restore_fl);
+#endif /* CONFIG_X86_32 */
 #undef SITE
 
 	patch_site:
@@ -1323,6 +1327,7 @@ asmlinkage void __init xen_start_kernel(void)
 #ifdef CONFIG_X86_64
 	/* Disable until direct per-cpu data access. */
 	have_vcpu_info_placement = 0;
+	x86_64_init_pda();
 #endif
 
 	xen_smp_init();

commit a9e7062d7339f1a1df2b6d7e5d595c7d55b56bfb
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:41 2008 -0700

    xen: move smp setup into smp.c
    
    Move all the smp_ops setup into smp.c, allowing a lot of things to
    become static.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ada2e1a141df..a85f447b8d00 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1237,21 +1237,6 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.set_fixmap = xen_set_fixmap,
 };
 
-#ifdef CONFIG_SMP
-static const struct smp_ops xen_smp_ops __initdata = {
-	.smp_prepare_boot_cpu = xen_smp_prepare_boot_cpu,
-	.smp_prepare_cpus = xen_smp_prepare_cpus,
-	.cpu_up = xen_cpu_up,
-	.smp_cpus_done = xen_smp_cpus_done,
-
-	.smp_send_stop = xen_smp_send_stop,
-	.smp_send_reschedule = xen_smp_send_reschedule,
-
-	.send_call_func_ipi = xen_smp_send_call_function_ipi,
-	.send_call_func_single_ipi = xen_smp_send_call_function_single_ipi,
-};
-#endif	/* CONFIG_SMP */
-
 static void xen_reboot(int reason)
 {
 	struct sched_shutdown r = { .reason = reason };
@@ -1340,9 +1325,7 @@ asmlinkage void __init xen_start_kernel(void)
 	have_vcpu_info_placement = 0;
 #endif
 
-#ifdef CONFIG_SMP
-	smp_ops = xen_smp_ops;
-#endif
+	xen_smp_init();
 
 	/* Get mfn list */
 	if (!xen_feature(XENFEAT_auto_translated_physmap))

commit f5d36de069f4b343f64e858e7377cfc9c772c4fb
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:39 2008 -0700

    xen64: random ifdefs to mask out 32-bit only code
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index afb047e30bdc..ada2e1a141df 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1296,6 +1296,7 @@ static const struct machine_ops __initdata xen_machine_ops = {
 
 static void __init xen_reserve_top(void)
 {
+#ifdef CONFIG_X86_32
 	unsigned long top = HYPERVISOR_VIRT_START;
 	struct xen_platform_parameters pp;
 
@@ -1303,6 +1304,7 @@ static void __init xen_reserve_top(void)
 		top = pp.virt_start;
 
 	reserve_top_address(-top + 2 * PAGE_SIZE);
+#endif	/* CONFIG_X86_32 */
 }
 
 /* First C function to be called on Xen boot */
@@ -1333,6 +1335,11 @@ asmlinkage void __init xen_start_kernel(void)
 
 	machine_ops = xen_machine_ops;
 
+#ifdef CONFIG_X86_64
+	/* Disable until direct per-cpu data access. */
+	have_vcpu_info_placement = 0;
+#endif
+
 #ifdef CONFIG_SMP
 	smp_ops = xen_smp_ops;
 #endif
@@ -1343,9 +1350,11 @@ asmlinkage void __init xen_start_kernel(void)
 
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
+#ifdef CONFIG_X86_32
 	init_pg_tables_start = __pa(pgd);
 	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
 	max_pfn_mapped = (init_pg_tables_end + 512*1024) >> PAGE_SHIFT;
+#endif
 
 	init_mm.pgd = pgd; /* use the Xen pagetables to start */
 
@@ -1372,7 +1381,9 @@ asmlinkage void __init xen_start_kernel(void)
 
 	/* set up basic CPUID stuff */
 	cpu_detect(&new_cpu_data);
+#ifdef CONFIG_X86_32
 	new_cpu_data.hard_math = 1;
+#endif
 	new_cpu_data.x86_capability[0] = cpuid_edx(1);
 
 	/* Poke various useful things into boot_params */
@@ -1388,5 +1399,9 @@ asmlinkage void __init xen_start_kernel(void)
 	}
 
 	/* Start the world */
+#ifdef CONFIG_X86_32
 	i386_start_kernel();
+#else
+	x86_64_start_kernel((char *)&boot_params);
+#endif
 }

commit f6e587325b3bc7e5c829a407ddc25b52c1e73851
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:38 2008 -0700

    xen64: add extra pv_mmu_ops
    
    We need extra pv_mmu_ops for 64-bit, to deal with the extra level of
    pagetable.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c5f0b40aa39d..afb047e30bdc 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -803,6 +803,18 @@ static void xen_release_pmd(u32 pfn)
 	xen_release_ptpage(pfn, PT_PMD);
 }
 
+#if PAGETABLE_LEVELS == 4
+static void xen_alloc_pud(struct mm_struct *mm, u32 pfn)
+{
+	xen_alloc_ptpage(mm, pfn, PT_PUD);
+}
+
+static void xen_release_pud(u32 pfn)
+{
+	xen_release_ptpage(pfn, PT_PUD);
+}
+#endif
+
 #ifdef CONFIG_HIGHPTE
 static void *xen_kmap_atomic_pte(struct page *page, enum km_type type)
 {
@@ -922,6 +934,11 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 	pv_mmu_ops.alloc_pmd = xen_alloc_pmd;
 	pv_mmu_ops.release_pte = xen_release_pte;
 	pv_mmu_ops.release_pmd = xen_release_pmd;
+#if PAGETABLE_LEVELS == 4
+	pv_mmu_ops.alloc_pud = xen_alloc_pud;
+	pv_mmu_ops.release_pud = xen_release_pud;
+#endif
+
 	pv_mmu_ops.set_pte = xen_set_pte;
 
 	xen_setup_shared_info();
@@ -937,6 +954,9 @@ static __init void xen_post_allocator_init(void)
 {
 	pv_mmu_ops.set_pmd = xen_set_pmd;
 	pv_mmu_ops.set_pud = xen_set_pud;
+#if PAGETABLE_LEVELS == 4
+	pv_mmu_ops.set_pgd = xen_set_pgd;
+#endif
 
 	xen_mark_init_mm_pinned();
 }
@@ -1185,15 +1205,26 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.make_pte = xen_make_pte,
 	.make_pgd = xen_make_pgd,
 
+#ifdef CONFIG_X86_PAE
 	.set_pte_atomic = xen_set_pte_atomic,
 	.set_pte_present = xen_set_pte_at,
-	.set_pud = xen_set_pud_hyper,
 	.pte_clear = xen_pte_clear,
 	.pmd_clear = xen_pmd_clear,
+#endif	/* CONFIG_X86_PAE */
+	.set_pud = xen_set_pud_hyper,
 
 	.make_pmd = xen_make_pmd,
 	.pmd_val = xen_pmd_val,
 
+#if PAGETABLE_LEVELS == 4
+	.pud_val = xen_pud_val,
+	.make_pud = xen_make_pud,
+	.set_pgd = xen_set_pgd_hyper,
+
+	.alloc_pud = xen_alloc_pte_init,
+	.release_pud = xen_release_pte_init,
+#endif	/* PAGETABLE_LEVELS == 4 */
+
 	.activate_mm = xen_activate_mm,
 	.dup_mmap = xen_dup_mmap,
 	.exit_mmap = xen_exit_mmap,

commit 851fa3c4e7b50d6a946d8b4c0a68683b5e56b2f1
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:33 2008 -0700

    xen: define set_pte from the outset
    
    We need set_pte to work from a relatively early point, so enable it
    from the start.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index eaab6c9b4a84..c5f0b40aa39d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -845,9 +845,6 @@ static __init void xen_pagetable_setup_start(pgd_t *base)
 	pgd_t *xen_pgd = (pgd_t *)xen_start_info->pt_base;
 	int i;
 
-	/* special set_pte for pagetable initialization */
-	pv_mmu_ops.set_pte = xen_set_pte_init;
-
 	init_mm.pgd = base;
 	/*
 	 * copy top-level of Xen-supplied pagetable into place.  This
@@ -1174,7 +1171,7 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.kmap_atomic_pte = xen_kmap_atomic_pte,
 #endif
 
-	.set_pte = NULL,	/* see xen_pagetable_setup_* */
+	.set_pte = xen_set_pte_init,
 	.set_pte_at = xen_set_pte_at,
 	.set_pmd = xen_set_pmd_hyper,
 

commit a312b37b2a212fd2e227d1d6321f903b91b65ec7
Author: Eduardo Habkost <ehabkost@redhat.com>
Date:   Tue Jul 8 15:06:23 2008 -0700

    x86/paravirt: call paravirt_pagetable_setup_{start, done}
    
    Call paravirt_pagetable_setup_{start,done}
    
    These paravirt_ops functions were not being called on x86_64.
    
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bb508456ef52..eaab6c9b4a84 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -841,6 +841,7 @@ static __init void xen_set_pte_init(pte_t *ptep, pte_t pte)
 
 static __init void xen_pagetable_setup_start(pgd_t *base)
 {
+#ifdef CONFIG_X86_32
 	pgd_t *xen_pgd = (pgd_t *)xen_start_info->pt_base;
 	int i;
 
@@ -886,6 +887,7 @@ static __init void xen_pagetable_setup_start(pgd_t *base)
 	/* Unpin initial Xen pagetable */
 	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE,
 			  PFN_DOWN(__pa(xen_start_info->pt_base)));
+#endif	/* CONFIG_X86_32 */
 }
 
 void xen_setup_shared_info(void)
@@ -927,9 +929,11 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 
 	xen_setup_shared_info();
 
+#ifdef CONFIG_X86_32
 	/* Actually pin the pagetable down, but we can't set PG_pinned
 	   yet because the page structures don't exist yet. */
 	pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(__pa(base)));
+#endif
 }
 
 static __init void xen_post_allocator_init(void)

commit 1a781a777b2f6ac46523fe92396215762ced624d
Merge: b9d2252c1e44 42a2f217a5e3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 15 21:55:59 2008 +0200

    Merge branch 'generic-ipi' into generic-ipi-for-linus
    
    Conflicts:
    
            arch/powerpc/Kconfig
            arch/s390/kernel/time.c
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/cpu/perfctr-watchdog.c
            arch/x86/kernel/i8259_64.c
            arch/x86/kernel/ldt.c
            arch/x86/kernel/nmi_64.c
            arch/x86/kernel/smpboot.c
            arch/x86/xen/smp.c
            include/asm-x86/hw_irq_32.h
            include/asm-x86/hw_irq_64.h
            include/asm-x86/mach-default/irq_vectors.h
            include/asm-x86/mach-voyager/irq_vectors.h
            include/asm-x86/smp.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 94a8c3c2437c8946f1b6c8e0b2c560a7db8ed3c6
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Jul 13 22:19:35 2008 -0700

    x86: let 32bit use apic_ops too - fix
    
    fix for pv - clean up the namespace there too.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 54e255667530..d11dda7ebd7a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -559,7 +559,6 @@ static void xen_apic_write(u32 reg, u32 val)
 	WARN_ON(1);
 }
 
-#ifdef CONFIG_X86_64
 static u64 xen_apic_icr_read(void)
 {
 	return 0;
@@ -576,6 +575,11 @@ static void xen_apic_wait_icr_idle(void)
         return;
 }
 
+static u32 xen_safe_apic_wait_icr_idle(void)
+{
+        return 0;
+}
+
 static struct apic_ops xen_basic_apic_ops = {
 	.read = xen_apic_read,
 	.write = xen_apic_write,
@@ -583,9 +587,8 @@ static struct apic_ops xen_basic_apic_ops = {
 	.icr_read = xen_apic_icr_read,
 	.icr_write = xen_apic_icr_write,
 	.wait_icr_idle = xen_apic_wait_icr_idle,
-	.safe_wait_icr_idle = xen_apic_wait_icr_idle,
+	.safe_wait_icr_idle = xen_safe_apic_wait_icr_idle,
 };
-#endif
 
 #endif
 
@@ -1159,11 +1162,6 @@ static const struct pv_irq_ops xen_irq_ops __initdata = {
 
 static const struct pv_apic_ops xen_apic_ops __initdata = {
 #ifdef CONFIG_X86_LOCAL_APIC
-#ifndef CONFIG_X86_64
-	.apic_write = xen_apic_write,
-	.apic_write_atomic = xen_apic_write,
-	.apic_read = xen_apic_read,
-#endif
 	.setup_boot_clock = paravirt_nop,
 	.setup_secondary_clock = paravirt_nop,
 	.startup_ipi_hook = paravirt_nop,
@@ -1322,9 +1320,10 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_irq_ops = xen_irq_ops;
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
-#ifdef CONFIG_X86_64
+
+#ifdef CONFIG_X86_LOCAL_APIC
 	/*
-	 * for 64bit, set up the basic apic ops aswell.
+	 * set up the basic apic ops.
 	 */
 	apic_ops = &xen_basic_apic_ops;
 #endif

commit ad66dd340f561bdde2285992314d9e4fd9b6191e
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Jul 11 13:11:56 2008 -0700

    x2apic: xen64 paravirt basic apic ops
    
    Define the Xen specific basic apic ops, in additon to paravirt apic ops,
    with some misc warning fixes.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: akpm@linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index dcd4e51f2f16..54e255667530 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -548,16 +548,45 @@ static void xen_io_delay(void)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
-static u32 xen_apic_read(unsigned long reg)
+static u32 xen_apic_read(u32 reg)
 {
 	return 0;
 }
 
-static void xen_apic_write(unsigned long reg, u32 val)
+static void xen_apic_write(u32 reg, u32 val)
 {
 	/* Warn to see if there's any stray references */
 	WARN_ON(1);
 }
+
+#ifdef CONFIG_X86_64
+static u64 xen_apic_icr_read(void)
+{
+	return 0;
+}
+
+static void xen_apic_icr_write(u32 low, u32 id)
+{
+	/* Warn to see if there's any stray references */
+	WARN_ON(1);
+}
+
+static void xen_apic_wait_icr_idle(void)
+{
+        return;
+}
+
+static struct apic_ops xen_basic_apic_ops = {
+	.read = xen_apic_read,
+	.write = xen_apic_write,
+	.write_atomic = xen_apic_write,
+	.icr_read = xen_apic_icr_read,
+	.icr_write = xen_apic_icr_write,
+	.wait_icr_idle = xen_apic_wait_icr_idle,
+	.safe_wait_icr_idle = xen_apic_wait_icr_idle,
+};
+#endif
+
 #endif
 
 static void xen_flush_tlb(void)
@@ -1130,9 +1159,11 @@ static const struct pv_irq_ops xen_irq_ops __initdata = {
 
 static const struct pv_apic_ops xen_apic_ops __initdata = {
 #ifdef CONFIG_X86_LOCAL_APIC
+#ifndef CONFIG_X86_64
 	.apic_write = xen_apic_write,
 	.apic_write_atomic = xen_apic_write,
 	.apic_read = xen_apic_read,
+#endif
 	.setup_boot_clock = paravirt_nop,
 	.setup_secondary_clock = paravirt_nop,
 	.startup_ipi_hook = paravirt_nop,
@@ -1291,6 +1322,12 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_irq_ops = xen_irq_ops;
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
+#ifdef CONFIG_X86_64
+	/*
+	 * for 64bit, set up the basic apic ops aswell.
+	 */
+	apic_ops = &xen_basic_apic_ops;
+#endif
 
 	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {
 		pv_mmu_ops.ptep_modify_prot_start = xen_ptep_modify_prot_start;

commit e93ef949fd9a3f237aedfb8e64414b28980530b8
Author: Alok Kataria <akataria@vmware.com>
Date:   Tue Jul 1 11:43:36 2008 -0700

    x86: rename paravirtualized TSC functions
    
    Rename the paravirtualized calculate_cpu_khz to calibrate_tsc.
    In all cases, we actually calibrate_tsc and use that as the cpu_khz value.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Cc: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 3b980831602c..dcd4e51f2f16 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1062,7 +1062,7 @@ static const struct pv_time_ops xen_time_ops __initdata = {
 
 	.set_wallclock = xen_set_wallclock,
 	.get_wallclock = xen_get_wallclock,
-	.get_cpu_khz = xen_cpu_khz,
+	.get_tsc_khz = xen_tsc_khz,
 	.sched_clock = xen_sched_clock,
 };
 

commit fab58420ac0007a452b540cfb07923225ea4f48d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jun 25 00:19:31 2008 -0400

    x86/paravirt, 64-bit: add adjust_exception_frame
    
    64-bit Xen pushes a couple of extra words onto an exception frame.
    Add a hook to deal with them.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: xen-devel <xen-devel@lists.xensource.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 119c88fa769d..3b980831602c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1123,6 +1123,9 @@ static const struct pv_irq_ops xen_irq_ops __initdata = {
 	.irq_enable = xen_irq_enable,
 	.safe_halt = xen_safe_halt,
 	.halt = xen_halt,
+#ifdef CONFIG_X86_64
+	.adjust_exception_frame = paravirt_nop,
+#endif
 };
 
 static const struct pv_apic_ops xen_apic_ops __initdata = {

commit d75cd22fdd5f7d203fb60014d426942df33dd9a6
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jun 25 00:19:26 2008 -0400

    x86/paravirt: split sysret and sysexit
    
    Don't conflate sysret and sysexit; they're different instructions with
    different semantics, and may be in use at the same time (at least
    within the same kernel, depending on whether its an Intel or AMD
    system).
    
    sysexit - just return to userspace, does no register restoration of
        any kind; must explicitly atomically enable interrupts.
    
    sysret - reloads flags from r11, so no need to explicitly enable
        interrupts on 64-bit, responsible for restoring usermode %gs
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citirx.com>
    Cc: xen-devel <xen-devel@lists.xensource.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d62f14e20708..119c88fa769d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1089,7 +1089,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.read_pmc = native_read_pmc,
 
 	.iret = xen_iret,
-	.irq_enable_syscall_ret = xen_sysexit,
+	.irq_enable_sysexit = xen_sysexit,
 
 	.load_tr_desc = paravirt_nop,
 	.set_ldt = xen_set_ldt,

commit eba0045ff87bab465d3c80c289f3bf709c1800f5
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jun 25 00:19:12 2008 -0400

    x86/paravirt: add a pgd_alloc/free hooks
    
    Add hooks which are called at pgd_alloc/free time.  The pgd_alloc hook
    may return an error code, which if non-zero, causes the pgd allocation
    to be failed.  The hooks may be used to allocate/free auxillary
    per-pgd information.
    
    also fix:
    
    > * Ingo Molnar <mingo@elte.hu> wrote:
    >
    >  include/asm/pgalloc.h: In function ‘paravirt_pgd_free':
    >  include/asm/pgalloc.h:14: error: parameter name omitted
    >  arch/x86/kernel/entry_64.S: In file included from
    >  arch/x86/kernel/traps_64.c:51:include/asm/pgalloc.h: In function ‘paravirt_pgd_free':
    >  include/asm/pgalloc.h:14: error: parameter name omitted
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: xen-devel <xen-devel@lists.xensource.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 76ad1efaf09e..d62f14e20708 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -45,6 +45,7 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/reboot.h>
+#include <asm/pgalloc.h>
 
 #include "xen-ops.h"
 #include "mmu.h"
@@ -1153,6 +1154,9 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.pte_update = paravirt_nop,
 	.pte_update_defer = paravirt_nop,
 
+	.pgd_alloc = __paravirt_pgd_alloc,
+	.pgd_free = paravirt_nop,
+
 	.alloc_pte = xen_alloc_pte_init,
 	.release_pte = xen_release_pte_init,
 	.alloc_pmd = xen_alloc_pte_init,

commit 88a6846c70ad6bf33a545d554ace801d69e8a1a5
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jun 16 14:54:51 2008 -0700

    xen: set max_pfn_mapped
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: the arch/x86 maintainers <x86@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 316623cf411f..76ad1efaf09e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1304,6 +1304,7 @@ asmlinkage void __init xen_start_kernel(void)
 
 	init_pg_tables_start = __pa(pgd);
 	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
+	max_pfn_mapped = (init_pg_tables_end + 512*1024) >> PAGE_SHIFT;
 
 	init_mm.pgd = pgd; /* use the Xen pagetables to start */
 

commit 6236af82d8a989e150a02800c210eb61cb1e17be
Merge: e3ae0acf5924 8b7ef4ec5b1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 12:24:29 2008 +0200

    Merge branch 'x86/fixmap' into x86/devel
    
    Conflicts:
    
            arch/x86/mm/init_64.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 3de352bbd86f890dd0c5e1c09a6a1b0b29e0f8ce
Merge: 1b8ba39a3fad 9340e1ccdf7b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 11:14:58 2008 +0200

    Merge branch 'x86/mpparse' into x86/devel
    
    Conflicts:
    
            arch/x86/Kconfig
            arch/x86/kernel/io_apic_32.c
            arch/x86/kernel/setup_64.c
            arch/x86/mm/init_32.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 896395c290f902576270d84291c1f7f8bfbe339d
Merge: af1cf204ba2f 1b40a895df6c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 10:32:56 2008 +0200

    Merge branch 'linus' into tmp.x86.mpparse.new

commit 3b16cf874861436725c43ba0b68bdd799297be7c
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jun 26 11:21:54 2008 +0200

    x86: convert to generic helpers for IPI function calls
    
    This converts x86, x86-64, and xen to use the new helpers for
    smp_call_function() and friends, and adds support for
    smp_call_function_single().
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f09c1c69c37a..8e317782fe37 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1108,7 +1108,9 @@ static const struct smp_ops xen_smp_ops __initdata = {
 
 	.smp_send_stop = xen_smp_send_stop,
 	.smp_send_reschedule = xen_smp_send_reschedule,
-	.smp_call_function_mask = xen_smp_call_function_mask,
+
+	.send_call_func_ipi = xen_smp_send_call_function_ipi,
+	.send_call_func_single_ipi = xen_smp_send_call_function_single_ipi,
 };
 #endif	/* CONFIG_SMP */
 

commit e57778a1e30470c9f5b79e370511b9af29b59c48
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jun 16 04:30:02 2008 -0700

    xen: implement ptep_modify_prot_start/commit
    
    Xen has a pte update function which will update a pte while preserving
    its accessed and dirty bits.  This means that ptep_modify_prot_start() can be
    implemented as a simple read of the pte value.  The hardware may
    update the pte in the meantime, but ptep_modify_prot_commit() updates it while
    preserving any changes that may have happened in the meantime.
    
    The updates in ptep_modify_prot_commit() are batched if we're currently in lazy
    mmu mode.
    
    The mmu_update hypercall can take a batch of updates to perform, but
    this code doesn't make particular use of that feature, in favour of
    using generic multicall batching to get them all into the hypervisor.
    
    The net effect of this is that each mprotect pte update turns from two
    expensive trap-and-emulate faults into they hypervisor into a single
    hypercall whose cost is amortized in a batched multicall.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 0b7553cbc529..bd74229081c3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -168,7 +168,9 @@ static void __init xen_banner(void)
 {
 	printk(KERN_INFO "Booting paravirtualized kernel on %s\n",
 	       pv_info.name);
-	printk(KERN_INFO "Hypervisor signature: %s\n", xen_start_info->magic);
+	printk(KERN_INFO "Hypervisor signature: %s%s\n",
+	       xen_start_info->magic,
+	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
 }
 
 static void xen_cpuid(unsigned int *ax, unsigned int *bx,
@@ -1243,6 +1245,8 @@ asmlinkage void __init xen_start_kernel(void)
 
 	BUG_ON(memcmp(xen_start_info->magic, "xen-3", 5) != 0);
 
+	xen_setup_features();
+
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;
 	pv_init_ops = xen_init_ops;
@@ -1252,14 +1256,17 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
 
+	if (xen_feature(XENFEAT_mmu_pt_update_preserve_ad)) {
+		pv_mmu_ops.ptep_modify_prot_start = xen_ptep_modify_prot_start;
+		pv_mmu_ops.ptep_modify_prot_commit = xen_ptep_modify_prot_commit;
+	}
+
 	machine_ops = xen_machine_ops;
 
 #ifdef CONFIG_SMP
 	smp_ops = xen_smp_ops;
 #endif
 
-	xen_setup_features();
-
 	/* Get mfn list */
 	if (!xen_feature(XENFEAT_auto_translated_physmap))
 		xen_build_dynamic_phys_to_machine();

commit 08b882c627aeeeb3cfd3c4354f0d360d7949549d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jun 16 04:30:01 2008 -0700

    paravirt: add hooks for ptep_modify_prot_start/commit
    
    This patch adds paravirt-ops hooks in pv_mmu_ops for ptep_modify_prot_start and
    ptep_modify_prot_commit.  This allows the hypervisor-specific backends to
    implement these in some more efficient way.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 73fb0c4c150a..0b7553cbc529 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1138,6 +1138,9 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.set_pte_at = xen_set_pte_at,
 	.set_pmd = xen_set_pmd_hyper,
 
+	.ptep_modify_prot_start = __ptep_modify_prot_start,
+	.ptep_modify_prot_commit = __ptep_modify_prot_commit,
+
 	.pte_val = xen_pte_val,
 	.pte_flags = native_pte_val,
 	.pgd_val = xen_pgd_val,

commit 8b7ef4ec5b1ac8b6feebf5ae9cda85a7514728f8
Merge: a1d5a8691f1b 543cf4cb3fe6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 25 12:30:21 2008 +0200

    Merge branch 'linus' into x86/fixmap

commit 28499143933f19b28008a556ed59255d6009391a
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri May 9 12:05:57 2008 +0100

    xen: remove support for non-PAE 32-bit
    
    Non-PAE operation has been deprecated in Xen for a while, and is
    rarely tested or used.  xen-unstable has now officially dropped
    non-PAE support.  Since Xen/pvops' non-PAE support has also been
    broken for a while, we may as well completely drop it altogether.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c048de34d6a1..f09c1c69c37a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -785,38 +785,35 @@ static __init void xen_set_pte_init(pte_t *ptep, pte_t pte)
 static __init void xen_pagetable_setup_start(pgd_t *base)
 {
 	pgd_t *xen_pgd = (pgd_t *)xen_start_info->pt_base;
+	int i;
 
 	/* special set_pte for pagetable initialization */
 	pv_mmu_ops.set_pte = xen_set_pte_init;
 
 	init_mm.pgd = base;
 	/*
-	 * copy top-level of Xen-supplied pagetable into place.	 For
-	 * !PAE we can use this as-is, but for PAE it is a stand-in
-	 * while we copy the pmd pages.
+	 * copy top-level of Xen-supplied pagetable into place.  This
+	 * is a stand-in while we copy the pmd pages.
 	 */
 	memcpy(base, xen_pgd, PTRS_PER_PGD * sizeof(pgd_t));
 
-	if (PTRS_PER_PMD > 1) {
-		int i;
-		/*
-		 * For PAE, need to allocate new pmds, rather than
-		 * share Xen's, since Xen doesn't like pmd's being
-		 * shared between address spaces.
-		 */
-		for (i = 0; i < PTRS_PER_PGD; i++) {
-			if (pgd_val_ma(xen_pgd[i]) & _PAGE_PRESENT) {
-				pmd_t *pmd = (pmd_t *)alloc_bootmem_low_pages(PAGE_SIZE);
+	/*
+	 * For PAE, need to allocate new pmds, rather than
+	 * share Xen's, since Xen doesn't like pmd's being
+	 * shared between address spaces.
+	 */
+	for (i = 0; i < PTRS_PER_PGD; i++) {
+		if (pgd_val_ma(xen_pgd[i]) & _PAGE_PRESENT) {
+			pmd_t *pmd = (pmd_t *)alloc_bootmem_low_pages(PAGE_SIZE);
 
-				memcpy(pmd, (void *)pgd_page_vaddr(xen_pgd[i]),
-				       PAGE_SIZE);
+			memcpy(pmd, (void *)pgd_page_vaddr(xen_pgd[i]),
+			       PAGE_SIZE);
 
-				make_lowmem_page_readonly(pmd);
+			make_lowmem_page_readonly(pmd);
 
-				set_pgd(&base[i], __pgd(1 + __pa(pmd)));
-			} else
-				pgd_clear(&base[i]);
-		}
+			set_pgd(&base[i], __pgd(1 + __pa(pmd)));
+		} else
+			pgd_clear(&base[i]);
 	}
 
 	/* make sure zero_page is mapped RO so we can use it in pagetables */
@@ -873,17 +870,7 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 
 	/* Actually pin the pagetable down, but we can't set PG_pinned
 	   yet because the page structures don't exist yet. */
-	{
-		unsigned level;
-
-#ifdef CONFIG_X86_PAE
-		level = MMUEXT_PIN_L3_TABLE;
-#else
-		level = MMUEXT_PIN_L2_TABLE;
-#endif
-
-		pin_pagetable_pfn(level, PFN_DOWN(__pa(base)));
-	}
+	pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(__pa(base)));
 }
 
 /* This is called once we have the cpu_possible_map */
@@ -1093,7 +1080,6 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.make_pte = xen_make_pte,
 	.make_pgd = xen_make_pgd,
 
-#ifdef CONFIG_X86_PAE
 	.set_pte_atomic = xen_set_pte_atomic,
 	.set_pte_present = xen_set_pte_at,
 	.set_pud = xen_set_pud,
@@ -1102,7 +1088,6 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 
 	.make_pmd = xen_make_pmd,
 	.pmd_val = xen_pmd_val,
-#endif	/* PAE */
 
 	.activate_mm = xen_activate_mm,
 	.dup_mmap = xen_dup_mmap,

commit aeaaa59c7e15dcfaaf57ce069ef81683067d575d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jun 17 11:42:01 2008 -0700

    x86/paravirt/xen: add set_fixmap pv_mmu_ops
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Juan Quintela <quintela@redhat.com>
    Signed-off-by: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c8a56e457d61..0ad8a64a2e05 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -960,6 +960,33 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 	return ret;
 }
 
+static void xen_set_fixmap(unsigned idx, unsigned long phys, pgprot_t prot)
+{
+	pte_t pte;
+
+	phys >>= PAGE_SHIFT;
+
+	switch (idx) {
+	case FIX_BTMAP_END ... FIX_BTMAP_BEGIN:
+#ifdef CONFIG_X86_F00F_BUG
+	case FIX_F00F_IDT:
+#endif
+	case FIX_WP_TEST:
+	case FIX_VDSO:
+#ifdef CONFIG_X86_LOCAL_APIC
+	case FIX_APIC_BASE:	/* maps dummy local APIC */
+#endif
+		pte = pfn_pte(phys, prot);
+		break;
+
+	default:
+		pte = mfn_pte(phys, prot);
+		break;
+	}
+
+	__native_set_fixmap(idx, pte);
+}
+
 static const struct pv_info xen_info __initdata = {
 	.paravirt_enabled = 1,
 	.shared_kernel_pmd = 0,
@@ -1112,6 +1139,8 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 		.enter = paravirt_enter_lazy_mmu,
 		.leave = xen_leave_lazy,
 	},
+
+	.set_fixmap = xen_set_fixmap,
 };
 
 #ifdef CONFIG_SMP

commit 05345b0f006ac226d0d25d48fcb2d792ac44a071
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jun 16 15:01:53 2008 -0700

    xen: mask unwanted pte bits in __supported_pte_mask
    
    [ Stable: this isn't a bugfix in itself, but it's a pre-requiste
      for "xen: don't drop NX bit" ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>
    Cc: the arch/x86 maintainers <x86@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c8a56e457d61..c048de34d6a1 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1228,6 +1228,11 @@ asmlinkage void __init xen_start_kernel(void)
 	if (xen_feature(XENFEAT_supervisor_mode_kernel))
 		pv_info.kernel_rpl = 0;
 
+	/* Prevent unwanted bits from being set in PTEs. */
+	__supported_pte_mask &= ~_PAGE_GLOBAL;
+	if (!is_initial_xendomain())
+		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
+
 	/* set the limit of our address space */
 	xen_reserve_top();
 

commit eb179e443deb0a5c81a62b4c157124a4b7ff1813
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jun 16 15:01:53 2008 -0700

    xen: mask unwanted pte bits in __supported_pte_mask
    
    [ Stable: this isn't a bugfix in itself, but it's a pre-requiste
      for "xen: don't drop NX bit" ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>
    Cc: the arch/x86 maintainers <x86@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8e6152e6ed88..73fb0c4c150a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1280,6 +1280,11 @@ asmlinkage void __init xen_start_kernel(void)
 	if (xen_feature(XENFEAT_supervisor_mode_kernel))
 		pv_info.kernel_rpl = 0;
 
+	/* Prevent unwanted bits from being set in PTEs. */
+	__supported_pte_mask &= ~_PAGE_GLOBAL;
+	if (!is_initial_xendomain())
+		__supported_pte_mask &= ~(_PAGE_PWT | _PAGE_PCD);
+
 	/* set the limit of our address space */
 	xen_reserve_top();
 

commit 9c7a794209f8a91f47697c3be20597eb60531e6d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sat May 31 01:33:02 2008 +0100

    xen: restore vcpu_info mapping
    
    If we're using vcpu_info mapping, then make sure its restored on all
    processors before relasing them from stop_machine.
    
    The only complication is that if this fails, we can't continue because
    we've already made assumptions that the mapping is available (baked in
    calls to the _direct versions of the functions, for example).
    
    Fortunately this can only happen with a 32-bit hypervisor, which may
    possibly run out of mapping space.  On a 64-bit hypervisor, this is a
    non-issue.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ed9f04b3836d..8e6152e6ed88 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -98,7 +98,7 @@ struct shared_info *HYPERVISOR_shared_info = (void *)&xen_dummy_shared_info;
  */
 static int have_vcpu_info_placement = 1;
 
-static void __init xen_vcpu_setup(int cpu)
+static void xen_vcpu_setup(int cpu)
 {
 	struct vcpu_register_vcpu_info info;
 	int err;
@@ -136,6 +136,34 @@ static void __init xen_vcpu_setup(int cpu)
 	}
 }
 
+/*
+ * On restore, set the vcpu placement up again.
+ * If it fails, then we're in a bad state, since
+ * we can't back out from using it...
+ */
+void xen_vcpu_restore(void)
+{
+	if (have_vcpu_info_placement) {
+		int cpu;
+
+		for_each_online_cpu(cpu) {
+			bool other_cpu = (cpu != smp_processor_id());
+
+			if (other_cpu &&
+			    HYPERVISOR_vcpu_op(VCPUOP_down, cpu, NULL))
+				BUG();
+
+			xen_vcpu_setup(cpu);
+
+			if (other_cpu &&
+			    HYPERVISOR_vcpu_op(VCPUOP_up, cpu, NULL))
+				BUG();
+		}
+
+		BUG_ON(!have_vcpu_info_placement);
+	}
+}
+
 static void __init xen_banner(void)
 {
 	printk(KERN_INFO "Booting paravirtualized kernel on %s\n",

commit e2426cf85f8db5891fb5831323d2d0c176c4dadc
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sat May 31 01:24:27 2008 +0100

    xen: avoid hypercalls when updating unpinned pud/pmd
    
    When operating on an unpinned pagetable (ie, one under construction or
    destruction), it isn't necessary to use a hypercall to update a
    pud/pmd entry.  Jan Beulich observed that a similar optimisation
    avoided many thousands of hypercalls while doing a kernel build.
    
    One tricky part is that early in the kernel boot there's no page
    structure, so we can't check to see if the page is pinned.  In that
    case, we just always use the hypercall.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b94f63ac228b..ed9f04b3836d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -901,6 +901,14 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 	pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(__pa(base)));
 }
 
+static __init void xen_post_allocator_init(void)
+{
+	pv_mmu_ops.set_pmd = xen_set_pmd;
+	pv_mmu_ops.set_pud = xen_set_pud;
+
+	xen_mark_init_mm_pinned();
+}
+
 /* This is called once we have the cpu_possible_map */
 void xen_setup_vcpu_info_placement(void)
 {
@@ -988,7 +996,7 @@ static const struct pv_init_ops xen_init_ops __initdata = {
 	.banner = xen_banner,
 	.memory_setup = xen_memory_setup,
 	.arch_setup = xen_arch_setup,
-	.post_allocator_init = xen_mark_init_mm_pinned,
+	.post_allocator_init = xen_post_allocator_init,
 };
 
 static const struct pv_time_ops xen_time_ops __initdata = {
@@ -1100,7 +1108,7 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 
 	.set_pte = NULL,	/* see xen_pagetable_setup_* */
 	.set_pte_at = xen_set_pte_at,
-	.set_pmd = xen_set_pmd,
+	.set_pmd = xen_set_pmd_hyper,
 
 	.pte_val = xen_pte_val,
 	.pte_flags = native_pte_val,
@@ -1111,7 +1119,7 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 
 	.set_pte_atomic = xen_set_pte_atomic,
 	.set_pte_present = xen_set_pte_at,
-	.set_pud = xen_set_pud,
+	.set_pud = xen_set_pud_hyper,
 	.pte_clear = xen_pte_clear,
 	.pmd_clear = xen_pmd_clear,
 

commit f0d43100f13be0fa5bf52741d7084bb27f00e621
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu May 29 12:56:36 2008 -0700

    x86: extend e820 early_res support 32bit -fix #3
    
    introduce init_pg_table_start, so xen PV could specify the value.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c8a56e457d61..ccc7b84ddabf 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1211,6 +1211,7 @@ asmlinkage void __init xen_start_kernel(void)
 
 	pgd = (pgd_t *)xen_start_info->pt_base;
 
+	init_pg_tables_start = __pa(pgd);
 	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
 
 	init_mm.pgd = pgd; /* use the Xen pagetables to start */
@@ -1246,5 +1247,5 @@ asmlinkage void __init xen_start_kernel(void)
 		add_preferred_console("hvc", 0, NULL);
 
 	/* Start the world */
-	start_kernel();
+	i386_start_kernel();
 }

commit 0e91398f2a5d4eb6b07df8115917d0d1cf3e9b58
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:27 2008 +0100

    xen: implement save/restore
    
    This patch implements Xen save/restore and migration.
    
    Saving is triggered via xenbus, which is polled in
    drivers/xen/manage.c.  When a suspend request comes in, the kernel
    prepares itself for saving by:
    
    1 - Freeze all processes.  This is primarily to prevent any
        partially-completed pagetable updates from confusing the suspend
        process.  If CONFIG_PREEMPT isn't defined, then this isn't necessary.
    
    2 - Suspend xenbus and other devices
    
    3 - Stop_machine, to make sure all the other vcpus are quiescent.  The
        Xen tools require the domain to run its save off vcpu0.
    
    4 - Within the stop_machine state, it pins any unpinned pgds (under
        construction or destruction), performs canonicalizes various other
        pieces of state (mostly converting mfns to pfns), and finally
    
    5 - Suspend the domain
    
    Restore reverses the steps used to save the domain, ending when all
    the frozen processes are thawed.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index ce67dc8f36af..b94f63ac228b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -857,7 +857,7 @@ static __init void xen_pagetable_setup_start(pgd_t *base)
 			  PFN_DOWN(__pa(xen_start_info->pt_base)));
 }
 
-static __init void setup_shared_info(void)
+void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
 		unsigned long addr = fix_to_virt(FIX_PARAVIRT_BOOTMAP);
@@ -894,7 +894,7 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 	pv_mmu_ops.release_pmd = xen_release_pmd;
 	pv_mmu_ops.set_pte = xen_set_pte;
 
-	setup_shared_info();
+	xen_setup_shared_info();
 
 	/* Actually pin the pagetable down, but we can't set PG_pinned
 	   yet because the page structures don't exist yet. */
@@ -902,7 +902,7 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 }
 
 /* This is called once we have the cpu_possible_map */
-void __init xen_setup_vcpu_info_placement(void)
+void xen_setup_vcpu_info_placement(void)
 {
 	int cpu;
 

commit d5edbc1f75420935b1ec7e65df10c8f81cea82de
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:22 2008 +0100

    xen: add p2m mfn_list_list
    
    When saving a domain, the Xen tools need to remap all our mfns to
    portable pfns.  In order to remap our p2m table, it needs to know
    where all its pages are, so maintain the references to the p2m table
    for it to use.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d9faaa26c104..ce67dc8f36af 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -880,6 +880,8 @@ static __init void setup_shared_info(void)
 	/* In UP this is as good a place as any to set up shared info */
 	xen_setup_vcpu_info_placement();
 #endif
+
+	xen_setup_mfn_list_list();
 }
 
 static __init void xen_pagetable_setup_done(pgd_t *base)

commit a0d695c821544947342a2d372ec4108bc813b979
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:21 2008 +0100

    xen: make dummy_shared_info non-static
    
    Rename dummy_shared_info to xen_dummy_shared_info and make it
    non-static, in anticipation of users outside of enlighten.c
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 73d3c84a3495..d9faaa26c104 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -75,13 +75,13 @@ DEFINE_PER_CPU(unsigned long, xen_current_cr3);	 /* actual vcpu cr3 */
 struct start_info *xen_start_info;
 EXPORT_SYMBOL_GPL(xen_start_info);
 
-static /* __initdata */ struct shared_info dummy_shared_info;
+struct shared_info xen_dummy_shared_info;
 
 /*
  * Point at some empty memory to start with. We map the real shared_info
  * page as soon as fixmap is up and running.
  */
-struct shared_info *HYPERVISOR_shared_info = (void *)&dummy_shared_info;
+struct shared_info *HYPERVISOR_shared_info = (void *)&xen_dummy_shared_info;
 
 /*
  * Flag to determine whether vcpu info placement is available on all
@@ -104,7 +104,7 @@ static void __init xen_vcpu_setup(int cpu)
 	int err;
 	struct vcpu_info *vcpup;
 
-	BUG_ON(HYPERVISOR_shared_info == &dummy_shared_info);
+	BUG_ON(HYPERVISOR_shared_info == &xen_dummy_shared_info);
 	per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 
 	if (!have_vcpu_info_placement)

commit d451bb7aa852627bdf7be7937dc3d9d9f261b235
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:18 2008 +0100

    xen: make phys_to_machine structure dynamic
    
    We now support the use of memory hotplug, so the physical to machine
    page mapping structure must be dynamic.  This is implemented as a
    two-level radix tree structure, which allows us to efficiently
    incrementally allocate memory for the p2m table as new pages are
    added.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 5c0635a8bffd..73d3c84a3495 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1221,7 +1221,7 @@ asmlinkage void __init xen_start_kernel(void)
 
 	/* Get mfn list */
 	if (!xen_feature(XENFEAT_auto_translated_physmap))
-		phys_to_machine_mapping = (unsigned long *)xen_start_info->mfn_list;
+		xen_build_dynamic_phys_to_machine();
 
 	pgd = (pgd_t *)xen_start_info->pt_base;
 

commit 83abc70a4c6e306f4c1672e25884322f797e4fcb
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:12 2008 +0100

    xen: make earlyprintk=xen work again
    
    For some perverse reason, if you call add_preferred_console() it prevents
    setup_early_printk() from successfully enabling the boot console -
    unless you make it a preferred console too...
    
    Also, make xenboot console output distinct from normal console output,
    since it gets repeated when the console handover happens, and the
    duplicated output is confusing without disambiguation.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Markus Armbruster <armbru@redhat.com>
    Cc: Gerd Hoffmann <kraxel@redhat.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 6cfb708408e9..5c0635a8bffd 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1257,6 +1257,7 @@ asmlinkage void __init xen_start_kernel(void)
 	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
 
 	if (!is_initial_xendomain()) {
+		add_preferred_console("xenboot", 0, NULL);
 		add_preferred_console("tty", 0, NULL);
 		add_preferred_console("hvc", 0, NULL);
 	}

commit 9e124fe16ff24746d6de5a2ad685266d7bce0e08
Author: Markus Armbruster <armbru@redhat.com>
Date:   Mon May 26 23:31:07 2008 +0100

    xen: Enable console tty by default in domU if it's not a dummy
    
    Without console= arguments on the kernel command line, the first
    console to register becomes enabled and the preferred console (the one
    behind /dev/console).  This is normally tty (assuming
    CONFIG_VT_CONSOLE is enabled, which it commonly is).
    
    This is okay as long tty is a useful console.  But unless we have the
    PV framebuffer, and it is enabled for this domain, tty0 in domU is
    merely a dummy.  In that case, we want the preferred console to be the
    Xen console hvc0, and we want it without having to fiddle with the
    kernel command line.  Commit b8c2d3dfbc117dff26058fbac316b8acfc2cb5f7
    did that for us.
    
    Since we now have the PV framebuffer, we want to enable and prefer tty
    again, but only when PVFB is enabled.  But even then we still want to
    enable the Xen console as well.
    
    Problem: when tty registers, we can't yet know whether the PVFB is
    enabled.  By the time we can know (xenstore is up), the console setup
    game is over.
    
    Solution: enable console tty by default, but keep hvc as the preferred
    console.  Change the preferred console to tty when PVFB probes
    successfully, unless we've been given console kernel parameters.
    
    Signed-off-by: Markus Armbruster <armbru@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1b4b5fa498b3..6cfb708408e9 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1256,8 +1256,10 @@ asmlinkage void __init xen_start_kernel(void)
 		? __pa(xen_start_info->mod_start) : 0;
 	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
 
-	if (!is_initial_xendomain())
+	if (!is_initial_xendomain()) {
+		add_preferred_console("tty", 0, NULL);
 		add_preferred_console("hvc", 0, NULL);
+	}
 
 	/* Start the world */
 	start_kernel();

commit a15af1c9ea2750a9ff01e51615c45950bad8221b
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:06 2008 +0100

    x86/paravirt: add pte_flags to just get pte flags
    
    Add pte_flags() to extract the flags from a pte.  This is a special
    case of pte_val() which is only guaranteed to return the pte's flags
    correctly; the page number may be corrupted or missing.
    
    The intent is to allow paravirt implementations to return pte flags
    without having to do any translation of the page number (most notably,
    Xen).
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 4a372b71239d..1b4b5fa498b3 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1101,6 +1101,7 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.set_pmd = xen_set_pmd,
 
 	.pte_val = xen_pte_val,
+	.pte_flags = native_pte_val,
 	.pgd_val = xen_pgd_val,
 
 	.make_pte = xen_make_pte,

commit 239d1fc04ed0b58d638096b12a7f6d50269d30c9
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:05 2008 +0100

    xen: don't worry about preempt during xen_irq_enable()
    
    When enabling interrupts, we don't need to worry about preemption,
    because we either enter with interrupts disabled - so no preemption -
    or the caller is confused and is re-enabling interrupts on some
    indeterminate processor.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f6876485ee7d..4a372b71239d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -235,13 +235,13 @@ static void xen_irq_enable(void)
 {
 	struct vcpu_info *vcpu;
 
-	/* There's a one instruction preempt window here.  We need to
-	   make sure we're don't switch CPUs between getting the vcpu
-	   pointer and updating the mask. */
-	preempt_disable();
+	/* We don't need to worry about being preempted here, since
+	   either a) interrupts are disabled, so no preemption, or b)
+	   the caller is confused and is trying to re-enable interrupts
+	   on an indeterminate processor. */
+
 	vcpu = x86_read_percpu(xen_vcpu);
 	vcpu->evtchn_upcall_mask = 0;
-	preempt_enable_no_resched();
 
 	/* Doesn't matter if we get preempted here, because any
 	   pending event will get dealt with anyway. */

commit 2956a3511c8c5dccb1d4739ead17c7c3c23a24b7
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:04 2008 +0100

    xen: allow some cr4 updates
    
    The guest can legitimately change things like cr4.OSFXSR and
    OSXMMEXCPT, so let it.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 35ddaf50180a..f6876485ee7d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -648,8 +648,10 @@ static unsigned long xen_read_cr2_direct(void)
 
 static void xen_write_cr4(unsigned long cr4)
 {
-	/* Just ignore cr4 changes; Xen doesn't allow us to do
-	   anything anyway. */
+	cr4 &= ~X86_CR4_PGE;
+	cr4 &= ~X86_CR4_PSE;
+
+	native_write_cr4(cr4);
 }
 
 static unsigned long xen_read_cr3(void)

commit 349c709f42453707f74bece0d9d35ee5b3842893
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:02 2008 +0100

    xen: use new sched_op
    
    Use the new sched_op hypercall, mainly because xenner doesn't support
    the old one.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 446f4cd649e6..35ddaf50180a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -254,7 +254,7 @@ static void xen_irq_enable(void)
 static void xen_safe_halt(void)
 {
 	/* Blocking includes an implicit local_irq_enable(). */
-	if (HYPERVISOR_sched_op(SCHEDOP_block, 0) != 0)
+	if (HYPERVISOR_sched_op(SCHEDOP_block, NULL) != 0)
 		BUG();
 }
 
@@ -1138,11 +1138,13 @@ static const struct smp_ops xen_smp_ops __initdata = {
 
 static void xen_reboot(int reason)
 {
+	struct sched_shutdown r = { .reason = reason };
+
 #ifdef CONFIG_SMP
 	smp_send_stop();
 #endif
 
-	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, reason))
+	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
 		BUG();
 }
 

commit 7b1333aa4cb546ddeb9c05098a53d9a777623a05
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:01 2008 +0100

    xen: use hypercall rather than clts
    
    Xen will trap and emulate clts, but its better to use a hypercall.
    Also, xenner doesn't handle clts.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index a05b7721bc81..446f4cd649e6 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -607,6 +607,30 @@ static void xen_flush_tlb_others(const cpumask_t *cpus, struct mm_struct *mm,
 	xen_mc_issue(PARAVIRT_LAZY_MMU);
 }
 
+static void xen_clts(void)
+{
+	struct multicall_space mcs;
+
+	mcs = xen_mc_entry(0);
+
+	MULTI_fpu_taskswitch(mcs.mc, 0);
+
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
+}
+
+static void xen_write_cr0(unsigned long cr0)
+{
+	struct multicall_space mcs;
+
+	/* Only pay attention to cr0.TS; everything else is
+	   ignored. */
+	mcs = xen_mc_entry(0);
+
+	MULTI_fpu_taskswitch(mcs.mc, (cr0 & X86_CR0_TS) != 0);
+
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
+}
+
 static void xen_write_cr2(unsigned long cr2)
 {
 	x86_read_percpu(xen_vcpu)->arch.cr2 = cr2;
@@ -978,10 +1002,10 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.set_debugreg = xen_set_debugreg,
 	.get_debugreg = xen_get_debugreg,
 
-	.clts = native_clts,
+	.clts = xen_clts,
 
 	.read_cr0 = native_read_cr0,
-	.write_cr0 = native_write_cr0,
+	.write_cr0 = xen_write_cr0,
 
 	.read_cr4 = native_read_cr4,
 	.read_cr4_safe = native_read_cr4_safe,

commit 3843fc2575e3389f4f0ad0420a720240a5746a5d
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri May 9 12:05:57 2008 +0100

    xen: remove support for non-PAE 32-bit
    
    Non-PAE operation has been deprecated in Xen for a while, and is
    rarely tested or used.  xen-unstable has now officially dropped
    non-PAE support.  Since Xen/pvops' non-PAE support has also been
    broken for a while, we may as well completely drop it altogether.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c8a56e457d61..a05b7721bc81 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -785,38 +785,35 @@ static __init void xen_set_pte_init(pte_t *ptep, pte_t pte)
 static __init void xen_pagetable_setup_start(pgd_t *base)
 {
 	pgd_t *xen_pgd = (pgd_t *)xen_start_info->pt_base;
+	int i;
 
 	/* special set_pte for pagetable initialization */
 	pv_mmu_ops.set_pte = xen_set_pte_init;
 
 	init_mm.pgd = base;
 	/*
-	 * copy top-level of Xen-supplied pagetable into place.	 For
-	 * !PAE we can use this as-is, but for PAE it is a stand-in
-	 * while we copy the pmd pages.
+	 * copy top-level of Xen-supplied pagetable into place.  This
+	 * is a stand-in while we copy the pmd pages.
 	 */
 	memcpy(base, xen_pgd, PTRS_PER_PGD * sizeof(pgd_t));
 
-	if (PTRS_PER_PMD > 1) {
-		int i;
-		/*
-		 * For PAE, need to allocate new pmds, rather than
-		 * share Xen's, since Xen doesn't like pmd's being
-		 * shared between address spaces.
-		 */
-		for (i = 0; i < PTRS_PER_PGD; i++) {
-			if (pgd_val_ma(xen_pgd[i]) & _PAGE_PRESENT) {
-				pmd_t *pmd = (pmd_t *)alloc_bootmem_low_pages(PAGE_SIZE);
+	/*
+	 * For PAE, need to allocate new pmds, rather than
+	 * share Xen's, since Xen doesn't like pmd's being
+	 * shared between address spaces.
+	 */
+	for (i = 0; i < PTRS_PER_PGD; i++) {
+		if (pgd_val_ma(xen_pgd[i]) & _PAGE_PRESENT) {
+			pmd_t *pmd = (pmd_t *)alloc_bootmem_low_pages(PAGE_SIZE);
 
-				memcpy(pmd, (void *)pgd_page_vaddr(xen_pgd[i]),
-				       PAGE_SIZE);
+			memcpy(pmd, (void *)pgd_page_vaddr(xen_pgd[i]),
+			       PAGE_SIZE);
 
-				make_lowmem_page_readonly(pmd);
+			make_lowmem_page_readonly(pmd);
 
-				set_pgd(&base[i], __pgd(1 + __pa(pmd)));
-			} else
-				pgd_clear(&base[i]);
-		}
+			set_pgd(&base[i], __pgd(1 + __pa(pmd)));
+		} else
+			pgd_clear(&base[i]);
 	}
 
 	/* make sure zero_page is mapped RO so we can use it in pagetables */
@@ -873,17 +870,7 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 
 	/* Actually pin the pagetable down, but we can't set PG_pinned
 	   yet because the page structures don't exist yet. */
-	{
-		unsigned level;
-
-#ifdef CONFIG_X86_PAE
-		level = MMUEXT_PIN_L3_TABLE;
-#else
-		level = MMUEXT_PIN_L2_TABLE;
-#endif
-
-		pin_pagetable_pfn(level, PFN_DOWN(__pa(base)));
-	}
+	pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(__pa(base)));
 }
 
 /* This is called once we have the cpu_possible_map */
@@ -1093,7 +1080,6 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.make_pte = xen_make_pte,
 	.make_pgd = xen_make_pgd,
 
-#ifdef CONFIG_X86_PAE
 	.set_pte_atomic = xen_set_pte_atomic,
 	.set_pte_present = xen_set_pte_at,
 	.set_pud = xen_set_pud,
@@ -1102,7 +1088,6 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 
 	.make_pmd = xen_make_pmd,
 	.pmd_val = xen_pmd_val,
-#endif	/* PAE */
 
 	.activate_mm = xen_activate_mm,
 	.dup_mmap = xen_dup_mmap,

commit af7ae3b9c4a4c1337903f31131d58e3c0d2b6d55
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Apr 2 10:54:12 2008 -0700

    xen: allow compilation with non-flat memory
    
    There's no real reason we can't support sparsemem/discontigmem, so do so.
    This is mostly useful to support hotplug memory.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bc129146f99f..c8a56e457d61 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -669,7 +669,9 @@ static void xen_write_cr3(unsigned long cr3)
    everything is pinned. */
 static __init void xen_alloc_pte_init(struct mm_struct *mm, u32 pfn)
 {
+#ifdef CONFIG_FLATMEM
 	BUG_ON(mem_map);	/* should only be used early */
+#endif
 	make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
 }
 

commit 41e332b2a2dfe514cd441ed0ce1096ed1863e378
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Apr 2 10:54:09 2008 -0700

    xen: disable preemption during tlb flush
    
    Various places in the kernel flush the tlb even though preemption doens't
    guarantee the tlb flush is happening on any particular CPU.  In many cases
    this doesn't seem to matter, so don't make a fuss about it.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8c5ff24a492b..bc129146f99f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -532,26 +532,37 @@ static void xen_apic_write(unsigned long reg, u32 val)
 static void xen_flush_tlb(void)
 {
 	struct mmuext_op *op;
-	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
+	struct multicall_space mcs;
+
+	preempt_disable();
+
+	mcs = xen_mc_entry(sizeof(*op));
 
 	op = mcs.args;
 	op->cmd = MMUEXT_TLB_FLUSH_LOCAL;
 	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
 
 	xen_mc_issue(PARAVIRT_LAZY_MMU);
+
+	preempt_enable();
 }
 
 static void xen_flush_tlb_single(unsigned long addr)
 {
 	struct mmuext_op *op;
-	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
+	struct multicall_space mcs;
+
+	preempt_disable();
 
+	mcs = xen_mc_entry(sizeof(*op));
 	op = mcs.args;
 	op->cmd = MMUEXT_INVLPG_LOCAL;
 	op->arg1.linear_addr = addr & PAGE_MASK;
 	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
 
 	xen_mc_issue(PARAVIRT_LAZY_MMU);
+
+	preempt_enable();
 }
 
 static void xen_flush_tlb_others(const cpumask_t *cpus, struct mm_struct *mm,

commit dbe9e994c99ac9ac12d2b66ea42f44558f54fa52
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Mar 17 16:37:21 2008 -0700

    xen: no need for domU to worry about MCE/MCA
    
    Mask MCE/MCA out of cpu caps.  Its harmless to leave them there, but
    it does prevent the kernel from starting an unnecessary thread.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 943684566ebe..8c5ff24a492b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -155,6 +155,8 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 	if (*ax == 1)
 		maskedx = ~((1 << X86_FEATURE_APIC) |  /* disable APIC */
 			    (1 << X86_FEATURE_ACPI) |  /* disable ACPI */
+			    (1 << X86_FEATURE_MCE)  |  /* disable MCE */
+			    (1 << X86_FEATURE_MCA)  |  /* disable MCA */
 			    (1 << X86_FEATURE_ACC));   /* thermal monitoring */
 
 	asm(XEN_EMULATE_PREFIX "cpuid"

commit e2a81baf6604a2e08e10c7405b0349106f77c8af
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Mar 17 16:37:17 2008 -0700

    xen: support sysenter/sysexit if hypervisor does
    
    64-bit Xen supports sysenter for 32-bit guests, so support its
    use.  (sysenter is faster than int $0x80 in 32-on-64.)
    
    sysexit is still not supported, so we fake it up using iret.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 36f36e6b0874..943684566ebe 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -155,7 +155,6 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 	if (*ax == 1)
 		maskedx = ~((1 << X86_FEATURE_APIC) |  /* disable APIC */
 			    (1 << X86_FEATURE_ACPI) |  /* disable ACPI */
-			    (1 << X86_FEATURE_SEP)  |  /* disable SEP */
 			    (1 << X86_FEATURE_ACC));   /* thermal monitoring */
 
 	asm(XEN_EMULATE_PREFIX "cpuid"
@@ -994,7 +993,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.read_pmc = native_read_pmc,
 
 	.iret = xen_iret,
-	.irq_enable_syscall_ret = NULL,  /* never called */
+	.irq_enable_syscall_ret = xen_sysexit,
 
 	.load_tr_desc = paravirt_nop,
 	.set_ldt = xen_set_ldt,

commit 6944a9c8945212a0cc1de3589736d59ec542c539
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Mar 17 16:37:01 2008 -0700

    x86: rename paravirt_alloc_pt etc after the pagetable structure
    
    Rename (alloc|release)_(pt|pd) to pte/pmd to explicitly match the name
    of the appropriate pagetable level structure.
    
    [ x86.git merge work by Mark McLoughlin <markmc@redhat.com> ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c0388220cf97..36f36e6b0874 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -655,15 +655,15 @@ static void xen_write_cr3(unsigned long cr3)
 
 /* Early in boot, while setting up the initial pagetable, assume
    everything is pinned. */
-static __init void xen_alloc_pt_init(struct mm_struct *mm, u32 pfn)
+static __init void xen_alloc_pte_init(struct mm_struct *mm, u32 pfn)
 {
 	BUG_ON(mem_map);	/* should only be used early */
 	make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
 }
 
-/* Early release_pt assumes that all pts are pinned, since there's
+/* Early release_pte assumes that all pts are pinned, since there's
    only init_mm and anything attached to that is pinned. */
-static void xen_release_pt_init(u32 pfn)
+static void xen_release_pte_init(u32 pfn)
 {
 	make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
 }
@@ -697,12 +697,12 @@ static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 	}
 }
 
-static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
+static void xen_alloc_pte(struct mm_struct *mm, u32 pfn)
 {
 	xen_alloc_ptpage(mm, pfn, PT_PTE);
 }
 
-static void xen_alloc_pd(struct mm_struct *mm, u32 pfn)
+static void xen_alloc_pmd(struct mm_struct *mm, u32 pfn)
 {
 	xen_alloc_ptpage(mm, pfn, PT_PMD);
 }
@@ -722,12 +722,12 @@ static void xen_release_ptpage(u32 pfn, unsigned level)
 	}
 }
 
-static void xen_release_pt(u32 pfn)
+static void xen_release_pte(u32 pfn)
 {
 	xen_release_ptpage(pfn, PT_PTE);
 }
 
-static void xen_release_pd(u32 pfn)
+static void xen_release_pmd(u32 pfn)
 {
 	xen_release_ptpage(pfn, PT_PMD);
 }
@@ -849,10 +849,10 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 {
 	/* This will work as long as patching hasn't happened yet
 	   (which it hasn't) */
-	pv_mmu_ops.alloc_pt = xen_alloc_pt;
-	pv_mmu_ops.alloc_pd = xen_alloc_pd;
-	pv_mmu_ops.release_pt = xen_release_pt;
-	pv_mmu_ops.release_pd = xen_release_pd;
+	pv_mmu_ops.alloc_pte = xen_alloc_pte;
+	pv_mmu_ops.alloc_pmd = xen_alloc_pmd;
+	pv_mmu_ops.release_pte = xen_release_pte;
+	pv_mmu_ops.release_pmd = xen_release_pmd;
 	pv_mmu_ops.set_pte = xen_set_pte;
 
 	setup_shared_info();
@@ -1059,11 +1059,11 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.pte_update = paravirt_nop,
 	.pte_update_defer = paravirt_nop,
 
-	.alloc_pt = xen_alloc_pt_init,
-	.release_pt = xen_release_pt_init,
-	.alloc_pd = xen_alloc_pt_init,
-	.alloc_pd_clone = paravirt_nop,
-	.release_pd = xen_release_pt_init,
+	.alloc_pte = xen_alloc_pte_init,
+	.release_pte = xen_release_pte_init,
+	.alloc_pmd = xen_alloc_pte_init,
+	.alloc_pmd_clone = paravirt_nop,
+	.release_pmd = xen_release_pte_init,
 
 #ifdef CONFIG_HIGHPTE
 	.kmap_atomic_pte = xen_kmap_atomic_pte,

commit 81e103f1f1bb0d35000f0e99626bf7abf864b486
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Thu Apr 17 17:40:51 2008 +0200

    xen: use iret instruction all the time
    
    Change iret implementation to not be dependent on direct-access vcpu
    structure.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 198db49106b2..c0388220cf97 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -890,7 +890,6 @@ void __init xen_setup_vcpu_info_placement(void)
 		pv_irq_ops.irq_disable = xen_irq_disable_direct;
 		pv_irq_ops.irq_enable = xen_irq_enable_direct;
 		pv_mmu_ops.read_cr2 = xen_read_cr2_direct;
-		pv_cpu_ops.iret = xen_iret_direct;
 	}
 }
 
@@ -994,7 +993,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.read_tsc = native_read_tsc,
 	.read_pmc = native_read_pmc,
 
-	.iret = (void *)&hypercall_page[__HYPERVISOR_iret],
+	.iret = xen_iret,
 	.irq_enable_syscall_ret = NULL,  /* never called */
 
 	.load_tr_desc = paravirt_nop,

commit b8c2d3dfbc117dff26058fbac316b8acfc2cb5f7
Author: Markus Armbruster <armbru@redhat.com>
Date:   Wed Feb 27 14:56:35 2008 +0100

    xen: make hvc0 the preferred console in domU
    
    This makes the Xen console just work.  Before, you had to ask for it
    on the kernel command line with console=hvc0
    
    Signed-off-by: Markus Armbruster <armbru@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 27ee26aedf94..198db49106b2 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -25,6 +25,7 @@
 #include <linux/mm.h>
 #include <linux/page-flags.h>
 #include <linux/highmem.h>
+#include <linux/console.h>
 
 #include <xen/interface/xen.h>
 #include <xen/interface/physdev.h>
@@ -1228,6 +1229,9 @@ asmlinkage void __init xen_start_kernel(void)
 		? __pa(xen_start_info->mod_start) : 0;
 	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
 
+	if (!is_initial_xendomain())
+		add_preferred_console("hvc", 0, NULL);
+
 	/* Start the world */
 	start_kernel();
 }

commit c946c7de49a9ba50bc205d6359b41bbc8f01174c
Author: Mark McLoughlin <markmc@redhat.com>
Date:   Wed Apr 2 15:36:38 2008 +0100

    xen: Clear PG_pinned in release_{pt,pd}()
    
    Signed-off-by: Mark McLoughlin <markmc@redhat.com>
    Cc: xen-devel@lists.xensource.com
    Cc: Mark McLoughlin <markmc@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f16b056e5c56..27ee26aedf94 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -717,6 +717,7 @@ static void xen_release_ptpage(u32 pfn, unsigned level)
 				pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);
 			make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
 		}
+		ClearPagePinned(page);
 	}
 }
 

commit a684d69d15a8fafede7c5c0daac8c646bbee805c
Author: Mark McLoughlin <markmc@redhat.com>
Date:   Wed Apr 2 15:36:37 2008 +0100

    xen: Do not pin/unpin PMD pages
    
    i.e. with this simple test case:
    
        int fd = open("/dev/zero", O_RDONLY);
        munmap(mmap((void *)0x40000000, 0x1000_LEN, PROT_READ, MAP_PRIVATE, fd, 0), 0x1000);
        close(fd);
    
    we currently get:
    
       kernel BUG at arch/x86/xen/enlighten.c:678!
       ...
       EIP is at xen_release_pt+0x79/0xa9
       ...
       Call Trace:
        [<c041da25>] ? __pmd_free_tlb+0x1a/0x75
        [<c047a192>] ? free_pgd_range+0x1d2/0x2b5
        [<c047a2f3>] ? free_pgtables+0x7e/0x93
        [<c047b272>] ? unmap_region+0xb9/0xf5
        [<c047c1bd>] ? do_munmap+0x193/0x1f5
        [<c047c24f>] ? sys_munmap+0x30/0x3f
        [<c0408cce>] ? syscall_call+0x7/0xb
        =======================
    
    and xen complains:
    
      (XEN) mm.c:2241:d4 Mfn 1cc37 not pinned
    
    Further details at:
    
      https://bugzilla.redhat.com/436453
    
    Signed-off-by: Mark McLoughlin <markmc@redhat.com>
    Cc: xen-devel@lists.xensource.com
    Cc: Mark McLoughlin <markmc@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 16e2f8096a1a..f16b056e5c56 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -689,8 +689,6 @@ static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 			make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
 			if (level == PT_PTE)
 				pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
-			else if (level == PT_PMD)
-				pin_pagetable_pfn(MMUEXT_PIN_L2_TABLE, pfn);
 		} else
 			/* make sure there are no stray mappings of
 			   this page */
@@ -715,7 +713,8 @@ static void xen_release_ptpage(u32 pfn, unsigned level)
 
 	if (PagePinned(page)) {
 		if (!PageHighMem(page)) {
-			pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);
+			if (level == PT_PTE)
+				pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);
 			make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
 		}
 	}

commit f64337062c09c2c318fbcbf44ed1d739e8bc72ab
Author: Mark McLoughlin <markmc@redhat.com>
Date:   Wed Apr 2 15:36:36 2008 +0100

    xen: refactor xen_{alloc,release}_{pt,pd}()
    
    Signed-off-by: Mark McLoughlin <markmc@redhat.com>
    Cc: xen-devel@lists.xensource.com
    Cc: Mark McLoughlin <markmc@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index de4e6f05840b..16e2f8096a1a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -667,10 +667,10 @@ static void xen_release_pt_init(u32 pfn)
 	make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
 }
 
-static void pin_pagetable_pfn(unsigned level, unsigned long pfn)
+static void pin_pagetable_pfn(unsigned cmd, unsigned long pfn)
 {
 	struct mmuext_op op;
-	op.cmd = level;
+	op.cmd = cmd;
 	op.arg1.mfn = pfn_to_mfn(pfn);
 	if (HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF))
 		BUG();
@@ -687,7 +687,10 @@ static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 
 		if (!PageHighMem(page)) {
 			make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
-			pin_pagetable_pfn(level, pfn);
+			if (level == PT_PTE)
+				pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
+			else if (level == PT_PMD)
+				pin_pagetable_pfn(MMUEXT_PIN_L2_TABLE, pfn);
 		} else
 			/* make sure there are no stray mappings of
 			   this page */
@@ -697,16 +700,16 @@ static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 
 static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
 {
-	xen_alloc_ptpage(mm, pfn, MMUEXT_PIN_L1_TABLE);
+	xen_alloc_ptpage(mm, pfn, PT_PTE);
 }
 
 static void xen_alloc_pd(struct mm_struct *mm, u32 pfn)
 {
-	xen_alloc_ptpage(mm, pfn, MMUEXT_PIN_L2_TABLE);
+	xen_alloc_ptpage(mm, pfn, PT_PMD);
 }
 
 /* This should never happen until we're OK to use struct page */
-static void xen_release_pt(u32 pfn)
+static void xen_release_ptpage(u32 pfn, unsigned level)
 {
 	struct page *page = pfn_to_page(pfn);
 
@@ -718,6 +721,16 @@ static void xen_release_pt(u32 pfn)
 	}
 }
 
+static void xen_release_pt(u32 pfn)
+{
+	xen_release_ptpage(pfn, PT_PTE);
+}
+
+static void xen_release_pd(u32 pfn)
+{
+	xen_release_ptpage(pfn, PT_PMD);
+}
+
 #ifdef CONFIG_HIGHPTE
 static void *xen_kmap_atomic_pte(struct page *page, enum km_type type)
 {
@@ -838,7 +851,7 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 	pv_mmu_ops.alloc_pt = xen_alloc_pt;
 	pv_mmu_ops.alloc_pd = xen_alloc_pd;
 	pv_mmu_ops.release_pt = xen_release_pt;
-	pv_mmu_ops.release_pd = xen_release_pt;
+	pv_mmu_ops.release_pd = xen_release_pd;
 	pv_mmu_ops.set_pte = xen_set_pte;
 
 	setup_shared_info();

commit 2e8fe719b57bbdc9e313daed1204bb55fed3ed44
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Mar 17 16:36:53 2008 -0700

    xen: fix UP setup of shared_info
    
    We need to set up the shared_info pointer once we've mapped the real
    shared_info into its fixmap slot.  That needs to happen once the general
    pagetable setup has been done.  Previously, the UP shared_info was set
    up one in xen_start_kernel, but that was left pointing to the dummy
    shared info.  Unfortunately there's no really good place to do a later
    setup of the shared_info in UP, so just do it once the pagetable setup
    has been done.
    
    [ Stable: needed in 2.6.24.x ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable Kernel <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 1a20318c8cff..de4e6f05840b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -103,6 +103,7 @@ static void __init xen_vcpu_setup(int cpu)
 	int err;
 	struct vcpu_info *vcpup;
 
+	BUG_ON(HYPERVISOR_shared_info == &dummy_shared_info);
 	per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 
 	if (!have_vcpu_info_placement)
@@ -805,33 +806,43 @@ static __init void xen_pagetable_setup_start(pgd_t *base)
 			  PFN_DOWN(__pa(xen_start_info->pt_base)));
 }
 
-static __init void xen_pagetable_setup_done(pgd_t *base)
+static __init void setup_shared_info(void)
 {
-	/* This will work as long as patching hasn't happened yet
-	   (which it hasn't) */
-	pv_mmu_ops.alloc_pt = xen_alloc_pt;
-	pv_mmu_ops.alloc_pd = xen_alloc_pd;
-	pv_mmu_ops.release_pt = xen_release_pt;
-	pv_mmu_ops.release_pd = xen_release_pt;
-	pv_mmu_ops.set_pte = xen_set_pte;
-
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
+		unsigned long addr = fix_to_virt(FIX_PARAVIRT_BOOTMAP);
+
 		/*
 		 * Create a mapping for the shared info page.
 		 * Should be set_fixmap(), but shared_info is a machine
 		 * address with no corresponding pseudo-phys address.
 		 */
-		set_pte_mfn(fix_to_virt(FIX_PARAVIRT_BOOTMAP),
+		set_pte_mfn(addr,
 			    PFN_DOWN(xen_start_info->shared_info),
 			    PAGE_KERNEL);
 
-		HYPERVISOR_shared_info =
-			(struct shared_info *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);
-
+		HYPERVISOR_shared_info = (struct shared_info *)addr;
 	} else
 		HYPERVISOR_shared_info =
 			(struct shared_info *)__va(xen_start_info->shared_info);
 
+#ifndef CONFIG_SMP
+	/* In UP this is as good a place as any to set up shared info */
+	xen_setup_vcpu_info_placement();
+#endif
+}
+
+static __init void xen_pagetable_setup_done(pgd_t *base)
+{
+	/* This will work as long as patching hasn't happened yet
+	   (which it hasn't) */
+	pv_mmu_ops.alloc_pt = xen_alloc_pt;
+	pv_mmu_ops.alloc_pd = xen_alloc_pd;
+	pv_mmu_ops.release_pt = xen_release_pt;
+	pv_mmu_ops.release_pd = xen_release_pt;
+	pv_mmu_ops.set_pte = xen_set_pte;
+
+	setup_shared_info();
+
 	/* Actually pin the pagetable down, but we can't set PG_pinned
 	   yet because the page structures don't exist yet. */
 	{
@@ -1182,15 +1193,9 @@ asmlinkage void __init xen_start_kernel(void)
 	x86_write_percpu(xen_cr3, __pa(pgd));
 	x86_write_percpu(xen_current_cr3, __pa(pgd));
 
-#ifdef CONFIG_SMP
 	/* Don't do the full vcpu_info placement stuff until we have a
-	   possible map. */
+	   possible map and a non-dummy shared_info. */
 	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
-#else
-	/* May as well do it now, since there's no good time to call
-	   it later on UP. */
-	xen_setup_vcpu_info_placement();
-#endif
 
 	pv_info.kernel_rpl = 1;
 	if (xen_feature(XENFEAT_supervisor_mode_kernel))

commit 04c44a080d2f699a3042d4e743f7ad2ffae9d538
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Mar 17 16:36:52 2008 -0700

    xen: fix RMW when unmasking events
    
    xen_irq_enable_direct and xen_sysexit were using "andw $0x00ff,
    XEN_vcpu_info_pending(vcpu)" to unmask events and test for pending ones
    in one instuction.
    
    Unfortunately, the pending flag must be modified with a locked operation
    since it can be set by another CPU, and the unlocked form of this
    operation was causing the pending flag to get lost, allowing the processor
    to return to usermode with pending events and ultimately deadlock.
    
    The simple fix would be to make it a locked operation, but that's rather
    costly and unnecessary.  The fix here is to split the mask-clearing and
    pending-testing into two instructions; the interrupt window between
    them is of no concern because either way pending or new events will
    be processed.
    
    This should fix lingering bugs in using direct vcpu structure access too.
    
    [ Stable: needed in 2.6.24.x ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stable <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 8b9ee27805fd..1a20318c8cff 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -95,7 +95,7 @@ struct shared_info *HYPERVISOR_shared_info = (void *)&dummy_shared_info;
  *
  * 0: not available, 1: available
  */
-static int have_vcpu_info_placement = 0;
+static int have_vcpu_info_placement = 1;
 
 static void __init xen_vcpu_setup(int cpu)
 {

commit d40e705903397445c6861a0a56c23e5b2e8f9b9a
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Fri Feb 29 18:55:43 2008 +0100

    xen: mask out SEP from CPUID
    
    Fix 32-on-64 pvops kernel:
    
    we don't want userspace using syscall/sysenter, even if the hypervisor
    supports it, so mask it out from CPUID.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 49e5358f481a..8b9ee27805fd 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -153,6 +153,7 @@ static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 	if (*ax == 1)
 		maskedx = ~((1 << X86_FEATURE_APIC) |  /* disable APIC */
 			    (1 << X86_FEATURE_ACPI) |  /* disable ACPI */
+			    (1 << X86_FEATURE_SEP)  |  /* disable SEP */
 			    (1 << X86_FEATURE_ACC));   /* thermal monitoring */
 
 	asm(XEN_EMULATE_PREFIX "cpuid"

commit 2b5407811db755257ae53c75cc6b312ed5d2ad9e
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Wed Feb 13 16:20:35 2008 +0100

    xen: unpin initial Xen pagetable once we're finished with it
    
    Unpin the Xen-provided pagetable once we've finished with it, so it
    doesn't cause stray references which cause later swapper_pg_dir
    pagetable updates to fail.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Tested-by: Jody Belka <knew-linux@pimb.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index de647bc6e74d..49e5358f481a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -798,6 +798,10 @@ static __init void xen_pagetable_setup_start(pgd_t *base)
 	 * added to the table can be prepared properly for Xen.
 	 */
 	xen_write_cr3(__pa(base));
+
+	/* Unpin initial Xen pagetable */
+	pin_pagetable_pfn(MMUEXT_UNPIN_TABLE,
+			  PFN_DOWN(__pa(xen_start_info->pt_base)));
 }
 
 static __init void xen_pagetable_setup_done(pgd_t *base)

commit 1c70e9bd832642b712181e32d1bbf2436058a3df
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jan 30 13:33:39 2008 +0100

    xen: deal with pmd being allocated/freed
    
    Deal properly with pmd-level pages being allocated and freed
    dynamically.  We can handle them more or less the same as pte pages.
    
    Also, deal with early_ioremap pagetable manipulations.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 845b4fd94463..de647bc6e74d 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -658,6 +658,13 @@ static __init void xen_alloc_pt_init(struct mm_struct *mm, u32 pfn)
 	make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
 }
 
+/* Early release_pt assumes that all pts are pinned, since there's
+   only init_mm and anything attached to that is pinned. */
+static void xen_release_pt_init(u32 pfn)
+{
+	make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
+}
+
 static void pin_pagetable_pfn(unsigned level, unsigned long pfn)
 {
 	struct mmuext_op op;
@@ -669,7 +676,7 @@ static void pin_pagetable_pfn(unsigned level, unsigned long pfn)
 
 /* This needs to make sure the new pte page is pinned iff its being
    attached to a pinned pagetable. */
-static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
+static void xen_alloc_ptpage(struct mm_struct *mm, u32 pfn, unsigned level)
 {
 	struct page *page = pfn_to_page(pfn);
 
@@ -678,7 +685,7 @@ static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
 
 		if (!PageHighMem(page)) {
 			make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
-			pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
+			pin_pagetable_pfn(level, pfn);
 		} else
 			/* make sure there are no stray mappings of
 			   this page */
@@ -686,6 +693,16 @@ static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
 	}
 }
 
+static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
+{
+	xen_alloc_ptpage(mm, pfn, MMUEXT_PIN_L1_TABLE);
+}
+
+static void xen_alloc_pd(struct mm_struct *mm, u32 pfn)
+{
+	xen_alloc_ptpage(mm, pfn, MMUEXT_PIN_L2_TABLE);
+}
+
 /* This should never happen until we're OK to use struct page */
 static void xen_release_pt(u32 pfn)
 {
@@ -788,6 +805,9 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 	/* This will work as long as patching hasn't happened yet
 	   (which it hasn't) */
 	pv_mmu_ops.alloc_pt = xen_alloc_pt;
+	pv_mmu_ops.alloc_pd = xen_alloc_pd;
+	pv_mmu_ops.release_pt = xen_release_pt;
+	pv_mmu_ops.release_pd = xen_release_pt;
 	pv_mmu_ops.set_pte = xen_set_pte;
 
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
@@ -1011,10 +1031,10 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.pte_update_defer = paravirt_nop,
 
 	.alloc_pt = xen_alloc_pt_init,
-	.release_pt = xen_release_pt,
-	.alloc_pd = paravirt_nop,
+	.release_pt = xen_release_pt_init,
+	.alloc_pd = xen_alloc_pt_init,
 	.alloc_pd_clone = paravirt_nop,
-	.release_pd = paravirt_nop,
+	.release_pd = xen_release_pt_init,
 
 #ifdef CONFIG_HIGHPTE
 	.kmap_atomic_pte = xen_kmap_atomic_pte,

commit 4dbf7af6442a9a882855bed0d999659ac413e3ac
Author: Jan Beulich <jbeulich@novell.com>
Date:   Wed Jan 30 13:33:14 2008 +0100

    x86: adjust/fix LDT handling for Xen
    
    Based on patch from Jan Beulich <jbeulich@novell.com>.
    
    Don't rely on kmalloc(PAGE_SIZE) returning PAGE_SIZE aligned memory
    (Xen requires GDT *and* LDT to be page-aligned). Using the page
    allocator interface also removes the (albeit small) slab allocator
    overhead. The same change being done for 64-bits for consistency.
    
    Further, the Xen hypercall interface expects the LDT address to be
    virtual, not machine.
    
    [ Adjusted to unified ldt.c - Jeremy ]
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 72dd14d0685c..845b4fd94463 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -275,19 +275,12 @@ static unsigned long xen_store_tr(void)
 
 static void xen_set_ldt(const void *addr, unsigned entries)
 {
-	unsigned long linear_addr = (unsigned long)addr;
 	struct mmuext_op *op;
 	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
 
 	op = mcs.args;
 	op->cmd = MMUEXT_SET_LDT;
-	if (linear_addr) {
-		/* ldt my be vmalloced, use arbitrary_virt_to_machine */
-		xmaddr_t maddr;
-		maddr = arbitrary_virt_to_machine((unsigned long)addr);
-		linear_addr = (unsigned long)maddr.maddr;
-	}
-	op->arg1.linear_addr = linear_addr;
+	op->arg1.linear_addr = (unsigned long)addr;
 	op->arg2.nr_ents = entries;
 
 	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);

commit 75b8bb3e56ca09a467fbbe5229bc68627f7445be
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:13 2008 +0100

    x86: change write_ldt_entry signature
    
    this patch changes the signature of write_ldt_entry.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    CC: Zachary Amsden <zach@vmware.com>
    CC: Jeremy Fitzhardinge <Jeremy.Fitzhardinge.citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7f98c63f6381..72dd14d0685c 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -357,11 +357,11 @@ static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
 }
 
 static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
-				u32 low, u32 high)
+				const void *ptr)
 {
 	unsigned long lp = (unsigned long)&dt[entrynum];
 	xmaddr_t mach_lp = virt_to_machine(lp);
-	u64 entry = (u64)high << 32 | low;
+	u64 entry = *(u64 *)ptr;
 
 	preempt_disable();
 

commit 014b15be30c04622d130946ab7c0a9101b523a8a
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:13 2008 +0100

    x86: change write_gdt_entry signature.
    
    This patch changes the write_gdt_entry function signature.
    Instead of the old "a" and "b" parameters, it now receives
    a pointer to a desc_struct, and the size of the entry being
    handled. This is because x86_64 can have some 16-byte entries
    as well as 8-byte ones.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    CC: Zachary Amsden <zach@vmware.com>
    CC: Jeremy Fitzhardinge <Jeremy.Fitzhardinge.citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b7b7346d8cdc..7f98c63f6381 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -475,22 +475,21 @@ static void xen_load_idt(const struct desc_ptr *desc)
 /* Write a GDT descriptor entry.  Ignore LDT descriptors, since
    they're handled differently. */
 static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
-				u32 low, u32 high)
+				const void *desc, int type)
 {
 	preempt_disable();
 
-	switch ((high >> 8) & 0xff) {
-	case DESCTYPE_LDT:
-	case DESCTYPE_TSS:
+	switch (type) {
+	case DESC_LDT:
+	case DESC_TSS:
 		/* ignore */
 		break;
 
 	default: {
 		xmaddr_t maddr = virt_to_machine(&dt[entry]);
-		u64 desc = (u64)high << 32 | low;
 
 		xen_mc_flush();
-		if (HYPERVISOR_update_descriptor(maddr.maddr, desc))
+		if (HYPERVISOR_update_descriptor(maddr.maddr, *(u64 *)desc))
 			BUG();
 	}
 

commit 8d947344c47a40626730bb80d136d8daac9f2060
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:12 2008 +0100

    x86: change write_idt_entry signature
    
    this patch changes write_idt_entry signature. It now takes a gate_desc
    instead of the a and b parameters. It will allow it to be later unified
    between i386 and x86_64.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    CC: Zachary Amsden <zach@vmware.com>
    CC: Jeremy Fitzhardinge <Jeremy.Fitzhardinge.citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index c32e0fd0f838..b7b7346d8cdc 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -399,8 +399,7 @@ static DEFINE_PER_CPU(struct desc_ptr, idt_desc);
 
 /* Set an IDT entry.  If the entry is part of the current IDT, then
    also update Xen. */
-static void xen_write_idt_entry(struct desc_struct *dt, int entrynum,
-				u32 low, u32 high)
+static void xen_write_idt_entry(gate_desc *dt, int entrynum, const gate_desc *g)
 {
 	unsigned long p = (unsigned long)&dt[entrynum];
 	unsigned long start, end;
@@ -412,14 +411,15 @@ static void xen_write_idt_entry(struct desc_struct *dt, int entrynum,
 
 	xen_mc_flush();
 
-	write_dt_entry(dt, entrynum, low, high);
+	native_write_idt_entry(dt, entrynum, g);
 
 	if (p >= start && (p + 8) <= end) {
 		struct trap_info info[2];
+		u32 *desc = (u32 *)g;
 
 		info[1].address = 0;
 
-		if (cvt_gate_to_trap(entrynum, low, high, &info[0]))
+		if (cvt_gate_to_trap(entrynum, desc[0], desc[1], &info[0]))
 			if (HYPERVISOR_set_trap_table(info))
 				BUG();
 	}

commit 6b68f01baa810e9f63fbf39e9d5c3ef1d94a966f
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:12 2008 +0100

    x86: unify struct desc_ptr
    
    This patch unifies struct desc_ptr between i386 and x86_64.
    They can be expressed in the exact same way in C code, only
    having to change the name of one of them. As Xgt_desc_struct
    is ugly and big, this is the one that goes away.
    
    There's also a padding field in i386, but it is not really
    needed in the C structure definition.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d81e8d709102..c32e0fd0f838 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -295,7 +295,7 @@ static void xen_set_ldt(const void *addr, unsigned entries)
 	xen_mc_issue(PARAVIRT_LAZY_CPU);
 }
 
-static void xen_load_gdt(const struct Xgt_desc_struct *dtr)
+static void xen_load_gdt(const struct desc_ptr *dtr)
 {
 	unsigned long *frames;
 	unsigned long va = dtr->address;
@@ -395,7 +395,7 @@ static int cvt_gate_to_trap(int vector, u32 low, u32 high,
 }
 
 /* Locations of each CPU's IDT */
-static DEFINE_PER_CPU(struct Xgt_desc_struct, idt_desc);
+static DEFINE_PER_CPU(struct desc_ptr, idt_desc);
 
 /* Set an IDT entry.  If the entry is part of the current IDT, then
    also update Xen. */
@@ -427,7 +427,7 @@ static void xen_write_idt_entry(struct desc_struct *dt, int entrynum,
 	preempt_enable();
 }
 
-static void xen_convert_trap_info(const struct Xgt_desc_struct *desc,
+static void xen_convert_trap_info(const struct desc_ptr *desc,
 				  struct trap_info *traps)
 {
 	unsigned in, out, count;
@@ -446,7 +446,7 @@ static void xen_convert_trap_info(const struct Xgt_desc_struct *desc,
 
 void xen_copy_trap_info(struct trap_info *traps)
 {
-	const struct Xgt_desc_struct *desc = &__get_cpu_var(idt_desc);
+	const struct desc_ptr *desc = &__get_cpu_var(idt_desc);
 
 	xen_convert_trap_info(desc, traps);
 }
@@ -454,7 +454,7 @@ void xen_copy_trap_info(struct trap_info *traps)
 /* Load a new IDT into Xen.  In principle this can be per-CPU, so we
    hold a spinlock to protect the static traps[] array (static because
    it avoids allocation, and saves stack space). */
-static void xen_load_idt(const struct Xgt_desc_struct *desc)
+static void xen_load_idt(const struct desc_ptr *desc)
 {
 	static DEFINE_SPINLOCK(lock);
 	static struct trap_info traps[257];

commit faca62273b602ab482fb7d3d940dbf41ef08b00e
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Jan 30 13:31:02 2008 +0100

    x86: use generic register name in the thread and tss structures
    
    This changes size-specific register names (eip/rip, esp/rsp, etc.) to
    generic names in the thread and tss structures.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 29517faaa735..d81e8d709102 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -499,11 +499,11 @@ static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
 	preempt_enable();
 }
 
-static void xen_load_esp0(struct tss_struct *tss,
+static void xen_load_sp0(struct tss_struct *tss,
 			  struct thread_struct *thread)
 {
 	struct multicall_space mcs = xen_mc_entry(0);
-	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread->esp0);
+	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread->sp0);
 	xen_mc_issue(PARAVIRT_LAZY_CPU);
 }
 
@@ -968,7 +968,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.write_ldt_entry = xen_write_ldt_entry,
 	.write_gdt_entry = xen_write_gdt_entry,
 	.write_idt_entry = xen_write_idt_entry,
-	.load_esp0 = xen_load_esp0,
+	.load_sp0 = xen_load_sp0,
 
 	.set_iopl_mask = xen_set_iopl_mask,
 	.io_delay = xen_io_delay,

commit 65ea5b0349903585bfed9720fa06f5edb4f1cd25
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Jan 30 13:30:56 2008 +0100

    x86: rename the struct pt_regs members for 32/64-bit consistency
    
    We have a lot of code which differs only by the naming of specific
    members of structures that contain registers.  In order to enable
    additional unifications, this patch drops the e- or r- size prefix
    from the register names in struct pt_regs, and drops the x- prefixes
    for segment registers on the 32-bit side.
    
    This patch also performs the equivalent renames in some additional
    places that might be candidates for unification in the future.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index d3574485cb15..29517faaa735 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -141,8 +141,8 @@ static void __init xen_banner(void)
 	printk(KERN_INFO "Hypervisor signature: %s\n", xen_start_info->magic);
 }
 
-static void xen_cpuid(unsigned int *eax, unsigned int *ebx,
-		      unsigned int *ecx, unsigned int *edx)
+static void xen_cpuid(unsigned int *ax, unsigned int *bx,
+		      unsigned int *cx, unsigned int *dx)
 {
 	unsigned maskedx = ~0;
 
@@ -150,18 +150,18 @@ static void xen_cpuid(unsigned int *eax, unsigned int *ebx,
 	 * Mask out inconvenient features, to try and disable as many
 	 * unsupported kernel subsystems as possible.
 	 */
-	if (*eax == 1)
+	if (*ax == 1)
 		maskedx = ~((1 << X86_FEATURE_APIC) |  /* disable APIC */
 			    (1 << X86_FEATURE_ACPI) |  /* disable ACPI */
 			    (1 << X86_FEATURE_ACC));   /* thermal monitoring */
 
 	asm(XEN_EMULATE_PREFIX "cpuid"
-		: "=a" (*eax),
-		  "=b" (*ebx),
-		  "=c" (*ecx),
-		  "=d" (*edx)
-		: "0" (*eax), "2" (*ecx));
-	*edx &= maskedx;
+		: "=a" (*ax),
+		  "=b" (*bx),
+		  "=c" (*cx),
+		  "=d" (*dx)
+		: "0" (*ax), "2" (*cx));
+	*dx &= maskedx;
 }
 
 static void xen_set_debugreg(int reg, unsigned long val)

commit 6abcd98ffafbff81f0bfd7ee1d129e634af13245
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:30:33 2008 +0100

    x86: irqflags consolidation
    
    This patch consolidates the irqflags include files containing common
    paravirt definitions. The native definition for interrupt handling, halt,
    and such, are the same for 32 and 64 bit, and they are kept in irqflags.h.
    the differences are split in the arch-specific files.
    
    The syscall function, irq_enable_sysexit, has a very specific i386 naming,
    and its name is then changed to a more general one.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 00829401389e..d3574485cb15 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -953,7 +953,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.read_pmc = native_read_pmc,
 
 	.iret = (void *)&hypercall_page[__HYPERVISOR_iret],
-	.irq_enable_sysexit = NULL,  /* never called */
+	.irq_enable_syscall_ret = NULL,  /* never called */
 
 	.load_tr_desc = paravirt_nop,
 	.set_ldt = xen_set_ldt,

commit 42e0a9aa5d467188687c6b705412578e53c14af6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 30 13:30:15 2008 +0100

    x86: use u32 for some lapic functions
    
    Use u32 so 32 and 64bit have the same interface.
    
    Andrew Morton: xen, lguest build fixes
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 79ad15252150..00829401389e 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -521,12 +521,12 @@ static void xen_io_delay(void)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
-static unsigned long xen_apic_read(unsigned long reg)
+static u32 xen_apic_read(unsigned long reg)
 {
 	return 0;
 }
 
-static void xen_apic_write(unsigned long reg, unsigned long val)
+static void xen_apic_write(unsigned long reg, u32 val)
 {
 	/* Warn to see if there's any stray references */
 	WARN_ON(1);

commit f9c4cfe95451b7f4531d73d2a8a6651cba9760ab
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jan 23 16:07:17 2008 -0800

    xen: disable vcpu_info placement for now
    
    There have been several reports of Xen guest domains locking up when
    using vcpu_info structure placement.  Disable it for now.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index b6af3ea43c73..79ad15252150 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -95,7 +95,7 @@ struct shared_info *HYPERVISOR_shared_info = (void *)&dummy_shared_info;
  *
  * 0: not available, 1: available
  */
-static int have_vcpu_info_placement = 1;
+static int have_vcpu_info_placement = 0;
 
 static void __init xen_vcpu_setup(int cpu)
 {

commit 7999f4b4e56d36265d789d52bb0c0bb0167346a7
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Dec 10 13:00:41 2007 -0800

    xen: relax signature check
    
    Some versions of Xen 3.x set their magic number to "xen-3.[12]", so
    relax the test to match them.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 94c39aaf695f..b6af3ea43c73 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1131,7 +1131,7 @@ asmlinkage void __init xen_start_kernel(void)
 	if (!xen_start_info)
 		return;
 
-	BUG_ON(memcmp(xen_start_info->magic, "xen-3.0", 7) != 0);
+	BUG_ON(memcmp(xen_start_info->magic, "xen-3", 5) != 0);
 
 	/* Install Xen paravirt ops */
 	pv_info = xen_info;

commit fb9fc395174138983a49f2da982ed14caabbe741
Merge: 0eafaae84e21 ace2e92e1931
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed Oct 17 11:10:11 2007 -0700

    Merge branch 'xen-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'xen-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xfs: eagerly remove vmap mappings to avoid upsetting Xen
      xen: add some debug output for failed multicalls
      xen: fix incorrect vcpu_register_vcpu_info hypercall argument
      xen: ask the hypervisor how much space it needs reserved
      xen: lock pte pages while pinning/unpinning
      xen: deal with stale cr3 values when unpinning pagetables
      xen: add batch completion callbacks
      xen: yield to IPI target if necessary
      Clean up duplicate includes in arch/i386/xen/
      remove dead code in pgtable_cache_init
      paravirt: clean up lazy mode handling
      paravirt: refactor struct paravirt_ops into smaller pv_*_ops

commit 30c826451d3e5bbc6e11bba0e7fee5d2f49d9b75
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Oct 15 17:13:22 2007 -0700

    [x86] remove uses of magic macros for boot_params access
    
    Instead of using magic macros for boot_params access, simply use the
    boot_params structure.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f01bfcd4bdee..493a083f6886 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1137,9 +1137,10 @@ asmlinkage void __init xen_start_kernel(void)
 	new_cpu_data.x86_capability[0] = cpuid_edx(1);
 
 	/* Poke various useful things into boot_params */
-	LOADER_TYPE = (9 << 4) | 0;
-	INITRD_START = xen_start_info->mod_start ? __pa(xen_start_info->mod_start) : 0;
-	INITRD_SIZE = xen_start_info->mod_len;
+	boot_params.hdr.type_of_loader = (9 << 4) | 0;
+	boot_params.hdr.ramdisk_image = xen_start_info->mod_start
+		? __pa(xen_start_info->mod_start) : 0;
+	boot_params.hdr.ramdisk_size = xen_start_info->mod_len;
 
 	/* Start the world */
 	start_kernel();

commit e3d2697669abbe26c08dc9b95e2a71c634d096ed
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Tue Oct 16 11:51:31 2007 -0700

    xen: fix incorrect vcpu_register_vcpu_info hypercall argument
    
    The kernel's copy of struct vcpu_register_vcpu_info was out of date,
    at best causing the hypercall to fail and the guest kernel to fall
    back to the old mechanism, or worse, causing random memory corruption.
    
    [ Stable folks: applies to 2.6.23 ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Stable Kernel <stable@kernel.org>
    Cc: Morten =?utf-8?q?B=C3=B8geskov?= <xen-users@morten.bogeskov.dk>
    Cc: Mark Williamson <mark.williamson@cl.cam.ac.uk>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index bc7bf5f8d077..c89e5b407f90 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -113,7 +113,7 @@ static void __init xen_vcpu_setup(int cpu)
 	info.mfn = virt_to_mfn(vcpup);
 	info.offset = offset_in_page(vcpup);
 
-	printk(KERN_DEBUG "trying to map vcpu_info %d at %p, mfn %x, offset %d\n",
+	printk(KERN_DEBUG "trying to map vcpu_info %d at %p, mfn %llx, offset %d\n",
 	       cpu, vcpup, info.mfn, info.offset);
 
 	/* Check to see if the hypervisor will put the vcpu_info

commit fb1d84043ca73212b08ff57608f51b372529e6d6
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Tue Oct 16 11:51:31 2007 -0700

    xen: ask the hypervisor how much space it needs reserved
    
    Ask the hypervisor how much space it needs reserved, since 32-on-64
    doesn't need any space, and it may change in future.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 4186cb6a7f5a..bc7bf5f8d077 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1112,6 +1112,17 @@ static const struct machine_ops __initdata xen_machine_ops = {
 };
 
 
+static void __init xen_reserve_top(void)
+{
+	unsigned long top = HYPERVISOR_VIRT_START;
+	struct xen_platform_parameters pp;
+
+	if (HYPERVISOR_xen_version(XENVER_platform_parameters, &pp) == 0)
+		top = pp.virt_start;
+
+	reserve_top_address(-top + 2 * PAGE_SIZE);
+}
+
 /* First C function to be called on Xen boot */
 asmlinkage void __init xen_start_kernel(void)
 {
@@ -1169,7 +1180,7 @@ asmlinkage void __init xen_start_kernel(void)
 		pv_info.kernel_rpl = 0;
 
 	/* set the limit of our address space */
-	reserve_top_address(-HYPERVISOR_VIRT_START + 2 * PAGE_SIZE);
+	xen_reserve_top();
 
 	/* set up basic CPUID stuff */
 	cpu_detect(&new_cpu_data);

commit 74260714c56de4f967fcb2f17a8656bc574b75be
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Tue Oct 16 11:51:30 2007 -0700

    xen: lock pte pages while pinning/unpinning
    
    When a pagetable is created, it is made globally visible in the rmap
    prio tree before it is pinned via arch_dup_mmap(), and remains in the
    rmap tree while it is unpinned with arch_exit_mmap().
    
    This means that other CPUs may race with the pinning/unpinning
    process, and see a pte between when it gets marked RO and actually
    pinned, causing any pte updates to fail with write-protect faults.
    
    As a result, all pte pages must be properly locked, and only unlocked
    once the pinning/unpinning process has finished.
    
    In order to avoid taking spinlocks for the whole pagetable - which may
    overflow the PREEMPT_BITS portion of preempt counter - it locks and pins
    each pte page individually, and then finally pins the whole pagetable.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Hugh Dickens <hugh@veritas.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Keir Fraser <keir@xensource.com>
    Cc: Jan Beulich <jbeulich@novell.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index e334bf7cb327..4186cb6a7f5a 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -666,6 +666,15 @@ static __init void xen_alloc_pt_init(struct mm_struct *mm, u32 pfn)
 	make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
 }
 
+static void pin_pagetable_pfn(unsigned level, unsigned long pfn)
+{
+	struct mmuext_op op;
+	op.cmd = level;
+	op.arg1.mfn = pfn_to_mfn(pfn);
+	if (HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF))
+		BUG();
+}
+
 /* This needs to make sure the new pte page is pinned iff its being
    attached to a pinned pagetable. */
 static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
@@ -675,9 +684,10 @@ static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
 	if (PagePinned(virt_to_page(mm->pgd))) {
 		SetPagePinned(page);
 
-		if (!PageHighMem(page))
+		if (!PageHighMem(page)) {
 			make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
-		else
+			pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);
+		} else
 			/* make sure there are no stray mappings of
 			   this page */
 			kmap_flush_unused();
@@ -690,8 +700,10 @@ static void xen_release_pt(u32 pfn)
 	struct page *page = pfn_to_page(pfn);
 
 	if (PagePinned(page)) {
-		if (!PageHighMem(page))
+		if (!PageHighMem(page)) {
+			pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);
 			make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
+		}
 	}
 }
 
@@ -806,15 +818,15 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 	/* Actually pin the pagetable down, but we can't set PG_pinned
 	   yet because the page structures don't exist yet. */
 	{
-		struct mmuext_op op;
+		unsigned level;
+
 #ifdef CONFIG_X86_PAE
-		op.cmd = MMUEXT_PIN_L3_TABLE;
+		level = MMUEXT_PIN_L3_TABLE;
 #else
-		op.cmd = MMUEXT_PIN_L3_TABLE;
+		level = MMUEXT_PIN_L2_TABLE;
 #endif
-		op.arg1.mfn = pfn_to_mfn(PFN_DOWN(__pa(base)));
-		if (HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF))
-			BUG();
+
+		pin_pagetable_pfn(level, PFN_DOWN(__pa(base)));
 	}
 }
 

commit 9f79991d4186089e228274196413572cc000143b
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Tue Oct 16 11:51:30 2007 -0700

    xen: deal with stale cr3 values when unpinning pagetables
    
    When a pagetable is no longer in use, it must be unpinned so that its
    pages can be freed.  However, this is only possible if there are no
    stray uses of the pagetable.  The code currently deals with all the
    usual cases, but there's a rare case where a vcpu is changing cr3, but
    is doing so lazily, and the change hasn't actually happened by the time
    the pagetable is unpinned, even though it appears to have been completed.
    
    This change adds a second per-cpu cr3 variable - xen_current_cr3 -
    which tracks the actual state of the vcpu cr3.  It is only updated once
    the actual hypercall to set cr3 has been completed.  Other processors
    wishing to unpin a pagetable can check other vcpu's xen_current_cr3
    values to see if any cross-cpu IPIs are needed to clean things up.
    
    [ Stable folks: 2.6.23 bugfix ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Stable Kernel <stable@kernel.org>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 75fd36971d0b..e334bf7cb327 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -53,7 +53,23 @@ EXPORT_SYMBOL_GPL(hypercall_page);
 
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
-DEFINE_PER_CPU(unsigned long, xen_cr3);
+
+/*
+ * Note about cr3 (pagetable base) values:
+ *
+ * xen_cr3 contains the current logical cr3 value; it contains the
+ * last set cr3.  This may not be the current effective cr3, because
+ * its update may be being lazily deferred.  However, a vcpu looking
+ * at its own cr3 can use this value knowing that it everything will
+ * be self-consistent.
+ *
+ * xen_current_cr3 contains the actual vcpu cr3; it is set once the
+ * hypercall to set the vcpu cr3 is complete (so it may be a little
+ * out of date, but it will never be set early).  If one vcpu is
+ * looking at another vcpu's cr3 value, it should use this variable.
+ */
+DEFINE_PER_CPU(unsigned long, xen_cr3);	 /* cr3 stored as physaddr */
+DEFINE_PER_CPU(unsigned long, xen_current_cr3);	 /* actual vcpu cr3 */
 
 struct start_info *xen_start_info;
 EXPORT_SYMBOL_GPL(xen_start_info);
@@ -610,32 +626,36 @@ static unsigned long xen_read_cr3(void)
 	return x86_read_percpu(xen_cr3);
 }
 
+static void set_current_cr3(void *v)
+{
+	x86_write_percpu(xen_current_cr3, (unsigned long)v);
+}
+
 static void xen_write_cr3(unsigned long cr3)
 {
+	struct mmuext_op *op;
+	struct multicall_space mcs;
+	unsigned long mfn = pfn_to_mfn(PFN_DOWN(cr3));
+
 	BUG_ON(preemptible());
 
-	if (cr3 == x86_read_percpu(xen_cr3)) {
-		/* just a simple tlb flush */
-		xen_flush_tlb();
-		return;
-	}
+	mcs = xen_mc_entry(sizeof(*op));  /* disables interrupts */
 
+	/* Update while interrupts are disabled, so its atomic with
+	   respect to ipis */
 	x86_write_percpu(xen_cr3, cr3);
 
+	op = mcs.args;
+	op->cmd = MMUEXT_NEW_BASEPTR;
+	op->arg1.mfn = mfn;
 
-	{
-		struct mmuext_op *op;
-		struct multicall_space mcs = xen_mc_entry(sizeof(*op));
-		unsigned long mfn = pfn_to_mfn(PFN_DOWN(cr3));
-
-		op = mcs.args;
-		op->cmd = MMUEXT_NEW_BASEPTR;
-		op->arg1.mfn = mfn;
+	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
 
-		MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
+	/* Update xen_update_cr3 once the batch has actually
+	   been submitted. */
+	xen_mc_callback(set_current_cr3, (void *)cr3);
 
-		xen_mc_issue(PARAVIRT_LAZY_CPU);
-	}
+	xen_mc_issue(PARAVIRT_LAZY_CPU);  /* interrupts restored */
 }
 
 /* Early in boot, while setting up the initial pagetable, assume
@@ -1120,6 +1140,7 @@ asmlinkage void __init xen_start_kernel(void)
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 	x86_write_percpu(xen_cr3, __pa(pgd));
+	x86_write_percpu(xen_current_cr3, __pa(pgd));
 
 #ifdef CONFIG_SMP
 	/* Don't do the full vcpu_info placement stuff until we have a

commit d626a1f1cbbdfac90c529216e40a4fa2a22ecc3d
Author: Jesper Juhl <jesper.juhl@gmail.com>
Date:   Tue Oct 16 11:51:29 2007 -0700

    Clean up duplicate includes in arch/i386/xen/
    
    This patch cleans up duplicate includes in
            arch/i386/xen/
    
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 7171a0736071..75fd36971d0b 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -25,7 +25,6 @@
 #include <linux/mm.h>
 #include <linux/page-flags.h>
 #include <linux/highmem.h>
-#include <linux/smp.h>
 
 #include <xen/interface/xen.h>
 #include <xen/interface/physdev.h>

commit 8965c1c0950d459d99b8b81dfc1ab02e3d2cfb08
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Tue Oct 16 11:51:29 2007 -0700

    paravirt: clean up lazy mode handling
    
    Currently, the set_lazy_mode pv_op is overloaded with 5 functions:
     1. enter lazy cpu mode
     2. leave lazy cpu mode
     3. enter lazy mmu mode
     4. leave lazy mmu mode
     5. flush pending batched operations
    
    This complicates each paravirt backend, since it needs to deal with
    all the possible state transitions, handling flushing, etc. In
    particular, flushing is quite distinct from the other 4 functions, and
    seems to just cause complication.
    
    This patch removes the set_lazy_mode operation, and adds "enter" and
    "leave" lazy mode operations on mmu_ops and cpu_ops.  All the logic
    associated with enter and leaving lazy states is now in common code
    (basically BUG_ONs to make sure that no mode is current when entering
    a lazy mode, and make sure that the mode is current when leaving).
    Also, flush is handled in a common way, by simply leaving and
    re-entering the lazy mode.
    
    The result is that the Xen, lguest and VMI lazy mode implementations
    are much simpler.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Zach Amsden <zach@vmware.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Avi Kivity <avi@qumranet.com>
    Cc: Anthony Liguory <aliguori@us.ibm.com>
    Cc: "Glauber de Oliveira Costa" <glommer@gmail.com>
    Cc: Jun Nakajima <jun.nakajima@intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index 3d3bf05dec7f..7171a0736071 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -52,8 +52,6 @@
 
 EXPORT_SYMBOL_GPL(hypercall_page);
 
-DEFINE_PER_CPU(enum paravirt_lazy_mode, xen_lazy_mode);
-
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 DEFINE_PER_CPU(unsigned long, xen_cr3);
@@ -249,29 +247,10 @@ static void xen_halt(void)
 		xen_safe_halt();
 }
 
-static void xen_set_lazy_mode(enum paravirt_lazy_mode mode)
+static void xen_leave_lazy(void)
 {
-	BUG_ON(preemptible());
-
-	switch (mode) {
-	case PARAVIRT_LAZY_NONE:
-		BUG_ON(x86_read_percpu(xen_lazy_mode) == PARAVIRT_LAZY_NONE);
-		break;
-
-	case PARAVIRT_LAZY_MMU:
-	case PARAVIRT_LAZY_CPU:
-		BUG_ON(x86_read_percpu(xen_lazy_mode) != PARAVIRT_LAZY_NONE);
-		break;
-
-	case PARAVIRT_LAZY_FLUSH:
-		/* flush if necessary, but don't change state */
-		if (x86_read_percpu(xen_lazy_mode) != PARAVIRT_LAZY_NONE)
-			xen_mc_flush();
-		return;
-	}
-
+	paravirt_leave_lazy(paravirt_get_lazy_mode());
 	xen_mc_flush();
-	x86_write_percpu(xen_lazy_mode, mode);
 }
 
 static unsigned long xen_store_tr(void)
@@ -358,7 +337,7 @@ static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
 	 * loaded properly.  This will go away as soon as Xen has been
 	 * modified to not save/restore %gs for normal hypercalls.
 	 */
-	if (xen_get_lazy_mode() == PARAVIRT_LAZY_CPU)
+	if (paravirt_get_lazy_mode() == PARAVIRT_LAZY_CPU)
 		loadsegment(gs, 0);
 }
 
@@ -962,6 +941,11 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 
 	.set_iopl_mask = xen_set_iopl_mask,
 	.io_delay = xen_io_delay,
+
+	.lazy_mode = {
+		.enter = paravirt_enter_lazy_cpu,
+		.leave = xen_leave_lazy,
+	},
 };
 
 static const struct pv_irq_ops xen_irq_ops __initdata = {
@@ -1037,10 +1021,11 @@ static const struct pv_mmu_ops xen_mmu_ops __initdata = {
 	.activate_mm = xen_activate_mm,
 	.dup_mmap = xen_dup_mmap,
 	.exit_mmap = xen_exit_mmap,
-};
 
-static const struct pv_misc_ops xen_misc_ops __initdata = {
-	.set_lazy_mode = xen_set_lazy_mode,
+	.lazy_mode = {
+		.enter = paravirt_enter_lazy_mmu,
+		.leave = xen_leave_lazy,
+	},
 };
 
 #ifdef CONFIG_SMP
@@ -1114,7 +1099,6 @@ asmlinkage void __init xen_start_kernel(void)
 	pv_irq_ops = xen_irq_ops;
 	pv_apic_ops = xen_apic_ops;
 	pv_mmu_ops = xen_mmu_ops;
-	pv_misc_ops = xen_misc_ops;
 
 	machine_ops = xen_machine_ops;
 

commit 93b1eab3d29e7ea32ee583de3362da84db06ded8
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Tue Oct 16 11:51:29 2007 -0700

    paravirt: refactor struct paravirt_ops into smaller pv_*_ops
    
    This patch refactors the paravirt_ops structure into groups of
    functionally related ops:
    
    pv_info - random info, rather than function entrypoints
    pv_init_ops - functions used at boot time (some for module_init too)
    pv_misc_ops - lazy mode, which didn't fit well anywhere else
    pv_time_ops - time-related functions
    pv_cpu_ops - various privileged instruction ops
    pv_irq_ops - operations for managing interrupt state
    pv_apic_ops - APIC operations
    pv_mmu_ops - operations for managing pagetables
    
    There are several motivations for this:
    
    1. Some of these ops will be general to all x86, and some will be
       i386/x86-64 specific.  This makes it easier to share common stuff
       while allowing separate implementations where needed.
    
    2. At the moment we must export all of paravirt_ops, but modules only
       need selected parts of it.  This allows us to export on a case by case
       basis (and also choose which export license we want to apply).
    
    3. Functional groupings make things a bit more readable.
    
    Struct paravirt_ops is now only used as a template to generate
    patch-site identifiers, and to extract function pointers for inserting
    into jmp/calls when patching.  It is only instantiated when needed.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Zach Amsden <zach@vmware.com>
    Cc: Avi Kivity <avi@qumranet.com>
    Cc: Anthony Liguory <aliguori@us.ibm.com>
    Cc: "Glauber de Oliveira Costa" <glommer@gmail.com>
    Cc: Jun Nakajima <jun.nakajima@intel.com>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index f01bfcd4bdee..3d3bf05dec7f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -124,7 +124,7 @@ static void __init xen_vcpu_setup(int cpu)
 static void __init xen_banner(void)
 {
 	printk(KERN_INFO "Booting paravirtualized kernel on %s\n",
-	       paravirt_ops.name);
+	       pv_info.name);
 	printk(KERN_INFO "Hypervisor signature: %s\n", xen_start_info->magic);
 }
 
@@ -738,7 +738,7 @@ static __init void xen_pagetable_setup_start(pgd_t *base)
 	pgd_t *xen_pgd = (pgd_t *)xen_start_info->pt_base;
 
 	/* special set_pte for pagetable initialization */
-	paravirt_ops.set_pte = xen_set_pte_init;
+	pv_mmu_ops.set_pte = xen_set_pte_init;
 
 	init_mm.pgd = base;
 	/*
@@ -785,8 +785,8 @@ static __init void xen_pagetable_setup_done(pgd_t *base)
 {
 	/* This will work as long as patching hasn't happened yet
 	   (which it hasn't) */
-	paravirt_ops.alloc_pt = xen_alloc_pt;
-	paravirt_ops.set_pte = xen_set_pte;
+	pv_mmu_ops.alloc_pt = xen_alloc_pt;
+	pv_mmu_ops.set_pte = xen_set_pte;
 
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
 		/*
@@ -833,12 +833,12 @@ void __init xen_setup_vcpu_info_placement(void)
 	if (have_vcpu_info_placement) {
 		printk(KERN_INFO "Xen: using vcpu_info placement\n");
 
-		paravirt_ops.save_fl = xen_save_fl_direct;
-		paravirt_ops.restore_fl = xen_restore_fl_direct;
-		paravirt_ops.irq_disable = xen_irq_disable_direct;
-		paravirt_ops.irq_enable = xen_irq_enable_direct;
-		paravirt_ops.read_cr2 = xen_read_cr2_direct;
-		paravirt_ops.iret = xen_iret_direct;
+		pv_irq_ops.save_fl = xen_save_fl_direct;
+		pv_irq_ops.restore_fl = xen_restore_fl_direct;
+		pv_irq_ops.irq_disable = xen_irq_disable_direct;
+		pv_irq_ops.irq_enable = xen_irq_enable_direct;
+		pv_mmu_ops.read_cr2 = xen_read_cr2_direct;
+		pv_cpu_ops.iret = xen_iret_direct;
 	}
 }
 
@@ -850,8 +850,8 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 
 	start = end = reloc = NULL;
 
-#define SITE(x)								\
-	case PARAVIRT_PATCH(x):						\
+#define SITE(op, x)							\
+	case PARAVIRT_PATCH(op.x):					\
 	if (have_vcpu_info_placement) {					\
 		start = (char *)xen_##x##_direct;			\
 		end = xen_##x##_direct_end;				\
@@ -860,10 +860,10 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 	goto patch_site
 
 	switch (type) {
-		SITE(irq_enable);
-		SITE(irq_disable);
-		SITE(save_fl);
-		SITE(restore_fl);
+		SITE(pv_irq_ops, irq_enable);
+		SITE(pv_irq_ops, irq_disable);
+		SITE(pv_irq_ops, save_fl);
+		SITE(pv_irq_ops, restore_fl);
 #undef SITE
 
 	patch_site:
@@ -895,26 +895,32 @@ static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
 	return ret;
 }
 
-static const struct paravirt_ops xen_paravirt_ops __initdata = {
+static const struct pv_info xen_info __initdata = {
 	.paravirt_enabled = 1,
 	.shared_kernel_pmd = 0,
 
 	.name = "Xen",
-	.banner = xen_banner,
+};
 
+static const struct pv_init_ops xen_init_ops __initdata = {
 	.patch = xen_patch,
 
+	.banner = xen_banner,
 	.memory_setup = xen_memory_setup,
 	.arch_setup = xen_arch_setup,
-	.init_IRQ = xen_init_IRQ,
 	.post_allocator_init = xen_mark_init_mm_pinned,
+};
 
+static const struct pv_time_ops xen_time_ops __initdata = {
 	.time_init = xen_time_init,
+
 	.set_wallclock = xen_set_wallclock,
 	.get_wallclock = xen_get_wallclock,
 	.get_cpu_khz = xen_cpu_khz,
 	.sched_clock = xen_sched_clock,
+};
 
+static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.cpuid = xen_cpuid,
 
 	.set_debugreg = xen_set_debugreg,
@@ -925,22 +931,10 @@ static const struct paravirt_ops xen_paravirt_ops __initdata = {
 	.read_cr0 = native_read_cr0,
 	.write_cr0 = native_write_cr0,
 
-	.read_cr2 = xen_read_cr2,
-	.write_cr2 = xen_write_cr2,
-
-	.read_cr3 = xen_read_cr3,
-	.write_cr3 = xen_write_cr3,
-
 	.read_cr4 = native_read_cr4,
 	.read_cr4_safe = native_read_cr4_safe,
 	.write_cr4 = xen_write_cr4,
 
-	.save_fl = xen_save_fl,
-	.restore_fl = xen_restore_fl,
-	.irq_disable = xen_irq_disable,
-	.irq_enable = xen_irq_enable,
-	.safe_halt = xen_safe_halt,
-	.halt = xen_halt,
 	.wbinvd = native_wbinvd,
 
 	.read_msr = native_read_msr_safe,
@@ -968,7 +962,19 @@ static const struct paravirt_ops xen_paravirt_ops __initdata = {
 
 	.set_iopl_mask = xen_set_iopl_mask,
 	.io_delay = xen_io_delay,
+};
+
+static const struct pv_irq_ops xen_irq_ops __initdata = {
+	.init_IRQ = xen_init_IRQ,
+	.save_fl = xen_save_fl,
+	.restore_fl = xen_restore_fl,
+	.irq_disable = xen_irq_disable,
+	.irq_enable = xen_irq_enable,
+	.safe_halt = xen_safe_halt,
+	.halt = xen_halt,
+};
 
+static const struct pv_apic_ops xen_apic_ops __initdata = {
 #ifdef CONFIG_X86_LOCAL_APIC
 	.apic_write = xen_apic_write,
 	.apic_write_atomic = xen_apic_write,
@@ -977,6 +983,17 @@ static const struct paravirt_ops xen_paravirt_ops __initdata = {
 	.setup_secondary_clock = paravirt_nop,
 	.startup_ipi_hook = paravirt_nop,
 #endif
+};
+
+static const struct pv_mmu_ops xen_mmu_ops __initdata = {
+	.pagetable_setup_start = xen_pagetable_setup_start,
+	.pagetable_setup_done = xen_pagetable_setup_done,
+
+	.read_cr2 = xen_read_cr2,
+	.write_cr2 = xen_write_cr2,
+
+	.read_cr3 = xen_read_cr3,
+	.write_cr3 = xen_write_cr3,
 
 	.flush_tlb_user = xen_flush_tlb,
 	.flush_tlb_kernel = xen_flush_tlb,
@@ -986,9 +1003,6 @@ static const struct paravirt_ops xen_paravirt_ops __initdata = {
 	.pte_update = paravirt_nop,
 	.pte_update_defer = paravirt_nop,
 
-	.pagetable_setup_start = xen_pagetable_setup_start,
-	.pagetable_setup_done = xen_pagetable_setup_done,
-
 	.alloc_pt = xen_alloc_pt_init,
 	.release_pt = xen_release_pt,
 	.alloc_pd = paravirt_nop,
@@ -1023,7 +1037,9 @@ static const struct paravirt_ops xen_paravirt_ops __initdata = {
 	.activate_mm = xen_activate_mm,
 	.dup_mmap = xen_dup_mmap,
 	.exit_mmap = xen_exit_mmap,
+};
 
+static const struct pv_misc_ops xen_misc_ops __initdata = {
 	.set_lazy_mode = xen_set_lazy_mode,
 };
 
@@ -1091,7 +1107,15 @@ asmlinkage void __init xen_start_kernel(void)
 	BUG_ON(memcmp(xen_start_info->magic, "xen-3.0", 7) != 0);
 
 	/* Install Xen paravirt ops */
-	paravirt_ops = xen_paravirt_ops;
+	pv_info = xen_info;
+	pv_init_ops = xen_init_ops;
+	pv_time_ops = xen_time_ops;
+	pv_cpu_ops = xen_cpu_ops;
+	pv_irq_ops = xen_irq_ops;
+	pv_apic_ops = xen_apic_ops;
+	pv_mmu_ops = xen_mmu_ops;
+	pv_misc_ops = xen_misc_ops;
+
 	machine_ops = xen_machine_ops;
 
 #ifdef CONFIG_SMP
@@ -1124,9 +1148,9 @@ asmlinkage void __init xen_start_kernel(void)
 	xen_setup_vcpu_info_placement();
 #endif
 
-	paravirt_ops.kernel_rpl = 1;
+	pv_info.kernel_rpl = 1;
 	if (xen_feature(XENFEAT_supervisor_mode_kernel))
-		paravirt_ops.kernel_rpl = 0;
+		pv_info.kernel_rpl = 0;
 
 	/* set the limit of our address space */
 	reserve_top_address(-HYPERVISOR_VIRT_START + 2 * PAGE_SIZE);

commit 9702785a747aa27baf46ff504beab6528f21f2dd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:51 2007 +0200

    i386: move xen
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
new file mode 100644
index 000000000000..f01bfcd4bdee
--- /dev/null
+++ b/arch/x86/xen/enlighten.c
@@ -0,0 +1,1146 @@
+/*
+ * Core of Xen paravirt_ops implementation.
+ *
+ * This file contains the xen_paravirt_ops structure itself, and the
+ * implementations for:
+ * - privileged instructions
+ * - interrupt flags
+ * - segment operations
+ * - booting and setup
+ *
+ * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007
+ */
+
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <linux/preempt.h>
+#include <linux/hardirq.h>
+#include <linux/percpu.h>
+#include <linux/delay.h>
+#include <linux/start_kernel.h>
+#include <linux/sched.h>
+#include <linux/bootmem.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/page-flags.h>
+#include <linux/highmem.h>
+#include <linux/smp.h>
+
+#include <xen/interface/xen.h>
+#include <xen/interface/physdev.h>
+#include <xen/interface/vcpu.h>
+#include <xen/interface/sched.h>
+#include <xen/features.h>
+#include <xen/page.h>
+
+#include <asm/paravirt.h>
+#include <asm/page.h>
+#include <asm/xen/hypercall.h>
+#include <asm/xen/hypervisor.h>
+#include <asm/fixmap.h>
+#include <asm/processor.h>
+#include <asm/setup.h>
+#include <asm/desc.h>
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+#include <asm/reboot.h>
+
+#include "xen-ops.h"
+#include "mmu.h"
+#include "multicalls.h"
+
+EXPORT_SYMBOL_GPL(hypercall_page);
+
+DEFINE_PER_CPU(enum paravirt_lazy_mode, xen_lazy_mode);
+
+DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
+DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
+DEFINE_PER_CPU(unsigned long, xen_cr3);
+
+struct start_info *xen_start_info;
+EXPORT_SYMBOL_GPL(xen_start_info);
+
+static /* __initdata */ struct shared_info dummy_shared_info;
+
+/*
+ * Point at some empty memory to start with. We map the real shared_info
+ * page as soon as fixmap is up and running.
+ */
+struct shared_info *HYPERVISOR_shared_info = (void *)&dummy_shared_info;
+
+/*
+ * Flag to determine whether vcpu info placement is available on all
+ * VCPUs.  We assume it is to start with, and then set it to zero on
+ * the first failure.  This is because it can succeed on some VCPUs
+ * and not others, since it can involve hypervisor memory allocation,
+ * or because the guest failed to guarantee all the appropriate
+ * constraints on all VCPUs (ie buffer can't cross a page boundary).
+ *
+ * Note that any particular CPU may be using a placed vcpu structure,
+ * but we can only optimise if the all are.
+ *
+ * 0: not available, 1: available
+ */
+static int have_vcpu_info_placement = 1;
+
+static void __init xen_vcpu_setup(int cpu)
+{
+	struct vcpu_register_vcpu_info info;
+	int err;
+	struct vcpu_info *vcpup;
+
+	per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
+
+	if (!have_vcpu_info_placement)
+		return;		/* already tested, not available */
+
+	vcpup = &per_cpu(xen_vcpu_info, cpu);
+
+	info.mfn = virt_to_mfn(vcpup);
+	info.offset = offset_in_page(vcpup);
+
+	printk(KERN_DEBUG "trying to map vcpu_info %d at %p, mfn %x, offset %d\n",
+	       cpu, vcpup, info.mfn, info.offset);
+
+	/* Check to see if the hypervisor will put the vcpu_info
+	   structure where we want it, which allows direct access via
+	   a percpu-variable. */
+	err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, cpu, &info);
+
+	if (err) {
+		printk(KERN_DEBUG "register_vcpu_info failed: err=%d\n", err);
+		have_vcpu_info_placement = 0;
+	} else {
+		/* This cpu is using the registered vcpu info, even if
+		   later ones fail to. */
+		per_cpu(xen_vcpu, cpu) = vcpup;
+
+		printk(KERN_DEBUG "cpu %d using vcpu_info at %p\n",
+		       cpu, vcpup);
+	}
+}
+
+static void __init xen_banner(void)
+{
+	printk(KERN_INFO "Booting paravirtualized kernel on %s\n",
+	       paravirt_ops.name);
+	printk(KERN_INFO "Hypervisor signature: %s\n", xen_start_info->magic);
+}
+
+static void xen_cpuid(unsigned int *eax, unsigned int *ebx,
+		      unsigned int *ecx, unsigned int *edx)
+{
+	unsigned maskedx = ~0;
+
+	/*
+	 * Mask out inconvenient features, to try and disable as many
+	 * unsupported kernel subsystems as possible.
+	 */
+	if (*eax == 1)
+		maskedx = ~((1 << X86_FEATURE_APIC) |  /* disable APIC */
+			    (1 << X86_FEATURE_ACPI) |  /* disable ACPI */
+			    (1 << X86_FEATURE_ACC));   /* thermal monitoring */
+
+	asm(XEN_EMULATE_PREFIX "cpuid"
+		: "=a" (*eax),
+		  "=b" (*ebx),
+		  "=c" (*ecx),
+		  "=d" (*edx)
+		: "0" (*eax), "2" (*ecx));
+	*edx &= maskedx;
+}
+
+static void xen_set_debugreg(int reg, unsigned long val)
+{
+	HYPERVISOR_set_debugreg(reg, val);
+}
+
+static unsigned long xen_get_debugreg(int reg)
+{
+	return HYPERVISOR_get_debugreg(reg);
+}
+
+static unsigned long xen_save_fl(void)
+{
+	struct vcpu_info *vcpu;
+	unsigned long flags;
+
+	vcpu = x86_read_percpu(xen_vcpu);
+
+	/* flag has opposite sense of mask */
+	flags = !vcpu->evtchn_upcall_mask;
+
+	/* convert to IF type flag
+	   -0 -> 0x00000000
+	   -1 -> 0xffffffff
+	*/
+	return (-flags) & X86_EFLAGS_IF;
+}
+
+static void xen_restore_fl(unsigned long flags)
+{
+	struct vcpu_info *vcpu;
+
+	/* convert from IF type flag */
+	flags = !(flags & X86_EFLAGS_IF);
+
+	/* There's a one instruction preempt window here.  We need to
+	   make sure we're don't switch CPUs between getting the vcpu
+	   pointer and updating the mask. */
+	preempt_disable();
+	vcpu = x86_read_percpu(xen_vcpu);
+	vcpu->evtchn_upcall_mask = flags;
+	preempt_enable_no_resched();
+
+	/* Doesn't matter if we get preempted here, because any
+	   pending event will get dealt with anyway. */
+
+	if (flags == 0) {
+		preempt_check_resched();
+		barrier(); /* unmask then check (avoid races) */
+		if (unlikely(vcpu->evtchn_upcall_pending))
+			force_evtchn_callback();
+	}
+}
+
+static void xen_irq_disable(void)
+{
+	/* There's a one instruction preempt window here.  We need to
+	   make sure we're don't switch CPUs between getting the vcpu
+	   pointer and updating the mask. */
+	preempt_disable();
+	x86_read_percpu(xen_vcpu)->evtchn_upcall_mask = 1;
+	preempt_enable_no_resched();
+}
+
+static void xen_irq_enable(void)
+{
+	struct vcpu_info *vcpu;
+
+	/* There's a one instruction preempt window here.  We need to
+	   make sure we're don't switch CPUs between getting the vcpu
+	   pointer and updating the mask. */
+	preempt_disable();
+	vcpu = x86_read_percpu(xen_vcpu);
+	vcpu->evtchn_upcall_mask = 0;
+	preempt_enable_no_resched();
+
+	/* Doesn't matter if we get preempted here, because any
+	   pending event will get dealt with anyway. */
+
+	barrier(); /* unmask then check (avoid races) */
+	if (unlikely(vcpu->evtchn_upcall_pending))
+		force_evtchn_callback();
+}
+
+static void xen_safe_halt(void)
+{
+	/* Blocking includes an implicit local_irq_enable(). */
+	if (HYPERVISOR_sched_op(SCHEDOP_block, 0) != 0)
+		BUG();
+}
+
+static void xen_halt(void)
+{
+	if (irqs_disabled())
+		HYPERVISOR_vcpu_op(VCPUOP_down, smp_processor_id(), NULL);
+	else
+		xen_safe_halt();
+}
+
+static void xen_set_lazy_mode(enum paravirt_lazy_mode mode)
+{
+	BUG_ON(preemptible());
+
+	switch (mode) {
+	case PARAVIRT_LAZY_NONE:
+		BUG_ON(x86_read_percpu(xen_lazy_mode) == PARAVIRT_LAZY_NONE);
+		break;
+
+	case PARAVIRT_LAZY_MMU:
+	case PARAVIRT_LAZY_CPU:
+		BUG_ON(x86_read_percpu(xen_lazy_mode) != PARAVIRT_LAZY_NONE);
+		break;
+
+	case PARAVIRT_LAZY_FLUSH:
+		/* flush if necessary, but don't change state */
+		if (x86_read_percpu(xen_lazy_mode) != PARAVIRT_LAZY_NONE)
+			xen_mc_flush();
+		return;
+	}
+
+	xen_mc_flush();
+	x86_write_percpu(xen_lazy_mode, mode);
+}
+
+static unsigned long xen_store_tr(void)
+{
+	return 0;
+}
+
+static void xen_set_ldt(const void *addr, unsigned entries)
+{
+	unsigned long linear_addr = (unsigned long)addr;
+	struct mmuext_op *op;
+	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
+
+	op = mcs.args;
+	op->cmd = MMUEXT_SET_LDT;
+	if (linear_addr) {
+		/* ldt my be vmalloced, use arbitrary_virt_to_machine */
+		xmaddr_t maddr;
+		maddr = arbitrary_virt_to_machine((unsigned long)addr);
+		linear_addr = (unsigned long)maddr.maddr;
+	}
+	op->arg1.linear_addr = linear_addr;
+	op->arg2.nr_ents = entries;
+
+	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
+
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
+}
+
+static void xen_load_gdt(const struct Xgt_desc_struct *dtr)
+{
+	unsigned long *frames;
+	unsigned long va = dtr->address;
+	unsigned int size = dtr->size + 1;
+	unsigned pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	int f;
+	struct multicall_space mcs;
+
+	/* A GDT can be up to 64k in size, which corresponds to 8192
+	   8-byte entries, or 16 4k pages.. */
+
+	BUG_ON(size > 65536);
+	BUG_ON(va & ~PAGE_MASK);
+
+	mcs = xen_mc_entry(sizeof(*frames) * pages);
+	frames = mcs.args;
+
+	for (f = 0; va < dtr->address + size; va += PAGE_SIZE, f++) {
+		frames[f] = virt_to_mfn(va);
+		make_lowmem_page_readonly((void *)va);
+	}
+
+	MULTI_set_gdt(mcs.mc, frames, size / sizeof(struct desc_struct));
+
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
+}
+
+static void load_TLS_descriptor(struct thread_struct *t,
+				unsigned int cpu, unsigned int i)
+{
+	struct desc_struct *gdt = get_cpu_gdt_table(cpu);
+	xmaddr_t maddr = virt_to_machine(&gdt[GDT_ENTRY_TLS_MIN+i]);
+	struct multicall_space mc = __xen_mc_entry(0);
+
+	MULTI_update_descriptor(mc.mc, maddr.maddr, t->tls_array[i]);
+}
+
+static void xen_load_tls(struct thread_struct *t, unsigned int cpu)
+{
+	xen_mc_batch();
+
+	load_TLS_descriptor(t, cpu, 0);
+	load_TLS_descriptor(t, cpu, 1);
+	load_TLS_descriptor(t, cpu, 2);
+
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
+
+	/*
+	 * XXX sleazy hack: If we're being called in a lazy-cpu zone,
+	 * it means we're in a context switch, and %gs has just been
+	 * saved.  This means we can zero it out to prevent faults on
+	 * exit from the hypervisor if the next process has no %gs.
+	 * Either way, it has been saved, and the new value will get
+	 * loaded properly.  This will go away as soon as Xen has been
+	 * modified to not save/restore %gs for normal hypercalls.
+	 */
+	if (xen_get_lazy_mode() == PARAVIRT_LAZY_CPU)
+		loadsegment(gs, 0);
+}
+
+static void xen_write_ldt_entry(struct desc_struct *dt, int entrynum,
+				u32 low, u32 high)
+{
+	unsigned long lp = (unsigned long)&dt[entrynum];
+	xmaddr_t mach_lp = virt_to_machine(lp);
+	u64 entry = (u64)high << 32 | low;
+
+	preempt_disable();
+
+	xen_mc_flush();
+	if (HYPERVISOR_update_descriptor(mach_lp.maddr, entry))
+		BUG();
+
+	preempt_enable();
+}
+
+static int cvt_gate_to_trap(int vector, u32 low, u32 high,
+			    struct trap_info *info)
+{
+	u8 type, dpl;
+
+	type = (high >> 8) & 0x1f;
+	dpl = (high >> 13) & 3;
+
+	if (type != 0xf && type != 0xe)
+		return 0;
+
+	info->vector = vector;
+	info->address = (high & 0xffff0000) | (low & 0x0000ffff);
+	info->cs = low >> 16;
+	info->flags = dpl;
+	/* interrupt gates clear IF */
+	if (type == 0xe)
+		info->flags |= 4;
+
+	return 1;
+}
+
+/* Locations of each CPU's IDT */
+static DEFINE_PER_CPU(struct Xgt_desc_struct, idt_desc);
+
+/* Set an IDT entry.  If the entry is part of the current IDT, then
+   also update Xen. */
+static void xen_write_idt_entry(struct desc_struct *dt, int entrynum,
+				u32 low, u32 high)
+{
+	unsigned long p = (unsigned long)&dt[entrynum];
+	unsigned long start, end;
+
+	preempt_disable();
+
+	start = __get_cpu_var(idt_desc).address;
+	end = start + __get_cpu_var(idt_desc).size + 1;
+
+	xen_mc_flush();
+
+	write_dt_entry(dt, entrynum, low, high);
+
+	if (p >= start && (p + 8) <= end) {
+		struct trap_info info[2];
+
+		info[1].address = 0;
+
+		if (cvt_gate_to_trap(entrynum, low, high, &info[0]))
+			if (HYPERVISOR_set_trap_table(info))
+				BUG();
+	}
+
+	preempt_enable();
+}
+
+static void xen_convert_trap_info(const struct Xgt_desc_struct *desc,
+				  struct trap_info *traps)
+{
+	unsigned in, out, count;
+
+	count = (desc->size+1) / 8;
+	BUG_ON(count > 256);
+
+	for (in = out = 0; in < count; in++) {
+		const u32 *entry = (u32 *)(desc->address + in * 8);
+
+		if (cvt_gate_to_trap(in, entry[0], entry[1], &traps[out]))
+			out++;
+	}
+	traps[out].address = 0;
+}
+
+void xen_copy_trap_info(struct trap_info *traps)
+{
+	const struct Xgt_desc_struct *desc = &__get_cpu_var(idt_desc);
+
+	xen_convert_trap_info(desc, traps);
+}
+
+/* Load a new IDT into Xen.  In principle this can be per-CPU, so we
+   hold a spinlock to protect the static traps[] array (static because
+   it avoids allocation, and saves stack space). */
+static void xen_load_idt(const struct Xgt_desc_struct *desc)
+{
+	static DEFINE_SPINLOCK(lock);
+	static struct trap_info traps[257];
+
+	spin_lock(&lock);
+
+	__get_cpu_var(idt_desc) = *desc;
+
+	xen_convert_trap_info(desc, traps);
+
+	xen_mc_flush();
+	if (HYPERVISOR_set_trap_table(traps))
+		BUG();
+
+	spin_unlock(&lock);
+}
+
+/* Write a GDT descriptor entry.  Ignore LDT descriptors, since
+   they're handled differently. */
+static void xen_write_gdt_entry(struct desc_struct *dt, int entry,
+				u32 low, u32 high)
+{
+	preempt_disable();
+
+	switch ((high >> 8) & 0xff) {
+	case DESCTYPE_LDT:
+	case DESCTYPE_TSS:
+		/* ignore */
+		break;
+
+	default: {
+		xmaddr_t maddr = virt_to_machine(&dt[entry]);
+		u64 desc = (u64)high << 32 | low;
+
+		xen_mc_flush();
+		if (HYPERVISOR_update_descriptor(maddr.maddr, desc))
+			BUG();
+	}
+
+	}
+
+	preempt_enable();
+}
+
+static void xen_load_esp0(struct tss_struct *tss,
+			  struct thread_struct *thread)
+{
+	struct multicall_space mcs = xen_mc_entry(0);
+	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread->esp0);
+	xen_mc_issue(PARAVIRT_LAZY_CPU);
+}
+
+static void xen_set_iopl_mask(unsigned mask)
+{
+	struct physdev_set_iopl set_iopl;
+
+	/* Force the change at ring 0. */
+	set_iopl.iopl = (mask == 0) ? 1 : (mask >> 12) & 3;
+	HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
+}
+
+static void xen_io_delay(void)
+{
+}
+
+#ifdef CONFIG_X86_LOCAL_APIC
+static unsigned long xen_apic_read(unsigned long reg)
+{
+	return 0;
+}
+
+static void xen_apic_write(unsigned long reg, unsigned long val)
+{
+	/* Warn to see if there's any stray references */
+	WARN_ON(1);
+}
+#endif
+
+static void xen_flush_tlb(void)
+{
+	struct mmuext_op *op;
+	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
+
+	op = mcs.args;
+	op->cmd = MMUEXT_TLB_FLUSH_LOCAL;
+	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
+
+	xen_mc_issue(PARAVIRT_LAZY_MMU);
+}
+
+static void xen_flush_tlb_single(unsigned long addr)
+{
+	struct mmuext_op *op;
+	struct multicall_space mcs = xen_mc_entry(sizeof(*op));
+
+	op = mcs.args;
+	op->cmd = MMUEXT_INVLPG_LOCAL;
+	op->arg1.linear_addr = addr & PAGE_MASK;
+	MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
+
+	xen_mc_issue(PARAVIRT_LAZY_MMU);
+}
+
+static void xen_flush_tlb_others(const cpumask_t *cpus, struct mm_struct *mm,
+				 unsigned long va)
+{
+	struct {
+		struct mmuext_op op;
+		cpumask_t mask;
+	} *args;
+	cpumask_t cpumask = *cpus;
+	struct multicall_space mcs;
+
+	/*
+	 * A couple of (to be removed) sanity checks:
+	 *
+	 * - current CPU must not be in mask
+	 * - mask must exist :)
+	 */
+	BUG_ON(cpus_empty(cpumask));
+	BUG_ON(cpu_isset(smp_processor_id(), cpumask));
+	BUG_ON(!mm);
+
+	/* If a CPU which we ran on has gone down, OK. */
+	cpus_and(cpumask, cpumask, cpu_online_map);
+	if (cpus_empty(cpumask))
+		return;
+
+	mcs = xen_mc_entry(sizeof(*args));
+	args = mcs.args;
+	args->mask = cpumask;
+	args->op.arg2.vcpumask = &args->mask;
+
+	if (va == TLB_FLUSH_ALL) {
+		args->op.cmd = MMUEXT_TLB_FLUSH_MULTI;
+	} else {
+		args->op.cmd = MMUEXT_INVLPG_MULTI;
+		args->op.arg1.linear_addr = va;
+	}
+
+	MULTI_mmuext_op(mcs.mc, &args->op, 1, NULL, DOMID_SELF);
+
+	xen_mc_issue(PARAVIRT_LAZY_MMU);
+}
+
+static void xen_write_cr2(unsigned long cr2)
+{
+	x86_read_percpu(xen_vcpu)->arch.cr2 = cr2;
+}
+
+static unsigned long xen_read_cr2(void)
+{
+	return x86_read_percpu(xen_vcpu)->arch.cr2;
+}
+
+static unsigned long xen_read_cr2_direct(void)
+{
+	return x86_read_percpu(xen_vcpu_info.arch.cr2);
+}
+
+static void xen_write_cr4(unsigned long cr4)
+{
+	/* Just ignore cr4 changes; Xen doesn't allow us to do
+	   anything anyway. */
+}
+
+static unsigned long xen_read_cr3(void)
+{
+	return x86_read_percpu(xen_cr3);
+}
+
+static void xen_write_cr3(unsigned long cr3)
+{
+	BUG_ON(preemptible());
+
+	if (cr3 == x86_read_percpu(xen_cr3)) {
+		/* just a simple tlb flush */
+		xen_flush_tlb();
+		return;
+	}
+
+	x86_write_percpu(xen_cr3, cr3);
+
+
+	{
+		struct mmuext_op *op;
+		struct multicall_space mcs = xen_mc_entry(sizeof(*op));
+		unsigned long mfn = pfn_to_mfn(PFN_DOWN(cr3));
+
+		op = mcs.args;
+		op->cmd = MMUEXT_NEW_BASEPTR;
+		op->arg1.mfn = mfn;
+
+		MULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);
+
+		xen_mc_issue(PARAVIRT_LAZY_CPU);
+	}
+}
+
+/* Early in boot, while setting up the initial pagetable, assume
+   everything is pinned. */
+static __init void xen_alloc_pt_init(struct mm_struct *mm, u32 pfn)
+{
+	BUG_ON(mem_map);	/* should only be used early */
+	make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
+}
+
+/* This needs to make sure the new pte page is pinned iff its being
+   attached to a pinned pagetable. */
+static void xen_alloc_pt(struct mm_struct *mm, u32 pfn)
+{
+	struct page *page = pfn_to_page(pfn);
+
+	if (PagePinned(virt_to_page(mm->pgd))) {
+		SetPagePinned(page);
+
+		if (!PageHighMem(page))
+			make_lowmem_page_readonly(__va(PFN_PHYS(pfn)));
+		else
+			/* make sure there are no stray mappings of
+			   this page */
+			kmap_flush_unused();
+	}
+}
+
+/* This should never happen until we're OK to use struct page */
+static void xen_release_pt(u32 pfn)
+{
+	struct page *page = pfn_to_page(pfn);
+
+	if (PagePinned(page)) {
+		if (!PageHighMem(page))
+			make_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));
+	}
+}
+
+#ifdef CONFIG_HIGHPTE
+static void *xen_kmap_atomic_pte(struct page *page, enum km_type type)
+{
+	pgprot_t prot = PAGE_KERNEL;
+
+	if (PagePinned(page))
+		prot = PAGE_KERNEL_RO;
+
+	if (0 && PageHighMem(page))
+		printk("mapping highpte %lx type %d prot %s\n",
+		       page_to_pfn(page), type,
+		       (unsigned long)pgprot_val(prot) & _PAGE_RW ? "WRITE" : "READ");
+
+	return kmap_atomic_prot(page, type, prot);
+}
+#endif
+
+static __init pte_t mask_rw_pte(pte_t *ptep, pte_t pte)
+{
+	/* If there's an existing pte, then don't allow _PAGE_RW to be set */
+	if (pte_val_ma(*ptep) & _PAGE_PRESENT)
+		pte = __pte_ma(((pte_val_ma(*ptep) & _PAGE_RW) | ~_PAGE_RW) &
+			       pte_val_ma(pte));
+
+	return pte;
+}
+
+/* Init-time set_pte while constructing initial pagetables, which
+   doesn't allow RO pagetable pages to be remapped RW */
+static __init void xen_set_pte_init(pte_t *ptep, pte_t pte)
+{
+	pte = mask_rw_pte(ptep, pte);
+
+	xen_set_pte(ptep, pte);
+}
+
+static __init void xen_pagetable_setup_start(pgd_t *base)
+{
+	pgd_t *xen_pgd = (pgd_t *)xen_start_info->pt_base;
+
+	/* special set_pte for pagetable initialization */
+	paravirt_ops.set_pte = xen_set_pte_init;
+
+	init_mm.pgd = base;
+	/*
+	 * copy top-level of Xen-supplied pagetable into place.	 For
+	 * !PAE we can use this as-is, but for PAE it is a stand-in
+	 * while we copy the pmd pages.
+	 */
+	memcpy(base, xen_pgd, PTRS_PER_PGD * sizeof(pgd_t));
+
+	if (PTRS_PER_PMD > 1) {
+		int i;
+		/*
+		 * For PAE, need to allocate new pmds, rather than
+		 * share Xen's, since Xen doesn't like pmd's being
+		 * shared between address spaces.
+		 */
+		for (i = 0; i < PTRS_PER_PGD; i++) {
+			if (pgd_val_ma(xen_pgd[i]) & _PAGE_PRESENT) {
+				pmd_t *pmd = (pmd_t *)alloc_bootmem_low_pages(PAGE_SIZE);
+
+				memcpy(pmd, (void *)pgd_page_vaddr(xen_pgd[i]),
+				       PAGE_SIZE);
+
+				make_lowmem_page_readonly(pmd);
+
+				set_pgd(&base[i], __pgd(1 + __pa(pmd)));
+			} else
+				pgd_clear(&base[i]);
+		}
+	}
+
+	/* make sure zero_page is mapped RO so we can use it in pagetables */
+	make_lowmem_page_readonly(empty_zero_page);
+	make_lowmem_page_readonly(base);
+	/*
+	 * Switch to new pagetable.  This is done before
+	 * pagetable_init has done anything so that the new pages
+	 * added to the table can be prepared properly for Xen.
+	 */
+	xen_write_cr3(__pa(base));
+}
+
+static __init void xen_pagetable_setup_done(pgd_t *base)
+{
+	/* This will work as long as patching hasn't happened yet
+	   (which it hasn't) */
+	paravirt_ops.alloc_pt = xen_alloc_pt;
+	paravirt_ops.set_pte = xen_set_pte;
+
+	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
+		/*
+		 * Create a mapping for the shared info page.
+		 * Should be set_fixmap(), but shared_info is a machine
+		 * address with no corresponding pseudo-phys address.
+		 */
+		set_pte_mfn(fix_to_virt(FIX_PARAVIRT_BOOTMAP),
+			    PFN_DOWN(xen_start_info->shared_info),
+			    PAGE_KERNEL);
+
+		HYPERVISOR_shared_info =
+			(struct shared_info *)fix_to_virt(FIX_PARAVIRT_BOOTMAP);
+
+	} else
+		HYPERVISOR_shared_info =
+			(struct shared_info *)__va(xen_start_info->shared_info);
+
+	/* Actually pin the pagetable down, but we can't set PG_pinned
+	   yet because the page structures don't exist yet. */
+	{
+		struct mmuext_op op;
+#ifdef CONFIG_X86_PAE
+		op.cmd = MMUEXT_PIN_L3_TABLE;
+#else
+		op.cmd = MMUEXT_PIN_L3_TABLE;
+#endif
+		op.arg1.mfn = pfn_to_mfn(PFN_DOWN(__pa(base)));
+		if (HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF))
+			BUG();
+	}
+}
+
+/* This is called once we have the cpu_possible_map */
+void __init xen_setup_vcpu_info_placement(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		xen_vcpu_setup(cpu);
+
+	/* xen_vcpu_setup managed to place the vcpu_info within the
+	   percpu area for all cpus, so make use of it */
+	if (have_vcpu_info_placement) {
+		printk(KERN_INFO "Xen: using vcpu_info placement\n");
+
+		paravirt_ops.save_fl = xen_save_fl_direct;
+		paravirt_ops.restore_fl = xen_restore_fl_direct;
+		paravirt_ops.irq_disable = xen_irq_disable_direct;
+		paravirt_ops.irq_enable = xen_irq_enable_direct;
+		paravirt_ops.read_cr2 = xen_read_cr2_direct;
+		paravirt_ops.iret = xen_iret_direct;
+	}
+}
+
+static unsigned xen_patch(u8 type, u16 clobbers, void *insnbuf,
+			  unsigned long addr, unsigned len)
+{
+	char *start, *end, *reloc;
+	unsigned ret;
+
+	start = end = reloc = NULL;
+
+#define SITE(x)								\
+	case PARAVIRT_PATCH(x):						\
+	if (have_vcpu_info_placement) {					\
+		start = (char *)xen_##x##_direct;			\
+		end = xen_##x##_direct_end;				\
+		reloc = xen_##x##_direct_reloc;				\
+	}								\
+	goto patch_site
+
+	switch (type) {
+		SITE(irq_enable);
+		SITE(irq_disable);
+		SITE(save_fl);
+		SITE(restore_fl);
+#undef SITE
+
+	patch_site:
+		if (start == NULL || (end-start) > len)
+			goto default_patch;
+
+		ret = paravirt_patch_insns(insnbuf, len, start, end);
+
+		/* Note: because reloc is assigned from something that
+		   appears to be an array, gcc assumes it's non-null,
+		   but doesn't know its relationship with start and
+		   end. */
+		if (reloc > start && reloc < end) {
+			int reloc_off = reloc - start;
+			long *relocp = (long *)(insnbuf + reloc_off);
+			long delta = start - (char *)addr;
+
+			*relocp += delta;
+		}
+		break;
+
+	default_patch:
+	default:
+		ret = paravirt_patch_default(type, clobbers, insnbuf,
+					     addr, len);
+		break;
+	}
+
+	return ret;
+}
+
+static const struct paravirt_ops xen_paravirt_ops __initdata = {
+	.paravirt_enabled = 1,
+	.shared_kernel_pmd = 0,
+
+	.name = "Xen",
+	.banner = xen_banner,
+
+	.patch = xen_patch,
+
+	.memory_setup = xen_memory_setup,
+	.arch_setup = xen_arch_setup,
+	.init_IRQ = xen_init_IRQ,
+	.post_allocator_init = xen_mark_init_mm_pinned,
+
+	.time_init = xen_time_init,
+	.set_wallclock = xen_set_wallclock,
+	.get_wallclock = xen_get_wallclock,
+	.get_cpu_khz = xen_cpu_khz,
+	.sched_clock = xen_sched_clock,
+
+	.cpuid = xen_cpuid,
+
+	.set_debugreg = xen_set_debugreg,
+	.get_debugreg = xen_get_debugreg,
+
+	.clts = native_clts,
+
+	.read_cr0 = native_read_cr0,
+	.write_cr0 = native_write_cr0,
+
+	.read_cr2 = xen_read_cr2,
+	.write_cr2 = xen_write_cr2,
+
+	.read_cr3 = xen_read_cr3,
+	.write_cr3 = xen_write_cr3,
+
+	.read_cr4 = native_read_cr4,
+	.read_cr4_safe = native_read_cr4_safe,
+	.write_cr4 = xen_write_cr4,
+
+	.save_fl = xen_save_fl,
+	.restore_fl = xen_restore_fl,
+	.irq_disable = xen_irq_disable,
+	.irq_enable = xen_irq_enable,
+	.safe_halt = xen_safe_halt,
+	.halt = xen_halt,
+	.wbinvd = native_wbinvd,
+
+	.read_msr = native_read_msr_safe,
+	.write_msr = native_write_msr_safe,
+	.read_tsc = native_read_tsc,
+	.read_pmc = native_read_pmc,
+
+	.iret = (void *)&hypercall_page[__HYPERVISOR_iret],
+	.irq_enable_sysexit = NULL,  /* never called */
+
+	.load_tr_desc = paravirt_nop,
+	.set_ldt = xen_set_ldt,
+	.load_gdt = xen_load_gdt,
+	.load_idt = xen_load_idt,
+	.load_tls = xen_load_tls,
+
+	.store_gdt = native_store_gdt,
+	.store_idt = native_store_idt,
+	.store_tr = xen_store_tr,
+
+	.write_ldt_entry = xen_write_ldt_entry,
+	.write_gdt_entry = xen_write_gdt_entry,
+	.write_idt_entry = xen_write_idt_entry,
+	.load_esp0 = xen_load_esp0,
+
+	.set_iopl_mask = xen_set_iopl_mask,
+	.io_delay = xen_io_delay,
+
+#ifdef CONFIG_X86_LOCAL_APIC
+	.apic_write = xen_apic_write,
+	.apic_write_atomic = xen_apic_write,
+	.apic_read = xen_apic_read,
+	.setup_boot_clock = paravirt_nop,
+	.setup_secondary_clock = paravirt_nop,
+	.startup_ipi_hook = paravirt_nop,
+#endif
+
+	.flush_tlb_user = xen_flush_tlb,
+	.flush_tlb_kernel = xen_flush_tlb,
+	.flush_tlb_single = xen_flush_tlb_single,
+	.flush_tlb_others = xen_flush_tlb_others,
+
+	.pte_update = paravirt_nop,
+	.pte_update_defer = paravirt_nop,
+
+	.pagetable_setup_start = xen_pagetable_setup_start,
+	.pagetable_setup_done = xen_pagetable_setup_done,
+
+	.alloc_pt = xen_alloc_pt_init,
+	.release_pt = xen_release_pt,
+	.alloc_pd = paravirt_nop,
+	.alloc_pd_clone = paravirt_nop,
+	.release_pd = paravirt_nop,
+
+#ifdef CONFIG_HIGHPTE
+	.kmap_atomic_pte = xen_kmap_atomic_pte,
+#endif
+
+	.set_pte = NULL,	/* see xen_pagetable_setup_* */
+	.set_pte_at = xen_set_pte_at,
+	.set_pmd = xen_set_pmd,
+
+	.pte_val = xen_pte_val,
+	.pgd_val = xen_pgd_val,
+
+	.make_pte = xen_make_pte,
+	.make_pgd = xen_make_pgd,
+
+#ifdef CONFIG_X86_PAE
+	.set_pte_atomic = xen_set_pte_atomic,
+	.set_pte_present = xen_set_pte_at,
+	.set_pud = xen_set_pud,
+	.pte_clear = xen_pte_clear,
+	.pmd_clear = xen_pmd_clear,
+
+	.make_pmd = xen_make_pmd,
+	.pmd_val = xen_pmd_val,
+#endif	/* PAE */
+
+	.activate_mm = xen_activate_mm,
+	.dup_mmap = xen_dup_mmap,
+	.exit_mmap = xen_exit_mmap,
+
+	.set_lazy_mode = xen_set_lazy_mode,
+};
+
+#ifdef CONFIG_SMP
+static const struct smp_ops xen_smp_ops __initdata = {
+	.smp_prepare_boot_cpu = xen_smp_prepare_boot_cpu,
+	.smp_prepare_cpus = xen_smp_prepare_cpus,
+	.cpu_up = xen_cpu_up,
+	.smp_cpus_done = xen_smp_cpus_done,
+
+	.smp_send_stop = xen_smp_send_stop,
+	.smp_send_reschedule = xen_smp_send_reschedule,
+	.smp_call_function_mask = xen_smp_call_function_mask,
+};
+#endif	/* CONFIG_SMP */
+
+static void xen_reboot(int reason)
+{
+#ifdef CONFIG_SMP
+	smp_send_stop();
+#endif
+
+	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, reason))
+		BUG();
+}
+
+static void xen_restart(char *msg)
+{
+	xen_reboot(SHUTDOWN_reboot);
+}
+
+static void xen_emergency_restart(void)
+{
+	xen_reboot(SHUTDOWN_reboot);
+}
+
+static void xen_machine_halt(void)
+{
+	xen_reboot(SHUTDOWN_poweroff);
+}
+
+static void xen_crash_shutdown(struct pt_regs *regs)
+{
+	xen_reboot(SHUTDOWN_crash);
+}
+
+static const struct machine_ops __initdata xen_machine_ops = {
+	.restart = xen_restart,
+	.halt = xen_machine_halt,
+	.power_off = xen_machine_halt,
+	.shutdown = xen_machine_halt,
+	.crash_shutdown = xen_crash_shutdown,
+	.emergency_restart = xen_emergency_restart,
+};
+
+
+/* First C function to be called on Xen boot */
+asmlinkage void __init xen_start_kernel(void)
+{
+	pgd_t *pgd;
+
+	if (!xen_start_info)
+		return;
+
+	BUG_ON(memcmp(xen_start_info->magic, "xen-3.0", 7) != 0);
+
+	/* Install Xen paravirt ops */
+	paravirt_ops = xen_paravirt_ops;
+	machine_ops = xen_machine_ops;
+
+#ifdef CONFIG_SMP
+	smp_ops = xen_smp_ops;
+#endif
+
+	xen_setup_features();
+
+	/* Get mfn list */
+	if (!xen_feature(XENFEAT_auto_translated_physmap))
+		phys_to_machine_mapping = (unsigned long *)xen_start_info->mfn_list;
+
+	pgd = (pgd_t *)xen_start_info->pt_base;
+
+	init_pg_tables_end = __pa(pgd) + xen_start_info->nr_pt_frames*PAGE_SIZE;
+
+	init_mm.pgd = pgd; /* use the Xen pagetables to start */
+
+	/* keep using Xen gdt for now; no urgent need to change it */
+
+	x86_write_percpu(xen_cr3, __pa(pgd));
+
+#ifdef CONFIG_SMP
+	/* Don't do the full vcpu_info placement stuff until we have a
+	   possible map. */
+	per_cpu(xen_vcpu, 0) = &HYPERVISOR_shared_info->vcpu_info[0];
+#else
+	/* May as well do it now, since there's no good time to call
+	   it later on UP. */
+	xen_setup_vcpu_info_placement();
+#endif
+
+	paravirt_ops.kernel_rpl = 1;
+	if (xen_feature(XENFEAT_supervisor_mode_kernel))
+		paravirt_ops.kernel_rpl = 0;
+
+	/* set the limit of our address space */
+	reserve_top_address(-HYPERVISOR_VIRT_START + 2 * PAGE_SIZE);
+
+	/* set up basic CPUID stuff */
+	cpu_detect(&new_cpu_data);
+	new_cpu_data.hard_math = 1;
+	new_cpu_data.x86_capability[0] = cpuid_edx(1);
+
+	/* Poke various useful things into boot_params */
+	LOADER_TYPE = (9 << 4) | 0;
+	INITRD_START = xen_start_info->mod_start ? __pa(xen_start_info->mod_start) : 0;
+	INITRD_SIZE = xen_start_info->mod_len;
+
+	/* Start the world */
+	start_kernel();
+}
