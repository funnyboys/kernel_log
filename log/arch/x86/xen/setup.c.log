commit 2f6474e4636bcc68af6c44abb2703f12d7f083da
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:26 2020 +0200

    x86/entry: Switch XEN/PV hypercall entry to IDTENTRY
    
    Convert the XEN/PV hypercall to IDTENTRY:
    
      - Emit the ASM stub with DECLARE_IDTENTRY
      - Remove the ASM idtentry in 64-bit
      - Remove the open coded ASM entry code in 32-bit
      - Remove the old prototypes
    
    The handler stubs need to stay in ASM code as they need corner case handling
    and adjustment of the stack pointer.
    
    Provide a new C function which invokes the entry/exit handling and calls
    into the XEN handler on the interrupt stack if required.
    
    The exit code is slightly different from the regular idtentry_exit() on
    non-preemptible kernels. If the hypercall is preemptible and need_resched()
    is set then XEN provides a preempt hypercall scheduling function.
    
    Move this functionality into the entry code so it can use the existing
    idtentry functionality.
    
    [ mingo: Build fixes. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Juergen Gross <jgross@suse.com>
    Tested-by: Juergen Gross <jgross@suse.com>
    Link: https://lore.kernel.org/r/20200521202118.055270078@linutronix.de

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 1a2d8a50dac4..3566e37241d7 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -20,6 +20,7 @@
 #include <asm/setup.h>
 #include <asm/acpi.h>
 #include <asm/numa.h>
+#include <asm/idtentry.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/xen/hypercall.h>
 
@@ -993,7 +994,8 @@ static void __init xen_pvmmu_arch_setup(void)
 	HYPERVISOR_vm_assist(VMASST_CMD_enable,
 			     VMASST_TYPE_pae_extended_cr3);
 
-	if (register_callback(CALLBACKTYPE_event, xen_hypervisor_callback) ||
+	if (register_callback(CALLBACKTYPE_event,
+			      xen_asm_exc_xen_hypervisor_callback) ||
 	    register_callback(CALLBACKTYPE_failsafe, xen_failsafe_callback))
 		BUG();
 

commit 0e1b4271078787d3408d3dd314d80b290578cc00
Author: Jason Yan <yanaijie@huawei.com>
Date:   Wed Apr 8 10:46:05 2020 +0800

    x86/xen: make xen_pvmmu_arch_setup() static
    
    Fix the following sparse warning:
    
    arch/x86/xen/setup.c:998:12: warning: symbol 'xen_pvmmu_arch_setup' was not
    declared. Should it be static?
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Link: https://lore.kernel.org/r/20200408024605.42394-1-yanaijie@huawei.com
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 33b0e20df7fc..1a2d8a50dac4 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -985,7 +985,7 @@ void xen_enable_syscall(void)
 #endif /* CONFIG_X86_64 */
 }
 
-void __init xen_pvmmu_arch_setup(void)
+static void __init xen_pvmmu_arch_setup(void)
 {
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_4gb_segments);
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_writable_pagetables);

commit 8d3bcc441e6cddbb5fe49b59f7766f01f1e2493b
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Fri Oct 18 11:18:24 2019 +0800

    x86: Use pr_warn instead of pr_warning
    
    As said in commit f2c2cbcc35d4 ("powerpc: Use pr_warn instead of
    pr_warning"), removing pr_warning so all logging messages use a
    consistent <prefix>_warn style. Let's do it.
    
    Link: http://lkml.kernel.org/r/20191018031850.48498-7-wangkefeng.wang@huawei.com
    To: linux-kernel@vger.kernel.org
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Darren Hart <dvhart@infradead.org>
    Cc: Andy Shevchenko <andy@infradead.org>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Reviewed-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 548d1e0a5ba1..33b0e20df7fc 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -412,7 +412,7 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 
 		remap_range_size = xen_find_pfn_range(&remap_pfn);
 		if (!remap_range_size) {
-			pr_warning("Unable to find available pfn range, not remapping identity pages\n");
+			pr_warn("Unable to find available pfn range, not remapping identity pages\n");
 			xen_set_identity_and_release_chunk(cur_pfn,
 						cur_pfn + left, nr_pages);
 			break;

commit 1d988ed46543ca36c010634c97ac32114362ddb1
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Feb 14 11:42:40 2019 +0100

    x86/xen: dont add memory above max allowed allocation
    
    Don't allow memory to be added above the allowed maximum allocation
    limit set by Xen.
    
    Trying to do so would result in cases like the following:
    
    [  584.559652] ------------[ cut here ]------------
    [  584.564897] WARNING: CPU: 2 PID: 1 at ../arch/x86/xen/multicalls.c:129 xen_alloc_pte+0x1c7/0x390()
    [  584.575151] Modules linked in:
    [  584.578643] Supported: Yes
    [  584.581750] CPU: 2 PID: 1 Comm: swapper/0 Not tainted 4.4.120-92.70-default #1
    [  584.590000] Hardware name: Cisco Systems Inc UCSC-C460-M4/UCSC-C460-M4, BIOS C460M4.4.0.1b.0.0629181419 06/29/2018
    [  584.601862]  0000000000000000 ffffffff813175a0 0000000000000000 ffffffff8184777c
    [  584.610200]  ffffffff8107f4e1 ffff880487eb7000 ffff8801862b79c0 ffff88048608d290
    [  584.618537]  0000000000487eb7 ffffea0000000201 ffffffff81009de7 ffffffff81068561
    [  584.626876] Call Trace:
    [  584.629699]  [<ffffffff81019ad9>] dump_trace+0x59/0x340
    [  584.635645]  [<ffffffff81019eaa>] show_stack_log_lvl+0xea/0x170
    [  584.642391]  [<ffffffff8101ac51>] show_stack+0x21/0x40
    [  584.648238]  [<ffffffff813175a0>] dump_stack+0x5c/0x7c
    [  584.654085]  [<ffffffff8107f4e1>] warn_slowpath_common+0x81/0xb0
    [  584.660932]  [<ffffffff81009de7>] xen_alloc_pte+0x1c7/0x390
    [  584.667289]  [<ffffffff810647f0>] pmd_populate_kernel.constprop.6+0x40/0x80
    [  584.675241]  [<ffffffff815ecfe8>] phys_pmd_init+0x210/0x255
    [  584.681587]  [<ffffffff815ed207>] phys_pud_init+0x1da/0x247
    [  584.687931]  [<ffffffff815edb3b>] kernel_physical_mapping_init+0xf5/0x1d4
    [  584.695682]  [<ffffffff815e9bdd>] init_memory_mapping+0x18d/0x380
    [  584.702631]  [<ffffffff81064699>] arch_add_memory+0x59/0xf0
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index d5f303c0e656..548d1e0a5ba1 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -12,6 +12,7 @@
 #include <linux/memblock.h>
 #include <linux/cpuidle.h>
 #include <linux/cpufreq.h>
+#include <linux/memory_hotplug.h>
 
 #include <asm/elf.h>
 #include <asm/vdso.h>
@@ -589,6 +590,14 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 	if (type == E820_TYPE_RAM) {
 		start = PAGE_ALIGN(start);
 		end &= ~((phys_addr_t)PAGE_SIZE - 1);
+#ifdef CONFIG_MEMORY_HOTPLUG
+		/*
+		 * Don't allow adding memory not in E820 map while booting the
+		 * system. Once the balloon driver is up it will remove that
+		 * restriction again.
+		 */
+		max_mem_size = end;
+#endif
 	}
 
 	e820__range_add(start, end - start, type);
@@ -748,6 +757,10 @@ char * __init xen_memory_setup(void)
 	memmap.nr_entries = ARRAY_SIZE(xen_e820_table.entries);
 	set_xen_guest_handle(memmap.buffer, xen_e820_table.entries);
 
+#if defined(CONFIG_MEMORY_HOTPLUG) && defined(CONFIG_XEN_BALLOON)
+	xen_saved_max_mem_size = max_mem_size;
+#endif
+
 	op = xen_initial_domain() ?
 		XENMEM_machine_memory_map :
 		XENMEM_memory_map;

commit a97673a1c43d005a3ae215f4ca8b4bbb5691aea1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 3 10:47:34 2018 +0100

    x86: Fix various typos in comments
    
    Go over arch/x86/ and fix common typos in comments,
    and a typo in an actual function argument name.
    
    No change in functionality intended.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 075ed47993bb..d5f303c0e656 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -493,7 +493,7 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
  * The remap information (which mfn remap to which pfn) is contained in the
  * to be remapped memory itself in a linked list anchored at xen_remap_mfn.
  * This scheme allows to remap the different chunks in arbitrary order while
- * the resulting mapping will be independant from the order.
+ * the resulting mapping will be independent from the order.
  */
 void __init xen_remap_memory(void)
 {

commit 123664101aa2156d05251704fc63f9bcbf77741a
Author: Igor Druzhinin <igor.druzhinin@citrix.com>
Date:   Tue Nov 27 20:58:21 2018 +0000

    Revert "xen/balloon: Mark unallocated host memory as UNUSABLE"
    
    This reverts commit b3cf8528bb21febb650a7ecbf080d0647be40b9f.
    
    That commit unintentionally broke Xen balloon memory hotplug with
    "hotplug_unpopulated" set to 1. As long as "System RAM" resource
    got assigned under a new "Unusable memory" resource in IO/Mem tree
    any attempt to online this memory would fail due to general kernel
    restrictions on having "System RAM" resources as 1st level only.
    
    The original issue that commit has tried to workaround fa564ad96366
    ("x86/PCI: Enable a 64bit BAR on AMD Family 15h (Models 00-1f, 30-3f,
    60-7f)") also got amended by the following 03a551734 ("x86/PCI: Move
    and shrink AMD 64-bit window to avoid conflict") which made the
    original fix to Xen ballooning unnecessary.
    
    Signed-off-by: Igor Druzhinin <igor.druzhinin@citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 1163e33121fb..075ed47993bb 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -808,6 +808,7 @@ char * __init xen_memory_setup(void)
 	addr = xen_e820_table.entries[0].addr;
 	size = xen_e820_table.entries[0].size;
 	while (i < xen_e820_table.nr_entries) {
+		bool discard = false;
 
 		chunk_size = size;
 		type = xen_e820_table.entries[i].type;
@@ -823,10 +824,11 @@ char * __init xen_memory_setup(void)
 				xen_add_extra_mem(pfn_s, n_pfns);
 				xen_max_p2m_pfn = pfn_s + n_pfns;
 			} else
-				type = E820_TYPE_UNUSABLE;
+				discard = true;
 		}
 
-		xen_align_and_add_e820_region(addr, chunk_size, type);
+		if (!discard)
+			xen_align_and_add_e820_region(addr, chunk_size, type);
 
 		addr += chunk_size;
 		size -= chunk_size;

commit df11b6912f63ab86e694db5e9dee4df050f4c137
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Aug 20 15:02:03 2018 +0200

    x86/xen: remove unused function xen_auto_xlated_memory_setup()
    
    xen_auto_xlated_memory_setup() is a leftover from PVH V1. Remove it.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 6e0d2086eacb..1163e33121fb 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -905,37 +905,6 @@ char * __init xen_memory_setup(void)
 	return "Xen";
 }
 
-/*
- * Machine specific memory setup for auto-translated guests.
- */
-char * __init xen_auto_xlated_memory_setup(void)
-{
-	struct xen_memory_map memmap;
-	int i;
-	int rc;
-
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_table.entries);
-	set_xen_guest_handle(memmap.buffer, xen_e820_table.entries);
-
-	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
-	if (rc < 0)
-		panic("No memory map (%d)\n", rc);
-
-	xen_e820_table.nr_entries = memmap.nr_entries;
-
-	e820__update_table(&xen_e820_table);
-
-	for (i = 0; i < xen_e820_table.nr_entries; i++)
-		e820__range_add(xen_e820_table.entries[i].addr, xen_e820_table.entries[i].size, xen_e820_table.entries[i].type);
-
-	/* Remove p2m info, it is not needed. */
-	xen_start_info->mfn_list = 0;
-	xen_start_info->first_p2m_pfn = 0;
-	xen_start_info->nr_p2m_frames = 0;
-
-	return "Xen";
-}
-
 /*
  * Set the bit indicating "nosegneg" library variants should be used.
  * We only need to bother in pure 32-bit mode; compat 32-bit processes

commit 9ad95bdacaa3296460fc23e31feaead1d4ffc8b9
Merge: fca0e39b2bd2 b3cf8528bb21
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 22 12:30:10 2017 -0800

    Merge tag 'for-linus-4.15-rc5-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen fixes from Juergen Gross:
     "This contains two fixes for running under Xen:
    
       - a fix avoiding resource conflicts between adding mmio areas and
         memory hotplug
    
       - a fix setting NX bits in page table entries copied from Xen when
         running a PV guest"
    
    * tag 'for-linus-4.15-rc5-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/balloon: Mark unallocated host memory as UNUSABLE
      x86-64/Xen: eliminate W+X mappings

commit b3cf8528bb21febb650a7ecbf080d0647be40b9f
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Tue Dec 12 15:08:21 2017 -0500

    xen/balloon: Mark unallocated host memory as UNUSABLE
    
    Commit f5775e0b6116 ("x86/xen: discard RAM regions above the maximum
    reservation") left host memory not assigned to dom0 as available for
    memory hotplug.
    
    Unfortunately this also meant that those regions could be used by
    others. Specifically, commit fa564ad96366 ("x86/PCI: Enable a 64bit BAR
    on AMD Family 15h (Models 00-1f, 30-3f, 60-7f)") may try to map those
    addresses as MMIO.
    
    To prevent this mark unallocated host memory as E820_TYPE_UNUSABLE (thus
    effectively reverting f5775e0b6116) and keep track of that region as
    a hostmem resource that can be used for the hotplug.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ac55c02f98e9..e9011e1ee3de 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -807,7 +807,6 @@ char * __init xen_memory_setup(void)
 	addr = xen_e820_table.entries[0].addr;
 	size = xen_e820_table.entries[0].size;
 	while (i < xen_e820_table.nr_entries) {
-		bool discard = false;
 
 		chunk_size = size;
 		type = xen_e820_table.entries[i].type;
@@ -823,11 +822,10 @@ char * __init xen_memory_setup(void)
 				xen_add_extra_mem(pfn_s, n_pfns);
 				xen_max_p2m_pfn = pfn_s + n_pfns;
 			} else
-				discard = true;
+				type = E820_TYPE_UNUSABLE;
 		}
 
-		if (!discard)
-			xen_align_and_add_e820_region(addr, chunk_size, type);
+		xen_align_and_add_e820_region(addr, chunk_size, type);
 
 		addr += chunk_size;
 		size -= chunk_size;

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ac55c02f98e9..c114ca767b3b 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Machine specific setup for xen
  *

commit 82616f9599a707e8225ceca6000dc5ea5aa78e11
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Aug 4 13:36:11 2017 +0200

    xen: remove tests for pvh mode in pure pv paths
    
    Remove the last tests for XENFEAT_auto_translated_physmap in pure
    PV-domain specific paths. PVH V1 is gone and the feature will always
    be "false" in PV guests.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index c81046323ebc..ac55c02f98e9 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -340,8 +340,6 @@ static void __init xen_do_set_identity_and_remap_chunk(
 
 	WARN_ON(size == 0);
 
-	BUG_ON(xen_feature(XENFEAT_auto_translated_physmap));
-
 	mfn_save = virt_to_mfn(buf);
 
 	for (ident_pfn_iter = start_pfn, remap_pfn_iter = remap_pfn;
@@ -1024,8 +1022,7 @@ void __init xen_pvmmu_arch_setup(void)
 void __init xen_arch_setup(void)
 {
 	xen_panic_handler_init();
-	if (!xen_feature(XENFEAT_auto_translated_physmap))
-		xen_pvmmu_arch_setup();
+	xen_pvmmu_arch_setup();
 
 #ifdef CONFIG_ACPI
 	if (!(xen_start_info->flags & SIF_INITDOMAIN)) {

commit bf1b9ddf181d29ec91f87d9c52bcb551ccf04157
Author: Gustavo A. R. Silva <garsilva@embeddedor.com>
Date:   Fri Jun 23 17:01:20 2017 -0500

    x86: xen: remove unnecessary variable in xen_foreach_remap_area()
    
    Remove unnecessary variable mfn in function xen_foreach_remap_area() and,
    refactor the code.
    
    Variable mfn at line 518:mfn = xen_remap_buf.mfns[i];
    is only being used to store a value to be passed as
    an argument to the xen_update_mem_tables() function.
    This value can be passed directly, which makes variable
    mfn unnecessary. Also, value assigned to variable mfn
    at line 534:mfn = xen_remap_mfn; is never used.
    
    Addresses-Coverity-ID: 1260110
    Signed-off-by: Gustavo A. R. Silva <garsilva@embeddedor.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index a5bf7c451435..c81046323ebc 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -499,7 +499,7 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 void __init xen_remap_memory(void)
 {
 	unsigned long buf = (unsigned long)&xen_remap_buf;
-	unsigned long mfn_save, mfn, pfn;
+	unsigned long mfn_save, pfn;
 	unsigned long remapped = 0;
 	unsigned int i;
 	unsigned long pfn_s = ~0UL;
@@ -515,8 +515,7 @@ void __init xen_remap_memory(void)
 
 		pfn = xen_remap_buf.target_pfn;
 		for (i = 0; i < xen_remap_buf.size; i++) {
-			mfn = xen_remap_buf.mfns[i];
-			xen_update_mem_tables(pfn, mfn);
+			xen_update_mem_tables(pfn, xen_remap_buf.mfns[i]);
 			remapped++;
 			pfn++;
 		}
@@ -530,8 +529,6 @@ void __init xen_remap_memory(void)
 			pfn_s = xen_remap_buf.target_pfn;
 			len = xen_remap_buf.size;
 		}
-
-		mfn = xen_remap_mfn;
 		xen_remap_mfn = xen_remap_buf.next_area_mfn;
 	}
 

commit f9748fa045851041ba69a1d2899971746f29c9d5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 28 18:00:35 2017 +0100

    x86/boot/e820: Simplify the e820__update_table() interface
    
    The e820__update_table() parameters are pretty complex:
    
      arch/x86/include/asm/e820/api.h:extern int  e820__update_table(struct e820_entry *biosmap, int max_nr_map, u32 *pnr_map);
    
    But 90% of the usage is trivial:
    
      arch/x86/kernel/e820.c:       if (e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries))
      arch/x86/kernel/e820.c:       e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
      arch/x86/kernel/e820.c:       e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
      arch/x86/kernel/e820.c:               if (e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries) < 0)
      arch/x86/kernel/e820.c:       e820__update_table(boot_params.e820_table, ARRAY_SIZE(boot_params.e820_table), &new_nr);
      arch/x86/kernel/early-quirks.c:       e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
      arch/x86/kernel/setup.c:      e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
      arch/x86/kernel/setup.c:              e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
      arch/x86/platform/efi/efi.c:  e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
      arch/x86/xen/setup.c: e820__update_table(xen_e820_table.entries, ARRAY_SIZE(xen_e820_table.entries),
      arch/x86/xen/setup.c: e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
      arch/x86/xen/setup.c: e820__update_table(xen_e820_table.entries, ARRAY_SIZE(xen_e820_table.entries),
    
    as it only uses an exiting struct e820_table's entries array, its size and
    its current number of entries as input and output arguments.
    
    Only one use is non-trivial:
    
      arch/x86/kernel/e820.c:       e820__update_table(boot_params.e820_table, ARRAY_SIZE(boot_params.e820_table), &new_nr);
    
    ... which call updates the E820 table in the zeropage in-situ, and the layout there does not
    match that of 'struct e820_table' (in particular nr_entries is at a different offset,
    hardcoded by the boot protocol).
    
    Simplify all this by introducing a low level __e820__update_table() API that
    the zeropage update call can use, and simplifying the main e820__update_table()
    call signature down to:
    
            int e820__update_table(struct e820_table *table);
    
    This visibly simplifies all the call sites:
    
      arch/x86/include/asm/e820/api.h:extern int  e820__update_table(struct e820_table *table);
      arch/x86/include/asm/e820/types.h: * call to e820__update_table() to remove duplicates.  The allowance
      arch/x86/kernel/e820.c: * The return value from e820__update_table() is zero if it
      arch/x86/kernel/e820.c:int __init e820__update_table(struct e820_table *table)
      arch/x86/kernel/e820.c:       if (e820__update_table(e820_table))
      arch/x86/kernel/e820.c:       e820__update_table(e820_table_firmware);
      arch/x86/kernel/e820.c:       e820__update_table(e820_table);
      arch/x86/kernel/e820.c:       e820__update_table(e820_table);
      arch/x86/kernel/e820.c:               if (e820__update_table(e820_table) < 0)
      arch/x86/kernel/early-quirks.c:       e820__update_table(e820_table);
      arch/x86/kernel/setup.c:      e820__update_table(e820_table);
      arch/x86/kernel/setup.c:              e820__update_table(e820_table);
      arch/x86/platform/efi/efi.c:  e820__update_table(e820_table);
      arch/x86/xen/setup.c: e820__update_table(&xen_e820_table);
      arch/x86/xen/setup.c: e820__update_table(e820_table);
      arch/x86/xen/setup.c: e820__update_table(&xen_e820_table);
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index cf29abfc392c..a5bf7c451435 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -782,7 +782,7 @@ char * __init xen_memory_setup(void)
 		xen_ignore_unusable();
 
 	/* Make sure the Xen-supplied memory map is well-ordered. */
-	e820__update_table(xen_e820_table.entries, ARRAY_SIZE(xen_e820_table.entries), &xen_e820_table.nr_entries);
+	e820__update_table(&xen_e820_table);
 
 	max_pages = xen_get_max_pages();
 
@@ -856,10 +856,9 @@ char * __init xen_memory_setup(void)
 	 * reserve ISA memory anyway because too many things poke
 	 * about in there.
 	 */
-	e820__range_add(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
-			E820_TYPE_RESERVED);
+	e820__range_add(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS, E820_TYPE_RESERVED);
 
-	e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
+	e820__update_table(e820_table);
 
 	/*
 	 * Check whether the kernel itself conflicts with the target E820 map.
@@ -930,7 +929,7 @@ char * __init xen_auto_xlated_memory_setup(void)
 
 	xen_e820_table.nr_entries = memmap.nr_entries;
 
-	e820__update_table(xen_e820_table.entries, ARRAY_SIZE(xen_e820_table.entries), &xen_e820_table.nr_entries);
+	e820__update_table(&xen_e820_table);
 
 	for (i = 0; i < xen_e820_table.nr_entries; i++)
 		e820__range_add(xen_e820_table.entries[i].addr, xen_e820_table.entries[i].size, xen_e820_table.entries[i].type);

commit e7dbf7ad4172ef79bcfdb432f9463eda892951ca
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 28 18:19:01 2017 +0100

    xen, x86/boot/e820: Simplify Xen's xen_e820_table construct
    
    The Xen guest memory setup code has:
    
            static struct e820_entry xen_e820_table[E820_MAX_ENTRIES] __initdata;
            static u32 xen_e820_table_entries __initdata;
    
    ... which is really a 'struct e820_table', open-coded.
    
    Convert the Xen code over to use a single struct e820_table, as this
    will allow the simplification of the e820__update_table() API.
    
    No intended change in functionality, but not runtime tested.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 381a0d3577a7..cf29abfc392c 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -41,8 +41,7 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 unsigned long xen_released_pages;
 
 /* E820 map used during setting up memory. */
-static struct e820_entry xen_e820_table[E820_MAX_ENTRIES] __initdata;
-static u32 xen_e820_table_entries __initdata;
+static struct e820_table xen_e820_table __initdata;
 
 /*
  * Buffer used to remap identity mapped pages. We only need the virtual space.
@@ -198,11 +197,11 @@ void __init xen_inv_extra_mem(void)
  */
 static unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)
 {
-	const struct e820_entry *entry = xen_e820_table;
+	const struct e820_entry *entry = xen_e820_table.entries;
 	unsigned int i;
 	unsigned long done = 0;
 
-	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_table.nr_entries; i++, entry++) {
 		unsigned long s_pfn;
 		unsigned long e_pfn;
 
@@ -457,7 +456,7 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 {
 	phys_addr_t start = 0;
 	unsigned long ret_val = 0;
-	const struct e820_entry *entry = xen_e820_table;
+	const struct e820_entry *entry = xen_e820_table.entries;
 	int i;
 
 	/*
@@ -471,9 +470,9 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 	 * example) the DMI tables in a reserved region that begins on
 	 * a non-page boundary.
 	 */
-	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_table.nr_entries; i++, entry++) {
 		phys_addr_t end = entry->addr + entry->size;
-		if (entry->type == E820_TYPE_RAM || i == xen_e820_table_entries - 1) {
+		if (entry->type == E820_TYPE_RAM || i == xen_e820_table.nr_entries - 1) {
 			unsigned long start_pfn = PFN_DOWN(start);
 			unsigned long end_pfn = PFN_UP(end);
 
@@ -601,10 +600,10 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 
 static void __init xen_ignore_unusable(void)
 {
-	struct e820_entry *entry = xen_e820_table;
+	struct e820_entry *entry = xen_e820_table.entries;
 	unsigned int i;
 
-	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_table.nr_entries; i++, entry++) {
 		if (entry->type == E820_TYPE_UNUSABLE)
 			entry->type = E820_TYPE_RAM;
 	}
@@ -620,9 +619,9 @@ bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
 		return false;
 
 	end = start + size;
-	entry = xen_e820_table;
+	entry = xen_e820_table.entries;
 
-	for (mapcnt = 0; mapcnt < xen_e820_table_entries; mapcnt++) {
+	for (mapcnt = 0; mapcnt < xen_e820_table.nr_entries; mapcnt++) {
 		if (entry->type == E820_TYPE_RAM && entry->addr <= start &&
 		    (entry->addr + entry->size) >= end)
 			return false;
@@ -645,9 +644,9 @@ phys_addr_t __init xen_find_free_area(phys_addr_t size)
 {
 	unsigned mapcnt;
 	phys_addr_t addr, start;
-	struct e820_entry *entry = xen_e820_table;
+	struct e820_entry *entry = xen_e820_table.entries;
 
-	for (mapcnt = 0; mapcnt < xen_e820_table_entries; mapcnt++, entry++) {
+	for (mapcnt = 0; mapcnt < xen_e820_table.nr_entries; mapcnt++, entry++) {
 		if (entry->type != E820_TYPE_RAM || entry->size < size)
 			continue;
 		start = entry->addr;
@@ -750,8 +749,8 @@ char * __init xen_memory_setup(void)
 	max_pfn = min(max_pfn, xen_start_info->nr_pages);
 	mem_end = PFN_PHYS(max_pfn);
 
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_table);
-	set_xen_guest_handle(memmap.buffer, xen_e820_table);
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_table.entries);
+	set_xen_guest_handle(memmap.buffer, xen_e820_table.entries);
 
 	op = xen_initial_domain() ?
 		XENMEM_machine_memory_map :
@@ -760,16 +759,16 @@ char * __init xen_memory_setup(void)
 	if (rc == -ENOSYS) {
 		BUG_ON(xen_initial_domain());
 		memmap.nr_entries = 1;
-		xen_e820_table[0].addr = 0ULL;
-		xen_e820_table[0].size = mem_end;
+		xen_e820_table.entries[0].addr = 0ULL;
+		xen_e820_table.entries[0].size = mem_end;
 		/* 8MB slack (to balance backend allocations). */
-		xen_e820_table[0].size += 8ULL << 20;
-		xen_e820_table[0].type = E820_TYPE_RAM;
+		xen_e820_table.entries[0].size += 8ULL << 20;
+		xen_e820_table.entries[0].type = E820_TYPE_RAM;
 		rc = 0;
 	}
 	BUG_ON(rc);
 	BUG_ON(memmap.nr_entries == 0);
-	xen_e820_table_entries = memmap.nr_entries;
+	xen_e820_table.nr_entries = memmap.nr_entries;
 
 	/*
 	 * Xen won't allow a 1:1 mapping to be created to UNUSABLE
@@ -783,8 +782,7 @@ char * __init xen_memory_setup(void)
 		xen_ignore_unusable();
 
 	/* Make sure the Xen-supplied memory map is well-ordered. */
-	e820__update_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
-			  &xen_e820_table_entries);
+	e820__update_table(xen_e820_table.entries, ARRAY_SIZE(xen_e820_table.entries), &xen_e820_table.nr_entries);
 
 	max_pages = xen_get_max_pages();
 
@@ -811,13 +809,13 @@ char * __init xen_memory_setup(void)
 	extra_pages = min3(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
 			   extra_pages, max_pages - max_pfn);
 	i = 0;
-	addr = xen_e820_table[0].addr;
-	size = xen_e820_table[0].size;
-	while (i < xen_e820_table_entries) {
+	addr = xen_e820_table.entries[0].addr;
+	size = xen_e820_table.entries[0].size;
+	while (i < xen_e820_table.nr_entries) {
 		bool discard = false;
 
 		chunk_size = size;
-		type = xen_e820_table[i].type;
+		type = xen_e820_table.entries[i].type;
 
 		if (type == E820_TYPE_RAM) {
 			if (addr < mem_end) {
@@ -840,9 +838,9 @@ char * __init xen_memory_setup(void)
 		size -= chunk_size;
 		if (size == 0) {
 			i++;
-			if (i < xen_e820_table_entries) {
-				addr = xen_e820_table[i].addr;
-				size = xen_e820_table[i].size;
+			if (i < xen_e820_table.nr_entries) {
+				addr = xen_e820_table.entries[i].addr;
+				size = xen_e820_table.entries[i].size;
 			}
 		}
 	}
@@ -923,21 +921,19 @@ char * __init xen_auto_xlated_memory_setup(void)
 	int i;
 	int rc;
 
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_table);
-	set_xen_guest_handle(memmap.buffer, xen_e820_table);
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_table.entries);
+	set_xen_guest_handle(memmap.buffer, xen_e820_table.entries);
 
 	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
 	if (rc < 0)
 		panic("No memory map (%d)\n", rc);
 
-	xen_e820_table_entries = memmap.nr_entries;
+	xen_e820_table.nr_entries = memmap.nr_entries;
 
-	e820__update_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
-			  &xen_e820_table_entries);
+	e820__update_table(xen_e820_table.entries, ARRAY_SIZE(xen_e820_table.entries), &xen_e820_table.nr_entries);
 
-	for (i = 0; i < xen_e820_table_entries; i++)
-		e820__range_add(xen_e820_table[i].addr, xen_e820_table[i].size,
-				xen_e820_table[i].type);
+	for (i = 0; i < xen_e820_table.nr_entries; i++)
+		e820__range_add(xen_e820_table.entries[i].addr, xen_e820_table.entries[i].size, xen_e820_table.entries[i].type);
 
 	/* Remove p2m info, it is not needed. */
 	xen_start_info->mfn_list = 0;

commit 08b46d5dd869ea631d7c1c15535c930c8ea462e0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 28 17:29:08 2017 +0100

    x86/boot/e820: Clean up the E820 table size define names
    
    We've got a number of defines related to the E820 table and its size:
    
            E820MAP
            E820NR
            E820_X_MAX
            E820MAX
    
    The first two denote byte offsets into the zeropage (struct boot_params),
    and can are not used in the kernel and can be removed.
    
    The E820_*_MAX values have an inconsistent structure and it's unclear in any
    case what they mean. 'X' presuably goes for extended - but it's not very
    expressive altogether.
    
    Change these over to:
    
            E820_MAX_ENTRIES_ZEROPAGE
            E820_MAX_ENTRIES
    
    ... which are self-explanatory names.
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 625ceafeb7a0..381a0d3577a7 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -41,7 +41,7 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 unsigned long xen_released_pages;
 
 /* E820 map used during setting up memory. */
-static struct e820_entry xen_e820_table[E820_X_MAX] __initdata;
+static struct e820_entry xen_e820_table[E820_MAX_ENTRIES] __initdata;
 static u32 xen_e820_table_entries __initdata;
 
 /*

commit 09821ff1d50a1ecade182c2a68a90f835e257eef
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 28 17:09:33 2017 +0100

    x86/boot/e820: Prefix the E820_* type names with "E820_TYPE_"
    
    So there's a number of constants that start with "E820" but which
    are not types - these create a confusing mixture when seen together
    with 'enum e820_type' values:
    
            E820MAP
            E820NR
            E820_X_MAX
            E820MAX
    
    To better differentiate the 'enum e820_type' values prefix them
    with E820_TYPE_.
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index a634723e660f..625ceafeb7a0 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -206,7 +206,7 @@ static unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)
 		unsigned long s_pfn;
 		unsigned long e_pfn;
 
-		if (entry->type != E820_RAM)
+		if (entry->type != E820_TYPE_RAM)
 			continue;
 
 		e_pfn = PFN_DOWN(entry->addr + entry->size);
@@ -473,11 +473,11 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 	 */
 	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
 		phys_addr_t end = entry->addr + entry->size;
-		if (entry->type == E820_RAM || i == xen_e820_table_entries - 1) {
+		if (entry->type == E820_TYPE_RAM || i == xen_e820_table_entries - 1) {
 			unsigned long start_pfn = PFN_DOWN(start);
 			unsigned long end_pfn = PFN_UP(end);
 
-			if (entry->type == E820_RAM)
+			if (entry->type == E820_TYPE_RAM)
 				end_pfn = PFN_UP(entry->addr);
 
 			if (start_pfn < end_pfn)
@@ -591,7 +591,7 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 	phys_addr_t end = start + size;
 
 	/* Align RAM regions to page boundaries. */
-	if (type == E820_RAM) {
+	if (type == E820_TYPE_RAM) {
 		start = PAGE_ALIGN(start);
 		end &= ~((phys_addr_t)PAGE_SIZE - 1);
 	}
@@ -605,8 +605,8 @@ static void __init xen_ignore_unusable(void)
 	unsigned int i;
 
 	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
-		if (entry->type == E820_UNUSABLE)
-			entry->type = E820_RAM;
+		if (entry->type == E820_TYPE_UNUSABLE)
+			entry->type = E820_TYPE_RAM;
 	}
 }
 
@@ -623,7 +623,7 @@ bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
 	entry = xen_e820_table;
 
 	for (mapcnt = 0; mapcnt < xen_e820_table_entries; mapcnt++) {
-		if (entry->type == E820_RAM && entry->addr <= start &&
+		if (entry->type == E820_TYPE_RAM && entry->addr <= start &&
 		    (entry->addr + entry->size) >= end)
 			return false;
 
@@ -648,7 +648,7 @@ phys_addr_t __init xen_find_free_area(phys_addr_t size)
 	struct e820_entry *entry = xen_e820_table;
 
 	for (mapcnt = 0; mapcnt < xen_e820_table_entries; mapcnt++, entry++) {
-		if (entry->type != E820_RAM || entry->size < size)
+		if (entry->type != E820_TYPE_RAM || entry->size < size)
 			continue;
 		start = entry->addr;
 		for (addr = start; addr < start + size; addr += PAGE_SIZE) {
@@ -764,7 +764,7 @@ char * __init xen_memory_setup(void)
 		xen_e820_table[0].size = mem_end;
 		/* 8MB slack (to balance backend allocations). */
 		xen_e820_table[0].size += 8ULL << 20;
-		xen_e820_table[0].type = E820_RAM;
+		xen_e820_table[0].type = E820_TYPE_RAM;
 		rc = 0;
 	}
 	BUG_ON(rc);
@@ -819,7 +819,7 @@ char * __init xen_memory_setup(void)
 		chunk_size = size;
 		type = xen_e820_table[i].type;
 
-		if (type == E820_RAM) {
+		if (type == E820_TYPE_RAM) {
 			if (addr < mem_end) {
 				chunk_size = min(size, mem_end - addr);
 			} else if (extra_pages) {
@@ -859,7 +859,7 @@ char * __init xen_memory_setup(void)
 	 * about in there.
 	 */
 	e820__range_add(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
-			E820_RESERVED);
+			E820_TYPE_RESERVED);
 
 	e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
 

commit ab6bc04cfdbd5da00a85909c054770a606e7c804
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 28 14:19:36 2017 +0100

    x86/boot/e820: Create coherent API function names for E820 range operations
    
    We have these three related functions:
    
     extern void e820_add_region(u64 start, u64 size, int type);
     extern u64  e820_update_range(u64 start, u64 size, unsigned old_type, unsigned new_type);
     extern u64  e820_remove_range(u64 start, u64 size, unsigned old_type, int checktype);
    
    But it's not clear from the naming that they are 3 operations based around the
    same 'memory range' concept. Rename them to better signal this, and move
    the prototypes next to each other:
    
     extern void e820__range_add   (u64 start, u64 size, int type);
     extern u64  e820__range_update(u64 start, u64 size, unsigned old_type, unsigned new_type);
     extern u64  e820__range_remove(u64 start, u64 size, unsigned old_type, int checktype);
    
    Note that this improved organization of the functions shows another problem that was easy
    to miss before: sometimes the E820 entry type is 'int', sometimes 'unsigned int' - but this
    will be fixed in a separate patch.
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index c2f8e4ab55bd..a634723e660f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -596,7 +596,7 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 		end &= ~((phys_addr_t)PAGE_SIZE - 1);
 	}
 
-	e820_add_region(start, end - start, type);
+	e820__range_add(start, end - start, type);
 }
 
 static void __init xen_ignore_unusable(void)
@@ -858,7 +858,7 @@ char * __init xen_memory_setup(void)
 	 * reserve ISA memory anyway because too many things poke
 	 * about in there.
 	 */
-	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
+	e820__range_add(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
 	e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
@@ -936,7 +936,7 @@ char * __init xen_auto_xlated_memory_setup(void)
 			  &xen_e820_table_entries);
 
 	for (i = 0; i < xen_e820_table_entries; i++)
-		e820_add_region(xen_e820_table[i].addr, xen_e820_table[i].size,
+		e820__range_add(xen_e820_table[i].addr, xen_e820_table[i].size,
 				xen_e820_table[i].type);
 
 	/* Remove p2m info, it is not needed. */

commit f52355a99fc06f609ca6a61098d78e476d56f526
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 28 14:09:20 2017 +0100

    x86/boot/e820: Rename sanitize_e820_table() to e820__update_table()
    
    sanitize_e820_table() is a minor misnomer in that it suggests that
    the E820 table requires sanitizing - which implies that it will only
    do anything if the E820 table is irregular (not sane).
    
    That is wrong, because sanitize_e820_table() also does a very regular
    sorting of the E820 table, which is a necessity in the basic
    append-only flow of E820 updates the kernel is allowed to perform to
    it.
    
    So rename it to e820__update_table() to include that purpose as well.
    
    This also lines up all the table-update functions into a coherent
    naming family:
    
      int  e820__update_table(struct e820_entry *biosmap, int max_nr_map, u32 *pnr_map);
    
      void e820__update_table_print(void);
      void e820__update_table_firmware(void);
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 8b26b9282070..c2f8e4ab55bd 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -783,7 +783,7 @@ char * __init xen_memory_setup(void)
 		xen_ignore_unusable();
 
 	/* Make sure the Xen-supplied memory map is well-ordered. */
-	sanitize_e820_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
+	e820__update_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
 			  &xen_e820_table_entries);
 
 	max_pages = xen_get_max_pages();
@@ -861,7 +861,7 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
-	sanitize_e820_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
+	e820__update_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
 
 	/*
 	 * Check whether the kernel itself conflicts with the target E820 map.
@@ -932,7 +932,7 @@ char * __init xen_auto_xlated_memory_setup(void)
 
 	xen_e820_table_entries = memmap.nr_entries;
 
-	sanitize_e820_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
+	e820__update_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
 			  &xen_e820_table_entries);
 
 	for (i = 0; i < xen_e820_table_entries; i++)

commit bf495573fae84451a8a26215fafb5b62e387ddaf
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jan 27 14:06:21 2017 +0100

    x86/boot/e820: Harmonize the 'struct e820_table' fields
    
    So the e820_table->map and e820_table->nr_map names are a bit
    confusing, because it's not clear what a 'map' really means
    (it could be a bitmap, or some other data structure), nor is
    it clear what nr_map means (is it a current index, or some
    other count).
    
    Rename the fields from:
    
     e820_table->map        =>     e820_table->entries
     e820_table->nr_map     =>     e820_table->nr_entries
    
    which makes it abundantly clear that these are entries
    of the table, and that the size of the table is ->nr_entries.
    
    Propagate the changes to all affected files. Where necessary,
    adjust local variable names to better reflect the new field names.
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f98713e5a705..8b26b9282070 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -861,7 +861,7 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
-	sanitize_e820_table(e820_table->map, ARRAY_SIZE(e820_table->map), &e820_table->nr_map);
+	sanitize_e820_table(e820_table->entries, ARRAY_SIZE(e820_table->entries), &e820_table->nr_entries);
 
 	/*
 	 * Check whether the kernel itself conflicts with the target E820 map.

commit 61a50101638254d38e3f4281265b44de0f2cba4e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jan 27 13:54:38 2017 +0100

    x86/boot/e820: Rename everything to e820_table
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 3cc90e902064..f98713e5a705 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -41,8 +41,8 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 unsigned long xen_released_pages;
 
 /* E820 map used during setting up memory. */
-static struct e820_entry xen_e820_array[E820_X_MAX] __initdata;
-static u32 xen_e820_array_entries __initdata;
+static struct e820_entry xen_e820_table[E820_X_MAX] __initdata;
+static u32 xen_e820_table_entries __initdata;
 
 /*
  * Buffer used to remap identity mapped pages. We only need the virtual space.
@@ -198,11 +198,11 @@ void __init xen_inv_extra_mem(void)
  */
 static unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)
 {
-	const struct e820_entry *entry = xen_e820_array;
+	const struct e820_entry *entry = xen_e820_table;
 	unsigned int i;
 	unsigned long done = 0;
 
-	for (i = 0; i < xen_e820_array_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
 		unsigned long s_pfn;
 		unsigned long e_pfn;
 
@@ -457,7 +457,7 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 {
 	phys_addr_t start = 0;
 	unsigned long ret_val = 0;
-	const struct e820_entry *entry = xen_e820_array;
+	const struct e820_entry *entry = xen_e820_table;
 	int i;
 
 	/*
@@ -471,9 +471,9 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 	 * example) the DMI tables in a reserved region that begins on
 	 * a non-page boundary.
 	 */
-	for (i = 0; i < xen_e820_array_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
 		phys_addr_t end = entry->addr + entry->size;
-		if (entry->type == E820_RAM || i == xen_e820_array_entries - 1) {
+		if (entry->type == E820_RAM || i == xen_e820_table_entries - 1) {
 			unsigned long start_pfn = PFN_DOWN(start);
 			unsigned long end_pfn = PFN_UP(end);
 
@@ -601,10 +601,10 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 
 static void __init xen_ignore_unusable(void)
 {
-	struct e820_entry *entry = xen_e820_array;
+	struct e820_entry *entry = xen_e820_table;
 	unsigned int i;
 
-	for (i = 0; i < xen_e820_array_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_table_entries; i++, entry++) {
 		if (entry->type == E820_UNUSABLE)
 			entry->type = E820_RAM;
 	}
@@ -620,9 +620,9 @@ bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
 		return false;
 
 	end = start + size;
-	entry = xen_e820_array;
+	entry = xen_e820_table;
 
-	for (mapcnt = 0; mapcnt < xen_e820_array_entries; mapcnt++) {
+	for (mapcnt = 0; mapcnt < xen_e820_table_entries; mapcnt++) {
 		if (entry->type == E820_RAM && entry->addr <= start &&
 		    (entry->addr + entry->size) >= end)
 			return false;
@@ -645,9 +645,9 @@ phys_addr_t __init xen_find_free_area(phys_addr_t size)
 {
 	unsigned mapcnt;
 	phys_addr_t addr, start;
-	struct e820_entry *entry = xen_e820_array;
+	struct e820_entry *entry = xen_e820_table;
 
-	for (mapcnt = 0; mapcnt < xen_e820_array_entries; mapcnt++, entry++) {
+	for (mapcnt = 0; mapcnt < xen_e820_table_entries; mapcnt++, entry++) {
 		if (entry->type != E820_RAM || entry->size < size)
 			continue;
 		start = entry->addr;
@@ -750,8 +750,8 @@ char * __init xen_memory_setup(void)
 	max_pfn = min(max_pfn, xen_start_info->nr_pages);
 	mem_end = PFN_PHYS(max_pfn);
 
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_array);
-	set_xen_guest_handle(memmap.buffer, xen_e820_array);
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_table);
+	set_xen_guest_handle(memmap.buffer, xen_e820_table);
 
 	op = xen_initial_domain() ?
 		XENMEM_machine_memory_map :
@@ -760,16 +760,16 @@ char * __init xen_memory_setup(void)
 	if (rc == -ENOSYS) {
 		BUG_ON(xen_initial_domain());
 		memmap.nr_entries = 1;
-		xen_e820_array[0].addr = 0ULL;
-		xen_e820_array[0].size = mem_end;
+		xen_e820_table[0].addr = 0ULL;
+		xen_e820_table[0].size = mem_end;
 		/* 8MB slack (to balance backend allocations). */
-		xen_e820_array[0].size += 8ULL << 20;
-		xen_e820_array[0].type = E820_RAM;
+		xen_e820_table[0].size += 8ULL << 20;
+		xen_e820_table[0].type = E820_RAM;
 		rc = 0;
 	}
 	BUG_ON(rc);
 	BUG_ON(memmap.nr_entries == 0);
-	xen_e820_array_entries = memmap.nr_entries;
+	xen_e820_table_entries = memmap.nr_entries;
 
 	/*
 	 * Xen won't allow a 1:1 mapping to be created to UNUSABLE
@@ -783,8 +783,8 @@ char * __init xen_memory_setup(void)
 		xen_ignore_unusable();
 
 	/* Make sure the Xen-supplied memory map is well-ordered. */
-	sanitize_e820_array(xen_e820_array, ARRAY_SIZE(xen_e820_array),
-			  &xen_e820_array_entries);
+	sanitize_e820_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
+			  &xen_e820_table_entries);
 
 	max_pages = xen_get_max_pages();
 
@@ -811,13 +811,13 @@ char * __init xen_memory_setup(void)
 	extra_pages = min3(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
 			   extra_pages, max_pages - max_pfn);
 	i = 0;
-	addr = xen_e820_array[0].addr;
-	size = xen_e820_array[0].size;
-	while (i < xen_e820_array_entries) {
+	addr = xen_e820_table[0].addr;
+	size = xen_e820_table[0].size;
+	while (i < xen_e820_table_entries) {
 		bool discard = false;
 
 		chunk_size = size;
-		type = xen_e820_array[i].type;
+		type = xen_e820_table[i].type;
 
 		if (type == E820_RAM) {
 			if (addr < mem_end) {
@@ -840,9 +840,9 @@ char * __init xen_memory_setup(void)
 		size -= chunk_size;
 		if (size == 0) {
 			i++;
-			if (i < xen_e820_array_entries) {
-				addr = xen_e820_array[i].addr;
-				size = xen_e820_array[i].size;
+			if (i < xen_e820_table_entries) {
+				addr = xen_e820_table[i].addr;
+				size = xen_e820_table[i].size;
 			}
 		}
 	}
@@ -861,7 +861,7 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
-	sanitize_e820_array(e820_array->map, ARRAY_SIZE(e820_array->map), &e820_array->nr_map);
+	sanitize_e820_table(e820_table->map, ARRAY_SIZE(e820_table->map), &e820_table->nr_map);
 
 	/*
 	 * Check whether the kernel itself conflicts with the target E820 map.
@@ -923,21 +923,21 @@ char * __init xen_auto_xlated_memory_setup(void)
 	int i;
 	int rc;
 
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_array);
-	set_xen_guest_handle(memmap.buffer, xen_e820_array);
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_table);
+	set_xen_guest_handle(memmap.buffer, xen_e820_table);
 
 	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
 	if (rc < 0)
 		panic("No memory map (%d)\n", rc);
 
-	xen_e820_array_entries = memmap.nr_entries;
+	xen_e820_table_entries = memmap.nr_entries;
 
-	sanitize_e820_array(xen_e820_array, ARRAY_SIZE(xen_e820_array),
-			  &xen_e820_array_entries);
+	sanitize_e820_table(xen_e820_table, ARRAY_SIZE(xen_e820_table),
+			  &xen_e820_table_entries);
 
-	for (i = 0; i < xen_e820_array_entries; i++)
-		e820_add_region(xen_e820_array[i].addr, xen_e820_array[i].size,
-				xen_e820_array[i].type);
+	for (i = 0; i < xen_e820_table_entries; i++)
+		e820_add_region(xen_e820_table[i].addr, xen_e820_table[i].size,
+				xen_e820_table[i].type);
 
 	/* Remove p2m info, it is not needed. */
 	xen_start_info->mfn_list = 0;

commit acd4c048728814505fae8e224cf9074bd1ad291e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jan 27 13:20:53 2017 +0100

    x86/boot/e820: Rename 'e820_map' variables to 'e820_array'
    
    In line with the rename to 'struct e820_array', harmonize the naming of common e820
    table variable names as well:
    
     e820          =>  e820_array
     e820_saved    =>  e820_array_saved
     e820_map      =>  e820_array
     initial_e820  =>  e820_array_init
    
    This makes the variable names more consistent  and easier to grep for.
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 7386a6e4c072..3cc90e902064 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -41,8 +41,8 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 unsigned long xen_released_pages;
 
 /* E820 map used during setting up memory. */
-static struct e820_entry xen_e820_map[E820_X_MAX] __initdata;
-static u32 xen_e820_map_entries __initdata;
+static struct e820_entry xen_e820_array[E820_X_MAX] __initdata;
+static u32 xen_e820_array_entries __initdata;
 
 /*
  * Buffer used to remap identity mapped pages. We only need the virtual space.
@@ -198,11 +198,11 @@ void __init xen_inv_extra_mem(void)
  */
 static unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)
 {
-	const struct e820_entry *entry = xen_e820_map;
+	const struct e820_entry *entry = xen_e820_array;
 	unsigned int i;
 	unsigned long done = 0;
 
-	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_array_entries; i++, entry++) {
 		unsigned long s_pfn;
 		unsigned long e_pfn;
 
@@ -457,7 +457,7 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 {
 	phys_addr_t start = 0;
 	unsigned long ret_val = 0;
-	const struct e820_entry *entry = xen_e820_map;
+	const struct e820_entry *entry = xen_e820_array;
 	int i;
 
 	/*
@@ -471,9 +471,9 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 	 * example) the DMI tables in a reserved region that begins on
 	 * a non-page boundary.
 	 */
-	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_array_entries; i++, entry++) {
 		phys_addr_t end = entry->addr + entry->size;
-		if (entry->type == E820_RAM || i == xen_e820_map_entries - 1) {
+		if (entry->type == E820_RAM || i == xen_e820_array_entries - 1) {
 			unsigned long start_pfn = PFN_DOWN(start);
 			unsigned long end_pfn = PFN_UP(end);
 
@@ -601,10 +601,10 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 
 static void __init xen_ignore_unusable(void)
 {
-	struct e820_entry *entry = xen_e820_map;
+	struct e820_entry *entry = xen_e820_array;
 	unsigned int i;
 
-	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
+	for (i = 0; i < xen_e820_array_entries; i++, entry++) {
 		if (entry->type == E820_UNUSABLE)
 			entry->type = E820_RAM;
 	}
@@ -620,9 +620,9 @@ bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
 		return false;
 
 	end = start + size;
-	entry = xen_e820_map;
+	entry = xen_e820_array;
 
-	for (mapcnt = 0; mapcnt < xen_e820_map_entries; mapcnt++) {
+	for (mapcnt = 0; mapcnt < xen_e820_array_entries; mapcnt++) {
 		if (entry->type == E820_RAM && entry->addr <= start &&
 		    (entry->addr + entry->size) >= end)
 			return false;
@@ -645,9 +645,9 @@ phys_addr_t __init xen_find_free_area(phys_addr_t size)
 {
 	unsigned mapcnt;
 	phys_addr_t addr, start;
-	struct e820_entry *entry = xen_e820_map;
+	struct e820_entry *entry = xen_e820_array;
 
-	for (mapcnt = 0; mapcnt < xen_e820_map_entries; mapcnt++, entry++) {
+	for (mapcnt = 0; mapcnt < xen_e820_array_entries; mapcnt++, entry++) {
 		if (entry->type != E820_RAM || entry->size < size)
 			continue;
 		start = entry->addr;
@@ -750,8 +750,8 @@ char * __init xen_memory_setup(void)
 	max_pfn = min(max_pfn, xen_start_info->nr_pages);
 	mem_end = PFN_PHYS(max_pfn);
 
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_map);
-	set_xen_guest_handle(memmap.buffer, xen_e820_map);
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_array);
+	set_xen_guest_handle(memmap.buffer, xen_e820_array);
 
 	op = xen_initial_domain() ?
 		XENMEM_machine_memory_map :
@@ -760,16 +760,16 @@ char * __init xen_memory_setup(void)
 	if (rc == -ENOSYS) {
 		BUG_ON(xen_initial_domain());
 		memmap.nr_entries = 1;
-		xen_e820_map[0].addr = 0ULL;
-		xen_e820_map[0].size = mem_end;
+		xen_e820_array[0].addr = 0ULL;
+		xen_e820_array[0].size = mem_end;
 		/* 8MB slack (to balance backend allocations). */
-		xen_e820_map[0].size += 8ULL << 20;
-		xen_e820_map[0].type = E820_RAM;
+		xen_e820_array[0].size += 8ULL << 20;
+		xen_e820_array[0].type = E820_RAM;
 		rc = 0;
 	}
 	BUG_ON(rc);
 	BUG_ON(memmap.nr_entries == 0);
-	xen_e820_map_entries = memmap.nr_entries;
+	xen_e820_array_entries = memmap.nr_entries;
 
 	/*
 	 * Xen won't allow a 1:1 mapping to be created to UNUSABLE
@@ -783,8 +783,8 @@ char * __init xen_memory_setup(void)
 		xen_ignore_unusable();
 
 	/* Make sure the Xen-supplied memory map is well-ordered. */
-	sanitize_e820_map(xen_e820_map, ARRAY_SIZE(xen_e820_map),
-			  &xen_e820_map_entries);
+	sanitize_e820_array(xen_e820_array, ARRAY_SIZE(xen_e820_array),
+			  &xen_e820_array_entries);
 
 	max_pages = xen_get_max_pages();
 
@@ -811,13 +811,13 @@ char * __init xen_memory_setup(void)
 	extra_pages = min3(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
 			   extra_pages, max_pages - max_pfn);
 	i = 0;
-	addr = xen_e820_map[0].addr;
-	size = xen_e820_map[0].size;
-	while (i < xen_e820_map_entries) {
+	addr = xen_e820_array[0].addr;
+	size = xen_e820_array[0].size;
+	while (i < xen_e820_array_entries) {
 		bool discard = false;
 
 		chunk_size = size;
-		type = xen_e820_map[i].type;
+		type = xen_e820_array[i].type;
 
 		if (type == E820_RAM) {
 			if (addr < mem_end) {
@@ -840,9 +840,9 @@ char * __init xen_memory_setup(void)
 		size -= chunk_size;
 		if (size == 0) {
 			i++;
-			if (i < xen_e820_map_entries) {
-				addr = xen_e820_map[i].addr;
-				size = xen_e820_map[i].size;
+			if (i < xen_e820_array_entries) {
+				addr = xen_e820_array[i].addr;
+				size = xen_e820_array[i].size;
 			}
 		}
 	}
@@ -861,7 +861,7 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
-	sanitize_e820_map(e820->map, ARRAY_SIZE(e820->map), &e820->nr_map);
+	sanitize_e820_array(e820_array->map, ARRAY_SIZE(e820_array->map), &e820_array->nr_map);
 
 	/*
 	 * Check whether the kernel itself conflicts with the target E820 map.
@@ -923,21 +923,21 @@ char * __init xen_auto_xlated_memory_setup(void)
 	int i;
 	int rc;
 
-	memmap.nr_entries = ARRAY_SIZE(xen_e820_map);
-	set_xen_guest_handle(memmap.buffer, xen_e820_map);
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_array);
+	set_xen_guest_handle(memmap.buffer, xen_e820_array);
 
 	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
 	if (rc < 0)
 		panic("No memory map (%d)\n", rc);
 
-	xen_e820_map_entries = memmap.nr_entries;
+	xen_e820_array_entries = memmap.nr_entries;
 
-	sanitize_e820_map(xen_e820_map, ARRAY_SIZE(xen_e820_map),
-			  &xen_e820_map_entries);
+	sanitize_e820_array(xen_e820_array, ARRAY_SIZE(xen_e820_array),
+			  &xen_e820_array_entries);
 
-	for (i = 0; i < xen_e820_map_entries; i++)
-		e820_add_region(xen_e820_map[i].addr, xen_e820_map[i].size,
-				xen_e820_map[i].type);
+	for (i = 0; i < xen_e820_array_entries; i++)
+		e820_add_region(xen_e820_array[i].addr, xen_e820_array[i].size,
+				xen_e820_array[i].type);
 
 	/* Remove p2m info, it is not needed. */
 	xen_start_info->mfn_list = 0;

commit 8ec67d97bff592cc5b5325d1ee3646ebd7d635fc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jan 27 12:54:38 2017 +0100

    x86/boot/e820: Rename the basic e820 data types to 'struct e820_entry' and 'struct e820_array'
    
    The 'e820entry' and 'e820map' names have various annoyances:
    
     - the missing underscore departs from the usual kernel style
       and makes the code look weird,
    
     - in the past I kept confusing the 'map' with the 'entry', because
       a 'map' is ambiguous in that regard,
    
     - it's not really clear from the 'e820map' that this is a regular
       C array.
    
    Rename them to 'struct e820_entry' and 'struct e820_array' accordingly.
    
    ( Leave the legacy UAPI header alone but do the rename in the bootparam.h
      and e820/types.h file - outside tools relying on these defines should
      either adjust their code, or should use the legacy header, or should
      create their private copies for the definitions. )
    
    No change in functionality.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index dd6ec15f91d4..7386a6e4c072 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -41,7 +41,7 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 unsigned long xen_released_pages;
 
 /* E820 map used during setting up memory. */
-static struct e820entry xen_e820_map[E820_X_MAX] __initdata;
+static struct e820_entry xen_e820_map[E820_X_MAX] __initdata;
 static u32 xen_e820_map_entries __initdata;
 
 /*
@@ -198,7 +198,7 @@ void __init xen_inv_extra_mem(void)
  */
 static unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)
 {
-	const struct e820entry *entry = xen_e820_map;
+	const struct e820_entry *entry = xen_e820_map;
 	unsigned int i;
 	unsigned long done = 0;
 
@@ -457,7 +457,7 @@ static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
 {
 	phys_addr_t start = 0;
 	unsigned long ret_val = 0;
-	const struct e820entry *entry = xen_e820_map;
+	const struct e820_entry *entry = xen_e820_map;
 	int i;
 
 	/*
@@ -601,7 +601,7 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 
 static void __init xen_ignore_unusable(void)
 {
-	struct e820entry *entry = xen_e820_map;
+	struct e820_entry *entry = xen_e820_map;
 	unsigned int i;
 
 	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
@@ -612,7 +612,7 @@ static void __init xen_ignore_unusable(void)
 
 bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
 {
-	struct e820entry *entry;
+	struct e820_entry *entry;
 	unsigned mapcnt;
 	phys_addr_t end;
 
@@ -645,7 +645,7 @@ phys_addr_t __init xen_find_free_area(phys_addr_t size)
 {
 	unsigned mapcnt;
 	phys_addr_t addr, start;
-	struct e820entry *entry = xen_e820_map;
+	struct e820_entry *entry = xen_e820_map;
 
 	for (mapcnt = 0; mapcnt < xen_e820_map_entries; mapcnt++, entry++) {
 		if (entry->type != E820_RAM || entry->size < size)

commit 66441bd3cfdcc03816b7009a296c284d70f629e1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jan 27 10:27:10 2017 +0100

    x86/boot/e820: Move asm/e820.h to asm/e820/api.h
    
    In line with asm/e820/types.h, move the e820 API declarations to
    asm/e820/api.h and update all usage sites.
    
    This is just a mechanical, obviously correct move & replace patch,
    there will be subsequent changes to clean up the code and to make
    better use of the new header organization.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f3f7b41116f7..dd6ec15f91d4 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -14,7 +14,7 @@
 
 #include <asm/elf.h>
 #include <asm/vdso.h>
-#include <asm/e820.h>
+#include <asm/e820/api.h>
 #include <asm/setup.h>
 #include <asm/acpi.h>
 #include <asm/numa.h>

commit 7ecec8503af37de6be4f96b53828d640a968705f
Author: Ross Lagerwall <ross.lagerwall@citrix.com>
Date:   Mon Dec 12 14:35:13 2016 +0000

    xen/setup: Don't relocate p2m over existing one
    
    When relocating the p2m, take special care not to relocate it so
    that is overlaps with the current location of the p2m/initrd. This is
    needed since the full extent of the current location is not marked as a
    reserved region in the e820.
    
    This was seen to happen to a dom0 with a large initial p2m and a small
    reserved region in the middle of the initial p2m.
    
    Signed-off-by: Ross Lagerwall <ross.lagerwall@citrix.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 8c394e30e5fe..f3f7b41116f7 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -713,10 +713,9 @@ static void __init xen_reserve_xen_mfnlist(void)
 		size = PFN_PHYS(xen_start_info->nr_p2m_frames);
 	}
 
-	if (!xen_is_e820_reserved(start, size)) {
-		memblock_reserve(start, size);
+	memblock_reserve(start, size);
+	if (!xen_is_e820_reserved(start, size))
 		return;
-	}
 
 #ifdef CONFIG_X86_32
 	/*
@@ -727,6 +726,7 @@ static void __init xen_reserve_xen_mfnlist(void)
 	BUG();
 #else
 	xen_relocate_p2m();
+	memblock_free(start, size);
 #endif
 }
 

commit 738662c35c491fc360bb6adcb8a0db88d87b5d88
Author: Alex Thorlton <athorlton@sgi.com>
Date:   Mon Dec 5 11:49:14 2016 -0600

    xen/x86: Increase xen_e820_map to E820_X_MAX possible entries
    
    On systems with sufficiently large e820 tables, and several IOAPICs, it
    is possible for the XENMEM_machine_memory_map callback (and its
    counterpart, XENMEM_memory_map) to attempt to return an e820 table with
    more than 128 entries.  This callback adds entries to the BIOS-provided
    e820 table to account for IOAPIC registers, which, on sufficiently large
    systems, can result in an e820 table that is too large to copy back into
    xen_e820_map.
    
    This change simply increases the size of xen_e820_map to E820_X_MAX to
    ensure that there is enough room to store the entire e820 map returned
    from this callback.
    
    Signed-off-by: Alex Thorlton <athorlton@sgi.com>
    Suggested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f8960fca0827..8c394e30e5fe 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -41,7 +41,7 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 unsigned long xen_released_pages;
 
 /* E820 map used during setting up memory. */
-static struct e820entry xen_e820_map[E820MAX] __initdata;
+static struct e820entry xen_e820_map[E820_X_MAX] __initdata;
 static u32 xen_e820_map_entries __initdata;
 
 /*
@@ -750,7 +750,7 @@ char * __init xen_memory_setup(void)
 	max_pfn = min(max_pfn, xen_start_info->nr_pages);
 	mem_end = PFN_PHYS(max_pfn);
 
-	memmap.nr_entries = E820MAX;
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_map);
 	set_xen_guest_handle(memmap.buffer, xen_e820_map);
 
 	op = xen_initial_domain() ?
@@ -923,7 +923,7 @@ char * __init xen_auto_xlated_memory_setup(void)
 	int i;
 	int rc;
 
-	memmap.nr_entries = E820MAX;
+	memmap.nr_entries = ARRAY_SIZE(xen_e820_map);
 	set_xen_guest_handle(memmap.buffer, xen_e820_map);
 
 	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);

commit 475339684ef19e46f4702e2d185a869a5c454688
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Sat Sep 17 23:39:26 2016 +0200

    x86/e820: Prepare e280 code for switch to dynamic storage
    
    This patch turns e820 and e820_saved into pointers to e820 tables,
    of the same size as before.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160917213927.1787-2-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 176425233e4d..f8960fca0827 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -861,7 +861,7 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
-	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
+	sanitize_e820_map(e820->map, ARRAY_SIZE(e820->map), &e820->nr_map);
 
 	/*
 	 * Check whether the kernel itself conflicts with the target E820 map.

commit 7a2463dcacee3f2f36c78418c201756372eeea6b
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:59 2016 -0400

    x86/xen: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20160714001901.31603-7-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index e345891450c3..176425233e4d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -4,7 +4,7 @@
  * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007
  */
 
-#include <linux/module.h>
+#include <linux/init.h>
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/pm.h>

commit dd14be92fbf5bc1ef7343f34968440e44e21b46a
Author: Juergen Gross <jgross@suse.com>
Date:   Wed May 18 16:44:54 2016 +0200

    xen: use same main loop for counting and remapping pages
    
    Instead of having two functions for cycling through the E820 map in
    order to count to be remapped pages and remap them later, just use one
    function with a caller supplied sub-function called for each region to
    be processed. This eliminates the possibility of a mismatch between
    both loops which showed up in certain configurations.
    
    Suggested-by: Ed Swierk <eswierk@skyportsystems.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 7ab29518a3b9..e345891450c3 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -393,6 +393,9 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 	unsigned long i = 0;
 	unsigned long n = end_pfn - start_pfn;
 
+	if (remap_pfn == 0)
+		remap_pfn = nr_pages;
+
 	while (i < n) {
 		unsigned long cur_pfn = start_pfn + i;
 		unsigned long left = n - i;
@@ -438,17 +441,29 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 	return remap_pfn;
 }
 
-static void __init xen_set_identity_and_remap(unsigned long nr_pages)
+static unsigned long __init xen_count_remap_pages(
+	unsigned long start_pfn, unsigned long end_pfn, unsigned long nr_pages,
+	unsigned long remap_pages)
+{
+	if (start_pfn >= nr_pages)
+		return remap_pages;
+
+	return remap_pages + min(end_pfn, nr_pages) - start_pfn;
+}
+
+static unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,
+	unsigned long (*func)(unsigned long start_pfn, unsigned long end_pfn,
+			      unsigned long nr_pages, unsigned long last_val))
 {
 	phys_addr_t start = 0;
-	unsigned long last_pfn = nr_pages;
+	unsigned long ret_val = 0;
 	const struct e820entry *entry = xen_e820_map;
 	int i;
 
 	/*
 	 * Combine non-RAM regions and gaps until a RAM region (or the
-	 * end of the map) is reached, then set the 1:1 map and
-	 * remap the memory in those non-RAM regions.
+	 * end of the map) is reached, then call the provided function
+	 * to perform its duty on the non-RAM region.
 	 *
 	 * The combined non-RAM regions are rounded to a whole number
 	 * of pages so any partial pages are accessible via the 1:1
@@ -466,14 +481,13 @@ static void __init xen_set_identity_and_remap(unsigned long nr_pages)
 				end_pfn = PFN_UP(entry->addr);
 
 			if (start_pfn < end_pfn)
-				last_pfn = xen_set_identity_and_remap_chunk(
-						start_pfn, end_pfn, nr_pages,
-						last_pfn);
+				ret_val = func(start_pfn, end_pfn, nr_pages,
+					       ret_val);
 			start = end;
 		}
 	}
 
-	pr_info("Released %ld page(s)\n", xen_released_pages);
+	return ret_val;
 }
 
 /*
@@ -596,35 +610,6 @@ static void __init xen_ignore_unusable(void)
 	}
 }
 
-static unsigned long __init xen_count_remap_pages(unsigned long max_pfn)
-{
-	unsigned long extra = 0;
-	unsigned long start_pfn, end_pfn;
-	const struct e820entry *entry = xen_e820_map;
-	int i;
-
-	end_pfn = 0;
-	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
-		start_pfn = PFN_DOWN(entry->addr);
-		/* Adjacent regions on non-page boundaries handling! */
-		end_pfn = min(end_pfn, start_pfn);
-
-		if (start_pfn >= max_pfn)
-			return extra + max_pfn - end_pfn;
-
-		/* Add any holes in map to result. */
-		extra += start_pfn - end_pfn;
-
-		end_pfn = PFN_UP(entry->addr + entry->size);
-		end_pfn = min(end_pfn, max_pfn);
-
-		if (entry->type != E820_RAM)
-			extra += end_pfn - start_pfn;
-	}
-
-	return extra;
-}
-
 bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
 {
 	struct e820entry *entry;
@@ -804,7 +789,7 @@ char * __init xen_memory_setup(void)
 	max_pages = xen_get_max_pages();
 
 	/* How many extra pages do we need due to remapping? */
-	max_pages += xen_count_remap_pages(max_pfn);
+	max_pages += xen_foreach_remap_area(max_pfn, xen_count_remap_pages);
 
 	if (max_pages > max_pfn)
 		extra_pages += max_pages - max_pfn;
@@ -922,7 +907,9 @@ char * __init xen_memory_setup(void)
 	 * Set identity map on non-RAM pages and prepare remapping the
 	 * underlying RAM.
 	 */
-	xen_set_identity_and_remap(max_pfn);
+	xen_foreach_remap_area(max_pfn, xen_set_identity_and_remap_chunk);
+
+	pr_info("Released %ld page(s)\n", xen_released_pages);
 
 	return "Xen";
 }

commit 41ecf1404b34d9975eb97f5005d9e4274eaeb76a
Merge: 2dc10ad81fc0 abed7d0710e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 17:32:42 2015 -0800

    Merge tag 'for-linus-4.4-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
    
     - Improve balloon driver memory hotplug placement.
    
     - Use unpopulated hotplugged memory for foreign pages (if
       supported/enabled).
    
     - Support 64 KiB guest pages on arm64.
    
     - CPU hotplug support on arm/arm64.
    
    * tag 'for-linus-4.4-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (44 commits)
      xen: fix the check of e_pfn in xen_find_pfn_range
      x86/xen: add reschedule point when mapping foreign GFNs
      xen/arm: don't try to re-register vcpu_info on cpu_hotplug.
      xen, cpu_hotplug: call device_offline instead of cpu_down
      xen/arm: Enable cpu_hotplug.c
      xenbus: Support multiple grants ring with 64KB
      xen/grant-table: Add an helper to iterate over a specific number of grants
      xen/xenbus: Rename *RING_PAGE* to *RING_GRANT*
      xen/arm: correct comment in enlighten.c
      xen/gntdev: use types from linux/types.h in userspace headers
      xen/gntalloc: use types from linux/types.h in userspace headers
      xen/balloon: Use the correct sizeof when declaring frame_list
      xen/swiotlb: Add support for 64KB page granularity
      xen/swiotlb: Pass addresses rather than frame numbers to xen_arch_need_swiotlb
      arm/xen: Add support for 64KB page granularity
      xen/privcmd: Add support for Linux 64KB page granularity
      net/xen-netback: Make it running on 64KB page granularity
      net/xen-netfront: Make it running on 64KB page granularity
      block/xen-blkback: Make it running on 64KB page granularity
      block/xen-blkfront: Make it running on 64KB page granularity
      ...

commit abed7d0710e8f892c267932a9492ccf447674fb8
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Tue Oct 27 15:19:52 2015 -0400

    xen: fix the check of e_pfn in xen_find_pfn_range
    
    On some NUMA system, after dom0 up, we see below warning even if there are
    enough pfn ranges that could be used for remapping:
    "Unable to find available pfn range, not remapping identity pages"
    
    Fix it to avoid getting a memory region of zero size in xen_find_pfn_range.
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 387b60d9bd0e..d1fac0e53d57 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -212,7 +212,7 @@ static unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)
 		e_pfn = PFN_DOWN(entry->addr + entry->size);
 
 		/* We only care about E820 after this */
-		if (e_pfn < *min_pfn)
+		if (e_pfn <= *min_pfn)
 			continue;
 
 		s_pfn = PFN_UP(entry->addr);

commit f5775e0b6116b7e2425ccf535243b21768566d87
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jan 19 11:08:05 2015 +0000

    x86/xen: discard RAM regions above the maximum reservation
    
    During setup, discard RAM regions that are above the maximum
    reservation (instead of marking them as E820_UNUSABLE).  This allows
    hotplug memory to be placed at these addresses.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Daniel Kiper <daniel.kiper@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 1c30e4ab1022..387b60d9bd0e 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -829,6 +829,8 @@ char * __init xen_memory_setup(void)
 	addr = xen_e820_map[0].addr;
 	size = xen_e820_map[0].size;
 	while (i < xen_e820_map_entries) {
+		bool discard = false;
+
 		chunk_size = size;
 		type = xen_e820_map[i].type;
 
@@ -843,10 +845,11 @@ char * __init xen_memory_setup(void)
 				xen_add_extra_mem(pfn_s, n_pfns);
 				xen_max_p2m_pfn = pfn_s + n_pfns;
 			} else
-				type = E820_UNUSABLE;
+				discard = true;
 		}
 
-		xen_align_and_add_e820_region(addr, chunk_size, type);
+		if (!discard)
+			xen_align_and_add_e820_region(addr, chunk_size, type);
 
 		addr += chunk_size;
 		size -= chunk_size;

commit 0a6d1fa0d2b48fbae444e46e7f37a4832b2f8bdf
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Oct 5 17:47:56 2015 -0700

    x86/vdso: Remove runtime 32-bit vDSO selection
    
    32-bit userspace will now always see the same vDSO, which is
    exactly what used to be the int80 vDSO.  Subsequent patches will
    clean it up and make it support SYSENTER and SYSCALL using
    alternatives.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/e7e6b3526fa442502e6125fe69486aab50813c32.1444091584.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 1c30e4ab1022..63320b6d35bc 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -965,17 +965,8 @@ char * __init xen_auto_xlated_memory_setup(void)
 static void __init fiddle_vdso(void)
 {
 #ifdef CONFIG_X86_32
-	/*
-	 * This could be called before selected_vdso32 is initialized, so
-	 * just fiddle with both possible images.  vdso_image_32_syscall
-	 * can't be selected, since it only exists on 64-bit systems.
-	 */
-	u32 *mask;
-	mask = vdso_image_32_int80.data +
-		vdso_image_32_int80.sym_VDSO32_NOTE_MASK;
-	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
-	mask = vdso_image_32_sysenter.data +
-		vdso_image_32_sysenter.sym_VDSO32_NOTE_MASK;
+	u32 *mask = vdso_image_32.data +
+		vdso_image_32.sym_VDSO32_NOTE_MASK;
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
 #endif
 }

commit 64c98e7f49100b637cd20a6c63508caed6bbba7a
Author: Malcolm Crossley <malcolm.crossley@citrix.com>
Date:   Mon Sep 28 11:36:52 2015 +0100

    x86/xen: Do not clip xen_e820_map to xen_e820_map_entries when sanitizing map
    
    Sanitizing the e820 map may produce extra E820 entries which would result in
    the topmost E820 entries being removed. The removed entries would typically
    include the top E820 usable RAM region and thus result in the domain having
    signicantly less RAM available to it.
    
    Fix by allowing sanitize_e820_map to use the full size of the allocated E820
    array.
    
    Signed-off-by: Malcolm Crossley <malcolm.crossley@citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 4ebfcecc2a8b..1c30e4ab1022 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -798,7 +798,7 @@ char * __init xen_memory_setup(void)
 		xen_ignore_unusable();
 
 	/* Make sure the Xen-supplied memory map is well-ordered. */
-	sanitize_e820_map(xen_e820_map, xen_e820_map_entries,
+	sanitize_e820_map(xen_e820_map, ARRAY_SIZE(xen_e820_map),
 			  &xen_e820_map_entries);
 
 	max_pages = xen_get_max_pages();

commit 24f775a6605a8ffc697c0767fc7ea85656ddb958
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Sep 4 14:50:33 2015 +0200

    xen: use correct type for HYPERVISOR_memory_op()
    
    HYPERVISOR_memory_op() is defined to return an "int" value. This is
    wrong, as the Xen hypervisor will return "long".
    
    The sub-function XENMEM_maximum_reservation returns the maximum
    number of pages for the current domain. An int will overflow for a
    domain configured with 8TB of memory or more.
    
    Correct this by using the correct type.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f5ef6746d47a..4ebfcecc2a8b 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -548,7 +548,7 @@ static unsigned long __init xen_get_max_pages(void)
 {
 	unsigned long max_pages, limit;
 	domid_t domid = DOMID_SELF;
-	int ret;
+	long ret;
 
 	limit = xen_get_pages_limit();
 	max_pages = limit;

commit 626d7508664c4bc8e67f496da4387ecd0c410b8c
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Sep 4 14:05:51 2015 +0200

    xen: switch extra memory accounting to use pfns
    
    Instead of using physical addresses for accounting of extra memory
    areas available for ballooning switch to pfns as this is much less
    error prone regarding partial pages.
    
    Reported-by: Roger Pau Monné <roger.pau@citrix.com>
    Tested-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 70de4c8b8f27..f5ef6746d47a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -90,62 +90,69 @@ static void __init xen_parse_512gb(void)
 	xen_512gb_limit = val;
 }
 
-static void __init xen_add_extra_mem(phys_addr_t start, phys_addr_t size)
+static void __init xen_add_extra_mem(unsigned long start_pfn,
+				     unsigned long n_pfns)
 {
 	int i;
 
+	/*
+	 * No need to check for zero size, should happen rarely and will only
+	 * write a new entry regarded to be unused due to zero size.
+	 */
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
 		/* Add new region. */
-		if (xen_extra_mem[i].size == 0) {
-			xen_extra_mem[i].start = start;
-			xen_extra_mem[i].size  = size;
+		if (xen_extra_mem[i].n_pfns == 0) {
+			xen_extra_mem[i].start_pfn = start_pfn;
+			xen_extra_mem[i].n_pfns = n_pfns;
 			break;
 		}
 		/* Append to existing region. */
-		if (xen_extra_mem[i].start + xen_extra_mem[i].size == start) {
-			xen_extra_mem[i].size += size;
+		if (xen_extra_mem[i].start_pfn + xen_extra_mem[i].n_pfns ==
+		    start_pfn) {
+			xen_extra_mem[i].n_pfns += n_pfns;
 			break;
 		}
 	}
 	if (i == XEN_EXTRA_MEM_MAX_REGIONS)
 		printk(KERN_WARNING "Warning: not enough extra memory regions\n");
 
-	memblock_reserve(start, size);
+	memblock_reserve(PFN_PHYS(start_pfn), PFN_PHYS(n_pfns));
 }
 
-static void __init xen_del_extra_mem(phys_addr_t start, phys_addr_t size)
+static void __init xen_del_extra_mem(unsigned long start_pfn,
+				     unsigned long n_pfns)
 {
 	int i;
-	phys_addr_t start_r, size_r;
+	unsigned long start_r, size_r;
 
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
-		start_r = xen_extra_mem[i].start;
-		size_r = xen_extra_mem[i].size;
+		start_r = xen_extra_mem[i].start_pfn;
+		size_r = xen_extra_mem[i].n_pfns;
 
 		/* Start of region. */
-		if (start_r == start) {
-			BUG_ON(size > size_r);
-			xen_extra_mem[i].start += size;
-			xen_extra_mem[i].size -= size;
+		if (start_r == start_pfn) {
+			BUG_ON(n_pfns > size_r);
+			xen_extra_mem[i].start_pfn += n_pfns;
+			xen_extra_mem[i].n_pfns -= n_pfns;
 			break;
 		}
 		/* End of region. */
-		if (start_r + size_r == start + size) {
-			BUG_ON(size > size_r);
-			xen_extra_mem[i].size -= size;
+		if (start_r + size_r == start_pfn + n_pfns) {
+			BUG_ON(n_pfns > size_r);
+			xen_extra_mem[i].n_pfns -= n_pfns;
 			break;
 		}
 		/* Mid of region. */
-		if (start > start_r && start < start_r + size_r) {
-			BUG_ON(start + size > start_r + size_r);
-			xen_extra_mem[i].size = start - start_r;
+		if (start_pfn > start_r && start_pfn < start_r + size_r) {
+			BUG_ON(start_pfn + n_pfns > start_r + size_r);
+			xen_extra_mem[i].n_pfns = start_pfn - start_r;
 			/* Calling memblock_reserve() again is okay. */
-			xen_add_extra_mem(start + size, start_r + size_r -
-					  (start + size));
+			xen_add_extra_mem(start_pfn + n_pfns, start_r + size_r -
+					  (start_pfn + n_pfns));
 			break;
 		}
 	}
-	memblock_free(start, size);
+	memblock_free(PFN_PHYS(start_pfn), PFN_PHYS(n_pfns));
 }
 
 /*
@@ -156,11 +163,10 @@ static void __init xen_del_extra_mem(phys_addr_t start, phys_addr_t size)
 unsigned long __ref xen_chk_extra_mem(unsigned long pfn)
 {
 	int i;
-	phys_addr_t addr = PFN_PHYS(pfn);
 
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
-		if (addr >= xen_extra_mem[i].start &&
-		    addr < xen_extra_mem[i].start + xen_extra_mem[i].size)
+		if (pfn >= xen_extra_mem[i].start_pfn &&
+		    pfn < xen_extra_mem[i].start_pfn + xen_extra_mem[i].n_pfns)
 			return INVALID_P2M_ENTRY;
 	}
 
@@ -176,10 +182,10 @@ void __init xen_inv_extra_mem(void)
 	int i;
 
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
-		if (!xen_extra_mem[i].size)
+		if (!xen_extra_mem[i].n_pfns)
 			continue;
-		pfn_s = PFN_DOWN(xen_extra_mem[i].start);
-		pfn_e = PFN_UP(xen_extra_mem[i].start + xen_extra_mem[i].size);
+		pfn_s = xen_extra_mem[i].start_pfn;
+		pfn_e = pfn_s + xen_extra_mem[i].n_pfns;
 		for (pfn = pfn_s; pfn < pfn_e; pfn++)
 			set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 	}
@@ -507,7 +513,7 @@ void __init xen_remap_memory(void)
 		} else if (pfn_s + len == xen_remap_buf.target_pfn) {
 			len += xen_remap_buf.size;
 		} else {
-			xen_del_extra_mem(PFN_PHYS(pfn_s), PFN_PHYS(len));
+			xen_del_extra_mem(pfn_s, len);
 			pfn_s = xen_remap_buf.target_pfn;
 			len = xen_remap_buf.size;
 		}
@@ -517,7 +523,7 @@ void __init xen_remap_memory(void)
 	}
 
 	if (pfn_s != ~0UL && len)
-		xen_del_extra_mem(PFN_PHYS(pfn_s), PFN_PHYS(len));
+		xen_del_extra_mem(pfn_s, len);
 
 	set_pte_mfn(buf, mfn_save, PAGE_KERNEL);
 
@@ -744,7 +750,7 @@ static void __init xen_reserve_xen_mfnlist(void)
  **/
 char * __init xen_memory_setup(void)
 {
-	unsigned long max_pfn;
+	unsigned long max_pfn, pfn_s, n_pfns;
 	phys_addr_t mem_end, addr, size, chunk_size;
 	u32 type;
 	int rc;
@@ -831,9 +837,11 @@ char * __init xen_memory_setup(void)
 				chunk_size = min(size, mem_end - addr);
 			} else if (extra_pages) {
 				chunk_size = min(size, PFN_PHYS(extra_pages));
-				extra_pages -= PFN_DOWN(chunk_size);
-				xen_add_extra_mem(addr, chunk_size);
-				xen_max_p2m_pfn = PFN_DOWN(addr + chunk_size);
+				pfn_s = PFN_UP(addr);
+				n_pfns = PFN_DOWN(addr + chunk_size) - pfn_s;
+				extra_pages -= n_pfns;
+				xen_add_extra_mem(pfn_s, n_pfns);
+				xen_max_p2m_pfn = pfn_s + n_pfns;
 			} else
 				type = E820_UNUSABLE;
 		}

commit cb9e444b5aaa900bb4310da411315b6947c53e37
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Sep 4 14:18:08 2015 +0200

    xen: limit memory to architectural maximum
    
    When a pv-domain (including dom0) is started it tries to size it's
    p2m list according to the maximum possible memory amount it ever can
    achieve. Limit the initial maximum memory size to the architectural
    limit of the hardware in order to avoid overflows during remapping
    of memory.
    
    This problem will occur when dom0 is started with an initial memory
    size being a multiple of 1GB, but without specifying it's maximum
    memory size. The kernel must be configured without
    CONFIG_XEN_BALLOON_MEMORY_HOTPLUG for the problem to happen.
    
    Reported-by: Roger Pau Monné <roger.pau@citrix.com>
    Tested-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 7a5d5666677f..70de4c8b8f27 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -531,7 +531,7 @@ static unsigned long __init xen_get_pages_limit(void)
 #ifdef CONFIG_X86_32
 	limit = GB(64) / PAGE_SIZE;
 #else
-	limit = ~0ul;
+	limit = MAXMEM / PAGE_SIZE;
 	if (!xen_initial_domain() && xen_512gb_limit)
 		limit = GB(512) / PAGE_SIZE;
 #endif

commit ab24507cfae8d916814bb6c16f66e453184a29a5
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Aug 19 18:53:11 2015 +0200

    xen: avoid another early crash of memory limited dom0
    
    Commit b1c9f169047b ("xen: split counting of extra memory pages...")
    introduced an error when dom0 was started with limited memory occurring
    only on some hardware.
    
    The problem arises in case dom0 is started with initial memory and
    maximum memory being the same. The kernel must be configured without
    CONFIG_XEN_BALLOON_MEMORY_HOTPLUG for the problem to happen. If all
    of this is true and the E820 map of the machine is sparse (some areas
    are not covered) then the machine might crash early in the boot
    process.
    
    An example E820 map triggering the problem looks like this:
    
    [    0.000000] e820: BIOS-provided physical RAM map:
    [    0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009d7ff] usable
    [    0.000000] BIOS-e820: [mem 0x000000000009d800-0x000000000009ffff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000000e0000-0x00000000000fffff] reserved
    [    0.000000] BIOS-e820: [mem 0x0000000000100000-0x00000000cf7fafff] usable
    [    0.000000] BIOS-e820: [mem 0x00000000cf7fb000-0x00000000cf95ffff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000cf960000-0x00000000cfb62fff] ACPI NVS
    [    0.000000] BIOS-e820: [mem 0x00000000cfb63000-0x00000000cfd14fff] usable
    [    0.000000] BIOS-e820: [mem 0x00000000cfd15000-0x00000000cfd61fff] ACPI NVS
    [    0.000000] BIOS-e820: [mem 0x00000000cfd62000-0x00000000cfd6cfff] ACPI data
    [    0.000000] BIOS-e820: [mem 0x00000000cfd6d000-0x00000000cfd6ffff] ACPI NVS
    [    0.000000] BIOS-e820: [mem 0x00000000cfd70000-0x00000000cfd70fff] usable
    [    0.000000] BIOS-e820: [mem 0x00000000cfd71000-0x00000000cfea8fff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000cfea9000-0x00000000cfeb9fff] ACPI NVS
    [    0.000000] BIOS-e820: [mem 0x00000000cfeba000-0x00000000cfecafff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000cfecb000-0x00000000cfecbfff] ACPI NVS
    [    0.000000] BIOS-e820: [mem 0x00000000cfecc000-0x00000000cfedbfff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000cfedc000-0x00000000cfedcfff] ACPI NVS
    [    0.000000] BIOS-e820: [mem 0x00000000cfedd000-0x00000000cfeddfff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000cfede000-0x00000000cfee3fff] ACPI NVS
    [    0.000000] BIOS-e820: [mem 0x00000000cfee4000-0x00000000cfef6fff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000cfef7000-0x00000000cfefffff] usable
    [    0.000000] BIOS-e820: [mem 0x00000000e0000000-0x00000000efffffff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000fec00000-0x00000000fec00fff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000fec10000-0x00000000fec10fff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000fed00000-0x00000000fed00fff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000fed40000-0x00000000fed44fff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000fed61000-0x00000000fed70fff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000fed80000-0x00000000fed8ffff] reserved
    [    0.000000] BIOS-e820: [mem 0x00000000ff000000-0x00000000ffffffff] reserved
    [    0.000000] BIOS-e820: [mem 0x0000000100001000-0x000000020effffff] usable
    
    In this case the area a0000-dffff isn't present in the map. This will
    confuse the memory setup of the domain when remapping the memory from
    such holes to populated areas.
    
    To avoid the problem the accounting of to be remapped memory has to
    count such holes in the E820 map as well.
    
    Reported-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ead0d363bfba..7a5d5666677f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -593,20 +593,27 @@ static void __init xen_ignore_unusable(void)
 static unsigned long __init xen_count_remap_pages(unsigned long max_pfn)
 {
 	unsigned long extra = 0;
+	unsigned long start_pfn, end_pfn;
 	const struct e820entry *entry = xen_e820_map;
 	int i;
 
+	end_pfn = 0;
 	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
-		unsigned long start_pfn = PFN_DOWN(entry->addr);
-		unsigned long end_pfn = PFN_UP(entry->addr + entry->size);
+		start_pfn = PFN_DOWN(entry->addr);
+		/* Adjacent regions on non-page boundaries handling! */
+		end_pfn = min(end_pfn, start_pfn);
 
 		if (start_pfn >= max_pfn)
-			break;
-		if (entry->type == E820_RAM)
-			continue;
-		if (end_pfn >= max_pfn)
-			end_pfn = max_pfn;
-		extra += end_pfn - start_pfn;
+			return extra + max_pfn - end_pfn;
+
+		/* Add any holes in map to result. */
+		extra += start_pfn - end_pfn;
+
+		end_pfn = PFN_UP(entry->addr + entry->size);
+		end_pfn = min(end_pfn, max_pfn);
+
+		if (entry->type != E820_RAM)
+			extra += end_pfn - start_pfn;
 	}
 
 	return extra;

commit eafd72e016c69df511b14a98b61e439c58ad9c51
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Aug 19 18:52:34 2015 +0200

    xen: avoid early crash of memory limited dom0
    
    Commit b1c9f169047b ("xen: split counting of extra memory pages...")
    introduced an error when dom0 was started with limited memory.
    
    The problem arises in case dom0 is started with initial memory and
    maximum memory being the same and exactly a multiple of 1 GB. The
    kernel must be configured without CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
    for the problem to happen. In this case it will crash very early
    during boot due to the virtual mapped p2m list not being large
    enough to be able to remap any memory:
    
    (XEN) Freed 304kB init memory.
    mapping kernel into physical memory
    about to get started...
    (XEN) traps.c:459:d0v0 Unhandled invalid opcode fault/trap [#6] on VCPU 0 [ec=0000]
    (XEN) domain_crash_sync called from entry.S: fault at ffff82d080229a93 create_bounce_frame+0x12b/0x13a
    (XEN) Domain 0 (vcpu#0) crashed on cpu#0:
    (XEN) ----[ Xen-4.5.2-pre  x86_64  debug=n Not tainted ]----
    (XEN) CPU:    0
    (XEN) RIP:    e033:[<ffffffff81d120cb>]
    (XEN) RFLAGS: 0000000000000206   EM: 1 CONTEXT: pv guest (d0v0)
    (XEN) rax: ffffffff81db2000   rbx: 000000004d000000   rcx: 0000000000000000
    (XEN) rdx: 000000004d000000   rsi: 0000000000063000   rdi: 000000004d063000
    (XEN) rbp: ffffffff81c03d78   rsp: ffffffff81c03d28   r8:  0000000000023000
    (XEN) r9:  00000001040ff000   r10: 0000000000007ff0   r11: 0000000000000000
    (XEN) r12: 0000000000063000   r13: 000000000004d000   r14: 0000000000000063
    (XEN) r15: 0000000000000063   cr0: 0000000080050033   cr4: 00000000000006f0
    (XEN) cr3: 0000000105c0f000   cr2: ffffc90000268000
    (XEN) ds: 0000   es: 0000   fs: 0000   gs: 0000   ss: e02b   cs: e033
    (XEN) Guest stack trace from rsp=ffffffff81c03d28:
    (XEN)   0000000000000000 0000000000000000 ffffffff81d120cb 000000010000e030
    (XEN)   0000000000010006 ffffffff81c03d68 000000000000e02b ffffffffffffffff
    (XEN)   0000000000000063 000000000004d063 ffffffff81c03de8 ffffffff81d130a7
    (XEN)   ffffffff81c03de8 000000000004d000 00000001040ff000 0000000000105db1
    (XEN)   00000001040ff001 000000000004d062 ffff8800092d6ff8 0000000002027000
    (XEN)   ffff8800094d8340 ffff8800092d6ff8 00003ffffffff000 ffff8800092d7ff8
    (XEN)   ffffffff81c03e48 ffffffff81d13c43 ffff8800094d8000 ffff8800094d9000
    (XEN)   0000000000000000 ffff8800092d6000 00000000092d6000 000000004cfbf000
    (XEN)   00000000092d6000 00000000052d5442 0000000000000000 0000000000000000
    (XEN)   ffffffff81c03ed8 ffffffff81d185c1 0000000000000000 0000000000000000
    (XEN)   ffffffff81c03e78 ffffffff810f8ca4 ffffffff81c03ed8 ffffffff8171a15d
    (XEN)   0000000000000010 ffffffff81c03ee8 0000000000000000 0000000000000000
    (XEN)   ffffffff81f0e402 ffffffffffffffff ffffffff81dae900 0000000000000000
    (XEN)   0000000000000000 0000000000000000 ffffffff81c03f28 ffffffff81d0cf0f
    (XEN)   0000000000000000 0000000000000000 0000000000000000 ffffffff81db82e0
    (XEN)   0000000000000000 0000000000000000 0000000000000000 0000000000000000
    (XEN)   ffffffff81c03f38 ffffffff81d0c603 ffffffff81c03ff8 ffffffff81d11c86
    (XEN)   0300000100000032 0000000000000005 0000000000000020 0000000000000000
    (XEN)   0000000000000000 0000000000000000 0000000000000000 0000000000000000
    (XEN)   0000000000000000 0000000000000000 0000000000000000 0000000000000000
    (XEN) Domain 0 crashed: rebooting machine in 5 seconds.
    
    This can be avoided by allocating aneough space for the p2m to cover
    the maximum memory of dom0 plus the identity mapped holes required
    for PCI space, BIOS etc.
    
    Reported-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index a1a77eabe858..ead0d363bfba 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -789,11 +789,12 @@ char * __init xen_memory_setup(void)
 			  &xen_e820_map_entries);
 
 	max_pages = xen_get_max_pages();
-	if (max_pages > max_pfn)
-		extra_pages += max_pages - max_pfn;
 
 	/* How many extra pages do we need due to remapping? */
-	extra_pages += xen_count_remap_pages(max_pfn);
+	max_pages += xen_count_remap_pages(max_pfn);
+
+	if (max_pages > max_pfn)
+		extra_pages += max_pages - max_pfn;
 
 	/*
 	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO

commit cb3eb850137cd43fc3e25d2062525f5ba5fd884a
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:37 2015 +0200

    xen: remove no longer needed p2m.h
    
    Cleanup by removing arch/x86/xen/p2m.h as it isn't needed any more.
    
    Most definitions in this file are used in p2m.c only. Move those into
    p2m.c.
    
    set_phys_range_identity() is already declared in
    arch/x86/include/asm/xen/page.h, add __init annotation there.
    
    MAX_REMAP_RANGES isn't used at all, just delete it.
    
    The only define left is P2M_PER_PAGE which is moved to page.h as well.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f96002107691..a1a77eabe858 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -30,7 +30,6 @@
 #include <xen/hvc-console.h>
 #include "xen-ops.h"
 #include "vdso.h"
-#include "p2m.h"
 #include "mmu.h"
 
 #define GB(x) ((uint64_t)(x) * 1024 * 1024 * 1024)

commit c70727a5bc18a5a233fddc6056d1de9144d7a293
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:36 2015 +0200

    xen: allow more than 512 GB of RAM for 64 bit pv-domains
    
    64 bit pv-domains under Xen are limited to 512 GB of RAM today. The
    main reason has been the 3 level p2m tree, which was replaced by the
    virtual mapped linear p2m list. Parallel to the p2m list which is
    being used by the kernel itself there is a 3 level mfn tree for usage
    by the Xen tools and eventually for crash dump analysis. For this tree
    the linear p2m list can serve as a replacement, too. As the kernel
    can't know whether the tools are capable of dealing with the p2m list
    instead of the mfn tree, the limit of 512 GB can't be dropped in all
    cases.
    
    This patch replaces the hard limit by a kernel parameter which tells
    the kernel to obey the 512 GB limit or not. The default is selected by
    a configuration parameter which specifies whether the 512 GB limit
    should be active per default for domUs (domain save/restore/migration
    and crash dump analysis are affected).
    
    Memory above the domain limit is returned to the hypervisor instead of
    being identity mapped, which was wrong anyway.
    
    The kernel configuration parameter to specify the maximum size of a
    domain can be deleted, as it is not relevant any more.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index b096d02d34ac..f96002107691 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -33,6 +33,8 @@
 #include "p2m.h"
 #include "mmu.h"
 
+#define GB(x) ((uint64_t)(x) * 1024 * 1024 * 1024)
+
 /* Amount of extra memory space we add to the e820 ranges */
 struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 
@@ -69,6 +71,26 @@ static unsigned long xen_remap_mfn __initdata = INVALID_P2M_ENTRY;
  */
 #define EXTRA_MEM_RATIO		(10)
 
+static bool xen_512gb_limit __initdata = IS_ENABLED(CONFIG_XEN_512GB);
+
+static void __init xen_parse_512gb(void)
+{
+	bool val = false;
+	char *arg;
+
+	arg = strstr(xen_start_info->cmd_line, "xen_512gb_limit");
+	if (!arg)
+		return;
+
+	arg = strstr(xen_start_info->cmd_line, "xen_512gb_limit=");
+	if (!arg)
+		val = true;
+	else if (strtobool(arg + strlen("xen_512gb_limit="), &val))
+		return;
+
+	xen_512gb_limit = val;
+}
+
 static void __init xen_add_extra_mem(phys_addr_t start, phys_addr_t size)
 {
 	int i;
@@ -503,12 +525,29 @@ void __init xen_remap_memory(void)
 	pr_info("Remapped %ld page(s)\n", remapped);
 }
 
+static unsigned long __init xen_get_pages_limit(void)
+{
+	unsigned long limit;
+
+#ifdef CONFIG_X86_32
+	limit = GB(64) / PAGE_SIZE;
+#else
+	limit = ~0ul;
+	if (!xen_initial_domain() && xen_512gb_limit)
+		limit = GB(512) / PAGE_SIZE;
+#endif
+	return limit;
+}
+
 static unsigned long __init xen_get_max_pages(void)
 {
-	unsigned long max_pages = MAX_DOMAIN_PAGES;
+	unsigned long max_pages, limit;
 	domid_t domid = DOMID_SELF;
 	int ret;
 
+	limit = xen_get_pages_limit();
+	max_pages = limit;
+
 	/*
 	 * For the initial domain we use the maximum reservation as
 	 * the maximum page.
@@ -524,7 +563,7 @@ static unsigned long __init xen_get_max_pages(void)
 			max_pages = ret;
 	}
 
-	return min(max_pages, MAX_DOMAIN_PAGES);
+	return min(max_pages, limit);
 }
 
 static void __init xen_align_and_add_e820_region(phys_addr_t start,
@@ -699,7 +738,7 @@ static void __init xen_reserve_xen_mfnlist(void)
  **/
 char * __init xen_memory_setup(void)
 {
-	unsigned long max_pfn = xen_start_info->nr_pages;
+	unsigned long max_pfn;
 	phys_addr_t mem_end, addr, size, chunk_size;
 	u32 type;
 	int rc;
@@ -709,7 +748,9 @@ char * __init xen_memory_setup(void)
 	int i;
 	int op;
 
-	max_pfn = min(MAX_DOMAIN_PAGES, max_pfn);
+	xen_parse_512gb();
+	max_pfn = xen_get_pages_limit();
+	max_pfn = min(max_pfn, xen_start_info->nr_pages);
 	mem_end = PFN_PHYS(max_pfn);
 
 	memmap.nr_entries = E820MAX;
@@ -762,12 +803,15 @@ char * __init xen_memory_setup(void)
 	 * is limited to the max size of lowmem, so that it doesn't
 	 * get completely filled.
 	 *
+	 * Make sure we have no memory above max_pages, as this area
+	 * isn't handled by the p2m management.
+	 *
 	 * In principle there could be a problem in lowmem systems if
 	 * the initial memory is also very large with respect to
 	 * lowmem, but we won't try to deal with that here.
 	 */
-	extra_pages = min(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
-			  extra_pages);
+	extra_pages = min3(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
+			   extra_pages, max_pages - max_pfn);
 	i = 0;
 	addr = xen_e820_map[0].addr;
 	size = xen_e820_map[0].size;
@@ -803,9 +847,6 @@ char * __init xen_memory_setup(void)
 	/*
 	 * Set the rest as identity mapped, in case PCI BARs are
 	 * located here.
-	 *
-	 * PFNs above MAX_P2M_PFN are considered identity mapped as
-	 * well.
 	 */
 	set_phys_range_identity(addr / PAGE_SIZE, ~0ul);
 

commit 70e61199559a09c62714694cd5ac3c3640c41552
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:35 2015 +0200

    xen: move p2m list if conflicting with e820 map
    
    Check whether the hypervisor supplied p2m list is placed at a location
    which is conflicting with the target E820 map. If this is the case
    relocate it to a new area unused up to now and compliant to the E820
    map.
    
    As the p2m list might by huge (up to several GB) and is required to be
    mapped virtually, set up a temporary mapping for the copied list.
    
    For pvh domains just delete the p2m related information from start
    info instead of reserving the p2m memory, as we don't need it at all.
    
    For 32 bit kernels adjust the memblock_reserve() parameters in order
    to cover the page tables only. This requires to memblock_reserve() the
    start_info page on it's own.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 0d7f8818f8ca..b096d02d34ac 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -663,37 +663,35 @@ static void __init xen_phys_memcpy(phys_addr_t dest, phys_addr_t src,
 
 /*
  * Reserve Xen mfn_list.
- * See comment above "struct start_info" in <xen/interface/xen.h>
- * We tried to make the the memblock_reserve more selective so
- * that it would be clear what region is reserved. Sadly we ran
- * in the problem wherein on a 64-bit hypervisor with a 32-bit
- * initial domain, the pt_base has the cr3 value which is not
- * neccessarily where the pagetable starts! As Jan put it: "
- * Actually, the adjustment turns out to be correct: The page
- * tables for a 32-on-64 dom0 get allocated in the order "first L1",
- * "first L2", "first L3", so the offset to the page table base is
- * indeed 2. When reading xen/include/public/xen.h's comment
- * very strictly, this is not a violation (since there nothing is said
- * that the first thing in the page table space is pointed to by
- * pt_base; I admit that this seems to be implied though, namely
- * do I think that it is implied that the page table space is the
- * range [pt_base, pt_base + nt_pt_frames), whereas that
- * range here indeed is [pt_base - 2, pt_base - 2 + nt_pt_frames),
- * which - without a priori knowledge - the kernel would have
- * difficulty to figure out)." - so lets just fall back to the
- * easy way and reserve the whole region.
  */
 static void __init xen_reserve_xen_mfnlist(void)
 {
+	phys_addr_t start, size;
+
 	if (xen_start_info->mfn_list >= __START_KERNEL_map) {
-		memblock_reserve(__pa(xen_start_info->mfn_list),
-				 xen_start_info->pt_base -
-				 xen_start_info->mfn_list);
+		start = __pa(xen_start_info->mfn_list);
+		size = PFN_ALIGN(xen_start_info->nr_pages *
+				 sizeof(unsigned long));
+	} else {
+		start = PFN_PHYS(xen_start_info->first_p2m_pfn);
+		size = PFN_PHYS(xen_start_info->nr_p2m_frames);
+	}
+
+	if (!xen_is_e820_reserved(start, size)) {
+		memblock_reserve(start, size);
 		return;
 	}
 
-	memblock_reserve(PFN_PHYS(xen_start_info->first_p2m_pfn),
-			 PFN_PHYS(xen_start_info->nr_p2m_frames));
+#ifdef CONFIG_X86_32
+	/*
+	 * Relocating the p2m on 32 bit system to an arbitrary virtual address
+	 * is not supported, so just give up.
+	 */
+	xen_raw_console_write("Xen hypervisor allocated p2m list conflicts with E820 map\n");
+	BUG();
+#else
+	xen_relocate_p2m();
+#endif
 }
 
 /**
@@ -895,7 +893,10 @@ char * __init xen_auto_xlated_memory_setup(void)
 		e820_add_region(xen_e820_map[i].addr, xen_e820_map[i].size,
 				xen_e820_map[i].type);
 
-	xen_reserve_xen_mfnlist();
+	/* Remove p2m info, it is not needed. */
+	xen_start_info->mfn_list = 0;
+	xen_start_info->first_p2m_pfn = 0;
+	xen_start_info->nr_p2m_frames = 0;
 
 	return "Xen";
 }

commit 4b9c15377f96e241be347fd3bbeeff74fbad0b44
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:32 2015 +0200

    xen: check for initrd conflicting with e820 map
    
    Check whether the initrd is placed at a location which is conflicting
    with the target E820 map. If this is the case relocate it to a new
    area unused up to now and compliant to the E820 map.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 3fca9c114828..0d7f8818f8ca 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -631,6 +631,36 @@ phys_addr_t __init xen_find_free_area(phys_addr_t size)
 	return 0;
 }
 
+/*
+ * Like memcpy, but with physical addresses for dest and src.
+ */
+static void __init xen_phys_memcpy(phys_addr_t dest, phys_addr_t src,
+				   phys_addr_t n)
+{
+	phys_addr_t dest_off, src_off, dest_len, src_len, len;
+	void *from, *to;
+
+	while (n) {
+		dest_off = dest & ~PAGE_MASK;
+		src_off = src & ~PAGE_MASK;
+		dest_len = n;
+		if (dest_len > (NR_FIX_BTMAPS << PAGE_SHIFT) - dest_off)
+			dest_len = (NR_FIX_BTMAPS << PAGE_SHIFT) - dest_off;
+		src_len = n;
+		if (src_len > (NR_FIX_BTMAPS << PAGE_SHIFT) - src_off)
+			src_len = (NR_FIX_BTMAPS << PAGE_SHIFT) - src_off;
+		len = min(dest_len, src_len);
+		to = early_memremap(dest - dest_off, dest_len + dest_off);
+		from = early_memremap(src - src_off, src_len + src_off);
+		memcpy(to, from, len);
+		early_memunmap(to, dest_len + dest_off);
+		early_memunmap(from, src_len + src_off);
+		n -= len;
+		dest += len;
+		src += len;
+	}
+}
+
 /*
  * Reserve Xen mfn_list.
  * See comment above "struct start_info" in <xen/interface/xen.h>
@@ -810,6 +840,27 @@ char * __init xen_memory_setup(void)
 
 	xen_reserve_xen_mfnlist();
 
+	/* Check for a conflict of the initrd with the target E820 map. */
+	if (xen_is_e820_reserved(boot_params.hdr.ramdisk_image,
+				 boot_params.hdr.ramdisk_size)) {
+		phys_addr_t new_area, start, size;
+
+		new_area = xen_find_free_area(boot_params.hdr.ramdisk_size);
+		if (!new_area) {
+			xen_raw_console_write("Can't find new memory area for initrd needed due to E820 map conflict\n");
+			BUG();
+		}
+
+		start = boot_params.hdr.ramdisk_image;
+		size = boot_params.hdr.ramdisk_size;
+		xen_phys_memcpy(new_area, start, size);
+		pr_info("initrd moved from [mem %#010llx-%#010llx] to [mem %#010llx-%#010llx]\n",
+			start, start + size, new_area, new_area + size);
+		memblock_free(start, size);
+		boot_params.hdr.ramdisk_image = new_area;
+		boot_params.ext_ramdisk_image = new_area >> 32;
+	}
+
 	/*
 	 * Set identity map on non-RAM pages and prepare remapping the
 	 * underlying RAM.

commit 04414baab5ba862b10bde837c4773ffdbb78f0e0
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:31 2015 +0200

    xen: check pre-allocated page tables for conflict with memory map
    
    Check whether the page tables built by the domain builder are at
    memory addresses which are in conflict with the target memory map.
    If this is the case just panic instead of running into problems
    later.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 9bd3f358f3d9..3fca9c114828 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -802,6 +802,12 @@ char * __init xen_memory_setup(void)
 		BUG();
 	}
 
+	/*
+	 * Check for a conflict of the hypervisor supplied page tables with
+	 * the target E820 map.
+	 */
+	xen_pt_check_e820();
+
 	xen_reserve_xen_mfnlist();
 
 	/*

commit 808fdb71936c41d46245f0e3aa6ec889cba70d97
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:30 2015 +0200

    xen: check for kernel memory conflicting with memory layout
    
    Checks whether the pre-allocated memory of the loaded kernel is in
    conflict with the target memory map. If this is the case, just panic
    instead of run into problems later, as there is nothing we can do
    to repair this situation.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 973d29441f96..9bd3f358f3d9 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -27,6 +27,7 @@
 #include <xen/interface/memory.h>
 #include <xen/interface/physdev.h>
 #include <xen/features.h>
+#include <xen/hvc-console.h>
 #include "xen-ops.h"
 #include "vdso.h"
 #include "p2m.h"
@@ -790,6 +791,17 @@ char * __init xen_memory_setup(void)
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
+	/*
+	 * Check whether the kernel itself conflicts with the target E820 map.
+	 * Failing now is better than running into weird problems later due
+	 * to relocating (and even reusing) pages with kernel text or data.
+	 */
+	if (xen_is_e820_reserved(__pa_symbol(_text),
+			__pa_symbol(__bss_stop) - __pa_symbol(_text))) {
+		xen_raw_console_write("Xen hypervisor allocated kernel memory conflicts with E820 map\n");
+		BUG();
+	}
+
 	xen_reserve_xen_mfnlist();
 
 	/*

commit 9ddac5b724a9465e27f25a0aa943e92c8341a85b
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:29 2015 +0200

    xen: find unused contiguous memory area
    
    For being able to relocate pre-allocated data areas like initrd or
    p2m list it is mandatory to find a contiguous memory area which is
    not yet in use and doesn't conflict with the memory map we want to
    be in effect.
    
    In case such an area is found reserve it at once as this will be
    required to be done in any case.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 99ef82cc4edc..973d29441f96 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -596,6 +596,40 @@ bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
 	return true;
 }
 
+/*
+ * Find a free area in physical memory not yet reserved and compliant with
+ * E820 map.
+ * Used to relocate pre-allocated areas like initrd or p2m list which are in
+ * conflict with the to be used E820 map.
+ * In case no area is found, return 0. Otherwise return the physical address
+ * of the area which is already reserved for convenience.
+ */
+phys_addr_t __init xen_find_free_area(phys_addr_t size)
+{
+	unsigned mapcnt;
+	phys_addr_t addr, start;
+	struct e820entry *entry = xen_e820_map;
+
+	for (mapcnt = 0; mapcnt < xen_e820_map_entries; mapcnt++, entry++) {
+		if (entry->type != E820_RAM || entry->size < size)
+			continue;
+		start = entry->addr;
+		for (addr = start; addr < start + size; addr += PAGE_SIZE) {
+			if (!memblock_is_reserved(addr))
+				continue;
+			start = addr + PAGE_SIZE;
+			if (start + size > entry->addr + entry->size)
+				break;
+		}
+		if (addr >= start + size) {
+			memblock_reserve(start, size);
+			return start;
+		}
+	}
+
+	return 0;
+}
+
 /*
  * Reserve Xen mfn_list.
  * See comment above "struct start_info" in <xen/interface/xen.h>

commit e612b4a7db4ae1dd8c2bbe171e10c21723de95b2
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:28 2015 +0200

    xen: check memory area against e820 map
    
    Provide a service routine to check a physical memory area against the
    E820 map. The routine will return false if the complete area is RAM
    according to the E820 map and true otherwise.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 87251b4c2e30..99ef82cc4edc 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -573,6 +573,29 @@ static unsigned long __init xen_count_remap_pages(unsigned long max_pfn)
 	return extra;
 }
 
+bool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)
+{
+	struct e820entry *entry;
+	unsigned mapcnt;
+	phys_addr_t end;
+
+	if (!size)
+		return false;
+
+	end = start + size;
+	entry = xen_e820_map;
+
+	for (mapcnt = 0; mapcnt < xen_e820_map_entries; mapcnt++) {
+		if (entry->type == E820_RAM && entry->addr <= start &&
+		    (entry->addr + entry->size) >= end)
+			return false;
+
+		entry++;
+	}
+
+	return true;
+}
+
 /*
  * Reserve Xen mfn_list.
  * See comment above "struct start_info" in <xen/interface/xen.h>

commit 5097cdf6cef15439f971df54f9abcf143d7ca698
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:27 2015 +0200

    xen: split counting of extra memory pages from remapping
    
    Memory pages in the initial memory setup done by the Xen hypervisor
    conflicting with the target E820 map are remapped. In order to do this
    those pages are counted and remapped in xen_set_identity_and_remap().
    
    Split the counting from the remapping operation to be able to setup
    the needed memory sizes in time but doing the remap operation at a
    later time. This enables us to simplify the interface to
    xen_set_identity_and_remap() as the number of remapped and released
    pages is no longer needed here.
    
    Finally move the remapping further down to prepare relocating
    conflicting memory contents before the memory might be clobbered by
    xen_set_identity_and_remap(). This requires to not destroy the Xen
    E820 map when the one for the system is being constructed.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ab6c36e1801b..87251b4c2e30 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -223,7 +223,7 @@ static int __init xen_free_mfn(unsigned long mfn)
  * as a fallback if the remapping fails.
  */
 static void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,
-	unsigned long end_pfn, unsigned long nr_pages, unsigned long *released)
+			unsigned long end_pfn, unsigned long nr_pages)
 {
 	unsigned long pfn, end;
 	int ret;
@@ -243,7 +243,7 @@ static void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,
 		WARN(ret != 1, "Failed to release pfn %lx err=%d\n", pfn, ret);
 
 		if (ret == 1) {
-			(*released)++;
+			xen_released_pages++;
 			if (!__set_phys_to_machine(pfn, INVALID_P2M_ENTRY))
 				break;
 		} else
@@ -359,8 +359,7 @@ static void __init xen_do_set_identity_and_remap_chunk(
  */
 static unsigned long __init xen_set_identity_and_remap_chunk(
 	unsigned long start_pfn, unsigned long end_pfn, unsigned long nr_pages,
-	unsigned long remap_pfn, unsigned long *released,
-	unsigned long *remapped)
+	unsigned long remap_pfn)
 {
 	unsigned long pfn;
 	unsigned long i = 0;
@@ -385,7 +384,7 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		if (!remap_range_size) {
 			pr_warning("Unable to find available pfn range, not remapping identity pages\n");
 			xen_set_identity_and_release_chunk(cur_pfn,
-				cur_pfn + left, nr_pages, released);
+						cur_pfn + left, nr_pages);
 			break;
 		}
 		/* Adjust size to fit in current e820 RAM region */
@@ -397,7 +396,6 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		/* Update variables to reflect new mappings. */
 		i += size;
 		remap_pfn += size;
-		*remapped += size;
 	}
 
 	/*
@@ -412,14 +410,11 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 	return remap_pfn;
 }
 
-static void __init xen_set_identity_and_remap(unsigned long nr_pages,
-			unsigned long *released, unsigned long *remapped)
+static void __init xen_set_identity_and_remap(unsigned long nr_pages)
 {
 	phys_addr_t start = 0;
 	unsigned long last_pfn = nr_pages;
 	const struct e820entry *entry = xen_e820_map;
-	unsigned long num_released = 0;
-	unsigned long num_remapped = 0;
 	int i;
 
 	/*
@@ -445,16 +440,12 @@ static void __init xen_set_identity_and_remap(unsigned long nr_pages,
 			if (start_pfn < end_pfn)
 				last_pfn = xen_set_identity_and_remap_chunk(
 						start_pfn, end_pfn, nr_pages,
-						last_pfn, &num_released,
-						&num_remapped);
+						last_pfn);
 			start = end;
 		}
 	}
 
-	*released = num_released;
-	*remapped = num_remapped;
-
-	pr_info("Released %ld page(s)\n", num_released);
+	pr_info("Released %ld page(s)\n", xen_released_pages);
 }
 
 /*
@@ -560,6 +551,28 @@ static void __init xen_ignore_unusable(void)
 	}
 }
 
+static unsigned long __init xen_count_remap_pages(unsigned long max_pfn)
+{
+	unsigned long extra = 0;
+	const struct e820entry *entry = xen_e820_map;
+	int i;
+
+	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
+		unsigned long start_pfn = PFN_DOWN(entry->addr);
+		unsigned long end_pfn = PFN_UP(entry->addr + entry->size);
+
+		if (start_pfn >= max_pfn)
+			break;
+		if (entry->type == E820_RAM)
+			continue;
+		if (end_pfn >= max_pfn)
+			end_pfn = max_pfn;
+		extra += end_pfn - start_pfn;
+	}
+
+	return extra;
+}
+
 /*
  * Reserve Xen mfn_list.
  * See comment above "struct start_info" in <xen/interface/xen.h>
@@ -601,12 +614,12 @@ static void __init xen_reserve_xen_mfnlist(void)
 char * __init xen_memory_setup(void)
 {
 	unsigned long max_pfn = xen_start_info->nr_pages;
-	phys_addr_t mem_end;
+	phys_addr_t mem_end, addr, size, chunk_size;
+	u32 type;
 	int rc;
 	struct xen_memory_map memmap;
 	unsigned long max_pages;
 	unsigned long extra_pages = 0;
-	unsigned long remapped_pages;
 	int i;
 	int op;
 
@@ -653,15 +666,8 @@ char * __init xen_memory_setup(void)
 	if (max_pages > max_pfn)
 		extra_pages += max_pages - max_pfn;
 
-	/*
-	 * Set identity map on non-RAM pages and prepare remapping the
-	 * underlying RAM.
-	 */
-	xen_set_identity_and_remap(max_pfn, &xen_released_pages,
-				   &remapped_pages);
-
-	extra_pages += xen_released_pages;
-	extra_pages += remapped_pages;
+	/* How many extra pages do we need due to remapping? */
+	extra_pages += xen_count_remap_pages(max_pfn);
 
 	/*
 	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO
@@ -677,29 +683,35 @@ char * __init xen_memory_setup(void)
 	extra_pages = min(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
 			  extra_pages);
 	i = 0;
+	addr = xen_e820_map[0].addr;
+	size = xen_e820_map[0].size;
 	while (i < xen_e820_map_entries) {
-		phys_addr_t addr = xen_e820_map[i].addr;
-		phys_addr_t size = xen_e820_map[i].size;
-		u32 type = xen_e820_map[i].type;
+		chunk_size = size;
+		type = xen_e820_map[i].type;
 
 		if (type == E820_RAM) {
 			if (addr < mem_end) {
-				size = min(size, mem_end - addr);
+				chunk_size = min(size, mem_end - addr);
 			} else if (extra_pages) {
-				size = min(size, PFN_PHYS(extra_pages));
-				extra_pages -= PFN_DOWN(size);
-				xen_add_extra_mem(addr, size);
-				xen_max_p2m_pfn = PFN_DOWN(addr + size);
+				chunk_size = min(size, PFN_PHYS(extra_pages));
+				extra_pages -= PFN_DOWN(chunk_size);
+				xen_add_extra_mem(addr, chunk_size);
+				xen_max_p2m_pfn = PFN_DOWN(addr + chunk_size);
 			} else
 				type = E820_UNUSABLE;
 		}
 
-		xen_align_and_add_e820_region(addr, size, type);
+		xen_align_and_add_e820_region(addr, chunk_size, type);
 
-		xen_e820_map[i].addr += size;
-		xen_e820_map[i].size -= size;
-		if (xen_e820_map[i].size == 0)
+		addr += chunk_size;
+		size -= chunk_size;
+		if (size == 0) {
 			i++;
+			if (i < xen_e820_map_entries) {
+				addr = xen_e820_map[i].addr;
+				size = xen_e820_map[i].size;
+			}
+		}
 	}
 
 	/*
@@ -709,7 +721,7 @@ char * __init xen_memory_setup(void)
 	 * PFNs above MAX_P2M_PFN are considered identity mapped as
 	 * well.
 	 */
-	set_phys_range_identity(xen_e820_map[i - 1].addr / PAGE_SIZE, ~0ul);
+	set_phys_range_identity(addr / PAGE_SIZE, ~0ul);
 
 	/*
 	 * In domU, the ISA region is normal, usable memory, but we
@@ -723,6 +735,12 @@ char * __init xen_memory_setup(void)
 
 	xen_reserve_xen_mfnlist();
 
+	/*
+	 * Set identity map on non-RAM pages and prepare remapping the
+	 * underlying RAM.
+	 */
+	xen_set_identity_and_remap(max_pfn);
+
 	return "Xen";
 }
 

commit 69632ecfcd03b12202ed62dfa0aabac83904f8ac
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:26 2015 +0200

    xen: move static e820 map to global scope
    
    Instead of using a function local static e820 map in xen_memory_setup()
    and calling various functions in the same source with the map as a
    parameter use a map directly accessible by all functions in the source.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index adad417ca1ba..ab6c36e1801b 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -38,6 +38,10 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 /* Number of pages released from the initial allocation. */
 unsigned long xen_released_pages;
 
+/* E820 map used during setting up memory. */
+static struct e820entry xen_e820_map[E820MAX] __initdata;
+static u32 xen_e820_map_entries __initdata;
+
 /*
  * Buffer used to remap identity mapped pages. We only need the virtual space.
  * The physical page behind this address is remapped as needed to different
@@ -164,15 +168,13 @@ void __init xen_inv_extra_mem(void)
  * This function updates min_pfn with the pfn found and returns
  * the size of that range or zero if not found.
  */
-static unsigned long __init xen_find_pfn_range(
-	const struct e820entry *list, size_t map_size,
-	unsigned long *min_pfn)
+static unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)
 {
-	const struct e820entry *entry;
+	const struct e820entry *entry = xen_e820_map;
 	unsigned int i;
 	unsigned long done = 0;
 
-	for (i = 0, entry = list; i < map_size; i++, entry++) {
+	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
 		unsigned long s_pfn;
 		unsigned long e_pfn;
 
@@ -356,9 +358,9 @@ static void __init xen_do_set_identity_and_remap_chunk(
  * to Xen and not remapped.
  */
 static unsigned long __init xen_set_identity_and_remap_chunk(
-        const struct e820entry *list, size_t map_size, unsigned long start_pfn,
-	unsigned long end_pfn, unsigned long nr_pages, unsigned long remap_pfn,
-	unsigned long *released, unsigned long *remapped)
+	unsigned long start_pfn, unsigned long end_pfn, unsigned long nr_pages,
+	unsigned long remap_pfn, unsigned long *released,
+	unsigned long *remapped)
 {
 	unsigned long pfn;
 	unsigned long i = 0;
@@ -379,8 +381,7 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		if (cur_pfn + size > nr_pages)
 			size = nr_pages - cur_pfn;
 
-		remap_range_size = xen_find_pfn_range(list, map_size,
-						      &remap_pfn);
+		remap_range_size = xen_find_pfn_range(&remap_pfn);
 		if (!remap_range_size) {
 			pr_warning("Unable to find available pfn range, not remapping identity pages\n");
 			xen_set_identity_and_release_chunk(cur_pfn,
@@ -411,13 +412,12 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 	return remap_pfn;
 }
 
-static void __init xen_set_identity_and_remap(
-	const struct e820entry *list, size_t map_size, unsigned long nr_pages,
-	unsigned long *released, unsigned long *remapped)
+static void __init xen_set_identity_and_remap(unsigned long nr_pages,
+			unsigned long *released, unsigned long *remapped)
 {
 	phys_addr_t start = 0;
 	unsigned long last_pfn = nr_pages;
-	const struct e820entry *entry;
+	const struct e820entry *entry = xen_e820_map;
 	unsigned long num_released = 0;
 	unsigned long num_remapped = 0;
 	int i;
@@ -433,9 +433,9 @@ static void __init xen_set_identity_and_remap(
 	 * example) the DMI tables in a reserved region that begins on
 	 * a non-page boundary.
 	 */
-	for (i = 0, entry = list; i < map_size; i++, entry++) {
+	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
 		phys_addr_t end = entry->addr + entry->size;
-		if (entry->type == E820_RAM || i == map_size - 1) {
+		if (entry->type == E820_RAM || i == xen_e820_map_entries - 1) {
 			unsigned long start_pfn = PFN_DOWN(start);
 			unsigned long end_pfn = PFN_UP(end);
 
@@ -444,9 +444,9 @@ static void __init xen_set_identity_and_remap(
 
 			if (start_pfn < end_pfn)
 				last_pfn = xen_set_identity_and_remap_chunk(
-						list, map_size, start_pfn,
-						end_pfn, nr_pages, last_pfn,
-						&num_released, &num_remapped);
+						start_pfn, end_pfn, nr_pages,
+						last_pfn, &num_released,
+						&num_remapped);
 			start = end;
 		}
 	}
@@ -549,12 +549,12 @@ static void __init xen_align_and_add_e820_region(phys_addr_t start,
 	e820_add_region(start, end - start, type);
 }
 
-static void __init xen_ignore_unusable(struct e820entry *list, size_t map_size)
+static void __init xen_ignore_unusable(void)
 {
-	struct e820entry *entry;
+	struct e820entry *entry = xen_e820_map;
 	unsigned int i;
 
-	for (i = 0, entry = list; i < map_size; i++, entry++) {
+	for (i = 0; i < xen_e820_map_entries; i++, entry++) {
 		if (entry->type == E820_UNUSABLE)
 			entry->type = E820_RAM;
 	}
@@ -600,8 +600,6 @@ static void __init xen_reserve_xen_mfnlist(void)
  **/
 char * __init xen_memory_setup(void)
 {
-	static struct e820entry map[E820MAX] __initdata;
-
 	unsigned long max_pfn = xen_start_info->nr_pages;
 	phys_addr_t mem_end;
 	int rc;
@@ -616,7 +614,7 @@ char * __init xen_memory_setup(void)
 	mem_end = PFN_PHYS(max_pfn);
 
 	memmap.nr_entries = E820MAX;
-	set_xen_guest_handle(memmap.buffer, map);
+	set_xen_guest_handle(memmap.buffer, xen_e820_map);
 
 	op = xen_initial_domain() ?
 		XENMEM_machine_memory_map :
@@ -625,15 +623,16 @@ char * __init xen_memory_setup(void)
 	if (rc == -ENOSYS) {
 		BUG_ON(xen_initial_domain());
 		memmap.nr_entries = 1;
-		map[0].addr = 0ULL;
-		map[0].size = mem_end;
+		xen_e820_map[0].addr = 0ULL;
+		xen_e820_map[0].size = mem_end;
 		/* 8MB slack (to balance backend allocations). */
-		map[0].size += 8ULL << 20;
-		map[0].type = E820_RAM;
+		xen_e820_map[0].size += 8ULL << 20;
+		xen_e820_map[0].type = E820_RAM;
 		rc = 0;
 	}
 	BUG_ON(rc);
 	BUG_ON(memmap.nr_entries == 0);
+	xen_e820_map_entries = memmap.nr_entries;
 
 	/*
 	 * Xen won't allow a 1:1 mapping to be created to UNUSABLE
@@ -644,10 +643,11 @@ char * __init xen_memory_setup(void)
 	 * a patch in the future.
 	 */
 	if (xen_initial_domain())
-		xen_ignore_unusable(map, memmap.nr_entries);
+		xen_ignore_unusable();
 
 	/* Make sure the Xen-supplied memory map is well-ordered. */
-	sanitize_e820_map(map, memmap.nr_entries, &memmap.nr_entries);
+	sanitize_e820_map(xen_e820_map, xen_e820_map_entries,
+			  &xen_e820_map_entries);
 
 	max_pages = xen_get_max_pages();
 	if (max_pages > max_pfn)
@@ -657,8 +657,8 @@ char * __init xen_memory_setup(void)
 	 * Set identity map on non-RAM pages and prepare remapping the
 	 * underlying RAM.
 	 */
-	xen_set_identity_and_remap(map, memmap.nr_entries, max_pfn,
-				   &xen_released_pages, &remapped_pages);
+	xen_set_identity_and_remap(max_pfn, &xen_released_pages,
+				   &remapped_pages);
 
 	extra_pages += xen_released_pages;
 	extra_pages += remapped_pages;
@@ -677,10 +677,10 @@ char * __init xen_memory_setup(void)
 	extra_pages = min(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
 			  extra_pages);
 	i = 0;
-	while (i < memmap.nr_entries) {
-		phys_addr_t addr = map[i].addr;
-		phys_addr_t size = map[i].size;
-		u32 type = map[i].type;
+	while (i < xen_e820_map_entries) {
+		phys_addr_t addr = xen_e820_map[i].addr;
+		phys_addr_t size = xen_e820_map[i].size;
+		u32 type = xen_e820_map[i].type;
 
 		if (type == E820_RAM) {
 			if (addr < mem_end) {
@@ -696,9 +696,9 @@ char * __init xen_memory_setup(void)
 
 		xen_align_and_add_e820_region(addr, size, type);
 
-		map[i].addr += size;
-		map[i].size -= size;
-		if (map[i].size == 0)
+		xen_e820_map[i].addr += size;
+		xen_e820_map[i].size -= size;
+		if (xen_e820_map[i].size == 0)
 			i++;
 	}
 
@@ -709,7 +709,7 @@ char * __init xen_memory_setup(void)
 	 * PFNs above MAX_P2M_PFN are considered identity mapped as
 	 * well.
 	 */
-	set_phys_range_identity(map[i-1].addr / PAGE_SIZE, ~0ul);
+	set_phys_range_identity(xen_e820_map[i - 1].addr / PAGE_SIZE, ~0ul);
 
 	/*
 	 * In domU, the ISA region is normal, usable memory, but we
@@ -731,23 +731,25 @@ char * __init xen_memory_setup(void)
  */
 char * __init xen_auto_xlated_memory_setup(void)
 {
-	static struct e820entry map[E820MAX] __initdata;
-
 	struct xen_memory_map memmap;
 	int i;
 	int rc;
 
 	memmap.nr_entries = E820MAX;
-	set_xen_guest_handle(memmap.buffer, map);
+	set_xen_guest_handle(memmap.buffer, xen_e820_map);
 
 	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
 	if (rc < 0)
 		panic("No memory map (%d)\n", rc);
 
-	sanitize_e820_map(map, ARRAY_SIZE(map), &memmap.nr_entries);
+	xen_e820_map_entries = memmap.nr_entries;
+
+	sanitize_e820_map(xen_e820_map, ARRAY_SIZE(xen_e820_map),
+			  &xen_e820_map_entries);
 
-	for (i = 0; i < memmap.nr_entries; i++)
-		e820_add_region(map[i].addr, map[i].size, map[i].type);
+	for (i = 0; i < xen_e820_map_entries; i++)
+		e820_add_region(xen_e820_map[i].addr, xen_e820_map[i].size,
+				xen_e820_map[i].type);
 
 	xen_reserve_xen_mfnlist();
 

commit 8f5b0c63987207fd5c3c1f89c9eb6cb95b30386e
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Jul 17 06:51:25 2015 +0200

    xen: eliminate scalability issues from initial mapping setup
    
    Direct Xen to place the initial P->M table outside of the initial
    mapping, as otherwise the 1G (implementation) / 2G (theoretical)
    restriction on the size of the initial mapping limits the amount
    of memory a domain can be handed initially.
    
    As the initial P->M table is copied rather early during boot to
    domain private memory and it's initial virtual mapping is dropped,
    the easiest way to avoid virtual address conflicts with other
    addresses in the kernel is to use a user address area for the
    virtual address of the initial P->M table. This allows us to just
    throw away the page tables of the initial mapping after the copy
    without having to care about address invalidation.
    
    It should be noted that this patch won't enable a pv-domain to USE
    more than 512 GB of RAM. It just enables it to be started with a
    P->M table covering more memory. This is especially important for
    being able to boot a Dom0 on a system with more than 512 GB memory.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Based-on-patch-by: Jan Beulich <jbeulich@suse.com>
    Acked-by: Konrad Rzeszutek Wilk <Konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 55f388ef481a..adad417ca1ba 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -560,6 +560,41 @@ static void __init xen_ignore_unusable(struct e820entry *list, size_t map_size)
 	}
 }
 
+/*
+ * Reserve Xen mfn_list.
+ * See comment above "struct start_info" in <xen/interface/xen.h>
+ * We tried to make the the memblock_reserve more selective so
+ * that it would be clear what region is reserved. Sadly we ran
+ * in the problem wherein on a 64-bit hypervisor with a 32-bit
+ * initial domain, the pt_base has the cr3 value which is not
+ * neccessarily where the pagetable starts! As Jan put it: "
+ * Actually, the adjustment turns out to be correct: The page
+ * tables for a 32-on-64 dom0 get allocated in the order "first L1",
+ * "first L2", "first L3", so the offset to the page table base is
+ * indeed 2. When reading xen/include/public/xen.h's comment
+ * very strictly, this is not a violation (since there nothing is said
+ * that the first thing in the page table space is pointed to by
+ * pt_base; I admit that this seems to be implied though, namely
+ * do I think that it is implied that the page table space is the
+ * range [pt_base, pt_base + nt_pt_frames), whereas that
+ * range here indeed is [pt_base - 2, pt_base - 2 + nt_pt_frames),
+ * which - without a priori knowledge - the kernel would have
+ * difficulty to figure out)." - so lets just fall back to the
+ * easy way and reserve the whole region.
+ */
+static void __init xen_reserve_xen_mfnlist(void)
+{
+	if (xen_start_info->mfn_list >= __START_KERNEL_map) {
+		memblock_reserve(__pa(xen_start_info->mfn_list),
+				 xen_start_info->pt_base -
+				 xen_start_info->mfn_list);
+		return;
+	}
+
+	memblock_reserve(PFN_PHYS(xen_start_info->first_p2m_pfn),
+			 PFN_PHYS(xen_start_info->nr_p2m_frames));
+}
+
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.
  **/
@@ -684,35 +719,10 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
-	/*
-	 * Reserve Xen bits:
-	 *  - mfn_list
-	 *  - xen_start_info
-	 * See comment above "struct start_info" in <xen/interface/xen.h>
-	 * We tried to make the the memblock_reserve more selective so
-	 * that it would be clear what region is reserved. Sadly we ran
-	 * in the problem wherein on a 64-bit hypervisor with a 32-bit
-	 * initial domain, the pt_base has the cr3 value which is not
-	 * neccessarily where the pagetable starts! As Jan put it: "
-	 * Actually, the adjustment turns out to be correct: The page
-	 * tables for a 32-on-64 dom0 get allocated in the order "first L1",
-	 * "first L2", "first L3", so the offset to the page table base is
-	 * indeed 2. When reading xen/include/public/xen.h's comment
-	 * very strictly, this is not a violation (since there nothing is said
-	 * that the first thing in the page table space is pointed to by
-	 * pt_base; I admit that this seems to be implied though, namely
-	 * do I think that it is implied that the page table space is the
-	 * range [pt_base, pt_base + nt_pt_frames), whereas that
-	 * range here indeed is [pt_base - 2, pt_base - 2 + nt_pt_frames),
-	 * which - without a priori knowledge - the kernel would have
-	 * difficulty to figure out)." - so lets just fall back to the
-	 * easy way and reserve the whole region.
-	 */
-	memblock_reserve(__pa(xen_start_info->mfn_list),
-			 xen_start_info->pt_base - xen_start_info->mfn_list);
-
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
+	xen_reserve_xen_mfnlist();
+
 	return "Xen";
 }
 
@@ -739,8 +749,7 @@ char * __init xen_auto_xlated_memory_setup(void)
 	for (i = 0; i < memmap.nr_entries; i++)
 		e820_add_region(map[i].addr, map[i].size, map[i].type);
 
-	memblock_reserve(__pa(xen_start_info->mfn_list),
-			 xen_start_info->pt_base - xen_start_info->mfn_list);
+	xen_reserve_xen_mfnlist();
 
 	return "Xen";
 }

commit a3f5239650a9c08df0473261aedd6f50f7775410
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Jan 28 07:44:23 2015 +0100

    x86/xen: add some __init and static annotations in arch/x86/xen/setup.c
    
    Some more functions in arch/x86/xen/setup.c can be made "__init".
    xen_ignore_unusable() can be made "static".
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 4dcc60819eda..55f388ef481a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -535,8 +535,8 @@ static unsigned long __init xen_get_max_pages(void)
 	return min(max_pages, MAX_DOMAIN_PAGES);
 }
 
-static void xen_align_and_add_e820_region(phys_addr_t start, phys_addr_t size,
-					  int type)
+static void __init xen_align_and_add_e820_region(phys_addr_t start,
+						 phys_addr_t size, int type)
 {
 	phys_addr_t end = start + size;
 
@@ -549,7 +549,7 @@ static void xen_align_and_add_e820_region(phys_addr_t start, phys_addr_t size,
 	e820_add_region(start, end - start, type);
 }
 
-void xen_ignore_unusable(struct e820entry *list, size_t map_size)
+static void __init xen_ignore_unusable(struct e820entry *list, size_t map_size)
 {
 	struct e820entry *entry;
 	unsigned int i;

commit 3ba5c867ca504a9fbdadbbd1b36b690c7e2718df
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Jan 28 07:44:22 2015 +0100

    x86/xen: use correct types for addresses in arch/x86/xen/setup.c
    
    In many places in arch/x86/xen/setup.c wrong types are used for
    physical addresses (u64 or unsigned long long). Use phys_addr_t
    instead.
    
    Use macros already defined instead of open coding them.
    
    Correct some other type mismatches.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index d2520c30c2f2..4dcc60819eda 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -64,7 +64,7 @@ static unsigned long xen_remap_mfn __initdata = INVALID_P2M_ENTRY;
  */
 #define EXTRA_MEM_RATIO		(10)
 
-static void __init xen_add_extra_mem(u64 start, u64 size)
+static void __init xen_add_extra_mem(phys_addr_t start, phys_addr_t size)
 {
 	int i;
 
@@ -87,10 +87,10 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 	memblock_reserve(start, size);
 }
 
-static void __init xen_del_extra_mem(u64 start, u64 size)
+static void __init xen_del_extra_mem(phys_addr_t start, phys_addr_t size)
 {
 	int i;
-	u64 start_r, size_r;
+	phys_addr_t start_r, size_r;
 
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
 		start_r = xen_extra_mem[i].start;
@@ -257,7 +257,7 @@ static void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,
 static void __init xen_update_mem_tables(unsigned long pfn, unsigned long mfn)
 {
 	struct mmu_update update = {
-		.ptr = ((unsigned long long)mfn << PAGE_SHIFT) | MMU_MACHPHYS_UPDATE,
+		.ptr = ((uint64_t)mfn << PAGE_SHIFT) | MMU_MACHPHYS_UPDATE,
 		.val = pfn
 	};
 
@@ -535,14 +535,15 @@ static unsigned long __init xen_get_max_pages(void)
 	return min(max_pages, MAX_DOMAIN_PAGES);
 }
 
-static void xen_align_and_add_e820_region(u64 start, u64 size, int type)
+static void xen_align_and_add_e820_region(phys_addr_t start, phys_addr_t size,
+					  int type)
 {
-	u64 end = start + size;
+	phys_addr_t end = start + size;
 
 	/* Align RAM regions to page boundaries. */
 	if (type == E820_RAM) {
 		start = PAGE_ALIGN(start);
-		end &= ~((u64)PAGE_SIZE - 1);
+		end &= ~((phys_addr_t)PAGE_SIZE - 1);
 	}
 
 	e820_add_region(start, end - start, type);
@@ -567,7 +568,7 @@ char * __init xen_memory_setup(void)
 	static struct e820entry map[E820MAX] __initdata;
 
 	unsigned long max_pfn = xen_start_info->nr_pages;
-	unsigned long long mem_end;
+	phys_addr_t mem_end;
 	int rc;
 	struct xen_memory_map memmap;
 	unsigned long max_pages;
@@ -642,16 +643,16 @@ char * __init xen_memory_setup(void)
 			  extra_pages);
 	i = 0;
 	while (i < memmap.nr_entries) {
-		u64 addr = map[i].addr;
-		u64 size = map[i].size;
+		phys_addr_t addr = map[i].addr;
+		phys_addr_t size = map[i].size;
 		u32 type = map[i].type;
 
 		if (type == E820_RAM) {
 			if (addr < mem_end) {
 				size = min(size, mem_end - addr);
 			} else if (extra_pages) {
-				size = min(size, (u64)extra_pages * PAGE_SIZE);
-				extra_pages -= size / PAGE_SIZE;
+				size = min(size, PFN_PHYS(extra_pages));
+				extra_pages -= PFN_DOWN(size);
 				xen_add_extra_mem(addr, size);
 				xen_max_p2m_pfn = PFN_DOWN(addr + size);
 			} else

commit f0feed10aa3ef6b6f6254bf9a66abd58c0011d90
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Jan 28 07:44:21 2015 +0100

    x86/xen: cleanup arch/x86/xen/setup.c
    
    Remove extern declarations in arch/x86/xen/setup.c which are either
    not used or redundant. Move needed other extern declarations to
    xen-ops.h
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 865e56cea7a0..d2520c30c2f2 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -32,16 +32,6 @@
 #include "p2m.h"
 #include "mmu.h"
 
-/* These are code, but not functions.  Defined in entry.S */
-extern const char xen_hypervisor_callback[];
-extern const char xen_failsafe_callback[];
-#ifdef CONFIG_X86_64
-extern asmlinkage void nmi(void);
-#endif
-extern void xen_sysenter_target(void);
-extern void xen_syscall_target(void);
-extern void xen_syscall32_target(void);
-
 /* Amount of extra memory space we add to the e820 ranges */
 struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 

commit 9a17ad7f3db17db0c6375de96672f16ab1aa51ae
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Jan 12 06:05:10 2015 +0100

    xen: check for zero sized area when invalidating memory
    
    With the introduction of the linear mapped p2m list setting memory
    areas to "invalid" had to be delayed. When doing the invalidation
    make sure no zero sized areas are processed.
    
    Signed-off-by: Juegren Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 410210f279bf..865e56cea7a0 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -160,6 +160,8 @@ void __init xen_inv_extra_mem(void)
 	int i;
 
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
+		if (!xen_extra_mem[i].size)
+			continue;
 		pfn_s = PFN_DOWN(xen_extra_mem[i].start);
 		pfn_e = PFN_UP(xen_extra_mem[i].start + xen_extra_mem[i].size);
 		for (pfn = pfn_s; pfn < pfn_e; pfn++)

commit e86f949667127509d95b6c678fdd928b93128d9d
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Jan 12 06:05:09 2015 +0100

    xen: use correct type for physical addresses
    
    When converting a pfn to a physical address be sure to use 64 bit
    wide types or convert the physical address to a pfn if possible.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index feb6d86fa0a0..410210f279bf 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -140,7 +140,7 @@ static void __init xen_del_extra_mem(u64 start, u64 size)
 unsigned long __ref xen_chk_extra_mem(unsigned long pfn)
 {
 	int i;
-	unsigned long addr = PFN_PHYS(pfn);
+	phys_addr_t addr = PFN_PHYS(pfn);
 
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
 		if (addr >= xen_extra_mem[i].start &&
@@ -284,7 +284,7 @@ static void __init xen_update_mem_tables(unsigned long pfn, unsigned long mfn)
 	}
 
 	/* Update kernel mapping, but not for highmem. */
-	if ((pfn << PAGE_SHIFT) >= __pa(high_memory))
+	if (pfn >= PFN_UP(__pa(high_memory - 1)))
 		return;
 
 	if (HYPERVISOR_update_va_mapping((unsigned long)__va(pfn << PAGE_SHIFT),

commit a97dae1a2e92e728d28515e88e8eda151f5796f5
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Jan 7 11:21:50 2015 +0000

    x86/xen: add extra memory for remapped frames during setup
    
    If the non-RAM regions in the e820 memory map are larger than the size
    of the initial balloon, a BUG was triggered as the frames are remaped
    beyond the limit of the linear p2m.  The frames are remapped into the
    initial balloon area (xen_extra_mem) but not enough of this is
    available.
    
    Ensure enough extra memory regions are added for these remapped
    frames.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 664dffc29b6b..feb6d86fa0a0 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -366,7 +366,7 @@ static void __init xen_do_set_identity_and_remap_chunk(
 static unsigned long __init xen_set_identity_and_remap_chunk(
         const struct e820entry *list, size_t map_size, unsigned long start_pfn,
 	unsigned long end_pfn, unsigned long nr_pages, unsigned long remap_pfn,
-	unsigned long *released)
+	unsigned long *released, unsigned long *remapped)
 {
 	unsigned long pfn;
 	unsigned long i = 0;
@@ -404,6 +404,7 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		/* Update variables to reflect new mappings. */
 		i += size;
 		remap_pfn += size;
+		*remapped += size;
 	}
 
 	/*
@@ -420,12 +421,13 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 
 static void __init xen_set_identity_and_remap(
 	const struct e820entry *list, size_t map_size, unsigned long nr_pages,
-	unsigned long *released)
+	unsigned long *released, unsigned long *remapped)
 {
 	phys_addr_t start = 0;
 	unsigned long last_pfn = nr_pages;
 	const struct e820entry *entry;
 	unsigned long num_released = 0;
+	unsigned long num_remapped = 0;
 	int i;
 
 	/*
@@ -452,12 +454,13 @@ static void __init xen_set_identity_and_remap(
 				last_pfn = xen_set_identity_and_remap_chunk(
 						list, map_size, start_pfn,
 						end_pfn, nr_pages, last_pfn,
-						&num_released);
+						&num_released, &num_remapped);
 			start = end;
 		}
 	}
 
 	*released = num_released;
+	*remapped = num_remapped;
 
 	pr_info("Released %ld page(s)\n", num_released);
 }
@@ -577,6 +580,7 @@ char * __init xen_memory_setup(void)
 	struct xen_memory_map memmap;
 	unsigned long max_pages;
 	unsigned long extra_pages = 0;
+	unsigned long remapped_pages;
 	int i;
 	int op;
 
@@ -626,9 +630,10 @@ char * __init xen_memory_setup(void)
 	 * underlying RAM.
 	 */
 	xen_set_identity_and_remap(map, memmap.nr_entries, max_pfn,
-				   &xen_released_pages);
+				   &xen_released_pages, &remapped_pages);
 
 	extra_pages += xen_released_pages;
+	extra_pages += remapped_pages;
 
 	/*
 	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO

commit bc7142cf798ae77628ae8c29bfdf6aa6dd2378e9
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Jan 7 11:01:08 2015 +0000

    x86/xen: don't count how many PFNs are identity mapped
    
    This accounting is just used to print a diagnostic message that isn't
    very useful.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index dfd77dec8e2b..664dffc29b6b 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -229,15 +229,14 @@ static int __init xen_free_mfn(unsigned long mfn)
  * as a fallback if the remapping fails.
  */
 static void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,
-	unsigned long end_pfn, unsigned long nr_pages, unsigned long *identity,
-	unsigned long *released)
+	unsigned long end_pfn, unsigned long nr_pages, unsigned long *released)
 {
-	unsigned long len = 0;
 	unsigned long pfn, end;
 	int ret;
 
 	WARN_ON(start_pfn > end_pfn);
 
+	/* Release pages first. */
 	end = min(end_pfn, nr_pages);
 	for (pfn = start_pfn; pfn < end; pfn++) {
 		unsigned long mfn = pfn_to_mfn(pfn);
@@ -250,16 +249,14 @@ static void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,
 		WARN(ret != 1, "Failed to release pfn %lx err=%d\n", pfn, ret);
 
 		if (ret == 1) {
+			(*released)++;
 			if (!__set_phys_to_machine(pfn, INVALID_P2M_ENTRY))
 				break;
-			len++;
 		} else
 			break;
 	}
 
-	/* Need to release pages first */
-	*released += len;
-	*identity += set_phys_range_identity(start_pfn, end_pfn);
+	set_phys_range_identity(start_pfn, end_pfn);
 }
 
 /*
@@ -318,7 +315,6 @@ static void __init xen_do_set_identity_and_remap_chunk(
 	unsigned long ident_pfn_iter, remap_pfn_iter;
 	unsigned long ident_end_pfn = start_pfn + size;
 	unsigned long left = size;
-	unsigned long ident_cnt = 0;
 	unsigned int i, chunk;
 
 	WARN_ON(size == 0);
@@ -347,8 +343,7 @@ static void __init xen_do_set_identity_and_remap_chunk(
 		xen_remap_mfn = mfn;
 
 		/* Set identity map */
-		ident_cnt += set_phys_range_identity(ident_pfn_iter,
-			ident_pfn_iter + chunk);
+		set_phys_range_identity(ident_pfn_iter, ident_pfn_iter + chunk);
 
 		left -= chunk;
 	}
@@ -371,7 +366,7 @@ static void __init xen_do_set_identity_and_remap_chunk(
 static unsigned long __init xen_set_identity_and_remap_chunk(
         const struct e820entry *list, size_t map_size, unsigned long start_pfn,
 	unsigned long end_pfn, unsigned long nr_pages, unsigned long remap_pfn,
-	unsigned long *identity, unsigned long *released)
+	unsigned long *released)
 {
 	unsigned long pfn;
 	unsigned long i = 0;
@@ -386,8 +381,7 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		/* Do not remap pages beyond the current allocation */
 		if (cur_pfn >= nr_pages) {
 			/* Identity map remaining pages */
-			*identity += set_phys_range_identity(cur_pfn,
-				cur_pfn + size);
+			set_phys_range_identity(cur_pfn, cur_pfn + size);
 			break;
 		}
 		if (cur_pfn + size > nr_pages)
@@ -398,7 +392,7 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		if (!remap_range_size) {
 			pr_warning("Unable to find available pfn range, not remapping identity pages\n");
 			xen_set_identity_and_release_chunk(cur_pfn,
-				cur_pfn + left, nr_pages, identity, released);
+				cur_pfn + left, nr_pages, released);
 			break;
 		}
 		/* Adjust size to fit in current e820 RAM region */
@@ -410,7 +404,6 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		/* Update variables to reflect new mappings. */
 		i += size;
 		remap_pfn += size;
-		*identity += size;
 	}
 
 	/*
@@ -430,7 +423,6 @@ static void __init xen_set_identity_and_remap(
 	unsigned long *released)
 {
 	phys_addr_t start = 0;
-	unsigned long identity = 0;
 	unsigned long last_pfn = nr_pages;
 	const struct e820entry *entry;
 	unsigned long num_released = 0;
@@ -460,14 +452,13 @@ static void __init xen_set_identity_and_remap(
 				last_pfn = xen_set_identity_and_remap_chunk(
 						list, map_size, start_pfn,
 						end_pfn, nr_pages, last_pfn,
-						&identity, &num_released);
+						&num_released);
 			start = end;
 		}
 	}
 
 	*released = num_released;
 
-	pr_info("Set %ld page(s) to 1-1 mapping\n", identity);
 	pr_info("Released %ld page(s)\n", num_released);
 }
 

commit 76f0a486faa8d22aa4b3ef482fd97a230d981d68
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Dec 8 06:32:19 2014 +0100

    xen: annotate xen_set_identity_and_remap_chunk() with __init
    
    Commit 5b8e7d80542487ff1bf17b4cf2922a01dee13d3a removed the __init
    annotation from xen_set_identity_and_remap_chunk(). Add it again.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 12cd199e4c9a..dfd77dec8e2b 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -368,7 +368,7 @@ static void __init xen_do_set_identity_and_remap_chunk(
  * pages. In the case of an error the underlying memory is simply released back
  * to Xen and not remapped.
  */
-static unsigned long xen_set_identity_and_remap_chunk(
+static unsigned long __init xen_set_identity_and_remap_chunk(
         const struct e820entry *list, size_t map_size, unsigned long start_pfn,
 	unsigned long end_pfn, unsigned long nr_pages, unsigned long remap_pfn,
 	unsigned long *identity, unsigned long *released)

commit 5b8e7d80542487ff1bf17b4cf2922a01dee13d3a
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Nov 28 11:53:55 2014 +0100

    xen: Delay invalidating extra memory
    
    When the physical memory configuration is initialized the p2m entries
    for not pouplated memory pages are set to "invalid". As those pages
    are beyond the hypervisor built p2m list the p2m tree has to be
    extended.
    
    This patch delays processing the extra memory related p2m entries
    during the boot process until some more basic memory management
    functions are callable. This removes the need to create new p2m
    entries until virtual memory management is available.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index e0b6912f9cad..12cd199e4c9a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -76,7 +76,6 @@ static unsigned long xen_remap_mfn __initdata = INVALID_P2M_ENTRY;
 
 static void __init xen_add_extra_mem(u64 start, u64 size)
 {
-	unsigned long pfn;
 	int i;
 
 	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
@@ -96,17 +95,75 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 		printk(KERN_WARNING "Warning: not enough extra memory regions\n");
 
 	memblock_reserve(start, size);
+}
 
-	xen_max_p2m_pfn = PFN_DOWN(start + size);
-	for (pfn = PFN_DOWN(start); pfn < xen_max_p2m_pfn; pfn++) {
-		unsigned long mfn = pfn_to_mfn(pfn);
+static void __init xen_del_extra_mem(u64 start, u64 size)
+{
+	int i;
+	u64 start_r, size_r;
 
-		if (WARN_ONCE(mfn == pfn, "Trying to over-write 1-1 mapping (pfn: %lx)\n", pfn))
-			continue;
-		WARN_ONCE(mfn != INVALID_P2M_ENTRY, "Trying to remove %lx which has %lx mfn!\n",
-			  pfn, mfn);
+	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
+		start_r = xen_extra_mem[i].start;
+		size_r = xen_extra_mem[i].size;
+
+		/* Start of region. */
+		if (start_r == start) {
+			BUG_ON(size > size_r);
+			xen_extra_mem[i].start += size;
+			xen_extra_mem[i].size -= size;
+			break;
+		}
+		/* End of region. */
+		if (start_r + size_r == start + size) {
+			BUG_ON(size > size_r);
+			xen_extra_mem[i].size -= size;
+			break;
+		}
+		/* Mid of region. */
+		if (start > start_r && start < start_r + size_r) {
+			BUG_ON(start + size > start_r + size_r);
+			xen_extra_mem[i].size = start - start_r;
+			/* Calling memblock_reserve() again is okay. */
+			xen_add_extra_mem(start + size, start_r + size_r -
+					  (start + size));
+			break;
+		}
+	}
+	memblock_free(start, size);
+}
+
+/*
+ * Called during boot before the p2m list can take entries beyond the
+ * hypervisor supplied p2m list. Entries in extra mem are to be regarded as
+ * invalid.
+ */
+unsigned long __ref xen_chk_extra_mem(unsigned long pfn)
+{
+	int i;
+	unsigned long addr = PFN_PHYS(pfn);
 
-		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
+		if (addr >= xen_extra_mem[i].start &&
+		    addr < xen_extra_mem[i].start + xen_extra_mem[i].size)
+			return INVALID_P2M_ENTRY;
+	}
+
+	return IDENTITY_FRAME(pfn);
+}
+
+/*
+ * Mark all pfns of extra mem as invalid in p2m list.
+ */
+void __init xen_inv_extra_mem(void)
+{
+	unsigned long pfn, pfn_s, pfn_e;
+	int i;
+
+	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
+		pfn_s = PFN_DOWN(xen_extra_mem[i].start);
+		pfn_e = PFN_UP(xen_extra_mem[i].start + xen_extra_mem[i].size);
+		for (pfn = pfn_s; pfn < pfn_e; pfn++)
+			set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 	}
 }
 
@@ -268,9 +325,6 @@ static void __init xen_do_set_identity_and_remap_chunk(
 
 	BUG_ON(xen_feature(XENFEAT_auto_translated_physmap));
 
-	/* Don't use memory until remapped */
-	memblock_reserve(PFN_PHYS(remap_pfn), PFN_PHYS(size));
-
 	mfn_save = virt_to_mfn(buf);
 
 	for (ident_pfn_iter = start_pfn, remap_pfn_iter = remap_pfn;
@@ -314,7 +368,7 @@ static void __init xen_do_set_identity_and_remap_chunk(
  * pages. In the case of an error the underlying memory is simply released back
  * to Xen and not remapped.
  */
-static unsigned long __init xen_set_identity_and_remap_chunk(
+static unsigned long xen_set_identity_and_remap_chunk(
         const struct e820entry *list, size_t map_size, unsigned long start_pfn,
 	unsigned long end_pfn, unsigned long nr_pages, unsigned long remap_pfn,
 	unsigned long *identity, unsigned long *released)
@@ -371,7 +425,7 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 	return remap_pfn;
 }
 
-static unsigned long __init xen_set_identity_and_remap(
+static void __init xen_set_identity_and_remap(
 	const struct e820entry *list, size_t map_size, unsigned long nr_pages,
 	unsigned long *released)
 {
@@ -415,8 +469,6 @@ static unsigned long __init xen_set_identity_and_remap(
 
 	pr_info("Set %ld page(s) to 1-1 mapping\n", identity);
 	pr_info("Released %ld page(s)\n", num_released);
-
-	return last_pfn;
 }
 
 /*
@@ -456,7 +508,7 @@ void __init xen_remap_memory(void)
 		} else if (pfn_s + len == xen_remap_buf.target_pfn) {
 			len += xen_remap_buf.size;
 		} else {
-			memblock_free(PFN_PHYS(pfn_s), PFN_PHYS(len));
+			xen_del_extra_mem(PFN_PHYS(pfn_s), PFN_PHYS(len));
 			pfn_s = xen_remap_buf.target_pfn;
 			len = xen_remap_buf.size;
 		}
@@ -466,7 +518,7 @@ void __init xen_remap_memory(void)
 	}
 
 	if (pfn_s != ~0UL && len)
-		memblock_free(PFN_PHYS(pfn_s), PFN_PHYS(len));
+		xen_del_extra_mem(PFN_PHYS(pfn_s), PFN_PHYS(len));
 
 	set_pte_mfn(buf, mfn_save, PAGE_KERNEL);
 
@@ -533,7 +585,6 @@ char * __init xen_memory_setup(void)
 	int rc;
 	struct xen_memory_map memmap;
 	unsigned long max_pages;
-	unsigned long last_pfn = 0;
 	unsigned long extra_pages = 0;
 	int i;
 	int op;
@@ -583,15 +634,11 @@ char * __init xen_memory_setup(void)
 	 * Set identity map on non-RAM pages and prepare remapping the
 	 * underlying RAM.
 	 */
-	last_pfn = xen_set_identity_and_remap(map, memmap.nr_entries, max_pfn,
-					      &xen_released_pages);
+	xen_set_identity_and_remap(map, memmap.nr_entries, max_pfn,
+				   &xen_released_pages);
 
 	extra_pages += xen_released_pages;
 
-	if (last_pfn > max_pfn) {
-		max_pfn = min(MAX_DOMAIN_PAGES, last_pfn);
-		mem_end = PFN_PHYS(max_pfn);
-	}
 	/*
 	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO
 	 * factor the base size.  On non-highmem systems, the base
@@ -618,6 +665,7 @@ char * __init xen_memory_setup(void)
 				size = min(size, (u64)extra_pages * PAGE_SIZE);
 				extra_pages -= size / PAGE_SIZE;
 				xen_add_extra_mem(addr, size);
+				xen_max_p2m_pfn = PFN_DOWN(addr + size);
 			} else
 				type = E820_UNUSABLE;
 		}

commit 1f3ac86b4c45a146e090d24bf66c49b95e72f071
Author: Juergen Gross <jgross@suse.com>
Date:   Fri Nov 28 11:53:53 2014 +0100

    xen: Delay remapping memory of pv-domain
    
    Early in the boot process the memory layout of a pv-domain is changed
    to match the E820 map (either the host one for Dom0 or the Xen one)
    regarding placement of RAM and PCI holes. This requires removing memory
    pages initially located at positions not suitable for RAM and adding
    them later at higher addresses where no restrictions apply.
    
    To be able to operate on the hypervisor supported p2m list until a
    virtual mapped linear p2m list can be constructed, remapping must
    be delayed until virtual memory management is initialized, as the
    initial p2m list can't be extended unlimited at physical memory
    initialization time due to it's fixed structure.
    
    A further advantage is the reduction in complexity and code volume as
    we don't have to be careful regarding memory restrictions during p2m
    updates.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 29834b3fd87f..e0b6912f9cad 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -30,6 +30,7 @@
 #include "xen-ops.h"
 #include "vdso.h"
 #include "p2m.h"
+#include "mmu.h"
 
 /* These are code, but not functions.  Defined in entry.S */
 extern const char xen_hypervisor_callback[];
@@ -47,8 +48,19 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 /* Number of pages released from the initial allocation. */
 unsigned long xen_released_pages;
 
-/* Buffer used to remap identity mapped pages */
-unsigned long xen_remap_buf[P2M_PER_PAGE] __initdata;
+/*
+ * Buffer used to remap identity mapped pages. We only need the virtual space.
+ * The physical page behind this address is remapped as needed to different
+ * buffer pages.
+ */
+#define REMAP_SIZE	(P2M_PER_PAGE - 3)
+static struct {
+	unsigned long	next_area_mfn;
+	unsigned long	target_pfn;
+	unsigned long	size;
+	unsigned long	mfns[REMAP_SIZE];
+} xen_remap_buf __initdata __aligned(PAGE_SIZE);
+static unsigned long xen_remap_mfn __initdata = INVALID_P2M_ENTRY;
 
 /* 
  * The maximum amount of extra memory compared to the base size.  The
@@ -98,63 +110,6 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 	}
 }
 
-static unsigned long __init xen_do_chunk(unsigned long start,
-					 unsigned long end, bool release)
-{
-	struct xen_memory_reservation reservation = {
-		.address_bits = 0,
-		.extent_order = 0,
-		.domid        = DOMID_SELF
-	};
-	unsigned long len = 0;
-	unsigned long pfn;
-	int ret;
-
-	for (pfn = start; pfn < end; pfn++) {
-		unsigned long frame;
-		unsigned long mfn = pfn_to_mfn(pfn);
-
-		if (release) {
-			/* Make sure pfn exists to start with */
-			if (mfn == INVALID_P2M_ENTRY || mfn_to_pfn(mfn) != pfn)
-				continue;
-			frame = mfn;
-		} else {
-			if (mfn != INVALID_P2M_ENTRY)
-				continue;
-			frame = pfn;
-		}
-		set_xen_guest_handle(reservation.extent_start, &frame);
-		reservation.nr_extents = 1;
-
-		ret = HYPERVISOR_memory_op(release ? XENMEM_decrease_reservation : XENMEM_populate_physmap,
-					   &reservation);
-		WARN(ret != 1, "Failed to %s pfn %lx err=%d\n",
-		     release ? "release" : "populate", pfn, ret);
-
-		if (ret == 1) {
-			if (!early_set_phys_to_machine(pfn, release ? INVALID_P2M_ENTRY : frame)) {
-				if (release)
-					break;
-				set_xen_guest_handle(reservation.extent_start, &frame);
-				reservation.nr_extents = 1;
-				ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation,
-							   &reservation);
-				break;
-			}
-			len++;
-		} else
-			break;
-	}
-	if (len)
-		printk(KERN_INFO "%s %lx-%lx pfn range: %lu pages %s\n",
-		       release ? "Freeing" : "Populating",
-		       start, end, len,
-		       release ? "freed" : "added");
-
-	return len;
-}
-
 /*
  * Finds the next RAM pfn available in the E820 map after min_pfn.
  * This function updates min_pfn with the pfn found and returns
@@ -198,26 +153,62 @@ static unsigned long __init xen_find_pfn_range(
 	return done;
 }
 
+static int __init xen_free_mfn(unsigned long mfn)
+{
+	struct xen_memory_reservation reservation = {
+		.address_bits = 0,
+		.extent_order = 0,
+		.domid        = DOMID_SELF
+	};
+
+	set_xen_guest_handle(reservation.extent_start, &mfn);
+	reservation.nr_extents = 1;
+
+	return HYPERVISOR_memory_op(XENMEM_decrease_reservation, &reservation);
+}
+
 /*
- * This releases a chunk of memory and then does the identity map. It's used as
+ * This releases a chunk of memory and then does the identity map. It's used
  * as a fallback if the remapping fails.
  */
 static void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,
 	unsigned long end_pfn, unsigned long nr_pages, unsigned long *identity,
 	unsigned long *released)
 {
+	unsigned long len = 0;
+	unsigned long pfn, end;
+	int ret;
+
 	WARN_ON(start_pfn > end_pfn);
 
+	end = min(end_pfn, nr_pages);
+	for (pfn = start_pfn; pfn < end; pfn++) {
+		unsigned long mfn = pfn_to_mfn(pfn);
+
+		/* Make sure pfn exists to start with */
+		if (mfn == INVALID_P2M_ENTRY || mfn_to_pfn(mfn) != pfn)
+			continue;
+
+		ret = xen_free_mfn(mfn);
+		WARN(ret != 1, "Failed to release pfn %lx err=%d\n", pfn, ret);
+
+		if (ret == 1) {
+			if (!__set_phys_to_machine(pfn, INVALID_P2M_ENTRY))
+				break;
+			len++;
+		} else
+			break;
+	}
+
 	/* Need to release pages first */
-	*released += xen_do_chunk(start_pfn, min(end_pfn, nr_pages), true);
+	*released += len;
 	*identity += set_phys_range_identity(start_pfn, end_pfn);
 }
 
 /*
- * Helper function to update both the p2m and m2p tables.
+ * Helper function to update the p2m and m2p tables and kernel mapping.
  */
-static unsigned long __init xen_update_mem_tables(unsigned long pfn,
-						  unsigned long mfn)
+static void __init xen_update_mem_tables(unsigned long pfn, unsigned long mfn)
 {
 	struct mmu_update update = {
 		.ptr = ((unsigned long long)mfn << PAGE_SHIFT) | MMU_MACHPHYS_UPDATE,
@@ -225,161 +216,91 @@ static unsigned long __init xen_update_mem_tables(unsigned long pfn,
 	};
 
 	/* Update p2m */
-	if (!early_set_phys_to_machine(pfn, mfn)) {
+	if (!set_phys_to_machine(pfn, mfn)) {
 		WARN(1, "Failed to set p2m mapping for pfn=%ld mfn=%ld\n",
 		     pfn, mfn);
-		return false;
+		BUG();
 	}
 
 	/* Update m2p */
 	if (HYPERVISOR_mmu_update(&update, 1, NULL, DOMID_SELF) < 0) {
 		WARN(1, "Failed to set m2p mapping for mfn=%ld pfn=%ld\n",
 		     mfn, pfn);
-		return false;
+		BUG();
 	}
 
-	return true;
+	/* Update kernel mapping, but not for highmem. */
+	if ((pfn << PAGE_SHIFT) >= __pa(high_memory))
+		return;
+
+	if (HYPERVISOR_update_va_mapping((unsigned long)__va(pfn << PAGE_SHIFT),
+					 mfn_pte(mfn, PAGE_KERNEL), 0)) {
+		WARN(1, "Failed to update kernel mapping for mfn=%ld pfn=%ld\n",
+		      mfn, pfn);
+		BUG();
+	}
 }
 
 /*
  * This function updates the p2m and m2p tables with an identity map from
- * start_pfn to start_pfn+size and remaps the underlying RAM of the original
- * allocation at remap_pfn. It must do so carefully in P2M_PER_PAGE sized blocks
- * to not exhaust the reserved brk space. Doing it in properly aligned blocks
- * ensures we only allocate the minimum required leaf pages in the p2m table. It
- * copies the existing mfns from the p2m table under the 1:1 map, overwrites
- * them with the identity map and then updates the p2m and m2p tables with the
- * remapped memory.
+ * start_pfn to start_pfn+size and prepares remapping the underlying RAM of the
+ * original allocation at remap_pfn. The information needed for remapping is
+ * saved in the memory itself to avoid the need for allocating buffers. The
+ * complete remap information is contained in a list of MFNs each containing
+ * up to REMAP_SIZE MFNs and the start target PFN for doing the remap.
+ * This enables us to preserve the original mfn sequence while doing the
+ * remapping at a time when the memory management is capable of allocating
+ * virtual and physical memory in arbitrary amounts, see 'xen_remap_memory' and
+ * its callers.
  */
-static unsigned long __init xen_do_set_identity_and_remap_chunk(
+static void __init xen_do_set_identity_and_remap_chunk(
         unsigned long start_pfn, unsigned long size, unsigned long remap_pfn)
 {
+	unsigned long buf = (unsigned long)&xen_remap_buf;
+	unsigned long mfn_save, mfn;
 	unsigned long ident_pfn_iter, remap_pfn_iter;
-	unsigned long ident_start_pfn_align, remap_start_pfn_align;
-	unsigned long ident_end_pfn_align, remap_end_pfn_align;
-	unsigned long ident_boundary_pfn, remap_boundary_pfn;
-	unsigned long ident_cnt = 0;
-	unsigned long remap_cnt = 0;
+	unsigned long ident_end_pfn = start_pfn + size;
 	unsigned long left = size;
-	unsigned long mod;
-	int i;
+	unsigned long ident_cnt = 0;
+	unsigned int i, chunk;
 
 	WARN_ON(size == 0);
 
 	BUG_ON(xen_feature(XENFEAT_auto_translated_physmap));
 
-	/*
-	 * Determine the proper alignment to remap memory in P2M_PER_PAGE sized
-	 * blocks. We need to keep track of both the existing pfn mapping and
-	 * the new pfn remapping.
-	 */
-	mod = start_pfn % P2M_PER_PAGE;
-	ident_start_pfn_align =
-		mod ? (start_pfn - mod + P2M_PER_PAGE) : start_pfn;
-	mod = remap_pfn % P2M_PER_PAGE;
-	remap_start_pfn_align =
-		mod ? (remap_pfn - mod + P2M_PER_PAGE) : remap_pfn;
-	mod = (start_pfn + size) % P2M_PER_PAGE;
-	ident_end_pfn_align = start_pfn + size - mod;
-	mod = (remap_pfn + size) % P2M_PER_PAGE;
-	remap_end_pfn_align = remap_pfn + size - mod;
-
-	/* Iterate over each p2m leaf node in each range */
-	for (ident_pfn_iter = ident_start_pfn_align, remap_pfn_iter = remap_start_pfn_align;
-	     ident_pfn_iter < ident_end_pfn_align && remap_pfn_iter < remap_end_pfn_align;
-	     ident_pfn_iter += P2M_PER_PAGE, remap_pfn_iter += P2M_PER_PAGE) {
-		/* Check we aren't past the end */
-		BUG_ON(ident_pfn_iter + P2M_PER_PAGE > start_pfn + size);
-		BUG_ON(remap_pfn_iter + P2M_PER_PAGE > remap_pfn + size);
-
-		/* Save p2m mappings */
-		for (i = 0; i < P2M_PER_PAGE; i++)
-			xen_remap_buf[i] = pfn_to_mfn(ident_pfn_iter + i);
-
-		/* Set identity map which will free a p2m leaf */
-		ident_cnt += set_phys_range_identity(ident_pfn_iter,
-			ident_pfn_iter + P2M_PER_PAGE);
-
-#ifdef DEBUG
-		/* Helps verify a p2m leaf has been freed */
-		for (i = 0; i < P2M_PER_PAGE; i++) {
-			unsigned int pfn = ident_pfn_iter + i;
-			BUG_ON(pfn_to_mfn(pfn) != pfn);
-		}
-#endif
-		/* Now remap memory */
-		for (i = 0; i < P2M_PER_PAGE; i++) {
-			unsigned long mfn = xen_remap_buf[i];
-
-			/* This will use the p2m leaf freed above */
-			if (!xen_update_mem_tables(remap_pfn_iter + i, mfn)) {
-				WARN(1, "Failed to update mem mapping for pfn=%ld mfn=%ld\n",
-					remap_pfn_iter + i, mfn);
-				return 0;
-			}
-
-			remap_cnt++;
-		}
+	/* Don't use memory until remapped */
+	memblock_reserve(PFN_PHYS(remap_pfn), PFN_PHYS(size));
 
-		left -= P2M_PER_PAGE;
-	}
-
-	/* Max boundary space possible */
-	BUG_ON(left > (P2M_PER_PAGE - 1) * 2);
+	mfn_save = virt_to_mfn(buf);
 
-	/* Now handle the boundary conditions */
-	ident_boundary_pfn = start_pfn;
-	remap_boundary_pfn = remap_pfn;
-	for (i = 0; i < left; i++) {
-		unsigned long mfn;
+	for (ident_pfn_iter = start_pfn, remap_pfn_iter = remap_pfn;
+	     ident_pfn_iter < ident_end_pfn;
+	     ident_pfn_iter += REMAP_SIZE, remap_pfn_iter += REMAP_SIZE) {
+		chunk = (left < REMAP_SIZE) ? left : REMAP_SIZE;
 
-		/* These two checks move from the start to end boundaries */
-		if (ident_boundary_pfn == ident_start_pfn_align)
-			ident_boundary_pfn = ident_pfn_iter;
-		if (remap_boundary_pfn == remap_start_pfn_align)
-			remap_boundary_pfn = remap_pfn_iter;
+		/* Map first pfn to xen_remap_buf */
+		mfn = pfn_to_mfn(ident_pfn_iter);
+		set_pte_mfn(buf, mfn, PAGE_KERNEL);
 
-		/* Check we aren't past the end */
-		BUG_ON(ident_boundary_pfn >= start_pfn + size);
-		BUG_ON(remap_boundary_pfn >= remap_pfn + size);
+		/* Save mapping information in page */
+		xen_remap_buf.next_area_mfn = xen_remap_mfn;
+		xen_remap_buf.target_pfn = remap_pfn_iter;
+		xen_remap_buf.size = chunk;
+		for (i = 0; i < chunk; i++)
+			xen_remap_buf.mfns[i] = pfn_to_mfn(ident_pfn_iter + i);
 
-		mfn = pfn_to_mfn(ident_boundary_pfn);
+		/* Put remap buf into list. */
+		xen_remap_mfn = mfn;
 
-		if (!xen_update_mem_tables(remap_boundary_pfn, mfn)) {
-			WARN(1, "Failed to update mem mapping for pfn=%ld mfn=%ld\n",
-				remap_pfn_iter + i, mfn);
-			return 0;
-		}
-		remap_cnt++;
-
-		ident_boundary_pfn++;
-		remap_boundary_pfn++;
-	}
+		/* Set identity map */
+		ident_cnt += set_phys_range_identity(ident_pfn_iter,
+			ident_pfn_iter + chunk);
 
-	/* Finish up the identity map */
-	if (ident_start_pfn_align >= ident_end_pfn_align) {
-		/*
-                 * In this case we have an identity range which does not span an
-                 * aligned block so everything needs to be identity mapped here.
-                 * If we didn't check this we might remap too many pages since
-                 * the align boundaries are not meaningful in this case.
-	         */
-		ident_cnt += set_phys_range_identity(start_pfn,
-			start_pfn + size);
-	} else {
-		/* Remapped above so check each end of the chunk */
-		if (start_pfn < ident_start_pfn_align)
-			ident_cnt += set_phys_range_identity(start_pfn,
-				ident_start_pfn_align);
-		if (start_pfn + size > ident_pfn_iter)
-			ident_cnt += set_phys_range_identity(ident_pfn_iter,
-				start_pfn + size);
+		left -= chunk;
 	}
 
-	BUG_ON(ident_cnt != size);
-	BUG_ON(remap_cnt != size);
-
-	return size;
+	/* Restore old xen_remap_buf mapping */
+	set_pte_mfn(buf, mfn_save, PAGE_KERNEL);
 }
 
 /*
@@ -396,8 +317,7 @@ static unsigned long __init xen_do_set_identity_and_remap_chunk(
 static unsigned long __init xen_set_identity_and_remap_chunk(
         const struct e820entry *list, size_t map_size, unsigned long start_pfn,
 	unsigned long end_pfn, unsigned long nr_pages, unsigned long remap_pfn,
-	unsigned long *identity, unsigned long *remapped,
-	unsigned long *released)
+	unsigned long *identity, unsigned long *released)
 {
 	unsigned long pfn;
 	unsigned long i = 0;
@@ -431,19 +351,12 @@ static unsigned long __init xen_set_identity_and_remap_chunk(
 		if (size > remap_range_size)
 			size = remap_range_size;
 
-		if (!xen_do_set_identity_and_remap_chunk(cur_pfn, size, remap_pfn)) {
-			WARN(1, "Failed to remap 1:1 memory cur_pfn=%ld size=%ld remap_pfn=%ld\n",
-				cur_pfn, size, remap_pfn);
-			xen_set_identity_and_release_chunk(cur_pfn,
-				cur_pfn + left, nr_pages, identity, released);
-			break;
-		}
+		xen_do_set_identity_and_remap_chunk(cur_pfn, size, remap_pfn);
 
 		/* Update variables to reflect new mappings. */
 		i += size;
 		remap_pfn += size;
 		*identity += size;
-		*remapped += size;
 	}
 
 	/*
@@ -464,7 +377,6 @@ static unsigned long __init xen_set_identity_and_remap(
 {
 	phys_addr_t start = 0;
 	unsigned long identity = 0;
-	unsigned long remapped = 0;
 	unsigned long last_pfn = nr_pages;
 	const struct e820entry *entry;
 	unsigned long num_released = 0;
@@ -494,8 +406,7 @@ static unsigned long __init xen_set_identity_and_remap(
 				last_pfn = xen_set_identity_and_remap_chunk(
 						list, map_size, start_pfn,
 						end_pfn, nr_pages, last_pfn,
-						&identity, &remapped,
-						&num_released);
+						&identity, &num_released);
 			start = end;
 		}
 	}
@@ -503,12 +414,65 @@ static unsigned long __init xen_set_identity_and_remap(
 	*released = num_released;
 
 	pr_info("Set %ld page(s) to 1-1 mapping\n", identity);
-	pr_info("Remapped %ld page(s), last_pfn=%ld\n", remapped,
-		last_pfn);
 	pr_info("Released %ld page(s)\n", num_released);
 
 	return last_pfn;
 }
+
+/*
+ * Remap the memory prepared in xen_do_set_identity_and_remap_chunk().
+ * The remap information (which mfn remap to which pfn) is contained in the
+ * to be remapped memory itself in a linked list anchored at xen_remap_mfn.
+ * This scheme allows to remap the different chunks in arbitrary order while
+ * the resulting mapping will be independant from the order.
+ */
+void __init xen_remap_memory(void)
+{
+	unsigned long buf = (unsigned long)&xen_remap_buf;
+	unsigned long mfn_save, mfn, pfn;
+	unsigned long remapped = 0;
+	unsigned int i;
+	unsigned long pfn_s = ~0UL;
+	unsigned long len = 0;
+
+	mfn_save = virt_to_mfn(buf);
+
+	while (xen_remap_mfn != INVALID_P2M_ENTRY) {
+		/* Map the remap information */
+		set_pte_mfn(buf, xen_remap_mfn, PAGE_KERNEL);
+
+		BUG_ON(xen_remap_mfn != xen_remap_buf.mfns[0]);
+
+		pfn = xen_remap_buf.target_pfn;
+		for (i = 0; i < xen_remap_buf.size; i++) {
+			mfn = xen_remap_buf.mfns[i];
+			xen_update_mem_tables(pfn, mfn);
+			remapped++;
+			pfn++;
+		}
+		if (pfn_s == ~0UL || pfn == pfn_s) {
+			pfn_s = xen_remap_buf.target_pfn;
+			len += xen_remap_buf.size;
+		} else if (pfn_s + len == xen_remap_buf.target_pfn) {
+			len += xen_remap_buf.size;
+		} else {
+			memblock_free(PFN_PHYS(pfn_s), PFN_PHYS(len));
+			pfn_s = xen_remap_buf.target_pfn;
+			len = xen_remap_buf.size;
+		}
+
+		mfn = xen_remap_mfn;
+		xen_remap_mfn = xen_remap_buf.next_area_mfn;
+	}
+
+	if (pfn_s != ~0UL && len)
+		memblock_free(PFN_PHYS(pfn_s), PFN_PHYS(len));
+
+	set_pte_mfn(buf, mfn_save, PAGE_KERNEL);
+
+	pr_info("Remapped %ld page(s)\n", remapped);
+}
+
 static unsigned long __init xen_get_max_pages(void)
 {
 	unsigned long max_pages = MAX_DOMAIN_PAGES;
@@ -616,7 +580,8 @@ char * __init xen_memory_setup(void)
 		extra_pages += max_pages - max_pfn;
 
 	/*
-	 * Set identity map on non-RAM pages and remap the underlying RAM.
+	 * Set identity map on non-RAM pages and prepare remapping the
+	 * underlying RAM.
 	 */
 	last_pfn = xen_set_identity_and_remap(map, memmap.nr_entries, max_pfn,
 					      &xen_released_pages);

commit 1ea644c8f93f3435760d8b88c25d56475d8b7778
Author: Martin Kelly <martin@martingkelly.com>
Date:   Thu Oct 16 20:48:11 2014 -0700

    x86/xen: panic on bad Xen-provided memory map
    
    Panic if Xen provides a memory map with 0 entries. Although this is
    unlikely, it is better to catch the error at the point of seeing the map
    than later on as a symptom of some other crash.
    
    Signed-off-by: Martin Kelly <martkell@amazon.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index af7216128d93..29834b3fd87f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -595,6 +595,7 @@ char * __init xen_memory_setup(void)
 		rc = 0;
 	}
 	BUG_ON(rc);
+	BUG_ON(memmap.nr_entries == 0);
 
 	/*
 	 * Xen won't allow a 1:1 mapping to be created to UNUSABLE

commit 4fbb67e3c87b806ad54445a1b4a9c6bde2359c98
Author: Matt Rushton <mvrushton@gmail.com>
Date:   Mon Aug 11 11:57:57 2014 -0700

    xen/setup: Remap Xen Identity Mapped RAM
    
    Instead of ballooning up and down dom0 memory this remaps the existing mfns
    that were replaced by the identity map. The reason for this is that the
    existing implementation ballooned memory up and and down which caused dom0
    to have discontiguous pages. In some cases this resulted in the use of bounce
    buffers which reduced network I/O performance significantly. This change will
    honor the existing order of the pages with the exception of some boundary
    conditions.
    
    To do this we need to update both the Linux p2m table and the Xen m2p table.
    Particular care must be taken when updating the p2m table since it's important
    to limit table memory consumption and reuse the existing leaf pages which get
    freed when an entire leaf page is set to the identity map. To implement this,
    mapping updates are grouped into blocks with table entries getting cached
    temporarily and then released.
    
    On my test system before:
    Total pages: 2105014
    Total contiguous: 1640635
    
    After:
    Total pages: 2105014
    Total contiguous: 2098904
    
    Signed-off-by: Matthew Rushton <mrushton@amazon.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 2e555163c2fe..af7216128d93 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -29,6 +29,7 @@
 #include <xen/features.h>
 #include "xen-ops.h"
 #include "vdso.h"
+#include "p2m.h"
 
 /* These are code, but not functions.  Defined in entry.S */
 extern const char xen_hypervisor_callback[];
@@ -46,6 +47,9 @@ struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 /* Number of pages released from the initial allocation. */
 unsigned long xen_released_pages;
 
+/* Buffer used to remap identity mapped pages */
+unsigned long xen_remap_buf[P2M_PER_PAGE] __initdata;
+
 /* 
  * The maximum amount of extra memory compared to the base size.  The
  * main scaling factor is the size of struct page.  At extreme ratios
@@ -151,107 +155,325 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 	return len;
 }
 
-static unsigned long __init xen_release_chunk(unsigned long start,
-					      unsigned long end)
-{
-	return xen_do_chunk(start, end, true);
-}
-
-static unsigned long __init xen_populate_chunk(
+/*
+ * Finds the next RAM pfn available in the E820 map after min_pfn.
+ * This function updates min_pfn with the pfn found and returns
+ * the size of that range or zero if not found.
+ */
+static unsigned long __init xen_find_pfn_range(
 	const struct e820entry *list, size_t map_size,
-	unsigned long max_pfn, unsigned long *last_pfn,
-	unsigned long credits_left)
+	unsigned long *min_pfn)
 {
 	const struct e820entry *entry;
 	unsigned int i;
 	unsigned long done = 0;
-	unsigned long dest_pfn;
 
 	for (i = 0, entry = list; i < map_size; i++, entry++) {
 		unsigned long s_pfn;
 		unsigned long e_pfn;
-		unsigned long pfns;
-		long capacity;
-
-		if (credits_left <= 0)
-			break;
 
 		if (entry->type != E820_RAM)
 			continue;
 
 		e_pfn = PFN_DOWN(entry->addr + entry->size);
 
-		/* We only care about E820 after the xen_start_info->nr_pages */
-		if (e_pfn <= max_pfn)
+		/* We only care about E820 after this */
+		if (e_pfn < *min_pfn)
 			continue;
 
 		s_pfn = PFN_UP(entry->addr);
-		/* If the E820 falls within the nr_pages, we want to start
-		 * at the nr_pages PFN.
-		 * If that would mean going past the E820 entry, skip it
+
+		/* If min_pfn falls within the E820 entry, we want to start
+		 * at the min_pfn PFN.
 		 */
-		if (s_pfn <= max_pfn) {
-			capacity = e_pfn - max_pfn;
-			dest_pfn = max_pfn;
+		if (s_pfn <= *min_pfn) {
+			done = e_pfn - *min_pfn;
 		} else {
-			capacity = e_pfn - s_pfn;
-			dest_pfn = s_pfn;
+			done = e_pfn - s_pfn;
+			*min_pfn = s_pfn;
 		}
+		break;
+	}
 
-		if (credits_left < capacity)
-			capacity = credits_left;
+	return done;
+}
 
-		pfns = xen_do_chunk(dest_pfn, dest_pfn + capacity, false);
-		done += pfns;
-		*last_pfn = (dest_pfn + pfns);
-		if (pfns < capacity)
-			break;
-		credits_left -= pfns;
+/*
+ * This releases a chunk of memory and then does the identity map. It's used as
+ * as a fallback if the remapping fails.
+ */
+static void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,
+	unsigned long end_pfn, unsigned long nr_pages, unsigned long *identity,
+	unsigned long *released)
+{
+	WARN_ON(start_pfn > end_pfn);
+
+	/* Need to release pages first */
+	*released += xen_do_chunk(start_pfn, min(end_pfn, nr_pages), true);
+	*identity += set_phys_range_identity(start_pfn, end_pfn);
+}
+
+/*
+ * Helper function to update both the p2m and m2p tables.
+ */
+static unsigned long __init xen_update_mem_tables(unsigned long pfn,
+						  unsigned long mfn)
+{
+	struct mmu_update update = {
+		.ptr = ((unsigned long long)mfn << PAGE_SHIFT) | MMU_MACHPHYS_UPDATE,
+		.val = pfn
+	};
+
+	/* Update p2m */
+	if (!early_set_phys_to_machine(pfn, mfn)) {
+		WARN(1, "Failed to set p2m mapping for pfn=%ld mfn=%ld\n",
+		     pfn, mfn);
+		return false;
 	}
-	return done;
+
+	/* Update m2p */
+	if (HYPERVISOR_mmu_update(&update, 1, NULL, DOMID_SELF) < 0) {
+		WARN(1, "Failed to set m2p mapping for mfn=%ld pfn=%ld\n",
+		     mfn, pfn);
+		return false;
+	}
+
+	return true;
 }
 
-static void __init xen_set_identity_and_release_chunk(
-	unsigned long start_pfn, unsigned long end_pfn, unsigned long nr_pages,
-	unsigned long *released, unsigned long *identity)
+/*
+ * This function updates the p2m and m2p tables with an identity map from
+ * start_pfn to start_pfn+size and remaps the underlying RAM of the original
+ * allocation at remap_pfn. It must do so carefully in P2M_PER_PAGE sized blocks
+ * to not exhaust the reserved brk space. Doing it in properly aligned blocks
+ * ensures we only allocate the minimum required leaf pages in the p2m table. It
+ * copies the existing mfns from the p2m table under the 1:1 map, overwrites
+ * them with the identity map and then updates the p2m and m2p tables with the
+ * remapped memory.
+ */
+static unsigned long __init xen_do_set_identity_and_remap_chunk(
+        unsigned long start_pfn, unsigned long size, unsigned long remap_pfn)
 {
-	unsigned long pfn;
+	unsigned long ident_pfn_iter, remap_pfn_iter;
+	unsigned long ident_start_pfn_align, remap_start_pfn_align;
+	unsigned long ident_end_pfn_align, remap_end_pfn_align;
+	unsigned long ident_boundary_pfn, remap_boundary_pfn;
+	unsigned long ident_cnt = 0;
+	unsigned long remap_cnt = 0;
+	unsigned long left = size;
+	unsigned long mod;
+	int i;
+
+	WARN_ON(size == 0);
+
+	BUG_ON(xen_feature(XENFEAT_auto_translated_physmap));
 
 	/*
-	 * If the PFNs are currently mapped, clear the mappings
-	 * (except for the ISA region which must be 1:1 mapped) to
-	 * release the refcounts (in Xen) on the original frames.
+	 * Determine the proper alignment to remap memory in P2M_PER_PAGE sized
+	 * blocks. We need to keep track of both the existing pfn mapping and
+	 * the new pfn remapping.
 	 */
-	for (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++) {
-		pte_t pte = __pte_ma(0);
+	mod = start_pfn % P2M_PER_PAGE;
+	ident_start_pfn_align =
+		mod ? (start_pfn - mod + P2M_PER_PAGE) : start_pfn;
+	mod = remap_pfn % P2M_PER_PAGE;
+	remap_start_pfn_align =
+		mod ? (remap_pfn - mod + P2M_PER_PAGE) : remap_pfn;
+	mod = (start_pfn + size) % P2M_PER_PAGE;
+	ident_end_pfn_align = start_pfn + size - mod;
+	mod = (remap_pfn + size) % P2M_PER_PAGE;
+	remap_end_pfn_align = remap_pfn + size - mod;
+
+	/* Iterate over each p2m leaf node in each range */
+	for (ident_pfn_iter = ident_start_pfn_align, remap_pfn_iter = remap_start_pfn_align;
+	     ident_pfn_iter < ident_end_pfn_align && remap_pfn_iter < remap_end_pfn_align;
+	     ident_pfn_iter += P2M_PER_PAGE, remap_pfn_iter += P2M_PER_PAGE) {
+		/* Check we aren't past the end */
+		BUG_ON(ident_pfn_iter + P2M_PER_PAGE > start_pfn + size);
+		BUG_ON(remap_pfn_iter + P2M_PER_PAGE > remap_pfn + size);
+
+		/* Save p2m mappings */
+		for (i = 0; i < P2M_PER_PAGE; i++)
+			xen_remap_buf[i] = pfn_to_mfn(ident_pfn_iter + i);
+
+		/* Set identity map which will free a p2m leaf */
+		ident_cnt += set_phys_range_identity(ident_pfn_iter,
+			ident_pfn_iter + P2M_PER_PAGE);
+
+#ifdef DEBUG
+		/* Helps verify a p2m leaf has been freed */
+		for (i = 0; i < P2M_PER_PAGE; i++) {
+			unsigned int pfn = ident_pfn_iter + i;
+			BUG_ON(pfn_to_mfn(pfn) != pfn);
+		}
+#endif
+		/* Now remap memory */
+		for (i = 0; i < P2M_PER_PAGE; i++) {
+			unsigned long mfn = xen_remap_buf[i];
+
+			/* This will use the p2m leaf freed above */
+			if (!xen_update_mem_tables(remap_pfn_iter + i, mfn)) {
+				WARN(1, "Failed to update mem mapping for pfn=%ld mfn=%ld\n",
+					remap_pfn_iter + i, mfn);
+				return 0;
+			}
+
+			remap_cnt++;
+		}
 
-		if (pfn < PFN_UP(ISA_END_ADDRESS))
-			pte = mfn_pte(pfn, PAGE_KERNEL_IO);
+		left -= P2M_PER_PAGE;
+	}
 
-		(void)HYPERVISOR_update_va_mapping(
-			(unsigned long)__va(pfn << PAGE_SHIFT), pte, 0);
+	/* Max boundary space possible */
+	BUG_ON(left > (P2M_PER_PAGE - 1) * 2);
+
+	/* Now handle the boundary conditions */
+	ident_boundary_pfn = start_pfn;
+	remap_boundary_pfn = remap_pfn;
+	for (i = 0; i < left; i++) {
+		unsigned long mfn;
+
+		/* These two checks move from the start to end boundaries */
+		if (ident_boundary_pfn == ident_start_pfn_align)
+			ident_boundary_pfn = ident_pfn_iter;
+		if (remap_boundary_pfn == remap_start_pfn_align)
+			remap_boundary_pfn = remap_pfn_iter;
+
+		/* Check we aren't past the end */
+		BUG_ON(ident_boundary_pfn >= start_pfn + size);
+		BUG_ON(remap_boundary_pfn >= remap_pfn + size);
+
+		mfn = pfn_to_mfn(ident_boundary_pfn);
+
+		if (!xen_update_mem_tables(remap_boundary_pfn, mfn)) {
+			WARN(1, "Failed to update mem mapping for pfn=%ld mfn=%ld\n",
+				remap_pfn_iter + i, mfn);
+			return 0;
+		}
+		remap_cnt++;
+
+		ident_boundary_pfn++;
+		remap_boundary_pfn++;
 	}
 
-	if (start_pfn < nr_pages)
-		*released += xen_release_chunk(
-			start_pfn, min(end_pfn, nr_pages));
+	/* Finish up the identity map */
+	if (ident_start_pfn_align >= ident_end_pfn_align) {
+		/*
+                 * In this case we have an identity range which does not span an
+                 * aligned block so everything needs to be identity mapped here.
+                 * If we didn't check this we might remap too many pages since
+                 * the align boundaries are not meaningful in this case.
+	         */
+		ident_cnt += set_phys_range_identity(start_pfn,
+			start_pfn + size);
+	} else {
+		/* Remapped above so check each end of the chunk */
+		if (start_pfn < ident_start_pfn_align)
+			ident_cnt += set_phys_range_identity(start_pfn,
+				ident_start_pfn_align);
+		if (start_pfn + size > ident_pfn_iter)
+			ident_cnt += set_phys_range_identity(ident_pfn_iter,
+				start_pfn + size);
+	}
 
-	*identity += set_phys_range_identity(start_pfn, end_pfn);
+	BUG_ON(ident_cnt != size);
+	BUG_ON(remap_cnt != size);
+
+	return size;
 }
 
-static unsigned long __init xen_set_identity_and_release(
-	const struct e820entry *list, size_t map_size, unsigned long nr_pages)
+/*
+ * This function takes a contiguous pfn range that needs to be identity mapped
+ * and:
+ *
+ *  1) Finds a new range of pfns to use to remap based on E820 and remap_pfn.
+ *  2) Calls the do_ function to actually do the mapping/remapping work.
+ *
+ * The goal is to not allocate additional memory but to remap the existing
+ * pages. In the case of an error the underlying memory is simply released back
+ * to Xen and not remapped.
+ */
+static unsigned long __init xen_set_identity_and_remap_chunk(
+        const struct e820entry *list, size_t map_size, unsigned long start_pfn,
+	unsigned long end_pfn, unsigned long nr_pages, unsigned long remap_pfn,
+	unsigned long *identity, unsigned long *remapped,
+	unsigned long *released)
+{
+	unsigned long pfn;
+	unsigned long i = 0;
+	unsigned long n = end_pfn - start_pfn;
+
+	while (i < n) {
+		unsigned long cur_pfn = start_pfn + i;
+		unsigned long left = n - i;
+		unsigned long size = left;
+		unsigned long remap_range_size;
+
+		/* Do not remap pages beyond the current allocation */
+		if (cur_pfn >= nr_pages) {
+			/* Identity map remaining pages */
+			*identity += set_phys_range_identity(cur_pfn,
+				cur_pfn + size);
+			break;
+		}
+		if (cur_pfn + size > nr_pages)
+			size = nr_pages - cur_pfn;
+
+		remap_range_size = xen_find_pfn_range(list, map_size,
+						      &remap_pfn);
+		if (!remap_range_size) {
+			pr_warning("Unable to find available pfn range, not remapping identity pages\n");
+			xen_set_identity_and_release_chunk(cur_pfn,
+				cur_pfn + left, nr_pages, identity, released);
+			break;
+		}
+		/* Adjust size to fit in current e820 RAM region */
+		if (size > remap_range_size)
+			size = remap_range_size;
+
+		if (!xen_do_set_identity_and_remap_chunk(cur_pfn, size, remap_pfn)) {
+			WARN(1, "Failed to remap 1:1 memory cur_pfn=%ld size=%ld remap_pfn=%ld\n",
+				cur_pfn, size, remap_pfn);
+			xen_set_identity_and_release_chunk(cur_pfn,
+				cur_pfn + left, nr_pages, identity, released);
+			break;
+		}
+
+		/* Update variables to reflect new mappings. */
+		i += size;
+		remap_pfn += size;
+		*identity += size;
+		*remapped += size;
+	}
+
+	/*
+	 * If the PFNs are currently mapped, the VA mapping also needs
+	 * to be updated to be 1:1.
+	 */
+	for (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++)
+		(void)HYPERVISOR_update_va_mapping(
+			(unsigned long)__va(pfn << PAGE_SHIFT),
+			mfn_pte(pfn, PAGE_KERNEL_IO), 0);
+
+	return remap_pfn;
+}
+
+static unsigned long __init xen_set_identity_and_remap(
+	const struct e820entry *list, size_t map_size, unsigned long nr_pages,
+	unsigned long *released)
 {
 	phys_addr_t start = 0;
-	unsigned long released = 0;
 	unsigned long identity = 0;
+	unsigned long remapped = 0;
+	unsigned long last_pfn = nr_pages;
 	const struct e820entry *entry;
+	unsigned long num_released = 0;
 	int i;
 
 	/*
 	 * Combine non-RAM regions and gaps until a RAM region (or the
 	 * end of the map) is reached, then set the 1:1 map and
-	 * release the pages (if available) in those non-RAM regions.
+	 * remap the memory in those non-RAM regions.
 	 *
 	 * The combined non-RAM regions are rounded to a whole number
 	 * of pages so any partial pages are accessible via the 1:1
@@ -269,22 +491,24 @@ static unsigned long __init xen_set_identity_and_release(
 				end_pfn = PFN_UP(entry->addr);
 
 			if (start_pfn < end_pfn)
-				xen_set_identity_and_release_chunk(
-					start_pfn, end_pfn, nr_pages,
-					&released, &identity);
-
+				last_pfn = xen_set_identity_and_remap_chunk(
+						list, map_size, start_pfn,
+						end_pfn, nr_pages, last_pfn,
+						&identity, &remapped,
+						&num_released);
 			start = end;
 		}
 	}
 
-	if (released)
-		printk(KERN_INFO "Released %lu pages of unused memory\n", released);
-	if (identity)
-		printk(KERN_INFO "Set %ld page(s) to 1-1 mapping\n", identity);
+	*released = num_released;
 
-	return released;
-}
+	pr_info("Set %ld page(s) to 1-1 mapping\n", identity);
+	pr_info("Remapped %ld page(s), last_pfn=%ld\n", remapped,
+		last_pfn);
+	pr_info("Released %ld page(s)\n", num_released);
 
+	return last_pfn;
+}
 static unsigned long __init xen_get_max_pages(void)
 {
 	unsigned long max_pages = MAX_DOMAIN_PAGES;
@@ -347,7 +571,6 @@ char * __init xen_memory_setup(void)
 	unsigned long max_pages;
 	unsigned long last_pfn = 0;
 	unsigned long extra_pages = 0;
-	unsigned long populated;
 	int i;
 	int op;
 
@@ -392,20 +615,11 @@ char * __init xen_memory_setup(void)
 		extra_pages += max_pages - max_pfn;
 
 	/*
-	 * Set P2M for all non-RAM pages and E820 gaps to be identity
-	 * type PFNs.  Any RAM pages that would be made inaccesible by
-	 * this are first released.
+	 * Set identity map on non-RAM pages and remap the underlying RAM.
 	 */
-	xen_released_pages = xen_set_identity_and_release(
-		map, memmap.nr_entries, max_pfn);
-
-	/*
-	 * Populate back the non-RAM pages and E820 gaps that had been
-	 * released. */
-	populated = xen_populate_chunk(map, memmap.nr_entries,
-			max_pfn, &last_pfn, xen_released_pages);
+	last_pfn = xen_set_identity_and_remap(map, memmap.nr_entries, max_pfn,
+					      &xen_released_pages);
 
-	xen_released_pages -= populated;
 	extra_pages += xen_released_pages;
 
 	if (last_pfn > max_pfn) {

commit 3d09c623947a855ab9a21267c31f926cafa2c5bb
Merge: 92b944170d67 13cd36a37a06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 19 07:53:27 2014 -1000

    Merge tag 'stable/for-linus-3.16-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen fixes from David Vrabel:
     "Xen regression and PVH fixes for 3.16-rc1
    
       - fix dom0 PVH memory setup on latest unstable Xen releases
       - fix 64-bit x86 PV guest boot failure on Xen 3.1 and earlier
       - fix resume regression on non-PV (auto-translated physmap) guests"
    
    * tag 'stable/for-linus-3.16-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/grant-table: fix suspend for non-PV guests
      x86/xen: no need to explicitly register an NMI callback
      Revert "xen/pvh: Update E820 to work with PVH (v2)"
      x86/xen: fix memory setup for PVH dom0

commit ea9f9274bf4337ba7cbab241c780487651642d63
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jun 16 13:07:00 2014 +0200

    x86/xen: no need to explicitly register an NMI callback
    
    Remove xen_enable_nmi() to fix a 64-bit guest crash when registering
    the NMI callback on Xen 3.1 and earlier.
    
    It's not needed since the NMI callback is set by a set_trap_table
    hypercall (in xen_load_idt() or xen_write_idt_entry()).
    
    It's also broken since it only set the current VCPU's callback.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reported-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Tested-by: Vitaly Kuznetsov <vkuznets@redhat.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index bdd22ece586f..e366b9855376 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -594,13 +594,7 @@ void xen_enable_syscall(void)
 	}
 #endif /* CONFIG_X86_64 */
 }
-void xen_enable_nmi(void)
-{
-#ifdef CONFIG_X86_64
-	if (register_callback(CALLBACKTYPE_nmi, (char *)nmi))
-		BUG();
-#endif
-}
+
 void __init xen_pvmmu_arch_setup(void)
 {
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_4gb_segments);
@@ -615,7 +609,6 @@ void __init xen_pvmmu_arch_setup(void)
 
 	xen_enable_sysenter();
 	xen_enable_syscall();
-	xen_enable_nmi();
 }
 
 /* This function is not called for HVM domains */

commit a0abcf2e8f8017051830f738ac1bf5ef42703243
Merge: 2071b3e34fd3 c191920f737a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 5 08:05:29 2014 -0700

    Merge branch 'x86/vdso' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into next
    
    Pull x86 cdso updates from Peter Anvin:
     "Vdso cleanups and improvements largely from Andy Lutomirski.  This
      makes the vdso a lot less ''special''"
    
    * 'x86/vdso' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vdso, build: Make LE access macros clearer, host-safe
      x86/vdso, build: Fix cross-compilation from big-endian architectures
      x86/vdso, build: When vdso2c fails, unlink the output
      x86, vdso: Fix an OOPS accessing the HPET mapping w/o an HPET
      x86, mm: Replace arch_vma_name with vm_ops->name for vsyscalls
      x86, mm: Improve _install_special_mapping and fix x86 vdso naming
      mm, fs: Add vm_ops->name as an alternative to arch_vma_name
      x86, vdso: Fix an OOPS accessing the HPET mapping w/o an HPET
      x86, vdso: Remove vestiges of VDSO_PRELINK and some outdated comments
      x86, vdso: Move the vvar and hpet mappings next to the 64-bit vDSO
      x86, vdso: Move the 32-bit vdso special pages after the text
      x86, vdso: Reimplement vdso.so preparation in build-time C
      x86, vdso: Move syscall and sysenter setup into kernel/cpu/common.c
      x86, vdso: Clean up 32-bit vs 64-bit vdso params
      x86, mm: Ensure correct alignment of the fixmap

commit 562658f3dc7c95c4a918841731e3e91c9fced4d3
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jun 2 17:53:06 2014 +0100

    Revert "xen/pvh: Update E820 to work with PVH (v2)"
    
    This reverts commit 9103bb0f8240b2a55aac3ff7ecba9c7dcf66b08b.
    
    Now than xen_memory_setup() is not called for auto-translated guests,
    we can remove this commit.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>
    Tested-by: Roger Pau Monné <roger.pau@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 37cfbfacd59a..bdd22ece586f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -27,7 +27,6 @@
 #include <xen/interface/memory.h>
 #include <xen/interface/physdev.h>
 #include <xen/features.h>
-#include "mmu.h"
 #include "xen-ops.h"
 #include "vdso.h"
 
@@ -82,9 +81,6 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 
 	memblock_reserve(start, size);
 
-	if (xen_feature(XENFEAT_auto_translated_physmap))
-		return;
-
 	xen_max_p2m_pfn = PFN_DOWN(start + size);
 	for (pfn = PFN_DOWN(start); pfn < xen_max_p2m_pfn; pfn++) {
 		unsigned long mfn = pfn_to_mfn(pfn);
@@ -107,7 +103,6 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 		.domid        = DOMID_SELF
 	};
 	unsigned long len = 0;
-	int xlated_phys = xen_feature(XENFEAT_auto_translated_physmap);
 	unsigned long pfn;
 	int ret;
 
@@ -121,7 +116,7 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 				continue;
 			frame = mfn;
 		} else {
-			if (!xlated_phys && mfn != INVALID_P2M_ENTRY)
+			if (mfn != INVALID_P2M_ENTRY)
 				continue;
 			frame = pfn;
 		}
@@ -159,13 +154,6 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 static unsigned long __init xen_release_chunk(unsigned long start,
 					      unsigned long end)
 {
-	/*
-	 * Xen already ballooned out the E820 non RAM regions for us
-	 * and set them up properly in EPT.
-	 */
-	if (xen_feature(XENFEAT_auto_translated_physmap))
-		return end - start;
-
 	return xen_do_chunk(start, end, true);
 }
 
@@ -234,13 +222,7 @@ static void __init xen_set_identity_and_release_chunk(
 	 * (except for the ISA region which must be 1:1 mapped) to
 	 * release the refcounts (in Xen) on the original frames.
 	 */
-
-	/*
-	 * PVH E820 matches the hypervisor's P2M which means we need to
-	 * account for the proper values of *release and *identity.
-	 */
-	for (pfn = start_pfn; !xen_feature(XENFEAT_auto_translated_physmap) &&
-	     pfn <= max_pfn_mapped && pfn < end_pfn; pfn++) {
+	for (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++) {
 		pte_t pte = __pte_ma(0);
 
 		if (pfn < PFN_UP(ISA_END_ADDRESS))

commit abacaadc4144a8849782cc0917a624a7114ffbb1
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jun 2 17:58:01 2014 +0100

    x86/xen: fix memory setup for PVH dom0
    
    Since af06d66ee32b (x86: fix setup of PVH Dom0 memory map) in Xen, PVH
    dom0 need only use the memory memory provided by Xen which has already
    setup all the correct holes.
    
    xen_memory_setup() then ends up being trivial for a PVH guest so
    introduce a new function (xen_auto_xlated_memory_setup()).
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>
    Tested-by: Roger Pau Monné <roger.pau@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 210426a26cc0..37cfbfacd59a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -517,6 +517,35 @@ char * __init xen_memory_setup(void)
 	return "Xen";
 }
 
+/*
+ * Machine specific memory setup for auto-translated guests.
+ */
+char * __init xen_auto_xlated_memory_setup(void)
+{
+	static struct e820entry map[E820MAX] __initdata;
+
+	struct xen_memory_map memmap;
+	int i;
+	int rc;
+
+	memmap.nr_entries = E820MAX;
+	set_xen_guest_handle(memmap.buffer, map);
+
+	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
+	if (rc < 0)
+		panic("No memory map (%d)\n", rc);
+
+	sanitize_e820_map(map, ARRAY_SIZE(map), &memmap.nr_entries);
+
+	for (i = 0; i < memmap.nr_entries; i++)
+		e820_add_region(map[i].addr, map[i].size, map[i].type);
+
+	memblock_reserve(__pa(xen_start_info->mfn_list),
+			 xen_start_info->pt_base - xen_start_info->mfn_list);
+
+	return "Xen";
+}
+
 /*
  * Set the bit indicating "nosegneg" library variants should be used.
  * We only need to bother in pure 32-bit mode; compat 32-bit processes

commit 25b884a83d487fd62c3de7ac1ab5549979188482
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Jan 3 15:46:10 2014 +0000

    x86/xen: set regions above the end of RAM as 1:1
    
    PCI devices may have BARs located above the end of RAM so mark such
    frames as identity frames in the p2m (instead of the default of
    missing).
    
    PFNs outside the p2m (above MAX_P2M_PFN) are also considered to be
    identity frames for the same reason.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 2afe55e21d59..210426a26cc0 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -468,6 +468,15 @@ char * __init xen_memory_setup(void)
 			i++;
 	}
 
+	/*
+	 * Set the rest as identity mapped, in case PCI BARs are
+	 * located here.
+	 *
+	 * PFNs above MAX_P2M_PFN are considered identity mapped as
+	 * well.
+	 */
+	set_phys_range_identity(map[i-1].addr / PAGE_SIZE, ~0ul);
+
 	/*
 	 * In domU, the ISA region is normal, usable memory, but we
 	 * reserve ISA memory anyway because too many things poke

commit 2dcc9a3de1d7f77bb4dbc108358a75776328e887
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Tue Jan 7 11:36:53 2014 +0000

    x86/xen: only warn once if bad MFNs are found during setup
    
    In xen_add_extra_mem(), if the WARN() checks for bad MFNs trigger it is
    likely that they will trigger at lot, spamming the log.
    
    Use WARN_ONCE() instead.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 0982233b9b84..2afe55e21d59 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -89,10 +89,10 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 	for (pfn = PFN_DOWN(start); pfn < xen_max_p2m_pfn; pfn++) {
 		unsigned long mfn = pfn_to_mfn(pfn);
 
-		if (WARN(mfn == pfn, "Trying to over-write 1-1 mapping (pfn: %lx)\n", pfn))
+		if (WARN_ONCE(mfn == pfn, "Trying to over-write 1-1 mapping (pfn: %lx)\n", pfn))
 			continue;
-		WARN(mfn != INVALID_P2M_ENTRY, "Trying to remove %lx which has %lx mfn!\n",
-			pfn, mfn);
+		WARN_ONCE(mfn != INVALID_P2M_ENTRY, "Trying to remove %lx which has %lx mfn!\n",
+			  pfn, mfn);
 
 		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 	}

commit 6f121e548f83674ab4920a4e60afb58d4f61b829
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon May 5 12:19:34 2014 -0700

    x86, vdso: Reimplement vdso.so preparation in build-time C
    
    Currently, vdso.so files are prepared and analyzed by a combination
    of objcopy, nm, some linker script tricks, and some simple ELF
    parsers in the kernel.  Replace all of that with plain C code that
    runs at build time.
    
    All five vdso images now generate .c files that are compiled and
    linked in to the kernel image.
    
    This should cause only one userspace-visible change: the loaded vDSO
    images are stripped more heavily than they used to be.  Everything
    outside the loadable segment is dropped.  In particular, this causes
    the section table and section name strings to be missing.  This
    should be fine: real dynamic loaders don't load or inspect these
    tables anyway.  The result is roughly equivalent to eu-strip's
    --strip-sections option.
    
    The purpose of this change is to enable the vvar and hpet mappings
    to be moved to the page following the vDSO load segment.  Currently,
    it is possible for the section table to extend into the page after
    the load segment, so, if we map it, it risks overlapping the vvar or
    hpet page.  This happens whenever the load segment is just under a
    multiple of PAGE_SIZE.
    
    The only real subtlety here is that the old code had a C file with
    inline assembler that did 'call VDSO32_vsyscall' and a linker script
    that defined 'VDSO32_vsyscall = __kernel_vsyscall'.  This most
    likely worked by accident: the linker script entry defines a symbol
    associated with an address as opposed to an alias for the real
    dynamic symbol __kernel_vsyscall.  That caused ld to relocate the
    reference at link time instead of leaving an interposable dynamic
    relocation.  Since the VDSO32_vsyscall hack is no longer needed, I
    now use 'call __kernel_vsyscall', and I added -Bsymbolic to make it
    work.  vdso2c will generate an error and abort the build if the
    resulting image contains any dynamic relocations, so we won't
    silently generate bad vdso images.
    
    (Dynamic relocations are a problem because nothing will even attempt
    to relocate the vdso.)
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/2c4fcf45524162a34d87fdda1eb046b2a5cecee7.1399317206.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 0982233b9b84..7225a9557ee2 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -516,10 +516,17 @@ char * __init xen_memory_setup(void)
 static void __init fiddle_vdso(void)
 {
 #ifdef CONFIG_X86_32
+	/*
+	 * This could be called before selected_vdso32 is initialized, so
+	 * just fiddle with both possible images.  vdso_image_32_syscall
+	 * can't be selected, since it only exists on 64-bit systems.
+	 */
 	u32 *mask;
-	mask = VDSO32_SYMBOL(&vdso32_int80_start, NOTE_MASK);
+	mask = vdso_image_32_int80.data +
+		vdso_image_32_int80.sym_VDSO32_NOTE_MASK;
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
-	mask = VDSO32_SYMBOL(&vdso32_sysenter_start, NOTE_MASK);
+	mask = vdso_image_32_sysenter.data +
+		vdso_image_32_sysenter.sym_VDSO32_NOTE_MASK;
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
 #endif
 }

commit 12f2bbd609006f983c1c99d240cf61e6e829a14c
Merge: 10ffe3dbf7f9 07ba06d9d293
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 30 18:15:32 2014 -0800

    Merge branch 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asmlinkage (LTO) changes from Peter Anvin:
     "This patchset adds more infrastructure for link time optimization
      (LTO).
    
      This patchset was pulled into my tree late because of a
      miscommunication (part of the patchset was picked up by other
      maintainers).  However, the patchset is strictly build-related and
      seems to be okay in testing"
    
    * 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, asmlinkage, xen: Fix type of NMI
      x86, asmlinkage, xen, kvm: Make {xen,kvm}_lock_spinning global and visible
      x86: Use inline assembler instead of global register variable to get sp
      x86, asmlinkage, paravirt: Make paravirt thunks global
      x86, asmlinkage, paravirt: Don't rely on local assembler labels
      x86, asmlinkage, lguest: Fix C functions used by inline assembler

commit 07ba06d9d293d3c0a512f1cb9189645c6e0424e2
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Oct 22 09:07:59 2013 -0700

    x86, asmlinkage, xen: Fix type of NMI
    
    LTO requires consistent types of symbols over all files.
    
    So "nmi" cannot be declared as a char [] here, need to use the
    correct function type.
    
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1382458079-24450-8-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 68c054f59de6..518ab4a17b8a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -34,7 +34,7 @@
 extern const char xen_hypervisor_callback[];
 extern const char xen_failsafe_callback[];
 #ifdef CONFIG_X86_64
-extern const char nmi[];
+extern asmlinkage void nmi(void);
 #endif
 extern void xen_sysenter_target(void);
 extern void xen_syscall_target(void);
@@ -559,7 +559,7 @@ void xen_enable_syscall(void)
 void xen_enable_nmi(void)
 {
 #ifdef CONFIG_X86_64
-	if (register_callback(CALLBACKTYPE_nmi, nmi))
+	if (register_callback(CALLBACKTYPE_nmi, (char *)nmi))
 		BUG();
 #endif
 }

commit 9103bb0f8240b2a55aac3ff7ecba9c7dcf66b08b
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Fri Dec 13 13:02:58 2013 -0500

    xen/pvh: Update E820 to work with PVH (v2)
    
    In xen_add_extra_mem() we can skip updating P2M as it's managed
    by Xen. PVH maps the entire IO space, but only RAM pages need
    to be repopulated.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 2137c5101dac..dd5f905e33d5 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -27,6 +27,7 @@
 #include <xen/interface/memory.h>
 #include <xen/interface/physdev.h>
 #include <xen/features.h>
+#include "mmu.h"
 #include "xen-ops.h"
 #include "vdso.h"
 
@@ -81,6 +82,9 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 
 	memblock_reserve(start, size);
 
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return;
+
 	xen_max_p2m_pfn = PFN_DOWN(start + size);
 	for (pfn = PFN_DOWN(start); pfn < xen_max_p2m_pfn; pfn++) {
 		unsigned long mfn = pfn_to_mfn(pfn);
@@ -103,6 +107,7 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 		.domid        = DOMID_SELF
 	};
 	unsigned long len = 0;
+	int xlated_phys = xen_feature(XENFEAT_auto_translated_physmap);
 	unsigned long pfn;
 	int ret;
 
@@ -116,7 +121,7 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 				continue;
 			frame = mfn;
 		} else {
-			if (mfn != INVALID_P2M_ENTRY)
+			if (!xlated_phys && mfn != INVALID_P2M_ENTRY)
 				continue;
 			frame = pfn;
 		}
@@ -154,6 +159,13 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 static unsigned long __init xen_release_chunk(unsigned long start,
 					      unsigned long end)
 {
+	/*
+	 * Xen already ballooned out the E820 non RAM regions for us
+	 * and set them up properly in EPT.
+	 */
+	if (xen_feature(XENFEAT_auto_translated_physmap))
+		return end - start;
+
 	return xen_do_chunk(start, end, true);
 }
 
@@ -222,7 +234,13 @@ static void __init xen_set_identity_and_release_chunk(
 	 * (except for the ISA region which must be 1:1 mapped) to
 	 * release the refcounts (in Xen) on the original frames.
 	 */
-	for (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++) {
+
+	/*
+	 * PVH E820 matches the hypervisor's P2M which means we need to
+	 * account for the proper values of *release and *identity.
+	 */
+	for (pfn = start_pfn; !xen_feature(XENFEAT_auto_translated_physmap) &&
+	     pfn <= max_pfn_mapped && pfn < end_pfn; pfn++) {
 		pte_t pte = __pte_ma(0);
 
 		if (pfn < PFN_UP(ISA_END_ADDRESS))

commit d285d68314af49c4456b71d248e355dd33ae375c
Author: Mukesh Rathor <mukesh.rathor@oracle.com>
Date:   Fri Dec 13 12:45:31 2013 -0500

    xen/pvh: Early bootup changes in PV code (v4).
    
    We don't use the filtering that 'xen_cpuid' is doing
    because the hypervisor treats 'XEN_EMULATE_PREFIX' as
    an invalid instruction. This means that all of the filtering
    will have to be done in the hypervisor/toolstack.
    
    Without the filtering we expose to the guest the:
    
     - cpu topology (sockets, cores, etc);
     - the APERF (which the generic scheduler likes to
        use), see  5e626254206a709c6e937f3dda69bf26c7344f6f
        "xen/setup: filter APERFMPERF cpuid feature out"
     - and the inability to figure out whether MWAIT_LEAF
       should be exposed or not. See
       df88b2d96e36d9a9e325bfcd12eb45671cbbc937
       "xen/enlighten: Disable MWAIT_LEAF so that acpi-pad won't be loaded."
     - x2apic, see  4ea9b9aca90cfc71e6872ed3522356755162932c
       "xen: mask x2APIC feature in PV"
    
    We also check for vector callback early on, as it is a required
    feature. PVH also runs at default kernel IOPL.
    
    Finally, pure PV settings are moved to a separate function that are
    only called for pure PV, ie, pv with pvmmu. They are also #ifdef
    with CONFIG_XEN_PVMMU.
    
    Signed-off-by: Mukesh Rathor <mukesh.rathor@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 68c054f59de6..2137c5101dac 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -563,16 +563,13 @@ void xen_enable_nmi(void)
 		BUG();
 #endif
 }
-void __init xen_arch_setup(void)
+void __init xen_pvmmu_arch_setup(void)
 {
-	xen_panic_handler_init();
-
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_4gb_segments);
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_writable_pagetables);
 
-	if (!xen_feature(XENFEAT_auto_translated_physmap))
-		HYPERVISOR_vm_assist(VMASST_CMD_enable,
-				     VMASST_TYPE_pae_extended_cr3);
+	HYPERVISOR_vm_assist(VMASST_CMD_enable,
+			     VMASST_TYPE_pae_extended_cr3);
 
 	if (register_callback(CALLBACKTYPE_event, xen_hypervisor_callback) ||
 	    register_callback(CALLBACKTYPE_failsafe, xen_failsafe_callback))
@@ -581,6 +578,15 @@ void __init xen_arch_setup(void)
 	xen_enable_sysenter();
 	xen_enable_syscall();
 	xen_enable_nmi();
+}
+
+/* This function is not called for HVM domains */
+void __init xen_arch_setup(void)
+{
+	xen_panic_handler_init();
+	if (!xen_feature(XENFEAT_auto_translated_physmap))
+		xen_pvmmu_arch_setup();
+
 #ifdef CONFIG_ACPI
 	if (!(xen_start_info->flags & SIF_INITDOMAIN)) {
 		printk(KERN_INFO "ACPI in unprivileged domain disabled\n");

commit 3b284bde702c191e3c63168e2d627d90d2b2515b
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Nov 8 11:21:51 2013 -0500

    xen: delete new instances of added __cpuinit
    
    commit 6efa20e49b9cb1db1ab66870cc37323474a75a13
    ("xen: Support 64-bit PV guest receiving NMIs") and
    commit cd9151e26d31048b2b5e00fd02e110e07d2200c9
    ( "xen/balloon: set a mapping for ballooned out pages")
    added new instances of __cpuinit usage.
    
    We removed this a couple versions ago; we now want to remove
    the compat no-op stubs.  Introducing new users is not what
    we want to see at this point in time, as it will break once
    the stubs are gone.
    
    Cc: Konrad Rzeszutek Wilk <konrad@kernel.org>
    Cc: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 09f3059cb00b..68c054f59de6 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -556,7 +556,7 @@ void xen_enable_syscall(void)
 	}
 #endif /* CONFIG_X86_64 */
 }
-void __cpuinit xen_enable_nmi(void)
+void xen_enable_nmi(void)
 {
 #ifdef CONFIG_X86_64
 	if (register_callback(CALLBACKTYPE_nmi, nmi))

commit cf39c8e5352b4fb9efedfe7e9acb566a85ed847c
Merge: 3398d252a4da 23b7eaf82207
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 17:45:39 2013 -0700

    Merge tag 'stable/for-linus-3.12-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen updates from Konrad Rzeszutek Wilk:
     "A couple of features and a ton of bug-fixes.  There is also some
      maintership changes.  Jeremy is enjoying the full-time work at the
      startup and as much as he would love to help - he can't find the time.
      I have a bunch of other things that I promised to work on - paravirt
      diet, get SWIOTLB working everywhere, etc, but haven't been able to
      find the time.
    
      As such both David Vrabel and Boris Ostrovsky have graciously
      volunteered to help with the maintership role.  They will keep the lid
      on regressions, bug-fixes, etc.  I will be in the background to help -
      but eventually there will be less of me doing the Xen GIT pulls and
      more of them.  Stefano is still doing the ARM/ARM64 and will continue
      on doing so.
    
      Features:
       - Xen Trusted Platform Module (TPM) frontend driver - with the
         backend in MiniOS.
       - Scalability improvements in event channel.
       - Two extra Xen co-maintainers (David, Boris) and one going away (Jeremy)
    
      Bug-fixes:
       - Make the 1:1 mapping work during early bootup on selective regions.
       - Add scratch page to balloon driver to deal with unexpected code
         still holding on stale pages.
       - Allow NMIs on PV guests (64-bit only)
       - Remove unnecessary TLB flush in M2P code.
       - Fixes duplicate callbacks in Xen granttable code.
       - Fixes in PRIVCMD_MMAPBATCH ioctls to allow retries
       - Fix for events being lost due to rescheduling on different VCPUs.
       - More documentation"
    
    * tag 'stable/for-linus-3.12-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (23 commits)
      hvc_xen: Remove unnecessary __GFP_ZERO from kzalloc
      drivers/xen-tpmfront: Fix compile issue with missing option.
      xen/balloon: don't set P2M entry for auto translated guest
      xen/evtchn: double free on error
      Xen: Fix retry calls into PRIVCMD_MMAPBATCH*.
      xen/pvhvm: Initialize xen panic handler for PVHVM guests
      xen/m2p: use GNTTABOP_unmap_and_replace to reinstate the original mapping
      xen: fix ARM build after 6efa20e4
      MAINTAINERS: Remove Jeremy from the Xen subsystem.
      xen/events: document behaviour when scanning the start word for events
      x86/xen: during early setup, only 1:1 map the ISA region
      x86/xen: disable premption when enabling local irqs
      swiotlb-xen: replace dma_length with sg_dma_len() macro
      swiotlb: replace dma_length with sg_dma_len() macro
      xen/balloon: set a mapping for ballooned out pages
      xen/evtchn: improve scalability by using per-user locks
      xen/p2m: avoid unneccesary TLB flush in m2p_remove_override()
      MAINTAINERS: Add in two extra co-maintainers of the Xen tree.
      MAINTAINERS: Update the Xen subsystem's with proper mailing list.
      xen: replace strict_strtoul() with kstrtoul()
      ...

commit d936d2d452ca1848cc4b397bdfb96d4278b9f934
Merge: 0903391acbc1 fc78d343fa74
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 21 16:38:33 2013 -0700

    Merge tag 'stable/for-linus-3.11-rc6-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen bug-fixes from Konrad Rzeszutek Wilk:
     - On ARM did not have balanced calls to get/put_cpu.
     - Fix to make tboot + Xen + Linux correctly.
     - Fix events VCPU binding issues.
     - Fix a vCPU online race where IPIs are sent to not-yet-online vCPU.
    
    * tag 'stable/for-linus-3.11-rc6-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/smp: initialize IPI vectors before marking CPU online
      xen/events: mask events when changing their VCPU binding
      xen/events: initialize local per-cpu mask for all possible events
      x86/xen: do not identity map UNUSABLE regions in the machine E820
      xen/arm: missing put_cpu in xen_percpu_init

commit e201bfcc5c4fe6d58ce2b21fb4f4c2a2f0ab7ab8
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jul 22 15:29:00 2013 +0100

    x86/xen: during early setup, only 1:1 map the ISA region
    
    During early setup, when the reserved regions and MMIO holes are being
    setup as 1:1 in the p2m, clear any mappings instead of making them 1:1
    (execept for the ISA region which is expected to be mapped).
    
    This fixes a regression introduced in 3.5 by 83d51ab473dd (xen/setup:
    update VA mapping when releasing memory during setup) which caused
    hosts with tboot to fail to boot.
    
    tboot marks a region in the e820 map as unusable and the dom0 kernel
    would attempt to map this region and Xen does not permit unusable
    regions to be mapped by guests.
    
    (XEN)  0000000000000000 - 0000000000060000 (usable)
    (XEN)  0000000000060000 - 0000000000068000 (reserved)
    (XEN)  0000000000068000 - 000000000009e000 (usable)
    (XEN)  0000000000100000 - 0000000000800000 (usable)
    (XEN)  0000000000800000 - 0000000000972000 (unusable)
    
    tboot marked this region as unusable.
    
    (XEN)  0000000000972000 - 00000000cf200000 (usable)
    (XEN)  00000000cf200000 - 00000000cf38f000 (reserved)
    (XEN)  00000000cf38f000 - 00000000cf3ce000 (ACPI data)
    (XEN)  00000000cf3ce000 - 00000000d0000000 (reserved)
    (XEN)  00000000e0000000 - 00000000f0000000 (reserved)
    (XEN)  00000000fe000000 - 0000000100000000 (reserved)
    (XEN)  0000000100000000 - 0000000630000000 (usable)
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 403f5cc5415b..6243651623d5 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -218,13 +218,19 @@ static void __init xen_set_identity_and_release_chunk(
 	unsigned long pfn;
 
 	/*
-	 * If the PFNs are currently mapped, the VA mapping also needs
-	 * to be updated to be 1:1.
+	 * If the PFNs are currently mapped, clear the mappings
+	 * (except for the ISA region which must be 1:1 mapped) to
+	 * release the refcounts (in Xen) on the original frames.
 	 */
-	for (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++)
+	for (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++) {
+		pte_t pte = __pte_ma(0);
+
+		if (pfn < PFN_UP(ISA_END_ADDRESS))
+			pte = mfn_pte(pfn, PAGE_KERNEL_IO);
+
 		(void)HYPERVISOR_update_va_mapping(
-			(unsigned long)__va(pfn << PAGE_SHIFT),
-			mfn_pte(pfn, PAGE_KERNEL_IO), 0);
+			(unsigned long)__va(pfn << PAGE_SHIFT), pte, 0);
+	}
 
 	if (start_pfn < nr_pages)
 		*released += xen_release_chunk(

commit 3bc38cbceb85881a8eb789ee1aa56678038b1909
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Aug 16 15:42:55 2013 +0100

    x86/xen: do not identity map UNUSABLE regions in the machine E820
    
    If there are UNUSABLE regions in the machine memory map, dom0 will
    attempt to map them 1:1 which is not permitted by Xen and the kernel
    will crash.
    
    There isn't anything interesting in the UNUSABLE region that the dom0
    kernel needs access to so we can avoid making the 1:1 mapping and
    treat it as RAM.
    
    We only do this for dom0, as that is where tboot case shows up.
    A PV domU could have an UNUSABLE region in its pseudo-physical map
    and would need to be handled in another patch.
    
    This fixes a boot failure on hosts with tboot.
    
    tboot marks a region in the e820 map as unusable and the dom0 kernel
    would attempt to map this region and Xen does not permit unusable
    regions to be mapped by guests.
    
      (XEN)  0000000000000000 - 0000000000060000 (usable)
      (XEN)  0000000000060000 - 0000000000068000 (reserved)
      (XEN)  0000000000068000 - 000000000009e000 (usable)
      (XEN)  0000000000100000 - 0000000000800000 (usable)
      (XEN)  0000000000800000 - 0000000000972000 (unusable)
    
    tboot marked this region as unusable.
    
      (XEN)  0000000000972000 - 00000000cf200000 (usable)
      (XEN)  00000000cf200000 - 00000000cf38f000 (reserved)
      (XEN)  00000000cf38f000 - 00000000cf3ce000 (ACPI data)
      (XEN)  00000000cf3ce000 - 00000000d0000000 (reserved)
      (XEN)  00000000e0000000 - 00000000f0000000 (reserved)
      (XEN)  00000000fe000000 - 0000000100000000 (reserved)
      (XEN)  0000000100000000 - 0000000630000000 (usable)
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    [v1: Altered the patch and description with domU's with UNUSABLE regions]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 94eac5c85cdc..0a9fb7a0b452 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -313,6 +313,17 @@ static void xen_align_and_add_e820_region(u64 start, u64 size, int type)
 	e820_add_region(start, end - start, type);
 }
 
+void xen_ignore_unusable(struct e820entry *list, size_t map_size)
+{
+	struct e820entry *entry;
+	unsigned int i;
+
+	for (i = 0, entry = list; i < map_size; i++, entry++) {
+		if (entry->type == E820_UNUSABLE)
+			entry->type = E820_RAM;
+	}
+}
+
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.
  **/
@@ -353,6 +364,17 @@ char * __init xen_memory_setup(void)
 	}
 	BUG_ON(rc);
 
+	/*
+	 * Xen won't allow a 1:1 mapping to be created to UNUSABLE
+	 * regions, so if we're using the machine memory map leave the
+	 * region as RAM as it is in the pseudo-physical map.
+	 *
+	 * UNUSABLE regions in domUs are not handled and will need
+	 * a patch in the future.
+	 */
+	if (xen_initial_domain())
+		xen_ignore_unusable(map, memmap.nr_entries);
+
 	/* Make sure the Xen-supplied memory map is well-ordered. */
 	sanitize_e820_map(map, memmap.nr_entries, &memmap.nr_entries);
 

commit 6efa20e49b9cb1db1ab66870cc37323474a75a13
Author: Konrad Rzeszutek Wilk <konrad@kernel.org>
Date:   Fri Jul 19 11:51:31 2013 -0400

    xen: Support 64-bit PV guest receiving NMIs
    
    This is based on a patch that Zhenzhong Duan had sent - which
    was missing some of the remaining pieces. The kernel has the
    logic to handle Xen-type-exceptions using the paravirt interface
    in the assembler code (see PARAVIRT_ADJUST_EXCEPTION_FRAME -
    pv_irq_ops.adjust_exception_frame and and INTERRUPT_RETURN -
    pv_cpu_ops.iret).
    
    That means the nmi handler (and other exception handlers) use
    the hypervisor iret.
    
    The other changes that would be neccessary for this would
    be to translate the NMI_VECTOR to one of the entries on the
    ipi_vector and make xen_send_IPI_mask_allbutself use different
    events.
    
    Fortunately for us commit 1db01b4903639fcfaec213701a494fe3fb2c490b
    (xen: Clean up apic ipi interface) implemented this and we piggyback
    on the cleanup such that the apic IPI interface will pass the right
    vector value for NMI.
    
    With this patch we can trigger NMIs within a PV guest (only tested
    x86_64).
    
    For this to work with normal PV guests (not initial domain)
    we need the domain to be able to use the APIC ops - they are
    already implemented to use the Xen event channels. For that
    to be turned on in a PV domU we need to remove the masking
    of X86_FEATURE_APIC.
    
    Incidentally that means kgdb will also now work within
    a PV guest without using the 'nokgdbroundup' workaround.
    
    Note that the 32-bit version is different and this patch
    does not enable that.
    
    CC: Lisa Nguyen <lisa@xenapiadmin.com>
    CC: Ben Guthro <benjamin.guthro@citrix.com>
    CC: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    [v1: Fixed up per David Vrabel comments]
    Reviewed-by: Ben Guthro <benjamin.guthro@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 056d11faef21..403f5cc5415b 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -33,6 +33,9 @@
 /* These are code, but not functions.  Defined in entry.S */
 extern const char xen_hypervisor_callback[];
 extern const char xen_failsafe_callback[];
+#ifdef CONFIG_X86_64
+extern const char nmi[];
+#endif
 extern void xen_sysenter_target(void);
 extern void xen_syscall_target(void);
 extern void xen_syscall32_target(void);
@@ -525,7 +528,13 @@ void xen_enable_syscall(void)
 	}
 #endif /* CONFIG_X86_64 */
 }
-
+void __cpuinit xen_enable_nmi(void)
+{
+#ifdef CONFIG_X86_64
+	if (register_callback(CALLBACKTYPE_nmi, nmi))
+		BUG();
+#endif
+}
 void __init xen_arch_setup(void)
 {
 	xen_panic_handler_init();
@@ -543,7 +552,7 @@ void __init xen_arch_setup(void)
 
 	xen_enable_sysenter();
 	xen_enable_syscall();
-
+	xen_enable_nmi();
 #ifdef CONFIG_ACPI
 	if (!(xen_start_info->flags & SIF_INITDOMAIN)) {
 		printk(KERN_INFO "ACPI in unprivileged domain disabled\n");

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 94eac5c85cdc..056d11faef21 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -475,7 +475,7 @@ static void __init fiddle_vdso(void)
 #endif
 }
 
-static int __cpuinit register_callback(unsigned type, const void *func)
+static int register_callback(unsigned type, const void *func)
 {
 	struct callback_register callback = {
 		.type = type,
@@ -486,7 +486,7 @@ static int __cpuinit register_callback(unsigned type, const void *func)
 	return HYPERVISOR_callback_op(CALLBACKOP_register, &callback);
 }
 
-void __cpuinit xen_enable_sysenter(void)
+void xen_enable_sysenter(void)
 {
 	int ret;
 	unsigned sysenter_feature;
@@ -505,7 +505,7 @@ void __cpuinit xen_enable_sysenter(void)
 		setup_clear_cpu_cap(sysenter_feature);
 }
 
-void __cpuinit xen_enable_syscall(void)
+void xen_enable_syscall(void)
 {
 #ifdef CONFIG_X86_64
 	int ret;

commit 27be457000211a6903968dfce06d5f73f051a217
Author: Len Brown <len.brown@intel.com>
Date:   Sun Feb 10 02:28:46 2013 -0500

    x86 idle: remove 32-bit-only "no-hlt" parameter, hlt_works_ok flag
    
    Remove 32-bit x86 a cmdline param "no-hlt",
    and the cpuinfo_x86.hlt_works_ok that it sets.
    
    If a user wants to avoid HLT, then "idle=poll"
    is much more useful, as it avoids invocation of HLT
    in idle, while "no-hlt" failed to do so.
    
    Indeed, hlt_works_ok was consulted in only 3 places.
    
    First, in /proc/cpuinfo where "hlt_bug yes"
    would be printed if and only if the user booted
    the system with "no-hlt" -- as there was no other code
    to set that flag.
    
    Second, check_hlt() would not invoke halt() if "no-hlt"
    were on the cmdline.
    
    Third, it was consulted in stop_this_cpu(), which is invoked
    by native_machine_halt()/reboot_interrupt()/smp_stop_nmi_callback() --
    all cases where the machine is being shutdown/reset.
    The flag was not consulted in the more frequently invoked
    play_dead()/hlt_play_dead() used in processor offline and suspend.
    
    Since Linux-3.0 there has been a run-time notice upon "no-hlt" invocations
    indicating that it would be removed in 2012.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 2b73b5c8555f..94eac5c85cdc 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -556,9 +556,6 @@ void __init xen_arch_setup(void)
 	       COMMAND_LINE_SIZE : MAX_GUEST_CMDLINE);
 
 	/* Set up idle, making sure it calls safe_halt() pvop */
-#ifdef CONFIG_X86_32
-	boot_cpu_data.hlt_works_ok = 1;
-#endif
 	disable_cpuidle();
 	disable_cpufreq();
 	WARN_ON(xen_set_default_idle());

commit 6a377ddc4e4ede2eeb9cd46ada23bbe417704fc9
Author: Len Brown <len.brown@intel.com>
Date:   Sat Feb 9 23:08:07 2013 -0500

    xen idle: make xen-specific macro xen-specific
    
    This macro is only invoked by Xen,
    so make its definition specific to Xen.
    
    > set_pm_idle_to_default()
    < xen_set_default_idle()
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: xen-devel@lists.xensource.com

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 8971a26d21ab..2b73b5c8555f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -561,7 +561,7 @@ void __init xen_arch_setup(void)
 #endif
 	disable_cpuidle();
 	disable_cpufreq();
-	WARN_ON(set_pm_idle_to_default());
+	WARN_ON(xen_set_default_idle());
 	fiddle_vdso();
 #ifdef CONFIG_NUMA
 	numa_off = 1;

commit 56d92aa5cf7c96c70f81d0350c94faf46a9fb76d
Merge: 33c2a174120b c341ca45ce56
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 22:09:10 2012 -0700

    Merge tag 'stable/for-linus-3.7-x86-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen update from Konrad Rzeszutek Wilk:
     "Features:
       - When hotplugging PCI devices in a PV guest we can allocate
         Xen-SWIOTLB later.
       - Cleanup Xen SWIOTLB.
       - Support pages out grants from HVM domains in the backends.
       - Support wild cards in xen-pciback.hide=(BDF) arguments.
       - Update grant status updates with upstream hypervisor.
       - Boot PV guests with more than 128GB.
       - Cleanup Xen MMU code/add comments.
       - Obtain XENVERS using a preferred method.
       - Lay out generic changes to support Xen ARM.
       - Allow privcmd ioctl for HVM (used to do only PV).
       - Do v2 of mmap_batch for privcmd ioctls.
       - If hypervisor saves the LED keyboard light - we will now instruct
         the kernel about its state.
      Fixes:
       - More fixes to Xen PCI backend for various calls/FLR/etc.
       - With more than 4GB in a 64-bit PV guest disable native SWIOTLB.
       - Fix up smatch warnings.
       - Fix up various return values in privmcmd and mm."
    
    * tag 'stable/for-linus-3.7-x86-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen: (48 commits)
      xen/pciback: Restore the PCI config space after an FLR.
      xen-pciback: properly clean up after calling pcistub_device_find()
      xen/vga: add the xen EFI video mode support
      xen/x86: retrieve keyboard shift status flags from hypervisor.
      xen/gndev: Xen backend support for paged out grant targets V4.
      xen-pciback: support wild cards in slot specifications
      xen/swiotlb: Fix compile warnings when using plain integer instead of NULL pointer.
      xen/swiotlb: Remove functions not needed anymore.
      xen/pcifront: Use Xen-SWIOTLB when initting if required.
      xen/swiotlb: For early initialization, return zero on success.
      xen/swiotlb: Use the swiotlb_late_init_with_tbl to init Xen-SWIOTLB late when PV PCI is used.
      xen/swiotlb: Move the error strings to its own function.
      xen/swiotlb: Move the nr_tbl determination in its own function.
      xen/arm: compile and run xenbus
      xen: resynchronise grant table status codes with upstream
      xen/privcmd: return -EFAULT on error
      xen/privcmd: Fix mmap batch ioctl error status copy back.
      xen/privcmd: add PRIVCMD_MMAPBATCH_V2 ioctl
      xen/mm: return more precise error from xen_remap_domain_range()
      xen/mmu: If the revector fails, don't attempt to revector anything else.
      ...

commit 8d54db795dfb1049d45dc34f0dddbc5347ec5642
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Aug 17 10:22:37 2012 -0400

    xen/boot: Disable NUMA for PV guests.
    
    The hypervisor is in charge of allocating the proper "NUMA" memory
    and dealing with the CPU scheduler to keep them bound to the proper
    NUMA node. The PV guests (and PVHVM) have no inkling of where they
    run and do not need to know that right now. In the future we will
    need to inject NUMA configuration data (if a guest spans two or more
    NUMA nodes) so that the kernel can make the right choices. But those
    patches are not yet present.
    
    In the meantime, disable the NUMA capability in the PV guest, which
    also fixes a bootup issue. Andre says:
    
    "we see Dom0 crashes due to the kernel detecting the NUMA topology not
    by ACPI, but directly from the northbridge (CONFIG_AMD_NUMA).
    
    This will detect the actual NUMA config of the physical machine, but
    will crash about the mismatch with Dom0's virtual memory. Variation of
    the theme: Dom0 sees what it's not supposed to see.
    
    This happens with the said config option enabled and on a machine where
    this scanning is still enabled (K8 and Fam10h, not Bulldozer class)
    
    We have this dump then:
    NUMA: Warning: node ids are out of bound, from=-1 to=-1 distance=10
    Scanning NUMA topology in Northbridge 24
    Number of physical nodes 4
    Node 0 MemBase 0000000000000000 Limit 0000000040000000
    Node 1 MemBase 0000000040000000 Limit 0000000138000000
    Node 2 MemBase 0000000138000000 Limit 00000001f8000000
    Node 3 MemBase 00000001f8000000 Limit 0000000238000000
    Initmem setup node 0 0000000000000000-0000000040000000
      NODE_DATA [000000003ffd9000 - 000000003fffffff]
    Initmem setup node 1 0000000040000000-0000000138000000
      NODE_DATA [0000000137fd9000 - 0000000137ffffff]
    Initmem setup node 2 0000000138000000-00000001f8000000
      NODE_DATA [00000001f095e000 - 00000001f0984fff]
    Initmem setup node 3 00000001f8000000-0000000238000000
    Cannot find 159744 bytes in node 3
    BUG: unable to handle kernel NULL pointer dereference at (null)
    IP: [<ffffffff81d220e6>] __alloc_bootmem_node+0x43/0x96
    Pid: 0, comm: swapper Not tainted 3.3.6 #1 AMD Dinar/Dinar
    RIP: e030:[<ffffffff81d220e6>]  [<ffffffff81d220e6>] __alloc_bootmem_node+0x43/0x96
    .. snip..
      [<ffffffff81d23024>] sparse_early_usemaps_alloc_node+0x64/0x178
      [<ffffffff81d23348>] sparse_init+0xe4/0x25a
      [<ffffffff81d16840>] paging_init+0x13/0x22
      [<ffffffff81d07fbb>] setup_arch+0x9c6/0xa9b
      [<ffffffff81683954>] ? printk+0x3c/0x3e
      [<ffffffff81d01a38>] start_kernel+0xe5/0x468
      [<ffffffff81d012cf>] x86_64_start_reservations+0xba/0xc1
      [<ffffffff81007153>] ? xen_setup_runstate_info+0x2c/0x36
      [<ffffffff81d050ee>] xen_start_kernel+0x565/0x56c
    "
    
    so we just disable NUMA scanning by setting numa_off=1.
    
    CC: stable@vger.kernel.org
    Reported-and-Tested-by: Andre Przywara <andre.przywara@amd.com>
    Acked-by: Andre Przywara <andre.przywara@amd.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index d11ca11d14fc..e2d62d697b5d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -17,6 +17,7 @@
 #include <asm/e820.h>
 #include <asm/setup.h>
 #include <asm/acpi.h>
+#include <asm/numa.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/xen/hypercall.h>
 
@@ -544,4 +545,7 @@ void __init xen_arch_setup(void)
 	disable_cpufreq();
 	WARN_ON(set_pm_idle_to_default());
 	fiddle_vdso();
+#ifdef CONFIG_NUMA
+	numa_off = 1;
+#endif
 }

commit 98104c3480e568d9c145adbc7dc56c9d4d170e30
Merge: 25a765b7f05c 328731876451
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Sep 12 11:16:27 2012 -0400

    Merge branch 'stable/128gb.v5.1' into stable/for-linus-3.7
    
    * stable/128gb.v5.1:
      xen/mmu: If the revector fails, don't attempt to revector anything else.
      xen/p2m: When revectoring deal with holes in the P2M array.
      xen/mmu: Release just the MFN list, not MFN list and part of pagetables.
      xen/mmu: Remove from __ka space PMD entries for pagetables.
      xen/mmu: Copy and revector the P2M tree.
      xen/p2m: Add logic to revector a P2M tree to use __va leafs.
      xen/mmu: Recycle the Xen provided L4, L3, and L2 pages
      xen/mmu: For 64-bit do not call xen_map_identity_early
      xen/mmu: use copy_page instead of memcpy.
      xen/mmu: Provide comments describing the _ka and _va aliasing issue
      xen/mmu: The xen_setup_kernel_pagetable doesn't need to return anything.
      Revert "xen/x86: Workaround 64-bit hypervisor and 32-bit initial domain." and "xen/x86: Use memblock_reserve for sensitive areas."
      xen/x86: Workaround 64-bit hypervisor and 32-bit initial domain.
      xen/x86: Use memblock_reserve for sensitive areas.
      xen/p2m: Fix the comment describing the P2M tree.
    
    Conflicts:
            arch/x86/xen/mmu.c
    
    The pagetable_init is the old xen_pagetable_setup_done and xen_pagetable_setup_start
    rolled in one.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 51faaf2b0d5c7f44d82964f0c70b1c4e44d4e633
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Aug 22 13:00:10 2012 -0400

    Revert "xen/x86: Workaround 64-bit hypervisor and 32-bit initial domain." and "xen/x86: Use memblock_reserve for sensitive areas."
    
    This reverts commit 806c312e50f122c47913145cf884f53dd09d9199 and
    commit 59b294403e9814e7c1154043567f0d71bac7a511.
    
    And also documents setup.c and why we want to do it that way, which
    is that we tried to make the the memblock_reserve more selective so
    that it would be clear what region is reserved. Sadly we ran
    in the problem wherein on a 64-bit hypervisor with a 32-bit
    initial domain, the pt_base has the cr3 value which is not
    neccessarily where the pagetable starts! As Jan put it: "
    Actually, the adjustment turns out to be correct: The page
    tables for a 32-on-64 dom0 get allocated in the order "first L1",
    "first L2", "first L3", so the offset to the page table base is
    indeed 2. When reading xen/include/public/xen.h's comment
    very strictly, this is not a violation (since there nothing is said
    that the first thing in the page table space is pointed to by
    pt_base; I admit that this seems to be implied though, namely
    do I think that it is implied that the page table space is the
    range [pt_base, pt_base + nt_pt_frames), whereas that
    range here indeed is [pt_base - 2, pt_base - 2 + nt_pt_frames),
    which - without a priori knowledge - the kernel would have
    difficulty to figure out)." - so lets just fall back to the
    easy way and reserve the whole region.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 9efca750405d..740517be4da5 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -424,6 +424,33 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
+	/*
+	 * Reserve Xen bits:
+	 *  - mfn_list
+	 *  - xen_start_info
+	 * See comment above "struct start_info" in <xen/interface/xen.h>
+	 * We tried to make the the memblock_reserve more selective so
+	 * that it would be clear what region is reserved. Sadly we ran
+	 * in the problem wherein on a 64-bit hypervisor with a 32-bit
+	 * initial domain, the pt_base has the cr3 value which is not
+	 * neccessarily where the pagetable starts! As Jan put it: "
+	 * Actually, the adjustment turns out to be correct: The page
+	 * tables for a 32-on-64 dom0 get allocated in the order "first L1",
+	 * "first L2", "first L3", so the offset to the page table base is
+	 * indeed 2. When reading xen/include/public/xen.h's comment
+	 * very strictly, this is not a violation (since there nothing is said
+	 * that the first thing in the page table space is pointed to by
+	 * pt_base; I admit that this seems to be implied though, namely
+	 * do I think that it is implied that the page table space is the
+	 * range [pt_base, pt_base + nt_pt_frames), whereas that
+	 * range here indeed is [pt_base - 2, pt_base - 2 + nt_pt_frames),
+	 * which - without a priori knowledge - the kernel would have
+	 * difficulty to figure out)." - so lets just fall back to the
+	 * easy way and reserve the whole region.
+	 */
+	memblock_reserve(__pa(xen_start_info->mfn_list),
+			 xen_start_info->pt_base - xen_start_info->mfn_list);
+
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
 	return "Xen";

commit c96aae1f7f393387d160211f60398d58463a7e65
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Aug 17 16:43:28 2012 -0400

    xen/setup: Fix one-off error when adding for-balloon PFNs to the P2M.
    
    When we are finished with return PFNs to the hypervisor, then
    populate it back, and also mark the E820 MMIO and E820 gaps
    as IDENTITY_FRAMEs, we then call P2M to set areas that can
    be used for ballooning. We were off by one, and ended up
    over-writting a P2M entry that most likely was an IDENTITY_FRAME.
    For example:
    
    1-1 mapping on 40000->40200
    1-1 mapping on bc558->bc5ac
    1-1 mapping on bc5b4->bc8c5
    1-1 mapping on bc8c6->bcb7c
    1-1 mapping on bcd00->100000
    Released 614 pages of unused memory
    Set 277889 page(s) to 1-1 mapping
    Populating 40200-40466 pfn range: 614 pages added
    
    => here we set from 40466 up to bc559 P2M tree to be
    INVALID_P2M_ENTRY. We should have done it up to bc558.
    
    The end result is that if anybody is trying to construct
    a PTE for PFN bc558 they end up with ~PAGE_PRESENT.
    
    CC: stable@vger.kernel.org
    Reported-by-and-Tested-by: Andre Przywara <andre.przywara@amd.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ead85576d54a..d11ca11d14fc 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -78,9 +78,16 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 	memblock_reserve(start, size);
 
 	xen_max_p2m_pfn = PFN_DOWN(start + size);
+	for (pfn = PFN_DOWN(start); pfn < xen_max_p2m_pfn; pfn++) {
+		unsigned long mfn = pfn_to_mfn(pfn);
+
+		if (WARN(mfn == pfn, "Trying to over-write 1-1 mapping (pfn: %lx)\n", pfn))
+			continue;
+		WARN(mfn != INVALID_P2M_ENTRY, "Trying to remove %lx which has %lx mfn!\n",
+			pfn, mfn);
 
-	for (pfn = PFN_DOWN(start); pfn <= xen_max_p2m_pfn; pfn++)
 		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+	}
 }
 
 static unsigned long __init xen_do_chunk(unsigned long start,

commit 59b294403e9814e7c1154043567f0d71bac7a511
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Jul 19 10:23:47 2012 -0400

    xen/x86: Use memblock_reserve for sensitive areas.
    
    instead of a big memblock_reserve. This way we can be more
    selective in freeing regions (and it also makes it easier
    to understand where is what).
    
    [v1: Move the auto_translate_physmap to proper line]
    [v2: Per Stefano suggestion add more comments]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index a4790bf22c59..9efca750405d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -424,15 +424,6 @@ char * __init xen_memory_setup(void)
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
 
-	/*
-	 * Reserve Xen bits:
-	 *  - mfn_list
-	 *  - xen_start_info
-	 * See comment above "struct start_info" in <xen/interface/xen.h>
-	 */
-	memblock_reserve(__pa(xen_start_info->mfn_list),
-			 xen_start_info->pt_base - xen_start_info->mfn_list);
-
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
 	return "Xen";

commit c3d93f880197953f86ab90d9da4744e926b38e33
Author: zhenzhong.duan <zhenzhong.duan@oracle.com>
Date:   Wed Jul 18 13:06:39 2012 +0800

    xen: populate correct number of pages when across mem boundary (v2)
    
    When populate pages across a mem boundary at bootup, the page count
    populated isn't correct. This is due to mem populated to non-mem
    region and ignored.
    
    Pfn range is also wrongly aligned when mem boundary isn't page aligned.
    
    For a dom0 booted with dom_mem=3368952K(0xcd9ff000-4k) dmesg diff is:
     [    0.000000] Freeing 9e-100 pfn range: 98 pages freed
     [    0.000000] 1-1 mapping on 9e->100
     [    0.000000] 1-1 mapping on cd9ff->100000
     [    0.000000] Released 98 pages of unused memory
     [    0.000000] Set 206435 page(s) to 1-1 mapping
    -[    0.000000] Populating cd9fe-cda00 pfn range: 1 pages added
    +[    0.000000] Populating cd9fe-cd9ff pfn range: 1 pages added
    +[    0.000000] Populating 100000-100061 pfn range: 97 pages added
     [    0.000000] BIOS-provided physical RAM map:
     [    0.000000] Xen: 0000000000000000 - 000000000009e000 (usable)
     [    0.000000] Xen: 00000000000a0000 - 0000000000100000 (reserved)
     [    0.000000] Xen: 0000000000100000 - 00000000cd9ff000 (usable)
     [    0.000000] Xen: 00000000cd9ffc00 - 00000000cda53c00 (ACPI NVS)
    ...
     [    0.000000] Xen: 0000000100000000 - 0000000100061000 (usable)
     [    0.000000] Xen: 0000000100061000 - 000000012c000000 (unusable)
    ...
     [    0.000000] MEMBLOCK configuration:
    ...
    -[    0.000000]  reserved[0x4]       [0x000000cd9ff000-0x000000cd9ffbff], 0xc00 bytes
    -[    0.000000]  reserved[0x5]       [0x00000100000000-0x00000100060fff], 0x61000 bytes
    
    Related xen memory layout:
    (XEN) Xen-e820 RAM map:
    (XEN)  0000000000000000 - 000000000009ec00 (usable)
    (XEN)  00000000000f0000 - 0000000000100000 (reserved)
    (XEN)  0000000000100000 - 00000000cd9ffc00 (usable)
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    [v2: If xen_do_chunk fail(populate), abort this chunk and any others]
    Suggested by David, thanks.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index a4790bf22c59..ead85576d54a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -157,25 +157,24 @@ static unsigned long __init xen_populate_chunk(
 	unsigned long dest_pfn;
 
 	for (i = 0, entry = list; i < map_size; i++, entry++) {
-		unsigned long credits = credits_left;
 		unsigned long s_pfn;
 		unsigned long e_pfn;
 		unsigned long pfns;
 		long capacity;
 
-		if (credits <= 0)
+		if (credits_left <= 0)
 			break;
 
 		if (entry->type != E820_RAM)
 			continue;
 
-		e_pfn = PFN_UP(entry->addr + entry->size);
+		e_pfn = PFN_DOWN(entry->addr + entry->size);
 
 		/* We only care about E820 after the xen_start_info->nr_pages */
 		if (e_pfn <= max_pfn)
 			continue;
 
-		s_pfn = PFN_DOWN(entry->addr);
+		s_pfn = PFN_UP(entry->addr);
 		/* If the E820 falls within the nr_pages, we want to start
 		 * at the nr_pages PFN.
 		 * If that would mean going past the E820 entry, skip it
@@ -184,23 +183,19 @@ static unsigned long __init xen_populate_chunk(
 			capacity = e_pfn - max_pfn;
 			dest_pfn = max_pfn;
 		} else {
-			/* last_pfn MUST be within E820_RAM regions */
-			if (*last_pfn && e_pfn >= *last_pfn)
-				s_pfn = *last_pfn;
 			capacity = e_pfn - s_pfn;
 			dest_pfn = s_pfn;
 		}
-		/* If we had filled this E820_RAM entry, go to the next one. */
-		if (capacity <= 0)
-			continue;
 
-		if (credits > capacity)
-			credits = capacity;
+		if (credits_left < capacity)
+			capacity = credits_left;
 
-		pfns = xen_do_chunk(dest_pfn, dest_pfn + credits, false);
+		pfns = xen_do_chunk(dest_pfn, dest_pfn + capacity, false);
 		done += pfns;
-		credits_left -= pfns;
 		*last_pfn = (dest_pfn + pfns);
+		if (pfns < capacity)
+			break;
+		credits_left -= pfns;
 	}
 	return done;
 }

commit 58b7b53a36b0be8081fbfc91aeea24b83c20ca1b
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue May 29 12:36:43 2012 -0400

    xen/balloon: Subtract from xen_released_pages the count that is populated.
    
    We did not take into account that xen_released_pages would be
    used outside the initial E820 parsing code. As such we would
    did not subtract from xen_released_pages the count of pages
    that we had populated back (instead we just did a simple
    extra_pages = released - populated).
    
    The balloon driver uses xen_released_pages to set the initial
    current_pages count.  If this is wrong (too low) then when a new
    (higher) target is set, the balloon driver will request too many pages
    from Xen."
    
    This fixes errors such as:
    
    (XEN) memory.c:133:d0 Could not allocate order=0 extent: id=0 memflags=0 (51 of 512)
    during bootup and
    free_memory            : 0
    
    where the free_memory should be 128.
    
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    [v1: Per David's review made the git commit better]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 3ebba0753d38..a4790bf22c59 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -371,7 +371,8 @@ char * __init xen_memory_setup(void)
 	populated = xen_populate_chunk(map, memmap.nr_entries,
 			max_pfn, &last_pfn, xen_released_pages);
 
-	extra_pages += (xen_released_pages - populated);
+	xen_released_pages -= populated;
+	extra_pages += xen_released_pages;
 
 	if (last_pfn > max_pfn) {
 		max_pfn = min(MAX_DOMAIN_PAGES, last_pfn);

commit 83d51ab473dddde7df858015070ed22b84ebe9a9
Author: David Vrabel <dvrabel@cantab.net>
Date:   Thu May 3 16:15:42 2012 +0100

    xen/setup: update VA mapping when releasing memory during setup
    
    In xen_memory_setup(), if a page that is being released has a VA
    mapping this must also be updated.  Otherwise, the page will be not
    released completely -- it will still be referenced in Xen and won't be
    freed util the mapping is removed and this prevents it from being
    reallocated at a different PFN.
    
    This was already being done for the ISA memory region in
    xen_ident_map_ISA() but on many systems this was omitting a few pages
    as many systems marked a few pages below the ISA memory region as
    reserved in the e820 map.
    
    This fixes errors such as:
    
    (XEN) page_alloc.c:1148:d0 Over-allocation for domain 0: 2097153 > 2097152
    (XEN) memory.c:133:d0 Could not allocate order=0 extent: id=0 memflags=0 (0 of 17)
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 30ac05a8d28f..3ebba0753d38 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -139,6 +139,13 @@ static unsigned long __init xen_do_chunk(unsigned long start,
 
 	return len;
 }
+
+static unsigned long __init xen_release_chunk(unsigned long start,
+					      unsigned long end)
+{
+	return xen_do_chunk(start, end, true);
+}
+
 static unsigned long __init xen_populate_chunk(
 	const struct e820entry *list, size_t map_size,
 	unsigned long max_pfn, unsigned long *last_pfn,
@@ -197,6 +204,29 @@ static unsigned long __init xen_populate_chunk(
 	}
 	return done;
 }
+
+static void __init xen_set_identity_and_release_chunk(
+	unsigned long start_pfn, unsigned long end_pfn, unsigned long nr_pages,
+	unsigned long *released, unsigned long *identity)
+{
+	unsigned long pfn;
+
+	/*
+	 * If the PFNs are currently mapped, the VA mapping also needs
+	 * to be updated to be 1:1.
+	 */
+	for (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++)
+		(void)HYPERVISOR_update_va_mapping(
+			(unsigned long)__va(pfn << PAGE_SHIFT),
+			mfn_pte(pfn, PAGE_KERNEL_IO), 0);
+
+	if (start_pfn < nr_pages)
+		*released += xen_release_chunk(
+			start_pfn, min(end_pfn, nr_pages));
+
+	*identity += set_phys_range_identity(start_pfn, end_pfn);
+}
+
 static unsigned long __init xen_set_identity_and_release(
 	const struct e820entry *list, size_t map_size, unsigned long nr_pages)
 {
@@ -226,14 +256,11 @@ static unsigned long __init xen_set_identity_and_release(
 			if (entry->type == E820_RAM)
 				end_pfn = PFN_UP(entry->addr);
 
-			if (start_pfn < end_pfn) {
-				if (start_pfn < nr_pages)
-					released += xen_do_chunk(
-						start_pfn, min(end_pfn, nr_pages), true);
+			if (start_pfn < end_pfn)
+				xen_set_identity_and_release_chunk(
+					start_pfn, end_pfn, nr_pages,
+					&released, &identity);
 
-				identity += set_phys_range_identity(
-					start_pfn, end_pfn);
-			}
 			start = end;
 		}
 	}

commit 96dc08b35c4af8cb5810450602590706f2593a5f
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Apr 6 16:10:20 2012 -0400

    xen/setup: Combine the two hypercall functions - since they are quite similar.
    
    They use the same set of arguments, so it is just the matter
    of using the proper hypercall.
    
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 710af36e6dfb..30ac05a8d28f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -83,8 +83,8 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 }
 
-static unsigned long __init xen_release_chunk(unsigned long start,
-					      unsigned long end)
+static unsigned long __init xen_do_chunk(unsigned long start,
+					 unsigned long end, bool release)
 {
 	struct xen_memory_reservation reservation = {
 		.address_bits = 0,
@@ -95,60 +95,36 @@ static unsigned long __init xen_release_chunk(unsigned long start,
 	unsigned long pfn;
 	int ret;
 
-	for(pfn = start; pfn < end; pfn++) {
-		unsigned long mfn = pfn_to_mfn(pfn);
-
-		/* Make sure pfn exists to start with */
-		if (mfn == INVALID_P2M_ENTRY || mfn_to_pfn(mfn) != pfn)
-			continue;
-
-		set_xen_guest_handle(reservation.extent_start, &mfn);
-		reservation.nr_extents = 1;
-
-		ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation,
-					   &reservation);
-		WARN(ret != 1, "Failed to release pfn %lx err=%d\n", pfn, ret);
-		if (ret == 1) {
-			__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
-			len++;
-		}
-	}
-	if (len)
-		printk(KERN_INFO "Freeing  %lx-%lx pfn range: %lu pages freed\n",
-		       start, end, len);
-
-	return len;
-}
-static unsigned long __init xen_populate_physmap(unsigned long start,
-						 unsigned long end)
-{
-	struct xen_memory_reservation reservation = {
-		.address_bits = 0,
-		.extent_order = 0,
-		.domid        = DOMID_SELF
-	};
-	unsigned long len = 0;
-	int ret;
-
 	for (pfn = start; pfn < end; pfn++) {
 		unsigned long frame;
+		unsigned long mfn = pfn_to_mfn(pfn);
 
-		/* Make sure pfn does not exists to start with */
-		if (pfn_to_mfn(pfn) != INVALID_P2M_ENTRY)
-			continue;
-
-		frame = pfn;
+		if (release) {
+			/* Make sure pfn exists to start with */
+			if (mfn == INVALID_P2M_ENTRY || mfn_to_pfn(mfn) != pfn)
+				continue;
+			frame = mfn;
+		} else {
+			if (mfn != INVALID_P2M_ENTRY)
+				continue;
+			frame = pfn;
+		}
 		set_xen_guest_handle(reservation.extent_start, &frame);
 		reservation.nr_extents = 1;
 
-		ret = HYPERVISOR_memory_op(XENMEM_populate_physmap, &reservation);
-		WARN(ret != 1, "Failed to populate pfn %lx err=%d\n", pfn, ret);
+		ret = HYPERVISOR_memory_op(release ? XENMEM_decrease_reservation : XENMEM_populate_physmap,
+					   &reservation);
+		WARN(ret != 1, "Failed to %s pfn %lx err=%d\n",
+		     release ? "release" : "populate", pfn, ret);
+
 		if (ret == 1) {
-			if (!early_set_phys_to_machine(pfn, frame)) {
+			if (!early_set_phys_to_machine(pfn, release ? INVALID_P2M_ENTRY : frame)) {
+				if (release)
+					break;
 				set_xen_guest_handle(reservation.extent_start, &frame);
 				reservation.nr_extents = 1;
 				ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation,
-							&reservation);
+							   &reservation);
 				break;
 			}
 			len++;
@@ -156,8 +132,11 @@ static unsigned long __init xen_populate_physmap(unsigned long start,
 			break;
 	}
 	if (len)
-		printk(KERN_INFO "Populated %lx-%lx pfn range: %lu pages added\n",
-		       start, end, len);
+		printk(KERN_INFO "%s %lx-%lx pfn range: %lu pages %s\n",
+		       release ? "Freeing" : "Populating",
+		       start, end, len,
+		       release ? "freed" : "added");
+
 	return len;
 }
 static unsigned long __init xen_populate_chunk(
@@ -211,7 +190,7 @@ static unsigned long __init xen_populate_chunk(
 		if (credits > capacity)
 			credits = capacity;
 
-		pfns = xen_populate_physmap(dest_pfn, dest_pfn + credits);
+		pfns = xen_do_chunk(dest_pfn, dest_pfn + credits, false);
 		done += pfns;
 		credits_left -= pfns;
 		*last_pfn = (dest_pfn + pfns);
@@ -249,8 +228,8 @@ static unsigned long __init xen_set_identity_and_release(
 
 			if (start_pfn < end_pfn) {
 				if (start_pfn < nr_pages)
-					released += xen_release_chunk(
-						start_pfn, min(end_pfn, nr_pages));
+					released += xen_do_chunk(
+						start_pfn, min(end_pfn, nr_pages), true);
 
 				identity += set_phys_range_identity(
 					start_pfn, end_pfn);

commit 2e2fb75475c2fc74c98100f1468c8195fee49f3b
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Apr 6 10:07:11 2012 -0400

    xen/setup: Populate freed MFNs from non-RAM E820 entries and gaps to E820 RAM
    
    When the Xen hypervisor boots a PV kernel it hands it two pieces
    of information: nr_pages and a made up E820 entry.
    
    The nr_pages value defines the range from zero to nr_pages of PFNs
    which have a valid Machine Frame Number (MFN) underneath it. The
    E820 mirrors that (with the VGA hole):
    BIOS-provided physical RAM map:
     Xen: 0000000000000000 - 00000000000a0000 (usable)
     Xen: 00000000000a0000 - 0000000000100000 (reserved)
     Xen: 0000000000100000 - 0000000080800000 (usable)
    
    The fun comes when a PV guest that is run with a machine E820 - that
    can either be the initial domain or a PCI PV guest, where the E820
    looks like the normal thing:
    
    BIOS-provided physical RAM map:
     Xen: 0000000000000000 - 000000000009e000 (usable)
     Xen: 000000000009ec00 - 0000000000100000 (reserved)
     Xen: 0000000000100000 - 0000000020000000 (usable)
     Xen: 0000000020000000 - 0000000020200000 (reserved)
     Xen: 0000000020200000 - 0000000040000000 (usable)
     Xen: 0000000040000000 - 0000000040200000 (reserved)
     Xen: 0000000040200000 - 00000000bad80000 (usable)
     Xen: 00000000bad80000 - 00000000badc9000 (ACPI NVS)
    ..
    With that overlaying the nr_pages directly on the E820 does not
    work as there are gaps and non-RAM regions that won't be used
    by the memory allocator. The 'xen_release_chunk' helps with that
    by punching holes in the P2M (PFN to MFN lookup tree) for those
    regions and tells us that:
    
    Freeing  20000-20200 pfn range: 512 pages freed
    Freeing  40000-40200 pfn range: 512 pages freed
    Freeing  bad80-badf4 pfn range: 116 pages freed
    Freeing  badf6-bae7f pfn range: 137 pages freed
    Freeing  bb000-100000 pfn range: 282624 pages freed
    Released 283999 pages of unused memory
    
    Those 283999 pages are subtracted from the nr_pages and are returned
    to the hypervisor. The end result is that the initial domain
    boots with 1GB less memory as the nr_pages has been subtracted by
    the amount of pages residing within the PCI hole. It can balloon up
    to that if desired using 'xl mem-set 0 8092', but the balloon driver
    is not always compiled in for the initial domain.
    
    This patch, implements the populate hypercall (XENMEM_populate_physmap)
    which increases the the domain with the same amount of pages that
    were released.
    
    The other solution (that did not work) was to transplant the MFN in
    the P2M tree - the ones that were going to be freed were put in
    the E820_RAM regions past the nr_pages. But the modifications to the
    M2P array (the other side of creating PTEs) were not carried away.
    As the hypervisor is the only one capable of modifying that and the
    only two hypercalls that would do this are: the update_va_mapping
    (which won't work, as during initial bootup only PFNs up to nr_pages
    are mapped in the guest) or via the populate hypercall.
    
    The end result is that the kernel can now boot with the
    nr_pages without having to subtract the 283999 pages.
    
    On a 8GB machine, with various dom0_mem= parameters this is what we get:
    
    no dom0_mem
    -Memory: 6485264k/9435136k available (5817k kernel code, 1136060k absent, 1813812k reserved, 2899k data, 696k init)
    +Memory: 7619036k/9435136k available (5817k kernel code, 1136060k absent, 680040k reserved, 2899k data, 696k init)
    
    dom0_mem=3G
    -Memory: 2616536k/9435136k available (5817k kernel code, 1136060k absent, 5682540k reserved, 2899k data, 696k init)
    +Memory: 2703776k/9435136k available (5817k kernel code, 1136060k absent, 5595300k reserved, 2899k data, 696k init)
    
    dom0_mem=max:3G
    -Memory: 2696732k/4281724k available (5817k kernel code, 1136060k absent, 448932k reserved, 2899k data, 696k init)
    +Memory: 2702204k/4281724k available (5817k kernel code, 1136060k absent, 443460k reserved, 2899k data, 696k init)
    
    And the 'xm list' or 'xl list' now reflect what the dom0_mem=
    argument is.
    
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    [v2: Use populate hypercall]
    [v3: Remove debug printks]
    [v4: Simplify code]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 7b0ab77b8479..710af36e6dfb 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -26,7 +26,6 @@
 #include <xen/interface/memory.h>
 #include <xen/interface/physdev.h>
 #include <xen/features.h>
-
 #include "xen-ops.h"
 #include "vdso.h"
 
@@ -120,7 +119,105 @@ static unsigned long __init xen_release_chunk(unsigned long start,
 
 	return len;
 }
+static unsigned long __init xen_populate_physmap(unsigned long start,
+						 unsigned long end)
+{
+	struct xen_memory_reservation reservation = {
+		.address_bits = 0,
+		.extent_order = 0,
+		.domid        = DOMID_SELF
+	};
+	unsigned long len = 0;
+	int ret;
+
+	for (pfn = start; pfn < end; pfn++) {
+		unsigned long frame;
+
+		/* Make sure pfn does not exists to start with */
+		if (pfn_to_mfn(pfn) != INVALID_P2M_ENTRY)
+			continue;
 
+		frame = pfn;
+		set_xen_guest_handle(reservation.extent_start, &frame);
+		reservation.nr_extents = 1;
+
+		ret = HYPERVISOR_memory_op(XENMEM_populate_physmap, &reservation);
+		WARN(ret != 1, "Failed to populate pfn %lx err=%d\n", pfn, ret);
+		if (ret == 1) {
+			if (!early_set_phys_to_machine(pfn, frame)) {
+				set_xen_guest_handle(reservation.extent_start, &frame);
+				reservation.nr_extents = 1;
+				ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation,
+							&reservation);
+				break;
+			}
+			len++;
+		} else
+			break;
+	}
+	if (len)
+		printk(KERN_INFO "Populated %lx-%lx pfn range: %lu pages added\n",
+		       start, end, len);
+	return len;
+}
+static unsigned long __init xen_populate_chunk(
+	const struct e820entry *list, size_t map_size,
+	unsigned long max_pfn, unsigned long *last_pfn,
+	unsigned long credits_left)
+{
+	const struct e820entry *entry;
+	unsigned int i;
+	unsigned long done = 0;
+	unsigned long dest_pfn;
+
+	for (i = 0, entry = list; i < map_size; i++, entry++) {
+		unsigned long credits = credits_left;
+		unsigned long s_pfn;
+		unsigned long e_pfn;
+		unsigned long pfns;
+		long capacity;
+
+		if (credits <= 0)
+			break;
+
+		if (entry->type != E820_RAM)
+			continue;
+
+		e_pfn = PFN_UP(entry->addr + entry->size);
+
+		/* We only care about E820 after the xen_start_info->nr_pages */
+		if (e_pfn <= max_pfn)
+			continue;
+
+		s_pfn = PFN_DOWN(entry->addr);
+		/* If the E820 falls within the nr_pages, we want to start
+		 * at the nr_pages PFN.
+		 * If that would mean going past the E820 entry, skip it
+		 */
+		if (s_pfn <= max_pfn) {
+			capacity = e_pfn - max_pfn;
+			dest_pfn = max_pfn;
+		} else {
+			/* last_pfn MUST be within E820_RAM regions */
+			if (*last_pfn && e_pfn >= *last_pfn)
+				s_pfn = *last_pfn;
+			capacity = e_pfn - s_pfn;
+			dest_pfn = s_pfn;
+		}
+		/* If we had filled this E820_RAM entry, go to the next one. */
+		if (capacity <= 0)
+			continue;
+
+		if (credits > capacity)
+			credits = capacity;
+
+		pfns = xen_populate_physmap(dest_pfn, dest_pfn + credits);
+		done += pfns;
+		credits_left -= pfns;
+		*last_pfn = (dest_pfn + pfns);
+	}
+	return done;
+}
 static unsigned long __init xen_set_identity_and_release(
 	const struct e820entry *list, size_t map_size, unsigned long nr_pages)
 {
@@ -143,7 +240,6 @@ static unsigned long __init xen_set_identity_and_release(
 	 */
 	for (i = 0, entry = list; i < map_size; i++, entry++) {
 		phys_addr_t end = entry->addr + entry->size;
-
 		if (entry->type == E820_RAM || i == map_size - 1) {
 			unsigned long start_pfn = PFN_DOWN(start);
 			unsigned long end_pfn = PFN_UP(end);
@@ -220,7 +316,9 @@ char * __init xen_memory_setup(void)
 	int rc;
 	struct xen_memory_map memmap;
 	unsigned long max_pages;
+	unsigned long last_pfn = 0;
 	unsigned long extra_pages = 0;
+	unsigned long populated;
 	int i;
 	int op;
 
@@ -260,8 +358,19 @@ char * __init xen_memory_setup(void)
 	 */
 	xen_released_pages = xen_set_identity_and_release(
 		map, memmap.nr_entries, max_pfn);
-	extra_pages += xen_released_pages;
 
+	/*
+	 * Populate back the non-RAM pages and E820 gaps that had been
+	 * released. */
+	populated = xen_populate_chunk(map, memmap.nr_entries,
+			max_pfn, &last_pfn, xen_released_pages);
+
+	extra_pages += (xen_released_pages - populated);
+
+	if (last_pfn > max_pfn) {
+		max_pfn = min(MAX_DOMAIN_PAGES, last_pfn);
+		mem_end = PFN_PHYS(max_pfn);
+	}
 	/*
 	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO
 	 * factor the base size.  On non-highmem systems, the base
@@ -275,7 +384,6 @@ char * __init xen_memory_setup(void)
 	 */
 	extra_pages = min(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
 			  extra_pages);
-
 	i = 0;
 	while (i < memmap.nr_entries) {
 		u64 addr = map[i].addr;

commit ca1182387e57470460294ce1e39e2d5518809811
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Mar 30 15:37:07 2012 -0400

    xen/setup: Only print "Freeing XXX-YYY pfn range: Z pages freed" if Z > 0
    
    Otherwise we can get these meaningless:
    Freeing  bad80-badf4 pfn range: 0 pages freed
    
    We also can do this for the summary ones - no point of printing
    "Set 0 page(s) to 1-1 mapping"
    
    Acked-by: David Vrabel <david.vrabel@citrix.com>
    [v1: Extended to the summary printks]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 1ba8dff26753..7b0ab77b8479 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -114,8 +114,9 @@ static unsigned long __init xen_release_chunk(unsigned long start,
 			len++;
 		}
 	}
-	printk(KERN_INFO "Freeing  %lx-%lx pfn range: %lu pages freed\n",
-	       start, end, len);
+	if (len)
+		printk(KERN_INFO "Freeing  %lx-%lx pfn range: %lu pages freed\n",
+		       start, end, len);
 
 	return len;
 }
@@ -162,8 +163,10 @@ static unsigned long __init xen_set_identity_and_release(
 		}
 	}
 
-	printk(KERN_INFO "Released %lu pages of unused memory\n", released);
-	printk(KERN_INFO "Set %ld page(s) to 1-1 mapping\n", identity);
+	if (released)
+		printk(KERN_INFO "Released %lu pages of unused memory\n", released);
+	if (identity)
+		printk(KERN_INFO "Set %ld page(s) to 1-1 mapping\n", identity);
 
 	return released;
 }

commit 48cdd8287f47a3cdad5b9273a5ef81bf605f7826
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Mar 13 20:06:57 2012 -0400

    xen/cpufreq: Disable the cpu frequency scaling drivers from loading.
    
    By using the functionality provided by "[CPUFREQ]: provide
    disable_cpuidle() function to disable the API."
    
    Under the Xen hypervisor we do not want the initial domain to exercise
    the cpufreq scaling drivers. This is b/c the Xen hypervisor is
    in charge of doing this as well and we can end up with both the
    Linux kernel and the hypervisor trying to change the P-states
    leading to weird performance issues.
    
    Acked-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    [v2: Fix compile error spotted by Benjamin Schweikert <b.schweikert@googlemail.com>]

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 12366238d07d..1ba8dff26753 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -10,6 +10,7 @@
 #include <linux/pm.h>
 #include <linux/memblock.h>
 #include <linux/cpuidle.h>
+#include <linux/cpufreq.h>
 
 #include <asm/elf.h>
 #include <asm/vdso.h>
@@ -420,6 +421,7 @@ void __init xen_arch_setup(void)
 	boot_cpu_data.hlt_works_ok = 1;
 #endif
 	disable_cpuidle();
+	disable_cpufreq();
 	WARN_ON(set_pm_idle_to_default());
 	fiddle_vdso();
 }

commit cc7335b2f6acc0f24c7fac80ce536301f7d52214
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Jan 23 10:53:57 2012 -0500

    xen/setup/pm/acpi: Remove the call to boot_option_idle_override.
    
    We needed that call in the past to force the kernel to use
    default_idle (which called safe_halt, which called xen_safe_halt).
    
    But set_pm_idle_to_default() does now that, so there is no need
    to use this boot option operand.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index e03c63692176..12366238d07d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -420,7 +420,6 @@ void __init xen_arch_setup(void)
 	boot_cpu_data.hlt_works_ok = 1;
 #endif
 	disable_cpuidle();
-	boot_option_idle_override = IDLE_HALT;
 	WARN_ON(set_pm_idle_to_default());
 	fiddle_vdso();
 }

commit 45aa0663cc408617b79a2b53f0a5f50e94688a48
Merge: 511585a28e5b 7bd0b0f0da3b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 20 12:14:26 2011 +0100

    Merge branch 'memblock-kill-early_node_map' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc into core/memblock

commit 42ebfc61cfcb13af3e638db1c497dcbde7abfed8
Merge: 55b02d2f4445 63a741757d15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 15 10:52:40 2011 -0800

    Merge branch 'stable/for-linus-fixes-3.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/for-linus-fixes-3.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/swiotlb: Use page alignment for early buffer allocation.
      xen: only limit memory map to maximum reservation for domain 0.

commit d3db728125c4470a2d061ac10fa7395e18237263
Author: Ian Campbell <Ian.Campbell@citrix.com>
Date:   Wed Dec 14 12:16:08 2011 +0000

    xen: only limit memory map to maximum reservation for domain 0.
    
    d312ae878b6a "xen: use maximum reservation to limit amount of usable RAM"
    clamped the total amount of RAM to the current maximum reservation. This is
    correct for dom0 but is not correct for guest domains. In order to boot a guest
    "pre-ballooned" (e.g. with memory=1G but maxmem=2G) in order to allow for
    future memory expansion the guest must derive max_pfn from the e820 provided by
    the toolstack and not the current maximum reservation (which can reflect only
    the current maximum, not the guest lifetime max). The existing algorithm
    already behaves this correctly if we do not artificially limit the maximum
    number of pages for the guest case.
    
    For a guest booted with maxmem=512, memory=128 this results in:
     [    0.000000] BIOS-provided physical RAM map:
     [    0.000000]  Xen: 0000000000000000 - 00000000000a0000 (usable)
     [    0.000000]  Xen: 00000000000a0000 - 0000000000100000 (reserved)
    -[    0.000000]  Xen: 0000000000100000 - 0000000008100000 (usable)
    -[    0.000000]  Xen: 0000000008100000 - 0000000020800000 (unusable)
    +[    0.000000]  Xen: 0000000000100000 - 0000000020800000 (usable)
    ...
     [    0.000000] NX (Execute Disable) protection: active
     [    0.000000] DMI not present or invalid.
     [    0.000000] e820 update range: 0000000000000000 - 0000000000010000 (usable) ==> (reserved)
     [    0.000000] e820 remove range: 00000000000a0000 - 0000000000100000 (usable)
    -[    0.000000] last_pfn = 0x8100 max_arch_pfn = 0x1000000
    +[    0.000000] last_pfn = 0x20800 max_arch_pfn = 0x1000000
     [    0.000000] initial memory mapped : 0 - 027ff000
     [    0.000000] Base memory trampoline at [c009f000] 9f000 size 4096
    -[    0.000000] init_memory_mapping: 0000000000000000-0000000008100000
    -[    0.000000]  0000000000 - 0008100000 page 4k
    -[    0.000000] kernel direct mapping tables up to 8100000 @ 27bb000-27ff000
    +[    0.000000] init_memory_mapping: 0000000000000000-0000000020800000
    +[    0.000000]  0000000000 - 0020800000 page 4k
    +[    0.000000] kernel direct mapping tables up to 20800000 @ 26f8000-27ff000
     [    0.000000] xen: setting RW the range 27e8000 - 27ff000
     [    0.000000] 0MB HIGHMEM available.
    -[    0.000000] 129MB LOWMEM available.
    -[    0.000000]   mapped low ram: 0 - 08100000
    -[    0.000000]   low ram: 0 - 08100000
    +[    0.000000] 520MB LOWMEM available.
    +[    0.000000]   mapped low ram: 0 - 20800000
    +[    0.000000]   low ram: 0 - 20800000
    
    With this change "xl mem-set <domain> 512M" will successfully increase the
    guest RAM (by reducing the balloon).
    
    There is no change for dom0.
    
    Reported-and-Tested-by:  George Shuklin <george.shuklin@gmail.com>
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Cc: stable@kernel.org
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 38d0af4fefec..a54ff1aab6eb 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -173,9 +173,21 @@ static unsigned long __init xen_get_max_pages(void)
 	domid_t domid = DOMID_SELF;
 	int ret;
 
-	ret = HYPERVISOR_memory_op(XENMEM_maximum_reservation, &domid);
-	if (ret > 0)
-		max_pages = ret;
+	/*
+	 * For the initial domain we use the maximum reservation as
+	 * the maximum page.
+	 *
+	 * For guest domains the current maximum reservation reflects
+	 * the current maximum rather than the static maximum. In this
+	 * case the e820 map provided to us will cover the static
+	 * maximum region.
+	 */
+	if (xen_initial_domain()) {
+		ret = HYPERVISOR_memory_op(XENMEM_maximum_reservation, &domid);
+		if (ret > 0)
+			max_pages = ret;
+	}
+
 	return min(max_pages, MAX_DOMAIN_PAGES);
 }
 

commit e5fd47bfab2df0c2184cc0bf4245d8e1bb7724fb
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Nov 21 18:02:02 2011 -0500

    xen/pm_idle: Make pm_idle be default_idle under Xen.
    
    The idea behind commit d91ee5863b71 ("cpuidle: replace xen access to x86
    pm_idle and default_idle") was to have one call - disable_cpuidle()
    which would make pm_idle not be molested by other code.  It disallows
    cpuidle_idle_call to be set to pm_idle (which is excellent).
    
    But in the select_idle_routine() and idle_setup(), the pm_idle can still
    be set to either: amd_e400_idle, mwait_idle or default_idle.  This
    depends on some CPU flags (MWAIT) and in AMD case on the type of CPU.
    
    In case of mwait_idle we can hit some instances where the hypervisor
    (Amazon EC2 specifically) sets the MWAIT and we get:
    
      Brought up 2 CPUs
      invalid opcode: 0000 [#1] SMP
    
      Pid: 0, comm: swapper Not tainted 3.1.0-0.rc6.git0.3.fc16.x86_64 #1
      RIP: e030:[<ffffffff81015d1d>]  [<ffffffff81015d1d>] mwait_idle+0x6f/0xb4
      ...
      Call Trace:
       [<ffffffff8100e2ed>] cpu_idle+0xae/0xe8
       [<ffffffff8149ee78>] cpu_bringup_and_idle+0xe/0x10
      RIP  [<ffffffff81015d1d>] mwait_idle+0x6f/0xb4
       RSP <ffff8801d28ddf10>
    
    In the case of amd_e400_idle we don't get so spectacular crashes, but we
    do end up making an MSR which is trapped in the hypervisor, and then
    follow it up with a yield hypercall.  Meaning we end up going to
    hypervisor twice instead of just once.
    
    The previous behavior before v3.0 was that pm_idle was set to
    default_idle regardless of select_idle_routine/idle_setup.
    
    We want to do that, but only for one specific case: Xen.  This patch
    does that.
    
    Fixes RH BZ #739499 and Ubuntu #881076
    Reported-by: Stefan Bader <stefan.bader@canonical.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 38d0af4fefec..1093f80c162d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -410,6 +410,6 @@ void __init xen_arch_setup(void)
 #endif
 	disable_cpuidle();
 	boot_option_idle_override = IDLE_HALT;
-
+	WARN_ON(set_pm_idle_to_default());
 	fiddle_vdso();
 }

commit d4bbf7e7759afc172e2bfbc5c416324590049cdd
Merge: a150439c4a97 401d0069cb34
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 28 09:46:22 2011 -0800

    Merge branch 'master' into x86/memblock
    
    Conflicts & resolutions:
    
    * arch/x86/xen/setup.c
    
            dc91c728fd "xen: allow extra memory to be in multiple regions"
            24aa07882b "memblock, x86: Replace memblock_x86_reserve/free..."
    
            conflicted on xen_add_extra_mem() updates.  The resolution is
            trivial as the latter just want to replace
            memblock_x86_reserve_range() with memblock_reserve().
    
    * drivers/pci/intel-iommu.c
    
            166e9278a3f "x86/ia64: intel-iommu: move to drivers/iommu/"
            5dfe8660a3d "bootmem: Replace work_with_active_regions() with..."
    
            conflicted as the former moved the file under drivers/iommu/.
            Resolved by applying the chnages from the latter on the moved
            file.
    
    * mm/Kconfig
    
            6661672053a "memblock: add NO_BOOTMEM config symbol"
            c378ddd53f9 "memblock, x86: Make ARCH_DISCARD_MEMBLOCK a config option"
    
            conflicted trivially.  Both added config options.  Just
            letting both add their own options resolves the conflict.
    
    * mm/memblock.c
    
            d1f0ece6cdc "mm/memblock.c: small function definition fixes"
            ed7b56a799c "memblock: Remove memblock_memory_can_coalesce()"
    
            confliected.  The former updates function removed by the
            latter.  Resolution is trivial.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit f3f436e33b925ead21e3f9b47b1e2aed965511d9
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Sep 28 17:46:36 2011 +0100

    xen: release all pages within 1-1 p2m mappings
    
    In xen_memory_setup() all reserved regions and gaps are set to an
    identity (1-1) p2m mapping.  If an available page has a PFN within one
    of these 1-1 mappings it will become inaccessible (as it MFN is lost)
    so release them before setting up the mapping.
    
    This can make an additional 256 MiB or more of RAM available
    (depending on the size of the reserved regions in the memory map) if
    the initial pages overlap with reserved regions.
    
    The 1:1 p2m mappings are also extended to cover partial pages.  This
    fixes an issue with (for example) systems with a BIOS that puts the
    DMI tables in a reserved region that begins on a non-page boundary.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 2ad2fd53bd32..38d0af4fefec 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -83,25 +83,18 @@ static void __init xen_add_extra_mem(u64 start, u64 size)
 		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 }
 
-static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
-					      phys_addr_t end_addr)
+static unsigned long __init xen_release_chunk(unsigned long start,
+					      unsigned long end)
 {
 	struct xen_memory_reservation reservation = {
 		.address_bits = 0,
 		.extent_order = 0,
 		.domid        = DOMID_SELF
 	};
-	unsigned long start, end;
 	unsigned long len = 0;
 	unsigned long pfn;
 	int ret;
 
-	start = PFN_UP(start_addr);
-	end = PFN_DOWN(end_addr);
-
-	if (end <= start)
-		return 0;
-
 	for(pfn = start; pfn < end; pfn++) {
 		unsigned long mfn = pfn_to_mfn(pfn);
 
@@ -126,72 +119,52 @@ static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
 	return len;
 }
 
-static unsigned long __init xen_return_unused_memory(
-	unsigned long max_pfn, const struct e820entry *map, int nr_map)
+static unsigned long __init xen_set_identity_and_release(
+	const struct e820entry *list, size_t map_size, unsigned long nr_pages)
 {
-	phys_addr_t max_addr = PFN_PHYS(max_pfn);
-	phys_addr_t last_end = ISA_END_ADDRESS;
+	phys_addr_t start = 0;
 	unsigned long released = 0;
-	int i;
-
-	/* Free any unused memory above the low 1Mbyte. */
-	for (i = 0; i < nr_map && last_end < max_addr; i++) {
-		phys_addr_t end = map[i].addr;
-		end = min(max_addr, end);
-
-		if (last_end < end)
-			released += xen_release_chunk(last_end, end);
-		last_end = max(last_end, map[i].addr + map[i].size);
-	}
-
-	if (last_end < max_addr)
-		released += xen_release_chunk(last_end, max_addr);
-
-	printk(KERN_INFO "released %lu pages of unused memory\n", released);
-	return released;
-}
-
-static unsigned long __init xen_set_identity(const struct e820entry *list,
-					     ssize_t map_size)
-{
-	phys_addr_t last = xen_initial_domain() ? 0 : ISA_END_ADDRESS;
-	phys_addr_t start_pci = last;
-	const struct e820entry *entry;
 	unsigned long identity = 0;
+	const struct e820entry *entry;
 	int i;
 
+	/*
+	 * Combine non-RAM regions and gaps until a RAM region (or the
+	 * end of the map) is reached, then set the 1:1 map and
+	 * release the pages (if available) in those non-RAM regions.
+	 *
+	 * The combined non-RAM regions are rounded to a whole number
+	 * of pages so any partial pages are accessible via the 1:1
+	 * mapping.  This is needed for some BIOSes that put (for
+	 * example) the DMI tables in a reserved region that begins on
+	 * a non-page boundary.
+	 */
 	for (i = 0, entry = list; i < map_size; i++, entry++) {
-		phys_addr_t start = entry->addr;
-		phys_addr_t end = start + entry->size;
+		phys_addr_t end = entry->addr + entry->size;
 
-		if (start < last)
-			start = last;
+		if (entry->type == E820_RAM || i == map_size - 1) {
+			unsigned long start_pfn = PFN_DOWN(start);
+			unsigned long end_pfn = PFN_UP(end);
 
-		if (end <= start)
-			continue;
+			if (entry->type == E820_RAM)
+				end_pfn = PFN_UP(entry->addr);
 
-		/* Skip over the 1MB region. */
-		if (last > end)
-			continue;
+			if (start_pfn < end_pfn) {
+				if (start_pfn < nr_pages)
+					released += xen_release_chunk(
+						start_pfn, min(end_pfn, nr_pages));
 
-		if ((entry->type == E820_RAM) || (entry->type == E820_UNUSABLE)) {
-			if (start > start_pci)
 				identity += set_phys_range_identity(
-						PFN_UP(start_pci), PFN_DOWN(start));
-
-			/* Without saving 'last' we would gooble RAM too
-			 * at the end of the loop. */
-			last = end;
-			start_pci = end;
-			continue;
+					start_pfn, end_pfn);
+			}
+			start = end;
 		}
-		start_pci = min(start, start_pci);
-		last = end;
 	}
-	if (last > start_pci)
-		identity += set_phys_range_identity(
-					PFN_UP(start_pci), PFN_DOWN(last));
-	return identity;
+
+	printk(KERN_INFO "Released %lu pages of unused memory\n", released);
+	printk(KERN_INFO "Set %ld page(s) to 1-1 mapping\n", identity);
+
+	return released;
 }
 
 static unsigned long __init xen_get_max_pages(void)
@@ -232,7 +205,6 @@ char * __init xen_memory_setup(void)
 	struct xen_memory_map memmap;
 	unsigned long max_pages;
 	unsigned long extra_pages = 0;
-	unsigned long identity_pages = 0;
 	int i;
 	int op;
 
@@ -265,8 +237,13 @@ char * __init xen_memory_setup(void)
 	if (max_pages > max_pfn)
 		extra_pages += max_pages - max_pfn;
 
-	xen_released_pages = xen_return_unused_memory(max_pfn, map,
-						      memmap.nr_entries);
+	/*
+	 * Set P2M for all non-RAM pages and E820 gaps to be identity
+	 * type PFNs.  Any RAM pages that would be made inaccesible by
+	 * this are first released.
+	 */
+	xen_released_pages = xen_set_identity_and_release(
+		map, memmap.nr_entries, max_pfn);
 	extra_pages += xen_released_pages;
 
 	/*
@@ -312,10 +289,6 @@ char * __init xen_memory_setup(void)
 	 * In domU, the ISA region is normal, usable memory, but we
 	 * reserve ISA memory anyway because too many things poke
 	 * about in there.
-	 *
-	 * In Dom0, the host E820 information can leave gaps in the
-	 * ISA range, which would cause us to release those pages.  To
-	 * avoid this, we unconditionally reserve them here.
 	 */
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);
@@ -332,12 +305,6 @@ char * __init xen_memory_setup(void)
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
-	/*
-	 * Set P2M for all non-RAM pages and E820 gaps to be identity
-	 * type PFNs.
-	 */
-	identity_pages = xen_set_identity(e820.map, e820.nr_map);
-	printk(KERN_INFO "Set %ld page(s) to 1-1 mapping.\n", identity_pages);
 	return "Xen";
 }
 

commit dc91c728fddc29dfed1ae96f6807216b5f42d3a1
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Sep 29 12:26:19 2011 +0100

    xen: allow extra memory to be in multiple regions
    
    Allow the extra memory (used by the balloon driver) to be in multiple
    regions (typically two regions, one for low memory and one for high
    memory).  This allows the balloon driver to increase the number of
    available low pages (if the initial number if pages is small).
    
    As a side effect, the algorithm for building the e820 memory map is
    simpler and more obviously correct as the map supplied by the
    hypervisor is (almost) used as is (in particular, all reserved regions
    and gaps are preserved).  Only RAM regions are altered and RAM regions
    above max_pfn + extra_pages are marked as unused (the region is split
    in two if necessary).
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 0c8e974c738a..2ad2fd53bd32 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -54,26 +54,32 @@ unsigned long xen_released_pages;
  */
 #define EXTRA_MEM_RATIO		(10)
 
-static void __init xen_add_extra_mem(unsigned long pages)
+static void __init xen_add_extra_mem(u64 start, u64 size)
 {
 	unsigned long pfn;
+	int i;
 
-	u64 size = (u64)pages * PAGE_SIZE;
-	u64 extra_start = xen_extra_mem[0].start + xen_extra_mem[0].size;
-
-	if (!pages)
-		return;
-
-	e820_add_region(extra_start, size, E820_RAM);
-	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
-
-	memblock_x86_reserve_range(extra_start, extra_start + size, "XEN EXTRA");
+	for (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {
+		/* Add new region. */
+		if (xen_extra_mem[i].size == 0) {
+			xen_extra_mem[i].start = start;
+			xen_extra_mem[i].size  = size;
+			break;
+		}
+		/* Append to existing region. */
+		if (xen_extra_mem[i].start + xen_extra_mem[i].size == start) {
+			xen_extra_mem[i].size += size;
+			break;
+		}
+	}
+	if (i == XEN_EXTRA_MEM_MAX_REGIONS)
+		printk(KERN_WARNING "Warning: not enough extra memory regions\n");
 
-	xen_extra_mem[0].size += size;
+	memblock_x86_reserve_range(start, start + size, "XEN EXTRA");
 
-	xen_max_p2m_pfn = PFN_DOWN(extra_start + size);
+	xen_max_p2m_pfn = PFN_DOWN(start + size);
 
-	for (pfn = PFN_DOWN(extra_start); pfn <= xen_max_p2m_pfn; pfn++)
+	for (pfn = PFN_DOWN(start); pfn <= xen_max_p2m_pfn; pfn++)
 		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 }
 
@@ -120,8 +126,8 @@ static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
 	return len;
 }
 
-static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
-						     const struct e820map *e820)
+static unsigned long __init xen_return_unused_memory(
+	unsigned long max_pfn, const struct e820entry *map, int nr_map)
 {
 	phys_addr_t max_addr = PFN_PHYS(max_pfn);
 	phys_addr_t last_end = ISA_END_ADDRESS;
@@ -129,13 +135,13 @@ static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
 	int i;
 
 	/* Free any unused memory above the low 1Mbyte. */
-	for (i = 0; i < e820->nr_map && last_end < max_addr; i++) {
-		phys_addr_t end = e820->map[i].addr;
+	for (i = 0; i < nr_map && last_end < max_addr; i++) {
+		phys_addr_t end = map[i].addr;
 		end = min(max_addr, end);
 
 		if (last_end < end)
 			released += xen_release_chunk(last_end, end);
-		last_end = max(last_end, e820->map[i].addr + e820->map[i].size);
+		last_end = max(last_end, map[i].addr + map[i].size);
 	}
 
 	if (last_end < max_addr)
@@ -200,20 +206,32 @@ static unsigned long __init xen_get_max_pages(void)
 	return min(max_pages, MAX_DOMAIN_PAGES);
 }
 
+static void xen_align_and_add_e820_region(u64 start, u64 size, int type)
+{
+	u64 end = start + size;
+
+	/* Align RAM regions to page boundaries. */
+	if (type == E820_RAM) {
+		start = PAGE_ALIGN(start);
+		end &= ~((u64)PAGE_SIZE - 1);
+	}
+
+	e820_add_region(start, end - start, type);
+}
+
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.
  **/
 char * __init xen_memory_setup(void)
 {
 	static struct e820entry map[E820MAX] __initdata;
-	static struct e820entry map_raw[E820MAX] __initdata;
 
 	unsigned long max_pfn = xen_start_info->nr_pages;
 	unsigned long long mem_end;
 	int rc;
 	struct xen_memory_map memmap;
+	unsigned long max_pages;
 	unsigned long extra_pages = 0;
-	unsigned long extra_limit;
 	unsigned long identity_pages = 0;
 	int i;
 	int op;
@@ -240,49 +258,55 @@ char * __init xen_memory_setup(void)
 	}
 	BUG_ON(rc);
 
-	memcpy(map_raw, map, sizeof(map));
-	e820.nr_map = 0;
-	xen_extra_mem[0].start = mem_end;
-	for (i = 0; i < memmap.nr_entries; i++) {
-		unsigned long long end;
-
-		/* Guard against non-page aligned E820 entries. */
-		if (map[i].type == E820_RAM)
-			map[i].size -= (map[i].size + map[i].addr) % PAGE_SIZE;
-
-		end = map[i].addr + map[i].size;
-		if (map[i].type == E820_RAM && end > mem_end) {
-			/* RAM off the end - may be partially included */
-			u64 delta = min(map[i].size, end - mem_end);
-
-			map[i].size -= delta;
-			end -= delta;
-
-			extra_pages += PFN_DOWN(delta);
-			/*
-			 * Set RAM below 4GB that is not for us to be unusable.
-			 * This prevents "System RAM" address space from being
-			 * used as potential resource for I/O address (happens
-			 * when 'allocate_resource' is called).
-			 */
-			if (delta &&
-				(xen_initial_domain() && end < 0x100000000ULL))
-				e820_add_region(end, delta, E820_UNUSABLE);
+	/* Make sure the Xen-supplied memory map is well-ordered. */
+	sanitize_e820_map(map, memmap.nr_entries, &memmap.nr_entries);
+
+	max_pages = xen_get_max_pages();
+	if (max_pages > max_pfn)
+		extra_pages += max_pages - max_pfn;
+
+	xen_released_pages = xen_return_unused_memory(max_pfn, map,
+						      memmap.nr_entries);
+	extra_pages += xen_released_pages;
+
+	/*
+	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO
+	 * factor the base size.  On non-highmem systems, the base
+	 * size is the full initial memory allocation; on highmem it
+	 * is limited to the max size of lowmem, so that it doesn't
+	 * get completely filled.
+	 *
+	 * In principle there could be a problem in lowmem systems if
+	 * the initial memory is also very large with respect to
+	 * lowmem, but we won't try to deal with that here.
+	 */
+	extra_pages = min(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
+			  extra_pages);
+
+	i = 0;
+	while (i < memmap.nr_entries) {
+		u64 addr = map[i].addr;
+		u64 size = map[i].size;
+		u32 type = map[i].type;
+
+		if (type == E820_RAM) {
+			if (addr < mem_end) {
+				size = min(size, mem_end - addr);
+			} else if (extra_pages) {
+				size = min(size, (u64)extra_pages * PAGE_SIZE);
+				extra_pages -= size / PAGE_SIZE;
+				xen_add_extra_mem(addr, size);
+			} else
+				type = E820_UNUSABLE;
 		}
 
-		if (map[i].size > 0 && end > xen_extra_mem[0].start)
-			xen_extra_mem[0].start = end;
+		xen_align_and_add_e820_region(addr, size, type);
 
-		/* Add region if any remains */
-		if (map[i].size > 0)
-			e820_add_region(map[i].addr, map[i].size, map[i].type);
+		map[i].addr += size;
+		map[i].size -= size;
+		if (map[i].size == 0)
+			i++;
 	}
-	/* Align the balloon area so that max_low_pfn does not get set
-	 * to be at the _end_ of the PCI gap at the far end (fee01000).
-	 * Note that the start of balloon area gets set in the loop above
-	 * to be past the last E820 region. */
-	if (xen_initial_domain() && (xen_extra_mem[0].start < (1ULL<<32)))
-		xen_extra_mem[0].start = (1ULL<<32);
 
 	/*
 	 * In domU, the ISA region is normal, usable memory, but we
@@ -308,45 +332,11 @@ char * __init xen_memory_setup(void)
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
-	extra_limit = xen_get_max_pages();
-	if (max_pfn + extra_pages > extra_limit) {
-		if (extra_limit > max_pfn)
-			extra_pages = extra_limit - max_pfn;
-		else
-			extra_pages = 0;
-	}
-
-	xen_released_pages = xen_return_unused_memory(xen_start_info->nr_pages,
-						      &e820);
-	extra_pages += xen_released_pages;
-
-	/*
-	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO
-	 * factor the base size.  On non-highmem systems, the base
-	 * size is the full initial memory allocation; on highmem it
-	 * is limited to the max size of lowmem, so that it doesn't
-	 * get completely filled.
-	 *
-	 * In principle there could be a problem in lowmem systems if
-	 * the initial memory is also very large with respect to
-	 * lowmem, but we won't try to deal with that here.
-	 */
-	extra_limit = min(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
-			  max_pfn + extra_pages);
-
-	if (extra_limit >= max_pfn)
-		extra_pages = extra_limit - max_pfn;
-	else
-		extra_pages = 0;
-
-	xen_add_extra_mem(extra_pages);
-
 	/*
 	 * Set P2M for all non-RAM pages and E820 gaps to be identity
-	 * type PFNs. We supply it with the non-sanitized version
-	 * of the E820.
+	 * type PFNs.
 	 */
-	identity_pages = xen_set_identity(map_raw, memmap.nr_entries);
+	identity_pages = xen_set_identity(e820.map, e820.nr_map);
 	printk(KERN_INFO "Set %ld page(s) to 1-1 mapping.\n", identity_pages);
 	return "Xen";
 }

commit 8b5d44a5ac93cd7a1b044db3ff0ba4955b4ba5ec
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Sep 28 17:46:34 2011 +0100

    xen: allow balloon driver to use more than one memory region
    
    Allow the xen balloon driver to populate its list of extra pages from
    more than one region of memory.  This will allow platforms to provide
    (for example) a region of low memory and a region of high memory.
    
    The maximum possible number of extra regions is 128 (== E820MAX) which
    is quite large so xen_extra_mem is placed in __initdata.  This is safe
    as both xen_memory_setup() and balloon_init() are in __init.
    
    The balloon regions themselves are not altered (i.e., there is still
    only the one region).
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index c983717c018c..0c8e974c738a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -37,7 +37,7 @@ extern void xen_syscall_target(void);
 extern void xen_syscall32_target(void);
 
 /* Amount of extra memory space we add to the e820 ranges */
-phys_addr_t xen_extra_mem_start, xen_extra_mem_size;
+struct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;
 
 /* Number of pages released from the initial allocation. */
 unsigned long xen_released_pages;
@@ -59,7 +59,7 @@ static void __init xen_add_extra_mem(unsigned long pages)
 	unsigned long pfn;
 
 	u64 size = (u64)pages * PAGE_SIZE;
-	u64 extra_start = xen_extra_mem_start + xen_extra_mem_size;
+	u64 extra_start = xen_extra_mem[0].start + xen_extra_mem[0].size;
 
 	if (!pages)
 		return;
@@ -69,7 +69,7 @@ static void __init xen_add_extra_mem(unsigned long pages)
 
 	memblock_x86_reserve_range(extra_start, extra_start + size, "XEN EXTRA");
 
-	xen_extra_mem_size += size;
+	xen_extra_mem[0].size += size;
 
 	xen_max_p2m_pfn = PFN_DOWN(extra_start + size);
 
@@ -242,7 +242,7 @@ char * __init xen_memory_setup(void)
 
 	memcpy(map_raw, map, sizeof(map));
 	e820.nr_map = 0;
-	xen_extra_mem_start = mem_end;
+	xen_extra_mem[0].start = mem_end;
 	for (i = 0; i < memmap.nr_entries; i++) {
 		unsigned long long end;
 
@@ -270,8 +270,8 @@ char * __init xen_memory_setup(void)
 				e820_add_region(end, delta, E820_UNUSABLE);
 		}
 
-		if (map[i].size > 0 && end > xen_extra_mem_start)
-			xen_extra_mem_start = end;
+		if (map[i].size > 0 && end > xen_extra_mem[0].start)
+			xen_extra_mem[0].start = end;
 
 		/* Add region if any remains */
 		if (map[i].size > 0)
@@ -279,10 +279,10 @@ char * __init xen_memory_setup(void)
 	}
 	/* Align the balloon area so that max_low_pfn does not get set
 	 * to be at the _end_ of the PCI gap at the far end (fee01000).
-	 * Note that xen_extra_mem_start gets set in the loop above to be
-	 * past the last E820 region. */
-	if (xen_initial_domain() && (xen_extra_mem_start < (1ULL<<32)))
-		xen_extra_mem_start = (1ULL<<32);
+	 * Note that the start of balloon area gets set in the loop above
+	 * to be past the last E820 region. */
+	if (xen_initial_domain() && (xen_extra_mem[0].start < (1ULL<<32)))
+		xen_extra_mem[0].start = (1ULL<<32);
 
 	/*
 	 * In domU, the ISA region is normal, usable memory, but we

commit aa24411b6717fd1e6ecef281bec497f6f30bbd66
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Sep 28 17:46:32 2011 +0100

    xen/balloon: account for pages released during memory setup
    
    In xen_memory_setup() pages that occur in gaps in the memory map are
    released back to Xen.  This reduces the domain's current page count in
    the hypervisor.  The Xen balloon driver does not correctly decrease
    its initial current_pages count to reflect this.  If 'delta' pages are
    released and the target is adjusted the resulting reservation is
    always 'delta' less than the requested target.
    
    This affects dom0 if the initial allocation of pages overlaps the PCI
    memory region but won't affect most domU guests that have been setup
    with pseudo-physical memory maps that don't have gaps.
    
    Fix this by accouting for the released pages when starting the balloon
    driver.
    
    If the domain's targets are managed by xapi, the domain may eventually
    run out of memory and die because xapi currently gets its target
    calculations wrong and whenever it is restarted it always reduces the
    target by 'delta'.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 46d6d21dbdbe..c983717c018c 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -39,6 +39,9 @@ extern void xen_syscall32_target(void);
 /* Amount of extra memory space we add to the e820 ranges */
 phys_addr_t xen_extra_mem_start, xen_extra_mem_size;
 
+/* Number of pages released from the initial allocation. */
+unsigned long xen_released_pages;
+
 /* 
  * The maximum amount of extra memory compared to the base size.  The
  * main scaling factor is the size of struct page.  At extreme ratios
@@ -313,7 +316,9 @@ char * __init xen_memory_setup(void)
 			extra_pages = 0;
 	}
 
-	extra_pages += xen_return_unused_memory(xen_start_info->nr_pages, &e820);
+	xen_released_pages = xen_return_unused_memory(xen_start_info->nr_pages,
+						      &e820);
+	extra_pages += xen_released_pages;
 
 	/*
 	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO

commit abbe0d3c26c545930492981cbd64be340ff41e05
Merge: c455ea4f122d 61cca2fab7ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 16 11:28:11 2011 -0700

    Merge branch 'stable/bug.fixes' of git://oss.oracle.com/git/kwilk/xen
    
    * 'stable/bug.fixes' of git://oss.oracle.com/git/kwilk/xen:
      xen/i386: follow-up to "replace order-based range checking of M2P table by linear one"
      xen/irq: Alter the locking to use a mutex instead of a spinlock.
      xen/e820: if there is no dom0_mem=, don't tweak extra_pages.
      xen: disable PV spinlocks on HVM

commit e3b73c4a25e9a5705b4ef28b91676caf01f9bc9f
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Tue Sep 13 10:17:32 2011 -0400

    xen/e820: if there is no dom0_mem=, don't tweak extra_pages.
    
    The patch "xen: use maximum reservation to limit amount of usable RAM"
    (d312ae878b6aed3912e1acaaf5d0b2a9d08a4f11) breaks machines that
    do not use 'dom0_mem=' argument with:
    
    reserve RAM buffer: 000000133f2e2000 - 000000133fffffff
    (XEN) mm.c:4976:d0 Global bit is set to kernel page fffff8117e
    (XEN) domain_crash_sync called from entry.S
    (XEN) Domain 0 (vcpu#0) crashed on cpu#0:
    ...
    
    The reason being that the last E820 entry is created using the
    'extra_pages' (which is based on how many pages have been freed).
    The mentioned git commit sets the initial value of 'extra_pages'
    using a hypercall which returns the number of pages (if dom0_mem
    has been used) or -1 otherwise. If the later we return with
    MAX_DOMAIN_PAGES as basis for calculation:
    
        return min(max_pages, MAX_DOMAIN_PAGES);
    
    and use it:
    
         extra_limit = xen_get_max_pages();
         if (extra_limit >= max_pfn)
                 extra_pages = extra_limit - max_pfn;
         else
                 extra_pages = 0;
    
    which means we end up with extra_pages = 128GB in PFNs (33554432)
    - 8GB in PFNs (2097152, on this specific box, can be larger or smaller),
    and then we add that value to the E820 making it:
    
      Xen: 00000000ff000000 - 0000000100000000 (reserved)
      Xen: 0000000100000000 - 000000133f2e2000 (usable)
    
    which is clearly wrong. It should look as so:
    
      Xen: 00000000ff000000 - 0000000100000000 (reserved)
      Xen: 0000000100000000 - 000000027fbda000 (usable)
    
    Naturally this problem does not present itself if dom0_mem=max:X
    is used.
    
    CC: stable@kernel.org
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ff3dfa176814..09688eb4a899 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -305,10 +305,12 @@ char * __init xen_memory_setup(void)
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
 	extra_limit = xen_get_max_pages();
-	if (extra_limit >= max_pfn)
-		extra_pages = extra_limit - max_pfn;
-	else
-		extra_pages = 0;
+	if (max_pfn + extra_pages > extra_limit) {
+		if (extra_limit > max_pfn)
+			extra_pages = extra_limit - max_pfn;
+		else
+			extra_pages = 0;
+	}
 
 	extra_pages += xen_return_unused_memory(xen_start_info->nr_pages, &e820);
 

commit 115452675361dbef2dfb59f4cbd922637be3dc5b
Merge: 768b56f59869 ed467e69f16e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 7 07:46:48 2011 -0700

    Merge branch 'stable/bug.fixes' of git://oss.oracle.com/git/kwilk/xen
    
    * 'stable/bug.fixes' of git://oss.oracle.com/git/kwilk/xen:
      xen/smp: Warn user why they keel over - nosmp or noapic and what to use instead.
      xen: x86_32: do not enable iterrupts when returning from exception in interrupt context
      xen: use maximum reservation to limit amount of usable RAM

commit d312ae878b6aed3912e1acaaf5d0b2a9d08a4f11
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Aug 19 15:57:16 2011 +0100

    xen: use maximum reservation to limit amount of usable RAM
    
    Use the domain's maximum reservation to limit the amount of extra RAM
    for the memory balloon. This reduces the size of the pages tables and
    the amount of reserved low memory (which defaults to about 1/32 of the
    total RAM).
    
    On a system with 8 GiB of RAM with the domain limited to 1 GiB the
    kernel reports:
    
    Before:
    
    Memory: 627792k/4472000k available
    
    After:
    
    Memory: 549740k/11132224k available
    
    A increase of about 76 MiB (~1.5% of the unused 7 GiB).  The reserved
    low memory is also reduced from 253 MiB to 32 MiB.  The total
    additional usable RAM is 329 MiB.
    
    For dom0, this requires at patch to Xen ('x86: use 'dom0_mem' to limit
    the number of pages for dom0') (c/s 23790)
    
    CC: stable@kernel.org
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 02ffd9e48c9f..ff3dfa176814 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -183,6 +183,19 @@ static unsigned long __init xen_set_identity(const struct e820entry *list,
 					PFN_UP(start_pci), PFN_DOWN(last));
 	return identity;
 }
+
+static unsigned long __init xen_get_max_pages(void)
+{
+	unsigned long max_pages = MAX_DOMAIN_PAGES;
+	domid_t domid = DOMID_SELF;
+	int ret;
+
+	ret = HYPERVISOR_memory_op(XENMEM_maximum_reservation, &domid);
+	if (ret > 0)
+		max_pages = ret;
+	return min(max_pages, MAX_DOMAIN_PAGES);
+}
+
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.
  **/
@@ -291,6 +304,12 @@ char * __init xen_memory_setup(void)
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
+	extra_limit = xen_get_max_pages();
+	if (extra_limit >= max_pfn)
+		extra_pages = extra_limit - max_pfn;
+	else
+		extra_pages = 0;
+
 	extra_pages += xen_return_unused_memory(xen_start_info->nr_pages, &e820);
 
 	/*

commit 45a05f9488911851f7a5c536df80b41f0105caf0
Merge: f38092b50f6e c00c8aa2d976
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Aug 6 12:22:30 2011 -0700

    Merge branch 'stable/bug.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/bug.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/trace: Fix compile error when CONFIG_XEN_PRIVILEGED_GUEST is not set
      xen: Fix misleading WARN message at xen_release_chunk
      xen: Fix printk() format in xen/setup.c
      xen/tracing: it looks like we wanted CONFIG_FTRACE
      xen/self-balloon: Add dependency on tmem.
      xen/balloon: Fix compile errors - missing header files.
      xen/grant: Fix compile warning.
      xen/pciback: remove duplicated #include

commit 98f531da8479eec3d828adc7f0865baaab99a543
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Tue Aug 2 11:45:25 2011 +0200

    xen: Fix misleading WARN message at xen_release_chunk
    
    WARN message should not complain
     "Failed to release memory %lx-%lx err=%d\n"
                               ^^^^^^^
    about range when it fails to release just one page,
    instead it should say what pfn is not freed.
    
    In addition line:
     printk(KERN_INFO "xen_release_chunk: looking at area pfn %lx-%lx: "
     ...
     printk(KERN_CONT "%lu pages freed\n", len);
    will be broken if WARN in between this line is fired. So fix it
    by using a single printk for this.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index e4056321b243..02ffd9e48c9f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -92,8 +92,6 @@ static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
 	if (end <= start)
 		return 0;
 
-	printk(KERN_INFO "xen_release_chunk: looking at area pfn %lx-%lx: ",
-	       start, end);
 	for(pfn = start; pfn < end; pfn++) {
 		unsigned long mfn = pfn_to_mfn(pfn);
 
@@ -106,14 +104,14 @@ static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
 
 		ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation,
 					   &reservation);
-		WARN(ret != 1, "Failed to release memory %lx-%lx err=%d\n",
-		     start, end, ret);
+		WARN(ret != 1, "Failed to release pfn %lx err=%d\n", pfn, ret);
 		if (ret == 1) {
 			__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 			len++;
 		}
 	}
-	printk(KERN_CONT "%lu pages freed\n", len);
+	printk(KERN_INFO "Freeing  %lx-%lx pfn range: %lu pages freed\n",
+	       start, end, len);
 
 	return len;
 }

commit 8f3c5883d840d079a5d2db20893b5ac8ba453b40
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Tue Aug 2 11:45:24 2011 +0200

    xen: Fix printk() format in xen/setup.c
    
    Use correct format specifier for unsigned long.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 60aeeb56948f..e4056321b243 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -113,7 +113,7 @@ static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
 			len++;
 		}
 	}
-	printk(KERN_CONT "%ld pages freed\n", len);
+	printk(KERN_CONT "%lu pages freed\n", len);
 
 	return len;
 }
@@ -139,7 +139,7 @@ static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
 	if (last_end < max_addr)
 		released += xen_release_chunk(last_end, max_addr);
 
-	printk(KERN_INFO "released %ld pages of unused memory\n", released);
+	printk(KERN_INFO "released %lu pages of unused memory\n", released);
 	return released;
 }
 

commit d91ee5863b71e8c90eaf6035bff3078a85e2e7b5
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 18:28:35 2011 -0400

    cpuidle: replace xen access to x86 pm_idle and default_idle
    
    When a Xen Dom0 kernel boots on a hypervisor, it gets access
    to the raw-hardware ACPI tables.  While it parses the idle tables
    for the hypervisor's beneift, it uses HLT for its own idle.
    
    Rather than have xen scribble on pm_idle and access default_idle,
    have it simply disable_cpuidle() so acpi_idle will not load and
    architecture default HLT will be used.
    
    cc: xen-devel@lists.xensource.com
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 60aeeb56948f..a9627e2e3295 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -9,6 +9,7 @@
 #include <linux/mm.h>
 #include <linux/pm.h>
 #include <linux/memblock.h>
+#include <linux/cpuidle.h>
 
 #include <asm/elf.h>
 #include <asm/vdso.h>
@@ -426,7 +427,7 @@ void __init xen_arch_setup(void)
 #ifdef CONFIG_X86_32
 	boot_cpu_data.hlt_works_ok = 1;
 #endif
-	pm_idle = default_idle;
+	disable_cpuidle();
 	boot_option_idle_override = IDLE_HALT;
 
 	fiddle_vdso();

commit 24aa07882b672fff2da2f5c955759f0bd13d32d5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 12 11:16:06 2011 +0200

    memblock, x86: Replace memblock_x86_reserve/free_range() with generic ones
    
    Other than sanity check and debug message, the x86 specific version of
    memblock reserve/free functions are simple wrappers around the generic
    versions - memblock_reserve/free().
    
    This patch adds debug messages with caller identification to the
    generic versions and replaces x86 specific ones and kills them.
    arch/x86/include/asm/memblock.h and arch/x86/mm/memblock.c are empty
    after this change and removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1310462166-31469-14-git-send-email-tj@kernel.org
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 60aeeb56948f..73daaf75801a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -63,7 +63,7 @@ static void __init xen_add_extra_mem(unsigned long pages)
 	e820_add_region(extra_start, size, E820_RAM);
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
-	memblock_x86_reserve_range(extra_start, extra_start + size, "XEN EXTRA");
+	memblock_reserve(extra_start, size);
 
 	xen_extra_mem_size += size;
 
@@ -287,9 +287,8 @@ char * __init xen_memory_setup(void)
 	 *  - xen_start_info
 	 * See comment above "struct start_info" in <xen/interface/xen.h>
 	 */
-	memblock_x86_reserve_range(__pa(xen_start_info->mfn_list),
-		      __pa(xen_start_info->pt_base),
-			"XEN START INFO");
+	memblock_reserve(__pa(xen_start_info->mfn_list),
+			 xen_start_info->pt_base - xen_start_info->mfn_list);
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 

commit acd049c6e99d2ad1195666195230f6881d1c1588
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Jun 16 13:07:19 2011 -0400

    xen/setup: Fix for incorrect xen_extra_mem_start.
    
    The earlier attempts (24bdb0b62cc82120924762ae6bc85afc8c3f2b26)
    at fixing this problem caused other problems to surface (PV guests
    with no PCI passthrough would have SWIOTLB turned on - which meant
    64MB of precious contingous DMA32 memory being eaten up per guest).
    The problem was: "on xen we add an extra memory region at the end of
    the e820, and on this particular machine this extra memory region
    would start below 4g and cross over the 4g boundary:
    
    [0xfee01000-0x192655000)
    
    Unfortunately e820_end_of_low_ram_pfn does not expect an
    e820 layout like that so it returns 4g, therefore initial_memory_mapping
    will map [0 - 0x100000000), that is a memory range that includes some
    reserved memory regions."
    
    The memory range was the IOAPIC regions, and with the 1-1 mapping
    turned on, it would map them as RAM, not as MMIO regions. This caused
    the hypervisor to complain. Fortunately this is experienced only under
    the initial domain so we guard for it.
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index be1a464f6d66..60aeeb56948f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -227,11 +227,7 @@ char * __init xen_memory_setup(void)
 
 	memcpy(map_raw, map, sizeof(map));
 	e820.nr_map = 0;
-#ifdef CONFIG_X86_32
 	xen_extra_mem_start = mem_end;
-#else
-	xen_extra_mem_start = max((1ULL << 32), mem_end);
-#endif
 	for (i = 0; i < memmap.nr_entries; i++) {
 		unsigned long long end;
 
@@ -266,6 +262,12 @@ char * __init xen_memory_setup(void)
 		if (map[i].size > 0)
 			e820_add_region(map[i].addr, map[i].size, map[i].type);
 	}
+	/* Align the balloon area so that max_low_pfn does not get set
+	 * to be at the _end_ of the PCI gap at the far end (fee01000).
+	 * Note that xen_extra_mem_start gets set in the loop above to be
+	 * past the last E820 region. */
+	if (xen_initial_domain() && (xen_extra_mem_start < (1ULL<<32)))
+		xen_extra_mem_start = (1ULL<<32);
 
 	/*
 	 * In domU, the ISA region is normal, usable memory, but we

commit e33ab8f275cf6e0e0bf6c9c44149de46222b36cc
Merge: 3bfccb74973d 7e186bdd0098 8c5950881c3b 0f16d0dfcdb5 7899891c7d16
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 16:14:58 2011 -0700

    Merge branches 'stable/irq', 'stable/p2m.bugfixes', 'stable/e820.bugfixes' and 'stable/mmu.bugfixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/irq' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: do not clear and mask evtchns in __xen_evtchn_do_upcall
    
    * 'stable/p2m.bugfixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/p2m: Create entries in the P2M_MFN trees's to track 1-1 mappings
    
    * 'stable/e820.bugfixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/setup: Fix for incorrect xen_extra_mem_start initialization under 32-bit
      xen/setup: Ignore E820_UNUSABLE when setting 1-1 mappings.
    
    * 'stable/mmu.bugfixes' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen mmu: fix a race window causing leave_mm BUG()

commit ae15a3b4d1374b733016ce4b4148b2ba42bbeb0f
Author: Daniel Kiper <dkiper@net-space.pl>
Date:   Wed May 4 20:17:21 2011 +0200

    arch/x86/xen/setup: Cleanup code/data sections definitions
    
    Cleanup code/data sections definitions
    accordingly to include/linux/init.h.
    
    Signed-off-by: Daniel Kiper <dkiper@net-space.pl>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 90bac0aac3a5..d3663df2f967 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -50,7 +50,7 @@ phys_addr_t xen_extra_mem_start, xen_extra_mem_size;
  */
 #define EXTRA_MEM_RATIO		(10)
 
-static __init void xen_add_extra_mem(unsigned long pages)
+static void __init xen_add_extra_mem(unsigned long pages)
 {
 	unsigned long pfn;
 
@@ -336,7 +336,7 @@ static void __init fiddle_vdso(void)
 #endif
 }
 
-static __cpuinit int register_callback(unsigned type, const void *func)
+static int __cpuinit register_callback(unsigned type, const void *func)
 {
 	struct callback_register callback = {
 		.type = type,

commit 0f16d0dfcdb5aab97d9e368f008b070b5b3ec6d3
Author: Daniel Kiper <dkiper@net-space.pl>
Date:   Wed May 11 22:34:38 2011 +0200

    xen/setup: Fix for incorrect xen_extra_mem_start initialization under 32-bit
    
    git commit 24bdb0b62cc82120924762ae6bc85afc8c3f2b26 (xen: do not create
    the extra e820 region at an addr lower than 4G) does not take into
    account that ifdef CONFIG_X86_32 instead of e820_end_of_low_ram_pfn()
    find_low_pfn_range() is called (both calls are from arch/x86/kernel/setup.c).
    find_low_pfn_range() behaves correctly and does not require change in
    xen_extra_mem_start initialization. Additionally, if xen_extra_mem_start
    is initialized in the same way as ifdef CONFIG_X86_64 then memory hotplug
    support for Xen balloon driver (under development) is broken.
    
    Signed-off-by: Daniel Kiper <dkiper@net-space.pl>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index fba4a6cc3a2d..ca6297bd4e3c 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -227,7 +227,11 @@ char * __init xen_memory_setup(void)
 
 	memcpy(map_raw, map, sizeof(map));
 	e820.nr_map = 0;
+#ifdef CONFIG_X86_32
+	xen_extra_mem_start = mem_end;
+#else
 	xen_extra_mem_start = max((1ULL << 32), mem_end);
+#endif
 	for (i = 0; i < memmap.nr_entries; i++) {
 		unsigned long long end;
 

commit 15bfc094517db2ddf38ca7ed47f3a1c0ad24f7c4
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 12 07:57:15 2011 -0400

    xen/setup: Ignore E820_UNUSABLE when setting 1-1 mappings.
    
    When we parse the raw E820, the Xen hypervisor can set "E820_RAM"
    to "E820_UNUSABLE" if the mem=X argument is used. As such we
    should _not_ consider the E820_UNUSABLE as an 1-1 identity
    mapping, but instead use the same case as for E820_RAM.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 90bac0aac3a5..fba4a6cc3a2d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -166,7 +166,7 @@ static unsigned long __init xen_set_identity(const struct e820entry *list,
 		if (last > end)
 			continue;
 
-		if (entry->type == E820_RAM) {
+		if ((entry->type == E820_RAM) || (entry->type == E820_UNUSABLE)) {
 			if (start > start_pci)
 				identity += set_phys_range_identity(
 						PFN_UP(start_pci), PFN_DOWN(start));

commit 24bdb0b62cc82120924762ae6bc85afc8c3f2b26
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Tue Apr 12 12:19:52 2011 +0100

    xen: do not create the extra e820 region at an addr lower than 4G
    
    Do not add the extra e820 region at a physical address lower than 4G
    because it breaks e820_end_of_low_ram_pfn().
    
    It is OK for us to move the xen_extra_mem_start up and down because this
    is the index of the memory that can be ballooned in/out - it is memory
    not available to the kernel during bootup.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index fa0269a99377..90bac0aac3a5 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -227,7 +227,7 @@ char * __init xen_memory_setup(void)
 
 	memcpy(map_raw, map, sizeof(map));
 	e820.nr_map = 0;
-	xen_extra_mem_start = mem_end;
+	xen_extra_mem_start = max((1ULL << 32), mem_end);
 	for (i = 0; i < memmap.nr_entries; i++) {
 		unsigned long long end;
 

commit c7146dd0090b9c98ae8525900abf1c38fc7e4e0d
Merge: 521cb40b0c44 706cc9d2a4cb 86b32122fd54
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 10:32:15 2011 -0700

    Merge branches 'stable/p2m-identity.v4.9.1' and 'stable/e820' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/p2m-identity.v4.9.1' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/m2p: Check whether the MFN has IDENTITY_FRAME bit set..
      xen/m2p: No need to catch exceptions when we know that there is no RAM
      xen/debug: WARN_ON when identity PFN has no _PAGE_IOMAP flag set.
      xen/debugfs: Add 'p2m' file for printing out the P2M layout.
      xen/setup: Set identity mapping for non-RAM E820 and E820 gaps.
      xen/mmu: WARN_ON when racing to swap middle leaf.
      xen/mmu: Set _PAGE_IOMAP if PFN is an identity PFN.
      xen/mmu: Add the notion of identity (1-1) mapping.
      xen: Mark all initial reserved pages for the balloon as INVALID_P2M_ENTRY.
    
    * 'stable/e820' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/e820: Don't mark balloon memory as E820_UNUSABLE when running as guest and fix overflow.
      xen/setup: Inhibit resource API from using System RAM E820 gaps as PCI mem gaps.

commit 68df0da7f42be6ae017fe9f48ac414c43a7b9d32
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Feb 1 17:15:30 2011 -0500

    xen/setup: Set identity mapping for non-RAM E820 and E820 gaps.
    
    We walk the E820 region and start at 0 (for PV guests we start
    at ISA_END_ADDRESS) and skip any E820 RAM regions. For all other
    regions and as well the gaps we set them to be identity mappings.
    
    The reasons we do not want to set the identity mapping from 0->
    ISA_END_ADDRESS when running as PV is b/c that the kernel would
    try to read DMI information and fail (no permissions to read that).
    There is a lot of gnarly code to deal with that weird region so
    we won't try to do a cleanup in this patch.
    
    This code ends up calling 'set_phys_to_identity' with the start
    and end PFN of the the E820 that are non-RAM or have gaps.
    On 99% of machines that means one big region right underneath the
    4GB mark. Usually starts at 0xc0000 (or 0x80000) and goes to
    0x100000.
    
    [v2: Fix for E820 crossing 1MB region and clamp the start]
    [v3: Squshed in code that does this over ranges]
    [v4: Moved the comment to the correct spot]
    [v5: Use the "raw" E820 from the hypervisor]
    [v6: Added Review-by tag]
    
    Reviewed-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 7201800e55a4..54d93791ddb9 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -143,12 +143,55 @@ static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
 	return released;
 }
 
+static unsigned long __init xen_set_identity(const struct e820entry *list,
+					     ssize_t map_size)
+{
+	phys_addr_t last = xen_initial_domain() ? 0 : ISA_END_ADDRESS;
+	phys_addr_t start_pci = last;
+	const struct e820entry *entry;
+	unsigned long identity = 0;
+	int i;
+
+	for (i = 0, entry = list; i < map_size; i++, entry++) {
+		phys_addr_t start = entry->addr;
+		phys_addr_t end = start + entry->size;
+
+		if (start < last)
+			start = last;
+
+		if (end <= start)
+			continue;
+
+		/* Skip over the 1MB region. */
+		if (last > end)
+			continue;
+
+		if (entry->type == E820_RAM) {
+			if (start > start_pci)
+				identity += set_phys_range_identity(
+						PFN_UP(start_pci), PFN_DOWN(start));
+
+			/* Without saving 'last' we would gooble RAM too
+			 * at the end of the loop. */
+			last = end;
+			start_pci = end;
+			continue;
+		}
+		start_pci = min(start, start_pci);
+		last = end;
+	}
+	if (last > start_pci)
+		identity += set_phys_range_identity(
+					PFN_UP(start_pci), PFN_DOWN(last));
+	return identity;
+}
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.
  **/
 char * __init xen_memory_setup(void)
 {
 	static struct e820entry map[E820MAX] __initdata;
+	static struct e820entry map_raw[E820MAX] __initdata;
 
 	unsigned long max_pfn = xen_start_info->nr_pages;
 	unsigned long long mem_end;
@@ -156,6 +199,7 @@ char * __init xen_memory_setup(void)
 	struct xen_memory_map memmap;
 	unsigned long extra_pages = 0;
 	unsigned long extra_limit;
+	unsigned long identity_pages = 0;
 	int i;
 	int op;
 
@@ -181,6 +225,7 @@ char * __init xen_memory_setup(void)
 	}
 	BUG_ON(rc);
 
+	memcpy(map_raw, map, sizeof(map));
 	e820.nr_map = 0;
 	xen_extra_mem_start = mem_end;
 	for (i = 0; i < memmap.nr_entries; i++) {
@@ -251,6 +296,13 @@ char * __init xen_memory_setup(void)
 
 	xen_add_extra_mem(extra_pages);
 
+	/*
+	 * Set P2M for all non-RAM pages and E820 gaps to be identity
+	 * type PFNs. We supply it with the non-sanitized version
+	 * of the E820.
+	 */
+	identity_pages = xen_set_identity(map_raw, memmap.nr_entries);
+	printk(KERN_INFO "Set %ld page(s) to 1-1 mapping.\n", identity_pages);
 	return "Xen";
 }
 

commit 86b32122fd54addc9af01f8b919c65d3f49090a3
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Mar 9 22:03:16 2011 -0500

    xen/e820: Don't mark balloon memory as E820_UNUSABLE when running as guest and fix overflow.
    
    If we have a guest that asked for:
    
    memory=1024
    maxmem=2048
    
    Which means we want 1GB now, and create pagetables so that we can expand
    up to 2GB, we would have this E820 layout:
    
    [    0.000000] BIOS-provided physical RAM map:
    [    0.000000]  Xen: 0000000000000000 - 00000000000a0000 (usable)
    [    0.000000]  Xen: 00000000000a0000 - 0000000000100000 (reserved)
    [    0.000000]  Xen: 0000000000100000 - 0000000080800000 (usable)
    
    Due to patch: "xen/setup: Inhibit resource API from using System RAM E820 gaps as PCI mem gaps."
    we would mark the memory past the 1GB mark as unusuable resulting in:
    
    [    0.000000] BIOS-provided physical RAM map:
    [    0.000000]  Xen: 0000000000000000 - 00000000000a0000 (usable)
    [    0.000000]  Xen: 00000000000a0000 - 0000000000100000 (reserved)
    [    0.000000]  Xen: 0000000000100000 - 0000000040000000 (usable)
    [    0.000000]  Xen: 0000000040000000 - 0000000080800000 (unusable)
    
    which meant that we could not balloon up anymore. We could
    balloon the guest down. The fix is to run the code introduced
    by the above mentioned patch only for the initial domain.
    
    We will have to revisit this once we start introducing a modified
    E820 for PCI passthrough so that we can utilize the P2M identity code.
    
    We also fix an overflow by having UL instead of ULL on 32-bit machines.
    
    [v2: Ian pointed to the overflow issue]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 2a4add9634ce..010ba2e62de5 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -200,7 +200,8 @@ char * __init xen_memory_setup(void)
 			 * used as potential resource for I/O address (happens
 			 * when 'allocate_resource' is called).
 			 */
-			if (delta && end < 0x100000000UL)
+			if (delta &&
+				(xen_initial_domain() && end < 0x100000000ULL))
 				e820_add_region(end, delta, E820_UNUSABLE);
 		}
 

commit 6eaa412f2753d98566b777836a98c6e7f672a3bb
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Jan 18 20:09:41 2011 -0500

    xen: Mark all initial reserved pages for the balloon as INVALID_P2M_ENTRY.
    
    With this patch, we diligently set regions that will be used by the
    balloon driver to be INVALID_P2M_ENTRY and under the ownership
    of the balloon driver. We are OK using the __set_phys_to_machine
    as we do not expect to be allocating any P2M middle or entries pages.
    The set_phys_to_machine has the side-effect of potentially allocating
    new pages and we do not want that at this stage.
    
    We can do this because xen_build_mfn_list_list will have already
    allocated all such pages up to xen_max_p2m_pfn.
    
    We also move the check for auto translated physmap down the
    stack so it is present in __set_phys_to_machine.
    
    [v2: Rebased with mmu->p2m code split]
    Reviewed-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index b5a7f928234b..7201800e55a4 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -52,6 +52,8 @@ phys_addr_t xen_extra_mem_start, xen_extra_mem_size;
 
 static __init void xen_add_extra_mem(unsigned long pages)
 {
+	unsigned long pfn;
+
 	u64 size = (u64)pages * PAGE_SIZE;
 	u64 extra_start = xen_extra_mem_start + xen_extra_mem_size;
 
@@ -66,6 +68,9 @@ static __init void xen_add_extra_mem(unsigned long pages)
 	xen_extra_mem_size += size;
 
 	xen_max_p2m_pfn = PFN_DOWN(extra_start + size);
+
+	for (pfn = PFN_DOWN(extra_start); pfn <= xen_max_p2m_pfn; pfn++)
+		__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 }
 
 static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
@@ -104,7 +109,7 @@ static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
 		WARN(ret != 1, "Failed to release memory %lx-%lx err=%d\n",
 		     start, end, ret);
 		if (ret == 1) {
-			set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+			__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
 			len++;
 		}
 	}

commit 2f14ddc3a7146ea4cd5a3d1ecd993f85f2e4f948
Author: Zhang, Fengzhe <fengzhe.zhang@intel.com>
Date:   Wed Feb 16 22:26:20 2011 +0800

    xen/setup: Inhibit resource API from using System RAM E820 gaps as PCI mem gaps.
    
    With the hypervisor argument of dom0_mem=X we iterate over the physical
    (only for the initial domain) E820 and subtract the the size from each
    E820_RAM region the delta so that the cumulative size of all E820_RAM regions
    is equal to 'X'. This sometimes ends up with E820_RAM regions with zero size
    (which are removed by e820_sanitize) and E820_RAM that are smaller
    than physically.
    
    Later on the PCI API looks at the E820 and attempts to set up an
    resource region for the "PCI mem". The E820 (assume dom0_mem=1GB is
    set) compared to the physical looks as so:
    
     [    0.000000] BIOS-provided physical RAM map:
     [    0.000000]  Xen: 0000000000000000 - 0000000000097c00 (usable)
     [    0.000000]  Xen: 0000000000097c00 - 0000000000100000 (reserved)
    -[    0.000000]  Xen: 0000000000100000 - 00000000defafe00 (usable)
    +[    0.000000]  Xen: 0000000000100000 - 0000000040000000 (usable)
     [    0.000000]  Xen: 00000000defafe00 - 00000000defb1ea0 (ACPI NVS)
     [    0.000000]  Xen: 00000000defb1ea0 - 00000000e0000000 (reserved)
     [    0.000000]  Xen: 00000000f4000000 - 00000000f8000000 (reserved)
    ..
    And we get
    [    0.000000] Allocating PCI resources starting at 40000000 (gap: 40000000:9efafe00)
    
    while it should have started at e0000000 (a nice big gap up to
    f4000000 exists). The "Allocating PCI" is part of the resource API.
    
    The users that end up using those PCI I/O regions usually supply their
    own BARs when calling the resource API (request_resource, or allocate_resource),
    but there are exceptions which provide an empty 'struct resource' and
    expect the API to provide the 'struct resource' to be populated with valid values.
    The one that triggered this bug was the intel AGP driver that requested
    a region for the flush page (intel_i9xx_setup_flush).
    
    Before this patch, when running under Xen hypervisor, the 'struct resource'
    returned could have (depending on the dom0_mem size) physical ranges of a 'System RAM'
    instead of 'I/O' regions. This ended up with the Hypervisor failing a request
    to populate PTE's with those PFNs as the domain did not have access to those
    'System RAM' regions (rightly so).
    
    After this patch, the left-over E820_RAM region from the truncation, will be
    labeled as E820_UNUSABLE. The E820 will look as so:
    
     [    0.000000] BIOS-provided physical RAM map:
     [    0.000000]  Xen: 0000000000000000 - 0000000000097c00 (usable)
     [    0.000000]  Xen: 0000000000097c00 - 0000000000100000 (reserved)
    -[    0.000000]  Xen: 0000000000100000 - 00000000defafe00 (usable)
    +[    0.000000]  Xen: 0000000000100000 - 0000000040000000 (usable)
    +[    0.000000]  Xen: 0000000040000000 - 00000000defafe00 (unusable)
     [    0.000000]  Xen: 00000000defafe00 - 00000000defb1ea0 (ACPI NVS)
     [    0.000000]  Xen: 00000000defb1ea0 - 00000000e0000000 (reserved)
     [    0.000000]  Xen: 00000000f4000000 - 00000000f8000000 (reserved)
    
    For more information:
    http://mid.gmane.org/1A42CE6F5F474C41B63392A5F80372B2335E978C@shsmsx501.ccr.corp.intel.com
    
    BugLink: http://bugzilla.xensource.com/bugzilla/show_bug.cgi?id=1726
    
    Signed-off-by: Fengzhe Zhang <fengzhe.zhang@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index a8a66a50d446..2a4add9634ce 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -194,6 +194,14 @@ char * __init xen_memory_setup(void)
 			end -= delta;
 
 			extra_pages += PFN_DOWN(delta);
+			/*
+			 * Set RAM below 4GB that is not for us to be unusable.
+			 * This prevents "System RAM" address space from being
+			 * used as potential resource for I/O address (happens
+			 * when 'allocate_resource' is called).
+			 */
+			if (delta && end < 0x100000000UL)
+				e820_add_region(end, delta, E820_UNUSABLE);
 		}
 
 		if (map[i].size > 0 && end > xen_extra_mem_start)

commit 23febeddbe67e5160929f7c48f7bfe83c2eecb99
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Wed Jan 26 17:07:27 2011 +0000

    xen/setup: Route halt operations to safe_halt pvop.
    
    With this patch, the cpuidle driver does not load and
    does not issue the mwait operations. Instead the hypervisor
    is doing them (b/c we call the safe_halt pvops call).
    
    This fixes quite a lot of bootup issues wherein the user had
    to force interrupts for the continuation of the bootup.
    
    Details are discussed in:
    
    http://lists.xensource.com/archives/html/xen-devel/2011-01/msg00535.html
    
    [v2: Wrote the commit description]
    
    Reported-by: Daniel De Graaf <dgdegra@tycho.nsa.gov>
    Tested-by: Daniel De Graaf <dgdegra@tycho.nsa.gov>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 75bdf2ab3d7c..a8a66a50d446 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -355,6 +355,7 @@ void __init xen_arch_setup(void)
 	boot_cpu_data.hlt_works_ok = 1;
 #endif
 	pm_idle = default_idle;
+	boot_option_idle_override = IDLE_HALT;
 
 	fiddle_vdso();
 }

commit 7cb31b752c71e0bd405c1139e1907c3335877dff
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Jan 27 10:13:25 2011 -0500

    xen/e820: Guard against E820_RAM not having page-aligned size or start.
    
    Under Dell Inspiron 1525, and Intel SandyBridge SDP's the
    BIOS e820 RAM is not page-aligned:
    
    [   0.000000]  Xen: 0000000000100000 - 00000000df66d800 (usable)
    
    We were not handling that and ended up setting up a pagetable
    that included up to df66e000 with the disastrous effect that when
    
            memset(NODE_DATA(nodeid), 0, sizeof(pg_data_t));
    
    tried to clear the page it would crash at the 2K mark.
    
    Initially reported by Michael Young @
    http://lists.xensource.com/archives/html/xen-devel/2011-01/msg00108.html
    
    The fix is to page-align the size and also take into consideration
    the start of the E820 (in case that is not page-aligned either). This
    fixes the bootup failure on those affected machines.
    
    This patch is a rework of the Micheal A Young initial patch and
    considers the case if the start is not page-aligned.
    
    Reported-by: Michael A Young <m.a.young@durham.ac.uk>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Michael A Young <m.a.young@durham.ac.uk>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index b5a7f928234b..75bdf2ab3d7c 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -179,8 +179,13 @@ char * __init xen_memory_setup(void)
 	e820.nr_map = 0;
 	xen_extra_mem_start = mem_end;
 	for (i = 0; i < memmap.nr_entries; i++) {
-		unsigned long long end = map[i].addr + map[i].size;
+		unsigned long long end;
 
+		/* Guard against non-page aligned E820 entries. */
+		if (map[i].type == E820_RAM)
+			map[i].size -= (map[i].size + map[i].addr) % PAGE_SIZE;
+
+		end = map[i].addr + map[i].size;
 		if (map[i].type == E820_RAM && end > mem_end) {
 			/* RAM off the end - may be partially included */
 			u64 delta = min(map[i].size, end - mem_end);

commit 8338fded137681bc3c1e99a69ac937a4fb016fe4
Merge: 9cd6315357ec 29dcbc5c25d6 805e3f495057
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 3 10:08:52 2010 -0800

    Merge branches 'upstream/core' and 'upstream/bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen: allocate irq descs on any NUMA node
      xen: prevent crashes with non-HIGHMEM 32-bit kernels with largeish memory
      xen: use default_idle
      xen: clean up "extra" memory handling some more
    
    * 'upstream/bugfix' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen: x86/32: perform initial startup on initial_page_table
      xen: don't bother to stop other cpus on shutdown/reboot

commit 8a3fbc9fdb4fd8f0970ab1a98f694ff25b7d5b47
Merge: c12ae95ccc2d e6d4a76dbf2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 25 08:35:53 2010 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: remove duplicated #include
      xen: x86/32: perform initial startup on initial_page_table

commit e6d4a76dbf2ff27314e09291dfb9e4afcb9ecd60
Author: Huang Weiyi <weiyi.huang@gmail.com>
Date:   Sat Nov 20 20:05:46 2010 +0800

    xen: remove duplicated #include
    
    Remove duplicated #include('s) in
      arch/x86/xen/setup.c
    
    Signed-off-by: Huang Weiyi <weiyi.huang@gmail.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 769c4b01fa32..d392486179e7 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -23,7 +23,6 @@
 #include <xen/interface/callback.h>
 #include <xen/interface/memory.h>
 #include <xen/interface/physdev.h>
-#include <xen/interface/memory.h>
 #include <xen/features.h>
 
 #include "xen-ops.h"

commit bc15fde77fc5d9ec2eec6066a5ab554ea1266a0a
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Nov 22 17:17:50 2010 -0800

    xen: use default_idle
    
    We just need the idle loop to drop into safe_halt, which default_idle()
    is perfectly capable of doing.  There's no need to duplicate it.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index b85dceef56f7..95fb68a8c20d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -250,20 +250,6 @@ char * __init xen_memory_setup(void)
 	return "Xen";
 }
 
-static void xen_idle(void)
-{
-	local_irq_disable();
-
-	if (need_resched())
-		local_irq_enable();
-	else {
-		current_thread_info()->status &= ~TS_POLLING;
-		smp_mb__after_clear_bit();
-		safe_halt();
-		current_thread_info()->status |= TS_POLLING;
-	}
-}
-
 /*
  * Set the bit indicating "nosegneg" library variants should be used.
  * We only need to bother in pure 32-bit mode; compat 32-bit processes
@@ -360,7 +346,11 @@ void __init xen_arch_setup(void)
 	       MAX_GUEST_CMDLINE > COMMAND_LINE_SIZE ?
 	       COMMAND_LINE_SIZE : MAX_GUEST_CMDLINE);
 
-	pm_idle = xen_idle;
+	/* Set up idle, making sure it calls safe_halt() pvop */
+#ifdef CONFIG_X86_32
+	boot_cpu_data.hlt_works_ok = 1;
+#endif
+	pm_idle = default_idle;
 
 	fiddle_vdso();
 }

commit c2d0879112825cddddd6c4f9b2645ff32acd6dc5
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Nov 22 16:31:35 2010 -0800

    xen: clean up "extra" memory handling some more
    
    Make sure that extra_pages is added for all E820_RAM regions beyond
    mem_end - completely excluded regions as well as the remains of partially
    included regions.
    
    Also makes sure the extra region is not unnecessarily high, and simplifies
    the logic to decide which regions should be added.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 38fdffaa71d3..b85dceef56f7 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -182,24 +182,21 @@ char * __init xen_memory_setup(void)
 	for (i = 0; i < memmap.nr_entries; i++) {
 		unsigned long long end = map[i].addr + map[i].size;
 
-		if (map[i].type == E820_RAM) {
-			if (map[i].addr < mem_end && end > mem_end) {
-				/* Truncate region to max_mem. */
-				u64 delta = end - mem_end;
+		if (map[i].type == E820_RAM && end > mem_end) {
+			/* RAM off the end - may be partially included */
+			u64 delta = min(map[i].size, end - mem_end);
 
-				map[i].size -= delta;
-				extra_pages += PFN_DOWN(delta);
+			map[i].size -= delta;
+			end -= delta;
 
-				end = mem_end;
-			}
+			extra_pages += PFN_DOWN(delta);
 		}
 
-		if (end > xen_extra_mem_start)
+		if (map[i].size > 0 && end > xen_extra_mem_start)
 			xen_extra_mem_start = end;
 
-		/* If region is non-RAM or below mem_end, add what remains */
-		if ((map[i].type != E820_RAM || map[i].addr < mem_end) &&
-		    map[i].size > 0)
+		/* Add region if any remains */
+		if (map[i].size > 0)
 			e820_add_region(map[i].addr, map[i].size, map[i].type);
 	}
 

commit ec35a69c467026437519bafcf325a7362e422db9
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Nov 16 12:09:59 2010 -0500

    xen: set IO permission early (before early_cpu_init())
    
    This patch is based off "xen dom0: Set up basic IO permissions for dom0."
    by Juan Quintela <quintela@redhat.com>.
    
    On AMD machines when we boot the kernel as Domain 0 we get this nasty:
    
    mapping kernel into physical memory
    Xen: setup ISA identity maps
    about to get started...
    (XEN) traps.c:475:d0 Unhandled general protection fault fault/trap [#13] on VCPU 0 [ec=0000]
    (XEN) domain_crash_sync called from entry.S
    (XEN) Domain 0 (vcpu#0) crashed on cpu#0:
    (XEN) ----[ Xen-4.1-101116  x86_64  debug=y  Not tainted ]----
    (XEN) CPU:    0
    (XEN) RIP:    e033:[<ffffffff8130271b>]
    (XEN) RFLAGS: 0000000000000282   EM: 1   CONTEXT: pv guest
    (XEN) rax: 000000008000c068   rbx: ffffffff8186c680   rcx: 0000000000000068
    (XEN) rdx: 0000000000000cf8   rsi: 000000000000c000   rdi: 0000000000000000
    (XEN) rbp: ffffffff81801e98   rsp: ffffffff81801e50   r8:  ffffffff81801eac
    (XEN) r9:  ffffffff81801ea8   r10: ffffffff81801eb4   r11: 00000000ffffffff
    (XEN) r12: ffffffff8186c694   r13: ffffffff81801f90   r14: ffffffffffffffff
    (XEN) r15: 0000000000000000   cr0: 000000008005003b   cr4: 00000000000006f0
    (XEN) cr3: 0000000221803000   cr2: 0000000000000000
    (XEN) ds: 0000   es: 0000   fs: 0000   gs: 0000   ss: e02b   cs: e033
    (XEN) Guest stack trace from rsp=ffffffff81801e50:
    
    RIP points to read_pci_config() function.
    
    The issue is that we don't set IO permissions for the Linux kernel early enough.
    
    The call sequence used to be:
    
        xen_start_kernel()
            x86_init.oem.arch_setup = xen_setup_arch;
            setup_arch:
               - early_cpu_init
                   - early_init_amd
                      - read_pci_config
               - x86_init.oem.arch_setup [ xen_arch_setup ]
                   - set IO permissions.
    
    We need to set the IO permissions earlier on, which this patch does.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 630fb53c95f3..38fdffaa71d3 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -336,9 +336,6 @@ void __cpuinit xen_enable_syscall(void)
 
 void __init xen_arch_setup(void)
 {
-	struct physdev_set_iopl set_iopl;
-	int rc;
-
 	xen_panic_handler_init();
 
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_4gb_segments);
@@ -355,11 +352,6 @@ void __init xen_arch_setup(void)
 	xen_enable_sysenter();
 	xen_enable_syscall();
 
-	set_iopl.iopl = 1;
-	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
-	if (rc != 0)
-		printk(KERN_INFO "physdev_op failed %d\n", rc);
-
 #ifdef CONFIG_ACPI
 	if (!(xen_start_info->flags & SIF_INITDOMAIN)) {
 		printk(KERN_INFO "ACPI in unprivileged domain disabled\n");

commit d2a817130cdc142f1c80a8e60eca824a321926af
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Nov 19 23:27:06 2010 -0800

    xen: re-enable boot-time ballooning
    
    Now that the balloon driver doesn't stumble over non-RAM pages, we
    can enable the extra space for ballooning.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 769c4b01fa32..630fb53c95f3 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -248,8 +248,7 @@ char * __init xen_memory_setup(void)
 	else
 		extra_pages = 0;
 
-	if (!xen_initial_domain())
-		xen_add_extra_mem(extra_pages);
+	xen_add_extra_mem(extra_pages);
 
 	return "Xen";
 }

commit 9ec23a7f6d2537faf14368e066e307c06812c4ca
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Thu Oct 28 11:32:29 2010 -0700

    xen: do not release any memory under 1M in domain 0
    
    We already deliberately setup a 1-1 P2M for the region up to 1M in
    order to allow code which assumes this region is already mapped to
    work without having to convert everything to ioremap.
    
    Domain 0 should not return any apparently unused memory regions
    (reserved or otherwise) in this region to Xen since the e820 may not
    accurately reflect what the BIOS has stashed in this region.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index b1dbdaa23ecc..769c4b01fa32 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -118,16 +118,18 @@ static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
 						     const struct e820map *e820)
 {
 	phys_addr_t max_addr = PFN_PHYS(max_pfn);
-	phys_addr_t last_end = 0;
+	phys_addr_t last_end = ISA_END_ADDRESS;
 	unsigned long released = 0;
 	int i;
 
+	/* Free any unused memory above the low 1Mbyte. */
 	for (i = 0; i < e820->nr_map && last_end < max_addr; i++) {
 		phys_addr_t end = e820->map[i].addr;
 		end = min(max_addr, end);
 
-		released += xen_release_chunk(last_end, end);
-		last_end = e820->map[i].addr + e820->map[i].size;
+		if (last_end < end)
+			released += xen_release_chunk(last_end, end);
+		last_end = max(last_end, e820->map[i].addr + e820->map[i].size);
 	}
 
 	if (last_end < max_addr)
@@ -164,6 +166,7 @@ char * __init xen_memory_setup(void)
 		XENMEM_memory_map;
 	rc = HYPERVISOR_memory_op(op, &memmap);
 	if (rc == -ENOSYS) {
+		BUG_ON(xen_initial_domain());
 		memmap.nr_entries = 1;
 		map[0].addr = 0ULL;
 		map[0].size = mem_end;
@@ -201,12 +204,13 @@ char * __init xen_memory_setup(void)
 	}
 
 	/*
-	 * Even though this is normal, usable memory under Xen, reserve
-	 * ISA memory anyway because too many things think they can poke
+	 * In domU, the ISA region is normal, usable memory, but we
+	 * reserve ISA memory anyway because too many things poke
 	 * about in there.
 	 *
-	 * In a dom0 kernel, this region is identity mapped with the
-	 * hardware ISA area, so it really is out of bounds.
+	 * In Dom0, the host E820 information can leave gaps in the
+	 * ISA range, which would cause us to release those pages.  To
+	 * avoid this, we unconditionally reserve them here.
 	 */
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);

commit 18cb657ca1bafe635f368346a1676fb04c512edf
Merge: 2301b65b86df e28c31a96b15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 28 17:11:17 2010 -0700

    Merge branch 'stable/xen-pcifront-0.8.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
      and branch 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm
    
    * 'for-linus' of git://xenbits.xen.org/people/sstabellini/linux-pvhvm:
      xen: register xen pci notifier
      xen: initialize cpu masks for pv guests in xen_smp_init
      xen: add a missing #include to arch/x86/pci/xen.c
      xen: mask the MTRR feature from the cpuid
      xen: make hvc_xen console work for dom0.
      xen: add the direct mapping area for ISA bus access
      xen: Initialize xenbus for dom0.
      xen: use vcpu_ops to setup cpu masks
      xen: map a dummy page for local apic and ioapic in xen_set_fixmap
      xen: remap MSIs into pirqs when running as initial domain
      xen: remap GSIs as pirqs when running as initial domain
      xen: introduce XEN_DOM0 as a silent option
      xen: map MSIs into pirqs
      xen: support GSI -> pirq remapping in PV on HVM guests
      xen: add xen hvm acpi_register_gsi variant
      acpi: use indirect call to register gsi in different modes
      xen: implement xen_hvm_register_pirq
      xen: get the maximum number of pirqs from xen
      xen: support pirq != irq
    
    * 'stable/xen-pcifront-0.8.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen: (27 commits)
      X86/PCI: Remove the dependency on isapnp_disable.
      xen: Update Makefile with CONFIG_BLOCK dependency for biomerge.c
      MAINTAINERS: Add myself to the Xen Hypervisor Interface and remove Chris Wright.
      x86: xen: Sanitse irq handling (part two)
      swiotlb-xen: On x86-32 builts, select SWIOTLB instead of depending on it.
      MAINTAINERS: Add myself for Xen PCI and Xen SWIOTLB maintainer.
      xen/pci: Request ACS when Xen-SWIOTLB is activated.
      xen-pcifront: Xen PCI frontend driver.
      xenbus: prevent warnings on unhandled enumeration values
      xenbus: Xen paravirtualised PCI hotplug support.
      xen/x86/PCI: Add support for the Xen PCI subsystem
      x86: Introduce x86_msi_ops
      msi: Introduce default_[teardown|setup]_msi_irqs with fallback.
      x86/PCI: Export pci_walk_bus function.
      x86/PCI: make sure _PAGE_IOMAP it set on pci mappings
      x86/PCI: Clean up pci_cache_line_size
      xen: fix shared irq device passthrough
      xen: Provide a variant of xen_poll_irq with timeout.
      xen: Find an unbound irq number in reverse order (high to low).
      xen: statically initialize cpu_evtchn_mask_p
      ...
    
    Fix up trivial conflicts in drivers/pci/Makefile

commit 520045db940a381d2bee1c1b2179f7921b40fb10
Merge: 426e1f5cec48 9387377eb79a 45263cb0993d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 26 18:20:19 2010 -0700

    Merge branches 'upstream/xenfs' and 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen
    
    * 'upstream/xenfs' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen:
      xen/privcmd: make privcmd visible in domU
      xen/privcmd: move remap_domain_mfn_range() to core xen code and export.
      privcmd: MMAPBATCH: Fix error handling/reporting
      xenbus: export xen_store_interface for xenfs
      xen/privcmd: make sure vma is ours before doing anything to it
      xen/privcmd: print SIGBUS faults
      xen/xenfs: set_page_dirty is supposed to return true if it dirties
      xen/privcmd: create address space to allow writable mmaps
      xen: add privcmd driver
      xen: add variable hypercall caller
      xen: add xen_set_domain_pte()
      xen: add /proc/xen/xsd_{kva,port} to xenfs
    
    * 'upstream/core' of git://git.kernel.org/pub/scm/linux/kernel/git/jeremy/xen: (29 commits)
      xen: include xen/xen.h for definition of xen_initial_domain()
      xen: use host E820 map for dom0
      xen: correctly rebuild mfn list list after migration.
      xen: improvements to VIRQ_DEBUG output
      xen: set up IRQ before binding virq to evtchn
      xen: ensure that all event channels start off bound to VCPU 0
      xen/hvc: only notify if we actually sent something
      xen: don't add extra_pages for RAM after mem_end
      xen: add support for PAT
      xen: make sure xen_max_p2m_pfn is up to date
      xen: limit extra memory to a certain ratio of base
      xen: add extra pages for E820 RAM regions, even if beyond mem_end
      xen: make sure xen_extra_mem_start is beyond all non-RAM e820
      xen: implement "extra" memory to reserve space for pages not present at boot
      xen: Use host-provided E820 map
      xen: don't map missing memory
      xen: defer building p2m mfn structures until kernel is mapped
      xen: add return value to set_phys_to_machine()
      xen: convert p2m to a 3 level tree
      xen: make install_p2mtop_page() static
      ...
    
    Fix up trivial conflict in arch/x86/xen/mmu.c, and fix the use of
    'reserve_early()' - in the new memblock world order it is now
    'memblock_x86_reserve_range()' instead. Pointed out by Jeremy.

commit 45263cb0993de738e158c625c84a5feb18bed317
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Mon Oct 25 16:32:29 2010 -0700

    xen: include xen/xen.h for definition of xen_initial_domain()
    
              CC      arch/x86/xen/setup.o
            arch/x86/xen/setup.c: In function 'xen_memory_setup':
            arch/x86/xen/setup.c:161: error: implicit declaration of function 'xen_initial_domain'
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 0ce9d58cb29d..8e2c9f21fa37 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -17,6 +17,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/xen/hypercall.h>
 
+#include <xen/xen.h>
 #include <xen/page.h>
 #include <xen/interface/callback.h>
 #include <xen/interface/memory.h>

commit 4ec5387cc36c6472a2ff2c82e9865abe8cab96c2
Author: Juan Quintela <quintela@redhat.com>
Date:   Thu Sep 2 15:45:43 2010 +0100

    xen: add the direct mapping area for ISA bus access
    
    add the direct mapping area for ISA bus access when running as initial
    domain
    
    Signed-off-by: Juan Quintela <quintela@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index c413132702f7..62ceb7864017 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -119,6 +119,9 @@ char * __init xen_memory_setup(void)
 	 * Even though this is normal, usable memory under Xen, reserve
 	 * ISA memory anyway because too many things think they can poke
 	 * about in there.
+	 *
+	 * In a dom0 kernel, this region is identity mapped with the
+	 * hardware ISA area, so it really is out of bounds.
 	 */
 	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
 			E820_RESERVED);

commit 9e9a5fcb04e3af077d1be32710298b852210d93f
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Thu Sep 2 16:16:00 2010 +0100

    xen: use host E820 map for dom0
    
    When running as initial domain, get the real physical memory map from
    xen using the XENMEM_machine_memory_map hypercall and use it to setup
    the e820 regions.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 7a4ab05cff8a..0ce9d58cb29d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -149,6 +149,7 @@ char * __init xen_memory_setup(void)
 	unsigned long extra_pages = 0;
 	unsigned long extra_limit;
 	int i;
+	int op;
 
 	max_pfn = min(MAX_DOMAIN_PAGES, max_pfn);
 	mem_end = PFN_PHYS(max_pfn);
@@ -156,7 +157,10 @@ char * __init xen_memory_setup(void)
 	memmap.nr_entries = E820MAX;
 	set_xen_guest_handle(memmap.buffer, map);
 
-	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
+	op = xen_initial_domain() ?
+		XENMEM_machine_memory_map :
+		XENMEM_memory_map;
+	rc = HYPERVISOR_memory_op(op, &memmap);
 	if (rc == -ENOSYS) {
 		memmap.nr_entries = 1;
 		map[0].addr = 0ULL;
@@ -235,7 +239,8 @@ char * __init xen_memory_setup(void)
 	else
 		extra_pages = 0;
 
-	xen_add_extra_mem(extra_pages);
+	if (!xen_initial_domain())
+		xen_add_extra_mem(extra_pages);
 
 	return "Xen";
 }

commit 3654581e47adc07072aebe239818485b68ea04f0
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Sep 29 16:54:33 2010 -0700

    xen: don't add extra_pages for RAM after mem_end
    
    If an E820 region is entirely beyond mem_end, don't attempt to truncate
    it and add the truncated pages to extra_pages, as they will be negative.
    
    Also, make sure the extra memory region starts after all BIOS provided
    E820 regions (and in the case of RAM regions, post-clipping).
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index cad2fcd130ec..7a4ab05cff8a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -52,20 +52,19 @@ phys_addr_t xen_extra_mem_start, xen_extra_mem_size;
 static __init void xen_add_extra_mem(unsigned long pages)
 {
 	u64 size = (u64)pages * PAGE_SIZE;
+	u64 extra_start = xen_extra_mem_start + xen_extra_mem_size;
 
 	if (!pages)
 		return;
 
-	e820_add_region(xen_extra_mem_start + xen_extra_mem_size, size, E820_RAM);
+	e820_add_region(extra_start, size, E820_RAM);
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
-	reserve_early(xen_extra_mem_start + xen_extra_mem_size,
-		      xen_extra_mem_start + xen_extra_mem_size + size,
-		      "XEN EXTRA");
+	reserve_early(extra_start, extra_start + size, "XEN EXTRA");
 
 	xen_extra_mem_size += size;
 
-	xen_max_p2m_pfn = PFN_DOWN(xen_extra_mem_start + xen_extra_mem_size);
+	xen_max_p2m_pfn = PFN_DOWN(extra_start + size);
 }
 
 static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
@@ -175,15 +174,21 @@ char * __init xen_memory_setup(void)
 		unsigned long long end = map[i].addr + map[i].size;
 
 		if (map[i].type == E820_RAM) {
-			if (end > mem_end) {
+			if (map[i].addr < mem_end && end > mem_end) {
 				/* Truncate region to max_mem. */
-				map[i].size -= end - mem_end;
+				u64 delta = end - mem_end;
 
-				extra_pages += PFN_DOWN(end - mem_end);
+				map[i].size -= delta;
+				extra_pages += PFN_DOWN(delta);
+
+				end = mem_end;
 			}
-		} else if (map[i].type != E820_RAM)
+		}
+
+		if (end > xen_extra_mem_start)
 			xen_extra_mem_start = end;
 
+		/* If region is non-RAM or below mem_end, add what remains */
 		if ((map[i].type != E820_RAM || map[i].addr < mem_end) &&
 		    map[i].size > 0)
 			e820_add_region(map[i].addr, map[i].size, map[i].type);

commit 2f7acb208523a3bf5f1830f01c29f7feda045169
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Sep 15 13:32:49 2010 -0700

    xen: make sure xen_max_p2m_pfn is up to date
    
    Keep xen_max_p2m_pfn up to date with the end of the extra memory
    we're adding.  It is possible that it will be too high since memory
    may be truncated by a "mem=" option on the kernel command line, but
    that won't matter.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 6c9039e92f81..cad2fcd130ec 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -64,6 +64,8 @@ static __init void xen_add_extra_mem(unsigned long pages)
 		      "XEN EXTRA");
 
 	xen_extra_mem_size += size;
+
+	xen_max_p2m_pfn = PFN_DOWN(xen_extra_mem_start + xen_extra_mem_size);
 }
 
 static unsigned long __init xen_release_chunk(phys_addr_t start_addr,

commit 698bb8d14a5b577b6841acaccdf5095d3b7c7389
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Tue Sep 14 10:19:14 2010 -0700

    xen: limit extra memory to a certain ratio of base
    
    If extra memory is very much larger than the base memory size
    then all of the base memory can be filled with structures reserved to
    describe the extra memory, leaving no space for anything else.
    
    Even at the maximum ratio there will be little space for anything else,
    but this change is intended to at least allow the system to boot rather
    than crash mysteriously.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 1e85e26efa69..6c9039e92f81 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -37,6 +37,18 @@ extern void xen_syscall32_target(void);
 /* Amount of extra memory space we add to the e820 ranges */
 phys_addr_t xen_extra_mem_start, xen_extra_mem_size;
 
+/* 
+ * The maximum amount of extra memory compared to the base size.  The
+ * main scaling factor is the size of struct page.  At extreme ratios
+ * of base:extra, all the base memory can be filled with page
+ * structures for the extra memory, leaving no space for anything
+ * else.
+ * 
+ * 10x seems like a reasonable balance between scaling flexibility and
+ * leaving a practically usable system.
+ */
+#define EXTRA_MEM_RATIO		(10)
+
 static __init void xen_add_extra_mem(unsigned long pages)
 {
 	u64 size = (u64)pages * PAGE_SIZE;
@@ -134,6 +146,7 @@ char * __init xen_memory_setup(void)
 	int rc;
 	struct xen_memory_map memmap;
 	unsigned long extra_pages = 0;
+	unsigned long extra_limit;
 	int i;
 
 	max_pfn = min(MAX_DOMAIN_PAGES, max_pfn);
@@ -196,6 +209,25 @@ char * __init xen_memory_setup(void)
 
 	extra_pages += xen_return_unused_memory(xen_start_info->nr_pages, &e820);
 
+	/*
+	 * Clamp the amount of extra memory to a EXTRA_MEM_RATIO
+	 * factor the base size.  On non-highmem systems, the base
+	 * size is the full initial memory allocation; on highmem it
+	 * is limited to the max size of lowmem, so that it doesn't
+	 * get completely filled.
+	 *
+	 * In principle there could be a problem in lowmem systems if
+	 * the initial memory is also very large with respect to
+	 * lowmem, but we won't try to deal with that here.
+	 */
+	extra_limit = min(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),
+			  max_pfn + extra_pages);
+
+	if (extra_limit >= max_pfn)
+		extra_pages = extra_limit - max_pfn;
+	else
+		extra_pages = 0;
+
 	xen_add_extra_mem(extra_pages);
 
 	return "Xen";

commit b5b43ced7a6e79d30df3232b37dc82c5d8dfa843
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Thu Sep 2 17:10:12 2010 -0700

    xen: add extra pages for E820 RAM regions, even if beyond mem_end
    
    If an entire E820 RAM region is beyond mem_end, still add its
    pages to the extra area so that space can be used by the kernel.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index eac010008cd2..1e85e26efa69 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -158,9 +158,8 @@ char * __init xen_memory_setup(void)
 	xen_extra_mem_start = mem_end;
 	for (i = 0; i < memmap.nr_entries; i++) {
 		unsigned long long end = map[i].addr + map[i].size;
+
 		if (map[i].type == E820_RAM) {
-			if (map[i].addr > mem_end)
-				continue;
 			if (end > mem_end) {
 				/* Truncate region to max_mem. */
 				map[i].size -= end - mem_end;
@@ -169,7 +168,9 @@ char * __init xen_memory_setup(void)
 			}
 		} else if (map[i].type != E820_RAM)
 			xen_extra_mem_start = end;
-		if (map[i].size > 0)
+
+		if ((map[i].type != E820_RAM || map[i].addr < mem_end) &&
+		    map[i].size > 0)
 			e820_add_region(map[i].addr, map[i].size, map[i].type);
 	}
 

commit 36bc251b87f88147e9d8346e4b431f42353c3d38
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Thu Sep 2 17:07:03 2010 -0700

    xen: make sure xen_extra_mem_start is beyond all non-RAM e820
    
    If Xen gives us non-RAM E820 entries (dom0 only, typically), then
    make sure the extra RAM region is beyond them.  It's OK for
    the extra space to grow into E820 regions, however.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f9a99eaddcdc..eac010008cd2 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -167,7 +167,8 @@ char * __init xen_memory_setup(void)
 
 				extra_pages += PFN_DOWN(end - mem_end);
 			}
-		}
+		} else if (map[i].type != E820_RAM)
+			xen_extra_mem_start = end;
 		if (map[i].size > 0)
 			e820_add_region(map[i].addr, map[i].size, map[i].type);
 	}

commit 42ee1471e9b879479a15debac752314a596c738e
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Aug 30 16:41:02 2010 -0700

    xen: implement "extra" memory to reserve space for pages not present at boot
    
    When using the e820 map to get the initial pseudo-physical address space,
    look for either Xen-provided memory which doesn't lie within an E820
    region, or an E820 RAM region which extends beyond the Xen-provided
    memory range.
    
    Count these pages, and add them to a new "extra memory" range.  This range
    has an E820 RAM range to describe it - so the kernel will allocate page
    structures for it - but it is also marked reserved so that the kernel
    will not attempt to use it.
    
    The balloon driver can then add this range as a set of currently
    ballooned-out pages, which can be used to extend the domain beyond its
    original size.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index dd2eb2a9303f..f9a99eaddcdc 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -34,6 +34,26 @@ extern void xen_sysenter_target(void);
 extern void xen_syscall_target(void);
 extern void xen_syscall32_target(void);
 
+/* Amount of extra memory space we add to the e820 ranges */
+phys_addr_t xen_extra_mem_start, xen_extra_mem_size;
+
+static __init void xen_add_extra_mem(unsigned long pages)
+{
+	u64 size = (u64)pages * PAGE_SIZE;
+
+	if (!pages)
+		return;
+
+	e820_add_region(xen_extra_mem_start + xen_extra_mem_size, size, E820_RAM);
+	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
+
+	reserve_early(xen_extra_mem_start + xen_extra_mem_size,
+		      xen_extra_mem_start + xen_extra_mem_size + size,
+		      "XEN EXTRA");
+
+	xen_extra_mem_size += size;
+}
+
 static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
 					      phys_addr_t end_addr)
 {
@@ -105,7 +125,6 @@ static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.
  **/
-
 char * __init xen_memory_setup(void)
 {
 	static struct e820entry map[E820MAX] __initdata;
@@ -114,6 +133,7 @@ char * __init xen_memory_setup(void)
 	unsigned long long mem_end;
 	int rc;
 	struct xen_memory_map memmap;
+	unsigned long extra_pages = 0;
 	int i;
 
 	max_pfn = min(MAX_DOMAIN_PAGES, max_pfn);
@@ -135,6 +155,7 @@ char * __init xen_memory_setup(void)
 	BUG_ON(rc);
 
 	e820.nr_map = 0;
+	xen_extra_mem_start = mem_end;
 	for (i = 0; i < memmap.nr_entries; i++) {
 		unsigned long long end = map[i].addr + map[i].size;
 		if (map[i].type == E820_RAM) {
@@ -143,6 +164,8 @@ char * __init xen_memory_setup(void)
 			if (end > mem_end) {
 				/* Truncate region to max_mem. */
 				map[i].size -= end - mem_end;
+
+				extra_pages += PFN_DOWN(end - mem_end);
 			}
 		}
 		if (map[i].size > 0)
@@ -169,7 +192,9 @@ char * __init xen_memory_setup(void)
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
-	xen_return_unused_memory(xen_start_info->nr_pages, &e820);
+	extra_pages += xen_return_unused_memory(xen_start_info->nr_pages, &e820);
+
+	xen_add_extra_mem(extra_pages);
 
 	return "Xen";
 }

commit 35ae11fd146384d222f3bb1f17eed1970cc92c36
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Fri Feb 6 19:09:48 2009 -0800

    xen: Use host-provided E820 map
    
    Rather than simply using a flat memory map from Xen, use its provided
    E820 map.  This allows the domain builder to tell the domain to reserve
    space for more pages than those initially provided at domain-build time.
    
    It also allows the host to specify holes in the address space (for
    PCI-passthrough, for example).
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 328b00305426..dd2eb2a9303f 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -19,6 +19,7 @@
 
 #include <xen/page.h>
 #include <xen/interface/callback.h>
+#include <xen/interface/memory.h>
 #include <xen/interface/physdev.h>
 #include <xen/interface/memory.h>
 #include <xen/features.h>
@@ -107,13 +108,46 @@ static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
 
 char * __init xen_memory_setup(void)
 {
+	static struct e820entry map[E820MAX] __initdata;
+
 	unsigned long max_pfn = xen_start_info->nr_pages;
+	unsigned long long mem_end;
+	int rc;
+	struct xen_memory_map memmap;
+	int i;
 
 	max_pfn = min(MAX_DOMAIN_PAGES, max_pfn);
+	mem_end = PFN_PHYS(max_pfn);
+
+	memmap.nr_entries = E820MAX;
+	set_xen_guest_handle(memmap.buffer, map);
+
+	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
+	if (rc == -ENOSYS) {
+		memmap.nr_entries = 1;
+		map[0].addr = 0ULL;
+		map[0].size = mem_end;
+		/* 8MB slack (to balance backend allocations). */
+		map[0].size += 8ULL << 20;
+		map[0].type = E820_RAM;
+		rc = 0;
+	}
+	BUG_ON(rc);
 
 	e820.nr_map = 0;
-
-	e820_add_region(0, PFN_PHYS((u64)max_pfn), E820_RAM);
+	for (i = 0; i < memmap.nr_entries; i++) {
+		unsigned long long end = map[i].addr + map[i].size;
+		if (map[i].type == E820_RAM) {
+			if (map[i].addr > mem_end)
+				continue;
+			if (end > mem_end) {
+				/* Truncate region to max_mem. */
+				map[i].size -= end - mem_end;
+			}
+		}
+		if (map[i].size > 0)
+			e820_add_region(map[i].addr, map[i].size, map[i].type);
+	}
 
 	/*
 	 * Even though this is normal, usable memory under Xen, reserve

commit 23ace955c22cb9bdf703e4bdc9bf7379166113cd
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Mon Feb 9 12:05:46 2009 -0800

    xen: Don't disable the I/O space
    
    If a guest domain wants to access PCI devices through the frontend
    driver (coming later in the patch series), it will need access to the
    I/O space.
    
    [ Impact: Allow for domU IO access, preparing for pci passthrough ]
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 328b00305426..c413132702f7 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -260,7 +260,5 @@ void __init xen_arch_setup(void)
 
 	pm_idle = xen_idle;
 
-	paravirt_disable_iospace();
-
 	fiddle_vdso();
 }

commit daab7fc734a53fdeaf844b7c03053118ad1769da
Merge: 774ea0bcb27f 2bfc96a127bc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Aug 31 09:45:21 2010 +0200

    Merge commit 'v2.6.36-rc3' into x86/memblock
    
    Conflicts:
            arch/x86/kernel/trampoline.c
            mm/memblock.c
    
    Merge reason: Resolve the conflicts, update to latest upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a9ce6bc15100023b411f8117e53a016d61889800
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Wed Aug 25 13:39:17 2010 -0700

    x86, memblock: Replace e820_/_early string with memblock_
    
    1.include linux/memblock.h directly. so later could reduce e820.h reference.
    2 this patch is done by sed scripts mainly
    
    -v2: use MEMBLOCK_ERROR instead of -1ULL or -1UL
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ad0047f47cd4..2ac8f29f89cb 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -8,6 +8,7 @@
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/pm.h>
+#include <linux/memblock.h>
 
 #include <asm/elf.h>
 #include <asm/vdso.h>
@@ -61,7 +62,7 @@ char * __init xen_memory_setup(void)
 	 *  - xen_start_info
 	 * See comment above "struct start_info" in <xen/interface/xen.h>
 	 */
-	reserve_early(__pa(xen_start_info->mfn_list),
+	memblock_x86_reserve_range(__pa(xen_start_info->mfn_list),
 		      __pa(xen_start_info->pt_base),
 			"XEN START INFO");
 

commit f09f6d194d85043e0eb105a577e7ad6d8170ab66
Author: Donald Dutile <ddutile@redhat.com>
Date:   Thu Jul 15 14:56:49 2010 -0400

    Xen: register panic notifier to take crashes of xen guests on panic
    
    Register a panic notifier so that when the guest crashes it can shut
    down the domain and indicate it was a crash to the host.
    
    Signed-off-by: Donald Dutile <ddutile@redhat.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 9deb6bab6c78..328b00305426 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -226,6 +226,8 @@ void __init xen_arch_setup(void)
 	struct physdev_set_iopl set_iopl;
 	int rc;
 
+	xen_panic_handler_init();
+
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_4gb_segments);
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_writable_pagetables);
 

commit f89e048e76da7ac0b4c89e75606ca7a3422886b1
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Sep 16 12:38:33 2009 -0700

    xen: make sure pages are really part of domain before freeing
    
    Scan the set of pages we're freeing and make sure they're actually
    owned by the domain before freeing.  This generally won't happen on a
    domU (since Xen gives us contigious memory), but it could happen if
    there are some hardware mappings passed through.
    
    We only bother going up to the highest page Xen actually claimed to
    give us, since there's definitely nothing of ours above that.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index e0942630d47a..9deb6bab6c78 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -33,52 +33,69 @@ extern void xen_sysenter_target(void);
 extern void xen_syscall_target(void);
 extern void xen_syscall32_target(void);
 
-static unsigned long __init xen_release_chunk(phys_addr_t start_addr, phys_addr_t end_addr)
+static unsigned long __init xen_release_chunk(phys_addr_t start_addr,
+					      phys_addr_t end_addr)
 {
 	struct xen_memory_reservation reservation = {
 		.address_bits = 0,
 		.extent_order = 0,
 		.domid        = DOMID_SELF
 	};
-	unsigned long *mfn_list = (unsigned long *)xen_start_info->mfn_list;
 	unsigned long start, end;
-	unsigned long len;
+	unsigned long len = 0;
 	unsigned long pfn;
 	int ret;
 
 	start = PFN_UP(start_addr);
-	end = PFN_UP(end_addr);
+	end = PFN_DOWN(end_addr);
 
 	if (end <= start)
 		return 0;
 
-	len = end - start;
-
-	set_xen_guest_handle(reservation.extent_start, &mfn_list[start]);
-	reservation.nr_extents = len;
-
-	ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation, &reservation);
-	WARN(ret != (end - start), "Failed to release memory %lx-%lx err=%d\n",
-	     start, end, ret);
-
-	for(pfn = start; pfn < end; pfn++)
-		set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+	printk(KERN_INFO "xen_release_chunk: looking at area pfn %lx-%lx: ",
+	       start, end);
+	for(pfn = start; pfn < end; pfn++) {
+		unsigned long mfn = pfn_to_mfn(pfn);
+
+		/* Make sure pfn exists to start with */
+		if (mfn == INVALID_P2M_ENTRY || mfn_to_pfn(mfn) != pfn)
+			continue;
+
+		set_xen_guest_handle(reservation.extent_start, &mfn);
+		reservation.nr_extents = 1;
+
+		ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation,
+					   &reservation);
+		WARN(ret != 1, "Failed to release memory %lx-%lx err=%d\n",
+		     start, end, ret);
+		if (ret == 1) {
+			set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+			len++;
+		}
+	}
+	printk(KERN_CONT "%ld pages freed\n", len);
 
 	return len;
 }
 
-static unsigned long __init xen_return_unused_memory(const struct e820map *e820)
+static unsigned long __init xen_return_unused_memory(unsigned long max_pfn,
+						     const struct e820map *e820)
 {
-	unsigned long last_end = 0;
+	phys_addr_t max_addr = PFN_PHYS(max_pfn);
+	phys_addr_t last_end = 0;
 	unsigned long released = 0;
 	int i;
 
-	for (i = 0; i < e820->nr_map; i++) {
-		released += xen_release_chunk(last_end, e820->map[i].addr);
+	for (i = 0; i < e820->nr_map && last_end < max_addr; i++) {
+		phys_addr_t end = e820->map[i].addr;
+		end = min(max_addr, end);
+
+		released += xen_release_chunk(last_end, end);
 		last_end = e820->map[i].addr + e820->map[i].size;
 	}
 
-	released += xen_release_chunk(last_end, PFN_PHYS(xen_start_info->nr_pages));
+	if (last_end < max_addr)
+		released += xen_release_chunk(last_end, max_addr);
 
 	printk(KERN_INFO "released %ld pages of unused memory\n", released);
 	return released;
@@ -118,7 +135,7 @@ char * __init xen_memory_setup(void)
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
-	xen_return_unused_memory(&e820);
+	xen_return_unused_memory(xen_start_info->nr_pages, &e820);
 
 	return "Xen";
 }

commit 093d7b4639951ea3021a6f70d376c3ff31f4740c
Author: Miroslav Rezanina <mrezanin@redhat.com>
Date:   Wed Sep 16 03:56:17 2009 -0400

    xen: release unused free memory
    
    Scan an e820 table and release any memory which lies between e820 entries,
    as it won't be used and would just be wasted.  At present this is just to
    release any memory beyond the end of the e820 map, but it will also deal
    with holes being punched in the map.
    
    Derived from patch by Miroslav Rezanina <mrezanin@redhat.com>
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index ad0047f47cd4..e0942630d47a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -20,6 +20,7 @@
 #include <xen/page.h>
 #include <xen/interface/callback.h>
 #include <xen/interface/physdev.h>
+#include <xen/interface/memory.h>
 #include <xen/features.h>
 
 #include "xen-ops.h"
@@ -32,6 +33,56 @@ extern void xen_sysenter_target(void);
 extern void xen_syscall_target(void);
 extern void xen_syscall32_target(void);
 
+static unsigned long __init xen_release_chunk(phys_addr_t start_addr, phys_addr_t end_addr)
+{
+	struct xen_memory_reservation reservation = {
+		.address_bits = 0,
+		.extent_order = 0,
+		.domid        = DOMID_SELF
+	};
+	unsigned long *mfn_list = (unsigned long *)xen_start_info->mfn_list;
+	unsigned long start, end;
+	unsigned long len;
+	unsigned long pfn;
+	int ret;
+
+	start = PFN_UP(start_addr);
+	end = PFN_UP(end_addr);
+
+	if (end <= start)
+		return 0;
+
+	len = end - start;
+
+	set_xen_guest_handle(reservation.extent_start, &mfn_list[start]);
+	reservation.nr_extents = len;
+
+	ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation, &reservation);
+	WARN(ret != (end - start), "Failed to release memory %lx-%lx err=%d\n",
+	     start, end, ret);
+
+	for(pfn = start; pfn < end; pfn++)
+		set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+
+	return len;
+}
+
+static unsigned long __init xen_return_unused_memory(const struct e820map *e820)
+{
+	unsigned long last_end = 0;
+	unsigned long released = 0;
+	int i;
+
+	for (i = 0; i < e820->nr_map; i++) {
+		released += xen_release_chunk(last_end, e820->map[i].addr);
+		last_end = e820->map[i].addr + e820->map[i].size;
+	}
+
+	released += xen_release_chunk(last_end, PFN_PHYS(xen_start_info->nr_pages));
+
+	printk(KERN_INFO "released %ld pages of unused memory\n", released);
+	return released;
+}
 
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.
@@ -67,6 +118,8 @@ char * __init xen_memory_setup(void)
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 
+	xen_return_unused_memory(&e820);
+
 	return "Xen";
 }
 

commit 6b2e8523df148c15ea5abf13075026fb8bdb3f86
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu May 7 11:56:49 2009 -0700

    xen: reserve Xen start_info rather than e820 reserving
    
    Use reserve_early rather than e820 reservations for Xen start info and mfn->pfn
    table, so that the memory use is a bit more self-documenting.
    
    [ Impact: cleanup ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Xen-devel <xen-devel@lists.xensource.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <4A032EF1.6070708@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 15c6c68db6a2..ad0047f47cd4 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -61,9 +61,9 @@ char * __init xen_memory_setup(void)
 	 *  - xen_start_info
 	 * See comment above "struct start_info" in <xen/interface/xen.h>
 	 */
-	e820_add_region(__pa(xen_start_info->mfn_list),
-			xen_start_info->pt_base - xen_start_info->mfn_list,
-			E820_RESERVED);
+	reserve_early(__pa(xen_start_info->mfn_list),
+		      __pa(xen_start_info->pt_base),
+			"XEN START INFO");
 
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
 

commit f63c2f248959366cd11bfa476f866737047cf663
Author: Tej <bewith.tej@gmail.com>
Date:   Tue Dec 16 11:56:06 2008 -0800

    xen: whitespace/checkpatch cleanup
    
    Impact: cleanup
    
    Signed-off-by: Tej <bewith.tej@gmail.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index d67901083888..15c6c68db6a2 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -28,6 +28,9 @@
 /* These are code, but not functions.  Defined in entry.S */
 extern const char xen_hypervisor_callback[];
 extern const char xen_failsafe_callback[];
+extern void xen_sysenter_target(void);
+extern void xen_syscall_target(void);
+extern void xen_syscall32_target(void);
 
 
 /**
@@ -110,7 +113,6 @@ static __cpuinit int register_callback(unsigned type, const void *func)
 
 void __cpuinit xen_enable_sysenter(void)
 {
-	extern void xen_sysenter_target(void);
 	int ret;
 	unsigned sysenter_feature;
 
@@ -132,8 +134,6 @@ void __cpuinit xen_enable_syscall(void)
 {
 #ifdef CONFIG_X86_64
 	int ret;
-	extern void xen_syscall_target(void);
-	extern void xen_syscall32_target(void);
 
 	ret = register_callback(CALLBACKTYPE_syscall, xen_syscall_target);
 	if (ret != 0) {
@@ -160,7 +160,8 @@ void __init xen_arch_setup(void)
 	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_writable_pagetables);
 
 	if (!xen_feature(XENFEAT_auto_translated_physmap))
-		HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_pae_extended_cr3);
+		HYPERVISOR_vm_assist(VMASST_CMD_enable,
+				     VMASST_TYPE_pae_extended_cr3);
 
 	if (register_callback(CALLBACKTYPE_event, xen_hypervisor_callback) ||
 	    register_callback(CALLBACKTYPE_failsafe, xen_failsafe_callback))

commit 5670a43d710a12fcbbfaefd3991002768b488d82
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sun Sep 14 07:42:23 2008 -0700

    xen: fix for xen guest with mem > 3.7G
    
    PFN_PHYS() can truncate large addresses unless its passed a suitable
    large type.  This is fixed more generally in the patch series
    introducing phys_addr_t, but we need a short-term fix to solve a
    Xen regression reported by Roberto De Ioris.
    
    Reported-by: Roberto De Ioris <roberto@unbit.it>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index b6acc3a0af46..d67901083888 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -42,7 +42,7 @@ char * __init xen_memory_setup(void)
 
 	e820.nr_map = 0;
 
-	e820_add_region(0, PFN_PHYS(max_pfn), E820_RAM);
+	e820_add_region(0, PFN_PHYS((u64)max_pfn), E820_RAM);
 
 	/*
 	 * Even though this is normal, usable memory under Xen, reserve

commit d5303b811b9d6dad2e7396d545eb7db414d42a61
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sat Jul 12 02:22:06 2008 -0700

    x86: xen: no need to disable vdso32
    
    Now that the vdso32 code can cope with both syscall and sysenter
    missing for 32-bit compat processes, just disable the features without
    disabling vdso altogether.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index e3648e64a637..b6acc3a0af46 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -137,7 +137,7 @@ void __cpuinit xen_enable_syscall(void)
 
 	ret = register_callback(CALLBACKTYPE_syscall, xen_syscall_target);
 	if (ret != 0) {
-		printk(KERN_ERR "Failed to set syscall: %d\n", ret);
+		printk(KERN_ERR "Failed to set syscall callback: %d\n", ret);
 		/* Pretty fatal; 64-bit userspace has no other
 		   mechanism for syscalls. */
 	}
@@ -145,13 +145,8 @@ void __cpuinit xen_enable_syscall(void)
 	if (boot_cpu_has(X86_FEATURE_SYSCALL32)) {
 		ret = register_callback(CALLBACKTYPE_syscall32,
 					xen_syscall32_target);
-		if (ret != 0) {
-			printk(KERN_INFO "Xen: 32-bit syscall not supported: disabling vdso\n");
+		if (ret != 0)
 			setup_clear_cpu_cap(X86_FEATURE_SYSCALL32);
-#ifdef CONFIG_COMPAT
-			sysctl_vsyscall32 = 0;
-#endif
-		}
 	}
 #endif /* CONFIG_X86_64 */
 }

commit 6a52e4b1cddd90fbfde8fb67021657936ee74b07
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sat Jul 12 02:22:00 2008 -0700

    x86_64: further cleanup of 32-bit compat syscall mechanisms
    
    AMD only supports "syscall" from 32-bit compat usermode.
    Intel and Centaur(?) only support "sysenter" from 32-bit compat usermode.
    
    Set the X86 feature bits accordingly, and set up the vdso in
    accordance with those bits.  On the offchance we run on in a 64-bit
    environment which supports neither syscall nor sysenter from 32-bit
    mode, then fall back to the int $0x80 vdso.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 3e11779755c3..e3648e64a637 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -83,12 +83,16 @@ static void xen_idle(void)
 
 /*
  * Set the bit indicating "nosegneg" library variants should be used.
+ * We only need to bother in pure 32-bit mode; compat 32-bit processes
+ * can have un-truncated segments, so wrapping around is allowed.
  */
 static void __init fiddle_vdso(void)
 {
-#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)
-	extern const char vdso32_default_start;
-	u32 *mask = VDSO32_SYMBOL(&vdso32_default_start, NOTE_MASK);
+#ifdef CONFIG_X86_32
+	u32 *mask;
+	mask = VDSO32_SYMBOL(&vdso32_int80_start, NOTE_MASK);
+	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
+	mask = VDSO32_SYMBOL(&vdso32_sysenter_start, NOTE_MASK);
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
 #endif
 }

commit 71415c6a0877d5944d5dc3060f3b03513746158d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 11 22:41:34 2008 +0200

    x86, xen, vdso: fix build error
    
    fix:
    
       arch/x86/xen/built-in.o: In function `xen_enable_syscall':
       (.cpuinit.text+0xdb): undefined reference to `sysctl_vsyscall32'
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 9cce4a92aac0..3e11779755c3 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -144,7 +144,9 @@ void __cpuinit xen_enable_syscall(void)
 		if (ret != 0) {
 			printk(KERN_INFO "Xen: 32-bit syscall not supported: disabling vdso\n");
 			setup_clear_cpu_cap(X86_FEATURE_SYSCALL32);
+#ifdef CONFIG_COMPAT
 			sysctl_vsyscall32 = 0;
+#endif
 		}
 	}
 #endif /* CONFIG_X86_64 */

commit 62541c376668042e20122864a044360707b2fb82
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Jul 10 16:24:08 2008 -0700

    xen64: disable 32-bit syscall/sysenter if not supported.
    
    Old versions of Xen (3.1 and before) don't support sysenter or syscall
    from 32-bit compat userspaces.  If we can't set the appropriate
    syscall callback, then disable the corresponding feature bit, which
    will cause the vdso32 setup to fall back appropriately.
    
    Linux assumes that syscall is always available to 32-bit userspace,
    and installs it by default if sysenter isn't available.  In that case,
    we just disable vdso altogether, forcing userspace libc to fall back
    to int $0x80.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 9d7a14402895..9cce4a92aac0 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -106,46 +106,46 @@ static __cpuinit int register_callback(unsigned type, const void *func)
 
 void __cpuinit xen_enable_sysenter(void)
 {
-	int cpu = smp_processor_id();
 	extern void xen_sysenter_target(void);
 	int ret;
+	unsigned sysenter_feature;
 
 #ifdef CONFIG_X86_32
-	if (!boot_cpu_has(X86_FEATURE_SEP)) {
-		return;
-	}
+	sysenter_feature = X86_FEATURE_SEP;
 #else
-	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL &&
-	    boot_cpu_data.x86_vendor != X86_VENDOR_CENTAUR) {
-		return;
-	}
+	sysenter_feature = X86_FEATURE_SYSENTER32;
 #endif
 
+	if (!boot_cpu_has(sysenter_feature))
+		return;
+
 	ret = register_callback(CALLBACKTYPE_sysenter, xen_sysenter_target);
-	if(ret != 0) {
-		clear_cpu_cap(&cpu_data(cpu), X86_FEATURE_SEP);
-		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_SEP);
-	}
+	if(ret != 0)
+		setup_clear_cpu_cap(sysenter_feature);
 }
 
 void __cpuinit xen_enable_syscall(void)
 {
 #ifdef CONFIG_X86_64
-	int cpu = smp_processor_id();
 	int ret;
 	extern void xen_syscall_target(void);
 	extern void xen_syscall32_target(void);
 
 	ret = register_callback(CALLBACKTYPE_syscall, xen_syscall_target);
 	if (ret != 0) {
-		printk("failed to set syscall: %d\n", ret);
-		clear_cpu_cap(&cpu_data(cpu), X86_FEATURE_SYSCALL);
-		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_SYSCALL);
-	} else {
+		printk(KERN_ERR "Failed to set syscall: %d\n", ret);
+		/* Pretty fatal; 64-bit userspace has no other
+		   mechanism for syscalls. */
+	}
+
+	if (boot_cpu_has(X86_FEATURE_SYSCALL32)) {
 		ret = register_callback(CALLBACKTYPE_syscall32,
 					xen_syscall32_target);
-		if (ret != 0)
-			printk("failed to set 32-bit syscall: %d\n", ret);
+		if (ret != 0) {
+			printk(KERN_INFO "Xen: 32-bit syscall not supported: disabling vdso\n");
+			setup_clear_cpu_cap(X86_FEATURE_SYSCALL32);
+			sysctl_vsyscall32 = 0;
+		}
 	}
 #endif /* CONFIG_X86_64 */
 }

commit 6fcac6d305e8238939e169f4c52e8ec8a552a31f
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:14 2008 -0700

    xen64: set up syscall and sysenter entrypoints for 64-bit
    
    We set up entrypoints for syscall and sysenter.  sysenter is only used
    for 32-bit compat processes, whereas syscall can be used in by both 32
    and 64-bit processes.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index bea3d4f779db..9d7a14402895 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -86,9 +86,11 @@ static void xen_idle(void)
  */
 static void __init fiddle_vdso(void)
 {
+#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)
 	extern const char vdso32_default_start;
 	u32 *mask = VDSO32_SYMBOL(&vdso32_default_start, NOTE_MASK);
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
+#endif
 }
 
 static __cpuinit int register_callback(unsigned type, const void *func)
@@ -106,15 +108,48 @@ void __cpuinit xen_enable_sysenter(void)
 {
 	int cpu = smp_processor_id();
 	extern void xen_sysenter_target(void);
+	int ret;
+
+#ifdef CONFIG_X86_32
+	if (!boot_cpu_has(X86_FEATURE_SEP)) {
+		return;
+	}
+#else
+	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL &&
+	    boot_cpu_data.x86_vendor != X86_VENDOR_CENTAUR) {
+		return;
+	}
+#endif
 
-	if (!boot_cpu_has(X86_FEATURE_SEP) ||
-	    register_callback(CALLBACKTYPE_sysenter,
-			      xen_sysenter_target) != 0) {
+	ret = register_callback(CALLBACKTYPE_sysenter, xen_sysenter_target);
+	if(ret != 0) {
 		clear_cpu_cap(&cpu_data(cpu), X86_FEATURE_SEP);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_SEP);
 	}
 }
 
+void __cpuinit xen_enable_syscall(void)
+{
+#ifdef CONFIG_X86_64
+	int cpu = smp_processor_id();
+	int ret;
+	extern void xen_syscall_target(void);
+	extern void xen_syscall32_target(void);
+
+	ret = register_callback(CALLBACKTYPE_syscall, xen_syscall_target);
+	if (ret != 0) {
+		printk("failed to set syscall: %d\n", ret);
+		clear_cpu_cap(&cpu_data(cpu), X86_FEATURE_SYSCALL);
+		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_SYSCALL);
+	} else {
+		ret = register_callback(CALLBACKTYPE_syscall32,
+					xen_syscall32_target);
+		if (ret != 0)
+			printk("failed to set 32-bit syscall: %d\n", ret);
+	}
+#endif /* CONFIG_X86_64 */
+}
+
 void __init xen_arch_setup(void)
 {
 	struct physdev_set_iopl set_iopl;
@@ -131,6 +166,7 @@ void __init xen_arch_setup(void)
 		BUG();
 
 	xen_enable_sysenter();
+	xen_enable_syscall();
 
 	set_iopl.iopl = 1;
 	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);

commit 88459d4c7eb68c4a15609e00e5d100e2a305f040
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:07:02 2008 -0700

    xen64: register callbacks in arch-independent way
    
    Use callback_op hypercall to register callbacks in a 32/64-bit
    independent way (64-bit doesn't need a code segment, but that detail
    is hidden in XEN_CALLBACK).
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f52f3855fb6b..bea3d4f779db 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -91,19 +91,25 @@ static void __init fiddle_vdso(void)
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
 }
 
-void xen_enable_sysenter(void)
+static __cpuinit int register_callback(unsigned type, const void *func)
 {
-	int cpu = smp_processor_id();
-	extern void xen_sysenter_target(void);
-	/* Mask events on entry, even though they get enabled immediately */
-	static struct callback_register sysenter = {
-		.type = CALLBACKTYPE_sysenter,
-		.address = XEN_CALLBACK(__KERNEL_CS, xen_sysenter_target),
+	struct callback_register callback = {
+		.type = type,
+		.address = XEN_CALLBACK(__KERNEL_CS, func),
 		.flags = CALLBACKF_mask_events,
 	};
 
+	return HYPERVISOR_callback_op(CALLBACKOP_register, &callback);
+}
+
+void __cpuinit xen_enable_sysenter(void)
+{
+	int cpu = smp_processor_id();
+	extern void xen_sysenter_target(void);
+
 	if (!boot_cpu_has(X86_FEATURE_SEP) ||
-	    HYPERVISOR_callback_op(CALLBACKOP_register, &sysenter) != 0) {
+	    register_callback(CALLBACKTYPE_sysenter,
+			      xen_sysenter_target) != 0) {
 		clear_cpu_cap(&cpu_data(cpu), X86_FEATURE_SEP);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_SEP);
 	}
@@ -120,8 +126,9 @@ void __init xen_arch_setup(void)
 	if (!xen_feature(XENFEAT_auto_translated_physmap))
 		HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_pae_extended_cr3);
 
-	HYPERVISOR_set_callbacks(__KERNEL_CS, (unsigned long)xen_hypervisor_callback,
-				 __KERNEL_CS, (unsigned long)xen_failsafe_callback);
+	if (register_callback(CALLBACKTYPE_event, xen_hypervisor_callback) ||
+	    register_callback(CALLBACKTYPE_failsafe, xen_failsafe_callback))
+		BUG();
 
 	xen_enable_sysenter();
 

commit c7b75947f89d45493562ede6d9ee7311dfa5c4ce
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Jul 8 15:06:43 2008 -0700

    xen64: smp.c compile hacking
    
    A number of random changes to make xen/smp.c compile in 64-bit mode.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>a
    Cc: Stephen Tweedie <sct@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index e0a39595bde3..f52f3855fb6b 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -98,7 +98,7 @@ void xen_enable_sysenter(void)
 	/* Mask events on entry, even though they get enabled immediately */
 	static struct callback_register sysenter = {
 		.type = CALLBACKTYPE_sysenter,
-		.address = { __KERNEL_CS, (unsigned long)xen_sysenter_target },
+		.address = XEN_CALLBACK(__KERNEL_CS, xen_sysenter_target),
 		.flags = CALLBACKF_mask_events,
 	};
 
@@ -143,11 +143,6 @@ void __init xen_arch_setup(void)
 
 	pm_idle = xen_idle;
 
-#ifdef CONFIG_SMP
-	/* fill cpus_possible with all available cpus */
-	xen_fill_possible_map();
-#endif
-
 	paravirt_disable_iospace();
 
 	fiddle_vdso();

commit b792c755907cffceab393585b626ef2553c38538
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jun 16 14:54:49 2008 -0700

    xen: reserve ISA space in e820 map
    
    [ TODO: release the underlying memory back to Xen. ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: the arch/x86 maintainers <x86@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index dc2ca8ad3603..e0a39595bde3 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -13,6 +13,7 @@
 #include <asm/vdso.h>
 #include <asm/e820.h>
 #include <asm/setup.h>
+#include <asm/acpi.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/xen/hypercall.h>
 
@@ -41,8 +42,15 @@ char * __init xen_memory_setup(void)
 
 	e820.nr_map = 0;
 
-	e820_add_region(0, LOWMEMSIZE(), E820_RAM);
-	e820_add_region(HIGH_MEMORY, PFN_PHYS(max_pfn)-HIGH_MEMORY, E820_RAM);
+	e820_add_region(0, PFN_PHYS(max_pfn), E820_RAM);
+
+	/*
+	 * Even though this is normal, usable memory under Xen, reserve
+	 * ISA memory anyway because too many things think they can poke
+	 * about in there.
+	 */
+	e820_add_region(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS,
+			E820_RESERVED);
 
 	/*
 	 * Reserve Xen bits:

commit be5bf9fa1c327fa6fd6e7ba44665437dd558dfe3
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Jun 16 14:54:46 2008 -0700

    xen: reserve Xen-specific memory in e820 map
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: the arch/x86 maintainers <x86@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index a29575803204..dc2ca8ad3603 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -40,9 +40,22 @@ char * __init xen_memory_setup(void)
 	max_pfn = min(MAX_DOMAIN_PAGES, max_pfn);
 
 	e820.nr_map = 0;
+
 	e820_add_region(0, LOWMEMSIZE(), E820_RAM);
 	e820_add_region(HIGH_MEMORY, PFN_PHYS(max_pfn)-HIGH_MEMORY, E820_RAM);
 
+	/*
+	 * Reserve Xen bits:
+	 *  - mfn_list
+	 *  - xen_start_info
+	 * See comment above "struct start_info" in <xen/interface/xen.h>
+	 */
+	e820_add_region(__pa(xen_start_info->mfn_list),
+			xen_start_info->pt_base - xen_start_info->mfn_list,
+			E820_RESERVED);
+
+	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
+
 	return "Xen";
 }
 

commit 3de352bbd86f890dd0c5e1c09a6a1b0b29e0f8ce
Merge: 1b8ba39a3fad 9340e1ccdf7b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 8 11:14:58 2008 +0200

    Merge branch 'x86/mpparse' into x86/devel
    
    Conflicts:
    
            arch/x86/Kconfig
            arch/x86/kernel/io_apic_32.c
            arch/x86/kernel/setup_64.c
            arch/x86/mm/init_32.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit d0be6bdea103b8d04c8a3495538b7c0011ae4129
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Jun 15 18:58:51 2008 -0700

    x86: rename two e820 related functions
    
    rename update_memory_range to e820_update_range
    rename add_memory_region to e820_add_region
    
    to make it more clear that they are about e820 map operations.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 82517e4a752a..9001c9df04d8 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -39,8 +39,8 @@ char * __init xen_memory_setup(void)
 	unsigned long max_pfn = xen_start_info->nr_pages;
 
 	e820.nr_map = 0;
-	add_memory_region(0, LOWMEMSIZE(), E820_RAM);
-	add_memory_region(HIGH_MEMORY, PFN_PHYS(max_pfn)-HIGH_MEMORY, E820_RAM);
+	e820_add_region(0, LOWMEMSIZE(), E820_RAM);
+	e820_add_region(HIGH_MEMORY, PFN_PHYS(max_pfn)-HIGH_MEMORY, E820_RAM);
 
 	return "Xen";
 }

commit 8006ec3e911f93d702e1d4a4e387e244ab434924
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:19 2008 +0100

    xen: add configurable max domain size
    
    Add a config option to set the max size of a Xen domain.  This is used
    to scale the size of the physical-to-machine array; it ends up using
    around 1 page/GByte, so there's no reason to be very restrictive.
    
    For a 32-bit guest, the default value of 8GB is probably sufficient;
    there's not much point in giving a 32-bit machine much more memory
    than that.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 37f8f0b8f743..488447878a9d 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -16,6 +16,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/xen/hypercall.h>
 
+#include <xen/page.h>
 #include <xen/interface/callback.h>
 #include <xen/interface/physdev.h>
 #include <xen/features.h>
@@ -36,6 +37,8 @@ char * __init xen_memory_setup(void)
 {
 	unsigned long max_pfn = xen_start_info->nr_pages;
 
+	max_pfn = min(MAX_DOMAIN_PAGES, max_pfn);
+
 	e820.nr_map = 0;
 	add_memory_region(0, LOWMEMSIZE(), E820_RAM);
 	add_memory_region(HIGH_MEMORY, PFN_PHYS(max_pfn)-HIGH_MEMORY, E820_RAM);

commit d451bb7aa852627bdf7be7937dc3d9d9f261b235
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon May 26 23:31:18 2008 +0100

    xen: make phys_to_machine structure dynamic
    
    We now support the use of memory hotplug, so the physical to machine
    page mapping structure must be dynamic.  This is implemented as a
    two-level radix tree structure, which allows us to efficiently
    incrementally allocate memory for the p2m table as new pages are
    added.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 82517e4a752a..37f8f0b8f743 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -27,8 +27,6 @@
 extern const char xen_hypervisor_callback[];
 extern const char xen_failsafe_callback[];
 
-unsigned long *phys_to_machine_mapping;
-EXPORT_SYMBOL(phys_to_machine_mapping);
 
 /**
  * machine_specific_memory_setup - Hook for machine specific memory setup.

commit e2a81baf6604a2e08e10c7405b0349106f77c8af
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Mar 17 16:37:17 2008 -0700

    xen: support sysenter/sysexit if hypervisor does
    
    64-bit Xen supports sysenter for 32-bit guests, so support its
    use.  (sysenter is faster than int $0x80 in 32-on-64.)
    
    sysexit is still not supported, so we fake it up using iret.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 2341492bf7a0..82517e4a752a 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -16,6 +16,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/xen/hypercall.h>
 
+#include <xen/interface/callback.h>
 #include <xen/interface/physdev.h>
 #include <xen/features.h>
 
@@ -68,6 +69,24 @@ static void __init fiddle_vdso(void)
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
 }
 
+void xen_enable_sysenter(void)
+{
+	int cpu = smp_processor_id();
+	extern void xen_sysenter_target(void);
+	/* Mask events on entry, even though they get enabled immediately */
+	static struct callback_register sysenter = {
+		.type = CALLBACKTYPE_sysenter,
+		.address = { __KERNEL_CS, (unsigned long)xen_sysenter_target },
+		.flags = CALLBACKF_mask_events,
+	};
+
+	if (!boot_cpu_has(X86_FEATURE_SEP) ||
+	    HYPERVISOR_callback_op(CALLBACKOP_register, &sysenter) != 0) {
+		clear_cpu_cap(&cpu_data(cpu), X86_FEATURE_SEP);
+		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_SEP);
+	}
+}
+
 void __init xen_arch_setup(void)
 {
 	struct physdev_set_iopl set_iopl;
@@ -82,6 +101,8 @@ void __init xen_arch_setup(void)
 	HYPERVISOR_set_callbacks(__KERNEL_CS, (unsigned long)xen_hypervisor_callback,
 				 __KERNEL_CS, (unsigned long)xen_failsafe_callback);
 
+	xen_enable_sysenter();
+
 	set_iopl.iopl = 1;
 	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
 	if (rc != 0)

commit 87d034f3139b5f0d93df2ba58f37d6f2c2c7eeb6
Author: Ian Campbell <ijc@hellion.org.uk>
Date:   Thu Feb 28 23:16:49 2008 +0000

    x86/xen: fix DomU boot problem
    
    Construct Xen guest e820 map with a hole between 640K-1M.
    
    It's pure luck that Xen kernels have gotten away with it in the past.
    
    The patch below seems like the right thing to do. It certainly boots in
    a domU without the DMI problem (without any of the other related patches
    such as Alexander's).
    
    Signed-off-by: Ian Campbell <ijc@hellion.org.uk>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Tested-by: Mark McLoughlin <markmc@redhat.com>
    Acked-by: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 3bad4773a2f3..2341492bf7a0 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -38,7 +38,8 @@ char * __init xen_memory_setup(void)
 	unsigned long max_pfn = xen_start_info->nr_pages;
 
 	e820.nr_map = 0;
-	add_memory_region(0, PFN_PHYS(max_pfn), E820_RAM);
+	add_memory_region(0, LOWMEMSIZE(), E820_RAM);
+	add_memory_region(HIGH_MEMORY, PFN_PHYS(max_pfn)-HIGH_MEMORY, E820_RAM);
 
 	return "Xen";
 }

commit 08b6d290f977d8145804fd2b9bc2c331f2484f8e
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Wed Jan 30 13:33:25 2008 +0100

    xen: fix section usage in xen-head.S and setup.c
    
    additional section for .init.text appending a number.
    
    A side effect of this was a section mismatch warning because modpost did
    not recognize a .init.text section named .init.text.1: WARNING:
    vmlinux.o(.text.head+0x247): Section mismatch: reference to
    .init.text.1:start_kernel (between 'is386' and 'check_x87')
    
    Fix this by hardcoding the "ax" in the pushsection.  Thanks to Torlaf for
    reporting this.
    
    Alan Modra provided the hint that made me able to locate the root cause of
    this warning.  And Mike Frysinger told me how to properly fix it using
    __INIT/__FINIT.
    
    Fix following Section mismatch warning in addition:
    WARNING: vmlinux.o(.text+0x14c8): Section mismatch: reference to .init.data:vsyscall_int80_start (between 'fiddle_vdso' and 'xen_setup_features')
    
    fiddle_vdso was only used from a __init function - so declare it __init.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: Toralf Förster <toralf.foerster@gmx.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index 7d6d0ef55890..3bad4773a2f3 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -60,7 +60,7 @@ static void xen_idle(void)
 /*
  * Set the bit indicating "nosegneg" library variants should be used.
  */
-static void fiddle_vdso(void)
+static void __init fiddle_vdso(void)
 {
 	extern const char vdso32_default_start;
 	u32 *mask = VDSO32_SYMBOL(&vdso32_default_start, NOTE_MASK);

commit af65d64845a90c8f2fc90b97e2148ff74672e979
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:30:43 2008 +0100

    x86 vDSO: consolidate vdso32
    
    This makes x86_64's ia32 emulation support share the sources used in the
    32-bit kernel for the 32-bit vDSO and much of its setup code.
    
    The 32-bit vDSO mapping now behaves the same on x86_64 as on native 32-bit.
    The abi.syscall32 sysctl on x86_64 now takes the same values that
    vm.vdso_enabled takes on the 32-bit kernel.  That is, 1 means a randomized
    vDSO location, 2 means the fixed old address.  The CONFIG_COMPAT_VDSO
    option is now available to make this the default setting, the same meaning
    it has for the 32-bit kernel.  (This does not affect the 64-bit vDSO.)
    
    The argument vdso32=[012] can be used on both 32-bit and 64-bit kernels to
    set this paramter at boot time.  The vdso=[012] argument still does this
    same thing on the 32-bit kernel.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index fd91568090f4..7d6d0ef55890 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -62,8 +62,8 @@ static void xen_idle(void)
  */
 static void fiddle_vdso(void)
 {
-	extern char vsyscall_int80_start;
-	u32 *mask = VDSO32_SYMBOL(&vsyscall_int80_start, NOTE_MASK);
+	extern const char vdso32_default_start;
+	u32 *mask = VDSO32_SYMBOL(&vdso32_default_start, NOTE_MASK);
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
 }
 

commit 6c3652efcafa6a6d795093362cb4290c84994b5c
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Jan 30 13:30:42 2008 +0100

    x86 vDSO: i386 vdso32
    
    This makes the i386 kernel use the new vDSO build in arch/x86/vdso/vdso32/
    to replace the old one from arch/x86/kernel/.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index f84e77226646..fd91568090f4 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -10,6 +10,7 @@
 #include <linux/pm.h>
 
 #include <asm/elf.h>
+#include <asm/vdso.h>
 #include <asm/e820.h>
 #include <asm/setup.h>
 #include <asm/xen/hypervisor.h>
@@ -61,10 +62,8 @@ static void xen_idle(void)
  */
 static void fiddle_vdso(void)
 {
-	extern u32 VDSO_NOTE_MASK; /* See ../kernel/vsyscall-note.S.  */
 	extern char vsyscall_int80_start;
-	u32 *mask = (u32 *) ((unsigned long) &VDSO_NOTE_MASK - VDSO_PRELINK +
-			     &vsyscall_int80_start);
+	u32 *mask = VDSO32_SYMBOL(&vsyscall_int80_start, NOTE_MASK);
 	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
 }
 

commit 9702785a747aa27baf46ff504beab6528f21f2dd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:51 2007 +0200

    i386: move xen
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
new file mode 100644
index 000000000000..f84e77226646
--- /dev/null
+++ b/arch/x86/xen/setup.c
@@ -0,0 +1,111 @@
+/*
+ * Machine specific setup for xen
+ *
+ * Jeremy Fitzhardinge <jeremy@xensource.com>, XenSource Inc, 2007
+ */
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/pm.h>
+
+#include <asm/elf.h>
+#include <asm/e820.h>
+#include <asm/setup.h>
+#include <asm/xen/hypervisor.h>
+#include <asm/xen/hypercall.h>
+
+#include <xen/interface/physdev.h>
+#include <xen/features.h>
+
+#include "xen-ops.h"
+#include "vdso.h"
+
+/* These are code, but not functions.  Defined in entry.S */
+extern const char xen_hypervisor_callback[];
+extern const char xen_failsafe_callback[];
+
+unsigned long *phys_to_machine_mapping;
+EXPORT_SYMBOL(phys_to_machine_mapping);
+
+/**
+ * machine_specific_memory_setup - Hook for machine specific memory setup.
+ **/
+
+char * __init xen_memory_setup(void)
+{
+	unsigned long max_pfn = xen_start_info->nr_pages;
+
+	e820.nr_map = 0;
+	add_memory_region(0, PFN_PHYS(max_pfn), E820_RAM);
+
+	return "Xen";
+}
+
+static void xen_idle(void)
+{
+	local_irq_disable();
+
+	if (need_resched())
+		local_irq_enable();
+	else {
+		current_thread_info()->status &= ~TS_POLLING;
+		smp_mb__after_clear_bit();
+		safe_halt();
+		current_thread_info()->status |= TS_POLLING;
+	}
+}
+
+/*
+ * Set the bit indicating "nosegneg" library variants should be used.
+ */
+static void fiddle_vdso(void)
+{
+	extern u32 VDSO_NOTE_MASK; /* See ../kernel/vsyscall-note.S.  */
+	extern char vsyscall_int80_start;
+	u32 *mask = (u32 *) ((unsigned long) &VDSO_NOTE_MASK - VDSO_PRELINK +
+			     &vsyscall_int80_start);
+	*mask |= 1 << VDSO_NOTE_NONEGSEG_BIT;
+}
+
+void __init xen_arch_setup(void)
+{
+	struct physdev_set_iopl set_iopl;
+	int rc;
+
+	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_4gb_segments);
+	HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_writable_pagetables);
+
+	if (!xen_feature(XENFEAT_auto_translated_physmap))
+		HYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_pae_extended_cr3);
+
+	HYPERVISOR_set_callbacks(__KERNEL_CS, (unsigned long)xen_hypervisor_callback,
+				 __KERNEL_CS, (unsigned long)xen_failsafe_callback);
+
+	set_iopl.iopl = 1;
+	rc = HYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);
+	if (rc != 0)
+		printk(KERN_INFO "physdev_op failed %d\n", rc);
+
+#ifdef CONFIG_ACPI
+	if (!(xen_start_info->flags & SIF_INITDOMAIN)) {
+		printk(KERN_INFO "ACPI in unprivileged domain disabled\n");
+		disable_acpi();
+	}
+#endif
+
+	memcpy(boot_command_line, xen_start_info->cmd_line,
+	       MAX_GUEST_CMDLINE > COMMAND_LINE_SIZE ?
+	       COMMAND_LINE_SIZE : MAX_GUEST_CMDLINE);
+
+	pm_idle = xen_idle;
+
+#ifdef CONFIG_SMP
+	/* fill cpus_possible with all available cpus */
+	xen_fill_possible_map();
+#endif
+
+	paravirt_disable_iospace();
+
+	fiddle_vdso();
+}
