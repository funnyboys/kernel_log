commit bc9331a19d758706493cbebba67ca70382edddac
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:58:09 2020 -0700

    mm: rename free_area_init_node() to free_area_init_memoryless_node()
    
    free_area_init_node() is only used by x86 to initialize a memory-less
    nodes.  Make its name reflect this and drop all the function parameters
    except node ID as they are anyway zero.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-19-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index fe024b2ac796..8ee952038c80 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -737,12 +737,9 @@ void __init x86_numa_init(void)
 
 static void __init init_memory_less_node(int nid)
 {
-	unsigned long zones_size[MAX_NR_ZONES] = {0};
-	unsigned long zholes_size[MAX_NR_ZONES] = {0};
-
 	/* Allocate and initialize node data. Memory-less node is now online.*/
 	alloc_node_data(nid);
-	free_area_init_node(nid, zones_size, 0, zholes_size);
+	free_area_init_memoryless_node(nid);
 
 	/*
 	 * All zonelists will be built later in start_kernel() after per cpu

commit d622abf74f3d81365e41c3bfdbbda50ecd99ba3d
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:56:53 2020 -0700

    mm: memblock: replace dereferences of memblock_region.nid with API calls
    
    Patch series "mm: rework free_area_init*() funcitons".
    
    After the discussion [1] about removal of CONFIG_NODES_SPAN_OTHER_NODES
    and CONFIG_HAVE_MEMBLOCK_NODE_MAP options, I took it a bit further and
    updated the node/zone initialization.
    
    Since all architectures have memblock, it is possible to use only the
    newer version of free_area_init_node() that calculates the zone and node
    boundaries based on memblock node mapping and architectural limits on
    possible zone PFNs.
    
    The architectures that still determined zone and hole sizes can be
    switched to the generic code and the old code that took those zone and
    hole sizes can be simply removed.
    
    And, since it all started from the removal of
    CONFIG_NODES_SPAN_OTHER_NODES, the memmap_init() is now updated to iterate
    over memblocks and so it does not need to perform early_pfn_to_nid() query
    for every PFN.
    
    [1] https://lore.kernel.org/lkml/1585420282-25630-1-git-send-email-Hoan@os.amperecomputing.com
    
    This patch (of 21):
    
    There are several places in the code that directly dereference
    memblock_region.nid despite this field being defined only when
    CONFIG_HAVE_MEMBLOCK_NODE_MAP=y.
    
    Replace these with calls to memblock_get_region_nid() to improve code
    robustness and to avoid possible breakage when
    CONFIG_HAVE_MEMBLOCK_NODE_MAP will be removed.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200412194859.12663-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 59ba008504dc..fe024b2ac796 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -517,8 +517,10 @@ static void __init numa_clear_kernel_node_hotplug(void)
 	 *   reserve specific pages for Sandy Bridge graphics. ]
 	 */
 	for_each_memblock(reserved, mb_region) {
-		if (mb_region->nid != MAX_NUMNODES)
-			node_set(mb_region->nid, reserved_nodemask);
+		int nid = memblock_get_region_node(mb_region);
+
+		if (nid != MAX_NUMNODES)
+			node_set(nid, reserved_nodemask);
 	}
 
 	/*

commit 5d30f92e7631286b8617777c5400c8eadcae50a1
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Feb 16 12:01:09 2020 -0800

    x86/NUMA: Provide a range-to-target_node lookup facility
    
    The DEV_DAX_KMEM facility is a generic mechanism to allow device-dax
    instances, fronting performance-differentiated-memory like pmem, to be
    added to the System RAM pool. The NUMA node for that hot-added memory is
    derived from the device-dax instance's 'target_node' attribute.
    
    Recall that the 'target_node' is the ACPI-PXM-to-node translation for
    memory when it comes online whereas the 'numa_node' attribute of the
    device represents the closest online cpu node.
    
    Presently useful target_node information from the ACPI SRAT is discarded
    with the expectation that "Reserved" memory will never be onlined. Now,
    DEV_DAX_KMEM violates that assumption, there is a need to retain the
    translation. Move, rather than discard, numa_memblk data to a secondary
    array that memory_add_physaddr_to_target_node() may consider at a later
    point in time.
    
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <x86@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Reported-by: kbuild test robot <lkp@intel.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/158188326978.894464.217282995221175417.stgit@dwillia2-desk3.amr.corp.intel.com

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 2450b21cc28a..59ba008504dc 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -26,6 +26,7 @@ struct pglist_data *node_data[MAX_NUMNODES] __read_mostly;
 EXPORT_SYMBOL(node_data);
 
 static struct numa_meminfo numa_meminfo __initdata_or_meminfo;
+static struct numa_meminfo numa_reserved_meminfo __initdata_or_meminfo;
 
 static int numa_distance_cnt;
 static u8 *numa_distance;
@@ -164,6 +165,19 @@ void __init numa_remove_memblk_from(int idx, struct numa_meminfo *mi)
 		(mi->nr_blks - idx) * sizeof(mi->blk[0]));
 }
 
+/**
+ * numa_move_tail_memblk - Move a numa_memblk from one numa_meminfo to another
+ * @dst: numa_meminfo to append block to
+ * @idx: Index of memblk to remove
+ * @src: numa_meminfo to remove memblk from
+ */
+static void __init numa_move_tail_memblk(struct numa_meminfo *dst, int idx,
+					 struct numa_meminfo *src)
+{
+	dst->blk[dst->nr_blks++] = src->blk[idx];
+	numa_remove_memblk_from(idx, src);
+}
+
 /**
  * numa_add_memblk - Add one numa_memblk to numa_meminfo
  * @nid: NUMA node ID of the new memblk
@@ -233,14 +247,19 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 	for (i = 0; i < mi->nr_blks; i++) {
 		struct numa_memblk *bi = &mi->blk[i];
 
-		/* make sure all blocks are inside the limits */
+		/* move / save reserved memory ranges */
+		if (!memblock_overlaps_region(&memblock.memory,
+					bi->start, bi->end - bi->start)) {
+			numa_move_tail_memblk(&numa_reserved_meminfo, i--, mi);
+			continue;
+		}
+
+		/* make sure all non-reserved blocks are inside the limits */
 		bi->start = max(bi->start, low);
 		bi->end = min(bi->end, high);
 
-		/* and there's no empty or non-exist block */
-		if (bi->start >= bi->end ||
-		    !memblock_overlaps_region(&memblock.memory,
-			bi->start, bi->end - bi->start))
+		/* and there's no empty block */
+		if (bi->start >= bi->end)
 			numa_remove_memblk_from(i--, mi);
 	}
 
@@ -877,16 +896,38 @@ EXPORT_SYMBOL(cpumask_of_node);
 
 #endif	/* !CONFIG_DEBUG_PER_CPU_MAPS */
 
-#ifdef CONFIG_MEMORY_HOTPLUG
-int memory_add_physaddr_to_nid(u64 start)
+#ifdef CONFIG_NUMA_KEEP_MEMINFO
+static int meminfo_to_nid(struct numa_meminfo *mi, u64 start)
 {
-	struct numa_meminfo *mi = &numa_meminfo;
-	int nid = mi->blk[0].nid;
 	int i;
 
 	for (i = 0; i < mi->nr_blks; i++)
 		if (mi->blk[i].start <= start && mi->blk[i].end > start)
-			nid = mi->blk[i].nid;
+			return mi->blk[i].nid;
+	return NUMA_NO_NODE;
+}
+
+int phys_to_target_node(phys_addr_t start)
+{
+	int nid = meminfo_to_nid(&numa_meminfo, start);
+
+	/*
+	 * Prefer online nodes, but if reserved memory might be
+	 * hot-added continue the search with reserved ranges.
+	 */
+	if (nid != NUMA_NO_NODE)
+		return nid;
+
+	return meminfo_to_nid(&numa_reserved_meminfo, start);
+}
+EXPORT_SYMBOL_GPL(phys_to_target_node);
+
+int memory_add_physaddr_to_nid(u64 start)
+{
+	int nid = meminfo_to_nid(&numa_meminfo, start);
+
+	if (nid == NUMA_NO_NODE)
+		nid = numa_meminfo.blk[0].nid;
 	return nid;
 }
 EXPORT_SYMBOL_GPL(memory_add_physaddr_to_nid);

commit 1e5d8e1e47afde23e3249aed25d7d124feff5c1c
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Feb 16 12:01:04 2020 -0800

    x86/mm: Introduce CONFIG_NUMA_KEEP_MEMINFO
    
    Currently x86 numa_meminfo is marked __initdata in the
    CONFIG_MEMORY_HOTPLUG=n case. In support of a new facility to allow
    drivers to map reserved memory to a 'target_node'
    (phys_to_target_node()), add support for removing the __initdata
    designation for those users. Both memory hotplug and
    phys_to_target_node() users select CONFIG_NUMA_KEEP_MEMINFO to tell the
    arch to maintain its physical address to NUMA mapping infrastructure
    post init.
    
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <x86@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/158188326422.894464.15742054998046628934.stgit@dwillia2-desk3.amr.corp.intel.com

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 99f7a68738f0..2450b21cc28a 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -25,11 +25,7 @@ nodemask_t numa_nodes_parsed __initdata;
 struct pglist_data *node_data[MAX_NUMNODES] __read_mostly;
 EXPORT_SYMBOL(node_data);
 
-static struct numa_meminfo numa_meminfo
-#ifndef CONFIG_MEMORY_HOTPLUG
-__initdata
-#endif
-;
+static struct numa_meminfo numa_meminfo __initdata_or_meminfo;
 
 static int numa_distance_cnt;
 static u8 *numa_distance;

commit 11a98f37a5c11fd3cec9c7a566dfa902bceb5bde
Author: Cao jin <caoj.fnst@cn.fujitsu.com>
Date:   Mon Nov 18 15:00:12 2019 +0800

    x86: Fix typos in comments
    
    BIOSen -> BIOSes; paing -> paging. Append to 640 its proper unit "Kb".
    encomapssing -> encompassing.
    
     [ bp: Merge into a single patch, fix one more typo, massage. ]
    
    Signed-off-by: Cao jin <caoj.fnst@cn.fujitsu.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Robert Richter <rrichter@marvell.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20191118070012.27850-1-caoj.fnst@cn.fujitsu.com

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 4123100e0eaf..99f7a68738f0 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -699,7 +699,7 @@ static int __init dummy_numa_init(void)
  * x86_numa_init - Initialize NUMA
  *
  * Try each configured NUMA initialization method until one succeeds.  The
- * last fallback is dummy single node config encomapssing whole memory and
+ * last fallback is dummy single node config encompassing whole memory and
  * never fails.
  */
 void __init x86_numa_init(void)

commit bc04a049f058a472695aa22905d57e2b1f4c77d9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 3 09:53:52 2019 +0200

    x86/mm: Fix cpumask_of_node() error condition
    
    When CONFIG_DEBUG_PER_CPU_MAPS=y we validate that the @node argument of
    cpumask_of_node() is a valid node_id. It however forgets to check for
    negative numbers. Fix this by explicitly casting to unsigned int.
    
      (unsigned)node >= nr_node_ids
    
    verifies: 0 <= node < nr_node_ids
    
    Also ammend the error message to match the condition.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Yunsheng Lin <linyunsheng@huawei.com>
    Link: https://lkml.kernel.org/r/20190903075352.GY2369@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index e6dad600614c..4123100e0eaf 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -861,9 +861,9 @@ void numa_remove_cpu(int cpu)
  */
 const struct cpumask *cpumask_of_node(int node)
 {
-	if (node >= nr_node_ids) {
+	if ((unsigned)node >= nr_node_ids) {
 		printk(KERN_WARNING
-			"cpumask_of_node(%d): node > nr_node_ids(%u)\n",
+			"cpumask_of_node(%d): (unsigned)node >= nr_node_ids(%u)\n",
 			node, nr_node_ids);
 		dump_stack();
 		return cpu_none_mask;

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index dfb6c4df639a..e6dad600614c 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /* Common code for 32 and 64-bit NUMA */
 #include <linux/acpi.h>
 #include <linux/kernel.h>

commit 42b46aeff2e366bad54bd1c069b7b5381d9be8b3
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Mar 11 23:29:31 2019 -0700

    memblock: drop __memblock_alloc_base()
    
    The __memblock_alloc_base() function tries to allocate a memory up to
    the limit specified by its max_addr parameter.  Depending on the value
    of this parameter, the __memblock_alloc_base() can is replaced with the
    appropriate memblock_phys_alloc*() variant.
    
    Link: http://lkml.kernel.org/r/1548057848-15136-9-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Acked-by: Rob Herring <robh@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Guo Ren <ren_guo@c-sky.com>                         [c-sky]
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Juergen Gross <jgross@suse.com>                     [Xen]
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 12c1b7a83ed7..dfb6c4df639a 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -195,15 +195,11 @@ static void __init alloc_node_data(int nid)
 	 * Allocate node data.  Try node-local memory and then any node.
 	 * Never allocate in DMA zone.
 	 */
-	nd_pa = memblock_phys_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
+	nd_pa = memblock_phys_alloc_try_nid(nd_size, SMP_CACHE_BYTES, nid);
 	if (!nd_pa) {
-		nd_pa = __memblock_alloc_base(nd_size, SMP_CACHE_BYTES,
-					      MEMBLOCK_ALLOC_ACCESSIBLE);
-		if (!nd_pa) {
-			pr_err("Cannot find %zu bytes in any node (initial node: %d)\n",
-			       nd_size, nid);
-			return;
-		}
+		pr_err("Cannot find %zu bytes in any node (initial node: %d)\n",
+		       nd_size, nid);
+		return;
 	}
 	nd = __va(nd_pa);
 

commit b9726c26dc21b15a2faea96fae3a42f2f7fffdcb
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Mar 5 15:48:26 2019 -0800

    numa: make "nr_node_ids" unsigned int
    
    Number of NUMA nodes can't be negative.
    
    This saves a few bytes on x86_64:
    
            add/remove: 0/0 grow/shrink: 4/21 up/down: 27/-265 (-238)
            Function                                     old     new   delta
            hv_synic_alloc.cold                           88     110     +22
            prealloc_shrinker                            260     262      +2
            bootstrap                                    249     251      +2
            sched_init_numa                             1566    1567      +1
            show_slab_objects                            778     777      -1
            s_show                                      1201    1200      -1
            kmem_cache_init                              346     345      -1
            __alloc_workqueue_key                       1146    1145      -1
            mem_cgroup_css_alloc                        1614    1612      -2
            __do_sys_swapon                             4702    4699      -3
            __list_lru_init                              655     651      -4
            nic_probe                                   2379    2374      -5
            store_user_store                             118     111      -7
            red_zone_store                               106      99      -7
            poison_store                                 106      99      -7
            wq_numa_init                                 348     338     -10
            __kmem_cache_empty                            75      65     -10
            task_numa_free                               186     173     -13
            merge_across_nodes_store                     351     336     -15
            irq_create_affinity_masks                   1261    1246     -15
            do_numa_crng_init                            343     321     -22
            task_numa_fault                             4760    4737     -23
            swapfile_init                                179     156     -23
            hv_synic_alloc                               536     492     -44
            apply_wqattrs_prepare                        746     695     -51
    
    Link: http://lkml.kernel.org/r/20190201223029.GA15820@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 1308f5408bf7..12c1b7a83ed7 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -123,7 +123,7 @@ void __init setup_node_to_cpumask_map(void)
 		alloc_bootmem_cpumask_var(&node_to_cpumask_map[node]);
 
 	/* cpumask_of_node() will now work */
-	pr_debug("Node to cpumask map for %d nodes\n", nr_node_ids);
+	pr_debug("Node to cpumask map for %u nodes\n", nr_node_ids);
 }
 
 static int __init numa_add_memblk_to(int nid, u64 start, u64 end,
@@ -866,7 +866,7 @@ const struct cpumask *cpumask_of_node(int node)
 {
 	if (node >= nr_node_ids) {
 		printk(KERN_WARNING
-			"cpumask_of_node(%d): node > nr_node_ids(%d)\n",
+			"cpumask_of_node(%d): node > nr_node_ids(%u)\n",
 			node, nr_node_ids);
 		dump_stack();
 		return cpu_none_mask;

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 16e37d712ffd..1308f5408bf7 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -4,7 +4,6 @@
 #include <linux/mm.h>
 #include <linux/string.h>
 #include <linux/init.h>
-#include <linux/bootmem.h>
 #include <linux/memblock.h>
 #include <linux/mmzone.h>
 #include <linux/ctype.h>

commit 9a8dd708d547268c899f1cb443c49bd4d8c84eb3
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:07:59 2018 -0700

    memblock: rename memblock_alloc{_nid,_try_nid} to memblock_phys_alloc*
    
    Make it explicit that the caller gets a physical address rather than a
    virtual one.
    
    This will also allow using meblock_alloc prefix for memblock allocations
    returning virtual address, which is done in the following patches.
    
    The conversion is done using the following semantic patch:
    
    @@
    expression e1, e2, e3;
    @@
    (
    - memblock_alloc(e1, e2)
    + memblock_phys_alloc(e1, e2)
    |
    - memblock_alloc_nid(e1, e2, e3)
    + memblock_phys_alloc_nid(e1, e2, e3)
    |
    - memblock_alloc_try_nid(e1, e2, e3)
    + memblock_phys_alloc_try_nid(e1, e2, e3)
    )
    
    Link: http://lkml.kernel.org/r/1536927045-23536-7-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index fa150855647c..16e37d712ffd 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -196,7 +196,7 @@ static void __init alloc_node_data(int nid)
 	 * Allocate node data.  Try node-local memory and then any node.
 	 * Never allocate in DMA zone.
 	 */
-	nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
+	nd_pa = memblock_phys_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
 	if (!nd_pa) {
 		nd_pa = __memblock_alloc_base(nd_size, SMP_CACHE_BYTES,
 					      MEMBLOCK_ALLOC_ACCESSIBLE);

commit 1de392f5d5e803663abbd8ed084233f154152bcd
Author: Joe Perches <joe@perches.com>
Date:   Thu May 10 08:45:30 2018 -0700

    x86: Remove pr_fmt duplicate logging prefixes
    
    Converting pr_fmt from a default simple #define to use KBUILD_MODNAME
    added some duplicate prefixes.
    
    Remove the duplicate prefixes.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/e7b709a2b040af7faa81b0aa2c3a125aed628a82.1525964383.git.joe@perches.com

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 25504d5aa816..fa150855647c 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -136,13 +136,13 @@ static int __init numa_add_memblk_to(int nid, u64 start, u64 end,
 
 	/* whine about and ignore invalid blks */
 	if (start > end || nid < 0 || nid >= MAX_NUMNODES) {
-		pr_warning("NUMA: Warning: invalid memblk node %d [mem %#010Lx-%#010Lx]\n",
-			   nid, start, end - 1);
+		pr_warn("Warning: invalid memblk node %d [mem %#010Lx-%#010Lx]\n",
+			nid, start, end - 1);
 		return 0;
 	}
 
 	if (mi->nr_blks >= NR_NODE_MEMBLKS) {
-		pr_err("NUMA: too many memblk ranges\n");
+		pr_err("too many memblk ranges\n");
 		return -EINVAL;
 	}
 
@@ -267,14 +267,14 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 			 */
 			if (bi->end > bj->start && bi->start < bj->end) {
 				if (bi->nid != bj->nid) {
-					pr_err("NUMA: node %d [mem %#010Lx-%#010Lx] overlaps with node %d [mem %#010Lx-%#010Lx]\n",
+					pr_err("node %d [mem %#010Lx-%#010Lx] overlaps with node %d [mem %#010Lx-%#010Lx]\n",
 					       bi->nid, bi->start, bi->end - 1,
 					       bj->nid, bj->start, bj->end - 1);
 					return -EINVAL;
 				}
-				pr_warning("NUMA: Warning: node %d [mem %#010Lx-%#010Lx] overlaps with itself [mem %#010Lx-%#010Lx]\n",
-					   bi->nid, bi->start, bi->end - 1,
-					   bj->start, bj->end - 1);
+				pr_warn("Warning: node %d [mem %#010Lx-%#010Lx] overlaps with itself [mem %#010Lx-%#010Lx]\n",
+					bi->nid, bi->start, bi->end - 1,
+					bj->start, bj->end - 1);
 			}
 
 			/*
@@ -364,7 +364,7 @@ static int __init numa_alloc_distance(void)
 	phys = memblock_find_in_range(0, PFN_PHYS(max_pfn_mapped),
 				      size, PAGE_SIZE);
 	if (!phys) {
-		pr_warning("NUMA: Warning: can't allocate distance table!\n");
+		pr_warn("Warning: can't allocate distance table!\n");
 		/* don't retry until explicitly reset */
 		numa_distance = (void *)1LU;
 		return -ENOMEM;
@@ -410,14 +410,14 @@ void __init numa_set_distance(int from, int to, int distance)
 
 	if (from >= numa_distance_cnt || to >= numa_distance_cnt ||
 			from < 0 || to < 0) {
-		pr_warn_once("NUMA: Warning: node ids are out of bound, from=%d to=%d distance=%d\n",
-			    from, to, distance);
+		pr_warn_once("Warning: node ids are out of bound, from=%d to=%d distance=%d\n",
+			     from, to, distance);
 		return;
 	}
 
 	if ((u8)distance != distance ||
 	    (from == to && distance != LOCAL_DISTANCE)) {
-		pr_warn_once("NUMA: Warning: invalid distance parameter, from=%d to=%d distance=%d\n",
+		pr_warn_once("Warning: invalid distance parameter, from=%d to=%d distance=%d\n",
 			     from, to, distance);
 		return;
 	}

commit e5185a76a23b2d56fb2327ad8bd58fb1bcaa52b1
Merge: b678c91aefa7 4729277156cf
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Apr 11 08:56:05 2017 +0200

    Merge branch 'x86/boot' into x86/mm, to avoid conflict
    
    There's a conflict between ongoing level-5 paging support and
    the E820 rewrite. Since the E820 rewrite is essentially ready,
    merge it into x86/mm to reduce tree conflicts.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b678c91aefa7ce05a5d195e0a5c7a357b62d3283
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Apr 8 00:00:53 2017 +0200

    Revert "x86/mm/numa: Remove numa_nodemask_from_meminfo()"
    
    This reverts commit 474aeffd88b87746a75583f356183d5c6caa4213 due to testing
    failures.
    
    Reported-by: "Kirill A. Shutemov" <kirill@shutemov.name>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20170406124459.dwn5zhpr2xqg3lqm@node.shutemov.name

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 175f54ac6772..93671d8b3b0d 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -314,6 +314,20 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 	return 0;
 }
 
+/*
+ * Set nodes, which have memory in @mi, in *@nodemask.
+ */
+static void __init numa_nodemask_from_meminfo(nodemask_t *nodemask,
+					      const struct numa_meminfo *mi)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(mi->blk); i++)
+		if (mi->blk[i].start != mi->blk[i].end &&
+		    mi->blk[i].nid != NUMA_NO_NODE)
+			node_set(mi->blk[i].nid, *nodemask);
+}
+
 /**
  * numa_reset_distance - Reset NUMA distance table
  *
@@ -333,12 +347,16 @@ void __init numa_reset_distance(void)
 
 static int __init numa_alloc_distance(void)
 {
+	nodemask_t nodes_parsed;
 	size_t size;
 	int i, j, cnt = 0;
 	u64 phys;
 
 	/* size the new table and allocate it */
-	for_each_node_mask(i, numa_nodes_parsed)
+	nodes_parsed = numa_nodes_parsed;
+	numa_nodemask_from_meminfo(&nodes_parsed, &numa_meminfo);
+
+	for_each_node_mask(i, nodes_parsed)
 		cnt = i;
 	cnt++;
 	size = cnt * cnt * sizeof(numa_distance[0]);
@@ -517,6 +535,7 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 
 	/* Account for nodes with cpus and no memory */
 	node_possible_map = numa_nodes_parsed;
+	numa_nodemask_from_meminfo(&node_possible_map, mi);
 	if (WARN_ON(nodes_empty(node_possible_map)))
 		return -EINVAL;
 

commit 474aeffd88b87746a75583f356183d5c6caa4213
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Tue Mar 14 11:08:01 2017 +0800

    x86/mm/numa: Remove numa_nodemask_from_meminfo()
    
    numa_nodemask_from_meminfo() generates a nodemask of nodes which have
    memory according to a meminfo descriptor.
    
    The two callsites of that function both set bits in copies of the
    numa_nodes_parsed nodemask. In both cases, the information in supplied
    numa_meminfo is a subset of numa_nodes_parsed. So setting those bits
    again is not really necessary.
    
    Here are the three call paths which show that the supplied numa_meminfo
    argument describes memory regions in nodes which are already in
    numa_nodes_parsed:
    
        x86_numa_init()
            numa_init()
                Case 1:
                acpi_numa_init()
                acpi_parse_memory_affinity()
                        numa_add_memblk()
                        node_set(numa_nodes_parsed)
                    acpi_parse_slit()
                     acpi_numa_slit_init()
                      numa_set_distance()
                       numa_alloc_distance()
                        numa_nodemask_from_meminfo()
    
                Case 2:
                amd_numa_init()
                    numa_add_memblk()
                    node_set(numa_nodes_parsed)
    
                Case 3
                dummy_numa_init()
                    node_set(numa_nodes_parsed)
                    numa_add_memblk()
    
                numa_register_memblks()
                    numa_nodemask_from_meminfo()
    
    Thus, in all three cases, the respective bit in numa_nodes_parsed is
    set, which means it is not necessary to set it again in a copy of
    numa_nodes_parsed.
    
    So remove that function.
    
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Cc: x86-ml <x86@kernel.org>
    Link: http://lkml.kernel.org/r/20170314030801.13656-2-richard.weiyang@gmail.com
    [ Heavily massage commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 93671d8b3b0d..175f54ac6772 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -314,20 +314,6 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 	return 0;
 }
 
-/*
- * Set nodes, which have memory in @mi, in *@nodemask.
- */
-static void __init numa_nodemask_from_meminfo(nodemask_t *nodemask,
-					      const struct numa_meminfo *mi)
-{
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(mi->blk); i++)
-		if (mi->blk[i].start != mi->blk[i].end &&
-		    mi->blk[i].nid != NUMA_NO_NODE)
-			node_set(mi->blk[i].nid, *nodemask);
-}
-
 /**
  * numa_reset_distance - Reset NUMA distance table
  *
@@ -347,16 +333,12 @@ void __init numa_reset_distance(void)
 
 static int __init numa_alloc_distance(void)
 {
-	nodemask_t nodes_parsed;
 	size_t size;
 	int i, j, cnt = 0;
 	u64 phys;
 
 	/* size the new table and allocate it */
-	nodes_parsed = numa_nodes_parsed;
-	numa_nodemask_from_meminfo(&nodes_parsed, &numa_meminfo);
-
-	for_each_node_mask(i, nodes_parsed)
+	for_each_node_mask(i, numa_nodes_parsed)
 		cnt = i;
 	cnt++;
 	size = cnt * cnt * sizeof(numa_distance[0]);
@@ -535,7 +517,6 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 
 	/* Account for nodes with cpus and no memory */
 	node_possible_map = numa_nodes_parsed;
-	numa_nodemask_from_meminfo(&node_possible_map, mi);
 	if (WARN_ON(nodes_empty(node_possible_map)))
 		return -EINVAL;
 

commit 43dac8f6a74c9811454f4efbe52b48f7a802c277
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Tue Mar 14 11:08:00 2017 +0800

    x86/mm/numa: Improve alloc_node_data() error path message
    
    alloc_node_data() tries to allocate from the local node first and, if
    that attempt fails, falls back to any node. Improve the error message to
    issue the initial node for ease during debugging.
    
    Fix a typo in the comments, while at it.
    
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Link: http://lkml.kernel.org/r/20170314030801.13656-1-richard.weiyang@gmail.com
    [ Masssage commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 12dcad7297a5..93671d8b3b0d 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -201,7 +201,7 @@ static void __init alloc_node_data(int nid)
 		nd_pa = __memblock_alloc_base(nd_size, SMP_CACHE_BYTES,
 					      MEMBLOCK_ALLOC_ACCESSIBLE);
 		if (!nd_pa) {
-			pr_err("Cannot find %zu bytes in node %d\n",
+			pr_err("Cannot find %zu bytes in any node (initial node: %d)\n",
 			       nd_size, nid);
 			return;
 		}
@@ -225,7 +225,7 @@ static void __init alloc_node_data(int nid)
  * numa_cleanup_meminfo - Cleanup a numa_meminfo
  * @mi: numa_meminfo to clean up
  *
- * Sanitize @mi by merging and removing unncessary memblks.  Also check for
+ * Sanitize @mi by merging and removing unnecessary memblks.  Also check for
  * conflicts and clear unused memblks.
  *
  * RETURNS:

commit 66441bd3cfdcc03816b7009a296c284d70f629e1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jan 27 10:27:10 2017 +0100

    x86/boot/e820: Move asm/e820.h to asm/e820/api.h
    
    In line with asm/e820/types.h, move the e820 API declarations to
    asm/e820/api.h and update all usage sites.
    
    This is just a mechanical, obviously correct move & replace patch,
    there will be subsequent changes to clean up the code and to make
    better use of the new header organization.
    
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang, Ying <ying.huang@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 12dcad7297a5..f9d99535f233 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -12,7 +12,7 @@
 #include <linux/sched.h>
 #include <linux/topology.h>
 
-#include <asm/e820.h>
+#include <asm/e820/api.h>
 #include <asm/proto.h>
 #include <asm/dma.h>
 #include <asm/amd_nb.h>

commit aec03f89e905dca9ce4b061e03ee1da3a3eb3432
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Dec 12 23:18:29 2016 -0500

    ACPI/NUMA: Do not map pxm to node when NUMA is turned off
    
    acpi_map_pxm_to_node() unconditially maps nodes even when NUMA is turned
    off. So acpi_get_node() might return a node > 0, which is fatal when NUMA
    is disabled as the rest of the kernel assumes that only node 0 exists.
    
    Expose numa_off to the acpi code and return NUMA_NO_NODE when it's set.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: fenghua.yu@intel.com
    Cc: tony.luck@intel.com
    Cc: linux-ia64@vger.kernel.org
    Cc: catalin.marinas@arm.com
    Cc: rjw@rjwysocki.net
    Cc: will.deacon@arm.com
    Cc: linux-acpi@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: lenb@kernel.org
    Link: http://lkml.kernel.org/r/1481602709-18260-1-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 3f35b48d1d9d..12dcad7297a5 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -19,7 +19,7 @@
 
 #include "numa_internal.h"
 
-int __initdata numa_off;
+int numa_off;
 nodemask_t numa_nodes_parsed __initdata;
 
 struct pglist_data *node_data[MAX_NUMNODES] __read_mostly;

commit 2532fc318db0e1fe68e01407ee27634c76916e44
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Thu Aug 25 16:35:14 2016 +0800

    x86/numa: Online memory-less nodes at boot time
    
    For now, x86 does not support memory-less node. A node without memory
    will not be onlined, and the cpus on it will be mapped to the other
    online nodes with memory in init_cpu_to_node(). The reason of doing this
    is to ensure each cpu has mapped to a node with memory, so that it will
    be able to allocate local memory for that cpu.
    
    But we don't have to do it in this way.
    
    In this series of patches, we are going to construct cpu <-> node mapping
    for all possible cpus at boot time, which is a persistent mapping. It means
    that the cpu will be mapped to the node which it belongs to, and will never
    be changed. If a node has only cpus but no memory, the cpus on it will be
    mapped to a memory-less node. And the memory-less node should be onlined.
    
    Allocate pgdats for all memory-less nodes and online them at boot
    time. Then build zonelists for these nodes. As a result, when cpus on these
    memory-less nodes try to allocate memory from local node, it will
    automatically fall back to the proper zones in the zonelists.
    
    Signed-off-by: Zhu Guihua <zhugh.fnst@cn.fujitsu.com>
    Signed-off-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: mika.j.penttila@gmail.com
    Cc: len.brown@intel.com
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: rafael@kernel.org
    Cc: rjw@rjwysocki.net
    Cc: yasu.isimatu@gmail.com
    Cc: linux-mm@kvack.org
    Cc: linux-acpi@vger.kernel.org
    Cc: isimatu.yasuaki@jp.fujitsu.com
    Cc: gongzhaogang@inspur.com
    Cc: tj@kernel.org
    Cc: izumi.taku@jp.fujitsu.com
    Cc: cl@linux.com
    Cc: chen.tang@easystack.cn
    Cc: akpm@linux-foundation.org
    Cc: kamezawa.hiroyu@jp.fujitsu.com
    Cc: lenb@kernel.org
    Link: http://lkml.kernel.org/r/1472114120-3281-2-git-send-email-douly.fnst@cn.fujitsu.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index fb682108f4dc..3f35b48d1d9d 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -722,22 +722,19 @@ void __init x86_numa_init(void)
 	numa_init(dummy_numa_init);
 }
 
-static __init int find_near_online_node(int node)
+static void __init init_memory_less_node(int nid)
 {
-	int n, val;
-	int min_val = INT_MAX;
-	int best_node = -1;
+	unsigned long zones_size[MAX_NR_ZONES] = {0};
+	unsigned long zholes_size[MAX_NR_ZONES] = {0};
 
-	for_each_online_node(n) {
-		val = node_distance(node, n);
+	/* Allocate and initialize node data. Memory-less node is now online.*/
+	alloc_node_data(nid);
+	free_area_init_node(nid, zones_size, 0, zholes_size);
 
-		if (val < min_val) {
-			min_val = val;
-			best_node = n;
-		}
-	}
-
-	return best_node;
+	/*
+	 * All zonelists will be built later in start_kernel() after per cpu
+	 * areas are initialized.
+	 */
 }
 
 /*
@@ -766,8 +763,10 @@ void __init init_cpu_to_node(void)
 
 		if (node == NUMA_NO_NODE)
 			continue;
+
 		if (!node_online(node))
-			node = find_near_online_node(node);
+			init_memory_less_node(node);
+
 		numa_set_node(cpu, node);
 	}
 }

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit 4b599fedb7eeea4c995e655a938b5ec419386ddf
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:55 2016 -0400

    x86/mm: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace accordingly where needed.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-3-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 9c086c57105c..fe939a84232f 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -7,7 +7,6 @@
 #include <linux/memblock.h>
 #include <linux/mmzone.h>
 #include <linux/ctype.h>
-#include <linux/module.h>
 #include <linux/nodemask.h>
 #include <linux/sched.h>
 #include <linux/topology.h>

commit e84025e274e66986277e11f0dda03373e246ffad
Author: David Daney <david.daney@cavium.com>
Date:   Tue May 24 15:35:39 2016 -0700

    ACPI / NUMA: move bad_srat() and srat_disabled() to drivers/acpi/numa.c
    
    bad_srat() and srat_disabled() are shared by x86 and follow-on arm64
    patches.  Move them to drivers/acpi/numa.c in preparation for arm64
    support.
    
    Signed-off-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Robert Richter <rrichter@cavium.com>
    [david.daney@cavium.com moved definitions to drivers/acpi/numa.c]
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 9c086c57105c..968ac028c34e 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -1,4 +1,5 @@
 /* Common code for 32 and 64-bit NUMA */
+#include <linux/acpi.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/string.h>
@@ -15,7 +16,6 @@
 #include <asm/e820.h>
 #include <asm/proto.h>
 #include <asm/dma.h>
-#include <asm/acpi.h>
 #include <asm/amd_nb.h>
 
 #include "numa_internal.h"

commit 0edaf86cf1a6a97d811fc34765ddbcbc310de564
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu May 19 17:10:58 2016 -0700

    include/linux/nodemask.h: create next_node_in() helper
    
    Lots of code does
    
            node = next_node(node, XXX);
            if (node == MAX_NUMNODES)
                    node = first_node(XXX);
    
    so create next_node_in() to do this and use it in various places.
    
    [mhocko@suse.com: use next_node_in() helper]
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Hui Zhu <zhuhui@xiaomi.com>
    Cc: Wang Xiaoqiang <wangxq10@lzu.edu.cn>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index f70c1ff46125..9c086c57105c 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -617,9 +617,7 @@ static void __init numa_init_array(void)
 		if (early_cpu_to_node(i) != NUMA_NO_NODE)
 			continue;
 		numa_set_node(i, rr);
-		rr = next_node(rr, node_online_map);
-		if (rr == MAX_NUMNODES)
-			rr = first_node(node_online_map);
+		rr = next_node_in(rr, node_online_map);
 	}
 }
 

commit 5f7ee246850ba18a6a7bcb3d5eddb6db68354688
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 8 12:14:55 2016 +0100

    x86/mm/numa: Check for failures in numa_clear_kernel_node_hotplug()
    
    numa_clear_kernel_node_hotplug() uses memblock_set_node() without
    checking for failures.
    
    memblock_set_node() is a complex function that might extend the
    memblock array - which extension might fail - so check for this
    possibility.
    
    It's not supposed to happen (because realistically if we have so
    little memory that this fails then we likely won't be able to
    boot anyway), but do the check nevertheless.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Chen Tang <imtangchen@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: y14sg1 <y14sg1@comcast.net>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index ede4506abb18..f70c1ff46125 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -491,8 +491,10 @@ static void __init numa_clear_kernel_node_hotplug(void)
 	 */
 	for (i = 0; i < numa_meminfo.nr_blks; i++) {
 		struct numa_memblk *mb = numa_meminfo.blk + i;
+		int ret;
 
-		memblock_set_node(mb->start, mb->end - mb->start, &memblock.reserved, mb->nid);
+		ret = memblock_set_node(mb->start, mb->end - mb->start, &memblock.reserved, mb->nid);
+		WARN_ON_ONCE(ret);
 	}
 
 	/*

commit c1a0bf347c40dd4b0a5bb10fdf4de76a1fbbbe8c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 8 09:57:34 2016 +0100

    x86/mm/numa: Clean up numa_clear_kernel_node_hotplug()
    
    So we fixed an overflow bug in numa_clear_kernel_node_hotplug():
    
      2b54ab3c66d4 ("x86/mm/numa: Fix memory corruption on 32-bit NUMA kernels")
    
    ... and the bug was indirectly caused by poor coding style,
    such as using start/end local variables unnecessarily, which
    lost the physaddr_t type.
    
    So make the code more readable and try to fully comment all
    the thinking behind the logic.
    
    No change in functionality.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Chen Tang <imtangchen@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: y14sg1 <y14sg1@comcast.net>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index d04f8094bc23..ede4506abb18 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -465,46 +465,65 @@ static bool __init numa_meminfo_cover_memory(const struct numa_meminfo *mi)
 	return true;
 }
 
+/*
+ * Mark all currently memblock-reserved physical memory (which covers the
+ * kernel's own memory ranges) as hot-unswappable.
+ */
 static void __init numa_clear_kernel_node_hotplug(void)
 {
-	int i, nid;
-	nodemask_t numa_kernel_nodes = NODE_MASK_NONE;
-	phys_addr_t start, end;
-	struct memblock_region *r;
+	nodemask_t reserved_nodemask = NODE_MASK_NONE;
+	struct memblock_region *mb_region;
+	int i;
 
 	/*
+	 * We have to do some preprocessing of memblock regions, to
+	 * make them suitable for reservation.
+	 *
 	 * At this time, all memory regions reserved by memblock are
-	 * used by the kernel. Set the nid in memblock.reserved will
-	 * mark out all the nodes the kernel resides in.
+	 * used by the kernel, but those regions are not split up
+	 * along node boundaries yet, and don't necessarily have their
+	 * node ID set yet either.
+	 *
+	 * So iterate over all memory known to the x86 architecture,
+	 * and use those ranges to set the nid in memblock.reserved.
+	 * This will split up the memblock regions along node
+	 * boundaries and will set the node IDs as well.
 	 */
 	for (i = 0; i < numa_meminfo.nr_blks; i++) {
-		struct numa_memblk *mb = &numa_meminfo.blk[i];
+		struct numa_memblk *mb = numa_meminfo.blk + i;
 
-		memblock_set_node(mb->start, mb->end - mb->start,
-				  &memblock.reserved, mb->nid);
+		memblock_set_node(mb->start, mb->end - mb->start, &memblock.reserved, mb->nid);
 	}
 
 	/*
-	 * Mark all kernel nodes.
+	 * Now go over all reserved memblock regions, to construct a
+	 * node mask of all kernel reserved memory areas.
 	 *
-	 * When booting with mem=nn[kMG] or in a kdump kernel, numa_meminfo
-	 * may not include all the memblock.reserved memory ranges because
-	 * trim_snb_memory() reserves specific pages for Sandy Bridge graphics.
+	 * [ Note, when booting with mem=nn[kMG] or in a kdump kernel,
+	 *   numa_meminfo might not include all memblock.reserved
+	 *   memory ranges, because quirks such as trim_snb_memory()
+	 *   reserve specific pages for Sandy Bridge graphics. ]
 	 */
-	for_each_memblock(reserved, r)
-		if (r->nid != MAX_NUMNODES)
-			node_set(r->nid, numa_kernel_nodes);
+	for_each_memblock(reserved, mb_region) {
+		if (mb_region->nid != MAX_NUMNODES)
+			node_set(mb_region->nid, reserved_nodemask);
+	}
 
-	/* Clear MEMBLOCK_HOTPLUG flag for memory in kernel nodes. */
+	/*
+	 * Finally, clear the MEMBLOCK_HOTPLUG flag for all memory
+	 * belonging to the reserved node mask.
+	 *
+	 * Note that this will include memory regions that reside
+	 * on nodes that contain kernel memory - entire nodes
+	 * become hot-unpluggable:
+	 */
 	for (i = 0; i < numa_meminfo.nr_blks; i++) {
-		nid = numa_meminfo.blk[i].nid;
-		if (!node_isset(nid, numa_kernel_nodes))
-			continue;
+		struct numa_memblk *mb = numa_meminfo.blk + i;
 
-		start = numa_meminfo.blk[i].start;
-		end = numa_meminfo.blk[i].end;
+		if (!node_isset(mb->nid, reserved_nodemask))
+			continue;
 
-		memblock_clear_hotplug(start, end - start);
+		memblock_clear_hotplug(mb->start, mb->end - mb->start);
 	}
 }
 

commit 59fd1214561921343305a0e9dc218bf3d40068f3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 8 08:47:48 2016 +0100

    x86/mm/numa: Fix 32-bit memblock range truncation bug on 32-bit NUMA kernels
    
    The following commit:
    
      a0acda917284 ("acpi, numa, mem_hotplug: mark all nodes the kernel resides un-hotpluggable")
    
    Introduced numa_clear_kernel_node_hotplug(), which function is executed
    during early bootup, and which marks all currently reserved memblock
    regions as hot-memory-unswappable as well.
    
    y14sg1 <y14sg1@comcast.net> reported that when running 32-bit NUMA kernels,
    the grsecurity/PAX kernel patch flagged a size overflow in this function:
    
      PAX: size overflow detected in function x86_numa_init arch/x86/mm/numa.c:691 [...]
    
    ... the reason for the overflow is that memblock_clear_hotplug() takes physical
    addresses as arguments, while the start/end variables used by
    numa_clear_kernel_node_hotplug() are 'unsigned long', which is 32-bit on PAE
    kernels, but which has 64-bit physical addresses.
    
    So on 32-bit PAE kernels that have physical memory above the 4GB boundary,
    we truncate a 64-bit physical address range to 32 bits and pass it to
    memblock_clear_hotplug(), which at minimum prevents the original memory-hotplug
    bugfix from working, but might have other side effects as well.
    
    The fix is to use the proper type to handle physical addresses, phys_addr_t.
    
    Reported-by: y14sg1 <y14sg1@comcast.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Chen Tang <imtangchen@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index c3b3f653ed0c..d04f8094bc23 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -469,7 +469,7 @@ static void __init numa_clear_kernel_node_hotplug(void)
 {
 	int i, nid;
 	nodemask_t numa_kernel_nodes = NODE_MASK_NONE;
-	unsigned long start, end;
+	phys_addr_t start, end;
 	struct memblock_region *r;
 
 	/*

commit 95cf82ecc1fcb44df1768162343cc8eb88083b86
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Sep 8 15:02:03 2015 -0700

    mem-hotplug: handle node hole when initializing numa_meminfo.
    
    When parsing SRAT, all memory ranges are added into numa_meminfo.  In
    numa_init(), before entering numa_cleanup_meminfo(), all possible memory
    ranges are in numa_meminfo.  And numa_cleanup_meminfo() removes all
    ranges over max_pfn or empty.
    
    But, this only works if the nodes are continuous.  Let's have a look at
    the following example:
    
    We have an SRAT like this:
    SRAT: Node 0 PXM 0 [mem 0x00000000-0x5fffffff]
    SRAT: Node 0 PXM 0 [mem 0x100000000-0x1ffffffffff]
    SRAT: Node 1 PXM 1 [mem 0x20000000000-0x3ffffffffff]
    SRAT: Node 4 PXM 2 [mem 0x40000000000-0x5ffffffffff] hotplug
    SRAT: Node 5 PXM 3 [mem 0x60000000000-0x7ffffffffff] hotplug
    SRAT: Node 2 PXM 4 [mem 0x80000000000-0x9ffffffffff] hotplug
    SRAT: Node 3 PXM 5 [mem 0xa0000000000-0xbffffffffff] hotplug
    SRAT: Node 6 PXM 6 [mem 0xc0000000000-0xdffffffffff] hotplug
    SRAT: Node 7 PXM 7 [mem 0xe0000000000-0xfffffffffff] hotplug
    
    On boot, only node 0,1,2,3 exist.
    
    And the numa_meminfo will look like this:
    numa_meminfo.nr_blks = 9
    1. on node 0: [0, 60000000]
    2. on node 0: [100000000, 20000000000]
    3. on node 1: [20000000000, 40000000000]
    4. on node 4: [40000000000, 60000000000]
    5. on node 5: [60000000000, 80000000000]
    6. on node 2: [80000000000, a0000000000]
    7. on node 3: [a0000000000, a0800000000]
    8. on node 6: [c0000000000, a0800000000]
    9. on node 7: [e0000000000, a0800000000]
    
    And numa_cleanup_meminfo() will merge 1 and 2, and remove 8,9 because the
    end address is over max_pfn, which is a0800000000.  But 4 and 5 are not
    removed because their end addresses are less then max_pfn.  But in fact,
    node 4 and 5 don't exist.
    
    In a word, numa_cleanup_meminfo() is not able to handle holes between nodes.
    
    Since memory ranges in node 4 and 5 are in numa_meminfo, in
    numa_register_memblks(), node 4 and 5 will be mistakenly set to online.
    
    If you run lscpu, it will show:
    NUMA node0 CPU(s):     0-14,128-142
    NUMA node1 CPU(s):     15-29,143-157
    NUMA node2 CPU(s):
    NUMA node3 CPU(s):
    NUMA node4 CPU(s):     62-76,190-204
    NUMA node5 CPU(s):     78-92,206-220
    
    In this patch, we use memblock_overlaps_region() to check if ranges in
    numa_meminfo overlap with ranges in memory_block.  Since memory_block
    contains all available memory at boot time, if they overlap, it means the
    ranges exist.  If not, then remove them from numa_meminfo.
    
    After this patch, lscpu will show:
    NUMA node0 CPU(s):     0-14,128-142
    NUMA node1 CPU(s):     15-29,143-157
    NUMA node4 CPU(s):     62-76,190-204
    NUMA node5 CPU(s):     78-92,206-220
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Vladimir Murzin <vladimir.murzin@arm.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Alexander Kuleshov <kuleshovmail@gmail.com>
    Cc: Baoquan He <bhe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 4053bb58bf92..c3b3f653ed0c 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -246,8 +246,10 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 		bi->start = max(bi->start, low);
 		bi->end = min(bi->end, high);
 
-		/* and there's no empty block */
-		if (bi->start >= bi->end)
+		/* and there's no empty or non-exist block */
+		if (bi->start >= bi->end ||
+		    !memblock_overlaps_region(&memblock.memory,
+			bi->start, bi->end - bi->start))
 			numa_remove_memblk_from(i--, mi);
 	}
 

commit 22ef882e6b5bd2bf668d10b1e2be3dc2fc365b99
Author: Dave Young <dyoung@redhat.com>
Date:   Tue Apr 7 21:41:32 2015 +0800

    x86/mm/numa: Fix kernel stack corruption in numa_init()->numa_clear_kernel_node_hotplug()
    
    I got below kernel panic during kdump test on Thinkpad T420
    laptop:
    
    [    0.000000] No NUMA configuration found
    [    0.000000] Faking a node at [mem 0x0000000000000000-0x0000000037ba4fff]
    [    0.000000] Kernel panic - not syncing: stack-protector: Kernel stack is corrupted in: ffffffff81d21910
     ...
    [    0.000000] Call Trace:
    [    0.000000]  [<ffffffff817c2a26>] dump_stack+0x45/0x57
    [    0.000000]  [<ffffffff817bc8d2>] panic+0xd0/0x204
    [    0.000000]  [<ffffffff81d21910>] ? numa_clear_kernel_node_hotplug+0xe6/0xf2
    [    0.000000]  [<ffffffff8107741b>] __stack_chk_fail+0x1b/0x20
    [    0.000000]  [<ffffffff81d21910>] numa_clear_kernel_node_hotplug+0xe6/0xf2
    [    0.000000]  [<ffffffff81d21e5d>] numa_init+0x1a5/0x520
    [    0.000000]  [<ffffffff81d222b1>] x86_numa_init+0x19/0x3d
    [    0.000000]  [<ffffffff81d22460>] initmem_init+0x9/0xb
    [    0.000000]  [<ffffffff81d0d00c>] setup_arch+0x94f/0xc82
    [    0.000000]  [<ffffffff81d05120>] ? early_idt_handlers+0x120/0x120
    [    0.000000]  [<ffffffff817bd0bb>] ? printk+0x55/0x6b
    [    0.000000]  [<ffffffff81d05120>] ? early_idt_handlers+0x120/0x120
    [    0.000000]  [<ffffffff81d05d9b>] start_kernel+0xe8/0x4d6
    [    0.000000]  [<ffffffff81d05120>] ? early_idt_handlers+0x120/0x120
    [    0.000000]  [<ffffffff81d05120>] ? early_idt_handlers+0x120/0x120
    [    0.000000]  [<ffffffff81d055ee>] x86_64_start_reservations+0x2a/0x2c
    [    0.000000]  [<ffffffff81d05751>] x86_64_start_kernel+0x161/0x184
    [    0.000000] ---[ end Kernel panic - not syncing: stack-protector: Kernel sta
    
    This is caused by writing over the end of numa mask bitmap
    in numa_clear_kernel_node().
    
    numa_clear_kernel_node() tries to set the node id in a mask bitmap,
    by iterating all reserved regions and assuming that every region
    has a valid nid.
    
    This assumption is not true because there's an exception for some
    graphic memory quirks. See trim_snb_memory() in arch/x86/kernel/setup.c
    
    It is easily to reproduce the bug in the kdump kernel because kdump
    kernel use pre-reserved memory instead of the whole memory, but
    kexec pass other reserved memory ranges to 2nd kernel as well.
    like below in my test:
    
    kdump kernel ram 0x2d000000 - 0x37bfffff
    One of the reserved regions: 0x40000000 - 0x40100000 which
    includes 0x40004000, a page excluded in trim_snb_memory(). For
    this memblock reserved region the nid is not set, it is still
    default value MAX_NUMNODES. later node_set will set bit
    MAX_NUMNODES thus stack corruption happen.
    
    This also happens when booting with mem= kernel commandline
    during my test.
    
    Fixing it by adding a check, do not call node_set in case nid is
    MAX_NUMNODES.
    
    Signed-off-by: Dave Young <dyoung@redhat.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bhe@redhat.com
    Cc: qiuxishi@huawei.com
    Link: http://lkml.kernel.org/r/20150407134132.GA23522@dhcp-16-198.nay.redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index cd4785bbacb9..4053bb58bf92 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -482,9 +482,16 @@ static void __init numa_clear_kernel_node_hotplug(void)
 				  &memblock.reserved, mb->nid);
 	}
 
-	/* Mark all kernel nodes. */
+	/*
+	 * Mark all kernel nodes.
+	 *
+	 * When booting with mem=nn[kMG] or in a kdump kernel, numa_meminfo
+	 * may not include all the memblock.reserved memory ranges because
+	 * trim_snb_memory() reserves specific pages for Sandy Bridge graphics.
+	 */
 	for_each_memblock(reserved, r)
-		node_set(r->nid, numa_kernel_nodes);
+		if (r->nid != MAX_NUMNODES)
+			node_set(r->nid, numa_kernel_nodes);
 
 	/* Clear MEMBLOCK_HOTPLUG flag for memory in kernel nodes. */
 	for (i = 0; i < numa_meminfo.nr_blks; i++) {

commit bf58b4879c33b3475a33740562ebf6583f531d4a
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Feb 13 14:37:12 2015 -0800

    x86: use %*pb[l] to print bitmaps including cpumasks and nodemasks
    
    printk and friends can now format bitmaps using '%*pb[l]'.  cpumask
    and nodemask also provide cpumask_pr_args() and nodemask_pr_args()
    respectively which can be used to generate the two printf arguments
    necessary to format the specified cpu/nodemask.
    
    * Unnecessary buffer size calculation and condition on the lenght
      removed from intel_cacheinfo.c::show_shared_cpu_map_func().
    
    * uv_nmi_nr_cpus_pr() got overly smart and implemented "..."
      abbreviation if the output stretched over the predefined 1024 byte
      buffer.  Replaced with plain printk.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Mike Travis <travis@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 1a883705a12a..cd4785bbacb9 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -794,7 +794,6 @@ int early_cpu_to_node(int cpu)
 void debug_cpumask_set_cpu(int cpu, int node, bool enable)
 {
 	struct cpumask *mask;
-	char buf[64];
 
 	if (node == NUMA_NO_NODE) {
 		/* early_cpu_to_node() already emits a warning and trace */
@@ -812,10 +811,9 @@ void debug_cpumask_set_cpu(int cpu, int node, bool enable)
 	else
 		cpumask_clear_cpu(cpu, mask);
 
-	cpulist_scnprintf(buf, sizeof(buf), mask);
-	printk(KERN_DEBUG "%s cpu %d node %d: mask now %s\n",
+	printk(KERN_DEBUG "%s cpu %d node %d: mask now %*pbl\n",
 		enable ? "numa_add_cpu" : "numa_remove_cpu",
-		cpu, node, buf);
+		cpu, node, cpumask_pr_args(mask));
 	return;
 }
 

commit dfe2c6dcc8ca2cdc662d7c0473e9811b72ef3370
Merge: a45d572841a2 64e455079e1b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 14 03:54:50 2014 +0200

    Merge branch 'akpm' (patches from Andrew Morton)
    
    Merge second patch-bomb from Andrew Morton:
     - a few hotfixes
     - drivers/dma updates
     - MAINTAINERS updates
     - Quite a lot of lib/ updates
     - checkpatch updates
     - binfmt updates
     - autofs4
     - drivers/rtc/
     - various small tweaks to less used filesystems
     - ipc/ updates
     - kernel/watchdog.c changes
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (135 commits)
      mm: softdirty: enable write notifications on VMAs after VM_SOFTDIRTY cleared
      kernel/param: consolidate __{start,stop}___param[] in <linux/moduleparam.h>
      ia64: remove duplicate declarations of __per_cpu_start[] and __per_cpu_end[]
      frv: remove unused declarations of __start___ex_table and __stop___ex_table
      kvm: ensure hard lockup detection is disabled by default
      kernel/watchdog.c: control hard lockup detection default
      staging: rtl8192u: use %*pEn to escape buffer
      staging: rtl8192e: use %*pEn to escape buffer
      staging: wlan-ng: use %*pEhp to print SN
      lib80211: remove unused print_ssid()
      wireless: hostap: proc: print properly escaped SSID
      wireless: ipw2x00: print SSID via %*pE
      wireless: libertas: print esaped string via %*pE
      lib/vsprintf: add %*pE[achnops] format specifier
      lib / string_helpers: introduce string_escape_mem()
      lib / string_helpers: refactoring the test suite
      lib / string_helpers: move documentation to c-file
      include/linux: remove strict_strto* definitions
      arch/x86/mm/numa.c: fix boot failure when all nodes are hotpluggable
      fs: check bh blocknr earlier when searching lru
      ...

commit bd5cfb8977fbb49d9350f7c81cf1516142e35a6a
Author: Xishi Qiu <qiuxishi@huawei.com>
Date:   Mon Oct 13 15:55:07 2014 -0700

    arch/x86/mm/numa.c: fix boot failure when all nodes are hotpluggable
    
    If all the nodes are marked hotpluggable, alloc node data will fail.
    Because __next_mem_range_rev() will skip the hotpluggable memory
    regions.  numa_clear_kernel_node_hotplug() is called after alloc node
    data.
    
    numa_init()
        ...
        ret = init_func();  // this will mark hotpluggable flag from SRAT
        ...
        memblock_set_bottom_up(false);
        ...
        ret = numa_register_memblks(&numa_meminfo);  // this will alloc node data(pglist_data)
        ...
        numa_clear_kernel_node_hotplug();  // in case all the nodes are hotpluggable
        ...
    
    numa_register_memblks()
        setup_node_data()
            memblock_find_in_range_node()
                __memblock_find_range_top_down()
                    for_each_mem_range_rev()
                        __next_mem_range_rev()
    
    This patch moves numa_clear_kernel_node_hotplug() into
    numa_register_memblks(), clear kernel node hotpluggable flag before
    alloc node data, then alloc node data won't fail even all the nodes
    are hotpluggable.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index a32b706c401a..4e1e5709fe17 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -478,6 +478,42 @@ static bool __init numa_meminfo_cover_memory(const struct numa_meminfo *mi)
 	return true;
 }
 
+static void __init numa_clear_kernel_node_hotplug(void)
+{
+	int i, nid;
+	nodemask_t numa_kernel_nodes = NODE_MASK_NONE;
+	unsigned long start, end;
+	struct memblock_region *r;
+
+	/*
+	 * At this time, all memory regions reserved by memblock are
+	 * used by the kernel. Set the nid in memblock.reserved will
+	 * mark out all the nodes the kernel resides in.
+	 */
+	for (i = 0; i < numa_meminfo.nr_blks; i++) {
+		struct numa_memblk *mb = &numa_meminfo.blk[i];
+
+		memblock_set_node(mb->start, mb->end - mb->start,
+				  &memblock.reserved, mb->nid);
+	}
+
+	/* Mark all kernel nodes. */
+	for_each_memblock(reserved, r)
+		node_set(r->nid, numa_kernel_nodes);
+
+	/* Clear MEMBLOCK_HOTPLUG flag for memory in kernel nodes. */
+	for (i = 0; i < numa_meminfo.nr_blks; i++) {
+		nid = numa_meminfo.blk[i].nid;
+		if (!node_isset(nid, numa_kernel_nodes))
+			continue;
+
+		start = numa_meminfo.blk[i].start;
+		end = numa_meminfo.blk[i].end;
+
+		memblock_clear_hotplug(start, end - start);
+	}
+}
+
 static int __init numa_register_memblks(struct numa_meminfo *mi)
 {
 	unsigned long uninitialized_var(pfn_align);
@@ -495,6 +531,15 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 				  &memblock.memory, mb->nid);
 	}
 
+	/*
+	 * At very early time, the kernel have to use some memory such as
+	 * loading the kernel image. We cannot prevent this anyway. So any
+	 * node the kernel resides in should be un-hotpluggable.
+	 *
+	 * And when we come here, alloc node data won't fail.
+	 */
+	numa_clear_kernel_node_hotplug();
+
 	/*
 	 * If sections array is gonna be used for pfn -> nid mapping, check
 	 * whether its granularity is fine enough.
@@ -554,41 +599,6 @@ static void __init numa_init_array(void)
 	}
 }
 
-static void __init numa_clear_kernel_node_hotplug(void)
-{
-	int i, nid;
-	nodemask_t numa_kernel_nodes = NODE_MASK_NONE;
-	unsigned long start, end;
-	struct memblock_region *r;
-
-	/*
-	 * At this time, all memory regions reserved by memblock are
-	 * used by the kernel. Set the nid in memblock.reserved will
-	 * mark out all the nodes the kernel resides in.
-	 */
-	for (i = 0; i < numa_meminfo.nr_blks; i++) {
-		struct numa_memblk *mb = &numa_meminfo.blk[i];
-		memblock_set_node(mb->start, mb->end - mb->start,
-				  &memblock.reserved, mb->nid);
-	}
-
-	/* Mark all kernel nodes. */
-	for_each_memblock(reserved, r)
-		node_set(r->nid, numa_kernel_nodes);
-
-	/* Clear MEMBLOCK_HOTPLUG flag for memory in kernel nodes. */
-	for (i = 0; i < numa_meminfo.nr_blks; i++) {
-		nid = numa_meminfo.blk[i].nid;
-		if (!node_isset(nid, numa_kernel_nodes))
-			continue;
-
-		start = numa_meminfo.blk[i].start;
-		end = numa_meminfo.blk[i].end;
-
-		memblock_clear_hotplug(start, end - start);
-	}
-}
-
 static int __init numa_init(int (*init_func)(void))
 {
 	int i;
@@ -643,15 +653,6 @@ static int __init numa_init(int (*init_func)(void))
 	}
 	numa_init_array();
 
-	/*
-	 * At very early time, the kernel have to use some memory such as
-	 * loading the kernel image. We cannot prevent this anyway. So any
-	 * node the kernel resides in should be un-hotpluggable.
-	 *
-	 * And when we come here, numa_init() won't fail.
-	 */
-	numa_clear_kernel_node_hotplug();
-
 	return 0;
 }
 

commit 8b375f64dcf45ba5cfb36398b69b877dc35410fa
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Fri Aug 22 13:27:36 2014 -0700

    x86/mm/numa: Drop dead code and rename setup_node_data() to setup_alloc_data()
    
    The setup_node_data() function allocates a pg_data_t object,
    inserts it into the node_data[] array and initializes the
    following fields: node_id, node_start_pfn and
    node_spanned_pages.
    
    However, a few function calls later during the kernel boot,
    free_area_init_node() re-initializes those fields, possibly with
    setup_node_data() is not used.
    
    This causes a small glitch when running Linux as a hyperv numa
    guest:
    
      SRAT: PXM 0 -> APIC 0x00 -> Node 0
      SRAT: PXM 0 -> APIC 0x01 -> Node 0
      SRAT: PXM 1 -> APIC 0x02 -> Node 1
      SRAT: PXM 1 -> APIC 0x03 -> Node 1
      SRAT: Node 0 PXM 0 [mem 0x00000000-0x7fffffff]
      SRAT: Node 1 PXM 1 [mem 0x80200000-0xf7ffffff]
      SRAT: Node 1 PXM 1 [mem 0x100000000-0x1081fffff]
      NUMA: Node 1 [mem 0x80200000-0xf7ffffff] + [mem 0x100000000-0x1081fffff] -> [mem 0x80200000-0x1081fffff]
      Initmem setup node 0 [mem 0x00000000-0x7fffffff]
        NODE_DATA [mem 0x7ffdc000-0x7ffeffff]
      Initmem setup node 1 [mem 0x80800000-0x1081fffff]
        NODE_DATA [mem 0x1081ea000-0x1081fdfff]
      crashkernel: memory value expected
       [ffffea0000000000-ffffea0001ffffff] PMD -> [ffff88007de00000-ffff88007fdfffff] on node 0
       [ffffea0002000000-ffffea00043fffff] PMD -> [ffff880105600000-ffff8801077fffff] on node 1
      Zone ranges:
        DMA      [mem 0x00001000-0x00ffffff]
        DMA32    [mem 0x01000000-0xffffffff]
        Normal   [mem 0x100000000-0x1081fffff]
      Movable zone start for each node
      Early memory node ranges
        node   0: [mem 0x00001000-0x0009efff]
        node   0: [mem 0x00100000-0x7ffeffff]
        node   1: [mem 0x80200000-0xf7ffffff]
        node   1: [mem 0x100000000-0x1081fffff]
      On node 0 totalpages: 524174
        DMA zone: 64 pages used for memmap
        DMA zone: 21 pages reserved
        DMA zone: 3998 pages, LIFO batch:0
        DMA32 zone: 8128 pages used for memmap
        DMA32 zone: 520176 pages, LIFO batch:31
      On node 1 totalpages: 524288
        DMA32 zone: 7672 pages used for memmap
        DMA32 zone: 491008 pages, LIFO batch:31
        Normal zone: 520 pages used for memmap
        Normal zone: 33280 pages, LIFO batch:7
    
    In this dmesg, the SRAT table reports that the memory range for
    node 1 starts at 0x80200000.  However, the line starting with
    "Initmem" reports that node 1 memory range starts at 0x80800000.
     The "Initmem" line is reported by setup_node_data() and is
    wrong, because the kernel ends up using the range as reported in
    the SRAT table.
    
    This commit drops all that dead code from setup_node_data(),
    renames it to alloc_node_data() and adds a printk() to
    free_area_init_node() so that we report a node's memory range
    accurately.
    
    Here's the same dmesg section with this patch applied:
    
       SRAT: PXM 0 -> APIC 0x00 -> Node 0
       SRAT: PXM 0 -> APIC 0x01 -> Node 0
       SRAT: PXM 1 -> APIC 0x02 -> Node 1
       SRAT: PXM 1 -> APIC 0x03 -> Node 1
       SRAT: Node 0 PXM 0 [mem 0x00000000-0x7fffffff]
       SRAT: Node 1 PXM 1 [mem 0x80200000-0xf7ffffff]
       SRAT: Node 1 PXM 1 [mem 0x100000000-0x1081fffff]
       NUMA: Node 1 [mem 0x80200000-0xf7ffffff] + [mem 0x100000000-0x1081fffff] -> [mem 0x80200000-0x1081fffff]
       NODE_DATA(0) allocated [mem 0x7ffdc000-0x7ffeffff]
       NODE_DATA(1) allocated [mem 0x1081ea000-0x1081fdfff]
       crashkernel: memory value expected
        [ffffea0000000000-ffffea0001ffffff] PMD -> [ffff88007de00000-ffff88007fdfffff] on node 0
        [ffffea0002000000-ffffea00043fffff] PMD -> [ffff880105600000-ffff8801077fffff] on node 1
       Zone ranges:
         DMA      [mem 0x00001000-0x00ffffff]
         DMA32    [mem 0x01000000-0xffffffff]
         Normal   [mem 0x100000000-0x1081fffff]
       Movable zone start for each node
       Early memory node ranges
         node   0: [mem 0x00001000-0x0009efff]
         node   0: [mem 0x00100000-0x7ffeffff]
         node   1: [mem 0x80200000-0xf7ffffff]
         node   1: [mem 0x100000000-0x1081fffff]
       Initmem setup node 0 [mem 0x00001000-0x7ffeffff]
       On node 0 totalpages: 524174
         DMA zone: 64 pages used for memmap
         DMA zone: 21 pages reserved
         DMA zone: 3998 pages, LIFO batch:0
         DMA32 zone: 8128 pages used for memmap
         DMA32 zone: 520176 pages, LIFO batch:31
       Initmem setup node 1 [mem 0x80200000-0x1081fffff]
       On node 1 totalpages: 524288
         DMA32 zone: 7672 pages used for memmap
         DMA32 zone: 491008 pages, LIFO batch:31
         Normal zone: 520 pages used for memmap
         Normal zone: 33280 pages, LIFO batch:7
    
    This commit was tested on a two node bare-metal NUMA machine and
    Linux as a numa guest on hyperv and qemu/kvm.
    
    PS: The wrong memory range reported by setup_node_data() seems to be
        harmless in the current kernel because it's just not used.  However,
        that bad range is used in kernel 2.6.32 to initialize the old boot
        memory allocator, which causes a crash during boot.
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index a32b706c401a..d221374d5ce8 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -185,26 +185,14 @@ int __init numa_add_memblk(int nid, u64 start, u64 end)
 	return numa_add_memblk_to(nid, start, end, &numa_meminfo);
 }
 
-/* Initialize NODE_DATA for a node on the local memory */
-static void __init setup_node_data(int nid, u64 start, u64 end)
+/* Allocate NODE_DATA for a node on the local memory */
+static void __init alloc_node_data(int nid)
 {
 	const size_t nd_size = roundup(sizeof(pg_data_t), PAGE_SIZE);
 	u64 nd_pa;
 	void *nd;
 	int tnid;
 
-	/*
-	 * Don't confuse VM with a node that doesn't have the
-	 * minimum amount of memory:
-	 */
-	if (end && (end - start) < NODE_MIN_SIZE)
-		return;
-
-	start = roundup(start, ZONE_ALIGN);
-
-	printk(KERN_INFO "Initmem setup node %d [mem %#010Lx-%#010Lx]\n",
-	       nid, start, end - 1);
-
 	/*
 	 * Allocate node data.  Try node-local memory and then any node.
 	 * Never allocate in DMA zone.
@@ -222,7 +210,7 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	nd = __va(nd_pa);
 
 	/* report and initialize */
-	printk(KERN_INFO "  NODE_DATA [mem %#010Lx-%#010Lx]\n",
+	printk(KERN_INFO "NODE_DATA(%d) allocated [mem %#010Lx-%#010Lx]\n", nid,
 	       nd_pa, nd_pa + nd_size - 1);
 	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
 	if (tnid != nid)
@@ -230,9 +218,6 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 
 	node_data[nid] = nd;
 	memset(NODE_DATA(nid), 0, sizeof(pg_data_t));
-	NODE_DATA(nid)->node_id = nid;
-	NODE_DATA(nid)->node_start_pfn = start >> PAGE_SHIFT;
-	NODE_DATA(nid)->node_spanned_pages = (end - start) >> PAGE_SHIFT;
 
 	node_set_online(nid);
 }
@@ -523,8 +508,17 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 			end = max(mi->blk[i].end, end);
 		}
 
-		if (start < end)
-			setup_node_data(nid, start, end);
+		if (start >= end)
+			continue;
+
+		/*
+		 * Don't confuse VM with a node that doesn't have the
+		 * minimum amount of memory:
+		 */
+		if (end && (end - start) < NODE_MIN_SIZE)
+			continue;
+
+		alloc_node_data(nid);
 	}
 
 	/* Dump memblock with node info and return. */

commit af4459d3636790735fccd83f0337c8380a0a4cc2
Author: Emil Medve <Emilian.Medve@Freescale.com>
Date:   Wed Jun 4 16:08:19 2014 -0700

    arch/x86/mm/numa.c: use for_each_memblock()
    
    Signed-off-by: Emil Medve <Emilian.Medve@Freescale.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 1d045f9c390f..a32b706c401a 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -559,7 +559,7 @@ static void __init numa_clear_kernel_node_hotplug(void)
 	int i, nid;
 	nodemask_t numa_kernel_nodes = NODE_MASK_NONE;
 	unsigned long start, end;
-	struct memblock_type *type = &memblock.reserved;
+	struct memblock_region *r;
 
 	/*
 	 * At this time, all memory regions reserved by memblock are
@@ -573,8 +573,8 @@ static void __init numa_clear_kernel_node_hotplug(void)
 	}
 
 	/* Mark all kernel nodes. */
-	for (i = 0; i < type->cnt; i++)
-		node_set(type->regions[i].nid, numa_kernel_nodes);
+	for_each_memblock(reserved, r)
+		node_set(r->nid, numa_kernel_nodes);
 
 	/* Clear MEMBLOCK_HOTPLUG flag for memory in kernel nodes. */
 	for (i = 0; i < numa_meminfo.nr_blks; i++) {

commit b5660ba76b41af69a0c09d434927bb4b4cadd4b1
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Feb 25 12:14:06 2014 -0800

    x86, platforms: Remove NUMAQ
    
    The NUMAQ support seems to be unmaintained, remove it.
    
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: David Rientjes <rientjes@google.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/n/530CFD6C.7040705@zytor.com

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 27aa0455fab3..1d045f9c390f 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -687,10 +687,6 @@ static int __init dummy_numa_init(void)
 void __init x86_numa_init(void)
 {
 	if (!numa_off) {
-#ifdef CONFIG_X86_NUMAQ
-		if (!numa_init(numaq_numa_init))
-			return;
-#endif
 #ifdef CONFIG_ACPI_NUMA
 		if (!numa_init(x86_acpi_numa_init))
 			return;

commit 7bc35fdde6724549a0239b71e08b9f33d8bf2bfb
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Thu Feb 6 12:04:27 2014 -0800

    arch/x86/mm/numa.c: fix array index overflow when synchronizing nid to memblock.reserved.
    
    The following path will cause array out of bound.
    
    memblock_add_region() will always set nid in memblock.reserved to
    MAX_NUMNODES.  In numa_register_memblks(), after we set all nid to
    correct valus in memblock.reserved, we called setup_node_data(), and
    used memblock_alloc_nid() to allocate memory, with nid set to
    MAX_NUMNODES.
    
    The nodemask_t type can be seen as a bit array.  And the index is 0 ~
    MAX_NUMNODES-1.
    
    After that, when we call node_set() in numa_clear_kernel_node_hotplug(),
    the nodemask_t got an index of value MAX_NUMNODES, which is out of [0 ~
    MAX_NUMNODES-1].
    
    See below:
    
    numa_init()
     |---> numa_register_memblks()
     |      |---> memblock_set_node(memory)         set correct nid in memblock.memory
     |      |---> memblock_set_node(reserved)       set correct nid in memblock.reserved
     |      |......
     |      |---> setup_node_data()
     |             |---> memblock_alloc_nid()       here, nid is set to MAX_NUMNODES (1024)
     |......
     |---> numa_clear_kernel_node_hotplug()
            |---> node_set()                        here, we have an index 1024, and overflowed
    
    This patch moves nid setting to numa_clear_kernel_node_hotplug() to fix
    this problem.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Tested-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Reported-by: Dave Jones <davej@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Tested-by: Dave Jones <davej@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 45ec9d72b6dd..27aa0455fab3 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -493,14 +493,6 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 		struct numa_memblk *mb = &mi->blk[i];
 		memblock_set_node(mb->start, mb->end - mb->start,
 				  &memblock.memory, mb->nid);
-
-		/*
-		 * At this time, all memory regions reserved by memblock are
-		 * used by the kernel. Set the nid in memblock.reserved will
-		 * mark out all the nodes the kernel resides in.
-		 */
-		memblock_set_node(mb->start, mb->end - mb->start,
-				  &memblock.reserved, mb->nid);
 	}
 
 	/*
@@ -569,6 +561,17 @@ static void __init numa_clear_kernel_node_hotplug(void)
 	unsigned long start, end;
 	struct memblock_type *type = &memblock.reserved;
 
+	/*
+	 * At this time, all memory regions reserved by memblock are
+	 * used by the kernel. Set the nid in memblock.reserved will
+	 * mark out all the nodes the kernel resides in.
+	 */
+	for (i = 0; i < numa_meminfo.nr_blks; i++) {
+		struct numa_memblk *mb = &numa_meminfo.blk[i];
+		memblock_set_node(mb->start, mb->end - mb->start,
+				  &memblock.reserved, mb->nid);
+	}
+
 	/* Mark all kernel nodes. */
 	for (i = 0; i < type->cnt; i++)
 		node_set(type->regions[i].nid, numa_kernel_nodes);

commit 017c217a26e9bf6948482f751b30d0507e30a7d0
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Thu Feb 6 12:04:25 2014 -0800

    arch/x86/mm/numa.c: initialize numa_kernel_nodes in numa_clear_kernel_node_hotplug()
    
    On-stack variable numa_kernel_nodes in numa_clear_kernel_node_hotplug()
    was not initialized.  So we need to initialize it.
    
    [akpm@linux-foundation.org: use NODE_MASK_NONE, per David]
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Tested-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Reported-by: Dave Jones <davej@redhat.com>
    Reported-by: David Rientjes <rientjes@google.com>
    Tested-by: Dave Jones <davej@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 81b2750f3666..45ec9d72b6dd 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -565,7 +565,7 @@ static void __init numa_init_array(void)
 static void __init numa_clear_kernel_node_hotplug(void)
 {
 	int i, nid;
-	nodemask_t numa_kernel_nodes;
+	nodemask_t numa_kernel_nodes = NODE_MASK_NONE;
 	unsigned long start, end;
 	struct memblock_type *type = &memblock.reserved;
 

commit a0acda917284183f9b71e2d08b0aa0aea722b321
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Jan 21 15:49:32 2014 -0800

    acpi, numa, mem_hotplug: mark all nodes the kernel resides un-hotpluggable
    
    At very early time, the kernel have to use some memory such as loading
    the kernel image.  We cannot prevent this anyway.  So any node the
    kernel resides in should be un-hotpluggable.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Rafael J . Wysocki" <rjw@sisk.pl>
    Cc: Chen Tang <imtangchen@gmail.com>
    Cc: Gong Chen <gong.chen@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Liu Jiang <jiang.liu@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Renninger <trenn@suse.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasilis Liaskovitis <vasilis.liaskovitis@profitbricks.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 78d6a9e5830e..81b2750f3666 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -493,6 +493,14 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 		struct numa_memblk *mb = &mi->blk[i];
 		memblock_set_node(mb->start, mb->end - mb->start,
 				  &memblock.memory, mb->nid);
+
+		/*
+		 * At this time, all memory regions reserved by memblock are
+		 * used by the kernel. Set the nid in memblock.reserved will
+		 * mark out all the nodes the kernel resides in.
+		 */
+		memblock_set_node(mb->start, mb->end - mb->start,
+				  &memblock.reserved, mb->nid);
 	}
 
 	/*
@@ -554,6 +562,30 @@ static void __init numa_init_array(void)
 	}
 }
 
+static void __init numa_clear_kernel_node_hotplug(void)
+{
+	int i, nid;
+	nodemask_t numa_kernel_nodes;
+	unsigned long start, end;
+	struct memblock_type *type = &memblock.reserved;
+
+	/* Mark all kernel nodes. */
+	for (i = 0; i < type->cnt; i++)
+		node_set(type->regions[i].nid, numa_kernel_nodes);
+
+	/* Clear MEMBLOCK_HOTPLUG flag for memory in kernel nodes. */
+	for (i = 0; i < numa_meminfo.nr_blks; i++) {
+		nid = numa_meminfo.blk[i].nid;
+		if (!node_isset(nid, numa_kernel_nodes))
+			continue;
+
+		start = numa_meminfo.blk[i].start;
+		end = numa_meminfo.blk[i].end;
+
+		memblock_clear_hotplug(start, end - start);
+	}
+}
+
 static int __init numa_init(int (*init_func)(void))
 {
 	int i;
@@ -568,6 +600,8 @@ static int __init numa_init(int (*init_func)(void))
 	memset(&numa_meminfo, 0, sizeof(numa_meminfo));
 	WARN_ON(memblock_set_node(0, ULLONG_MAX, &memblock.memory,
 				  MAX_NUMNODES));
+	WARN_ON(memblock_set_node(0, ULLONG_MAX, &memblock.reserved,
+				  MAX_NUMNODES));
 	/* In case that parsing SRAT failed. */
 	WARN_ON(memblock_clear_hotplug(0, ULLONG_MAX));
 	numa_reset_distance();
@@ -605,6 +639,16 @@ static int __init numa_init(int (*init_func)(void))
 			numa_clear_node(i);
 	}
 	numa_init_array();
+
+	/*
+	 * At very early time, the kernel have to use some memory such as
+	 * loading the kernel image. We cannot prevent this anyway. So any
+	 * node the kernel resides in should be un-hotpluggable.
+	 *
+	 * And when we come here, numa_init() won't fail.
+	 */
+	numa_clear_kernel_node_hotplug();
+
 	return 0;
 }
 

commit 05d1d8cb1c7c25b7c7197817b3418524ace61372
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Jan 21 15:49:29 2014 -0800

    acpi, numa, mem_hotplug: mark hotpluggable memory in memblock
    
    When parsing SRAT, we know that which memory area is hotpluggable.  So we
    invoke function memblock_mark_hotplug() introduced by previous patch to
    mark hotpluggable memory in memblock.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Rafael J . Wysocki" <rjw@sisk.pl>
    Cc: Chen Tang <imtangchen@gmail.com>
    Cc: Gong Chen <gong.chen@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Liu Jiang <jiang.liu@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Renninger <trenn@suse.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasilis Liaskovitis <vasilis.liaskovitis@profitbricks.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 82e079a0d363..78d6a9e5830e 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -568,6 +568,8 @@ static int __init numa_init(int (*init_func)(void))
 	memset(&numa_meminfo, 0, sizeof(numa_meminfo));
 	WARN_ON(memblock_set_node(0, ULLONG_MAX, &memblock.memory,
 				  MAX_NUMNODES));
+	/* In case that parsing SRAT failed. */
+	WARN_ON(memblock_clear_hotplug(0, ULLONG_MAX));
 	numa_reset_distance();
 
 	ret = init_func();

commit e7e8de5918dd6a07cbddae559600d7765ad6a56e
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Jan 21 15:49:26 2014 -0800

    memblock: make memblock_set_node() support different memblock_type
    
    [sfr@canb.auug.org.au: fix powerpc build]
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Rafael J . Wysocki" <rjw@sisk.pl>
    Cc: Chen Tang <imtangchen@gmail.com>
    Cc: Gong Chen <gong.chen@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Liu Jiang <jiang.liu@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Renninger <trenn@suse.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasilis Liaskovitis <vasilis.liaskovitis@profitbricks.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index c85da7bb6b60..82e079a0d363 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -491,7 +491,8 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 
 	for (i = 0; i < mi->nr_blks; i++) {
 		struct numa_memblk *mb = &mi->blk[i];
-		memblock_set_node(mb->start, mb->end - mb->start, mb->nid);
+		memblock_set_node(mb->start, mb->end - mb->start,
+				  &memblock.memory, mb->nid);
 	}
 
 	/*
@@ -565,7 +566,8 @@ static int __init numa_init(int (*init_func)(void))
 	nodes_clear(node_possible_map);
 	nodes_clear(node_online_map);
 	memset(&numa_meminfo, 0, sizeof(numa_meminfo));
-	WARN_ON(memblock_set_node(0, ULLONG_MAX, MAX_NUMNODES));
+	WARN_ON(memblock_set_node(0, ULLONG_MAX, &memblock.memory,
+				  MAX_NUMNODES));
 	numa_reset_distance();
 
 	ret = init_func();

commit f3d815cb854b2f6262ade56a4d91a1ed3f1e50c4
Author: Lans Zhang <jia.zhang@windriver.com>
Date:   Fri Dec 6 12:18:30 2013 +0800

    x86/mm/numa: Fix 32-bit kernel NUMA boot
    
    When booting a 32-bit x86 kernel on a NUMA machine, node data
    cannot be allocated from local node if the account of memory for
    node 0 covers the low memory space entirely:
    
      [    0.000000] Initmem setup node 0 [mem 0x00000000-0x83fffffff]
      [    0.000000]   NODE_DATA [mem 0x367ed000-0x367edfff]
      [    0.000000] Initmem setup node 1 [mem 0x840000000-0xfffffffff]
      [    0.000000] Cannot find 4096 bytes in node 1
      [    0.000000] 64664MB HIGHMEM available.
      [    0.000000] 871MB LOWMEM available.
    
    To fix this issue, node data is allowed to be allocated from
    other nodes if the memory of local node is still not mapped. The
    expected result looks like this:
    
      [    0.000000] Initmem setup node 0 [mem 0x00000000-0x83fffffff]
      [    0.000000]   NODE_DATA [mem 0x367ed000-0x367edfff]
      [    0.000000] Initmem setup node 1 [mem 0x840000000-0xfffffffff]
      [    0.000000]   NODE_DATA [mem 0x367ec000-0x367ecfff]
      [    0.000000]     NODE_DATA(1) on node 0
      [    0.000000] 64664MB HIGHMEM available.
      [    0.000000] 871MB LOWMEM available.
    
    Signed-off-by: Lans Zhang <jia.zhang@windriver.com>
    Cc: <andi@firstfloor.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1386303510-18574-1-git-send-email-jia.zhang@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 24aec58d6afd..c85da7bb6b60 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -211,9 +211,13 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	 */
 	nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
 	if (!nd_pa) {
-		pr_err("Cannot find %zu bytes in node %d\n",
-		       nd_size, nid);
-		return;
+		nd_pa = __memblock_alloc_base(nd_size, SMP_CACHE_BYTES,
+					      MEMBLOCK_ALLOC_ACCESSIBLE);
+		if (!nd_pa) {
+			pr_err("Cannot find %zu bytes in node %d\n",
+			       nd_size, nid);
+			return;
+		}
 	}
 	nd = __va(nd_pa);
 

commit c5320926e370b4cfb8f10c2169e26f960079cf67
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Nov 12 15:08:10 2013 -0800

    mem-hotplug: introduce movable_node boot option
    
    The hot-Pluggable field in SRAT specifies which memory is hotpluggable.
    As we mentioned before, if hotpluggable memory is used by the kernel, it
    cannot be hot-removed.  So memory hotplug users may want to set all
    hotpluggable memory in ZONE_MOVABLE so that the kernel won't use it.
    
    Memory hotplug users may also set a node as movable node, which has
    ZONE_MOVABLE only, so that the whole node can be hot-removed.
    
    But the kernel cannot use memory in ZONE_MOVABLE.  By doing this, the
    kernel cannot use memory in movable nodes.  This will cause NUMA
    performance down.  And other users may be unhappy.
    
    So we need a way to allow users to enable and disable this functionality.
    In this patch, we introduce movable_node boot option to allow users to
    choose to not to consume hotpluggable memory at early boot time and later
    we can set it as ZONE_MOVABLE.
    
    To achieve this, the movable_node boot option will control the memblock
    allocation direction.  That said, after memblock is ready, before SRAT is
    parsed, we should allocate memory near the kernel image as we explained in
    the previous patches.  So if movable_node boot option is set, the kernel
    does the following:
    
    1. After memblock is ready, make memblock allocate memory bottom up.
    2. After SRAT is parsed, make memblock behave as default, allocate memory
       top down.
    
    Users can specify "movable_node" in kernel commandline to enable this
    functionality.  For those who don't use memory hotplug or who don't want
    to lose their NUMA performance, just don't specify anything.  The kernel
    will work as before.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Suggested-by: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Thomas Renninger <trenn@suse.de>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 8bf93bae1f13..24aec58d6afd 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -567,6 +567,17 @@ static int __init numa_init(int (*init_func)(void))
 	ret = init_func();
 	if (ret < 0)
 		return ret;
+
+	/*
+	 * We reset memblock back to the top-down direction
+	 * here because if we configured ACPI_NUMA, we have
+	 * parsed SRAT in init_func(). It is ok to have the
+	 * reset here even if we did't configure ACPI_NUMA
+	 * or acpi numa init fails and fallbacks to dummy
+	 * numa init.
+	 */
+	memblock_set_bottom_up(false);
+
 	ret = numa_cleanup_meminfo(&numa_meminfo);
 	if (ret < 0)
 		return ret;

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index a71c4e207679..8bf93bae1f13 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -60,7 +60,7 @@ s16 __apicid_to_node[MAX_LOCAL_APIC] = {
 	[0 ... MAX_LOCAL_APIC-1] = NUMA_NO_NODE
 };
 
-int __cpuinit numa_cpu_node(int cpu)
+int numa_cpu_node(int cpu)
 {
 	int apicid = early_per_cpu(x86_cpu_to_apicid, cpu);
 
@@ -691,12 +691,12 @@ void __init init_cpu_to_node(void)
 #ifndef CONFIG_DEBUG_PER_CPU_MAPS
 
 # ifndef CONFIG_NUMA_EMU
-void __cpuinit numa_add_cpu(int cpu)
+void numa_add_cpu(int cpu)
 {
 	cpumask_set_cpu(cpu, node_to_cpumask_map[early_cpu_to_node(cpu)]);
 }
 
-void __cpuinit numa_remove_cpu(int cpu)
+void numa_remove_cpu(int cpu)
 {
 	cpumask_clear_cpu(cpu, node_to_cpumask_map[early_cpu_to_node(cpu)]);
 }
@@ -763,17 +763,17 @@ void debug_cpumask_set_cpu(int cpu, int node, bool enable)
 }
 
 # ifndef CONFIG_NUMA_EMU
-static void __cpuinit numa_set_cpumask(int cpu, bool enable)
+static void numa_set_cpumask(int cpu, bool enable)
 {
 	debug_cpumask_set_cpu(cpu, early_cpu_to_node(cpu), enable);
 }
 
-void __cpuinit numa_add_cpu(int cpu)
+void numa_add_cpu(int cpu)
 {
 	numa_set_cpumask(cpu, true);
 }
 
-void __cpuinit numa_remove_cpu(int cpu)
+void numa_remove_cpu(int cpu)
 {
 	numa_set_cpumask(cpu, false);
 }

commit d2ad351e36e333c6e1a474b2c5d635828f53a173
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Mon Apr 29 15:08:02 2013 -0700

    x86/mm/numa: use setup_nr_node_ids() instead of opencoding.
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 72fe01e9e414..a71c4e207679 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -114,14 +114,11 @@ void numa_clear_node(int cpu)
  */
 void __init setup_node_to_cpumask_map(void)
 {
-	unsigned int node, num = 0;
+	unsigned int node;
 
 	/* setup nr_node_ids if not done yet */
-	if (nr_node_ids == MAX_NUMNODES) {
-		for_each_node_mask(node, node_possible_map)
-			num = node;
-		nr_node_ids = num + 1;
-	}
+	if (nr_node_ids == MAX_NUMNODES)
+		setup_nr_node_ids();
 
 	/* allocate the map */
 	for (node = 0; node < nr_node_ids; node++)

commit 20e6926dcbafa1b361f1c29d967688be14b6ca4b
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Mar 1 14:51:27 2013 -0800

    x86, ACPI, mm: Revert movablemem_map support
    
    Tim found:
    
      WARNING: at arch/x86/kernel/smpboot.c:324 topology_sane.isra.2+0x6f/0x80()
      Hardware name: S2600CP
      sched: CPU #1's llc-sibling CPU #0 is not on the same node! [node: 1 != 0]. Ignoring dependency.
      smpboot: Booting Node   1, Processors  #1
      Modules linked in:
      Pid: 0, comm: swapper/1 Not tainted 3.9.0-0-generic #1
      Call Trace:
        set_cpu_sibling_map+0x279/0x449
        start_secondary+0x11d/0x1e5
    
    Don Morris reproduced on a HP z620 workstation, and bisected it to
    commit e8d195525809 ("acpi, memory-hotplug: parse SRAT before memblock
    is ready")
    
    It turns out movable_map has some problems, and it breaks several things
    
    1. numa_init is called several times, NOT just for srat. so those
            nodes_clear(numa_nodes_parsed)
            memset(&numa_meminfo, 0, sizeof(numa_meminfo))
       can not be just removed.  Need to consider sequence is: numaq, srat, amd, dummy.
       and make fall back path working.
    
    2. simply split acpi_numa_init to early_parse_srat.
       a. that early_parse_srat is NOT called for ia64, so you break ia64.
       b.  for (i = 0; i < MAX_LOCAL_APIC; i++)
                 set_apicid_to_node(i, NUMA_NO_NODE)
         still left in numa_init. So it will just clear result from early_parse_srat.
         it should be moved before that....
       c.  it breaks ACPI_TABLE_OVERIDE...as the acpi table scan is moved
           early before override from INITRD is settled.
    
    3. that patch TITLE is total misleading, there is NO x86 in the title,
       but it changes critical x86 code. It caused x86 guys did not
       pay attention to find the problem early. Those patches really should
       be routed via tip/x86/mm.
    
    4. after that commit, following range can not use movable ram:
      a. real_mode code.... well..funny, legacy Node0 [0,1M) could be hot-removed?
      b. initrd... it will be freed after booting, so it could be on movable...
      c. crashkernel for kdump...: looks like we can not put kdump kernel above 4G
            anymore.
      d. init_mem_mapping: can not put page table high anymore.
      e. initmem_init: vmemmap can not be high local node anymore. That is
         not good.
    
    If node is hotplugable, the mem related range like page table and
    vmemmap could be on the that node without problem and should be on that
    node.
    
    We have workaround patch that could fix some problems, but some can not
    be fixed.
    
    So just remove that offending commit and related ones including:
    
     f7210e6c4ac7 ("mm/memblock.c: use CONFIG_HAVE_MEMBLOCK_NODE_MAP to
        protect movablecore_map in memblock_overlaps_region().")
    
     01a178a94e8e ("acpi, memory-hotplug: support getting hotplug info from
        SRAT")
    
     27168d38fa20 ("acpi, memory-hotplug: extend movablemem_map ranges to
        the end of node")
    
     e8d195525809 ("acpi, memory-hotplug: parse SRAT before memblock is
        ready")
    
     fb06bc8e5f42 ("page_alloc: bootmem limit with movablecore_map")
    
     42f47e27e761 ("page_alloc: make movablemem_map have higher priority")
    
     6981ec31146c ("page_alloc: introduce zone_movable_limit[] to keep
        movable limit for nodes")
    
     34b71f1e04fc ("page_alloc: add movable_memmap kernel parameter")
    
     4d59a75125d5 ("x86: get pg_data_t's memory from other node")
    
    Later we should have patches that will make sure kernel put page table
    and vmemmap on local node ram instead of push them down to node0.  Also
    need to find way to put other kernel used ram to local node ram.
    
    Reported-by: Tim Gardner <tim.gardner@canonical.com>
    Reported-by: Don Morris <don.morris@hp.com>
    Bisected-by: Don Morris <don.morris@hp.com>
    Tested-by: Don Morris <don.morris@hp.com>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Thomas Renninger <trenn@suse.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index ff3633c794c6..72fe01e9e414 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -212,9 +212,10 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	 * Allocate node data.  Try node-local memory and then any node.
 	 * Never allocate in DMA zone.
 	 */
-	nd_pa = memblock_alloc_try_nid(nd_size, SMP_CACHE_BYTES, nid);
+	nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
 	if (!nd_pa) {
-		pr_err("Cannot find %zu bytes in any node\n", nd_size);
+		pr_err("Cannot find %zu bytes in node %d\n",
+		       nd_size, nid);
 		return;
 	}
 	nd = __va(nd_pa);
@@ -559,12 +560,10 @@ static int __init numa_init(int (*init_func)(void))
 	for (i = 0; i < MAX_LOCAL_APIC; i++)
 		set_apicid_to_node(i, NUMA_NO_NODE);
 
-	/*
-	 * Do not clear numa_nodes_parsed or zero numa_meminfo here, because
-	 * SRAT was parsed earlier in early_parse_srat().
-	 */
+	nodes_clear(numa_nodes_parsed);
 	nodes_clear(node_possible_map);
 	nodes_clear(node_online_map);
+	memset(&numa_meminfo, 0, sizeof(numa_meminfo));
 	WARN_ON(memblock_set_node(0, ULLONG_MAX, MAX_NUMNODES));
 	numa_reset_distance();
 

commit 2003cd90c473f66d34114bc61c49e7d74d370894
Merge: 24e55910e480 a8aed3e0752b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 26 19:45:29 2013 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar.
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm/pageattr: Prevent PSE and GLOABL leftovers to confuse pmd/pte_present and pmd_huge
      Revert "x86, mm: Make spurious_fault check explicitly check explicitly check the PRESENT bit"
      x86/mm/numa: Don't check if node is NUMA_NO_NODE
      x86, efi: Make "noefi" really disable EFI runtime serivces
      x86/apic: Fix parsing of the 'lapic' cmdline option

commit 942670d0dc41b5fe9b735c31ca9234d80729bf7e
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Fri Feb 22 15:11:47 2013 -0800

    x86/mm/numa: Don't check if node is NUMA_NO_NODE
    
    If we aren't debugging per_cpu maps, the cpu's node is stored in
    per_cpu variable numa_node.  If `node' is NUMA_NO_NODE, it means
    the caller wants to clear the cpu's node.  So we should also
    call set_cpu_numa_node() in this case.
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 2d125be1bae9..21d02f0d7a2c 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -97,8 +97,7 @@ void __cpuinit numa_set_node(int cpu, int node)
 #endif
 	per_cpu(x86_cpu_to_node_map, cpu) = node;
 
-	if (node != NUMA_NO_NODE)
-		set_cpu_numa_node(cpu, node);
+	set_cpu_numa_node(cpu, node);
 }
 
 void __cpuinit numa_clear_node(int cpu)

commit e8d1955258091e4c92d5a975ebd7fd8a98f5d30f
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Fri Feb 22 16:33:44 2013 -0800

    acpi, memory-hotplug: parse SRAT before memblock is ready
    
    On linux, the pages used by kernel could not be migrated.  As a result,
    if a memory range is used by kernel, it cannot be hot-removed.  So if we
    want to hot-remove memory, we should prevent kernel from using it.
    
    The way now used to prevent this is specify a memory range by
    movablemem_map boot option and set it as ZONE_MOVABLE.
    
    But when the system is booting, memblock will allocate memory, and
    reserve the memory for kernel.  And before we parse SRAT, and know the
    node memory ranges, memblock is working.  And it may allocate memory in
    ranges to be set as ZONE_MOVABLE.  This memory can be used by kernel,
    and never be freed.
    
    So, let's parse SRAT before memblock is called first.  And it is early
    enough.
    
    The first call of memblock_find_in_range_node() is in:
    
      setup_arch()
        |-->setup_real_mode()
    
    so, this patch add a function early_parse_srat() to parse SRAT, and call
    it before setup_real_mode() is called.
    
    NOTE:
    
    1) early_parse_srat() is called before numa_init(), and has initialized
       numa_meminfo.  So DO NOT clear numa_nodes_parsed in numa_init() and DO
       NOT zero numa_meminfo in numa_init(), otherwise we will lose memory
       numa info.
    
    2) I don't know why using count of memory affinities parsed from SRAT
       as a return value in original acpi_numa_init().  So I add a static
       variable srat_mem_cnt to remember this count and use it as the return
       value of the new acpi_numa_init()
    
    [mhocko@suse.cz: parse SRAT before memblock is ready fix]
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Wu Jianguo <wujianguo@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: "Brown, Len" <len.brown@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index e3963f52aaea..dfd30259eb89 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -560,10 +560,12 @@ static int __init numa_init(int (*init_func)(void))
 	for (i = 0; i < MAX_LOCAL_APIC; i++)
 		set_apicid_to_node(i, NUMA_NO_NODE);
 
-	nodes_clear(numa_nodes_parsed);
+	/*
+	 * Do not clear numa_nodes_parsed or zero numa_meminfo here, because
+	 * SRAT was parsed earlier in early_parse_srat().
+	 */
 	nodes_clear(node_possible_map);
 	nodes_clear(node_online_map);
-	memset(&numa_meminfo, 0, sizeof(numa_meminfo));
 	WARN_ON(memblock_set_node(0, ULLONG_MAX, MAX_NUMNODES));
 	numa_reset_distance();
 

commit 4d59a75125d5a4717e57e9fc62c64b3d346e603e
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Fri Feb 22 16:33:35 2013 -0800

    x86: get pg_data_t's memory from other node
    
    During the implementation of SRAT support, we met a problem.  In
    setup_arch(), we have the following call series:
    
     1) memblock is ready;
     2) some functions use memblock to allocate memory;
     3) parse ACPI tables, such as SRAT.
    
    Before 3), we don't know which memory is hotpluggable, and as a result,
    we cannot prevent memblock from allocating hotpluggable memory.  So, in
    2), there could be some hotpluggable memory allocated by memblock.
    
    Now, we are trying to parse SRAT earlier, before memblock is ready.  But
    I think we need more investigation on this topic.  So in this v5, I
    dropped all the SRAT support, and v5 is just the same as v3, and it is
    based on 3.8-rc3.
    
    As we planned, we will support getting info from SRAT without users'
    participation at last.  And we will post another patch-set to do so.
    
    And also, I think for now, we can add this boot option as the first step
    of supporting movable node.  Since Linux cannot migrate the direct
    mapped pages, the only way for now is to limit the whole node containing
    only movable memory.
    
    Using SRAT is one way.  But even if we can use SRAT, users still need an
    interface to enable/disable this functionality if they don't want to
    loose their NUMA performance.  So I think, a user interface is always
    needed.
    
    For now, users can disable this functionality by not specifying the boot
    option.  Later, we will post SRAT support, and add another option value
    "movablecore_map=acpi" to using SRAT.
    
    This patch:
    
    If system can create movable node which all memory of the node is
    allocated as ZONE_MOVABLE, setup_node_data() cannot allocate memory for
    the node's pg_data_t.  So, use memblock_alloc_try_nid() instead of
    memblock_alloc_nid() to retry when the first allocation fails.
    
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Wu Jianguo <wujianguo@huawei.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index a713d081b59c..e3963f52aaea 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -213,10 +213,9 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	 * Allocate node data.  Try node-local memory and then any node.
 	 * Never allocate in DMA zone.
 	 */
-	nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
+	nd_pa = memblock_alloc_try_nid(nd_size, SMP_CACHE_BYTES, nid);
 	if (!nd_pa) {
-		pr_err("Cannot find %zu bytes in node %d\n",
-		       nd_size, nid);
+		pr_err("Cannot find %zu bytes in any node\n", nd_size);
 		return;
 	}
 	nd = __va(nd_pa);

commit e13fe8695c57fed678877a9f3f8e99fc637ff4fb
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Fri Feb 22 16:33:31 2013 -0800

    cpu-hotplug,memory-hotplug: clear cpu_to_node() when offlining the node
    
    When the node is offlined, there is no memory/cpu on the node.  If a
    sleep task runs on a cpu of this node, it will be migrated to the cpu on
    the other node.  So we can clear cpu-to-node mapping.
    
    [akpm@linux-foundation.org: numa_clear_node() and numa_set_node() can no longer be __cpuinit]
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index f22680bf69ec..a713d081b59c 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -78,7 +78,7 @@ EXPORT_SYMBOL(node_to_cpumask_map);
 DEFINE_EARLY_PER_CPU(int, x86_cpu_to_node_map, NUMA_NO_NODE);
 EXPORT_EARLY_PER_CPU_SYMBOL(x86_cpu_to_node_map);
 
-void __cpuinit numa_set_node(int cpu, int node)
+void numa_set_node(int cpu, int node)
 {
 	int *cpu_to_node_map = early_per_cpu_ptr(x86_cpu_to_node_map);
 
@@ -101,7 +101,7 @@ void __cpuinit numa_set_node(int cpu, int node)
 		set_cpu_numa_node(cpu, node);
 }
 
-void __cpuinit numa_clear_node(int cpu)
+void numa_clear_node(int cpu)
 {
 	numa_set_node(cpu, NUMA_NO_NODE);
 }

commit c4c605246452d0e578945ea95a8e72877e97e8c6
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Fri Feb 22 16:33:24 2013 -0800

    cpu_hotplug: clear apicid to node when the cpu is hotremoved
    
    When a cpu is hotpluged, we call acpi_map_cpu2node() in
    _acpi_map_lsapic() to store the cpu's node and apicid's node.  But we
    don't clear the cpu's node in acpi_unmap_lsapic() when this cpu is
    hotremoved.  If the node is also hotremoved, we will get the following
    messages:
    
      kernel BUG at include/linux/gfp.h:329!
      invalid opcode: 0000 [#1] SMP
      Modules linked in: ebtable_nat ebtables ipt_MASQUERADE iptable_nat nf_nat xt_CHECKSUM iptable_mangle bridge stp llc sunrpc ipt_REJECT nf_conntrack_ipv4 nf_defrag_ipv4 iptable_filter ip_tables ip6t_REJECT nf_conntrack_ipv6 nf_defrag_ipv6 xt_state nf_conntrack ip6table_filter ip6_tables binfmt_misc dm_mirror dm_region_hash dm_log dm_mod vhost_net macvtap macvlan tun uinput iTCO_wdt iTCO_vendor_support coretemp kvm_intel kvm crc32c_intel microcode pcspkr i2c_i801 i2c_core lpc_ich mfd_core ioatdma e1000e i7core_edac edac_core sg acpi_memhotplug igb dca sd_mod crc_t10dif megaraid_sas mptsas mptscsih mptbase scsi_transport_sas scsi_mod
      Pid: 3126, comm: init Not tainted 3.6.0-rc3-tangchen-hostbridge+ #13 FUJITSU-SV PRIMEQUEST 1800E/SB
      RIP: 0010:[<ffffffff811bc3fd>]  [<ffffffff811bc3fd>] allocate_slab+0x28d/0x300
      RSP: 0018:ffff88078a049cf8  EFLAGS: 00010246
      RAX: 0000000000000000 RBX: 0000000000000001 RCX: 0000000000000000
      RDX: 0000000000000001 RSI: 0000000000000001 RDI: 0000000000000246
      RBP: ffff88078a049d38 R08: 00000000000040d0 R09: 0000000000000001
      R10: 0000000000000000 R11: 0000000000000b5f R12: 00000000000052d0
      R13: ffff8807c1417300 R14: 0000000000030038 R15: 0000000000000003
      FS:  00007fa9b1b44700(0000) GS:ffff8807c3800000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
      CR2: 00007fa9b09acca0 CR3: 000000078b855000 CR4: 00000000000007e0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
      Process init (pid: 3126, threadinfo ffff88078a048000, task ffff8807bb6f2650)
      Call Trace:
        new_slab+0x30/0x1b0
        __slab_alloc+0x358/0x4c0
        kmem_cache_alloc_node_trace+0xb4/0x1e0
        alloc_fair_sched_group+0xd0/0x1b0
        sched_create_group+0x3e/0x110
        sched_autogroup_create_attach+0x4d/0x180
        sys_setsid+0xd4/0xf0
        system_call_fastpath+0x16/0x1b
      Code: 89 c4 e9 73 fe ff ff 31 c0 89 de 48 c7 c7 45 de 9e 81 44 89 45 c8 e8 22 05 4b 00 85 db 44 8b 45 c8 0f 89 4f ff ff ff 0f 0b eb fe <0f> 0b 90 eb fd 0f 0b eb fe 89 de 48 c7 c7 45 de 9e 81 31 c0 44
      RIP  [<ffffffff811bc3fd>] allocate_slab+0x28d/0x300
       RSP <ffff88078a049cf8>
      ---[ end trace adf84c90f3fea3e5 ]---
    
    The reason is that the cpu's node is not NUMA_NO_NODE, we will call
    alloc_pages_exact_node() to alloc memory on the node, but the node is
    offlined.
    
    If the node is onlined, we still need cpu's node.  For example: a task
    on the cpu is sleeped when the cpu is hotremoved.  We will choose
    another cpu to run this task when it is waked up.  If we know the cpu's
    node, we will choose the cpu on the same node first.  So we should clear
    cpu-to-node mapping when the node is offlined.
    
    This patch only clears apicid-to-node mapping when the cpu is
    hotremoved.
    
    [akpm@linux-foundation.org: fix section error]
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 8504f3698753..f22680bf69ec 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -56,7 +56,7 @@ early_param("numa", numa_setup);
 /*
  * apicid, cpu, node mappings
  */
-s16 __apicid_to_node[MAX_LOCAL_APIC] __cpuinitdata = {
+s16 __apicid_to_node[MAX_LOCAL_APIC] = {
 	[0 ... MAX_LOCAL_APIC-1] = NUMA_NO_NODE
 };
 

commit 07f4207a305c834f528d08428df4531744e25678
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Jan 31 14:00:48 2013 -0800

    x86-32, mm: Remove reference to alloc_remap()
    
    We have removed the remap allocator for x86-32, and x86-64 never had
    it (and doesn't need it).  Remove residual reference to it.
    
    Reported-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org>
    Link: http://lkml.kernel.org/r/CAE9FiQVn6_QZi3fNQ-JHYiR-7jeDJ5hT0SyT_%2BzVvfOj=PzF3w@mail.gmail.com

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 61c2b6f5ff88..8504f3698753 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -193,7 +193,6 @@ int __init numa_add_memblk(int nid, u64 start, u64 end)
 static void __init setup_node_data(int nid, u64 start, u64 end)
 {
 	const size_t nd_size = roundup(sizeof(pg_data_t), PAGE_SIZE);
-	bool remapped = false;
 	u64 nd_pa;
 	void *nd;
 	int tnid;
@@ -211,28 +210,22 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	       nid, start, end - 1);
 
 	/*
-	 * Allocate node data.  Try remap allocator first, node-local
-	 * memory and then any node.  Never allocate in DMA zone.
+	 * Allocate node data.  Try node-local memory and then any node.
+	 * Never allocate in DMA zone.
 	 */
-	nd = alloc_remap(nid, nd_size);
-	if (nd) {
-		nd_pa = __pa_nodebug(nd);
-		remapped = true;
-	} else {
-		nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
-		if (!nd_pa) {
-			pr_err("Cannot find %zu bytes in node %d\n",
-			       nd_size, nid);
-			return;
-		}
-		nd = __va(nd_pa);
+	nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
+	if (!nd_pa) {
+		pr_err("Cannot find %zu bytes in node %d\n",
+		       nd_size, nid);
+		return;
 	}
+	nd = __va(nd_pa);
 
 	/* report and initialize */
-	printk(KERN_INFO "  NODE_DATA [mem %#010Lx-%#010Lx]%s\n",
-	       nd_pa, nd_pa + nd_size - 1, remapped ? " (remapped)" : "");
+	printk(KERN_INFO "  NODE_DATA [mem %#010Lx-%#010Lx]\n",
+	       nd_pa, nd_pa + nd_size - 1);
 	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
-	if (!remapped && tnid != nid)
+	if (tnid != nid)
 		printk(KERN_INFO "    NODE_DATA(%d) on node %d\n", nid, tnid);
 
 	node_data[nid] = nd;

commit f03574f2d5b2d6229dcdf2d322848065f72953c7
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Wed Jan 30 16:56:16 2013 -0800

    x86-32, mm: Rip out x86_32 NUMA remapping code
    
    This code was an optimization for 32-bit NUMA systems.
    
    It has probably been the cause of a number of subtle bugs over
    the years, although the conditions to excite them would have
    been hard to trigger.  Essentially, we remap part of the kernel
    linear mapping area, and then sometimes part of that area gets
    freed back in to the bootmem allocator.  If those pages get
    used by kernel data structures (say mem_map[] or a dentry),
    there's no big deal.  But, if anyone ever tried to use the
    linear mapping for these pages _and_ cared about their physical
    address, bad things happen.
    
    For instance, say you passed __GFP_ZERO to the page allocator
    and then happened to get handed one of these pages, it zero the
    remapped page, but it would make a pte to the _old_ page.
    There are probably a hundred other ways that it could screw
    with things.
    
    We don't need to hang on to performance optimizations for
    these old boxes any more.  All my 32-bit NUMA systems are long
    dead and buried, and I probably had access to more than most
    people.
    
    This code is causing real things to break today:
    
            https://lkml.org/lkml/2013/1/9/376
    
    I looked in to actually fixing this, but it requires surgery
    to way too much brittle code, as well as stuff like
    per_cpu_ptr_to_phys().
    
    [ hpa: Cc: this for -stable, since it is a memory corruption issue.
      However, an alternative is to simply mark NUMA as depends BROKEN
      rather than EXPERIMENTAL in the X86_32 subclause... ]
    
    Link: http://lkml.kernel.org/r/20130131005616.1C79F411@kernel.stglabs.ibm.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index b2313c6739f5..61c2b6f5ff88 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -205,9 +205,6 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	if (end && (end - start) < NODE_MIN_SIZE)
 		return;
 
-	/* initialize remap allocator before aligning to ZONE_ALIGN */
-	init_alloc_remap(nid, start, end);
-
 	start = roundup(start, ZONE_ALIGN);
 
 	printk(KERN_INFO "Initmem setup node %d [mem %#010Lx-%#010Lx]\n",

commit 1e9209edc71b851d81f0316ca03a0e6335c0ef9a
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Jan 27 01:18:21 2013 +0100

    x86/numa: Use __pa_nodebug() instead
    
    ... and fix the following warning:
    
      arch/x86/mm/numa.c: In function setup_node_data:
      arch/x86/mm/numa.c:222:3: warning: passing argument 1 of __phys_addr_nodebug makes integer from pointer without a cast
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1359245901-8512-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 76604eb9e4b0..b2313c6739f5 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -219,7 +219,7 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	 */
 	nd = alloc_remap(nid, nd_size);
 	if (nd) {
-		nd_pa = __phys_addr_nodebug(nd);
+		nd_pa = __pa_nodebug(nd);
 		remapped = true;
 	} else {
 		nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);

commit a25b9316841c5afa226f8f70a457861b35276a92
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Tue Jan 22 13:24:30 2013 -0800

    x86, mm: Make DEBUG_VIRTUAL work earlier in boot
    
    The KVM code has some repeated bugs in it around use of __pa() on
    per-cpu data.  Those data are not in an area on which using
    __pa() is valid.  However, they are also called early enough in
    boot that __vmalloc_start_set is not set, and thus the
    CONFIG_DEBUG_VIRTUAL debugging does not catch them.
    
    This adds a check to also verify __pa() calls against max_low_pfn,
    which we can use earler in boot than is_vmalloc_addr().  However,
    if we are super-early in boot, max_low_pfn=0 and this will trip
    on every call, so also make sure that max_low_pfn is set before
    we try to use it.
    
    With this patch applied, CONFIG_DEBUG_VIRTUAL will actually
    catch the bug I was chasing (and fix later in this series).
    
    I'd love to find a generic way so that any __pa() call on percpu
    areas could do a BUG_ON(), but there don't appear to be any nice
    and easy ways to check if an address is a percpu one.  Anybody
    have ideas on a way to do this?
    
    Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20130122212430.F46F8159@kernel.stglabs.ibm.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 2d125be1bae9..76604eb9e4b0 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -219,7 +219,7 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	 */
 	nd = alloc_remap(nid, nd_size);
 	if (nd) {
-		nd_pa = __pa(nd);
+		nd_pa = __phys_addr_nodebug(nd);
 		remapped = true;
 	} else {
 		nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);

commit 365811d6f9bd98543bedc02b72d94f0f0faf3670
Author: Bjorn Helgaas <bhelgaas@google.com>
Date:   Tue May 29 15:06:29 2012 -0700

    x86: print physical addresses consistently with other parts of kernel
    
    Print physical address info in a style consistent with the %pR style used
    elsewhere in the kernel.  For example:
    
        -found SMP MP-table at [ffff8800000fce90] fce90
        +found SMP MP-table at [mem 0x000fce90-0x000fce9f] mapped at [ffff8800000fce90]
        -initial memory mapped : 0 - 20000000
        +initial memory mapped: [mem 0x00000000-0x1fffffff]
        -Base memory trampoline at [ffff88000009c000] 9c000 size 8192
        +Base memory trampoline [mem 0x0009c000-0x0009dfff] mapped at [ffff88000009c000]
        -SRAT: Node 0 PXM 0 0-80000000
        +SRAT: Node 0 PXM 0 [mem 0x00000000-0x7fffffff]
    
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 19d3fa08b119..2d125be1bae9 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -141,8 +141,8 @@ static int __init numa_add_memblk_to(int nid, u64 start, u64 end,
 
 	/* whine about and ignore invalid blks */
 	if (start > end || nid < 0 || nid >= MAX_NUMNODES) {
-		pr_warning("NUMA: Warning: invalid memblk node %d (%Lx-%Lx)\n",
-			   nid, start, end);
+		pr_warning("NUMA: Warning: invalid memblk node %d [mem %#010Lx-%#010Lx]\n",
+			   nid, start, end - 1);
 		return 0;
 	}
 
@@ -210,8 +210,8 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 
 	start = roundup(start, ZONE_ALIGN);
 
-	printk(KERN_INFO "Initmem setup node %d %016Lx-%016Lx\n",
-	       nid, start, end);
+	printk(KERN_INFO "Initmem setup node %d [mem %#010Lx-%#010Lx]\n",
+	       nid, start, end - 1);
 
 	/*
 	 * Allocate node data.  Try remap allocator first, node-local
@@ -232,7 +232,7 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	}
 
 	/* report and initialize */
-	printk(KERN_INFO "  NODE_DATA [%016Lx - %016Lx]%s\n",
+	printk(KERN_INFO "  NODE_DATA [mem %#010Lx-%#010Lx]%s\n",
 	       nd_pa, nd_pa + nd_size - 1, remapped ? " (remapped)" : "");
 	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
 	if (!remapped && tnid != nid)
@@ -291,14 +291,14 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 			 */
 			if (bi->end > bj->start && bi->start < bj->end) {
 				if (bi->nid != bj->nid) {
-					pr_err("NUMA: node %d (%Lx-%Lx) overlaps with node %d (%Lx-%Lx)\n",
-					       bi->nid, bi->start, bi->end,
-					       bj->nid, bj->start, bj->end);
+					pr_err("NUMA: node %d [mem %#010Lx-%#010Lx] overlaps with node %d [mem %#010Lx-%#010Lx]\n",
+					       bi->nid, bi->start, bi->end - 1,
+					       bj->nid, bj->start, bj->end - 1);
 					return -EINVAL;
 				}
-				pr_warning("NUMA: Warning: node %d (%Lx-%Lx) overlaps with itself (%Lx-%Lx)\n",
-					   bi->nid, bi->start, bi->end,
-					   bj->start, bj->end);
+				pr_warning("NUMA: Warning: node %d [mem %#010Lx-%#010Lx] overlaps with itself [mem %#010Lx-%#010Lx]\n",
+					   bi->nid, bi->start, bi->end - 1,
+					   bj->start, bj->end - 1);
 			}
 
 			/*
@@ -320,9 +320,9 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 			}
 			if (k < mi->nr_blks)
 				continue;
-			printk(KERN_INFO "NUMA: Node %d [%Lx,%Lx) + [%Lx,%Lx) -> [%Lx,%Lx)\n",
-			       bi->nid, bi->start, bi->end, bj->start, bj->end,
-			       start, end);
+			printk(KERN_INFO "NUMA: Node %d [mem %#010Lx-%#010Lx] + [mem %#010Lx-%#010Lx] -> [mem %#010Lx-%#010Lx]\n",
+			       bi->nid, bi->start, bi->end - 1, bj->start,
+			       bj->end - 1, start, end - 1);
 			bi->start = start;
 			bi->end = end;
 			numa_remove_memblk_from(j--, mi);
@@ -616,8 +616,8 @@ static int __init dummy_numa_init(void)
 {
 	printk(KERN_INFO "%s\n",
 	       numa_off ? "NUMA turned off" : "No NUMA configuration found");
-	printk(KERN_INFO "Faking a node at %016Lx-%016Lx\n",
-	       0LLU, PFN_PHYS(max_pfn));
+	printk(KERN_INFO "Faking a node at [mem %#018Lx-%#018Lx]\n",
+	       0LLU, PFN_PHYS(max_pfn) - 1);
 
 	node_set(0, numa_nodes_parsed);
 	numa_add_memblk(0, 0, PFN_PHYS(max_pfn));

commit 9512938b885304f72c847379611d6018064af840
Author: Wanlong Gao <gaowanlong@cn.fujitsu.com>
Date:   Thu Jan 12 17:20:09 2012 -0800

    cpumask: update setup_node_to_cpumask_map() comments
    
    node_to_cpumask() has been replaced by cpumask_of_node(), and wholly
    removed since commit 29c337a0 ("cpumask: remove obsolete node_to_cpumask
    now everyone uses cpumask_of_node").
    
    So update the comments for setup_node_to_cpumask_map().
    
    Signed-off-by: Wanlong Gao <gaowanlong@cn.fujitsu.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 020cd2e80873..19d3fa08b119 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -110,7 +110,7 @@ void __cpuinit numa_clear_node(int cpu)
  * Allocate node_to_cpumask_map based on number of available nodes
  * Requires node_possible_map to be valid.
  *
- * Note: node_to_cpumask() is not valid until after this is done.
+ * Note: cpumask_of_node() is not valid until after this is done.
  * (Use CONFIG_DEBUG_PER_CPU_MAPS to check this.)
  */
 void __init setup_node_to_cpumask_map(void)

commit d0b9706c20ebb4ba181dc26e52ac9a6861abf425
Merge: 02d929502ce7 54eed6cb16ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 11 19:12:10 2012 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/numa: Add constraints check for nid parameters
      mm, x86: Remove debug_pagealloc_enabled
      x86/mm: Initialize high mem before free_all_bootmem()
      arch/x86/kernel/e820.c: quiet sparse noise about plain integer as NULL pointer
      arch/x86/kernel/e820.c: Eliminate bubble sort from sanitize_e820_map()
      x86: Fix mmap random address range
      x86, mm: Unify zone_sizes_init()
      x86, mm: Prepare zone_sizes_init() for unification
      x86, mm: Use max_low_pfn for ZONE_NORMAL on 64-bit
      x86, mm: Wrap ZONE_DMA32 with CONFIG_ZONE_DMA32
      x86, mm: Use max_pfn instead of highend_pfn
      x86, mm: Move zone init from paging_init() on 64-bit
      x86, mm: Use MAX_DMA_PFN for ZONE_DMA on 32-bit

commit 54eed6cb16ec315565aaaf8e34252ca253a68b7b
Author: Petr Holasek <pholasek@redhat.com>
Date:   Thu Dec 8 13:16:41 2011 +0100

    x86/numa: Add constraints check for nid parameters
    
    This patch adds constraint checks to the numa_set_distance()
    function.
    
    When the check triggers (this should not happen normally) it
    emits a warning and avoids a store to a negative index in
    numa_distance[] array - i.e. avoids memory corruption.
    
    Negative ids can be passed when the pxm-to-nids mapping is not
    properly filled while parsing the SRAT.
    
    Signed-off-by: Petr Holasek <pholasek@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Anton Arapov <anton@redhat.com>
    Link: http://lkml.kernel.org/r/20111208121640.GA2229@dhcp-27-244.brq.redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index fbeaaf416610..cdc00543d375 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -430,8 +430,9 @@ static int __init numa_alloc_distance(void)
  * calls are ignored until the distance table is reset with
  * numa_reset_distance().
  *
- * If @from or @to is higher than the highest known node at the time of
- * table creation or @distance doesn't make sense, the call is ignored.
+ * If @from or @to is higher than the highest known node or lower than zero
+ * at the time of table creation or @distance doesn't make sense, the call
+ * is ignored.
  * This is to allow simplification of specific NUMA config implementations.
  */
 void __init numa_set_distance(int from, int to, int distance)
@@ -439,8 +440,9 @@ void __init numa_set_distance(int from, int to, int distance)
 	if (!numa_distance && numa_alloc_distance() < 0)
 		return;
 
-	if (from >= numa_distance_cnt || to >= numa_distance_cnt) {
-		printk_once(KERN_DEBUG "NUMA: Debug: distance out of bound, from=%d to=%d distance=%d\n",
+	if (from >= numa_distance_cnt || to >= numa_distance_cnt ||
+			from < 0 || to < 0) {
+		pr_warn_once("NUMA: Warning: node ids are out of bound, from=%d to=%d distance=%d\n",
 			    from, to, distance);
 		return;
 	}

commit 24aa07882b672fff2da2f5c955759f0bd13d32d5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 12 11:16:06 2011 +0200

    memblock, x86: Replace memblock_x86_reserve/free_range() with generic ones
    
    Other than sanity check and debug message, the x86 specific version of
    memblock reserve/free functions are simple wrappers around the generic
    versions - memblock_reserve/free().
    
    This patch adds debug messages with caller identification to the
    generic versions and replaces x86 specific ones and kills them.
    arch/x86/include/asm/memblock.h and arch/x86/mm/memblock.c are empty
    after this change and removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1310462166-31469-14-git-send-email-tj@kernel.org
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 88e562729967..496f494593bf 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -364,8 +364,7 @@ void __init numa_reset_distance(void)
 
 	/* numa_distance could be 1LU marking allocation failure, test cnt */
 	if (numa_distance_cnt)
-		memblock_x86_free_range(__pa(numa_distance),
-					__pa(numa_distance) + size);
+		memblock_free(__pa(numa_distance), size);
 	numa_distance_cnt = 0;
 	numa_distance = NULL;	/* enable table creation */
 }
@@ -394,7 +393,7 @@ static int __init numa_alloc_distance(void)
 		numa_distance = (void *)1LU;
 		return -ENOMEM;
 	}
-	memblock_x86_reserve_range(phys, phys + size, "NUMA DIST");
+	memblock_reserve(phys, size);
 
 	numa_distance = __va(phys);
 	numa_distance_cnt = cnt;

commit 474b881bf4ee86aba55d46a4fdf293de32cba91b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 12 11:16:04 2011 +0200

    x86: Use absent_pages_in_range() instead of memblock_x86_hole_size()
    
    memblock_x86_hole_size() calculates the total size of holes in a given
    range according to memblock and is used by numa emulation code and
    numa_meminfo_cover_memory().
    
    Since conversion to MEMBLOCK_NODE_MAP, absent_pages_in_range() also
    uses memblock and gives the same result.  This patch replaces
    memblock_x86_hole_size() uses with absent_pages_in_range().  After the
    conversion the x86 function doesn't have any user left and is killed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1310462166-31469-12-git-send-email-tj@kernel.org
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index f4a40bdb2e4e..88e562729967 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -475,8 +475,8 @@ static bool __init numa_meminfo_cover_memory(const struct numa_meminfo *mi)
 			numaram = 0;
 	}
 
-	e820ram = max_pfn - (memblock_x86_hole_size(0,
-					PFN_PHYS(max_pfn)) >> PAGE_SHIFT);
+	e820ram = max_pfn - absent_pages_in_range(0, max_pfn);
+
 	/* We seem to lose 3 pages somewhere. Allow 1M of slack. */
 	if ((s64)(e820ram - numaram) >= (1 << (20 - PAGE_SHIFT))) {
 		printk(KERN_ERR "NUMA: nodes only cover %LuMB of your %LuMB e820 RAM. Not used.\n",

commit 0608f70c78a384c2f225f2de226ca057a196f108
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 14 11:44:23 2011 +0200

    x86: Use HAVE_MEMBLOCK_NODE_MAP
    
    From 5732e1247898d67cbf837585150fe9f68974671d Mon Sep 17 00:00:00 2001
    From: Tejun Heo <tj@kernel.org>
    Date: Thu, 14 Jul 2011 11:22:16 +0200
    
    Convert x86 to HAVE_MEMBLOCK_NODE_MAP.  The only difference in memory
    handling is that allocations can't no longer cross node boundaries
    whether they're node affine or not, which shouldn't matter at all.
    
    This conversion will enable further simplification of boot memory
    handling.
    
    -v2: Fix build failure on !NUMA configurations discovered by hpa.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/20110714094423.GG3455@htj.dyndns.org
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 824efadc5741..f4a40bdb2e4e 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -498,13 +498,10 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 	if (WARN_ON(nodes_empty(node_possible_map)))
 		return -EINVAL;
 
-	for (i = 0; i < mi->nr_blks; i++)
-		memblock_x86_register_active_regions(mi->blk[i].nid,
-					mi->blk[i].start >> PAGE_SHIFT,
-					mi->blk[i].end >> PAGE_SHIFT);
-
-	/* for out of order entries */
-	sort_node_map();
+	for (i = 0; i < mi->nr_blks; i++) {
+		struct numa_memblk *mb = &mi->blk[i];
+		memblock_set_node(mb->start, mb->end - mb->start, mb->nid);
+	}
 
 	/*
 	 * If sections array is gonna be used for pfn -> nid mapping, check
@@ -538,6 +535,8 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 			setup_node_data(nid, start, end);
 	}
 
+	/* Dump memblock with node info and return. */
+	memblock_dump_all();
 	return 0;
 }
 
@@ -575,7 +574,7 @@ static int __init numa_init(int (*init_func)(void))
 	nodes_clear(node_possible_map);
 	nodes_clear(node_online_map);
 	memset(&numa_meminfo, 0, sizeof(numa_meminfo));
-	remove_all_active_ranges();
+	WARN_ON(memblock_set_node(0, ULLONG_MAX, MAX_NUMNODES));
 	numa_reset_distance();
 
 	ret = init_func();

commit eb40c4c27f1722f058e4713ccfedebac577d5190
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 12 10:46:35 2011 +0200

    memblock, x86: Replace memblock_x86_find_in_range_node() with generic memblock calls
    
    With the previous changes, generic NUMA aware memblock API has feature
    parity with memblock_x86_find_in_range_node().  There currently are
    two users - x86 setup_node_data() and __alloc_memory_core_early() in
    nobootmem.c.
    
    This patch converts the former to use memblock_alloc_nid() and the
    latter memblock_find_range_in_node(), and kills
    memblock_x86_find_in_range_node() and related functions including
    find_memory_early_core_early() in page_alloc.c.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1310460395-30913-9-git-send-email-tj@kernel.org
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index fa1015de5cc0..824efadc5741 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -192,8 +192,6 @@ int __init numa_add_memblk(int nid, u64 start, u64 end)
 /* Initialize NODE_DATA for a node on the local memory */
 static void __init setup_node_data(int nid, u64 start, u64 end)
 {
-	const u64 nd_low = PFN_PHYS(MAX_DMA_PFN);
-	const u64 nd_high = PFN_PHYS(max_pfn_mapped);
 	const size_t nd_size = roundup(sizeof(pg_data_t), PAGE_SIZE);
 	bool remapped = false;
 	u64 nd_pa;
@@ -224,17 +222,12 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 		nd_pa = __pa(nd);
 		remapped = true;
 	} else {
-		nd_pa = memblock_x86_find_in_range_node(nid, nd_low, nd_high,
-						nd_size, SMP_CACHE_BYTES);
-		if (!nd_pa)
-			nd_pa = memblock_find_in_range(nd_low, nd_high,
-						nd_size, SMP_CACHE_BYTES);
+		nd_pa = memblock_alloc_nid(nd_size, SMP_CACHE_BYTES, nid);
 		if (!nd_pa) {
 			pr_err("Cannot find %zu bytes in node %d\n",
 			       nd_size, nid);
 			return;
 		}
-		memblock_x86_reserve_range(nd_pa, nd_pa + nd_size, "NODE_DATA");
 		nd = __va(nd_pa);
 	}
 

commit 1f5026a7e21e409c2b9dd54f6dfb9446511fb7c5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 12 09:58:09 2011 +0200

    memblock: Kill MEMBLOCK_ERROR
    
    25818f0f28 (memblock: Make MEMBLOCK_ERROR be 0) thankfully made
    MEMBLOCK_ERROR 0 and there already are codes which expect error return
    to be 0.  There's no point in keeping MEMBLOCK_ERROR around.  End its
    misery.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1310457490-3356-6-git-send-email-tj@kernel.org
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index fbeaaf416610..fa1015de5cc0 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -226,10 +226,10 @@ static void __init setup_node_data(int nid, u64 start, u64 end)
 	} else {
 		nd_pa = memblock_x86_find_in_range_node(nid, nd_low, nd_high,
 						nd_size, SMP_CACHE_BYTES);
-		if (nd_pa == MEMBLOCK_ERROR)
+		if (!nd_pa)
 			nd_pa = memblock_find_in_range(nd_low, nd_high,
 						nd_size, SMP_CACHE_BYTES);
-		if (nd_pa == MEMBLOCK_ERROR) {
+		if (!nd_pa) {
 			pr_err("Cannot find %zu bytes in node %d\n",
 			       nd_size, nid);
 			return;
@@ -395,7 +395,7 @@ static int __init numa_alloc_distance(void)
 
 	phys = memblock_find_in_range(0, PFN_PHYS(max_pfn_mapped),
 				      size, PAGE_SIZE);
-	if (phys == MEMBLOCK_ERROR) {
+	if (!phys) {
 		pr_warning("NUMA: Warning: can't allocate distance table!\n");
 		/* don't retry until explicitly reset */
 		numa_distance = (void *)1LU;

commit 1e01979c8f502ac13e3cdece4f38712c5944e6e8
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 12 09:45:34 2011 +0200

    x86, numa: Implement pfn -> nid mapping granularity check
    
    SPARSEMEM w/o VMEMMAP and DISCONTIGMEM, both used only on 32bit, use
    sections array to map pfn to nid which is limited in granularity.  If
    NUMA nodes are laid out such that the mapping cannot be accurate, boot
    will fail triggering BUG_ON() in mminit_verify_page_links().
    
    On 32bit, it's 512MiB w/ PAE and SPARSEMEM.  This seems to have been
    granular enough until commit 2706a0bf7b (x86, NUMA: Enable
    CONFIG_AMD_NUMA on 32bit too).  Apparently, there is a machine which
    aligns NUMA nodes to 128MiB and has only AMD NUMA but not SRAT.  This
    led to the following BUG_ON().
    
     On node 0 totalpages: 2096615
       DMA zone: 32 pages used for memmap
       DMA zone: 0 pages reserved
       DMA zone: 3927 pages, LIFO batch:0
       Normal zone: 1740 pages used for memmap
       Normal zone: 220978 pages, LIFO batch:31
       HighMem zone: 16405 pages used for memmap
       HighMem zone: 1853533 pages, LIFO batch:31
     BUG: Int 6: CR2   (null)
          EDI   (null)  ESI 00000002  EBP 00000002  ESP c1543ecc
          EBX f2400000  EDX 00000006  ECX   (null)  EAX 00000001
          err   (null)  EIP c16209aa   CS 00000060  flg 00010002
     Stack: f2400000 00220000 f7200800 c1620613 00220000 01000000 04400000 00238000
              (null) f7200000 00000002 f7200b58 f7200800 c1620929 000375fe   (null)
            f7200b80 c16395f0 00200a02 f7200a80   (null) 000375fe 00000002   (null)
     Pid: 0, comm: swapper Not tainted 2.6.39-rc5-00181-g2706a0b #17
     Call Trace:
      [<c136b1e5>] ? early_fault+0x2e/0x2e
      [<c16209aa>] ? mminit_verify_page_links+0x12/0x42
      [<c1620613>] ? memmap_init_zone+0xaf/0x10c
      [<c1620929>] ? free_area_init_node+0x2b9/0x2e3
      [<c1607e99>] ? free_area_init_nodes+0x3f2/0x451
      [<c1601d80>] ? paging_init+0x112/0x118
      [<c15f578d>] ? setup_arch+0x791/0x82f
      [<c15f43d9>] ? start_kernel+0x6a/0x257
    
    This patch implements node_map_pfn_alignment() which determines
    maximum internode alignment and update numa_register_memblks() to
    reject NUMA configuration if alignment exceeds the pfn -> nid mapping
    granularity of the memory model as determined by PAGES_PER_SECTION.
    
    This makes the problematic machine boot w/ flatmem by rejecting the
    NUMA config and provides protection against crazy NUMA configurations.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/20110712074534.GB2872@htj.dyndns.org
    LKML-Reference: <20110628174613.GP478@escobedo.osrc.amd.com>
    Reported-and-Tested-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    Cc: Conny Seidel <conny.seidel@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index f5510d889a22..fbeaaf416610 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -496,6 +496,7 @@ static bool __init numa_meminfo_cover_memory(const struct numa_meminfo *mi)
 
 static int __init numa_register_memblks(struct numa_meminfo *mi)
 {
+	unsigned long uninitialized_var(pfn_align);
 	int i, nid;
 
 	/* Account for nodes with cpus and no memory */
@@ -511,6 +512,20 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 
 	/* for out of order entries */
 	sort_node_map();
+
+	/*
+	 * If sections array is gonna be used for pfn -> nid mapping, check
+	 * whether its granularity is fine enough.
+	 */
+#ifdef NODE_NOT_IN_PAGE_FLAGS
+	pfn_align = node_map_pfn_alignment();
+	if (pfn_align && pfn_align < PAGES_PER_SECTION) {
+		printk(KERN_WARNING "Node alignment %LuMB < min %LuMB, rejecting NUMA config\n",
+		       PFN_PHYS(pfn_align) >> 20,
+		       PFN_PHYS(PAGES_PER_SECTION) >> 20);
+		return -EINVAL;
+	}
+#endif
 	if (!numa_meminfo_cover_memory(mi))
 		return -EINVAL;
 

commit e5a10c1bd12a5d71bbb6406c1b0dbbc9d8958397
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon May 2 17:24:49 2011 +0200

    x86, NUMA: Trim numa meminfo with max_pfn in a separate loop
    
    During testing 32bit numa unifying code from tj, found one system with
    more than 64g fails to use numa.  It turns out we do not trim numa
    meminfo correctly against max_pfn in case start address of a node is
    higher than 64GiB.  Bug fix made it to tip tree.
    
    This patch moves the checking and trimming to a separate loop.  So we
    don't need to compare low/high in following merge loops.  It makes the
    code more readable.
    
    Also it makes the node merge printouts less strange.  On a 512GiB numa
    system with 32bit,
    
    before:
    > NUMA: Node 0 [0,a0000) + [100000,80000000) -> [0,80000000)
    > NUMA: Node 0 [0,80000000) + [100000000,1080000000) -> [0,1000000000)
    
    after:
    > NUMA: Node 0 [0,a0000) + [100000,80000000) -> [0,80000000)
    > NUMA: Node 0 [0,80000000) + [100000000,1000000000) -> [0,1000000000)
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    [Updated patch description and comment slightly.]
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 9a0ed312b830..f5510d889a22 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -270,6 +270,7 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 	const u64 high = PFN_PHYS(max_pfn);
 	int i, j, k;
 
+	/* first, trim all entries */
 	for (i = 0; i < mi->nr_blks; i++) {
 		struct numa_memblk *bi = &mi->blk[i];
 
@@ -278,10 +279,13 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 		bi->end = min(bi->end, high);
 
 		/* and there's no empty block */
-		if (bi->start >= bi->end) {
+		if (bi->start >= bi->end)
 			numa_remove_memblk_from(i--, mi);
-			continue;
-		}
+	}
+
+	/* merge neighboring / overlapping entries */
+	for (i = 0; i < mi->nr_blks; i++) {
+		struct numa_memblk *bi = &mi->blk[i];
 
 		for (j = i + 1; j < mi->nr_blks; j++) {
 			struct numa_memblk *bj = &mi->blk[j];
@@ -311,8 +315,8 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 			 */
 			if (bi->nid != bj->nid)
 				continue;
-			start = max(min(bi->start, bj->start), low);
-			end = min(max(bi->end, bj->end), high);
+			start = min(bi->start, bj->start);
+			end = max(bi->end, bj->end);
 			for (k = 0; k < mi->nr_blks; k++) {
 				struct numa_memblk *bk = &mi->blk[k];
 
@@ -332,6 +336,7 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 		}
 	}
 
+	/* clear unused ones */
 	for (i = mi->nr_blks; i < ARRAY_SIZE(mi->blk); i++) {
 		mi->blk[i].start = mi->blk[i].end = 0;
 		mi->blk[i].nid = NUMA_NO_NODE;

commit a56bca80db8903bb557b9ac38da68dc5b98ea672
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon May 2 17:24:49 2011 +0200

    x86, NUMA: Rename setup_node_bootmem() to setup_node_data()
    
    After using memblock to replace bootmem, that function only sets up
    node_data now.
    
    Change the name to reflect what it actually does.
    
    tj: Minor adjustment to the patch description.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index ecb5685d7d20..9a0ed312b830 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -189,8 +189,8 @@ int __init numa_add_memblk(int nid, u64 start, u64 end)
 	return numa_add_memblk_to(nid, start, end, &numa_meminfo);
 }
 
-/* Initialize bootmem allocator for a node */
-static void __init setup_node_bootmem(int nid, u64 start, u64 end)
+/* Initialize NODE_DATA for a node on the local memory */
+static void __init setup_node_data(int nid, u64 start, u64 end)
 {
 	const u64 nd_low = PFN_PHYS(MAX_DMA_PFN);
 	const u64 nd_high = PFN_PHYS(max_pfn_mapped);
@@ -522,7 +522,7 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 		}
 
 		if (start < end)
-			setup_node_bootmem(nid, start, end);
+			setup_node_data(nid, start, end);
 	}
 
 	return 0;

commit 752d4f372f90a2f6eb562aaffb639957890cbcab
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 17:24:48 2011 +0200

    x86, NUMA: Make numa_init_array() static
    
    numa_init_array() no longer has users outside of numa.c.  Make it
    static.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 56ed714ed24f..ecb5685d7d20 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -535,7 +535,7 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
  * as the number of CPUs is not known yet. We round robin the existing
  * nodes.
  */
-void __init numa_init_array(void)
+static void __init numa_init_array(void)
 {
 	int rr, i;
 

commit bd6709a91a593d8fe35d08da542e9f93bb74a304
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 17:24:48 2011 +0200

    x86, NUMA: Make 32bit use common NUMA init path
    
    With both _numa_init() methods converted and the rest of init code
    adjusted, numa_32.c now can switch from the 32bit only init code to
    the common one in numa.c.
    
    * Shim get_memcfg_*()'s are dropped and initmem_init() calls
      x86_numa_init(), which is updated to handle NUMAQ.
    
    * All boilerplate operations including node range limiting, pgdat
      alloc/init are handled by numa_init().  32bit only implementation is
      removed.
    
    * 32bit numa_add_memblk(), numa_set_distance() and
      memory_add_physaddr_to_nid() removed and common versions in
      numa_32.c enabled for 32bit.
    
    This change causes the following behavior changes.
    
    * NODE_DATA()->node_start_pfn/node_spanned_pages properly initialized
      for 32bit too.
    
    * Much more sanity checks and configuration cleanups.
    
    * Proper handling of node distances.
    
    * The same NUMA init messages as 64bit.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index a72317ae74c5..56ed714ed24f 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -173,7 +173,6 @@ void __init numa_remove_memblk_from(int idx, struct numa_meminfo *mi)
 		(mi->nr_blks - idx) * sizeof(mi->blk[0]));
 }
 
-#ifdef CONFIG_X86_64
 /**
  * numa_add_memblk - Add one numa_memblk to numa_meminfo
  * @nid: NUMA node ID of the new memblk
@@ -189,7 +188,6 @@ int __init numa_add_memblk(int nid, u64 start, u64 end)
 {
 	return numa_add_memblk_to(nid, start, end, &numa_meminfo);
 }
-#endif
 
 /* Initialize bootmem allocator for a node */
 static void __init setup_node_bootmem(int nid, u64 start, u64 end)
@@ -413,7 +411,6 @@ static int __init numa_alloc_distance(void)
 	return 0;
 }
 
-#ifdef CONFIG_X86_64
 /**
  * numa_set_distance - Set NUMA distance from one NUMA to another
  * @from: the 'from' node to set distance
@@ -452,7 +449,6 @@ void __init numa_set_distance(int from, int to, int distance)
 
 	numa_distance[from * numa_distance_cnt + to] = distance;
 }
-#endif
 
 int __node_distance(int from, int to)
 {
@@ -626,6 +622,10 @@ static int __init dummy_numa_init(void)
 void __init x86_numa_init(void)
 {
 	if (!numa_off) {
+#ifdef CONFIG_X86_NUMAQ
+		if (!numa_init(numaq_numa_init))
+			return;
+#endif
 #ifdef CONFIG_ACPI_NUMA
 		if (!numa_init(x86_acpi_numa_init))
 			return;
@@ -805,7 +805,7 @@ EXPORT_SYMBOL(cpumask_of_node);
 
 #endif	/* !CONFIG_DEBUG_PER_CPU_MAPS */
 
-#if defined(CONFIG_X86_64) && defined(CONFIG_MEMORY_HOTPLUG)
+#ifdef CONFIG_MEMORY_HOTPLUG
 int memory_add_physaddr_to_nid(u64 start)
 {
 	struct numa_meminfo *mi = &numa_meminfo;

commit 7888e96b264fad27f97f58c0f3a4d20326eaf181
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 14:18:54 2011 +0200

    x86, NUMA: Initialize and use remap allocator from setup_node_bootmem()
    
    setup_node_bootmem() is taken from 64bit and doesn't use remap
    allocator.  It's about to be shared with 32bit so add support for it.
    If NODE_DATA is remapped, it's noted in the debug message and node
    locality check is skipped as the __pa() of the remapped address
    doesn't reflect the actual physical address.
    
    On 64bit, remap allocator becomes noop and doesn't affect the
    behavior.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index b45caa39f7cf..a72317ae74c5 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -197,7 +197,9 @@ static void __init setup_node_bootmem(int nid, u64 start, u64 end)
 	const u64 nd_low = PFN_PHYS(MAX_DMA_PFN);
 	const u64 nd_high = PFN_PHYS(max_pfn_mapped);
 	const size_t nd_size = roundup(sizeof(pg_data_t), PAGE_SIZE);
+	bool remapped = false;
 	u64 nd_pa;
+	void *nd;
 	int tnid;
 
 	/*
@@ -207,34 +209,45 @@ static void __init setup_node_bootmem(int nid, u64 start, u64 end)
 	if (end && (end - start) < NODE_MIN_SIZE)
 		return;
 
+	/* initialize remap allocator before aligning to ZONE_ALIGN */
+	init_alloc_remap(nid, start, end);
+
 	start = roundup(start, ZONE_ALIGN);
 
 	printk(KERN_INFO "Initmem setup node %d %016Lx-%016Lx\n",
 	       nid, start, end);
 
 	/*
-	 * Try to allocate node data on local node and then fall back to
-	 * all nodes.  Never allocate in DMA zone.
+	 * Allocate node data.  Try remap allocator first, node-local
+	 * memory and then any node.  Never allocate in DMA zone.
 	 */
-	nd_pa = memblock_x86_find_in_range_node(nid, nd_low, nd_high,
+	nd = alloc_remap(nid, nd_size);
+	if (nd) {
+		nd_pa = __pa(nd);
+		remapped = true;
+	} else {
+		nd_pa = memblock_x86_find_in_range_node(nid, nd_low, nd_high,
 						nd_size, SMP_CACHE_BYTES);
-	if (nd_pa == MEMBLOCK_ERROR)
-		nd_pa = memblock_find_in_range(nd_low, nd_high,
-					       nd_size, SMP_CACHE_BYTES);
-	if (nd_pa == MEMBLOCK_ERROR) {
-		pr_err("Cannot find %zu bytes in node %d\n", nd_size, nid);
-		return;
+		if (nd_pa == MEMBLOCK_ERROR)
+			nd_pa = memblock_find_in_range(nd_low, nd_high,
+						nd_size, SMP_CACHE_BYTES);
+		if (nd_pa == MEMBLOCK_ERROR) {
+			pr_err("Cannot find %zu bytes in node %d\n",
+			       nd_size, nid);
+			return;
+		}
+		memblock_x86_reserve_range(nd_pa, nd_pa + nd_size, "NODE_DATA");
+		nd = __va(nd_pa);
 	}
-	memblock_x86_reserve_range(nd_pa, nd_pa + nd_size, "NODE_DATA");
 
 	/* report and initialize */
-	printk(KERN_INFO "  NODE_DATA [%016Lx - %016Lx]\n",
-	       nd_pa, nd_pa + nd_size - 1);
+	printk(KERN_INFO "  NODE_DATA [%016Lx - %016Lx]%s\n",
+	       nd_pa, nd_pa + nd_size - 1, remapped ? " (remapped)" : "");
 	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
-	if (tnid != nid)
+	if (!remapped && tnid != nid)
 		printk(KERN_INFO "    NODE_DATA(%d) on node %d\n", nid, tnid);
 
-	node_data[nid] = __va(nd_pa);
+	node_data[nid] = nd;
 	memset(NODE_DATA(nid), 0, sizeof(pg_data_t));
 	NODE_DATA(nid)->node_id = nid;
 	NODE_DATA(nid)->node_start_pfn = start >> PAGE_SHIFT;

commit 38f3e1ca24cc3ec416855e02676f91c898a8a262
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 14:18:53 2011 +0200

    x86, NUMA: Remove long 64bit assumption from numa.c
    
    Code moved from numa_64.c has assumption that long is 64bit in several
    places.  This patch removes the assumption by using {s|u}64_t
    explicity, using PFN_PHYS() for page number -> addr conversions and
    adjusting printf formats.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index c400f3b2b93e..b45caa39f7cf 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -192,13 +192,12 @@ int __init numa_add_memblk(int nid, u64 start, u64 end)
 #endif
 
 /* Initialize bootmem allocator for a node */
-static void __init
-setup_node_bootmem(int nid, unsigned long start, unsigned long end)
+static void __init setup_node_bootmem(int nid, u64 start, u64 end)
 {
-	const u64 nd_low = (u64)MAX_DMA_PFN << PAGE_SHIFT;
-	const u64 nd_high = (u64)max_pfn_mapped << PAGE_SHIFT;
+	const u64 nd_low = PFN_PHYS(MAX_DMA_PFN);
+	const u64 nd_high = PFN_PHYS(max_pfn_mapped);
 	const size_t nd_size = roundup(sizeof(pg_data_t), PAGE_SIZE);
-	unsigned long nd_pa;
+	u64 nd_pa;
 	int tnid;
 
 	/*
@@ -210,7 +209,7 @@ setup_node_bootmem(int nid, unsigned long start, unsigned long end)
 
 	start = roundup(start, ZONE_ALIGN);
 
-	printk(KERN_INFO "Initmem setup node %d %016lx-%016lx\n",
+	printk(KERN_INFO "Initmem setup node %d %016Lx-%016Lx\n",
 	       nid, start, end);
 
 	/*
@@ -223,13 +222,13 @@ setup_node_bootmem(int nid, unsigned long start, unsigned long end)
 		nd_pa = memblock_find_in_range(nd_low, nd_high,
 					       nd_size, SMP_CACHE_BYTES);
 	if (nd_pa == MEMBLOCK_ERROR) {
-		pr_err("Cannot find %lu bytes in node %d\n", nd_size, nid);
+		pr_err("Cannot find %zu bytes in node %d\n", nd_size, nid);
 		return;
 	}
 	memblock_x86_reserve_range(nd_pa, nd_pa + nd_size, "NODE_DATA");
 
 	/* report and initialize */
-	printk(KERN_INFO "  NODE_DATA [%016lx - %016lx]\n",
+	printk(KERN_INFO "  NODE_DATA [%016Lx - %016Lx]\n",
 	       nd_pa, nd_pa + nd_size - 1);
 	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
 	if (tnid != nid)
@@ -257,7 +256,7 @@ setup_node_bootmem(int nid, unsigned long start, unsigned long end)
 int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 {
 	const u64 low = 0;
-	const u64 high = (u64)max_pfn << PAGE_SHIFT;
+	const u64 high = PFN_PHYS(max_pfn);
 	int i, j, k;
 
 	for (i = 0; i < mi->nr_blks; i++) {
@@ -275,7 +274,7 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 
 		for (j = i + 1; j < mi->nr_blks; j++) {
 			struct numa_memblk *bj = &mi->blk[j];
-			unsigned long start, end;
+			u64 start, end;
 
 			/*
 			 * See whether there are overlapping blocks.  Whine
@@ -313,7 +312,7 @@ int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
 			}
 			if (k < mi->nr_blks)
 				continue;
-			printk(KERN_INFO "NUMA: Node %d [%Lx,%Lx) + [%Lx,%Lx) -> [%lx,%lx)\n",
+			printk(KERN_INFO "NUMA: Node %d [%Lx,%Lx) + [%Lx,%Lx) -> [%Lx,%Lx)\n",
 			       bi->nid, bi->start, bi->end, bj->start, bj->end,
 			       start, end);
 			bi->start = start;
@@ -378,7 +377,7 @@ static int __init numa_alloc_distance(void)
 	cnt++;
 	size = cnt * cnt * sizeof(numa_distance[0]);
 
-	phys = memblock_find_in_range(0, (u64)max_pfn_mapped << PAGE_SHIFT,
+	phys = memblock_find_in_range(0, PFN_PHYS(max_pfn_mapped),
 				      size, PAGE_SIZE);
 	if (phys == MEMBLOCK_ERROR) {
 		pr_warning("NUMA: Warning: can't allocate distance table!\n");
@@ -456,24 +455,24 @@ EXPORT_SYMBOL(__node_distance);
  */
 static bool __init numa_meminfo_cover_memory(const struct numa_meminfo *mi)
 {
-	unsigned long numaram, e820ram;
+	u64 numaram, e820ram;
 	int i;
 
 	numaram = 0;
 	for (i = 0; i < mi->nr_blks; i++) {
-		unsigned long s = mi->blk[i].start >> PAGE_SHIFT;
-		unsigned long e = mi->blk[i].end >> PAGE_SHIFT;
+		u64 s = mi->blk[i].start >> PAGE_SHIFT;
+		u64 e = mi->blk[i].end >> PAGE_SHIFT;
 		numaram += e - s;
 		numaram -= __absent_pages_in_range(mi->blk[i].nid, s, e);
-		if ((long)numaram < 0)
+		if ((s64)numaram < 0)
 			numaram = 0;
 	}
 
 	e820ram = max_pfn - (memblock_x86_hole_size(0,
-					max_pfn << PAGE_SHIFT) >> PAGE_SHIFT);
+					PFN_PHYS(max_pfn)) >> PAGE_SHIFT);
 	/* We seem to lose 3 pages somewhere. Allow 1M of slack. */
-	if ((long)(e820ram - numaram) >= (1 << (20 - PAGE_SHIFT))) {
-		printk(KERN_ERR "NUMA: nodes only cover %luMB of your %luMB e820 RAM. Not used.\n",
+	if ((s64)(e820ram - numaram) >= (1 << (20 - PAGE_SHIFT))) {
+		printk(KERN_ERR "NUMA: nodes only cover %LuMB of your %LuMB e820 RAM. Not used.\n",
 		       (numaram << PAGE_SHIFT) >> 20,
 		       (e820ram << PAGE_SHIFT) >> 20);
 		return false;
@@ -503,7 +502,7 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 
 	/* Finally register nodes. */
 	for_each_node_mask(nid, node_possible_map) {
-		u64 start = (u64)max_pfn << PAGE_SHIFT;
+		u64 start = PFN_PHYS(max_pfn);
 		u64 end = 0;
 
 		for (i = 0; i < mi->nr_blks; i++) {
@@ -595,11 +594,11 @@ static int __init dummy_numa_init(void)
 {
 	printk(KERN_INFO "%s\n",
 	       numa_off ? "NUMA turned off" : "No NUMA configuration found");
-	printk(KERN_INFO "Faking a node at %016lx-%016lx\n",
-	       0LU, max_pfn << PAGE_SHIFT);
+	printk(KERN_INFO "Faking a node at %016Lx-%016Lx\n",
+	       0LLU, PFN_PHYS(max_pfn));
 
 	node_set(0, numa_nodes_parsed);
-	numa_add_memblk(0, 0, (u64)max_pfn << PAGE_SHIFT);
+	numa_add_memblk(0, 0, PFN_PHYS(max_pfn));
 
 	return 0;
 }

commit 744baba0c4072b04664952a89292e4708eaf949a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 14:18:53 2011 +0200

    x86, NUMA: Enable build of generic NUMA init code on 32bit
    
    Generic NUMA init code was moved to numa.c from numa_64.c but is still
    guaraded by CONFIG_X86_64.  This patch removes the compile guard and
    enables compiling on 32bit.
    
    * numa_add_memblk() and numa_set_distance() clash with the shim
      implementation in numa_32.c and are left out.
    
    * memory_add_physaddr_to_nid() clashes with 32bit implementation and
      is left out.
    
    * MAX_DMA_PFN definition in dma.h moved out of !CONFIG_X86_32.
    
    * node_data definition in numa_32.c removed in favor of the one in
      numa.c.
    
    There are places where ulong is assumed to be 64bit.  The next patch
    will fix them up.  Note that although the code is compiled it isn't
    used yet and this patch doesn't cause any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index ed1daba54906..c400f3b2b93e 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -23,7 +23,6 @@
 int __initdata numa_off;
 nodemask_t numa_nodes_parsed __initdata;
 
-#ifdef CONFIG_X86_64
 struct pglist_data *node_data[MAX_NUMNODES] __read_mostly;
 EXPORT_SYMBOL(node_data);
 
@@ -35,7 +34,6 @@ __initdata
 
 static int numa_distance_cnt;
 static u8 *numa_distance;
-#endif
 
 static __init int numa_setup(char *opt)
 {
@@ -134,7 +132,6 @@ void __init setup_node_to_cpumask_map(void)
 	pr_debug("Node to cpumask map for %d nodes\n", nr_node_ids);
 }
 
-#ifdef CONFIG_X86_64
 static int __init numa_add_memblk_to(int nid, u64 start, u64 end,
 				     struct numa_meminfo *mi)
 {
@@ -176,6 +173,7 @@ void __init numa_remove_memblk_from(int idx, struct numa_meminfo *mi)
 		(mi->nr_blks - idx) * sizeof(mi->blk[0]));
 }
 
+#ifdef CONFIG_X86_64
 /**
  * numa_add_memblk - Add one numa_memblk to numa_meminfo
  * @nid: NUMA node ID of the new memblk
@@ -191,6 +189,7 @@ int __init numa_add_memblk(int nid, u64 start, u64 end)
 {
 	return numa_add_memblk_to(nid, start, end, &numa_meminfo);
 }
+#endif
 
 /* Initialize bootmem allocator for a node */
 static void __init
@@ -402,6 +401,7 @@ static int __init numa_alloc_distance(void)
 	return 0;
 }
 
+#ifdef CONFIG_X86_64
 /**
  * numa_set_distance - Set NUMA distance from one NUMA to another
  * @from: the 'from' node to set distance
@@ -440,6 +440,7 @@ void __init numa_set_distance(int from, int to, int distance)
 
 	numa_distance[from * numa_distance_cnt + to] = distance;
 }
+#endif
 
 int __node_distance(int from, int to)
 {
@@ -518,7 +519,6 @@ static int __init numa_register_memblks(struct numa_meminfo *mi)
 
 	return 0;
 }
-#endif
 
 /*
  * There are unfortunately some poorly designed mainboards around that
@@ -542,7 +542,6 @@ void __init numa_init_array(void)
 	}
 }
 
-#ifdef CONFIG_X86_64
 static int __init numa_init(int (*init_func)(void))
 {
 	int i;
@@ -627,7 +626,6 @@ void __init x86_numa_init(void)
 
 	numa_init(dummy_numa_init);
 }
-#endif
 
 static __init int find_near_online_node(int node)
 {

commit a4106eae650a4d5d30fcdd36d998edfa5ccb0ec4
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 14:18:53 2011 +0200

    x86, NUMA: Move NUMA init logic from numa_64.c to numa.c
    
    Move the generic 64bit NUMA init machinery from numa_64.c to numa.c.
    
    * node_data[], numa_mem_info and numa_distance
    * numa_add_memblk[_to](), numa_remove_memblk[_from]()
    * numa_set_distance() and friends
    * numa_init() and all the numa_meminfo handling helpers called from it
    * dummy_numa_init()
    * memory_add_physaddr_to_nid()
    
    A new function x86_numa_init() is added and the content of
    numa_64.c::initmem_init() is moved into it.  initmem_init() now simply
    calls x86_numa_init().
    
    Constants and numa_off declaration are moved from numa_{32|64}.h to
    numa.h.
    
    This is code reorganization and doesn't involve any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index cce174109ca9..ed1daba54906 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -1,13 +1,42 @@
 /* Common code for 32 and 64-bit NUMA */
-#include <linux/topology.h>
-#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/string.h>
+#include <linux/init.h>
 #include <linux/bootmem.h>
-#include <asm/numa.h>
+#include <linux/memblock.h>
+#include <linux/mmzone.h>
+#include <linux/ctype.h>
+#include <linux/module.h>
+#include <linux/nodemask.h>
+#include <linux/sched.h>
+#include <linux/topology.h>
+
+#include <asm/e820.h>
+#include <asm/proto.h>
+#include <asm/dma.h>
 #include <asm/acpi.h>
+#include <asm/amd_nb.h>
+
+#include "numa_internal.h"
 
 int __initdata numa_off;
 nodemask_t numa_nodes_parsed __initdata;
 
+#ifdef CONFIG_X86_64
+struct pglist_data *node_data[MAX_NUMNODES] __read_mostly;
+EXPORT_SYMBOL(node_data);
+
+static struct numa_meminfo numa_meminfo
+#ifndef CONFIG_MEMORY_HOTPLUG
+__initdata
+#endif
+;
+
+static int numa_distance_cnt;
+static u8 *numa_distance;
+#endif
+
 static __init int numa_setup(char *opt)
 {
 	if (!opt)
@@ -105,6 +134,392 @@ void __init setup_node_to_cpumask_map(void)
 	pr_debug("Node to cpumask map for %d nodes\n", nr_node_ids);
 }
 
+#ifdef CONFIG_X86_64
+static int __init numa_add_memblk_to(int nid, u64 start, u64 end,
+				     struct numa_meminfo *mi)
+{
+	/* ignore zero length blks */
+	if (start == end)
+		return 0;
+
+	/* whine about and ignore invalid blks */
+	if (start > end || nid < 0 || nid >= MAX_NUMNODES) {
+		pr_warning("NUMA: Warning: invalid memblk node %d (%Lx-%Lx)\n",
+			   nid, start, end);
+		return 0;
+	}
+
+	if (mi->nr_blks >= NR_NODE_MEMBLKS) {
+		pr_err("NUMA: too many memblk ranges\n");
+		return -EINVAL;
+	}
+
+	mi->blk[mi->nr_blks].start = start;
+	mi->blk[mi->nr_blks].end = end;
+	mi->blk[mi->nr_blks].nid = nid;
+	mi->nr_blks++;
+	return 0;
+}
+
+/**
+ * numa_remove_memblk_from - Remove one numa_memblk from a numa_meminfo
+ * @idx: Index of memblk to remove
+ * @mi: numa_meminfo to remove memblk from
+ *
+ * Remove @idx'th numa_memblk from @mi by shifting @mi->blk[] and
+ * decrementing @mi->nr_blks.
+ */
+void __init numa_remove_memblk_from(int idx, struct numa_meminfo *mi)
+{
+	mi->nr_blks--;
+	memmove(&mi->blk[idx], &mi->blk[idx + 1],
+		(mi->nr_blks - idx) * sizeof(mi->blk[0]));
+}
+
+/**
+ * numa_add_memblk - Add one numa_memblk to numa_meminfo
+ * @nid: NUMA node ID of the new memblk
+ * @start: Start address of the new memblk
+ * @end: End address of the new memblk
+ *
+ * Add a new memblk to the default numa_meminfo.
+ *
+ * RETURNS:
+ * 0 on success, -errno on failure.
+ */
+int __init numa_add_memblk(int nid, u64 start, u64 end)
+{
+	return numa_add_memblk_to(nid, start, end, &numa_meminfo);
+}
+
+/* Initialize bootmem allocator for a node */
+static void __init
+setup_node_bootmem(int nid, unsigned long start, unsigned long end)
+{
+	const u64 nd_low = (u64)MAX_DMA_PFN << PAGE_SHIFT;
+	const u64 nd_high = (u64)max_pfn_mapped << PAGE_SHIFT;
+	const size_t nd_size = roundup(sizeof(pg_data_t), PAGE_SIZE);
+	unsigned long nd_pa;
+	int tnid;
+
+	/*
+	 * Don't confuse VM with a node that doesn't have the
+	 * minimum amount of memory:
+	 */
+	if (end && (end - start) < NODE_MIN_SIZE)
+		return;
+
+	start = roundup(start, ZONE_ALIGN);
+
+	printk(KERN_INFO "Initmem setup node %d %016lx-%016lx\n",
+	       nid, start, end);
+
+	/*
+	 * Try to allocate node data on local node and then fall back to
+	 * all nodes.  Never allocate in DMA zone.
+	 */
+	nd_pa = memblock_x86_find_in_range_node(nid, nd_low, nd_high,
+						nd_size, SMP_CACHE_BYTES);
+	if (nd_pa == MEMBLOCK_ERROR)
+		nd_pa = memblock_find_in_range(nd_low, nd_high,
+					       nd_size, SMP_CACHE_BYTES);
+	if (nd_pa == MEMBLOCK_ERROR) {
+		pr_err("Cannot find %lu bytes in node %d\n", nd_size, nid);
+		return;
+	}
+	memblock_x86_reserve_range(nd_pa, nd_pa + nd_size, "NODE_DATA");
+
+	/* report and initialize */
+	printk(KERN_INFO "  NODE_DATA [%016lx - %016lx]\n",
+	       nd_pa, nd_pa + nd_size - 1);
+	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
+	if (tnid != nid)
+		printk(KERN_INFO "    NODE_DATA(%d) on node %d\n", nid, tnid);
+
+	node_data[nid] = __va(nd_pa);
+	memset(NODE_DATA(nid), 0, sizeof(pg_data_t));
+	NODE_DATA(nid)->node_id = nid;
+	NODE_DATA(nid)->node_start_pfn = start >> PAGE_SHIFT;
+	NODE_DATA(nid)->node_spanned_pages = (end - start) >> PAGE_SHIFT;
+
+	node_set_online(nid);
+}
+
+/**
+ * numa_cleanup_meminfo - Cleanup a numa_meminfo
+ * @mi: numa_meminfo to clean up
+ *
+ * Sanitize @mi by merging and removing unncessary memblks.  Also check for
+ * conflicts and clear unused memblks.
+ *
+ * RETURNS:
+ * 0 on success, -errno on failure.
+ */
+int __init numa_cleanup_meminfo(struct numa_meminfo *mi)
+{
+	const u64 low = 0;
+	const u64 high = (u64)max_pfn << PAGE_SHIFT;
+	int i, j, k;
+
+	for (i = 0; i < mi->nr_blks; i++) {
+		struct numa_memblk *bi = &mi->blk[i];
+
+		/* make sure all blocks are inside the limits */
+		bi->start = max(bi->start, low);
+		bi->end = min(bi->end, high);
+
+		/* and there's no empty block */
+		if (bi->start >= bi->end) {
+			numa_remove_memblk_from(i--, mi);
+			continue;
+		}
+
+		for (j = i + 1; j < mi->nr_blks; j++) {
+			struct numa_memblk *bj = &mi->blk[j];
+			unsigned long start, end;
+
+			/*
+			 * See whether there are overlapping blocks.  Whine
+			 * about but allow overlaps of the same nid.  They
+			 * will be merged below.
+			 */
+			if (bi->end > bj->start && bi->start < bj->end) {
+				if (bi->nid != bj->nid) {
+					pr_err("NUMA: node %d (%Lx-%Lx) overlaps with node %d (%Lx-%Lx)\n",
+					       bi->nid, bi->start, bi->end,
+					       bj->nid, bj->start, bj->end);
+					return -EINVAL;
+				}
+				pr_warning("NUMA: Warning: node %d (%Lx-%Lx) overlaps with itself (%Lx-%Lx)\n",
+					   bi->nid, bi->start, bi->end,
+					   bj->start, bj->end);
+			}
+
+			/*
+			 * Join together blocks on the same node, holes
+			 * between which don't overlap with memory on other
+			 * nodes.
+			 */
+			if (bi->nid != bj->nid)
+				continue;
+			start = max(min(bi->start, bj->start), low);
+			end = min(max(bi->end, bj->end), high);
+			for (k = 0; k < mi->nr_blks; k++) {
+				struct numa_memblk *bk = &mi->blk[k];
+
+				if (bi->nid == bk->nid)
+					continue;
+				if (start < bk->end && end > bk->start)
+					break;
+			}
+			if (k < mi->nr_blks)
+				continue;
+			printk(KERN_INFO "NUMA: Node %d [%Lx,%Lx) + [%Lx,%Lx) -> [%lx,%lx)\n",
+			       bi->nid, bi->start, bi->end, bj->start, bj->end,
+			       start, end);
+			bi->start = start;
+			bi->end = end;
+			numa_remove_memblk_from(j--, mi);
+		}
+	}
+
+	for (i = mi->nr_blks; i < ARRAY_SIZE(mi->blk); i++) {
+		mi->blk[i].start = mi->blk[i].end = 0;
+		mi->blk[i].nid = NUMA_NO_NODE;
+	}
+
+	return 0;
+}
+
+/*
+ * Set nodes, which have memory in @mi, in *@nodemask.
+ */
+static void __init numa_nodemask_from_meminfo(nodemask_t *nodemask,
+					      const struct numa_meminfo *mi)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(mi->blk); i++)
+		if (mi->blk[i].start != mi->blk[i].end &&
+		    mi->blk[i].nid != NUMA_NO_NODE)
+			node_set(mi->blk[i].nid, *nodemask);
+}
+
+/**
+ * numa_reset_distance - Reset NUMA distance table
+ *
+ * The current table is freed.  The next numa_set_distance() call will
+ * create a new one.
+ */
+void __init numa_reset_distance(void)
+{
+	size_t size = numa_distance_cnt * numa_distance_cnt * sizeof(numa_distance[0]);
+
+	/* numa_distance could be 1LU marking allocation failure, test cnt */
+	if (numa_distance_cnt)
+		memblock_x86_free_range(__pa(numa_distance),
+					__pa(numa_distance) + size);
+	numa_distance_cnt = 0;
+	numa_distance = NULL;	/* enable table creation */
+}
+
+static int __init numa_alloc_distance(void)
+{
+	nodemask_t nodes_parsed;
+	size_t size;
+	int i, j, cnt = 0;
+	u64 phys;
+
+	/* size the new table and allocate it */
+	nodes_parsed = numa_nodes_parsed;
+	numa_nodemask_from_meminfo(&nodes_parsed, &numa_meminfo);
+
+	for_each_node_mask(i, nodes_parsed)
+		cnt = i;
+	cnt++;
+	size = cnt * cnt * sizeof(numa_distance[0]);
+
+	phys = memblock_find_in_range(0, (u64)max_pfn_mapped << PAGE_SHIFT,
+				      size, PAGE_SIZE);
+	if (phys == MEMBLOCK_ERROR) {
+		pr_warning("NUMA: Warning: can't allocate distance table!\n");
+		/* don't retry until explicitly reset */
+		numa_distance = (void *)1LU;
+		return -ENOMEM;
+	}
+	memblock_x86_reserve_range(phys, phys + size, "NUMA DIST");
+
+	numa_distance = __va(phys);
+	numa_distance_cnt = cnt;
+
+	/* fill with the default distances */
+	for (i = 0; i < cnt; i++)
+		for (j = 0; j < cnt; j++)
+			numa_distance[i * cnt + j] = i == j ?
+				LOCAL_DISTANCE : REMOTE_DISTANCE;
+	printk(KERN_DEBUG "NUMA: Initialized distance table, cnt=%d\n", cnt);
+
+	return 0;
+}
+
+/**
+ * numa_set_distance - Set NUMA distance from one NUMA to another
+ * @from: the 'from' node to set distance
+ * @to: the 'to'  node to set distance
+ * @distance: NUMA distance
+ *
+ * Set the distance from node @from to @to to @distance.  If distance table
+ * doesn't exist, one which is large enough to accommodate all the currently
+ * known nodes will be created.
+ *
+ * If such table cannot be allocated, a warning is printed and further
+ * calls are ignored until the distance table is reset with
+ * numa_reset_distance().
+ *
+ * If @from or @to is higher than the highest known node at the time of
+ * table creation or @distance doesn't make sense, the call is ignored.
+ * This is to allow simplification of specific NUMA config implementations.
+ */
+void __init numa_set_distance(int from, int to, int distance)
+{
+	if (!numa_distance && numa_alloc_distance() < 0)
+		return;
+
+	if (from >= numa_distance_cnt || to >= numa_distance_cnt) {
+		printk_once(KERN_DEBUG "NUMA: Debug: distance out of bound, from=%d to=%d distance=%d\n",
+			    from, to, distance);
+		return;
+	}
+
+	if ((u8)distance != distance ||
+	    (from == to && distance != LOCAL_DISTANCE)) {
+		pr_warn_once("NUMA: Warning: invalid distance parameter, from=%d to=%d distance=%d\n",
+			     from, to, distance);
+		return;
+	}
+
+	numa_distance[from * numa_distance_cnt + to] = distance;
+}
+
+int __node_distance(int from, int to)
+{
+	if (from >= numa_distance_cnt || to >= numa_distance_cnt)
+		return from == to ? LOCAL_DISTANCE : REMOTE_DISTANCE;
+	return numa_distance[from * numa_distance_cnt + to];
+}
+EXPORT_SYMBOL(__node_distance);
+
+/*
+ * Sanity check to catch more bad NUMA configurations (they are amazingly
+ * common).  Make sure the nodes cover all memory.
+ */
+static bool __init numa_meminfo_cover_memory(const struct numa_meminfo *mi)
+{
+	unsigned long numaram, e820ram;
+	int i;
+
+	numaram = 0;
+	for (i = 0; i < mi->nr_blks; i++) {
+		unsigned long s = mi->blk[i].start >> PAGE_SHIFT;
+		unsigned long e = mi->blk[i].end >> PAGE_SHIFT;
+		numaram += e - s;
+		numaram -= __absent_pages_in_range(mi->blk[i].nid, s, e);
+		if ((long)numaram < 0)
+			numaram = 0;
+	}
+
+	e820ram = max_pfn - (memblock_x86_hole_size(0,
+					max_pfn << PAGE_SHIFT) >> PAGE_SHIFT);
+	/* We seem to lose 3 pages somewhere. Allow 1M of slack. */
+	if ((long)(e820ram - numaram) >= (1 << (20 - PAGE_SHIFT))) {
+		printk(KERN_ERR "NUMA: nodes only cover %luMB of your %luMB e820 RAM. Not used.\n",
+		       (numaram << PAGE_SHIFT) >> 20,
+		       (e820ram << PAGE_SHIFT) >> 20);
+		return false;
+	}
+	return true;
+}
+
+static int __init numa_register_memblks(struct numa_meminfo *mi)
+{
+	int i, nid;
+
+	/* Account for nodes with cpus and no memory */
+	node_possible_map = numa_nodes_parsed;
+	numa_nodemask_from_meminfo(&node_possible_map, mi);
+	if (WARN_ON(nodes_empty(node_possible_map)))
+		return -EINVAL;
+
+	for (i = 0; i < mi->nr_blks; i++)
+		memblock_x86_register_active_regions(mi->blk[i].nid,
+					mi->blk[i].start >> PAGE_SHIFT,
+					mi->blk[i].end >> PAGE_SHIFT);
+
+	/* for out of order entries */
+	sort_node_map();
+	if (!numa_meminfo_cover_memory(mi))
+		return -EINVAL;
+
+	/* Finally register nodes. */
+	for_each_node_mask(nid, node_possible_map) {
+		u64 start = (u64)max_pfn << PAGE_SHIFT;
+		u64 end = 0;
+
+		for (i = 0; i < mi->nr_blks; i++) {
+			if (nid != mi->blk[i].nid)
+				continue;
+			start = min(mi->blk[i].start, start);
+			end = max(mi->blk[i].end, end);
+		}
+
+		if (start < end)
+			setup_node_bootmem(nid, start, end);
+	}
+
+	return 0;
+}
+#endif
+
 /*
  * There are unfortunately some poorly designed mainboards around that
  * only connect memory to a single CPU. This breaks the 1:1 cpu->node
@@ -127,6 +542,93 @@ void __init numa_init_array(void)
 	}
 }
 
+#ifdef CONFIG_X86_64
+static int __init numa_init(int (*init_func)(void))
+{
+	int i;
+	int ret;
+
+	for (i = 0; i < MAX_LOCAL_APIC; i++)
+		set_apicid_to_node(i, NUMA_NO_NODE);
+
+	nodes_clear(numa_nodes_parsed);
+	nodes_clear(node_possible_map);
+	nodes_clear(node_online_map);
+	memset(&numa_meminfo, 0, sizeof(numa_meminfo));
+	remove_all_active_ranges();
+	numa_reset_distance();
+
+	ret = init_func();
+	if (ret < 0)
+		return ret;
+	ret = numa_cleanup_meminfo(&numa_meminfo);
+	if (ret < 0)
+		return ret;
+
+	numa_emulation(&numa_meminfo, numa_distance_cnt);
+
+	ret = numa_register_memblks(&numa_meminfo);
+	if (ret < 0)
+		return ret;
+
+	for (i = 0; i < nr_cpu_ids; i++) {
+		int nid = early_cpu_to_node(i);
+
+		if (nid == NUMA_NO_NODE)
+			continue;
+		if (!node_online(nid))
+			numa_clear_node(i);
+	}
+	numa_init_array();
+	return 0;
+}
+
+/**
+ * dummy_numa_init - Fallback dummy NUMA init
+ *
+ * Used if there's no underlying NUMA architecture, NUMA initialization
+ * fails, or NUMA is disabled on the command line.
+ *
+ * Must online at least one node and add memory blocks that cover all
+ * allowed memory.  This function must not fail.
+ */
+static int __init dummy_numa_init(void)
+{
+	printk(KERN_INFO "%s\n",
+	       numa_off ? "NUMA turned off" : "No NUMA configuration found");
+	printk(KERN_INFO "Faking a node at %016lx-%016lx\n",
+	       0LU, max_pfn << PAGE_SHIFT);
+
+	node_set(0, numa_nodes_parsed);
+	numa_add_memblk(0, 0, (u64)max_pfn << PAGE_SHIFT);
+
+	return 0;
+}
+
+/**
+ * x86_numa_init - Initialize NUMA
+ *
+ * Try each configured NUMA initialization method until one succeeds.  The
+ * last fallback is dummy single node config encomapssing whole memory and
+ * never fails.
+ */
+void __init x86_numa_init(void)
+{
+	if (!numa_off) {
+#ifdef CONFIG_ACPI_NUMA
+		if (!numa_init(x86_acpi_numa_init))
+			return;
+#endif
+#ifdef CONFIG_AMD_NUMA
+		if (!numa_init(amd_numa_init))
+			return;
+#endif
+	}
+
+	numa_init(dummy_numa_init);
+}
+#endif
+
 static __init int find_near_online_node(int node)
 {
 	int n, val;
@@ -292,3 +794,18 @@ const struct cpumask *cpumask_of_node(int node)
 EXPORT_SYMBOL(cpumask_of_node);
 
 #endif	/* !CONFIG_DEBUG_PER_CPU_MAPS */
+
+#if defined(CONFIG_X86_64) && defined(CONFIG_MEMORY_HOTPLUG)
+int memory_add_physaddr_to_nid(u64 start)
+{
+	struct numa_meminfo *mi = &numa_meminfo;
+	int nid = mi->blk[0].nid;
+	int i;
+
+	for (i = 0; i < mi->nr_blks; i++)
+		if (mi->blk[i].start <= start && mi->blk[i].end > start)
+			nid = mi->blk[i].nid;
+	return nid;
+}
+EXPORT_SYMBOL_GPL(memory_add_physaddr_to_nid);
+#endif

commit e6df595b37c7c033ef7400b4fdd382a2dc4f4131
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 14:18:53 2011 +0200

    x86, NUMA: Move numa_nodes_parsed to numa.[hc]
    
    Move numa_nodes_parsed from numa_64.[hc] to numa.[hc] to prepare for
    NUMA init path unification.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index e9005c4ea29a..cce174109ca9 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -6,6 +6,7 @@
 #include <asm/acpi.h>
 
 int __initdata numa_off;
+nodemask_t numa_nodes_parsed __initdata;
 
 static __init int numa_setup(char *opt)
 {

commit 6bd262731bf7559bab8c749786e8652e2df1fb4e
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 2 14:18:52 2011 +0200

    x86, NUMA: Unify 32/64bit numa_cpu_node() implementation
    
    Currently, the only meaningful user of apic->x86_32_numa_cpu_node() is
    NUMAQ which returns valid mapping only after CPU is initialized during
    SMP bringup; thus, the previous patch to set apicid -> node in
    setup_local_APIC() makes __apicid_to_node[] always contain the correct
    mapping whether custom apic->x86_32_numa_cpu_node() is used or not.
    
    So, there is no reason to keep separate 32bit implementation.  We can
    always consult __apicid_to_node[].  Move 64bit implementation from
    numa_64.c to numa.c and remove 32bit implementation from numa_32.c.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 745258dfc4dc..e9005c4ea29a 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -32,6 +32,15 @@ s16 __apicid_to_node[MAX_LOCAL_APIC] __cpuinitdata = {
 	[0 ... MAX_LOCAL_APIC-1] = NUMA_NO_NODE
 };
 
+int __cpuinit numa_cpu_node(int cpu)
+{
+	int apicid = early_per_cpu(x86_cpu_to_apicid, cpu);
+
+	if (apicid != BAD_APICID)
+		return __apicid_to_node[apicid];
+	return NUMA_NO_NODE;
+}
+
 cpumask_var_t node_to_cpumask_map[MAX_NUMNODES];
 EXPORT_SYMBOL(node_to_cpumask_map);
 

commit 7a6c6547825a2324faa76cff856db11d78de075e
Author: David Rientjes <rientjes@google.com>
Date:   Wed Apr 20 19:19:13 2011 -0700

    x86, numa: Fix cpu nodemasks for NUMA emulation and CONFIG_DEBUG_PER_CPU_MAPS
    
    The cpu<->node mappings under CONFIG_DEBUG_PER_CPU_MAPS=y
    when NUMA emulation is enabled is currently broken because it does
    not iterate through every emulated node and bind cpus that have
    affinity to it.
    
    NUMA emulation should bind each cpu to every local node to
    accurately represent the true NUMA topology of the underlying
    machine.
    
    debug_cpumask_set_cpu() needs to be fixed at the same time so
    that the debugging information that it emits shows the new
    cpumask of the node being assigned when the cpu is being added
    or removed.
    
    It can now take responsibility of setting or clearing the cpu
    itself to remove the need for duplicate code.
    
    Also change its last parameter, "enable", to have the correct bool
    type since it can only be true or false.
    
     -v2: Fix the return statements, by Kosaki Motohiro
    
    Acked-and-Tested-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Andreas Herrmann <herrmann.der.user@googlemail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.00.1104201918470.12634@chino.kir.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 9559d360fde7..745258dfc4dc 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -213,53 +213,48 @@ int early_cpu_to_node(int cpu)
 	return per_cpu(x86_cpu_to_node_map, cpu);
 }
 
-struct cpumask __cpuinit *debug_cpumask_set_cpu(int cpu, int enable)
+void debug_cpumask_set_cpu(int cpu, int node, bool enable)
 {
-	int node = early_cpu_to_node(cpu);
 	struct cpumask *mask;
 	char buf[64];
 
 	if (node == NUMA_NO_NODE) {
 		/* early_cpu_to_node() already emits a warning and trace */
-		return NULL;
+		return;
 	}
 	mask = node_to_cpumask_map[node];
 	if (!mask) {
 		pr_err("node_to_cpumask_map[%i] NULL\n", node);
 		dump_stack();
-		return NULL;
+		return;
 	}
 
+	if (enable)
+		cpumask_set_cpu(cpu, mask);
+	else
+		cpumask_clear_cpu(cpu, mask);
+
 	cpulist_scnprintf(buf, sizeof(buf), mask);
 	printk(KERN_DEBUG "%s cpu %d node %d: mask now %s\n",
 		enable ? "numa_add_cpu" : "numa_remove_cpu",
 		cpu, node, buf);
-	return mask;
+	return;
 }
 
 # ifndef CONFIG_NUMA_EMU
-static void __cpuinit numa_set_cpumask(int cpu, int enable)
+static void __cpuinit numa_set_cpumask(int cpu, bool enable)
 {
-	struct cpumask *mask;
-
-	mask = debug_cpumask_set_cpu(cpu, enable);
-	if (!mask)
-		return;
-
-	if (enable)
-		cpumask_set_cpu(cpu, mask);
-	else
-		cpumask_clear_cpu(cpu, mask);
+	debug_cpumask_set_cpu(cpu, early_cpu_to_node(cpu), enable);
 }
 
 void __cpuinit numa_add_cpu(int cpu)
 {
-	numa_set_cpumask(cpu, 1);
+	numa_set_cpumask(cpu, true);
 }
 
 void __cpuinit numa_remove_cpu(int cpu)
 {
-	numa_set_cpumask(cpu, 0);
+	numa_set_cpumask(cpu, false);
 }
 # endif	/* !CONFIG_NUMA_EMU */
 

commit 14392fd329eca9b59d51c0aa5d0acfb4965424d1
Author: David Rientjes <rientjes@google.com>
Date:   Mon Feb 7 14:08:53 2011 -0800

    x86, numa: Add error handling for bad cpu-to-node mappings
    
    CONFIG_DEBUG_PER_CPU_MAPS may return NUMA_NO_NODE when an
    early_cpu_to_node() mapping hasn't been initialized.  In such a
    case, it emits a warning and continues without an issue but
    callers may try to use the return value to index into an array.
    
    We can catch those errors and fail silently since a warning has
    already been emitted.  No current user of numa_add_cpu()
    requires this error checking to avoid a crash, but it's better
    to be proactive in case a future user happens to have a bug and
    a user tries to diagnose it with CONFIG_DEBUG_PER_CPU_MAPS.
    
    Reported-by: Jesper Juhl <jj@chaosbits.net>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    LKML-Reference: <alpine.DEB.2.00.1102071407250.7812@chino.kir.corp.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index bf60715bd1b7..9559d360fde7 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -219,6 +219,10 @@ struct cpumask __cpuinit *debug_cpumask_set_cpu(int cpu, int enable)
 	struct cpumask *mask;
 	char buf[64];
 
+	if (node == NUMA_NO_NODE) {
+		/* early_cpu_to_node() already emits a warning and trace */
+		return NULL;
+	}
 	mask = node_to_cpumask_map[node];
 	if (!mask) {
 		pr_err("node_to_cpumask_map[%i] NULL\n", node);

commit 8db78cc4b4048e3add40bca1bc3e55057c319256
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:42 2011 +0100

    x86: Unify NUMA initialization between 32 and 64bit
    
    Now that everything else is unified, NUMA initialization can be
    unified too.
    
    * numa_init_array() and init_cpu_to_node() are moved from
      numa_64 to numa.
    
    * numa_32::initmem_init() is updated to call numa_init_array()
      and setup_arch() to call init_cpu_to_node() on 32bit too.
    
    * x86_cpu_to_node_map is now initialized to NUMA_NO_NODE on
      32bit too. This is safe now as numa_init_array() will initialize
      it early during boot.
    
    This makes NUMA mapping fully initialized before
    setup_per_cpu_areas() on 32bit too and thus makes the first
    percpu chunk which contains all the static variables and some of
    dynamic area allocated with NUMA affinity correctly considered.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-17-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Reported-by: Eric Dumazet <eric.dumazet@gmail.com>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 75abecb614c9..bf60715bd1b7 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -38,11 +38,7 @@ EXPORT_SYMBOL(node_to_cpumask_map);
 /*
  * Map cpu index to node index
  */
-#ifdef CONFIG_X86_32
-DEFINE_EARLY_PER_CPU(int, x86_cpu_to_node_map, 0);
-#else
 DEFINE_EARLY_PER_CPU(int, x86_cpu_to_node_map, NUMA_NO_NODE);
-#endif
 EXPORT_EARLY_PER_CPU_SYMBOL(x86_cpu_to_node_map);
 
 void __cpuinit numa_set_node(int cpu, int node)
@@ -99,6 +95,78 @@ void __init setup_node_to_cpumask_map(void)
 	pr_debug("Node to cpumask map for %d nodes\n", nr_node_ids);
 }
 
+/*
+ * There are unfortunately some poorly designed mainboards around that
+ * only connect memory to a single CPU. This breaks the 1:1 cpu->node
+ * mapping. To avoid this fill in the mapping for all possible CPUs,
+ * as the number of CPUs is not known yet. We round robin the existing
+ * nodes.
+ */
+void __init numa_init_array(void)
+{
+	int rr, i;
+
+	rr = first_node(node_online_map);
+	for (i = 0; i < nr_cpu_ids; i++) {
+		if (early_cpu_to_node(i) != NUMA_NO_NODE)
+			continue;
+		numa_set_node(i, rr);
+		rr = next_node(rr, node_online_map);
+		if (rr == MAX_NUMNODES)
+			rr = first_node(node_online_map);
+	}
+}
+
+static __init int find_near_online_node(int node)
+{
+	int n, val;
+	int min_val = INT_MAX;
+	int best_node = -1;
+
+	for_each_online_node(n) {
+		val = node_distance(node, n);
+
+		if (val < min_val) {
+			min_val = val;
+			best_node = n;
+		}
+	}
+
+	return best_node;
+}
+
+/*
+ * Setup early cpu_to_node.
+ *
+ * Populate cpu_to_node[] only if x86_cpu_to_apicid[],
+ * and apicid_to_node[] tables have valid entries for a CPU.
+ * This means we skip cpu_to_node[] initialisation for NUMA
+ * emulation and faking node case (when running a kernel compiled
+ * for NUMA on a non NUMA box), which is OK as cpu_to_node[]
+ * is already initialized in a round robin manner at numa_init_array,
+ * prior to this call, and this initialization is good enough
+ * for the fake NUMA cases.
+ *
+ * Called before the per_cpu areas are setup.
+ */
+void __init init_cpu_to_node(void)
+{
+	int cpu;
+	u16 *cpu_to_apicid = early_per_cpu_ptr(x86_cpu_to_apicid);
+
+	BUG_ON(cpu_to_apicid == NULL);
+
+	for_each_possible_cpu(cpu) {
+		int node = numa_cpu_node(cpu);
+
+		if (node == NUMA_NO_NODE)
+			continue;
+		if (!node_online(node))
+			node = find_near_online_node(node);
+		numa_set_node(cpu, node);
+	}
+}
+
 #ifndef CONFIG_DEBUG_PER_CPU_MAPS
 
 # ifndef CONFIG_NUMA_EMU

commit de2d9445f1627830ed2ebd00ee9d851986c940b5
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:41 2011 +0100

    x86: Unify node_to_cpumask_map handling between 32 and 64bit
    
    x86_32 has been managing node_to_cpumask_map explicitly from
    map_cpu_to_node() and friends in a rather ugly way.  With
    previous changes, it's now possible to share the code with
    64bit.
    
    * When CONFIG_NUMA_EMU is disabled, numa_add/remove_cpu() are
      implemented in numa.c and shared by 32 and 64bit.  CONFIG_NUMA_EMU
      versions still live in numa_64.c.
    
      NUMA_EMU's dependency on 64bit is planned to be removed and the
      above should go away together.
    
    * identify_cpu() now calls numa_add_cpu() for 32bit too.  This
      makes the explicit mask management from map_cpu_to_node() unnecessary.
    
    * The whole x86_32 specific map_cpu_to_node() chunk is no longer
      necessary.  Dropped.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-16-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Shaohui Zheng <shaohui.zheng@intel.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 187810be3d6c..75abecb614c9 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -99,7 +99,21 @@ void __init setup_node_to_cpumask_map(void)
 	pr_debug("Node to cpumask map for %d nodes\n", nr_node_ids);
 }
 
-#ifdef CONFIG_DEBUG_PER_CPU_MAPS
+#ifndef CONFIG_DEBUG_PER_CPU_MAPS
+
+# ifndef CONFIG_NUMA_EMU
+void __cpuinit numa_add_cpu(int cpu)
+{
+	cpumask_set_cpu(cpu, node_to_cpumask_map[early_cpu_to_node(cpu)]);
+}
+
+void __cpuinit numa_remove_cpu(int cpu)
+{
+	cpumask_clear_cpu(cpu, node_to_cpumask_map[early_cpu_to_node(cpu)]);
+}
+# endif	/* !CONFIG_NUMA_EMU */
+
+#else	/* !CONFIG_DEBUG_PER_CPU_MAPS */
 
 int __cpu_to_node(int cpu)
 {
@@ -131,6 +145,52 @@ int early_cpu_to_node(int cpu)
 	return per_cpu(x86_cpu_to_node_map, cpu);
 }
 
+struct cpumask __cpuinit *debug_cpumask_set_cpu(int cpu, int enable)
+{
+	int node = early_cpu_to_node(cpu);
+	struct cpumask *mask;
+	char buf[64];
+
+	mask = node_to_cpumask_map[node];
+	if (!mask) {
+		pr_err("node_to_cpumask_map[%i] NULL\n", node);
+		dump_stack();
+		return NULL;
+	}
+
+	cpulist_scnprintf(buf, sizeof(buf), mask);
+	printk(KERN_DEBUG "%s cpu %d node %d: mask now %s\n",
+		enable ? "numa_add_cpu" : "numa_remove_cpu",
+		cpu, node, buf);
+	return mask;
+}
+
+# ifndef CONFIG_NUMA_EMU
+static void __cpuinit numa_set_cpumask(int cpu, int enable)
+{
+	struct cpumask *mask;
+
+	mask = debug_cpumask_set_cpu(cpu, enable);
+	if (!mask)
+		return;
+
+	if (enable)
+		cpumask_set_cpu(cpu, mask);
+	else
+		cpumask_clear_cpu(cpu, mask);
+}
+
+void __cpuinit numa_add_cpu(int cpu)
+{
+	numa_set_cpumask(cpu, 1);
+}
+
+void __cpuinit numa_remove_cpu(int cpu)
+{
+	numa_set_cpumask(cpu, 0);
+}
+# endif	/* !CONFIG_NUMA_EMU */
+
 /*
  * Returns a pointer to the bitmask of CPUs on Node 'node'.
  */
@@ -154,4 +214,4 @@ const struct cpumask *cpumask_of_node(int node)
 }
 EXPORT_SYMBOL(cpumask_of_node);
 
-#endif	/* CONFIG_DEBUG_PER_CPU_MAPS */
+#endif	/* !CONFIG_DEBUG_PER_CPU_MAPS */

commit 645a79195f66eb68ef3ab2b21d9829ac3aa085a9
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:40 2011 +0100

    x86: Unify CPU -> NUMA node mapping between 32 and 64bit
    
    Unlike 64bit, 32bit has been using its own cpu_to_node_map[] for
    CPU -> NUMA node mapping.  Replace it with early_percpu variable
    x86_cpu_to_node_map and share the mapping code with 64bit.
    
    * USE_PERCPU_NUMA_NODE_ID is now enabled for 32bit too.
    
    * x86_cpu_to_node_map and numa_set/clear_node() are moved from
      numa_64 to numa.  For now, on 32bit, x86_cpu_to_node_map is initialized
      with 0 instead of NUMA_NO_NODE.  This is to avoid introducing unexpected
      behavior change and will be updated once init path is unified.
    
    * srat_detect_node() is now enabled for x86_32 too.  It calls
      numa_set_node() and initializes the mapping making explicit
      cpu_to_node_map[] updates from map/unmap_cpu_to_node() unnecessary.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: penberg@kernel.org
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-15-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 480b3571c8b1..187810be3d6c 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -35,6 +35,44 @@ s16 __apicid_to_node[MAX_LOCAL_APIC] __cpuinitdata = {
 cpumask_var_t node_to_cpumask_map[MAX_NUMNODES];
 EXPORT_SYMBOL(node_to_cpumask_map);
 
+/*
+ * Map cpu index to node index
+ */
+#ifdef CONFIG_X86_32
+DEFINE_EARLY_PER_CPU(int, x86_cpu_to_node_map, 0);
+#else
+DEFINE_EARLY_PER_CPU(int, x86_cpu_to_node_map, NUMA_NO_NODE);
+#endif
+EXPORT_EARLY_PER_CPU_SYMBOL(x86_cpu_to_node_map);
+
+void __cpuinit numa_set_node(int cpu, int node)
+{
+	int *cpu_to_node_map = early_per_cpu_ptr(x86_cpu_to_node_map);
+
+	/* early setting, no percpu area yet */
+	if (cpu_to_node_map) {
+		cpu_to_node_map[cpu] = node;
+		return;
+	}
+
+#ifdef CONFIG_DEBUG_PER_CPU_MAPS
+	if (cpu >= nr_cpu_ids || !cpu_possible(cpu)) {
+		printk(KERN_ERR "numa_set_node: invalid cpu# (%d)\n", cpu);
+		dump_stack();
+		return;
+	}
+#endif
+	per_cpu(x86_cpu_to_node_map, cpu) = node;
+
+	if (node != NUMA_NO_NODE)
+		set_cpu_numa_node(cpu, node);
+}
+
+void __cpuinit numa_clear_node(int cpu)
+{
+	numa_set_node(cpu, NUMA_NO_NODE);
+}
+
 /*
  * Allocate node_to_cpumask_map based on number of available nodes
  * Requires node_possible_map to be valid.
@@ -62,6 +100,37 @@ void __init setup_node_to_cpumask_map(void)
 }
 
 #ifdef CONFIG_DEBUG_PER_CPU_MAPS
+
+int __cpu_to_node(int cpu)
+{
+	if (early_per_cpu_ptr(x86_cpu_to_node_map)) {
+		printk(KERN_WARNING
+			"cpu_to_node(%d): usage too early!\n", cpu);
+		dump_stack();
+		return early_per_cpu_ptr(x86_cpu_to_node_map)[cpu];
+	}
+	return per_cpu(x86_cpu_to_node_map, cpu);
+}
+EXPORT_SYMBOL(__cpu_to_node);
+
+/*
+ * Same function as cpu_to_node() but used if called before the
+ * per_cpu areas are setup.
+ */
+int early_cpu_to_node(int cpu)
+{
+	if (early_per_cpu_ptr(x86_cpu_to_node_map))
+		return early_per_cpu_ptr(x86_cpu_to_node_map)[cpu];
+
+	if (!cpu_possible(cpu)) {
+		printk(KERN_WARNING
+			"early_cpu_to_node(%d): no per_cpu area!\n", cpu);
+		dump_stack();
+		return NUMA_NO_NODE;
+	}
+	return per_cpu(x86_cpu_to_node_map, cpu);
+}
+
 /*
  * Returns a pointer to the bitmask of CPUs on Node 'node'.
  */
@@ -84,4 +153,5 @@ const struct cpumask *cpumask_of_node(int node)
 	return node_to_cpumask_map[node];
 }
 EXPORT_SYMBOL(cpumask_of_node);
-#endif
+
+#endif	/* CONFIG_DEBUG_PER_CPU_MAPS */

commit bbc9e2f452d9c4b166d1f9a78d941d80173312fe
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:39 2011 +0100

    x86: Unify cpu/apicid <-> NUMA node mapping between 32 and 64bit
    
    The mapping between cpu/apicid and node is done via
    apicid_to_node[] on 64bit and apicid_2_node[] +
    apic->x86_32_numa_cpu_node() on 32bit. This difference makes it
    difficult to further unify 32 and 64bit NUMA handling.
    
    This patch unifies it by replacing both apicid_to_node[] and
    apicid_2_node[] with __apicid_to_node[] array, which is accessed
    by two accessors - set_apicid_to_node() and numa_cpu_node().  On
    64bit, numa_cpu_node() always consults __apicid_to_node[]
    directly while 32bit goes through apic->numa_cpu_node() method
    to allow apic implementations to override it.
    
    srat_detect_node() for amd cpus contains workaround for broken
    NUMA configuration which assumes relationship between APIC ID,
    HT node ID and NUMA topology.  Leave it to access
    __apicid_to_node[] directly as mapping through CPU might result
    in undesirable behavior change.  The comment is reformatted and
    updated to note the ugliness.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-14-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index ebf6d7887a38..480b3571c8b1 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -26,8 +26,12 @@ static __init int numa_setup(char *opt)
 early_param("numa", numa_setup);
 
 /*
- * Which logical CPUs are on which nodes
+ * apicid, cpu, node mappings
  */
+s16 __apicid_to_node[MAX_LOCAL_APIC] __cpuinitdata = {
+	[0 ... MAX_LOCAL_APIC-1] = NUMA_NO_NODE
+};
+
 cpumask_var_t node_to_cpumask_map[MAX_NUMNODES];
 EXPORT_SYMBOL(node_to_cpumask_map);
 

commit 9032160275ba018003ff390835ff8ed2b5b788b8
Author: Jan Beulich <JBeulich@novell.com>
Date:   Wed Jan 19 08:57:21 2011 +0000

    x86: Unify "numa=" command line option handling
    
    In order to be able to suppress the use of SRAT tables that
    32-bit Linux can't deal with (in one case known to lead to a
    non-bootable system, unless disabling ACPI altogether), move the
    "numa=" option handling to common code.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Renninger <trenn@suse.de>
    LKML-Reference: <4D36B581020000780002D0FF@vpn.id2.novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 787c52ca49c3..ebf6d7887a38 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -2,6 +2,28 @@
 #include <linux/topology.h>
 #include <linux/module.h>
 #include <linux/bootmem.h>
+#include <asm/numa.h>
+#include <asm/acpi.h>
+
+int __initdata numa_off;
+
+static __init int numa_setup(char *opt)
+{
+	if (!opt)
+		return -EINVAL;
+	if (!strncmp(opt, "off", 3))
+		numa_off = 1;
+#ifdef CONFIG_NUMA_EMU
+	if (!strncmp(opt, "fake=", 5))
+		numa_emu_cmdline(opt + 5);
+#endif
+#ifdef CONFIG_ACPI_NUMA
+	if (!strncmp(opt, "noacpi", 6))
+		acpi_numa = -1;
+#endif
+	return 0;
+}
+early_param("numa", numa_setup);
 
 /*
  * Which logical CPUs are on which nodes

commit e565813ab95875af0d51a6bcd537068380bb06ea
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Mon May 24 22:04:51 2010 +0900

    x86/mm: Remove unused DBG() macro
    
    DBG() macro for CONFIG_DEBUG_PER_CPU_MAPS is unused.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    LKML-Reference: <1274706291-13554-1-git-send-email-akinobu.mita@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 550df481accd..787c52ca49c3 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -3,12 +3,6 @@
 #include <linux/module.h>
 #include <linux/bootmem.h>
 
-#ifdef CONFIG_DEBUG_PER_CPU_MAPS
-# define DBG(x...) printk(KERN_DEBUG x)
-#else
-# define DBG(x...)
-#endif
-
 /*
  * Which logical CPUs are on which nodes
  */

commit 35926ff5fba8245bd1c6ac04155048f6f89232b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 30 09:00:03 2010 -0700

    Revert "cpusets: randomize node rotor used in cpuset_mem_spread_node()"
    
    This reverts commit 0ac0c0d0f837c499afd02a802f9cf52d3027fa3b, which
    caused cross-architecture build problems for all the wrong reasons.
    IA64 already added its own version of __node_random(), but the fact is,
    there is nothing architectural about the function, and the original
    commit was just badly done. Revert it, since no fix is forthcoming.
    
    Requested-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 10c27bb1e95f..550df481accd 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -2,7 +2,6 @@
 #include <linux/topology.h>
 #include <linux/module.h>
 #include <linux/bootmem.h>
-#include <linux/random.h>
 
 #ifdef CONFIG_DEBUG_PER_CPU_MAPS
 # define DBG(x...) printk(KERN_DEBUG x)
@@ -66,19 +65,3 @@ const struct cpumask *cpumask_of_node(int node)
 }
 EXPORT_SYMBOL(cpumask_of_node);
 #endif
-
-/*
- * Return the bit number of a random bit set in the nodemask.
- *   (returns -1 if nodemask is empty)
- */
-int __node_random(const nodemask_t *maskp)
-{
-	int w, bit = -1;
-
-	w = nodes_weight(*maskp);
-	if (w)
-		bit = bitmap_ord_to_pos(maskp->bits,
-			get_random_int() % w, MAX_NUMNODES);
-	return bit;
-}
-EXPORT_SYMBOL(__node_random);

commit 0ac0c0d0f837c499afd02a802f9cf52d3027fa3b
Author: Jack Steiner <steiner@sgi.com>
Date:   Wed May 26 14:42:51 2010 -0700

    cpusets: randomize node rotor used in cpuset_mem_spread_node()
    
    Some workloads that create a large number of small files tend to assign
    too many pages to node 0 (multi-node systems).  Part of the reason is that
    the rotor (in cpuset_mem_spread_node()) used to assign nodes starts at
    node 0 for newly created tasks.
    
    This patch changes the rotor to be initialized to a random node number of
    the cpuset.
    
    [akpm@linux-foundation.org: fix layout]
    [Lee.Schermerhorn@hp.com: Define stub numa_random() for !NUMA configuration]
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Paul Menage <menage@google.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 550df481accd..10c27bb1e95f 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -2,6 +2,7 @@
 #include <linux/topology.h>
 #include <linux/module.h>
 #include <linux/bootmem.h>
+#include <linux/random.h>
 
 #ifdef CONFIG_DEBUG_PER_CPU_MAPS
 # define DBG(x...) printk(KERN_DEBUG x)
@@ -65,3 +66,19 @@ const struct cpumask *cpumask_of_node(int node)
 }
 EXPORT_SYMBOL(cpumask_of_node);
 #endif
+
+/*
+ * Return the bit number of a random bit set in the nodemask.
+ *   (returns -1 if nodemask is empty)
+ */
+int __node_random(const nodemask_t *maskp)
+{
+	int w, bit = -1;
+
+	w = nodes_weight(*maskp);
+	if (w)
+		bit = bitmap_ord_to_pos(maskp->bits,
+			get_random_int() % w, MAX_NUMNODES);
+	return bit;
+}
+EXPORT_SYMBOL(__node_random);

commit 0b966252d9e5d95ec2d11e63d7e55b42913aa5b7
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 23:42:42 2009 +1030

    cpumask: convert node_to_cpumask_map[] to cpumask_var_t
    
    Impact: fix (CONFIG_MAXSMP=y only) boot crash
    
    c032ef60d1aa9af33730b7a35bbea751b131adc1 "cpumask: convert
    node_to_cpumask_map[] to cpumask_var_t" didn't get this one
    conversion.  There was a compile warning, but I missed it.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Mike Travis <travis@sgi.com>
    LKML-Reference: <200903132342.42813.rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index ce255e32a593..550df481accd 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -61,7 +61,7 @@ const struct cpumask *cpumask_of_node(int node)
 		dump_stack();
 		return cpu_online_mask;
 	}
-	return &node_to_cpumask_map[node];
+	return node_to_cpumask_map[node];
 }
 EXPORT_SYMBOL(cpumask_of_node);
 #endif

commit 73e907de7d5cecef43d9949ab8f4fdca508168c7
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:57 2009 +1030

    cpumask: remove x86 cpumask_t uses.
    
    Impact: cleanup
    
    We are removing cpumask_t in favour of struct cpumask: mainly as a
    marker of what code is now CONFIG_CPUMASK_OFFSTACK-safe.
    
    The only non-trivial change here is vector_allocation_domain():
    explicitly clear the mask and set the first word, rather than using
    assignment.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 429dc2d191fd..ce255e32a593 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -45,7 +45,7 @@ void __init setup_node_to_cpumask_map(void)
 /*
  * Returns a pointer to the bitmask of CPUs on Node 'node'.
  */
-const cpumask_t *cpumask_of_node(int node)
+const struct cpumask *cpumask_of_node(int node)
 {
 	if (node >= nr_node_ids) {
 		printk(KERN_WARNING

commit c032ef60d1aa9af33730b7a35bbea751b131adc1
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:53 2009 +1030

    cpumask: convert node_to_cpumask_map[] to cpumask_var_t
    
    Impact: reduce kernel memory usage when CONFIG_CPUMASK_OFFSTACK=y
    
    Straightforward conversion: done for 32 and 64 bit kernels.
    node_to_cpumask_map is now a cpumask_var_t array.
    
    64-bit used to be a dynamic cpumask_t array, and 32-bit used to be a
    static cpumask_t array.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index f3a19e939e80..429dc2d191fd 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -12,7 +12,7 @@
 /*
  * Which logical CPUs are on which nodes
  */
-cpumask_t *node_to_cpumask_map;
+cpumask_var_t node_to_cpumask_map[MAX_NUMNODES];
 EXPORT_SYMBOL(node_to_cpumask_map);
 
 /*
@@ -25,7 +25,6 @@ EXPORT_SYMBOL(node_to_cpumask_map);
 void __init setup_node_to_cpumask_map(void)
 {
 	unsigned int node, num = 0;
-	cpumask_t *map;
 
 	/* setup nr_node_ids if not done yet */
 	if (nr_node_ids == MAX_NUMNODES) {
@@ -35,14 +34,11 @@ void __init setup_node_to_cpumask_map(void)
 	}
 
 	/* allocate the map */
-	map = alloc_bootmem_low(nr_node_ids * sizeof(cpumask_t));
-	DBG("node_to_cpumask_map at %p for %d nodes\n", map, nr_node_ids);
+	for (node = 0; node < nr_node_ids; node++)
+		alloc_bootmem_cpumask_var(&node_to_cpumask_map[node]);
 
-	pr_debug("Node to cpumask map at %p for %d nodes\n",
-		 map, nr_node_ids);
-
-	/* node_to_cpumask() will now work */
-	node_to_cpumask_map = map;
+	/* cpumask_of_node() will now work */
+	pr_debug("Node to cpumask map for %d nodes\n", nr_node_ids);
 }
 
 #ifdef CONFIG_DEBUG_PER_CPU_MAPS
@@ -51,13 +47,6 @@ void __init setup_node_to_cpumask_map(void)
  */
 const cpumask_t *cpumask_of_node(int node)
 {
-	if (node_to_cpumask_map == NULL) {
-		printk(KERN_WARNING
-			"cpumask_of_node(%d): no node_to_cpumask_map!\n",
-			node);
-		dump_stack();
-		return cpu_online_mask;
-	}
 	if (node >= nr_node_ids) {
 		printk(KERN_WARNING
 			"cpumask_of_node(%d): node > nr_node_ids(%d)\n",
@@ -65,6 +54,13 @@ const cpumask_t *cpumask_of_node(int node)
 		dump_stack();
 		return cpu_none_mask;
 	}
+	if (node_to_cpumask_map[node] == NULL) {
+		printk(KERN_WARNING
+			"cpumask_of_node(%d): no node_to_cpumask_map!\n",
+			node);
+		dump_stack();
+		return cpu_online_mask;
+	}
 	return &node_to_cpumask_map[node];
 }
 EXPORT_SYMBOL(cpumask_of_node);

commit 71ee73e72228775a076a502b3c92028fa59e2889
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:52 2009 +1030

    x86: unify 32 and 64-bit node_to_cpumask_map
    
    Impact: cleanup
    
    We take the 64-bit code and use it on 32-bit as well.  The new file
    is called mm/numa.c.
    
    In a minor cleanup, we use cpu_none_mask instead of declaring a local
    cpu_mask_none.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
new file mode 100644
index 000000000000..f3a19e939e80
--- /dev/null
+++ b/arch/x86/mm/numa.c
@@ -0,0 +1,71 @@
+/* Common code for 32 and 64-bit NUMA */
+#include <linux/topology.h>
+#include <linux/module.h>
+#include <linux/bootmem.h>
+
+#ifdef CONFIG_DEBUG_PER_CPU_MAPS
+# define DBG(x...) printk(KERN_DEBUG x)
+#else
+# define DBG(x...)
+#endif
+
+/*
+ * Which logical CPUs are on which nodes
+ */
+cpumask_t *node_to_cpumask_map;
+EXPORT_SYMBOL(node_to_cpumask_map);
+
+/*
+ * Allocate node_to_cpumask_map based on number of available nodes
+ * Requires node_possible_map to be valid.
+ *
+ * Note: node_to_cpumask() is not valid until after this is done.
+ * (Use CONFIG_DEBUG_PER_CPU_MAPS to check this.)
+ */
+void __init setup_node_to_cpumask_map(void)
+{
+	unsigned int node, num = 0;
+	cpumask_t *map;
+
+	/* setup nr_node_ids if not done yet */
+	if (nr_node_ids == MAX_NUMNODES) {
+		for_each_node_mask(node, node_possible_map)
+			num = node;
+		nr_node_ids = num + 1;
+	}
+
+	/* allocate the map */
+	map = alloc_bootmem_low(nr_node_ids * sizeof(cpumask_t));
+	DBG("node_to_cpumask_map at %p for %d nodes\n", map, nr_node_ids);
+
+	pr_debug("Node to cpumask map at %p for %d nodes\n",
+		 map, nr_node_ids);
+
+	/* node_to_cpumask() will now work */
+	node_to_cpumask_map = map;
+}
+
+#ifdef CONFIG_DEBUG_PER_CPU_MAPS
+/*
+ * Returns a pointer to the bitmask of CPUs on Node 'node'.
+ */
+const cpumask_t *cpumask_of_node(int node)
+{
+	if (node_to_cpumask_map == NULL) {
+		printk(KERN_WARNING
+			"cpumask_of_node(%d): no node_to_cpumask_map!\n",
+			node);
+		dump_stack();
+		return cpu_online_mask;
+	}
+	if (node >= nr_node_ids) {
+		printk(KERN_WARNING
+			"cpumask_of_node(%d): node > nr_node_ids(%d)\n",
+			node, nr_node_ids);
+		dump_stack();
+		return cpu_none_mask;
+	}
+	return &node_to_cpumask_map[node];
+}
+EXPORT_SYMBOL(cpumask_of_node);
+#endif
