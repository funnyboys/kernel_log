commit 8fefe9dacdb0a1347d3dac871bb1bba3cbc32945
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Mon Mar 23 19:57:07 2020 +0000

    x86/vmware: Use bool type for vmw_sched_clock
    
    To be aligned with other bool variables.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200323195707.31242-6-amakhalov@vmware.com

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index e885f73bebd4..9b6fafa69be9 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -122,14 +122,14 @@ static unsigned long vmware_get_tsc_khz(void)
 
 #ifdef CONFIG_PARAVIRT
 static struct cyc2ns_data vmware_cyc2ns __ro_after_init;
-static int vmw_sched_clock __initdata = 1;
+static bool vmw_sched_clock __initdata = true;
 static DEFINE_PER_CPU_DECRYPTED(struct vmware_steal_time, vmw_steal_time) __aligned(64);
 static bool has_steal_clock;
 static bool steal_acc __initdata = true; /* steal time accounting */
 
 static __init int setup_vmw_sched_clock(char *s)
 {
-	vmw_sched_clock = 0;
+	vmw_sched_clock = false;
 	return 0;
 }
 early_param("no-vmw-sched-clock", setup_vmw_sched_clock);

commit e73a8f38f82dd1c41b70a06556bea7dc250cc384
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Mon Mar 23 19:57:06 2020 +0000

    x86/vmware: Enable steal time accounting
    
    Set paravirt_steal_rq_enabled if steal clock present.
    paravirt_steal_rq_enabled is used in sched/core.c to adjust task
    progress by offsetting stolen time. Use 'no-steal-acc' off switch (share
    same name with KVM) to disable steal time accounting.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200323195707.31242-5-amakhalov@vmware.com

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index cc604614f0ae..e885f73bebd4 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -125,6 +125,7 @@ static struct cyc2ns_data vmware_cyc2ns __ro_after_init;
 static int vmw_sched_clock __initdata = 1;
 static DEFINE_PER_CPU_DECRYPTED(struct vmware_steal_time, vmw_steal_time) __aligned(64);
 static bool has_steal_clock;
+static bool steal_acc __initdata = true; /* steal time accounting */
 
 static __init int setup_vmw_sched_clock(char *s)
 {
@@ -133,6 +134,13 @@ static __init int setup_vmw_sched_clock(char *s)
 }
 early_param("no-vmw-sched-clock", setup_vmw_sched_clock);
 
+static __init int parse_no_stealacc(char *arg)
+{
+	steal_acc = false;
+	return 0;
+}
+early_param("no-steal-acc", parse_no_stealacc);
+
 static unsigned long long notrace vmware_sched_clock(void)
 {
 	unsigned long long ns;
@@ -306,8 +314,11 @@ static int vmware_cpu_down_prepare(unsigned int cpu)
 
 static __init int activate_jump_labels(void)
 {
-	if (has_steal_clock)
+	if (has_steal_clock) {
 		static_key_slow_inc(&paravirt_steal_enabled);
+		if (steal_acc)
+			static_key_slow_inc(&paravirt_steal_rq_enabled);
+	}
 
 	return 0;
 }

commit ab02bb3f55f58e7608a88188000c3353398ebe3b
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Mon Mar 23 19:57:05 2020 +0000

    x86/vmware: Add steal time clock support for VMware guests
    
    Steal time is the amount of CPU time needed by a guest virtual machine
    that is not provided by the host. Steal time occurs when the host
    allocates this CPU time elsewhere, for example, to another guest.
    
    Steal time can be enabled by adding the VM configuration option
    stealclock.enable = "TRUE". It is supported by VMs that run hardware
    version 13 or newer.
    
    Introduce the VMware steal time infrastructure. The high level code
    (such as enabling, disabling and hot-plug routines) was derived from KVM.
    
     [ Tomer: use READ_ONCE macros and 32bit guests support. ]
     [ bp: Massage. ]
    
    Co-developed-by: Tomer Zeltzer <tomerr90@gmail.com>
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Signed-off-by: Tomer Zeltzer <tomerr90@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200323195707.31242-4-amakhalov@vmware.com

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index efb22fa76ba4..cc604614f0ae 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -25,6 +25,8 @@
 #include <linux/init.h>
 #include <linux/export.h>
 #include <linux/clocksource.h>
+#include <linux/cpu.h>
+#include <linux/reboot.h>
 #include <asm/div64.h>
 #include <asm/x86_init.h>
 #include <asm/hypervisor.h>
@@ -47,6 +49,11 @@
 #define VMWARE_CMD_GETVCPU_INFO  68
 #define VMWARE_CMD_LEGACY_X2APIC  3
 #define VMWARE_CMD_VCPU_RESERVED 31
+#define VMWARE_CMD_STEALCLOCK    91
+
+#define STEALCLOCK_NOT_AVAILABLE (-1)
+#define STEALCLOCK_DISABLED        0
+#define STEALCLOCK_ENABLED         1
 
 #define VMWARE_PORT(cmd, eax, ebx, ecx, edx)				\
 	__asm__("inl (%%dx), %%eax" :					\
@@ -86,6 +93,18 @@
 	}							\
 	} while (0)
 
+struct vmware_steal_time {
+	union {
+		uint64_t clock;	/* stolen time counter in units of vtsc */
+		struct {
+			/* only for little-endian */
+			uint32_t clock_low;
+			uint32_t clock_high;
+		};
+	};
+	uint64_t reserved[7];
+};
+
 static unsigned long vmware_tsc_khz __ro_after_init;
 static u8 vmware_hypercall_mode     __ro_after_init;
 
@@ -104,6 +123,8 @@ static unsigned long vmware_get_tsc_khz(void)
 #ifdef CONFIG_PARAVIRT
 static struct cyc2ns_data vmware_cyc2ns __ro_after_init;
 static int vmw_sched_clock __initdata = 1;
+static DEFINE_PER_CPU_DECRYPTED(struct vmware_steal_time, vmw_steal_time) __aligned(64);
+static bool has_steal_clock;
 
 static __init int setup_vmw_sched_clock(char *s)
 {
@@ -135,6 +156,163 @@ static void __init vmware_cyc2ns_setup(void)
 	pr_info("using clock offset of %llu ns\n", d->cyc2ns_offset);
 }
 
+static int vmware_cmd_stealclock(uint32_t arg1, uint32_t arg2)
+{
+	uint32_t result, info;
+
+	asm volatile (VMWARE_HYPERCALL :
+		"=a"(result),
+		"=c"(info) :
+		"a"(VMWARE_HYPERVISOR_MAGIC),
+		"b"(0),
+		"c"(VMWARE_CMD_STEALCLOCK),
+		"d"(0),
+		"S"(arg1),
+		"D"(arg2) :
+		"memory");
+	return result;
+}
+
+static bool stealclock_enable(phys_addr_t pa)
+{
+	return vmware_cmd_stealclock(upper_32_bits(pa),
+				     lower_32_bits(pa)) == STEALCLOCK_ENABLED;
+}
+
+static int __stealclock_disable(void)
+{
+	return vmware_cmd_stealclock(0, 1);
+}
+
+static void stealclock_disable(void)
+{
+	__stealclock_disable();
+}
+
+static bool vmware_is_stealclock_available(void)
+{
+	return __stealclock_disable() != STEALCLOCK_NOT_AVAILABLE;
+}
+
+/**
+ * vmware_steal_clock() - read the per-cpu steal clock
+ * @cpu:            the cpu number whose steal clock we want to read
+ *
+ * The function reads the steal clock if we are on a 64-bit system, otherwise
+ * reads it in parts, checking that the high part didn't change in the
+ * meantime.
+ *
+ * Return:
+ *      The steal clock reading in ns.
+ */
+static uint64_t vmware_steal_clock(int cpu)
+{
+	struct vmware_steal_time *steal = &per_cpu(vmw_steal_time, cpu);
+	uint64_t clock;
+
+	if (IS_ENABLED(CONFIG_64BIT))
+		clock = READ_ONCE(steal->clock);
+	else {
+		uint32_t initial_high, low, high;
+
+		do {
+			initial_high = READ_ONCE(steal->clock_high);
+			/* Do not reorder initial_high and high readings */
+			virt_rmb();
+			low = READ_ONCE(steal->clock_low);
+			/* Keep low reading in between */
+			virt_rmb();
+			high = READ_ONCE(steal->clock_high);
+		} while (initial_high != high);
+
+		clock = ((uint64_t)high << 32) | low;
+	}
+
+	return mul_u64_u32_shr(clock, vmware_cyc2ns.cyc2ns_mul,
+			     vmware_cyc2ns.cyc2ns_shift);
+}
+
+static void vmware_register_steal_time(void)
+{
+	int cpu = smp_processor_id();
+	struct vmware_steal_time *st = &per_cpu(vmw_steal_time, cpu);
+
+	if (!has_steal_clock)
+		return;
+
+	if (!stealclock_enable(slow_virt_to_phys(st))) {
+		has_steal_clock = false;
+		return;
+	}
+
+	pr_info("vmware-stealtime: cpu %d, pa %llx\n",
+		cpu, (unsigned long long) slow_virt_to_phys(st));
+}
+
+static void vmware_disable_steal_time(void)
+{
+	if (!has_steal_clock)
+		return;
+
+	stealclock_disable();
+}
+
+static void vmware_guest_cpu_init(void)
+{
+	if (has_steal_clock)
+		vmware_register_steal_time();
+}
+
+static void vmware_pv_guest_cpu_reboot(void *unused)
+{
+	vmware_disable_steal_time();
+}
+
+static int vmware_pv_reboot_notify(struct notifier_block *nb,
+				unsigned long code, void *unused)
+{
+	if (code == SYS_RESTART)
+		on_each_cpu(vmware_pv_guest_cpu_reboot, NULL, 1);
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block vmware_pv_reboot_nb = {
+	.notifier_call = vmware_pv_reboot_notify,
+};
+
+#ifdef CONFIG_SMP
+static void __init vmware_smp_prepare_boot_cpu(void)
+{
+	vmware_guest_cpu_init();
+	native_smp_prepare_boot_cpu();
+}
+
+static int vmware_cpu_online(unsigned int cpu)
+{
+	local_irq_disable();
+	vmware_guest_cpu_init();
+	local_irq_enable();
+	return 0;
+}
+
+static int vmware_cpu_down_prepare(unsigned int cpu)
+{
+	local_irq_disable();
+	vmware_disable_steal_time();
+	local_irq_enable();
+	return 0;
+}
+#endif
+
+static __init int activate_jump_labels(void)
+{
+	if (has_steal_clock)
+		static_key_slow_inc(&paravirt_steal_enabled);
+
+	return 0;
+}
+arch_initcall(activate_jump_labels);
+
 static void __init vmware_paravirt_ops_setup(void)
 {
 	pv_info.name = "VMware hypervisor";
@@ -148,6 +326,25 @@ static void __init vmware_paravirt_ops_setup(void)
 	if (vmw_sched_clock)
 		pv_ops.time.sched_clock = vmware_sched_clock;
 
+	if (vmware_is_stealclock_available()) {
+		has_steal_clock = true;
+		pv_ops.time.steal_clock = vmware_steal_clock;
+
+		/* We use reboot notifier only to disable steal clock */
+		register_reboot_notifier(&vmware_pv_reboot_nb);
+
+#ifdef CONFIG_SMP
+		smp_ops.smp_prepare_boot_cpu =
+			vmware_smp_prepare_boot_cpu;
+		if (cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
+					      "x86/vmware:online",
+					      vmware_cpu_online,
+					      vmware_cpu_down_prepare) < 0)
+			pr_err("vmware_guest: Failed to install cpu hotplug callbacks\n");
+#else
+		vmware_guest_cpu_init();
+#endif
+	}
 }
 #else
 #define vmware_paravirt_ops_setup() do {} while (0)

commit dd735f4707e603ac5b541b5f30de87c3c7bd60dd
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Mon Mar 23 19:57:04 2020 +0000

    x86/vmware: Remove vmware_sched_clock_setup()
    
    Move cyc2ns setup logic to separate function.
    This separation will allow to use cyc2ns mult/shift pair
    not only for the sched_clock but also for other clocks
    such as steal_clock.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200323195707.31242-3-amakhalov@vmware.com

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index d280560fd75e..efb22fa76ba4 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -122,7 +122,7 @@ static unsigned long long notrace vmware_sched_clock(void)
 	return ns;
 }
 
-static void __init vmware_sched_clock_setup(void)
+static void __init vmware_cyc2ns_setup(void)
 {
 	struct cyc2ns_data *d = &vmware_cyc2ns;
 	unsigned long long tsc_now = rdtsc();
@@ -132,8 +132,7 @@ static void __init vmware_sched_clock_setup(void)
 	d->cyc2ns_offset = mul_u64_u32_shr(tsc_now, d->cyc2ns_mul,
 					   d->cyc2ns_shift);
 
-	pv_ops.time.sched_clock = vmware_sched_clock;
-	pr_info("using sched offset of %llu ns\n", d->cyc2ns_offset);
+	pr_info("using clock offset of %llu ns\n", d->cyc2ns_offset);
 }
 
 static void __init vmware_paravirt_ops_setup(void)
@@ -141,8 +140,14 @@ static void __init vmware_paravirt_ops_setup(void)
 	pv_info.name = "VMware hypervisor";
 	pv_ops.cpu.io_delay = paravirt_nop;
 
-	if (vmware_tsc_khz && vmw_sched_clock)
-		vmware_sched_clock_setup();
+	if (vmware_tsc_khz == 0)
+		return;
+
+	vmware_cyc2ns_setup();
+
+	if (vmw_sched_clock)
+		pv_ops.time.sched_clock = vmware_sched_clock;
+
 }
 #else
 #define vmware_paravirt_ops_setup() do {} while (0)

commit 14388ae24544c4cc09056773c886839a2c8d256b
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Mon Mar 23 19:57:03 2020 +0000

    x86/vmware: Make vmware_select_hypercall() __init
    
    vmware_select_hypercall() is used only by the __init
    functions, and should be annotated with __init as well.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200323195707.31242-2-amakhalov@vmware.com

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 46d732696c1c..d280560fd75e 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -213,7 +213,7 @@ static void __init vmware_platform_setup(void)
 	vmware_set_capabilities();
 }
 
-static u8 vmware_select_hypercall(void)
+static u8 __init vmware_select_hypercall(void)
 {
 	int eax, ebx, ecx, edx;
 

commit fbcfb8f0270bf24fe348393540b17a7f2ba577d7
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Mon Oct 7 12:21:29 2019 -0700

    x86/cpu/vmware: Use the full form of INL in VMWARE_PORT
    
    LLVM's assembler doesn't accept the short form INL instruction:
    
      inl (%%dx)
    
    but instead insists on the output register to be explicitly specified:
    
      <inline asm>:1:7: error: invalid operand for instruction
              inl (%dx)
                 ^
      LLVM ERROR: Error parsing inline asm
    
    Use the full form of the instruction to fix the build.
    
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Thomas Hellstrom <thellstrom@vmware.com>
    Cc: clang-built-linux@googlegroups.com
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: "VMware, Inc." <pv-drivers@vmware.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://github.com/ClangBuiltLinux/linux/issues/734
    Link: https://lkml.kernel.org/r/20191007192129.104336-1-samitolvanen@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 9735139cfdf8..46d732696c1c 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -49,7 +49,7 @@
 #define VMWARE_CMD_VCPU_RESERVED 31
 
 #define VMWARE_PORT(cmd, eax, ebx, ecx, edx)				\
-	__asm__("inl (%%dx)" :						\
+	__asm__("inl (%%dx), %%eax" :					\
 		"=a"(eax), "=c"(ecx), "=d"(edx), "=b"(ebx) :		\
 		"a"(VMWARE_HYPERVISOR_MAGIC),				\
 		"c"(VMWARE_CMD_##cmd),					\

commit b4dd4f6e3648dfd66576515f2222d885a9a765c0
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Wed Aug 28 10:03:51 2019 +0200

    x86/vmware: Add a header file for hypercall definitions
    
    The new header is intended to be used by drivers using the backdoor.
    Follow the KVM example using alternatives self-patching to choose
    between vmcall, vmmcall and io instructions.
    
    Also define two new CPU feature flags to indicate hypervisor support
    for vmcall- and vmmcall instructions. The new XF86_FEATURE_VMW_VMMCALL
    flag is needed because using XF86_FEATURE_VMMCALL might break QEMU/KVM
    setups using the vmmouse driver. They rely on XF86_FEATURE_VMMCALL
    on AMD to get the kvm_hypercall() right. But they do not yet implement
    vmmcall for the VMware hypercall used by the vmmouse driver.
    
     [ bp: reflow hypercall %edx usage explanation comment. ]
    
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Doug Covelli <dcovelli@vmware.com>
    Cc: Aaron Lewis <aaronlewis@google.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: linux-graphics-maintainer@vmware.com
    Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Cc: Nicolas Ferre <nicolas.ferre@microchip.com>
    Cc: Robert Hoo <robert.hu@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: <pv-drivers@vmware.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190828080353.12658-3-thomas_os@shipmail.org

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 757dded223af..9735139cfdf8 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -30,6 +30,7 @@
 #include <asm/hypervisor.h>
 #include <asm/timer.h>
 #include <asm/apic.h>
+#include <asm/vmware.h>
 
 #undef pr_fmt
 #define pr_fmt(fmt)	"vmware: " fmt
@@ -40,7 +41,6 @@
 #define CPUID_VMWARE_FEATURES_ECX_VMCALL     BIT(1)
 
 #define VMWARE_HYPERVISOR_MAGIC	0x564D5868
-#define VMWARE_HYPERVISOR_PORT	0x5658
 
 #define VMWARE_CMD_GETVERSION    10
 #define VMWARE_CMD_GETHZ         45
@@ -164,6 +164,10 @@ static void __init vmware_set_capabilities(void)
 {
 	setup_force_cpu_cap(X86_FEATURE_CONSTANT_TSC);
 	setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
+	if (vmware_hypercall_mode == CPUID_VMWARE_FEATURES_ECX_VMCALL)
+		setup_force_cpu_cap(X86_FEATURE_VMCALL);
+	else if (vmware_hypercall_mode == CPUID_VMWARE_FEATURES_ECX_VMMCALL)
+		setup_force_cpu_cap(X86_FEATURE_VMW_VMMCALL);
 }
 
 static void __init vmware_platform_setup(void)

commit bac7b4e843232a3a49a042410cf743341eb0887e
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Wed Aug 28 10:03:50 2019 +0200

    x86/vmware: Update platform detection code for VMCALL/VMMCALL hypercalls
    
    Vmware has historically used an INL instruction for this, but recent
    hardware versions support using VMCALL/VMMCALL instead, so use this
    method if supported at platform detection time. Explicitly code separate
    macro versions since the alternatives self-patching has not been
    performed at platform detection time.
    
    Also put tighter constraints on the assembly input parameters.
    
    Co-developed-by: Doug Covelli <dcovelli@vmware.com>
    Signed-off-by: Doug Covelli <dcovelli@vmware.com>
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Doug Covelli <dcovelli@vmware.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-graphics-maintainer@vmware.com
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: <pv-drivers@vmware.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190828080353.12658-2-thomas_os@shipmail.org

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 3c648476d4fb..757dded223af 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -34,30 +34,65 @@
 #undef pr_fmt
 #define pr_fmt(fmt)	"vmware: " fmt
 
-#define CPUID_VMWARE_INFO_LEAF	0x40000000
+#define CPUID_VMWARE_INFO_LEAF               0x40000000
+#define CPUID_VMWARE_FEATURES_LEAF           0x40000010
+#define CPUID_VMWARE_FEATURES_ECX_VMMCALL    BIT(0)
+#define CPUID_VMWARE_FEATURES_ECX_VMCALL     BIT(1)
+
 #define VMWARE_HYPERVISOR_MAGIC	0x564D5868
 #define VMWARE_HYPERVISOR_PORT	0x5658
 
-#define VMWARE_PORT_CMD_GETVERSION	10
-#define VMWARE_PORT_CMD_GETHZ		45
-#define VMWARE_PORT_CMD_GETVCPU_INFO	68
-#define VMWARE_PORT_CMD_LEGACY_X2APIC	3
-#define VMWARE_PORT_CMD_VCPU_RESERVED	31
+#define VMWARE_CMD_GETVERSION    10
+#define VMWARE_CMD_GETHZ         45
+#define VMWARE_CMD_GETVCPU_INFO  68
+#define VMWARE_CMD_LEGACY_X2APIC  3
+#define VMWARE_CMD_VCPU_RESERVED 31
 
 #define VMWARE_PORT(cmd, eax, ebx, ecx, edx)				\
 	__asm__("inl (%%dx)" :						\
-			"=a"(eax), "=c"(ecx), "=d"(edx), "=b"(ebx) :	\
-			"0"(VMWARE_HYPERVISOR_MAGIC),			\
-			"1"(VMWARE_PORT_CMD_##cmd),			\
-			"2"(VMWARE_HYPERVISOR_PORT), "3"(UINT_MAX) :	\
-			"memory");
+		"=a"(eax), "=c"(ecx), "=d"(edx), "=b"(ebx) :		\
+		"a"(VMWARE_HYPERVISOR_MAGIC),				\
+		"c"(VMWARE_CMD_##cmd),					\
+		"d"(VMWARE_HYPERVISOR_PORT), "b"(UINT_MAX) :		\
+		"memory")
+
+#define VMWARE_VMCALL(cmd, eax, ebx, ecx, edx)				\
+	__asm__("vmcall" :						\
+		"=a"(eax), "=c"(ecx), "=d"(edx), "=b"(ebx) :		\
+		"a"(VMWARE_HYPERVISOR_MAGIC),				\
+		"c"(VMWARE_CMD_##cmd),					\
+		"d"(0), "b"(UINT_MAX) :					\
+		"memory")
+
+#define VMWARE_VMMCALL(cmd, eax, ebx, ecx, edx)                         \
+	__asm__("vmmcall" :						\
+		"=a"(eax), "=c"(ecx), "=d"(edx), "=b"(ebx) :		\
+		"a"(VMWARE_HYPERVISOR_MAGIC),				\
+		"c"(VMWARE_CMD_##cmd),					\
+		"d"(0), "b"(UINT_MAX) :					\
+		"memory")
+
+#define VMWARE_CMD(cmd, eax, ebx, ecx, edx) do {		\
+	switch (vmware_hypercall_mode) {			\
+	case CPUID_VMWARE_FEATURES_ECX_VMCALL:			\
+		VMWARE_VMCALL(cmd, eax, ebx, ecx, edx);		\
+		break;						\
+	case CPUID_VMWARE_FEATURES_ECX_VMMCALL:			\
+		VMWARE_VMMCALL(cmd, eax, ebx, ecx, edx);	\
+		break;						\
+	default:						\
+		VMWARE_PORT(cmd, eax, ebx, ecx, edx);		\
+		break;						\
+	}							\
+	} while (0)
 
 static unsigned long vmware_tsc_khz __ro_after_init;
+static u8 vmware_hypercall_mode     __ro_after_init;
 
 static inline int __vmware_platform(void)
 {
 	uint32_t eax, ebx, ecx, edx;
-	VMWARE_PORT(GETVERSION, eax, ebx, ecx, edx);
+	VMWARE_CMD(GETVERSION, eax, ebx, ecx, edx);
 	return eax != (uint32_t)-1 && ebx == VMWARE_HYPERVISOR_MAGIC;
 }
 
@@ -136,7 +171,7 @@ static void __init vmware_platform_setup(void)
 	uint32_t eax, ebx, ecx, edx;
 	uint64_t lpj, tsc_khz;
 
-	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
+	VMWARE_CMD(GETHZ, eax, ebx, ecx, edx);
 
 	if (ebx != UINT_MAX) {
 		lpj = tsc_khz = eax | (((uint64_t)ebx) << 32);
@@ -174,10 +209,21 @@ static void __init vmware_platform_setup(void)
 	vmware_set_capabilities();
 }
 
+static u8 vmware_select_hypercall(void)
+{
+	int eax, ebx, ecx, edx;
+
+	cpuid(CPUID_VMWARE_FEATURES_LEAF, &eax, &ebx, &ecx, &edx);
+	return (ecx & (CPUID_VMWARE_FEATURES_ECX_VMMCALL |
+		       CPUID_VMWARE_FEATURES_ECX_VMCALL));
+}
+
 /*
  * While checking the dmi string information, just checking the product
  * serial key should be enough, as this will always have a VMware
  * specific string when running under VMware hypervisor.
+ * If !boot_cpu_has(X86_FEATURE_HYPERVISOR), vmware_hypercall_mode
+ * intentionally defaults to 0.
  */
 static uint32_t __init vmware_platform(void)
 {
@@ -187,8 +233,16 @@ static uint32_t __init vmware_platform(void)
 
 		cpuid(CPUID_VMWARE_INFO_LEAF, &eax, &hyper_vendor_id[0],
 		      &hyper_vendor_id[1], &hyper_vendor_id[2]);
-		if (!memcmp(hyper_vendor_id, "VMwareVMware", 12))
+		if (!memcmp(hyper_vendor_id, "VMwareVMware", 12)) {
+			if (eax >= CPUID_VMWARE_FEATURES_LEAF)
+				vmware_hypercall_mode =
+					vmware_select_hypercall();
+
+			pr_info("hypercall mode: 0x%02x\n",
+				(unsigned int) vmware_hypercall_mode);
+
 			return CPUID_VMWARE_INFO_LEAF;
+		}
 	} else if (dmi_available && dmi_name_in_serial("VMware") &&
 		   __vmware_platform())
 		return 1;
@@ -200,9 +254,9 @@ static uint32_t __init vmware_platform(void)
 static bool __init vmware_legacy_x2apic_available(void)
 {
 	uint32_t eax, ebx, ecx, edx;
-	VMWARE_PORT(GETVCPU_INFO, eax, ebx, ecx, edx);
-	return (eax & (1 << VMWARE_PORT_CMD_VCPU_RESERVED)) == 0 &&
-	       (eax & (1 << VMWARE_PORT_CMD_LEGACY_X2APIC)) != 0;
+	VMWARE_CMD(GETVCPU_INFO, eax, ebx, ecx, edx);
+	return (eax & (1 << VMWARE_CMD_VCPU_RESERVED)) == 0 &&
+	       (eax & (1 << VMWARE_CMD_LEGACY_X2APIC)) != 0;
 }
 
 const __initconst struct hypervisor_x86 x86_hyper_vmware = {

commit 52ae346bd26c7a8b17ea82e9a09671e98c5402b7
Author: Daniel Drake <drake@endlessm.com>
Date:   Thu May 9 13:54:16 2019 +0800

    x86/apic: Rename 'lapic_timer_frequency' to 'lapic_timer_period'
    
    This variable is a period unit (number of clock cycles per jiffy),
    not a frequency (which is number of cycles per second).
    
    Give it a more appropriate name.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Daniel Drake <drake@endlessm.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: len.brown@intel.com
    Cc: linux@endlessm.com
    Cc: rafael.j.wysocki@intel.com
    Link: http://lkml.kernel.org/r/20190509055417.13152-2-drake@endlessm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 0eda91f8eeac..3c648476d4fb 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -157,7 +157,7 @@ static void __init vmware_platform_setup(void)
 
 #ifdef CONFIG_X86_LOCAL_APIC
 		/* Skip lapic calibration since we know the bus frequency. */
-		lapic_timer_frequency = ecx / HZ;
+		lapic_timer_period = ecx / HZ;
 		pr_info("Host bus clock speed read from hypervisor : %u Hz\n",
 			ecx);
 #endif

commit 15035388439f892017d38b05214d3cda6578af64
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Nov 9 15:22:07 2018 -0500

    x86/cpu/vmware: Do not trace vmware_sched_clock()
    
    When running function tracing on a Linux guest running on VMware
    Workstation, the guest would crash. This is due to tracing of the
    sched_clock internal call of the VMware vmware_sched_clock(), which
    causes an infinite recursion within the tracing code (clock calls must
    not be traced).
    
    Make vmware_sched_clock() not traced by ftrace.
    
    Fixes: 80e9a4f21fd7c ("x86/vmware: Add paravirt sched clock")
    Reported-by: GwanYeong Kim <gy741.kim@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    CC: Alok Kataria <akataria@vmware.com>
    CC: GwanYeong Kim <gy741.kim@gmail.com>
    CC: "H. Peter Anvin" <hpa@zytor.com>
    CC: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: virtualization@lists.linux-foundation.org
    CC: x86-ml <x86@kernel.org>
    Link: http://lkml.kernel.org/r/20181109152207.4d3e7d70@gandalf.local.home

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index d9ab49bed8af..0eda91f8eeac 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -77,7 +77,7 @@ static __init int setup_vmw_sched_clock(char *s)
 }
 early_param("no-vmw-sched-clock", setup_vmw_sched_clock);
 
-static unsigned long long vmware_sched_clock(void)
+static unsigned long long notrace vmware_sched_clock(void)
 {
 	unsigned long long ns;
 

commit 5c83511bdb9832c86be20fb86b783356e2f58062
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:19 2018 +0200

    x86/paravirt: Use a single ops structure
    
    Instead of using six globally visible paravirt ops structures combine
    them in a single structure, keeping the original structures as
    sub-structures.
    
    This avoids the need to assemble struct paravirt_patch_template at
    runtime on the stack each time apply_paravirt() is being called (i.e.
    when loading a module).
    
    [ tglx: Made the struct and the initializer tabular for readability sake ]
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-9-jgross@suse.com

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 8e005329648b..d9ab49bed8af 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -97,14 +97,14 @@ static void __init vmware_sched_clock_setup(void)
 	d->cyc2ns_offset = mul_u64_u32_shr(tsc_now, d->cyc2ns_mul,
 					   d->cyc2ns_shift);
 
-	pv_time_ops.sched_clock = vmware_sched_clock;
+	pv_ops.time.sched_clock = vmware_sched_clock;
 	pr_info("using sched offset of %llu ns\n", d->cyc2ns_offset);
 }
 
 static void __init vmware_paravirt_ops_setup(void)
 {
 	pv_info.name = "VMware hypervisor";
-	pv_cpu_ops.io_delay = paravirt_nop;
+	pv_ops.cpu.io_delay = paravirt_nop;
 
 	if (vmware_tsc_khz && vmw_sched_clock)
 		vmware_sched_clock_setup();

commit 03b2a320b19f1424e9ac9c21696be9c60b6d0d93
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Nov 9 14:27:36 2017 +0100

    x86/virt: Add enum for hypervisors to replace x86_hyper
    
    The x86_hyper pointer is only used for checking whether a virtual
    device is supporting the hypervisor the system is running on.
    
    Use an enum for that purpose instead and drop the x86_hyper pointer.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Xavier Deguillard <xdeguillard@vmware.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: akataria@vmware.com
    Cc: arnd@arndb.de
    Cc: boris.ostrovsky@oracle.com
    Cc: devel@linuxdriverproject.org
    Cc: dmitry.torokhov@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: haiyangz@microsoft.com
    Cc: kvm@vger.kernel.org
    Cc: kys@microsoft.com
    Cc: linux-graphics-maintainer@vmware.com
    Cc: linux-input@vger.kernel.org
    Cc: moltmann@vmware.com
    Cc: pbonzini@redhat.com
    Cc: pv-drivers@vmware.com
    Cc: rkrcmar@redhat.com
    Cc: sthemmin@microsoft.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20171109132739.23465-3-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 4804c1d063c8..8e005329648b 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -205,10 +205,10 @@ static bool __init vmware_legacy_x2apic_available(void)
 	       (eax & (1 << VMWARE_PORT_CMD_LEGACY_X2APIC)) != 0;
 }
 
-const __refconst struct hypervisor_x86 x86_hyper_vmware = {
+const __initconst struct hypervisor_x86 x86_hyper_vmware = {
 	.name			= "VMware",
 	.detect			= vmware_platform,
+	.type			= X86_HYPER_VMWARE,
 	.init.init_platform	= vmware_platform_setup,
 	.init.x2apic_available	= vmware_legacy_x2apic_available,
 };
-EXPORT_SYMBOL(x86_hyper_vmware);

commit f72e38e8ec8869ac0ba5a75d7d2f897d98a1454e
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Nov 9 14:27:35 2017 +0100

    x86/virt, x86/platform: Merge 'struct x86_hyper' into 'struct x86_platform' and 'struct x86_init'
    
    Instead of x86_hyper being either NULL on bare metal or a pointer to a
    struct hypervisor_x86 in case of the kernel running as a guest merge
    the struct into x86_platform and x86_init.
    
    This will remove the need for wrappers making it hard to find out what
    is being called. With dummy functions added for all callbacks testing
    for a NULL function pointer can be removed, too.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: akataria@vmware.com
    Cc: boris.ostrovsky@oracle.com
    Cc: devel@linuxdriverproject.org
    Cc: haiyangz@microsoft.com
    Cc: kvm@vger.kernel.org
    Cc: kys@microsoft.com
    Cc: pbonzini@redhat.com
    Cc: rkrcmar@redhat.com
    Cc: rusty@rustcorp.com.au
    Cc: sthemmin@microsoft.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20171109132739.23465-2-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 40ed26852ebd..4804c1d063c8 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -208,7 +208,7 @@ static bool __init vmware_legacy_x2apic_available(void)
 const __refconst struct hypervisor_x86 x86_hyper_vmware = {
 	.name			= "VMware",
 	.detect			= vmware_platform,
-	.init_platform		= vmware_platform_setup,
-	.x2apic_available	= vmware_legacy_x2apic_available,
+	.init.init_platform	= vmware_platform_setup,
+	.init.x2apic_available	= vmware_legacy_x2apic_available,
 };
 EXPORT_SYMBOL(x86_hyper_vmware);

commit d40342a2ac035444897e5952ea72a50440a2a028
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Apr 13 09:37:20 2017 +0200

    vmware: set cpu capabilities during platform initialization
    
    There is no need to set the same capabilities for each cpu
    individually. This can be done for all cpus in platform initialization.
    
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Acked-by: Alok Kataria <akataria@vmware.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 22403a28caf5..40ed26852ebd 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -113,6 +113,24 @@ static void __init vmware_paravirt_ops_setup(void)
 #define vmware_paravirt_ops_setup() do {} while (0)
 #endif
 
+/*
+ * VMware hypervisor takes care of exporting a reliable TSC to the guest.
+ * Still, due to timing difference when running on virtual cpus, the TSC can
+ * be marked as unstable in some cases. For example, the TSC sync check at
+ * bootup can fail due to a marginal offset between vcpus' TSCs (though the
+ * TSCs do not drift from each other).  Also, the ACPI PM timer clocksource
+ * is not suitable as a watchdog when running on a hypervisor because the
+ * kernel may miss a wrap of the counter if the vcpu is descheduled for a
+ * long time. To skip these checks at runtime we set these capability bits,
+ * so that the kernel could just trust the hypervisor with providing a
+ * reliable virtual TSC that is suitable for timekeeping.
+ */
+static void __init vmware_set_capabilities(void)
+{
+	setup_force_cpu_cap(X86_FEATURE_CONSTANT_TSC);
+	setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
+}
+
 static void __init vmware_platform_setup(void)
 {
 	uint32_t eax, ebx, ecx, edx;
@@ -152,6 +170,8 @@ static void __init vmware_platform_setup(void)
 #ifdef CONFIG_X86_IO_APIC
 	no_timer_check = 1;
 #endif
+
+	vmware_set_capabilities();
 }
 
 /*
@@ -176,24 +196,6 @@ static uint32_t __init vmware_platform(void)
 	return 0;
 }
 
-/*
- * VMware hypervisor takes care of exporting a reliable TSC to the guest.
- * Still, due to timing difference when running on virtual cpus, the TSC can
- * be marked as unstable in some cases. For example, the TSC sync check at
- * bootup can fail due to a marginal offset between vcpus' TSCs (though the
- * TSCs do not drift from each other).  Also, the ACPI PM timer clocksource
- * is not suitable as a watchdog when running on a hypervisor because the
- * kernel may miss a wrap of the counter if the vcpu is descheduled for a
- * long time. To skip these checks at runtime we set these capability bits,
- * so that the kernel could just trust the hypervisor with providing a
- * reliable virtual TSC that is suitable for timekeeping.
- */
-static void vmware_set_cpu_features(struct cpuinfo_x86 *c)
-{
-	set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
-	set_cpu_cap(c, X86_FEATURE_TSC_RELIABLE);
-}
-
 /* Checks if hypervisor supports x2apic without VT-D interrupt remapping. */
 static bool __init vmware_legacy_x2apic_available(void)
 {
@@ -206,7 +208,6 @@ static bool __init vmware_legacy_x2apic_available(void)
 const __refconst struct hypervisor_x86 x86_hyper_vmware = {
 	.name			= "VMware",
 	.detect			= vmware_platform,
-	.set_cpu_features	= vmware_set_cpu_features,
 	.init_platform		= vmware_platform_setup,
 	.x2apic_available	= vmware_legacy_x2apic_available,
 };

commit aa5ec3f715d576353c7d8f4f8085634bd845b73f
Author: Masanari Iida <standby24x7@gmail.com>
Date:   Mon Feb 27 21:29:22 2017 +0900

    x86/vmware: Remove duplicate inclusion of asm/timer.h
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>
    Cc: akataria@vmware.com
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20170227122922.26230-1-standby24x7@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 891f4dad7b2c..22403a28caf5 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -30,7 +30,6 @@
 #include <asm/hypervisor.h>
 #include <asm/timer.h>
 #include <asm/apic.h>
-#include <asm/timer.h>
 
 #undef pr_fmt
 #define pr_fmt(fmt)	"vmware: " fmt

commit f797484c26300fec842fb669c69a3a60eb66e240
Merge: 991bc3625445 80e9a4f21fd7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 15:29:06 2016 -0800

    Merge branch 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 platform updates from Ingo Molnar:
     "Two changes:
    
       - implement various VMWare guest OS improvements/fixes (Alexey
         Makhalov)
    
       - unexport a spurious export from the intel-mid platform driver
         (Lukas Wunner)"
    
    * 'x86-platform-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vmware: Add paravirt sched clock
      x86/vmware: Add basic paravirt ops support
      x86/vmware: Use tsc_khz value for calibrate_cpu()
      x86/platform/intel-mid: Unexport intel_mid_pci_set_power_state()
      x86/vmware: Read tsc_khz only once at boot time

commit 80e9a4f21fd7ccce7e9b8439986fd028c9946dda
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Fri Oct 28 00:54:32 2016 -0700

    x86/vmware: Add paravirt sched clock
    
    The default sched_clock() implementation is native_sched_clock(). It
    contains code to handle non constant frequency TSCs, which creates
    overhead for systems with constant frequency TSCs.
    
    The vmware hypervisor guarantees a constant frequency TSC, so
    native_sched_clock() is not required and slower than a dedicated function
    which operates with one time calculated conversion factors.
    
    Calculate the conversion factors at boot time from the tsc frequency and
    install an optimized sched_clock() function via paravirt ops.
    
    The paravirtualized clock can be disabled on the kernel command line with
    the new 'no-vmw-sched-clock' option.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Acked-by: Alok N Kataria <akataria@vmware.com>
    Cc: linux-doc@vger.kernel.org
    Cc: pv-drivers@vmware.com
    Cc: corbet@lwn.net
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20161028075432.90579-4-amakhalov@vmware.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 098a524a1646..cdbe38be28fd 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -24,10 +24,15 @@
 #include <linux/dmi.h>
 #include <linux/init.h>
 #include <linux/export.h>
+#include <linux/clocksource.h>
 #include <asm/div64.h>
 #include <asm/x86_init.h>
 #include <asm/hypervisor.h>
 #include <asm/apic.h>
+#include <asm/timer.h>
+
+#undef pr_fmt
+#define pr_fmt(fmt)	"vmware: " fmt
 
 #define CPUID_VMWARE_INFO_LEAF	0x40000000
 #define VMWARE_HYPERVISOR_MAGIC	0x564D5868
@@ -62,10 +67,47 @@ static unsigned long vmware_get_tsc_khz(void)
 }
 
 #ifdef CONFIG_PARAVIRT
+static struct cyc2ns_data vmware_cyc2ns __ro_after_init;
+static int vmw_sched_clock __initdata = 1;
+
+static __init int setup_vmw_sched_clock(char *s)
+{
+	vmw_sched_clock = 0;
+	return 0;
+}
+early_param("no-vmw-sched-clock", setup_vmw_sched_clock);
+
+static unsigned long long vmware_sched_clock(void)
+{
+	unsigned long long ns;
+
+	ns = mul_u64_u32_shr(rdtsc(), vmware_cyc2ns.cyc2ns_mul,
+			     vmware_cyc2ns.cyc2ns_shift);
+	ns -= vmware_cyc2ns.cyc2ns_offset;
+	return ns;
+}
+
+static void __init vmware_sched_clock_setup(void)
+{
+	struct cyc2ns_data *d = &vmware_cyc2ns;
+	unsigned long long tsc_now = rdtsc();
+
+	clocks_calc_mult_shift(&d->cyc2ns_mul, &d->cyc2ns_shift,
+			       vmware_tsc_khz, NSEC_PER_MSEC, 0);
+	d->cyc2ns_offset = mul_u64_u32_shr(tsc_now, d->cyc2ns_mul,
+					   d->cyc2ns_shift);
+
+	pv_time_ops.sched_clock = vmware_sched_clock;
+	pr_info("using sched offset of %llu ns\n", d->cyc2ns_offset);
+}
+
 static void __init vmware_paravirt_ops_setup(void)
 {
 	pv_info.name = "VMware hypervisor";
 	pv_cpu_ops.io_delay = paravirt_nop;
+
+	if (vmware_tsc_khz && vmw_sched_clock)
+		vmware_sched_clock_setup();
 }
 #else
 #define vmware_paravirt_ops_setup() do {} while (0)

commit 91d1e54ebd1615d216b7f57324a5e69166a344e0
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Fri Oct 28 00:54:31 2016 -0700

    x86/vmware: Add basic paravirt ops support
    
    Add basic paravirt support:
    
     1. Set pv_info.name to "VMware hypervisor" to have proper boot log message
            Booting paravirtualized kernel on VMware hypervisor
        instead of "... on bare hardware"
    
     2. Set pv_cpu_ops.io_delay() to empty function - paravirt_noop() to
        avoid vm-exits on IO delays because io delays they are not required.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Acked-by: Alok N Kataria <akataria@vmware.com>
    Cc: linux-doc@vger.kernel.org
    Cc: pv-drivers@vmware.com
    Cc: corbet@lwn.net
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20161028075432.90579-3-amakhalov@vmware.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 480790fe2463..098a524a1646 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -61,6 +61,16 @@ static unsigned long vmware_get_tsc_khz(void)
 	return vmware_tsc_khz;
 }
 
+#ifdef CONFIG_PARAVIRT
+static void __init vmware_paravirt_ops_setup(void)
+{
+	pv_info.name = "VMware hypervisor";
+	pv_cpu_ops.io_delay = paravirt_nop;
+}
+#else
+#define vmware_paravirt_ops_setup() do {} while (0)
+#endif
+
 static void __init vmware_platform_setup(void)
 {
 	uint32_t eax, ebx, ecx, edx;
@@ -94,6 +104,8 @@ static void __init vmware_platform_setup(void)
 	} else {
 		pr_warn("Failed to get TSC freq from the hypervisor\n");
 	}
+
+	vmware_paravirt_ops_setup();
 }
 
 /*

commit 687bca8d664ac9b098005b57846773eb62040ae0
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Fri Oct 28 00:54:30 2016 -0700

    x86/vmware: Use tsc_khz value for calibrate_cpu()
    
    Commit aa297292d708 ("x86/tsc: Enumerate SKL cpu_khz and tsc_khz via
    CPUID") separated the calibration mechanisms for cpu_khz and tsc_khz.
    
    Since the vmware hypervisor provides a constant frequency TSC to the guest,
    this change can lead to divergence between the tsc and the cpu frequency
    after vmotion, which might confuse the user.
    
    Solve this by overriding the x86 platform cpu calibration callback with the
    vmware specific tsc calibration function.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Acked-by: Alok N Kataria <akataria@vmware.com>
    Cc: linux-doc@vger.kernel.org
    Cc: pv-drivers@vmware.com
    Cc: corbet@lwn.net
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20161028075432.90579-2-amakhalov@vmware.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 4e34da4be297..480790fe2463 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -83,6 +83,7 @@ static void __init vmware_platform_setup(void)
 
 		vmware_tsc_khz = tsc_khz;
 		x86_platform.calibrate_tsc = vmware_get_tsc_khz;
+		x86_platform.calibrate_cpu = vmware_get_tsc_khz;
 
 #ifdef CONFIG_X86_LOCAL_APIC
 		/* Skip lapic calibration since we know the bus frequency. */

commit cf11372949434fc4a6194b1ed9eb861d08032d08
Author: Alexey Makhalov <amakhalov@vmware.com>
Date:   Wed Oct 19 22:02:11 2016 -0700

    x86/vmware: Read tsc_khz only once at boot time
    
    Re-factor the vmware platform setup code to query the hypervisor for tsc
    frequency only once during boot. Since the VMware hypervisor guarantees
    constant TSC, calibrate_tsc now uses the saved value.
    
    Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
    Acked-by: Alok N Kataria <akataria@vmware.com>
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20161020050211.GA25304@amakhalov-virtual-machine
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 81160578b91a..4e34da4be297 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -47,6 +47,8 @@
 			"2"(VMWARE_HYPERVISOR_PORT), "3"(UINT_MAX) :	\
 			"memory");
 
+static unsigned long vmware_tsc_khz __ro_after_init;
+
 static inline int __vmware_platform(void)
 {
 	uint32_t eax, ebx, ecx, edx;
@@ -56,35 +58,32 @@ static inline int __vmware_platform(void)
 
 static unsigned long vmware_get_tsc_khz(void)
 {
-	uint64_t tsc_hz, lpj;
-	uint32_t eax, ebx, ecx, edx;
-
-	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
-
-	tsc_hz = eax | (((uint64_t)ebx) << 32);
-	do_div(tsc_hz, 1000);
-	BUG_ON(tsc_hz >> 32);
-	pr_info("TSC freq read from hypervisor : %lu.%03lu MHz\n",
-			 (unsigned long) tsc_hz / 1000,
-			 (unsigned long) tsc_hz % 1000);
-
-	if (!preset_lpj) {
-		lpj = ((u64)tsc_hz * 1000);
-		do_div(lpj, HZ);
-		preset_lpj = lpj;
-	}
-
-	return tsc_hz;
+	return vmware_tsc_khz;
 }
 
 static void __init vmware_platform_setup(void)
 {
 	uint32_t eax, ebx, ecx, edx;
+	uint64_t lpj, tsc_khz;
 
 	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
 
 	if (ebx != UINT_MAX) {
+		lpj = tsc_khz = eax | (((uint64_t)ebx) << 32);
+		do_div(tsc_khz, 1000);
+		WARN_ON(tsc_khz >> 32);
+		pr_info("TSC freq read from hypervisor : %lu.%03lu MHz\n",
+			(unsigned long) tsc_khz / 1000,
+			(unsigned long) tsc_khz % 1000);
+
+		if (!preset_lpj) {
+			do_div(lpj, HZ);
+			preset_lpj = lpj;
+		}
+
+		vmware_tsc_khz = tsc_khz;
 		x86_platform.calibrate_tsc = vmware_get_tsc_khz;
+
 #ifdef CONFIG_X86_LOCAL_APIC
 		/* Skip lapic calibration since we know the bus frequency. */
 		lapic_timer_frequency = ecx / HZ;

commit 854dd54245f7f1b1175b1bada613929396a571be
Author: Renat Valiullin <rvaliullin@vmware.com>
Date:   Thu Oct 13 11:45:39 2016 -0700

    x86/vmware: Skip timer_irq_works() check on VMware
    
    The timer_irq_works() boot check may sometimes fail in a VM, when
    the Host is overcommitted or when the Guest is running nested.
    
    Since the intended check is unnecessary on VMware's virtual
    hardware, by-pass it.
    
    Signed-off-by: Renat Valiullin <rvaliullin@vmware.com>
    Acked-by: Alok N Kataria <akataria@vmware.com>
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20161013184539.GA11497@rvaliullin-vm
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 81160578b91a..5130985b758b 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -27,6 +27,7 @@
 #include <asm/div64.h>
 #include <asm/x86_init.h>
 #include <asm/hypervisor.h>
+#include <asm/timer.h>
 #include <asm/apic.h>
 
 #define CPUID_VMWARE_INFO_LEAF	0x40000000
@@ -94,6 +95,10 @@ static void __init vmware_platform_setup(void)
 	} else {
 		pr_warn("Failed to get TSC freq from the hypervisor\n");
 	}
+
+#ifdef CONFIG_X86_IO_APIC
+	no_timer_check = 1;
+#endif
 }
 
 /*

commit b91688f528fe96e09d17e6d87c1b2805eb0c445e
Author: Renat Valiullin <rvaliullin@vmware.com>
Date:   Tue Oct 4 13:11:48 2016 -0700

    x86/vmware: Skip lapic calibration on VMware
    
    In a virtualized environment the APIC timer calibration can go wrong when
    the host is overcommitted or the guest is running nested. This results
    in the APIC timers operating at an incorrect frequency.
    
    Since VMware supports a mechanism to retrieve the local APIC frequency we
    can ask the hypervisor for it and skip the APIC calibration loop.
    
    Signed-off-by: Renat Valiullin <rvaliullin@vmware.com>
    Acked-by: Alok N Kataria <akataria@vmware.com>
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/20161004201148.GA1421@uu64vm
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 1ff0598d309c..81160578b91a 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -27,6 +27,7 @@
 #include <asm/div64.h>
 #include <asm/x86_init.h>
 #include <asm/hypervisor.h>
+#include <asm/apic.h>
 
 #define CPUID_VMWARE_INFO_LEAF	0x40000000
 #define VMWARE_HYPERVISOR_MAGIC	0x564D5868
@@ -82,10 +83,17 @@ static void __init vmware_platform_setup(void)
 
 	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
 
-	if (ebx != UINT_MAX)
+	if (ebx != UINT_MAX) {
 		x86_platform.calibrate_tsc = vmware_get_tsc_khz;
-	else
+#ifdef CONFIG_X86_LOCAL_APIC
+		/* Skip lapic calibration since we know the bus frequency. */
+		lapic_timer_frequency = ecx / HZ;
+		pr_info("Host bus clock speed read from hypervisor : %u Hz\n",
+			ecx);
+#endif
+	} else {
 		pr_warn("Failed to get TSC freq from the hypervisor\n");
+	}
 }
 
 /*

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 8cac429b6a1d..1ff0598d309c 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -22,7 +22,8 @@
  */
 
 #include <linux/dmi.h>
-#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/export.h>
 #include <asm/div64.h>
 #include <asm/x86_init.h>
 #include <asm/hypervisor.h>

commit 0c9f3536cc712dfd5ec3127d55cd7b807cc0adb5
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:41:55 2016 +0200

    x86/cpufeature: Remove cpu_has_hypervisor
    
    Use boot_cpu_has() instead.
    
    Tested-by: David Kershner <david.kershner@unisys.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: sparmaintainer@unisys.com
    Cc: virtualization@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/1459266123-21878-3-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 364e58346897..8cac429b6a1d 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -94,7 +94,7 @@ static void __init vmware_platform_setup(void)
  */
 static uint32_t __init vmware_platform(void)
 {
-	if (cpu_has_hypervisor) {
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {
 		unsigned int eax;
 		unsigned int hyper_vendor_id[3];
 

commit 1b74dde7c47c19a73ea3e9fac95ac27b5d3d50c5
Author: Chen Yucong <slaoub@gmail.com>
Date:   Tue Feb 2 11:45:02 2016 +0800

    x86/cpu: Convert printk(KERN_<LEVEL> ...) to pr_<level>(...)
    
     - Use the more current logging style pr_<level>(...) instead of the old
       printk(KERN_<LEVEL> ...).
    
     - Convert pr_warning() to pr_warn().
    
    Signed-off-by: Chen Yucong <slaoub@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454384702-21707-1-git-send-email-slaoub@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 628a059a9a06..364e58346897 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -62,7 +62,7 @@ static unsigned long vmware_get_tsc_khz(void)
 	tsc_hz = eax | (((uint64_t)ebx) << 32);
 	do_div(tsc_hz, 1000);
 	BUG_ON(tsc_hz >> 32);
-	printk(KERN_INFO "TSC freq read from hypervisor : %lu.%03lu MHz\n",
+	pr_info("TSC freq read from hypervisor : %lu.%03lu MHz\n",
 			 (unsigned long) tsc_hz / 1000,
 			 (unsigned long) tsc_hz % 1000);
 
@@ -84,8 +84,7 @@ static void __init vmware_platform_setup(void)
 	if (ebx != UINT_MAX)
 		x86_platform.calibrate_tsc = vmware_get_tsc_khz;
 	else
-		printk(KERN_WARNING
-		       "Failed to get TSC freq from the hypervisor\n");
+		pr_warn("Failed to get TSC freq from the hypervisor\n");
 }
 
 /*

commit 9df56f19a500bea90d160be1bf77e4fbcd204d3f
Author: Jason Wang <jasowang@redhat.com>
Date:   Thu Jul 25 16:54:35 2013 +0800

    x86: Correctly detect hypervisor
    
    We try to handle the hypervisor compatibility mode by detecting hypervisor
    through a specific order. This is not robust, since hypervisors may implement
    each others features.
    
    This patch tries to handle this situation by always choosing the last one in the
    CPUID leaves. This is done by letting .detect() return a priority instead of
    true/false and just re-using the CPUID leaf where the signature were found as
    the priority (or 1 if it was found by DMI). Then we can just pick hypervisor who
    has the highest priority. Other sophisticated detection method could also be
    implemented on top.
    
    Suggested by H. Peter Anvin and Paolo Bonzini.
    
    Acked-by: K. Y. Srinivasan <kys@microsoft.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Doug Covelli <dcovelli@vmware.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dan Hecht <dhecht@vmware.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jason Wang <jasowang@redhat.com>
    Link: http://lkml.kernel.org/r/1374742475-2485-4-git-send-email-jasowang@redhat.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 7076878404ec..628a059a9a06 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -93,7 +93,7 @@ static void __init vmware_platform_setup(void)
  * serial key should be enough, as this will always have a VMware
  * specific string when running under VMware hypervisor.
  */
-static bool __init vmware_platform(void)
+static uint32_t __init vmware_platform(void)
 {
 	if (cpu_has_hypervisor) {
 		unsigned int eax;
@@ -102,12 +102,12 @@ static bool __init vmware_platform(void)
 		cpuid(CPUID_VMWARE_INFO_LEAF, &eax, &hyper_vendor_id[0],
 		      &hyper_vendor_id[1], &hyper_vendor_id[2]);
 		if (!memcmp(hyper_vendor_id, "VMwareVMware", 12))
-			return true;
+			return CPUID_VMWARE_INFO_LEAF;
 	} else if (dmi_available && dmi_name_in_serial("VMware") &&
 		   __vmware_platform())
-		return true;
+		return 1;
 
-	return false;
+	return 0;
 }
 
 /*

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 03a36321ec54..7076878404ec 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -122,7 +122,7 @@ static bool __init vmware_platform(void)
  * so that the kernel could just trust the hypervisor with providing a
  * reliable virtual TSC that is suitable for timekeeping.
  */
-static void __cpuinit vmware_set_cpu_features(struct cpuinfo_x86 *c)
+static void vmware_set_cpu_features(struct cpuinfo_x86 *c)
 {
 	set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 	set_cpu_cap(c, X86_FEATURE_TSC_RELIABLE);

commit 4cca6ea04d31c22a7d0436949c072b27bde41f86
Author: Alok N Kataria <akataria@vmware.com>
Date:   Thu Jan 17 15:44:42 2013 -0800

    x86/apic: Allow x2apic without IR on VMware platform
    
    This patch updates x2apic initializaition code to allow x2apic
    on VMware platform even without interrupt remapping support.
    The hypervisor_x2apic_available hook was added in x2apic
    initialization code and used by KVM and XEN, before this.
    I have also cleaned up that code to export this hook through the
    hypervisor_x86 structure.
    
    Compile tested for KVM and XEN configs, this patch doesn't have
    any functional effect on those two platforms.
    
    On VMware platform, verified that x2apic is used in physical
    mode on products that support this.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Reviewed-by: Doug Covelli <dcovelli@vmware.com>
    Reviewed-by: Dan Hecht <dhecht@vmware.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Avi Kivity <avi@redhat.com>
    Link: http://lkml.kernel.org/r/1358466282.423.60.camel@akataria-dtop.eng.vmware.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index d22d0c4edcfd..03a36321ec54 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -33,6 +33,9 @@
 
 #define VMWARE_PORT_CMD_GETVERSION	10
 #define VMWARE_PORT_CMD_GETHZ		45
+#define VMWARE_PORT_CMD_GETVCPU_INFO	68
+#define VMWARE_PORT_CMD_LEGACY_X2APIC	3
+#define VMWARE_PORT_CMD_VCPU_RESERVED	31
 
 #define VMWARE_PORT(cmd, eax, ebx, ecx, edx)				\
 	__asm__("inl (%%dx)" :						\
@@ -125,10 +128,20 @@ static void __cpuinit vmware_set_cpu_features(struct cpuinfo_x86 *c)
 	set_cpu_cap(c, X86_FEATURE_TSC_RELIABLE);
 }
 
+/* Checks if hypervisor supports x2apic without VT-D interrupt remapping. */
+static bool __init vmware_legacy_x2apic_available(void)
+{
+	uint32_t eax, ebx, ecx, edx;
+	VMWARE_PORT(GETVCPU_INFO, eax, ebx, ecx, edx);
+	return (eax & (1 << VMWARE_PORT_CMD_VCPU_RESERVED)) == 0 &&
+	       (eax & (1 << VMWARE_PORT_CMD_LEGACY_X2APIC)) != 0;
+}
+
 const __refconst struct hypervisor_x86 x86_hyper_vmware = {
 	.name			= "VMware",
 	.detect			= vmware_platform,
 	.set_cpu_features	= vmware_set_cpu_features,
 	.init_platform		= vmware_platform_setup,
+	.x2apic_available	= vmware_legacy_x2apic_available,
 };
 EXPORT_SYMBOL(x86_hyper_vmware);

commit 0d2eb44f631d9d0a826efa3156f157477fdaecf4
Author: Lucas De Marchi <lucas.de.marchi@gmail.com>
Date:   Thu Mar 17 16:24:16 2011 -0300

    x86: Fix common misspellings
    
    They were generated by 'codespell' and then manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>
    Cc: trivial@kernel.org
    LKML-Reference: <1300389856-1099-3-git-send-email-lucas.demarchi@profusion.mobi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 227b0448960d..d22d0c4edcfd 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -86,7 +86,7 @@ static void __init vmware_platform_setup(void)
 }
 
 /*
- * While checking the dmi string infomation, just checking the product
+ * While checking the dmi string information, just checking the product
  * serial key should be enough, as this will always have a VMware
  * specific string when running under VMware hypervisor.
  */

commit 9f242dc10e0c3c1eb32d8c83c18650a35fd7f80d
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Aug 2 16:10:37 2010 -0700

    x86, vmware: Preset lpj values when on VMware.
    
    When running on VMware's platform, we have seen situations where
    the AP's try to calibrate the lpj values and fail to get good calibration
    runs becasue of timing issues. As a result delays don't work correctly
    on all cpus.
    
    The solutions is to set preset_lpj value based on the current tsc frequency
    value. This is similar to what KVM does as well.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    LKML-Reference: <1280790637.14933.29.camel@ank32.eng.vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index b9d1ff588445..227b0448960d 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -51,7 +51,7 @@ static inline int __vmware_platform(void)
 
 static unsigned long vmware_get_tsc_khz(void)
 {
-	uint64_t tsc_hz;
+	uint64_t tsc_hz, lpj;
 	uint32_t eax, ebx, ecx, edx;
 
 	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
@@ -62,6 +62,13 @@ static unsigned long vmware_get_tsc_khz(void)
 	printk(KERN_INFO "TSC freq read from hypervisor : %lu.%03lu MHz\n",
 			 (unsigned long) tsc_hz / 1000,
 			 (unsigned long) tsc_hz % 1000);
+
+	if (!preset_lpj) {
+		lpj = ((u64)tsc_hz * 1000);
+		do_div(lpj, HZ);
+		preset_lpj = lpj;
+	}
+
 	return tsc_hz;
 }
 

commit 96f6e775b58687d85ee33004d414419b5ec34106
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sun May 9 01:10:34 2010 -0700

    x86, hypervisor: Export the x86_hyper* symbols
    
    Export x86_hyper and the related specific structures, allowing for
    hypervisor identification by modules.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Greg KH <greg@kroah.com>
    Cc: Hank Janssen <hjanssen@microsoft.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Ky Srinivasan <ksrinivasan@novell.com>
    Cc: Dmitry Torokhov <dtor@vmware.com>
    LKML-Reference: <4BE49778.6060800@zytor.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 265b432f6e64..b9d1ff588445 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -99,7 +99,6 @@ static bool __init vmware_platform(void)
 
 	return false;
 }
-EXPORT_SYMBOL(vmware_platform);
 
 /*
  * VMware hypervisor takes care of exporting a reliable TSC to the guest.
@@ -125,3 +124,4 @@ const __refconst struct hypervisor_x86 x86_hyper_vmware = {
 	.set_cpu_features	= vmware_set_cpu_features,
 	.init_platform		= vmware_platform_setup,
 };
+EXPORT_SYMBOL(x86_hyper_vmware);

commit d7be0ce6afb1df60bc786f57410407ceae92b994
Merge: e08cae4181af 66f41d4c5c8a
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sat May 8 14:59:58 2010 -0700

    Merge commit 'v2.6.34-rc6' into x86/cpu

commit e08cae4181af9483b04ecfac48f01c8e5a5f27bf
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri May 7 16:57:28 2010 -0700

    x86: Clean up the hypervisor layer
    
    Clean up the hypervisor layer and the hypervisor drivers, using an ops
    structure instead of an enumeration with if statements.
    
    The identity of the hypervisor, if needed, can be tested by testing
    the pointer value in x86_hyper.
    
    The MS-HyperV private state is moved into a normal global variable
    (it's per-system state, not per-CPU state).  Being a normal bss
    variable, it will be left at all zero on non-HyperV platforms, and so
    can generally be tested for HyperV-specific features without
    additional qualification.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Greg KH <greg@kroah.com>
    Cc: Hank Janssen <hjanssen@microsoft.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Ky Srinivasan <ksrinivasan@novell.com>
    LKML-Reference: <4BE49778.6060800@zytor.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 1cbed97b59cf..46a5b5d3ba5e 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -23,8 +23,8 @@
 
 #include <linux/dmi.h>
 #include <asm/div64.h>
-#include <asm/vmware.h>
 #include <asm/x86_init.h>
+#include <asm/hypervisor.h>
 
 #define CPUID_VMWARE_INFO_LEAF	0x40000000
 #define VMWARE_HYPERVISOR_MAGIC	0x564D5868
@@ -64,7 +64,7 @@ static unsigned long vmware_get_tsc_khz(void)
 	return tsc_hz;
 }
 
-void __init vmware_platform_setup(void)
+static void __init vmware_platform_setup(void)
 {
 	uint32_t eax, ebx, ecx, edx;
 
@@ -82,24 +82,21 @@ void __init vmware_platform_setup(void)
  * serial key should be enough, as this will always have a VMware
  * specific string when running under VMware hypervisor.
  */
-int vmware_platform(void)
+static bool __init vmware_platform(void)
 {
 	if (cpu_has_hypervisor) {
-		unsigned int eax, ebx, ecx, edx;
-		char hyper_vendor_id[13];
-
-		cpuid(CPUID_VMWARE_INFO_LEAF, &eax, &ebx, &ecx, &edx);
-		memcpy(hyper_vendor_id + 0, &ebx, 4);
-		memcpy(hyper_vendor_id + 4, &ecx, 4);
-		memcpy(hyper_vendor_id + 8, &edx, 4);
-		hyper_vendor_id[12] = '\0';
-		if (!strcmp(hyper_vendor_id, "VMwareVMware"))
-			return 1;
+		unsigned int eax;
+		unsigned int hyper_vendor_id[3];
+
+		cpuid(CPUID_VMWARE_INFO_LEAF, &eax, &hyper_vendor_id[0],
+		      &hyper_vendor_id[1], &hyper_vendor_id[2]);
+		if (!memcmp(hyper_vendor_id, "VMwareVMware", 12))
+			return true;
 	} else if (dmi_available && dmi_name_in_serial("VMware") &&
 		   __vmware_platform())
-		return 1;
+		return true;
 
-	return 0;
+	return false;
 }
 
 /*
@@ -114,8 +111,15 @@ int vmware_platform(void)
  * so that the kernel could just trust the hypervisor with providing a
  * reliable virtual TSC that is suitable for timekeeping.
  */
-void __cpuinit vmware_set_feature_bits(struct cpuinfo_x86 *c)
+static void __cpuinit vmware_set_cpu_features(struct cpuinfo_x86 *c)
 {
 	set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 	set_cpu_cap(c, X86_FEATURE_TSC_RELIABLE);
 }
+
+const __refconst struct hypervisor_x86 x86_hyper_vmware = {
+	.name			= "VMware",
+	.detect			= vmware_platform,
+	.set_cpu_features	= vmware_set_cpu_features,
+	.init_platform		= vmware_platform_setup,
+};

commit 453dc65931915abc61f92e12bba1fc4747ff5542
Author: Dmitry Torokhov <dtor@vmware.com>
Date:   Fri Apr 23 13:18:08 2010 -0400

    VMware Balloon driver
    
    This is a standalone version of VMware Balloon driver.  Ballooning is a
    technique that allows hypervisor dynamically limit the amount of memory
    available to the guest (with guest cooperation).  In the overcommit
    scenario, when hypervisor set detects that it needs to shuffle some
    memory, it instructs the driver to allocate certain number of pages, and
    the underlying memory gets returned to the hypervisor.  Later hypervisor
    may return memory to the guest by reattaching memory to the pageframes and
    instructing the driver to "deflate" balloon.
    
    We are submitting a standalone driver because KVM maintainer (Avi Kivity)
    expressed opinion (rightly) that our transport does not fit well into
    virtqueue paradigm and thus it does not make much sense to integrate with
    virtio.
    
    There were also some concerns whether current ballooning technique is the
    right thing.  If there appears a better framework to achieve this we are
    prepared to evaluate and switch to using it, but in the meantime we'd like
    to get this driver upstream.
    
    We want to get the driver accepted in distributions so that users do not
    have to deal with an out-of-tree module and many distributions have
    "upstream first" requirement.
    
    The driver has been shipping for a number of years and users running on
    VMware platform will have it installed as part of VMware Tools even if it
    will not come from a distribution, thus there should not be additional
    risk in pulling the driver into mainline.  The driver will only activate
    if host is VMware so everyone else should not be affected at all.
    
    Signed-off-by: Dmitry Torokhov <dtor@vmware.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 1cbed97b59cf..dfdb4dba2320 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -22,6 +22,7 @@
  */
 
 #include <linux/dmi.h>
+#include <linux/module.h>
 #include <asm/div64.h>
 #include <asm/vmware.h>
 #include <asm/x86_init.h>
@@ -101,6 +102,7 @@ int vmware_platform(void)
 
 	return 0;
 }
+EXPORT_SYMBOL(vmware_platform);
 
 /*
  * VMware hypervisor takes care of exporting a reliable TSC to the guest.

commit 6399c087458859cddff2d6b46befb95b866df3e0
Author: Alok Kataria <akataria@vmware.com>
Date:   Fri Sep 4 13:13:39 2009 -0700

    x86: Print the hypervisor returned tsc_khz during boot
    
    On an AMD-64 system the processor frequency that is printed during
    system boot, may be different than the tsc frequency that was
    returned by the hypervisor, due to the value returned from
    calibrate_cpu.
    
    For debugging timekeeping or other related issues it might be
    better to get the tsc_khz value returned by the hypervisor.
    
    The patch below now prints the tsc frequency that the VMware
    hypervisor returned.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    LKML-Reference: <1252095219.12518.13.camel@ank32.eng.vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 0a46b4df5d80..1cbed97b59cf 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -58,6 +58,9 @@ static unsigned long vmware_get_tsc_khz(void)
 	tsc_hz = eax | (((uint64_t)ebx) << 32);
 	do_div(tsc_hz, 1000);
 	BUG_ON(tsc_hz >> 32);
+	printk(KERN_INFO "TSC freq read from hypervisor : %lu.%03lu MHz\n",
+			 (unsigned long) tsc_hz / 1000,
+			 (unsigned long) tsc_hz % 1000);
 	return tsc_hz;
 }
 
@@ -69,6 +72,9 @@ void __init vmware_platform_setup(void)
 
 	if (ebx != UINT_MAX)
 		x86_platform.calibrate_tsc = vmware_get_tsc_khz;
+	else
+		printk(KERN_WARNING
+		       "Failed to get TSC freq from the hypervisor\n");
 }
 
 /*

commit 2d826404f0bdcac2a4dd7e3c446b70d6a3b63b78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Aug 20 17:06:25 2009 +0200

    x86: Move tsc_calibration to x86_init_ops
    
    TSC calibration is modified by the vmware hypervisor and paravirt by
    separate means. Moorestown wants to add its own calibration routine as
    well. So make calibrate_tsc a proper x86_init_ops function and
    override it by paravirt or by the early setup of the vmware
    hypervisor.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index bc24f514ec93..0a46b4df5d80 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -24,6 +24,7 @@
 #include <linux/dmi.h>
 #include <asm/div64.h>
 #include <asm/vmware.h>
+#include <asm/x86_init.h>
 
 #define CPUID_VMWARE_INFO_LEAF	0x40000000
 #define VMWARE_HYPERVISOR_MAGIC	0x564D5868
@@ -47,21 +48,29 @@ static inline int __vmware_platform(void)
 	return eax != (uint32_t)-1 && ebx == VMWARE_HYPERVISOR_MAGIC;
 }
 
-static unsigned long __vmware_get_tsc_khz(void)
+static unsigned long vmware_get_tsc_khz(void)
 {
 	uint64_t tsc_hz;
 	uint32_t eax, ebx, ecx, edx;
 
 	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
 
-	if (ebx == UINT_MAX)
-		return 0;
 	tsc_hz = eax | (((uint64_t)ebx) << 32);
 	do_div(tsc_hz, 1000);
 	BUG_ON(tsc_hz >> 32);
 	return tsc_hz;
 }
 
+void __init vmware_platform_setup(void)
+{
+	uint32_t eax, ebx, ecx, edx;
+
+	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
+
+	if (ebx != UINT_MAX)
+		x86_platform.calibrate_tsc = vmware_get_tsc_khz;
+}
+
 /*
  * While checking the dmi string infomation, just checking the product
  * serial key should be enough, as this will always have a VMware
@@ -87,12 +96,6 @@ int vmware_platform(void)
 	return 0;
 }
 
-unsigned long vmware_get_tsc_khz(void)
-{
-	BUG_ON(!vmware_platform());
-	return __vmware_get_tsc_khz();
-}
-
 /*
  * VMware hypervisor takes care of exporting a reliable TSC to the guest.
  * Still, due to timing difference when running on virtual cpus, the TSC can

commit 8bdbd962ecfcbdd96f9dbb02d780b4553afd2543
Author: Alan Cox <alan@linux.intel.com>
Date:   Sat Jul 4 00:35:45 2009 +0100

    x86/cpu: Clean up various files a bit
    
    No code changes except printk levels (although some of the K6
    mtrr code might be clearer if there were a few as would
    splitting out some of the intel cache code).
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 284c399e3234..bc24f514ec93 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -49,17 +49,17 @@ static inline int __vmware_platform(void)
 
 static unsigned long __vmware_get_tsc_khz(void)
 {
-        uint64_t tsc_hz;
-        uint32_t eax, ebx, ecx, edx;
+	uint64_t tsc_hz;
+	uint32_t eax, ebx, ecx, edx;
 
-        VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
+	VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
 
-        if (ebx == UINT_MAX)
-                return 0;
-        tsc_hz = eax | (((uint64_t)ebx) << 32);
-        do_div(tsc_hz, 1000);
-        BUG_ON(tsc_hz >> 32);
-        return tsc_hz;
+	if (ebx == UINT_MAX)
+		return 0;
+	tsc_hz = eax | (((uint64_t)ebx) << 32);
+	do_div(tsc_hz, 1000);
+	BUG_ON(tsc_hz >> 32);
+	return tsc_hz;
 }
 
 /*

commit c450d7805b2c5cac8846c5f490fddfd9030d2207
Author: Hannes Eder <hannes@hanneseder.net>
Date:   Fri Nov 21 23:17:09 2008 +0100

    x86: vmware - fix sparse warnings
    
    Impact: fix sparse build warning
    
    Fix the following sparse warnings:
    
    arch/x86/kernel/cpu/vmware.c:69:5: warning: symbol 'vmware_platform'
    was not declared. Should it be static?
    arch/x86/kernel/cpu/vmware.c:89:15: warning: symbol
    'vmware_get_tsc_khz' was not declared. Should it be static?
    arch/x86/kernel/cpu/vmware.c:107:16: warning: symbol
    'vmware_set_feature_bits' was not declared. Should it be static?
    
    Signed-off-by: Hannes Eder <hannes@hanneseder.net>
    Cc: "Alok N Kataria" <akataria@vmware.com>
    Cc: "Dan Hecht" <dhecht@vmware.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index c034bda842d9..284c399e3234 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -23,6 +23,7 @@
 
 #include <linux/dmi.h>
 #include <asm/div64.h>
+#include <asm/vmware.h>
 
 #define CPUID_VMWARE_INFO_LEAF	0x40000000
 #define VMWARE_HYPERVISOR_MAGIC	0x564D5868

commit fd8cd7e1919fc1c27fe2fdccd2a1cd32f791ef0f
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Nov 3 15:50:38 2008 -0800

    x86: vmware: look for DMI string in the product serial key
    
    Impact: Should permit VMware detection on older platforms where the
    vendor is changed.  Could theoretically cause a regression if some
    weird serial number scheme contains the string "VMware" by pure
    chance.  Seems unlikely, especially with the mixed case.
    
    In some user configured cases, VMware may choose not to put a VMware specific
    DMI string, but the product serial key is always there and is VMware specific.
    Add a interface to check the serial key, when checking for VMware in the DMI
    information.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index a0905ecfe7d2..c034bda842d9 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -61,6 +61,11 @@ static unsigned long __vmware_get_tsc_khz(void)
         return tsc_hz;
 }
 
+/*
+ * While checking the dmi string infomation, just checking the product
+ * serial key should be enough, as this will always have a VMware
+ * specific string when running under VMware hypervisor.
+ */
 int vmware_platform(void)
 {
 	if (cpu_has_hypervisor) {
@@ -74,7 +79,7 @@ int vmware_platform(void)
 		hyper_vendor_id[12] = '\0';
 		if (!strcmp(hyper_vendor_id, "VMwareVMware"))
 			return 1;
-	} else if (dmi_available && dmi_name_in_vendors("VMware") &&
+	} else if (dmi_available && dmi_name_in_serial("VMware") &&
 		   __vmware_platform())
 		return 1;
 

commit 6bdbfe99916398dbb28d83833cc04757110f2738
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Nov 3 11:31:28 2008 -0800

    x86: VMware: Fix vmware_get_tsc code
    
    Impact: Fix possible failure to calibrate the TSC on Vmware near 4 GHz
    
    The current version of the code to get the tsc frequency from
    the VMware hypervisor, will be broken on processor with frequency
    (4G-1) HZ, because on such processors eax will have UINT_MAX
    and that would be legitimate.
    We instead check that EBX did change to decide if we were able to
    read the frequency from the hypervisor.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 2ac4394fcb90..a0905ecfe7d2 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -36,7 +36,7 @@
 			"=a"(eax), "=c"(ecx), "=d"(edx), "=b"(ebx) :	\
 			"0"(VMWARE_HYPERVISOR_MAGIC),			\
 			"1"(VMWARE_PORT_CMD_##cmd),			\
-			"2"(VMWARE_HYPERVISOR_PORT), "3"(0) :		\
+			"2"(VMWARE_HYPERVISOR_PORT), "3"(UINT_MAX) :	\
 			"memory");
 
 static inline int __vmware_platform(void)
@@ -53,7 +53,7 @@ static unsigned long __vmware_get_tsc_khz(void)
 
         VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
 
-        if (eax == (uint32_t)-1)
+        if (ebx == UINT_MAX)
                 return 0;
         tsc_hz = eax | (((uint64_t)ebx) << 32);
         do_div(tsc_hz, 1000);

commit eca0cd028bdf0f6aaceb0d023e9c7501079a7dda
Author: Alok Kataria <akataria@vmware.com>
Date:   Fri Oct 31 12:01:58 2008 -0700

    x86: Add a synthetic TSC_RELIABLE feature bit.
    
    Impact: Changes timebase calibration on Vmware.
    
    Use the synthetic TSC_RELIABLE bit to workaround virtualization anomalies.
    
    Virtual TSCs can be kept nearly in sync, but because the virtual TSC
    offset is set by software, it's not perfect.  So, the TSC
    synchronization test can fail. Even then the TSC can be used as a
    clocksource since the VMware platform exports a reliable TSC to the
    guest for timekeeping purposes. Use this bit to check if we need to
    skip the TSC sync checks.
    
    Along with this also set the CONSTANT_TSC bit when on VMware, since we
    still want to use TSC as clocksource on VM running over hardware which
    has unsynchronized TSC's (opteron's), since the hypervisor will take
    care of providing consistent TSC to the guest.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index d5d1b75a4b77..2ac4394fcb90 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -86,3 +86,21 @@ unsigned long vmware_get_tsc_khz(void)
 	BUG_ON(!vmware_platform());
 	return __vmware_get_tsc_khz();
 }
+
+/*
+ * VMware hypervisor takes care of exporting a reliable TSC to the guest.
+ * Still, due to timing difference when running on virtual cpus, the TSC can
+ * be marked as unstable in some cases. For example, the TSC sync check at
+ * bootup can fail due to a marginal offset between vcpus' TSCs (though the
+ * TSCs do not drift from each other).  Also, the ACPI PM timer clocksource
+ * is not suitable as a watchdog when running on a hypervisor because the
+ * kernel may miss a wrap of the counter if the vcpu is descheduled for a
+ * long time. To skip these checks at runtime we set these capability bits,
+ * so that the kernel could just trust the hypervisor with providing a
+ * reliable virtual TSC that is suitable for timekeeping.
+ */
+void __cpuinit vmware_set_feature_bits(struct cpuinfo_x86 *c)
+{
+	set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
+	set_cpu_cap(c, X86_FEATURE_TSC_RELIABLE);
+}

commit 88b094fb8d4fe43b7025ea8d487059e8813e02cd
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Oct 27 10:41:46 2008 -0700

    x86: Hypervisor detection and get tsc_freq from hypervisor
    
    Impact: Changes timebase calibration on Vmware.
    
    v3->v2 : Abstract the hypervisor detection and feature (tsc_freq) request
             behind a hypervisor.c file
    v2->v1 : Add a x86_hyper_vendor field to the cpuinfo_x86 structure.
             This avoids multiple calls to the hypervisor detection function.
    
    This patch adds function to detect if we are running under VMware.
    The current way to check if we are on VMware is following,
    #  check if "hypervisor present bit" is set, if so read the 0x40000000
       cpuid leaf and check for "VMwareVMware" signature.
    #  if the above fails, check the DMI vendors name for "VMware" string
       if we find one we query the VMware hypervisor port to check if we are
       under VMware.
    
    The DMI + "VMware hypervisor port check" is needed for older VMware products,
    which don't implement the hypervisor signature cpuid leaf.
    Also note that since we are checking for the DMI signature the hypervisor
    port should never be accessed on native hardware.
    
    This patch also adds a hypervisor_get_tsc_freq function, instead of
    calibrating the frequency which can be error prone in virtualized
    environment, we ask the hypervisor for it. We get the frequency from
    the hypervisor by accessing the hypervisor port if we are running on VMware.
    Other hypervisors too can add code to the generic routine to get frequency on
    their platform.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
new file mode 100644
index 000000000000..d5d1b75a4b77
--- /dev/null
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -0,0 +1,88 @@
+/*
+ * VMware Detection code.
+ *
+ * Copyright (C) 2008, VMware, Inc.
+ * Author : Alok N Kataria <akataria@vmware.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ * NON INFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ */
+
+#include <linux/dmi.h>
+#include <asm/div64.h>
+
+#define CPUID_VMWARE_INFO_LEAF	0x40000000
+#define VMWARE_HYPERVISOR_MAGIC	0x564D5868
+#define VMWARE_HYPERVISOR_PORT	0x5658
+
+#define VMWARE_PORT_CMD_GETVERSION	10
+#define VMWARE_PORT_CMD_GETHZ		45
+
+#define VMWARE_PORT(cmd, eax, ebx, ecx, edx)				\
+	__asm__("inl (%%dx)" :						\
+			"=a"(eax), "=c"(ecx), "=d"(edx), "=b"(ebx) :	\
+			"0"(VMWARE_HYPERVISOR_MAGIC),			\
+			"1"(VMWARE_PORT_CMD_##cmd),			\
+			"2"(VMWARE_HYPERVISOR_PORT), "3"(0) :		\
+			"memory");
+
+static inline int __vmware_platform(void)
+{
+	uint32_t eax, ebx, ecx, edx;
+	VMWARE_PORT(GETVERSION, eax, ebx, ecx, edx);
+	return eax != (uint32_t)-1 && ebx == VMWARE_HYPERVISOR_MAGIC;
+}
+
+static unsigned long __vmware_get_tsc_khz(void)
+{
+        uint64_t tsc_hz;
+        uint32_t eax, ebx, ecx, edx;
+
+        VMWARE_PORT(GETHZ, eax, ebx, ecx, edx);
+
+        if (eax == (uint32_t)-1)
+                return 0;
+        tsc_hz = eax | (((uint64_t)ebx) << 32);
+        do_div(tsc_hz, 1000);
+        BUG_ON(tsc_hz >> 32);
+        return tsc_hz;
+}
+
+int vmware_platform(void)
+{
+	if (cpu_has_hypervisor) {
+		unsigned int eax, ebx, ecx, edx;
+		char hyper_vendor_id[13];
+
+		cpuid(CPUID_VMWARE_INFO_LEAF, &eax, &ebx, &ecx, &edx);
+		memcpy(hyper_vendor_id + 0, &ebx, 4);
+		memcpy(hyper_vendor_id + 4, &ecx, 4);
+		memcpy(hyper_vendor_id + 8, &edx, 4);
+		hyper_vendor_id[12] = '\0';
+		if (!strcmp(hyper_vendor_id, "VMwareVMware"))
+			return 1;
+	} else if (dmi_available && dmi_name_in_vendors("VMware") &&
+		   __vmware_platform())
+		return 1;
+
+	return 0;
+}
+
+unsigned long vmware_get_tsc_khz(void)
+{
+	BUG_ON(!vmware_platform());
+	return __vmware_get_tsc_khz();
+}
