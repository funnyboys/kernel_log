commit cc9e303c91f5c25c49a4312552841f4c23fa2b69
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Wed May 15 09:59:00 2019 +0300

    x86/cpu: Disable frequency requests via aperfmperf IPI for nohz_full CPUs
    
    Since commit 7d5905dc14a8 ("x86 / CPU: Always show current CPU frequency
    in /proc/cpuinfo") open and read of /proc/cpuinfo sends IPI to all CPUs.
    Many applications read /proc/cpuinfo at the start for trivial reasons like
    counting cores or detecting cpu features. While sensitive workloads like
    DPDK network polling don't like any interrupts.
    
    Integrates this feature with cpu isolation and do not send IPIs to CPUs
    without housekeeping flag HK_FLAG_MISC (set by nohz_full).
    
    Code that requests cpu frequency like show_cpuinfo() falls back to the last
    frequency set by the cpufreq driver if this method returns 0.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Link: https://lkml.kernel.org/r/155790354043.1104.15333317408370209.stgit@buzz

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index e71a6ff8a67e..e2f319dc992d 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -13,6 +13,7 @@
 #include <linux/percpu.h>
 #include <linux/cpufreq.h>
 #include <linux/smp.h>
+#include <linux/sched/isolation.h>
 
 #include "cpu.h"
 
@@ -85,6 +86,9 @@ unsigned int aperfmperf_get_khz(int cpu)
 	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
+	if (!housekeeping_cpu(cpu, HK_FLAG_MISC))
+		return 0;
+
 	aperfmperf_snapshot_cpu(cpu, ktime_get(), true);
 	return per_cpu(samples.khz, cpu);
 }
@@ -101,9 +105,12 @@ void arch_freq_prepare_all(void)
 	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))
 		return;
 
-	for_each_online_cpu(cpu)
+	for_each_online_cpu(cpu) {
+		if (!housekeeping_cpu(cpu, HK_FLAG_MISC))
+			continue;
 		if (!aperfmperf_snapshot_cpu(cpu, now, false))
 			wait = true;
+	}
 
 	if (wait)
 		msleep(APERFMPERF_REFRESH_DELAY_MS);
@@ -117,6 +124,9 @@ unsigned int arch_freq_get_on_cpu(int cpu)
 	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
+	if (!housekeeping_cpu(cpu, HK_FLAG_MISC))
+		return 0;
+
 	if (aperfmperf_snapshot_cpu(cpu, ktime_get(), true))
 		return per_cpu(samples.khz, cpu);
 

commit 82664963ee5f03b5c604f4c593a5e43f26193610
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 1 10:08:51 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 437
    
    Based on 1 normalized pattern(s):
    
      this file is licensed under gplv2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 22 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190531190115.129548190@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 64d5aec24203..e71a6ff8a67e 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -1,11 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * x86 APERF/MPERF KHz calculation for
  * /sys/.../cpufreq/scaling_cur_freq
  *
  * Copyright (C) 2017 Intel Corp.
  * Author: Len Brown <len.brown@intel.com>
- *
- * This file is licensed under GPLv2.
  */
 
 #include <linux/delay.h>

commit 67e87d43b794a8886b5d075b3e0fdd0c615a595f
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 29 19:52:59 2019 +0100

    x86: Convert some slow-path static_cpu_has() callers to boot_cpu_has()
    
    Using static_cpu_has() is pointless on those paths, convert them to the
    boot_cpu_has() variant.
    
    No functional changes.
    
    Reported-by: Nadav Amit <nadav.amit@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Reviewed-by: Juergen Gross <jgross@suse.com> # for paravirt
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: linux-edac@vger.kernel.org
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: virtualization@lists.linux-foundation.org
    Cc: x86@kernel.org
    Link: https://lkml.kernel.org/r/20190330112022.28888-3-bp@alien8.de

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 804c49493938..64d5aec24203 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -83,7 +83,7 @@ unsigned int aperfmperf_get_khz(int cpu)
 	if (!cpu_khz)
 		return 0;
 
-	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
+	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
 	aperfmperf_snapshot_cpu(cpu, ktime_get(), true);
@@ -99,7 +99,7 @@ void arch_freq_prepare_all(void)
 	if (!cpu_khz)
 		return;
 
-	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
+	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))
 		return;
 
 	for_each_online_cpu(cpu)
@@ -115,7 +115,7 @@ unsigned int arch_freq_get_on_cpu(int cpu)
 	if (!cpu_khz)
 		return 0;
 
-	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
+	if (!boot_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
 	if (aperfmperf_snapshot_cpu(cpu, ktime_get(), true))

commit ad3bc25a320742f42b3015115384f5aec69c7ce2
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Dec 5 00:34:56 2018 +0100

    x86/kernel: Fix more -Wmissing-prototypes warnings
    
    ... with the goal of eventually enabling -Wmissing-prototypes by
    default. At least on x86.
    
    Make functions static where possible, otherwise add prototypes or make
    them visible through includes.
    
    asm/trace/ changes courtesy of Steven Rostedt <rostedt@goodmis.org>.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com> # ACPI + cpufreq bits
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Dimitri Sivanich <dimitri.sivanich@hpe.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Mike Travis <mike.travis@hpe.com>
    Cc: "Steven Rostedt (VMware)" <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Cc: linux-acpi@vger.kernel.org

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 7eba34df54c3..804c49493938 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -12,6 +12,7 @@
 #include <linux/ktime.h>
 #include <linux/math64.h>
 #include <linux/percpu.h>
+#include <linux/cpufreq.h>
 #include <linux/smp.h>
 
 #include "cpu.h"

commit 7d5905dc14a87805a59f3c5bf70173aac2bb18f8
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Nov 15 02:13:40 2017 +0100

    x86 / CPU: Always show current CPU frequency in /proc/cpuinfo
    
    After commit 890da9cf0983 (Revert "x86: do not use cpufreq_quick_get()
    for /proc/cpuinfo "cpu MHz"") the "cpu MHz" number in /proc/cpuinfo
    on x86 can be either the nominal CPU frequency (which is constant)
    or the frequency most recently requested by a scaling governor in
    cpufreq, depending on the cpufreq configuration.  That is somewhat
    inconsistent and is different from what it was before 4.13, so in
    order to restore the previous behavior, make it report the current
    CPU frequency like the scaling_cur_freq sysfs file in cpufreq.
    
    To that end, modify the /proc/cpuinfo implementation on x86 to use
    aperfmperf_snapshot_khz() to snapshot the APERF and MPERF feedback
    registers, if available, and use their values to compute the CPU
    frequency to be reported as "cpu MHz".
    
    However, do that carefully enough to avoid accumulating delays that
    lead to unacceptable access times for /proc/cpuinfo on systems with
    many CPUs.  Run aperfmperf_snapshot_khz() once on all CPUs
    asynchronously at the /proc/cpuinfo open time, add a single delay
    upfront (if necessary) at that point and simply compute the current
    frequency while running show_cpuinfo() for each individual CPU.
    
    Also, to avoid slowing down /proc/cpuinfo accesses too much, reduce
    the default delay between consecutive APERF and MPERF reads to 10 ms,
    which should be sufficient to get large enough numbers for the
    frequency computation in all cases.
    
    Fixes: 890da9cf0983 (Revert "x86: do not use cpufreq_quick_get() for /proc/cpuinfo "cpu MHz"")
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 957813e0180d..7eba34df54c3 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -14,6 +14,8 @@
 #include <linux/percpu.h>
 #include <linux/smp.h>
 
+#include "cpu.h"
+
 struct aperfmperf_sample {
 	unsigned int	khz;
 	ktime_t	time;
@@ -24,7 +26,7 @@ struct aperfmperf_sample {
 static DEFINE_PER_CPU(struct aperfmperf_sample, samples);
 
 #define APERFMPERF_CACHE_THRESHOLD_MS	10
-#define APERFMPERF_REFRESH_DELAY_MS	20
+#define APERFMPERF_REFRESH_DELAY_MS	10
 #define APERFMPERF_STALE_THRESHOLD_MS	1000
 
 /*
@@ -38,8 +40,6 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	u64 aperf, aperf_delta;
 	u64 mperf, mperf_delta;
 	struct aperfmperf_sample *s = this_cpu_ptr(&samples);
-	ktime_t now = ktime_get();
-	s64 time_delta = ktime_ms_delta(now, s->time);
 	unsigned long flags;
 
 	local_irq_save(flags);
@@ -57,38 +57,68 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	if (mperf_delta == 0)
 		return;
 
-	s->time = now;
+	s->time = ktime_get();
 	s->aperf = aperf;
 	s->mperf = mperf;
-
-	/* If the previous iteration was too long ago, discard it. */
-	if (time_delta > APERFMPERF_STALE_THRESHOLD_MS)
-		s->khz = 0;
-	else
-		s->khz = div64_u64((cpu_khz * aperf_delta), mperf_delta);
+	s->khz = div64_u64((cpu_khz * aperf_delta), mperf_delta);
 }
 
-unsigned int arch_freq_get_on_cpu(int cpu)
+static bool aperfmperf_snapshot_cpu(int cpu, ktime_t now, bool wait)
 {
-	s64 time_delta;
-	unsigned int khz;
+	s64 time_delta = ktime_ms_delta(now, per_cpu(samples.time, cpu));
+
+	/* Don't bother re-computing within the cache threshold time. */
+	if (time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
+		return true;
+
+	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, wait);
+
+	/* Return false if the previous iteration was too long ago. */
+	return time_delta <= APERFMPERF_STALE_THRESHOLD_MS;
+}
 
+unsigned int aperfmperf_get_khz(int cpu)
+{
 	if (!cpu_khz)
 		return 0;
 
 	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
-	/* Don't bother re-computing within the cache threshold time. */
-	time_delta = ktime_ms_delta(ktime_get(), per_cpu(samples.time, cpu));
-	khz = per_cpu(samples.khz, cpu);
-	if (khz && time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
-		return khz;
+	aperfmperf_snapshot_cpu(cpu, ktime_get(), true);
+	return per_cpu(samples.khz, cpu);
+}
 
-	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
-	khz = per_cpu(samples.khz, cpu);
-	if (khz)
-		return khz;
+void arch_freq_prepare_all(void)
+{
+	ktime_t now = ktime_get();
+	bool wait = false;
+	int cpu;
+
+	if (!cpu_khz)
+		return;
+
+	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
+		return;
+
+	for_each_online_cpu(cpu)
+		if (!aperfmperf_snapshot_cpu(cpu, now, false))
+			wait = true;
+
+	if (wait)
+		msleep(APERFMPERF_REFRESH_DELAY_MS);
+}
+
+unsigned int arch_freq_get_on_cpu(int cpu)
+{
+	if (!cpu_khz)
+		return 0;
+
+	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
+		return 0;
+
+	if (aperfmperf_snapshot_cpu(cpu, ktime_get(), true))
+		return per_cpu(samples.khz, cpu);
 
 	msleep(APERFMPERF_REFRESH_DELAY_MS);
 	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);

commit b29c6ef7bb1257853c1e31616d84f55e561cf631
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Nov 13 02:15:39 2017 +0100

    x86 / CPU: Avoid unnecessary IPIs in arch_freq_get_on_cpu()
    
    Even though aperfmperf_snapshot_khz() caches the samples.khz value to
    return if called again in a sufficiently short time, its caller,
    arch_freq_get_on_cpu(), still uses smp_call_function_single() to run it
    which may allow user space to trigger an IPI storm by reading from the
    scaling_cur_freq cpufreq sysfs file in a tight loop.
    
    To avoid that, move the decision on whether or not to return the cached
    samples.khz value to arch_freq_get_on_cpu().
    
    This change was part of commit 941f5f0f6ef5 ("x86: CPU: Fix up "cpu MHz"
    in /proc/cpuinfo"), but it was not the reason for the revert and it
    remains applicable.
    
    Fixes: 4815d3c56d1e (cpufreq: x86: Make scaling_cur_freq behave more as expected)
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: WANG Chao <chao.wang@ucloud.cn>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 0ee83321a313..957813e0180d 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -42,10 +42,6 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	s64 time_delta = ktime_ms_delta(now, s->time);
 	unsigned long flags;
 
-	/* Don't bother re-computing within the cache threshold time. */
-	if (time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
-		return;
-
 	local_irq_save(flags);
 	rdmsrl(MSR_IA32_APERF, aperf);
 	rdmsrl(MSR_IA32_MPERF, mperf);
@@ -74,6 +70,7 @@ static void aperfmperf_snapshot_khz(void *dummy)
 
 unsigned int arch_freq_get_on_cpu(int cpu)
 {
+	s64 time_delta;
 	unsigned int khz;
 
 	if (!cpu_khz)
@@ -82,6 +79,12 @@ unsigned int arch_freq_get_on_cpu(int cpu)
 	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
+	/* Don't bother re-computing within the cache threshold time. */
+	time_delta = ktime_ms_delta(ktime_get(), per_cpu(samples.time, cpu));
+	khz = per_cpu(samples.khz, cpu);
+	if (khz && time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
+		return khz;
+
 	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
 	khz = per_cpu(samples.khz, cpu);
 	if (khz)

commit ea0ee33988778fb73e4f45e7c73fb735787e2f32
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 10 11:19:11 2017 -0800

    Revert "x86: CPU: Fix up "cpu MHz" in /proc/cpuinfo"
    
    This reverts commit 941f5f0f6ef5338814145cf2b813cf1f98873e2f.
    
    Sadly, it turns out that we really can't just do the cross-CPU IPI to
    all CPU's to get their proper frequencies, because it's much too
    expensive on systems with lots of cores.
    
    So we'll have to revert this for now, and revisit it using a smarter
    model (probably doing one system-wide IPI at open time, and doing all
    the frequency calculations in parallel).
    
    Reported-by: WANG Chao <chao.wang@ucloud.cn>
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Cc: Rafael J Wysocki <rafael.j.wysocki@intel.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 957813e0180d..0ee83321a313 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -42,6 +42,10 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	s64 time_delta = ktime_ms_delta(now, s->time);
 	unsigned long flags;
 
+	/* Don't bother re-computing within the cache threshold time. */
+	if (time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
+		return;
+
 	local_irq_save(flags);
 	rdmsrl(MSR_IA32_APERF, aperf);
 	rdmsrl(MSR_IA32_MPERF, mperf);
@@ -70,7 +74,6 @@ static void aperfmperf_snapshot_khz(void *dummy)
 
 unsigned int arch_freq_get_on_cpu(int cpu)
 {
-	s64 time_delta;
 	unsigned int khz;
 
 	if (!cpu_khz)
@@ -79,12 +82,6 @@ unsigned int arch_freq_get_on_cpu(int cpu)
 	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
-	/* Don't bother re-computing within the cache threshold time. */
-	time_delta = ktime_ms_delta(ktime_get(), per_cpu(samples.time, cpu));
-	khz = per_cpu(samples.khz, cpu);
-	if (khz && time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
-		return khz;
-
 	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
 	khz = per_cpu(samples.khz, cpu);
 	if (khz)

commit 941f5f0f6ef5338814145cf2b813cf1f98873e2f
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Nov 3 16:35:49 2017 +0100

    x86: CPU: Fix up "cpu MHz" in /proc/cpuinfo
    
    Commit 890da9cf0983 (Revert "x86: do not use cpufreq_quick_get() for
    /proc/cpuinfo "cpu MHz"") is not sufficient to restore the previous
    behavior of "cpu MHz" in /proc/cpuinfo on x86 due to some changes
    made after the commit it has reverted.
    
    To address this, make the code in question use arch_freq_get_on_cpu()
    which also is used by cpufreq for reporting the current frequency of
    CPUs and since that function doesn't really depend on cpufreq in any
    way, drop the CONFIG_CPU_FREQ dependency for the object file
    containing it.
    
    Also refactor arch_freq_get_on_cpu() somewhat to avoid IPIs and
    return cached values right away if it is called very often over a
    short time (to prevent user space from triggering IPI storms through
    it).
    
    Fixes: 890da9cf0983 (Revert "x86: do not use cpufreq_quick_get() for /proc/cpuinfo "cpu MHz"")
    Cc: stable@kernel.org   # 4.13 - together with 890da9cf0983
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 0ee83321a313..957813e0180d 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -42,10 +42,6 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	s64 time_delta = ktime_ms_delta(now, s->time);
 	unsigned long flags;
 
-	/* Don't bother re-computing within the cache threshold time. */
-	if (time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
-		return;
-
 	local_irq_save(flags);
 	rdmsrl(MSR_IA32_APERF, aperf);
 	rdmsrl(MSR_IA32_MPERF, mperf);
@@ -74,6 +70,7 @@ static void aperfmperf_snapshot_khz(void *dummy)
 
 unsigned int arch_freq_get_on_cpu(int cpu)
 {
+	s64 time_delta;
 	unsigned int khz;
 
 	if (!cpu_khz)
@@ -82,6 +79,12 @@ unsigned int arch_freq_get_on_cpu(int cpu)
 	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
+	/* Don't bother re-computing within the cache threshold time. */
+	time_delta = ktime_ms_delta(ktime_get(), per_cpu(samples.time, cpu));
+	khz = per_cpu(samples.khz, cpu);
+	if (khz && time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
+		return khz;
+
 	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
 	khz = per_cpu(samples.khz, cpu);
 	if (khz)

commit 8e2f3bce05e056575c2c84a344a8291fdabb5f21
Author: Doug Smythies <doug.smythies@gmail.com>
Date:   Tue Aug 8 14:12:49 2017 -0700

    cpufreq: x86: Disable interrupts during MSRs reading
    
    According to Intel 64 and IA-32 Architectures SDM, Volume 3,
    Chapter 14.2, "Software needs to exercise care to avoid delays
    between the two RDMSRs (for example interrupts)".
    
    So, disable interrupts during reading MSRs IA32_APERF and IA32_MPERF.
    
    See also: commit 4ab60c3f32c7 (cpufreq: intel_pstate: Disable
    interrupts during MSRs reading).
    
    Signed-off-by: Doug Smythies <dsmythies@telus.net>
    Reviewed-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index 7cf7c70b6ef2..0ee83321a313 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -40,13 +40,16 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	struct aperfmperf_sample *s = this_cpu_ptr(&samples);
 	ktime_t now = ktime_get();
 	s64 time_delta = ktime_ms_delta(now, s->time);
+	unsigned long flags;
 
 	/* Don't bother re-computing within the cache threshold time. */
 	if (time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
 		return;
 
+	local_irq_save(flags);
 	rdmsrl(MSR_IA32_APERF, aperf);
 	rdmsrl(MSR_IA32_MPERF, mperf);
+	local_irq_restore(flags);
 
 	aperf_delta = aperf - s->aperf;
 	mperf_delta = mperf - s->mperf;

commit 4815d3c56d1e10449a44089a47544d9ba84fad0d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Jul 28 14:45:03 2017 +0200

    cpufreq: x86: Make scaling_cur_freq behave more as expected
    
    After commit f8475cef9008 "x86: use common aperfmperf_khz_on_cpu() to
    calculate KHz using APERF/MPERF" the scaling_cur_freq policy attribute
    in sysfs only behaves as expected on x86 with APERF/MPERF registers
    available when it is read from at least twice in a row.  The value
    returned by the first read may not be meaningful, because the
    computations in there use cached values from the previous iteration
    of aperfmperf_snapshot_khz() which may be stale.
    
    To prevent that from happening, modify arch_freq_get_on_cpu() to
    call aperfmperf_snapshot_khz() twice, with a short delay between
    these calls, if the previous invocation of aperfmperf_snapshot_khz()
    was too far back in the past (specifically, more that 1s ago).
    
    Also, as pointed out by Doug Smythies, aperf_delta is limited now
    and the multiplication of it by cpu_khz won't overflow, so simplify
    the s->khz computations too.
    
    Fixes: f8475cef9008 "x86: use common aperfmperf_khz_on_cpu() to calculate KHz using APERF/MPERF"
    Reported-by: Doug Smythies <dsmythies@telus.net>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index d869c8671e36..7cf7c70b6ef2 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -8,20 +8,25 @@
  * This file is licensed under GPLv2.
  */
 
-#include <linux/jiffies.h>
+#include <linux/delay.h>
+#include <linux/ktime.h>
 #include <linux/math64.h>
 #include <linux/percpu.h>
 #include <linux/smp.h>
 
 struct aperfmperf_sample {
 	unsigned int	khz;
-	unsigned long	jiffies;
+	ktime_t	time;
 	u64	aperf;
 	u64	mperf;
 };
 
 static DEFINE_PER_CPU(struct aperfmperf_sample, samples);
 
+#define APERFMPERF_CACHE_THRESHOLD_MS	10
+#define APERFMPERF_REFRESH_DELAY_MS	20
+#define APERFMPERF_STALE_THRESHOLD_MS	1000
+
 /*
  * aperfmperf_snapshot_khz()
  * On the current CPU, snapshot APERF, MPERF, and jiffies
@@ -33,9 +38,11 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	u64 aperf, aperf_delta;
 	u64 mperf, mperf_delta;
 	struct aperfmperf_sample *s = this_cpu_ptr(&samples);
+	ktime_t now = ktime_get();
+	s64 time_delta = ktime_ms_delta(now, s->time);
 
-	/* Don't bother re-computing within 10 ms */
-	if (time_before(jiffies, s->jiffies + HZ/100))
+	/* Don't bother re-computing within the cache threshold time. */
+	if (time_delta < APERFMPERF_CACHE_THRESHOLD_MS)
 		return;
 
 	rdmsrl(MSR_IA32_APERF, aperf);
@@ -51,22 +58,21 @@ static void aperfmperf_snapshot_khz(void *dummy)
 	if (mperf_delta == 0)
 		return;
 
-	/*
-	 * if (cpu_khz * aperf_delta) fits into ULLONG_MAX, then
-	 *	khz = (cpu_khz * aperf_delta) / mperf_delta
-	 */
-	if (div64_u64(ULLONG_MAX, cpu_khz) > aperf_delta)
-		s->khz = div64_u64((cpu_khz * aperf_delta), mperf_delta);
-	else	/* khz = aperf_delta / (mperf_delta / cpu_khz) */
-		s->khz = div64_u64(aperf_delta,
-			div64_u64(mperf_delta, cpu_khz));
-	s->jiffies = jiffies;
+	s->time = now;
 	s->aperf = aperf;
 	s->mperf = mperf;
+
+	/* If the previous iteration was too long ago, discard it. */
+	if (time_delta > APERFMPERF_STALE_THRESHOLD_MS)
+		s->khz = 0;
+	else
+		s->khz = div64_u64((cpu_khz * aperf_delta), mperf_delta);
 }
 
 unsigned int arch_freq_get_on_cpu(int cpu)
 {
+	unsigned int khz;
+
 	if (!cpu_khz)
 		return 0;
 
@@ -74,6 +80,12 @@ unsigned int arch_freq_get_on_cpu(int cpu)
 		return 0;
 
 	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
+	khz = per_cpu(samples.khz, cpu);
+	if (khz)
+		return khz;
+
+	msleep(APERFMPERF_REFRESH_DELAY_MS);
+	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
 
 	return per_cpu(samples.khz, cpu);
 }

commit f8475cef90082bf0902ddab106112de130d90395
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jun 23 22:11:52 2017 -0700

    x86: use common aperfmperf_khz_on_cpu() to calculate KHz using APERF/MPERF
    
    The goal of this change is to give users a uniform and meaningful
    result when they read /sys/...cpufreq/scaling_cur_freq
    on modern x86 hardware, as compared to what they get today.
    
    Modern x86 processors include the hardware needed
    to accurately calculate frequency over an interval --
    APERF, MPERF, and the TSC.
    
    Here we provide an x86 routine to make this calculation
    on supported hardware, and use it in preference to any
    driver driver-specific cpufreq_driver.get() routine.
    
    MHz is computed like so:
    
    MHz = base_MHz * delta_APERF / delta_MPERF
    
    MHz is the average frequency of the busy processor
    over a measurement interval.  The interval is
    defined to be the time between successive invocations
    of aperfmperf_khz_on_cpu(), which are expected to to
    happen on-demand when users read sysfs attribute
    cpufreq/scaling_cur_freq.
    
    As with previous methods of calculating MHz,
    idle time is excluded.
    
    base_MHz above is from TSC calibration global "cpu_khz".
    
    This x86 native method to calculate MHz returns a meaningful result
    no matter if P-states are controlled by hardware or firmware
    and/or if the Linux cpufreq sub-system is or is-not installed.
    
    When this routine is invoked more frequently, the measurement
    interval becomes shorter.  However, the code limits re-computation
    to 10ms intervals so that average frequency remains meaningful.
    
    Discerning users are encouraged to take advantage of
    the turbostat(8) utility, which can gracefully handle
    concurrent measurement intervals of arbitrary length.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
new file mode 100644
index 000000000000..d869c8671e36
--- /dev/null
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -0,0 +1,79 @@
+/*
+ * x86 APERF/MPERF KHz calculation for
+ * /sys/.../cpufreq/scaling_cur_freq
+ *
+ * Copyright (C) 2017 Intel Corp.
+ * Author: Len Brown <len.brown@intel.com>
+ *
+ * This file is licensed under GPLv2.
+ */
+
+#include <linux/jiffies.h>
+#include <linux/math64.h>
+#include <linux/percpu.h>
+#include <linux/smp.h>
+
+struct aperfmperf_sample {
+	unsigned int	khz;
+	unsigned long	jiffies;
+	u64	aperf;
+	u64	mperf;
+};
+
+static DEFINE_PER_CPU(struct aperfmperf_sample, samples);
+
+/*
+ * aperfmperf_snapshot_khz()
+ * On the current CPU, snapshot APERF, MPERF, and jiffies
+ * unless we already did it within 10ms
+ * calculate kHz, save snapshot
+ */
+static void aperfmperf_snapshot_khz(void *dummy)
+{
+	u64 aperf, aperf_delta;
+	u64 mperf, mperf_delta;
+	struct aperfmperf_sample *s = this_cpu_ptr(&samples);
+
+	/* Don't bother re-computing within 10 ms */
+	if (time_before(jiffies, s->jiffies + HZ/100))
+		return;
+
+	rdmsrl(MSR_IA32_APERF, aperf);
+	rdmsrl(MSR_IA32_MPERF, mperf);
+
+	aperf_delta = aperf - s->aperf;
+	mperf_delta = mperf - s->mperf;
+
+	/*
+	 * There is no architectural guarantee that MPERF
+	 * increments faster than we can read it.
+	 */
+	if (mperf_delta == 0)
+		return;
+
+	/*
+	 * if (cpu_khz * aperf_delta) fits into ULLONG_MAX, then
+	 *	khz = (cpu_khz * aperf_delta) / mperf_delta
+	 */
+	if (div64_u64(ULLONG_MAX, cpu_khz) > aperf_delta)
+		s->khz = div64_u64((cpu_khz * aperf_delta), mperf_delta);
+	else	/* khz = aperf_delta / (mperf_delta / cpu_khz) */
+		s->khz = div64_u64(aperf_delta,
+			div64_u64(mperf_delta, cpu_khz));
+	s->jiffies = jiffies;
+	s->aperf = aperf;
+	s->mperf = mperf;
+}
+
+unsigned int arch_freq_get_on_cpu(int cpu)
+{
+	if (!cpu_khz)
+		return 0;
+
+	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
+		return 0;
+
+	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
+
+	return per_cpu(samples.khz, cpu);
+}
