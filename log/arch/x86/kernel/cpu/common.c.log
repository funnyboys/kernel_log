commit a13b9d0b97211579ea63b96c606de79b963c0f47
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Jun 8 20:15:09 2020 -0700

    x86/cpu: Use pinning mask for CR4 bits needing to be 0
    
    The X86_CR4_FSGSBASE bit of CR4 should not change after boot[1]. Older
    kernels should enforce this bit to zero, and newer kernels need to
    enforce it depending on boot-time configuration (e.g. "nofsgsbase").
    To support a pinned bit being either 1 or 0, use an explicit mask in
    combination with the expected pinned bit values.
    
    [1] https://lore.kernel.org/lkml/20200527103147.GI325280@hirez.programming.kicks-ass.net
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/202006082013.71E29A42@keescook

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 043d93cdcaad..95c090a45b4b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -347,6 +347,9 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 	cr4_clear_bits(X86_CR4_UMIP);
 }
 
+/* These bits should not change their value after CPU init is finished. */
+static const unsigned long cr4_pinned_mask =
+	X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP | X86_CR4_FSGSBASE;
 static DEFINE_STATIC_KEY_FALSE_RO(cr_pinning);
 static unsigned long cr4_pinned_bits __ro_after_init;
 
@@ -371,20 +374,20 @@ EXPORT_SYMBOL(native_write_cr0);
 
 void native_write_cr4(unsigned long val)
 {
-	unsigned long bits_missing = 0;
+	unsigned long bits_changed = 0;
 
 set_register:
 	asm volatile("mov %0,%%cr4": "+r" (val), "+m" (cr4_pinned_bits));
 
 	if (static_branch_likely(&cr_pinning)) {
-		if (unlikely((val & cr4_pinned_bits) != cr4_pinned_bits)) {
-			bits_missing = ~val & cr4_pinned_bits;
-			val |= bits_missing;
+		if (unlikely((val & cr4_pinned_mask) != cr4_pinned_bits)) {
+			bits_changed = (val & cr4_pinned_mask) ^ cr4_pinned_bits;
+			val = (val & ~cr4_pinned_mask) | cr4_pinned_bits;
 			goto set_register;
 		}
-		/* Warn after we've set the missing bits. */
-		WARN_ONCE(bits_missing, "CR4 bits went missing: %lx!?\n",
-			  bits_missing);
+		/* Warn after we've corrected the changed bits. */
+		WARN_ONCE(bits_changed, "pinned CR4 bits changed: 0x%lx!?\n",
+			  bits_changed);
 	}
 }
 #if IS_MODULE(CONFIG_LKDTM)
@@ -419,7 +422,7 @@ void cr4_init(void)
 	if (boot_cpu_has(X86_FEATURE_PCID))
 		cr4 |= X86_CR4_PCIDE;
 	if (static_branch_likely(&cr_pinning))
-		cr4 |= cr4_pinned_bits;
+		cr4 = (cr4 & ~cr4_pinned_mask) | cr4_pinned_bits;
 
 	__write_cr4(cr4);
 
@@ -434,10 +437,7 @@ void cr4_init(void)
  */
 static void __init setup_cr_pinning(void)
 {
-	unsigned long mask;
-
-	mask = (X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP);
-	cr4_pinned_bits = this_cpu_read(cpu_tlbstate.cr4) & mask;
+	cr4_pinned_bits = this_cpu_read(cpu_tlbstate.cr4) & cr4_pinned_mask;
 	static_key_enable(&cr_pinning.key);
 }
 

commit f9912ada82862df341b3e86864cbd532d0d24b84
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 29 23:27:37 2020 +0200

    x86/entry: Remove debug IDT frobbing
    
    This is all unused now.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200529213321.245019500@infradead.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f4645f9ff9cb..043d93cdcaad 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1706,23 +1706,6 @@ void syscall_init(void)
 	       X86_EFLAGS_IOPL|X86_EFLAGS_AC|X86_EFLAGS_NT);
 }
 
-DEFINE_PER_CPU(int, debug_stack_usage);
-DEFINE_PER_CPU(u32, debug_idt_ctr);
-
-noinstr void debug_stack_set_zero(void)
-{
-	this_cpu_inc(debug_idt_ctr);
-	load_current_idt();
-}
-
-noinstr void debug_stack_reset(void)
-{
-	if (WARN_ON(!this_cpu_read(debug_idt_ctr)))
-		return;
-	if (this_cpu_dec_return(debug_idt_ctr) == 0)
-		load_current_idt();
-}
-
 #else	/* CONFIG_X86_64 */
 
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;

commit f051f697955049c7cf10a635ab8149aa619243b2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Apr 6 15:55:06 2020 +0200

    x86/nmi: Protect NMI entry against instrumentation
    
    Mark all functions in the fragile code parts noinstr or force inlining so
    they can't be instrumented.
    
    Also make the hardware latency tracer invocation explicit outside of
    non-instrumentable section.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20200505135314.716186134@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8be042df12c3..f4645f9ff9cb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1709,21 +1709,19 @@ void syscall_init(void)
 DEFINE_PER_CPU(int, debug_stack_usage);
 DEFINE_PER_CPU(u32, debug_idt_ctr);
 
-void debug_stack_set_zero(void)
+noinstr void debug_stack_set_zero(void)
 {
 	this_cpu_inc(debug_idt_ctr);
 	load_current_idt();
 }
-NOKPROBE_SYMBOL(debug_stack_set_zero);
 
-void debug_stack_reset(void)
+noinstr void debug_stack_reset(void)
 {
 	if (WARN_ON(!this_cpu_read(debug_idt_ctr)))
 		return;
 	if (this_cpu_dec_return(debug_idt_ctr) == 0)
 		load_current_idt();
 }
-NOKPROBE_SYMBOL(debug_stack_reset);
 
 #else	/* CONFIG_X86_64 */
 

commit a5ad5742f671de906adbf29fbedf0a04705cebad
Merge: 013b2deba9a6 4fa7252338a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 9 09:54:46 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge even more updates from Andrew Morton:
    
     - a kernel-wide sweep of show_stack()
    
     - pagetable cleanups
    
     - abstract out accesses to mmap_sem - prep for mmap_sem scalability work
    
     - hch's user acess work
    
    Subsystems affected by this patch series: debug, mm/pagemap, mm/maccess,
    mm/documentation.
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (93 commits)
      include/linux/cache.h: expand documentation over __read_mostly
      maccess: return -ERANGE when probe_kernel_read() fails
      x86: use non-set_fs based maccess routines
      maccess: allow architectures to provide kernel probing directly
      maccess: move user access routines together
      maccess: always use strict semantics for probe_kernel_read
      maccess: remove strncpy_from_unsafe
      tracing/kprobes: handle mixed kernel/userspace probes better
      bpf: rework the compat kernel probe handling
      bpf:bpf_seq_printf(): handle potentially unsafe format string better
      bpf: handle the compat string in bpf_trace_copy_string better
      bpf: factor out a bpf_trace_copy_string helper
      maccess: unify the probe kernel arch hooks
      maccess: remove probe_read_common and probe_write_common
      maccess: rename strnlen_unsafe_user to strnlen_user_nofault
      maccess: rename strncpy_from_unsafe_strict to strncpy_from_kernel_nofault
      maccess: rename strncpy_from_unsafe_user to strncpy_from_user_nofault
      maccess: update the top of file comment
      maccess: clarify kerneldoc comments
      maccess: remove duplicate kerneldoc comments
      ...

commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index efdeaf21aa5f..b367df8f54e7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -21,6 +21,7 @@
 #include <linux/smp.h>
 #include <linux/io.h>
 #include <linux/syscore_ops.h>
+#include <linux/pgtable.h>
 
 #include <asm/stackprotector.h>
 #include <asm/perf_event.h>
@@ -35,7 +36,6 @@
 #include <asm/vsyscall.h>
 #include <linux/topology.h>
 #include <linux/cpumask.h>
-#include <linux/pgtable.h>
 #include <linux/atomic.h>
 #include <asm/proto.h>
 #include <asm/setup.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 74682b8d09b0..efdeaf21aa5f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -35,7 +35,7 @@
 #include <asm/vsyscall.h>
 #include <linux/topology.h>
 #include <linux/cpumask.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <linux/atomic.h>
 #include <asm/proto.h>
 #include <asm/setup.h>

commit 8b4d37db9a566deaf22065ba1ba9b19c9fb964b4
Merge: abfbb29297c2 3798cc4d106e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 9 09:30:21 2020 -0700

    Merge branch 'x86/srbds' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 srbds fixes from Thomas Gleixner:
     "The 9th episode of the dime novel "The performance killer" with the
      subtitle "Slow Randomizing Boosts Denial of Service".
    
      SRBDS is an MDS-like speculative side channel that can leak bits from
      the random number generator (RNG) across cores and threads. New
      microcode serializes the processor access during the execution of
      RDRAND and RDSEED. This ensures that the shared buffer is overwritten
      before it is released for reuse. This is equivalent to a full bus
      lock, which means that many threads running the RNG instructions in
      parallel have the same effect as the same amount of threads issuing a
      locked instruction targeting an address which requires locking of two
      cachelines at once.
    
      The mitigation support comes with the usual pile of unpleasant
      ingredients:
    
       - command line options
    
       - sysfs file
    
       - microcode checks
    
       - a list of vulnerable CPUs identified by model and stepping this
         time which requires stepping match support for the cpu match logic.
    
       - the inevitable slowdown of affected CPUs"
    
    * branch 'x86/srbds' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/speculation: Add Ivy Bridge to affected list
      x86/speculation: Add SRBDS vulnerability and mitigation documentation
      x86/speculation: Add Special Register Buffer Data Sampling (SRBDS) mitigation
      x86/cpu: Add 'table' argument to cpu_matches()

commit f4dd60a3d4c7656dcaa0ba2afb503528c86f913f
Merge: 435faf5c218a bd1de2a7aace
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 5 11:18:53 2020 -0700

    Merge tag 'x86-mm-2020-06-05' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "Misc changes:
    
       - Unexport various PAT primitives
    
       - Unexport per-CPU tlbstate and uninline TLB helpers"
    
    * tag 'x86-mm-2020-06-05' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/tlb/uv: Add a forward declaration for struct flush_tlb_info
      x86/cpu: Export native_write_cr4() only when CONFIG_LKTDM=m
      x86/tlb: Restrict access to tlbstate
      xen/privcmd: Remove unneeded asm/tlb.h include
      x86/tlb: Move PCID helpers where they are used
      x86/tlb: Uninline nmi_uaccess_okay()
      x86/tlb: Move cr4_set_bits_and_update_boot() to the usage site
      x86/tlb: Move paravirt_tlb_remove_table() to the usage site
      x86/tlb: Move __flush_tlb_all() out of line
      x86/tlb: Move flush_tlb_others() out of line
      x86/tlb: Move __flush_tlb_one_kernel() out of line
      x86/tlb: Move __flush_tlb_one_user() out of line
      x86/tlb: Move __flush_tlb_global() out of line
      x86/tlb: Move __flush_tlb() out of line
      x86/alternatives: Move temporary_mm helpers into C
      x86/cr4: Sanitize CR4.PCE update
      x86/cpu: Uninline CR4 accessors
      x86/tlb: Uninline __get_current_cr3_fast()
      x86/mm: Use pgprotval_t in protval_4k_2_large() and protval_large_2_4k()
      x86/mm: Unexport __cachemode2pte_tbl
      ...

commit 923f3a2b48bdccb6a1d1f0dd48de03de7ad936d9
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Tue May 5 15:36:15 2020 -0700

    x86/resctrl: Query LLC monitoring properties once during boot
    
    Cache and memory bandwidth monitoring are features that are part of
    x86 CPU resource control that is supported by the resctrl subsystem.
    The monitoring properties are obtained via CPUID from every CPU
    and only used within the resctrl subsystem where the properties are
    only read from boot_cpu_data.
    
    Obtain the monitoring properties once, placed in boot_cpu_data, via the
    ->c_bsp_init() helpers of the vendors that support X86_FEATURE_CQM_LLC.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/6d74a6ac3e69f4b7a8b4115835f9455faf0f468d.1588715690.git.reinette.chatre@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 556a96d05a6c..d07809286b95 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -56,7 +56,6 @@
 #include <asm/intel-family.h>
 #include <asm/cpu_device_id.h>
 #include <asm/uv/uv.h>
-#include <asm/resctrl.h>
 
 #include "cpu.h"
 
@@ -922,7 +921,6 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 
 	init_scattered_cpuid_features(c);
 	init_speculation_control(c);
-	resctrl_cpu_detect(c);
 
 	/*
 	 * Clear/Set all flags overridden by options, after probe.

commit f0d339db56478e3bcd98d5e985d3d69cacf27549
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Tue May 5 15:36:14 2020 -0700

    x86/resctrl: Remove unnecessary RMID checks
    
    The cache and memory bandwidth monitoring properties are read using
    CPUID on every CPU. After the information is read from the system a
    sanity check is run to
    
     (1) ensure that the RMID data is initialized for the boot CPU in case
         the information was not available on the boot CPU and
    
     (2) the boot CPU's RMID is set to the minimum of RMID obtained
         from all CPUs.
    
    Every known platform that supports resctrl has the same maximum RMID
    on all CPUs. Both sanity checks found in x86_init_cache_qos() can thus
    safely be removed.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/c9a3b60d34091840c8b0bd1c6fab15e5ba92cb17.1588715690.git.reinette.chatre@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a8f0f22ee5c1..556a96d05a6c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1354,20 +1354,6 @@ static void generic_identify(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void x86_init_cache_qos(struct cpuinfo_x86 *c)
-{
-	/*
-	 * The heavy lifting of max_rmid and cache_occ_scale are handled
-	 * in get_cpu_cap().  Here we just set the max_rmid for the boot_cpu
-	 * in case CQM bits really aren't there in this CPU.
-	 */
-	if (c != &boot_cpu_data) {
-		boot_cpu_data.x86_cache_max_rmid =
-			min(boot_cpu_data.x86_cache_max_rmid,
-			    c->x86_cache_max_rmid);
-	}
-}
-
 /*
  * Validate that ACPI/mptables have the same information about the
  * effective APIC id and update the package map.
@@ -1480,7 +1466,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 #endif
 
 	x86_init_rdrand(c);
-	x86_init_cache_qos(c);
 	setup_pku(c);
 
 	/*

commit 0118ad82c2a64ebcf15d7565ed35361407efadfa
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Tue May 5 15:36:13 2020 -0700

    x86/cpu: Move resctrl CPUID code to resctrl/
    
    The function determining a platform's support and properties of cache
    occupancy and memory bandwidth monitoring (properties of
    X86_FEATURE_CQM_LLC) can be found among the common CPU code. After
    the feature's properties is populated in the per-CPU data the resctrl
    subsystem is the only consumer (via boot_cpu_data).
    
    Move the function that obtains the CPU information used by resctrl to
    the resctrl subsystem and rename it from init_cqm() to
    resctrl_cpu_detect(). The function continues to be called from the
    common CPU code. This move is done in preparation of the addition of some
    vendor specific code.
    
    No functional change.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/38433b99f9d16c8f4ee796f8cc42b871531fa203.1588715690.git.reinette.chatre@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index bed0cb83fe24..a8f0f22ee5c1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -56,6 +56,7 @@
 #include <asm/intel-family.h>
 #include <asm/cpu_device_id.h>
 #include <asm/uv/uv.h>
+#include <asm/resctrl.h>
 
 #include "cpu.h"
 
@@ -854,30 +855,6 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 	}
 }
 
-static void init_cqm(struct cpuinfo_x86 *c)
-{
-	if (!cpu_has(c, X86_FEATURE_CQM_LLC)) {
-		c->x86_cache_max_rmid  = -1;
-		c->x86_cache_occ_scale = -1;
-		return;
-	}
-
-	/* will be overridden if occupancy monitoring exists */
-	c->x86_cache_max_rmid = cpuid_ebx(0xf);
-
-	if (cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC) ||
-	    cpu_has(c, X86_FEATURE_CQM_MBM_TOTAL) ||
-	    cpu_has(c, X86_FEATURE_CQM_MBM_LOCAL)) {
-		u32 eax, ebx, ecx, edx;
-
-		/* QoS sub-leaf, EAX=0Fh, ECX=1 */
-		cpuid_count(0xf, 1, &eax, &ebx, &ecx, &edx);
-
-		c->x86_cache_max_rmid  = ecx;
-		c->x86_cache_occ_scale = ebx;
-	}
-}
-
 void get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
@@ -945,7 +922,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 
 	init_scattered_cpuid_features(c);
 	init_speculation_control(c);
-	init_cqm(c);
+	resctrl_cpu_detect(c);
 
 	/*
 	 * Clear/Set all flags overridden by options, after probe.

commit 21953ee5013d6632bee90ec89f2df59c69050db0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 26 18:55:15 2020 +0200

    x86/cpu: Export native_write_cr4() only when CONFIG_LKTDM=m
    
    Modules have no business poking into this but fixing this is for later.
    
     [ bp: Carve out from an earlier patch. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20200421092558.939985695@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 82042f40fc45..eab3ebd22927 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -387,7 +387,9 @@ void native_write_cr4(unsigned long val)
 			  bits_missing);
 	}
 }
+#if IS_MODULE(CONFIG_LKDTM)
 EXPORT_SYMBOL_GPL(native_write_cr4);
+#endif
 
 void cr4_update_irqsoff(unsigned long set, unsigned long clear)
 {

commit d8f0b35331c4423e033f81f10eb5e0c7e4e1dcec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 21 11:20:29 2020 +0200

    x86/cpu: Uninline CR4 accessors
    
    cpu_tlbstate is exported because various TLB-related functions need
    access to it, but cpu_tlbstate is sensitive information which should
    only be accessed by well-contained kernel functions and not be directly
    exposed to modules.
    
    The various CR4 accessors require cpu_tlbstate as the CR4 shadow cache
    is located there.
    
    In preparation for unexporting cpu_tlbstate, create a builtin function
    for manipulating CR4 and rework the various helpers to use it.
    
    No functional change.
    
     [ bp: push the export of native_write_cr4() only when CONFIG_LKTDM=m to
       the last patch in the series. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200421092558.939985695@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index bed0cb83fe24..82042f40fc45 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -387,7 +387,28 @@ void native_write_cr4(unsigned long val)
 			  bits_missing);
 	}
 }
-EXPORT_SYMBOL(native_write_cr4);
+EXPORT_SYMBOL_GPL(native_write_cr4);
+
+void cr4_update_irqsoff(unsigned long set, unsigned long clear)
+{
+	unsigned long newval, cr4 = this_cpu_read(cpu_tlbstate.cr4);
+
+	lockdep_assert_irqs_disabled();
+
+	newval = (cr4 & ~clear) | set;
+	if (newval != cr4) {
+		this_cpu_write(cpu_tlbstate.cr4, newval);
+		__write_cr4(newval);
+	}
+}
+EXPORT_SYMBOL(cr4_update_irqsoff);
+
+/* Read the CR4 shadow. */
+unsigned long cr4_read_shadow(void)
+{
+	return this_cpu_read(cpu_tlbstate.cr4);
+}
+EXPORT_SYMBOL_GPL(cr4_read_shadow);
 
 void cr4_init(void)
 {

commit 7e5b3c267d256822407a22fdce6afdf9cd13f9fb
Author: Mark Gross <mgross@linux.intel.com>
Date:   Thu Apr 16 17:54:04 2020 +0200

    x86/speculation: Add Special Register Buffer Data Sampling (SRBDS) mitigation
    
    SRBDS is an MDS-like speculative side channel that can leak bits from the
    random number generator (RNG) across cores and threads. New microcode
    serializes the processor access during the execution of RDRAND and
    RDSEED. This ensures that the shared buffer is overwritten before it is
    released for reuse.
    
    While it is present on all affected CPU models, the microcode mitigation
    is not needed on models that enumerate ARCH_CAPABILITIES[MDS_NO] in the
    cases where TSX is not supported or has been disabled with TSX_CTRL.
    
    The mitigation is activated by default on affected processors and it
    increases latency for RDRAND and RDSEED instructions. Among other
    effects this will reduce throughput from /dev/urandom.
    
    * Enable administrator to configure the mitigation off when desired using
      either mitigations=off or srbds=off.
    
    * Export vulnerability status via sysfs
    
    * Rename file-scoped macros to apply for non-whitelist table initializations.
    
     [ bp: Massage,
       - s/VULNBL_INTEL_STEPPING/VULNBL_INTEL_STEPPINGS/g,
       - do not read arch cap MSR a second time in tsx_fused_off() - just pass it in,
       - flip check in cpu_set_bug_bits() to save an indentation level,
       - reflow comments.
       jpoimboe: s/Mitigated/Mitigation/ in user-visible strings
       tglx: Dropped the fused off magic for now
     ]
    
    Signed-off-by: Mark Gross <mgross@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Neelima Krishnan <neelima.krishnan@intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1131ae032bf2..8293ee514975 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1075,6 +1075,27 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	{}
 };
 
+#define VULNBL_INTEL_STEPPINGS(model, steppings, issues)		   \
+	X86_MATCH_VENDOR_FAM_MODEL_STEPPINGS_FEATURE(INTEL, 6,		   \
+					    INTEL_FAM6_##model, steppings, \
+					    X86_FEATURE_ANY, issues)
+
+#define SRBDS		BIT(0)
+
+static const struct x86_cpu_id cpu_vuln_blacklist[] __initconst = {
+	VULNBL_INTEL_STEPPINGS(IVYBRIDGE,	X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(HASWELL,		X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(HASWELL_L,	X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(HASWELL_G,	X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(BROADWELL_G,	X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(BROADWELL,	X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(SKYLAKE_L,	X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(SKYLAKE,		X86_STEPPING_ANY,		SRBDS),
+	VULNBL_INTEL_STEPPINGS(KABYLAKE_L,	X86_STEPPINGS(0x0, 0xC),	SRBDS),
+	VULNBL_INTEL_STEPPINGS(KABYLAKE,	X86_STEPPINGS(0x0, 0xD),	SRBDS),
+	{}
+};
+
 static bool __init cpu_matches(const struct x86_cpu_id *table, unsigned long which)
 {
 	const struct x86_cpu_id *m = x86_match_cpu(table);
@@ -1142,6 +1163,15 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	     (ia32_cap & ARCH_CAP_TSX_CTRL_MSR)))
 		setup_force_cpu_bug(X86_BUG_TAA);
 
+	/*
+	 * SRBDS affects CPUs which support RDRAND or RDSEED and are listed
+	 * in the vulnerability blacklist.
+	 */
+	if ((cpu_has(c, X86_FEATURE_RDRAND) ||
+	     cpu_has(c, X86_FEATURE_RDSEED)) &&
+	    cpu_matches(cpu_vuln_blacklist, SRBDS))
+		    setup_force_cpu_bug(X86_BUG_SRBDS);
+
 	if (cpu_matches(cpu_vuln_whitelist, NO_MELTDOWN))
 		return;
 
@@ -1594,6 +1624,7 @@ void identify_secondary_cpu(struct cpuinfo_x86 *c)
 	mtrr_ap_init();
 	validate_apic_and_package_id(c);
 	x86_spec_ctrl_setup_ap();
+	update_srbds_msr();
 }
 
 static __init int setup_noclflush(char *arg)

commit 93920f61c2ad7edb01e63323832585796af75fc9
Author: Mark Gross <mgross@linux.intel.com>
Date:   Thu Apr 16 17:32:42 2020 +0200

    x86/cpu: Add 'table' argument to cpu_matches()
    
    To make cpu_matches() reusable for other matching tables, have it take a
    pointer to a x86_cpu_id table as an argument.
    
     [ bp: Flip arguments order. ]
    
    Signed-off-by: Mark Gross <mgross@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index bed0cb83fe24..1131ae032bf2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1075,9 +1075,9 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	{}
 };
 
-static bool __init cpu_matches(unsigned long which)
+static bool __init cpu_matches(const struct x86_cpu_id *table, unsigned long which)
 {
-	const struct x86_cpu_id *m = x86_match_cpu(cpu_vuln_whitelist);
+	const struct x86_cpu_id *m = x86_match_cpu(table);
 
 	return m && !!(m->driver_data & which);
 }
@@ -1097,31 +1097,34 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	u64 ia32_cap = x86_read_arch_cap_msr();
 
 	/* Set ITLB_MULTIHIT bug if cpu is not in the whitelist and not mitigated */
-	if (!cpu_matches(NO_ITLB_MULTIHIT) && !(ia32_cap & ARCH_CAP_PSCHANGE_MC_NO))
+	if (!cpu_matches(cpu_vuln_whitelist, NO_ITLB_MULTIHIT) &&
+	    !(ia32_cap & ARCH_CAP_PSCHANGE_MC_NO))
 		setup_force_cpu_bug(X86_BUG_ITLB_MULTIHIT);
 
-	if (cpu_matches(NO_SPECULATION))
+	if (cpu_matches(cpu_vuln_whitelist, NO_SPECULATION))
 		return;
 
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
 
-	if (!cpu_matches(NO_SPECTRE_V2))
+	if (!cpu_matches(cpu_vuln_whitelist, NO_SPECTRE_V2))
 		setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
 
-	if (!cpu_matches(NO_SSB) && !(ia32_cap & ARCH_CAP_SSB_NO) &&
+	if (!cpu_matches(cpu_vuln_whitelist, NO_SSB) &&
+	    !(ia32_cap & ARCH_CAP_SSB_NO) &&
 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
 	if (ia32_cap & ARCH_CAP_IBRS_ALL)
 		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
 
-	if (!cpu_matches(NO_MDS) && !(ia32_cap & ARCH_CAP_MDS_NO)) {
+	if (!cpu_matches(cpu_vuln_whitelist, NO_MDS) &&
+	    !(ia32_cap & ARCH_CAP_MDS_NO)) {
 		setup_force_cpu_bug(X86_BUG_MDS);
-		if (cpu_matches(MSBDS_ONLY))
+		if (cpu_matches(cpu_vuln_whitelist, MSBDS_ONLY))
 			setup_force_cpu_bug(X86_BUG_MSBDS_ONLY);
 	}
 
-	if (!cpu_matches(NO_SWAPGS))
+	if (!cpu_matches(cpu_vuln_whitelist, NO_SWAPGS))
 		setup_force_cpu_bug(X86_BUG_SWAPGS);
 
 	/*
@@ -1139,7 +1142,7 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	     (ia32_cap & ARCH_CAP_TSX_CTRL_MSR)))
 		setup_force_cpu_bug(X86_BUG_TAA);
 
-	if (cpu_matches(NO_MELTDOWN))
+	if (cpu_matches(cpu_vuln_whitelist, NO_MELTDOWN))
 		return;
 
 	/* Rogue Data Cache Load? No! */
@@ -1148,7 +1151,7 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 
 	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 
-	if (cpu_matches(NO_L1TF))
+	if (cpu_matches(cpu_vuln_whitelist, NO_L1TF))
 		return;
 
 	setup_force_cpu_bug(X86_BUG_L1TF);

commit 2853d5fafb1e21707e89aa2e227c90bb2c1ea4a9
Merge: d5f744f9a2ac a6a60741035b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 19:35:52 2020 -0700

    Merge tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 splitlock updates from Thomas Gleixner:
     "Support for 'split lock' detection:
    
      Atomic operations (lock prefixed instructions) which span two cache
      lines have to acquire the global bus lock. This is at least 1k cycles
      slower than an atomic operation within a cache line and disrupts
      performance on other cores. Aside of performance disruption this is a
      unpriviledged form of DoS.
    
      Some newer CPUs have the capability to raise an #AC trap when such an
      operation is attempted. The detection is by default enabled in warning
      mode which will warn once when a user space application is caught. A
      command line option allows to disable the detection or to select fatal
      mode which will terminate offending applications with SIGBUS"
    
    * tag 'x86-splitlock-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/split_lock: Avoid runtime reads of the TEST_CTRL MSR
      x86/split_lock: Rework the initialization flow of split lock detection
      x86/split_lock: Enable split lock detection by kernel

commit 629b3df7ecb01fddfdf71cb5d3c563d143117c33
Merge: 3442a9ecb8e7 d97828072d0b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Mar 25 15:20:44 2020 +0100

    Merge branch 'x86/cpu' into perf/core, to resolve conflict
    
    Conflicts:
            arch/x86/events/intel/uncore.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f6d502fcfc51ae025f18a8de8f1ed2ab34503742
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 20 14:13:48 2020 +0100

    x86/cpu/bugs: Convert to new matching macros
    
    The new macro set has a consistent namespace and uses C99 initializers
    instead of the grufty C89 ones.
    
    The local wrappers have to stay as they are tailored to tame the hardware
    vulnerability mess.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Link: https://lkml.kernel.org/r/20200320131508.934926587@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 52c9bfbbdb2a..f4c672e5ce9e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1008,8 +1008,8 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #define NO_ITLB_MULTIHIT	BIT(7)
 #define NO_SPECTRE_V2		BIT(8)
 
-#define VULNWL(_vendor, _family, _model, _whitelist)	\
-	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }
+#define VULNWL(vendor, family, model, whitelist)	\
+	X86_MATCH_VENDOR_FAM_MODEL(vendor, family, model, whitelist)
 
 #define VULNWL_INTEL(model, whitelist)		\
 	VULNWL(INTEL, 6, INTEL_FAM6_##model, whitelist)

commit 735a6dd02222d8d070c7bb748f25895239ca8c92
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Feb 26 15:16:15 2020 -0800

    x86/pkeys: Manually set X86_FEATURE_OSPKE to preserve existing changes
    
    Explicitly set X86_FEATURE_OSPKE via set_cpu_cap() instead of calling
    get_cpu_cap() to pull the feature bit from CPUID after enabling CR4.PKE.
    Invoking get_cpu_cap() effectively wipes out any {set,clear}_cpu_cap()
    changes that were made between this_cpu->c_init() and setup_pku(), as
    all non-synthetic feature words are reinitialized from the CPU's CPUID
    values.
    
    Blasting away capability updates manifests most visibility when running
    on a VMX capable CPU, but with VMX disabled by BIOS.  To indicate that
    VMX is disabled, init_ia32_feat_ctl() clears X86_FEATURE_VMX, using
    clear_cpu_cap() instead of setup_clear_cpu_cap() so that KVM can report
    which CPU is misconfigured (KVM needs to probe every CPU anyways).
    Restoring X86_FEATURE_VMX from CPUID causes KVM to think VMX is enabled,
    ultimately leading to an unexpected #GP when KVM attempts to do VMXON.
    
    Arguably, init_ia32_feat_ctl() should use setup_clear_cpu_cap() and let
    KVM figure out a different way to report the misconfigured CPU, but VMX
    is not the only feature bit that is affected, i.e. there is precedent
    that tweaking feature bits via {set,clear}_cpu_cap() after ->c_init()
    is expected to work.  Most notably, x86_init_rdrand()'s clearing of
    X86_FEATURE_RDRAND when RDRAND malfunctions is also overwritten.
    
    Fixes: 0697694564c8 ("x86/mm/pkeys: Actually enable Memory Protection Keys in the CPU")
    Reported-by: Jacob Keller <jacob.e.keller@intel.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Dave Hansen <dave.hansen@linux.intel.com>
    Tested-by: Jacob Keller <jacob.e.keller@intel.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20200226231615.13664-1-sean.j.christopherson@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 52c9bfbbdb2a..4cdb123ff66a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -445,7 +445,7 @@ static __always_inline void setup_pku(struct cpuinfo_x86 *c)
 	 * cpuid bit to be set.  We need to ensure that we
 	 * update that bit in this CPU's "cpu_info".
 	 */
-	get_cpu_cap(c);
+	set_cpu_cap(c, X86_FEATURE_OSPKE);
 }
 
 #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS

commit 6650cdd9a8ccf00555dbbe743d58541ad8feb6a7
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Sun Jan 26 12:05:35 2020 -0800

    x86/split_lock: Enable split lock detection by kernel
    
    A split-lock occurs when an atomic instruction operates on data that spans
    two cache lines. In order to maintain atomicity the core takes a global bus
    lock.
    
    This is typically >1000 cycles slower than an atomic operation within a
    cache line. It also disrupts performance on other cores (which must wait
    for the bus lock to be released before their memory operations can
    complete). For real-time systems this may mean missing deadlines. For other
    systems it may just be very annoying.
    
    Some CPUs have the capability to raise an #AC trap when a split lock is
    attempted.
    
    Provide a command line option to give the user choices on how to handle
    this:
    
    split_lock_detect=
            off     - not enabled (no traps for split locks)
            warn    - warn once when an application does a
                      split lock, but allow it to continue
                      running.
            fatal   - Send SIGBUS to applications that cause split lock
    
    On systems that support split lock detection the default is "warn". Note
    that if the kernel hits a split lock in any mode other than "off" it will
    OOPs.
    
    One implementation wrinkle is that the MSR to control the split lock
    detection is per-core, not per thread. This might result in some short
    lived races on HT systems in "warn" mode if Linux tries to enable on one
    thread while disabling on the other. Race analysis by Sean Christopherson:
    
      - Toggling of split-lock is only done in "warn" mode.  Worst case
        scenario of a race is that a misbehaving task will generate multiple
        #AC exceptions on the same instruction.  And this race will only occur
        if both siblings are running tasks that generate split-lock #ACs, e.g.
        a race where sibling threads are writing different values will only
        occur if CPUx is disabling split-lock after an #AC and CPUy is
        re-enabling split-lock after *its* previous task generated an #AC.
      - Transitioning between off/warn/fatal modes at runtime isn't supported
        and disabling is tracked per task, so hardware will always reach a steady
        state that matches the configured mode.  I.e. split-lock is guaranteed to
        be enabled in hardware once all _TIF_SLD threads have been scheduled out.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Co-developed-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20200126200535.GB30377@agluck-desk2.amr.corp.intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 52c9bfbbdb2a..bf282d7805de 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1224,6 +1224,8 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	cpu_set_bug_bits(c);
 
+	cpu_set_core_cap_bits(c);
+
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32

commit ccaaaf6fe5a5e1fffca5cca0f3fc4ec84d7ae752
Merge: 35c222fd3236 45fc24e89b7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 30 16:11:50 2020 -0800

    Merge tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx
    
    Pull x86 MPX removal from Dave Hansen:
     "MPX requires recompiling applications, which requires compiler
      support. Unfortunately, GCC 9.1 is expected to be be released without
      support for MPX. This means that there was only a relatively small
      window where folks could have ever used MPX. It failed to gain wide
      adoption in the industry, and Linux was the only mainstream OS to ever
      support it widely.
    
      Support for the feature may also disappear on future processors.
    
      This set completes the process that we started during the 5.4 merge
      window when the MPX prctl()s were removed. XSAVE support is left in
      place, which allows MPX-using KVM guests to continue to function"
    
    * tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx:
      x86/mpx: remove MPX from arch/x86
      mm: remove arch_bprm_mm_init() hook
      x86/mpx: remove bounds exception code
      x86/mpx: remove build infrastructure
      x86/alternatives: add missing insn.h include

commit c0275ae758f8ce72306da200b195d1e1d66d0a8d
Merge: f6170f0afbe2 283bab980978
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 28 12:46:42 2020 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu-features updates from Ingo Molnar:
     "The biggest change in this cycle was a large series from Sean
      Christopherson to clean up the handling of VMX features. This both
      fixes bugs/inconsistencies and makes the code more coherent and
      future-proof.
    
      There are also two cleanups and a minor TSX syslog messages
      enhancement"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/cpu: Remove redundant cpu_detect_cache_sizes() call
      x86/cpu: Print "VMX disabled" error message iff KVM is enabled
      KVM: VMX: Allow KVM_INTEL when building for Centaur and/or Zhaoxin CPUs
      perf/x86: Provide stubs of KVM helpers for non-Intel CPUs
      KVM: VMX: Use VMX_FEATURE_* flags to define VMCS control bits
      KVM: VMX: Check for full VMX support when verifying CPU compatibility
      KVM: VMX: Use VMX feature flag to query BIOS enabling
      KVM: VMX: Drop initialization of IA32_FEAT_CTL MSR
      x86/cpufeatures: Add flag to track whether MSR IA32_FEAT_CTL is configured
      x86/cpu: Set synthetic VMX cpufeatures during init_ia32_feat_ctl()
      x86/cpu: Print VMX flags in /proc/cpuinfo using VMX_FEATURES_*
      x86/cpu: Detect VMX features on Intel, Centaur and Zhaoxin CPUs
      x86/vmx: Introduce VMX_FEATURES_*
      x86/cpu: Clear VMX feature flag if VMX is not fully enabled
      x86/zhaoxin: Use common IA32_FEAT_CTL MSR initialization
      x86/centaur: Use common IA32_FEAT_CTL MSR initialization
      x86/mce: WARN once if IA32_FEAT_CTL MSR is left unlocked
      x86/intel: Initialize IA32_FEAT_CTL MSR at boot
      tools/x86: Sync msr-index.h from kernel sources
      selftests, kvm: Replace manual MSR defs with common msr-index.h
      ...

commit 6da49d1abd2c2b70063f3606e3974c13d22497a1
Merge: 4244057c3da1 3c749b81ee99
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 28 12:11:23 2020 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups all around the map"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/CPU/AMD: Remove amd_get_topology_early()
      x86/tsc: Remove redundant assignment
      x86/crash: Use resource_size()
      x86/cpu: Add a missing prototype for arch_smt_update()
      x86/nospec: Remove unused RSB_FILL_LOOPS
      x86/vdso: Provide missing include file
      x86/Kconfig: Correct spelling and punctuation
      Documentation/x86/boot: Fix typo
      x86/boot: Fix a comment's incorrect file reference
      x86/process: Remove set but not used variables prev and next
      x86/Kconfig: Fix Kconfig indentation

commit 634cd4b6afe15dca8df02bcba242b9b0c5e9b5a5
Merge: d99391ec2b42 ac6119e7f25b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 28 09:03:40 2020 -0800

    Merge branch 'efi-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull EFI updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Cleanup of the GOP [graphics output] handling code in the EFI stub
    
       - Complete refactoring of the mixed mode handling in the x86 EFI stub
    
       - Overhaul of the x86 EFI boot/runtime code
    
       - Increase robustness for mixed mode code
    
       - Add the ability to disable DMA at the root port level in the EFI
         stub
    
       - Get rid of RWX mappings in the EFI memory map and page tables,
         where possible
    
       - Move the support code for the old EFI memory mapping style into its
         only user, the SGI UV1+ support code.
    
       - plus misc fixes, updates, smaller cleanups.
    
      ... and due to interactions with the RWX changes, another round of PAT
      cleanups make a guest appearance via the EFI tree - with no side
      effects intended"
    
    * 'efi-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (75 commits)
      efi/x86: Disable instrumentation in the EFI runtime handling code
      efi/libstub/x86: Fix EFI server boot failure
      efi/x86: Disallow efi=old_map in mixed mode
      x86/boot/compressed: Relax sed symbol type regex for LLVM ld.lld
      efi/x86: avoid KASAN false positives when accessing the 1: 1 mapping
      efi: Fix handling of multiple efi_fake_mem= entries
      efi: Fix efi_memmap_alloc() leaks
      efi: Add tracking for dynamically allocated memmaps
      efi: Add a flags parameter to efi_memory_map
      efi: Fix comment for efi_mem_type() wrt absent physical addresses
      efi/arm: Defer probe of PCIe backed efifb on DT systems
      efi/x86: Limit EFI old memory map to SGI UV machines
      efi/x86: Avoid RWX mappings for all of DRAM
      efi/x86: Don't map the entire kernel text RW for mixed mode
      x86/mm: Fix NX bit clearing issue in kernel_map_pages_in_pgd
      efi/libstub/x86: Fix unused-variable warning
      efi/libstub/x86: Use mandatory 16-byte stack alignment in mixed mode
      efi/libstub/x86: Use const attribute for efi_is_64bit()
      efi: Allow disabling PCI busmastering on bridges during boot
      efi/x86: Allow translating 64-bit arguments for mixed mode calls
      ...

commit 45fc24e89b7cc2e227b2f03d99dda0a2204bf383
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jan 23 10:41:20 2020 -0800

    x86/mpx: remove MPX from arch/x86
    
    From: Dave Hansen <dave.hansen@linux.intel.com>
    
    MPX is being removed from the kernel due to a lack of support
    in the toolchain going forward (gcc).
    
    This removes all the remaining (dead at this point) MPX handling
    code remaining in the tree.  The only remaining code is the XSAVE
    support for MPX state which is currently needd for KVM to handle
    VMs which might use MPX.
    
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: x86@kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fffe21945374..be2c85bf9c56 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -165,22 +165,6 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
-static int __init x86_mpx_setup(char *s)
-{
-	/* require an exact match without trailing characters */
-	if (strlen(s))
-		return 0;
-
-	/* do not emit a message if the feature is not present */
-	if (!boot_cpu_has(X86_FEATURE_MPX))
-		return 1;
-
-	setup_clear_cpu_cap(X86_FEATURE_MPX);
-	pr_info("nompx: Intel Memory Protection Extensions (MPX) disabled\n");
-	return 1;
-}
-__setup("nompx", x86_mpx_setup);
-
 #ifdef CONFIG_X86_64
 static int __init x86_nopcid_setup(char *s)
 {
@@ -307,8 +291,6 @@ static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 static __init int setup_disable_smep(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_SMEP);
-	/* Check for things that depend on SMEP being enabled: */
-	check_mpx_erratum(&boot_cpu_data);
 	return 1;
 }
 __setup("nosmep", setup_disable_smep);

commit a84de2fa962c1b0551653fe245d6cb5f6129179c
Author: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
Date:   Fri Jan 17 10:24:32 2020 +0800

    x86/speculation/swapgs: Exclude Zhaoxin CPUs from SWAPGS vulnerability
    
    New Zhaoxin family 7 CPUs are not affected by the SWAPGS vulnerability. So
    mark these CPUs in the cpu vulnerability whitelist accordingly.
    
    Signed-off-by: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/1579227872-26972-3-git-send-email-TonyWWang-oc@zhaoxin.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6048bd374133..ca4a0d2cc88f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1087,8 +1087,8 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
 
 	/* Zhaoxin Family 7 */
-	VULNWL(CENTAUR,	7, X86_MODEL_ANY,	NO_SPECTRE_V2),
-	VULNWL(ZHAOXIN,	7, X86_MODEL_ANY,	NO_SPECTRE_V2),
+	VULNWL(CENTAUR,	7, X86_MODEL_ANY,	NO_SPECTRE_V2 | NO_SWAPGS),
+	VULNWL(ZHAOXIN,	7, X86_MODEL_ANY,	NO_SPECTRE_V2 | NO_SWAPGS),
 	{}
 };
 

commit 1e41a766c98b481400ab8c5a7aa8ea63a1bb03de
Author: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
Date:   Fri Jan 17 10:24:31 2020 +0800

    x86/speculation/spectre_v2: Exclude Zhaoxin CPUs from SPECTRE_V2
    
    New Zhaoxin family 7 CPUs are not affected by SPECTRE_V2. So define a
    separate cpu_vuln_whitelist bit NO_SPECTRE_V2 and add these CPUs to the cpu
    vulnerability whitelist.
    
    Signed-off-by: Tony W Wang-oc <TonyWWang-oc@zhaoxin.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/1579227872-26972-2-git-send-email-TonyWWang-oc@zhaoxin.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2e4d90294fe6..6048bd374133 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1023,6 +1023,7 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #define MSBDS_ONLY		BIT(5)
 #define NO_SWAPGS		BIT(6)
 #define NO_ITLB_MULTIHIT	BIT(7)
+#define NO_SPECTRE_V2		BIT(8)
 
 #define VULNWL(_vendor, _family, _model, _whitelist)	\
 	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }
@@ -1084,6 +1085,10 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	/* FAMILY_ANY must be last, otherwise 0x0f - 0x12 matches won't work */
 	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
 	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
+
+	/* Zhaoxin Family 7 */
+	VULNWL(CENTAUR,	7, X86_MODEL_ANY,	NO_SPECTRE_V2),
+	VULNWL(ZHAOXIN,	7, X86_MODEL_ANY,	NO_SPECTRE_V2),
 	{}
 };
 
@@ -1116,7 +1121,9 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 		return;
 
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
-	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+
+	if (!cpu_matches(NO_SPECTRE_V2))
+		setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
 
 	if (!cpu_matches(NO_SSB) && !(ia32_cap & ARCH_CAP_SSB_NO) &&
 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))

commit b47ce1fed42eeb9ac8c07fcda6c795884826723d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 20 20:45:04 2019 -0800

    x86/cpu: Detect VMX features on Intel, Centaur and Zhaoxin CPUs
    
    Add an entry in struct cpuinfo_x86 to track VMX capabilities and fill
    the capabilities during IA32_FEAT_CTL MSR initialization.
    
    Make the VMX capabilities dependent on IA32_FEAT_CTL and
    X86_FEATURE_NAMES so as to avoid unnecessary overhead on CPUs that can't
    possibly support VMX, or when /proc/cpuinfo is not available.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20191221044513.21680-11-sean.j.christopherson@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2e4d90294fe6..a5c526e004ae 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1449,6 +1449,9 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 #endif
 	c->x86_cache_alignment = c->x86_clflush_size;
 	memset(&c->x86_capability, 0, sizeof(c->x86_capability));
+#ifdef CONFIG_X86_VMX_FEATURE_NAMES
+	memset(&c->vmx_capability, 0, sizeof(c->vmx_capability));
+#endif
 
 	generic_identify(c);
 

commit b47a36982dbdcd9d96fdce5169af984ef5bdd110
Author: Benjamin Thiel <b.thiel@posteo.de>
Date:   Thu Jan 9 13:17:23 2020 +0100

    x86/cpu: Add a missing prototype for arch_smt_update()
    
    .. in order to fix a -Wmissing-prototype warning.
    
    No functional change.
    
    Signed-off-by: Benjamin Thiel <b.thiel@posteo.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20200109121723.8151-1-b.thiel@posteo.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2e4d90294fe6..6b95f18255fa 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -14,6 +14,7 @@
 #include <linux/sched/mm.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/task.h>
+#include <linux/sched/smt.h>
 #include <linux/init.h>
 #include <linux/kprobes.h>
 #include <linux/kgdb.h>

commit eb243d1d28663c9b92010973a6a3ffa947f682ba
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 20 15:33:57 2019 +0100

    x86/mm/pat: Rename <asm/pat.h> => <asm/memtype.h>
    
    pat.h is a file whose main purpose is to provide the memtype_*() APIs.
    
    PAT is the low level hardware mechanism - but the high level abstraction
    is memtype.
    
    So name the header <memtype.h> as well - this goes hand in hand with memtype.c
    and memtype_interval.c.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2e4d90294fe6..9d6a35a4586e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -49,7 +49,7 @@
 #include <asm/cpu.h>
 #include <asm/mce.h>
 #include <asm/msr.h>
-#include <asm/pat.h>
+#include <asm/memtype.h>
 #include <asm/microcode.h>
 #include <asm/microcode_intel.h>
 #include <asm/intel-family.h>

commit dc4e0021b00b5a4ecba56fae509217776592b0aa
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Nov 26 18:27:16 2019 +0100

    x86/doublefault/32: Move #DF stack and TSS to cpu_entry_area
    
    There are three problems with the current layout of the doublefault
    stack and TSS.  First, the TSS is only cacheline-aligned, which is
    not enough -- if the hardware portion of the TSS (struct x86_hw_tss)
    crosses a page boundary, horrible things happen [0].  Second, the
    stack and TSS are global, so simultaneous double faults on different
    CPUs will cause massive corruption.  Third, the whole mechanism
    won't work if user CR3 is loaded, resulting in a triple fault [1].
    
    Let the doublefault stack and TSS share a page (which prevents the
    TSS from spanning a page boundary), make it percpu, and move it into
    cpu_entry_area.  Teach the stack dump code about the doublefault
    stack.
    
    [0] Real hardware will read past the end of the page onto the next
        *physical* page if a task switch happens.  Virtual machines may
        have any number of bugs, and I would consider it reasonable for
        a VM to summarily kill the guest if it tries to task-switch to
        a page-spanning TSS.
    
    [1] Real hardware triple faults.  At least some VMs seem to hang.
        I'm not sure what's going on.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index baa2fed8deb6..2e4d90294fe6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -24,6 +24,7 @@
 #include <asm/stackprotector.h>
 #include <asm/perf_event.h>
 #include <asm/mmu_context.h>
+#include <asm/doublefault.h>
 #include <asm/archrandom.h>
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
@@ -1814,8 +1815,6 @@ static inline void tss_setup_ist(struct tss_struct *tss)
 	tss->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
 }
 
-static inline void gdt_setup_doublefault_tss(int cpu) { }
-
 #else /* CONFIG_X86_64 */
 
 static inline void setup_getcpu(int cpu) { }
@@ -1827,13 +1826,6 @@ static inline void ucode_cpu_init(int cpu)
 
 static inline void tss_setup_ist(struct tss_struct *tss) { }
 
-static inline void gdt_setup_doublefault_tss(int cpu)
-{
-#ifdef CONFIG_DOUBLEFAULT
-	/* Set up the doublefault TSS pointer in the GDT */
-	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
-#endif
-}
 #endif /* !CONFIG_X86_64 */
 
 static inline void tss_setup_io_bitmap(struct tss_struct *tss)
@@ -1923,7 +1915,7 @@ void cpu_init(void)
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
 
-	gdt_setup_doublefault_tss(cpu);
+	doublefault_init_cpu_tss();
 
 	fpu__init_cpu();
 

commit ab851d49f6bfc781edd8bd44c72ec1e49211670b
Merge: 1d87200446f1 e3cb0c7102f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 11:12:02 2019 -0800

    Merge branch 'x86-iopl-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 iopl updates from Ingo Molnar:
     "This implements a nice simplification of the iopl and ioperm code that
      Thomas Gleixner discovered: we can implement the IO privilege features
      of the iopl system call by using the IO permission bitmap in
      permissive mode, while trapping CLI/STI/POPF/PUSHF uses in user-space
      if they change the interrupt flag.
    
      This implements that feature, with testing facilities and related
      cleanups"
    
    [ "Simplification" may be an over-statement. The main goal is to avoid
      the cli/sti of iopl by effectively implementing the IO port access
      parts of iopl in terms of ioperm.
    
      This may end up not workign well in case people actually depend on
      cli/sti being available, or if there are mixed uses of iopl and
      ioperm. We will see..       - Linus ]
    
    * 'x86-iopl-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      x86/ioperm: Fix use of deprecated config option
      x86/entry/32: Clarify register saving in __switch_to_asm()
      selftests/x86/iopl: Extend test to cover IOPL emulation
      x86/ioperm: Extend IOPL config to control ioperm() as well
      x86/iopl: Remove legacy IOPL option
      x86/iopl: Restrict iopl() permission scope
      x86/iopl: Fixup misleading comment
      selftests/x86/ioperm: Extend testing so the shared bitmap is exercised
      x86/ioperm: Share I/O bitmap if identical
      x86/ioperm: Remove bitmap if all permissions dropped
      x86/ioperm: Move TSS bitmap update to exit to user work
      x86/ioperm: Add bitmap sequence number
      x86/ioperm: Move iobitmap data into a struct
      x86/tss: Move I/O bitmap data into a seperate struct
      x86/io: Speedup schedule out of I/O bitmap user
      x86/ioperm: Avoid bitmap allocation if no permissions are set
      x86/ioperm: Simplify first ioperm() invocation logic
      x86/iopl: Cleanup include maze
      x86/tss: Fix and move VMX BUILD_BUG_ON()
      x86/cpu: Unify cpu_init()
      ...

commit a25bbc2644f01a9e680af4f760b54bd4834fdfec
Merge: 85fbf15bc9ac db8c33f8b5be 446e693ca30b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 08:58:08 2019 -0800

    Merge branches 'x86-cpu-for-linus' and 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu and fpu updates from Ingo Molnar:
    
     - math-emu fixes
    
     - CPUID updates
    
     - sanity-check RDRAND output to see whether the CPU at least pretends
       to produce random data
    
     - various unaligned-access across cachelines fixes in preparation of
       hardware level split-lock detection
    
     - fix MAXSMP constraints to not allow !CPUMASK_OFFSTACK kernels with
       larger than 512 NR_CPUS
    
     - misc FPU related cleanups
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpu: Align the x86_capability array to size of unsigned long
      x86/cpu: Align cpu_caps_cleared and cpu_caps_set to unsigned long
      x86/umip: Make the comments vendor-agnostic
      x86/Kconfig: Rename UMIP config parameter
      x86/Kconfig: Enforce limit of 512 CPUs with MAXSMP and no CPUMASK_OFFSTACK
      x86/cpufeatures: Add feature bit RDPRU on AMD
      x86/math-emu: Limit MATH_EMULATION to 486SX compatibles
      x86/math-emu: Check __copy_from_user() result
      x86/rdrand: Sanity-check RDRAND output
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Use XFEATURE_FP/SSE enum values instead of hardcoded numbers
      x86/fpu: Shrink space allocated for xstate_comp_offsets
      x86/fpu: Update stale variable name in comment

commit 111e7b15cf10f6e973ccf537c70c66a5de539060
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 12 21:40:33 2019 +0100

    x86/ioperm: Extend IOPL config to control ioperm() as well
    
    If iopl() is disabled, then providing ioperm() does not make much sense.
    
    Rename the config option and disable/enable both syscalls with it. Guard
    the code with #ifdefs where appropriate.
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7bf402be13bb..6f6ca6bd58d6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1804,6 +1804,22 @@ static inline void gdt_setup_doublefault_tss(int cpu)
 }
 #endif /* !CONFIG_X86_64 */
 
+static inline void tss_setup_io_bitmap(struct tss_struct *tss)
+{
+	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
+
+#ifdef CONFIG_X86_IOPL_IOPERM
+	tss->io_bitmap.prev_max = 0;
+	tss->io_bitmap.prev_sequence = 0;
+	memset(tss->io_bitmap.bitmap, 0xff, sizeof(tss->io_bitmap.bitmap));
+	/*
+	 * Invalidate the extra array entry past the end of the all
+	 * permission bitmap as required by the hardware.
+	 */
+	tss->io_bitmap.mapall[IO_BITMAP_LONGS] = ~0UL;
+#endif
+}
+
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT
@@ -1860,15 +1876,7 @@ void cpu_init(void)
 
 	/* Initialize the TSS. */
 	tss_setup_ist(tss);
-	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
-	tss->io_bitmap.prev_max = 0;
-	tss->io_bitmap.prev_sequence = 0;
-	memset(tss->io_bitmap.bitmap, 0xff, sizeof(tss->io_bitmap.bitmap));
-	/*
-	 * Invalidate the extra array entry past the end of the all
-	 * permission bitmap as required by the hardware.
-	 */
-	tss->io_bitmap.mapall[IO_BITMAP_LONGS] = ~0UL;
+	tss_setup_io_bitmap(tss);
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 
 	load_TR_desc();

commit c8137ace56383688af911fea5934c71ad158135e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:28 2019 +0100

    x86/iopl: Restrict iopl() permission scope
    
    The access to the full I/O port range can be also provided by the TSS I/O
    bitmap, but that would require to copy 8k of data on scheduling in the
    task. As shown with the sched out optimization TSS.io_bitmap_base can be
    used to switch the incoming task to a preallocated I/O bitmap which has all
    bits zero, i.e. allows access to all I/O ports.
    
    Implementing this allows to provide an iopl() emulation mode which restricts
    the IOPL level 3 permissions to I/O port access but removes the STI/CLI
    permission which is coming with the hardware IOPL mechansim.
    
    Provide a config option to switch IOPL to emulation mode, make it the
    default and while at it also provide an option to disable IOPL completely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 79dd544bb974..7bf402be13bb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1864,6 +1864,11 @@ void cpu_init(void)
 	tss->io_bitmap.prev_max = 0;
 	tss->io_bitmap.prev_sequence = 0;
 	memset(tss->io_bitmap.bitmap, 0xff, sizeof(tss->io_bitmap.bitmap));
+	/*
+	 * Invalidate the extra array entry past the end of the all
+	 * permission bitmap as required by the hardware.
+	 */
+	tss->io_bitmap.mapall[IO_BITMAP_LONGS] = ~0UL;
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 
 	load_TR_desc();

commit 060aa16fdb7c5078a4159a76e5dc87d6a493af9b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:22 2019 +0100

    x86/ioperm: Add bitmap sequence number
    
    Add a globally unique sequence number which is incremented when ioperm() is
    changing the I/O bitmap of a task. Store the new sequence number in the
    io_bitmap structure and compare it with the sequence number of the I/O
    bitmap which was last loaded on a CPU. Only update the bitmap if the
    sequence is different.
    
    That should further reduce the overhead of I/O bitmap scheduling when there
    are only a few I/O bitmap users on the system.
    
    The 64bit sequence counter is sufficient. A wraparound of the sequence
    counter assuming an ioperm() call every nanosecond would require about 584
    years of uptime.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3aee167246f7..79dd544bb974 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1862,6 +1862,7 @@ void cpu_init(void)
 	tss_setup_ist(tss);
 	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
 	tss->io_bitmap.prev_max = 0;
+	tss->io_bitmap.prev_sequence = 0;
 	memset(tss->io_bitmap.bitmap, 0xff, sizeof(tss->io_bitmap.bitmap));
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 

commit f5848e5fd2f813c3a8009a642dfbcf635287c199
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 12 18:45:29 2019 +0100

    x86/tss: Move I/O bitmap data into a seperate struct
    
    Move the non hardware portion of I/O bitmap data into a seperate struct for
    readability sake.
    
    Originally-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8c1000a57b55..3aee167246f7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1861,8 +1861,8 @@ void cpu_init(void)
 	/* Initialize the TSS. */
 	tss_setup_ist(tss);
 	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
-	tss->io_bitmap_prev_max = 0;
-	memset(tss->io_bitmap, 0xff, sizeof(tss->io_bitmap));
+	tss->io_bitmap.prev_max = 0;
+	memset(tss->io_bitmap.bitmap, 0xff, sizeof(tss->io_bitmap.bitmap));
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 
 	load_TR_desc();

commit ecc7e37d4dadd16f6be125ca496feccd05454da4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:20 2019 +0100

    x86/io: Speedup schedule out of I/O bitmap user
    
    There is no requirement to update the TSS I/O bitmap when a thread using it is
    scheduled out and the incoming thread does not use it.
    
    For the permission check based on the TSS I/O bitmap the CPU calculates the memory
    location of the I/O bitmap by the address of the TSS and the io_bitmap_base member
    of the tss_struct. The easiest way to invalidate the I/O bitmap is to switch the
    offset to an address outside of the TSS limit.
    
    If an I/O instruction is issued from user space the TSS limit causes #GP to be
    raised in the same was as valid I/O bitmap with all bits set to 1 would do.
    
    This removes the extra work when an I/O bitmap using task is scheduled out
    and puts the burden on the rare I/O bitmap users when they are scheduled
    in.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d52ec1a82120..8c1000a57b55 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1860,7 +1860,8 @@ void cpu_init(void)
 
 	/* Initialize the TSS. */
 	tss_setup_ist(tss);
-	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
+	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
+	tss->io_bitmap_prev_max = 0;
 	memset(tss->io_bitmap, 0xff, sizeof(tss->io_bitmap));
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 

commit 505b789996f64bdbfcc5847dd4b5076fc7c50274
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 11 23:03:17 2019 +0100

    x86/cpu: Unify cpu_init()
    
    Similar to copy_thread_tls() the 32bit and 64bit implementations of
    cpu_init() are very similar and unification avoids duplicate changes in the
    future.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9ae7d1bcd4f4..d52ec1a82120 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -53,10 +53,7 @@
 #include <asm/microcode_intel.h>
 #include <asm/intel-family.h>
 #include <asm/cpu_device_id.h>
-
-#ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/uv/uv.h>
-#endif
 
 #include "cpu.h"
 
@@ -1749,7 +1746,7 @@ static void wait_for_master_cpu(int cpu)
 }
 
 #ifdef CONFIG_X86_64
-static void setup_getcpu(int cpu)
+static inline void setup_getcpu(int cpu)
 {
 	unsigned long cpudata = vdso_encode_cpunode(cpu, early_cpu_to_node(cpu));
 	struct desc_struct d = { };
@@ -1769,7 +1766,43 @@ static void setup_getcpu(int cpu)
 
 	write_gdt_entry(get_cpu_gdt_rw(cpu), GDT_ENTRY_CPUNODE, &d, DESCTYPE_S);
 }
+
+static inline void ucode_cpu_init(int cpu)
+{
+	if (cpu)
+		load_ucode_ap();
+}
+
+static inline void tss_setup_ist(struct tss_struct *tss)
+{
+	/* Set up the per-CPU TSS IST stacks */
+	tss->x86_tss.ist[IST_INDEX_DF] = __this_cpu_ist_top_va(DF);
+	tss->x86_tss.ist[IST_INDEX_NMI] = __this_cpu_ist_top_va(NMI);
+	tss->x86_tss.ist[IST_INDEX_DB] = __this_cpu_ist_top_va(DB);
+	tss->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
+}
+
+static inline void gdt_setup_doublefault_tss(int cpu) { }
+
+#else /* CONFIG_X86_64 */
+
+static inline void setup_getcpu(int cpu) { }
+
+static inline void ucode_cpu_init(int cpu)
+{
+	show_ucode_info_early();
+}
+
+static inline void tss_setup_ist(struct tss_struct *tss) { }
+
+static inline void gdt_setup_doublefault_tss(int cpu)
+{
+#ifdef CONFIG_DOUBLEFAULT
+	/* Set up the doublefault TSS pointer in the GDT */
+	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
 #endif
+}
+#endif /* !CONFIG_X86_64 */
 
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
@@ -1777,21 +1810,15 @@ static void setup_getcpu(int cpu)
  * and IDT. We reload them nevertheless, this function acts as a
  * 'CPU state barrier', nothing should get across.
  */
-#ifdef CONFIG_X86_64
-
 void cpu_init(void)
 {
+	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
+	struct task_struct *cur = current;
 	int cpu = raw_smp_processor_id();
-	struct task_struct *me;
-	struct tss_struct *t;
-	int i;
 
 	wait_for_master_cpu(cpu);
 
-	if (cpu)
-		load_ucode_ap();
-
-	t = &per_cpu(cpu_tss_rw, cpu);
+	ucode_cpu_init(cpu);
 
 #ifdef CONFIG_NUMA
 	if (this_cpu_read(numa_node) == 0 &&
@@ -1800,63 +1827,48 @@ void cpu_init(void)
 #endif
 	setup_getcpu(cpu);
 
-	me = current;
-
 	pr_debug("Initializing CPU#%d\n", cpu);
 
-	cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+	if (IS_ENABLED(CONFIG_X86_64) || cpu_feature_enabled(X86_FEATURE_VME) ||
+	    boot_cpu_has(X86_FEATURE_TSC) || boot_cpu_has(X86_FEATURE_DE))
+		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
 	 * and set up the GDT descriptor:
 	 */
-
 	switch_to_new_gdt(cpu);
-	loadsegment(fs, 0);
-
 	load_current_idt();
 
-	memset(me->thread.tls_array, 0, GDT_ENTRY_TLS_ENTRIES * 8);
-	syscall_init();
+	if (IS_ENABLED(CONFIG_X86_64)) {
+		loadsegment(fs, 0);
+		memset(cur->thread.tls_array, 0, GDT_ENTRY_TLS_ENTRIES * 8);
+		syscall_init();
 
-	wrmsrl(MSR_FS_BASE, 0);
-	wrmsrl(MSR_KERNEL_GS_BASE, 0);
-	barrier();
+		wrmsrl(MSR_FS_BASE, 0);
+		wrmsrl(MSR_KERNEL_GS_BASE, 0);
+		barrier();
 
-	x86_configure_nx();
-	x2apic_setup();
-
-	/*
-	 * set up and load the per-CPU TSS
-	 */
-	if (!t->x86_tss.ist[0]) {
-		t->x86_tss.ist[IST_INDEX_DF] = __this_cpu_ist_top_va(DF);
-		t->x86_tss.ist[IST_INDEX_NMI] = __this_cpu_ist_top_va(NMI);
-		t->x86_tss.ist[IST_INDEX_DB] = __this_cpu_ist_top_va(DB);
-		t->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
+		x2apic_setup();
 	}
 
-	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
-
-	/*
-	 * <= is required because the CPU will access up to
-	 * 8 bits beyond the end of the IO permission bitmap.
-	 */
-	for (i = 0; i <= IO_BITMAP_LONGS; i++)
-		t->io_bitmap[i] = ~0UL;
-
 	mmgrab(&init_mm);
-	me->active_mm = &init_mm;
-	BUG_ON(me->mm);
+	cur->active_mm = &init_mm;
+	BUG_ON(cur->mm);
 	initialize_tlbstate_and_flush();
-	enter_lazy_tlb(&init_mm, me);
+	enter_lazy_tlb(&init_mm, cur);
 
-	/*
-	 * Initialize the TSS.  sp0 points to the entry trampoline stack
-	 * regardless of what task is running.
-	 */
+	/* Initialize the TSS. */
+	tss_setup_ist(tss);
+	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
+	memset(tss->io_bitmap, 0xff, sizeof(tss->io_bitmap));
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
+
 	load_TR_desc();
+	/*
+	 * sp0 points to the entry trampoline stack regardless of what task
+	 * is running.
+	 */
 	load_sp0((unsigned long)(cpu_entry_stack(cpu) + 1));
 
 	load_mm_ldt(&init_mm);
@@ -1864,6 +1876,8 @@ void cpu_init(void)
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
 
+	gdt_setup_doublefault_tss(cpu);
+
 	fpu__init_cpu();
 
 	if (is_uv_system())
@@ -1872,63 +1886,6 @@ void cpu_init(void)
 	load_fixmap_gdt(cpu);
 }
 
-#else
-
-void cpu_init(void)
-{
-	int cpu = smp_processor_id();
-	struct task_struct *curr = current;
-	struct tss_struct *t = &per_cpu(cpu_tss_rw, cpu);
-
-	wait_for_master_cpu(cpu);
-
-	show_ucode_info_early();
-
-	pr_info("Initializing CPU#%d\n", cpu);
-
-	if (cpu_feature_enabled(X86_FEATURE_VME) ||
-	    boot_cpu_has(X86_FEATURE_TSC) ||
-	    boot_cpu_has(X86_FEATURE_DE))
-		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
-
-	load_current_idt();
-	switch_to_new_gdt(cpu);
-
-	/*
-	 * Set up and load the per-CPU TSS and LDT
-	 */
-	mmgrab(&init_mm);
-	curr->active_mm = &init_mm;
-	BUG_ON(curr->mm);
-	initialize_tlbstate_and_flush();
-	enter_lazy_tlb(&init_mm, curr);
-
-	/*
-	 * Initialize the TSS.  sp0 points to the entry trampoline stack
-	 * regardless of what task is running.
-	 */
-	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
-	load_TR_desc();
-	load_sp0((unsigned long)(cpu_entry_stack(cpu) + 1));
-
-	load_mm_ldt(&init_mm);
-
-	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
-
-#ifdef CONFIG_DOUBLEFAULT
-	/* Set up doublefault TSS pointer in the GDT */
-	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
-#endif
-
-	clear_all_debug_regs();
-	dbg_restore_debug_regs();
-
-	fpu__init_cpu();
-
-	load_fixmap_gdt(cpu);
-}
-#endif
-
 /*
  * The microcode loader calls this upon late microcode load to recheck features,
  * only when microcode has been updated. Caller holds microcode_mutex and CPU

commit f6a892ddd53e555362dbf64d31b47fde0f550ec4
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Mon Sep 16 15:39:56 2019 -0700

    x86/cpu: Align cpu_caps_cleared and cpu_caps_set to unsigned long
    
    cpu_caps_cleared[] and cpu_caps_set[] are arrays of type u32 and therefore
    naturally aligned to 4 bytes, which is also unsigned long aligned on
    32-bit, but not on 64-bit.
    
    The array pointer is handed into atomic bit operations. If the access not
    aligned to unsigned long then the atomic bit operations can end up crossing
    a cache line boundary, which causes the CPU to do a full bus lock as it
    can't lock both cache lines at once. The bus lock operation is heavy weight
    and can cause severe performance degradation.
    
    The upcoming #AC split lock detection mechanism will issue warnings for
    this kind of access.
    
    Force the alignment of these arrays to unsigned long. This avoids the
    massive code changes which would be required when converting the array data
    type to unsigned long.
    
    [ tglx: Rewrote changelog ]
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20190916223958.27048-2-tony.luck@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9ae7d1bcd4f4..1e9430bed75b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -565,8 +565,9 @@ static const char *table_lookup_model(struct cpuinfo_x86 *c)
 	return NULL;		/* Not found */
 }
 
-__u32 cpu_caps_cleared[NCAPINTS + NBUGINTS];
-__u32 cpu_caps_set[NCAPINTS + NBUGINTS];
+/* Aligned to unsigned long to avoid split lock in atomic bitmap ops */
+__u32 cpu_caps_cleared[NCAPINTS + NBUGINTS] __aligned(sizeof(unsigned long));
+__u32 cpu_caps_set[NCAPINTS + NBUGINTS] __aligned(sizeof(unsigned long));
 
 void load_percpu_segment(int cpu)
 {

commit cad14885a8d32c1c0d8eaa7bf5c0152a22b6080e
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Mon Nov 4 12:22:01 2019 +0100

    x86/cpu: Add Tremont to the cpu vulnerability whitelist
    
    Add the new cpu family ATOM_TREMONT_D to the cpu vunerability
    whitelist. ATOM_TREMONT_D is not affected by X86_BUG_ITLB_MULTIHIT.
    
    ATOM_TREMONT_D might have mitigations against other issues as well, but
    only the ITLB multihit mitigation is confirmed at this point.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d29b71ca3ca7..fffe21945374 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1074,6 +1074,8 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	 * good enough for our purposes.
 	 */
 
+	VULNWL_INTEL(ATOM_TREMONT_D,		NO_ITLB_MULTIHIT),
+
 	/* AMD Family 0xf - 0x12 */
 	VULNWL_AMD(0x0f,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
 	VULNWL_AMD(0x10,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),

commit db4d30fbb71b47e4ecb11c4efa5d8aad4b03dfae
Author: Vineela Tummalapalli <vineela.tummalapalli@intel.com>
Date:   Mon Nov 4 12:22:01 2019 +0100

    x86/bugs: Add ITLB_MULTIHIT bug infrastructure
    
    Some processors may incur a machine check error possibly resulting in an
    unrecoverable CPU lockup when an instruction fetch encounters a TLB
    multi-hit in the instruction TLB. This can occur when the page size is
    changed along with either the physical address or cache type. The relevant
    erratum can be found here:
    
       https://bugzilla.kernel.org/show_bug.cgi?id=205195
    
    There are other processors affected for which the erratum does not fully
    disclose the impact.
    
    This issue affects both bare-metal x86 page tables and EPT.
    
    It can be mitigated by either eliminating the use of large pages or by
    using careful TLB invalidations when changing the page size in the page
    tables.
    
    Just like Spectre, Meltdown, L1TF and MDS, a new bit has been allocated in
    MSR_IA32_ARCH_CAPABILITIES (PSCHANGE_MC_NO) and will be set on CPUs which
    are mitigated against this issue.
    
    Signed-off-by: Vineela Tummalapalli <vineela.tummalapalli@intel.com>
    Co-developed-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f8b8afc8f5b5..d29b71ca3ca7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1016,13 +1016,14 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #endif
 }
 
-#define NO_SPECULATION	BIT(0)
-#define NO_MELTDOWN	BIT(1)
-#define NO_SSB		BIT(2)
-#define NO_L1TF		BIT(3)
-#define NO_MDS		BIT(4)
-#define MSBDS_ONLY	BIT(5)
-#define NO_SWAPGS	BIT(6)
+#define NO_SPECULATION		BIT(0)
+#define NO_MELTDOWN		BIT(1)
+#define NO_SSB			BIT(2)
+#define NO_L1TF			BIT(3)
+#define NO_MDS			BIT(4)
+#define MSBDS_ONLY		BIT(5)
+#define NO_SWAPGS		BIT(6)
+#define NO_ITLB_MULTIHIT	BIT(7)
 
 #define VULNWL(_vendor, _family, _model, _whitelist)	\
 	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }
@@ -1043,27 +1044,27 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL(NSC,	5, X86_MODEL_ANY,	NO_SPECULATION),
 
 	/* Intel Family 6 */
-	VULNWL_INTEL(ATOM_SALTWELL,		NO_SPECULATION),
-	VULNWL_INTEL(ATOM_SALTWELL_TABLET,	NO_SPECULATION),
-	VULNWL_INTEL(ATOM_SALTWELL_MID,		NO_SPECULATION),
-	VULNWL_INTEL(ATOM_BONNELL,		NO_SPECULATION),
-	VULNWL_INTEL(ATOM_BONNELL_MID,		NO_SPECULATION),
-
-	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_SILVERMONT_D,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
-	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
-	VULNWL_INTEL(XEON_PHI_KNM,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_SALTWELL,		NO_SPECULATION | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_SALTWELL_TABLET,	NO_SPECULATION | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_SALTWELL_MID,		NO_SPECULATION | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_BONNELL,		NO_SPECULATION | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_BONNELL_MID,		NO_SPECULATION | NO_ITLB_MULTIHIT),
+
+	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_SILVERMONT_D,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(XEON_PHI_KNM,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS | NO_ITLB_MULTIHIT),
 
 	VULNWL_INTEL(CORE_YONAH,		NO_SSB),
 
-	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_AIRMONT_NP,		NO_L1TF | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF | MSBDS_ONLY | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_AIRMONT_NP,		NO_L1TF | NO_SWAPGS | NO_ITLB_MULTIHIT),
 
-	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_GOLDMONT_D,		NO_MDS | NO_L1TF | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_MDS | NO_L1TF | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_GOLDMONT_D,		NO_MDS | NO_L1TF | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_MDS | NO_L1TF | NO_SWAPGS | NO_ITLB_MULTIHIT),
 
 	/*
 	 * Technically, swapgs isn't serializing on AMD (despite it previously
@@ -1074,14 +1075,14 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	 */
 
 	/* AMD Family 0xf - 0x12 */
-	VULNWL_AMD(0x0f,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
-	VULNWL_AMD(0x10,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
-	VULNWL_AMD(0x11,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
-	VULNWL_AMD(0x12,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
+	VULNWL_AMD(0x0f,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_AMD(0x10,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_AMD(0x11,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_AMD(0x12,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
 
 	/* FAMILY_ANY must be last, otherwise 0x0f - 0x12 matches won't work */
-	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS),
-	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS),
+	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
+	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS | NO_ITLB_MULTIHIT),
 	{}
 };
 
@@ -1106,6 +1107,10 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = x86_read_arch_cap_msr();
 
+	/* Set ITLB_MULTIHIT bug if cpu is not in the whitelist and not mitigated */
+	if (!cpu_matches(NO_ITLB_MULTIHIT) && !(ia32_cap & ARCH_CAP_PSCHANGE_MC_NO))
+		setup_force_cpu_bug(X86_BUG_ITLB_MULTIHIT);
+
 	if (cpu_matches(NO_SPECULATION))
 		return;
 

commit 1b42f017415b46c317e71d41c34ec088417a1883
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 11:30:45 2019 +0200

    x86/speculation/taa: Add mitigation for TSX Async Abort
    
    TSX Async Abort (TAA) is a side channel vulnerability to the internal
    buffers in some Intel processors similar to Microachitectural Data
    Sampling (MDS). In this case, certain loads may speculatively pass
    invalid data to dependent operations when an asynchronous abort
    condition is pending in a TSX transaction.
    
    This includes loads with no fault or assist condition. Such loads may
    speculatively expose stale data from the uarch data structures as in
    MDS. Scope of exposure is within the same-thread and cross-thread. This
    issue affects all current processors that support TSX, but do not have
    ARCH_CAP_TAA_NO (bit 8) set in MSR_IA32_ARCH_CAPABILITIES.
    
    On CPUs which have their IA32_ARCH_CAPABILITIES MSR bit MDS_NO=0,
    CPUID.MD_CLEAR=1 and the MDS mitigation is clearing the CPU buffers
    using VERW or L1D_FLUSH, there is no additional mitigation needed for
    TAA. On affected CPUs with MDS_NO=1 this issue can be mitigated by
    disabling the Transactional Synchronization Extensions (TSX) feature.
    
    A new MSR IA32_TSX_CTRL in future and current processors after a
    microcode update can be used to control the TSX feature. There are two
    bits in that MSR:
    
    * TSX_CTRL_RTM_DISABLE disables the TSX sub-feature Restricted
    Transactional Memory (RTM).
    
    * TSX_CTRL_CPUID_CLEAR clears the RTM enumeration in CPUID. The other
    TSX sub-feature, Hardware Lock Elision (HLE), is unconditionally
    disabled with updated microcode but still enumerated as present by
    CPUID(EAX=7).EBX{bit4}.
    
    The second mitigation approach is similar to MDS which is clearing the
    affected CPU buffers on return to user space and when entering a guest.
    Relevant microcode update is required for the mitigation to work.  More
    details on this approach can be found here:
    
      https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html
    
    The TSX feature can be controlled by the "tsx" command line parameter.
    If it is force-enabled then "Clear CPU buffers" (MDS mitigation) is
    deployed. The effective mitigation state can be read from sysfs.
    
     [ bp:
       - massage + comments cleanup
       - s/TAA_MITIGATION_TSX_DISABLE/TAA_MITIGATION_TSX_DISABLED/g - Josh.
       - remove partial TAA mitigation in update_mds_branch_idle() - Josh.
       - s/tsx_async_abort_cmdline/tsx_async_abort_parse_cmdline/g
     ]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 885d4ac2111a..f8b8afc8f5b5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1128,6 +1128,21 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (!cpu_matches(NO_SWAPGS))
 		setup_force_cpu_bug(X86_BUG_SWAPGS);
 
+	/*
+	 * When the CPU is not mitigated for TAA (TAA_NO=0) set TAA bug when:
+	 *	- TSX is supported or
+	 *	- TSX_CTRL is present
+	 *
+	 * TSX_CTRL check is needed for cases when TSX could be disabled before
+	 * the kernel boot e.g. kexec.
+	 * TSX_CTRL check alone is not sufficient for cases when the microcode
+	 * update is not present or running as guest that don't get TSX_CTRL.
+	 */
+	if (!(ia32_cap & ARCH_CAP_TAA_NO) &&
+	    (cpu_has(c, X86_FEATURE_RTM) ||
+	     (ia32_cap & ARCH_CAP_TSX_CTRL_MSR)))
+		setup_force_cpu_bug(X86_BUG_TAA);
+
 	if (cpu_matches(NO_MELTDOWN))
 		return;
 

commit 95c5824f75f3ba4c9e8e5a4b1a623c95390ac266
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 11:01:53 2019 +0200

    x86/cpu: Add a "tsx=" cmdline option with TSX disabled by default
    
    Add a kernel cmdline parameter "tsx" to control the Transactional
    Synchronization Extensions (TSX) feature. On CPUs that support TSX
    control, use "tsx=on|off" to enable or disable TSX. Not specifying this
    option is equivalent to "tsx=off". This is because on certain processors
    TSX may be used as a part of a speculative side channel attack.
    
    Carve out the TSX controlling functionality into a separate compilation
    unit because TSX is a CPU feature while the TSX async abort control
    machinery will go to cpu/bugs.c.
    
     [ bp: - Massage, shorten and clear the arg buffer.
           - Clarifications of the tsx= possible options - Josh.
           - Expand on TSX_CTRL availability - Pawan. ]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 897c8302d982..885d4ac2111a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1561,6 +1561,8 @@ void __init identify_boot_cpu(void)
 #endif
 	cpu_detect_tlb(&boot_cpu_data);
 	setup_cr_pinning();
+
+	tsx_init();
 }
 
 void identify_secondary_cpu(struct cpuinfo_x86 *c)

commit 286836a70433fb64131d2590f4bf512097c255e1
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 10:52:35 2019 +0200

    x86/cpu: Add a helper function x86_read_arch_cap_msr()
    
    Add a helper function to read the IA32_ARCH_CAPABILITIES MSR.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Neelima Krishnan <neelima.krishnan@intel.com>
    Reviewed-by: Mark Gross <mgross@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9ae7d1bcd4f4..897c8302d982 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1092,19 +1092,26 @@ static bool __init cpu_matches(unsigned long which)
 	return m && !!(m->driver_data & which);
 }
 
-static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
+u64 x86_read_arch_cap_msr(void)
 {
 	u64 ia32_cap = 0;
 
+	if (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES))
+		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
+
+	return ia32_cap;
+}
+
+static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
+{
+	u64 ia32_cap = x86_read_arch_cap_msr();
+
 	if (cpu_matches(NO_SPECULATION))
 		return;
 
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
 
-	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
-		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
-
 	if (!cpu_matches(NO_SSB) && !(ia32_cap & ARCH_CAP_SSB_NO) &&
 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);

commit c5f12fdb8bd873aa3ffdb79512e6bdac92b257b0
Merge: a572ba63298d 743dac494d61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 12:04:39 2019 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic updates from Thomas Gleixner:
    
     - Cleanup the apic IPI implementation by removing duplicated code and
       consolidating the functions into the APIC core.
    
     - Implement a safe variant of the IPI broadcast mode. Contrary to
       earlier attempts this uses the core tracking of which CPUs have been
       brought online at least once so that a broadcast does not end up in
       some dead end in BIOS/SMM code when the CPU is still waiting for
       init. Once all CPUs have been brought up once, IPI broadcasting is
       enabled. Before that regular one by one IPIs are issued.
    
     - Drop the paravirt CR8 related functions as they have no user anymore
    
     - Initialize the APIC TPR to block interrupt 16-31 as they are reserved
       for CPU exceptions and should never be raised by any well behaving
       device.
    
     - Emit a warning when vector space exhaustion breaks the admin set
       affinity of an interrupt.
    
     - Make sure to use the NMI fallback when shutdown via reboot vector IPI
       fails. The original code had conditions which prevent the code path
       to be reached.
    
     - Annotate various APIC config variables as RO after init.
    
    [ The ipi broadcase change came in earlier through the cpu hotplug
      branch, but I left the explanation in the commit message since it was
      shared between the two different branches    - Linus ]
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (28 commits)
      x86/apic/vector: Warn when vector space exhaustion breaks affinity
      x86/apic: Annotate global config variables as "read-only after init"
      x86/apic/x2apic: Implement IPI shorthands support
      x86/apic/flat64: Remove the IPI shorthand decision logic
      x86/apic: Share common IPI helpers
      x86/apic: Remove the shorthand decision logic
      x86/smp: Enhance native_send_call_func_ipi()
      x86/smp: Move smp_function_call implementations into IPI code
      x86/apic: Provide and use helper for send_IPI_allbutself()
      x86/apic: Add static key to Control IPI shorthands
      x86/apic: Move no_ipi_broadcast() out of 32bit
      x86/apic: Add NMI_VECTOR wait to IPI shorthand
      x86/apic: Remove dest argument from __default_send_IPI_shortcut()
      x86/hotplug: Silence APIC and NMI when CPU is dead
      x86/cpu: Move arch_smt_update() to a neutral place
      x86/apic/uv: Make x2apic_extra_bits static
      x86/apic: Consolidate the apic local headers
      x86/apic: Move apic_flat_64 header into apic directory
      x86/apic: Move ipi header into apic directory
      x86/apic: Cleanup the include maze
      ...

commit 0cc5359d8fd45bc410906e009117e78e2b5b2322
Author: Rahul Tanwar <rahul.tanwar@linux.intel.com>
Date:   Thu Sep 5 12:30:20 2019 -0700

    x86/cpu: Update init data for new Airmont CPU model
    
    Update properties for newly added Airmont CPU variant.
    
    Signed-off-by: Rahul Tanwar <rahul.tanwar@linux.intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Cc: Gayatri Kammela <gayatri.kammela@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190905193020.14707-5-tony.luck@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b6a9e27755d7..030e52749a74 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1059,6 +1059,7 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL_INTEL(CORE_YONAH,		NO_SSB),
 
 	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_AIRMONT_NP,		NO_L1TF | NO_SWAPGS),
 
 	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF | NO_SWAPGS),
 	VULNWL_INTEL(ATOM_GOLDMONT_D,		NO_MDS | NO_L1TF | NO_SWAPGS),

commit 5ebb34edbefa8ea6a7e109179d5fc7b3529dbeba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 27 21:48:24 2019 +0200

    x86/intel: Aggregate microserver naming
    
    Currently big microservers have _XEON_D while small microservers have
    _X, Make it uniformly: _D.
    
    for i in `git grep -l "\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*_\(X\|XEON_D\)"`
    do
            sed -i -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*ATOM.*\)_X/\1_D/g' \
                   -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*\)_XEON_D/\1_D/g' ${i}
    done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20190827195122.677152989@infradead.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f125bf7ecb6f..b6a9e27755d7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1050,7 +1050,7 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL_INTEL(ATOM_BONNELL_MID,		NO_SPECULATION),
 
 	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_SILVERMONT_X,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_SILVERMONT_D,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
 	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
 	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
 	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
@@ -1061,7 +1061,7 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
 
 	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF | NO_SWAPGS),
-	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_MDS | NO_L1TF | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_GOLDMONT_D,		NO_MDS | NO_L1TF | NO_SWAPGS),
 	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_MDS | NO_L1TF | NO_SWAPGS),
 
 	/*

commit 7a30bdd99f37352b188575b27924c407c6ddff9e
Merge: f36cf386e3fe 609488bc979f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 28 22:22:40 2019 +0200

    Merge branch master from git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
    
    Pick up the spectre documentation so the Grand Schemozzle can be added.

commit f36cf386e3fec258a341d446915862eded3e13d8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 17 21:18:59 2019 +0200

    x86/speculation/swapgs: Exclude ATOMs from speculation through SWAPGS
    
    Intel provided the following information:
    
     On all current Atom processors, instructions that use a segment register
     value (e.g. a load or store) will not speculatively execute before the
     last writer of that segment retires. Thus they will not use a
     speculatively written segment value.
    
    That means on ATOMs there is no speculation through SWAPGS, so the SWAPGS
    entry paths can be excluded from the extra LFENCE if PTI is disabled.
    
    Create a separate bug flag for the through SWAPGS speculation and mark all
    out-of-order ATOMs and AMD/HYGON CPUs as not affected. The in-order ATOMs
    are excluded from the whole mitigation mess anyway.
    
    Reported-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Tyler Hicks <tyhicks@canonical.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 309b6b9b49d4..300dcf00d287 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -970,6 +970,7 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #define NO_L1TF		BIT(3)
 #define NO_MDS		BIT(4)
 #define MSBDS_ONLY	BIT(5)
+#define NO_SWAPGS	BIT(6)
 
 #define VULNWL(_vendor, _family, _model, _whitelist)	\
 	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }
@@ -996,30 +997,38 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL_INTEL(ATOM_BONNELL,		NO_SPECULATION),
 	VULNWL_INTEL(ATOM_BONNELL_MID,		NO_SPECULATION),
 
-	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY),
-	VULNWL_INTEL(ATOM_SILVERMONT_X,		NO_SSB | NO_L1TF | MSBDS_ONLY),
-	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF | MSBDS_ONLY),
-	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY),
-	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF | MSBDS_ONLY),
-	VULNWL_INTEL(XEON_PHI_KNM,		NO_SSB | NO_L1TF | MSBDS_ONLY),
+	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_SILVERMONT_X,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
+	VULNWL_INTEL(XEON_PHI_KNM,		NO_SSB | NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
 
 	VULNWL_INTEL(CORE_YONAH,		NO_SSB),
 
-	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF | MSBDS_ONLY),
+	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF | MSBDS_ONLY | NO_SWAPGS),
 
-	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF),
-	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_MDS | NO_L1TF),
-	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_MDS | NO_L1TF),
+	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_MDS | NO_L1TF | NO_SWAPGS),
+	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_MDS | NO_L1TF | NO_SWAPGS),
+
+	/*
+	 * Technically, swapgs isn't serializing on AMD (despite it previously
+	 * being documented as such in the APM).  But according to AMD, %gs is
+	 * updated non-speculatively, and the issuing of %gs-relative memory
+	 * operands will be blocked until the %gs update completes, which is
+	 * good enough for our purposes.
+	 */
 
 	/* AMD Family 0xf - 0x12 */
-	VULNWL_AMD(0x0f,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
-	VULNWL_AMD(0x10,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
-	VULNWL_AMD(0x11,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
-	VULNWL_AMD(0x12,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
+	VULNWL_AMD(0x0f,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
+	VULNWL_AMD(0x10,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
+	VULNWL_AMD(0x11,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
+	VULNWL_AMD(0x12,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS | NO_SWAPGS),
 
 	/* FAMILY_ANY must be last, otherwise 0x0f - 0x12 matches won't work */
-	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS),
-	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS),
+	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS),
+	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS | NO_SWAPGS),
 	{}
 };
 
@@ -1056,6 +1065,9 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 			setup_force_cpu_bug(X86_BUG_MSBDS_ONLY);
 	}
 
+	if (!cpu_matches(NO_SWAPGS))
+		setup_force_cpu_bug(X86_BUG_SWAPGS);
+
 	if (cpu_matches(NO_MELTDOWN))
 		return;
 

commit 6a1cb5f5c6413222b8532722562dd1edb5fdfd38
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 22 20:47:22 2019 +0200

    x86/apic: Add static key to Control IPI shorthands
    
    The IPI shorthand functionality delivers IPI/NMI broadcasts to all CPUs in
    the system. This can have similar side effects as the MCE broadcasting when
    CPUs are waiting in the BIOS or are offlined.
    
    The kernel tracks already the state of offlined CPUs whether they have been
    brought up at least once so that the CR4 MCE bit is set to make sure that
    MCE broadcasts can't brick the machine.
    
    Utilize that information and compare it to the cpu_present_mask. If all
    present CPUs have been brought up at least once then the broadcast side
    effect is mitigated by disabling regular interrupt/IPI delivery in the APIC
    itself and by the cpu offline check at the begin of the NMI handler.
    
    Use a static key to switch between broadcasting via shorthands or sending
    the IPI/NMI one by one.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20190722105220.386410643@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1ee6598c5d83..e0489d2860d3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1953,4 +1953,6 @@ void arch_smt_update(void)
 {
 	/* Handle the speculative execution misfeatures */
 	cpu_bugs_smt_update();
+	/* Check whether IPI broadcasting can be enabled */
+	apic_smt_update();
 }

commit 9c92374b631d233abf5bd355cb4253d3d83d5578
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 22 20:47:17 2019 +0200

    x86/cpu: Move arch_smt_update() to a neutral place
    
    arch_smt_update() will be used to control IPI/NMI broadcasting via the
    shorthand mechanism. Keeping it in the bugs file and calling the apic
    function from there is possible, but not really intuitive.
    
    Move it to a neutral place and invoke the bugs function from there.
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20190722105219.910317273@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 11472178e17f..1ee6598c5d83 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1945,3 +1945,12 @@ void microcode_check(void)
 	pr_warn("x86/CPU: CPU features have changed after loading microcode, but might not take effect.\n");
 	pr_warn("x86/CPU: Please consider either early loading through initrd/built-in or a potential BIOS update.\n");
 }
+
+/*
+ * Invoked from core CPU hotplug code after hotplug operations
+ */
+void arch_smt_update(void)
+{
+	/* Handle the speculative execution misfeatures */
+	cpu_bugs_smt_update();
+}

commit 7652ac92018536eb807b6c2130100c85f1ba7e3b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 10 21:42:46 2019 +0200

    x86/asm: Move native_write_cr0/4() out of line
    
    The pinning of sensitive CR0 and CR4 bits caused a boot crash when loading
    the kvm_intel module on a kernel compiled with CONFIG_PARAVIRT=n.
    
    The reason is that the static key which controls the pinning is marked RO
    after init. The kvm_intel module contains a CR4 write which requires to
    update the static key entry list. That obviously does not work when the key
    is in a RO section.
    
    With CONFIG_PARAVIRT enabled this does not happen because the CR4 write
    uses the paravirt indirection and the actual write function is built in.
    
    As the key is intended to be immutable after init, move
    native_write_cr0/4() out of line.
    
    While at it consolidate the update of the cr4 shadow variable and store the
    value right away when the pinning is initialized on a booting CPU. No point
    in reading it back 20 instructions later. This allows to confine the static
    key and the pinning variable to cpu/common and allows to mark them static.
    
    Fixes: 8dbec27a242c ("x86/asm: Pin sensitive CR0 bits")
    Fixes: 873d50d58f67 ("x86/asm: Pin sensitive CR4 bits")
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Xi Ruoyao <xry111@mengyan1223.wang>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Xi Ruoyao <xry111@mengyan1223.wang>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1907102140340.1758@nanos.tec.linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 309b6b9b49d4..11472178e17f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -366,10 +366,62 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 	cr4_clear_bits(X86_CR4_UMIP);
 }
 
-DEFINE_STATIC_KEY_FALSE_RO(cr_pinning);
-EXPORT_SYMBOL(cr_pinning);
-unsigned long cr4_pinned_bits __ro_after_init;
-EXPORT_SYMBOL(cr4_pinned_bits);
+static DEFINE_STATIC_KEY_FALSE_RO(cr_pinning);
+static unsigned long cr4_pinned_bits __ro_after_init;
+
+void native_write_cr0(unsigned long val)
+{
+	unsigned long bits_missing = 0;
+
+set_register:
+	asm volatile("mov %0,%%cr0": "+r" (val), "+m" (__force_order));
+
+	if (static_branch_likely(&cr_pinning)) {
+		if (unlikely((val & X86_CR0_WP) != X86_CR0_WP)) {
+			bits_missing = X86_CR0_WP;
+			val |= bits_missing;
+			goto set_register;
+		}
+		/* Warn after we've set the missing bits. */
+		WARN_ONCE(bits_missing, "CR0 WP bit went missing!?\n");
+	}
+}
+EXPORT_SYMBOL(native_write_cr0);
+
+void native_write_cr4(unsigned long val)
+{
+	unsigned long bits_missing = 0;
+
+set_register:
+	asm volatile("mov %0,%%cr4": "+r" (val), "+m" (cr4_pinned_bits));
+
+	if (static_branch_likely(&cr_pinning)) {
+		if (unlikely((val & cr4_pinned_bits) != cr4_pinned_bits)) {
+			bits_missing = ~val & cr4_pinned_bits;
+			val |= bits_missing;
+			goto set_register;
+		}
+		/* Warn after we've set the missing bits. */
+		WARN_ONCE(bits_missing, "CR4 bits went missing: %lx!?\n",
+			  bits_missing);
+	}
+}
+EXPORT_SYMBOL(native_write_cr4);
+
+void cr4_init(void)
+{
+	unsigned long cr4 = __read_cr4();
+
+	if (boot_cpu_has(X86_FEATURE_PCID))
+		cr4 |= X86_CR4_PCIDE;
+	if (static_branch_likely(&cr_pinning))
+		cr4 |= cr4_pinned_bits;
+
+	__write_cr4(cr4);
+
+	/* Initialize cr4 shadow for this CPU. */
+	this_cpu_write(cpu_tlbstate.cr4, cr4);
+}
 
 /*
  * Once CPU feature detection is finished (and boot params have been
@@ -1723,12 +1775,6 @@ void cpu_init(void)
 
 	wait_for_master_cpu(cpu);
 
-	/*
-	 * Initialize the CR4 shadow before doing anything that could
-	 * try to read it.
-	 */
-	cr4_init_shadow();
-
 	if (cpu)
 		load_ucode_ap();
 
@@ -1823,12 +1869,6 @@ void cpu_init(void)
 
 	wait_for_master_cpu(cpu);
 
-	/*
-	 * Initialize the CR4 shadow before doing anything that could
-	 * try to read it.
-	 */
-	cr4_init_shadow();
-
 	show_ucode_info_early();
 
 	pr_info("Initializing CPU#%d\n", cpu);

commit 222a21d29521d144f3dd7a0bc4d4020e448f0126
Merge: 8faef7125d02 eb876fbc248e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 18:28:44 2019 -0700

    Merge branch 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 topology updates from Ingo Molnar:
     "Implement multi-die topology support on Intel CPUs and expose the die
      topology to user-space tooling, by Len Brown, Kan Liang and Zhang Rui.
    
      These changes should have no effect on the kernel's existing
      understanding of topologies, i.e. there should be no behavioral impact
      on cache, NUMA, scheduler, perf and other topologies and overall
      system performance"
    
    * 'x86-topology-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86/intel/rapl: Cosmetic rename internal variables in response to multi-die/pkg support
      perf/x86/intel/uncore: Cosmetic renames in response to multi-die/pkg support
      hwmon/coretemp: Cosmetic: Rename internal variables to zones from packages
      thermal/x86_pkg_temp_thermal: Cosmetic: Rename internal variables to zones from packages
      perf/x86/intel/cstate: Support multi-die/package
      perf/x86/intel/rapl: Support multi-die/package
      perf/x86/intel/uncore: Support multi-die/package
      topology: Create core_cpus and die_cpus sysfs attributes
      topology: Create package_cpus sysfs attribute
      hwmon/coretemp: Support multi-die/package
      powercap/intel_rapl: Update RAPL domain name and debug messages
      thermal/x86_pkg_temp_thermal: Support multi-die/package
      powercap/intel_rapl: Support multi-die/package
      powercap/intel_rapl: Simplify rapl_find_package()
      x86/topology: Define topology_logical_die_id()
      x86/topology: Define topology_die_id()
      cpu/topology: Export die_id
      x86/topology: Create topology_max_die_per_package()
      x86/topology: Add CPUID.1F multi-die/package support

commit a1aab6f3d295f078c008893ee7fa2c011626c46f
Merge: dad1c12ed831 7457c0da024b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 16:59:34 2019 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "Most of the changes relate to Peter Zijlstra's cleanup of ptregs
      handling, in particular the i386 part is now much simplified and
      standardized - no more partial ptregs stack frames via the esp/ss
      oddity. This simplifies ftrace, kprobes, the unwinder, ptrace, kdump
      and kgdb.
    
      There's also a CR4 hardening enhancements by Kees Cook, to make the
      generic platform functions such as native_write_cr4() less useful as
      ROP gadgets that disable SMEP/SMAP. Also protect the WP bit of CR0
      against similar attacks.
    
      The rest is smaller cleanups/fixes"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/alternatives: Add int3_emulate_call() selftest
      x86/stackframe/32: Allow int3_emulate_push()
      x86/stackframe/32: Provide consistent pt_regs
      x86/stackframe, x86/ftrace: Add pt_regs frame annotations
      x86/stackframe, x86/kprobes: Fix frame pointer annotations
      x86/stackframe: Move ENCODE_FRAME_POINTER to asm/frame.h
      x86/entry/32: Clean up return from interrupt preemption path
      x86/asm: Pin sensitive CR0 bits
      x86/asm: Pin sensitive CR4 bits
      Documentation/x86: Fix path to entry_32.S
      x86/asm: Remove unused TASK_TI_flags from asm-offsets.c

commit 049331f277fef1c3f2527c2c9afa1d285e9a1247
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 3 14:19:36 2019 +0200

    x86/fsgsbase: Revert FSGSBASE support
    
    The FSGSBASE series turned out to have serious bugs and there is still an
    open issue which is not fully understood yet.
    
    The confidence in those changes has become close to zero especially as the
    test cases which have been shipped with that series were obviously never
    run before sending the final series out to LKML.
    
      ./fsgsbase_64 >/dev/null
      Segmentation fault
    
    As the merge window is close, the only sane decision is to revert FSGSBASE
    support. The revert is necessary as this branch has been merged into
    perf/core already and rebasing all of that a few days before the merge
    window is not the most brilliant idea.
    
    I could definitely slap myself for not noticing the test case fail when
    merging that series, but TBH my expectations weren't that low back
    then. Won't happen again.
    
    Revert the following commits:
    539bca535dec ("x86/entry/64: Fix and clean up paranoid_exit")
    2c7b5ac5d5a9 ("Documentation/x86/64: Add documentation for GS/FS addressing mode")
    f987c955c745 ("x86/elf: Enumerate kernel FSGSBASE capability in AT_HWCAP2")
    2032f1f96ee0 ("x86/cpu: Enable FSGSBASE on 64bit by default and add a chicken bit")
    5bf0cab60ee2 ("x86/entry/64: Document GSBASE handling in the paranoid path")
    708078f65721 ("x86/entry/64: Handle FSGSBASE enabled paranoid entry/exit")
    79e1932fa3ce ("x86/entry/64: Introduce the FIND_PERCPU_BASE macro")
    1d07316b1363 ("x86/entry/64: Switch CR3 before SWAPGS in paranoid entry")
    f60a83df4593 ("x86/process/64: Use FSGSBASE instructions on thread copy and ptrace")
    1ab5f3f7fe3d ("x86/process/64: Use FSBSBASE in switch_to() if available")
    a86b4625138d ("x86/fsgsbase/64: Enable FSGSBASE instructions in helper functions")
    8b71340d702e ("x86/fsgsbase/64: Add intrinsics for FSGSBASE instructions")
    b64ed19b93c3 ("x86/cpu: Add 'unsafe_fsgsbase' to enable CR4.FSGSBASE")
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Chang S. Bae <chang.seok.bae@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 637c9117d5ae..dad20bc891d5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -366,22 +366,6 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 	cr4_clear_bits(X86_CR4_UMIP);
 }
 
-static __init int x86_nofsgsbase_setup(char *arg)
-{
-	/* Require an exact match without trailing characters. */
-	if (strlen(arg))
-		return 0;
-
-	/* Do not emit a message if the feature is not present. */
-	if (!boot_cpu_has(X86_FEATURE_FSGSBASE))
-		return 1;
-
-	setup_clear_cpu_cap(X86_FEATURE_FSGSBASE);
-	pr_info("FSGSBASE disabled via kernel command line\n");
-	return 1;
-}
-__setup("nofsgsbase", x86_nofsgsbase_setup);
-
 /*
  * Protection Keys are not available in 32-bit mode.
  */
@@ -1386,12 +1370,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	setup_smap(c);
 	setup_umip(c);
 
-	/* Enable FSGSBASE instructions if available. */
-	if (cpu_has(c, X86_FEATURE_FSGSBASE)) {
-		cr4_set_bits(X86_CR4_FSGSBASE);
-		elf_hwcap2 |= HWCAP2_FSGSBASE;
-	}
-
 	/*
 	 * The vendor-specific functions might have changed features.
 	 * Now we do "generic changes."

commit 873d50d58f67ef15d2777b5e7f7a5268bb1fbae2
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Jun 17 21:55:02 2019 -0700

    x86/asm: Pin sensitive CR4 bits
    
    Several recent exploits have used direct calls to the native_write_cr4()
    function to disable SMEP and SMAP before then continuing their exploits
    using userspace memory access.
    
    Direct calls of this form can be mitigate by pinning bits of CR4 so that
    they cannot be changed through a common function. This is not intended to
    be a general ROP protection (which would require CFI to defend against
    properly), but rather a way to avoid trivial direct function calling (or
    CFI bypasses via a matching function prototype) as seen in:
    
    https://googleprojectzero.blogspot.com/2017/05/exploiting-linux-kernel-via-packet.html
    (https://github.com/xairy/kernel-exploits/tree/master/CVE-2017-7308)
    
    The goals of this change:
    
     - Pin specific bits (SMEP, SMAP, and UMIP) when writing CR4.
    
     - Avoid setting the bits too early (they must become pinned only after
       CPU feature detection and selection has finished).
    
     - Pinning mask needs to be read-only during normal runtime.
    
     - Pinning needs to be checked after write to validate the cr4 state
    
    Using __ro_after_init on the mask is done so it can't be first disabled
    with a malicious write.
    
    Since these bits are global state (once established by the boot CPU and
    kernel boot parameters), they are safe to write to secondary CPUs before
    those CPUs have finished feature detection. As such, the bits are set at
    the first cr4 write, so that cr4 write bugs can be detected (instead of
    silently papered over). This uses a few bytes less storage of a location we
    don't have: read-only per-CPU data.
    
    A check is performed after the register write because an attack could just
    skip directly to the register write. Such a direct jump is possible because
    of how this function may be built by the compiler (especially due to the
    removal of frame pointers) where it doesn't add a stack frame (function
    exit may only be a retq without pops) which is sufficient for trivial
    exploitation like in the timer overwrites mentioned above).
    
    The asm argument constraints gain the "+" modifier to convince the compiler
    that it shouldn't make ordering assumptions about the arguments or memory,
    and treat them as changed.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: kernel-hardening@lists.openwall.com
    Link: https://lkml.kernel.org/r/20190618045503.39105-3-keescook@chromium.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2c57fffebf9b..c578addfcf8a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -366,6 +366,25 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 	cr4_clear_bits(X86_CR4_UMIP);
 }
 
+DEFINE_STATIC_KEY_FALSE_RO(cr_pinning);
+EXPORT_SYMBOL(cr_pinning);
+unsigned long cr4_pinned_bits __ro_after_init;
+EXPORT_SYMBOL(cr4_pinned_bits);
+
+/*
+ * Once CPU feature detection is finished (and boot params have been
+ * parsed), record any of the sensitive CR bits that are set, and
+ * enable CR pinning.
+ */
+static void __init setup_cr_pinning(void)
+{
+	unsigned long mask;
+
+	mask = (X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP);
+	cr4_pinned_bits = this_cpu_read(cpu_tlbstate.cr4) & mask;
+	static_key_enable(&cr_pinning.key);
+}
+
 /*
  * Protection Keys are not available in 32-bit mode.
  */
@@ -1464,6 +1483,7 @@ void __init identify_boot_cpu(void)
 	enable_sep_cpu();
 #endif
 	cpu_detect_tlb(&boot_cpu_data);
+	setup_cr_pinning();
 }
 
 void identify_secondary_cpu(struct cpuinfo_x86 *c)

commit f987c955c74501c9295a81372c7d363cbe07c8a6
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed May 8 03:02:32 2019 -0700

    x86/elf: Enumerate kernel FSGSBASE capability in AT_HWCAP2
    
    The kernel needs to explicitly enable FSGSBASE. So, the application needs
    to know if it can safely use these instructions. Just looking at the CPUID
    bit is not enough because it may be running in a kernel that does not
    enable the instructions.
    
    One way for the application would be to just try and catch the SIGILL.
    But that is difficult to do in libraries which may not want to overwrite
    the signal handlers of the main application.
    
    Enumerate the enabled FSGSBASE capability in bit 1 of AT_HWCAP2 in the ELF
    aux vector. AT_HWCAP2 is already used by PPC for similar purposes.
    
    The application can access it open coded or by using the getauxval()
    function in newer versions of glibc.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/1557309753-24073-18-git-send-email-chang.seok.bae@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1305f16b6105..637c9117d5ae 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1387,8 +1387,10 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	setup_umip(c);
 
 	/* Enable FSGSBASE instructions if available. */
-	if (cpu_has(c, X86_FEATURE_FSGSBASE))
+	if (cpu_has(c, X86_FEATURE_FSGSBASE)) {
 		cr4_set_bits(X86_CR4_FSGSBASE);
+		elf_hwcap2 |= HWCAP2_FSGSBASE;
+	}
 
 	/*
 	 * The vendor-specific functions might have changed features.

commit 2032f1f96ee0da600633c6c627b9c0a2e0f8b8a6
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed May 8 03:02:31 2019 -0700

    x86/cpu: Enable FSGSBASE on 64bit by default and add a chicken bit
    
    Now that FSGSBASE is fully supported, remove unsafe_fsgsbase, enable
    FSGSBASE by default, and add nofsgsbase to disable it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/1557309753-24073-17-git-send-email-chang.seok.bae@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 71defe2d1b7c..1305f16b6105 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -366,21 +366,21 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 	cr4_clear_bits(X86_CR4_UMIP);
 }
 
-/*
- * Temporary hack: FSGSBASE is unsafe until a few kernel code paths are
- * updated. This allows us to get the kernel ready incrementally.
- *
- * Once all the pieces are in place, these will go away and be replaced with
- * a nofsgsbase chicken flag.
- */
-static bool unsafe_fsgsbase;
-
-static __init int setup_unsafe_fsgsbase(char *arg)
+static __init int x86_nofsgsbase_setup(char *arg)
 {
-	unsafe_fsgsbase = true;
+	/* Require an exact match without trailing characters. */
+	if (strlen(arg))
+		return 0;
+
+	/* Do not emit a message if the feature is not present. */
+	if (!boot_cpu_has(X86_FEATURE_FSGSBASE))
+		return 1;
+
+	setup_clear_cpu_cap(X86_FEATURE_FSGSBASE);
+	pr_info("FSGSBASE disabled via kernel command line\n");
 	return 1;
 }
-__setup("unsafe_fsgsbase", setup_unsafe_fsgsbase);
+__setup("nofsgsbase", x86_nofsgsbase_setup);
 
 /*
  * Protection Keys are not available in 32-bit mode.
@@ -1387,12 +1387,8 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	setup_umip(c);
 
 	/* Enable FSGSBASE instructions if available. */
-	if (cpu_has(c, X86_FEATURE_FSGSBASE)) {
-		if (unsafe_fsgsbase)
-			cr4_set_bits(X86_CR4_FSGSBASE);
-		else
-			clear_cpu_cap(c, X86_FEATURE_FSGSBASE);
-	}
+	if (cpu_has(c, X86_FEATURE_FSGSBASE))
+		cr4_set_bits(X86_CR4_FSGSBASE);
 
 	/*
 	 * The vendor-specific functions might have changed features.

commit b64ed19b93c368be0fb6acf05377e8e3a694c92b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed May 8 03:02:18 2019 -0700

    x86/cpu: Add 'unsafe_fsgsbase' to enable CR4.FSGSBASE
    
    This is temporary.  It will allow the next few patches to be tested
    incrementally.
    
    Setting unsafe_fsgsbase is a root hole.  Don't do it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/1557309753-24073-4-git-send-email-chang.seok.bae@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index dad20bc891d5..71defe2d1b7c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -366,6 +366,22 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 	cr4_clear_bits(X86_CR4_UMIP);
 }
 
+/*
+ * Temporary hack: FSGSBASE is unsafe until a few kernel code paths are
+ * updated. This allows us to get the kernel ready incrementally.
+ *
+ * Once all the pieces are in place, these will go away and be replaced with
+ * a nofsgsbase chicken flag.
+ */
+static bool unsafe_fsgsbase;
+
+static __init int setup_unsafe_fsgsbase(char *arg)
+{
+	unsafe_fsgsbase = true;
+	return 1;
+}
+__setup("unsafe_fsgsbase", setup_unsafe_fsgsbase);
+
 /*
  * Protection Keys are not available in 32-bit mode.
  */
@@ -1370,6 +1386,14 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	setup_smap(c);
 	setup_umip(c);
 
+	/* Enable FSGSBASE instructions if available. */
+	if (cpu_has(c, X86_FEATURE_FSGSBASE)) {
+		if (unsafe_fsgsbase)
+			cr4_set_bits(X86_CR4_FSGSBASE);
+		else
+			clear_cpu_cap(c, X86_FEATURE_FSGSBASE);
+	}
+
 	/*
 	 * The vendor-specific functions might have changed features.
 	 * Now we do "generic changes."

commit b302e4b176d00e1cbc80148c5d0aee36751f7480
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Mon Jun 17 11:00:16 2019 -0700

    x86/cpufeatures: Enumerate the new AVX512 BFLOAT16 instructions
    
    AVX512 BFLOAT16 instructions support 16-bit BFLOAT16 floating-point
    format (BF16) for deep learning optimization.
    
    BF16 is a short version of 32-bit single-precision floating-point
    format (FP32) and has several advantages over 16-bit half-precision
    floating-point format (FP16). BF16 keeps FP32 accumulation after
    multiplication without loss of precision, offers more than enough
    range for deep learning training tasks, and doesn't need to handle
    hardware exception.
    
    AVX512 BFLOAT16 instructions are enumerated in CPUID.7.1:EAX[bit 5]
    AVX512_BF16.
    
    CPUID.7.1:EAX contains only feature bits. Reuse the currently empty
    word 12 as a pure features word to hold the feature bits including
    AVX512_BF16.
    
    Detailed information of the CPUID bit and AVX512 BFLOAT16 instructions
    can be found in the latest Intel Architecture Instruction Set Extensions
    and Future Features Programming Reference.
    
     [ bp: Check CPUID(7) subleaf validity before accessing subleaf 1. ]
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nadav Amit <namit@vmware.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Peter Feiner <pfeiner@google.com>
    Cc: Radim Krcmar <rkrcmar@redhat.com>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: "Ravi V Shankar" <ravi.v.shankar@intel.com>
    Cc: Robert Hoo <robert.hu@linux.intel.com>
    Cc: "Sean J Christopherson" <sean.j.christopherson@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: x86 <x86@kernel.org>
    Link: https://lkml.kernel.org/r/1560794416-217638-3-git-send-email-fenghua.yu@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index efb114298cfb..dad20bc891d5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -847,6 +847,12 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[CPUID_7_0_EBX] = ebx;
 		c->x86_capability[CPUID_7_ECX] = ecx;
 		c->x86_capability[CPUID_7_EDX] = edx;
+
+		/* Check valid sub-leaf index before accessing it */
+		if (eax >= 1) {
+			cpuid_count(0x00000007, 1, &eax, &ebx, &ecx, &edx);
+			c->x86_capability[CPUID_7_1_EAX] = eax;
+		}
 	}
 
 	/* Extended state features: level 0x0000000d */

commit acec0ce081de0c36459eea91647faf99296445a3
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Wed Jun 19 18:51:09 2019 +0200

    x86/cpufeatures: Combine word 11 and 12 into a new scattered features word
    
    It's a waste for the four X86_FEATURE_CQM_* feature bits to occupy two
    whole feature bits words. To better utilize feature words, re-define
    word 11 to host scattered features and move the four X86_FEATURE_CQM_*
    features into Linux defined word 11. More scattered features can be
    added in word 11 in the future.
    
    Rename leaf 11 in cpuid_leafs to CPUID_LNX_4 to reflect it's a
    Linux-defined leaf.
    
    Rename leaf 12 as CPUID_DUMMY which will be replaced by a meaningful
    name in the next patch when CPUID.7.1:EAX occupies world 12.
    
    Maximum number of RMID and cache occupancy scale are retrieved from
    CPUID.0xf.1 after scattered CQM features are enumerated. Carve out the
    code into a separate function.
    
    KVM doesn't support resctrl now. So it's safe to move the
    X86_FEATURE_CQM_* features to scattered features word 11 for KVM.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Aaron Lewis <aaronlewis@google.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Babu Moger <babu.moger@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: "Sean J Christopherson" <sean.j.christopherson@intel.com>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Nadav Amit <namit@vmware.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Peter Feiner <pfeiner@google.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: "Radim Krm" <rkrcmar@redhat.com>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Ravi V Shankar <ravi.v.shankar@intel.com>
    Cc: Sherry Hurwitz <sherry.hurwitz@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: x86 <x86@kernel.org>
    Link: https://lkml.kernel.org/r/1560794416-217638-2-git-send-email-fenghua.yu@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fe6ed9696467..efb114298cfb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -803,33 +803,25 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 
 static void init_cqm(struct cpuinfo_x86 *c)
 {
-	u32 eax, ebx, ecx, edx;
-
-	/* Additional Intel-defined flags: level 0x0000000F */
-	if (c->cpuid_level >= 0x0000000F) {
+	if (!cpu_has(c, X86_FEATURE_CQM_LLC)) {
+		c->x86_cache_max_rmid  = -1;
+		c->x86_cache_occ_scale = -1;
+		return;
+	}
 
-		/* QoS sub-leaf, EAX=0Fh, ECX=0 */
-		cpuid_count(0x0000000F, 0, &eax, &ebx, &ecx, &edx);
-		c->x86_capability[CPUID_F_0_EDX] = edx;
+	/* will be overridden if occupancy monitoring exists */
+	c->x86_cache_max_rmid = cpuid_ebx(0xf);
 
-		if (cpu_has(c, X86_FEATURE_CQM_LLC)) {
-			/* will be overridden if occupancy monitoring exists */
-			c->x86_cache_max_rmid = ebx;
+	if (cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC) ||
+	    cpu_has(c, X86_FEATURE_CQM_MBM_TOTAL) ||
+	    cpu_has(c, X86_FEATURE_CQM_MBM_LOCAL)) {
+		u32 eax, ebx, ecx, edx;
 
-			/* QoS sub-leaf, EAX=0Fh, ECX=1 */
-			cpuid_count(0x0000000F, 1, &eax, &ebx, &ecx, &edx);
-			c->x86_capability[CPUID_F_1_EDX] = edx;
+		/* QoS sub-leaf, EAX=0Fh, ECX=1 */
+		cpuid_count(0xf, 1, &eax, &ebx, &ecx, &edx);
 
-			if ((cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC)) ||
-			      ((cpu_has(c, X86_FEATURE_CQM_MBM_TOTAL)) ||
-			       (cpu_has(c, X86_FEATURE_CQM_MBM_LOCAL)))) {
-				c->x86_cache_max_rmid = ecx;
-				c->x86_cache_occ_scale = ebx;
-			}
-		} else {
-			c->x86_cache_max_rmid = -1;
-			c->x86_cache_occ_scale = -1;
-		}
+		c->x86_cache_max_rmid  = ecx;
+		c->x86_cache_occ_scale = ebx;
 	}
 }
 

commit 45fc56e629caa451467e7664fbd4c797c434a6c4
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jun 19 17:24:34 2019 +0200

    x86/cpufeatures: Carve out CQM features retrieval
    
    ... into a separate function for better readability. Split out from a
    patch from Fenghua Yu <fenghua.yu@intel.com> to keep the mechanical,
    sole code movement separate for easy review.
    
    No functional changes.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2c57fffebf9b..fe6ed9696467 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -801,6 +801,38 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 	}
 }
 
+static void init_cqm(struct cpuinfo_x86 *c)
+{
+	u32 eax, ebx, ecx, edx;
+
+	/* Additional Intel-defined flags: level 0x0000000F */
+	if (c->cpuid_level >= 0x0000000F) {
+
+		/* QoS sub-leaf, EAX=0Fh, ECX=0 */
+		cpuid_count(0x0000000F, 0, &eax, &ebx, &ecx, &edx);
+		c->x86_capability[CPUID_F_0_EDX] = edx;
+
+		if (cpu_has(c, X86_FEATURE_CQM_LLC)) {
+			/* will be overridden if occupancy monitoring exists */
+			c->x86_cache_max_rmid = ebx;
+
+			/* QoS sub-leaf, EAX=0Fh, ECX=1 */
+			cpuid_count(0x0000000F, 1, &eax, &ebx, &ecx, &edx);
+			c->x86_capability[CPUID_F_1_EDX] = edx;
+
+			if ((cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC)) ||
+			      ((cpu_has(c, X86_FEATURE_CQM_MBM_TOTAL)) ||
+			       (cpu_has(c, X86_FEATURE_CQM_MBM_LOCAL)))) {
+				c->x86_cache_max_rmid = ecx;
+				c->x86_cache_occ_scale = ebx;
+			}
+		} else {
+			c->x86_cache_max_rmid = -1;
+			c->x86_cache_occ_scale = -1;
+		}
+	}
+}
+
 void get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
@@ -832,33 +864,6 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[CPUID_D_1_EAX] = eax;
 	}
 
-	/* Additional Intel-defined flags: level 0x0000000F */
-	if (c->cpuid_level >= 0x0000000F) {
-
-		/* QoS sub-leaf, EAX=0Fh, ECX=0 */
-		cpuid_count(0x0000000F, 0, &eax, &ebx, &ecx, &edx);
-		c->x86_capability[CPUID_F_0_EDX] = edx;
-
-		if (cpu_has(c, X86_FEATURE_CQM_LLC)) {
-			/* will be overridden if occupancy monitoring exists */
-			c->x86_cache_max_rmid = ebx;
-
-			/* QoS sub-leaf, EAX=0Fh, ECX=1 */
-			cpuid_count(0x0000000F, 1, &eax, &ebx, &ecx, &edx);
-			c->x86_capability[CPUID_F_1_EDX] = edx;
-
-			if ((cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC)) ||
-			      ((cpu_has(c, X86_FEATURE_CQM_MBM_TOTAL)) ||
-			       (cpu_has(c, X86_FEATURE_CQM_MBM_LOCAL)))) {
-				c->x86_cache_max_rmid = ecx;
-				c->x86_cache_occ_scale = ebx;
-			}
-		} else {
-			c->x86_cache_max_rmid = -1;
-			c->x86_cache_occ_scale = -1;
-		}
-	}
-
 	/* AMD-defined flags: level 0x80000001 */
 	eax = cpuid_eax(0x80000000);
 	c->extended_cpuid_level = eax;
@@ -889,6 +894,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 
 	init_scattered_cpuid_features(c);
 	init_speculation_control(c);
+	init_cqm(c);
 
 	/*
 	 * Clear/Set all flags overridden by options, after probe.

commit 212bf4fdb7f9eeeb99afd97ebad677d43e7b55ac
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:49 2019 -0400

    x86/topology: Define topology_logical_die_id()
    
    Define topology_logical_die_id() ala existing topology_logical_package_id()
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Zhang Rui <rui.zhang@intel.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/2f3526e25ae14fbeff26fb26e877d159df8946d9.1557769318.git.len.brown@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d7f55ad2dfb1..8cdca1223b0f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1298,6 +1298,7 @@ static void validate_apic_and_package_id(struct cpuinfo_x86 *c)
 		       cpu, apicid, c->initial_apicid);
 	}
 	BUG_ON(topology_update_package_map(c->phys_proc_id, cpu));
+	BUG_ON(topology_update_die_map(c->cpu_die_id, cpu));
 #else
 	c->logical_proc_id = 0;
 #endif

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d7f55ad2dfb1..2c57fffebf9b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /* cpu_feature_enabled() cannot be used this early */
 #define USE_EARLY_PGTABLE_L5
 

commit fa4bff165070dc40a3de35b78e4f8da8e8d85ec5
Merge: 63863ee8e2f6 95310e348a32
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 14 07:57:29 2019 -0700

    Merge branch 'x86-mds-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 MDS mitigations from Thomas Gleixner:
     "Microarchitectural Data Sampling (MDS) is a hardware vulnerability
      which allows unprivileged speculative access to data which is
      available in various CPU internal buffers. This new set of misfeatures
      has the following CVEs assigned:
    
         CVE-2018-12126  MSBDS  Microarchitectural Store Buffer Data Sampling
         CVE-2018-12130  MFBDS  Microarchitectural Fill Buffer Data Sampling
         CVE-2018-12127  MLPDS  Microarchitectural Load Port Data Sampling
         CVE-2019-11091  MDSUM  Microarchitectural Data Sampling Uncacheable Memory
    
      MDS attacks target microarchitectural buffers which speculatively
      forward data under certain conditions. Disclosure gadgets can expose
      this data via cache side channels.
    
      Contrary to other speculation based vulnerabilities the MDS
      vulnerability does not allow the attacker to control the memory target
      address. As a consequence the attacks are purely sampling based, but
      as demonstrated with the TLBleed attack samples can be postprocessed
      successfully.
    
      The mitigation is to flush the microarchitectural buffers on return to
      user space and before entering a VM. It's bolted on the VERW
      instruction and requires a microcode update. As some of the attacks
      exploit data structures shared between hyperthreads, full protection
      requires to disable hyperthreading. The kernel does not do that by
      default to avoid breaking unattended updates.
    
      The mitigation set comes with documentation for administrators and a
      deeper technical view"
    
    * 'x86-mds-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/speculation/mds: Fix documentation typo
      Documentation: Correct the possible MDS sysfs values
      x86/mds: Add MDSUM variant to the MDS documentation
      x86/speculation/mds: Add 'mitigations=' support for MDS
      x86/speculation/mds: Print SMT vulnerable on MSBDS with mitigations off
      x86/speculation/mds: Fix comment
      x86/speculation/mds: Add SMT warning message
      x86/speculation: Move arch_smt_update() call to after mitigation decisions
      x86/speculation/mds: Add mds=full,nosmt cmdline option
      Documentation: Add MDS vulnerability documentation
      Documentation: Move L1TF to separate directory
      x86/speculation/mds: Add mitigation mode VMWERV
      x86/speculation/mds: Add sysfs reporting for MDS
      x86/speculation/mds: Add mitigation control for MDS
      x86/speculation/mds: Conditionally clear CPU buffers on idle entry
      x86/kvm/vmx: Add MDS protection when L1D Flush is not active
      x86/speculation/mds: Clear CPU buffers on exit to user
      x86/speculation/mds: Add mds_clear_cpu_buffers()
      x86/kvm: Expose X86_FEATURE_MD_CLEAR to guests
      x86/speculation/mds: Add BUG_MSBDS_ONLY
      ...

commit 8ff468c29e9a9c3afe9152c10c7b141343270bf3
Merge: 68253e718c27 d9c9ce34ed5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 10:24:10 2019 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU state handling updates from Borislav Petkov:
     "This contains work started by Rik van Riel and brought to fruition by
      Sebastian Andrzej Siewior with the main goal to optimize when to load
      FPU registers: only when returning to userspace and not on every
      context switch (while the task remains in the kernel).
    
      In addition, this optimization makes kernel_fpu_begin() cheaper by
      requiring registers saving only on the first invocation and skipping
      that in following ones.
    
      What is more, this series cleans up and streamlines many aspects of
      the already complex FPU code, hopefully making it more palatable for
      future improvements and simplifications.
    
      Finally, there's a __user annotations fix from Jann Horn"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (29 commits)
      x86/fpu: Fault-in user stack if copy_fpstate_to_sigframe() fails
      x86/pkeys: Add PKRU value to init_fpstate
      x86/fpu: Restore regs in copy_fpstate_to_sigframe() in order to use the fastpath
      x86/fpu: Add a fastpath to copy_fpstate_to_sigframe()
      x86/fpu: Add a fastpath to __fpu__restore_sig()
      x86/fpu: Defer FPU state load until return to userspace
      x86/fpu: Merge the two code paths in __fpu__restore_sig()
      x86/fpu: Restore from kernel memory on the 64-bit path too
      x86/fpu: Inline copy_user_to_fpregs_zeroing()
      x86/fpu: Update xstate's PKRU value on write_pkru()
      x86/fpu: Prepare copy_fpstate_to_sigframe() for TIF_NEED_FPU_LOAD
      x86/fpu: Always store the registers in copy_fpstate_to_sigframe()
      x86/entry: Add TIF_NEED_FPU_LOAD
      x86/fpu: Eager switch PKRU state
      x86/pkeys: Don't check if PKRU is zero before writing it
      x86/fpu: Only write PKRU if it is different from current
      x86/pkeys: Provide *pkru() helpers
      x86/fpu: Use a feature number instead of mask in two more helpers
      x86/fpu: Make __raw_xsave_addr() use a feature number instead of mask
      x86/fpu: Add an __fpregs_load_activate() internal helper
      ...

commit 8f5e823f9131a430b12f73e9436d7486e20c16f5
Merge: 59df1c2bdecb e07095c9bbcd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 19:40:31 2019 -0700

    Merge tag 'pm-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These fix the (Intel-specific) Performance and Energy Bias Hint (EPB)
      handling and expose it to user space via sysfs, fix and clean up
      several cpufreq drivers, add support for two new chips to the qoriq
      cpufreq driver, fix, simplify and clean up the cpufreq core and the
      schedutil governor, add support for "CPU" domains to the generic power
      domains (genpd) framework and provide low-level PSCI firmware support
      for that feature, fix the exynos cpuidle driver and fix a couple of
      issues in the devfreq subsystem and clean it up.
    
      Specifics:
    
       - Fix the handling of Performance and Energy Bias Hint (EPB) on Intel
         processors and expose it to user space via sysfs to avoid having to
         access it through the generic MSR I/F (Rafael Wysocki).
    
       - Improve the handling of global turbo changes made by the platform
         firmware in the intel_pstate driver (Rafael Wysocki).
    
       - Convert some slow-path static_cpu_has() callers to boot_cpu_has()
         in cpufreq (Borislav Petkov).
    
       - Fix the frequency calculation loop in the armada-37xx cpufreq
         driver (Gregory CLEMENT).
    
       - Fix possible object reference leaks in multuple cpufreq drivers
         (Wen Yang).
    
       - Fix kerneldoc comment in the centrino cpufreq driver (dongjian).
    
       - Clean up the ACPI and maple cpufreq drivers (Viresh Kumar, Mohan
         Kumar).
    
       - Add support for lx2160a and ls1028a to the qoriq cpufreq driver
         (Vabhav Sharma, Yuantian Tang).
    
       - Fix kobject memory leak in the cpufreq core (Viresh Kumar).
    
       - Simplify the IOwait boosting in the schedutil cpufreq governor and
         rework the TSC cpufreq notifier on x86 (Rafael Wysocki).
    
       - Clean up the cpufreq core and statistics code (Yue Hu, Kyle Lin).
    
       - Improve the cpufreq documentation, add SPDX license tags to some PM
         documentation files and unify copyright notices in them (Rafael
         Wysocki).
    
       - Add support for "CPU" domains to the generic power domains (genpd)
         framework and provide low-level PSCI firmware support for that
         feature (Ulf Hansson).
    
       - Rearrange the PSCI firmware support code and add support for
         SYSTEM_RESET2 to it (Ulf Hansson, Sudeep Holla).
    
       - Improve genpd support for devices in multiple power domains (Ulf
         Hansson).
    
       - Unify target residency for the AFTR and coupled AFTR states in the
         exynos cpuidle driver (Marek Szyprowski).
    
       - Introduce new helper routine in the operating performance points
         (OPP) framework (Andrew-sh.Cheng).
    
       - Add support for passing on-die termination (ODT) and auto power
         down parameters from the kernel to Trusted Firmware-A (TF-A) to the
         rk3399_dmc devfreq driver (Enric Balletbo i Serra).
    
       - Add tracing to devfreq (Lukasz Luba).
    
       - Make the exynos-bus devfreq driver suspend all devices on system
         shutdown (Marek Szyprowski).
    
       - Fix a few minor issues in the devfreq subsystem and clean it up
         somewhat (Enric Balletbo i Serra, MyungJoo Ham, Rob Herring,
         Saravana Kannan, Yangtao Li).
    
       - Improve system wakeup diagnostics (Stephen Boyd).
    
       - Rework filesystem sync messages emitted during system suspend and
         hibernation (Harry Pan)"
    
    * tag 'pm-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (72 commits)
      cpufreq: Fix kobject memleak
      cpufreq: armada-37xx: fix frequency calculation for opp
      cpufreq: centrino: Fix centrino_setpolicy() kerneldoc comment
      cpufreq: qoriq: add support for lx2160a
      x86: tsc: Rework time_cpufreq_notifier()
      PM / Domains: Allow to attach a CPU via genpd_dev_pm_attach_by_id|name()
      PM / Domains: Search for the CPU device outside the genpd lock
      PM / Domains: Drop unused in-parameter to some genpd functions
      PM / Domains: Use the base device for driver_deferred_probe_check_state()
      cpufreq: qoriq: Add ls1028a chip support
      PM / Domains: Enable genpd_dev_pm_attach_by_id|name() for single PM domain
      PM / Domains: Allow OF lookup for multi PM domain case from ->attach_dev()
      PM / Domains: Don't kfree() the virtual device in the error path
      cpufreq: Move ->get callback check outside of __cpufreq_get()
      PM / Domains: remove unnecessary unlikely()
      cpufreq: Remove needless bios_limit check in show_bios_limit()
      drivers/cpufreq/acpi-cpufreq.c: This fixes the following checkpatch warning
      firmware/psci: add support for SYSTEM_RESET2
      PM / devfreq: add tracing for scheduling work
      trace: events: add devfreq trace event file
      ...

commit 8f147727030bf9e81331ab9b8f42d4611bb6a3d9
Merge: 53f8b081c184 2c4645439e8f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 15:56:41 2019 -0700

    Merge branch 'x86-irq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 irq updates from Ingo Molnar:
     "Here are the main changes in this tree:
    
       - Introduce x86-64 IRQ/exception/debug stack guard pages to detect
         stack overflows immediately and deterministically.
    
       - Clean up over a decade worth of cruft accumulated.
    
      The outcome of this should be more clear-cut faults/crashes when any
      of the low level x86 CPU stacks overflow, instead of silent memory
      corruption and sporadic failures much later on"
    
    * 'x86-irq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      x86/irq: Fix outdated comments
      x86/irq/64: Remove stack overflow debug code
      x86/irq/64: Remap the IRQ stack with guard pages
      x86/irq/64: Split the IRQ stack into its own pages
      x86/irq/64: Init hardirq_stack_ptr during CPU hotplug
      x86/irq/32: Handle irq stack allocation failure proper
      x86/irq/32: Invoke irq_ctx_init() from init_IRQ()
      x86/irq/64: Rename irq_stack_ptr to hardirq_stack_ptr
      x86/irq/32: Rename hard/softirq_stack to hard/softirq_stack_ptr
      x86/irq/32: Make irq stack a character array
      x86/irq/32: Define IRQ_STACK_SIZE
      x86/dumpstack/64: Speedup in_exception_stack()
      x86/exceptions: Split debug IST stack
      x86/exceptions: Enable IST guard pages
      x86/exceptions: Disconnect IST index and stack order
      x86/cpu: Remove orig_ist array
      x86/cpu: Prepare TSS.IST setup for guard pages
      x86/dumpstack/64: Use cpu_entry_area instead of orig_ist
      x86/irq/64: Use cpu entry area instead of orig_ist
      x86/traps: Use cpu_entry_area instead of orig_ist
      ...

commit e6401c13093173aad709a5c6de00cf8d692ee786
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Apr 14 18:00:06 2019 +0200

    x86/irq/64: Split the IRQ stack into its own pages
    
    Currently, the IRQ stack is hardcoded as the first page of the percpu
    area, and the stack canary lives on the IRQ stack. The former gets in
    the way of adding an IRQ stack guard page, and the latter is a potential
    weakness in the stack canary mechanism.
    
    Split the IRQ stack into its own private percpu pages.
    
    [ tglx: Make 64 and 32 bit share struct irq_stack ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: Feng Tang <feng.tang@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jordan Borgner <mail@jordan-borgner.de>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Maran Wilson <maran.wilson@oracle.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: "Rafael vila de Espndola" <rafael@espindo.la>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Cc: xen-devel@lists.xenproject.org
    Link: https://lkml.kernel.org/r/20190414160146.267376656@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1222080838da..801c6f040faa 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1498,9 +1498,9 @@ static __init int setup_clearcpuid(char *arg)
 __setup("clearcpuid=", setup_clearcpuid);
 
 #ifdef CONFIG_X86_64
-DEFINE_PER_CPU_FIRST(union irq_stack_union,
-		     irq_stack_union) __aligned(PAGE_SIZE) __visible;
-EXPORT_PER_CPU_SYMBOL_GPL(irq_stack_union);
+DEFINE_PER_CPU_FIRST(struct fixed_percpu_data,
+		     fixed_percpu_data) __aligned(PAGE_SIZE) __visible;
+EXPORT_PER_CPU_SYMBOL_GPL(fixed_percpu_data);
 
 /*
  * The following percpu variables are hot.  Align current_task to
@@ -1510,7 +1510,7 @@ DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =
 	&init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
-DEFINE_PER_CPU(char *, hardirq_stack_ptr);
+DEFINE_PER_CPU(struct irq_stack *, hardirq_stack_ptr);
 DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 
 DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;

commit 0ac26104208450d35c4e68754ce0c67b3a4d7802
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:05 2019 +0200

    x86/irq/64: Init hardirq_stack_ptr during CPU hotplug
    
    Preparatory change for disentangling the irq stack union as a
    prerequisite for irq stacks with guard pages.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Link: https://lkml.kernel.org/r/20190414160146.177558566@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 13ec72bb8f36..1222080838da 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1510,9 +1510,7 @@ DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =
 	&init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
-DEFINE_PER_CPU(char *, hardirq_stack_ptr) =
-	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE;
-
+DEFINE_PER_CPU(char *, hardirq_stack_ptr);
 DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 
 DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;

commit 758a2e312228410f2f5092ade558109e93dc3ee8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:02 2019 +0200

    x86/irq/64: Rename irq_stack_ptr to hardirq_stack_ptr
    
    Preparatory patch to share code with 32bit.
    
    No functional changes.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.912584074@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 88cab45707a9..13ec72bb8f36 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1510,7 +1510,7 @@ DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =
 	&init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
-DEFINE_PER_CPU(char *, irq_stack_ptr) =
+DEFINE_PER_CPU(char *, hardirq_stack_ptr) =
 	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE;
 
 DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;

commit 2a594d4ccf3f10f80b77d71bd3dad10813ac0137
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:57 2019 +0200

    x86/exceptions: Split debug IST stack
    
    The debug IST stack is actually two separate debug stacks to handle #DB
    recursion. This is required because the CPU starts always at top of stack
    on exception entry, which means on #DB recursion the second #DB would
    overwrite the stack of the first.
    
    The low level entry code therefore adjusts the top of stack on entry so a
    secondary #DB starts from a different stack page. But the stack pages are
    adjacent without a guard page between them.
    
    Split the debug stack into 3 stacks which are separated by guard pages. The
    3rd stack is never mapped into the cpu_entry_area and is only there to
    catch triple #DB nesting:
    
          --- top of DB_stack       <- Initial stack
          --- end of DB_stack
              guard page
    
          --- top of DB1_stack      <- Top of stack after entering first #DB
          --- end of DB1_stack
              guard page
    
          --- top of DB2_stack      <- Top of stack after entering second #DB
          --- end of DB2_stack
              guard page
    
    If DB2 would not act as the final guard hole, a second #DB would point the
    top of #DB stack to the stack below #DB1 which would be valid and not catch
    the not so desired triple nesting.
    
    The backing store does not allocate any memory for DB2 and its guard page
    as it is not going to be mapped into the cpu_entry_area.
    
     - Adjust the low level entry code so it adjusts top of #DB with the offset
       between the stacks instead of exception stack size.
    
     - Make the dumpstack code aware of the new stacks.
    
     - Adjust the in_debug_stack() implementation and move it into the NMI code
       where it belongs. As this is NMI hotpath code, it just checks the full
       area between top of DB_stack and bottom of DB1_stack without checking
       for the guard page. That's correct because the NMI cannot hit a
       stackpointer pointing to the guard page between DB and DB1 stack.  Even
       if it would, then the NMI operation still is unaffected, but the resume
       of the debug exception on the topmost DB stack will crash by touching
       the guard page.
    
      [ bp: Make exception_stack_names static const char * const ]
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: linux-doc@vger.kernel.org
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.439944544@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 143aceaf9a9a..88cab45707a9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1549,17 +1549,7 @@ void syscall_init(void)
 	       X86_EFLAGS_IOPL|X86_EFLAGS_AC|X86_EFLAGS_NT);
 }
 
-static DEFINE_PER_CPU(unsigned long, debug_stack_addr);
 DEFINE_PER_CPU(int, debug_stack_usage);
-
-int is_debug_stack(unsigned long addr)
-{
-	return __this_cpu_read(debug_stack_usage) ||
-		(addr <= __this_cpu_read(debug_stack_addr) &&
-		 addr > (__this_cpu_read(debug_stack_addr) - DEBUG_STKSZ));
-}
-NOKPROBE_SYMBOL(is_debug_stack);
-
 DEFINE_PER_CPU(u32, debug_idt_ctr);
 
 void debug_stack_set_zero(void)
@@ -1735,7 +1725,6 @@ void cpu_init(void)
 		t->x86_tss.ist[IST_INDEX_NMI] = __this_cpu_ist_top_va(NMI);
 		t->x86_tss.ist[IST_INDEX_DB] = __this_cpu_ist_top_va(DB);
 		t->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
-		per_cpu(debug_stack_addr, cpu) = t->x86_tss.ist[IST_INDEX_DB];
 	}
 
 	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;

commit 3207426925d2b4da390be8068df1d1c2b36e5918
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:55 2019 +0200

    x86/exceptions: Disconnect IST index and stack order
    
    The entry order of the TSS.IST array and the order of the stack
    storage/mapping are not required to be the same.
    
    With the upcoming split of the debug stack this is going to fall apart as
    the number of TSS.IST array entries stays the same while the actual stacks
    are increasing.
    
    Make them separate so that code like dumpstack can just utilize the mapping
    order. The IST index is solely required for the actual TSS.IST array
    initialization.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.241588113@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8243f198fb7f..143aceaf9a9a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1731,11 +1731,11 @@ void cpu_init(void)
 	 * set up and load the per-CPU TSS
 	 */
 	if (!t->x86_tss.ist[0]) {
-		t->x86_tss.ist[ESTACK_DF] = __this_cpu_ist_top_va(DF);
-		t->x86_tss.ist[ESTACK_NMI] = __this_cpu_ist_top_va(NMI);
-		t->x86_tss.ist[ESTACK_DB] = __this_cpu_ist_top_va(DB);
-		t->x86_tss.ist[ESTACK_MCE] = __this_cpu_ist_top_va(MCE);
-		per_cpu(debug_stack_addr, cpu) = t->x86_tss.ist[ESTACK_DB];
+		t->x86_tss.ist[IST_INDEX_DF] = __this_cpu_ist_top_va(DF);
+		t->x86_tss.ist[IST_INDEX_NMI] = __this_cpu_ist_top_va(NMI);
+		t->x86_tss.ist[IST_INDEX_DB] = __this_cpu_ist_top_va(DB);
+		t->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
+		per_cpu(debug_stack_addr, cpu) = t->x86_tss.ist[IST_INDEX_DB];
 	}
 
 	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;

commit 4d68c3d0ecd5fcba8876e8a58ac41ffb360de43e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:54 2019 +0200

    x86/cpu: Remove orig_ist array
    
    All users gone.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.151435667@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4b01b71415f5..8243f198fb7f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1549,12 +1549,6 @@ void syscall_init(void)
 	       X86_EFLAGS_IOPL|X86_EFLAGS_AC|X86_EFLAGS_NT);
 }
 
-/*
- * Copies of the original ist values from the tss are only accessed during
- * debugging, no special alignment required.
- */
-DEFINE_PER_CPU(struct orig_ist, orig_ist);
-
 static DEFINE_PER_CPU(unsigned long, debug_stack_addr);
 DEFINE_PER_CPU(int, debug_stack_usage);
 

commit f6ef73224a0f0400c3979c8bc68b383f9d2eb9d8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:53 2019 +0200

    x86/cpu: Prepare TSS.IST setup for guard pages
    
    Convert the TSS.IST setup code to use the cpu entry area information
    directly instead of assuming a linear mapping of the IST stacks.
    
    The store to orig_ist[] is no longer required as there are no users
    anymore.
    
    This is the last preparatory step towards IST guard pages.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160145.061686012@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 24b801ea7522..4b01b71415f5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -507,19 +507,6 @@ void load_percpu_segment(int cpu)
 DEFINE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);
 #endif
 
-#ifdef CONFIG_X86_64
-/*
- * Special IST stacks which the CPU switches to when it calls
- * an IST-marked descriptor entry. Up to 7 stacks (hardware
- * limit), all of them are 4K, except the debug stack which
- * is 8K.
- */
-static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {
-	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,
-	  [ESTACK_DB]				= DEBUG_STKSZ
-};
-#endif
-
 /* Load the original GDT from the per-cpu structure */
 void load_direct_gdt(int cpu)
 {
@@ -1690,17 +1677,14 @@ static void setup_getcpu(int cpu)
  * initialized (naturally) in the bootstrap process, such as the GDT
  * and IDT. We reload them nevertheless, this function acts as a
  * 'CPU state barrier', nothing should get across.
- * A lot of state is already set up in PDA init for 64 bit
  */
 #ifdef CONFIG_X86_64
 
 void cpu_init(void)
 {
-	struct orig_ist *oist;
+	int cpu = raw_smp_processor_id();
 	struct task_struct *me;
 	struct tss_struct *t;
-	unsigned long v;
-	int cpu = raw_smp_processor_id();
 	int i;
 
 	wait_for_master_cpu(cpu);
@@ -1715,7 +1699,6 @@ void cpu_init(void)
 		load_ucode_ap();
 
 	t = &per_cpu(cpu_tss_rw, cpu);
-	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
 	if (this_cpu_read(numa_node) == 0 &&
@@ -1753,16 +1736,12 @@ void cpu_init(void)
 	/*
 	 * set up and load the per-CPU TSS
 	 */
-	if (!oist->ist[0]) {
-		char *estacks = (char *)&get_cpu_entry_area(cpu)->estacks;
-
-		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
-			estacks += exception_stack_sizes[v];
-			oist->ist[v] = t->x86_tss.ist[v] =
-					(unsigned long)estacks;
-			if (v == ESTACK_DB)
-				per_cpu(debug_stack_addr, cpu) = (unsigned long)estacks;
-		}
+	if (!t->x86_tss.ist[0]) {
+		t->x86_tss.ist[ESTACK_DF] = __this_cpu_ist_top_va(DF);
+		t->x86_tss.ist[ESTACK_NMI] = __this_cpu_ist_top_va(NMI);
+		t->x86_tss.ist[ESTACK_DB] = __this_cpu_ist_top_va(DB);
+		t->x86_tss.ist[ESTACK_MCE] = __this_cpu_ist_top_va(MCE);
+		per_cpu(debug_stack_addr, cpu) = t->x86_tss.ist[ESTACK_DB];
 	}
 
 	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;

commit 019b17b3ffe48100e52f609ca1c6ed6e5a40cba1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:47 2019 +0200

    x86/exceptions: Add structs for exception stacks
    
    At the moment everything assumes a full linear mapping of the various
    exception stacks. Adding guard pages to the cpu entry area mapping of the
    exception stacks will break that assumption.
    
    As a preparatory step convert both the real storage and the effective
    mapping in the cpu entry area from character arrays to structures.
    
    To ensure that both arrays have the same ordering and the same size of the
    individual stacks fill the members with a macro. The guard size is the only
    difference between the two resulting structures. For now both have guard
    size 0 until the preparation of all usage sites is done.
    
    Provide a couple of helper macros which are used in the following
    conversions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160144.506807893@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0e4cb718fc4a..24b801ea7522 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1754,7 +1754,7 @@ void cpu_init(void)
 	 * set up and load the per-CPU TSS
 	 */
 	if (!oist->ist[0]) {
-		char *estacks = get_cpu_entry_area(cpu)->exception_stacks;
+		char *estacks = (char *)&get_cpu_entry_area(cpu)->estacks;
 
 		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
 			estacks += exception_stack_sizes[v];

commit 8f34c5b5afce91d171bb0802631197484cb69b8b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 17:59:45 2019 +0200

    x86/exceptions: Make IST index zero based
    
    The defines for the exception stack (IST) array in the TSS are using the
    SDM convention IST1 - IST7. That causes all sorts of code to subtract 1 for
    array indices related to IST. That's confusing at best and does not provide
    any value.
    
    Make the indices zero based and fixup the usage sites. The only code which
    needs to adjust the 0 based index is the interrupt descriptor setup which
    needs to add 1 now.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: linux-doc@vger.kernel.org
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160144.331772825@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cb28e98a0659..0e4cb718fc4a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -516,7 +516,7 @@ DEFINE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);
  */
 static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {
 	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,
-	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
+	  [ESTACK_DB]				= DEBUG_STKSZ
 };
 #endif
 
@@ -1760,7 +1760,7 @@ void cpu_init(void)
 			estacks += exception_stack_sizes[v];
 			oist->ist[v] = t->x86_tss.ist[v] =
 					(unsigned long)estacks;
-			if (v == DEBUG_STACK-1)
+			if (v == ESTACK_DB)
 				per_cpu(debug_stack_addr, cpu) = (unsigned long)estacks;
 		}
 	}

commit a5eff7259790d5314eff10563d6e59d358cce482
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 3 18:41:56 2019 +0200

    x86/pkeys: Add PKRU value to init_fpstate
    
    The task's initial PKRU value is set partly for fpu__clear()/
    copy_init_pkru_to_fpregs(). It is not part of init_fpstate.xsave and
    instead it is set explicitly.
    
    If the user removes the PKRU state from XSAVE in the signal handler then
    __fpu__restore_sig() will restore the missing bits from `init_fpstate'
    and initialize the PKRU value to 0.
    
    Add the `init_pkru_value' to `init_fpstate' so it is set to the init
    value in such a case.
    
    In theory copy_init_pkru_to_fpregs() could be removed because restoring
    the PKRU at return-to-userland should be enough.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "Jason A. Donenfeld" <Jason@zx2c4.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: kvm ML <kvm@vger.kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190403164156.19645-28-bigeasy@linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cb28e98a0659..352fa19e6311 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -372,6 +372,8 @@ static bool pku_disabled;
 
 static __always_inline void setup_pku(struct cpuinfo_x86 *c)
 {
+	struct pkru_state *pk;
+
 	/* check the boot processor, plus compile options for PKU: */
 	if (!cpu_feature_enabled(X86_FEATURE_PKU))
 		return;
@@ -382,6 +384,9 @@ static __always_inline void setup_pku(struct cpuinfo_x86 *c)
 		return;
 
 	cr4_set_bits(X86_CR4_PKE);
+	pk = get_xsave_addr(&init_fpstate.xsave, XFEATURE_PKRU);
+	if (pk)
+		pk->pkru = init_pkru_value;
 	/*
 	 * Seting X86_CR4_PKE will cause the X86_FEATURE_OSPKE
 	 * cpuid bit to be set.  We need to ensure that we

commit 67e87d43b794a8886b5d075b3e0fdd0c615a595f
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 29 19:52:59 2019 +0100

    x86: Convert some slow-path static_cpu_has() callers to boot_cpu_has()
    
    Using static_cpu_has() is pointless on those paths, convert them to the
    boot_cpu_has() variant.
    
    No functional changes.
    
    Reported-by: Nadav Amit <nadav.amit@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Reviewed-by: Juergen Gross <jgross@suse.com> # for paravirt
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: linux-edac@vger.kernel.org
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: virtualization@lists.linux-foundation.org
    Cc: x86@kernel.org
    Link: https://lkml.kernel.org/r/20190330112022.28888-3-bp@alien8.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cb28e98a0659..95a5faf3a6a0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1668,7 +1668,7 @@ static void setup_getcpu(int cpu)
 	unsigned long cpudata = vdso_encode_cpunode(cpu, early_cpu_to_node(cpu));
 	struct desc_struct d = { };
 
-	if (static_cpu_has(X86_FEATURE_RDTSCP))
+	if (boot_cpu_has(X86_FEATURE_RDTSCP))
 		write_rdtscp_aux(cpudata);
 
 	/* Store CPU and node number in limit. */

commit 5861381d486601430cccf64849bd0a226154bc0d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 21 23:18:01 2019 +0100

    PM / arch: x86: Rework the MSR_IA32_ENERGY_PERF_BIAS handling
    
    The current handling of MSR_IA32_ENERGY_PERF_BIAS in the kernel is
    problematic, because it may cause changes made by user space to that
    MSR (with the help of the x86_energy_perf_policy tool, for example)
    to be lost every time a CPU goes offline and then back online as well
    as during system-wide power management transitions into sleep states
    and back into the working state.
    
    The first problem is that if the current EPB value for a CPU going
    online is 0 ('performance'), the kernel will change it to 6 ('normal')
    regardless of whether or not this is the first bring-up of that CPU.
    That also happens during system-wide resume from sleep states
    (including, but not limited to, hibernation).  However, the EPB may
    have been adjusted by user space this way and the kernel should not
    blindly override that setting.
    
    The second problem is that if the platform firmware resets the EPB
    values for any CPUs during system-wide resume from a sleep state,
    the kernel will not restore their previous EPB values that may
    have been set by user space before the preceding system-wide
    suspend transition.  Again, that behavior may at least be confusing
    from the user space perspective.
    
    In order to address these issues, rework the handling of
    MSR_IA32_ENERGY_PERF_BIAS so that the EPB value is saved on CPU
    offline and restored on CPU online as well as (for the boot CPU)
    during the syscore stages of system-wide suspend and resume
    transitions, respectively.
    
    However, retain the policy by which the EPB is set to 6 ('normal')
    on the first bring-up of each CPU if its initial value is 0, based
    on the observation that 0 may mean 'not initialized' just as well as
    'performance' in that case.
    
    While at it, move the MSR_IA32_ENERGY_PERF_BIAS handling code into
    a separate file and document it in Documentation/admin-guide.
    
    Fixes: abe48b108247 (x86, intel, power: Initialize MSR_IA32_ENERGY_PERF_BIAS)
    Fixes: b51ef52df71c (x86/cpu: Restore MSR_IA32_ENERGY_PERF_BIAS after resume)
    Reported-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cb28e98a0659..5e37dfa4d9df 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1864,23 +1864,6 @@ void cpu_init(void)
 }
 #endif
 
-static void bsp_resume(void)
-{
-	if (this_cpu->c_bsp_resume)
-		this_cpu->c_bsp_resume(&boot_cpu_data);
-}
-
-static struct syscore_ops cpu_syscore_ops = {
-	.resume		= bsp_resume,
-};
-
-static int __init init_cpu_syscore(void)
-{
-	register_syscore_ops(&cpu_syscore_ops);
-	return 0;
-}
-core_initcall(init_cpu_syscore);
-
 /*
  * The microcode loader calls this upon late microcode load to recheck features,
  * only when microcode has been updated. Caller holds microcode_mutex and CPU

commit e261f209c3666e842fd645a1e31f001c3a26def9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 1 20:21:08 2019 +0100

    x86/speculation/mds: Add BUG_MSBDS_ONLY
    
    This bug bit is set on CPUs which are only affected by Microarchitectural
    Store Buffer Data Sampling (MSBDS) and not by any other MDS variant.
    
    This is important because the Store Buffers are partitioned between
    Hyper-Threads so cross thread forwarding is not possible. But if a thread
    enters or exits a sleep state the store buffer is repartitioned which can
    expose data from one thread to the other. This transition can be mitigated.
    
    That means that for CPUs which are only affected by MSBDS SMT can be
    enabled, if the CPU is not affected by other SMT sensitive vulnerabilities,
    e.g. L1TF. The XEON PHI variants fall into that category. Also the
    Silvermont/Airmont ATOMs, but for them it's not really relevant as they do
    not support SMT, but mark them for completeness sake.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Jon Masters <jcm@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e34817bca504..132a63dc5a76 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -953,6 +953,7 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #define NO_SSB		BIT(2)
 #define NO_L1TF		BIT(3)
 #define NO_MDS		BIT(4)
+#define MSBDS_ONLY	BIT(5)
 
 #define VULNWL(_vendor, _family, _model, _whitelist)	\
 	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }
@@ -979,16 +980,16 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL_INTEL(ATOM_BONNELL,		NO_SPECULATION),
 	VULNWL_INTEL(ATOM_BONNELL_MID,		NO_SPECULATION),
 
-	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF),
-	VULNWL_INTEL(ATOM_SILVERMONT_X,		NO_SSB | NO_L1TF),
-	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF),
-	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF),
-	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF),
-	VULNWL_INTEL(XEON_PHI_KNM,		NO_SSB | NO_L1TF),
+	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY),
+	VULNWL_INTEL(ATOM_SILVERMONT_X,		NO_SSB | NO_L1TF | MSBDS_ONLY),
+	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF | MSBDS_ONLY),
+	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF | MSBDS_ONLY),
+	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF | MSBDS_ONLY),
+	VULNWL_INTEL(XEON_PHI_KNM,		NO_SSB | NO_L1TF | MSBDS_ONLY),
 
 	VULNWL_INTEL(CORE_YONAH,		NO_SSB),
 
-	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF),
+	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF | MSBDS_ONLY),
 
 	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF),
 	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_MDS | NO_L1TF),
@@ -1033,8 +1034,11 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (ia32_cap & ARCH_CAP_IBRS_ALL)
 		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
 
-	if (!cpu_matches(NO_MDS) && !(ia32_cap & ARCH_CAP_MDS_NO))
+	if (!cpu_matches(NO_MDS) && !(ia32_cap & ARCH_CAP_MDS_NO)) {
 		setup_force_cpu_bug(X86_BUG_MDS);
+		if (cpu_matches(MSBDS_ONLY))
+			setup_force_cpu_bug(X86_BUG_MSBDS_ONLY);
+	}
 
 	if (cpu_matches(NO_MELTDOWN))
 		return;

commit ed5194c2732c8084af9fd159c146ea92bf137128
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Jan 18 16:50:16 2019 -0800

    x86/speculation/mds: Add basic bug infrastructure for MDS
    
    Microarchitectural Data Sampling (MDS), is a class of side channel attacks
    on internal buffers in Intel CPUs. The variants are:
    
     - Microarchitectural Store Buffer Data Sampling (MSBDS) (CVE-2018-12126)
     - Microarchitectural Fill Buffer Data Sampling (MFBDS) (CVE-2018-12130)
     - Microarchitectural Load Port Data Sampling (MLPDS) (CVE-2018-12127)
    
    MSBDS leaks Store Buffer Entries which can be speculatively forwarded to a
    dependent load (store-to-load forwarding) as an optimization. The forward
    can also happen to a faulting or assisting load operation for a different
    memory address, which can be exploited under certain conditions. Store
    buffers are partitioned between Hyper-Threads so cross thread forwarding is
    not possible. But if a thread enters or exits a sleep state the store
    buffer is repartitioned which can expose data from one thread to the other.
    
    MFBDS leaks Fill Buffer Entries. Fill buffers are used internally to manage
    L1 miss situations and to hold data which is returned or sent in response
    to a memory or I/O operation. Fill buffers can forward data to a load
    operation and also write data to the cache. When the fill buffer is
    deallocated it can retain the stale data of the preceding operations which
    can then be forwarded to a faulting or assisting load operation, which can
    be exploited under certain conditions. Fill buffers are shared between
    Hyper-Threads so cross thread leakage is possible.
    
    MLDPS leaks Load Port Data. Load ports are used to perform load operations
    from memory or I/O. The received data is then forwarded to the register
    file or a subsequent operation. In some implementations the Load Port can
    contain stale data from a previous operation which can be forwarded to
    faulting or assisting loads under certain conditions, which again can be
    exploited eventually. Load ports are shared between Hyper-Threads so cross
    thread leakage is possible.
    
    All variants have the same mitigation for single CPU thread case (SMT off),
    so the kernel can treat them as one MDS issue.
    
    Add the basic infrastructure to detect if the current CPU is affected by
    MDS.
    
    [ tglx: Rewrote changelog ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Jon Masters <jcm@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 26ec15034f86..e34817bca504 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -952,6 +952,7 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #define NO_MELTDOWN	BIT(1)
 #define NO_SSB		BIT(2)
 #define NO_L1TF		BIT(3)
+#define NO_MDS		BIT(4)
 
 #define VULNWL(_vendor, _family, _model, _whitelist)	\
 	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }
@@ -971,6 +972,7 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL(INTEL,	5, X86_MODEL_ANY,	NO_SPECULATION),
 	VULNWL(NSC,	5, X86_MODEL_ANY,	NO_SPECULATION),
 
+	/* Intel Family 6 */
 	VULNWL_INTEL(ATOM_SALTWELL,		NO_SPECULATION),
 	VULNWL_INTEL(ATOM_SALTWELL_TABLET,	NO_SPECULATION),
 	VULNWL_INTEL(ATOM_SALTWELL_MID,		NO_SPECULATION),
@@ -987,18 +989,20 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL_INTEL(CORE_YONAH,		NO_SSB),
 
 	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF),
-	VULNWL_INTEL(ATOM_GOLDMONT,		NO_L1TF),
-	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_L1TF),
-	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_L1TF),
 
-	VULNWL_AMD(0x0f,		NO_MELTDOWN | NO_SSB | NO_L1TF),
-	VULNWL_AMD(0x10,		NO_MELTDOWN | NO_SSB | NO_L1TF),
-	VULNWL_AMD(0x11,		NO_MELTDOWN | NO_SSB | NO_L1TF),
-	VULNWL_AMD(0x12,		NO_MELTDOWN | NO_SSB | NO_L1TF),
+	VULNWL_INTEL(ATOM_GOLDMONT,		NO_MDS | NO_L1TF),
+	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_MDS | NO_L1TF),
+	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_MDS | NO_L1TF),
+
+	/* AMD Family 0xf - 0x12 */
+	VULNWL_AMD(0x0f,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
+	VULNWL_AMD(0x10,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
+	VULNWL_AMD(0x11,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
+	VULNWL_AMD(0x12,	NO_MELTDOWN | NO_SSB | NO_L1TF | NO_MDS),
 
 	/* FAMILY_ANY must be last, otherwise 0x0f - 0x12 matches won't work */
-	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF),
-	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF),
+	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS),
+	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF | NO_MDS),
 	{}
 };
 
@@ -1029,6 +1033,9 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (ia32_cap & ARCH_CAP_IBRS_ALL)
 		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
 
+	if (!cpu_matches(NO_MDS) && !(ia32_cap & ARCH_CAP_MDS_NO))
+		setup_force_cpu_bug(X86_BUG_MDS);
+
 	if (cpu_matches(NO_MELTDOWN))
 		return;
 

commit 36ad35131adacc29b328b9c8b6277a8bf0d6fd5d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 27 10:10:23 2019 +0100

    x86/speculation: Consolidate CPU whitelists
    
    The CPU vulnerability whitelists have some overlap and there are more
    whitelists coming along.
    
    Use the driver_data field in the x86_cpu_id struct to denote the
    whitelisted vulnerabilities and combine all whitelists into one.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jon Masters <jcm@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cb28e98a0659..26ec15034f86 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -948,61 +948,72 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #endif
 }
 
-static const __initconst struct x86_cpu_id cpu_no_speculation[] = {
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_SALTWELL,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_SALTWELL_TABLET,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_BONNELL_MID,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_SALTWELL_MID,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_BONNELL,	X86_FEATURE_ANY },
-	{ X86_VENDOR_CENTAUR,	5 },
-	{ X86_VENDOR_INTEL,	5 },
-	{ X86_VENDOR_NSC,	5 },
-	{ X86_VENDOR_ANY,	4 },
+#define NO_SPECULATION	BIT(0)
+#define NO_MELTDOWN	BIT(1)
+#define NO_SSB		BIT(2)
+#define NO_L1TF		BIT(3)
+
+#define VULNWL(_vendor, _family, _model, _whitelist)	\
+	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }
+
+#define VULNWL_INTEL(model, whitelist)		\
+	VULNWL(INTEL, 6, INTEL_FAM6_##model, whitelist)
+
+#define VULNWL_AMD(family, whitelist)		\
+	VULNWL(AMD, family, X86_MODEL_ANY, whitelist)
+
+#define VULNWL_HYGON(family, whitelist)		\
+	VULNWL(HYGON, family, X86_MODEL_ANY, whitelist)
+
+static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
+	VULNWL(ANY,	4, X86_MODEL_ANY,	NO_SPECULATION),
+	VULNWL(CENTAUR,	5, X86_MODEL_ANY,	NO_SPECULATION),
+	VULNWL(INTEL,	5, X86_MODEL_ANY,	NO_SPECULATION),
+	VULNWL(NSC,	5, X86_MODEL_ANY,	NO_SPECULATION),
+
+	VULNWL_INTEL(ATOM_SALTWELL,		NO_SPECULATION),
+	VULNWL_INTEL(ATOM_SALTWELL_TABLET,	NO_SPECULATION),
+	VULNWL_INTEL(ATOM_SALTWELL_MID,		NO_SPECULATION),
+	VULNWL_INTEL(ATOM_BONNELL,		NO_SPECULATION),
+	VULNWL_INTEL(ATOM_BONNELL_MID,		NO_SPECULATION),
+
+	VULNWL_INTEL(ATOM_SILVERMONT,		NO_SSB | NO_L1TF),
+	VULNWL_INTEL(ATOM_SILVERMONT_X,		NO_SSB | NO_L1TF),
+	VULNWL_INTEL(ATOM_SILVERMONT_MID,	NO_SSB | NO_L1TF),
+	VULNWL_INTEL(ATOM_AIRMONT,		NO_SSB | NO_L1TF),
+	VULNWL_INTEL(XEON_PHI_KNL,		NO_SSB | NO_L1TF),
+	VULNWL_INTEL(XEON_PHI_KNM,		NO_SSB | NO_L1TF),
+
+	VULNWL_INTEL(CORE_YONAH,		NO_SSB),
+
+	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF),
+	VULNWL_INTEL(ATOM_GOLDMONT,		NO_L1TF),
+	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_L1TF),
+	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_L1TF),
+
+	VULNWL_AMD(0x0f,		NO_MELTDOWN | NO_SSB | NO_L1TF),
+	VULNWL_AMD(0x10,		NO_MELTDOWN | NO_SSB | NO_L1TF),
+	VULNWL_AMD(0x11,		NO_MELTDOWN | NO_SSB | NO_L1TF),
+	VULNWL_AMD(0x12,		NO_MELTDOWN | NO_SSB | NO_L1TF),
+
+	/* FAMILY_ANY must be last, otherwise 0x0f - 0x12 matches won't work */
+	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF),
+	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF),
 	{}
 };
 
-static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
-	{ X86_VENDOR_AMD },
-	{ X86_VENDOR_HYGON },
-	{}
-};
-
-/* Only list CPUs which speculate but are non susceptible to SSB */
-static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_X	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_MID	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_CORE_YONAH		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
-	{ X86_VENDOR_AMD,	0x12,					},
-	{ X86_VENDOR_AMD,	0x11,					},
-	{ X86_VENDOR_AMD,	0x10,					},
-	{ X86_VENDOR_AMD,	0xf,					},
-	{}
-};
+static bool __init cpu_matches(unsigned long which)
+{
+	const struct x86_cpu_id *m = x86_match_cpu(cpu_vuln_whitelist);
 
-static const __initconst struct x86_cpu_id cpu_no_l1tf[] = {
-	/* in addition to cpu_no_speculation */
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_X	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_MID	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT_MID	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT_X	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT_PLUS	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
-	{}
-};
+	return m && !!(m->driver_data & which);
+}
 
 static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = 0;
 
-	if (x86_match_cpu(cpu_no_speculation))
+	if (cpu_matches(NO_SPECULATION))
 		return;
 
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
@@ -1011,15 +1022,14 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
 
-	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
-	   !(ia32_cap & ARCH_CAP_SSB_NO) &&
+	if (!cpu_matches(NO_SSB) && !(ia32_cap & ARCH_CAP_SSB_NO) &&
 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
 	if (ia32_cap & ARCH_CAP_IBRS_ALL)
 		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
 
-	if (x86_match_cpu(cpu_no_meltdown))
+	if (cpu_matches(NO_MELTDOWN))
 		return;
 
 	/* Rogue Data Cache Load? No! */
@@ -1028,7 +1038,7 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 
 	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 
-	if (x86_match_cpu(cpu_no_l1tf))
+	if (cpu_matches(NO_L1TF))
 		return;
 
 	setup_force_cpu_bug(X86_BUG_L1TF);

commit 438cbf8871246606f2fc1964d301fa02af39e4e4
Author: Lendacky, Thomas <Thomas.Lendacky@amd.com>
Date:   Tue Dec 4 22:27:20 2018 +0000

    x86/umip: Make the UMIP activated message generic
    
    The User Mode Instruction Prevention (UMIP) feature is part of the x86_64
    instruction set architecture and not specific to Intel.  Make the message
    generic.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2c56b8066280..cb28e98a0659 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -353,7 +353,7 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 
 	cr4_set_bits(X86_CR4_UMIP);
 
-	pr_info_once("x86/cpu: Intel User Mode Instruction Prevention (UMIP) activated\n");
+	pr_info_once("x86/cpu: User Mode Instruction Prevention (UMIP) activated\n");
 
 	return;
 

commit 0abbbc63d0251a25d3df6c8a56ab3d3c20af082c
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Nov 27 21:54:18 2018 +0100

    x86/umip: Print UMIP line only once
    
    ... instead of issuing it per CPU and flooding dmesg unnecessarily.
    Streamline the formulation, while at it.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20181127205936.30331-1-bp@alien8.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ffb181f959d2..2c56b8066280 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -353,7 +353,7 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 
 	cr4_set_bits(X86_CR4_UMIP);
 
-	pr_info("x86/cpu: Activated the Intel User Mode Instruction Prevention (UMIP) CPU feature\n");
+	pr_info_once("x86/cpu: Intel User Mode Instruction Prevention (UMIP) activated\n");
 
 	return;
 

commit 23a12ddee1ce28065b71f14ccc695b5a0c8a64ff
Merge: 98f76206b335 bcb6fb5da77c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Nov 3 23:42:16 2018 +0100

    Merge branch 'core/urgent' into x86/urgent, to pick up objtool fix
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 660d0b22e962..cbbd57ae06ee 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1,7 +1,7 @@
 /* cpu_feature_enabled() cannot be used this early */
 #define USE_EARLY_PGTABLE_L5
 
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/linkage.h>
 #include <linux/bitops.h>
 #include <linux/kernel.h>

commit 0e96f31ea4249b1e94e266fe4dff908c2983a9b3
Author: Jordan Borgner <mail@jordan-borgner.de>
Date:   Sun Oct 28 12:58:28 2018 +0000

    x86: Clean up 'sizeof x' => 'sizeof(x)'
    
    "sizeof(x)" is the canonical coding style used in arch/x86 most of the time.
    Fix the few places that didn't follow the convention.
    
    (Also do some whitespace cleanups in a few places while at it.)
    
    [ mingo: Rewrote the changelog. ]
    
    Signed-off-by: Jordan Borgner <mail@jordan-borgner.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20181028125828.7rgammkgzep2wpam@JordanDesktop
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 660d0b22e962..3a4eb2b25d71 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1074,7 +1074,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 #endif
 	c->x86_cache_alignment = c->x86_clflush_size;
 
-	memset(&c->x86_capability, 0, sizeof c->x86_capability);
+	memset(&c->x86_capability, 0, sizeof(c->x86_capability));
 	c->extended_cpuid_level = 0;
 
 	if (!have_cpuid_p())
@@ -1317,7 +1317,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	c->x86_virt_bits = 32;
 #endif
 	c->x86_cache_alignment = c->x86_clflush_size;
-	memset(&c->x86_capability, 0, sizeof c->x86_capability);
+	memset(&c->x86_capability, 0, sizeof(c->x86_capability));
 
 	generic_identify(c);
 

commit d82924c3b8d0607094b94fab290a33c5ad7d586c
Merge: d7197a5ad852 bb4b3b776273
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 18:43:04 2018 +0100

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 pti updates from Ingo Molnar:
     "The main changes:
    
       - Make the IBPB barrier more strict and add STIBP support (Jiri
         Kosina)
    
       - Micro-optimize and clean up the entry code (Andy Lutomirski)
    
       - ... plus misc other fixes"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/speculation: Propagate information about RSB filling mitigation to sysfs
      x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
      x86/speculation: Apply IBPB more strictly to avoid cross-process data leak
      x86/speculation: Add RETPOLINE_AMD support to the inline asm CALL_NOSPEC variant
      x86/CPU: Fix unused variable warning when !CONFIG_IA32_EMULATION
      x86/pti/64: Remove the SYSCALL64 entry trampoline
      x86/entry/64: Use the TSS sp2 slot for SYSCALL/SYSRET scratch space
      x86/entry/64: Document idtentry

commit f682a7920baf7b721d01dd317f3b532265357cbb
Merge: 99792e0cea1e 3a025de64bf8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:54:58 2018 +0100

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt updates from Ingo Molnar:
     "Two main changes:
    
       - Remove no longer used parts of the paravirt infrastructure and put
         large quantities of paravirt ops under a new config option
         PARAVIRT_XXL=y, which is selected by XEN_PV only. (Joergen Gross)
    
       - Enable PV spinlocks on Hyperv (Yi Sun)"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/hyperv: Enable PV qspinlock for Hyper-V
      x86/hyperv: Add GUEST_IDLE_MSR support
      x86/paravirt: Clean up native_patch()
      x86/paravirt: Prevent redefinition of SAVE_FLAGS macro
      x86/xen: Make xen_reservation_lock static
      x86/paravirt: Remove unneeded mmu related paravirt ops bits
      x86/paravirt: Move the Xen-only pv_mmu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the pv_irq_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move items in pv_info under PARAVIRT_XXL umbrella
      x86/paravirt: Introduce new config option PARAVIRT_XXL
      x86/paravirt: Remove unused paravirt bits
      x86/paravirt: Use a single ops structure
      x86/paravirt: Remove clobbers from struct paravirt_patch_site
      x86/paravirt: Remove clobbers parameter from paravirt patch functions
      x86/paravirt: Make paravirt_patch_call() and paravirt_patch_jmp() static
      x86/xen: Add SPDX identifier in arch/x86/xen files
      x86/xen: Link platform-pci-unplug.o only if CONFIG_XEN_PVHVM
      x86/xen: Move pv specific parts of arch/x86/xen/mmu.c to mmu_pv.c
      x86/xen: Move pv irq related functions under CONFIG_XEN_PV umbrella

commit fec98069fb72fb656304a3e52265e0c2fc9adf87
Merge: 04ce7fae3d4e 995d5f64b62f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 16:16:40 2018 +0100

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Add support for the "Dhyana" x86 CPUs by Hygon: these are licensed
         based on the AMD Zen architecture, and are built and sold in China,
         for domestic datacenter use. The code is pretty close to AMD
         support, mostly with a few quirks and enumeration differences. (Pu
         Wen)
    
       - Enable CPUID support on Cyrix 6x86/6x86L processors"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      tools/cpupower: Add Hygon Dhyana support
      cpufreq: Add Hygon Dhyana support
      ACPI: Add Hygon Dhyana support
      x86/xen: Add Hygon Dhyana support to Xen
      x86/kvm: Add Hygon Dhyana support to KVM
      x86/mce: Add Hygon Dhyana support to the MCA infrastructure
      x86/bugs: Add Hygon Dhyana to the respective mitigation machinery
      x86/apic: Add Hygon Dhyana support
      x86/pci, x86/amd_nb: Add Hygon Dhyana support to PCI and northbridge
      x86/amd_nb: Check vendor in AMD-only functions
      x86/alternative: Init ideal_nops for Hygon Dhyana
      x86/events: Add Hygon Dhyana support to PMU infrastructure
      x86/smpboot: Do not use BSP INIT delay and MWAIT to idle on Dhyana
      x86/cpu/mtrr: Support TOP_MEM2 and get MTRR number
      x86/cpu: Get cache info and setup cache cpumap for Hygon Dhyana
      x86/cpu: Create Hygon Dhyana architecture support file
      x86/CPU: Change query logic so CPUID is enabled before testing
      x86/CPU: Use correct macros for Cyrix calls

commit e1d20beae70eb918cca7f07a77ce199fd148fdd2
Merge: cbbfb0ae2ca9 ec3a94188df7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 15:24:22 2018 +0100

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were the fsgsbase related preparatory
      patches from Chang S. Bae - but there's also an optimized
      memcpy_flushcache() and a cleanup for the __cmpxchg_double() assembly
      glue"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fsgsbase/64: Clean up various details
      x86/segments: Introduce the 'CPUNODE' naming to better document the segment limit CPU/node NR trick
      x86/vdso: Initialize the CPU/node NR segment descriptor earlier
      x86/vdso: Introduce helper functions for CPU and node number
      x86/segments/64: Rename the GDT PER_CPU entry to CPU_NUMBER
      x86/fsgsbase/64: Factor out FS/GS segment loading from __switch_to()
      x86/fsgsbase/64: Convert the ELF core dump code to the new FSGSBASE helpers
      x86/fsgsbase/64: Make ptrace use the new FS/GS base helpers
      x86/fsgsbase/64: Introduce FS/GS base helper functions
      x86/fsgsbase/64: Fix ptrace() to read the FS/GS base accurately
      x86/asm: Use CC_SET()/CC_OUT() in __cmpxchg_double()
      x86/asm: Optimize memcpy_flushcache()

commit 22245bdf0ad805d6c29f82b6d5e977ee94bb2166
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Oct 8 10:41:59 2018 +0200

    x86/segments: Introduce the 'CPUNODE' naming to better document the segment limit CPU/node NR trick
    
    We have a special segment descriptor entry in the GDT, whose sole purpose is to
    encode the CPU and node numbers in its limit (size) field. There are user-space
    instructions that allow the reading of the limit field, which gives us a really
    fast way to read the CPU and node IDs from the vDSO for example.
    
    But the naming of related functionality does not make this clear, at all:
    
            VDSO_CPU_SIZE
            VDSO_CPU_MASK
            __CPU_NUMBER_SEG
            GDT_ENTRY_CPU_NUMBER
            vdso_encode_cpu_node
            vdso_read_cpu_node
    
    There's a number of problems:
    
     - The 'VDSO_CPU_SIZE' doesn't really make it clear that these are number
       of bits, nor does it make it clear which 'CPU' this refers to, i.e.
       that this is about a GDT entry whose limit encodes the CPU and node number.
    
     - Furthermore, the 'CPU_NUMBER' naming is actively misleading as well,
       because the segment limit encodes not just the CPU number but the
       node ID as well ...
    
    So use a better nomenclature all around: name everything related to this trick
    as 'CPUNODE', to make it clear that this is something special, and add
    _BITS to make it clear that these are number of bits, and propagate this to
    every affected name:
    
            VDSO_CPU_SIZE         =>  VDSO_CPUNODE_BITS
            VDSO_CPU_MASK         =>  VDSO_CPUNODE_MASK
            __CPU_NUMBER_SEG      =>  __CPUNODE_SEG
            GDT_ENTRY_CPU_NUMBER  =>  GDT_ENTRY_CPUNODE
            vdso_encode_cpu_node  =>  vdso_encode_cpunode
            vdso_read_cpu_node    =>  vdso_read_cpunode
    
    This, beyond being less confusing, also makes it easier to grep for all related
    functionality:
    
      $ git grep -i cpunode arch/x86
    
    Also, while at it, fix "return is not a function" style sloppiness in vdso_encode_cpunode().
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chang S. Bae <chang.seok.bae@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1537312139-5580-2-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a148d18a1ef0..7da587f4af52 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1672,7 +1672,7 @@ static void wait_for_master_cpu(int cpu)
 #ifdef CONFIG_X86_64
 static void setup_getcpu(int cpu)
 {
-	unsigned long cpudata = vdso_encode_cpu_node(cpu, early_cpu_to_node(cpu));
+	unsigned long cpudata = vdso_encode_cpunode(cpu, early_cpu_to_node(cpu));
 	struct desc_struct d = { };
 
 	if (static_cpu_has(X86_FEATURE_RDTSCP))
@@ -1688,7 +1688,7 @@ static void setup_getcpu(int cpu)
 	d.p = 1;		/* Present */
 	d.d = 1;		/* 32-bit */
 
-	write_gdt_entry(get_cpu_gdt_rw(cpu), GDT_ENTRY_CPU_NUMBER, &d, DESCTYPE_S);
+	write_gdt_entry(get_cpu_gdt_rw(cpu), GDT_ENTRY_CPUNODE, &d, DESCTYPE_S);
 }
 #endif
 

commit b2e2ba578e016a091eb31565849990fe68c7c599
Author: Chang S. Bae <chang.seok.bae@intel.com>
Date:   Tue Sep 18 16:08:59 2018 -0700

    x86/vdso: Initialize the CPU/node NR segment descriptor earlier
    
    Currently the CPU/node NR segment descriptor (GDT_ENTRY_CPU_NUMBER) is
    initialized relatively late during CPU init, from the vCPU code, which
    has a number of disadvantages, such as hotplug CPU notifiers and SMP
    cross-calls.
    
    Instead just initialize it much earlier, directly in cpu_init().
    
    This reduces complexity and increases robustness.
    
    [ mingo: Wrote new changelog. ]
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Chang S. Bae <chang.seok.bae@intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Markus T Metzger <markus.t.metzger@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1537312139-5580-9-git-send-email-chang.seok.bae@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 44c4ef3d989b..a148d18a1ef0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1669,6 +1669,29 @@ static void wait_for_master_cpu(int cpu)
 #endif
 }
 
+#ifdef CONFIG_X86_64
+static void setup_getcpu(int cpu)
+{
+	unsigned long cpudata = vdso_encode_cpu_node(cpu, early_cpu_to_node(cpu));
+	struct desc_struct d = { };
+
+	if (static_cpu_has(X86_FEATURE_RDTSCP))
+		write_rdtscp_aux(cpudata);
+
+	/* Store CPU and node number in limit. */
+	d.limit0 = cpudata;
+	d.limit1 = cpudata >> 16;
+
+	d.type = 5;		/* RO data, expand down, accessed */
+	d.dpl = 3;		/* Visible to user code */
+	d.s = 1;		/* Not a system segment */
+	d.p = 1;		/* Present */
+	d.d = 1;		/* 32-bit */
+
+	write_gdt_entry(get_cpu_gdt_rw(cpu), GDT_ENTRY_CPU_NUMBER, &d, DESCTYPE_S);
+}
+#endif
+
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT
@@ -1706,6 +1729,7 @@ void cpu_init(void)
 	    early_cpu_to_node(cpu) != NUMA_NO_NODE)
 		set_numa_node(early_cpu_to_node(cpu));
 #endif
+	setup_getcpu(cpu);
 
 	me = current;
 

commit f2c4db1bd80720cd8cb2a5aa220d9bc9f374f04e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 7 10:17:27 2018 -0700

    x86/cpu: Sanitize FAM6_ATOM naming
    
    Going primarily by:
    
      https://en.wikipedia.org/wiki/List_of_Intel_Atom_microprocessors
    
    with additional information gleaned from other related pages; notably:
    
     - Bonnell shrink was called Saltwell
     - Moorefield is the Merriefield refresh which makes it Airmont
    
    The general naming scheme is: FAM6_ATOM_UARCH_SOCTYPE
    
      for i in `git grep -l FAM6_ATOM` ; do
            sed -i  -e 's/ATOM_PINEVIEW/ATOM_BONNELL/g'             \
                    -e 's/ATOM_LINCROFT/ATOM_BONNELL_MID/'          \
                    -e 's/ATOM_PENWELL/ATOM_SALTWELL_MID/g'         \
                    -e 's/ATOM_CLOVERVIEW/ATOM_SALTWELL_TABLET/g'   \
                    -e 's/ATOM_CEDARVIEW/ATOM_SALTWELL/g'           \
                    -e 's/ATOM_SILVERMONT1/ATOM_SILVERMONT/g'       \
                    -e 's/ATOM_SILVERMONT2/ATOM_SILVERMONT_X/g'     \
                    -e 's/ATOM_MERRIFIELD/ATOM_SILVERMONT_MID/g'    \
                    -e 's/ATOM_MOOREFIELD/ATOM_AIRMONT_MID/g'       \
                    -e 's/ATOM_DENVERTON/ATOM_GOLDMONT_X/g'         \
                    -e 's/ATOM_GEMINI_LAKE/ATOM_GOLDMONT_PLUS/g' ${i}
      done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: dave.hansen@linux.intel.com
    Cc: len.brown@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 44c4ef3d989b..10e5ccfa9278 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -949,11 +949,11 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 }
 
 static const __initconst struct x86_cpu_id cpu_no_speculation[] = {
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CEDARVIEW,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CLOVERVIEW,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_LINCROFT,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PENWELL,	X86_FEATURE_ANY },
-	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PINEVIEW,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_SALTWELL,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_SALTWELL_TABLET,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_BONNELL_MID,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_SALTWELL_MID,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_BONNELL,	X86_FEATURE_ANY },
 	{ X86_VENDOR_CENTAUR,	5 },
 	{ X86_VENDOR_INTEL,	5 },
 	{ X86_VENDOR_NSC,	5 },
@@ -968,10 +968,10 @@ static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
 
 /* Only list CPUs which speculate but are non susceptible to SSB */
 static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT	},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_X	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_MID	},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_CORE_YONAH		},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
@@ -984,14 +984,14 @@ static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
 
 static const __initconst struct x86_cpu_id cpu_no_l1tf[] = {
 	/* in addition to cpu_no_speculation */
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_X	},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MOOREFIELD	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT_MID	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT_MID	},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_DENVERTON	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GEMINI_LAKE	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT_X	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT_PLUS	},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
 	{}

commit 1a576b23d63794f39a247fb31056eecccbf9a287
Author: Pu Wen <puwen@hygon.cn>
Date:   Sun Sep 23 17:35:50 2018 +0800

    x86/bugs: Add Hygon Dhyana to the respective mitigation machinery
    
    The Hygon Dhyana CPU has the same speculative execution as AMD family
    17h, so share AMD spectre mitigation code with Hygon Dhyana.
    
    Also Hygon Dhyana is not affected by meltdown, so add exception for it.
    
    Signed-off-by: Pu Wen <puwen@hygon.cn>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: tglx@linutronix.de
    Cc: mingo@redhat.com
    Cc: hpa@zytor.com
    Cc: x86@kernel.org
    Cc: thomas.lendacky@amd.com
    Link: https://lkml.kernel.org/r/0861d39c8a103fc0deca15bafbc85d403666d9ef.1537533369.git.puwen@hygon.cn

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 658c85d16a9b..d14c879ba7ba 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -963,6 +963,7 @@ static const __initconst struct x86_cpu_id cpu_no_speculation[] = {
 
 static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
 	{ X86_VENDOR_AMD },
+	{ X86_VENDOR_HYGON },
 	{}
 };
 

commit 2893cc8ff892fa74972d8dc0e1d0dc65116daaa3
Author: Matthew Whitehead <tedheadster@gmail.com>
Date:   Fri Sep 21 17:20:41 2018 -0400

    x86/CPU: Change query logic so CPUID is enabled before testing
    
    Presently we check first if CPUID is enabled. If it is not already
    enabled, then we next call identify_cpu_without_cpuid() and clear
    X86_FEATURE_CPUID.
    
    Unfortunately, identify_cpu_without_cpuid() is the function where CPUID
    becomes _enabled_ on Cyrix 6x86/6x86L CPUs.
    
    Reverse the calling sequence so that CPUID is first enabled, and then
    check a second time to see if the feature has now been activated.
    
    [ bp: Massage commit message and remove trailing whitespace. ]
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Matthew Whitehead <tedheadster@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Andy Lutomirski <luto@amacapital.net>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180921212041.13096-3-tedheadster@gmail.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 44c4ef3d989b..658c85d16a9b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1076,6 +1076,9 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
 	c->extended_cpuid_level = 0;
 
+	if (!have_cpuid_p())
+		identify_cpu_without_cpuid(c);
+
 	/* cyrix could have cpuid enabled via c_identify()*/
 	if (have_cpuid_p()) {
 		cpu_detect(c);
@@ -1093,7 +1096,6 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		if (this_cpu->c_bsp_init)
 			this_cpu->c_bsp_init(c);
 	} else {
-		identify_cpu_without_cpuid(c);
 		setup_clear_cpu_cap(X86_FEATURE_CPUID);
 	}
 

commit 8e6b65a1b6cd1711d3acd2aa5c60d38c3e15dabb
Author: zhong jiang <zhongjiang@huawei.com>
Date:   Thu Sep 13 10:49:45 2018 +0800

    x86/CPU: Fix unused variable warning when !CONFIG_IA32_EMULATION
    
    Get rid of local @cpu variable which is unused in the
    !CONFIG_IA32_EMULATION case.
    
    Signed-off-by: zhong jiang <zhongjiang@huawei.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: x86-ml <x86@kernel.org>
    Link: http://lkml.kernel.org/r/1536806985-24197-1-git-send-email-zhongjiang@huawei.com
    [ Clean up commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2d5b1fa5f9c6..8bffeae9bac2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1531,8 +1531,6 @@ EXPORT_PER_CPU_SYMBOL(__preempt_count);
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
-	int __maybe_unused cpu = smp_processor_id();
-
 	wrmsr(MSR_STAR, 0, (__USER32_CS << 16) | __KERNEL_CS);
 	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 
@@ -1545,7 +1543,8 @@ void syscall_init(void)
 	 * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit).
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
-	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_entry_stack(cpu) + 1));
+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP,
+		    (unsigned long)(cpu_entry_stack(smp_processor_id()) + 1));
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
 	wrmsrl(MSR_CSTAR, (unsigned long)ignore_sysret);

commit bf904d2762ee6fc1e4acfcb0772bbfb4a27ad8a6
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Sep 3 15:59:44 2018 -0700

    x86/pti/64: Remove the SYSCALL64 entry trampoline
    
    The SYSCALL64 trampoline has a couple of nice properties:
    
     - The usual sequence of SWAPGS followed by two GS-relative accesses to
       set up RSP is somewhat slow because the GS-relative accesses need
       to wait for SWAPGS to finish.  The trampoline approach allows
       RIP-relative accesses to set up RSP, which avoids the stall.
    
     - The trampoline avoids any percpu access before CR3 is set up,
       which means that no percpu memory needs to be mapped in the user
       page tables.  This prevents using Meltdown to read any percpu memory
       outside the cpu_entry_area and prevents using timing leaks
       to directly locate the percpu areas.
    
    The downsides of using a trampoline may outweigh the upsides, however.
    It adds an extra non-contiguous I$ cache line to system calls, and it
    forces an indirect jump to transfer control back to the normal kernel
    text after CR3 is set up.  The latter is because x86 lacks a 64-bit
    direct jump instruction that could jump from the trampoline to the entry
    text.  With retpolines enabled, the indirect jump is extremely slow.
    
    Change the code to map the percpu TSS into the user page tables to allow
    the non-trampoline SYSCALL64 path to work under PTI.  This does not add a
    new direct information leak, since the TSS is readable by Meltdown from the
    cpu_entry_area alias regardless.  It does allow a timing attack to locate
    the percpu area, but KASLR is more or less a lost cause against local
    attack on CPUs vulnerable to Meltdown regardless.  As far as I'm concerned,
    on current hardware, KASLR is only useful to mitigate remote attacks that
    try to attack the kernel without first gaining RCE against a vulnerable
    user process.
    
    On Skylake, with CONFIG_RETPOLINE=y and KPTI on, this reduces syscall
    overhead from ~237ns to ~228ns.
    
    There is a possible alternative approach: Move the trampoline within 2G of
    the entry text and make a separate copy for each CPU.  This would allow a
    direct jump to rejoin the normal entry path. There are pro's and con's for
    this approach:
    
     + It avoids a pipeline stall
    
     - It executes from an extra page and read from another extra page during
       the syscall. The latter is because it needs to use a relative
       addressing mode to find sp1 -- it's the same *cacheline*, but accessed
       using an alias, so it's an extra TLB entry.
    
     - Slightly more memory. This would be one page per CPU for a simple
       implementation and 64-ish bytes per CPU or one page per node for a more
       complex implementation.
    
     - More code complexity.
    
    The current approach is chosen for simplicity and because the alternative
    does not provide a significant benefit, which makes it worth.
    
    [ tglx: Added the alternative discussion to the changelog ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/8c7c6e483612c3e4e10ca89495dc160b1aa66878.1536015544.git.luto@kernel.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 44c4ef3d989b..2d5b1fa5f9c6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1531,19 +1531,10 @@ EXPORT_PER_CPU_SYMBOL(__preempt_count);
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
-	extern char _entry_trampoline[];
-	extern char entry_SYSCALL_64_trampoline[];
-
-	int cpu = smp_processor_id();
-	unsigned long SYSCALL64_entry_trampoline =
-		(unsigned long)get_cpu_entry_area(cpu)->entry_trampoline +
-		(entry_SYSCALL_64_trampoline - _entry_trampoline);
+	int __maybe_unused cpu = smp_processor_id();
 
 	wrmsr(MSR_STAR, 0, (__USER32_CS << 16) | __KERNEL_CS);
-	if (static_cpu_has(X86_FEATURE_PTI))
-		wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);
-	else
-		wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
+	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, (unsigned long)entry_SYSCALL_compat);

commit 9bad5658ea710f45e4ee68b88a01cfe1839d8b00
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:23 2018 +0200

    x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
    
    Most of the paravirt ops defined in pv_cpu_ops are for Xen PV guests
    only. Define them only if CONFIG_PARAVIRT_XXL is set.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-13-jgross@suse.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c4da9c23a96c..d45cd5e0fb11 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1240,7 +1240,7 @@ static void generic_identify(struct cpuinfo_x86 *c)
 	 * ESPFIX issue, we can change this.
 	 */
 #ifdef CONFIG_X86_32
-# ifdef CONFIG_PARAVIRT
+# ifdef CONFIG_PARAVIRT_XXL
 	do {
 		extern void native_iret(void);
 		if (pv_ops.cpu.iret == native_iret)

commit 5c83511bdb9832c86be20fb86b783356e2f58062
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Aug 28 09:40:19 2018 +0200

    x86/paravirt: Use a single ops structure
    
    Instead of using six globally visible paravirt ops structures combine
    them in a single structure, keeping the original structures as
    sub-structures.
    
    This avoids the need to assemble struct paravirt_patch_template at
    runtime on the stack each time apply_paravirt() is being called (i.e.
    when loading a module).
    
    [ tglx: Made the struct and the initializer tabular for readability sake ]
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: xen-devel@lists.xenproject.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: akataria@vmware.com
    Cc: rusty@rustcorp.com.au
    Cc: boris.ostrovsky@oracle.com
    Cc: hpa@zytor.com
    Link: https://lkml.kernel.org/r/20180828074026.820-9-jgross@suse.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 44c4ef3d989b..c4da9c23a96c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1243,7 +1243,7 @@ static void generic_identify(struct cpuinfo_x86 *c)
 # ifdef CONFIG_PARAVIRT
 	do {
 		extern void native_iret(void);
-		if (pv_cpu_ops.iret == native_iret)
+		if (pv_ops.cpu.iret == native_iret)
 			set_cpu_bug(c, X86_BUG_ESPFIX);
 	} while (0);
 # else

commit cc51e5428ea54f575d49cfcede1d4cb3a72b4ec4
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Aug 24 10:03:50 2018 -0700

    x86/speculation/l1tf: Increase l1tf memory limit for Nehalem+
    
    On Nehalem and newer core CPUs the CPU cache internally uses 44 bits
    physical address space. The L1TF workaround is limited by this internal
    cache address width, and needs to have one bit free there for the
    mitigation to work.
    
    Older client systems report only 36bit physical address space so the range
    check decides that L1TF is not mitigated for a 36bit phys/32GB system with
    some memory holes.
    
    But since these actually have the larger internal cache width this warning
    is bogus because it would only really be needed if the system had more than
    43bits of memory.
    
    Add a new internal x86_cache_bits field. Normally it is the same as the
    physical bits field reported by CPUID, but for Nehalem and newerforce it to
    be at least 44bits.
    
    Change the L1TF memory size warning to use the new cache_bits field to
    avoid bogus warnings and remove the bogus comment about memory size.
    
    Fixes: 17dbca119312 ("x86/speculation/l1tf: Add sysfs reporting for l1tf")
    Reported-by: George Anchev <studio@anchev.net>
    Reported-by: Christopher Snowhill <kode54@gmail.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Michael Hocko <mhocko@suse.com>
    Cc: vbabka@suse.cz
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180824170351.34874-1-andi@firstfloor.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 84dee5ab745a..44c4ef3d989b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -919,6 +919,7 @@ void get_cpu_address_sizes(struct cpuinfo_x86 *c)
 	else if (cpu_has(c, X86_FEATURE_PAE) || cpu_has(c, X86_FEATURE_PSE36))
 		c->x86_phys_bits = 36;
 #endif
+	c->x86_cache_bits = c->x86_phys_bits;
 }
 
 static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)

commit 31130a16d459de809cd1c03eabc9567d094aae6a
Merge: 1202f4fdbcb6 3596924a233e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 16:54:22 2018 -0700

    Merge tag 'for-linus-4.19-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from Juergen Gross:
    
     - add dma-buf functionality to Xen grant table handling
    
     - fix for booting the kernel as Xen PVH dom0
    
     - fix for booting the kernel as a Xen PV guest with
       CONFIG_DEBUG_VIRTUAL enabled
    
     - other minor performance and style fixes
    
    * tag 'for-linus-4.19-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/balloon: fix balloon initialization for PVH Dom0
      xen: don't use privcmd_call() from xen_mc_flush()
      xen/pv: Call get_cpu_address_sizes to set x86_virt/phys_bits
      xen/biomerge: Use true and false for boolean values
      xen/gntdev: don't dereference a null gntdev_dmabuf on allocation failure
      xen/spinlock: Don't use pvqspinlock if only 1 vCPU
      xen/gntdev: Implement dma-buf import functionality
      xen/gntdev: Implement dma-buf export functionality
      xen/gntdev: Add initial support for dma-buf UAPI
      xen/gntdev: Make private routines/structures accessible
      xen/gntdev: Allow mappings for DMA buffers
      xen/grant-table: Allow allocating buffers suitable for DMA
      xen/balloon: Share common memory reservation routines
      xen/grant-table: Make set/clear page private code shared

commit 958f338e96f874a0d29442396d6adf9c1e17aa2d
Merge: 781fca5b1046 07d981ad4cf1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 09:46:06 2018 -0700

    Merge branch 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Merge L1 Terminal Fault fixes from Thomas Gleixner:
     "L1TF, aka L1 Terminal Fault, is yet another speculative hardware
      engineering trainwreck. It's a hardware vulnerability which allows
      unprivileged speculative access to data which is available in the
      Level 1 Data Cache when the page table entry controlling the virtual
      address, which is used for the access, has the Present bit cleared or
      other reserved bits set.
    
      If an instruction accesses a virtual address for which the relevant
      page table entry (PTE) has the Present bit cleared or other reserved
      bits set, then speculative execution ignores the invalid PTE and loads
      the referenced data if it is present in the Level 1 Data Cache, as if
      the page referenced by the address bits in the PTE was still present
      and accessible.
    
      While this is a purely speculative mechanism and the instruction will
      raise a page fault when it is retired eventually, the pure act of
      loading the data and making it available to other speculative
      instructions opens up the opportunity for side channel attacks to
      unprivileged malicious code, similar to the Meltdown attack.
    
      While Meltdown breaks the user space to kernel space protection, L1TF
      allows to attack any physical memory address in the system and the
      attack works across all protection domains. It allows an attack of SGX
      and also works from inside virtual machines because the speculation
      bypasses the extended page table (EPT) protection mechanism.
    
      The assoicated CVEs are: CVE-2018-3615, CVE-2018-3620, CVE-2018-3646
    
      The mitigations provided by this pull request include:
    
       - Host side protection by inverting the upper address bits of a non
         present page table entry so the entry points to uncacheable memory.
    
       - Hypervisor protection by flushing L1 Data Cache on VMENTER.
    
       - SMT (HyperThreading) control knobs, which allow to 'turn off' SMT
         by offlining the sibling CPU threads. The knobs are available on
         the kernel command line and at runtime via sysfs
    
       - Control knobs for the hypervisor mitigation, related to L1D flush
         and SMT control. The knobs are available on the kernel command line
         and at runtime via sysfs
    
       - Extensive documentation about L1TF including various degrees of
         mitigations.
    
      Thanks to all people who have contributed to this in various ways -
      patches, review, testing, backporting - and the fruitful, sometimes
      heated, but at the end constructive discussions.
    
      There is work in progress to provide other forms of mitigations, which
      might be less horrible performance wise for a particular kind of
      workloads, but this is not yet ready for consumption due to their
      complexity and limitations"
    
    * 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (75 commits)
      x86/microcode: Allow late microcode loading with SMT disabled
      tools headers: Synchronise x86 cpufeatures.h for L1TF additions
      x86/mm/kmmio: Make the tracer robust against L1TF
      x86/mm/pat: Make set_memory_np() L1TF safe
      x86/speculation/l1tf: Make pmd/pud_mknotpresent() invert
      x86/speculation/l1tf: Invert all not present mappings
      cpu/hotplug: Fix SMT supported evaluation
      KVM: VMX: Tell the nested hypervisor to skip L1D flush on vmentry
      x86/speculation: Use ARCH_CAPABILITIES to skip L1D flush on vmentry
      x86/speculation: Simplify sysfs report of VMX L1TF vulnerability
      Documentation/l1tf: Remove Yonah processors from not vulnerable list
      x86/KVM/VMX: Don't set l1tf_flush_l1d from vmx_handle_external_intr()
      x86/irq: Let interrupt handlers set kvm_cpu_l1tf_flush_l1d
      x86: Don't include linux/irq.h from asm/hardirq.h
      x86/KVM/VMX: Introduce per-host-cpu analogue of l1tf_flush_l1d
      x86/irq: Demote irq_cpustat_t::__softirq_pending to u16
      x86/KVM/VMX: Move the l1tf_flush_l1d test to vmx_l1d_flush()
      x86/KVM/VMX: Replace 'vmx_l1d_flush_always' with 'vmx_l1d_flush_cond'
      x86/KVM/VMX: Don't set l1tf_flush_l1d to true from vmx_l1d_flush()
      cpu/hotplug: detect SMT disabled by BIOS
      ...

commit 13e091b6dd0e78a518a7d8756607d3acb8215768
Merge: eac341194426 1088c6eef261
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 18:28:19 2018 -0700

    Merge branch 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 timer updates from Thomas Gleixner:
     "Early TSC based time stamping to allow better boot time analysis.
    
      This comes with a general cleanup of the TSC calibration code which
      grew warts and duct taping over the years and removes 250 lines of
      code. Initiated and mostly implemented by Pavel with help from various
      folks"
    
    * 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      x86/kvmclock: Mark kvm_get_preset_lpj() as __init
      x86/tsc: Consolidate init code
      sched/clock: Disable interrupts when calling generic_sched_clock_init()
      timekeeping: Prevent false warning when persistent clock is not available
      sched/clock: Close a hole in sched_clock_init()
      x86/tsc: Make use of tsc_calibrate_cpu_early()
      x86/tsc: Split native_calibrate_cpu() into early and late parts
      sched/clock: Use static key for sched_clock_running
      sched/clock: Enable sched clock early
      sched/clock: Move sched clock initialization and merge with generic clock
      x86/tsc: Use TSC as sched clock early
      x86/tsc: Initialize cyc2ns when tsc frequency is determined
      x86/tsc: Calibrate tsc only once
      ARM/time: Remove read_boot_clock64()
      s390/time: Remove read_boot_clock64()
      timekeeping: Default boot time offset to local_clock()
      timekeeping: Replace read_boot_clock64() with read_persistent_wall_and_boot_offset()
      s390/time: Add read_persistent_wall_and_boot_offset()
      x86/xen/time: Output xen sched_clock time from 0
      x86/xen/time: Initialize pv xen time in init_hypervisor_platform()
      ...

commit 405c018a25fe464dc68057bbc8014a58f2bd4422
Author: M. Vefa Bicakci <m.v.b@runbox.com>
Date:   Tue Jul 24 08:45:47 2018 -0400

    xen/pv: Call get_cpu_address_sizes to set x86_virt/phys_bits
    
    Commit d94a155c59c9 ("x86/cpu: Prevent cpuinfo_x86::x86_phys_bits
    adjustment corruption") has moved the query and calculation of the
    x86_virt_bits and x86_phys_bits fields of the cpuinfo_x86 struct
    from the get_cpu_cap function to a new function named
    get_cpu_address_sizes.
    
    One of the call sites related to Xen PV VMs was unfortunately missed
    in the aforementioned commit. This prevents successful boot-up of
    kernel versions 4.17 and up in Xen PV VMs if CONFIG_DEBUG_VIRTUAL
    is enabled, due to the following code path:
    
      enlighten_pv.c::xen_start_kernel
        mmu_pv.c::xen_reserve_special_pages
          page.h::__pa
            physaddr.c::__phys_addr
              physaddr.h::phys_addr_valid
    
    phys_addr_valid uses boot_cpu_data.x86_phys_bits to validate physical
    addresses. boot_cpu_data.x86_phys_bits is no longer populated before
    the call to xen_reserve_special_pages due to the aforementioned commit
    though, so the validation performed by phys_addr_valid fails, which
    causes __phys_addr to trigger a BUG, preventing boot-up.
    
    Signed-off-by: M. Vefa Bicakci <m.v.b@runbox.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: xen-devel@lists.xenproject.org
    Cc: x86@kernel.org
    Cc: stable@vger.kernel.org # for v4.17 and up
    Fixes: d94a155c59c9 ("x86/cpu: Prevent cpuinfo_x86::x86_phys_bits adjustment corruption")
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index eb4cb3efd20e..2322d0c4bfd2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -911,7 +911,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 	apply_forced_caps(c);
 }
 
-static void get_cpu_address_sizes(struct cpuinfo_x86 *c)
+void get_cpu_address_sizes(struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
 

commit f2701b77bbd992f3df4631de8493f21db0830452
Merge: 18b57ce2eb8c acb1872577b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Aug 5 16:39:29 2018 +0200

    Merge 4.18-rc7 into master to pick up the KVM dependcy
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 706d51681d636a0c4a5ef53395ec3b803e45ed4d
Author: Sai Praneeth <sai.praneeth.prakhya@intel.com>
Date:   Wed Aug 1 11:42:25 2018 -0700

    x86/speculation: Support Enhanced IBRS on future CPUs
    
    Future Intel processors will support "Enhanced IBRS" which is an "always
    on" mode i.e. IBRS bit in SPEC_CTRL MSR is enabled once and never
    disabled.
    
    From the specification [1]:
    
     "With enhanced IBRS, the predicted targets of indirect branches
      executed cannot be controlled by software that was executed in a less
      privileged predictor mode or on another logical processor. As a
      result, software operating on a processor with enhanced IBRS need not
      use WRMSR to set IA32_SPEC_CTRL.IBRS after every transition to a more
      privileged predictor mode. Software can isolate predictor modes
      effectively simply by setting the bit once. Software need not disable
      enhanced IBRS prior to entering a sleep state such as MWAIT or HLT."
    
    If Enhanced IBRS is supported by the processor then use it as the
    preferred spectre v2 mitigation mechanism instead of Retpoline. Intel's
    Retpoline white paper [2] states:
    
     "Retpoline is known to be an effective branch target injection (Spectre
      variant 2) mitigation on Intel processors belonging to family 6
      (enumerated by the CPUID instruction) that do not have support for
      enhanced IBRS. On processors that support enhanced IBRS, it should be
      used for mitigation instead of retpoline."
    
    The reason why Enhanced IBRS is the recommended mitigation on processors
    which support it is that these processors also support CET which
    provides a defense against ROP attacks. Retpoline is very similar to ROP
    techniques and might trigger false positives in the CET defense.
    
    If Enhanced IBRS is selected as the mitigation technique for spectre v2,
    the IBRS bit in SPEC_CTRL MSR is set once at boot time and never
    cleared. Kernel also has to make sure that IBRS bit remains set after
    VMEXIT because the guest might have cleared the bit. This is already
    covered by the existing x86_spec_ctrl_set_guest() and
    x86_spec_ctrl_restore_host() speculation control functions.
    
    Enhanced IBRS still requires IBPB for full mitigation.
    
    [1] Speculative-Execution-Side-Channel-Mitigations.pdf
    [2] Retpoline-A-Branch-Target-Injection-Mitigation.pdf
    Both documents are available at:
    https://bugzilla.kernel.org/show_bug.cgi?id=199511
    
    Originally-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim C Chen <tim.c.chen@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Link: https://lkml.kernel.org/r/1533148945-24095-1-git-send-email-sai.praneeth.prakhya@intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 43a927eb9c09..df28e931d732 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1005,6 +1005,9 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
+	if (ia32_cap & ARCH_CAP_IBRS_ALL)
+		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+
 	if (x86_match_cpu(cpu_no_meltdown))
 		return;
 

commit 45d7b255747c21fc4b1f5043bee0754d39c3bdbf
Author: Joerg Roedel <jroedel@suse.de>
Date:   Wed Jul 18 11:40:44 2018 +0200

    x86/entry/32: Enter the kernel via trampoline stack
    
    Use the entry-stack as a trampoline to enter the kernel. The entry-stack is
    already in the cpu_entry_area and will be mapped to userspace when PTI is
    enabled.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Pavel Machek <pavel@ucw.cz>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: linux-mm@kvack.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Waiman Long <llong@redhat.com>
    Cc: "David H . Gutteridge" <dhgutteridge@sympatico.ca>
    Cc: joro@8bytes.org
    Link: https://lkml.kernel.org/r/1531906876-13451-8-git-send-email-joro@8bytes.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index eb4cb3efd20e..43a927eb9c09 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1804,11 +1804,12 @@ void cpu_init(void)
 	enter_lazy_tlb(&init_mm, curr);
 
 	/*
-	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
-	 * task never enters user mode.
+	 * Initialize the TSS.  sp0 points to the entry trampoline stack
+	 * regardless of what task is running.
 	 */
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 	load_TR_desc();
+	load_sp0((unsigned long)(cpu_entry_stack(cpu) + 1));
 
 	load_mm_ldt(&init_mm);
 

commit 9b3661cd7e5400689ed168a7275e75af333177e6
Author: Borislav Petkov <bp@alien8.de>
Date:   Thu Jul 19 16:55:29 2018 -0400

    x86/CPU: Call detect_nopl() only on the BSP
    
    Make it use the setup_* variants and have it be called only on the BSP and
    drop the call in generic_identify() - X86_FEATURE_NOPL will be replicated
    to the APs through the forced caps. Helps to keep the mess at a manageable
    level.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-11-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 71281ac43b15..46408a8cdf62 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1024,12 +1024,12 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
  * unless we can find a reliable way to detect all the broken cases.
  * Enable it explicitly on 64-bit for non-constant inputs of cpu_has().
  */
-static void detect_nopl(struct cpuinfo_x86 *c)
+static void detect_nopl(void)
 {
 #ifdef CONFIG_X86_32
-	clear_cpu_cap(c, X86_FEATURE_NOPL);
+	setup_clear_cpu_cap(X86_FEATURE_NOPL);
 #else
-	set_cpu_cap(c, X86_FEATURE_NOPL);
+	setup_force_cpu_cap(X86_FEATURE_NOPL);
 #endif
 }
 
@@ -1108,7 +1108,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	if (!pgtable_l5_enabled())
 		setup_clear_cpu_cap(X86_FEATURE_LA57);
 
-	detect_nopl(c);
+	detect_nopl();
 }
 
 void __init early_cpu_init(void)
@@ -1206,8 +1206,6 @@ static void generic_identify(struct cpuinfo_x86 *c)
 
 	get_model_name(c); /* Default name */
 
-	detect_nopl(c);
-
 	detect_null_seg_behavior(c);
 
 	/*

commit 8990cac6e5ea7fa57607736019fe8dca961b998f
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:28 2018 -0400

    x86/jump_label: Initialize static branching early
    
    Static branching is useful to runtime patch branches that are used in hot
    path, but are infrequently changed.
    
    The x86 clock framework is one example that uses static branches to setup
    the best clock during boot and never changes it again.
    
    It is desired to enable the TSC based sched clock early to allow fine
    grained boot time analysis early on. That requires the static branching
    functionality to be functional early as well.
    
    Static branching requires patching nop instructions, thus,
    arch_init_ideal_nops() must be called prior to jump_label_init().
    
    Do all the necessary steps to call arch_init_ideal_nops() right after
    early_cpu_init(), which also allows to insert a call to jump_label_init()
    right after that. jump_label_init() will be called again from the generic
    init code, but the code is protected against reinitialization already.
    
    [ tglx: Massaged changelog ]
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-10-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index eb4cb3efd20e..71281ac43b15 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1015,6 +1015,24 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 }
 
+/*
+ * The NOPL instruction is supposed to exist on all CPUs of family >= 6;
+ * unfortunately, that's not true in practice because of early VIA
+ * chips and (more importantly) broken virtualizers that are not easy
+ * to detect. In the latter case it doesn't even *fail* reliably, so
+ * probing for it doesn't even work. Disable it completely on 32-bit
+ * unless we can find a reliable way to detect all the broken cases.
+ * Enable it explicitly on 64-bit for non-constant inputs of cpu_has().
+ */
+static void detect_nopl(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_32
+	clear_cpu_cap(c, X86_FEATURE_NOPL);
+#else
+	set_cpu_cap(c, X86_FEATURE_NOPL);
+#endif
+}
+
 /*
  * Do minimum CPU detection early.
  * Fields really needed: vendor, cpuid_level, family, model, mask,
@@ -1089,6 +1107,8 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	 */
 	if (!pgtable_l5_enabled())
 		setup_clear_cpu_cap(X86_FEATURE_LA57);
+
+	detect_nopl(c);
 }
 
 void __init early_cpu_init(void)
@@ -1124,24 +1144,6 @@ void __init early_cpu_init(void)
 	early_identify_cpu(&boot_cpu_data);
 }
 
-/*
- * The NOPL instruction is supposed to exist on all CPUs of family >= 6;
- * unfortunately, that's not true in practice because of early VIA
- * chips and (more importantly) broken virtualizers that are not easy
- * to detect. In the latter case it doesn't even *fail* reliably, so
- * probing for it doesn't even work. Disable it completely on 32-bit
- * unless we can find a reliable way to detect all the broken cases.
- * Enable it explicitly on 64-bit for non-constant inputs of cpu_has().
- */
-static void detect_nopl(struct cpuinfo_x86 *c)
-{
-#ifdef CONFIG_X86_32
-	clear_cpu_cap(c, X86_FEATURE_NOPL);
-#else
-	set_cpu_cap(c, X86_FEATURE_NOPL);
-#endif
-}
-
 static void detect_null_seg_behavior(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_64

commit 2458e53ff74cd1063ed3e00459da1d35c559d369
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Sat Jun 23 01:08:41 2018 +0300

    x86/mm: Fix 'no5lvl' handling
    
    early_identify_cpu() has to use early version of pgtable_l5_enabled()
    that doesn't rely on cpu_feature_enabled().
    
    Defining USE_EARLY_PGTABLE_L5 before all includes does the trick.
    
    I lost the define in one of reworks of the original patch.
    
    Fixes: 372fddf70904 ("x86/mm: Introduce the 'no5lvl' kernel parameter")
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/20180622220841.54135-3-kirill.shutemov@linux.intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0df7151cfef4..eb4cb3efd20e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1,3 +1,6 @@
+/* cpu_feature_enabled() cannot be used this early */
+#define USE_EARLY_PGTABLE_L5
+
 #include <linux/bootmem.h>
 #include <linux/linkage.h>
 #include <linux/bitops.h>

commit 545401f4448a807b963ff17b575e0a393e68b523
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 6 00:53:57 2018 +0200

    x86/cpu/common: Provide detect_ht_early()
    
    To support force disabling of SMT it's required to know the number of
    thread siblings early. detect_ht() cannot be called before the APIC driver
    is selected, so split out the part which initializes smp_num_siblings.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 81fbb267489e..0768492f4687 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -658,32 +658,36 @@ static void cpu_detect_tlb(struct cpuinfo_x86 *c)
 		tlb_lld_4m[ENTRIES], tlb_lld_1g[ENTRIES]);
 }
 
-void detect_ht(struct cpuinfo_x86 *c)
+int detect_ht_early(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
 	u32 eax, ebx, ecx, edx;
-	int index_msb, core_bits;
 
 	if (!cpu_has(c, X86_FEATURE_HT))
-		return;
+		return -1;
 
 	if (cpu_has(c, X86_FEATURE_CMP_LEGACY))
-		return;
+		return -1;
 
 	if (cpu_has(c, X86_FEATURE_XTOPOLOGY))
-		return;
+		return -1;
 
 	cpuid(1, &eax, &ebx, &ecx, &edx);
 
 	smp_num_siblings = (ebx & 0xff0000) >> 16;
+	if (smp_num_siblings == 1)
+		pr_info_once("CPU0: Hyper-Threading is disabled\n");
+#endif
+	return 0;
+}
 
-	if (!smp_num_siblings)
-		smp_num_siblings = 1;
+void detect_ht(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_SMP
+	int index_msb, core_bits;
 
-	if (smp_num_siblings == 1) {
-		pr_info_once("CPU0: Hyper-Threading is disabled\n");
+	if (detect_ht_early(c) < 0)
 		return;
-	}
 
 	index_msb = get_count_order(smp_num_siblings);
 	c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid, index_msb);

commit 55e6d279abd92cfd7576bba031e7589be8475edb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 6 00:36:15 2018 +0200

    x86/cpu: Remove the pointless CPU printout
    
    The value of this printout is dubious at best and there is no point in
    having it in two different places along with convoluted ways to reach it.
    
    Remove it completely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9baf684e2b55..81fbb267489e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -663,13 +663,12 @@ void detect_ht(struct cpuinfo_x86 *c)
 #ifdef CONFIG_SMP
 	u32 eax, ebx, ecx, edx;
 	int index_msb, core_bits;
-	static bool printed;
 
 	if (!cpu_has(c, X86_FEATURE_HT))
 		return;
 
 	if (cpu_has(c, X86_FEATURE_CMP_LEGACY))
-		goto out;
+		return;
 
 	if (cpu_has(c, X86_FEATURE_XTOPOLOGY))
 		return;
@@ -678,14 +677,14 @@ void detect_ht(struct cpuinfo_x86 *c)
 
 	smp_num_siblings = (ebx & 0xff0000) >> 16;
 
+	if (!smp_num_siblings)
+		smp_num_siblings = 1;
+
 	if (smp_num_siblings == 1) {
 		pr_info_once("CPU0: Hyper-Threading is disabled\n");
-		goto out;
+		return;
 	}
 
-	if (smp_num_siblings <= 1)
-		goto out;
-
 	index_msb = get_count_order(smp_num_siblings);
 	c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid, index_msb);
 
@@ -697,15 +696,6 @@ void detect_ht(struct cpuinfo_x86 *c)
 
 	c->cpu_core_id = apic->phys_pkg_id(c->initial_apicid, index_msb) &
 				       ((1 << core_bits) - 1);
-
-out:
-	if (!printed && (c->x86_max_cores * smp_num_siblings) > 1) {
-		pr_info("CPU: Physical Processor ID: %d\n",
-			c->phys_proc_id);
-		pr_info("CPU: Processor Core ID: %d\n",
-			c->cpu_core_id);
-		printed = 1;
-	}
 #endif
 }
 

commit 17dbca119312b4e8173d4e25ff64262119fcef38
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Jun 13 15:48:26 2018 -0700

    x86/speculation/l1tf: Add sysfs reporting for l1tf
    
    L1TF core kernel workarounds are cheap and normally always enabled, However
    they still should be reported in sysfs if the system is vulnerable or
    mitigated. Add the necessary CPU feature/bug bits.
    
    - Extend the existing checks for Meltdowns to determine if the system is
      vulnerable. All CPUs which are not vulnerable to Meltdown are also not
      vulnerable to L1TF
    
    - Check for 32bit non PAE and emit a warning as there is no practical way
      for mitigation due to the limited physical address bits
    
    - If the system has more than MAX_PA/2 physical memory the invert page
      workarounds don't protect the system against the L1TF attack anymore,
      because an inverted physical address will also point to valid
      memory. Print a warning in this case and report that the system is
      vulnerable.
    
    Add a function which returns the PFN limit for the L1TF mitigation, which
    will be used in follow up patches for sanity and range checks.
    
    [ tglx: Renamed the CPU feature bit to L1TF_PTEINV ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Dave Hansen <dave.hansen@intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0df7151cfef4..9baf684e2b55 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -984,6 +984,21 @@ static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
 	{}
 };
 
+static const __initconst struct x86_cpu_id cpu_no_l1tf[] = {
+	/* in addition to cpu_no_speculation */
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MOOREFIELD	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_DENVERTON	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GEMINI_LAKE	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
+	{}
+};
+
 static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = 0;
@@ -1010,6 +1025,11 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 		return;
 
 	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
+
+	if (x86_match_cpu(cpu_no_l1tf))
+		return;
+
+	setup_force_cpu_bug(X86_BUG_L1TF);
 }
 
 /*

commit 050e9baa9dc9fbd9ce2b27f0056990fc9e0a08a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 14 12:21:18 2018 +0900

    Kbuild: rename CC_STACKPROTECTOR[_STRONG] config variables
    
    The changes to automatically test for working stack protector compiler
    support in the Kconfig files removed the special STACKPROTECTOR_AUTO
    option that picked the strongest stack protector that the compiler
    supported.
    
    That was all a nice cleanup - it makes no sense to have the AUTO case
    now that the Kconfig phase can just determine the compiler support
    directly.
    
    HOWEVER.
    
    It also meant that doing "make oldconfig" would now _disable_ the strong
    stackprotector if you had AUTO enabled, because in a legacy config file,
    the sane stack protector configuration would look like
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_NONE is not set
      # CONFIG_CC_STACKPROTECTOR_REGULAR is not set
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_STACKPROTECTOR_AUTO=y
    
    and when you ran this through "make oldconfig" with the Kbuild changes,
    it would ask you about the regular CONFIG_CC_STACKPROTECTOR (that had
    been renamed from CONFIG_CC_STACKPROTECTOR_REGULAR to just
    CONFIG_CC_STACKPROTECTOR), but it would think that the STRONG version
    used to be disabled (because it was really enabled by AUTO), and would
    disable it in the new config, resulting in:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    That's dangerously subtle - people could suddenly find themselves with
    the weaker stack protector setup without even realizing.
    
    The solution here is to just rename not just the old RECULAR stack
    protector option, but also the strong one.  This does that by just
    removing the CC_ prefix entirely for the user choices, because it really
    is not about the compiler support (the compiler support now instead
    automatially impacts _visibility_ of the options to users).
    
    This results in "make oldconfig" actually asking the user for their
    choice, so that we don't have any silent subtle security model changes.
    The end result would generally look like this:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_STACKPROTECTOR=y
      CONFIG_STACKPROTECTOR_STRONG=y
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    where the "CC_" versions really are about internal compiler
    infrastructure, not the user selections.
    
    Acked-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 910b47ee8078..0df7151cfef4 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1599,7 +1599,7 @@ DEFINE_PER_CPU(unsigned long, cpu_current_top_of_stack) =
 	(unsigned long)&init_thread_union + THREAD_SIZE;
 EXPORT_PER_CPU_SYMBOL(cpu_current_top_of_stack);
 
-#ifdef CONFIG_CC_STACKPROTECTOR
+#ifdef CONFIG_STACKPROTECTOR
 DEFINE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif
 

commit 6ac2f49edb1ef5446089c7c660017732886d62d6
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Jun 1 10:59:20 2018 -0400

    x86/bugs: Add AMD's SPEC_CTRL MSR usage
    
    The AMD document outlining the SSBD handling
    124441_AMD64_SpeculativeStoreBypassDisable_Whitepaper_final.pdf
    mentions that if CPUID 8000_0008.EBX[24] is set we should be using
    the SPEC_CTRL MSR (0x48) over the VIRT SPEC_CTRL MSR (0xC001_011f)
    for speculative store bypass disable.
    
    This in effect means we should clear the X86_FEATURE_VIRT_SSBD
    flag so that we would prefer the SPEC_CTRL MSR.
    
    See the document titled:
       124441_AMD64_SpeculativeStoreBypassDisable_Whitepaper_final.pdf
    
    A copy of this document is available at
       https://bugzilla.kernel.org/show_bug.cgi?id=199889
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
    Cc: kvm@vger.kernel.org
    Cc: KarimAllah Ahmed <karahmed@amazon.de>
    Cc: andrew.cooper3@citrix.com
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Kees Cook <keescook@chromium.org>
    Link: https://lkml.kernel.org/r/20180601145921.9500-3-konrad.wilk@oracle.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8e3f062f462d..910b47ee8078 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -803,6 +803,12 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_STIBP);
 		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
 	}
+
+	if (cpu_has(c, X86_FEATURE_AMD_SSBD)) {
+		set_cpu_cap(c, X86_FEATURE_SSBD);
+		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+		clear_cpu_cap(c, X86_FEATURE_VIRT_SSBD);
+	}
 }
 
 void get_cpu_cap(struct cpuinfo_x86 *c)

commit 24809860012e0130fbafe536709e08a22b3e959e
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Jun 1 10:59:19 2018 -0400

    x86/bugs: Add AMD's variant of SSB_NO
    
    The AMD document outlining the SSBD handling
    124441_AMD64_SpeculativeStoreBypassDisable_Whitepaper_final.pdf
    mentions that the CPUID 8000_0008.EBX[26] will mean that the
    speculative store bypass disable is no longer needed.
    
    A copy of this document is available at:
        https://bugzilla.kernel.org/show_bug.cgi?id=199889
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
    Cc: kvm@vger.kernel.org
    Cc: andrew.cooper3@citrix.com
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Link: https://lkml.kernel.org/r/20180601145921.9500-2-konrad.wilk@oracle.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 95c8e507580d..8e3f062f462d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -992,7 +992,8 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
 
 	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
-	   !(ia32_cap & ARCH_CAP_SSB_NO))
+	   !(ia32_cap & ARCH_CAP_SSB_NO) &&
+	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
 	if (x86_match_cpu(cpu_no_meltdown))

commit 5cef8c2a2289117b7f65de4313b7157578ec1a71
Merge: f7f4e7fc6c51 e4e961e36f06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 18:19:18 2018 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
    
     - Centaur CPU updates (David Wang)
    
     - AMD and other CPU topology enumeration improvements and fixes
       (Borislav Petkov, Thomas Gleixner, Suravee Suthikulpanit)
    
     - Continued 5-level paging work (Kirill A. Shutemov)
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Mark __pgtable_l5_enabled __initdata
      x86/mm: Mark p4d_offset() __always_inline
      x86/mm: Introduce the 'no5lvl' kernel parameter
      x86/mm: Stop pretending pgtable_l5_enabled is a variable
      x86/mm: Unify pgtable_l5_enabled usage in early boot code
      x86/boot/compressed/64: Fix trampoline page table address calculation
      x86/CPU: Move x86_cpuinfo::x86_max_cores assignment to detect_num_cpu_cores()
      x86/Centaur: Report correct CPU/cache topology
      x86/CPU: Move cpu_detect_cache_sizes() into init_intel_cacheinfo()
      x86/CPU: Make intel_num_cpu_cores() generic
      x86/CPU: Move cpu local function declarations to local header
      x86/CPU/AMD: Derive CPU topology from CPUID function 0xB when available
      x86/CPU: Modify detect_extended_topology() to return result
      x86/CPU/AMD: Calculate last level cache ID from number of sharing threads
      x86/CPU: Rename intel_cacheinfo.c to cacheinfo.c
      perf/events/amd/uncore: Fix amd_uncore_llc ID to use pre-defined cpu_llc_id
      x86/CPU/AMD: Have smp_num_siblings and cpu_llc_id always be present
      x86/Centaur: Initialize supported CPU features properly

commit 8ecc4979b1bd9c94168e6fc92960033b7a951336
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Tue May 22 11:05:39 2018 +0200

    x86/speculation: Simplify the CPU bug detection logic
    
    Only CPUs which speculate can speculate. Therefore, it seems prudent
    to test for cpu_no_speculation first and only then determine whether
    a specific speculating CPU is susceptible to store bypass speculation.
    This is underlined by all CPUs currently listed in cpu_no_speculation
    were present in cpu_no_spec_store_bypass as well.
    
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@suse.de
    Cc: konrad.wilk@oracle.com
    Link: https://lkml.kernel.org/r/20180522090539.GA24668@light.dominikbrodowski.net

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 78decc3e3067..38276f58d3bf 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -942,12 +942,8 @@ static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
 	{}
 };
 
+/* Only list CPUs which speculate but are non susceptible to SSB */
 static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_PINEVIEW	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_LINCROFT	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_PENWELL		},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_CLOVERVIEW	},
-	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_CEDARVIEW	},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
@@ -955,14 +951,10 @@ static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_CORE_YONAH		},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
-	{ X86_VENDOR_CENTAUR,	5,					},
-	{ X86_VENDOR_INTEL,	5,					},
-	{ X86_VENDOR_NSC,	5,					},
 	{ X86_VENDOR_AMD,	0x12,					},
 	{ X86_VENDOR_AMD,	0x11,					},
 	{ X86_VENDOR_AMD,	0x10,					},
 	{ X86_VENDOR_AMD,	0xf,					},
-	{ X86_VENDOR_ANY,	4,					},
 	{}
 };
 
@@ -970,6 +962,12 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = 0;
 
+	if (x86_match_cpu(cpu_no_speculation))
+		return;
+
+	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+
 	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
 
@@ -977,12 +975,6 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	   !(ia32_cap & ARCH_CAP_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
-	if (x86_match_cpu(cpu_no_speculation))
-		return;
-
-	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
-	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
-
 	if (x86_match_cpu(cpu_no_meltdown))
 		return;
 

commit 372fddf709041743a93e381556f4c41aad1e28f8
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri May 18 13:35:25 2018 +0300

    x86/mm: Introduce the 'no5lvl' kernel parameter
    
    This kernel parameter allows to force kernel to use 4-level paging even
    if hardware and kernel support 5-level paging.
    
    The option may be useful to work around regressions related to 5-level
    paging.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20180518103528.59260-5-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 39ed2e6ff8a0..27f68d14c962 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1028,6 +1028,21 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	 */
 	setup_clear_cpu_cap(X86_FEATURE_PCID);
 #endif
+
+	/*
+	 * Later in the boot process pgtable_l5_enabled() relies on
+	 * cpu_feature_enabled(X86_FEATURE_LA57). If 5-level paging is not
+	 * enabled by this point we need to clear the feature bit to avoid
+	 * false-positives at the later stage.
+	 *
+	 * pgtable_l5_enabled() can be false here for several reasons:
+	 *  - 5-level paging is disabled compile-time;
+	 *  - it's 32-bit kernel;
+	 *  - machine doesn't support 5-level paging;
+	 *  - user specified 'no5lvl' in kernel command line.
+	 */
+	if (!pgtable_l5_enabled())
+		setup_clear_cpu_cap(X86_FEATURE_LA57);
 }
 
 void __init early_cpu_init(void)

commit 177bfd725bd1b67c7254248cf19f0465d493e631
Merge: 9305bd6ca7b4 fed71f7d9879 7dec80ccbe31
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat May 19 08:18:56 2018 +0200

    Merge branches 'x86/urgent' and 'core/urgent' into x86/boot, to pick up fixes and avoid conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 240da953fcc6a9008c92fae5b1f727ee5ed167ab
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 16 23:18:09 2018 -0400

    x86/bugs: Rename SSBD_NO to SSB_NO
    
    The "336996 Speculative Execution Side Channel Mitigations" from
    May defines this as SSB_NO, hence lets sync-up.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b4247ed0c81e..78decc3e3067 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -974,7 +974,7 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
 
 	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
-	   !(ia32_cap & ARCH_CAP_SSBD_NO))
+	   !(ia32_cap & ARCH_CAP_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
 	if (x86_match_cpu(cpu_no_speculation))

commit bc226f07dcd3c9ef0b7f6236fe356ea4a9cb4769
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Thu May 10 22:06:39 2018 +0200

    KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD
    
    Expose the new virtualized architectural mechanism, VIRT_SSBD, for using
    speculative store bypass disable (SSBD) under SVM.  This will allow guests
    to use SSBD on hardware that uses non-architectural mechanisms for enabling
    SSBD.
    
    [ tglx: Folded the migration fixup from Paolo Bonzini ]
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 68282514c025..b4247ed0c81e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -767,7 +767,8 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
 		set_cpu_cap(c, X86_FEATURE_STIBP);
 
-	if (cpu_has(c, X86_FEATURE_SPEC_CTRL_SSBD))
+	if (cpu_has(c, X86_FEATURE_SPEC_CTRL_SSBD) ||
+	    cpu_has(c, X86_FEATURE_VIRT_SSBD))
 		set_cpu_cap(c, X86_FEATURE_SSBD);
 
 	if (cpu_has(c, X86_FEATURE_AMD_IBRS)) {

commit 52817587e706686fcdb27f14c1b000c92f266c96
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 20:21:36 2018 +0200

    x86/cpufeatures: Disentangle SSBD enumeration
    
    The SSBD enumeration is similarly to the other bits magically shared
    between Intel and AMD though the mechanisms are different.
    
    Make X86_FEATURE_SSBD synthetic and set it depending on the vendor specific
    features or family dependent setup.
    
    Change the Intel bit to X86_FEATURE_SPEC_CTRL_SSBD to denote that SSBD is
    controlled via MSR_SPEC_CTRL and fix up the usage sites.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index af54dbe2df9a..68282514c025 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -767,6 +767,9 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
 		set_cpu_cap(c, X86_FEATURE_STIBP);
 
+	if (cpu_has(c, X86_FEATURE_SPEC_CTRL_SSBD))
+		set_cpu_cap(c, X86_FEATURE_SSBD);
+
 	if (cpu_has(c, X86_FEATURE_AMD_IBRS)) {
 		set_cpu_cap(c, X86_FEATURE_IBRS);
 		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);

commit 7eb8956a7fec3c1f0abc2a5517dada99ccc8a961
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 19:13:18 2018 +0200

    x86/cpufeatures: Disentangle MSR_SPEC_CTRL enumeration from IBRS
    
    The availability of the SPEC_CTRL MSR is enumerated by a CPUID bit on
    Intel and implied by IBRS or STIBP support on AMD. That's just confusing
    and in case an AMD CPU has IBRS not supported because the underlying
    problem has been fixed but has another bit valid in the SPEC_CTRL MSR,
    the thing falls apart.
    
    Add a synthetic feature bit X86_FEATURE_MSR_SPEC_CTRL to denote the
    availability on both Intel and AMD.
    
    While at it replace the boot_cpu_has() checks with static_cpu_has() where
    possible. This prevents late microcode loading from exposing SPEC_CTRL, but
    late loading is already very limited as it does not reevaluate the
    mitigation options and other bits and pieces. Having static_cpu_has() is
    the simplest and least fragile solution.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e6cd38b20375..af54dbe2df9a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -761,19 +761,24 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {
 		set_cpu_cap(c, X86_FEATURE_IBRS);
 		set_cpu_cap(c, X86_FEATURE_IBPB);
+		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
 	}
 
 	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
 		set_cpu_cap(c, X86_FEATURE_STIBP);
 
-	if (cpu_has(c, X86_FEATURE_AMD_IBRS))
+	if (cpu_has(c, X86_FEATURE_AMD_IBRS)) {
 		set_cpu_cap(c, X86_FEATURE_IBRS);
+		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+	}
 
 	if (cpu_has(c, X86_FEATURE_AMD_IBPB))
 		set_cpu_cap(c, X86_FEATURE_IBPB);
 
-	if (cpu_has(c, X86_FEATURE_AMD_STIBP))
+	if (cpu_has(c, X86_FEATURE_AMD_STIBP)) {
 		set_cpu_cap(c, X86_FEATURE_STIBP);
+		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+	}
 }
 
 void get_cpu_cap(struct cpuinfo_x86 *c)

commit e7c587da125291db39ddf1f49b18e5970adbac17
Author: Borislav Petkov <bp@suse.de>
Date:   Wed May 2 18:15:14 2018 +0200

    x86/speculation: Use synthetic bits for IBRS/IBPB/STIBP
    
    Intel and AMD have different CPUID bits hence for those use synthetic bits
    which get set on the respective vendor's in init_speculation_control(). So
    that debacles like what the commit message of
    
      c65732e4f721 ("x86/cpu: Restore CPUID_8000_0008_EBX reload")
    
    talks about don't happen anymore.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Tested-by: Jrg Otte <jrg.otte@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Link: https://lkml.kernel.org/r/20180504161815.GG9257@pd.tnic

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9fbb388fadac..e6cd38b20375 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -757,17 +757,23 @@ static void init_speculation_control(struct cpuinfo_x86 *c)
 	 * and they also have a different bit for STIBP support. Also,
 	 * a hypervisor might have set the individual AMD bits even on
 	 * Intel CPUs, for finer-grained selection of what's available.
-	 *
-	 * We use the AMD bits in 0x8000_0008 EBX as the generic hardware
-	 * features, which are visible in /proc/cpuinfo and used by the
-	 * kernel. So set those accordingly from the Intel bits.
 	 */
 	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {
 		set_cpu_cap(c, X86_FEATURE_IBRS);
 		set_cpu_cap(c, X86_FEATURE_IBPB);
 	}
+
 	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
 		set_cpu_cap(c, X86_FEATURE_STIBP);
+
+	if (cpu_has(c, X86_FEATURE_AMD_IBRS))
+		set_cpu_cap(c, X86_FEATURE_IBRS);
+
+	if (cpu_has(c, X86_FEATURE_AMD_IBPB))
+		set_cpu_cap(c, X86_FEATURE_IBPB);
+
+	if (cpu_has(c, X86_FEATURE_AMD_STIBP))
+		set_cpu_cap(c, X86_FEATURE_STIBP);
 }
 
 void get_cpu_cap(struct cpuinfo_x86 *c)

commit 9305bd6ca7b40fece04d7a7a02765e9e8349f146
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 13 11:43:53 2018 +0200

    x86/CPU: Move x86_cpuinfo::x86_max_cores assignment to detect_num_cpu_cores()
    
    No point to have it at the call sites.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6993842e788c..f2085d54c2c8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -584,18 +584,17 @@ static void get_model_name(struct cpuinfo_x86 *c)
 	*(s + 1) = '\0';
 }
 
-int detect_num_cpu_cores(struct cpuinfo_x86 *c)
+void detect_num_cpu_cores(struct cpuinfo_x86 *c)
 {
 	unsigned int eax, ebx, ecx, edx;
 
+	c->x86_max_cores = 1;
 	if (!IS_ENABLED(CONFIG_SMP) || c->cpuid_level < 4)
-		return 1;
+		return;
 
 	cpuid_count(4, 0, &eax, &ebx, &ecx, &edx);
 	if (eax & 0x1f)
-		return (eax >> 26) + 1;
-	else
-		return 1;
+		c->x86_max_cores = (eax >> 26) + 1;
 }
 
 void cpu_detect_cache_sizes(struct cpuinfo_x86 *c)

commit 2cc61be60e37b1856a97ccbdcca3e86e593bf06a
Author: David Wang <davidwang@zhaoxin.com>
Date:   Thu May 3 10:32:44 2018 +0800

    x86/CPU: Make intel_num_cpu_cores() generic
    
    intel_num_cpu_cores() is a static function in intel.c which can't be used
    by other files. Define another function called detect_num_cpu_cores() in
    common.c to replace this function so it can be reused.
    
    Signed-off-by: David Wang <davidwang@zhaoxin.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: lukelin@viacpu.com
    Cc: qiyuanwang@zhaoxin.com
    Cc: gregkh@linuxfoundation.org
    Cc: brucechang@via-alliance.com
    Cc: timguo@zhaoxin.com
    Cc: cooperyan@zhaoxin.com
    Cc: hpa@zytor.com
    Cc: benjaminpan@viatech.com
    Link: https://lkml.kernel.org/r/1525314766-18910-2-git-send-email-davidwang@zhaoxin.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 37c7c8334a00..6993842e788c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -584,6 +584,20 @@ static void get_model_name(struct cpuinfo_x86 *c)
 	*(s + 1) = '\0';
 }
 
+int detect_num_cpu_cores(struct cpuinfo_x86 *c)
+{
+	unsigned int eax, ebx, ecx, edx;
+
+	if (!IS_ENABLED(CONFIG_SMP) || c->cpuid_level < 4)
+		return 1;
+
+	cpuid_count(4, 0, &eax, &ebx, &ecx, &edx);
+	if (eax & 0x1f)
+		return (eax >> 26) + 1;
+	else
+		return 1;
+}
+
 void cpu_detect_cache_sizes(struct cpuinfo_x86 *c)
 {
 	unsigned int n, dummy, ebx, ecx, edx, l2size;

commit 9f65fb29374ee37856dbad847b4e121aab72b510
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 9 21:41:38 2018 +0200

    x86/bugs: Rename _RDS to _SSBD
    
    Intel collateral will reference the SSB mitigation bit in IA32_SPEC_CTL[2]
    as SSBD (Speculative Store Bypass Disable).
    
    Hence changing it.
    
    It is unclear yet what the MSR_IA32_ARCH_CAPABILITIES (0x10a) Bit(4) name
    is going to be. Following the rename it would be SSBD_NO but that rolls out
    to Speculative Store Bypass Disable No.
    
    Also fixed the missing space in X86_FEATURE_AMD_SSBD.
    
    [ tglx: Fixup x86_amd_rds_enable() and rds_tif_to_amd_ls_cfg() as well ]
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e0517bcee446..9fbb388fadac 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -959,7 +959,7 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
 
 	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
-	   !(ia32_cap & ARCH_CAP_RDS_NO))
+	   !(ia32_cap & ARCH_CAP_SSBD_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
 	if (x86_match_cpu(cpu_no_speculation))

commit f8b64d08dde2714c62751d18ba77f4aeceb161d3
Author: Borislav Petkov <bpetkov@suse.de>
Date:   Fri Apr 27 16:34:34 2018 -0500

    x86/CPU/AMD: Have smp_num_siblings and cpu_llc_id always be present
    
    Move smp_num_siblings and cpu_llc_id to cpu/common.c so that they're
    always present as symbols and not only in the CONFIG_SMP case. Then,
    other code using them doesn't need ugly ifdeffery anymore. Get rid of
    some ifdeffery.
    
    Signed-off-by: Borislav Petkov <bpetkov@suse.de>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1524864877-111962-2-git-send-email-suravee.suthikulpanit@amd.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8a5b185735e1..37c7c8334a00 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -66,6 +66,13 @@ cpumask_var_t cpu_callin_mask;
 /* representing cpus for which sibling maps can be computed */
 cpumask_var_t cpu_sibling_setup_mask;
 
+/* Number of siblings per CPU package */
+int smp_num_siblings = 1;
+EXPORT_SYMBOL(smp_num_siblings);
+
+/* Last level cache ID of each logical CPU */
+DEFINE_PER_CPU_READ_MOSTLY(u16, cpu_llc_id) = BAD_APICID;
+
 /* correctly size the local cpu masks */
 void __init setup_cpu_local_masks(void)
 {

commit 764f3c21588a059cd783c6ba0734d4db2d72822d
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 25 22:04:24 2018 -0400

    x86/bugs/AMD: Add support to disable RDS on Fam[15,16,17]h if requested
    
    AMD does not need the Speculative Store Bypass mitigation to be enabled.
    
    The parameters for this are already available and can be done via MSR
    C001_1020. Each family uses a different bit in that MSR for this.
    
    [ tglx: Expose the bit mask via a variable and move the actual MSR fiddling
            into the bugs code as that's the right thing to do and also required
            to prepare for dynamic enable/disable ]
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d6dc71d616ea..e0517bcee446 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -943,6 +943,10 @@ static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
 	{ X86_VENDOR_CENTAUR,	5,					},
 	{ X86_VENDOR_INTEL,	5,					},
 	{ X86_VENDOR_NSC,	5,					},
+	{ X86_VENDOR_AMD,	0x12,					},
+	{ X86_VENDOR_AMD,	0x11,					},
+	{ X86_VENDOR_AMD,	0x10,					},
+	{ X86_VENDOR_AMD,	0xf,					},
 	{ X86_VENDOR_ANY,	4,					},
 	{}
 };

commit 772439717dbf703b39990be58d8d4e3e4ad0598a
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 25 22:04:22 2018 -0400

    x86/bugs/intel: Set proper CPU features and setup RDS
    
    Intel CPUs expose methods to:
    
     - Detect whether RDS capability is available via CPUID.7.0.EDX[31],
    
     - The SPEC_CTRL MSR(0x48), bit 2 set to enable RDS.
    
     - MSR_IA32_ARCH_CAPABILITIES, Bit(4) no need to enable RRS.
    
    With that in mind if spec_store_bypass_disable=[auto,on] is selected set at
    boot-time the SPEC_CTRL MSR to enable RDS if the platform requires it.
    
    Note that this does not fix the KVM case where the SPEC_CTRL is exposed to
    guests which can muck with it, see patch titled :
     KVM/SVM/VMX/x86/spectre_v2: Support the combination of guest and host IBRS.
    
    And for the firmware (IBRS to be set), see patch titled:
     x86/spectre_v2: Read SPEC_CTRL MSR during boot and re-use reserved bits
    
    [ tglx: Distangled it from the intel implementation and kept the call order ]
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6ff0cc441a59..d6dc71d616ea 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -951,7 +951,11 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = 0;
 
-	if (!x86_match_cpu(cpu_no_spec_store_bypass))
+	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
+		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
+
+	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
+	   !(ia32_cap & ARCH_CAP_RDS_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
 	if (x86_match_cpu(cpu_no_speculation))
@@ -963,9 +967,6 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (x86_match_cpu(cpu_no_meltdown))
 		return;
 
-	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
-		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
-
 	/* Rogue Data Cache Load? No! */
 	if (ia32_cap & ARCH_CAP_RDCL_NO)
 		return;
@@ -1383,6 +1384,7 @@ void identify_secondary_cpu(struct cpuinfo_x86 *c)
 #endif
 	mtrr_ap_init();
 	validate_apic_and_package_id(c);
+	x86_spec_ctrl_setup_ap();
 }
 
 static __init int setup_noclflush(char *arg)

commit c456442cd3a59eeb1d60293c26cbe2ff2c4e42cf
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 25 22:04:20 2018 -0400

    x86/bugs: Expose /sys/../spec_store_bypass
    
    Add the sysfs file for the new vulerability. It does not do much except
    show the words 'Vulnerable' for recent x86 cores.
    
    Intel cores prior to family 6 are known not to be vulnerable, and so are
    some Atoms and some Xeon Phi.
    
    It assumes that older Cyrix, Centaur, etc. cores are immune.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9ed0a18ff6f6..6ff0cc441a59 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -927,10 +927,33 @@ static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
 	{}
 };
 
+static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_PINEVIEW	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_LINCROFT	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_PENWELL		},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_CLOVERVIEW	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_CEDARVIEW	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_CORE_YONAH		},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
+	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
+	{ X86_VENDOR_CENTAUR,	5,					},
+	{ X86_VENDOR_INTEL,	5,					},
+	{ X86_VENDOR_NSC,	5,					},
+	{ X86_VENDOR_ANY,	4,					},
+	{}
+};
+
 static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = 0;
 
+	if (!x86_match_cpu(cpu_no_spec_store_bypass))
+		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
+
 	if (x86_match_cpu(cpu_no_speculation))
 		return;
 

commit 4a28bfe3267b68e22c663ac26185aa16c9b879ef
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 25 22:04:16 2018 -0400

    x86/bugs: Concentrate bug detection into a separate function
    
    Combine the various logic which goes through all those
    x86_cpu_id matching structures in one function.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ce243f7d2d4e..9ed0a18ff6f6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -927,21 +927,27 @@ static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
 	{}
 };
 
-static bool __init cpu_vulnerable_to_meltdown(struct cpuinfo_x86 *c)
+static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = 0;
 
+	if (x86_match_cpu(cpu_no_speculation))
+		return;
+
+	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+
 	if (x86_match_cpu(cpu_no_meltdown))
-		return false;
+		return;
 
 	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
 
 	/* Rogue Data Cache Load? No! */
 	if (ia32_cap & ARCH_CAP_RDCL_NO)
-		return false;
+		return;
 
-	return true;
+	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 }
 
 /*
@@ -992,12 +998,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 
-	if (!x86_match_cpu(cpu_no_speculation)) {
-		if (cpu_vulnerable_to_meltdown(c))
-			setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
-		setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
-		setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
-	}
+	cpu_set_bug_bits(c);
 
 	fpu__init_system(c);
 

commit c65732e4f72124ca5a3a0dd3bee0d3cee39c7170
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Apr 30 21:47:46 2018 +0200

    x86/cpu: Restore CPUID_8000_0008_EBX reload
    
    The recent commt which addresses the x86_phys_bits corruption with
    encrypted memory on CPUID reload after a microcode update lost the reload
    of CPUID_8000_0008_EBX as well.
    
    As a consequence IBRS and IBRS_FW are not longer detected
    
    Restore the behaviour by bringing the reload of CPUID_8000_0008_EBX
    back. This restore has a twist due to the convoluted way the cpuid analysis
    works:
    
    CPUID_8000_0008_EBX is used by AMD to enumerate IBRB, IBRS, STIBP. On Intel
    EBX is not used. But the speculation control code sets the AMD bits when
    running on Intel depending on the Intel specific speculation control
    bits. This was done to use the same bits for alternatives.
    
    The change which moved the 8000_0008 evaluation out of get_cpu_cap() broke
    this nasty scheme due to ordering. So that on Intel the store to
    CPUID_8000_0008_EBX clears the IBRB, IBRS, STIBP bits which had been set
    before by software.
    
    So the actual CPUID_8000_0008_EBX needs to go back to the place where it
    was and the phys/virt address space calculation cannot touch it.
    
    In hindsight this should have used completely synthetic bits for IBRB,
    IBRS, STIBP instead of reusing the AMD bits, but that's for 4.18.
    
    /me needs to find time to cleanup that steaming pile of ...
    
    Fixes: d94a155c59c9 ("x86/cpu: Prevent cpuinfo_x86::x86_phys_bits adjustment corruption")
    Reported-by: Jrg Otte <jrg.otte@gmail.com>
    Reported-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jrg Otte <jrg.otte@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: kirill.shutemov@linux.intel.com
    Cc: Borislav Petkov <bp@alien8.de
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1805021043510.1668@nanos.tec.linutronix.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8a5b185735e1..ce243f7d2d4e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -848,6 +848,11 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_power = edx;
 	}
 
+	if (c->extended_cpuid_level >= 0x80000008) {
+		cpuid(0x80000008, &eax, &ebx, &ecx, &edx);
+		c->x86_capability[CPUID_8000_0008_EBX] = ebx;
+	}
+
 	if (c->extended_cpuid_level >= 0x8000000a)
 		c->x86_capability[CPUID_8000_000A_EDX] = cpuid_edx(0x8000000a);
 
@@ -871,7 +876,6 @@ static void get_cpu_address_sizes(struct cpuinfo_x86 *c)
 
 		c->x86_virt_bits = (eax >> 8) & 0xff;
 		c->x86_phys_bits = eax & 0xff;
-		c->x86_capability[CPUID_8000_0008_EBX] = ebx;
 	}
 #ifdef CONFIG_X86_32
 	else if (cpu_has(c, X86_FEATURE_PAE) || cpu_has(c, X86_FEATURE_PSE36))

commit 9fb71c2f230df44bdd237e9a4457849a3909017d
Merge: 6b0a02e86c29 ef389b734691
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 15 16:12:35 2018 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "A set of fixes and updates for x86:
    
       - Address a swiotlb regression which was caused by the recent DMA
         rework and made driver fail because dma_direct_supported() returned
         false
    
       - Fix a signedness bug in the APIC ID validation which caused invalid
         APIC IDs to be detected as valid thereby bloating the CPU possible
         space.
    
       - Fix inconsisten config dependcy/select magic for the MFD_CS5535
         driver.
    
       - Fix a corruption of the physical address space bits when encryption
         has reduced the address space and late cpuinfo updates overwrite
         the reduced bit information with the original value.
    
       - Dominiks syscall rework which consolidates the architecture
         specific syscall functions so all syscalls can be wrapped with the
         same macros. This allows to switch x86/64 to struct pt_regs based
         syscalls. Extend the clearing of user space controlled registers in
         the entry patch to the lower registers"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic: Fix signedness bug in APIC ID validity checks
      x86/cpu: Prevent cpuinfo_x86::x86_phys_bits adjustment corruption
      x86/olpc: Fix inconsistent MFD_CS5535 configuration
      swiotlb: Use dma_direct_supported() for swiotlb_ops
      syscalls/x86: Adapt syscall_wrapper.h to the new syscall stub naming convention
      syscalls/core, syscalls/x86: Rename struct pt_regs-based sys_*() to __x64_sys_*()
      syscalls/core, syscalls/x86: Clean up compat syscall stub naming convention
      syscalls/core, syscalls/x86: Clean up syscall stub naming convention
      syscalls/x86: Extend register clearing on syscall entry to lower registers
      syscalls/x86: Unconditionally enable 'struct pt_regs' based syscalls on x86_64
      syscalls/x86: Use 'struct pt_regs' based syscall calling for IA32_EMULATION and x32
      syscalls/core: Prepare CONFIG_ARCH_HAS_SYSCALL_WRAPPER=y for compat syscalls
      syscalls/x86: Use 'struct pt_regs' based syscall calling convention for 64-bit syscalls
      syscalls/core: Introduce CONFIG_ARCH_HAS_SYSCALL_WRAPPER=y
      x86/syscalls: Don't pointlessly reload the system call number
      x86/mm: Fix documentation of module mapping range with 4-level paging
      x86/cpuid: Switch to 'static const' specifier

commit d94a155c59c98c19b98ee949eaab6a0312bbd6be
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Apr 10 12:27:04 2018 +0300

    x86/cpu: Prevent cpuinfo_x86::x86_phys_bits adjustment corruption
    
    Some features (Intel MKTME, AMD SME) reduce the number of effectively
    available physical address bits. cpuinfo_x86::x86_phys_bits is adjusted
    accordingly during the early cpu feature detection.
    
    Though if get_cpu_cap() is called later again then this adjustement is
    overwritten. That happens in setup_pku(), which is called after
    detect_tme().
    
    To address this, extract the address sizes enumeration into a separate
    function, which is only called only from early_identify_cpu() and from
    generic_identify().
    
    This makes get_cpu_cap() safe to be called later during boot proccess
    without overwriting cpuinfo_x86::x86_phys_bits.
    
    [ tglx: Massaged changelog ]
    
    Fixes: cb06d8e3d020 ("x86/tme: Detect if TME and MKTME is activated by BIOS")
    Reported-by: Kai Huang <kai.huang@linux.intel.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: linux-mm@kvack.org
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/20180410092704.41106-1-kirill.shutemov@linux.intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 348cf4821240..2981bf287ef5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -848,18 +848,6 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_power = edx;
 	}
 
-	if (c->extended_cpuid_level >= 0x80000008) {
-		cpuid(0x80000008, &eax, &ebx, &ecx, &edx);
-
-		c->x86_virt_bits = (eax >> 8) & 0xff;
-		c->x86_phys_bits = eax & 0xff;
-		c->x86_capability[CPUID_8000_0008_EBX] = ebx;
-	}
-#ifdef CONFIG_X86_32
-	else if (cpu_has(c, X86_FEATURE_PAE) || cpu_has(c, X86_FEATURE_PSE36))
-		c->x86_phys_bits = 36;
-#endif
-
 	if (c->extended_cpuid_level >= 0x8000000a)
 		c->x86_capability[CPUID_8000_000A_EDX] = cpuid_edx(0x8000000a);
 
@@ -874,6 +862,23 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 	apply_forced_caps(c);
 }
 
+static void get_cpu_address_sizes(struct cpuinfo_x86 *c)
+{
+	u32 eax, ebx, ecx, edx;
+
+	if (c->extended_cpuid_level >= 0x80000008) {
+		cpuid(0x80000008, &eax, &ebx, &ecx, &edx);
+
+		c->x86_virt_bits = (eax >> 8) & 0xff;
+		c->x86_phys_bits = eax & 0xff;
+		c->x86_capability[CPUID_8000_0008_EBX] = ebx;
+	}
+#ifdef CONFIG_X86_32
+	else if (cpu_has(c, X86_FEATURE_PAE) || cpu_has(c, X86_FEATURE_PSE36))
+		c->x86_phys_bits = 36;
+#endif
+}
+
 static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32
@@ -965,6 +970,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		cpu_detect(c);
 		get_cpu_vendor(c);
 		get_cpu_cap(c);
+		get_cpu_address_sizes(c);
 		setup_force_cpu_cap(X86_FEATURE_CPUID);
 
 		if (this_cpu->c_early_init)
@@ -1097,6 +1103,8 @@ static void generic_identify(struct cpuinfo_x86 *c)
 
 	get_cpu_cap(c);
 
+	get_cpu_address_sizes(c);
+
 	if (c->cpuid_level >= 0x00000001) {
 		c->initial_apicid = (cpuid_ebx(1) >> 24) & 0xFF;
 #ifdef CONFIG_X86_32

commit 35060ed6a1ffc92106904f5e4b5823ca4facfc73
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 13 18:48:05 2018 +0100

    x86/kvm/vmx: avoid expensive rdmsr for MSR_GS_BASE
    
    vmx_save_host_state() is only called from kvm_arch_vcpu_ioctl_run() so
    the context is pretty well defined and as we're past 'swapgs' MSR_GS_BASE
    should contain kernel's GS base which we point to irq_stack_union.
    
    Add new kernelmode_gs_base() API, irq_stack_union needs to be exported
    as KVM can be build as module.
    
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 348cf4821240..4702fbd98f92 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -487,7 +487,7 @@ void load_percpu_segment(int cpu)
 	loadsegment(fs, __KERNEL_PERCPU);
 #else
 	__loadsegment_simple(gs, 0);
-	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
+	wrmsrl(MSR_GS_BASE, cpu_kernelmode_gs_base(cpu));
 #endif
 	load_stack_canary_segment();
 }
@@ -1398,6 +1398,7 @@ __setup("clearcpuid=", setup_clearcpuid);
 #ifdef CONFIG_X86_64
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE) __visible;
+EXPORT_PER_CPU_SYMBOL_GPL(irq_stack_union);
 
 /*
  * The following percpu variables are hot.  Align current_task to

commit 42ca8082e260dcfd8afa2afa6ec1940b9d41724c
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Feb 16 12:26:40 2018 +0100

    x86/CPU: Check CPU feature bits after microcode upgrade
    
    With some microcode upgrades, new CPUID features can become visible on
    the CPU. Check what the kernel has mirrored now and issue a warning
    hinting at possible things the user/admin can do to make use of the
    newly visible features.
    
    Originally-by: Ashok Raj <ashok.raj@intel.com>
    Tested-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ashok Raj <ashok.raj@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180216112640.11554-4-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 84f1cd88608b..348cf4821240 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1757,5 +1757,25 @@ core_initcall(init_cpu_syscore);
  */
 void microcode_check(void)
 {
+	struct cpuinfo_x86 info;
+
 	perf_check_microcode();
+
+	/* Reload CPUID max function as it might've changed. */
+	info.cpuid_level = cpuid_eax(0);
+
+	/*
+	 * Copy all capability leafs to pick up the synthetic ones so that
+	 * memcmp() below doesn't fail on that. The ones coming from CPUID will
+	 * get overwritten in get_cpu_cap().
+	 */
+	memcpy(&info.x86_capability, &boot_cpu_data.x86_capability, sizeof(info.x86_capability));
+
+	get_cpu_cap(&info);
+
+	if (!memcmp(&info.x86_capability, &boot_cpu_data.x86_capability, sizeof(info.x86_capability)))
+		return;
+
+	pr_warn("x86/CPU: CPU features have changed after loading microcode, but might not take effect.\n");
+	pr_warn("x86/CPU: Please consider either early loading through initrd/built-in or a potential BIOS update.\n");
 }

commit 1008c52c09dcb23d93f8e0ea83a6246265d2cce0
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Feb 16 12:26:39 2018 +0100

    x86/CPU: Add a microcode loader callback
    
    Add a callback function which the microcode loader calls when microcode
    has been updated to a newer revision. Do the callback only when no error
    was encountered during loading.
    
    Tested-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ashok Raj <ashok.raj@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180216112640.11554-3-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 824aee0117bb..84f1cd88608b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1749,3 +1749,13 @@ static int __init init_cpu_syscore(void)
 	return 0;
 }
 core_initcall(init_cpu_syscore);
+
+/*
+ * The microcode loader calls this upon late microcode load to recheck features,
+ * only when microcode has been updated. Caller holds microcode_mutex and CPU
+ * hotplug lock.
+ */
+void microcode_check(void)
+{
+	perf_check_microcode();
+}

commit 24dbc6000f4b9b0ef5a9daecb161f1907733765a
Author: Gustavo A. R. Silva <garsilva@embeddedor.com>
Date:   Tue Feb 13 13:22:08 2018 -0600

    x86/cpu: Change type of x86_cache_size variable to unsigned int
    
    Currently, x86_cache_size is of type int, which makes no sense as we
    will never have a valid cache size equal or less than 0. So instead of
    initializing this variable to -1, it can perfectly be initialized to 0
    and use it as an unsigned variable instead.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Gustavo A. R. Silva <garsilva@embeddedor.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Addresses-Coverity-ID: 1464429
    Link: http://lkml.kernel.org/r/20180213192208.GA26414@embeddedor.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a7d8df641a4c..824aee0117bb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1184,7 +1184,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	int i;
 
 	c->loops_per_jiffy = loops_per_jiffy;
-	c->x86_cache_size = -1;
+	c->x86_cache_size = 0;
 	c->x86_vendor = X86_VENDOR_UNKNOWN;
 	c->x86_model = c->x86_stepping = 0;	/* So far unknown... */
 	c->x86_vendor_id[0] = '\0'; /* Unset */

commit b399151cb48db30ad1e0e93dd40d68c6d007b637
Author: Jia Zhang <qianyue.zj@alibaba-inc.com>
Date:   Mon Jan 1 09:52:10 2018 +0800

    x86/cpu: Rename cpu_data.x86_mask to cpu_data.x86_stepping
    
    x86_mask is a confusing name which is hard to associate with the
    processor's stepping.
    
    Additionally, correct an indent issue in lib/cpu.c.
    
    Signed-off-by: Jia Zhang <qianyue.zj@alibaba-inc.com>
    [ Updated it to more recent kernels. ]
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: tony.luck@intel.com
    Link: http://lkml.kernel.org/r/1514771530-70829-1-git-send-email-qianyue.zj@alibaba-inc.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d63f4b5706e4..a7d8df641a4c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -731,7 +731,7 @@ void cpu_detect(struct cpuinfo_x86 *c)
 		cpuid(0x00000001, &tfms, &misc, &junk, &cap0);
 		c->x86		= x86_family(tfms);
 		c->x86_model	= x86_model(tfms);
-		c->x86_mask	= x86_stepping(tfms);
+		c->x86_stepping	= x86_stepping(tfms);
 
 		if (cap0 & (1<<19)) {
 			c->x86_clflush_size = ((misc >> 8) & 0xff) * 8;
@@ -1186,7 +1186,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	c->loops_per_jiffy = loops_per_jiffy;
 	c->x86_cache_size = -1;
 	c->x86_vendor = X86_VENDOR_UNKNOWN;
-	c->x86_model = c->x86_mask = 0;	/* So far unknown... */
+	c->x86_model = c->x86_stepping = 0;	/* So far unknown... */
 	c->x86_vendor_id[0] = '\0'; /* Unset */
 	c->x86_model_id[0] = '\0';  /* Unset */
 	c->x86_max_cores = 1;
@@ -1378,8 +1378,8 @@ void print_cpu_info(struct cpuinfo_x86 *c)
 
 	pr_cont(" (family: 0x%x, model: 0x%x", c->x86, c->x86_model);
 
-	if (c->x86_mask || c->cpuid_level >= 0)
-		pr_cont(", stepping: 0x%x)\n", c->x86_mask);
+	if (c->x86_stepping || c->cpuid_level >= 0)
+		pr_cont(", stepping: 0x%x)\n", c->x86_stepping);
 	else
 		pr_cont(")\n");
 }

commit 4bf5d56d429cbc96c23d809a08f63cd29e1a702e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Feb 2 22:39:23 2018 +0100

    x86/pti: Mark constant arrays as __initconst
    
    I'm seeing build failures from the two newly introduced arrays that
    are marked 'const' and '__initdata', which are mutually exclusive:
    
    arch/x86/kernel/cpu/common.c:882:43: error: 'cpu_no_speculation' causes a section type conflict with 'e820_table_firmware_init'
    arch/x86/kernel/cpu/common.c:895:43: error: 'cpu_no_meltdown' causes a section type conflict with 'e820_table_firmware_init'
    
    The correct annotation is __initconst.
    
    Fixes: fec9434a12f3 ("x86/pti: Do not enable PTI on CPUs which are not vulnerable to Meltdown")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Link: https://lkml.kernel.org/r/20180202213959.611210-1-arnd@arndb.de

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index dd09270bb74e..d63f4b5706e4 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -900,7 +900,7 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #endif
 }
 
-static const __initdata struct x86_cpu_id cpu_no_speculation[] = {
+static const __initconst struct x86_cpu_id cpu_no_speculation[] = {
 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CEDARVIEW,	X86_FEATURE_ANY },
 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CLOVERVIEW,	X86_FEATURE_ANY },
 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_LINCROFT,	X86_FEATURE_ANY },
@@ -913,7 +913,7 @@ static const __initdata struct x86_cpu_id cpu_no_speculation[] = {
 	{}
 };
 
-static const __initdata struct x86_cpu_id cpu_no_meltdown[] = {
+static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
 	{ X86_VENDOR_AMD },
 	{}
 };

commit 7fcae1118f5fd44a862aa5c3525248e35ee67c3b
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Tue Jan 30 14:30:23 2018 +0000

    x86/cpuid: Fix up "virtual" IBRS/IBPB/STIBP feature bits on Intel
    
    Despite the fact that all the other code there seems to be doing it, just
    using set_cpu_cap() in early_intel_init() doesn't actually work.
    
    For CPUs with PKU support, setup_pku() calls get_cpu_cap() after
    c->c_init() has set those feature bits. That resets those bits back to what
    was queried from the hardware.
    
    Turning the bits off for bad microcode is easy to fix. That can just use
    setup_clear_cpu_cap() to force them off for all CPUs.
    
    I was less keen on forcing the feature bits *on* that way, just in case
    of inconsistencies. I appreciate that the kernel is going to get this
    utterly wrong if CPU features are not consistent, because it has already
    applied alternatives by the time secondary CPUs are brought up.
    
    But at least if setup_force_cpu_cap() isn't being used, we might have a
    chance of *detecting* the lack of the corresponding bit and either
    panicking or refusing to bring the offending CPU online.
    
    So ensure that the appropriate feature bits are set within get_cpu_cap()
    regardless of how many extra times it's called.
    
    Fixes: 2961298e ("x86/cpufeatures: Clean up Spectre v2 related CPUID flags")
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: karahmed@amazon.de
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Link: https://lkml.kernel.org/r/1517322623-15261-1-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c7c996a692fd..dd09270bb74e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -750,6 +750,26 @@ static void apply_forced_caps(struct cpuinfo_x86 *c)
 	}
 }
 
+static void init_speculation_control(struct cpuinfo_x86 *c)
+{
+	/*
+	 * The Intel SPEC_CTRL CPUID bit implies IBRS and IBPB support,
+	 * and they also have a different bit for STIBP support. Also,
+	 * a hypervisor might have set the individual AMD bits even on
+	 * Intel CPUs, for finer-grained selection of what's available.
+	 *
+	 * We use the AMD bits in 0x8000_0008 EBX as the generic hardware
+	 * features, which are visible in /proc/cpuinfo and used by the
+	 * kernel. So set those accordingly from the Intel bits.
+	 */
+	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {
+		set_cpu_cap(c, X86_FEATURE_IBRS);
+		set_cpu_cap(c, X86_FEATURE_IBPB);
+	}
+	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
+		set_cpu_cap(c, X86_FEATURE_STIBP);
+}
+
 void get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
@@ -844,6 +864,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[CPUID_8000_000A_EDX] = cpuid_edx(0x8000000a);
 
 	init_scattered_cpuid_features(c);
+	init_speculation_control(c);
 
 	/*
 	 * Clear/Set all flags overridden by options, after probe.

commit 7e86548e2cc8d308cb75439480f428137151b0de
Merge: 64e16720ea08 d8a5b80568a9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jan 30 15:08:27 2018 +0100

    Merge tag 'v4.15' into x86/pti, to be able to merge dependent changes
    
    Time has come to switch PTI development over to a v4.15 base - we'll still
    try to make sure that all PTI fixes backport cleanly to v4.14 and earlier.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fec9434a12f38d3aeafeb75711b71d8a1fdef621
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Thu Jan 25 16:14:13 2018 +0000

    x86/pti: Do not enable PTI on CPUs which are not vulnerable to Meltdown
    
    Also, for CPUs which don't speculate at all, don't report that they're
    vulnerable to the Spectre variants either.
    
    Leave the cpu_no_meltdown[] match table with just X86_VENDOR_AMD in it
    for now, even though that could be done with a simple comparison, on the
    assumption that we'll have more to add.
    
    Based on suggestions from Dave Hansen and Alan Cox.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Dave Hansen <dave.hansen@intel.com>
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: ak@linux.intel.com
    Cc: ashok.raj@intel.com
    Cc: karahmed@amazon.de
    Cc: arjan@linux.intel.com
    Cc: torvalds@linux-foundation.org
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Cc: pbonzini@redhat.com
    Cc: tim.c.chen@linux.intel.com
    Cc: gregkh@linux-foundation.org
    Link: https://lkml.kernel.org/r/1516896855-7642-6-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e5d66e93ed81..970ee06dc8aa 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -47,6 +47,8 @@
 #include <asm/pat.h>
 #include <asm/microcode.h>
 #include <asm/microcode_intel.h>
+#include <asm/intel-family.h>
+#include <asm/cpu_device_id.h>
 
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/uv/uv.h>
@@ -853,6 +855,41 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #endif
 }
 
+static const __initdata struct x86_cpu_id cpu_no_speculation[] = {
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CEDARVIEW,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CLOVERVIEW,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_LINCROFT,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PENWELL,	X86_FEATURE_ANY },
+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PINEVIEW,	X86_FEATURE_ANY },
+	{ X86_VENDOR_CENTAUR,	5 },
+	{ X86_VENDOR_INTEL,	5 },
+	{ X86_VENDOR_NSC,	5 },
+	{ X86_VENDOR_ANY,	4 },
+	{}
+};
+
+static const __initdata struct x86_cpu_id cpu_no_meltdown[] = {
+	{ X86_VENDOR_AMD },
+	{}
+};
+
+static bool __init cpu_vulnerable_to_meltdown(struct cpuinfo_x86 *c)
+{
+	u64 ia32_cap = 0;
+
+	if (x86_match_cpu(cpu_no_meltdown))
+		return false;
+
+	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
+		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
+
+	/* Rogue Data Cache Load? No! */
+	if (ia32_cap & ARCH_CAP_RDCL_NO)
+		return false;
+
+	return true;
+}
+
 /*
  * Do minimum CPU detection early.
  * Fields really needed: vendor, cpuid_level, family, model, mask,
@@ -900,11 +937,12 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 
-	if (c->x86_vendor != X86_VENDOR_AMD)
-		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
-
-	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
-	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+	if (!x86_match_cpu(cpu_no_speculation)) {
+		if (cpu_vulnerable_to_meltdown(c))
+			setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
+		setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+		setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+	}
 
 	fpu__init_system(c);
 

commit 95ca0ee8636059ea2800dfbac9ecac6212d6b38f
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Thu Jan 25 16:14:09 2018 +0000

    x86/cpufeatures: Add CPUID_7_EDX CPUID leaf
    
    This is a pure feature bits leaf. There are two AVX512 feature bits in it
    already which were handled as scattered bits, and three more from this leaf
    are going to be added for speculation control features.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: ak@linux.intel.com
    Cc: ashok.raj@intel.com
    Cc: dave.hansen@intel.com
    Cc: karahmed@amazon.de
    Cc: arjan@linux.intel.com
    Cc: torvalds@linux-foundation.org
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Cc: pbonzini@redhat.com
    Cc: tim.c.chen@linux.intel.com
    Cc: gregkh@linux-foundation.org
    Link: https://lkml.kernel.org/r/1516896855-7642-2-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 372ba3fb400f..e5d66e93ed81 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -745,6 +745,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		cpuid_count(0x00000007, 0, &eax, &ebx, &ecx, &edx);
 		c->x86_capability[CPUID_7_0_EBX] = ebx;
 		c->x86_capability[CPUID_7_ECX] = ecx;
+		c->x86_capability[CPUID_7_EDX] = edx;
 	}
 
 	/* Extended state features: level 0x0000000d */

commit 40548c6b6c134275c750eb372dc2cf8ee1bbc3d4
Merge: 2c1cfa499018 99a9dc98ba52
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 14 09:51:25 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 pti updates from Thomas Gleixner:
     "This contains:
    
       - a PTI bugfix to avoid setting reserved CR3 bits when PCID is
         disabled. This seems to cause issues on a virtual machine at least
         and is incorrect according to the AMD manual.
    
       - a PTI bugfix which disables the perf BTS facility if PTI is
         enabled. The BTS AUX buffer is not globally visible and causes the
         CPU to fault when the mapping disappears on switching CR3 to user
         space. A full fix which restores BTS on PTI is non trivial and will
         be worked on.
    
       - PTI bugfixes for EFI and trusted boot which make sure that the user
         space visible page table entries have the NX bit cleared
    
       - removal of dead code in the PTI pagetable setup functions
    
       - add PTI documentation
    
       - add a selftest for vsyscall to verify that the kernel actually
         implements what it advertises.
    
       - a sysfs interface to expose vulnerability and mitigation
         information so there is a coherent way for users to retrieve the
         status.
    
       - the initial spectre_v2 mitigations, aka retpoline:
    
          + The necessary ASM thunk and compiler support
    
          + The ASM variants of retpoline and the conversion of affected ASM
            code
    
          + Make LFENCE serializing on AMD so it can be used as speculation
            trap
    
          + The RSB fill after vmexit
    
       - initial objtool support for retpoline
    
      As I said in the status mail this is the most of the set of patches
      which should go into 4.15 except two straight forward patches still on
      hold:
    
       - the retpoline add on of LFENCE which waits for ACKs
    
       - the RSB fill after context switch
    
      Both should be ready to go early next week and with that we'll have
      covered the major holes of spectre_v2 and go back to normality"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (28 commits)
      x86,perf: Disable intel_bts when PTI
      security/Kconfig: Correct the Documentation reference for PTI
      x86/pti: Fix !PCID and sanitize defines
      selftests/x86: Add test_vsyscall
      x86/retpoline: Fill return stack buffer on vmexit
      x86/retpoline/irq32: Convert assembler indirect jumps
      x86/retpoline/checksum32: Convert assembler indirect jumps
      x86/retpoline/xen: Convert Xen hypercall indirect jumps
      x86/retpoline/hyperv: Convert assembler indirect jumps
      x86/retpoline/ftrace: Convert ftrace assembler indirect jumps
      x86/retpoline/entry: Convert entry assembler indirect jumps
      x86/retpoline/crypto: Convert crypto assembler indirect jumps
      x86/spectre: Add boot time option to select Spectre v2 mitigation
      x86/retpoline: Add initial retpoline support
      objtool: Allow alternatives to be ignored
      objtool: Detect jumps to retpoline thunks
      x86/pti: Make unpoison of pgd for trusted boot work for real
      x86/alternatives: Fix optimize_nops() checking
      sysfs/cpu: Fix typos in vulnerability documentation
      x86/cpu/AMD: Use LFENCE_RDTSC in preference to MFENCE_RDTSC
      ...

commit da285121560e769cc31797bba6422eea71d473e0
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Thu Jan 11 21:46:26 2018 +0000

    x86/spectre: Add boot time option to select Spectre v2 mitigation
    
    Add a spectre_v2= option to select the mitigation used for the indirect
    branch speculation vulnerability.
    
    Currently, the only option available is retpoline, in its various forms.
    This will be expanded to cover the new IBRS/IBPB microcode features.
    
    The RETPOLINE_AMD feature relies on a serializing LFENCE for speculation
    control. For AMD hardware, only set RETPOLINE_AMD if LFENCE is a
    serializing instruction, which is indicated by the LFENCE_RDTSC feature.
    
    [ tglx: Folded back the LFENCE/AMD fixes and reworked it so IBRS
            integration becomes simple ]
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: thomas.lendacky@amd.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Kees Cook <keescook@google.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: https://lkml.kernel.org/r/1515707194-20531-5-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7a671d1ae3cb..372ba3fb400f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -905,10 +905,6 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
 
-#ifdef CONFIG_RETPOLINE
-	setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
-#endif
-
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32

commit 76b043848fd22dbf7f8bf3a1452f8c70d557b860
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Thu Jan 11 21:46:25 2018 +0000

    x86/retpoline: Add initial retpoline support
    
    Enable the use of -mindirect-branch=thunk-extern in newer GCC, and provide
    the corresponding thunks. Provide assembler macros for invoking the thunks
    in the same way that GCC does, from native and inline assembler.
    
    This adds X86_FEATURE_RETPOLINE and sets it by default on all CPUs. In
    some circumstances, IBRS microcode features may be used instead, and the
    retpoline can be disabled.
    
    On AMD CPUs if lfence is serialising, the retpoline can be dramatically
    simplified to a simple "lfence; jmp *\reg". A future patch, after it has
    been verified that lfence really is serialising in all circumstances, can
    enable this by setting the X86_FEATURE_RETPOLINE_AMD feature bit in addition
    to X86_FEATURE_RETPOLINE.
    
    Do not align the retpoline in the altinstr section, because there is no
    guarantee that it stays aligned when it's copied over the oldinstr during
    alternative patching.
    
    [ Andi Kleen: Rename the macros, add CONFIG_RETPOLINE option, export thunks]
    [ tglx: Put actual function CALL/JMP in front of the macros, convert to
            symbolic labels ]
    [ dwmw2: Convert back to numeric labels, merge objtool fixes ]
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: thomas.lendacky@amd.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Kees Cook <keescook@google.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: https://lkml.kernel.org/r/1515707194-20531-4-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 372ba3fb400f..7a671d1ae3cb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -905,6 +905,10 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
 
+#ifdef CONFIG_RETPOLINE
+	setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+#endif
+
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32

commit 99c6fa2511d8a683e61468be91b83f85452115fa
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Sat Jan 6 11:49:23 2018 +0000

    x86/cpufeatures: Add X86_BUG_SPECTRE_V[12]
    
    Add the bug bits for spectre v1/2 and force them unconditionally for all
    cpus.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Kees Cook <keescook@google.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/1515239374-23361-2-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2d3bd2215e5b..372ba3fb400f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -902,6 +902,9 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	if (c->x86_vendor != X86_VENDOR_AMD)
 		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 
+	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32

commit abb7099dbc7a77f8674083050028c493ac601228
Merge: b03acc4cc2c3 de791821c295
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 5 12:23:57 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull  more x86 pti fixes from Thomas Gleixner:
     "Another small stash of fixes for fallout from the PTI work:
    
       - Fix the modules vs. KASAN breakage which was caused by making
         MODULES_END depend of the fixmap size. That was done when the cpu
         entry area moved into the fixmap, but now that we have a separate
         map space for that this is causing more issues than it solves.
    
       - Use the proper cache flush methods for the debugstore buffers as
         they are mapped/unmapped during runtime and not statically mapped
         at boot time like the rest of the cpu entry area.
    
       - Make the map layout of the cpu_entry_area consistent for 4 and 5
         level paging and fix the KASLR vaddr_end wreckage.
    
       - Use PER_CPU_EXPORT for per cpu variable and while at it unbreak
         nvidia gfx drivers by dropping the GPL export. The subject line of
         the commit tells it the other way around, but I noticed that too
         late.
    
       - Fix the ASM alternative macros so they can be used in the middle of
         an inline asm block.
    
       - Rename the BUG_CPU_INSECURE flag to BUG_CPU_MELTDOWN so the attack
         vector is properly identified. The Spectre mitigations will come
         with their own bug bits later"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/pti: Rename BUG_CPU_INSECURE to BUG_CPU_MELTDOWN
      x86/alternatives: Add missing '\n' at end of ALTERNATIVE inline asm
      x86/tlb: Drop the _GPL from the cpu_tlbstate export
      x86/events/intel/ds: Use the proper cache flush method for mapping ds buffers
      x86/kaslr: Fix the vaddr_end mess
      x86/mm: Map cpu_entry_area at the same place on 4/5 level
      x86/mm: Set MODULES_END to 0xffffffffff000000

commit de791821c295cc61419a06fe5562288417d1bc58
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 5 15:27:34 2018 +0100

    x86/pti: Rename BUG_CPU_INSECURE to BUG_CPU_MELTDOWN
    
    Use the name associated with the particular attack which needs page table
    isolation for mitigation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Alan Cox <gnomes@lxorguk.ukuu.org.uk>
    Cc: Jiri Koshina <jikos@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Andi Lutomirski  <luto@amacapital.net>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Greg KH <gregkh@linux-foundation.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Kees Cook <keescook@google.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801051525300.1724@nanos

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b1be494ab4e8..2d3bd2215e5b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -900,7 +900,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 
 	if (c->x86_vendor != X86_VENDOR_AMD)
-		setup_force_cpu_bug(X86_BUG_CPU_INSECURE);
+		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 
 	fpu__init_system(c);
 

commit 00a5ae218d57741088068799b810416ac249a9ce
Merge: d6bbd51587ec 2fd9c41aea47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 3 16:41:07 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 page table isolation fixes from Thomas Gleixner:
     "A couple of urgent fixes for PTI:
    
       - Fix a PTE mismatch between user and kernel visible mapping of the
         cpu entry area (differs vs. the GLB bit) and causes a TLB mismatch
         MCE on older AMD K8 machines
    
       - Fix the misplaced CR3 switch in the SYSCALL compat entry code which
         causes access to unmapped kernel memory resulting in double faults.
    
       - Fix the section mismatch of the cpu_tss_rw percpu storage caused by
         using a different mechanism for declaration and definition.
    
       - Two fixes for dumpstack which help to decode entry stack issues
         better
    
       - Enable PTI by default in Kconfig. We should have done that earlier,
         but it slipped through the cracks.
    
       - Exclude AMD from the PTI enforcement. Not necessarily a fix, but if
         AMD is so confident that they are not affected, then we should not
         burden users with the overhead"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/process: Define cpu_tss_rw in same section as declaration
      x86/pti: Switch to kernel CR3 at early in entry_SYSCALL_compat()
      x86/dumpstack: Print registers for first stack frame
      x86/dumpstack: Fix partial register dumps
      x86/pti: Make sure the user/kernel PTEs match
      x86/cpu, x86/pti: Do not enable PTI on AMD processors
      x86/pti: Enable PTI by default

commit 694d99d40972f12e59a3696effee8a376b79d7c8
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Tue Dec 26 23:43:54 2017 -0600

    x86/cpu, x86/pti: Do not enable PTI on AMD processors
    
    AMD processors are not subject to the types of attacks that the kernel
    page table isolation feature protects against.  The AMD microarchitecture
    does not allow memory references, including speculative references, that
    access higher privileged data when running in a lesser privileged mode
    when that access would result in a page fault.
    
    Disable page table isolation by default on AMD processors by not setting
    the X86_BUG_CPU_INSECURE feature, which controls whether X86_FEATURE_PTI
    is set.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20171227054354.20369.94587.stgit@tlendack-t1.amdoffice.net

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f2a94dfb434e..b1be494ab4e8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -899,8 +899,8 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 
-	/* Assume for now that ALL x86 CPUs are insecure */
-	setup_force_cpu_bug(X86_BUG_CPU_INSECURE);
+	if (c->x86_vendor != X86_VENDOR_AMD)
+		setup_force_cpu_bug(X86_BUG_CPU_INSECURE);
 
 	fpu__init_system(c);
 

commit 5aa90a84589282b87666f92b6c3c917c8080a9bf
Merge: 61233580f1f3 9f5cb6b32d9e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 29 17:02:49 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 page table isolation updates from Thomas Gleixner:
     "This is the final set of enabling page table isolation on x86:
    
       - Infrastructure patches for handling the extra page tables.
    
       - Patches which map the various bits and pieces which are required to
         get in and out of user space into the user space visible page
         tables.
    
       - The required changes to have CR3 switching in the entry/exit code.
    
       - Optimizations for the CR3 switching along with documentation how
         the ASID/PCID mechanism works.
    
       - Updates to dump pagetables to cover the user space page tables for
         W+X scans and extra debugfs files to analyze both the kernel and
         the user space visible page tables
    
      The whole functionality is compile time controlled via a config switch
      and can be turned on/off on the command line as well"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (32 commits)
      x86/ldt: Make the LDT mapping RO
      x86/mm/dump_pagetables: Allow dumping current pagetables
      x86/mm/dump_pagetables: Check user space page table for WX pages
      x86/mm/dump_pagetables: Add page table directory to the debugfs VFS hierarchy
      x86/mm/pti: Add Kconfig
      x86/dumpstack: Indicate in Oops whether PTI is configured and enabled
      x86/mm: Clarify the whole ASID/kernel PCID/user PCID naming
      x86/mm: Use INVPCID for __native_flush_tlb_single()
      x86/mm: Optimize RESTORE_CR3
      x86/mm: Use/Fix PCID to optimize user/kernel switches
      x86/mm: Abstract switching CR3
      x86/mm: Allow flushing for future ASID switches
      x86/pti: Map the vsyscall page if needed
      x86/pti: Put the LDT in its own PGD if PTI is on
      x86/mm/64: Make a full PGD-entry size hole in the memory map
      x86/events/intel/ds: Map debug buffers in cpu_entry_area
      x86/cpu_entry_area: Add debugstore entries to cpu_entry_area
      x86/mm/pti: Map ESPFIX into user space
      x86/mm/pti: Share entry text PMD
      x86/entry: Align entry text section to PMD boundary
      ...

commit 8d4b067895791ab9fdb1aadfc505f64d71239dd2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 4 15:07:43 2017 +0100

    x86/mm/pti: Force entry through trampoline when PTI active
    
    Force the entry through the trampoline only when PTI is active. Otherwise
    go through the normal entry code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a9210f9b7cf8..f2a94dfb434e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1339,7 +1339,10 @@ void syscall_init(void)
 		(entry_SYSCALL_64_trampoline - _entry_trampoline);
 
 	wrmsr(MSR_STAR, 0, (__USER32_CS << 16) | __KERNEL_CS);
-	wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);
+	if (static_cpu_has(X86_FEATURE_PTI))
+		wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);
+	else
+		wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, (unsigned long)entry_SYSCALL_compat);

commit a89f040fa34ec9cd682aed98b8f04e3c47d998bd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 4 15:07:33 2017 +0100

    x86/cpufeatures: Add X86_BUG_CPU_INSECURE
    
    Many x86 CPUs leak information to user space due to missing isolation of
    user space and kernel space page tables. There are many well documented
    ways to exploit that.
    
    The upcoming software migitation of isolating the user and kernel space
    page tables needs a misfeature flag so code can be made runtime
    conditional.
    
    Add the BUG bits which indicates that the CPU is affected and add a feature
    bit which indicates that the software migitation is enabled.
    
    Assume for now that _ALL_ x86 CPUs are affected by this. Exceptions can be
    made later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8ddcfa4d4165..a9210f9b7cf8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -898,6 +898,10 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	}
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
+
+	/* Assume for now that ALL x86 CPUs are insecure */
+	setup_force_cpu_bug(X86_BUG_CPU_INSECURE);
+
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32

commit caf9a82657b313106aae8f4a35936c116a152299
Merge: 9c294ec08408 f6c4fd506cb6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 23 11:53:04 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 PTI preparatory patches from Thomas Gleixner:
     "Todays Advent calendar window contains twentyfour easy to digest
      patches. The original plan was to have twenty three matching the date,
      but a late fixup made that moot.
    
       - Move the cpu_entry_area mapping out of the fixmap into a separate
         address space. That's necessary because the fixmap becomes too big
         with NRCPUS=8192 and this caused already subtle and hard to
         diagnose failures.
    
         The top most patch is fresh from today and cures a brain slip of
         that tall grumpy german greybeard, who ignored the intricacies of
         32bit wraparounds.
    
       - Limit the number of CPUs on 32bit to 64. That's insane big already,
         but at least it's small enough to prevent address space issues with
         the cpu_entry_area map, which have been observed and debugged with
         the fixmap code
    
       - A few TLB flush fixes in various places plus documentation which of
         the TLB functions should be used for what.
    
       - Rename the SYSENTER stack to CPU_ENTRY_AREA stack as it is used for
         more than sysenter now and keeping the name makes backtraces
         confusing.
    
       - Prevent LDT inheritance on exec() by moving it to arch_dup_mmap(),
         which is only invoked on fork().
    
       - Make vysycall more robust.
    
       - A few fixes and cleanups of the debug_pagetables code. Check
         PAGE_PRESENT instead of checking the PTE for 0 and a cleanup of the
         C89 initialization of the address hint array which already was out
         of sync with the index enums.
    
       - Move the ESPFIX init to a different place to prepare for PTI.
    
       - Several code moves with no functional change to make PTI
         integration simpler and header files less convoluted.
    
       - Documentation fixes and clarifications"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/cpu_entry_area: Prevent wraparound in setup_cpu_entry_area_ptes() on 32bit
      init: Invoke init_espfix_bsp() from mm_init()
      x86/cpu_entry_area: Move it out of the fixmap
      x86/cpu_entry_area: Move it to a separate unit
      x86/mm: Create asm/invpcid.h
      x86/mm: Put MMU to hardware ASID translation in one place
      x86/mm: Remove hard-coded ASID limit checks
      x86/mm: Move the CR3 construction functions to tlbflush.h
      x86/mm: Add comments to clarify which TLB-flush functions are supposed to flush what
      x86/mm: Remove superfluous barriers
      x86/mm: Use __flush_tlb_one() for kernel memory
      x86/microcode: Dont abuse the TLB-flush interface
      x86/uv: Use the right TLB-flush API
      x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
      x86/doc: Remove obvious weirdnesses from the x86 MM layout documentation
      x86/mm/64: Improve the memory map documentation
      x86/ldt: Prevent LDT inheritance on exec
      x86/ldt: Rework locking
      arch, mm: Allow arch_dup_mmap() to fail
      x86/vsyscall/64: Warn and fail vsyscall emulation in NATIVE mode
      ...

commit ed1bbc40a0d10e0c5c74fe7bdc6298295cf40255
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 20 18:28:54 2017 +0100

    x86/cpu_entry_area: Move it to a separate unit
    
    Separate the cpu_entry_area code out of cpu/common.c and the fixmap.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ed4acbce37a8..8ddcfa4d4165 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -482,102 +482,8 @@ static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {
 	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,
 	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
 };
-
-static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
-	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
-#endif
-
-static DEFINE_PER_CPU_PAGE_ALIGNED(struct entry_stack_page,
-				   entry_stack_storage);
-
-static void __init
-set_percpu_fixmap_pages(int idx, void *ptr, int pages, pgprot_t prot)
-{
-	for ( ; pages; pages--, idx--, ptr += PAGE_SIZE)
-		__set_fixmap(idx, per_cpu_ptr_to_phys(ptr), prot);
-}
-
-/* Setup the fixmap mappings only once per-processor */
-static void __init setup_cpu_entry_area(int cpu)
-{
-#ifdef CONFIG_X86_64
-	extern char _entry_trampoline[];
-
-	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */
-	pgprot_t gdt_prot = PAGE_KERNEL_RO;
-	pgprot_t tss_prot = PAGE_KERNEL_RO;
-#else
-	/*
-	 * On native 32-bit systems, the GDT cannot be read-only because
-	 * our double fault handler uses a task gate, and entering through
-	 * a task gate needs to change an available TSS to busy.  If the
-	 * GDT is read-only, that will triple fault.  The TSS cannot be
-	 * read-only because the CPU writes to it on task switches.
-	 *
-	 * On Xen PV, the GDT must be read-only because the hypervisor
-	 * requires it.
-	 */
-	pgprot_t gdt_prot = boot_cpu_has(X86_FEATURE_XENPV) ?
-		PAGE_KERNEL_RO : PAGE_KERNEL;
-	pgprot_t tss_prot = PAGE_KERNEL;
-#endif
-
-	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);
-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, entry_stack_page),
-				per_cpu_ptr(&entry_stack_storage, cpu), 1,
-				PAGE_KERNEL);
-
-	/*
-	 * The Intel SDM says (Volume 3, 7.2.1):
-	 *
-	 *  Avoid placing a page boundary in the part of the TSS that the
-	 *  processor reads during a task switch (the first 104 bytes). The
-	 *  processor may not correctly perform address translations if a
-	 *  boundary occurs in this area. During a task switch, the processor
-	 *  reads and writes into the first 104 bytes of each TSS (using
-	 *  contiguous physical addresses beginning with the physical address
-	 *  of the first byte of the TSS). So, after TSS access begins, if
-	 *  part of the 104 bytes is not physically contiguous, the processor
-	 *  will access incorrect information without generating a page-fault
-	 *  exception.
-	 *
-	 * There are also a lot of errata involving the TSS spanning a page
-	 * boundary.  Assert that we're not doing that.
-	 */
-	BUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^
-		      offsetofend(struct tss_struct, x86_tss)) & PAGE_MASK);
-	BUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);
-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, tss),
-				&per_cpu(cpu_tss_rw, cpu),
-				sizeof(struct tss_struct) / PAGE_SIZE,
-				tss_prot);
-
-#ifdef CONFIG_X86_32
-	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);
 #endif
 
-#ifdef CONFIG_X86_64
-	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);
-	BUILD_BUG_ON(sizeof(exception_stacks) !=
-		     sizeof(((struct cpu_entry_area *)0)->exception_stacks));
-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, exception_stacks),
-				&per_cpu(exception_stacks, cpu),
-				sizeof(exception_stacks) / PAGE_SIZE,
-				PAGE_KERNEL);
-
-	__set_fixmap(get_cpu_entry_area_index(cpu, entry_trampoline),
-		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);
-#endif
-}
-
-void __init setup_cpu_entry_areas(void)
-{
-	unsigned int cpu;
-
-	for_each_possible_cpu(cpu)
-		setup_cpu_entry_area(cpu);
-}
-
 /* Load the original GDT from the per-cpu structure */
 void load_direct_gdt(int cpu)
 {

commit 4fe2d8b11a370af286287a2661de9d4e6c9a145a
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Mon Dec 4 17:25:07 2017 -0800

    x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
    
    If the kernel oopses while on the trampoline stack, it will print
    "<SYSENTER>" even if SYSENTER is not involved.  That is rather confusing.
    
    The "SYSENTER" stack is used for a lot more than SYSENTER now.  Give it a
    better string to display in stack dumps, and rename the kernel code to
    match.
    
    Also move the 32-bit code over to the new naming even though it still uses
    the entry stack only for SYSENTER.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 034900623adf..ed4acbce37a8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -487,8 +487,8 @@ static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
 #endif
 
-static DEFINE_PER_CPU_PAGE_ALIGNED(struct SYSENTER_stack_page,
-				   SYSENTER_stack_storage);
+static DEFINE_PER_CPU_PAGE_ALIGNED(struct entry_stack_page,
+				   entry_stack_storage);
 
 static void __init
 set_percpu_fixmap_pages(int idx, void *ptr, int pages, pgprot_t prot)
@@ -523,8 +523,8 @@ static void __init setup_cpu_entry_area(int cpu)
 #endif
 
 	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);
-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, SYSENTER_stack_page),
-				per_cpu_ptr(&SYSENTER_stack_storage, cpu), 1,
+	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, entry_stack_page),
+				per_cpu_ptr(&entry_stack_storage, cpu), 1,
 				PAGE_KERNEL);
 
 	/*
@@ -1323,7 +1323,7 @@ void enable_sep_cpu(void)
 
 	tss->x86_tss.ss1 = __KERNEL_CS;
 	wrmsr(MSR_IA32_SYSENTER_CS, tss->x86_tss.ss1, 0);
-	wrmsr(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1), 0);
+	wrmsr(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_entry_stack(cpu) + 1), 0);
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
 
 	put_cpu();
@@ -1440,7 +1440,7 @@ void syscall_init(void)
 	 * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit).
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
-	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1));
+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_entry_stack(cpu) + 1));
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
 	wrmsrl(MSR_CSTAR, (unsigned long)ignore_sysret);
@@ -1655,7 +1655,7 @@ void cpu_init(void)
 	 */
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 	load_TR_desc();
-	load_sp0((unsigned long)(cpu_SYSENTER_stack(cpu) + 1));
+	load_sp0((unsigned long)(cpu_entry_stack(cpu) + 1));
 
 	load_mm_ldt(&init_mm);
 

commit 64a48099b3b31568ac45716b7fafcb74a0c2fcfe
Merge: 1291a0d5049d 6cbd2171e89b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 18 08:59:15 2017 -0800

    Merge branch 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 syscall entry code changes for PTI from Ingo Molnar:
     "The main changes here are Andy Lutomirski's changes to switch the
      x86-64 entry code to use the 'per CPU entry trampoline stack'. This,
      besides helping fix KASLR leaks (the pending Page Table Isolation
      (PTI) work), also robustifies the x86 entry code"
    
    * 'WIP.x86-pti.entry-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      x86/cpufeatures: Make CPU bugs sticky
      x86/paravirt: Provide a way to check for hypervisors
      x86/paravirt: Dont patch flush_tlb_single
      x86/entry/64: Make cpu_entry_area.tss read-only
      x86/entry: Clean up the SYSENTER_stack code
      x86/entry/64: Remove the SYSENTER stack canary
      x86/entry/64: Move the IST stacks into struct cpu_entry_area
      x86/entry/64: Create a per-CPU SYSCALL entry trampoline
      x86/entry/64: Return to userspace from the trampoline stack
      x86/entry/64: Use a per-CPU trampoline stack for IDT entries
      x86/espfix/64: Stop assuming that pt_regs is on the entry stack
      x86/entry/64: Separate cpu_current_top_of_stack from TSS.sp0
      x86/entry: Remap the TSS into the CPU entry area
      x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct
      x86/dumpstack: Handle stack overflow on all stacks
      x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
      x86/kasan/64: Teach KASAN about the cpu_entry_area
      x86/mm/fixmap: Generalize the GDT fixmap mechanism, introduce struct cpu_entry_area
      x86/entry/gdt: Put per-CPU GDT remaps in ascending order
      x86/dumpstack: Add get_stack_info() support for the SYSENTER stack
      ...

commit 6cbd2171e89b13377261d15e64384df60ecb530e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 4 15:07:32 2017 +0100

    x86/cpufeatures: Make CPU bugs sticky
    
    There is currently no way to force CPU bug bits like CPU feature bits. That
    makes it impossible to set a bug bit once at boot and have it stick for all
    upcoming CPUs.
    
    Extend the force set/clear arrays to handle bug bits as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.992156574@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c2eada1056de..034900623adf 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -452,8 +452,8 @@ static const char *table_lookup_model(struct cpuinfo_x86 *c)
 	return NULL;		/* Not found */
 }
 
-__u32 cpu_caps_cleared[NCAPINTS];
-__u32 cpu_caps_set[NCAPINTS];
+__u32 cpu_caps_cleared[NCAPINTS + NBUGINTS];
+__u32 cpu_caps_set[NCAPINTS + NBUGINTS];
 
 void load_percpu_segment(int cpu)
 {
@@ -812,7 +812,7 @@ static void apply_forced_caps(struct cpuinfo_x86 *c)
 {
 	int i;
 
-	for (i = 0; i < NCAPINTS; i++) {
+	for (i = 0; i < NCAPINTS + NBUGINTS; i++) {
 		c->x86_capability[i] &= ~cpu_caps_cleared[i];
 		c->x86_capability[i] |= cpu_caps_set[i];
 	}

commit c482feefe1aeb150156248ba0fd3e029bc886605
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:29 2017 +0100

    x86/entry/64: Make cpu_entry_area.tss read-only
    
    The TSS is a fairly juicy target for exploits, and, now that the TSS
    is in the cpu_entry_area, it's no longer protected by kASLR.  Make it
    read-only on x86_64.
    
    On x86_32, it can't be RO because it's written by the CPU during task
    switches, and we use a task gate for double faults.  I'd also be
    nervous about errata if we tried to make it RO even on configurations
    without double fault handling.
    
    [ tglx: AMD confirmed that there is no problem on 64-bit with TSS RO.  So
            it's probably safe to assume that it's a non issue, though Intel
            might have been creative in that area. Still waiting for
            confirmation. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.733700132@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3de7480e4f32..c2eada1056de 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -487,6 +487,9 @@ static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
 #endif
 
+static DEFINE_PER_CPU_PAGE_ALIGNED(struct SYSENTER_stack_page,
+				   SYSENTER_stack_storage);
+
 static void __init
 set_percpu_fixmap_pages(int idx, void *ptr, int pages, pgprot_t prot)
 {
@@ -500,23 +503,29 @@ static void __init setup_cpu_entry_area(int cpu)
 #ifdef CONFIG_X86_64
 	extern char _entry_trampoline[];
 
-	/* On 64-bit systems, we use a read-only fixmap GDT. */
+	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */
 	pgprot_t gdt_prot = PAGE_KERNEL_RO;
+	pgprot_t tss_prot = PAGE_KERNEL_RO;
 #else
 	/*
 	 * On native 32-bit systems, the GDT cannot be read-only because
 	 * our double fault handler uses a task gate, and entering through
-	 * a task gate needs to change an available TSS to busy.  If the GDT
-	 * is read-only, that will triple fault.
+	 * a task gate needs to change an available TSS to busy.  If the
+	 * GDT is read-only, that will triple fault.  The TSS cannot be
+	 * read-only because the CPU writes to it on task switches.
 	 *
-	 * On Xen PV, the GDT must be read-only because the hypervisor requires
-	 * it.
+	 * On Xen PV, the GDT must be read-only because the hypervisor
+	 * requires it.
 	 */
 	pgprot_t gdt_prot = boot_cpu_has(X86_FEATURE_XENPV) ?
 		PAGE_KERNEL_RO : PAGE_KERNEL;
+	pgprot_t tss_prot = PAGE_KERNEL;
 #endif
 
 	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);
+	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, SYSENTER_stack_page),
+				per_cpu_ptr(&SYSENTER_stack_storage, cpu), 1,
+				PAGE_KERNEL);
 
 	/*
 	 * The Intel SDM says (Volume 3, 7.2.1):
@@ -539,9 +548,9 @@ static void __init setup_cpu_entry_area(int cpu)
 		      offsetofend(struct tss_struct, x86_tss)) & PAGE_MASK);
 	BUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);
 	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, tss),
-				&per_cpu(cpu_tss, cpu),
+				&per_cpu(cpu_tss_rw, cpu),
 				sizeof(struct tss_struct) / PAGE_SIZE,
-				PAGE_KERNEL);
+				tss_prot);
 
 #ifdef CONFIG_X86_32
 	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);
@@ -1305,7 +1314,7 @@ void enable_sep_cpu(void)
 		return;
 
 	cpu = get_cpu();
-	tss = &per_cpu(cpu_tss, cpu);
+	tss = &per_cpu(cpu_tss_rw, cpu);
 
 	/*
 	 * We cache MSR_IA32_SYSENTER_CS's value in the TSS's ss1 field --
@@ -1575,7 +1584,7 @@ void cpu_init(void)
 	if (cpu)
 		load_ucode_ap();
 
-	t = &per_cpu(cpu_tss, cpu);
+	t = &per_cpu(cpu_tss_rw, cpu);
 	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
@@ -1667,7 +1676,7 @@ void cpu_init(void)
 {
 	int cpu = smp_processor_id();
 	struct task_struct *curr = current;
-	struct tss_struct *t = &per_cpu(cpu_tss, cpu);
+	struct tss_struct *t = &per_cpu(cpu_tss_rw, cpu);
 
 	wait_for_master_cpu(cpu);
 

commit 0f9a48100fba3f189724ae88a450c2261bf91c80
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:28 2017 +0100

    x86/entry: Clean up the SYSENTER_stack code
    
    The existing code was a mess, mainly because C arrays are nasty.  Turn
    SYSENTER_stack into a struct, add a helper to find it, and do all the
    obvious cleanups this enables.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.653244723@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fb01a8e5e9b7..3de7480e4f32 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1314,12 +1314,7 @@ void enable_sep_cpu(void)
 
 	tss->x86_tss.ss1 = __KERNEL_CS;
 	wrmsr(MSR_IA32_SYSENTER_CS, tss->x86_tss.ss1, 0);
-
-	wrmsr(MSR_IA32_SYSENTER_ESP,
-	      (unsigned long)&get_cpu_entry_area(cpu)->tss +
-	      offsetofend(struct tss_struct, SYSENTER_stack),
-	      0);
-
+	wrmsr(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1), 0);
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
 
 	put_cpu();
@@ -1436,9 +1431,7 @@ void syscall_init(void)
 	 * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit).
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
-	wrmsrl_safe(MSR_IA32_SYSENTER_ESP,
-		    (unsigned long)&get_cpu_entry_area(cpu)->tss +
-		    offsetofend(struct tss_struct, SYSENTER_stack));
+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1));
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
 	wrmsrl(MSR_CSTAR, (unsigned long)ignore_sysret);
@@ -1653,8 +1646,7 @@ void cpu_init(void)
 	 */
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 	load_TR_desc();
-	load_sp0((unsigned long)&get_cpu_entry_area(cpu)->tss +
-		 offsetofend(struct tss_struct, SYSENTER_stack));
+	load_sp0((unsigned long)(cpu_SYSENTER_stack(cpu) + 1));
 
 	load_mm_ldt(&init_mm);
 

commit 40e7f949e0d9a33968ebde5d67f7e3a47c97742a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:26 2017 +0100

    x86/entry/64: Move the IST stacks into struct cpu_entry_area
    
    The IST stacks are needed when an IST exception occurs and are accessed
    before any kernel code at all runs.  Move them into struct cpu_entry_area.
    
    The IST stacks are unlike the rest of cpu_entry_area: they're used even for
    entries from kernel mode.  This means that they should be set up before we
    load the final IDT.  Move cpu_entry_area setup to trap_init() for the boot
    CPU and set it up for all possible CPUs at once in native_smp_prepare_cpus().
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.480598743@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 430f950b0b7f..fb01a8e5e9b7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -466,24 +466,36 @@ void load_percpu_segment(int cpu)
 	load_stack_canary_segment();
 }
 
-static void set_percpu_fixmap_pages(int fixmap_index, void *ptr,
-				    int pages, pgprot_t prot)
-{
-	int i;
-
-	for (i = 0; i < pages; i++) {
-		__set_fixmap(fixmap_index - i,
-			     per_cpu_ptr_to_phys(ptr + i * PAGE_SIZE), prot);
-	}
-}
-
 #ifdef CONFIG_X86_32
 /* The 32-bit entry code needs to find cpu_entry_area. */
 DEFINE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);
 #endif
 
+#ifdef CONFIG_X86_64
+/*
+ * Special IST stacks which the CPU switches to when it calls
+ * an IST-marked descriptor entry. Up to 7 stacks (hardware
+ * limit), all of them are 4K, except the debug stack which
+ * is 8K.
+ */
+static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {
+	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,
+	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
+};
+
+static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
+	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
+#endif
+
+static void __init
+set_percpu_fixmap_pages(int idx, void *ptr, int pages, pgprot_t prot)
+{
+	for ( ; pages; pages--, idx--, ptr += PAGE_SIZE)
+		__set_fixmap(idx, per_cpu_ptr_to_phys(ptr), prot);
+}
+
 /* Setup the fixmap mappings only once per-processor */
-static inline void setup_cpu_entry_area(int cpu)
+static void __init setup_cpu_entry_area(int cpu)
 {
 #ifdef CONFIG_X86_64
 	extern char _entry_trampoline[];
@@ -532,15 +544,31 @@ static inline void setup_cpu_entry_area(int cpu)
 				PAGE_KERNEL);
 
 #ifdef CONFIG_X86_32
-	this_cpu_write(cpu_entry_area, get_cpu_entry_area(cpu));
+	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);
 #endif
 
 #ifdef CONFIG_X86_64
+	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);
+	BUILD_BUG_ON(sizeof(exception_stacks) !=
+		     sizeof(((struct cpu_entry_area *)0)->exception_stacks));
+	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, exception_stacks),
+				&per_cpu(exception_stacks, cpu),
+				sizeof(exception_stacks) / PAGE_SIZE,
+				PAGE_KERNEL);
+
 	__set_fixmap(get_cpu_entry_area_index(cpu, entry_trampoline),
 		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);
 #endif
 }
 
+void __init setup_cpu_entry_areas(void)
+{
+	unsigned int cpu;
+
+	for_each_possible_cpu(cpu)
+		setup_cpu_entry_area(cpu);
+}
+
 /* Load the original GDT from the per-cpu structure */
 void load_direct_gdt(int cpu)
 {
@@ -1385,20 +1413,6 @@ DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
 EXPORT_PER_CPU_SYMBOL(__preempt_count);
 
-/*
- * Special IST stacks which the CPU switches to when it calls
- * an IST-marked descriptor entry. Up to 7 stacks (hardware
- * limit), all of them are 4K, except the debug stack which
- * is 8K.
- */
-static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {
-	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,
-	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
-};
-
-static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
-	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
-
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
@@ -1607,7 +1621,7 @@ void cpu_init(void)
 	 * set up and load the per-CPU TSS
 	 */
 	if (!oist->ist[0]) {
-		char *estacks = per_cpu(exception_stacks, cpu);
+		char *estacks = get_cpu_entry_area(cpu)->exception_stacks;
 
 		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
 			estacks += exception_stack_sizes[v];
@@ -1633,8 +1647,6 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, me);
 
-	setup_cpu_entry_area(cpu);
-
 	/*
 	 * Initialize the TSS.  sp0 points to the entry trampoline stack
 	 * regardless of what task is running.
@@ -1694,8 +1706,6 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, curr);
 
-	setup_cpu_entry_area(cpu);
-
 	/*
 	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
 	 * task never enters user mode.

commit 3386bc8aed825e9f1f65ce38df4b109b2019b71a
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:25 2017 +0100

    x86/entry/64: Create a per-CPU SYSCALL entry trampoline
    
    Handling SYSCALL is tricky: the SYSCALL handler is entered with every
    single register (except FLAGS), including RSP, live.  It somehow needs
    to set RSP to point to a valid stack, which means it needs to save the
    user RSP somewhere and find its own stack pointer.  The canonical way
    to do this is with SWAPGS, which lets us access percpu data using the
    %gs prefix.
    
    With PAGE_TABLE_ISOLATION-like pagetable switching, this is
    problematic.  Without a scratch register, switching CR3 is impossible, so
    %gs-based percpu memory would need to be mapped in the user pagetables.
    Doing that without information leaks is difficult or impossible.
    
    Instead, use a different sneaky trick.  Map a copy of the first part
    of the SYSCALL asm at a different address for each CPU.  Now RIP
    varies depending on the CPU, so we can use RIP-relative memory access
    to access percpu memory.  By putting the relevant information (one
    scratch slot and the stack address) at a constant offset relative to
    RIP, we can make SYSCALL work without relying on %gs.
    
    A nice thing about this approach is that we can easily switch it on
    and off if we want pagetable switching to be configurable.
    
    The compat variant of SYSCALL doesn't have this problem in the first
    place -- there are plenty of scratch registers, since we don't care
    about preserving r8-r15.  This patch therefore doesn't touch SYSCALL32
    at all.
    
    This patch actually seems to be a small speedup.  With this patch,
    SYSCALL touches an extra cache line and an extra virtual page, but
    the pipeline no longer stalls waiting for SWAPGS.  It seems that, at
    least in a tight loop, the latter outweights the former.
    
    Thanks to David Laight for an optimization tip.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.403607157@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 57968880e39b..430f950b0b7f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -486,6 +486,8 @@ DEFINE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);
 static inline void setup_cpu_entry_area(int cpu)
 {
 #ifdef CONFIG_X86_64
+	extern char _entry_trampoline[];
+
 	/* On 64-bit systems, we use a read-only fixmap GDT. */
 	pgprot_t gdt_prot = PAGE_KERNEL_RO;
 #else
@@ -532,6 +534,11 @@ static inline void setup_cpu_entry_area(int cpu)
 #ifdef CONFIG_X86_32
 	this_cpu_write(cpu_entry_area, get_cpu_entry_area(cpu));
 #endif
+
+#ifdef CONFIG_X86_64
+	__set_fixmap(get_cpu_entry_area_index(cpu, entry_trampoline),
+		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);
+#endif
 }
 
 /* Load the original GDT from the per-cpu structure */
@@ -1395,10 +1402,16 @@ static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
+	extern char _entry_trampoline[];
+	extern char entry_SYSCALL_64_trampoline[];
+
 	int cpu = smp_processor_id();
+	unsigned long SYSCALL64_entry_trampoline =
+		(unsigned long)get_cpu_entry_area(cpu)->entry_trampoline +
+		(entry_SYSCALL_64_trampoline - _entry_trampoline);
 
 	wrmsr(MSR_STAR, 0, (__USER32_CS << 16) | __KERNEL_CS);
-	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
+	wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);
 
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, (unsigned long)entry_SYSCALL_compat);

commit 7f2590a110b837af5679d08fc25c6227c5a8c497
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:23 2017 +0100

    x86/entry/64: Use a per-CPU trampoline stack for IDT entries
    
    Historically, IDT entries from usermode have always gone directly
    to the running task's kernel stack.  Rearrange it so that we enter on
    a per-CPU trampoline stack and then manually switch to the task's stack.
    This touches a couple of extra cachelines, but it gives us a chance
    to run some code before we touch the kernel stack.
    
    The asm isn't exactly beautiful, but I think that fully refactoring
    it can wait.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150606.225330557@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e5837bd6c672..57968880e39b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1623,11 +1623,13 @@ void cpu_init(void)
 	setup_cpu_entry_area(cpu);
 
 	/*
-	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
-	 * task never enters user mode.
+	 * Initialize the TSS.  sp0 points to the entry trampoline stack
+	 * regardless of what task is running.
 	 */
 	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 	load_TR_desc();
+	load_sp0((unsigned long)&get_cpu_entry_area(cpu)->tss +
+		 offsetofend(struct tss_struct, SYSENTER_stack));
 
 	load_mm_ldt(&init_mm);
 

commit 72f5e08dbba2d01aa90b592cf76c378ea233b00b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:20 2017 +0100

    x86/entry: Remap the TSS into the CPU entry area
    
    This has a secondary purpose: it puts the entry stack into a region
    with a well-controlled layout.  A subsequent patch will take
    advantage of this to streamline the SYSCALL entry code to be able to
    find it more easily.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bpetkov@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.962042855@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 60b2dfd2a58b..e5837bd6c672 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -466,6 +466,22 @@ void load_percpu_segment(int cpu)
 	load_stack_canary_segment();
 }
 
+static void set_percpu_fixmap_pages(int fixmap_index, void *ptr,
+				    int pages, pgprot_t prot)
+{
+	int i;
+
+	for (i = 0; i < pages; i++) {
+		__set_fixmap(fixmap_index - i,
+			     per_cpu_ptr_to_phys(ptr + i * PAGE_SIZE), prot);
+	}
+}
+
+#ifdef CONFIG_X86_32
+/* The 32-bit entry code needs to find cpu_entry_area. */
+DEFINE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);
+#endif
+
 /* Setup the fixmap mappings only once per-processor */
 static inline void setup_cpu_entry_area(int cpu)
 {
@@ -507,7 +523,15 @@ static inline void setup_cpu_entry_area(int cpu)
 	 */
 	BUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^
 		      offsetofend(struct tss_struct, x86_tss)) & PAGE_MASK);
+	BUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);
+	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, tss),
+				&per_cpu(cpu_tss, cpu),
+				sizeof(struct tss_struct) / PAGE_SIZE,
+				PAGE_KERNEL);
 
+#ifdef CONFIG_X86_32
+	this_cpu_write(cpu_entry_area, get_cpu_entry_area(cpu));
+#endif
 }
 
 /* Load the original GDT from the per-cpu structure */
@@ -1257,7 +1281,8 @@ void enable_sep_cpu(void)
 	wrmsr(MSR_IA32_SYSENTER_CS, tss->x86_tss.ss1, 0);
 
 	wrmsr(MSR_IA32_SYSENTER_ESP,
-	      (unsigned long)tss + offsetofend(struct tss_struct, SYSENTER_stack),
+	      (unsigned long)&get_cpu_entry_area(cpu)->tss +
+	      offsetofend(struct tss_struct, SYSENTER_stack),
 	      0);
 
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
@@ -1370,6 +1395,8 @@ static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
+	int cpu = smp_processor_id();
+
 	wrmsr(MSR_STAR, 0, (__USER32_CS << 16) | __KERNEL_CS);
 	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 
@@ -1383,7 +1410,7 @@ void syscall_init(void)
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP,
-		    (unsigned long)this_cpu_ptr(&cpu_tss) +
+		    (unsigned long)&get_cpu_entry_area(cpu)->tss +
 		    offsetofend(struct tss_struct, SYSENTER_stack));
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
@@ -1593,11 +1620,13 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, me);
 
+	setup_cpu_entry_area(cpu);
+
 	/*
 	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
 	 * task never enters user mode.
 	 */
-	set_tss_desc(cpu, &t->x86_tss);
+	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 	load_TR_desc();
 
 	load_mm_ldt(&init_mm);
@@ -1610,7 +1639,6 @@ void cpu_init(void)
 	if (is_uv_system())
 		uv_cpu_init();
 
-	setup_cpu_entry_area(cpu);
 	load_fixmap_gdt(cpu);
 }
 
@@ -1651,11 +1679,13 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, curr);
 
+	setup_cpu_entry_area(cpu);
+
 	/*
 	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
 	 * task never enters user mode.
 	 */
-	set_tss_desc(cpu, &t->x86_tss);
+	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 	load_TR_desc();
 
 	load_mm_ldt(&init_mm);
@@ -1672,7 +1702,6 @@ void cpu_init(void)
 
 	fpu__init_cpu();
 
-	setup_cpu_entry_area(cpu);
 	load_fixmap_gdt(cpu);
 }
 #endif

commit 1a935bc3d4ea61556461a9e92a68ca3556232efd
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:19 2017 +0100

    x86/entry: Move SYSENTER_stack to the beginning of struct tss_struct
    
    SYSENTER_stack should have reliable overflow detection, which
    means that it needs to be at the bottom of a page, not the top.
    Move it to the beginning of struct tss_struct and page-align it.
    
    Also add an assertion to make sure that the fixed hardware TSS
    doesn't cross a page boundary.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.881827433@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3f285b973f50..60b2dfd2a58b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -487,6 +487,27 @@ static inline void setup_cpu_entry_area(int cpu)
 #endif
 
 	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);
+
+	/*
+	 * The Intel SDM says (Volume 3, 7.2.1):
+	 *
+	 *  Avoid placing a page boundary in the part of the TSS that the
+	 *  processor reads during a task switch (the first 104 bytes). The
+	 *  processor may not correctly perform address translations if a
+	 *  boundary occurs in this area. During a task switch, the processor
+	 *  reads and writes into the first 104 bytes of each TSS (using
+	 *  contiguous physical addresses beginning with the physical address
+	 *  of the first byte of the TSS). So, after TSS access begins, if
+	 *  part of the 104 bytes is not physically contiguous, the processor
+	 *  will access incorrect information without generating a page-fault
+	 *  exception.
+	 *
+	 * There are also a lot of errata involving the TSS spanning a page
+	 * boundary.  Assert that we're not doing that.
+	 */
+	BUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^
+		      offsetofend(struct tss_struct, x86_tss)) & PAGE_MASK);
+
 }
 
 /* Load the original GDT from the per-cpu structure */

commit 7fb983b4dd569e08564134a850dfd4eb1c63d9b8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:17 2017 +0100

    x86/entry: Fix assumptions that the HW TSS is at the beginning of cpu_tss
    
    A future patch will move SYSENTER_stack to the beginning of cpu_tss
    to help detect overflow.  Before this can happen, fix several code
    paths that hardcode assumptions about the old layout.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Dave Hansen <dave.hansen@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.722425540@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2cb394dc4153..3f285b973f50 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1557,7 +1557,7 @@ void cpu_init(void)
 		}
 	}
 
-	t->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
+	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
 
 	/*
 	 * <= is required because the CPU will access up to
@@ -1576,7 +1576,7 @@ void cpu_init(void)
 	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
 	 * task never enters user mode.
 	 */
-	set_tss_desc(cpu, t);
+	set_tss_desc(cpu, &t->x86_tss);
 	load_TR_desc();
 
 	load_mm_ldt(&init_mm);
@@ -1634,12 +1634,12 @@ void cpu_init(void)
 	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
 	 * task never enters user mode.
 	 */
-	set_tss_desc(cpu, t);
+	set_tss_desc(cpu, &t->x86_tss);
 	load_TR_desc();
 
 	load_mm_ldt(&init_mm);
 
-	t->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
+	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
 
 #ifdef CONFIG_DOUBLEFAULT
 	/* Set up doublefault TSS pointer in the GDT */

commit ef8813ab280507972bb57e4b1b502811ad4411e9
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:15 2017 +0100

    x86/mm/fixmap: Generalize the GDT fixmap mechanism, introduce struct cpu_entry_area
    
    Currently, the GDT is an ad-hoc array of pages, one per CPU, in the
    fixmap.  Generalize it to be an array of a new 'struct cpu_entry_area'
    so that we can cleanly add new things to it.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.563271721@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 22f542170198..2cb394dc4153 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -466,12 +466,12 @@ void load_percpu_segment(int cpu)
 	load_stack_canary_segment();
 }
 
-/* Setup the fixmap mapping only once per-processor */
-static inline void setup_fixmap_gdt(int cpu)
+/* Setup the fixmap mappings only once per-processor */
+static inline void setup_cpu_entry_area(int cpu)
 {
 #ifdef CONFIG_X86_64
 	/* On 64-bit systems, we use a read-only fixmap GDT. */
-	pgprot_t prot = PAGE_KERNEL_RO;
+	pgprot_t gdt_prot = PAGE_KERNEL_RO;
 #else
 	/*
 	 * On native 32-bit systems, the GDT cannot be read-only because
@@ -482,11 +482,11 @@ static inline void setup_fixmap_gdt(int cpu)
 	 * On Xen PV, the GDT must be read-only because the hypervisor requires
 	 * it.
 	 */
-	pgprot_t prot = boot_cpu_has(X86_FEATURE_XENPV) ?
+	pgprot_t gdt_prot = boot_cpu_has(X86_FEATURE_XENPV) ?
 		PAGE_KERNEL_RO : PAGE_KERNEL;
 #endif
 
-	__set_fixmap(get_cpu_gdt_ro_index(cpu), get_cpu_gdt_paddr(cpu), prot);
+	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);
 }
 
 /* Load the original GDT from the per-cpu structure */
@@ -1589,7 +1589,7 @@ void cpu_init(void)
 	if (is_uv_system())
 		uv_cpu_init();
 
-	setup_fixmap_gdt(cpu);
+	setup_cpu_entry_area(cpu);
 	load_fixmap_gdt(cpu);
 }
 
@@ -1651,7 +1651,7 @@ void cpu_init(void)
 
 	fpu__init_cpu();
 
-	setup_fixmap_gdt(cpu);
+	setup_cpu_entry_area(cpu);
 	load_fixmap_gdt(cpu);
 }
 #endif

commit 1a79797b58cddfa948420a7553241c79c013e3ca
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Dec 4 15:07:12 2017 +0100

    x86/entry/64: Allocate and enable the SYSENTER stack
    
    This will simplify future changes that want scratch variables early in
    the SYSENTER handler -- they'll be able to spill registers to the
    stack.  It also lets us get rid of a SWAPGS_UNSAFE_STACK user.
    
    This does not depend on CONFIG_IA32_EMULATION=y because we'll want the
    stack space even without IA32 emulation.
    
    As far as I can tell, the reason that this wasn't done from day 1 is
    that we use IST for #DB and #BP, which is IMO rather nasty and causes
    a lot more problems than it solves.  But, since #DB uses IST, we don't
    actually need a real stack for SYSENTER (because SYSENTER with TF set
    will invoke #DB on the IST stack rather than the SYSENTER stack).
    
    I want to remove IST usage from these vectors some day, and this patch
    is a prerequisite for that as well.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: daniel.gruss@iaik.tugraz.at
    Cc: hughd@google.com
    Cc: keescook@google.com
    Link: https://lkml.kernel.org/r/20171204150605.312726423@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cdf79ab628c2..22f542170198 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1361,7 +1361,9 @@ void syscall_init(void)
 	 * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit).
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
-	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP,
+		    (unsigned long)this_cpu_ptr(&cpu_tss) +
+		    offsetofend(struct tss_struct, SYSENTER_stack));
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
 	wrmsrl(MSR_CSTAR, (unsigned long)ignore_sysret);

commit 770c77557757873808a474016a3cae4b37690cb2
Author: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Date:   Mon Nov 13 22:29:43 2017 -0800

    x86/umip: Print a line in the boot log that UMIP has been enabled
    
    Indicate that this feature has been enabled.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi V. Shankar <ravi.v.shankar@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: ricardo.neri@intel.com
    Link: http://lkml.kernel.org/r/1510640985-18412-3-git-send-email-ricardo.neri-calderon@linux.intel.com
    [ Changelog tweaks. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 13ae9e5eec2f..fa998ca8aa5a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -341,6 +341,8 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 
 	cr4_set_bits(X86_CR4_UMIP);
 
+	pr_info("x86/cpu: Activated the Intel User Mode Instruction Prevention (UMIP) CPU feature\n");
+
 	return;
 
 out:

commit 6a9f70b0a5b3ca5db1dd5c7743ca555bfca2ae08
Merge: d6ec9d9a4def 6c3b56b19730
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 16:32:30 2017 -0800

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
     "Three smaller changes:
    
       - clang fix
    
       - boot message beautification
    
       - unnecessary header inclusion removal"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/boot: Disable Clang warnings about GNU extensions
      x86/boot: Remove unnecessary #include <generated/utsrelease.h>
      x86/boot: Spell out "boot CPU" for BP

commit aa35f896979d9610bb11df485cf7bb6ca241febb
Author: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Date:   Sun Nov 5 18:27:54 2017 -0800

    x86/umip: Enable User-Mode Instruction Prevention at runtime
    
    User-Mode Instruction Prevention (UMIP) is enabled by setting/clearing a
    bit in %cr4.
    
    It makes sense to enable UMIP at some point while booting, before user
    spaces come up. Like SMAP and SMEP, is not critical to have it enabled
    very early during boot. This is because UMIP is relevant only when there is
    a user space to be protected from. Given these similarities, UMIP can be
    enabled along with SMAP and SMEP.
    
    At the moment, UMIP is disabled by default at build time. It can be enabled
    at build time by selecting CONFIG_X86_INTEL_UMIP. If enabled at build time,
    it can be disabled at run time by adding clearcpuid=514 to the kernel
    parameters.
    
    Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chen Yucong <slaoub@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ravi V. Shankar <ravi.v.shankar@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: ricardo.neri@intel.com
    Link: http://lkml.kernel.org/r/1509935277-22138-10-git-send-email-ricardo.neri-calderon@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cdf79ab628c2..47f8a85be11c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -329,6 +329,28 @@ static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 	}
 }
 
+static __always_inline void setup_umip(struct cpuinfo_x86 *c)
+{
+	/* Check the boot processor, plus build option for UMIP. */
+	if (!cpu_feature_enabled(X86_FEATURE_UMIP))
+		goto out;
+
+	/* Check the current processor's cpuid bits. */
+	if (!cpu_has(c, X86_FEATURE_UMIP))
+		goto out;
+
+	cr4_set_bits(X86_CR4_UMIP);
+
+	return;
+
+out:
+	/*
+	 * Make sure UMIP is disabled in case it was enabled in a
+	 * previous boot (e.g., via kexec).
+	 */
+	cr4_clear_bits(X86_CR4_UMIP);
+}
+
 /*
  * Protection Keys are not available in 32-bit mode.
  */
@@ -1147,9 +1169,10 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	/* Disable the PN if appropriate */
 	squash_the_stupid_serial_number(c);
 
-	/* Set up SMEP/SMAP */
+	/* Set up SMEP/SMAP/UMIP */
 	setup_smep(c);
 	setup_smap(c);
+	setup_umip(c);
 
 	/*
 	 * The vendor-specific functions might have changed features.

commit 20bb83443ea79087b5e5f8dab4e9d80bb9bf7acb
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:13 2017 -0700

    x86/entry/64: Stop initializing TSS.sp0 at boot
    
    In my quest to get rid of thread_struct::sp0, I want to clean up or
    remove all of its readers.  Two of them are in cpu_init() (32-bit and
    64-bit), and they aren't needed.  This is because we never enter
    userspace at all on the threads that CPUs are initialized in.
    
    Poison the initial TSS.sp0 and stop initializing it on CPU init.
    
    The comment text mostly comes from Dave Hansen.  Thanks!
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/ee4a00540ad28c6cff475fbcc7769a4460acc861.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4e7fb9c3bfa5..cdf79ab628c2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1570,9 +1570,13 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, me);
 
-	load_sp0(current->thread.sp0);
+	/*
+	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
+	 * task never enters user mode.
+	 */
 	set_tss_desc(cpu, t);
 	load_TR_desc();
+
 	load_mm_ldt(&init_mm);
 
 	clear_all_debug_regs();
@@ -1594,7 +1598,6 @@ void cpu_init(void)
 	int cpu = smp_processor_id();
 	struct task_struct *curr = current;
 	struct tss_struct *t = &per_cpu(cpu_tss, cpu);
-	struct thread_struct *thread = &curr->thread;
 
 	wait_for_master_cpu(cpu);
 
@@ -1625,9 +1628,13 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, curr);
 
-	load_sp0(thread->sp0);
+	/*
+	 * Initialize the TSS.  Don't bother initializing sp0, as the initial
+	 * task never enters user mode.
+	 */
 	set_tss_desc(cpu, t);
 	load_TR_desc();
+
 	load_mm_ldt(&init_mm);
 
 	t->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);

commit da51da189a24bb9b7e2d5a123be096e51a4695a5
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 2 00:59:10 2017 -0700

    x86/entry/64: Pass SP0 directly to load_sp0()
    
    load_sp0() had an odd signature:
    
      void load_sp0(struct tss_struct *tss, struct thread_struct *thread);
    
    Simplify it to:
    
      void load_sp0(unsigned long sp0);
    
    Also simplify a few get_cpu()/put_cpu() sequences to
    preempt_disable()/preempt_enable().
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/2655d8b42ed940aa384fe18ee1129bbbcf730a08.1509609304.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 03bb004bb15e..4e7fb9c3bfa5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1570,7 +1570,7 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, me);
 
-	load_sp0(t, &current->thread);
+	load_sp0(current->thread.sp0);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
 	load_mm_ldt(&init_mm);
@@ -1625,7 +1625,7 @@ void cpu_init(void)
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, curr);
 
-	load_sp0(t, thread);
+	load_sp0(thread->sp0);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
 	load_mm_ldt(&init_mm);

commit 0c2a3913d6f50503f7c59d83a6219e39508cc898
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Oct 13 14:56:43 2017 -0700

    x86/fpu: Parse clearcpuid= as early XSAVE argument
    
    With a followon patch we want to make clearcpuid affect the XSAVE
    configuration. But xsave is currently initialized before arguments
    are parsed. Move the clearcpuid= parsing into the special
    early xsave argument parsing code.
    
    Since clearcpuid= contains a = we need to keep the old __setup
    around as a dummy, otherwise it would end up as a environment
    variable in init's environment.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20171013215645.23166-4-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c9176bae7fd8..03bb004bb15e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1301,18 +1301,16 @@ void print_cpu_info(struct cpuinfo_x86 *c)
 		pr_cont(")\n");
 }
 
-static __init int setup_disablecpuid(char *arg)
+/*
+ * clearcpuid= was already parsed in fpu__init_parse_early_param.
+ * But we need to keep a dummy __setup around otherwise it would
+ * show up as an environment variable for init.
+ */
+static __init int setup_clearcpuid(char *arg)
 {
-	int bit;
-
-	if (get_option(&arg, &bit) && bit >= 0 && bit < NCAPINTS * 32)
-		setup_clear_cpu_cap(bit);
-	else
-		return 0;
-
 	return 1;
 }
-__setup("clearcpuid=", setup_disablecpuid);
+__setup("clearcpuid=", setup_clearcpuid);
 
 #ifdef CONFIG_X86_64
 DEFINE_PER_CPU_FIRST(union irq_stack_union,

commit a1652bb8a01c1a830cacbe958aec17f880cc1e47
Author: Jean Delvare <jdelvare@suse.de>
Date:   Tue Oct 3 11:47:27 2017 +0200

    x86/boot: Spell out "boot CPU" for BP
    
    It's not obvious to everybody that BP stands for boot processor. At
    least it was not for me. And BP is also a CPU register on x86, so it
    is ambiguous. Spell out "boot CPU" everywhere instead.
    
    Signed-off-by: Jean Delvare <jdelvare@suse.de>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c9176bae7fd8..03f9a1a8a314 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -863,8 +863,8 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
  * cache alignment.
  * The others are not touched to avoid unwanted side effects.
  *
- * WARNING: this function is only called on the BP.  Don't add code here
- * that is supposed to run on all CPUs.
+ * WARNING: this function is only called on the boot CPU.  Don't add code
+ * here that is supposed to run on all CPUs.
  */
 static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 {

commit b8b7abaed7a49b350f8ba659ddc264b04931d581
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Sep 17 09:03:50 2017 -0700

    x86/mm/32: Move setup_clear_cpu_cap(X86_FEATURE_PCID) earlier
    
    Otherwise we might have the PCID feature bit set during cpu_init().
    
    This is just for robustness.  I haven't seen any actual bugs here.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: cba4671af755 ("x86/mm: Disable PCID on 32-bit kernels")
    Link: http://lkml.kernel.org/r/b16dae9d6b0db5d9801ddbebbfd83384097c61f3.1505663533.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 775f10100d7f..c9176bae7fd8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -904,6 +904,14 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 	fpu__init_system(c);
+
+#ifdef CONFIG_X86_32
+	/*
+	 * Regardless of whether PCID is enumerated, the SDM says
+	 * that it can't be enabled in 32-bit mode.
+	 */
+	setup_clear_cpu_cap(X86_FEATURE_PCID);
+#endif
 }
 
 void __init early_cpu_init(void)

commit c7ad5ad297e644601747d6dbee978bf85e14f7bc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Sep 10 17:48:27 2017 -0700

    x86/mm/64: Initialize CR4.PCIDE early
    
    cpu_init() is weird: it's called rather late (after early
    identification and after most MMU state is initialized) on the boot
    CPU but is called extremely early (before identification) on secondary
    CPUs.  It's called just late enough on the boot CPU that its CR4 value
    isn't propagated to mmu_cr4_features.
    
    Even if we put CR4.PCIDE into mmu_cr4_features, we'd hit two
    problems.  First, we'd crash in the trampoline code.  That's
    fixable, and I tried that.  It turns out that mmu_cr4_features is
    totally ignored by secondary_start_64(), though, so even with the
    trampoline code fixed, it wouldn't help.
    
    This means that we don't currently have CR4.PCIDE reliably initialized
    before we start playing with cpu_tlbstate.  This is very fragile and
    tends to cause boot failures if I make even small changes to the TLB
    handling code.
    
    Make it more robust: initialize CR4.PCIDE earlier on the boot CPU
    and propagate it to secondary CPUs in start_secondary().
    
    ( Yes, this is ugly.  I think we should have improved mmu_cr4_features
      to actually control CR4 during secondary bootup, but that would be
      fairly intrusive at this stage. )
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reported-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Tested-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Fixes: 660da7c9228f ("x86/mm: Enable CR4.PCIDE on supported systems")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fb1d3358a4af..775f10100d7f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -169,21 +169,21 @@ static int __init x86_mpx_setup(char *s)
 __setup("nompx", x86_mpx_setup);
 
 #ifdef CONFIG_X86_64
-static int __init x86_pcid_setup(char *s)
+static int __init x86_nopcid_setup(char *s)
 {
-	/* require an exact match without trailing characters */
-	if (strlen(s))
-		return 0;
+	/* nopcid doesn't accept parameters */
+	if (s)
+		return -EINVAL;
 
 	/* do not emit a message if the feature is not present */
 	if (!boot_cpu_has(X86_FEATURE_PCID))
-		return 1;
+		return 0;
 
 	setup_clear_cpu_cap(X86_FEATURE_PCID);
 	pr_info("nopcid: PCID feature disabled\n");
-	return 1;
+	return 0;
 }
-__setup("nopcid", x86_pcid_setup);
+early_param("nopcid", x86_nopcid_setup);
 #endif
 
 static int __init x86_noinvpcid_setup(char *s)
@@ -329,38 +329,6 @@ static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 	}
 }
 
-static void setup_pcid(struct cpuinfo_x86 *c)
-{
-	if (cpu_has(c, X86_FEATURE_PCID)) {
-		if (cpu_has(c, X86_FEATURE_PGE)) {
-			/*
-			 * We'd like to use cr4_set_bits_and_update_boot(),
-			 * but we can't.  CR4.PCIDE is special and can only
-			 * be set in long mode, and the early CPU init code
-			 * doesn't know this and would try to restore CR4.PCIDE
-			 * prior to entering long mode.
-			 *
-			 * Instead, we rely on the fact that hotplug, resume,
-			 * etc all fully restore CR4 before they write anything
-			 * that could have nonzero PCID bits to CR3.  CR4.PCIDE
-			 * has no effect on the page tables themselves, so we
-			 * don't need it to be restored early.
-			 */
-			cr4_set_bits(X86_CR4_PCIDE);
-		} else {
-			/*
-			 * flush_tlb_all(), as currently implemented, won't
-			 * work if PCID is on but PGE is not.  Since that
-			 * combination doesn't exist on real hardware, there's
-			 * no reason to try to fully support it, but it's
-			 * polite to avoid corrupting data if we're on
-			 * an improperly configured VM.
-			 */
-			clear_cpu_cap(c, X86_FEATURE_PCID);
-		}
-	}
-}
-
 /*
  * Protection Keys are not available in 32-bit mode.
  */
@@ -1175,9 +1143,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	setup_smep(c);
 	setup_smap(c);
 
-	/* Set up PCID */
-	setup_pcid(c);
-
 	/*
 	 * The vendor-specific functions might have changed features.
 	 * Now we do "generic changes."

commit 1c9fe4409ce3e9c78b1ed96ee8ed699d4f03bf33
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Sep 6 19:54:54 2017 -0700

    x86/mm: Document how CR4.PCIDE restore works
    
    While debugging a problem, I thought that using
    cr4_set_bits_and_update_boot() to restore CR4.PCIDE would be
    helpful.  It turns out to be counterproductive.
    
    Add a comment documenting how this works.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 40cb4d0a5982..fb1d3358a4af 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -333,6 +333,19 @@ static void setup_pcid(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_PCID)) {
 		if (cpu_has(c, X86_FEATURE_PGE)) {
+			/*
+			 * We'd like to use cr4_set_bits_and_update_boot(),
+			 * but we can't.  CR4.PCIDE is special and can only
+			 * be set in long mode, and the early CPU init code
+			 * doesn't know this and would try to restore CR4.PCIDE
+			 * prior to entering long mode.
+			 *
+			 * Instead, we rely on the fact that hotplug, resume,
+			 * etc all fully restore CR4 before they write anything
+			 * that could have nonzero PCID bits to CR3.  CR4.PCIDE
+			 * has no effect on the page tables themselves, so we
+			 * don't need it to be restored early.
+			 */
 			cr4_set_bits(X86_CR4_PCIDE);
 		} else {
 			/*

commit 72c0098d92cedb11c7e0151e84918840a4e96b31
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Sep 6 19:54:53 2017 -0700

    x86/mm: Reinitialize TLB state on hotplug and resume
    
    When Linux brings a CPU down and back up, it switches to init_mm and then
    loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect
    of masking off the ASID bits in CR3.
    
    This can result in some confusion in the TLB handling code.  If we
    bring a CPU down and back up with any ASID other than 0, we end up
    with the wrong ASID active on the CPU after resume.  This could
    cause our internal state to become corrupt, although major
    corruption is unlikely because init_mm doesn't have any user pages.
    More obviously, if CONFIG_DEBUG_VM=y, we'll trip over an assertion
    in the next context switch.  The result of *that* is a failure to
    resume from suspend with probability 1 - 1/6^(cpus-1).
    
    Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Jiri Kosina <jikos@kernel.org>
    Fixes: 10af6235e0d3 ("x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID")
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index efba8e3da3e2..40cb4d0a5982 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1583,6 +1583,7 @@ void cpu_init(void)
 	mmgrab(&init_mm);
 	me->active_mm = &init_mm;
 	BUG_ON(me->mm);
+	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, me);
 
 	load_sp0(t, &current->thread);
@@ -1637,6 +1638,7 @@ void cpu_init(void)
 	mmgrab(&init_mm);
 	curr->active_mm = &init_mm;
 	BUG_ON(curr->mm);
+	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&init_mm, curr);
 
 	load_sp0(t, thread);

commit 24e700e291d52bd200212487e2b654c0aa3f07a2
Merge: f57091767add c6ef89421e23
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 17:43:56 2017 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic updates from Thomas Gleixner:
     "This update provides:
    
       - Cleanup of the IDT management including the removal of the extra
         tracing IDT. A first step to cleanup the vector management code.
    
       - The removal of the paravirt op adjust_exception_frame. This is a
         XEN specific issue, but merged through this branch to avoid nasty
         merge collisions
    
       - Prevent dmesg spam about the TSC DEADLINE bug, when the CPU has
         disabled the TSC DEADLINE timer in CPUID.
    
       - Adjust a debug message in the ioapic code to print out the
         information correctly"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (51 commits)
      x86/idt: Fix the X86_TRAP_BP gate
      x86/xen: Get rid of paravirt op adjust_exception_frame
      x86/eisa: Add missing include
      x86/idt: Remove superfluous ALIGNment
      x86/apic: Silence "FW_BUG TSC_DEADLINE disabled due to Errata" on CPUs without the feature
      x86/idt: Remove the tracing IDT leftovers
      x86/idt: Hide set_intr_gate()
      x86/idt: Simplify alloc_intr_gate()
      x86/idt: Deinline setup functions
      x86/idt: Remove unused functions/inlines
      x86/idt: Move interrupt gate initialization to IDT code
      x86/idt: Move APIC gate initialization to tables
      x86/idt: Move regular trap init to tables
      x86/idt: Move IST stack based traps to table init
      x86/idt: Move debug stack init to table based
      x86/idt: Switch early trap init to IDT tables
      x86/idt: Prepare for table based init
      x86/idt: Move early IDT setup out of 32-bit asm
      x86/idt: Move early IDT handler setup to IDT code
      x86/idt: Consolidate IDT invalidation
      ...

commit d8ed9d48266a27ab02a4bbcb81e755d63aec108a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:43 2017 +0200

    x86/idt: Create file for IDT related code
    
    IDT related code lives scattered around in various places. Create a new
    source file in arch/x86/kernel/idt.c to hold it.
    
    Move the idt_tables and descriptors to it for a start. Follow up patches
    will gradually move more code over.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064958.367081121@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c8b39870f33e..71ab8a45cd66 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1289,15 +1289,6 @@ static __init int setup_disablecpuid(char *arg)
 __setup("clearcpuid=", setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
-struct desc_ptr idt_descr __ro_after_init = {
-	.size = NR_VECTORS * 16 - 1,
-	.address = (unsigned long) idt_table,
-};
-const struct desc_ptr debug_idt_descr = {
-	.size = NR_VECTORS * 16 - 1,
-	.address = (unsigned long) debug_idt_table,
-};
-
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE) __visible;
 

commit 660da7c9228f685b2ebe664f9fd69aaddcc420b5
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 29 08:53:21 2017 -0700

    x86/mm: Enable CR4.PCIDE on supported systems
    
    We can use PCID if the CPU has PCID and PGE and we're not on Xen.
    
    By itself, this has no effect. A followup patch will start using PCID.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Nadav Amit <nadav.amit@gmail.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/6327ecd907b32f79d5aa0d466f04503bbec5df88.1498751203.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 904485e7b230..b95cd94ca97b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -329,6 +329,25 @@ static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 	}
 }
 
+static void setup_pcid(struct cpuinfo_x86 *c)
+{
+	if (cpu_has(c, X86_FEATURE_PCID)) {
+		if (cpu_has(c, X86_FEATURE_PGE)) {
+			cr4_set_bits(X86_CR4_PCIDE);
+		} else {
+			/*
+			 * flush_tlb_all(), as currently implemented, won't
+			 * work if PCID is on but PGE is not.  Since that
+			 * combination doesn't exist on real hardware, there's
+			 * no reason to try to fully support it, but it's
+			 * polite to avoid corrupting data if we're on
+			 * an improperly configured VM.
+			 */
+			clear_cpu_cap(c, X86_FEATURE_PCID);
+		}
+	}
+}
+
 /*
  * Protection Keys are not available in 32-bit mode.
  */
@@ -1143,6 +1162,9 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	setup_smep(c);
 	setup_smap(c);
 
+	/* Set up PCID */
+	setup_pcid(c);
+
 	/*
 	 * The vendor-specific functions might have changed features.
 	 * Now we do "generic changes."

commit 0790c9aad84901ca1bdc14746175549c8b5da215
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 29 08:53:20 2017 -0700

    x86/mm: Add the 'nopcid' boot option to turn off PCID
    
    The parameter is only present on x86_64 systems to save a few bytes,
    as PCID is always disabled on x86_32.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Nadav Amit <nadav.amit@gmail.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/8bbb2e65bcd249a5f18bfb8128b4689f08ac2b60.1498751203.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c8b39870f33e..904485e7b230 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -168,6 +168,24 @@ static int __init x86_mpx_setup(char *s)
 }
 __setup("nompx", x86_mpx_setup);
 
+#ifdef CONFIG_X86_64
+static int __init x86_pcid_setup(char *s)
+{
+	/* require an exact match without trailing characters */
+	if (strlen(s))
+		return 0;
+
+	/* do not emit a message if the feature is not present */
+	if (!boot_cpu_has(X86_FEATURE_PCID))
+		return 1;
+
+	setup_clear_cpu_cap(X86_FEATURE_PCID);
+	pr_info("nopcid: PCID feature disabled\n");
+	return 1;
+}
+__setup("nopcid", x86_pcid_setup);
+#endif
+
 static int __init x86_noinvpcid_setup(char *s)
 {
 	/* noinvpcid doesn't accept parameters */

commit 65f9d65443b5512f74248a3eb56731fbb0b337b8
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Apr 13 09:42:03 2017 +0200

    x86/cpu: remove hypervisor specific set_cpu_features
    
    There is no user of x86_hyper->set_cpu_features() any more. Remove it.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8ee32119144d..c8b39870f33e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1149,7 +1149,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	detect_ht(c);
 #endif
 
-	init_hypervisor(c);
 	x86_init_rdrand(c);
 	x86_init_cache_qos(c);
 	setup_pku(c);

commit b23adb7d3f7d1d7cce03db9704de67a99ceeda38
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 22 14:32:34 2017 -0700

    x86/xen/gdt: Use X86_FEATURE_XENPV instead of globals for the GDT fixup
    
    Xen imposes special requirements on the GDT.  Rather than using a
    global variable for the pgprot, just use an explicit special case
    for Xen -- this makes it clearer what's going on.  It also debloats
    64-bit kernels very slightly.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/e9ea96abbfd6a8c87753849171bb5987ecfeb523.1490218061.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f6e20e2dbfa5..8ee32119144d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -448,21 +448,27 @@ void load_percpu_segment(int cpu)
 	load_stack_canary_segment();
 }
 
-/*
- * On 64-bit the GDT remapping is read-only.
- * A global is used for Xen to change the default when required.
- */
+/* Setup the fixmap mapping only once per-processor */
+static inline void setup_fixmap_gdt(int cpu)
+{
 #ifdef CONFIG_X86_64
-pgprot_t pg_fixmap_gdt_flags = PAGE_KERNEL_RO;
+	/* On 64-bit systems, we use a read-only fixmap GDT. */
+	pgprot_t prot = PAGE_KERNEL_RO;
 #else
-pgprot_t pg_fixmap_gdt_flags = PAGE_KERNEL;
+	/*
+	 * On native 32-bit systems, the GDT cannot be read-only because
+	 * our double fault handler uses a task gate, and entering through
+	 * a task gate needs to change an available TSS to busy.  If the GDT
+	 * is read-only, that will triple fault.
+	 *
+	 * On Xen PV, the GDT must be read-only because the hypervisor requires
+	 * it.
+	 */
+	pgprot_t prot = boot_cpu_has(X86_FEATURE_XENPV) ?
+		PAGE_KERNEL_RO : PAGE_KERNEL;
 #endif
 
-/* Setup the fixmap mapping only once per-processor */
-static inline void setup_fixmap_gdt(int cpu)
-{
-	__set_fixmap(get_cpu_gdt_ro_index(cpu), get_cpu_gdt_paddr(cpu),
-		     pg_fixmap_gdt_flags);
+	__set_fixmap(get_cpu_gdt_ro_index(cpu), get_cpu_gdt_paddr(cpu), prot);
 }
 
 /* Load the original GDT from the per-cpu structure */

commit aa4ea675528f3fa11e9663e5a32f55a81c34dcac
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Mar 22 14:32:30 2017 -0700

    x86/gdt: Fix setup_fixmap_gdt() to use the correct PA
    
    __pa() cannot be used on percpu pointers because they may be
    virtually mapped.  Use per_cpu_ptr_to_phys() instead.
    
    This fixes a boot crash on a some 32-bit configurations.  I assume
    this is related to which allocation strategy is chosen by the percpu
    core.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 69218e47994d x86: ("Remap GDT tables in the fixmap section")
    Link: http://lkml.kernel.org/r/22e0069c29fba31998f193201e359eebfdac4960.1490218061.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f8e22dbad86c..f6e20e2dbfa5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -461,8 +461,8 @@ pgprot_t pg_fixmap_gdt_flags = PAGE_KERNEL;
 /* Setup the fixmap mapping only once per-processor */
 static inline void setup_fixmap_gdt(int cpu)
 {
-	__set_fixmap(get_cpu_gdt_ro_index(cpu),
-		     __pa(get_cpu_gdt_rw(cpu)), pg_fixmap_gdt_flags);
+	__set_fixmap(get_cpu_gdt_ro_index(cpu), get_cpu_gdt_paddr(cpu),
+		     pg_fixmap_gdt_flags);
 }
 
 /* Load the original GDT from the per-cpu structure */

commit 45fc8757d1d2128e342b4e7ef39adedf7752faac
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Mar 14 10:05:08 2017 -0700

    x86: Make the GDT remapping read-only on 64-bit
    
    This patch makes the GDT remapped pages read-only, to prevent accidental
    (or intentional) corruption of this key data structure.
    
    This change is done only on 64-bit, because 32-bit needs it to be writable
    for TSS switches.
    
    The native_load_tr_desc function was adapted to correctly handle a
    read-only GDT. The LTR instruction always writes to the GDT TSS entry.
    This generates a page fault if the GDT is read-only. This change checks
    if the current GDT is a remap and swap GDTs as needed. This function was
    tested by booting multiple machines and checking hibernation works
    properly.
    
    KVM SVM and VMX were adapted to use the writeable GDT. On VMX, the
    per-cpu variable was removed for functions to fetch the original GDT.
    Instead of reloading the previous GDT, VMX will reload the fixmap GDT as
    expected. For testing, VMs were started and restored on multiple
    configurations.
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lorenzo Stoakes <lstoakes@gmail.com>
    Cc: Luis R . Rodriguez <mcgrof@kernel.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kvm@vger.kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-pm@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Cc: zijun_hu <zijun_hu@htc.com>
    Link: http://lkml.kernel.org/r/20170314170508.100882-3-thgarnie@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3cf1590ec9ce..f8e22dbad86c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -448,8 +448,15 @@ void load_percpu_segment(int cpu)
 	load_stack_canary_segment();
 }
 
-/* Used by XEN to force the GDT read-only when required */
+/*
+ * On 64-bit the GDT remapping is read-only.
+ * A global is used for Xen to change the default when required.
+ */
+#ifdef CONFIG_X86_64
+pgprot_t pg_fixmap_gdt_flags = PAGE_KERNEL_RO;
+#else
 pgprot_t pg_fixmap_gdt_flags = PAGE_KERNEL;
+#endif
 
 /* Setup the fixmap mapping only once per-processor */
 static inline void setup_fixmap_gdt(int cpu)
@@ -458,6 +465,17 @@ static inline void setup_fixmap_gdt(int cpu)
 		     __pa(get_cpu_gdt_rw(cpu)), pg_fixmap_gdt_flags);
 }
 
+/* Load the original GDT from the per-cpu structure */
+void load_direct_gdt(int cpu)
+{
+	struct desc_ptr gdt_descr;
+
+	gdt_descr.address = (long)get_cpu_gdt_rw(cpu);
+	gdt_descr.size = GDT_SIZE - 1;
+	load_gdt(&gdt_descr);
+}
+EXPORT_SYMBOL_GPL(load_direct_gdt);
+
 /* Load a fixmap remapping of the per-cpu GDT */
 void load_fixmap_gdt(int cpu)
 {
@@ -467,6 +485,7 @@ void load_fixmap_gdt(int cpu)
 	gdt_descr.size = GDT_SIZE - 1;
 	load_gdt(&gdt_descr);
 }
+EXPORT_SYMBOL_GPL(load_fixmap_gdt);
 
 /*
  * Current gdt points %fs at the "master" per-cpu area: after this,
@@ -474,11 +493,8 @@ void load_fixmap_gdt(int cpu)
  */
 void switch_to_new_gdt(int cpu)
 {
-	struct desc_ptr gdt_descr;
-
-	gdt_descr.address = (long)get_cpu_gdt_rw(cpu);
-	gdt_descr.size = GDT_SIZE - 1;
-	load_gdt(&gdt_descr);
+	/* Load the original GDT */
+	load_direct_gdt(cpu);
 	/* Reload the per-cpu base */
 	load_percpu_segment(cpu);
 }

commit 69218e47994da614e7af600bf06887750ab6657a
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Mar 14 10:05:07 2017 -0700

    x86: Remap GDT tables in the fixmap section
    
    Each processor holds a GDT in its per-cpu structure. The sgdt
    instruction gives the base address of the current GDT. This address can
    be used to bypass KASLR memory randomization. With another bug, an
    attacker could target other per-cpu structures or deduce the base of
    the main memory section (PAGE_OFFSET).
    
    This patch relocates the GDT table for each processor inside the
    fixmap section. The space is reserved based on number of supported
    processors.
    
    For consistency, the remapping is done by default on 32 and 64-bit.
    
    Each processor switches to its remapped GDT at the end of
    initialization. For hibernation, the main processor returns with the
    original GDT and switches back to the remapping at completion.
    
    This patch was tested on both architectures. Hibernation and KVM were
    both tested specially for their usage of the GDT.
    
    Thanks to Boris Ostrovsky <boris.ostrovsky@oracle.com> for testing and
    recommending changes for Xen support.
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lorenzo Stoakes <lstoakes@gmail.com>
    Cc: Luis R . Rodriguez <mcgrof@kernel.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kvm@vger.kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-pm@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Cc: zijun_hu <zijun_hu@htc.com>
    Link: http://lkml.kernel.org/r/20170314170508.100882-2-thgarnie@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 58094a1f9e9d..3cf1590ec9ce 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -448,6 +448,26 @@ void load_percpu_segment(int cpu)
 	load_stack_canary_segment();
 }
 
+/* Used by XEN to force the GDT read-only when required */
+pgprot_t pg_fixmap_gdt_flags = PAGE_KERNEL;
+
+/* Setup the fixmap mapping only once per-processor */
+static inline void setup_fixmap_gdt(int cpu)
+{
+	__set_fixmap(get_cpu_gdt_ro_index(cpu),
+		     __pa(get_cpu_gdt_rw(cpu)), pg_fixmap_gdt_flags);
+}
+
+/* Load a fixmap remapping of the per-cpu GDT */
+void load_fixmap_gdt(int cpu)
+{
+	struct desc_ptr gdt_descr;
+
+	gdt_descr.address = (long)get_cpu_gdt_ro(cpu);
+	gdt_descr.size = GDT_SIZE - 1;
+	load_gdt(&gdt_descr);
+}
+
 /*
  * Current gdt points %fs at the "master" per-cpu area: after this,
  * it's on the real one.
@@ -456,11 +476,10 @@ void switch_to_new_gdt(int cpu)
 {
 	struct desc_ptr gdt_descr;
 
-	gdt_descr.address = (long)get_cpu_gdt_table(cpu);
+	gdt_descr.address = (long)get_cpu_gdt_rw(cpu);
 	gdt_descr.size = GDT_SIZE - 1;
 	load_gdt(&gdt_descr);
 	/* Reload the per-cpu base */
-
 	load_percpu_segment(cpu);
 }
 
@@ -1526,6 +1545,9 @@ void cpu_init(void)
 
 	if (is_uv_system())
 		uv_cpu_init();
+
+	setup_fixmap_gdt(cpu);
+	load_fixmap_gdt(cpu);
 }
 
 #else
@@ -1581,6 +1603,9 @@ void cpu_init(void)
 	dbg_restore_debug_regs();
 
 	fpu__init_cpu();
+
+	setup_fixmap_gdt(cpu);
+	load_fixmap_gdt(cpu);
 }
 #endif
 

commit 609b07b72d3caaa8eed3a238886467946b78fa5e
Merge: c3abcabe813b f94c8d116997
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 7 14:42:34 2017 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "A fix for KVM's scheduler clock which (erroneously) was always marked
      unstable, a fix for RT/DL load balancing, plus latency fixes"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
      sched/core: Fix pick_next_task() for RT,DL
      sched/fair: Make select_idle_cpu() more aggressive

commit 68e21be2916b359fd8afb536c1911dc014cfd03e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 19:08:20 2017 +0100

    sched/headers: Move task->mm handling methods to <linux/sched/mm.h>
    
    Move the following task->mm helper APIs into a new header file,
    <linux/sched/mm.h>, to further reduce the size and complexity
    of <linux/sched.h>.
    
    Here are how the APIs are used in various kernel files:
    
      # mm_alloc():
      arch/arm/mach-rpc/ecard.c
      fs/exec.c
      include/linux/sched/mm.h
      kernel/fork.c
    
      # __mmdrop():
      arch/arc/include/asm/mmu_context.h
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmdrop():
      arch/arm/mach-rpc/ecard.c
      arch/m68k/sun3/mmu_emu.c
      arch/x86/mm/tlb.c
      drivers/gpu/drm/amd/amdkfd/kfd_process.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/hw/hfi1/file_ops.c
      drivers/vfio/vfio_iommu_spapr_tce.c
      fs/exec.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/mmu_notifier.h
      include/linux/sched/mm.h
      kernel/fork.c
      kernel/futex.c
      kernel/sched/core.c
      mm/khugepaged.c
      mm/ksm.c
      mm/mmu_context.c
      mm/mmu_notifier.c
      mm/oom_kill.c
      virt/kvm/kvm_main.c
    
      # mmdrop_async_fn():
      include/linux/sched/mm.h
    
      # mmdrop_async():
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmget_not_zero():
      fs/userfaultfd.c
      include/linux/sched/mm.h
      mm/oom_kill.c
    
      # mmput():
      arch/arc/include/asm/mmu_context.h
      arch/arc/kernel/troubleshoot.c
      arch/frv/mm/mmu-context.c
      arch/powerpc/platforms/cell/spufs/context.c
      arch/sparc/include/asm/mmu_context_32.h
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/core/uverbs_main.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/exec.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/events/uprobes.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/oom_kill.c
      mm/process_vm_access.c
      mm/rmap.c
      mm/swapfile.c
      mm/util.c
      virt/kvm/async_pf.c
    
      # mmput_async():
      include/linux/sched/mm.h
      kernel/fork.c
      mm/oom_kill.c
    
      # get_task_mm():
      arch/arc/kernel/troubleshoot.c
      arch/powerpc/platforms/cell/spufs/context.c
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/util.c
    
      # mm_access():
      fs/proc/base.c
      include/linux/sched/mm.h
      kernel/fork.c
      mm/process_vm_access.c
    
      # mm_release():
      arch/arc/include/asm/mmu_context.h
      fs/exec.c
      include/linux/sched/mm.h
      include/uapi/linux/sched.h
      kernel/exit.c
      kernel/fork.c
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f2fd8fefc589..b11b38c3b0bd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -7,7 +7,7 @@
 #include <linux/string.h>
 #include <linux/ctype.h>
 #include <linux/delay.h>
-#include <linux/sched.h>
+#include <linux/sched/mm.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/task.h>
 #include <linux/init.h>

commit f94c8d116997597fc00f0812b0ab9256e7b0c58f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 1 15:53:38 2017 +0100

    sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
    
    Wanpeng Li reported that since the following commit:
    
      acb04058de49 ("sched/clock: Fix hotplug crash")
    
    ... KVM always runs with unstable sched-clock even though KVM's
    kvm_clock _is_ stable.
    
    The problem is that we've tied clear_sched_clock_stable() to the TSC
    state, and overlooked that sched_clock() is a paravirt function.
    
    Solve this by doing two things:
    
     - tie the sched_clock() stable state more clearly to the TSC stable
       state for the normal (!paravirt) case.
    
     - only call clear_sched_clock_stable() when we mark TSC unstable
       when we use native_sched_clock().
    
    The first means we can actually run with stable sched_clock in more
    situations then before, which is good. And since commit:
    
      12907fbb1a69 ("sched/clock, clocksource: Add optional cs::mark_unstable() method")
    
    ... this should be reliable. Since any detection of TSC fail now results
    in marking the TSC unstable.
    
    Reported-by: Wanpeng Li <kernellwp@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Fixes: acb04058de49 ("sched/clock: Fix hotplug crash")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c64ca5929cb5..9d98e2e15d54 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -86,7 +86,6 @@ static void default_init(struct cpuinfo_x86 *c)
 			strcpy(c->x86_model_id, "386");
 	}
 #endif
-	clear_sched_clock_stable();
 }
 
 static const struct cpu_dev default_cpu = {
@@ -1075,8 +1074,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	 */
 	if (this_cpu->c_init)
 		this_cpu->c_init(c);
-	else
-		clear_sched_clock_stable();
 
 	/* Disable the PN if appropriate */
 	squash_the_stupid_serial_number(c);

commit 9164bb4a18dfa592cd0aca455ea57abf89ca4526
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 01:20:53 2017 +0100

    sched/headers: Prepare to move 'init_task' and 'init_thread_union' from <linux/sched.h> to <linux/sched/task.h>
    
    Update all usage sites first.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c7e2ca966386..f2fd8fefc589 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -9,6 +9,7 @@
 #include <linux/delay.h>
 #include <linux/sched.h>
 #include <linux/sched/clock.h>
+#include <linux/sched/task.h>
 #include <linux/init.h>
 #include <linux/kprobes.h>
 #include <linux/kgdb.h>

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c64ca5929cb5..c7e2ca966386 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -8,6 +8,7 @@
 #include <linux/ctype.h>
 #include <linux/delay.h>
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 #include <linux/init.h>
 #include <linux/kprobes.h>
 #include <linux/kgdb.h>

commit f1f1007644ffc8051a4c11427d58b1967ae7b75a
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Feb 27 14:30:07 2017 -0800

    mm: add new mmgrab() helper
    
    Apart from adding the helper function itself, the rest of the kernel is
    converted mechanically using:
    
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_count);/mmgrab\(\1\);/'
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_count);/mmgrab\(\&\1\);/'
    
    This is needed for a later patch that hooks into the helper, but might
    be a worthwhile cleanup on its own.
    
    (Michal Hocko provided most of the kerneldoc comment.)
    
    Link: http://lkml.kernel.org/r/20161218123229.22952-1-vegard.nossum@oracle.com
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f07005e6f461..c64ca5929cb5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1510,7 +1510,7 @@ void cpu_init(void)
 	for (i = 0; i <= IO_BITMAP_LONGS; i++)
 		t->io_bitmap[i] = ~0UL;
 
-	atomic_inc(&init_mm.mm_count);
+	mmgrab(&init_mm);
 	me->active_mm = &init_mm;
 	BUG_ON(me->mm);
 	enter_lazy_tlb(&init_mm, me);
@@ -1561,7 +1561,7 @@ void cpu_init(void)
 	/*
 	 * Set up and load the per-CPU TSS and LDT
 	 */
-	atomic_inc(&init_mm.mm_count);
+	mmgrab(&init_mm);
 	curr->active_mm = &init_mm;
 	BUG_ON(curr->mm);
 	enter_lazy_tlb(&init_mm, curr);

commit 280d7a1edef214eefb1cb34915c73767355dd1b3
Merge: 8a9365a4725a 9729017f8444
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 15:03:51 2017 -0800

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fpu updates from Ingo Molnar:
     "The main changes relate to fixes between (lack of) CPUID and FPU
      detection that should only affect old or weird CPUs, by Andy
      Lutomirski"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Fix the "Giving up, no FPU found" test
      x86/fpu: Fix CPUID-less FPU detection
      x86/fpu: Fix "x86/fpu: Legacy x87 FPU detected" message
      x86/cpu: Re-apply forced caps every time CPU caps are re-read
      x86/cpu: Factor out application of forced CPU caps
      x86/cpu: Add X86_FEATURE_CPUID
      x86/fpu/xstate: Move XSAVES state init to a function

commit 8a9365a4725a4d6265a416dd63bff937e300308e
Merge: 2891e8e66787 3bba73b1b7a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 14:37:08 2017 -0800

    Merge branch 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpufeature updates from Ingo Molnar:
     "The main changes in this cycle were related to enable ring-3
      MONITOR/MWAIT instructions support on supported CPUs, by Grzegorz
      Andrejczuk and Piotr Luc"
    
    * 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpufeature: Move RING3MWAIT feature to avoid conflicts
      x86/cpufeature: Enable RING3MWAIT for Knights Mill
      x86/cpufeature: Enable RING3MWAIT for Knights Landing
      x86/cpufeature: Add RING3MWAIT to CPU features
      x86/elf: Add HWCAP2 to expose ring 3 MONITOR/MWAIT
      x86/msr: Add MSR_MISC_FEATURE_ENABLES and RING3MWAIT bit
      x86/cpufeature: Add AVX512_VPOPCNTDQ feature

commit 828cad8ea05d194d8a9452e0793261c2024c23a2
Merge: 60c906bab124 bb3bac2ca9a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 12:52:55 2017 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this (fairly busy) cycle were:
    
       - There was a class of scheduler bugs related to forgetting to update
         the rq-clock timestamp which can cause weird and hard to debug
         problems, so there's a new debug facility for this: which uncovered
         a whole lot of bugs which convinced us that we want to keep the
         debug facility.
    
         (Peter Zijlstra, Matt Fleming)
    
       - Various cputime related updates: eliminate cputime and use u64
         nanoseconds directly, simplify and improve the arch interfaces,
         implement delayed accounting more widely, etc. - (Frederic
         Weisbecker)
    
       - Move code around for better structure plus cleanups (Ingo Molnar)
    
       - Move IO schedule accounting deeper into the scheduler plus related
         changes to improve the situation (Tejun Heo)
    
       - ... plus a round of sched/rt and sched/deadline fixes, plus other
         fixes, updats and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (85 commits)
      sched/core: Remove unlikely() annotation from sched_move_task()
      sched/autogroup: Rename auto_group.[ch] to autogroup.[ch]
      sched/topology: Split out scheduler topology code from core.c into topology.c
      sched/core: Remove unnecessary #include headers
      sched/rq_clock: Consolidate the ordering of the rq_clock methods
      delayacct: Include <uapi/linux/taskstats.h>
      sched/core: Clean up comments
      sched/rt: Show the 'sched_rr_timeslice' SCHED_RR timeslice tuning knob in milliseconds
      sched/clock: Add dummy clear_sched_clock_stable() stub function
      sched/cputime: Remove generic asm headers
      sched/cputime: Remove unused nsec_to_cputime()
      s390, sched/cputime: Remove unused cputime definitions
      powerpc, sched/cputime: Remove unused cputime definitions
      s390, sched/cputime: Make arch_cpu_idle_time() to return nsecs
      ia64, sched/cputime: Remove unused cputime definitions
      ia64: Convert vtime to use nsec units directly
      ia64, sched/cputime: Move the nsecs based cputime headers to the last arch using it
      sched/cputime: Remove jiffies based cputime
      sched/cputime, vtime: Return nsecs instead of cputime_t to account
      sched/cputime: Complete nsec conversion of tick based accounting
      ...

commit 79a8b9aa388b0620cc1d525d7c0f0d9a8a85e08e
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Feb 5 11:50:21 2017 +0100

    x86/CPU/AMD: Bring back Compute Unit ID
    
    Commit:
    
      a33d331761bc ("x86/CPU/AMD: Fix Bulldozer topology")
    
    restored the initial approach we had with the Fam15h topology of
    enumerating CU (Compute Unit) threads as cores. And this is still
    correct - they're beefier than HT threads but still have some
    shared functionality.
    
    Our current approach has a problem with the Mad Max Steam game, for
    example. Yves Dionne reported a certain "choppiness" while playing on
    v4.9.5.
    
    That problem stems most likely from the fact that the CU threads share
    resources within one CU and when we schedule to a thread of a different
    compute unit, this incurs latency due to migrating the working set to a
    different CU through the caches.
    
    When the thread siblings mask mirrors that aspect of the CUs and
    threads, the scheduler pays attention to it and tries to schedule within
    one CU first. Which takes care of the latency, of course.
    
    Reported-by: Yves Dionne <yves.dionne@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # 4.9
    Cc: Brice Goglin <Brice.Goglin@inria.fr>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20170205105022.8705-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9bab7a8a4293..ede03e849a8b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1015,6 +1015,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	c->x86_model_id[0] = '\0';  /* Unset */
 	c->x86_max_cores = 1;
 	c->x86_coreid_bits = 0;
+	c->cu_id = 0xff;
 #ifdef CONFIG_X86_64
 	c->x86_clflush_size = 64;
 	c->x86_phys_bits = 36;

commit 0274f9551eff55dbd63b5f5f3efe30fe5d4c801c
Author: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
Date:   Fri Jan 20 14:22:34 2017 +0100

    x86/elf: Add HWCAP2 to expose ring 3 MONITOR/MWAIT
    
    Introduce ELF_HWCAP2 variable for x86 and reserve its bit 0 to expose the
    ring 3 MONITOR/MWAIT.
    
    HWCAP variables contain bitmasks which can be used by userspace
    applications to detect which instruction sets are supported by CPU.  On x86
    architecture information about CPU capabilities can be checked via CPUID
    instructions, unfortunately presence of ring 3 MONITOR/MWAIT feature cannot
    be checked this way. ELF_HWCAP cannot be used as well, because on x86 it is
    set to CPUID[1].EDX which means that all bits are reserved there.
    
    HWCAP2 approach was chosen because it reuses existing solution present
    in other architectures, so only minor modifications are required to the
    kernel and userspace applications. When ELF_HWCAP2 is defined
    kernel maps it to AT_HWCAP2 during the start of the application.
    This way the ring 3 MONITOR/MWAIT feature can be detected using getauxval()
    API in a simple and fast manner. ELF_HWCAP2 type is u32 to be consistent
    with x86 ELF_HWCAP type.
    
    Signed-off-by: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: Piotr.Luc@intel.com
    Cc: dave.hansen@linux.intel.com
    Link: http://lkml.kernel.org/r/1484918557-15481-3-git-send-email-grzegorz.andrejczuk@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9bab7a8a4293..f879429cfcaa 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -35,6 +35,7 @@
 #include <asm/desc.h>
 #include <asm/fpu/internal.h>
 #include <asm/mtrr.h>
+#include <asm/hwcap2.h>
 #include <linux/numa.h>
 #include <asm/asm.h>
 #include <asm/bugs.h>
@@ -51,6 +52,8 @@
 
 #include "cpu.h"
 
+u32 elf_hwcap2 __read_mostly;
+
 /* all of these masks are initialized in setup_cpu_local_masks() */
 cpumask_var_t cpu_initialized_mask;
 cpumask_var_t cpu_callout_mask;

commit ed5c8c854f2b990dfa4d85c4995d115768a05d3c
Merge: 619bd4a71874 a2ca3d617944
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 09:12:25 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes and refresh the branch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 60d3450167433f2d099ce2869dc52dd9e7dc9b29
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Jan 18 11:15:39 2017 -0800

    x86/cpu: Re-apply forced caps every time CPU caps are re-read
    
    Calling get_cpu_cap() will reset a bunch of CPU features.  This will
    cause the system to lose track of force-set and force-cleared
    features in the words that are reset until the end of CPU
    initialization.  This can cause X86_FEATURE_FPU, for example, to
    change back and forth during boot and potentially confuse CPU setup.
    
    To minimize the chance of confusion, re-apply forced caps every time
    get_cpu_cap() is called.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Whitehead <tedheadster@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/c817eb373d2c67c2c81413a70fc9b845fa34a37e.1484705016.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2ea16e05de43..d09b5eefaddf 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -758,6 +758,13 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[CPUID_8000_000A_EDX] = cpuid_edx(0x8000000a);
 
 	init_scattered_cpuid_features(c);
+
+	/*
+	 * Clear/Set all flags overridden by options, after probe.
+	 * This needs to happen each time we re-probe, which may happen
+	 * several times during CPU initialization.
+	 */
+	apply_forced_caps(c);
 }
 
 static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)

commit 8bf1ebca215c262e48c15a4a15f175991776f57f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Jan 18 11:15:38 2017 -0800

    x86/cpu: Factor out application of forced CPU caps
    
    There are multiple call sites that apply forced CPU caps.  Factor
    them into a helper.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Whitehead <tedheadster@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/623ff7555488122143e4417de09b18be2085ad06.1484705016.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c3328d0e13f0..2ea16e05de43 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -655,6 +655,16 @@ void cpu_detect(struct cpuinfo_x86 *c)
 	}
 }
 
+static void apply_forced_caps(struct cpuinfo_x86 *c)
+{
+	int i;
+
+	for (i = 0; i < NCAPINTS; i++) {
+		c->x86_capability[i] &= ~cpu_caps_cleared[i];
+		c->x86_capability[i] |= cpu_caps_set[i];
+	}
+}
+
 void get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
@@ -1035,10 +1045,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 		this_cpu->c_identify(c);
 
 	/* Clear/Set all flags overridden by options, after probe */
-	for (i = 0; i < NCAPINTS; i++) {
-		c->x86_capability[i] &= ~cpu_caps_cleared[i];
-		c->x86_capability[i] |= cpu_caps_set[i];
-	}
+	apply_forced_caps(c);
 
 #ifdef CONFIG_X86_64
 	c->apicid = apic->phys_pkg_id(c->initial_apicid, 0);
@@ -1097,10 +1104,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	 * Clear/Set all flags overridden by options, need do it
 	 * before following smp all cpus cap AND.
 	 */
-	for (i = 0; i < NCAPINTS; i++) {
-		c->x86_capability[i] &= ~cpu_caps_cleared[i];
-		c->x86_capability[i] |= cpu_caps_set[i];
-	}
+	apply_forced_caps(c);
 
 	/*
 	 * On SMP, boot_cpu_data holds the common feature set between

commit 78d1b296843a719935277b0d24d1358416ed0204
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jan 18 11:15:37 2017 -0800

    x86/cpu: Add X86_FEATURE_CPUID
    
    Add a synthetic CPUID flag denoting whether the CPU sports the CPUID
    instruction or not. This will come useful later when accomodating
    CPUID-less CPUs.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [ Slightly prettified. ]
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Whitehead <tedheadster@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yu-cheng Yu <yu-cheng.yu@intel.com>
    Link: http://lkml.kernel.org/r/dcb355adae3ab812c79397056a61c212f1a0c7cc.1484705016.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9bab7a8a4293..c3328d0e13f0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -801,14 +801,12 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
 	c->extended_cpuid_level = 0;
 
-	if (!have_cpuid_p())
-		identify_cpu_without_cpuid(c);
-
 	/* cyrix could have cpuid enabled via c_identify()*/
 	if (have_cpuid_p()) {
 		cpu_detect(c);
 		get_cpu_vendor(c);
 		get_cpu_cap(c);
+		setup_force_cpu_cap(X86_FEATURE_CPUID);
 
 		if (this_cpu->c_early_init)
 			this_cpu->c_early_init(c);
@@ -818,6 +816,9 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 		if (this_cpu->c_bsp_init)
 			this_cpu->c_bsp_init(c);
+	} else {
+		identify_cpu_without_cpuid(c);
+		setup_clear_cpu_cap(X86_FEATURE_CPUID);
 	}
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);

commit acb04058de49458010c44bb35b849d45113fd668
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 19 14:36:33 2017 +0100

    sched/clock: Fix hotplug crash
    
    Mike reported that he could trigger the WARN_ON_ONCE() in
    set_sched_clock_stable() using hotplug.
    
    This exposed a fundamental problem with the interface, we should never
    mark the TSC stable if we ever find it to be unstable. Therefore
    set_sched_clock_stable() is a broken interface.
    
    The reason it existed is that not having it is a pain, it means all
    relevant architecture code needs to call clear_sched_clock_stable()
    where appropriate.
    
    Of the three architectures that select HAVE_UNSTABLE_SCHED_CLOCK ia64
    and parisc are trivial in that they never called
    set_sched_clock_stable(), so add an unconditional call to
    clear_sched_clock_stable() to them.
    
    For x86 the story is a lot more involved, and what this patch tries to
    do is ensure we preserve the status quo. So even is Cyrix or Transmeta
    have usable TSC they never called set_sched_clock_stable() so they now
    get an explicit mark unstable.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 9881b024b7d7 ("sched/clock: Delay switching sched_clock to stable")
    Link: http://lkml.kernel.org/r/20170119133633.GB6536@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index dc1697ca5191..3457186275a0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -83,6 +83,7 @@ static void default_init(struct cpuinfo_x86 *c)
 			strcpy(c->x86_model_id, "386");
 	}
 #endif
+	clear_sched_clock_stable();
 }
 
 static const struct cpu_dev default_cpu = {
@@ -1055,6 +1056,8 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	 */
 	if (this_cpu->c_init)
 		this_cpu->c_init(c);
+	else
+		clear_sched_clock_stable();
 
 	/* Disable the PN if appropriate */
 	squash_the_stupid_serial_number(c);

commit dd853fd216d1485ed3045ff772079cc8689a9a4a
Author: Lukasz Odzioba <lukasz.odzioba@intel.com>
Date:   Wed Dec 28 14:55:40 2016 +0100

    x86/cpu: Fix bootup crashes by sanitizing the argument of the 'clearcpuid=' command-line option
    
    A negative number can be specified in the cmdline which will be used as
    setup_clear_cpu_cap() argument. With that we can clear/set some bit in
    memory predceeding boot_cpu_data/cpu_caps_cleared which may cause kernel
    to misbehave. This patch adds lower bound check to setup_disablecpuid().
    
    Boris Petkov reproduced a crash:
    
      [    1.234575] BUG: unable to handle kernel paging request at ffffffff858bd540
      [    1.236535] IP: memcpy_erms+0x6/0x10
    
    Signed-off-by: Lukasz Odzioba <lukasz.odzioba@intel.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andi.kleen@intel.com
    Cc: bp@alien8.de
    Cc: dave.hansen@linux.intel.com
    Cc: luto@kernel.org
    Cc: slaoub@gmail.com
    Fixes: ac72e7888a61 ("x86: add generic clearcpuid=... option")
    Link: http://lkml.kernel.org/r/1482933340-11857-1-git-send-email-lukasz.odzioba@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index dc1697ca5191..9bab7a8a4293 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1221,7 +1221,7 @@ static __init int setup_disablecpuid(char *arg)
 {
 	int bit;
 
-	if (get_option(&arg, &bit) && bit < NCAPINTS*32)
+	if (get_option(&arg, &bit) && bit >= 0 && bit < NCAPINTS * 32)
 		setup_clear_cpu_cap(bit);
 	else
 		return 0;

commit 3df8d9208569ef0b2313e516566222d745f3b94b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Dec 15 10:14:42 2016 -0800

    x86/cpu: Probe CPUID leaf 6 even when cpuid_level == 6
    
    A typo (or mis-merge?) resulted in leaf 6 only being probed if
    cpuid_level >= 7.
    
    Fixes: 2ccd71f1b278 ("x86/cpufeature: Move some of the scattered feature bits to x86_capability")
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Link: http://lkml.kernel.org/r/6ea30c0e9daec21e488b54761881a6dfcf3e04d0.1481825597.git.luto@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1f6b50a449ab..dc1697ca5191 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -667,13 +667,14 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[CPUID_1_EDX] = edx;
 	}
 
+	/* Thermal and Power Management Leaf: level 0x00000006 (eax) */
+	if (c->cpuid_level >= 0x00000006)
+		c->x86_capability[CPUID_6_EAX] = cpuid_eax(0x00000006);
+
 	/* Additional Intel-defined flags: level 0x00000007 */
 	if (c->cpuid_level >= 0x00000007) {
 		cpuid_count(0x00000007, 0, &eax, &ebx, &ecx, &edx);
-
 		c->x86_capability[CPUID_7_0_EBX] = ebx;
-
-		c->x86_capability[CPUID_6_EAX] = cpuid_eax(0x00000006);
 		c->x86_capability[CPUID_7_ECX] = ecx;
 	}
 

commit 9d85eb9119f4eeeb48e87adfcd71f752655700e9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 12 11:04:53 2016 +0100

    x86/smpboot: Make logical package management more robust
    
    The logical package management has several issues:
    
     - The APIC ids provided by ACPI are not required to be the same as the
       initial APIC id which can be retrieved by CPUID. The APIC ids provided
       by ACPI are those which are written by the BIOS into the APIC. The
       initial id is set by hardware and can not be changed. The hardware
       provided ids contain the real hardware package information.
    
       Especially AMD sets the effective APIC id different from the hardware id
       as they need to reserve space for the IOAPIC ids starting at id 0.
    
       As a consequence those machines trigger the currently active firmware
       bug printouts in dmesg, These are obviously wrong.
    
     - Virtual machines have their own interesting of enumerating APICs and
       packages which are not reliably covered by the current implementation.
    
    The sizing of the mapping array has been tweaked to be generously large to
    handle systems which provide a wrong core count when HT is disabled so the
    whole magic which checks for space in the physical hotplug case is not
    needed anymore.
    
    Simplify the whole machinery and do the mapping when the CPU starts and the
    CPUID derived physical package information is available. This solves the
    observed problems on AMD machines and works for the virtualization issues
    as well.
    
    Remove the extra call from XEN cpu bringup code as it is not longer
    required.
    
    Fixes: d49597fd3bc7 ("x86/cpu: Deal with broken firmware (VMWare/XEN)")
    Reported-and-tested-by: Borislav Petkov <bp@suse.de>
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: M. Vefa Bicakci <m.v.b@runbox.com>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Cc: Charles (Chas) Williams <ciwillia@brocade.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1612121102260.3429@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 729f92ba8224..1f6b50a449ab 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -979,29 +979,21 @@ static void x86_init_cache_qos(struct cpuinfo_x86 *c)
 }
 
 /*
- * The physical to logical package id mapping is initialized from the
- * acpi/mptables information. Make sure that CPUID actually agrees with
- * that.
+ * Validate that ACPI/mptables have the same information about the
+ * effective APIC id and update the package map.
  */
-static void sanitize_package_id(struct cpuinfo_x86 *c)
+static void validate_apic_and_package_id(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
-	unsigned int pkg, apicid, cpu = smp_processor_id();
+	unsigned int apicid, cpu = smp_processor_id();
 
 	apicid = apic->cpu_present_to_apicid(cpu);
-	pkg = apicid >> boot_cpu_data.x86_coreid_bits;
 
-	if (apicid != c->initial_apicid) {
-		pr_err(FW_BUG "CPU%u: APIC id mismatch. Firmware: %x CPUID: %x\n",
+	if (apicid != c->apicid) {
+		pr_err(FW_BUG "CPU%u: APIC id mismatch. Firmware: %x APIC: %x\n",
 		       cpu, apicid, c->initial_apicid);
-		c->initial_apicid = apicid;
 	}
-	if (pkg != c->phys_proc_id) {
-		pr_err(FW_BUG "CPU%u: Using firmware package id %u instead of %u\n",
-		       cpu, pkg, c->phys_proc_id);
-		c->phys_proc_id = pkg;
-	}
-	c->logical_proc_id = topology_phys_to_logical_pkg(pkg);
+	BUG_ON(topology_update_package_map(c->phys_proc_id, cpu));
 #else
 	c->logical_proc_id = 0;
 #endif
@@ -1132,7 +1124,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 #ifdef CONFIG_NUMA
 	numa_add_cpu(smp_processor_id());
 #endif
-	sanitize_package_id(c);
 }
 
 /*
@@ -1187,6 +1178,7 @@ void identify_secondary_cpu(struct cpuinfo_x86 *c)
 	enable_sep_cpu();
 #endif
 	mtrr_ap_init();
+	validate_apic_and_package_id(c);
 }
 
 static __init int setup_noclflush(char *arg)

commit 991bc36254457f7f5695c0a28b39a91b104067a3
Merge: 212f30008a28 14cfbe55c75a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 15:23:02 2016 -0800

    Merge branch 'x86-microcode-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 microcode update from Ingo Molnar:
     "The biggest change (by Borislav Petkov) is a thorough rewrite of the
      Intel microcode loader and its interactions with the core code.
    
      The biggest conceptual change is the decoupling of the microcode
      loading on boot and application processors (which load the microcode
      in different scenarios), so that both parse the input patches with as
      few assumptions as possible - this also fixes various kernel address
      space randomization bugs. (The AP side then goes on and caches the
      result to improve boot performance.)
    
      Since the AMD side already did this, this change also opened up the
      path towards more unification/simplification of the core microcode
      loading infrastructure:
    
         10 files changed, 647 insertions(+), 940 deletions(-)
    
      which speaks for itself"
    
    * 'x86-microcode-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/microcode: Bump driver version, update copyrights
      x86/microcode: Rework microcode loading
      x86/microcode/intel: Remove intel_lib.c
      x86/microcode/amd: Move private inlines to .c and mark local functions static
      x86/microcode: Collect CPU info on resume
      x86/microcode: Issue the debug printk on resume only on success
      x86/microcode/amd: Hand down the CPU family
      x86/microcode: Export the microcode cache linked list
      x86/microcode: Remove one #ifdef clause
      x86/microcode/intel: Simplify generic_load_microcode()
      x86/microcode: Move driver authors to CREDITS
      x86/microcode: Run the AP-loading routine only on the application processors

commit 212f30008a284a9312d95dad6cc237ff81173d73
Merge: 6f3be0f04354 34bc3560c657
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:55:04 2016 -0800

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 idle updates from Ingo Molnar:
     "There were two bigger changes in this development cycle:
    
       - remove idle notifiers:
    
           32 files changed, 74 insertions(+), 803 deletions(-)
    
         These notifiers were of questionable value and the main usecase,
         the i7300 driver, was essentially unmaintained and can be removed,
         plus modern power management concepts don't need the callback - so
         use this golden opportunity and get rid of this opaque and fragile
         callback from a latency sensitive code path.
    
         (Len Brown, Thomas Gleixner)
    
       - improve the AMD Erratum 400 workaround that used high overhead MSR
         polling in the idle loop (Borisla Petkov, Thomas Gleixner)"
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove empty idle.h header
      x86/amd: Simplify AMD E400 aware idle routine
      x86/amd: Check for the C1E bug post ACPI subsystem init
      x86/bugs: Separate AMD E400 erratum and C1E bug
      x86/cpufeature: Provide helper to set bugs bits
      x86/idle: Remove enter_idle(), exit_idle()
      x86: Remove x86_test_and_clear_bit_percpu()
      x86/idle: Remove is_idle flag
      x86/idle: Remove idle_notifier
      i7300_idle: Remove this driver

commit 535b2f73f6f60fb227b700136c134c5d7c8f8ad3
Merge: ef486c599a1f b6a50cddbcbd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:25:21 2016 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 CPU updates from Ingo Molnar:
     "The changes in this development cycle were:
    
       - AMD CPU topology enhancements that are cleanups on current CPUs but
         which enable future Fam17 hardware. (Yazen Ghannam)
    
       - unify bugs.c and bugs_64.c (Borislav Petkov)
    
       - remove the show_msr= boot option (Borislav Petkov)
    
       - simplify a boot message (Borislav Petkov)"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpu/AMD: Clean up cpu_llc_id assignment per topology feature
      x86/cpu: Get rid of the show_msr= boot option
      x86/cpu: Merge bugs.c and bugs_64.c
      x86/cpu: Remove the printk format specifier in "CPU0: "

commit 07c94a38125376d70d156bd8bff98ddfe4c8ea95
Author: Borislav Petkov <bp@alien8.de>
Date:   Fri Dec 9 19:29:11 2016 +0100

    x86/amd: Simplify AMD E400 aware idle routine
    
    Reorganize the E400 detection now that we have everything in place:
    switch the CPUs to broadcast mode after the LAPIC has been initialized
    and remove the facilities that were used previously on the idle path.
    
    Unfortunately static_cpu_has_bug() cannpt be used in the E400 idle routine
    because alternatives have been applied when the actual detection happens,
    so the static switching does not take effect and the test will stay
    false. Use boot_cpu_has_bug() instead which is definitely an improvement
    over the RDMSR and the cpumask handling.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-5-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9bd910a7dd0a..e1f98ff9a3f0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1144,7 +1144,6 @@ void enable_sep_cpu(void)
 void __init identify_boot_cpu(void)
 {
 	identify_cpu(&boot_cpu_data);
-	init_amd_e400_c1e_mask();
 #ifdef CONFIG_X86_32
 	sysenter_setup();
 	enable_sep_cpu();

commit d49597fd3bc7d9534de55e9256767f073be1b33a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 9 16:35:51 2016 +0100

    x86/cpu: Deal with broken firmware (VMWare/XEN)
    
    Both ACPI and MP specifications require that the APIC id in the respective
    tables must be the same as the APIC id in CPUID.
    
    The kernel retrieves the physical package id from the APIC id during the
    ACPI/MP table scan and builds the physical to logical package map. The
    physical package id which is used after a CPU comes up is retrieved from
    CPUID. So we rely on ACPI/MP tables and CPUID agreeing in that respect.
    
    There exist VMware and XEN implementations which violate the spec. As a
    result the physical to logical package map, which relies on the ACPI/MP
    tables does not work on those systems, because the CPUID initialized
    physical package id does not match the firmware id. This causes system
    crashes and malfunction due to invalid package mappings.
    
    The only way to cure this is to sanitize the physical package id after the
    CPUID enumeration and yell when the APIC ids are different. Fix up the
    initial APIC id, which is fine as it is only used printout purposes.
    
    If the physical package IDs differ yell and use the package information
    from the ACPI/MP tables so the existing logical package map just works.
    
    Chas provided the resulting dmesg output for his affected 4 virtual
    sockets, 1 core per socket VM:
    
    [Firmware Bug]: CPU1: APIC id mismatch. Firmware: 1 CPUID: 2
    [Firmware Bug]: CPU1: Using firmware package id 1 instead of 2
    ....
    
    Reported-and-tested-by: "Charles (Chas) Williams" <ciwillia@brocade.com>,
    Reported-by: M. Vefa Bicakci <m.v.b@runbox.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: #4.6+ <stable@vger,kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1611091613540.3501@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9bd910a7dd0a..cc9e980c68ec 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -978,6 +978,35 @@ static void x86_init_cache_qos(struct cpuinfo_x86 *c)
 	}
 }
 
+/*
+ * The physical to logical package id mapping is initialized from the
+ * acpi/mptables information. Make sure that CPUID actually agrees with
+ * that.
+ */
+static void sanitize_package_id(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_SMP
+	unsigned int pkg, apicid, cpu = smp_processor_id();
+
+	apicid = apic->cpu_present_to_apicid(cpu);
+	pkg = apicid >> boot_cpu_data.x86_coreid_bits;
+
+	if (apicid != c->initial_apicid) {
+		pr_err(FW_BUG "CPU%u: APIC id mismatch. Firmware: %x CPUID: %x\n",
+		       cpu, apicid, c->initial_apicid);
+		c->initial_apicid = apicid;
+	}
+	if (pkg != c->phys_proc_id) {
+		pr_err(FW_BUG "CPU%u: Using firmware package id %u instead of %u\n",
+		       cpu, pkg, c->phys_proc_id);
+		c->phys_proc_id = pkg;
+	}
+	c->logical_proc_id = topology_phys_to_logical_pkg(pkg);
+#else
+	c->logical_proc_id = 0;
+#endif
+}
+
 /*
  * This does the hard work of actually picking apart the CPU stuff...
  */
@@ -1103,8 +1132,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 #ifdef CONFIG_NUMA
 	numa_add_cpu(smp_processor_id());
 #endif
-	/* The boot/hotplug time assigment got cleared, restore it */
-	c->logical_proc_id = topology_phys_to_logical_pkg(c->phys_proc_id);
+	sanitize_package_id(c);
 }
 
 /*

commit 777284b66f2326c7cb4c541e2224b638b562a9d3
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Oct 25 11:55:11 2016 +0200

    x86/microcode: Run the AP-loading routine only on the application processors
    
    cpu_init() is run also on the BSP (in addition to the APs):
    
     x86_64_start_kernel
     |-> x86_64_start_reservations
     |-> start_kernel
     |-> trap_init
     |-> cpu_init
     |-> load_ucode_ap
     ...
    
    but we run the AP (Application Processors) microcode loading routine
    there too even though we have a BSP-specific routine for that:
    load_ucode_bsp().
    
    Which is unnecessary. So let's limit the AP microcode loading routine to
    the APs only.
    
    Remove a useless comment while at it.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161025095522.11964-2-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9bd910a7dd0a..1a1d668263b9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1462,11 +1462,8 @@ void cpu_init(void)
 	 */
 	cr4_init_shadow();
 
-	/*
-	 * Load microcode on this cpu if a valid microcode is available.
-	 * This is early microcode loading procedure.
-	 */
-	load_ucode_ap();
+	if (cpu)
+		load_ucode_ap();
 
 	t = &per_cpu(cpu_tss, cpu);
 	oist = &per_cpu(orig_ist, cpu);

commit 59c6f278bdeea4147e8be92a3ed50a9907e60088
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Oct 24 19:38:44 2016 +0200

    x86/cpu: Get rid of the show_msr= boot option
    
    It is useless as it dumps the MSRs too early BUT(!) we do set MSRs later too.
    Also, it dumps only BSP MSRs as it gets called only for CPU 0.
    
    And the MSR range array would need constant updating anyway, and so on
    and so on...
    
    Oh, and we have msr.ko and msr-tools which are the much better solution
    anyway. So off it goes...
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161024173844.23038-4-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9bd910a7dd0a..2e6719f1b620 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1162,51 +1162,6 @@ void identify_secondary_cpu(struct cpuinfo_x86 *c)
 	mtrr_ap_init();
 }
 
-struct msr_range {
-	unsigned	min;
-	unsigned	max;
-};
-
-static const struct msr_range msr_range_array[] = {
-	{ 0x00000000, 0x00000418},
-	{ 0xc0000000, 0xc000040b},
-	{ 0xc0010000, 0xc0010142},
-	{ 0xc0011000, 0xc001103b},
-};
-
-static void __print_cpu_msr(void)
-{
-	unsigned index_min, index_max;
-	unsigned index;
-	u64 val;
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(msr_range_array); i++) {
-		index_min = msr_range_array[i].min;
-		index_max = msr_range_array[i].max;
-
-		for (index = index_min; index < index_max; index++) {
-			if (rdmsrl_safe(index, &val))
-				continue;
-			pr_info(" MSR%08x: %016llx\n", index, val);
-		}
-	}
-}
-
-static int show_msr;
-
-static __init int setup_show_msr(char *arg)
-{
-	int num;
-
-	get_option(&arg, &num);
-
-	if (num > 0)
-		show_msr = num;
-	return 1;
-}
-__setup("show_msr=", setup_show_msr);
-
 static __init int setup_noclflush(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_CLFLUSH);
@@ -1240,14 +1195,6 @@ void print_cpu_info(struct cpuinfo_x86 *c)
 		pr_cont(", stepping: 0x%x)\n", c->x86_mask);
 	else
 		pr_cont(")\n");
-
-	print_cpu_msr(c);
-}
-
-void print_cpu_msr(struct cpuinfo_x86 *c)
-{
-	if (c->cpu_index < show_msr)
-		__print_cpu_msr();
 }
 
 static __init int setup_disablecpuid(char *arg)

commit 1a4a2bc460721bc8f91e4c1294d39b38e5af132f
Merge: 110a9e42b687 1ef55be16ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 16:13:28 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull low-level x86 updates from Ingo Molnar:
     "In this cycle this topic tree has become one of those 'super topics'
      that accumulated a lot of changes:
    
       - Add CONFIG_VMAP_STACK=y support to the core kernel and enable it on
         x86 - preceded by an array of changes. v4.8 saw preparatory changes
         in this area already - this is the rest of the work. Includes the
         thread stack caching performance optimization. (Andy Lutomirski)
    
       - switch_to() cleanups and all around enhancements. (Brian Gerst)
    
       - A large number of dumpstack infrastructure enhancements and an
         unwinder abstraction. The secret long term plan is safe(r) live
         patching plus maybe another attempt at debuginfo based unwinding -
         but all these current bits are standalone enhancements in a frame
         pointer based debug environment as well. (Josh Poimboeuf)
    
       - More __ro_after_init and const annotations. (Kees Cook)
    
       - Enable KASLR for the vmemmap memory region. (Thomas Garnier)"
    
    [ The virtually mapped stack changes are pretty fundamental, and not
      x86-specific per se, even if they are only used on x86 right now. ]
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/asm: Get rid of __read_cr4_safe()
      thread_info: Use unsigned long for flags
      x86/alternatives: Add stack frame dependency to alternative_call_2()
      x86/dumpstack: Fix show_stack() task pointer regression
      x86/dumpstack: Remove dump_trace() and related callbacks
      x86/dumpstack: Convert show_trace_log_lvl() to use the new unwinder
      oprofile/x86: Convert x86_backtrace() to use the new unwinder
      x86/stacktrace: Convert save_stack_trace_*() to use the new unwinder
      perf/x86: Convert perf_callchain_kernel() to use the new unwinder
      x86/unwind: Add new unwind interface and implementations
      x86/dumpstack: Remove NULL task pointer convention
      fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y
      sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
      lib/syscall: Pin the task stack in collect_syscall()
      x86/process: Pin the target stack in get_wchan()
      x86/dumpstack: Pin the target stack when dumping it
      kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function
      sched/core: Add try_get_task_stack() and put_task_stack()
      x86/entry/64: Fix a minor comment rebase error
      iommu/amd: Don't put completion-wait semaphore on stack
      ...

commit 05fb3c199bb09f5b85de56cc3ede194ac95c5e1f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Wed Sep 28 16:06:33 2016 -0700

    x86/boot: Initialize FPU and X86_FEATURE_ALWAYS even if we don't have CPUID
    
    Otherwise arch_task_struct_size == 0 and we die.  While we're at it,
    set X86_FEATURE_ALWAYS, too.
    
    Reported-by: David Saggiorato <david@saggiorato.net>
    Tested-by: David Saggiorato <david@saggiorato.net>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Fixes: aaeb5c01c5b ("x86/fpu, sched: Introduce CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT and use it on x86")
    Link: http://lkml.kernel.org/r/8de723afbf0811071185039f9088733188b606c9.1475103911.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 809eda03c527..bcc9ccc220c9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -804,21 +804,20 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		identify_cpu_without_cpuid(c);
 
 	/* cyrix could have cpuid enabled via c_identify()*/
-	if (!have_cpuid_p())
-		return;
+	if (have_cpuid_p()) {
+		cpu_detect(c);
+		get_cpu_vendor(c);
+		get_cpu_cap(c);
 
-	cpu_detect(c);
-	get_cpu_vendor(c);
-	get_cpu_cap(c);
-
-	if (this_cpu->c_early_init)
-		this_cpu->c_early_init(c);
+		if (this_cpu->c_early_init)
+			this_cpu->c_early_init(c);
 
-	c->cpu_index = 0;
-	filter_cpuid_features(c, false);
+		c->cpu_index = 0;
+		filter_cpuid_features(c, false);
 
-	if (this_cpu->c_bsp_init)
-		this_cpu->c_bsp_init(c);
+		if (this_cpu->c_bsp_init)
+			this_cpu->c_bsp_init(c);
+	}
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 	fpu__init_system(c);

commit 2b3061c77ce7e429b25a25560ba088e8e7193a67
Merge: 01175255fd8e 25dfe4785332
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Sep 8 08:41:52 2016 +0200

    Merge branch 'x86/mm' into x86/asm, to unify the two branches for simplicity
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 556b6723689694ac9134bcc36a07828168e057f4
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Aug 23 19:23:56 2016 +0200

    x86/entry: Remove outdated comment about SYSCALL targets
    
    The comment probably meant some old AMD64 incarnation which most likely
    never saw the light of day. STAR and LSTAR are two different registers
    and STAR sets CS/SS(DS) selectors for *all* modes, not only 32-bit.
    
    So simply remove that comment.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160823172356.15879-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6ef55e83fb8a..e374c19b9ddc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1305,11 +1305,6 @@ static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
-	/*
-	 * LSTAR and STAR live in a bit strange symbiosis.
-	 * They both write to the same internal register. STAR allows to
-	 * set CS/DS but only a 32bit target. LSTAR sets the 64bit rip.
-	 */
 	wrmsr(MSR_STAR, 0, (__USER32_CS << 16) | __KERNEL_CS);
 	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 

commit 4950d6d48a0c43cc61d0bbb76fb10e0214b79c66
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Aug 18 10:59:08 2016 -0500

    x86/dumpstack: Remove 64-byte gap at end of irq stack
    
    There has been a 64-byte gap at the end of the irq stack for at least 12
    years.  It predates git history, and I can't find any good reason for
    it.  Remove it.  What's the worst that could happen?
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/14f9281c5475cc44af95945ea7546bff2e3836db.1471535549.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 809eda03c527..6ef55e83fb8a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1281,7 +1281,7 @@ DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =
 EXPORT_PER_CPU_SYMBOL(current_task);
 
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
-	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
+	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE;
 
 DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 

commit 404f6aac9b3ef595735feca99979db084ea48315
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Aug 8 16:29:06 2016 -0700

    x86: Apply more __ro_after_init and const
    
    Guided by grsecurity's analogous __read_only markings in arch/x86,
    this applies several uses of __ro_after_init to structures that are
    only updated during __init, and const for some structures that are
    never updated.  Additionally extends __init markings to some functions
    that are only used during __init, and cleans up some missing C99 style
    static initializers.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Brown <david.brown@linaro.org>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Emese Revfy <re.emese@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-hardening@lists.openwall.com
    Link: http://lkml.kernel.org/r/20160808232906.GA29731@www.outflux.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 809eda03c527..d3b91be4873b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1265,9 +1265,14 @@ static __init int setup_disablecpuid(char *arg)
 __setup("clearcpuid=", setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
-struct desc_ptr idt_descr = { NR_VECTORS * 16 - 1, (unsigned long) idt_table };
-struct desc_ptr debug_idt_descr = { NR_VECTORS * 16 - 1,
-				    (unsigned long) debug_idt_table };
+struct desc_ptr idt_descr __ro_after_init = {
+	.size = NR_VECTORS * 16 - 1,
+	.address = (unsigned long) idt_table,
+};
+const struct desc_ptr debug_idt_descr = {
+	.size = NR_VECTORS * 16 - 1,
+	.address = (unsigned long) debug_idt_table,
+};
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE) __visible;

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit fb59831b496a5bb7d0a06c7e702d88d1757edfca
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 14 13:22:58 2016 -0700

    x86/smp: Remove stack_smp_processor_id()
    
    It serves no purpose -- raw_smp_processor_id() works fine.  This
    change will be needed to move thread_info off the stack.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a2bf4f07fbc30fb32f9f7f3f8f94ad3580823847.1468527351.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0fe6953f421c..d22a7b9c4f0e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1452,7 +1452,7 @@ void cpu_init(void)
 	struct task_struct *me;
 	struct tss_struct *t;
 	unsigned long v;
-	int cpu = stack_smp_processor_id();
+	int cpu = raw_smp_processor_id();
 	int i;
 
 	wait_for_master_cpu(cpu);

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0fe6953f421c..ccf13ab9c7fc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -2,7 +2,7 @@
 #include <linux/linkage.h>
 #include <linux/bitops.h>
 #include <linux/kernel.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/percpu.h>
 #include <linux/string.h>
 #include <linux/ctype.h>

commit 06cd3d8c14bdd06f49f1c6a06acf219749c5598e
Merge: 0f6ff2bce0d4 bc231d9ede99
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri May 20 09:09:26 2016 +0200

    Merge branch 'linus' into x86/urgent, to refresh the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0f6ff2bce0d4c3e4ff34f5d2ffb7329025b30844
Author: Dave Hansen <dave@sr71.net>
Date:   Thu May 12 15:04:00 2016 -0700

    x86/mm/mpx: Work around MPX erratum SKD046
    
    This erratum essentially causes the CPU to forget which privilege
    level it is operating on (kernel vs. user) for the purposes of MPX.
    
    This erratum can only be triggered when a system is not using
    Supervisor Mode Execution Prevention (SMEP).  Our workaround for
    the erratum is to ensure that MPX can only be used in cases where
    SMEP is present in the processor and is enabled.
    
    This erratum only affects Core processors.  Atom is unaffected.
    But, there is no architectural way to determine Atom vs. Core.
    So, we just apply this workaround to all processors.  It's
    possible that it will mistakenly disable MPX on some Atom
    processsors or future unaffected Core processors.  There are
    currently no processors that have MPX and not SMEP.  It would
    take something akin to a hypervisor masking SMEP out on an Atom
    processor for this to present itself on current hardware.
    
    More details can be found at:
    
      http://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/desktop-6th-gen-core-family-spec-update.pdf
    
    "
      SKD046 Branch Instructions May Initialize MPX Bound Registers Incorrectly
    
      Problem:
    
      Depending on the current Intel MPX (Memory Protection
      Extensions) configuration, execution of certain branch
      instructions (near CALL, near RET, near JMP, and Jcc
      instructions) without a BND prefix (F2H) initialize the MPX bound
      registers. Due to this erratum, such a branch instruction that is
      executed both with CPL = 3 and with CPL < 3 may not use the
      correct MPX configuration register (BNDCFGU or BNDCFGS,
      respectively) for determining whether to initialize the bound
      registers; it may thus initialize the bound registers when it
      should not, or fail to initialize them when it should.
    
      Implication:
    
      A branch instruction that has executed both in user mode and in
      supervisor mode (from the same linear address) may cause a #BR
      (bound range fault) when it should not have or may not cause a
      #BR when it should have.  Workaround An operating system can
      avoid this erratum by setting CR4.SMEP[bit 20] to enable
      supervisor-mode execution prevention (SMEP). When SMEP is
      enabled, no code can be executed both with CPL = 3 and with CPL < 3.
    "
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160512220400.3B35F1BC@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f45a4b9d28c8..62ff5255ae16 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -37,6 +37,7 @@
 #include <asm/mtrr.h>
 #include <linux/numa.h>
 #include <asm/asm.h>
+#include <asm/bugs.h>
 #include <asm/cpu.h>
 #include <asm/mce.h>
 #include <asm/msr.h>
@@ -270,6 +271,8 @@ static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 static __init int setup_disable_smep(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_SMEP);
+	/* Check for things that depend on SMEP being enabled: */
+	check_mpx_erratum(&boot_cpu_data);
 	return 1;
 }
 __setup("nosmep", setup_disable_smep);

commit 168f1a7163b37294a0ef33829e1ed54d41e33c42
Merge: 825a3b2605c3 4afd0565552c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 15:15:17 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - MSR access API fixes and enhancements (Andy Lutomirski)
    
       - early exception handling improvements (Andy Lutomirski)
    
       - user-space FS/GS prctl usage fixes and improvements (Andy
         Lutomirski)
    
       - Remove the cpu_has_*() APIs and replace them with equivalents
         (Borislav Petkov)
    
       - task switch micro-optimization (Brian Gerst)
    
       - 32-bit entry code simplification (Denys Vlasenko)
    
       - enhance PAT handling in enumated CPUs (Toshi Kani)
    
      ... and lots of other cleanups/fixlets"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/arch_prctl/64: Restore accidentally removed put_cpu() in ARCH_SET_GS
      x86/entry/32: Remove asmlinkage_protect()
      x86/entry/32: Remove GET_THREAD_INFO() from entry code
      x86/entry, sched/x86: Don't save/restore EFLAGS on task switch
      x86/asm/entry/32: Simplify pushes of zeroed pt_regs->REGs
      selftests/x86/ldt_gdt: Test set_thread_area() deletion of an active segment
      x86/tls: Synchronize segment registers in set_thread_area()
      x86/asm/64: Rename thread_struct's fs and gs to fsbase and gsbase
      x86/arch_prctl/64: Remove FSBASE/GSBASE < 4G optimization
      x86/segments/64: When load_gs_index fails, clear the base
      x86/segments/64: When loadsegment(fs, ...) fails, clear the base
      x86/asm: Make asm/alternative.h safe from assembly
      x86/asm: Stop depending on ptrace.h in alternative.h
      x86/entry: Rename is_{ia32,x32}_task() to in_{ia32,x32}_syscall()
      x86/asm: Make sure verify_cpu() has a good stack
      x86/extable: Add a comment about early exception handlers
      x86/msr: Set the return value to zero when native_rdmsr_safe() fails
      x86/paravirt: Make "unsafe" MSR accesses unsafe even if PARAVIRT=y
      x86/paravirt: Add paravirt_{read,write}_msr()
      x86/msr: Carry on after a non-"safe" MSR access fails
      ...

commit e8df1a95b685af84a81698199ee206e0e66a8b44
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri May 13 15:13:28 2016 -0700

    x86/cpufeature, x86/mm/pkeys: Fix broken compile-time disabling of pkeys
    
    When I added support for the Memory Protection Keys processor
    feature, I had to reindent the REQUIRED/DISABLED_MASK macros, and
    also consult the later cpufeature words.
    
    I'm not quite sure how I bungled it, but I consulted the wrong
    word at the end.  This only affected required or disabled cpu
    features in cpufeature words 14, 15 and 16.  So, only Protection
    Keys itself was screwed over here.
    
    The result was that if you disabled pkeys in your .config, you
    might still see some code show up that should have been compiled
    out.  There should be no functional problems, though.
    
    In verifying this patch I also realized that the DISABLE_PKU/OSPKE
    macros were defined backwards and that the cpu_has() check in
    setup_pku() was not doing the compile-time disabled checks.
    
    So also fix the macro for DISABLE_PKU/OSPKE and add a compile-time
    check for pkeys being enabled in setup_pku().
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: <stable@vger.kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Fixes: dfb4a70f20c5 ("x86/cpufeature, x86/mm/pkeys: Add protection keys related CPUID definitions")
    Link: http://lkml.kernel.org/r/20160513221328.C200930B@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8394b3d1f94f..f45a4b9d28c8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -310,6 +310,10 @@ static bool pku_disabled;
 
 static __always_inline void setup_pku(struct cpuinfo_x86 *c)
 {
+	/* check the boot processor, plus compile options for PKU: */
+	if (!cpu_feature_enabled(X86_FEATURE_PKU))
+		return;
+	/* checks the actual processor's cpuid bits: */
 	if (!cpu_has(c, X86_FEATURE_PKU))
 		return;
 	if (pku_disabled)

commit 71faad43060d3d2040583635fbf7d1bdb3d04118
Author: Yazen Ghannam <Yazen.Ghannam@amd.com>
Date:   Wed May 11 14:58:26 2016 +0200

    x86/cpu: Add detection of AMD RAS Capabilities
    
    Add a new CPUID leaf to hold the contents of CPUID 0x80000007_EBX (RasCap).
    
    Define bits that are currently in use:
    
     Bit 0: McaOverflowRecov
     Bit 1: SUCCOR
     Bit 3: ScalableMca
    
    Signed-off-by: Yazen Ghannam <Yazen.Ghannam@amd.com>
    [ Shorten comment. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-edac <linux-edac@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1462971509-3856-5-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8394b3d1f94f..dbc6f066e231 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -717,6 +717,13 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		}
 	}
 
+	if (c->extended_cpuid_level >= 0x80000007) {
+		cpuid(0x80000007, &eax, &ebx, &ecx, &edx);
+
+		c->x86_capability[CPUID_8000_0007_EBX] = ebx;
+		c->x86_power = edx;
+	}
+
 	if (c->extended_cpuid_level >= 0x80000008) {
 		cpuid(0x80000008, &eax, &ebx, &ecx, &edx);
 
@@ -729,9 +736,6 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_phys_bits = 36;
 #endif
 
-	if (c->extended_cpuid_level >= 0x80000007)
-		c->x86_power = cpuid_edx(0x80000007);
-
 	if (c->extended_cpuid_level >= 0x8000000a)
 		c->x86_capability[CPUID_8000_000A_EDX] = cpuid_edx(0x8000000a);
 

commit 45e876f794e8e566bf827c25ef0791875081724f
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Apr 26 12:23:26 2016 -0700

    x86/segments/64: When loadsegment(fs, ...) fails, clear the base
    
    On AMD CPUs, a failed loadsegment currently may not clear the FS
    base.  Fix it.
    
    While we're at it, prevent loadsegment(gs, xyz) from even compiling
    on 64-bit kernels.  It shouldn't be used.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a084c1b93b7b1408b58d3fd0b5d6e47da8e7d7cf.1461698311.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6bfa36de6d9f..088106140c4b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -430,7 +430,7 @@ void load_percpu_segment(int cpu)
 #ifdef CONFIG_X86_32
 	loadsegment(fs, __KERNEL_PERCPU);
 #else
-	loadsegment(gs, 0);
+	__loadsegment_simple(gs, 0);
 	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
 #endif
 	load_stack_canary_segment();

commit 59e21e3d00e6bc23186763c3e0bf11baf8924124
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 4 22:24:59 2016 +0200

    x86/cpufeature: Replace cpu_has_tsc with boot_cpu_has() usage
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Sailer <t.sailer@alumni.ethz.ch>
    Link: http://lkml.kernel.org/r/1459801503-15600-7-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 28d3255edf00..6bfa36de6d9f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1558,7 +1558,7 @@ void cpu_init(void)
 	pr_info("Initializing CPU#%d\n", cpu);
 
 	if (cpu_feature_enabled(X86_FEATURE_VME) ||
-	    cpu_has_tsc ||
+	    boot_cpu_has(X86_FEATURE_TSC) ||
 	    boot_cpu_has(X86_FEATURE_DE))
 		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 

commit 0230bb038fa99af0c425fc4cffed307e545a9642
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Apr 7 17:31:48 2016 -0700

    x86/cpu: Move X86_BUG_ESPFIX initialization to generic_identify()
    
    It was in detect_nopl(), which was either a mistake by me or some kind
    of mis-merge.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rudolf Marek <r.marek@assembler.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: ff236456f072 ("x86/cpu: Move X86_BUG_ESPFIX initialization to generic_identify")
    Link: http://lkml.kernel.org/r/0949337f13660461edca08ab67d1a841441289c9.1460075211.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8e40eee5843a..28d3255edf00 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -861,31 +861,6 @@ static void detect_nopl(struct cpuinfo_x86 *c)
 	clear_cpu_cap(c, X86_FEATURE_NOPL);
 #else
 	set_cpu_cap(c, X86_FEATURE_NOPL);
-#endif
-
-	/*
-	 * ESPFIX is a strange bug.  All real CPUs have it.  Paravirt
-	 * systems that run Linux at CPL > 0 may or may not have the
-	 * issue, but, even if they have the issue, there's absolutely
-	 * nothing we can do about it because we can't use the real IRET
-	 * instruction.
-	 *
-	 * NB: For the time being, only 32-bit kernels support
-	 * X86_BUG_ESPFIX as such.  64-bit kernels directly choose
-	 * whether to apply espfix using paravirt hooks.  If any
-	 * non-paravirt system ever shows up that does *not* have the
-	 * ESPFIX issue, we can change this.
-	 */
-#ifdef CONFIG_X86_32
-#ifdef CONFIG_PARAVIRT
-	do {
-		extern void native_iret(void);
-		if (pv_cpu_ops.iret == native_iret)
-			set_cpu_bug(c, X86_BUG_ESPFIX);
-	} while (0);
-#else
-	set_cpu_bug(c, X86_BUG_ESPFIX);
-#endif
 #endif
 }
 
@@ -952,6 +927,31 @@ static void generic_identify(struct cpuinfo_x86 *c)
 	detect_nopl(c);
 
 	detect_null_seg_behavior(c);
+
+	/*
+	 * ESPFIX is a strange bug.  All real CPUs have it.  Paravirt
+	 * systems that run Linux at CPL > 0 may or may not have the
+	 * issue, but, even if they have the issue, there's absolutely
+	 * nothing we can do about it because we can't use the real IRET
+	 * instruction.
+	 *
+	 * NB: For the time being, only 32-bit kernels support
+	 * X86_BUG_ESPFIX as such.  64-bit kernels directly choose
+	 * whether to apply espfix using paravirt hooks.  If any
+	 * non-paravirt system ever shows up that does *not* have the
+	 * ESPFIX issue, we can change this.
+	 */
+#ifdef CONFIG_X86_32
+# ifdef CONFIG_PARAVIRT
+	do {
+		extern void native_iret(void);
+		if (pv_cpu_ops.iret == native_iret)
+			set_cpu_bug(c, X86_BUG_ESPFIX);
+	} while (0);
+# else
+	set_cpu_bug(c, X86_BUG_ESPFIX);
+# endif
+#endif
 }
 
 static void x86_init_cache_qos(struct cpuinfo_x86 *c)

commit 7a5d67048745e3eab62779c6d043a2e3d95dc848
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Apr 7 17:31:46 2016 -0700

    x86/cpu: Probe the behavior of nulling out a segment at boot time
    
    AMD and Intel do different things when writing zero to a segment
    selector.  Since neither vendor documents the behavior well and it's
    easy to test the behavior, try nulling fs to see what happens.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rudolf Marek <r.marek@assembler.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/61588ba0e0df35beafd363dc8b68a4c5878ef095.1460075211.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7fea4079d102..8e40eee5843a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -889,6 +889,35 @@ static void detect_nopl(struct cpuinfo_x86 *c)
 #endif
 }
 
+static void detect_null_seg_behavior(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_64
+	/*
+	 * Empirically, writing zero to a segment selector on AMD does
+	 * not clear the base, whereas writing zero to a segment
+	 * selector on Intel does clear the base.  Intel's behavior
+	 * allows slightly faster context switches in the common case
+	 * where GS is unused by the prev and next threads.
+	 *
+	 * Since neither vendor documents this anywhere that I can see,
+	 * detect it directly instead of hardcoding the choice by
+	 * vendor.
+	 *
+	 * I've designated AMD's behavior as the "bug" because it's
+	 * counterintuitive and less friendly.
+	 */
+
+	unsigned long old_base, tmp;
+	rdmsrl(MSR_FS_BASE, old_base);
+	wrmsrl(MSR_FS_BASE, 1);
+	loadsegment(fs, 0);
+	rdmsrl(MSR_FS_BASE, tmp);
+	if (tmp != 0)
+		set_cpu_bug(c, X86_BUG_NULL_SEG);
+	wrmsrl(MSR_FS_BASE, old_base);
+#endif
+}
+
 static void generic_identify(struct cpuinfo_x86 *c)
 {
 	c->extended_cpuid_level = 0;
@@ -921,6 +950,8 @@ static void generic_identify(struct cpuinfo_x86 *c)
 	get_model_name(c); /* Default name */
 
 	detect_nopl(c);
+
+	detect_null_seg_behavior(c);
 }
 
 static void x86_init_cache_qos(struct cpuinfo_x86 *c)

commit b3edfda4382ffaef5e5c1cffb25a33b3b9ef4546
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 16 13:19:29 2016 +0100

    x86/cpu: Do the feature test first in enable_sep_cpu()
    
    ... before assigning local vars. Kill out label too and simplify.
    
    No functionality change.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1458130769-24963-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8394b3d1f94f..7fea4079d102 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1076,12 +1076,12 @@ void enable_sep_cpu(void)
 	struct tss_struct *tss;
 	int cpu;
 
+	if (!boot_cpu_has(X86_FEATURE_SEP))
+		return;
+
 	cpu = get_cpu();
 	tss = &per_cpu(cpu_tss, cpu);
 
-	if (!boot_cpu_has(X86_FEATURE_SEP))
-		goto out;
-
 	/*
 	 * We cache MSR_IA32_SYSENTER_CS's value in the TSS's ss1 field --
 	 * see the big comment in struct x86_hw_tss's definition.
@@ -1096,7 +1096,6 @@ void enable_sep_cpu(void)
 
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
 
-out:
 	put_cpu();
 }
 #endif

commit 3fa2fe2ce09c5a16be69c5319eb3347271a99735
Merge: d88f48e12821 05f5ece76a88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 10:02:14 2016 -0700

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Ingo Molnar:
     "This tree contains various perf fixes on the kernel side, plus three
      hw/event-enablement late additions:
    
       - Intel Memory Bandwidth Monitoring events and handling
       - the AMD Accumulated Power Mechanism reporting facility
       - more IOMMU events
    
      ... and a final round of perf tooling updates/fixes"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      perf llvm: Use strerror_r instead of the thread unsafe strerror one
      perf llvm: Use realpath to canonicalize paths
      perf tools: Unexport some methods unused outside strbuf.c
      perf probe: No need to use formatting strbuf method
      perf help: Use asprintf instead of adhoc equivalents
      perf tools: Remove unused perf_pathdup, xstrdup functions
      perf tools: Do not include stringify.h from the kernel sources
      tools include: Copy linux/stringify.h from the kernel
      tools lib traceevent: Remove redundant CPU output
      perf tools: Remove needless 'extern' from function prototypes
      perf tools: Simplify die() mechanism
      perf tools: Remove unused DIE_IF macro
      perf script: Remove lots of unused arguments
      perf thread: Rename perf_event__preprocess_sample_addr to thread__resolve
      perf machine: Rename perf_event__preprocess_sample to machine__resolve
      perf tools: Add cpumode to struct perf_sample
      perf tests: Forward the perf_sample in the dwarf unwind test
      perf tools: Remove misplaced __maybe_unused
      perf list: Fix documentation of :ppp
      perf bench numa: Fix assertion for nodes bitfield
      ...

commit d88f48e12821ab4b2244124d50ac094568f48db5
Merge: be53f58fa0fc 9da77666d697
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 09:47:32 2016 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes:
    
       - fix hotplug bugs
       - fix irq live lock
       - fix various topology handling bugs
       - fix APIC ACK ordering
       - fix PV iopl handling
       - fix speling
       - fix/tweak memcpy_mcsafe() return value
       - fix fbcon bug
       - remove stray prototypes"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/msr: Remove unused native_read_tscp()
      x86/apic: Remove declaration of unused hw_nmi_is_cpu_stuck
      x86/oprofile/nmi: Add missing hotplug FROZEN handling
      x86/hpet: Use proper mask to modify hotplug action
      x86/apic/uv: Fix the hotplug notifier
      x86/apb/timer: Use proper mask to modify hotplug action
      x86/topology: Use total_cpus not nr_cpu_ids for logical packages
      x86/topology: Fix Intel HT disable
      x86/topology: Fix logical package mapping
      x86/irq: Cure live lock in fixup_irqs()
      x86/tsc: Prevent NULL pointer deref in calibrate_delay_is_known()
      x86/apic: Fix suspicious RCU usage in smp_trace_call_function_interrupt()
      x86/iopl: Fix iopl capability check on Xen PV
      x86/iopl/64: Properly context-switch IOPL on Xen PV
      selftests/x86: Add an iopl test
      x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe()
      x86/video: Don't assume all FB devices are PCI devices
      arch/x86/irq: Purge useless handler declarations from hw_irq.h
      x86: Fix misspellings in comments

commit 33c3cc7acfd95968d74247f1a4e1b0727a07ed43
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Thu Mar 10 15:32:09 2016 -0800

    perf/x86/mbm: Add Intel Memory B/W Monitoring enumeration and init
    
    The MBM init patch enumerates the Intel MBM (Memory b/w monitoring)
    and initializes the perf events and datastructures for monitoring the
    memory b/w.
    
    Its based on original patch series by Tony Luck and Kanaka Juvva.
    
    Memory bandwidth monitoring (MBM) provides OS/VMM a way to monitor
    bandwidth from one level of cache to another. The current patches
    support L3 external bandwidth monitoring. It supports both 'local
    bandwidth' and 'total bandwidth' monitoring for the socket. Local
    bandwidth measures the amount of data sent through the memory controller
    on the socket and total b/w measures the total system bandwidth.
    
    Extending the cache quality of service monitoring (CQM) we add two
    more events to the perf infrastructure:
    
      intel_cqm_llc/local_bytes - bytes sent through local socket memory controller
      intel_cqm_llc/total_bytes - total L3 external bytes sent
    
    The tasks are associated with a Resouce Monitoring ID (RMID) just like
    in CQM and OS uses a MSR write to indicate the RMID of the task during
    scheduling.
    
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: fenghua.yu@intel.com
    Cc: h.peter.anvin@intel.com
    Cc: ravi.v.shankar@intel.com
    Cc: vikas.shivappa@intel.com
    Link: http://lkml.kernel.org/r/1457652732-4499-4-git-send-email-vikas.shivappa@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 62590aa064c8..e601c1286e29 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -649,7 +649,9 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 			cpuid_count(0x0000000F, 1, &eax, &ebx, &ecx, &edx);
 			c->x86_capability[CPUID_F_1_EDX] = edx;
 
-			if (cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC)) {
+			if ((cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC)) ||
+			      ((cpu_has(c, X86_FEATURE_CQM_MBM_TOTAL)) ||
+			       (cpu_has(c, X86_FEATURE_CQM_MBM_LOCAL)))) {
 				c->x86_cache_max_rmid = ecx;
 				c->x86_cache_occ_scale = ebx;
 			}

commit 643ad15d47410d37d43daf3ef1c8ac52c281efa5
Merge: 24b5e20f11a7 0d47638f80a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 20 19:08:56 2016 -0700

    Merge branch 'mm-pkeys-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 protection key support from Ingo Molnar:
     "This tree adds support for a new memory protection hardware feature
      that is available in upcoming Intel CPUs: 'protection keys' (pkeys).
    
      There's a background article at LWN.net:
    
          https://lwn.net/Articles/643797/
    
      The gist is that protection keys allow the encoding of
      user-controllable permission masks in the pte.  So instead of having a
      fixed protection mask in the pte (which needs a system call to change
      and works on a per page basis), the user can map a (handful of)
      protection mask variants and can change the masks runtime relatively
      cheaply, without having to change every single page in the affected
      virtual memory range.
    
      This allows the dynamic switching of the protection bits of large
      amounts of virtual memory, via user-space instructions.  It also
      allows more precise control of MMU permission bits: for example the
      executable bit is separate from the read bit (see more about that
      below).
    
      This tree adds the MM infrastructure and low level x86 glue needed for
      that, plus it adds a high level API to make use of protection keys -
      if a user-space application calls:
    
            mmap(..., PROT_EXEC);
    
      or
    
            mprotect(ptr, sz, PROT_EXEC);
    
      (note PROT_EXEC-only, without PROT_READ/WRITE), the kernel will notice
      this special case, and will set a special protection key on this
      memory range.  It also sets the appropriate bits in the Protection
      Keys User Rights (PKRU) register so that the memory becomes unreadable
      and unwritable.
    
      So using protection keys the kernel is able to implement 'true'
      PROT_EXEC on x86 CPUs: without protection keys PROT_EXEC implies
      PROT_READ as well.  Unreadable executable mappings have security
      advantages: they cannot be read via information leaks to figure out
      ASLR details, nor can they be scanned for ROP gadgets - and they
      cannot be used by exploits for data purposes either.
    
      We know about no user-space code that relies on pure PROT_EXEC
      mappings today, but binary loaders could start making use of this new
      feature to map binaries and libraries in a more secure fashion.
    
      There is other pending pkeys work that offers more high level system
      call APIs to manage protection keys - but those are not part of this
      pull request.
    
      Right now there's a Kconfig that controls this feature
      (CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS) that is default enabled
      (like most x86 CPU feature enablement code that has no runtime
      overhead), but it's not user-configurable at the moment.  If there's
      any serious problem with this then we can make it configurable and/or
      flip the default"
    
    * 'mm-pkeys-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (38 commits)
      x86/mm/pkeys: Fix mismerge of protection keys CPUID bits
      mm/pkeys: Fix siginfo ABI breakage caused by new u64 field
      x86/mm/pkeys: Fix access_error() denial of writes to write-only VMA
      mm/core, x86/mm/pkeys: Add execute-only protection keys support
      x86/mm/pkeys: Create an x86 arch_calc_vm_prot_bits() for VMA flags
      x86/mm/pkeys: Allow kernel to modify user pkey rights register
      x86/fpu: Allow setting of XSAVE state
      x86/mm: Factor out LDT init from context init
      mm/core, x86/mm/pkeys: Add arch_validate_pkey()
      mm/core, arch, powerpc: Pass a protection key in to calc_vm_flag_bits()
      x86/mm/pkeys: Actually enable Memory Protection Keys in the CPU
      x86/mm/pkeys: Add Kconfig prompt to existing config option
      x86/mm/pkeys: Dump pkey from VMA in /proc/pid/smaps
      x86/mm/pkeys: Dump PKRU with other kernel registers
      mm/core, x86/mm/pkeys: Differentiate instruction fetches
      x86/mm/pkeys: Optimize fault handling in access_error()
      mm/core: Do not enforce PKEY permissions on remote mm access
      um, pkeys: Add UML arch_*_access_permitted() methods
      mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys
      x86/mm/gup: Simplify get_user_pages() PTE bit handling
      ...

commit 00f526850151e91fdad0896a1436341687ad2582
Merge: cbf8b5a2b649 d89abe2a1f0c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Mar 17 09:44:57 2016 +0100

    Merge branch 'x86/cleanups' into x86/urgent
    
    Pull in some merge window leftovers.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 13c76ad87216513db2487aac84155aa57dfd46ce
Merge: 9cf8d6360c15 8b8addf891de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 10:45:39 2016 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Enable full ASLR randomization for 32-bit programs (Hector
         Marco-Gisbert)
    
       - Add initial minimal INVPCI support, to flush global mappings (Andy
         Lutomirski)
    
       - Add KASAN enhancements (Andrey Ryabinin)
    
       - Fix mmiotrace for huge pages (Karol Herbst)
    
       - ... misc cleanups and small enhancements"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm/32: Enable full randomization on i386 and X86_32
      x86/mm/kmmio: Fix mmiotrace for hugepages
      x86/mm: Avoid premature success when changing page attributes
      x86/mm/ptdump: Remove paravirt_enabled()
      x86/mm: Fix INVPCID asm constraint
      x86/dmi: Switch dmi_remap() from ioremap() [uncached] to ioremap_cache()
      x86/mm: If INVPCID is available, use it to flush global mappings
      x86/mm: Add a 'noinvpcid' boot option to turn off INVPCID
      x86/mm: Add INVPCID helpers
      x86/kasan: Write protect kasan zero shadow
      x86/kasan: Clear kasan_zero_page after TLB flush
      x86/mm/numa: Check for failures in numa_clear_kernel_node_hotplug()
      x86/mm/numa: Clean up numa_clear_kernel_node_hotplug()
      x86/mm: Make kmap_prot into a #define
      x86/mm/32: Set NX in __supported_pte_mask before enabling paging
      x86/mm: Streamline and restore probe_memory_block_size()

commit ba33ea811e1ff6726abb7f8f96df38c2d7b50304
Merge: e23604edac2a d05004944206
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 09:32:27 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "This is another big update. Main changes are:
    
       - lots of x86 system call (and other traps/exceptions) entry code
         enhancements.  In particular the complex parts of the 64-bit entry
         code have been migrated to C code as well, and a number of dusty
         corners have been refreshed.  (Andy Lutomirski)
    
       - vDSO special mapping robustification and general cleanups (Andy
         Lutomirski)
    
       - cpufeature refactoring, cleanups and speedups (Borislav Petkov)
    
       - lots of other changes ..."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (64 commits)
      x86/cpufeature: Enable new AVX-512 features
      x86/entry/traps: Show unhandled signal for i386 in do_trap()
      x86/entry: Call enter_from_user_mode() with IRQs off
      x86/entry/32: Change INT80 to be an interrupt gate
      x86/entry: Improve system call entry comments
      x86/entry: Remove TIF_SINGLESTEP entry work
      x86/entry/32: Add and check a stack canary for the SYSENTER stack
      x86/entry/32: Simplify and fix up the SYSENTER stack #DB/NMI fixup
      x86/entry: Only allocate space for tss_struct::SYSENTER_stack if needed
      x86/entry: Vastly simplify SYSENTER TF (single-step) handling
      x86/entry/traps: Clear DR6 early in do_debug() and improve the comment
      x86/entry/traps: Clear TIF_BLOCKSTEP on all debug exceptions
      x86/entry/32: Restore FLAGS on SYSEXIT
      x86/entry/32: Filter NT and speed up AC filtering in SYSENTER
      x86/entry/compat: In SYSENTER, sink AC clearing below the existing FLAGS test
      selftests/x86: In syscall_nt, test NT|TF as well
      x86/asm-offsets: Remove PARAVIRT_enabled
      x86/entry/32: Introduce and use X86_BUG_ESPFIX instead of paravirt_enabled
      uprobes: __create_xol_area() must nullify xol_mapping.fault
      x86/cpufeature: Create a new synthetic cpu capability for machine check recovery
      ...

commit 58a5aac5331388a175a42b6ed2154f0559cefb21
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Feb 29 15:50:19 2016 -0800

    x86/entry/32: Introduce and use X86_BUG_ESPFIX instead of paravirt_enabled
    
    x86_64 has very clean espfix handling on paravirt: espfix64 is set
    up in native_iret, so paravirt systems that override iret bypass
    espfix64 automatically.  This is robust and straightforward.
    
    x86_32 is messier.  espfix is set up before the IRET paravirt patch
    point, so it can't be directly conditionalized on whether we use
    native_iret.  We also can't easily move it into native_iret without
    regressing performance due to a bizarre consideration.  Specifically,
    on 64-bit kernels, the logic is:
    
      if (regs->ss & 0x4)
              setup_espfix;
    
    On 32-bit kernels, the logic is:
    
      if ((regs->ss & 0x4) && (regs->cs & 0x3) == 3 &&
          (regs->flags & X86_EFLAGS_VM) == 0)
              setup_espfix;
    
    The performance of setup_espfix itself is essentially irrelevant, but
    the comparison happens on every IRET so its performance matters.  On
    x86_64, there's no need for any registers except flags to implement
    the comparison, so we fold the whole thing into native_iret.  On
    x86_32, we don't do that because we need a free register to
    implement the comparison efficiently.  We therefore do espfix setup
    before restoring registers on x86_32.
    
    This patch gets rid of the explicit paravirt_enabled check by
    introducing X86_BUG_ESPFIX on 32-bit systems and using an ALTERNATIVE
    to skip espfix on paravirt systems where iret != native_iret.  This is
    also messy, but it's at least in line with other things we do.
    
    This improves espfix performance by removing a branch, but no one
    cares.  More importantly, it removes a paravirt_enabled user, which is
    good because paravirt_enabled is ill-defined and is going away.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boris.ostrovsky@oracle.com
    Cc: david.vrabel@citrix.com
    Cc: konrad.wilk@oracle.com
    Cc: lguest@lists.ozlabs.org
    Cc: xen-devel@lists.xensource.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 079d83fc6488..d8337f34b5f4 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -802,6 +802,31 @@ static void detect_nopl(struct cpuinfo_x86 *c)
 	clear_cpu_cap(c, X86_FEATURE_NOPL);
 #else
 	set_cpu_cap(c, X86_FEATURE_NOPL);
+#endif
+
+	/*
+	 * ESPFIX is a strange bug.  All real CPUs have it.  Paravirt
+	 * systems that run Linux at CPL > 0 may or may not have the
+	 * issue, but, even if they have the issue, there's absolutely
+	 * nothing we can do about it because we can't use the real IRET
+	 * instruction.
+	 *
+	 * NB: For the time being, only 32-bit kernels support
+	 * X86_BUG_ESPFIX as such.  64-bit kernels directly choose
+	 * whether to apply espfix using paravirt hooks.  If any
+	 * non-paravirt system ever shows up that does *not* have the
+	 * ESPFIX issue, we can change this.
+	 */
+#ifdef CONFIG_X86_32
+#ifdef CONFIG_PARAVIRT
+	do {
+		extern void native_iret(void);
+		if (pv_cpu_ops.iret == native_iret)
+			set_cpu_bug(c, X86_BUG_ESPFIX);
+	} while (0);
+#else
+	set_cpu_bug(c, X86_BUG_ESPFIX);
+#endif
 #endif
 }
 

commit 1f12e32f4cd5243ae46d8b933181be0d022c6793
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 22 22:19:15 2016 +0000

    x86/topology: Create logical package id
    
    For per package oriented services we must be able to rely on the number of CPU
    packages to be within bounds. Create a tracking facility, which
    
    - calculates the number of possible packages depending on nr_cpu_ids after boot
    
    - makes sure that the package id is within the number of possible packages. If
      the apic id is outside we map it to a logical package id if there is enough
      space available.
    
    Provide interfaces for drivers to query the mapping and do translations from
    physcial to logical ids.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Harish Chegondi <harish.chegondi@intel.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160222221011.541071755@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 68a80e9b67fc..81cf716f6f97 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -975,6 +975,8 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 #ifdef CONFIG_NUMA
 	numa_add_cpu(smp_processor_id());
 #endif
+	/* The boot/hotplug time assigment got cleared, restore it */
+	c->logical_proc_id = topology_phys_to_logical_pkg(c->phys_proc_id);
 }
 
 /*

commit 6a6256f9e0ebaabf7ded1fef8977a4352dbe7784
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Tue Feb 23 15:34:30 2016 -0800

    x86: Fix misspellings in comments
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: trivial@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 68a80e9b67fc..7c3120f5177b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -884,7 +884,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	if (this_cpu->c_identify)
 		this_cpu->c_identify(c);
 
-	/* Clear/Set all flags overriden by options, after probe */
+	/* Clear/Set all flags overridden by options, after probe */
 	for (i = 0; i < NCAPINTS; i++) {
 		c->x86_capability[i] &= ~cpu_caps_cleared[i];
 		c->x86_capability[i] |= cpu_caps_set[i];
@@ -943,7 +943,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	x86_init_cache_qos(c);
 
 	/*
-	 * Clear/Set all flags overriden by options, need do it
+	 * Clear/Set all flags overridden by options, need do it
 	 * before following smp all cpus cap AND.
 	 */
 	for (i = 0; i < NCAPINTS; i++) {

commit 0697694564c84f4c9320e5d103d0191297a20023
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Feb 12 13:02:29 2016 -0800

    x86/mm/pkeys: Actually enable Memory Protection Keys in the CPU
    
    This sets the bit in 'cr4' to actually enable the protection
    keys feature.  We also include a boot-time disable for the
    feature "nopku".
    
    Seting X86_CR4_PKE will cause the X86_FEATURE_OSPKE cpuid
    bit to appear set.  At this point in boot, identify_cpu()
    has already run the actual CPUID instructions and populated
    the "cpu features" structures.  We need to go back and
    re-run identify_cpu() to make sure it gets updated values.
    
    We *could* simply re-populate the 11th word of the cpuid
    data, but this is probably quick enough.
    
    Also note that with the cpu_has() check and X86_FEATURE_PKU
    present in disabled-features.h, we do not need an #ifdef
    for setup_pku().
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20160212210229.6708027C@viggo.jf.intel.com
    [ Small readability edits. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a719ad7551d2..4fac2634ba19 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -303,6 +303,48 @@ static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 	}
 }
 
+/*
+ * Protection Keys are not available in 32-bit mode.
+ */
+static bool pku_disabled;
+
+static __always_inline void setup_pku(struct cpuinfo_x86 *c)
+{
+	if (!cpu_has(c, X86_FEATURE_PKU))
+		return;
+	if (pku_disabled)
+		return;
+
+	cr4_set_bits(X86_CR4_PKE);
+	/*
+	 * Seting X86_CR4_PKE will cause the X86_FEATURE_OSPKE
+	 * cpuid bit to be set.  We need to ensure that we
+	 * update that bit in this CPU's "cpu_info".
+	 */
+	get_cpu_cap(c);
+}
+
+#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
+static __init int setup_disable_pku(char *arg)
+{
+	/*
+	 * Do not clear the X86_FEATURE_PKU bit.  All of the
+	 * runtime checks are against OSPKE so clearing the
+	 * bit does nothing.
+	 *
+	 * This way, we will see "pku" in cpuinfo, but not
+	 * "ospke", which is exactly what we want.  It shows
+	 * that the CPU has PKU, but the OS has not enabled it.
+	 * This happens to be exactly how a system would look
+	 * if we disabled the config option.
+	 */
+	pr_info("x86: 'nopku' specified, disabling Memory Protection Keys\n");
+	pku_disabled = true;
+	return 1;
+}
+__setup("nopku", setup_disable_pku);
+#endif /* CONFIG_X86_64 */
+
 /*
  * Some CPU features depend on higher CPUID levels, which may not always
  * be available due to CPUID level capping or broken virtualization
@@ -960,6 +1002,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 	init_hypervisor(c);
 	x86_init_rdrand(c);
 	x86_init_cache_qos(c);
+	setup_pku(c);
 
 	/*
 	 * Clear/Set all flags overriden by options, need do it

commit dfb4a70f20c5b3880da56ee4c9484bdb4e8f1e65
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Feb 12 13:02:01 2016 -0800

    x86/cpufeature, x86/mm/pkeys: Add protection keys related CPUID definitions
    
    There are two CPUID bits for protection keys.  One is for whether
    the CPU contains the feature, and the other will appear set once
    the OS enables protection keys.  Specifically:
    
            Bit 04: OSPKE. If 1, OS has set CR4.PKE to enable
            Protection keys (and the RDPKRU/WRPKRU instructions)
    
    This is because userspace can not see CR4 contents, but it can
    see CPUID contents.
    
    X86_FEATURE_PKU is referred to as "PKU" in the hardware documentation:
    
            CPUID.(EAX=07H,ECX=0H):ECX.PKU [bit 3]
    
    X86_FEATURE_OSPKE is "OSPKU":
    
            CPUID.(EAX=07H,ECX=0H):ECX.OSPKE [bit 4]
    
    These are the first CPU features which need to look at the
    ECX word in CPUID leaf 0x7, so this patch also includes
    fetching that word in to the cpuinfo->x86_capability[] array.
    
    Add it to the disabled-features mask when its config option is
    off.  Even though we are not using it here, we also extend the
    REQUIRED_MASK_BIT_SET() macro to keep it mirroring the
    DISABLED_MASK_BIT_SET() version.
    
    This means that in almost all code, you should use:
    
            cpu_has(c, X86_FEATURE_PKU)
    
    and *not* the CONFIG option.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20160212210201.7714C250@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f4bb2c420969..a719ad7551d2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -627,6 +627,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[CPUID_7_0_EBX] = ebx;
 
 		c->x86_capability[CPUID_6_EAX] = cpuid_eax(0x00000006);
+		c->x86_capability[CPUID_7_ECX] = ecx;
 	}
 
 	/* Extended state features: level 0x0000000d */

commit 1fe3f29e4a908461be16a9388e73837157cc7942
Merge: 1926e54f1157 58122bf1d856 e2c7698cd61f f2cc8e0791c7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 16 09:37:37 2016 +0100

    Merge branches 'x86/fpu', 'x86/mm' and 'x86/asm' into x86/pkeys
    
    Provide a stable basis for the pkeys patches, which touches various
    x86 details.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d12a72b844a49d4162f24cefdab30bed3f86730e
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri Jan 29 11:42:58 2016 -0800

    x86/mm: Add a 'noinvpcid' boot option to turn off INVPCID
    
    This adds a chicken bit to turn off INVPCID in case something goes
    wrong.  It's an early_param() because we do TLB flushes before we
    parse __setup() parameters.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/f586317ed1bc2b87aee652267e515b90051af385.1454096309.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 37830de8f60a..f4d0aa64d934 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -162,6 +162,22 @@ static int __init x86_mpx_setup(char *s)
 }
 __setup("nompx", x86_mpx_setup);
 
+static int __init x86_noinvpcid_setup(char *s)
+{
+	/* noinvpcid doesn't accept parameters */
+	if (s)
+		return -EINVAL;
+
+	/* do not emit a message if the feature is not present */
+	if (!boot_cpu_has(X86_FEATURE_INVPCID))
+		return 0;
+
+	setup_clear_cpu_cap(X86_FEATURE_INVPCID);
+	pr_info("noinvpcid: INVPCID feature disabled\n");
+	return 0;
+}
+early_param("noinvpcid", x86_noinvpcid_setup);
+
 #ifdef CONFIG_X86_32
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;

commit 1b74dde7c47c19a73ea3e9fac95ac27b5d3d50c5
Author: Chen Yucong <slaoub@gmail.com>
Date:   Tue Feb 2 11:45:02 2016 +0800

    x86/cpu: Convert printk(KERN_<LEVEL> ...) to pr_<level>(...)
    
     - Use the more current logging style pr_<level>(...) instead of the old
       printk(KERN_<LEVEL> ...).
    
     - Convert pr_warning() to pr_warn().
    
    Signed-off-by: Chen Yucong <slaoub@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454384702-21707-1-git-send-email-slaoub@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 37830de8f60a..68a80e9b67fc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -228,7 +228,7 @@ static void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 	lo |= 0x200000;
 	wrmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
 
-	printk(KERN_NOTICE "CPU serial number disabled.\n");
+	pr_notice("CPU serial number disabled.\n");
 	clear_cpu_cap(c, X86_FEATURE_PN);
 
 	/* Disabling the serial number may affect the cpuid level */
@@ -329,9 +329,8 @@ static void filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 		if (!warn)
 			continue;
 
-		printk(KERN_WARNING
-		       "CPU: CPU feature " X86_CAP_FMT " disabled, no CPUID level 0x%x\n",
-				x86_cap_flag(df->feature), df->level);
+		pr_warn("CPU: CPU feature " X86_CAP_FMT " disabled, no CPUID level 0x%x\n",
+			x86_cap_flag(df->feature), df->level);
 	}
 }
 
@@ -510,7 +509,7 @@ void detect_ht(struct cpuinfo_x86 *c)
 	smp_num_siblings = (ebx & 0xff0000) >> 16;
 
 	if (smp_num_siblings == 1) {
-		printk_once(KERN_INFO "CPU0: Hyper-Threading is disabled\n");
+		pr_info_once("CPU0: Hyper-Threading is disabled\n");
 		goto out;
 	}
 
@@ -531,10 +530,10 @@ void detect_ht(struct cpuinfo_x86 *c)
 
 out:
 	if (!printed && (c->x86_max_cores * smp_num_siblings) > 1) {
-		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
-		       c->phys_proc_id);
-		printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
-		       c->cpu_core_id);
+		pr_info("CPU: Physical Processor ID: %d\n",
+			c->phys_proc_id);
+		pr_info("CPU: Processor Core ID: %d\n",
+			c->cpu_core_id);
 		printed = 1;
 	}
 #endif
@@ -559,9 +558,8 @@ static void get_cpu_vendor(struct cpuinfo_x86 *c)
 		}
 	}
 
-	printk_once(KERN_ERR
-			"CPU: vendor_id '%s' unknown, using generic init.\n" \
-			"CPU: Your system may be unstable.\n", v);
+	pr_err_once("CPU: vendor_id '%s' unknown, using generic init.\n" \
+		    "CPU: Your system may be unstable.\n", v);
 
 	c->x86_vendor = X86_VENDOR_UNKNOWN;
 	this_cpu = &default_cpu;
@@ -760,7 +758,7 @@ void __init early_cpu_init(void)
 	int count = 0;
 
 #ifdef CONFIG_PROCESSOR_SELECT
-	printk(KERN_INFO "KERNEL supported cpus:\n");
+	pr_info("KERNEL supported cpus:\n");
 #endif
 
 	for (cdev = __x86_cpu_dev_start; cdev < __x86_cpu_dev_end; cdev++) {
@@ -778,7 +776,7 @@ void __init early_cpu_init(void)
 			for (j = 0; j < 2; j++) {
 				if (!cpudev->c_ident[j])
 					continue;
-				printk(KERN_INFO "  %s %s\n", cpudev->c_vendor,
+				pr_info("  %s %s\n", cpudev->c_vendor,
 					cpudev->c_ident[j]);
 			}
 		}
@@ -1061,7 +1059,7 @@ static void __print_cpu_msr(void)
 		for (index = index_min; index < index_max; index++) {
 			if (rdmsrl_safe(index, &val))
 				continue;
-			printk(KERN_INFO " MSR%08x: %016llx\n", index, val);
+			pr_info(" MSR%08x: %016llx\n", index, val);
 		}
 	}
 }
@@ -1100,19 +1098,19 @@ void print_cpu_info(struct cpuinfo_x86 *c)
 	}
 
 	if (vendor && !strstr(c->x86_model_id, vendor))
-		printk(KERN_CONT "%s ", vendor);
+		pr_cont("%s ", vendor);
 
 	if (c->x86_model_id[0])
-		printk(KERN_CONT "%s", c->x86_model_id);
+		pr_cont("%s", c->x86_model_id);
 	else
-		printk(KERN_CONT "%d86", c->x86);
+		pr_cont("%d86", c->x86);
 
-	printk(KERN_CONT " (family: 0x%x, model: 0x%x", c->x86, c->x86_model);
+	pr_cont(" (family: 0x%x, model: 0x%x", c->x86, c->x86_model);
 
 	if (c->x86_mask || c->cpuid_level >= 0)
-		printk(KERN_CONT ", stepping: 0x%x)\n", c->x86_mask);
+		pr_cont(", stepping: 0x%x)\n", c->x86_mask);
 	else
-		printk(KERN_CONT ")\n");
+		pr_cont(")\n");
 
 	print_cpu_msr(c);
 }
@@ -1438,7 +1436,7 @@ void cpu_init(void)
 
 	show_ucode_info_early();
 
-	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
+	pr_info("Initializing CPU#%d\n", cpu);
 
 	if (cpu_feature_enabled(X86_FEATURE_VME) ||
 	    cpu_has_tsc ||

commit 2476f2fa20568bd5d9e09cd35bcd73e99a6f4cc6
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jan 27 09:45:25 2016 +0100

    x86/alternatives: Discard dynamic check after init
    
    Move the code to do the dynamic check to the altinstr_aux
    section so that it is discarded after alternatives have run and
    a static branch has been chosen.
    
    This way we're changing the dynamic branch from C code to
    assembly, which makes it *substantially* smaller while avoiding
    a completely unnecessary call to an out of line function.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    [ Changed it to do TESTB, as hpa suggested. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kristen Carlson Accardi <kristen@linux.intel.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1452972124-7380-1-git-send-email-brgerst@gmail.com
    Link: http://lkml.kernel.org/r/20160127084525.GC30712@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ee499817f3f5..079d83fc6488 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1475,12 +1475,6 @@ void cpu_init(void)
 }
 #endif
 
-inline bool __static_cpu_has(u16 bit)
-{
-	return boot_cpu_has(bit);
-}
-EXPORT_SYMBOL_GPL(__static_cpu_has);
-
 static void bsp_resume(void)
 {
 	if (this_cpu->c_bsp_resume)

commit bc696ca05f5a8927329ec276a892341e006b00ba
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 26 22:12:05 2016 +0100

    x86/cpufeature: Replace the old static_cpu_has() with safe variant
    
    So the old one didn't work properly before alternatives had run.
    And it was supposed to provide an optimized JMP because the
    assumption was that the offset it is jumping to is within a
    signed byte and thus a two-byte JMP.
    
    So I did an x86_64 allyesconfig build and dumped all possible
    sites where static_cpu_has() was used. The optimization amounted
    to all in all 12(!) places where static_cpu_has() had generated
    a 2-byte JMP. Which has saved us a whopping 36 bytes!
    
    This clearly is not worth the trouble so we can remove it. The
    only place where the optimization might count - in __switch_to()
    - we will handle differently. But that's not subject of this
    patch.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453842730-28463-6-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 37830de8f60a..ee499817f3f5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1475,19 +1475,11 @@ void cpu_init(void)
 }
 #endif
 
-#ifdef CONFIG_X86_DEBUG_STATIC_CPU_HAS
-void warn_pre_alternatives(void)
-{
-	WARN(1, "You're using static_cpu_has before alternatives have run!\n");
-}
-EXPORT_SYMBOL_GPL(warn_pre_alternatives);
-#endif
-
-inline bool __static_cpu_has_safe(u16 bit)
+inline bool __static_cpu_has(u16 bit)
 {
 	return boot_cpu_has(bit);
 }
-EXPORT_SYMBOL_GPL(__static_cpu_has_safe);
+EXPORT_SYMBOL_GPL(__static_cpu_has);
 
 static void bsp_resume(void)
 {

commit 671d5532aaad777782b66eff71bc4dfad25f942d
Merge: 67c707e451e1 0007bccc3cfd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 11 16:46:20 2016 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Improved CPU ID handling code and related enhancements (Borislav
         Petkov)
    
       - RDRAND fix (Len Brown)"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Replace RDRAND forced-reseed with simple sanity check
      x86/MSR: Chop off lower 32-bit value
      x86/cpu: Fix MSR value truncation issue
      x86/cpu/amd, kvm: Satisfy guest kernel reads of IC_CFG MSR
      kvm: Add accessors for guest CPU's family, model, stepping
      x86/cpu: Unify CPU family, model, stepping calculation

commit 362f924b64ba0f4be2ee0cb697690c33d40be721
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Dec 7 10:39:41 2015 +0100

    x86/cpufeature: Remove unused and seldomly used cpu_has_xx macros
    
    Those are stupid and code should use static_cpu_has_safe() or
    boot_cpu_has() instead. Kill the least used and unused ones.
    
    The remaining ones need more careful inspection before a conversion can
    happen. On the TODO.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1449481182-27541-4-git-send-email-bp@alien8.de
    Cc: David Sterba <dsterba@suse.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e14d5bd8671f..4d5279c95d5f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1445,7 +1445,9 @@ void cpu_init(void)
 
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
 
-	if (cpu_feature_enabled(X86_FEATURE_VME) || cpu_has_tsc || cpu_has_de)
+	if (cpu_feature_enabled(X86_FEATURE_VME) ||
+	    cpu_has_tsc ||
+	    boot_cpu_has(X86_FEATURE_DE))
 		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	load_current_idt();

commit 39c06df4dc10a41de5fe706f4378ee5f09beba73
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Dec 7 10:39:40 2015 +0100

    x86/cpufeature: Cleanup get_cpu_cap()
    
    Add an enum for the ->x86_capability array indices and cleanup
    get_cpu_cap() by killing some redundant local vars.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1449481182-27541-3-git-send-email-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c75517331989..e14d5bd8671f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -599,52 +599,47 @@ void cpu_detect(struct cpuinfo_x86 *c)
 
 void get_cpu_cap(struct cpuinfo_x86 *c)
 {
-	u32 tfms, xlvl;
-	u32 ebx;
+	u32 eax, ebx, ecx, edx;
 
 	/* Intel-defined flags: level 0x00000001 */
 	if (c->cpuid_level >= 0x00000001) {
-		u32 capability, excap;
+		cpuid(0x00000001, &eax, &ebx, &ecx, &edx);
 
-		cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
-		c->x86_capability[0] = capability;
-		c->x86_capability[4] = excap;
+		c->x86_capability[CPUID_1_ECX] = ecx;
+		c->x86_capability[CPUID_1_EDX] = edx;
 	}
 
 	/* Additional Intel-defined flags: level 0x00000007 */
 	if (c->cpuid_level >= 0x00000007) {
-		u32 eax, ebx, ecx, edx;
-
 		cpuid_count(0x00000007, 0, &eax, &ebx, &ecx, &edx);
 
-		c->x86_capability[9] = ebx;
+		c->x86_capability[CPUID_7_0_EBX] = ebx;
 
-		c->x86_capability[14] = cpuid_eax(0x00000006);
+		c->x86_capability[CPUID_6_EAX] = cpuid_eax(0x00000006);
 	}
 
 	/* Extended state features: level 0x0000000d */
 	if (c->cpuid_level >= 0x0000000d) {
-		u32 eax, ebx, ecx, edx;
-
 		cpuid_count(0x0000000d, 1, &eax, &ebx, &ecx, &edx);
 
-		c->x86_capability[10] = eax;
+		c->x86_capability[CPUID_D_1_EAX] = eax;
 	}
 
 	/* Additional Intel-defined flags: level 0x0000000F */
 	if (c->cpuid_level >= 0x0000000F) {
-		u32 eax, ebx, ecx, edx;
 
 		/* QoS sub-leaf, EAX=0Fh, ECX=0 */
 		cpuid_count(0x0000000F, 0, &eax, &ebx, &ecx, &edx);
-		c->x86_capability[11] = edx;
+		c->x86_capability[CPUID_F_0_EDX] = edx;
+
 		if (cpu_has(c, X86_FEATURE_CQM_LLC)) {
 			/* will be overridden if occupancy monitoring exists */
 			c->x86_cache_max_rmid = ebx;
 
 			/* QoS sub-leaf, EAX=0Fh, ECX=1 */
 			cpuid_count(0x0000000F, 1, &eax, &ebx, &ecx, &edx);
-			c->x86_capability[12] = edx;
+			c->x86_capability[CPUID_F_1_EDX] = edx;
+
 			if (cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC)) {
 				c->x86_cache_max_rmid = ecx;
 				c->x86_cache_occ_scale = ebx;
@@ -656,22 +651,24 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 	}
 
 	/* AMD-defined flags: level 0x80000001 */
-	xlvl = cpuid_eax(0x80000000);
-	c->extended_cpuid_level = xlvl;
+	eax = cpuid_eax(0x80000000);
+	c->extended_cpuid_level = eax;
+
+	if ((eax & 0xffff0000) == 0x80000000) {
+		if (eax >= 0x80000001) {
+			cpuid(0x80000001, &eax, &ebx, &ecx, &edx);
 
-	if ((xlvl & 0xffff0000) == 0x80000000) {
-		if (xlvl >= 0x80000001) {
-			c->x86_capability[1] = cpuid_edx(0x80000001);
-			c->x86_capability[6] = cpuid_ecx(0x80000001);
+			c->x86_capability[CPUID_8000_0001_ECX] = ecx;
+			c->x86_capability[CPUID_8000_0001_EDX] = edx;
 		}
 	}
 
 	if (c->extended_cpuid_level >= 0x80000008) {
-		u32 eax = cpuid_eax(0x80000008);
+		cpuid(0x80000008, &eax, &ebx, &ecx, &edx);
 
 		c->x86_virt_bits = (eax >> 8) & 0xff;
 		c->x86_phys_bits = eax & 0xff;
-		c->x86_capability[13] = cpuid_ebx(0x80000008);
+		c->x86_capability[CPUID_8000_0008_EBX] = ebx;
 	}
 #ifdef CONFIG_X86_32
 	else if (cpu_has(c, X86_FEATURE_PAE) || cpu_has(c, X86_FEATURE_PSE36))
@@ -682,7 +679,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_power = cpuid_edx(0x80000007);
 
 	if (c->extended_cpuid_level >= 0x8000000a)
-		c->x86_capability[15] = cpuid_edx(0x8000000a);
+		c->x86_capability[CPUID_8000_000A_EDX] = cpuid_edx(0x8000000a);
 
 	init_scattered_cpuid_features(c);
 }

commit 2ccd71f1b278d450a6f8c8c737c7fe237ca06dc6
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Dec 7 10:39:39 2015 +0100

    x86/cpufeature: Move some of the scattered feature bits to x86_capability
    
    Turn the CPUID leafs which are proper CPUID feature bit leafs into
    separate ->x86_capability words.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1449481182-27541-2-git-send-email-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c2b7522cbf35..c75517331989 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -618,6 +618,8 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		cpuid_count(0x00000007, 0, &eax, &ebx, &ecx, &edx);
 
 		c->x86_capability[9] = ebx;
+
+		c->x86_capability[14] = cpuid_eax(0x00000006);
 	}
 
 	/* Extended state features: level 0x0000000d */
@@ -679,6 +681,9 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 	if (c->extended_cpuid_level >= 0x80000007)
 		c->x86_power = cpuid_edx(0x80000007);
 
+	if (c->extended_cpuid_level >= 0x8000000a)
+		c->x86_capability[15] = cpuid_edx(0x8000000a);
+
 	init_scattered_cpuid_features(c);
 }
 

commit 31ac34ca5636e596485c6e03df1879643bde585e
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Nov 23 11:12:25 2015 +0100

    x86/cpu: Fix MSR value truncation issue
    
    So sparse rightfully complains that the u64 MSR value we're
    writing into the STAR MSR, i.e. 0xc0000081, is being truncated:
    
    ./arch/x86/include/asm/msr.h:193:36: warning: cast truncates
    bits from constant value (23001000000000 becomes 0)
    
    because the actual value doesn't fit into the unsigned 32-bit
    quantity which are the @low and @high wrmsrl() parameters.
    
    This is not a problem, practically, because gcc is actually
    being smart enough here and does the right thing:
    
      .loc 3 87 0
      xorl    %esi, %esi            # we needz a 32-bit zero
      movl    $2293776, %edx        # 0x00230010 == (__USER32_CS << 16) | __KERNEL_CS go into the high bits
      movl    $-1073741695, %ecx    # MSR_STAR, i.e., 0xc0000081
      movl    %esi, %eax            # low order 32 bits in the MSR which are 0
      #APP
      # 87 "./arch/x86/include/asm/msr.h" 1
              wrmsr
    
    More specifically, MSR_STAR[31:0] is being set to 0. That field
    is reserved on Intel and on AMD it is 32-bit SYSCALL Target EIP.
    
    I'd strongly guess because Intel doesn't have SYSCALL in
    compat/legacy mode and we're using SYSENTER and INT80 there. And
    for compat syscalls in long mode we use CSTAR.
    
    So let's fix the sparse warning by writing SYSRET and SYSCALL CS
    and SS into the high 32-bit half of STAR and 0 in the low half
    explicitly.
    
     [ Actually, if we had to be precise, we would have to read what's in
       STAR[31:0] and write it back unchanged on Intel and write 0 on AMD. I
       guess the current writing to 0 is still ok since Intel can apparently
       stomach it. ]
    
    The resulting code is identical to what we have above:
    
      .loc 3 87 0
      xorl    %esi, %esi      # tmp104
      movl    $2293776, %eax  #, tmp103
      movl    $-1073741695, %ecx      #, tmp102
      movl    %esi, %edx      # tmp104, tmp104
    
      ...
    
            wrmsr
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1448273546-2567-6-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0bed416f8c40..105da8df87ae 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1180,7 +1180,7 @@ void syscall_init(void)
 	 * They both write to the same internal register. STAR allows to
 	 * set CS/DS but only a 32bit target. LSTAR sets the 64bit rip.
 	 */
-	wrmsrl(MSR_STAR,  ((u64)__USER32_CS)<<48  | ((u64)__KERNEL_CS)<<32);
+	wrmsr(MSR_STAR, 0, (__USER32_CS << 16) | __KERNEL_CS);
 	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 
 #ifdef CONFIG_IA32_EMULATION

commit 99f925ce927e4ac313d9af8bd1bf55796e2cdcb1
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Nov 23 11:12:21 2015 +0100

    x86/cpu: Unify CPU family, model, stepping calculation
    
    Add generic functions which calc family, model and stepping from
    the CPUID_1.EAX leaf and stick them into the library we have.
    
    Rename those which do call CPUID with the prefix "x86_cpuid" as
    suggested by Paolo Bonzini.
    
    No functionality change.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1448273546-2567-2-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c2b7522cbf35..0bed416f8c40 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -581,14 +581,9 @@ void cpu_detect(struct cpuinfo_x86 *c)
 		u32 junk, tfms, cap0, misc;
 
 		cpuid(0x00000001, &tfms, &misc, &junk, &cap0);
-		c->x86 = (tfms >> 8) & 0xf;
-		c->x86_model = (tfms >> 4) & 0xf;
-		c->x86_mask = tfms & 0xf;
-
-		if (c->x86 == 0xf)
-			c->x86 += (tfms >> 20) & 0xff;
-		if (c->x86 >= 0x6)
-			c->x86_model += ((tfms >> 16) & 0xf) << 4;
+		c->x86		= x86_family(tfms);
+		c->x86_model	= x86_model(tfms);
+		c->x86_mask	= x86_stepping(tfms);
 
 		if (cap0 & (1<<19)) {
 			c->x86_clflush_size = ((misc >> 8) & 0xff) * 8;

commit 581b7f158fe0383b492acd1ce3fb4e99d4e57808
Author: Andrew Cooper <andrew.cooper3@citrix.com>
Date:   Wed Jun 3 10:31:14 2015 +0100

    x86/cpu: Fix SMAP check in PVOPS environments
    
    There appears to be no formal statement of what pv_irq_ops.save_fl() is
    supposed to return precisely.  Native returns the full flags, while lguest and
    Xen only return the Interrupt Flag, and both have comments by the
    implementations stating that only the Interrupt Flag is looked at.  This may
    have been true when initially implemented, but no longer is.
    
    To make matters worse, the Xen PVOP leaves the upper bits undefined, making
    the BUG_ON() undefined behaviour.  Experimentally, this now trips for 32bit PV
    guests on Broadwell hardware.  The BUG_ON() is consistent for an individual
    build, but not consistent for all builds.  It has also been a sitting timebomb
    since SMAP support was introduced.
    
    Use native_save_fl() instead, which will obtain an accurate view of the AC
    flag.
    
    Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Tested-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: <lguest@lists.ozlabs.org>
    Cc: Xen-devel <xen-devel@lists.xen.org>
    CC: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/1433323874-6927-1-git-send-email-andrew.cooper3@citrix.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4ddd780aeac9..c2b7522cbf35 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -273,10 +273,9 @@ __setup("nosmap", setup_disable_smap);
 
 static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 {
-	unsigned long eflags;
+	unsigned long eflags = native_save_fl();
 
 	/* This should have been cleared long ago */
-	raw_local_save_flags(eflags);
 	BUG_ON(eflags & X86_EFLAGS_AC);
 
 	if (cpu_has(c, X86_FEATURE_SMAP)) {

commit 2167ceabf34163727ca4e283c0f030e3960932e5
Author: Wan Zongshun <Vincent.Wan@amd.com>
Date:   Fri Oct 30 13:11:39 2015 +0100

    x86/cpu: Add CLZERO detection
    
    AMD Fam17h processors introduce support for the CLZERO
    instruction. It zeroes out the 64 byte cache line specified in
    RAX.
    
    Add the bit here to allow /proc/cpuinfo to list the feature.
    
    Boris: we're adding this as a separate ->x86_capability leaf
    because CPUID_80000008_EBX is going to contain more feature bits
    and it will fill out with time.
    
    Signed-off-by: Wan Zongshun <Vincent.Wan@amd.com>
    Signed-off-by: Aravind Gopalakrishnan <aravind.gopalakrishnan@amd.com>
    [ Wrap code in patch form, fix comments. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/1446207099-24948-4-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index de22ea7ff82f..4ddd780aeac9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -670,6 +670,7 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 
 		c->x86_virt_bits = (eax >> 8) & 0xff;
 		c->x86_phys_bits = eax & 0xff;
+		c->x86_capability[13] = cpuid_ebx(0x80000008);
 	}
 #ifdef CONFIG_X86_32
 	else if (cpu_has(c, X86_FEATURE_PAE) || cpu_has(c, X86_FEATURE_PSE36))

commit 7c5b190e115a2f7a51a85f261e7d7dca4b4bbe64
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Sep 10 21:55:27 2015 +0200

    x86/cpu: Print family/model/stepping in hex
    
    924e101a7ab6 ("x86/debug: Dump family, model, stepping of the
    boot CPU") had its good intentions to dump the exact F/M/S as an
    aid during debugging sessions but its output can be ambiguous.
    Fix that:
    
    -smpboot: CPU0: Intel Core Processor (Broadwell) (fam: 06, model: 47, stepping: 02)
    +smpboot: CPU0: Intel Core Processor (Broadwell) (family: 0x6, model: 0x47, stepping: 0x2)
    
    Also, spell out "family".
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1441914927-32037-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 07ce52c22ec8..de22ea7ff82f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1110,10 +1110,10 @@ void print_cpu_info(struct cpuinfo_x86 *c)
 	else
 		printk(KERN_CONT "%d86", c->x86);
 
-	printk(KERN_CONT " (fam: %02x, model: %02x", c->x86, c->x86_model);
+	printk(KERN_CONT " (family: 0x%x, model: 0x%x", c->x86, c->x86_model);
 
 	if (c->x86_mask || c->cpuid_level >= 0)
-		printk(KERN_CONT ", stepping: %02x)\n", c->x86_mask);
+		printk(KERN_CONT ", stepping: 0x%x)\n", c->x86_mask);
 	else
 		printk(KERN_CONT ")\n");
 

commit 6b2282aa372665c14ea1100b63ac0703051407e9
Merge: 0c0fee018d14 b1c599b8ff80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 09:41:03 2015 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu updates from Ingo Molnar:
     "Two changes: a suspend/resume quirk and a new CPUID bit definition"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpufeature: Add feature bit for Intel's Silicon Debug CPUID bit
      x86/cpu: Restore MSR_IA32_ENERGY_PERF_BIAS after resume

commit 47edb65178cb7056c2eea0b6c41a7d8c84547192
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 23 12:14:40 2015 -0700

    x86/asm/msr: Make wrmsrl() a function
    
    As of cf991de2f614 ("x86/asm/msr: Make wrmsrl_safe() a
    function"), wrmsrl_safe is a function, but wrmsrl is still a
    macro.  The wrmsrl macro performs invalid shifts if the value
    argument is 32 bits. This makes it unnecessarily awkward to
    write code that puts an unsigned long into an MSR.
    
    To make this work, syscall_init needs tweaking to stop passing
    a function pointer to wrmsrl.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Willy Tarreau <w@1wt.eu>
    Link: http://lkml.kernel.org/r/690f0c629a1085d054e2d1ef3da073cfb3f7db92.1437678821.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cb9e5df42dd2..b128808853a2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1185,10 +1185,10 @@ void syscall_init(void)
 	 * set CS/DS but only a 32bit target. LSTAR sets the 64bit rip.
 	 */
 	wrmsrl(MSR_STAR,  ((u64)__USER32_CS)<<48  | ((u64)__KERNEL_CS)<<32);
-	wrmsrl(MSR_LSTAR, entry_SYSCALL_64);
+	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 
 #ifdef CONFIG_IA32_EMULATION
-	wrmsrl(MSR_CSTAR, entry_SYSCALL_compat);
+	wrmsrl(MSR_CSTAR, (unsigned long)entry_SYSCALL_compat);
 	/*
 	 * This only works on Intel CPUs.
 	 * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.
@@ -1199,7 +1199,7 @@ void syscall_init(void)
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
-	wrmsrl(MSR_CSTAR, ignore_sysret);
+	wrmsrl(MSR_CSTAR, (unsigned long)ignore_sysret);
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)GDT_ENTRY_INVALID_SEG);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, 0ULL);

commit 37868fe113ff2ba814b3b4eb12df214df555f8dc
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 30 14:31:32 2015 -0700

    x86/ldt: Make modify_ldt synchronous
    
    modify_ldt() has questionable locking and does not synchronize
    threads.  Improve it: redesign the locking and synchronize all
    threads' LDTs using an IPI on all modifications.
    
    This will dramatically slow down modify_ldt in multithreaded
    programs, but there shouldn't be any multithreaded programs that
    care about modify_ldt's performance in the first place.
    
    This fixes some fallout from the CVE-2015-5157 fixes.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: security@kernel.org <security@kernel.org>
    Cc: <stable@vger.kernel.org>
    Cc: xen-devel <xen-devel@lists.xen.org>
    Link: http://lkml.kernel.org/r/4c6978476782160600471bd865b318db34c7b628.1438291540.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 922c5e0cea4c..cb9e5df42dd2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1410,7 +1410,7 @@ void cpu_init(void)
 	load_sp0(t, &current->thread);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
-	load_LDT(&init_mm.context);
+	load_mm_ldt(&init_mm);
 
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
@@ -1459,7 +1459,7 @@ void cpu_init(void)
 	load_sp0(t, thread);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
-	load_LDT(&init_mm.context);
+	load_mm_ldt(&init_mm);
 
 	t->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
 

commit b51ef52df71cb28e9d90cd1d48b79bf19f0bab06
Author: Laura Abbott <labbott@fedoraproject.org>
Date:   Mon Jul 20 14:47:58 2015 -0700

    x86/cpu: Restore MSR_IA32_ENERGY_PERF_BIAS after resume
    
    MSR_IA32_ENERGY_PERF_BIAS is lost after suspend/resume:
    
            x86_energy_perf_policy -r before
    
            cpu0: 0x0000000000000006
            cpu1: 0x0000000000000006
            cpu2: 0x0000000000000006
            cpu3: 0x0000000000000006
            cpu4: 0x0000000000000006
            cpu5: 0x0000000000000006
            cpu6: 0x0000000000000006
            cpu7: 0x0000000000000006
    
            after
    
            cpu0: 0x0000000000000000
            cpu1: 0x0000000000000006
            cpu2: 0x0000000000000006
            cpu3: 0x0000000000000006
            cpu4: 0x0000000000000006
            cpu5: 0x0000000000000006
            cpu6: 0x0000000000000006
            cpu7: 0x0000000000000006
    
    Resulting in inconsistent energy policy settings across CPUs.
    
    This register is set via init_intel() at bootup. During resume,
    the secondary CPUs are brought online again and init_intel() is
    callled which re-initializes the register. The boot CPU however
    never reinitializes the register.
    
    Add a syscore callback to reinitialize the register for the boot CPU.
    
    Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437428878-4105-1-git-send-email-labbott@fedoraproject.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 922c5e0cea4c..fa95bb8829ce 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -13,6 +13,7 @@
 #include <linux/kgdb.h>
 #include <linux/smp.h>
 #include <linux/io.h>
+#include <linux/syscore_ops.h>
 
 #include <asm/stackprotector.h>
 #include <asm/perf_event.h>
@@ -1488,3 +1489,20 @@ inline bool __static_cpu_has_safe(u16 bit)
 	return boot_cpu_has(bit);
 }
 EXPORT_SYMBOL_GPL(__static_cpu_has_safe);
+
+static void bsp_resume(void)
+{
+	if (this_cpu->c_bsp_resume)
+		this_cpu->c_bsp_resume(&boot_cpu_data);
+}
+
+static struct syscore_ops cpu_syscore_ops = {
+	.resume		= bsp_resume,
+};
+
+static int __init init_cpu_syscore(void)
+{
+	register_syscore_ops(&cpu_syscore_ops);
+	return 0;
+}
+core_initcall(init_cpu_syscore);

commit db52ef74b35dcb91fd154fa52c618bdd1b90e28e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jun 27 10:25:14 2015 +0200

    x86/fpu: Fix FPU related boot regression when CPUID masking BIOS feature is enabled
    
    Mike Galbraith reported:
    
      " My i7-4790 box is having one hell of a time with this merge
        window, dead in the water.
    
        BIOS setting "Limit CPUID Maximum" upsets new fpu code
        mightily. "
    
    It turns out that Linux does a double workaround here, as per:
    
      066941bd4eeb ("x86: unmask CPUID levels on Intel CPUs")
    
    it undoes the BIOS workaround - but as a side effect the CPUID
    state is not completely constant during early init anymore,
    and the new FPU init code did not take this into account.
    
    So what happened is that the xstate init code did not have full
    CPUID available, which broke subsequent attempts to use xstate
    features.
    
    Fix this by ordering the early FPU init code to after we've
    stabilized the CPUID state.
    
    Reported-bisected-and-tested-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20150627082514.GA10894@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9fc5e3d9d9c8..922c5e0cea4c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -742,7 +742,6 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	cpu_detect(c);
 	get_cpu_vendor(c);
 	get_cpu_cap(c);
-	fpu__init_system(c);
 
 	if (this_cpu->c_early_init)
 		this_cpu->c_early_init(c);
@@ -754,6 +753,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		this_cpu->c_bsp_init(c);
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
+	fpu__init_system(c);
 }
 
 void __init early_cpu_init(void)

commit d70b3ef54ceaf1c7c92209f5a662a670d04cbed9
Merge: 650ec5a6bd5d 7ef3d7d58d9d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:59:09 2015 -0700

    Merge branch 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "There were so many changes in the x86/asm, x86/apic and x86/mm topics
      in this cycle that the topical separation of -tip broke down somewhat -
      so the result is a more traditional architecture pull request,
      collected into the 'x86/core' topic.
    
      The topics were still maintained separately as far as possible, so
      bisectability and conceptual separation should still be pretty good -
      but there were a handful of merge points to avoid excessive
      dependencies (and conflicts) that would have been poorly tested in the
      end.
    
      The next cycle will hopefully be much more quiet (or at least will
      have fewer dependencies).
    
      The main changes in this cycle were:
    
       * x86/apic changes, with related IRQ core changes: (Jiang Liu, Thomas
         Gleixner)
    
         - This is the second and most intrusive part of changes to the x86
           interrupt handling - full conversion to hierarchical interrupt
           domains:
    
              [IOAPIC domain]   -----
                                     |
              [MSI domain]      --------[Remapping domain] ----- [ Vector domain ]
                                     |   (optional)          |
              [HPET MSI domain] -----                        |
                                                             |
              [DMAR domain]     -----------------------------
                                                             |
              [Legacy domain]   -----------------------------
    
           This now reflects the actual hardware and allowed us to distangle
           the domain specific code from the underlying parent domain, which
           can be optional in the case of interrupt remapping.  It's a clear
           separation of functionality and removes quite some duct tape
           constructs which plugged the remap code between ioapic/msi/hpet
           and the vector management.
    
         - Intel IOMMU IRQ remapping enhancements, to allow direct interrupt
           injection into guests (Feng Wu)
    
       * x86/asm changes:
    
         - Tons of cleanups and small speedups, micro-optimizations.  This
           is in preparation to move a good chunk of the low level entry
           code from assembly to C code (Denys Vlasenko, Andy Lutomirski,
           Brian Gerst)
    
         - Moved all system entry related code to a new home under
           arch/x86/entry/ (Ingo Molnar)
    
         - Removal of the fragile and ugly CFI dwarf debuginfo annotations.
           Conversion to C will reintroduce many of them - but meanwhile
           they are only getting in the way, and the upstream kernel does
           not rely on them (Ingo Molnar)
    
         - NOP handling refinements. (Borislav Petkov)
    
       * x86/mm changes:
    
         - Big PAT and MTRR rework: making the code more robust and
           preparing to phase out exposing direct MTRR interfaces to drivers -
           in favor of using PAT driven interfaces (Toshi Kani, Luis R
           Rodriguez, Borislav Petkov)
    
         - New ioremap_wt()/set_memory_wt() interfaces to support
           Write-Through cached memory mappings.  This is especially
           important for good performance on NVDIMM hardware (Toshi Kani)
    
       * x86/ras changes:
    
         - Add support for deferred errors on AMD (Aravind Gopalakrishnan)
    
           This is an important RAS feature which adds hardware support for
           poisoned data.  That means roughly that the hardware marks data
           which it has detected as corrupted but wasn't able to correct, as
           poisoned data and raises an APIC interrupt to signal that in the
           form of a deferred error.  It is the OS's responsibility then to
           take proper recovery action and thus prolonge system lifetime as
           far as possible.
    
         - Add support for Intel "Local MCE"s: upcoming CPUs will support
           CPU-local MCE interrupts, as opposed to the traditional system-
           wide broadcasted MCE interrupts (Ashok Raj)
    
         - Misc cleanups (Borislav Petkov)
    
       * x86/platform changes:
    
         - Intel Atom SoC updates
    
      ... and lots of other cleanups, fixlets and other changes - see the
      shortlog and the Git log for details"
    
    * 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (222 commits)
      x86/hpet: Use proper hpet device number for MSI allocation
      x86/hpet: Check for irq==0 when allocating hpet MSI interrupts
      x86/mm/pat, drivers/infiniband/ipath: Use arch_phys_wc_add() and require PAT disabled
      x86/mm/pat, drivers/media/ivtv: Use arch_phys_wc_add() and require PAT disabled
      x86/platform/intel/baytrail: Add comments about why we disabled HPET on Baytrail
      genirq: Prevent crash in irq_move_irq()
      genirq: Enhance irq_data_to_desc() to support hierarchy irqdomain
      iommu, x86: Properly handle posted interrupts for IOMMU hotplug
      iommu, x86: Provide irq_remapping_cap() interface
      iommu, x86: Setup Posted-Interrupts capability for Intel iommu
      iommu, x86: Add cap_pi_support() to detect VT-d PI capability
      iommu, x86: Avoid migrating VT-d posted interrupts
      iommu, x86: Save the mode (posted or remapped) of an IRTE
      iommu, x86: Implement irq_set_vcpu_affinity for intel_ir_chip
      iommu: dmar: Provide helper to copy shared irte fields
      iommu: dmar: Extend struct irte for VT-d Posted-Interrupts
      iommu: Add new member capability to struct irq_remap_ops
      x86/asm/entry/64: Disentangle error_entry/exit gsbase/ebx/usermode code
      x86/asm/entry/32: Shorten __audit_syscall_entry() args preparation
      x86/asm/entry/32: Explain reloading of registers after __audit_syscall_entry()
      ...

commit e75c73ad64478c12b3a44b86a3e7f62a4f65b93e
Merge: cfe3eceb7a2e a8424003679e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:16:11 2015 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "This tree contains two main changes:
    
       - The big FPU code rewrite: wide reaching cleanups and reorganization
         that pulls all the FPU code together into a clean base in
         arch/x86/fpu/.
    
         The resulting code is leaner and faster, and much easier to
         understand.  This enables future work to further simplify the FPU
         code (such as removing lazy FPU restores).
    
         By its nature these changes have a substantial regression risk: FPU
         code related bugs are long lived, because races are often subtle
         and bugs mask as user-space failures that are difficult to track
         back to kernel side backs.  I'm aware of no unfixed (or even
         suspected) FPU related regression so far.
    
       - MPX support rework/fixes.  As this is still not a released CPU
         feature, there were some buglets in the code - should be much more
         robust now (Dave Hansen)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (250 commits)
      x86/fpu: Fix double-increment in setup_xstate_features()
      x86/mpx: Allow 32-bit binaries on 64-bit kernels again
      x86/mpx: Do not count MPX VMAs as neighbors when unmapping
      x86/mpx: Rewrite the unmap code
      x86/mpx: Support 32-bit binaries on 64-bit kernels
      x86/mpx: Use 32-bit-only cmpxchg() for 32-bit apps
      x86/mpx: Introduce new 'directory entry' to 'addr' helper function
      x86/mpx: Add temporary variable to reduce masking
      x86: Make is_64bit_mm() widely available
      x86/mpx: Trace allocation of new bounds tables
      x86/mpx: Trace the attempts to find bounds tables
      x86/mpx: Trace entry to bounds exception paths
      x86/mpx: Trace #BR exceptions
      x86/mpx: Introduce a boot-time disable flag
      x86/mpx: Restrict the mmap() size check to bounds tables
      x86/mpx: Remove redundant MPX_BNDCFG_ADDR_MASK
      x86/mpx: Clean up the code by not passing a task pointer around when unnecessary
      x86/mpx: Use the new get_xsave_field_ptr()API
      x86/fpu/xstate: Wrap get_xsave_addr() to make it safer
      x86/fpu/xstate: Fix up bad get_xsave_addr() assumptions
      ...

commit 8c3641e957a948f41f0174290096ed7a3b95e703
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Sun Jun 7 11:37:02 2015 -0700

    x86/mpx: Introduce a boot-time disable flag
    
    MPX has the _potential_ to cause some issues.  Say part of your
    init system tried to protect one of its components from buffer
    overflows with MPX.  If there were a false positive, it's
    possible that MPX could keep a system from booting.
    
    MPX could also potentially cause performance issues since it is
    present in hot paths like the unmap path.
    
    Allow it to be disabled at boot time.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20150607183702.2E8B77AB@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 401ccb03054e..3956858e5312 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -144,6 +144,22 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
+static int __init x86_mpx_setup(char *s)
+{
+	/* require an exact match without trailing characters */
+	if (strlen(s))
+		return 0;
+
+	/* do not emit a message if the feature is not present */
+	if (!boot_cpu_has(X86_FEATURE_MPX))
+		return 1;
+
+	setup_clear_cpu_cap(X86_FEATURE_MPX);
+	pr_info("nompx: Intel Memory Protection Extensions (MPX) disabled\n");
+	return 1;
+}
+__setup("nompx", x86_mpx_setup);
+
 #ifdef CONFIG_X86_32
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;

commit 9dda1658a9bd450d65da5153a2427955785d17c2
Merge: b72e7464e4cf a49976d14f78
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 8 20:48:01 2015 +0200

    Merge branch 'x86/asm' into x86/core, to prepare for new patch
    
    Collect all changes to arch/x86/entry/entry_64.S, before applying
    patch that changes most of the file.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b2502b418e63fcde0fe1857732a476b5aa3789b1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 8 08:42:03 2015 +0200

    x86/asm/entry: Untangle 'system_call' into two entry points: entry_SYSCALL_64 and entry_INT80_32
    
    The 'system_call' entry points differ starkly between native 32-bit and 64-bit
    kernels: on 32-bit kernels it defines the INT 0x80 entry point, while on
    64-bit it's the SYSCALL entry point.
    
    This is pretty confusing when looking at generic code, and it also obscures
    the nature of the entry point at the assembly level.
    
    So unangle this by splitting the name into its two uses:
    
            system_call (32) -> entry_INT80_32
            system_call (64) -> entry_SYSCALL_64
    
    As per the generic naming scheme for x86 system call entry points:
    
            entry_MNEMONIC_qualifier
    
    where 'qualifier' is one of _32, _64 or _compat.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b2ae7cec33ca..914be4bbc2e5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1204,7 +1204,7 @@ void syscall_init(void)
 	 * set CS/DS but only a 32bit target. LSTAR sets the 64bit rip.
 	 */
 	wrmsrl(MSR_STAR,  ((u64)__USER32_CS)<<48  | ((u64)__KERNEL_CS)<<32);
-	wrmsrl(MSR_LSTAR, system_call);
+	wrmsrl(MSR_LSTAR, entry_SYSCALL_64);
 
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, entry_SYSCALL_compat);

commit 4c8cd0c50d0b1559727bf0ec7ff27caeba2dfe09
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 8 08:33:56 2015 +0200

    x86/asm/entry: Untangle 'ia32_sysenter_target' into two entry points: entry_SYSENTER_32 and entry_SYSENTER_compat
    
    So the SYSENTER instruction is pretty quirky and it has different behavior
    depending on bitness and CPU maker.
    
    Yet we create a false sense of coherency by naming it 'ia32_sysenter_target'
    in both of the cases.
    
    Split the name into its two uses:
    
            ia32_sysenter_target (32)    -> entry_SYSENTER_32
            ia32_sysenter_target (64)    -> entry_SYSENTER_compat
    
    As per the generic naming scheme for x86 system call entry points:
    
            entry_MNEMONIC_qualifier
    
    where 'qualifier' is one of _32, _64 or _compat.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f0b85c401014..b2ae7cec33ca 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1026,7 +1026,7 @@ void enable_sep_cpu(void)
 	      (unsigned long)tss + offsetofend(struct tss_struct, SYSENTER_stack),
 	      0);
 
-	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)ia32_sysenter_target, 0);
+	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
 
 out:
 	put_cpu();
@@ -1216,7 +1216,7 @@ void syscall_init(void)
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
-	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
+	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
 	wrmsrl(MSR_CSTAR, ignore_sysret);
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)GDT_ENTRY_INVALID_SEG);

commit 2cd23553b488589f287457b7396470f5e3c40698
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 8 08:28:07 2015 +0200

    x86/asm/entry: Rename compat syscall entry points
    
    Rename the following system call entry points:
    
            ia32_cstar_target       -> entry_SYSCALL_compat
            ia32_syscall            -> entry_INT80_compat
    
    The generic naming scheme for x86 system call entry points is:
    
            entry_MNEMONIC_qualifier
    
    where 'qualifier' is one of _32, _64 or _compat.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6bec0b55863e..f0b85c401014 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1207,7 +1207,7 @@ void syscall_init(void)
 	wrmsrl(MSR_LSTAR, system_call);
 
 #ifdef CONFIG_IA32_EMULATION
-	wrmsrl(MSR_CSTAR, ia32_cstar_target);
+	wrmsrl(MSR_CSTAR, entry_SYSCALL_compat);
 	/*
 	 * This only works on Intel CPUs.
 	 * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.

commit c8e56d20f2d190d54c0615775dcb6a23c1091681
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jun 4 18:55:25 2015 +0200

    x86: Kill CONFIG_X86_HT
    
    In talking to Aravind recently about making certain AMD topology
    attributes available to the MCE injection module, it seemed like
    that CONFIG_X86_HT thing is more or less superfluous. It is
    def_bool y, depends on SMP and gets enabled in the majority of
    .configs - distro and otherwise - out there.
    
    So let's kill it and make code behind it depend directly on SMP.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Daniel Walter <dwalter@google.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1433436928-31903-18-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6bec0b55863e..b6fe2e47f7f1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -508,7 +508,7 @@ static void cpu_detect_tlb(struct cpuinfo_x86 *c)
 
 void detect_ht(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_X86_HT
+#ifdef CONFIG_SMP
 	u32 eax, ebx, ecx, edx;
 	int index_msb, core_bits;
 	static bool printed;
@@ -844,7 +844,7 @@ static void generic_identify(struct cpuinfo_x86 *c)
 	if (c->cpuid_level >= 0x00000001) {
 		c->initial_apicid = (cpuid_ebx(1) >> 24) & 0xFF;
 #ifdef CONFIG_X86_32
-# ifdef CONFIG_X86_HT
+# ifdef CONFIG_SMP
 		c->apicid = apic->phys_pkg_id(c->initial_apicid, 0);
 # else
 		c->apicid = c->initial_apicid;

commit ee098e1aed67715f0ce4651813d0c33ab3a56e0b
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Jun 1 12:06:57 2015 +0200

    x86/cpu: Trim model ID whitespace
    
    We did try trimming whitespace surrounding the 'model name'
    field in /proc/cpuinfo since reportedly some userspace uses it
    in string comparisons and there were discrepancies:
    
      [thetango@prarit ~]# grep "^model name" /proc/cpuinfo | uniq -c | sed 's/\ /_/g'
      ______1_model_name      :_AMD_Opteron(TM)_Processor_6272
      _____63_model_name      :_AMD_Opteron(TM)_Processor_6272_________________
    
    However, there were issues with overlapping buffers, string
    sizes and non-byte-sized copies in the previous proposed
    solutions; see Link tags below for the whole farce.
    
    So, instead of diddling with this more, let's simply extend what
    was there originally with trimming any present trailing
    whitespace. Final result is really simple and obvious.
    
    Testing with the most insane model IDs qemu can generate, looks
    good:
    
      .model_id = "            My funny model ID CPU          ",
      ______4_model_name      :_My_funny_model_ID_CPU
    
      .model_id = "My funny model ID CPU          ",
      ______4_model_name      :_My_funny_model_ID_CPU
    
      .model_id = "            My funny model ID CPU",
      ______4_model_name      :_My_funny_model_ID_CPU
    
      .model_id = "            ",
      ______4_model_name      :__
    
      .model_id = "",
      ______4_model_name      :_15/02
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1432050210-32036-1-git-send-email-prarit@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 41a8e9cb30bc..351197cbbc8e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -5,6 +5,7 @@
 #include <linux/module.h>
 #include <linux/percpu.h>
 #include <linux/string.h>
+#include <linux/ctype.h>
 #include <linux/delay.h>
 #include <linux/sched.h>
 #include <linux/init.h>
@@ -419,6 +420,7 @@ static const struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 static void get_model_name(struct cpuinfo_x86 *c)
 {
 	unsigned int *v;
+	char *p, *q, *s;
 
 	if (c->extended_cpuid_level < 0x80000004)
 		return;
@@ -429,11 +431,21 @@ static void get_model_name(struct cpuinfo_x86 *c)
 	cpuid(0x80000004, &v[8], &v[9], &v[10], &v[11]);
 	c->x86_model_id[48] = 0;
 
-	/*
-	 * Remove leading whitespace on Intel processors and trailing
-	 * whitespace on AMD processors.
-	 */
-	memmove(c->x86_model_id, strim(c->x86_model_id), 48);
+	/* Trim whitespace */
+	p = q = s = &c->x86_model_id[0];
+
+	while (*p == ' ')
+		p++;
+
+	while (*p) {
+		/* Note the last non-whitespace index */
+		if (!isspace(*p))
+			s = q;
+
+		*q++ = *p++;
+	}
+
+	*(s + 1) = '\0';
 }
 
 void cpu_detect_cache_sizes(struct cpuinfo_x86 *c)

commit adafb98da6a7af5e45362933a7dae6ab0e5076bf
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Tue May 26 10:28:17 2015 +0200

    x86/cpu: Strip any /proc/cpuinfo model name field whitespace
    
    When comparing the 'model name' field of each core in
    /proc/cpuinfo it was noticed that there is a whitespace
    difference between the cores' model names.
    
    After some quick investigation it was noticed that the model
    name fields were actually different -- processor 0's model name
    field had trailing whitespace removed, while the other
    processors did not.
    
    Another way of seeing this behaviour is to convert spaces into
    underscores in the output of /proc/cpuinfo,
    
      [thetango@prarit ~]# grep "^model name" /proc/cpuinfo | uniq -c | sed 's/\ /_/g'
      ______1_model_name      :_AMD_Opteron(TM)_Processor_6272
      _____63_model_name      :_AMD_Opteron(TM)_Processor_6272_________________
    
    which shows the discrepancy.
    
    This occurs because the kernel calls strim() on cpu 0's
    x86_model_id field to output a pretty message to the console in
    print_cpu_info(), and as a result strips the whitespace at the
    end of the ->x86_model_id field.
    
    But, the ->x86_model_id field should be the same for the all
    identical CPUs in the box. Thus, we need to remove both leading
    and trailing whitespace.
    
    As a result, the print_cpu_info() output looks like
    
      smpboot: CPU0: AMD Opteron(TM) Processor 6272 (fam: 15, model: 01, stepping: 02)
    
    and the x86_model_id field is correct on all processors on AMD
    platforms:
    
      _____64_model_name      :_AMD_Opteron(TM)_Processor_6272
    
    Output is still correct on an Intel box:
    
      ____144_model_name      :_Intel(R)_Xeon(R)_CPU_E7-8890_v3_@_2.50GHz
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1432050210-32036-1-git-send-email-prarit@redhat.com
    Link: http://lkml.kernel.org/r/1432628901-18044-15-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a62cf04dac8a..41a8e9cb30bc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -419,7 +419,6 @@ static const struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 static void get_model_name(struct cpuinfo_x86 *c)
 {
 	unsigned int *v;
-	char *p, *q;
 
 	if (c->extended_cpuid_level < 0x80000004)
 		return;
@@ -431,18 +430,10 @@ static void get_model_name(struct cpuinfo_x86 *c)
 	c->x86_model_id[48] = 0;
 
 	/*
-	 * Intel chips right-justify this string for some dumb reason;
-	 * undo that brain damage:
+	 * Remove leading whitespace on Intel processors and trailing
+	 * whitespace on AMD processors.
 	 */
-	p = q = &c->x86_model_id[0];
-	while (*p == ' ')
-		p++;
-	if (p != q) {
-		while (*p)
-			*q++ = *p++;
-		while (q <= &c->x86_model_id[48])
-			*q++ = '\0';	/* Zero-pad the rest */
-	}
+	memmove(c->x86_model_id, strim(c->x86_model_id), 48);
 }
 
 void cpu_detect_cache_sizes(struct cpuinfo_x86 *c)
@@ -1122,7 +1113,7 @@ void print_cpu_info(struct cpuinfo_x86 *c)
 		printk(KERN_CONT "%s ", vendor);
 
 	if (c->x86_model_id[0])
-		printk(KERN_CONT "%s", strim(c->x86_model_id));
+		printk(KERN_CONT "%s", c->x86_model_id);
 	else
 		printk(KERN_CONT "%d86", c->x86);
 

commit 7cf82d33b613780a79fda91babf7b9e6c5c82d75
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 20 11:35:02 2015 +0200

    x86/fpu/init: Move __setup() functions to fpu/init.c
    
    We had a number of FPU init related boot option handlers
    in arch/x86/kernel/cpu/common.c - move them over into
    arch/x86/kernel/fpu/init.c to have them all in a
    single place.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d6fe512441e5..401ccb03054e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -144,42 +144,6 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
-static int __init x86_xsave_setup(char *s)
-{
-	if (strlen(s))
-		return 0;
-	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
-	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
-	setup_clear_cpu_cap(X86_FEATURE_XSAVES);
-	setup_clear_cpu_cap(X86_FEATURE_AVX);
-	setup_clear_cpu_cap(X86_FEATURE_AVX2);
-	return 1;
-}
-__setup("noxsave", x86_xsave_setup);
-
-static int __init x86_xsaveopt_setup(char *s)
-{
-	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
-	return 1;
-}
-__setup("noxsaveopt", x86_xsaveopt_setup);
-
-static int __init x86_xsaves_setup(char *s)
-{
-	setup_clear_cpu_cap(X86_FEATURE_XSAVES);
-	return 1;
-}
-__setup("noxsaves", x86_xsaves_setup);
-
-static int __init x86_fxsr_setup(char *s)
-{
-	setup_clear_cpu_cap(X86_FEATURE_FXSR);
-	setup_clear_cpu_cap(X86_FEATURE_FXSR_OPT);
-	setup_clear_cpu_cap(X86_FEATURE_XMM);
-	return 1;
-}
-__setup("nofxsr", x86_fxsr_setup);
-
 #ifdef CONFIG_X86_32
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;

commit d364a7656c1855c940dfa4baf4ebcc3c6a9e6fd2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue May 5 11:43:02 2015 +0200

    x86/fpu: Fix the 'nofxsr' boot parameter to also clear X86_FEATURE_FXSR_OPT
    
    I tried to simulate an ancient CPU via this option, and
    found that it still has fxsr_opt enabled, confusing the
    FPU code.
    
    Make the 'nofxsr' option also clear FXSR_OPT flag.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d15610b0a4cf..d6fe512441e5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -171,6 +171,15 @@ static int __init x86_xsaves_setup(char *s)
 }
 __setup("noxsaves", x86_xsaves_setup);
 
+static int __init x86_fxsr_setup(char *s)
+{
+	setup_clear_cpu_cap(X86_FEATURE_FXSR);
+	setup_clear_cpu_cap(X86_FEATURE_FXSR_OPT);
+	setup_clear_cpu_cap(X86_FEATURE_XMM);
+	return 1;
+}
+__setup("nofxsr", x86_fxsr_setup);
+
 #ifdef CONFIG_X86_32
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;
@@ -182,14 +191,6 @@ static int __init cachesize_setup(char *str)
 }
 __setup("cachesize=", cachesize_setup);
 
-static int __init x86_fxsr_setup(char *s)
-{
-	setup_clear_cpu_cap(X86_FEATURE_FXSR);
-	setup_clear_cpu_cap(X86_FEATURE_XMM);
-	return 1;
-}
-__setup("nofxsr", x86_fxsr_setup);
-
 static int __init x86_sep_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_SEP);

commit c66e3f28237199629358e9e5a76973c400a54041
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 15:12:44 2015 +0200

    x86/fpu: Remove the extra fpu__detect() layer
    
    Now that fpu__detect() has become an empty layer around
    fpu__init_system(), eliminate it and make fpu__init_system()
    the main system initialization routine.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d28f8ebc506d..d15610b0a4cf 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -758,7 +758,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	cpu_detect(c);
 	get_cpu_vendor(c);
 	get_cpu_cap(c);
-	fpu__detect(c);
+	fpu__init_system(c);
 
 	if (this_cpu->c_early_init)
 		this_cpu->c_early_init(c);

commit 21c4cd108a1b144ad645355bfee1f8be937f03a2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 26 14:27:17 2015 +0200

    x86/fpu: Simplify fpu__cpu_init()
    
    After the latest round of cleanups, fpu__cpu_init() has become
    a simple call to fpu__init_cpu().
    
    Rename fpu__init_cpu() to fpu__cpu_init() and remove the
    extra layer.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8f6a4ea39657..d28f8ebc506d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1435,7 +1435,7 @@ void cpu_init(void)
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
 
-	fpu__cpu_init();
+	fpu__init_cpu();
 
 	if (is_uv_system())
 		uv_cpu_init();
@@ -1491,7 +1491,7 @@ void cpu_init(void)
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
 
-	fpu__cpu_init();
+	fpu__init_cpu();
 }
 #endif
 

commit 78f7f1e54bac032b98956862a5bcf8c28ed22d07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 24 02:54:44 2015 +0200

    x86/fpu: Rename fpu-internal.h to fpu/internal.h
    
    This unifies all the FPU related header files under a unified, hiearchical
    naming scheme:
    
     - asm/fpu/types.h:      FPU related data types, needed for 'struct task_struct',
                             widely included in almost all kernel code, and hence kept
                             as small as possible.
    
     - asm/fpu/api.h:        FPU related 'public' methods exported to other subsystems.
    
     - asm/fpu/internal.h:   FPU subsystem internal methods
    
     - asm/fpu/xsave.h:      XSAVE support internal methods
    
    (Also standardize the header guard in asm/fpu/internal.h.)
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 88bb7a75f5c6..8f6a4ea39657 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -31,7 +31,7 @@
 #include <asm/setup.h>
 #include <asm/apic.h>
 #include <asm/desc.h>
-#include <asm/fpu-internal.h>
+#include <asm/fpu/internal.h>
 #include <asm/mtrr.h>
 #include <linux/numa.h>
 #include <asm/asm.h>

commit b0c050c5ba130c0ccb1b86b64f162a4601d160c7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Apr 23 12:13:04 2015 +0200

    x86/fpu: Move 'PER_CPU(fpu_owner_task)' to fpu/core.c
    
    Move it closer to other per-cpu FPU data structures.
    
    This also unifies the 32-bit and 64-bit code.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 220ad95e0e28..88bb7a75f5c6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1182,8 +1182,6 @@ DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
 EXPORT_PER_CPU_SYMBOL(__preempt_count);
 
-DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
-
 /*
  * Special IST stacks which the CPU switches to when it calls
  * an IST-marked descriptor entry. Up to 7 stacks (hardware
@@ -1274,7 +1272,6 @@ DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
 EXPORT_PER_CPU_SYMBOL(__preempt_count);
-DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 
 /*
  * On x86_32, vm86 modifies tss.sp0, so sp0 isn't a reliable way to find

commit f89e32e0a3df2f29d61fdc120ac62654ef267111
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 22 10:58:10 2015 +0200

    x86/fpu: Fix header file dependencies of fpu-internal.h
    
    Fix a minor header file dependency bug in asm/fpu-internal.h: it
    relies on i387.h but does not include it. All users of fpu-internal.h
    included it explicitly.
    
    Also remove unnecessary includes, to reduce compilation time.
    
    This also makes it easier to use it as a standalone header file
    for FPU internals, such as an upcoming C module in arch/x86/kernel/fpu/.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b8035b8dd186..220ad95e0e28 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -31,7 +31,6 @@
 #include <asm/setup.h>
 #include <asm/apic.h>
 #include <asm/desc.h>
-#include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/mtrr.h>
 #include <linux/numa.h>

commit 3a9c4b0d7e1a693beeac24d749f6938444148e86
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 13:16:51 2015 +0200

    x86/fpu: Rename fpu_init() to fpu__cpu_init()
    
    fpu_init() is a bit of a misnomer in that it (falsely) creates the
    impression that it's related to the (old) fpu_finit() function,
    which initializes FPU ctx state.
    
    Rename it to fpu__cpu_init() to make its boot time initialization
    clear, and to move it to the fpu__*() namespace.
    
    Also fix and extend its comment block to point out that it's
    called not only on the boot CPU, but on secondary CPUs as well.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 60a29d290e2a..b8035b8dd186 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1439,7 +1439,7 @@ void cpu_init(void)
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
 
-	fpu_init();
+	fpu__cpu_init();
 
 	if (is_uv_system())
 		uv_cpu_init();
@@ -1495,7 +1495,7 @@ void cpu_init(void)
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
 
-	fpu_init();
+	fpu__cpu_init();
 }
 #endif
 

commit 1a7dc0db7181ebc8b9ec17e5a15ad4c766c7d3d4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 3 11:33:52 2015 +0200

    x86/fpu: Rename fpu_detect() to fpu__detect()
    
    Use the fpu__*() namespace to organize FPU ops better.
    
    Also document fpu__detect() a bit.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a62cf04dac8a..60a29d290e2a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -759,7 +759,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	cpu_detect(c);
 	get_cpu_vendor(c);
 	get_cpu_cap(c);
-	fpu_detect(c);
+	fpu__detect(c);
 
 	if (this_cpu->c_early_init)
 		this_cpu->c_early_init(c);

commit fed7c3f0f750f225317828d691e9eb76eec887b3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Apr 24 17:31:34 2015 +0200

    x86/entry: Remove unused 'kernel_stack' per-cpu variable
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1429889495-27850-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a62cf04dac8a..6bec0b55863e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1155,10 +1155,6 @@ static __init int setup_disablecpuid(char *arg)
 }
 __setup("clearcpuid=", setup_disablecpuid);
 
-DEFINE_PER_CPU(unsigned long, kernel_stack) =
-	(unsigned long)&init_thread_union + THREAD_SIZE;
-EXPORT_PER_CPU_SYMBOL(kernel_stack);
-
 #ifdef CONFIG_X86_64
 struct desc_ptr idt_descr = { NR_VECTORS * 16 - 1, (unsigned long) idt_table };
 struct desc_ptr debug_idt_descr = { NR_VECTORS * 16 - 1,

commit 6c8a53c9e6a151fffb07f8b4c34bd1e33dddd467
Merge: e95e7f627062 066450be419f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 14:37:47 2015 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf changes from Ingo Molnar:
     "Core kernel changes:
    
       - One of the more interesting features in this cycle is the ability
         to attach eBPF programs (user-defined, sandboxed bytecode executed
         by the kernel) to kprobes.
    
         This allows user-defined instrumentation on a live kernel image
         that can never crash, hang or interfere with the kernel negatively.
         (Right now it's limited to root-only, but in the future we might
         allow unprivileged use as well.)
    
         (Alexei Starovoitov)
    
       - Another non-trivial feature is per event clockid support: this
         allows, amongst other things, the selection of different clock
         sources for event timestamps traced via perf.
    
         This feature is sought by people who'd like to merge perf generated
         events with external events that were measured with different
         clocks:
    
           - cluster wide profiling
    
           - for system wide tracing with user-space events,
    
           - JIT profiling events
    
         etc.  Matching perf tooling support is added as well, available via
         the -k, --clockid <clockid> parameter to perf record et al.
    
         (Peter Zijlstra)
    
      Hardware enablement kernel changes:
    
       - x86 Intel Processor Trace (PT) support: which is a hardware tracer
         on steroids, available on Broadwell CPUs.
    
         The hardware trace stream is directly output into the user-space
         ring-buffer, using the 'AUX' data format extension that was added
         to the perf core to support hardware constraints such as the
         necessity to have the tracing buffer physically contiguous.
    
         This patch-set was developed for two years and this is the result.
         A simple way to make use of this is to use BTS tracing, the PT
         driver emulates BTS output - available via the 'intel_bts' PMU.
         More explicit PT specific tooling support is in the works as well -
         will probably be ready by 4.2.
    
         (Alexander Shishkin, Peter Zijlstra)
    
       - x86 Intel Cache QoS Monitoring (CQM) support: this is a hardware
         feature of Intel Xeon CPUs that allows the measurement and
         allocation/partitioning of caches to individual workloads.
    
         These kernel changes expose the measurement side as a new PMU
         driver, which exposes various QoS related PMU events.  (The
         partitioning change is work in progress and is planned to be merged
         as a cgroup extension.)
    
         (Matt Fleming, Peter Zijlstra; CPU feature detection by Peter P
         Waskiewicz Jr)
    
       - x86 Intel Haswell LBR call stack support: this is a new Haswell
         feature that allows the hardware recording of call chains, plus
         tooling support.  To activate this feature you have to enable it
         via the new 'lbr' call-graph recording option:
    
            perf record --call-graph lbr
            perf report
    
         or:
    
            perf top --call-graph lbr
    
         This hardware feature is a lot faster than stack walk or dwarf
         based unwinding, but has some limitations:
    
           - It reuses the current LBR facility, so LBR call stack and
             branch record can not be enabled at the same time.
    
           - It is only available for user-space callchains.
    
         (Yan, Zheng)
    
       - x86 Intel Broadwell CPU support and various event constraints and
         event table fixes for earlier models.
    
         (Andi Kleen)
    
       - x86 Intel HT CPUs event scheduling workarounds.  This is a complex
         CPU bug affecting the SNB,IVB,HSW families that results in counter
         value corruption.  The mitigation code is automatically enabled and
         is transparent.
    
         (Maria Dimakopoulou, Stephane Eranian)
    
      The perf tooling side had a ton of changes in this cycle as well, so
      I'm only able to list the user visible changes here, in addition to
      the tooling changes outlined above:
    
      User visible changes affecting all tools:
    
          - Improve support of compressed kernel modules (Jiri Olsa)
          - Save DSO loading errno to better report errors (Arnaldo Carvalho de Melo)
          - Bash completion for subcommands (Yunlong Song)
          - Add 'I' event modifier for perf_event_attr.exclude_idle bit (Jiri Olsa)
          - Support missing -f to override perf.data file ownership. (Yunlong Song)
          - Show the first event with an invalid filter (David Ahern, Arnaldo Carvalho de Melo)
    
      User visible changes in individual tools:
    
        'perf data':
    
            New tool for converting perf.data to other formats, initially
            for the CTF (Common Trace Format) from LTTng (Jiri Olsa,
            Sebastian Siewior)
    
        'perf diff':
    
            Add --kallsyms option (David Ahern)
    
        'perf list':
    
            Allow listing events with 'tracepoint' prefix (Yunlong Song)
    
            Sort the output of the command (Yunlong Song)
    
        'perf kmem':
    
            Respect -i option (Jiri Olsa)
    
            Print big numbers using thousands' group (Namhyung Kim)
    
            Allow -v option (Namhyung Kim)
    
            Fix alignment of slab result table (Namhyung Kim)
    
        'perf probe':
    
            Support multiple probes on different binaries on the same command line (Masami Hiramatsu)
    
            Support unnamed union/structure members data collection. (Masami Hiramatsu)
    
            Check kprobes blacklist when adding new events. (Masami Hiramatsu)
    
        'perf record':
    
            Teach 'perf record' about perf_event_attr.clockid (Peter Zijlstra)
    
            Support recording running/enabled time (Andi Kleen)
    
        'perf sched':
    
            Improve the performance of 'perf sched replay' on high CPU core count machines (Yunlong Song)
    
        'perf report' and 'perf top':
    
            Allow annotating entries in callchains in the hists browser (Arnaldo Carvalho de Melo)
    
            Indicate which callchain entries are annotated in the
            TUI hists browser (Arnaldo Carvalho de Melo)
    
            Add pid/tid filtering to 'report' and 'script' commands (David Ahern)
    
            Consider PERF_RECORD_ events with cpumode == 0 in 'perf top', removing one
            cause of long term memory usage buildup, i.e. not processing PERF_RECORD_EXIT
            events (Arnaldo Carvalho de Melo)
    
        'perf stat':
    
            Report unsupported events properly (Suzuki K. Poulose)
    
            Output running time and run/enabled ratio in CSV mode (Andi Kleen)
    
        'perf trace':
    
            Handle legacy syscalls tracepoints (David Ahern, Arnaldo Carvalho de Melo)
    
            Only insert blank duration bracket when tracing syscalls (Arnaldo Carvalho de Melo)
    
            Filter out the trace pid when no threads are specified (Arnaldo Carvalho de Melo)
    
            Dump stack on segfaults (Arnaldo Carvalho de Melo)
    
            No need to explicitely enable evsels for workload started from perf, let it
            be enabled via perf_event_attr.enable_on_exec, removing some events that take
            place in the 'perf trace' before a workload is really started by it.
            (Arnaldo Carvalho de Melo)
    
            Allow mixing with tracepoints and suppressing plain syscalls. (Arnaldo Carvalho de Melo)
    
      There's also been a ton of infrastructure work done, such as the
      split-out of perf's build system into tools/build/ and other changes -
      see the shortlog and changelog for details"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (358 commits)
      perf/x86/intel/pt: Clean up the control flow in pt_pmu_hw_init()
      perf evlist: Fix type for references to data_head/tail
      perf probe: Check the orphaned -x option
      perf probe: Support multiple probes on different binaries
      perf buildid-list: Fix segfault when show DSOs with hits
      perf tools: Fix cross-endian analysis
      perf tools: Fix error path to do closedir() when synthesizing threads
      perf tools: Fix synthesizing fork_event.ppid for non-main thread
      perf tools: Add 'I' event modifier for exclude_idle bit
      perf report: Don't call map__kmap if map is NULL.
      perf tests: Fix attr tests
      perf probe: Fix ARM 32 building error
      perf tools: Merge all perf_event_attr print functions
      perf record: Add clockid parameter
      perf sched replay: Use replay_repeat to calculate the runavg of cpu usage instead of the default value 10
      perf sched replay: Support using -f to override perf.data file ownership
      perf sched replay: Fix the EMFILE error caused by the limitation of the maximum open files
      perf sched replay: Handle the dead halt of sem_wait when create_tasks() fails for any task
      perf sched replay: Fix the segmentation fault problem caused by pr_err in threads
      perf sched replay: Realloc the memory of pid_to_task stepwise to adapt to the different pid_max configurations
      ...

commit 6b51311c976593fb7311322b1647a912cd456ec4
Author: Borislav Petkov <bp@alien8.de>
Date:   Fri Apr 3 14:25:28 2015 +0200

    x86/asm/entry/64: Use a define for an invalid segment selector
    
    ... instead of a naked number, for better readability.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1428054130-25847-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d56d30714d43..3f70538012e2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1184,7 +1184,7 @@ void syscall_init(void)
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
 #else
 	wrmsrl(MSR_CSTAR, ignore_sysret);
-	wrmsrl_safe(MSR_IA32_SYSENTER_CS, 0ULL);
+	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)GDT_ENTRY_INVALID_SEG);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, 0ULL);
 #endif

commit 7c74d5b7b7b63fc279ed446ebd78de20d81f2824
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Apr 3 11:42:10 2015 +0200

    x86/asm/entry/64: Fix MSR_IA32_SYSENTER_CS MSR value
    
    Commit:
    
      d56fe4bf5f3c ("x86/asm/entry/64: Always set up SYSENTER MSRs")
    
    missed to add "ULL" to the 0 and wrmsrl_safe() complains:
    
      arch/x86/kernel/cpu/common.c: In function syscall_init:
      arch/x86/kernel/cpu/common.c:1226:2: warning: right shift count >= width of type wrmsrl_safe(MSR_IA32_SYSENTER_CS, 0);
    
    Fix it.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1428054130-25847-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a383d53bf0ed..d56d30714d43 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1184,7 +1184,7 @@ void syscall_init(void)
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
 #else
 	wrmsrl(MSR_CSTAR, ignore_sysret);
-	wrmsrl_safe(MSR_IA32_SYSENTER_CS, 0);
+	wrmsrl_safe(MSR_IA32_SYSENTER_CS, 0ULL);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, 0ULL);
 #endif

commit cf9328cc9989e028fdc64d8c0a7b1b043dc96735
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Apr 2 12:41:45 2015 -0700

    x86/asm/entry/32: Stop caching MSR_IA32_SYSENTER_ESP in tss.sp1
    
    We write a stack pointer to MSR_IA32_SYSENTER_ESP exactly once,
    and we unnecessarily cache the value in tss.sp1.  We never
    read the cached value.
    
    Remove all of the caching.  It serves no purpose.
    
    Suggested-by: Denys Vlasenko <dvlasenk@redhat.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/05a0163eb33ef5208363f0015496855da7cebadd.1428002830.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 71e4adcb15f1..a383d53bf0ed 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -976,15 +976,16 @@ void enable_sep_cpu(void)
 		goto out;
 
 	/*
-	 * The struct::SS1 and tss_struct::SP1 fields are not used by the hardware,
-	 * we cache the SYSENTER CS and ESP values there for easy access:
+	 * We cache MSR_IA32_SYSENTER_CS's value in the TSS's ss1 field --
+	 * see the big comment in struct x86_hw_tss's definition.
 	 */
 
 	tss->x86_tss.ss1 = __KERNEL_CS;
 	wrmsr(MSR_IA32_SYSENTER_CS, tss->x86_tss.ss1, 0);
 
-	tss->x86_tss.sp1 = (unsigned long)tss + offsetofend(struct tss_struct, SYSENTER_stack);
-	wrmsr(MSR_IA32_SYSENTER_ESP, tss->x86_tss.sp1, 0);
+	wrmsr(MSR_IA32_SYSENTER_ESP,
+	      (unsigned long)tss + offsetofend(struct tss_struct, SYSENTER_stack),
+	      0);
 
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)ia32_sysenter_target, 0);
 

commit 487d1edb9a6005cf790c7fe59f25ad1e5cb5817b
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Mar 27 11:59:16 2015 +0100

    x86/asm/entry/64: Fix comment about SYSENTER MSRs
    
    The comment is ancient, it dates to the time when only AMD's
    x86_64 implementation existed. AMD wasn't (and still isn't)
    supporting SYSENTER, so these writes were "just in case" back
    then.
    
    This has changed: Intel's x86_64 appeared, and Intel does
    support SYSENTER in long mode. "Some future 64-bit CPU" is here
    already.
    
    The code may appear "buggy" for AMD as it stands, since
    MSR_IA32_SYSENTER_EIP is only 32-bit for AMD CPUs. Writing a
    kernel function's address to it would drop high bits. Subsequent
    use of this MSR for branch via SYSENTER seem to allow user to
    transition to CPL0 while executing his code. Scary, eh?
    
    Explain why that is not a bug: because SYSENTER insn would not
    work on AMD CPU.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1427453956-21931-1-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c928a7ae1099..71e4adcb15f1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1173,8 +1173,10 @@ void syscall_init(void)
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, ia32_cstar_target);
 	/*
-	 * Always load these, in case some future 64-bit CPU supports
-	 * SYSENTER from compat mode too:
+	 * This only works on Intel CPUs.
+	 * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.
+	 * This does not cause SYSENTER to jump to the wrong location, because
+	 * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit).
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);

commit 936c663aed930972f7e185485fd6c2da69e33819
Merge: 072e5a1cfabc 50f16a8bf9d7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Mar 27 09:46:19 2015 +0100

    Merge branch 'perf/x86' into perf/core, because it's ready
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d56fe4bf5f3cc30d455c80520f7b71da36ae00e6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 24 14:41:37 2015 +0100

    x86/asm/entry/64: Always set up SYSENTER MSRs
    
    On CONFIG_IA32_EMULATION=y kernels we set up
    MSR_IA32_SYSENTER_CS/ESP/EIP, but on !CONFIG_IA32_EMULATION
    kernels we leave them unchanged.
    
    Clear them to make sure the instruction is disabled properly.
    
    SYSCALL is set up properly in both cases.
    
    Acked-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 002216ab9145..c928a7ae1099 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1169,9 +1169,8 @@ void syscall_init(void)
 	 */
 	wrmsrl(MSR_STAR,  ((u64)__USER32_CS)<<48  | ((u64)__KERNEL_CS)<<32);
 	wrmsrl(MSR_LSTAR, system_call);
-#ifndef CONFIG_IA32_EMULATION
-	wrmsrl(MSR_CSTAR, ignore_sysret);
-#else
+
+#ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, ia32_cstar_target);
 	/*
 	 * Always load these, in case some future 64-bit CPU supports
@@ -1180,6 +1179,11 @@ void syscall_init(void)
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
+#else
+	wrmsrl(MSR_CSTAR, ignore_sysret);
+	wrmsrl_safe(MSR_IA32_SYSENTER_CS, 0);
+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
+	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, 0ULL);
 #endif
 
 	/* Flags to clear on syscall */

commit ef593260f0cae2699874f098fb5b19fb46502cb3
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Thu Mar 19 18:17:46 2015 +0100

    x86/asm/entry: Get rid of KERNEL_STACK_OFFSET
    
    PER_CPU_VAR(kernel_stack) was set up in a way where it points
    five stack slots below the top of stack.
    
    Presumably, it was done to avoid one "sub $5*8,%rsp"
    in syscall/sysenter code paths, where iret frame needs to be
    created by hand.
    
    Ironically, none of them benefits from this optimization,
    since all of them need to allocate additional data on stack
    (struct pt_regs), so they still have to perform subtraction.
    
    This patch eliminates KERNEL_STACK_OFFSET.
    
    PER_CPU_VAR(kernel_stack) now points directly to top of stack.
    pt_regs allocations are adjusted to allocate iret frame as well.
    Hopefully we can merge it later with 32-bit specific
    PER_CPU_VAR(cpu_current_top_of_stack) variable...
    
    Net result in generated code is that constants in several insns
    are changed.
    
    This change is necessary for changing struct pt_regs creation
    in SYSCALL64 code path from MOV to PUSH instructions.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1426785469-15125-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 66d62aef7214..002216ab9145 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1116,7 +1116,7 @@ static __init int setup_disablecpuid(char *arg)
 __setup("clearcpuid=", setup_disablecpuid);
 
 DEFINE_PER_CPU(unsigned long, kernel_stack) =
-	(unsigned long)&init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;
+	(unsigned long)&init_thread_union + THREAD_SIZE;
 EXPORT_PER_CPU_SYMBOL(kernel_stack);
 
 #ifdef CONFIG_X86_64

commit a76c7f4604937bc781bfc411ef92c59474ddadda
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Sun Mar 22 20:48:14 2015 +0100

    x86/asm/entry/64: Fold syscall32_cpu_init() into its sole user
    
    Having syscall32/sysenter32 initialization in a separate tiny
    function, called from within a function that is already syscall
    init specific, serves no real purpose.
    
    Its existense also caused an unintended effect of having
    wrmsrl(MSR_CSTAR) performed twice: once we set it to a dummy
    function returning -ENOSYS, and immediately after
    (if CONFIG_IA32_EMULATION), we set it to point to the proper
    syscall32 entry point, ia32_cstar_target.
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d79f139871e0..66d62aef7214 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -959,24 +959,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 #endif
 }
 
-#ifdef CONFIG_X86_64
-# ifdef CONFIG_IA32_EMULATION
-/* May not be __init: called during resume */
-static void syscall32_cpu_init(void)
-{
-	/*
-	 * Always load these, in case some future 64-bit CPU supports
-	 * SYSENTER from compat mode too:
-	 */
-	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
-	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
-	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
-
-	wrmsrl(MSR_CSTAR, ia32_cstar_target);
-}
-# endif
-#endif
-
 /*
  * Set up the CPU state needed to execute SYSENTER/SYSEXIT instructions
  * on 32-bit kernels:
@@ -1187,10 +1169,17 @@ void syscall_init(void)
 	 */
 	wrmsrl(MSR_STAR,  ((u64)__USER32_CS)<<48  | ((u64)__KERNEL_CS)<<32);
 	wrmsrl(MSR_LSTAR, system_call);
+#ifndef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, ignore_sysret);
-
-#ifdef CONFIG_IA32_EMULATION
-	syscall32_cpu_init();
+#else
+	wrmsrl(MSR_CSTAR, ia32_cstar_target);
+	/*
+	 * Always load these, in case some future 64-bit CPU supports
+	 * SYSENTER from compat mode too:
+	 */
+	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
+	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
 #endif
 
 	/* Flags to clear on syscall */

commit 8b6c0ab1a1296ef6922160fa27018f25c60b8940
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Mar 16 10:32:20 2015 +0100

    x86/asm/entry: Document and clean up the enable_sep_cpu() and syscall32_cpu_init() functions
    
    Clean up the flow and document the functions a bit better.
    
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7a3dfb1db78d..d79f139871e0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -960,37 +960,53 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 }
 
 #ifdef CONFIG_X86_64
-#ifdef CONFIG_IA32_EMULATION
+# ifdef CONFIG_IA32_EMULATION
 /* May not be __init: called during resume */
 static void syscall32_cpu_init(void)
 {
-	/* Load these always in case some future AMD CPU supports
-	   SYSENTER from compat mode too. */
+	/*
+	 * Always load these, in case some future 64-bit CPU supports
+	 * SYSENTER from compat mode too:
+	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
 	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
 
 	wrmsrl(MSR_CSTAR, ia32_cstar_target);
 }
-#endif		/* CONFIG_IA32_EMULATION */
-#endif		/* CONFIG_X86_64 */
+# endif
+#endif
 
+/*
+ * Set up the CPU state needed to execute SYSENTER/SYSEXIT instructions
+ * on 32-bit kernels:
+ */
 #ifdef CONFIG_X86_32
 void enable_sep_cpu(void)
 {
-	int cpu = get_cpu();
-	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
+	struct tss_struct *tss;
+	int cpu;
 
-	if (!boot_cpu_has(X86_FEATURE_SEP)) {
-		put_cpu();
-		return;
-	}
+	cpu = get_cpu();
+	tss = &per_cpu(cpu_tss, cpu);
+
+	if (!boot_cpu_has(X86_FEATURE_SEP))
+		goto out;
+
+	/*
+	 * The struct::SS1 and tss_struct::SP1 fields are not used by the hardware,
+	 * we cache the SYSENTER CS and ESP values there for easy access:
+	 */
 
 	tss->x86_tss.ss1 = __KERNEL_CS;
+	wrmsr(MSR_IA32_SYSENTER_CS, tss->x86_tss.ss1, 0);
+
 	tss->x86_tss.sp1 = (unsigned long)tss + offsetofend(struct tss_struct, SYSENTER_stack);
-	wrmsr(MSR_IA32_SYSENTER_CS, __KERNEL_CS, 0);
 	wrmsr(MSR_IA32_SYSENTER_ESP, tss->x86_tss.sp1, 0);
-	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long) ia32_sysenter_target, 0);
+
+	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)ia32_sysenter_target, 0);
+
+out:
 	put_cpu();
 }
 #endif

commit d828c71fba8922b116b4ec56c3e5bca8c822d5ae
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Mon Mar 9 15:52:18 2015 +0100

    x86/asm/entry/32: Document the 32-bit SYSENTER "emergency stack" better
    
    Before the patch, the 'tss_struct::stack' field was not referenced anywhere.
    
    It was used only to set SYSENTER's stack to point after the last byte
    of tss_struct, thus the trailing field, stack[64], was used.
    
    But grep would not know it. You can comment it out, compile,
    and kernel will even run until an unlucky NMI corrupts
    io_bitmap[] (which is also not easily detectable).
    
    This patch changes code so that the purpose and usage of this
    field is not mysterious anymore, and can be easily grepped for.
    
    This does change generated code, for a subtle reason:
    since tss_struct is ____cacheline_aligned, there happens to be
    5 longs of padding at the end. Old code was using the padding
    too; new code will strictly use it only for SYSENTER_stack[].
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Drewry <wad@chromium.org>
    Link: http://lkml.kernel.org/r/1425912738-559-2-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 76348334b934..7a3dfb1db78d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -987,7 +987,7 @@ void enable_sep_cpu(void)
 	}
 
 	tss->x86_tss.ss1 = __KERNEL_CS;
-	tss->x86_tss.sp1 = sizeof(struct tss_struct) + (unsigned long) tss;
+	tss->x86_tss.sp1 = (unsigned long)tss + offsetofend(struct tss_struct, SYSENTER_stack);
 	wrmsr(MSR_IA32_SYSENTER_CS, __KERNEL_CS, 0);
 	wrmsr(MSR_IA32_SYSENTER_ESP, tss->x86_tss.sp1, 0);
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long) ia32_sysenter_target, 0);

commit a7fcf28d431ef70afaa91496e64e16dc51dccec4
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Mar 6 17:50:19 2015 -0800

    x86/asm/entry: Replace this_cpu_sp0() with current_top_of_stack() and fix it on x86_32
    
    I broke 32-bit kernels.  The implementation of sp0 was correct
    as far as I can tell, but sp0 was much weirder on x86_32 than I
    realized.  It has the following issues:
    
     - Init's sp0 is inconsistent with everything else's: non-init tasks
       are offset by 8 bytes.  (I have no idea why, and the comment is unhelpful.)
    
     - vm86 does crazy things to sp0.
    
    Fix it up by replacing this_cpu_sp0() with
    current_top_of_stack() and using a new percpu variable to track
    the top of the stack on x86_32.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 75182b1632a8 ("x86/asm/entry: Switch all C consumers of kernel_stack to this_cpu_sp0()")
    Link: http://lkml.kernel.org/r/d09dbe270883433776e0cbee3c7079433349e96d.1425692936.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 5d0f0cc7ea26..76348334b934 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1130,8 +1130,8 @@ DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE) __visible;
 
 /*
- * The following four percpu variables are hot.  Align current_task to
- * cacheline size such that all four fall in the same cacheline.
+ * The following percpu variables are hot.  Align current_task to
+ * cacheline size such that they fall in the same cacheline.
  */
 DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =
 	&init_task;
@@ -1226,6 +1226,15 @@ DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
 EXPORT_PER_CPU_SYMBOL(__preempt_count);
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 
+/*
+ * On x86_32, vm86 modifies tss.sp0, so sp0 isn't a reliable way to find
+ * the top of the kernel stack.  Use an extra percpu variable to track the
+ * top of the kernel stack directly.
+ */
+DEFINE_PER_CPU(unsigned long, cpu_current_top_of_stack) =
+	(unsigned long)&init_thread_union + THREAD_SIZE;
+EXPORT_PER_CPU_SYMBOL(cpu_current_top_of_stack);
+
 #ifdef CONFIG_CC_STACKPROTECTOR
 DEFINE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif

commit 24933b82c0d9a711475a5ef7904eb733f561e637
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Mar 5 19:19:05 2015 -0800

    x86/asm/entry: Rename 'init_tss' to 'cpu_tss'
    
    It has nothing to do with init -- there's only one TSS per cpu.
    
    Other names considered include:
    
     - current_tss: Confusing because we never switch the tss.
     - singleton_tss: Too long.
    
    This patch was generated with 's/init_tss/cpu_tss/g'.  Followup
    patches will fix INIT_TSS and INIT_TSS_IST by hand.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/da29fb2a793e4f649d93ce2d1ed320ebe8516262.1425611534.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2346c95c6ab1..5d0f0cc7ea26 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -979,7 +979,7 @@ static void syscall32_cpu_init(void)
 void enable_sep_cpu(void)
 {
 	int cpu = get_cpu();
-	struct tss_struct *tss = &per_cpu(init_tss, cpu);
+	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
 
 	if (!boot_cpu_has(X86_FEATURE_SEP)) {
 		put_cpu();
@@ -1307,7 +1307,7 @@ void cpu_init(void)
 	 */
 	load_ucode_ap();
 
-	t = &per_cpu(init_tss, cpu);
+	t = &per_cpu(cpu_tss, cpu);
 	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
@@ -1391,7 +1391,7 @@ void cpu_init(void)
 {
 	int cpu = smp_processor_id();
 	struct task_struct *curr = current;
-	struct tss_struct *t = &per_cpu(init_tss, cpu);
+	struct tss_struct *t = &per_cpu(cpu_tss, cpu);
 	struct thread_struct *thread = &curr->thread;
 
 	wait_for_master_cpu(cpu);

commit 5b2bdbc84556774afbe11bcfd24c2f6411cfa92b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Feb 27 14:50:19 2015 -0500

    x86: Init per-cpu shadow copy of CR4 on 32-bit CPUs too
    
    Commit:
    
       1e02ce4cccdc ("x86: Store a per-cpu shadow copy of CR4")
    
    added a shadow CR4 such that reads and writes that do not
    modify the CR4 execute much faster than always reading the
    register itself.
    
    The change modified cpu_init() in common.c, so that the
    shadow CR4 gets initialized before anything uses it.
    
    Unfortunately, there's two cpu_init()s in common.c. There's
    one for 64-bit and one for 32-bit. The commit only added
    the shadow init to the 64-bit path, but the 32-bit path
    needs the init too.
    
    Link: http://lkml.kernel.org/r/20150227125208.71c36402@gandalf.local.home Fixes: 1e02ce4cccdc "x86: Store a per-cpu shadow copy of CR4"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20150227145019.2bdd4354@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b5c8ff5e9dfc..2346c95c6ab1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1396,6 +1396,12 @@ void cpu_init(void)
 
 	wait_for_master_cpu(cpu);
 
+	/*
+	 * Initialize the CR4 shadow before doing anything that could
+	 * try to read it.
+	 */
+	cr4_init_shadow();
+
 	show_ucode_info_early();
 
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);

commit cbc82b17263877ea5d21e84c58ce03f0292458a1
Author: Peter P Waskiewicz Jr <peter.p.waskiewicz.jr@intel.com>
Date:   Fri Jan 23 18:45:43 2015 +0000

    x86: Add support for Intel Cache QoS Monitoring (CQM) detection
    
    This patch adds support for the new Cache QoS Monitoring (CQM)
    feature found in future Intel Xeon processors.  It includes the
    new values to track CQM resources to the cpuinfo_x86 structure,
    plus the CPUID detection routines for CQM.
    
    CQM allows a process, or set of processes, to be tracked by the CPU
    to determine the cache usage of that task group.  Using this data
    from the CPU, software can be written to extract this data and
    report cache usage and occupancy for a particular process, or
    group of processes.
    
    More information about Cache QoS Monitoring can be found in the
    Intel (R) x86 Architecture Software Developer Manual, section 17.14.
    
    Signed-off-by: Peter P Waskiewicz Jr <peter.p.waskiewicz.jr@intel.com>
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Chris Webb <chris@arachsys.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kanaka Juvva <kanaka.d.juvva@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Steven Honeyman <stevenhoneyman@gmail.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1422038748-21397-5-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 07f2fc3c13a4..9fa00b2ea0ee 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -645,6 +645,30 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[10] = eax;
 	}
 
+	/* Additional Intel-defined flags: level 0x0000000F */
+	if (c->cpuid_level >= 0x0000000F) {
+		u32 eax, ebx, ecx, edx;
+
+		/* QoS sub-leaf, EAX=0Fh, ECX=0 */
+		cpuid_count(0x0000000F, 0, &eax, &ebx, &ecx, &edx);
+		c->x86_capability[11] = edx;
+		if (cpu_has(c, X86_FEATURE_CQM_LLC)) {
+			/* will be overridden if occupancy monitoring exists */
+			c->x86_cache_max_rmid = ebx;
+
+			/* QoS sub-leaf, EAX=0Fh, ECX=1 */
+			cpuid_count(0x0000000F, 1, &eax, &ebx, &ecx, &edx);
+			c->x86_capability[12] = edx;
+			if (cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC)) {
+				c->x86_cache_max_rmid = ecx;
+				c->x86_cache_occ_scale = ebx;
+			}
+		} else {
+			c->x86_cache_max_rmid = -1;
+			c->x86_cache_occ_scale = -1;
+		}
+	}
+
 	/* AMD-defined flags: level 0x80000001 */
 	xlvl = cpuid_eax(0x80000000);
 	c->extended_cpuid_level = xlvl;
@@ -833,6 +857,20 @@ static void generic_identify(struct cpuinfo_x86 *c)
 	detect_nopl(c);
 }
 
+static void x86_init_cache_qos(struct cpuinfo_x86 *c)
+{
+	/*
+	 * The heavy lifting of max_rmid and cache_occ_scale are handled
+	 * in get_cpu_cap().  Here we just set the max_rmid for the boot_cpu
+	 * in case CQM bits really aren't there in this CPU.
+	 */
+	if (c != &boot_cpu_data) {
+		boot_cpu_data.x86_cache_max_rmid =
+			min(boot_cpu_data.x86_cache_max_rmid,
+			    c->x86_cache_max_rmid);
+	}
+}
+
 /*
  * This does the hard work of actually picking apart the CPU stuff...
  */
@@ -922,6 +960,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 
 	init_hypervisor(c);
 	x86_init_rdrand(c);
+	x86_init_cache_qos(c);
 
 	/*
 	 * Clear/Set all flags overriden by options, need do it

commit 37507717de51a8332a34ee07fd88700be88df5bf
Merge: a68fb48380bb a66734297f78
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 16 14:58:12 2015 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 perf updates from Ingo Molnar:
     "This series tightens up RDPMC permissions: currently even highly
      sandboxed x86 execution environments (such as seccomp) have permission
      to execute RDPMC, which may leak various perf events / PMU state such
      as timing information and other CPU execution details.
    
      This 'all is allowed' RDPMC mode is still preserved as the
      (non-default) /sys/devices/cpu/rdpmc=2 setting.  The new default is
      that RDPMC access is only allowed if a perf event is mmap-ed (which is
      needed to correctly interpret RDPMC counter values in any case).
    
      As a side effect of these changes CR4 handling is cleaned up in the
      x86 code and a shadow copy of the CR4 value is added.
    
      The extra CR4 manipulation adds ~ <50ns to the context switch cost
      between rdpmc-capable and rdpmc-non-capable mms"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86: Add /sys/devices/cpu/rdpmc=2 to allow rdpmc for all tasks
      perf/x86: Only allow rdpmc if a perf_event is mapped
      perf: Pass the event to arch_perf_update_userpage()
      perf: Add pmu callbacks to track event mapping and unmapping
      x86: Add a comment clarifying LDT context switching
      x86: Store a per-cpu shadow copy of CR4
      x86: Clean up cr4 manipulation

commit 80f33a5fdf66d4338789e8aa80589bda088cb35d
Merge: 7453311d68f1 d505ad1d66c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 9 17:50:09 2015 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/rtc: Remove duplicate const specifier
      x86, early_serial_console: Remove unnecessary check
      x86, early_serial_console: Remove unused macro XMTRDY
      x86, setup: Rename BOOT_ISDIGIT_H to BOOT_CTYPE_H
      x86, CPU: Fix trivial printk formatting issues with dmesg

commit 1e02ce4cccdcb9688386e5b8d2c9fa4660b45389
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:08 2014 -0700

    x86: Store a per-cpu shadow copy of CR4
    
    Context switches and TLB flushes can change individual bits of CR4.
    CR4 reads take several cycles, so store a shadow copy of CR4 in a
    per-cpu variable.
    
    To avoid wasting a cache line, I added the CR4 shadow to
    cpu_tlbstate, which is already touched in switch_mm.  The heaviest
    users of the cr4 shadow will be switch_mm and __switch_to_xtra, and
    __switch_to_xtra is called shortly after switch_mm during context
    switch, so the cacheline is likely to be hot.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/3a54dd3353fffbf84804398e00dfdc5b7c1afd7d.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9d8fc49f0922..07f2fc3c13a4 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -19,6 +19,7 @@
 #include <asm/archrandom.h>
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
+#include <asm/tlbflush.h>
 #include <asm/debugreg.h>
 #include <asm/sections.h>
 #include <asm/vsyscall.h>
@@ -1293,6 +1294,12 @@ void cpu_init(void)
 
 	wait_for_master_cpu(cpu);
 
+	/*
+	 * Initialize the CR4 shadow before doing anything that could
+	 * try to read it.
+	 */
+	cr4_init_shadow();
+
 	/*
 	 * Load microcode on this cpu if a valid microcode is available.
 	 * This is early microcode loading procedure.

commit 375074cc736ab1d89a708c0a8d7baa4a70d5d476
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:07 2014 -0700

    x86: Clean up cr4 manipulation
    
    CR4 manipulation was split, seemingly at random, between direct
    (write_cr4) and using a helper (set/clear_in_cr4).  Unfortunately,
    the set_in_cr4 and clear_in_cr4 helpers also poke at the boot code,
    which only a small subset of users actually wanted.
    
    This patch replaces all cr4 access in functions that don't leave cr4
    exactly the way they found it with new helpers cr4_set_bits,
    cr4_clear_bits, and cr4_set_bits_and_update_boot.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/495a10bdc9e67016b8fd3945700d46cfd5c12c2f.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c6049650c093..9d8fc49f0922 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -278,7 +278,7 @@ __setup("nosmep", setup_disable_smep);
 static __always_inline void setup_smep(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_SMEP))
-		set_in_cr4(X86_CR4_SMEP);
+		cr4_set_bits(X86_CR4_SMEP);
 }
 
 static __init int setup_disable_smap(char *arg)
@@ -298,9 +298,9 @@ static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 
 	if (cpu_has(c, X86_FEATURE_SMAP)) {
 #ifdef CONFIG_X86_SMAP
-		set_in_cr4(X86_CR4_SMAP);
+		cr4_set_bits(X86_CR4_SMAP);
 #else
-		clear_in_cr4(X86_CR4_SMAP);
+		cr4_clear_bits(X86_CR4_SMAP);
 #endif
 	}
 }
@@ -1312,7 +1312,7 @@ void cpu_init(void)
 
 	pr_debug("Initializing CPU#%d\n", cpu);
 
-	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+	cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
@@ -1393,7 +1393,7 @@ void cpu_init(void)
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
 
 	if (cpu_feature_enabled(X86_FEATURE_VME) || cpu_has_tsc || cpu_has_de)
-		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	load_current_idt();
 	switch_to_new_gdt(cpu);

commit 659006bf3ae37a08706907ce1a36ddf57c9131d2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:26 2015 +0000

    x86/x2apic: Split enable and setup function
    
    enable_x2apic() is a convoluted unreadable mess because it is used for
    both enablement in early boot and for setup in cpu_init().
    
    Split the code into x2apic_enable() for enablement and x2apic_setup()
    for setup of (secondary cpus). Make use of the new state tracking to
    simplify the logic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20150115211703.129287153@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c6049650c093..cb5692551b98 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1332,7 +1332,7 @@ void cpu_init(void)
 	barrier();
 
 	x86_configure_nx();
-	enable_x2apic();
+	x2apic_setup();
 
 	/*
 	 * set up and load the per-CPU TSS

commit f94fe119f2e53362a3038ee856fa58412f728bc9
Author: Steven Honeyman <stevenhoneyman@gmail.com>
Date:   Wed Nov 5 22:52:18 2014 +0000

    x86, CPU: Fix trivial printk formatting issues with dmesg
    
    dmesg (from util-linux) currently has two methods for reading the kernel
    message ring buffer: /dev/kmsg and syslog(2). Since kernel 3.5.0 kmsg
    has been the default, which escapes control characters (e.g. new lines)
    before they are shown.
    
    This change means that when dmesg is using /dev/kmsg, a 2 line printk
    makes the output messy, because the second line does not get a
    timestamp.
    
    For example:
    
    [    0.012863] CPU0: Thermal monitoring enabled (TM1)
    [    0.012869] Last level iTLB entries: 4KB 1024, 2MB 1024, 4MB 1024
    Last level dTLB entries: 4KB 1024, 2MB 1024, 4MB 1024, 1GB 4
    [    0.012958] Freeing SMP alternatives memory: 28K (ffffffff81d86000 - ffffffff81d8d000)
    [    0.014961] dmar: Host address width 39
    
    Because printk.c intentionally escapes control characters, they should
    not be there in the first place. This patch fixes two occurrences of
    this.
    
    Signed-off-by: Steven Honeyman <stevenhoneyman@gmail.com>
    Link: https://lkml.kernel.org/r/1414856696-8094-1-git-send-email-stevenhoneyman@gmail.com
    [ Boris: make cpu_detect_tlb() static, while at it. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c6049650c093..4973d6308938 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -491,17 +491,18 @@ u16 __read_mostly tlb_lld_2m[NR_INFO];
 u16 __read_mostly tlb_lld_4m[NR_INFO];
 u16 __read_mostly tlb_lld_1g[NR_INFO];
 
-void cpu_detect_tlb(struct cpuinfo_x86 *c)
+static void cpu_detect_tlb(struct cpuinfo_x86 *c)
 {
 	if (this_cpu->c_detect_tlb)
 		this_cpu->c_detect_tlb(c);
 
-	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n"
-		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n",
+	pr_info("Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n",
 		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
-		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
-		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES],
-		tlb_lld_1g[ENTRIES]);
+		tlb_lli_4m[ENTRIES]);
+
+	pr_info("Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n",
+		tlb_lld_4k[ENTRIES], tlb_lld_2m[ENTRIES],
+		tlb_lld_4m[ENTRIES], tlb_lld_1g[ENTRIES]);
 }
 
 void detect_ht(struct cpuinfo_x86 *c)

commit 3100e448e7d74489a96cb7b45d88fe6962774eaa
Merge: c9f861c77269 26893107aa71
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 14:24:20 2014 -0800

    Merge branch 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 vdso updates from Ingo Molnar:
     "Various vDSO updates from Andy Lutomirski, mostly cleanups and
      reorganization to improve maintainability, but also some
      micro-optimizations and robustization changes"
    
    * 'x86-vdso-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86_64/vsyscall: Restore orig_ax after vsyscall seccomp
      x86_64: Add a comment explaining the TASK_SIZE_MAX guard page
      x86_64,vsyscall: Make vsyscall emulation configurable
      x86_64, vsyscall: Rewrite comment and clean up headers in vsyscall code
      x86_64, vsyscall: Turn vsyscalls all the way off when vsyscall==none
      x86,vdso: Use LSL unconditionally for vgetcpu
      x86: vdso: Fix build with older gcc
      x86_64/vdso: Clean up vgetcpu init and merge the vdso initcalls
      x86_64/vdso: Remove jiffies from the vvar page
      x86/vdso: Make the PER_CPU segment 32 bits
      x86/vdso: Make the PER_CPU segment start out accessed
      x86/vdso: Change the PER_CPU segment to use struct desc_struct
      x86_64/vdso: Move getcpu code from vsyscall_64.c to vdso/vma.c
      x86_64/vsyscall: Move all of the gate_area code to vsyscall_64.c

commit 2cd3949f702692cf4c5d05b463f19cd706a92dd3
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Tue Nov 11 14:01:33 2014 -0800

    x86: Require exact match for 'noxsave' command line option
    
    We have some very similarly named command-line options:
    
    arch/x86/kernel/cpu/common.c:__setup("noxsave", x86_xsave_setup);
    arch/x86/kernel/cpu/common.c:__setup("noxsaveopt", x86_xsaveopt_setup);
    arch/x86/kernel/cpu/common.c:__setup("noxsaves", x86_xsaves_setup);
    
    __setup() is designed to match options that take arguments, like
    "foo=bar" where you would have:
    
            __setup("foo", x86_foo_func...);
    
    The problem is that "noxsave" actually _matches_ "noxsaves" in
    the same way that "foo" matches "foo=bar".  If you boot an old
    kernel that does not know about "noxsaves" with "noxsaves" on the
    command line, it will interpret the argument as "noxsave", which
    is not what you want at all.
    
    This makes the "noxsave" handler only return success when it finds
    an *exact* match.
    
    [ tglx: We really need to make __setup() more robust. ]
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: x86@kernel.org
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20141111220133.FE053984@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4b4f78c9ba19..cfa9b5b2c27a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -146,6 +146,8 @@ EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
 static int __init x86_xsave_setup(char *s)
 {
+	if (strlen(s))
+		return 0;
 	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
 	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
 	setup_clear_cpu_cap(X86_FEATURE_XSAVES);

commit e76b027e6408f5570dc940b731ec9ae870c6188a
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Oct 30 14:58:01 2014 -0700

    x86,vdso: Use LSL unconditionally for vgetcpu
    
    LSL is faster than RDTSCP and works everywhere; there's no need to
    switch between them depending on CPU.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Andi Kleen <andi@firstfloor.org>
    Link: http://lkml.kernel.org/r/72f73d5ec4514e02bba345b9759177ef03742efb.1414706021.git.luto@amacapital.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4b4f78c9ba19..175372b854be 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -956,14 +956,6 @@ static void identify_cpu(struct cpuinfo_x86 *c)
 }
 
 #ifdef CONFIG_X86_64
-static void vgetcpu_set_mode(void)
-{
-	if (cpu_has(&boot_cpu_data, X86_FEATURE_RDTSCP))
-		vgetcpu_mode = VGETCPU_RDTSCP;
-	else
-		vgetcpu_mode = VGETCPU_LSL;
-}
-
 #ifdef CONFIG_IA32_EMULATION
 /* May not be __init: called during resume */
 static void syscall32_cpu_init(void)
@@ -1006,8 +998,6 @@ void __init identify_boot_cpu(void)
 #ifdef CONFIG_X86_32
 	sysenter_setup();
 	enable_sep_cpu();
-#else
-	vgetcpu_set_mode();
 #endif
 	cpu_detect_tlb(&boot_cpu_data);
 }

commit 0429fbc0bdc297d64188483ba029a23773ae07b0
Merge: 6929c358972f 513d1a2884a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 15 07:48:18 2014 +0200

    Merge branch 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    Pull percpu consistent-ops changes from Tejun Heo:
     "Way back, before the current percpu allocator was implemented, static
      and dynamic percpu memory areas were allocated and handled separately
      and had their own accessors.  The distinction has been gone for many
      years now; however, the now duplicate two sets of accessors remained
      with the pointer based ones - this_cpu_*() - evolving various other
      operations over time.  During the process, we also accumulated other
      inconsistent operations.
    
      This pull request contains Christoph's patches to clean up the
      duplicate accessor situation.  __get_cpu_var() uses are replaced with
      with this_cpu_ptr() and __this_cpu_ptr() with raw_cpu_ptr().
    
      Unfortunately, the former sometimes is tricky thanks to C being a bit
      messy with the distinction between lvalues and pointers, which led to
      a rather ugly solution for cpumask_var_t involving the introduction of
      this_cpu_cpumask_var_ptr().
    
      This converts most of the uses but not all.  Christoph will follow up
      with the remaining conversions in this merge window and hopefully
      remove the obsolete accessors"
    
    * 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (38 commits)
      irqchip: Properly fetch the per cpu offset
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t -fix
      ia64: sn_nodepda cannot be assigned to after this_cpu conversion. Use __this_cpu_write.
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t
      Revert "powerpc: Replace __get_cpu_var uses"
      percpu: Remove __this_cpu_ptr
      clocksource: Replace __this_cpu_ptr with raw_cpu_ptr
      sparc: Replace __get_cpu_var uses
      avr32: Replace __get_cpu_var with __this_cpu_write
      blackfin: Replace __get_cpu_var uses
      tile: Use this_cpu_ptr() for hardware counters
      tile: Replace __get_cpu_var uses
      powerpc: Replace __get_cpu_var uses
      alpha: Replace __get_cpu_var
      ia64: Replace __get_cpu_var uses
      s390: cio driver &__get_cpu_var replacements
      s390: Replace __get_cpu_var uses
      mips: Replace __get_cpu_var uses
      MIPS: Replace __get_cpu_var uses in FPU emulator.
      arm: Replace __this_cpu_ptr with raw_cpu_ptr
      ...

commit dfe2c6dcc8ca2cdc662d7c0473e9811b72ef3370
Merge: a45d572841a2 64e455079e1b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 14 03:54:50 2014 +0200

    Merge branch 'akpm' (patches from Andrew Morton)
    
    Merge second patch-bomb from Andrew Morton:
     - a few hotfixes
     - drivers/dma updates
     - MAINTAINERS updates
     - Quite a lot of lib/ updates
     - checkpatch updates
     - binfmt updates
     - autofs4
     - drivers/rtc/
     - various small tweaks to less used filesystems
     - ipc/ updates
     - kernel/watchdog.c changes
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (135 commits)
      mm: softdirty: enable write notifications on VMAs after VM_SOFTDIRTY cleared
      kernel/param: consolidate __{start,stop}___param[] in <linux/moduleparam.h>
      ia64: remove duplicate declarations of __per_cpu_start[] and __per_cpu_end[]
      frv: remove unused declarations of __start___ex_table and __stop___ex_table
      kvm: ensure hard lockup detection is disabled by default
      kernel/watchdog.c: control hard lockup detection default
      staging: rtl8192u: use %*pEn to escape buffer
      staging: rtl8192e: use %*pEn to escape buffer
      staging: wlan-ng: use %*pEhp to print SN
      lib80211: remove unused print_ssid()
      wireless: hostap: proc: print properly escaped SSID
      wireless: ipw2x00: print SSID via %*pE
      wireless: libertas: print esaped string via %*pE
      lib/vsprintf: add %*pE[achnops] format specifier
      lib / string_helpers: introduce string_escape_mem()
      lib / string_helpers: refactoring the test suite
      lib / string_helpers: move documentation to c-file
      include/linux: remove strict_strto* definitions
      arch/x86/mm/numa.c: fix boot failure when all nodes are hotpluggable
      fs: check bh blocknr earlier when searching lru
      ...

commit 2fd7476de999124bbf6830aa59ac092c882280fe
Merge: ba1a96fc7ddc 4ea48a01bb1a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 14 02:28:16 2014 +0200

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc smaller fixes that missed the v3.17 cycle"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/build: Add arch/x86/purgatory/ make generated files to gitignore
      x86: Fix section conflict for numachip
      x86: Reject x32 executables if x32 ABI not supported
      x86_64, entry: Filter RFLAGS.NT on entry from userspace
      x86, boot, kaslr: Fix nuisance warning on 32-bit builds

commit 708d0b41a26907ac83cde41dd5a75b5a2f8f1218
Merge: f1d0d14120a8 9298b815efe5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 14 02:19:47 2014 +0200

    Merge branch 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpufeature updates from Ingo Molnar:
     "This tree includes the following changes:
    
       - Introduce DISABLED_MASK to list disabled CPU features, to simplify
         CPU feature handling and avoid excessive #ifdefs
    
       - Remove the lightly used cpu_has_pae() primitive"
    
    * 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Add more disabled features
      x86: Introduce disabled-features
      x86: Axe the lightly-used cpu_has_pae

commit e48510f45107613bf14060eeabd658c49a044242
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Oct 13 15:54:22 2014 -0700

    arch/x86/kernel/cpu/common.c: fix unused symbol warning
    
    x86_64 allnoconfig:
    
    arch/x86/kernel/cpu/common.c:968: warning: 'syscall32_cpu_init' defined but not used
    
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d8b1166c7888..8b699bff0099 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -964,6 +964,7 @@ static void vgetcpu_set_mode(void)
 		vgetcpu_mode = VGETCPU_LSL;
 }
 
+#ifdef CONFIG_IA32_EMULATION
 /* May not be __init: called during resume */
 static void syscall32_cpu_init(void)
 {
@@ -975,7 +976,8 @@ static void syscall32_cpu_init(void)
 
 	wrmsrl(MSR_CSTAR, ia32_cstar_target);
 }
-#endif
+#endif		/* CONFIG_IA32_EMULATION */
+#endif		/* CONFIG_X86_64 */
 
 #ifdef CONFIG_X86_32
 void enable_sep_cpu(void)

commit 19e00d593e9a273ecbfbe131676ed11c140670ac
Merge: 197fe6b0e684 eeeda4cd06e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 18:16:32 2014 +0200

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 bootup updates from Ingo Molnar:
     "The changes in this cycle were:
    
       - Fix rare SMP-boot hang (mostly in virtual environments)
    
       - Fix build warning with certain (rare) toolchains"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/relocs: Make per_cpu_load_addr static
      x86/smpboot: Initialize secondary CPU only if master CPU will wait for it

commit 8c7aa698baca5e8f1ba9edb68081f1e7a1abf455
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Wed Oct 1 11:49:04 2014 -0700

    x86_64, entry: Filter RFLAGS.NT on entry from userspace
    
    The NT flag doesn't do anything in long mode other than causing IRET
    to #GP.  Oddly, CPL3 code can still set NT using popf.
    
    Entry via hardware or software interrupt clears NT automatically, so
    the only relevant entries are fast syscalls.
    
    If user code causes kernel code to run with NT set, then there's at
    least some (small) chance that it could cause trouble.  For example,
    user code could cause a call to EFI code with NT set, and who knows
    what would happen?  Apparently some games on Wine sometimes do
    this (!), and, if an IRET return happens, they will segfault.  That
    segfault cannot be handled, because signal delivery fails, too.
    
    This patch programs the CPU to clear NT on entry via SYSCALL (both
    32-bit and 64-bit, by my reading of the AMD APM), and it clears NT
    in software on entry via SYSENTER.
    
    To save a few cycles, this borrows a trick from Jan Beulich in Xen:
    it checks whether NT is set before trying to clear it.  As a result,
    it seems to have very little effect on SYSENTER performance on my
    machine.
    
    There's another minor bug fix in here: it looks like the CFI
    annotations were wrong if CONFIG_AUDITSYSCALL=n.
    
    Testers beware: on Xen, SYSENTER with NT set turns into a GPF.
    
    I haven't touched anything on 32-bit kernels.
    
    The syscall mask change comes from a variant of this patch by Anish
    Bhatt.
    
    Note to stable maintainers: there is no known security issue here.
    A misguided program can set NT and cause the kernel to try and fail
    to deliver SIGSEGV, crashing the program.  This patch fixes Far Cry
    on Wine: https://bugs.winehq.org/show_bug.cgi?id=33275
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Anish Bhatt <anish@chelsio.com>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/395749a5d39a29bd3e4b35899cf3a3c1340e5595.1412189265.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e4ab2b42bd6f..31265580c38a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1184,7 +1184,7 @@ void syscall_init(void)
 	/* Flags to clear on syscall */
 	wrmsrl(MSR_SYSCALL_MASK,
 	       X86_EFLAGS_TF|X86_EFLAGS_DF|X86_EFLAGS_IF|
-	       X86_EFLAGS_IOPL|X86_EFLAGS_AC);
+	       X86_EFLAGS_IOPL|X86_EFLAGS_AC|X86_EFLAGS_NT);
 }
 
 /*

commit ce4b1b16502b182368cda20a61de2995762c8bcc
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Fri Jun 20 14:23:11 2014 +0200

    x86/smpboot: Initialize secondary CPU only if master CPU will wait for it
    
    Hang is observed on virtual machines during CPU hotplug,
    especially in big guests with many CPUs. (It reproducible
    more often if host is over-committed).
    
    It happens because master CPU gives up waiting on
    secondary CPU and allows it to run wild. As result
    AP causes locking or crashing system. For example
    as described here:
    
      https://lkml.org/lkml/2014/3/6/257
    
    If master CPU have sent STARTUP IPI successfully,
    and AP signalled to master CPU that it's ready
    to start initialization, make master CPU wait
    indefinitely till AP is onlined.
    
    To ensure that AP won't ever run wild, make it
    wait at early startup till master CPU confirms its
    intention to wait for AP. If AP doesn't respond in 10
    seconds, the master CPU will timeout and cancel
    AP onlining.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1403266991-12233-1-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e4ab2b42bd6f..426cfedefd04 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1266,6 +1266,19 @@ static void dbg_restore_debug_regs(void)
 #define dbg_restore_debug_regs()
 #endif /* ! CONFIG_KGDB */
 
+static void wait_for_master_cpu(int cpu)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * wait for ACK from master CPU before continuing
+	 * with AP initialization
+	 */
+	WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+	while (!cpumask_test_cpu(cpu, cpu_callout_mask))
+		cpu_relax();
+#endif
+}
+
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT
@@ -1281,16 +1294,17 @@ void cpu_init(void)
 	struct task_struct *me;
 	struct tss_struct *t;
 	unsigned long v;
-	int cpu;
+	int cpu = stack_smp_processor_id();
 	int i;
 
+	wait_for_master_cpu(cpu);
+
 	/*
 	 * Load microcode on this cpu if a valid microcode is available.
 	 * This is early microcode loading procedure.
 	 */
 	load_ucode_ap();
 
-	cpu = stack_smp_processor_id();
 	t = &per_cpu(init_tss, cpu);
 	oist = &per_cpu(orig_ist, cpu);
 
@@ -1302,9 +1316,6 @@ void cpu_init(void)
 
 	me = current;
 
-	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask))
-		panic("CPU#%d already initialized!\n", cpu);
-
 	pr_debug("Initializing CPU#%d\n", cpu);
 
 	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
@@ -1381,13 +1392,9 @@ void cpu_init(void)
 	struct tss_struct *t = &per_cpu(init_tss, cpu);
 	struct thread_struct *thread = &curr->thread;
 
-	show_ucode_info_early();
+	wait_for_master_cpu(cpu);
 
-	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask)) {
-		printk(KERN_WARNING "CPU#%d already initialized!\n", cpu);
-		for (;;)
-			local_irq_enable();
-	}
+	show_ucode_info_early();
 
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
 

commit 9298b815efe500b272e4084ed05eeae7a92b5340
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Sep 11 14:15:24 2014 -0700

    x86: Add more disabled features
    
    The original motivation for these patches was for an Intel CPU
    feature called MPX.  The patch to add a disabled feature for it
    will go in with the other parts of the support.
    
    But, in the meantime, there are a few other features than MPX
    that we can make assumptions about at compile-time based on
    compile options.  Add them to disabled-features.h and check them
    with cpu_feature_enabled().
    
    Note that this gets rid of the last things that needed an #ifdef
    CONFIG_X86_64 in cpufeature.h.  Yay!
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20140911211524.C0EC332A@viggo.jf.intel.com
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e4ab2b42bd6f..724d221107eb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1391,7 +1391,7 @@ void cpu_init(void)
 
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
 
-	if (cpu_has_vme || cpu_has_tsc || cpu_has_de)
+	if (cpu_feature_enabled(X86_FEATURE_VME) || cpu_has_tsc || cpu_has_de)
 		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	load_current_idt();

commit 89cbc76768c2fa4ed95545bf961f3a14ddfeed21
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:40 2014 -0500

    x86: Replace __get_cpu_var uses
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    #define __get_cpu_var(var) (*this_cpu_ptr(&(var)))
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e4ab2b42bd6f..5666eb9568fc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1198,9 +1198,9 @@ DEFINE_PER_CPU(int, debug_stack_usage);
 
 int is_debug_stack(unsigned long addr)
 {
-	return __get_cpu_var(debug_stack_usage) ||
-		(addr <= __get_cpu_var(debug_stack_addr) &&
-		 addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ));
+	return __this_cpu_read(debug_stack_usage) ||
+		(addr <= __this_cpu_read(debug_stack_addr) &&
+		 addr > (__this_cpu_read(debug_stack_addr) - DEBUG_STKSZ));
 }
 NOKPROBE_SYMBOL(is_debug_stack);
 

commit 9def39be4e960917fcb80514ff23651f9ec97193
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Wed Oct 30 08:09:45 2013 -0700

    x86: Support compiling out human-friendly processor feature names
    
    The table mapping CPUID bits to human-readable strings takes up a
    non-trivial amount of space, and only exists to support /proc/cpuinfo
    and a couple of kernel messages.  Since programs depend on the format of
    /proc/cpuinfo, force inclusion of the table when building with /proc
    support; otherwise, support omitting that table to save space, in which
    case the kernel messages will print features numerically instead.
    
    In addition to saving 1408 bytes out of vmlinux, this also saves 1373
    bytes out of the uncompressed setup code, which contributes directly to
    the size of bzImage.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e4ab2b42bd6f..c649f236e288 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -346,8 +346,8 @@ static void filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 			continue;
 
 		printk(KERN_WARNING
-		       "CPU: CPU feature %s disabled, no CPUID level 0x%x\n",
-				x86_cap_flags[df->feature], df->level);
+		       "CPU: CPU feature " X86_CAP_FMT " disabled, no CPUID level 0x%x\n",
+				x86_cap_flag(df->feature), df->level);
 	}
 }
 

commit 7453f33b2e07fc2835e24cda0893de83c78d8d76
Merge: fd1cf9058028 d0f2dd186133
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 13 18:20:04 2014 -0600

    Merge branch 'x86-xsave-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/xsave changes from Peter Anvin:
     "This is a patchset to support the XSAVES instruction required to
      support context switch of supervisor-only features in upcoming
      silicon.
    
      This patchset missed the 3.16 merge window, which is why it is based
      on 3.15-rc7"
    
    * 'x86-xsave-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, xsave: Add forgotten inline annotation
      x86/xsaves: Clean up code in xstate offsets computation in xsave area
      x86/xsave: Make it clear that the XSAVE macros use (%edi)/(%rdi)
      Define kernel API to get address of each state in xsave area
      x86/xsaves: Enable xsaves/xrstors
      x86/xsaves: Call booting time xsaves and xrstors in setup_init_fpu_buf
      x86/xsaves: Save xstate to task's xsave area in __save_fpu during booting time
      x86/xsaves: Add xsaves and xrstors support for booting time
      x86/xsaves: Clear reserved bits in xsave header
      x86/xsaves: Use xsave/xrstor for saving and restoring user space context
      x86/xsaves: Use xsaves/xrstors for context switch
      x86/xsaves: Use xsaves/xrstors to save and restore xsave area
      x86/xsaves: Define a macro for handling xsave/xrstor instruction fault
      x86/xsaves: Define macros for xsave instructions
      x86/xsaves: Change compacted format xsave area header
      x86/alternative: Add alternative_input_2 to support alternative with two features and input
      x86/xsaves: Add a kernel parameter noxsaves to disable xsaves/xrstors

commit ce4747963252a30613ebf1c1df3d83b9526a342e
Merge: 76f09aa464a1 a5102476a24b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 17:15:45 2014 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "The main change in this cycle is the rework of the TLB range flushing
      code, to simplify, fix and consolidate the code.  By Dave Hansen"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Set TLB flush tunable to sane value (33)
      x86/mm: New tunable for single vs full TLB flush
      x86/mm: Add tracepoints for TLB flushes
      x86/mm: Unify remote INVLPG code
      x86/mm: Fix missed global TLB flush stat
      x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
      x86/mm: Clean up the TLB flushing code
      x86/smep: Be more informative when signalling an SMEP fault

commit e9f4e0a9fe2723078b7a1a1169828dd46a7b2f9e
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jul 31 08:40:55 2014 -0700

    x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
    
    I think the flush_tlb_mm_range() code that tries to tune the
    flush sizes based on the CPU needs to get ripped out for
    several reasons:
    
    1. It is obviously buggy.  It uses mm->total_vm to judge the
       task's footprint in the TLB.  It should certainly be using
       some measure of RSS, *NOT* ->total_vm since only resident
       memory can populate the TLB.
    2. Haswell, and several other CPUs are missing from the
       intel_tlb_flushall_shift_set() function.  Thus, it has been
       demonstrated to bitrot quickly in practice.
    3. It is plain wrong in my vm:
            [    0.037444] Last level iTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] Last level dTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] tlb_flushall_shift: 6
       Which leads to it to never use invlpg.
    4. The assumptions about TLB refill costs are wrong:
            http://lkml.kernel.org/r/1337782555-8088-3-git-send-email-alex.shi@intel.com
        (more on this in later patches)
    5. I can not reproduce the original data: https://lkml.org/lkml/2012/5/17/59
       I believe the sample times were too short.  Running the
       benchmark in a loop yields times that vary quite a bit.
    
    Note that this leaves us with a static ceiling of 1 page.  This
    is a conservative, dumb setting, and will be revised in a later
    patch.
    
    This also removes the code which attempts to predict whether we
    are flushing data or instructions.  We expect instruction flushes
    to be relatively rare and not worth tuning for explicitly.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20140731154055.ABC88E89@viggo.jf.intel.com
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2cbbf88d8f2c..2c1782085121 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -480,26 +480,17 @@ u16 __read_mostly tlb_lld_2m[NR_INFO];
 u16 __read_mostly tlb_lld_4m[NR_INFO];
 u16 __read_mostly tlb_lld_1g[NR_INFO];
 
-/*
- * tlb_flushall_shift shows the balance point in replacing cr3 write
- * with multiple 'invlpg'. It will do this replacement when
- *   flush_tlb_lines <= active_lines/2^tlb_flushall_shift.
- * If tlb_flushall_shift is -1, means the replacement will be disabled.
- */
-s8  __read_mostly tlb_flushall_shift = -1;
-
 void cpu_detect_tlb(struct cpuinfo_x86 *c)
 {
 	if (this_cpu->c_detect_tlb)
 		this_cpu->c_detect_tlb(c);
 
 	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n"
-		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n"
-		"tlb_flushall_shift: %d\n",
+		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n",
 		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
 		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
 		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES],
-		tlb_lld_1g[ENTRIES], tlb_flushall_shift);
+		tlb_lld_1g[ENTRIES]);
 }
 
 void detect_ht(struct cpuinfo_x86 *c)

commit 03ab3da3b215bac4ebb093c808d54596e03e3225
Merge: 6229ad278ca7 7171511eaec5
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Jun 18 15:26:19 2014 -0700

    Merge tag 'v3.16-rc1' into x86/cpufeature
    
    Linux 3.16-rc1
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 3737a12761636ebde0f09ef49daebb8eed18cc8a
Merge: c29deef32e36 82b897782d10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:18:49 2014 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more perf updates from Ingo Molnar:
     "A second round of perf updates:
    
       - wide reaching kprobes sanitization and robustization, with the hope
         of fixing all 'probe this function crashes the kernel' bugs, by
         Masami Hiramatsu.
    
       - uprobes updates from Oleg Nesterov: tmpfs support, corner case
         fixes and robustization work.
    
       - perf tooling updates and fixes from Jiri Olsa, Namhyung Ki, Arnaldo
         et al:
            * Add support to accumulate hist periods (Namhyung Kim)
            * various fixes, refactorings and enhancements"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      perf: Differentiate exec() and non-exec() comm events
      perf: Fix perf_event_comm() vs. exec() assumption
      uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates
      perf/documentation: Add description for conditional branch filter
      perf/x86: Add conditional branch filtering support
      perf/tool: Add conditional branch filter 'cond' to perf record
      perf: Add new conditional branch filter 'PERF_SAMPLE_BRANCH_COND'
      uprobes: Teach copy_insn() to support tmpfs
      uprobes: Shift ->readpage check from __copy_insn() to uprobe_register()
      perf/x86: Use common PMU interrupt disabled code
      perf/ARM: Use common PMU interrupt disabled code
      perf: Disable sampled events if no PMU interrupt
      perf: Fix use after free in perf_remove_from_context()
      perf tools: Fix 'make help' message error
      perf record: Fix poll return value propagation
      perf tools: Move elide bool into perf_hpp_fmt struct
      perf tools: Remove elide setup for SORT_MODE__MEMORY mode
      perf tools: Fix "==" into "=" in ui_browser__warning assignment
      perf tools: Allow overriding sysfs and proc finding with env var
      perf tools: Consider header files outside perf directory in tags target
      ...

commit b6f42a4a3c886bd18baf319d433a841ac9942c02
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu May 29 11:12:31 2014 -0700

    x86/xsaves: Add a kernel parameter noxsaves to disable xsaves/xrstors
    
    This patch adds a kernel parameter noxsaves to disable xsaves/xrstors feature.
    The kernel will fall back to use xsaveopt and xrstor to save and restor
    xstates. By using this parameter, xsave area occupies more memory because
    standard form of xsave area in xsaveopt/xrstor occupies more memory than
    compacted form of xsave area.
    
    This patch adds a description of the kernel parameter noxsaveopt in doc.
    The code to support the parameter noxsaveopt has been in the kernel before.
    This patch just adds the description of this parameter in the doc.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1401387164-43416-4-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e7c4b979d504..cdc95852532d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -146,6 +146,7 @@ static int __init x86_xsave_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
 	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
+	setup_clear_cpu_cap(X86_FEATURE_XSAVES);
 	setup_clear_cpu_cap(X86_FEATURE_AVX);
 	setup_clear_cpu_cap(X86_FEATURE_AVX2);
 	return 1;
@@ -159,6 +160,13 @@ static int __init x86_xsaveopt_setup(char *s)
 }
 __setup("noxsaveopt", x86_xsaveopt_setup);
 
+static int __init x86_xsaves_setup(char *s)
+{
+	setup_clear_cpu_cap(X86_FEATURE_XSAVES);
+	return 1;
+}
+__setup("noxsaves", x86_xsaves_setup);
+
 #ifdef CONFIG_X86_32
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;

commit 6229ad278ca74acdbc8bd3a3d469322a3de91039
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu May 29 11:12:30 2014 -0700

    x86/xsaves: Detect xsaves/xrstors feature
    
    Detect the xsaveopt, xsavec, xgetbv, and xsaves features in processor extended
    state enumberation sub-leaf (eax=0x0d, ecx=1):
    Bit 00: XSAVEOPT is available
    Bit 01: Supports XSAVEC and the compacted form of XRSTOR if set
    Bit 02: Supports XGETBV with ECX = 1 if set
    Bit 03: Supports XSAVES/XRSTORS and IA32_XSS if set
    
    The above features are defined in the new word 10 in cpu features.
    
    The IA32_XSS MSR (index DA0H) contains a state-component bitmap that specifies
    the state components that software has enabled xsaves and xrstors to manage.
    If the bit corresponding to a state component is clear in XCR0 | IA32_XSS,
    xsaves and xrstors will not operate on that state component, regardless of
    the value of the instruction mask.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1401387164-43416-3-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a135239badb7..e7c4b979d504 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -632,6 +632,15 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[9] = ebx;
 	}
 
+	/* Extended state features: level 0x0000000d */
+	if (c->cpuid_level >= 0x0000000d) {
+		u32 eax, ebx, ecx, edx;
+
+		cpuid_count(0x0000000d, 1, &eax, &ebx, &ecx, &edx);
+
+		c->x86_capability[10] = eax;
+	}
+
 	/* AMD-defined flags: level 0x80000001 */
 	xlvl = cpuid_eax(0x80000000);
 	c->extended_cpuid_level = xlvl;

commit f40c330091c7aa9956ab66f97a3abc8a68b67240
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon May 5 12:19:36 2014 -0700

    x86, vdso: Move the vvar and hpet mappings next to the 64-bit vDSO
    
    This makes the 64-bit and x32 vdsos use the same mechanism as the
    32-bit vdso.  Most of the churn is deleting all the old fixmap code.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/8af87023f57f6bb96ec8d17fce3f88018195b49b.1399317206.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7c65b4666c24..2cbbf88d8f2c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -20,6 +20,7 @@
 #include <asm/processor.h>
 #include <asm/debugreg.h>
 #include <asm/sections.h>
+#include <asm/vsyscall.h>
 #include <linux/topology.h>
 #include <linux/cpumask.h>
 #include <asm/pgtable.h>

commit cfda7bb9ecbf9d96264bb5bade33a842966d1062
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Mon May 5 12:19:33 2014 -0700

    x86, vdso: Move syscall and sysenter setup into kernel/cpu/common.c
    
    This code is used during CPU setup, and it isn't strictly speaking
    related to the 32-bit vdso.  It's easier to understand how this
    works when the code is closer to its callers.
    
    This also lets syscall32_cpu_init be static, which might save some
    trivial amount of kernel text.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/r/4e466987204e232d7b55a53ff6b9739f12237461.1399317206.git.luto@amacapital.net
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a135239badb7..7c65b4666c24 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -953,6 +953,38 @@ static void vgetcpu_set_mode(void)
 	else
 		vgetcpu_mode = VGETCPU_LSL;
 }
+
+/* May not be __init: called during resume */
+static void syscall32_cpu_init(void)
+{
+	/* Load these always in case some future AMD CPU supports
+	   SYSENTER from compat mode too. */
+	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);
+	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)ia32_sysenter_target);
+
+	wrmsrl(MSR_CSTAR, ia32_cstar_target);
+}
+#endif
+
+#ifdef CONFIG_X86_32
+void enable_sep_cpu(void)
+{
+	int cpu = get_cpu();
+	struct tss_struct *tss = &per_cpu(init_tss, cpu);
+
+	if (!boot_cpu_has(X86_FEATURE_SEP)) {
+		put_cpu();
+		return;
+	}
+
+	tss->x86_tss.ss1 = __KERNEL_CS;
+	tss->x86_tss.sp1 = sizeof(struct tss_struct) + (unsigned long) tss;
+	wrmsr(MSR_IA32_SYSENTER_CS, __KERNEL_CS, 0);
+	wrmsr(MSR_IA32_SYSENTER_ESP, tss->x86_tss.sp1, 0);
+	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long) ia32_sysenter_target, 0);
+	put_cpu();
+}
 #endif
 
 void __init identify_boot_cpu(void)

commit 0f46efeb44e360b78f54a968b4d92e6877c35891
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:17:12 2014 +0900

    kprobes, x86: Prohibit probing on debug_stack_*()
    
    Prohibit probing on debug_stack_reset and debug_stack_set_zero.
    Since the both functions are called from TRACE_IRQS_ON/OFF_DEBUG
    macros which run in int3 ist entry, probing it may cause a soft
    lockup.
    
    This happens when the kernel built with CONFIG_DYNAMIC_FTRACE=y
    and CONFIG_TRACE_IRQFLAGS=y.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Jan Beulich <JBeulich@suse.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/20140417081712.26341.32994.stgit@ltc230.yrl.intra.hitachi.co.jp
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a135239badb7..5af696dddd1d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -8,6 +8,7 @@
 #include <linux/delay.h>
 #include <linux/sched.h>
 #include <linux/init.h>
+#include <linux/kprobes.h>
 #include <linux/kgdb.h>
 #include <linux/smp.h>
 #include <linux/io.h>
@@ -1160,6 +1161,7 @@ int is_debug_stack(unsigned long addr)
 		(addr <= __get_cpu_var(debug_stack_addr) &&
 		 addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ));
 }
+NOKPROBE_SYMBOL(is_debug_stack);
 
 DEFINE_PER_CPU(u32, debug_idt_ctr);
 
@@ -1168,6 +1170,7 @@ void debug_stack_set_zero(void)
 	this_cpu_inc(debug_idt_ctr);
 	load_current_idt();
 }
+NOKPROBE_SYMBOL(debug_stack_set_zero);
 
 void debug_stack_reset(void)
 {
@@ -1176,6 +1179,7 @@ void debug_stack_reset(void)
 	if (this_cpu_dec_return(debug_idt_ctr) == 0)
 		load_current_idt();
 }
+NOKPROBE_SYMBOL(debug_stack_reset);
 
 #else	/* CONFIG_X86_64 */
 

commit 99f7b025bfadd7fac5216dcfb2a08312804674c0
Merge: a21e40877ad1 6cce16f99d7b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 10:17:18 2014 -0700

    Merge branch 'x86-threadinfo-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 threadinfo changes from Ingo Molnar:
     "The main change here is the consolidation/unification of 32 and 64 bit
      thread_info handling methods, from Steve Rostedt"
    
    * 'x86-threadinfo-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, threadinfo: Redo "x86: Use inline assembler to get sp"
      x86: Clean up dumpstack_64.c code
      x86: Keep thread_info on thread stack in x86_32
      x86: Prepare removal of previous_esp from i386 thread_info structure
      x86: Nuke GET_THREAD_INFO_WITH_ESP() macro for i386
      x86: Nuke the supervisor_stack field in i386 thread_info

commit 198d208df4371734ac4728f69cb585c284d20a15
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 6 09:41:31 2014 -0500

    x86: Keep thread_info on thread stack in x86_32
    
    x86_64 uses a per_cpu variable kernel_stack to always point to
    the thread stack of current. This is where the thread_info is stored
    and is accessed from this location even when the irq or exception stack
    is in use. This removes the complexity of having to maintain the
    thread info on the stack when interrupts are running and having to
    copy the preempt_count and other fields to the interrupt stack.
    
    x86_32 uses the old method of copying the thread_info from the thread
    stack to the exception stack just before executing the exception.
    
    Having the two different requires #ifdefs and also the x86_32 way
    is a bit of a pain to maintain. By converting x86_32 to the same
    method of x86_64, we can remove #ifdefs, clean up the x86_32 code
    a little, and remove the overhead of the copy.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20110806012354.263834829@goodmis.org
    Link: http://lkml.kernel.org/r/20140206144321.852942014@goodmis.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8e28bf2fc3ef..29c1944a98ac 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1078,6 +1078,10 @@ static __init int setup_disablecpuid(char *arg)
 }
 __setup("clearcpuid=", setup_disablecpuid);
 
+DEFINE_PER_CPU(unsigned long, kernel_stack) =
+	(unsigned long)&init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;
+EXPORT_PER_CPU_SYMBOL(kernel_stack);
+
 #ifdef CONFIG_X86_64
 struct desc_ptr idt_descr = { NR_VECTORS * 16 - 1, (unsigned long) idt_table };
 struct desc_ptr debug_idt_descr = { NR_VECTORS * 16 - 1,
@@ -1094,10 +1098,6 @@ DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =
 	&init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
-DEFINE_PER_CPU(unsigned long, kernel_stack) =
-	(unsigned long)&init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;
-EXPORT_PER_CPU_SYMBOL(kernel_stack);
-
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
 	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
 

commit da4aaa7d860c63a1bfd3c73cf8309afe2840c5b9
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Feb 27 08:36:31 2014 -0800

    x86, cpufeature: If we disable CLFLUSH, we should disable CLFLUSHOPT
    
    If we explicitly disable the use of CLFLUSH, we should disable the use
    of CLFLUSHOPT as well.
    
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/n/tip-jtdv7btppr4jgzxm3sxx1e74@git.kernel.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2c6ac6f2b5b1..cca53d88762a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1026,6 +1026,7 @@ __setup("show_msr=", setup_show_msr);
 static __init int setup_noclflush(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_CLFLUSH);
+	setup_clear_cpu_cap(X86_FEATURE_CLFLUSHOPT);
 	return 1;
 }
 __setup("noclflush", setup_noclflush);

commit 840d2830e6e56b8fdacc7ff12915dd91bf91566b
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Feb 27 08:31:30 2014 -0800

    x86, cpufeature: Rename X86_FEATURE_CLFLSH to X86_FEATURE_CLFLUSH
    
    We call this "clflush" in /proc/cpuinfo, and have
    cpu_has_clflush()... let's be consistent and just call it that.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Link: http://lkml.kernel.org/n/tip-mlytfzjkvuf739okyn40p8a5@git.kernel.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8e28bf2fc3ef..2c6ac6f2b5b1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1025,7 +1025,7 @@ __setup("show_msr=", setup_show_msr);
 
 static __init int setup_noclflush(char *arg)
 {
-	setup_clear_cpu_cap(X86_FEATURE_CLFLSH);
+	setup_clear_cpu_cap(X86_FEATURE_CLFLUSH);
 	return 1;
 }
 __setup("noclflush", setup_noclflush);

commit 03bbd596ac04fef47ce93a730b8f086d797c3021
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Feb 13 07:34:30 2014 -0800

    x86, smap: Don't enable SMAP if CONFIG_X86_SMAP is disabled
    
    If SMAP support is not compiled into the kernel, don't enable SMAP in
    CR4 -- in fact, we should clear it, because the kernel doesn't contain
    the proper STAC/CLAC instructions for SMAP support.
    
    Found by Fengguang Wu's test system.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Link: http://lkml.kernel.org/r/20140213124550.GA30497@localhost
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: <stable@vger.kernel.org> # v3.7+

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 24b6fd10625a..8e28bf2fc3ef 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -284,8 +284,13 @@ static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 	raw_local_save_flags(eflags);
 	BUG_ON(eflags & X86_EFLAGS_AC);
 
-	if (cpu_has(c, X86_FEATURE_SMAP))
+	if (cpu_has(c, X86_FEATURE_SMAP)) {
+#ifdef CONFIG_X86_SMAP
 		set_in_cr4(X86_CR4_SMAP);
+#else
+		clear_in_cr4(X86_CR4_SMAP);
+#endif
+	}
 }
 
 /*

commit dd360393f4d948eb518372316e52101cf3b44212
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Dec 23 14:16:58 2013 +0200

    x86, cpu: Detect more TLB configuration
    
    The Intel Software Developers Manual covers few more TLB
    configurations exposed as CPUID 2 descriptors:
    
    61H Instruction TLB: 4 KByte pages, fully associative, 48 entries
    63H Data TLB: 1 GByte pages, 4-way set associative, 4 entries
    76H Instruction TLB: 2M/4M pages, fully associative, 8 entries
    B5H Instruction TLB: 4KByte pages, 8-way set associative, 64 entries
    B6H Instruction TLB: 4KByte pages, 8-way set associative, 128 entries
    C1H Shared 2nd-Level TLB: 4 KByte/2MByte pages, 8-way associative, 1024 entries
    C2H DTLB DTLB: 2 MByte/$MByte pages, 4-way associative, 16 entries
    
    Let's detect them as well.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/1387801018-14499-1-git-send-email-kirill.shutemov@linux.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6abc172b8258..24b6fd10625a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -472,6 +472,7 @@ u16 __read_mostly tlb_lli_4m[NR_INFO];
 u16 __read_mostly tlb_lld_4k[NR_INFO];
 u16 __read_mostly tlb_lld_2m[NR_INFO];
 u16 __read_mostly tlb_lld_4m[NR_INFO];
+u16 __read_mostly tlb_lld_1g[NR_INFO];
 
 /*
  * tlb_flushall_shift shows the balance point in replacing cr3 write
@@ -486,13 +487,13 @@ void cpu_detect_tlb(struct cpuinfo_x86 *c)
 	if (this_cpu->c_detect_tlb)
 		this_cpu->c_detect_tlb(c);
 
-	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n" \
-		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d\n"	     \
+	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n"
+		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n"
 		"tlb_flushall_shift: %d\n",
 		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
 		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
 		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES],
-		tlb_flushall_shift);
+		tlb_lld_1g[ENTRIES], tlb_flushall_shift);
 }
 
 void detect_ht(struct cpuinfo_x86 *c)

commit 6df1e7f2e96721dfdbfd8a034e52bc81916f978c
Merge: d96d8aa26170 b53b5eda8194
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 12 10:46:43 2013 +0900

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu changes from Ingo Molnar:
     "The biggest change that stands out is the increase of the
      CONFIG_NR_CPUS range from 4096 to 8192 - as real hardware out there
      already went beyond 4k CPUs ...
    
      We only allow more than 512 CPUs if offstack cpumasks are enabled.
    
      CONFIG_MAXSMP=y remains to be the 'you are nuts!' extreme testcase,
      which now means a max of 8192 CPUs"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpu: Increase max CPU count to 8192
      x86/cpu: Allow higher NR_CPUS values
      x86/cpu: Always print SMP information in /proc/cpuinfo
      x86/cpu: Track legacy CPU model data only on 32-bit kernels

commit 09dc68d958c67c76cf672ec78b7391af453010f8
Author: Jan Beulich <JBeulich@suse.com>
Date:   Mon Oct 21 09:35:20 2013 +0100

    x86/cpu: Track legacy CPU model data only on 32-bit kernels
    
    struct cpu_dev's c_models is only ever set inside CONFIG_X86_32
    conditionals (or code that's being built for 32-bit only), so
    there's no use of reserving the (empty) space for the model
    names in a 64-bit kernel.
    
    Similarly, c_size_cache is only used in the #else of a
    CONFIG_X86_64 conditional, so reserving space for (and in one
    case even initializing) that field is pointless for 64-bit
    kernels too.
    
    While moving both fields to the end of the structure, I also
    noticed that:
    
     - the c_models array size was one too small, potentially causing
       table_lookup_model() to return garbage on Intel CPUs (intel.c's
       instance was lacking the sentinel with family being zero), so the
       patch bumps that by one,
    
     - c_models' vendor sub-field was unused (and anyway redundant
       with the base structure's c_x86_vendor field), so the patch deletes it.
    
    Also rename the legacy fields so that their legacy nature stands out
    and comment their declarations.
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Link: http://lkml.kernel.org/r/5265036802000078000FC4DB@nat28.tlf.novell.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2793d1f095a2..9ada0b37ae07 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -346,7 +346,8 @@ static void filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 /* Look up CPU names by table lookup. */
 static const char *table_lookup_model(struct cpuinfo_x86 *c)
 {
-	const struct cpu_model_info *info;
+#ifdef CONFIG_X86_32
+	const struct legacy_cpu_model_info *info;
 
 	if (c->x86_model >= 16)
 		return NULL;	/* Range check */
@@ -354,13 +355,14 @@ static const char *table_lookup_model(struct cpuinfo_x86 *c)
 	if (!this_cpu)
 		return NULL;
 
-	info = this_cpu->c_models;
+	info = this_cpu->legacy_models;
 
-	while (info && info->family) {
+	while (info->family) {
 		if (info->family == c->x86)
 			return info->model_names[c->x86_model];
 		info++;
 	}
+#endif
 	return NULL;		/* Not found */
 }
 
@@ -450,8 +452,8 @@ void cpu_detect_cache_sizes(struct cpuinfo_x86 *c)
 	c->x86_tlbsize += ((ebx >> 16) & 0xfff) + (ebx & 0xfff);
 #else
 	/* do processor-specific cache resizing */
-	if (this_cpu->c_size_cache)
-		l2size = this_cpu->c_size_cache(c, l2size);
+	if (this_cpu->legacy_cache_size)
+		l2size = this_cpu->legacy_cache_size(c, l2size);
 
 	/* Allow user to override all this if necessary. */
 	if (cachesize_override != -1)

commit c2daa3bed53a81171cf8c1a36db798e82b91afe8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 14 14:51:00 2013 +0200

    sched, x86: Provide a per-cpu preempt_count implementation
    
    Convert x86 to use a per-cpu preemption count. The reason for doing so
    is that accessing per-cpu variables is a lot cheaper than accessing
    thread_info variables.
    
    We still need to save/restore the actual preemption count due to
    PREEMPT_ACTIVE so we place the per-cpu __preempt_count variable in the
    same cache-line as the other hot __switch_to() variables such as
    current_task.
    
    NOTE: this save/restore is required even for !PREEMPT kernels as
    cond_resched() also relies on preempt_count's PREEMPT_ACTIVE to ignore
    task_struct::state.
    
    Also rename thread_info::preempt_count to ensure nobody is
    'accidentally' still poking at it.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-gzn5rfsf8trgjoqx8hyayy3q@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2793d1f095a2..5223fe6dec7b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1095,6 +1095,9 @@ DEFINE_PER_CPU(char *, irq_stack_ptr) =
 
 DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 
+DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
+EXPORT_PER_CPU_SYMBOL(__preempt_count);
+
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 
 /*
@@ -1169,6 +1172,8 @@ void debug_stack_reset(void)
 
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
+DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
+EXPORT_PER_CPU_SYMBOL(__preempt_count);
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 
 #ifdef CONFIG_CC_STACKPROTECTOR

commit 277d5b40b7bf495d2d4193746181b17dd98441b2
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:43 2013 -0700

    x86, asmlinkage: Make several variables used from assembler/linker script visible
    
    Plus one function, load_gs_index().
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-10-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 25eb2747b063..2793d1f095a2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1076,7 +1076,7 @@ struct desc_ptr debug_idt_descr = { NR_VECTORS * 16 - 1,
 				    (unsigned long) debug_idt_table };
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
-		     irq_stack_union) __aligned(PAGE_SIZE);
+		     irq_stack_union) __aligned(PAGE_SIZE) __visible;
 
 /*
  * The following four percpu variables are hot.  Align current_task to
@@ -1093,7 +1093,7 @@ EXPORT_PER_CPU_SYMBOL(kernel_stack);
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
 	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
 
-DEFINE_PER_CPU(unsigned int, irq_count) = -1;
+DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 548bd039784e..25eb2747b063 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -63,7 +63,7 @@ void __init setup_cpu_local_masks(void)
 	alloc_bootmem_cpumask_var(&cpu_sibling_setup_mask);
 }
 
-static void __cpuinit default_init(struct cpuinfo_x86 *c)
+static void default_init(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_64
 	cpu_detect_cache_sizes(c);
@@ -80,13 +80,13 @@ static void __cpuinit default_init(struct cpuinfo_x86 *c)
 #endif
 }
 
-static const struct cpu_dev __cpuinitconst default_cpu = {
+static const struct cpu_dev default_cpu = {
 	.c_init		= default_init,
 	.c_vendor	= "Unknown",
 	.c_x86_vendor	= X86_VENDOR_UNKNOWN,
 };
 
-static const struct cpu_dev *this_cpu __cpuinitdata = &default_cpu;
+static const struct cpu_dev *this_cpu = &default_cpu;
 
 DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 #ifdef CONFIG_X86_64
@@ -160,8 +160,8 @@ static int __init x86_xsaveopt_setup(char *s)
 __setup("noxsaveopt", x86_xsaveopt_setup);
 
 #ifdef CONFIG_X86_32
-static int cachesize_override __cpuinitdata = -1;
-static int disable_x86_serial_nr __cpuinitdata = 1;
+static int cachesize_override = -1;
+static int disable_x86_serial_nr = 1;
 
 static int __init cachesize_setup(char *str)
 {
@@ -215,12 +215,12 @@ static inline int flag_is_changeable_p(u32 flag)
 }
 
 /* Probe for the CPUID instruction */
-int __cpuinit have_cpuid_p(void)
+int have_cpuid_p(void)
 {
 	return flag_is_changeable_p(X86_EFLAGS_ID);
 }
 
-static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
+static void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 {
 	unsigned long lo, hi;
 
@@ -298,7 +298,7 @@ struct cpuid_dependent_feature {
 	u32 level;
 };
 
-static const struct cpuid_dependent_feature __cpuinitconst
+static const struct cpuid_dependent_feature
 cpuid_dependent_features[] = {
 	{ X86_FEATURE_MWAIT,		0x00000005 },
 	{ X86_FEATURE_DCA,		0x00000009 },
@@ -306,7 +306,7 @@ cpuid_dependent_features[] = {
 	{ 0, 0 }
 };
 
-static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
+static void filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 {
 	const struct cpuid_dependent_feature *df;
 
@@ -344,7 +344,7 @@ static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
  */
 
 /* Look up CPU names by table lookup. */
-static const char *__cpuinit table_lookup_model(struct cpuinfo_x86 *c)
+static const char *table_lookup_model(struct cpuinfo_x86 *c)
 {
 	const struct cpu_model_info *info;
 
@@ -364,8 +364,8 @@ static const char *__cpuinit table_lookup_model(struct cpuinfo_x86 *c)
 	return NULL;		/* Not found */
 }
 
-__u32 cpu_caps_cleared[NCAPINTS] __cpuinitdata;
-__u32 cpu_caps_set[NCAPINTS] __cpuinitdata;
+__u32 cpu_caps_cleared[NCAPINTS];
+__u32 cpu_caps_set[NCAPINTS];
 
 void load_percpu_segment(int cpu)
 {
@@ -394,9 +394,9 @@ void switch_to_new_gdt(int cpu)
 	load_percpu_segment(cpu);
 }
 
-static const struct cpu_dev *__cpuinitdata cpu_devs[X86_VENDOR_NUM] = {};
+static const struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 
-static void __cpuinit get_model_name(struct cpuinfo_x86 *c)
+static void get_model_name(struct cpuinfo_x86 *c)
 {
 	unsigned int *v;
 	char *p, *q;
@@ -425,7 +425,7 @@ static void __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	}
 }
 
-void __cpuinit cpu_detect_cache_sizes(struct cpuinfo_x86 *c)
+void cpu_detect_cache_sizes(struct cpuinfo_x86 *c)
 {
 	unsigned int n, dummy, ebx, ecx, edx, l2size;
 
@@ -479,7 +479,7 @@ u16 __read_mostly tlb_lld_4m[NR_INFO];
  */
 s8  __read_mostly tlb_flushall_shift = -1;
 
-void __cpuinit cpu_detect_tlb(struct cpuinfo_x86 *c)
+void cpu_detect_tlb(struct cpuinfo_x86 *c)
 {
 	if (this_cpu->c_detect_tlb)
 		this_cpu->c_detect_tlb(c);
@@ -493,7 +493,7 @@ void __cpuinit cpu_detect_tlb(struct cpuinfo_x86 *c)
 		tlb_flushall_shift);
 }
 
-void __cpuinit detect_ht(struct cpuinfo_x86 *c)
+void detect_ht(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_HT
 	u32 eax, ebx, ecx, edx;
@@ -544,7 +544,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
+static void get_cpu_vendor(struct cpuinfo_x86 *c)
 {
 	char *v = c->x86_vendor_id;
 	int i;
@@ -571,7 +571,7 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 	this_cpu = &default_cpu;
 }
 
-void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
+void cpu_detect(struct cpuinfo_x86 *c)
 {
 	/* Get vendor name */
 	cpuid(0x00000000, (unsigned int *)&c->cpuid_level,
@@ -601,7 +601,7 @@ void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
 	}
 }
 
-void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
+void get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
 	u32 ebx;
@@ -652,7 +652,7 @@ void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 	init_scattered_cpuid_features(c);
 }
 
-static void __cpuinit identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
+static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32
 	int i;
@@ -769,7 +769,7 @@ void __init early_cpu_init(void)
  * unless we can find a reliable way to detect all the broken cases.
  * Enable it explicitly on 64-bit for non-constant inputs of cpu_has().
  */
-static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
+static void detect_nopl(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32
 	clear_cpu_cap(c, X86_FEATURE_NOPL);
@@ -778,7 +778,7 @@ static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
+static void generic_identify(struct cpuinfo_x86 *c)
 {
 	c->extended_cpuid_level = 0;
 
@@ -815,7 +815,7 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 /*
  * This does the hard work of actually picking apart the CPU stuff...
  */
-static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
+static void identify_cpu(struct cpuinfo_x86 *c)
 {
 	int i;
 
@@ -960,7 +960,7 @@ void __init identify_boot_cpu(void)
 	cpu_detect_tlb(&boot_cpu_data);
 }
 
-void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)
+void identify_secondary_cpu(struct cpuinfo_x86 *c)
 {
 	BUG_ON(c == &boot_cpu_data);
 	identify_cpu(c);
@@ -975,14 +975,14 @@ struct msr_range {
 	unsigned	max;
 };
 
-static const struct msr_range msr_range_array[] __cpuinitconst = {
+static const struct msr_range msr_range_array[] = {
 	{ 0x00000000, 0x00000418},
 	{ 0xc0000000, 0xc000040b},
 	{ 0xc0010000, 0xc0010142},
 	{ 0xc0011000, 0xc001103b},
 };
 
-static void __cpuinit __print_cpu_msr(void)
+static void __print_cpu_msr(void)
 {
 	unsigned index_min, index_max;
 	unsigned index;
@@ -1001,7 +1001,7 @@ static void __cpuinit __print_cpu_msr(void)
 	}
 }
 
-static int show_msr __cpuinitdata;
+static int show_msr;
 
 static __init int setup_show_msr(char *arg)
 {
@@ -1022,7 +1022,7 @@ static __init int setup_noclflush(char *arg)
 }
 __setup("noclflush", setup_noclflush);
 
-void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
+void print_cpu_info(struct cpuinfo_x86 *c)
 {
 	const char *vendor = NULL;
 
@@ -1051,7 +1051,7 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 	print_cpu_msr(c);
 }
 
-void __cpuinit print_cpu_msr(struct cpuinfo_x86 *c)
+void print_cpu_msr(struct cpuinfo_x86 *c)
 {
 	if (c->cpu_index < show_msr)
 		__print_cpu_msr();
@@ -1216,7 +1216,7 @@ static void dbg_restore_debug_regs(void)
  */
 #ifdef CONFIG_X86_64
 
-void __cpuinit cpu_init(void)
+void cpu_init(void)
 {
 	struct orig_ist *oist;
 	struct task_struct *me;
@@ -1315,7 +1315,7 @@ void __cpuinit cpu_init(void)
 
 #else
 
-void __cpuinit cpu_init(void)
+void cpu_init(void)
 {
 	int cpu = smp_processor_id();
 	struct task_struct *curr = current;

commit 96a3d998fb92c28b9862297fcf93a24d8a0eac1d
Merge: 3045f94a20cc 5236eb968ec2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 16:31:49 2013 -0700

    Merge branch 'x86-tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 tracing updates from Ingo Molnar:
     "This tree adds IRQ vector tracepoints that are named after the handler
      and which output the vector #, based on a zero-overhead approach that
      relies on changing the IDT entries, by Seiji Aguchi.
    
      The new tracepoints look like this:
    
       # perf list | grep -i irq_vector
        irq_vectors:local_timer_entry                      [Tracepoint event]
        irq_vectors:local_timer_exit                       [Tracepoint event]
        irq_vectors:reschedule_entry                       [Tracepoint event]
        irq_vectors:reschedule_exit                        [Tracepoint event]
        irq_vectors:spurious_apic_entry                    [Tracepoint event]
        irq_vectors:spurious_apic_exit                     [Tracepoint event]
        irq_vectors:error_apic_entry                       [Tracepoint event]
        irq_vectors:error_apic_exit                        [Tracepoint event]
       [...]"
    
    * 'x86-tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tracing: Add config option checking to the definitions of mce handlers
      trace,x86: Do not call local_irq_save() in load_current_idt()
      trace,x86: Move creation of irq tracepoints from apic.c to irq.c
      x86, trace: Add irq vector tracepoints
      x86: Rename variables for debugging
      x86, trace: Introduce entering/exiting_irq()
      tracing: Add DEFINE_EVENT_FN() macro

commit cf910e83ae23692fdeefc7e506e504c4c468d38a
Author: Seiji Aguchi <seiji.aguchi@hds.com>
Date:   Thu Jun 20 11:46:53 2013 -0400

    x86, trace: Add irq vector tracepoints
    
    [Purpose of this patch]
    
    As Vaibhav explained in the thread below, tracepoints for irq vectors
    are useful.
    
    http://www.spinics.net/lists/mm-commits/msg85707.html
    
    <snip>
    The current interrupt traces from irq_handler_entry and irq_handler_exit
    provide when an interrupt is handled.  They provide good data about when
    the system has switched to kernel space and how it affects the currently
    running processes.
    
    There are some IRQ vectors which trigger the system into kernel space,
    which are not handled in generic IRQ handlers.  Tracing such events gives
    us the information about IRQ interaction with other system events.
    
    The trace also tells where the system is spending its time.  We want to
    know which cores are handling interrupts and how they are affecting other
    processes in the system.  Also, the trace provides information about when
    the cores are idle and which interrupts are changing that state.
    <snip>
    
    On the other hand, my usecase is tracing just local timer event and
    getting a value of instruction pointer.
    
    I suggested to add an argument local timer event to get instruction pointer before.
    But there is another way to get it with external module like systemtap.
    So, I don't need to add any argument to irq vector tracepoints now.
    
    [Patch Description]
    
    Vaibhav's patch shared a trace point ,irq_vector_entry/irq_vector_exit, in all events.
    But there is an above use case to trace specific irq_vector rather than tracing all events.
    In this case, we are concerned about overhead due to unwanted events.
    
    So, add following tracepoints instead of introducing irq_vector_entry/exit.
    so that we can enable them independently.
       - local_timer_vector
       - reschedule_vector
       - call_function_vector
       - call_function_single_vector
       - irq_work_entry_vector
       - error_apic_vector
       - thermal_apic_vector
       - threshold_apic_vector
       - spurious_apic_vector
       - x86_platform_ipi_vector
    
    Also, introduce a logic switching IDT at enabling/disabling time so that a time penalty
    makes a zero when tracepoints are disabled. Detailed explanations are as follows.
     - Create trace irq handlers with entering_irq()/exiting_irq().
     - Create a new IDT, trace_idt_table, at boot time by adding a logic to
       _set_gate(). It is just a copy of original idt table.
     - Register the new handlers for tracpoints to the new IDT by introducing
       macros to alloc_intr_gate() called at registering time of irq_vector handlers.
     - Add checking, whether irq vector tracing is on/off, into load_current_idt().
       This has to be done below debug checking for these reasons.
       - Switching to debug IDT may be kicked while tracing is enabled.
       - On the other hands, switching to trace IDT is kicked only when debugging
         is disabled.
    
    In addition, the new IDT is created only when CONFIG_TRACING is enabled to avoid being
    used for other purposes.
    
    Signed-off-by: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/51C323ED.5050708@hds.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8f6a0f909d6f..d9c8c0947ec2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1257,7 +1257,7 @@ void __cpuinit cpu_init(void)
 	switch_to_new_gdt(cpu);
 	loadsegment(fs, 0);
 
-	load_idt((const struct desc_ptr *)&idt_descr);
+	load_current_idt();
 
 	memset(me->thread.tls_array, 0, GDT_ENTRY_TLS_ENTRIES * 8);
 	syscall_init();
@@ -1334,7 +1334,7 @@ void __cpuinit cpu_init(void)
 	if (cpu_has_vme || cpu_has_tsc || cpu_has_de)
 		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
-	load_idt(&idt_descr);
+	load_current_idt();
 	switch_to_new_gdt(cpu);
 
 	/*

commit 629f4f9d59a27d8e58aa612e886e6a9a63ea7aeb
Author: Seiji Aguchi <seiji.aguchi@hds.com>
Date:   Thu Jun 20 11:45:44 2013 -0400

    x86: Rename variables for debugging
    
    Rename variables for debugging to describe meaning of them precisely.
    
    Also, introduce a generic way to switch IDT by checking a current state,
    debug on/off.
    
    Signed-off-by: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/51C323A8.7050905@hds.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 22018f70a671..8f6a0f909d6f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1071,8 +1071,8 @@ __setup("clearcpuid=", setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
 struct desc_ptr idt_descr = { NR_VECTORS * 16 - 1, (unsigned long) idt_table };
-struct desc_ptr nmi_idt_descr = { NR_VECTORS * 16 - 1,
-				    (unsigned long) nmi_idt_table };
+struct desc_ptr debug_idt_descr = { NR_VECTORS * 16 - 1,
+				    (unsigned long) debug_idt_table };
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE);
@@ -1148,20 +1148,20 @@ int is_debug_stack(unsigned long addr)
 		 addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ));
 }
 
-static DEFINE_PER_CPU(u32, debug_stack_use_ctr);
+DEFINE_PER_CPU(u32, debug_idt_ctr);
 
 void debug_stack_set_zero(void)
 {
-	this_cpu_inc(debug_stack_use_ctr);
-	load_idt((const struct desc_ptr *)&nmi_idt_descr);
+	this_cpu_inc(debug_idt_ctr);
+	load_current_idt();
 }
 
 void debug_stack_reset(void)
 {
-	if (WARN_ON(!this_cpu_read(debug_stack_use_ctr)))
+	if (WARN_ON(!this_cpu_read(debug_idt_ctr)))
 		return;
-	if (this_cpu_dec_return(debug_stack_use_ctr) == 0)
-		load_idt((const struct desc_ptr *)&idt_descr);
+	if (this_cpu_dec_return(debug_idt_ctr) == 0)
+		load_current_idt();
 }
 
 #else	/* CONFIG_X86_64 */

commit 4a90a99c4f8002edaa6be11bd756872ebf3f3d97
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Jun 9 12:07:33 2013 +0200

    x86: Add a static_cpu_has_safe variant
    
    We want to use this in early code where alternatives might not have run
    yet and for that case we fall back to the dynamic boot_cpu_has.
    
    For that, force a 5-byte jump since the compiler could be generating
    differently sized jumps for each label.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1370772454-6106-5-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 59adaa17e220..a4a07c0acb1f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1372,3 +1372,9 @@ void warn_pre_alternatives(void)
 }
 EXPORT_SYMBOL_GPL(warn_pre_alternatives);
 #endif
+
+inline bool __static_cpu_has_safe(u16 bit)
+{
+	return boot_cpu_has(bit);
+}
+EXPORT_SYMBOL_GPL(__static_cpu_has_safe);

commit 5700f743b597951743da9c7d891d3989aac0486e
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Jun 9 12:07:32 2013 +0200

    x86: Sanity-check static_cpu_has usage
    
    static_cpu_has may be used only after alternatives have run. Before that
    it always returns false if constant folding with __builtin_constant_p()
    doesn't happen. And you don't want that.
    
    This patch is the result of me debugging an issue where I overzealously
    put static_cpu_has in code which executed before alternatives have run
    and had to spend some time with scratching head and cursing at the
    monitor.
    
    So add a jump to a warning which screams loudly when we use this
    function too early. The alternatives patch that check away in
    conjunction with patching the rest of the kernel image.
    
    [ hpa: factored this into its own configuration option.  If we want to
      have an overarching option, it should be an option which selects
      other options, not as a group option in the source code. ]
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1370772454-6106-4-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d388ce1f3e1b..59adaa17e220 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1364,3 +1364,11 @@ void __cpuinit cpu_init(void)
 	fpu_init();
 }
 #endif
+
+#ifdef CONFIG_X86_DEBUG_STATIC_CPU_HAS
+void warn_pre_alternatives(void)
+{
+	WARN(1, "You're using static_cpu_has before alternatives have run!\n");
+}
+EXPORT_SYMBOL_GPL(warn_pre_alternatives);
+#endif

commit c3b83598c1eeb1507603b461f5843ec2a49e3033
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Jun 9 12:07:30 2013 +0200

    x86, cpu: Add a synthetic, always true, cpu feature
    
    This will be used in alternatives later as an always-replace flag.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1370772454-6106-2-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d4dd99350e9d..d388ce1f3e1b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -723,6 +723,8 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	if (this_cpu->c_bsp_init)
 		this_cpu->c_bsp_init(c);
+
+	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 }
 
 void __init early_cpu_init(void)

commit 60e019eb37a8d989031ad47ae9810453536f3127
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Apr 29 16:04:20 2013 +0200

    x86: Get rid of ->hard_math and all the FPU asm fu
    
    Reimplement FPU detection code in C and drop old, not-so-recommended
    detection method in asm. Move all the relevant stuff into i387.c where
    it conceptually belongs. Finally drop cpuinfo_x86.hard_math.
    
    [ hpa: huge thanks to Borislav for taking my original concept patch
      and productizing it ]
    
    [ Boris, note to self: do not use static_cpu_has before alternatives! ]
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1367244262-29511-2-git-send-email-bp@alien8.de
    Link: http://lkml.kernel.org/r/1365436666-9837-2-git-send-email-bp@alien8.de
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 22018f70a671..d4dd99350e9d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -711,10 +711,9 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		return;
 
 	cpu_detect(c);
-
 	get_cpu_vendor(c);
-
 	get_cpu_cap(c);
+	fpu_detect(c);
 
 	if (this_cpu->c_early_init)
 		this_cpu->c_early_init(c);

commit 65fc985b37dc241c4db7cd32adcbc989193fe3c8
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:23 2013 +0100

    x86, cpu: Expand cpufeature facility to include cpu bugs
    
    We add another 32-bit vector at the end of the ->x86_capability
    bitvector which collects bugs present in CPUs. After all, a CPU bug is a
    kind of a capability, albeit a strange one.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-2-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d814772c5bed..22018f70a671 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -920,6 +920,10 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 		/* AND the already accumulated flags with these */
 		for (i = 0; i < NCAPINTS; i++)
 			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
+
+		/* OR, i.e. replicate the bug flags */
+		for (i = NCAPINTS; i < NCAPINTS + NBUGINTS; i++)
+			c->x86_capability[i] |= boot_cpu_data.x86_capability[i];
 	}
 
 	/* Init Machine Check Exception if available. */

commit e6ebf5deaaaa33b661f0db86380c232b162bd68c
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu Dec 20 23:44:24 2012 -0800

    x86/common.c: load ucode in 64 bit or show loading ucode info in 32 bit on AP
    
    In 64 bit, load ucode on AP in cpu_init().
    
    In 32 bit, show ucode loading info on AP in cpu_init(). Microcode has been
    loaded earlier before paging. Now it is safe to show the loading microcode
    info on this AP.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1356075872-3054-5-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d7fd2468752a..d814772c5bed 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1220,6 +1220,12 @@ void __cpuinit cpu_init(void)
 	int cpu;
 	int i;
 
+	/*
+	 * Load microcode on this cpu if a valid microcode is available.
+	 * This is early microcode loading procedure.
+	 */
+	load_ucode_ap();
+
 	cpu = stack_smp_processor_id();
 	t = &per_cpu(init_tss, cpu);
 	oist = &per_cpu(orig_ist, cpu);
@@ -1311,6 +1317,8 @@ void __cpuinit cpu_init(void)
 	struct tss_struct *t = &per_cpu(init_tss, cpu);
 	struct thread_struct *thread = &curr->thread;
 
+	show_ucode_info_early();
+
 	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask)) {
 		printk(KERN_WARNING "CPU#%d already initialized!\n", cpu);
 		for (;;)

commit d288e1cf8e62f3e4034f1f021f047009c4ac0b3c
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu Dec 20 23:44:23 2012 -0800

    x86/common.c: Make have_cpuid_p() a global function
    
    Remove static declaration in have_cpuid_p() to make it a global function. The
    function will be called in early loading microcode.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1356075872-3054-4-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9c3ab43a6954..d7fd2468752a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -37,6 +37,8 @@
 #include <asm/mce.h>
 #include <asm/msr.h>
 #include <asm/pat.h>
+#include <asm/microcode.h>
+#include <asm/microcode_intel.h>
 
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/uv/uv.h>
@@ -213,7 +215,7 @@ static inline int flag_is_changeable_p(u32 flag)
 }
 
 /* Probe for the CPUID instruction */
-static int __cpuinit have_cpuid_p(void)
+int __cpuinit have_cpuid_p(void)
 {
 	return flag_is_changeable_p(X86_EFLAGS_ID);
 }
@@ -249,11 +251,6 @@ static inline int flag_is_changeable_p(u32 flag)
 {
 	return 1;
 }
-/* Probe for the CPUID instruction */
-static inline int have_cpuid_p(void)
-{
-	return 1;
-}
 static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 {
 }

commit 9977d9b379cb77e0f67bd6f4563618106e58e11d
Merge: cf4af0122157 541880d9a2c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 12:22:13 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull big execve/kernel_thread/fork unification series from Al Viro:
     "All architectures are converted to new model.  Quite a bit of that
      stuff is actually shared with architecture trees; in such cases it's
      literally shared branch pulled by both, not a cherry-pick.
    
      A lot of ugliness and black magic is gone (-3KLoC total in this one):
    
       - kernel_thread()/kernel_execve()/sys_execve() redesign.
    
         We don't do syscalls from kernel anymore for either kernel_thread()
         or kernel_execve():
    
         kernel_thread() is essentially clone(2) with callback run before we
         return to userland, the callbacks either never return or do
         successful do_execve() before returning.
    
         kernel_execve() is a wrapper for do_execve() - it doesn't need to
         do transition to user mode anymore.
    
         As a result kernel_thread() and kernel_execve() are
         arch-independent now - they live in kernel/fork.c and fs/exec.c
         resp.  sys_execve() is also in fs/exec.c and it's completely
         architecture-independent.
    
       - daemonize() is gone, along with its parts in fs/*.c
    
       - struct pt_regs * is no longer passed to do_fork/copy_process/
         copy_thread/do_execve/search_binary_handler/->load_binary/do_coredump.
    
       - sys_fork()/sys_vfork()/sys_clone() unified; some architectures
         still need wrappers (ones with callee-saved registers not saved in
         pt_regs on syscall entry), but the main part of those suckers is in
         kernel/fork.c now."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (113 commits)
      do_coredump(): get rid of pt_regs argument
      print_fatal_signal(): get rid of pt_regs argument
      ptrace_signal(): get rid of unused arguments
      get rid of ptrace_signal_deliver() arguments
      new helper: signal_pt_regs()
      unify default ptrace_signal_deliver
      flagday: kill pt_regs argument of do_fork()
      death to idle_regs()
      don't pass regs to copy_process()
      flagday: don't pass regs to copy_thread()
      bfin: switch to generic vfork, get rid of pointless wrappers
      xtensa: switch to generic clone()
      openrisc: switch to use of generic fork and clone
      unicore32: switch to generic clone(2)
      score: switch to generic fork/vfork/clone
      c6x: sanitize copy_thread(), get rid of clone(2) wrapper, switch to generic clone()
      take sys_fork/sys_vfork/sys_clone prototypes to linux/syscalls.h
      mn10300: switch to generic fork/vfork/clone
      h8300: switch to generic fork/vfork/clone
      tile: switch to generic clone()
      ...
    
    Conflicts:
            arch/microblaze/include/asm/Kbuild

commit 18c26c27ae0abe82253cb2e2363df465dbbb657e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:53:20 2012 -0400

    death to idle_regs()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7505f7b13e71..6a6432cf89de 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1173,15 +1173,6 @@ DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 DEFINE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif
 
-/* Make sure %fs and %gs are initialized properly in idle threads */
-struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
-{
-	memset(regs, 0, sizeof(struct pt_regs));
-	regs->fs = __KERNEL_PERCPU;
-	regs->gs = __KERNEL_STACK_CANARY;
-
-	return regs;
-}
 #endif	/* CONFIG_X86_64 */
 
 /*

commit 27fd185f3d9e83c64b7a596881d7751a638c9683
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue Nov 13 11:32:47 2012 -0800

    x86, hotplug: During CPU0 online, enable x2apic, set_numa_node.
    
    Previously these functions were not run on the BSP (CPU 0, the boot processor)
    since the boot processor init would only be executed before this functionality
    was initialized.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352835171-3958-11-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7505f7b13e71..ca165ac6793b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1237,7 +1237,7 @@ void __cpuinit cpu_init(void)
 	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
-	if (cpu != 0 && this_cpu_read(numa_node) == 0 &&
+	if (this_cpu_read(numa_node) == 0 &&
 	    early_cpu_to_node(cpu) != NUMA_NO_NODE)
 		set_numa_node(early_cpu_to_node(cpu));
 #endif
@@ -1269,8 +1269,7 @@ void __cpuinit cpu_init(void)
 	barrier();
 
 	x86_configure_nx();
-	if (cpu != 0)
-		enable_x2apic();
+	enable_x2apic();
 
 	/*
 	 * set up and load the per-CPU TSS

commit 15385dfe7e0fa6866b204dd0d14aec2cc48fc0a7
Merge: a57d985e378c b2cc2a074de7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 13:59:17 2012 -0700

    Merge branch 'x86-smap-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/smap support from Ingo Molnar:
     "This adds support for the SMAP (Supervisor Mode Access Prevention) CPU
      feature on Intel CPUs: a hardware feature that prevents unintended
      user-space data access from kernel privileged code.
    
      It's turned on automatically when possible.
    
      This, in combination with SMEP, makes it even harder to exploit kernel
      bugs such as NULL pointer dereferences."
    
    Fix up trivial conflict in arch/x86/kernel/entry_64.S due to newly added
    includes right next to each other.
    
    * 'x86-smap-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, smep, smap: Make the switching functions one-way
      x86, suspend: On wakeup always initialize cr4 and EFER
      x86-32: Start out eflags and cr4 clean
      x86, smap: Do not abuse the [f][x]rstor_checking() functions for user space
      x86-32, smap: Add STAC/CLAC instructions to 32-bit kernel entry
      x86, smap: Reduce the SMAP overhead for signal handling
      x86, smap: A page fault due to SMAP is an oops
      x86, smap: Turn on Supervisor Mode Access Prevention
      x86, smap: Add STAC and CLAC instructions to control user space access
      x86, uaccess: Merge prototypes for clear_user/__clear_user
      x86, smap: Add a header file with macros for STAC/CLAC
      x86, alternative: Add header guards to <asm/alternative-asm.h>
      x86, alternative: Use .pushsection/.popsection
      x86, smap: Add CR4 bit for SMAP
      x86-32, mm: The WP test should be done on a kernel page

commit 229993001282e128a49a59ec43d255614775703a
Merge: 7687b80a4f5a fd0f5869724f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 11:13:33 2012 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/mm changes from Ingo Molnar:
     "The biggest change is new TLB partial flushing code for AMD CPUs.
      (The v3.6 kernel had the Intel CPU side code, see commits
      e0ba94f14f74..effee4b9b3b.)
    
      There's also various other refinements around the TLB flush code"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Distinguish TLB shootdown interrupts from other functions call interrupts
      x86/mm: Fix range check in tlbflush debugfs interface
      x86, cpu: Preset default tlb_flushall_shift on AMD
      x86, cpu: Add AMD TLB size detection
      x86, cpu: Push TLB detection CPUID check down
      x86, cpu: Fixup tlb_flushall_shift formatting

commit ac07f5c3cb0cf19258c55cdf210aa4ac91ca7330
Merge: 3b29b03a4623 b1a74bf82123
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 11:10:52 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/fpu update from Ingo Molnar:
     "The biggest change is the addition of the non-lazy (eager) FPU saving
      support model and enabling it on CPUs with optimized xsaveopt/xrstor
      FPU state saving instructions.
    
      There are also various Sparse fixes"
    
    Fix up trivial add-add conflict in arch/x86/kernel/traps.c
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, kvm: fix kvm's usage of kernel_fpu_begin/end()
      x86, fpu: remove cpu_has_xmm check in the fx_finit()
      x86, fpu: make eagerfpu= boot param tri-state
      x86, fpu: enable eagerfpu by default for xsaveopt
      x86, fpu: decouple non-lazy/eager fpu restore from xsave
      x86, fpu: use non-lazy fpu restore for processors supporting xsave
      lguest, x86: handle guest TS bit for lazy/non-lazy fpu host models
      x86, fpu: always use kernel_fpu_begin/end() for in-kernel FPU usage
      x86, kvm: use kernel_fpu_begin/end() in kvm_load/put_guest_fpu()
      x86, fpu: remove unnecessary user_fpu_end() in save_xstate_sig()
      x86, fpu: drop_fpu() before restoring new state from sigframe
      x86, fpu: Unify signal handling code paths for x86 and x86_64 kernels
      x86, fpu: Consolidate inline asm routines for saving/restoring fpu state
      x86, signal: Cleanup ifdefs and is_ia32, is_x32

commit 58ae9c0d54ae3916614fa1f3756dadb954f16e6c
Merge: 4a553e145082 924e101a7ab6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 11:07:31 2012 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 debug update from Ingo Molnar:
     "Various small enhancements"
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/debug: Dump family, model, stepping of the boot CPU
      x86/iommu: Use NULL instead of plain 0 for __IOMMU_INIT
      x86/iommu: Drop duplicate const in __IOMMU_INIT
      x86/fpu/xsave: Keep __user annotation in casts
      x86/pci/probe_roms: Add missing __iomem annotation to pci_map_biosrom()
      x86/signals: ia32_signal.c: add __user casts to fix sparse warnings
      x86/vdso: Add __user annotation to VDSO32_SYMBOL
      x86: Fix __user annotations in asm/sys_ia32.h

commit b2cc2a074de75671bbed5e2dda67a9252ef353ea
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Sep 26 18:02:28 2012 -0700

    x86, smep, smap: Make the switching functions one-way
    
    There is no fundamental reason why we should switch SMEP and SMAP on
    during early cpu initialization just to switch them off again.  Now
    with %eflags and %cr4 forced to be initialized to a clean state, we
    only need the one-way enable.  Also, make the functions inline to make
    them (somewhat) harder to abuse.
    
    This does mean that SMEP and SMAP do not get initialized anywhere near
    as early.  Even using early_param() instead of __setup() doesn't give
    us control early enough to do this during the early cpu initialization
    phase.  This seems reasonable to me, because SMEP and SMAP should not
    matter until we have userspace to protect ourselves from, but it does
    potentially make it possible for a bug involving a "leak of
    permissions to userspace" to get uncaught.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 44aec5d4dfaf..fefd9b7e93e1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -259,48 +259,36 @@ static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 }
 #endif
 
-static int disable_smep __cpuinitdata;
 static __init int setup_disable_smep(char *arg)
 {
-	disable_smep = 1;
+	setup_clear_cpu_cap(X86_FEATURE_SMEP);
 	return 1;
 }
 __setup("nosmep", setup_disable_smep);
 
-static __cpuinit void setup_smep(struct cpuinfo_x86 *c)
+static __always_inline void setup_smep(struct cpuinfo_x86 *c)
 {
-	if (cpu_has(c, X86_FEATURE_SMEP)) {
-		if (unlikely(disable_smep)) {
-			setup_clear_cpu_cap(X86_FEATURE_SMEP);
-			clear_in_cr4(X86_CR4_SMEP);
-		} else
-			set_in_cr4(X86_CR4_SMEP);
-	}
+	if (cpu_has(c, X86_FEATURE_SMEP))
+		set_in_cr4(X86_CR4_SMEP);
 }
 
-static int disable_smap __cpuinitdata;
 static __init int setup_disable_smap(char *arg)
 {
-	disable_smap = 1;
+	setup_clear_cpu_cap(X86_FEATURE_SMAP);
 	return 1;
 }
 __setup("nosmap", setup_disable_smap);
 
-static __cpuinit void setup_smap(struct cpuinfo_x86 *c)
+static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 {
-	if (cpu_has(c, X86_FEATURE_SMAP)) {
-		if (unlikely(disable_smap)) {
-			setup_clear_cpu_cap(X86_FEATURE_SMAP);
-			clear_in_cr4(X86_CR4_SMAP);
-		} else {
-			set_in_cr4(X86_CR4_SMAP);
-			/*
-			 * Don't use clac() here since alternatives
-			 * haven't run yet...
-			 */
-			asm volatile(__stringify(__ASM_CLAC) ::: "memory");
-		}
-	}
+	unsigned long eflags;
+
+	/* This should have been cleared long ago */
+	raw_local_save_flags(eflags);
+	BUG_ON(eflags & X86_EFLAGS_AC);
+
+	if (cpu_has(c, X86_FEATURE_SMAP))
+		set_in_cr4(X86_CR4_SMAP);
 }
 
 /*
@@ -737,9 +725,6 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	c->cpu_index = 0;
 	filter_cpuid_features(c, false);
 
-	setup_smep(c);
-	setup_smap(c);
-
 	if (this_cpu->c_bsp_init)
 		this_cpu->c_bsp_init(c);
 }
@@ -824,8 +809,6 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		c->phys_proc_id = c->initial_apicid;
 	}
 
-	setup_smep(c);
-
 	get_model_name(c); /* Default name */
 
 	detect_nopl(c);
@@ -890,6 +873,10 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	/* Disable the PN if appropriate */
 	squash_the_stupid_serial_number(c);
 
+	/* Set up SMEP/SMAP */
+	setup_smep(c);
+	setup_smap(c);
+
 	/*
 	 * The vendor-specific functions might have changed features.
 	 * Now we do "generic changes."

commit 49b8c695e331c9685e6ffdbf34872509d77c8459
Merge: e59d1b0a2419 b1a74bf82123
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 21 17:18:44 2012 -0700

    Merge branch 'x86/fpu' into x86/smap
    
    Reason for merge:
           x86/fpu changed the structure of some of the code that x86/smap
           changes; mostly fpu-internal.h but also minor changes to the
           signal code.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    
    Resolved Conflicts:
            arch/x86/ia32/ia32_signal.c
            arch/x86/include/asm/fpu-internal.h
            arch/x86/kernel/signal.c

commit 52b6179ac87d33c2eeaff5292786a10fe98cff64
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 21 12:43:13 2012 -0700

    x86, smap: Turn on Supervisor Mode Access Prevention
    
    If Supervisor Mode Access Prevention is available and not disabled by
    the user, turn it on.  Also fix the expansion of SMEP (Supervisor Mode
    Execution Prevention.)
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1348256595-29119-10-git-send-email-hpa@linux.intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cd43e525fde8..7d35d6594118 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -278,6 +278,31 @@ static __cpuinit void setup_smep(struct cpuinfo_x86 *c)
 	}
 }
 
+static int disable_smap __cpuinitdata;
+static __init int setup_disable_smap(char *arg)
+{
+	disable_smap = 1;
+	return 1;
+}
+__setup("nosmap", setup_disable_smap);
+
+static __cpuinit void setup_smap(struct cpuinfo_x86 *c)
+{
+	if (cpu_has(c, X86_FEATURE_SMAP)) {
+		if (unlikely(disable_smap)) {
+			setup_clear_cpu_cap(X86_FEATURE_SMAP);
+			clear_in_cr4(X86_CR4_SMAP);
+		} else {
+			set_in_cr4(X86_CR4_SMAP);
+			/*
+			 * Don't use clac() here since alternatives
+			 * haven't run yet...
+			 */
+			asm volatile(__stringify(__ASM_CLAC) ::: "memory");
+		}
+	}
+}
+
 /*
  * Some CPU features depend on higher CPUID levels, which may not always
  * be available due to CPUID level capping or broken virtualization
@@ -713,6 +738,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	filter_cpuid_features(c, false);
 
 	setup_smep(c);
+	setup_smap(c);
 
 	if (this_cpu->c_bsp_init)
 		this_cpu->c_bsp_init(c);

commit 63bcff2a307b9bcc712a8251eb27df8b2e117967
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Sep 21 12:43:12 2012 -0700

    x86, smap: Add STAC and CLAC instructions to control user space access
    
    When Supervisor Mode Access Prevention (SMAP) is enabled, access to
    userspace from the kernel is controlled by the AC flag.  To make the
    performance of manipulating that flag acceptable, there are two new
    instructions, STAC and CLAC, to set and clear it.
    
    This patch adds those instructions, via alternative(), when the SMAP
    feature is enabled.  It also adds X86_EFLAGS_AC unconditionally to the
    SYSCALL entry mask; there is simply no reason to make that one
    conditional.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1348256595-29119-9-git-send-email-hpa@linux.intel.com

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a5fbc3c5fccc..cd43e525fde8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1113,7 +1113,8 @@ void syscall_init(void)
 
 	/* Flags to clear on syscall */
 	wrmsrl(MSR_SYSCALL_MASK,
-	       X86_EFLAGS_TF|X86_EFLAGS_DF|X86_EFLAGS_IF|X86_EFLAGS_IOPL);
+	       X86_EFLAGS_TF|X86_EFLAGS_DF|X86_EFLAGS_IF|
+	       X86_EFLAGS_IOPL|X86_EFLAGS_AC);
 }
 
 unsigned long kernel_eflags;

commit 924e101a7ab6f884047f4344e5f1154a4bcd63a6
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Fri Sep 14 18:37:46 2012 +0200

    x86/debug: Dump family, model, stepping of the boot CPU
    
    When acting on a user bug report, we find ourselves constantly
    asking for /proc/cpuinfo in order to know the exact family,
    model, stepping of the CPU in question.
    
    Instead of having to ask this, add this to dmesg so that it is
    visible and no ambiguities can ensue from looking at the
    official name string of the CPU coming from CPUID and trying
    to map it to f/m/s.
    
    Output then looks like this:
    
    [    0.146041] smpboot: CPU0: AMD FX(tm)-8100 Eight-Core Processor (fam: 15, model: 01, stepping: 02)
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/1347640666-13638-1-git-send-email-bp@amd64.org
    [ tweaked it minimally to add commas. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a5fbc3c5fccc..1cc48ff91cb3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1023,14 +1023,16 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 		printk(KERN_CONT "%s ", vendor);
 
 	if (c->x86_model_id[0])
-		printk(KERN_CONT "%s", c->x86_model_id);
+		printk(KERN_CONT "%s", strim(c->x86_model_id));
 	else
 		printk(KERN_CONT "%d86", c->x86);
 
+	printk(KERN_CONT " (fam: %02x, model: %02x", c->x86, c->x86_model);
+
 	if (c->x86_mask || c->cpuid_level >= 0)
-		printk(KERN_CONT " stepping %02x\n", c->x86_mask);
+		printk(KERN_CONT ", stepping: %02x)\n", c->x86_mask);
 	else
-		printk(KERN_CONT "\n");
+		printk(KERN_CONT ")\n");
 
 	print_cpu_msr(c);
 }

commit 5d2bd7009f306c82afddd1ca4d9763ad8473c216
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Sep 6 14:58:52 2012 -0700

    x86, fpu: decouple non-lazy/eager fpu restore from xsave
    
    Decouple non-lazy/eager fpu restore policy from the existence of the xsave
    feature. Introduce a synthetic CPUID flag to represent the eagerfpu
    policy. "eagerfpu=on" boot paramter will enable the policy.
    
    Requested-by: H. Peter Anvin <hpa@zytor.com>
    Requested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1347300665-6209-2-git-send-email-suresh.b.siddha@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a5fbc3c5fccc..b0fe078614d8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1297,7 +1297,6 @@ void __cpuinit cpu_init(void)
 	dbg_restore_debug_regs();
 
 	fpu_init();
-	xsave_init();
 
 	raw_local_save_flags(kernel_eflags);
 
@@ -1352,6 +1351,5 @@ void __cpuinit cpu_init(void)
 	dbg_restore_debug_regs();
 
 	fpu_init();
-	xsave_init();
 }
 #endif

commit 6eebdda35e6b18d0dddb2a44e34211bd94f0cad6
Author: Ian Campbell <ian.campbell@citrix.com>
Date:   Fri Aug 24 23:58:47 2012 +0400

    x86: Drop unnecessary kernel_eflags variable on 64-bit
    
    On 64 bit x86 we save the current eflags in cpu_init for use in
    ret_from_fork. Strictly speaking reserved bits in EFLAGS should
    be read as written but in practise it is unlikely that EFLAGS
    could ever be extended in this way and the kernel alread clears
    any undefined flags early on.
    
    The equivalent 32 bit code simply hard codes 0x0202 as the new
    EFLAGS.
    
    This change makes 64 bit use the same mechanism to setup the
    initial EFLAGS on fork. Note that 64 bit resets EFLAGS before
    calling schedule_tail() as opposed to 32 bit which calls
    schedule_tail() first. Therefore the correct value for EFLAGS
    has opposite IF bit.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/20120824195847.GA31628@moon
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a5fbc3c5fccc..9961e2e23709 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1116,8 +1116,6 @@ void syscall_init(void)
 	       X86_EFLAGS_TF|X86_EFLAGS_DF|X86_EFLAGS_IF|X86_EFLAGS_IOPL);
 }
 
-unsigned long kernel_eflags;
-
 /*
  * Copies of the original ist values from the tss are only accessed during
  * debugging, no special alignment required.
@@ -1299,8 +1297,6 @@ void __cpuinit cpu_init(void)
 	fpu_init();
 	xsave_init();
 
-	raw_local_save_flags(kernel_eflags);
-
 	if (is_uv_system())
 		uv_cpu_init();
 }

commit c6fd893da927c6cefb2ece22402765379921a834
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Jul 31 10:29:14 2012 -0700

    x86, avx: don't use avx instructions with "noxsave" boot param
    
    Clear AVX, AVX2 features along with clearing XSAVE feature bits,
    as part of the parsing "noxsave" parameter.
    
    Fixes the kernel boot panic with "noxsave" boot parameter.
    
    We could have checked cpu_has_osxsave along with cpu_has_avx etc, but Peter
    mentioned clearing the feature bits will be better for uses like
    static_cpu_has() etc.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1343755754.2041.2.camel@sbsiddha-desk.sc.intel.com
    Cc: <stable@vger.kernel.org>    # v3.5
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 46d8786d655e..a5fbc3c5fccc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -144,6 +144,8 @@ static int __init x86_xsave_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
 	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
+	setup_clear_cpu_cap(X86_FEATURE_AVX);
+	setup_clear_cpu_cap(X86_FEATURE_AVX2);
 	return 1;
 }
 __setup("noxsave", x86_xsave_setup);

commit 5b556332c3ab19e6375836d35ca658776e9ba0f6
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon Aug 6 19:00:37 2012 +0200

    x86, cpu: Push TLB detection CPUID check down
    
    Push the max CPUID leaf check into the ->detect_tlb function and remove
    general test case from the generic path.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1344272439-29080-3-git-send-email-bp@amd64.org
    Acked-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d239977f361f..080f4a737e3e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -940,8 +940,7 @@ void __init identify_boot_cpu(void)
 #else
 	vgetcpu_set_mode();
 #endif
-	if (boot_cpu_data.cpuid_level >= 2)
-		cpu_detect_tlb(&boot_cpu_data);
+	cpu_detect_tlb(&boot_cpu_data);
 }
 
 void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)

commit a9ad773e0dd833651f0831020a0ea0265c29f2ea
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon Aug 6 19:00:36 2012 +0200

    x86, cpu: Fixup tlb_flushall_shift formatting
    
    The TLB characteristics appeared like this in dmesg:
    
    [    0.065817] Last level iTLB entries: 4KB 512, 2MB 1024, 4MB 512
    [    0.065817] Last level dTLB entries: 4KB 1024, 2MB 1024, 4MB 512
    [    0.065817] tlb_flushall_shift is 0xffffffff
    
    where tlb_flushall_shift is actually -1 but dumped as a hex number.
    However, the Kconfig option CONFIG_DEBUG_TLBFLUSH and the rest of the
    code treats this as a signed decimal and states "If you set it to -1,
    the code flushes the whole TLB unconditionally."
    
    So, fix its formatting in accordance with the other references to it.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1344272439-29080-2-git-send-email-bp@amd64.org
    Acked-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 46d8786d655e..d239977f361f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -474,7 +474,7 @@ void __cpuinit cpu_detect_tlb(struct cpuinfo_x86 *c)
 
 	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n" \
 		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d\n"	     \
-		"tlb_flushall_shift is 0x%x\n",
+		"tlb_flushall_shift: %d\n",
 		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
 		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
 		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES],

commit 4cb38750d49010ae72e718d46605ac9ba5a851b4
Merge: 0a2fe19ccc4b 7efa1c87963d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 26 13:17:17 2012 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/mm changes from Peter Anvin:
     "The big change here is the patchset by Alex Shi to use INVLPG to flush
      only the affected pages when we only need to flush a small page range.
    
      It also removes the special INVALIDATE_TLB_VECTOR interrupts (32
      vectors!) and replace it with an ordinary IPI function call."
    
    Fix up trivial conflicts in arch/x86/include/asm/apic.h (added code next
    to changed line)
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/tlb: Fix build warning and crash when building for !SMP
      x86/tlb: do flush_tlb_kernel_range by 'invlpg'
      x86/tlb: replace INVALIDATE_TLB_VECTOR by CALL_FUNCTION_VECTOR
      x86/tlb: enable tlb flush range support for x86
      mm/mmu_gather: enable tlb flush range in generic mmu_gather
      x86/tlb: add tlb_flushall_shift knob into debugfs
      x86/tlb: add tlb_flushall_shift for specific CPU
      x86/tlb: fall back to flush all when meet a THP large page
      x86/flush_tlb: try flush_tlb_single one by one in flush_tlb_range
      x86/tlb_info: get last level TLB entry number of CPU
      x86: Add read_mostly declaration/definition to variables from smp.h
      x86: Define early read-mostly per-cpu macros

commit c4211f42d3e66875298a5e26a75109878c80f15b
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 28 09:02:19 2012 +0800

    x86/tlb: add tlb_flushall_shift for specific CPU
    
    Testing show different CPU type(micro architectures and NUMA mode) has
    different balance points between the TLB flush all and multiple invlpg.
    And there also has cases the tlb flush change has no any help.
    
    This patch give a interface to let x86 vendor developers have a chance
    to set different shift for different CPU type.
    
    like some machine in my hands, balance points is 16 entries on
    Romely-EP; while it is at 8 entries on Bloomfield NHM-EP; and is 256 on
    IVB mobile CPU. but on model 15 core2 Xeon using invlpg has nothing
    help.
    
    For untested machine, do a conservative optimization, same as NHM CPU.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Link: http://lkml.kernel.org/r/1340845344-27557-5-git-send-email-alex.shi@intel.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b2016df00813..7595552600b8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -459,16 +459,26 @@ u16 __read_mostly tlb_lld_4k[NR_INFO];
 u16 __read_mostly tlb_lld_2m[NR_INFO];
 u16 __read_mostly tlb_lld_4m[NR_INFO];
 
+/*
+ * tlb_flushall_shift shows the balance point in replacing cr3 write
+ * with multiple 'invlpg'. It will do this replacement when
+ *   flush_tlb_lines <= active_lines/2^tlb_flushall_shift.
+ * If tlb_flushall_shift is -1, means the replacement will be disabled.
+ */
+s8  __read_mostly tlb_flushall_shift = -1;
+
 void __cpuinit cpu_detect_tlb(struct cpuinfo_x86 *c)
 {
 	if (this_cpu->c_detect_tlb)
 		this_cpu->c_detect_tlb(c);
 
 	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n" \
-		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d\n",
+		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d\n"	     \
+		"tlb_flushall_shift is 0x%x\n",
 		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
 		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
-		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES]);
+		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES],
+		tlb_flushall_shift);
 }
 
 void __cpuinit detect_ht(struct cpuinfo_x86 *c)

commit e0ba94f14f747c2661c4d21f8c44e5b0b8cd8e48
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 28 09:02:16 2012 +0800

    x86/tlb_info: get last level TLB entry number of CPU
    
    For 4KB pages, x86 CPU has 2 or 1 level TLB, first level is data TLB and
    instruction TLB, second level is shared TLB for both data and instructions.
    
    For hupe page TLB, usually there is just one level and seperated by 2MB/4MB
    and 1GB.
    
    Although each levels TLB size is important for performance tuning, but for
    genernal and rude optimizing, last level TLB entry number is suitable. And
    in fact, last level TLB always has the biggest entry number.
    
    This patch will get the biggest TLB entry number and use it in furture TLB
    optimizing.
    
    Accroding Borislav's suggestion, except tlb_ll[i/d]_* array, other
    function and data will be released after system boot up.
    
    For all kinds of x86 vendor friendly, vendor specific code was moved to its
    specific files.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Link: http://lkml.kernel.org/r/1340845344-27557-2-git-send-email-alex.shi@intel.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6b9333b429ba..b2016df00813 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -452,6 +452,25 @@ void __cpuinit cpu_detect_cache_sizes(struct cpuinfo_x86 *c)
 	c->x86_cache_size = l2size;
 }
 
+u16 __read_mostly tlb_lli_4k[NR_INFO];
+u16 __read_mostly tlb_lli_2m[NR_INFO];
+u16 __read_mostly tlb_lli_4m[NR_INFO];
+u16 __read_mostly tlb_lld_4k[NR_INFO];
+u16 __read_mostly tlb_lld_2m[NR_INFO];
+u16 __read_mostly tlb_lld_4m[NR_INFO];
+
+void __cpuinit cpu_detect_tlb(struct cpuinfo_x86 *c)
+{
+	if (this_cpu->c_detect_tlb)
+		this_cpu->c_detect_tlb(c);
+
+	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n" \
+		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d\n",
+		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
+		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
+		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES]);
+}
+
 void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_HT
@@ -911,6 +930,8 @@ void __init identify_boot_cpu(void)
 #else
 	vgetcpu_set_mode();
 #endif
+	if (boot_cpu_data.cpuid_level >= 2)
+		cpu_detect_tlb(&boot_cpu_data);
 }
 
 void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)

commit ecd431d95aa04257e977fd6878e4203ce462e7e9
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Fri Jun 1 16:52:36 2012 +0200

    x86, cpu: Fix show_msr MSR accessing function
    
    There's no real reason why, when showing the MSRs on a CPU at boottime,
    we should be using the AMD-specific variant. Simply use the generic safe
    one which handles #GPs just fine.
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1338562358-28182-3-git-send-email-bp@amd64.org
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6b9333b429ba..5bbc082c47ad 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -947,7 +947,7 @@ static void __cpuinit __print_cpu_msr(void)
 		index_max = msr_range_array[i].max;
 
 		for (index = index_min; index < index_max; index++) {
-			if (rdmsrl_amd_safe(index, &val))
+			if (rdmsrl_safe(index, &val))
 				continue;
 			printk(KERN_INFO " MSR%08x: %016llx\n", index, val);
 		}

commit f8988175fd70874d1fb3712b1c5d3bfc6d455202
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 30 11:47:00 2012 -0400

    x86: Allow nesting of the debug stack IDT setting
    
    When the NMI handler runs, it checks if it preempted a debug handler
    and if that handler is using the debug stack. If it is, it changes the
    IDT table not to update the stack, otherwise it will reset the debug
    stack and corrupt the debug handler it preempted.
    
    Now that ftrace uses breakpoints to change functions from nops to
    callers, many more places may hit a breakpoint. Unfortunately this
    includes some of the calls that lockdep performs. Which causes issues
    with the debug stack. It too needs to change the debug stack before
    tracing (if called from the debug handler).
    
    Allow the debug_stack_set_zero() and debug_stack_reset() to be nested
    so that the debug handlers can take advantage of them too.
    
    [ Used this_cpu_*() over __get_cpu_var() as suggested by H. Peter Anvin ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 82f29e70d058..6b9333b429ba 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1101,14 +1101,20 @@ int is_debug_stack(unsigned long addr)
 		 addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ));
 }
 
+static DEFINE_PER_CPU(u32, debug_stack_use_ctr);
+
 void debug_stack_set_zero(void)
 {
+	this_cpu_inc(debug_stack_use_ctr);
 	load_idt((const struct desc_ptr *)&nmi_idt_descr);
 }
 
 void debug_stack_reset(void)
 {
-	load_idt((const struct desc_ptr *)&idt_descr);
+	if (WARN_ON(!this_cpu_read(debug_stack_use_ctr)))
+		return;
+	if (this_cpu_dec_return(debug_stack_use_ctr) == 0)
+		load_idt((const struct desc_ptr *)&idt_descr);
 }
 
 #else	/* CONFIG_X86_64 */

commit c6ae41e7d469f00d9c92a2b2887c7235d121c009
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri May 11 15:35:27 2012 +0800

    x86: replace percpu_xxx funcs with this_cpu_xxx
    
    Since percpu_xxx() serial functions are duplicated with this_cpu_xxx().
    Removing percpu_xxx() definition and replacing them by this_cpu_xxx()
    in code. There is no function change in this patch, just preparation for
    later percpu_xxx serial function removing.
    
    On x86 machine the this_cpu_xxx() serial functions are same as
    __this_cpu_xxx() without no unnecessary premmpt enable/disable.
    
    Thanks for Stephen Rothwell, he found and fixed a i386 build error in
    the patch.
    
    Also thanks for Andrew Morton, he kept updating the patchset in Linus'
    tree.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Christoph Lameter <cl@gentwo.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cf79302198a6..82f29e70d058 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1185,7 +1185,7 @@ void __cpuinit cpu_init(void)
 	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
-	if (cpu != 0 && percpu_read(numa_node) == 0 &&
+	if (cpu != 0 && this_cpu_read(numa_node) == 0 &&
 	    early_cpu_to_node(cpu) != NUMA_NO_NODE)
 		set_numa_node(early_cpu_to_node(cpu));
 #endif

commit 68894632afb2729a1d8785c877840953894c7283
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Mon Apr 2 18:06:48 2012 +0200

    x86/platform: Remove incorrect error message in x86_default_fixup_cpu_id()
    
    It's only called from amd.c:srat_detect_node(). The introduced
    condition for calling the fixup code is true for all AMD
    multi-node processors, e.g. Magny-Cours and Interlagos. There we
    have 2 NUMA nodes on one socket. Thus there are cores having
    different numa-node-id but with equal phys_proc_id.
    
    There is no point to print error messages in such a situation.
    
    The confusing/misleading error message was introduced with
    commit 64be4c1c2428e148de6081af235e2418e6a66dda ("x86: Add
    x86_init platform override to fix up NUMA core numbering").
    
    Remove the default fixup function (especially the error message)
    and replace it by a NULL pointer check, move the
    Numascale-specific condition for calling the fixup into the
    fixup-function itself and slightly adapt the comment.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: <stable@kernel.org>
    Cc: <sp@numascale.com>
    Cc: <bp@amd64.org>
    Cc: <daniel@numascale-asia.com>
    Link: http://lkml.kernel.org/r/20120402160648.GR27684@alberich.amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 67e258362a3d..cf79302198a6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1162,15 +1162,6 @@ static void dbg_restore_debug_regs(void)
 #define dbg_restore_debug_regs()
 #endif /* ! CONFIG_KGDB */
 
-/*
- * Prints an error where the NUMA and configured core-number mismatch and the
- * platform didn't override this to fix it up
- */
-void __cpuinit x86_default_fixup_cpu_id(struct cpuinfo_x86 *c, int node)
-{
-	pr_err("NUMA core number %d differs from configured core number %d\n", node, c->phys_proc_id);
-}
-
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT

commit 6b8212a313dae341ef3a2e413dfec5c4dea59617
Merge: bcd550745fc5 8abc3122aa02
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 14:28:26 2012 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 updates from Ingo Molnar.
    
    This touches some non-x86 files due to the sanitized INLINE_SPIN_UNLOCK
    config usage.
    
    Fixed up trivial conflicts due to just header include changes (removing
    headers due to cpu_idle() merge clashing with the <asm/system.h> split).
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic/amd: Be more verbose about LVT offset assignments
      x86, tls: Off by one limit check
      x86/ioapic: Add io_apic_ops driver layer to allow interception
      x86/olpc: Add debugfs interface for EC commands
      x86: Merge the x86_32 and x86_64 cpu_idle() functions
      x86/kconfig: Remove CONFIG_TR=y from the defconfigs
      x86: Stop recursive fault in print_context_stack after stack overflow
      x86/io_apic: Move and reenable irq only when CONFIG_GENERIC_PENDING_IRQ=y
      x86/apic: Add separate apic_id_valid() functions for selected apic drivers
      locking/kconfig: Simplify INLINE_SPIN_UNLOCK usage
      x86/kconfig: Update defconfigs
      x86: Fix excessive MSR print out when show_msr is not specified

commit ed2d265d1266736bd294332d7f649003943ae36e
Merge: f1d38e423a69 6c03438edeb5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 24 10:08:39 2012 -0700

    Merge tag 'bug-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    Pull <linux/bug.h> cleanup from Paul Gortmaker:
     "The changes shown here are to unify linux's BUG support under the one
      <linux/bug.h> file.  Due to historical reasons, we have some BUG code
      in bug.h and some in kernel.h -- i.e.  the support for BUILD_BUG in
      linux/kernel.h predates the addition of linux/bug.h, but old code in
      kernel.h wasn't moved to bug.h at that time.  As a band-aid, kernel.h
      was including <asm/bug.h> to pseudo link them.
    
      This has caused confusion[1] and general yuck/WTF[2] reactions.  Here
      is an example that violates the principle of least surprise:
    
          CC      lib/string.o
          lib/string.c: In function 'strlcat':
          lib/string.c:225:2: error: implicit declaration of function 'BUILD_BUG_ON'
          make[2]: *** [lib/string.o] Error 1
          $
          $ grep linux/bug.h lib/string.c
          #include <linux/bug.h>
          $
    
      We've included <linux/bug.h> for the BUG infrastructure and yet we
      still get a compile fail! [We've not kernel.h for BUILD_BUG_ON.] Ugh -
      very confusing for someone who is new to kernel development.
    
      With the above in mind, the goals of this changeset are:
    
      1) find and fix any include/*.h files that were relying on the
         implicit presence of BUG code.
      2) find and fix any C files that were consuming kernel.h and hence
         relying on implicitly getting some/all BUG code.
      3) Move the BUG related code living in kernel.h to <linux/bug.h>
      4) remove the asm/bug.h from kernel.h to finally break the chain.
    
      During development, the order was more like 3-4, build-test, 1-2.  But
      to ensure that git history for bisect doesn't get needless build
      failures introduced, the commits have been reorderd to fix the problem
      areas in advance.
    
            [1]  https://lkml.org/lkml/2012/1/3/90
            [2]  https://lkml.org/lkml/2012/1/17/414"
    
    Fix up conflicts (new radeon file, reiserfs header cleanups) as per Paul
    and linux-next.
    
    * tag 'bug-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux:
      kernel.h: doesn't explicitly use bug.h, so don't include it.
      bug: consolidate BUILD_BUG_ON with other bug code
      BUG: headers with BUG/BUG_ON etc. need linux/bug.h
      bug.h: add include of it to various implicit C users
      lib: fix implicit users of kernel.h for TAINT_WARN
      spinlock: macroize assert_spin_locked to avoid bug.h dependency
      x86: relocate get/set debugreg fcns to include/asm/debugreg.

commit 0b8b8078cb4db2ebe9a20f2b2eaeb58988be32bd
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Mar 22 21:31:43 2012 -0700

    x86: Fix excessive MSR print out when show_msr is not specified
    
    Dave found:
    
    | During bootup, I now have 162 messages like this..
    | [    0.227346]  MSR0000001b: 00000000fee00900
    | [    0.227465]  MSR00000021: 0000000000000001
    | [    0.227584]  MSR0000002a: 00000000c1c81400
    |
    | commit 21c3fcf3e39353d4f21d50e257cc74f3204b1988 looks suspect.
    | It claims that it will only print these out if show_msr= is
    | passed, but that doesn't seem to be the case.
    
    Fix it by changing to the version that checks the index.
    
    Reported-and-tested-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1332477103-4595-1-git-send-email-yinghai@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ade9c794ed98..b24032355a76 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -998,7 +998,7 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 	else
 		printk(KERN_CONT "\n");
 
-	__print_cpu_msr();
+	print_cpu_msr(c);
 }
 
 void __cpuinit print_cpu_msr(struct cpuinfo_x86 *c)

commit 35cb8d9e18c0bb33b90d7e574abadbe23b65427d
Merge: 02c502566ef5 1361b83a13d4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:41:22 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/fpu changes from Ingo Molnar.
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      i387: Split up <asm/i387.h> into exported and internal interfaces
      i387: Uninline the generic FP helpers that we expose to kernel modules

commit 4c64616bb51b399886ded8f4f69bad4da2da1817
Merge: c5c7fb8fbd7c 943bc7e110f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:30:39 2012 -0700

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/debug changes from Ingo Molnar.
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Fix section warnings
      x86-64: Fix CFI data for common_interrupt()
      x86: Properly _init-annotate NMI selftest code
      x86/debug: Fix/improve the show_msr=<cpus> debug print out

commit f649e9388cd46ad1634164e56f96ae092ca59e4a
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jan 20 16:24:09 2012 -0500

    x86: relocate get/set debugreg fcns to include/asm/debugreg.
    
    Since we already have a debugreg.h header file, move the
    assoc. get/set functions to it.  In addition to it being the
    logical home for them, it has a secondary advantage.  The
    functions that are moved use BUG().  So we really need to
    have linux/bug.h in scope.  But asm/processor.h is used about
    600 times, vs. only about 15 for debugreg.h -- so adding bug.h
    to the latter reduces the amount of time we'll be processing
    it during a compile.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c0f7d68d318f..0d676dd923ac 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -18,6 +18,7 @@
 #include <asm/archrandom.h>
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
+#include <asm/debugreg.h>
 #include <asm/sections.h>
 #include <linux/topology.h>
 #include <linux/cpumask.h>

commit 1361b83a13d4d92e53fbb6c877528713e118b821
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 13:19:22 2012 -0800

    i387: Split up <asm/i387.h> into exported and internal interfaces
    
    While various modules include <asm/i387.h> to get access to things we
    actually *intend* for them to use, most of that header file was really
    pretty low-level internal stuff that we really don't want to expose to
    others.
    
    So split the header file into two: the small exported interfaces remain
    in <asm/i387.h>, while the internal definitions that are only used by
    core architecture code are now in <asm/fpu-internal.h>.
    
    The guiding principle for this was to expose functions that we export to
    modules, and leave them in <asm/i387.h>, while stuff that is used by
    task switching or was marked GPL-only is in <asm/fpu-internal.h>.
    
    The fpu-internal.h file could be further split up too, especially since
    arch/x86/kvm/ uses some of the remaining stuff for its module.  But that
    kvm usage should probably be abstracted out a bit, and at least now the
    internal FPU accessor functions are much more contained.  Even if it
    isn't perhaps as contained as it _could_ be.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1202211340330.5354@i5.linux-foundation.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cb71b01ab66e..89620b1725d4 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -28,6 +28,7 @@
 #include <asm/apic.h>
 #include <asm/desc.h>
 #include <asm/i387.h>
+#include <asm/fpu-internal.h>
 #include <asm/mtrr.h>
 #include <linux/numa.h>
 #include <asm/asm.h>

commit 8546c008924d5fd1724fa698eaa92b414bafd50d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 10:25:45 2012 -0800

    i387: Uninline the generic FP helpers that we expose to kernel modules
    
    Instead of exporting the very low-level internals of the FPU state
    save/restore code (ie things like 'fpu_owner_task'), we should export
    the higher-level interfaces.
    
    Inlining these things is pointless anyway: sure, sometimes the end
    result is small, but while 'stts()' can result in just three x86
    instructions, those are not cheap instructions (writing %cr0 is a
    serializing instruction and a very slow one at that).
    
    So the overhead of a function call is not noticeable, and we really
    don't want random modules mucking about with our internal state save
    logic anyway.
    
    So this unexports 'fpu_owner_task', and instead uninlines and exports
    the actual functions that modules can use: fpu_kernel_begin/end() and
    unlazy_fpu().
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1202211339590.5354@i5.linux-foundation.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c0f7d68d318f..cb71b01ab66e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1045,7 +1045,6 @@ DEFINE_PER_CPU(char *, irq_stack_ptr) =
 DEFINE_PER_CPU(unsigned int, irq_count) = -1;
 
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
-EXPORT_PER_CPU_SYMBOL(fpu_owner_task);
 
 /*
  * Special IST stacks which the CPU switches to when it calls
@@ -1115,7 +1114,6 @@ void debug_stack_reset(void)
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
-EXPORT_PER_CPU_SYMBOL(fpu_owner_task);
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 DEFINE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);

commit 27e74da9800289e69ba907777df1e2085231eff7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 19:34:10 2012 -0800

    i387: export 'fpu_owner_task' per-cpu variable
    
    (And define it properly for x86-32, which had its 'current_task'
    declaration in separate from x86-64)
    
    Bitten by my dislike for modules on the machines I use, and the fact
    that apparently nobody else actually wanted to test the patches I sent
    out.
    
    Snif. Nobody else cares.
    
    Anyway, we probably should uninline the 'kernel_fpu_begin()' function
    that is what modules actually use and that references this, but this is
    the minimal fix for now.
    
    Reported-by: Josh Boyer <jwboyer@gmail.com>
    Reported-and-tested-by: Jongman Heo <jongman.heo@samsung.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b667148dfad7..c0f7d68d318f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1045,6 +1045,7 @@ DEFINE_PER_CPU(char *, irq_stack_ptr) =
 DEFINE_PER_CPU(unsigned int, irq_count) = -1;
 
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
+EXPORT_PER_CPU_SYMBOL(fpu_owner_task);
 
 /*
  * Special IST stacks which the CPU switches to when it calls
@@ -1113,6 +1114,8 @@ void debug_stack_reset(void)
 
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
+DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
+EXPORT_PER_CPU_SYMBOL(fpu_owner_task);
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 DEFINE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);

commit 7e16838d94b566a17b65231073d179bc04d590c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 19 13:27:00 2012 -0800

    i387: support lazy restore of FPU state
    
    This makes us recognize when we try to restore FPU state that matches
    what we already have in the FPU on this CPU, and avoids the restore
    entirely if so.
    
    To do this, we add two new data fields:
    
     - a percpu 'fpu_owner_task' variable that gets written any time we
       update the "has_fpu" field, and thus acts as a kind of back-pointer
       to the task that owns the CPU.  The exception is when we save the FPU
       state as part of a context switch - if the save can keep the FPU
       state around, we leave the 'fpu_owner_task' variable pointing at the
       task whose FP state still remains on the CPU.
    
     - a per-thread 'last_cpu' field, that indicates which CPU that thread
       used its FPU on last.  We update this on every context switch
       (writing an invalid CPU number if the last context switch didn't
       leave the FPU in a lazily usable state), so we know that *that*
       thread has done nothing else with the FPU since.
    
    These two fields together can be used when next switching back to the
    task to see if the CPU still matches: if 'fpu_owner_task' matches the
    task we are switching to, we know that no other task (or kernel FPU
    usage) touched the FPU on this CPU in the meantime, and if the current
    CPU number matches the 'last_cpu' field, we know that this thread did no
    other FP work on any other CPU, so the FPU state on the CPU must match
    what was saved on last context switch.
    
    In that case, we can avoid the 'f[x]rstor' entirely, and just clear the
    CR0.TS bit.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d43cad74f166..b667148dfad7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1044,6 +1044,8 @@ DEFINE_PER_CPU(char *, irq_stack_ptr) =
 
 DEFINE_PER_CPU(unsigned int, irq_count) = -1;
 
+DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
+
 /*
  * Special IST stacks which the CPU switches to when it calls
  * an IST-marked descriptor entry. Up to 7 stacks (hardware

commit 21c3fcf3e39353d4f21d50e257cc74f3204b1988
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sun Feb 12 09:53:57 2012 -0800

    x86/debug: Fix/improve the show_msr=<cpus> debug print out
    
    Found out that show_msr=<cpus> is broken, when I asked a
    user to use it to capture debug info about broken MTRR's
    whose MTRR settings are probably different between CPUs.
    
    Only the first CPUs MSRs are printed, but that is not
    enough to track down the suspected bug.
    
    For years we called print_cpu_msr from print_cpu_info(),
    but this commit:
    
    | commit 2eaad1fddd7450a48ad464229775f97fbfe8af36
    | Author: Mike Travis <travis@sgi.com>
    | Date:   Thu Dec 10 17:19:36 2009 -0800
    |
    |    x86: Limit the number of processor bootup messages
    
    removed the print_cpu_info() call from all APs.
    
    Put it back - it will only print MSRs when the user
    specifically requests them via show_msr=<cpus>.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Mike Travis <travis@sgi.com>
    Link: http://lkml.kernel.org/r/1329069237-11483-1-git-send-email-yinghai@kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d43cad74f166..8b6a3bb57d8e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -933,7 +933,7 @@ static const struct msr_range msr_range_array[] __cpuinitconst = {
 	{ 0xc0011000, 0xc001103b},
 };
 
-static void __cpuinit print_cpu_msr(void)
+static void __cpuinit __print_cpu_msr(void)
 {
 	unsigned index_min, index_max;
 	unsigned index;
@@ -997,13 +997,13 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 	else
 		printk(KERN_CONT "\n");
 
-#ifdef CONFIG_SMP
+	__print_cpu_msr();
+}
+
+void __cpuinit print_cpu_msr(struct cpuinfo_x86 *c)
+{
 	if (c->cpu_index < show_msr)
-		print_cpu_msr();
-#else
-	if (show_msr)
-		print_cpu_msr();
-#endif
+		__print_cpu_msr();
 }
 
 static __init int setup_disablecpuid(char *arg)

commit 83c2f912b43c3a7babbb6cb7ae2a5276c1ed2a3e
Merge: f0ed5b9a2853 172d1b0b7325
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 15 11:26:35 2012 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (39 commits)
      perf tools: Fix compile error on x86_64 Ubuntu
      perf report: Fix --stdio output alignment when --showcpuutilization used
      perf annotate: Get rid of field_sep check
      perf annotate: Fix usage string
      perf kmem: Fix a memory leak
      perf kmem: Add missing closedir() calls
      perf top: Add error message for EMFILE
      perf test: Change type of '-v' option to INCR
      perf script: Add missing closedir() calls
      tracing: Fix compile error when static ftrace is enabled
      recordmcount: Fix handling of elf64 big-endian objects.
      perf tools: Add const.h to MANIFEST to make perf-tar-src-pkg work again
      perf tools: Add support for guest/host-only profiling
      perf kvm: Do guest-only counting by default
      perf top: Don't update total_period on process_sample
      perf hists: Stop using 'self' for struct hist_entry
      perf hists: Rename total_session to total_period
      x86: Add counter when debug stack is used with interrupts enabled
      x86: Allow NMIs to hit breakpoints in i386
      x86: Keep current stack in NMI breakpoints
      ...

commit 42181186ad4db986fcaa40ca95c6e407e9e79372
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Dec 16 11:43:02 2011 -0500

    x86: Add counter when debug stack is used with interrupts enabled
    
    Mathieu Desnoyers pointed out a case that can cause issues with
    NMIs running on the debug stack:
    
      int3 -> interrupt -> NMI -> int3
    
    Because the interrupt changes the stack, the NMI will not see that
    it preempted the debug stack. Looking deeper at this case,
    interrupts only happen when the int3 is from userspace or in
    an a location in the exception table (fixup).
    
      userspace -> int3 -> interurpt -> NMI -> int3
    
    All other int3s that happen in the kernel should be processed
    without ever enabling interrupts, as the do_trap() call will
    panic the kernel if it is called to process any other location
    within the kernel.
    
    Adding a counter around the sections that enable interrupts while
    using the debug stack allows the NMI to also check that case.
    If the NMI sees that it either interrupted a task using the debug
    stack or the debug counter is non-zero, then it will have to
    change the IDT table to make the int3 not change stacks (which will
    corrupt the stack if it does).
    
    Note, I had to move the debug_usage functions out of processor.h
    and into debugreg.h because of the static inlined functions to
    inc and dec the debug_usage counter. __get_cpu_var() requires
    smp.h which includes processor.h, and would fail to build.
    
    Link: http://lkml.kernel.org/r/1323976535.23971.112.camel@gandalf.stny.rr.com
    
    Reported-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul Turner <pjt@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index caa404556b9c..266e4649b1da 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1093,11 +1093,13 @@ unsigned long kernel_eflags;
 DEFINE_PER_CPU(struct orig_ist, orig_ist);
 
 static DEFINE_PER_CPU(unsigned long, debug_stack_addr);
+DEFINE_PER_CPU(int, debug_stack_usage);
 
 int is_debug_stack(unsigned long addr)
 {
-	return addr <= __get_cpu_var(debug_stack_addr) &&
-		addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ);
+	return __get_cpu_var(debug_stack_usage) ||
+		(addr <= __get_cpu_var(debug_stack_addr) &&
+		 addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ));
 }
 
 void debug_stack_set_zero(void)

commit 228bdaa95fb830e08b6acd1afd4d2c55093cabfa
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Dec 9 03:02:19 2011 -0500

    x86: Keep current stack in NMI breakpoints
    
    We want to allow NMI handlers to have breakpoints to be able to
    remove stop_machine from ftrace, kprobes and jump_labels. But if
    an NMI interrupts a current breakpoint, and then it triggers a
    breakpoint itself, it will switch to the breakpoint stack and
    corrupt the data on it for the breakpoint processing that it
    interrupted.
    
    Instead, have the NMI check if it interrupted breakpoint processing
    by checking if the stack that is currently used is a breakpoint
    stack. If it is, then load a special IDT that changes the IST
    for the debug exception to keep the same stack in kernel context.
    When the NMI is done, it puts it back.
    
    This way, if the NMI does trigger a breakpoint, it will keep
    using the same stack and not stomp on the breakpoint data for
    the breakpoint it interrupted.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index aa003b13a831..caa404556b9c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1026,6 +1026,8 @@ __setup("clearcpuid=", setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
 struct desc_ptr idt_descr = { NR_VECTORS * 16 - 1, (unsigned long) idt_table };
+struct desc_ptr nmi_idt_descr = { NR_VECTORS * 16 - 1,
+				    (unsigned long) nmi_idt_table };
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE);
@@ -1090,6 +1092,24 @@ unsigned long kernel_eflags;
  */
 DEFINE_PER_CPU(struct orig_ist, orig_ist);
 
+static DEFINE_PER_CPU(unsigned long, debug_stack_addr);
+
+int is_debug_stack(unsigned long addr)
+{
+	return addr <= __get_cpu_var(debug_stack_addr) &&
+		addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ);
+}
+
+void debug_stack_set_zero(void)
+{
+	load_idt((const struct desc_ptr *)&nmi_idt_descr);
+}
+
+void debug_stack_reset(void)
+{
+	load_idt((const struct desc_ptr *)&idt_descr);
+}
+
 #else	/* CONFIG_X86_64 */
 
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
@@ -1208,6 +1228,8 @@ void __cpuinit cpu_init(void)
 			estacks += exception_stack_sizes[v];
 			oist->ist[v] = t->x86_tss.ist[v] =
 					(unsigned long)estacks;
+			if (v == DEBUG_STACK-1)
+				per_cpu(debug_stack_addr, cpu) = (unsigned long)estacks;
 		}
 	}
 

commit 141168c36cdee3ff23d9c7700b0edc47cb65479f
Author: Kevin Winchester <kjwinchester@gmail.com>
Date:   Tue Dec 20 20:52:22 2011 -0400

    x86: Simplify code by removing a !SMP #ifdefs from 'struct cpuinfo_x86'
    
    Several fields in struct cpuinfo_x86 were not defined for the
    !SMP case, likely to save space.  However, those fields still
    have some meaning for UP, and keeping them allows some #ifdef
    removal from other files.  The additional size of the UP kernel
    from this change is not significant enough to worry about
    keeping up the distinction:
    
               text    data     bss     dec     hex filename
            4737168  506459  972040 6215667  5ed7f3 vmlinux.o.before
            4737444  506459  972040 6215943  5ed907 vmlinux.o.after
    
    for a difference of 276 bytes for an example UP config.
    
    If someone wants those 276 bytes back badly then it should
    be implemented in a cleaner way.
    
    Signed-off-by: Kevin Winchester <kjwinchester@gmail.com>
    Cc: Steffen Persvold <sp@numascale.com>
    Link: http://lkml.kernel.org/r/1324428742-12498-1-git-send-email-kjwinchester@gmail.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a70bd5b96b9e..850f2963a420 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -676,9 +676,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	if (this_cpu->c_early_init)
 		this_cpu->c_early_init(c);
 
-#ifdef CONFIG_SMP
 	c->cpu_index = 0;
-#endif
 	filter_cpuid_features(c, false);
 
 	setup_smep(c);
@@ -764,10 +762,7 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		c->apicid = c->initial_apicid;
 # endif
 #endif
-
-#ifdef CONFIG_X86_HT
 		c->phys_proc_id = c->initial_apicid;
-#endif
 	}
 
 	setup_smep(c);
@@ -1146,9 +1141,7 @@ static void dbg_restore_debug_regs(void)
  */
 void __cpuinit x86_default_fixup_cpu_id(struct cpuinfo_x86 *c, int node)
 {
-#ifdef CONFIG_NUMA
 	pr_err("NUMA core number %d differs from configured core number %d\n", node, c->phys_proc_id);
-#endif
 }
 
 /*

commit e4a02b4a951a7adf9d982b11c64686570c29fbe7
Author: Steffen Persvold <sp@numascale.com>
Date:   Tue Dec 6 01:10:31 2011 +0100

    x86: Fix the !CONFIG_NUMA build of the new CPU ID fixup code support
    
    I used "ifdef CONFIG_NUMA" simply because it doesn't make
    sense in a non-numa configuration even with SMP enabled.
    
    Besides, the only place where it is called right now is
    in kernel/cpu/amd.c:srat_detect_node() within the
    "CONFIG_NUMA" protected part.
    
    Signed-off-by: Steffen Persvold <sp@numascale.com>
    Cc: Daniel J Blueman <daniel@numascale-asia.com>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Link: http://lkml.kernel.org/r/1323073238-32686-2-git-send-email-daniel@numascale-asia.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ad4da45effb9..a70bd5b96b9e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1146,7 +1146,9 @@ static void dbg_restore_debug_regs(void)
  */
 void __cpuinit x86_default_fixup_cpu_id(struct cpuinfo_x86 *c, int node)
 {
+#ifdef CONFIG_NUMA
 	pr_err("NUMA core number %d differs from configured core number %d\n", node, c->phys_proc_id);
+#endif
 }
 
 /*

commit 64be4c1c2428e148de6081af235e2418e6a66dda
Author: Daniel J Blueman <daniel@numascale-asia.com>
Date:   Mon Dec 5 16:20:37 2011 +0800

    x86: Add x86_init platform override to fix up NUMA core numbering
    
    Add an x86_init vector for handling inconsistent core numbering.
    This is useful for multi-fabric platforms, such as Numascale
    NumaConnect.
    
    v2:
     - use struct x86_cpuinit_ops
     - provide default fall-back function to warn
    
    Signed-off-by: Daniel J Blueman <daniel@numascale-asia.com>
    Cc: Steffen Persvold <sp@numascale.com>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Link: http://lkml.kernel.org/r/1323073238-32686-2-git-send-email-daniel@numascale-asia.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index aa003b13a831..ad4da45effb9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1140,6 +1140,15 @@ static void dbg_restore_debug_regs(void)
 #define dbg_restore_debug_regs()
 #endif /* ! CONFIG_KGDB */
 
+/*
+ * Prints an error where the NUMA and configured core-number mismatch and the
+ * platform didn't override this to fix it up
+ */
+void __cpuinit x86_default_fixup_cpu_id(struct cpuinfo_x86 *c, int node)
+{
+	pr_err("NUMA core number %d differs from configured core number %d\n", node, c->phys_proc_id);
+}
+
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT

commit 8e6d539e0fd0c2124a20a207da70f2af7a9ae52c
Merge: 8237eb946a1a 49d859d78c5a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 28 05:29:07 2011 -0700

    Merge branch 'x86-rdrand-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-rdrand-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, random: Verify RDRAND functionality and allow it to be disabled
      x86, random: Architectural inlines to get random integers with RDRAND
      random: Add support for architectural random hooks
    
    Fix up trivial conflicts in drivers/char/random.c: the architectural
    random hooks touched "get_random_int()" that was simplified to use MD5
    and not do the keyptr thing any more (see commit 6e5714eaf77d: "net:
    Compute protocol sequence numbers and fragment IDs using MD5").

commit e34eb39c1c791fe79da6aae0d9057f0c74c2f0ed
Merge: 396e6e49c58b 910b2c5122ab
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 28 05:03:12 2011 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, amd: Include linux/elf.h since we use stuff from asm/elf.h
      x86: cache_info: Update calculation of AMD L3 cache indices
      x86: cache_info: Kill the atomic allocation in amd_init_l3_cache()
      x86: cache_info: Kill the moronic shadow struct
      x86: cache_info: Remove bogus free of amd_l3_cache data
      x86, amd: Include elf.h explicitly, prepare the code for the module.h split
      x86-32, amd: Move va_align definition to unbreak 32-bit build
      x86, amd: Move BSP code to cpu_dev helper
      x86: Add a BSP cpu_dev helper
      x86, amd: Avoid cache aliasing penalties on AMD family 15h

commit a110b5ec7371592eac856ac5c22dc7b518952d44
Author: Borislav Petkov <bp@amd64.org>
Date:   Fri Aug 5 20:01:16 2011 +0200

    x86: Add a BSP cpu_dev helper
    
    Add a function ptr to struct cpu_dev which is destined to be run only
    once on the BSP during boot.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/20110805180116.GB26217@aftab
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 22a073d7fbff..8ed394a8eb6e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -681,6 +681,9 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	filter_cpuid_features(c, false);
 
 	setup_smep(c);
+
+	if (this_cpu->c_bsp_init)
+		this_cpu->c_bsp_init(c);
 }
 
 void __init early_cpu_init(void)

commit 49d859d78c5aeb998b6936fcb5f288f78d713489
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sun Jul 31 14:02:19 2011 -0700

    x86, random: Verify RDRAND functionality and allow it to be disabled
    
    If the CPU declares that RDRAND is available, go through a guranteed
    reseed sequence, and make sure that it is actually working (producing
    data.)   If it does not, disable the CPU feature flag.
    
    Allow RDRAND to be disabled on the command line (as opposed to at
    compile time) for a user who has special requirements with regards to
    random numbers.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Theodore Ts'o" <tytso@mit.edu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 22a073d7fbff..8dbd9294f4f2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -15,6 +15,7 @@
 #include <asm/stackprotector.h>
 #include <asm/perf_event.h>
 #include <asm/mmu_context.h>
+#include <asm/archrandom.h>
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
 #include <asm/sections.h>
@@ -857,6 +858,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 #endif
 
 	init_hypervisor(c);
+	x86_init_rdrand(c);
 
 	/*
 	 * Clear/Set all flags overriden by options, need do it

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 22a073d7fbff..62184390a601 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -21,7 +21,7 @@
 #include <linux/topology.h>
 #include <linux/cpumask.h>
 #include <asm/pgtable.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/proto.h>
 #include <asm/setup.h>
 #include <asm/apic.h>

commit f310642123e0d32d919c60ca3fab5acd130c4ba3
Merge: ef1d57599dc9 5d4c47e0195b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 29 11:18:09 2011 -0700

    Merge branch 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6
    
    * 'idle-release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-idle-2.6:
      x86 idle: deprecate mwait_idle() and "idle=mwait" cmdline param
      x86 idle: deprecate "no-hlt" cmdline param
      x86 idle APM: deprecate CONFIG_APM_CPU_IDLE
      x86 idle floppy: deprecate disable_hlt()
      x86 idle: EXPORT_SYMBOL(default_idle, pm_idle) only when APM demands it
      x86 idle: clarify AMD erratum 400 workaround
      idle governor: Avoid lock acquisition to read pm_qos before entering idle
      cpuidle: menu: fixed wrapping timers at 4.294 seconds

commit 02c68a02018669d1817c43c42de800975cbec467
Author: Len Brown <len.brown@intel.com>
Date:   Fri Apr 1 16:59:53 2011 -0400

    x86 idle: clarify AMD erratum 400 workaround
    
    The workaround for AMD erratum 400 uses the term "c1e" falsely suggesting:
    1. Intel C1E is somehow involved
    2. All AMD processors with C1E are involved
    
    Use the string "amd_c1e" instead of simply "c1e" to clarify that
    this workaround is specific to AMD's version of C1E.
    Use the string "e400" to clarify that the workaround is specific
    to AMD processors with Erratum 400.
    
    This patch is text-substitution only, with no functional change.
    
    cc: x86@kernel.org
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1d59834396bd..745a602f204f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -887,7 +887,7 @@ static void vgetcpu_set_mode(void)
 void __init identify_boot_cpu(void)
 {
 	identify_cpu(&boot_cpu_data);
-	init_c1e_mask();
+	init_amd_e400_c1e_mask();
 #ifdef CONFIG_X86_32
 	sysenter_setup();
 	enable_sep_cpu();

commit 8b27f2ff7a24f0735c96055e676872f05398d99b
Author: Nikhil P Rao <nikhil.rao@intel.com>
Date:   Wed May 25 10:18:41 2011 -0700

    x86: Remove unnecessary check in detect_ht()
    
    This patch removes a check that causes incorrect scheduler
    domain setup (SMP instead of SMT) and bootlog warning messages
    when cpuid extensions for topology enumeration are not supported
    and the number of processors reported to the OS is smaller than
    smp_num_siblings.
    
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Nikhil P Rao <nikhil.rao@intel.com>
    Link: http://lkml.kernel.org/r/1306343921.19325.1.camel@fedora13
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c8b41623377f..53f02f5d8cce 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -477,13 +477,6 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 	if (smp_num_siblings <= 1)
 		goto out;
 
-	if (smp_num_siblings > nr_cpu_ids) {
-		pr_warning("CPU: Unsupported number of siblings %d",
-			   smp_num_siblings);
-		smp_num_siblings = 1;
-		return;
-	}
-
 	index_msb = get_count_order(smp_num_siblings);
 	c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid, index_msb);
 

commit 82da65dab5f438ac7df28eeb43e2f5b742aa00ef
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 22 21:37:01 2011 -0700

    x86: setup_smep needs to be __cpuinit
    
    The setup_smep function gets calle at resume time too, and is thus not a
    pure __init function.  When marked as __init, it gets thrown out after
    the kernel has initialized, and when the kernel is suspended and
    resumed, the code will no longer be around, and we'll get a nice "kernel
    tried to execute NX-protected page" oops because the page is no longer
    marked executable.
    
    Reported-and-tested-by: Parag Warudkar <parag.lkml@gmail.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: "H. Peter Anvin" <hpa@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cbc70a27430c..c8b41623377f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -254,7 +254,7 @@ static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 }
 #endif
 
-static int disable_smep __initdata;
+static int disable_smep __cpuinitdata;
 static __init int setup_disable_smep(char *arg)
 {
 	disable_smep = 1;
@@ -262,7 +262,7 @@ static __init int setup_disable_smep(char *arg)
 }
 __setup("nosmep", setup_disable_smep);
 
-static __init void setup_smep(struct cpuinfo_x86 *c)
+static __cpuinit void setup_smep(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_SMEP)) {
 		if (unlikely(disable_smep)) {

commit de5397ad5b9ad22e2401c4dacdf1bb3b19c05679
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Wed May 11 16:51:05 2011 -0700

    x86, cpu: Enable/disable Supervisor Mode Execution Protection
    
    Enable/disable newly documented SMEP (Supervisor Mode Execution Protection) CPU
    feature in kernel. CR4.SMEP (bit 20) is 0 at power-on. If the feature is
    supported by CPU (X86_FEATURE_SMEP), enable SMEP by setting CR4.SMEP. New kernel
    option nosmep disables the feature even if the feature is supported by CPU.
    
    [ hpa: moved the call to setup_smep() until after the vendor-specific
      initialization; that ensures that CPUID features are unmasked.  We
      will still run it before we have userspace (never mind uncontrolled
      userspace). ]
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    LKML-Reference: <1305157865-31727-1-git-send-email-fenghua.yu@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 173f3a3fa1a6..cbc70a27430c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -254,6 +254,25 @@ static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 }
 #endif
 
+static int disable_smep __initdata;
+static __init int setup_disable_smep(char *arg)
+{
+	disable_smep = 1;
+	return 1;
+}
+__setup("nosmep", setup_disable_smep);
+
+static __init void setup_smep(struct cpuinfo_x86 *c)
+{
+	if (cpu_has(c, X86_FEATURE_SMEP)) {
+		if (unlikely(disable_smep)) {
+			setup_clear_cpu_cap(X86_FEATURE_SMEP);
+			clear_in_cr4(X86_CR4_SMEP);
+		} else
+			set_in_cr4(X86_CR4_SMEP);
+	}
+}
+
 /*
  * Some CPU features depend on higher CPUID levels, which may not always
  * be available due to CPUID level capping or broken virtualization
@@ -667,6 +686,8 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	c->cpu_index = 0;
 #endif
 	filter_cpuid_features(c, false);
+
+	setup_smep(c);
 }
 
 void __init early_cpu_init(void)
@@ -752,6 +773,8 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 #endif
 	}
 
+	setup_smep(c);
+
 	get_model_name(c); /* Default name */
 
 	detect_nopl(c);

commit 2494b030ba9334c7dd7df9b9f7abe4eacc950ec5
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue May 17 12:33:26 2011 -0700

    x86, cpufeature: Fix cpuid leaf 7 feature detection
    
    CPUID leaf 7, subleaf 0 returns the maximum subleaf in EAX, not the
    number of subleaves.  Since so far only subleaf 0 is defined (and only
    the EBX bitfield) we do not need to qualify the test.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1305660806-17519-1-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: <stable@kernel.org> 2.6.36..39

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e2ced0074a45..173f3a3fa1a6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -565,8 +565,7 @@ void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 
 		cpuid_count(0x00000007, 0, &eax, &ebx, &ecx, &edx);
 
-		if (eax > 0)
-			c->x86_capability[9] = ebx;
+		c->x86_capability[9] = ebx;
 	}
 
 	/* AMD-defined flags: level 0x80000001 */

commit 181f977d134a9f8e3f8839f42af655b045fc059e
Merge: d5d42399bd7b 25542c646afb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 19:49:10 2011 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (93 commits)
      x86, tlb, UV: Do small micro-optimization for native_flush_tlb_others()
      x86-64, NUMA: Don't call numa_set_distanc() for all possible node combinations during emulation
      x86-64, NUMA: Don't assume phys node 0 is always online in numa_emulation()
      x86-64, NUMA: Clean up initmem_init()
      x86-64, NUMA: Fix numa_emulation code with node0 without RAM
      x86-64, NUMA: Revert NUMA affine page table allocation
      x86: Work around old gas bug
      x86-64, NUMA: Better explain numa_distance handling
      x86-64, NUMA: Fix distance table handling
      mm: Move early_node_map[] reverse scan helpers under HAVE_MEMBLOCK
      x86-64, NUMA: Fix size of numa_distance array
      x86: Rename e820_table_* to pgt_buf_*
      bootmem: Move __alloc_memory_core_early() to nobootmem.c
      bootmem: Move contig_page_data definition to bootmem.c/nobootmem.c
      bootmem: Separate out CONFIG_NO_BOOTMEM code into nobootmem.c
      x86-64, NUMA: Seperate out numa_alloc_distance() from numa_set_distance()
      x86-64, NUMA: Add proper function comments to global functions
      x86-64, NUMA: Move NUMA emulation into numa_emulation.c
      x86-64, NUMA: Prepare numa_emulation() for moving NUMA emulation into a separate file
      x86-64, NUMA: Do not scan two times for setup_node_bootmem()
      ...
    
    Fix up conflicts in arch/x86/kernel/smpboot.c

commit ac23f25355ef53f3d14352fcff3c6817527a9749
Author: Jan Beulich <JBeulich@novell.com>
Date:   Fri Mar 4 15:52:35 2011 +0000

    x86: Really print supported CPUs if PROCESSOR_SELECT=y
    
    I'm sure it was a mere oversight that the CONFIG_ prefixes are
    missing.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Cc: Dave Jones <davej@redhat.com>
    LKML-Reference: <4D7118D30200007800034F79@vpn.id2.novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1d59834396bd..5d98c46f876d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -675,7 +675,7 @@ void __init early_cpu_init(void)
 	const struct cpu_dev *const *cdev;
 	int count = 0;
 
-#ifdef PROCESSOR_SELECT
+#ifdef CONFIG_PROCESSOR_SELECT
 	printk(KERN_INFO "KERNEL supported cpus:\n");
 #endif
 
@@ -687,7 +687,7 @@ void __init early_cpu_init(void)
 		cpu_devs[count] = cpudev;
 		count++;
 
-#ifdef PROCESSOR_SELECT
+#ifdef CONFIG_PROCESSOR_SELECT
 		{
 			unsigned int j;
 

commit de2d9445f1627830ed2ebd00ee9d851986c940b5
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:41 2011 +0100

    x86: Unify node_to_cpumask_map handling between 32 and 64bit
    
    x86_32 has been managing node_to_cpumask_map explicitly from
    map_cpu_to_node() and friends in a rather ugly way.  With
    previous changes, it's now possible to share the code with
    64bit.
    
    * When CONFIG_NUMA_EMU is disabled, numa_add/remove_cpu() are
      implemented in numa.c and shared by 32 and 64bit.  CONFIG_NUMA_EMU
      versions still live in numa_64.c.
    
      NUMA_EMU's dependency on 64bit is planned to be removed and the
      above should go away together.
    
    * identify_cpu() now calls numa_add_cpu() for 32bit too.  This
      makes the explicit mask management from map_cpu_to_node() unnecessary.
    
    * The whole x86_32 specific map_cpu_to_node() chunk is no longer
      necessary.  Dropped.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-16-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Shaohui Zheng <shaohui.zheng@intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1d59834396bd..a2559c3ed500 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -869,7 +869,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 	select_idle_routine(c);
 
-#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+#ifdef CONFIG_NUMA
 	numa_add_cpu(smp_processor_id());
 #endif
 }

commit 004417a6d468e24399e383645c068b498eed84ad
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Nov 25 18:38:29 2010 +0100

    perf, arch: Cleanup perf-pmu init vs lockup-detector
    
    The perf hardware pmu got initialized at various points in the boot,
    some before early_initcall() some after (notably arch_initcall).
    
    The problem is that the NMI lockup detector is ran from early_initcall()
    and expects the hardware pmu to be present.
    
    Sanitize this by moving all architecture hardware pmu implementations to
    initialize at early_initcall() and move the lockup detector to an explicit
    initcall right after that.
    
    Cc: paulus <paulus@samba.org>
    Cc: davem <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1290707759.2145.119.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4b68bda30938..1d59834396bd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -894,7 +894,6 @@ void __init identify_boot_cpu(void)
 #else
 	vgetcpu_set_mode();
 #endif
-	init_hw_perf_events();
 }
 
 void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)

commit b6f7e38dbb310557fe890b04b1a376c93f638c3b
Merge: 214515b5787a b2b57fe053c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:34:32 2010 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, fpu: Merge fpu_save_init()
      x86-32, fpu: Rewrite fpu_save_init()
      x86, fpu: Remove PSHUFB_XMM5_* macros
      x86, fpu: Remove unnecessary ifdefs from i387 code.
      x86-32, fpu: Remove math_emulate stub
      x86-64, fpu: Simplify constraints for fxsave/fxtstor
      x86-64, fpu: Fix %cs value in convert_from_fxsr()
      x86-64, fpu: Disable preemption when using TS_USEDFPU
      x86, fpu: Merge __save_init_fpu()
      x86, fpu: Merge tolerant_fwait()
      x86, fpu: Merge fpu_init()
      x86: Use correct type for %cr4
      x86, xsave: Disable xsave in i387 emulation mode
    
    Fixed up fxsaveq-induced conflict in arch/x86/include/asm/i387.h

commit bf70030dc0b031f000c74721f2e9c88686b7da6d
Merge: d60a2793ba56 366d4a43b1a7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:18:36 2010 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, cpu: Fix X86_FEATURE_NOPL
      x86, cpu: Re-run get_cpu_cap() after adjusting the CPUID level

commit d60a2793ba562c6ea9bbf62112da3e6342adcf83
Merge: 781c5a67f152 40ffa9379198
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:18:06 2010 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Remove stale pmtimer_64.c
      x86, cleanups: Use clear_page/copy_page rather than memset/memcpy
      x86: Remove unnecessary #ifdef ACPI/X86_IO_ACPI
      x86, cleanup: Remove obsolete boot_cpu_id variable

commit 366d4a43b1a7a861c356a0e407c4f03f454d42ea
Author: Borislav Petkov <bp@alien8.de>
Date:   Mon Oct 4 09:31:27 2010 +0200

    x86, cpu: Fix X86_FEATURE_NOPL
    
    ba0593bf553c450a03dbc5f8c1f0ff58b778a0c8 cleared the aforementioned
    cpuid bit only on 32-bit due to various problems with Virtual PC. This
    somehow got lost during the 32- + 64-bit merge so restore the feature
    bit on 64-bit. For that, set it explicitly for non-constant arguments of
    cpu_has(). Update comment for future reference.
    
    Signed-off-by: Borislav Petkov <bp@alien8.de>
    LKML-Reference: <20101004073127.GA20305@liondog.tnic>
    Cc: Ryan O'Neill <ryan@innosecc.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f2f9ac7da25c..927e630aeaf3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -704,16 +704,21 @@ void __init early_cpu_init(void)
 }
 
 /*
- * The NOPL instruction is supposed to exist on all CPUs with
- * family >= 6; unfortunately, that's not true in practice because
- * of early VIA chips and (more importantly) broken virtualizers that
- * are not easy to detect.  In the latter case it doesn't even *fail*
- * reliably, so probing for it doesn't even work.  Disable it completely
+ * The NOPL instruction is supposed to exist on all CPUs of family >= 6;
+ * unfortunately, that's not true in practice because of early VIA
+ * chips and (more importantly) broken virtualizers that are not easy
+ * to detect. In the latter case it doesn't even *fail* reliably, so
+ * probing for it doesn't even work. Disable it completely on 32-bit
  * unless we can find a reliable way to detect all the broken cases.
+ * Enable it explicitly on 64-bit for non-constant inputs of cpu_has().
  */
 static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
 {
+#ifdef CONFIG_X86_32
 	clear_cpu_cap(c, X86_FEATURE_NOPL);
+#else
+	set_cpu_cap(c, X86_FEATURE_NOPL);
+#endif
 }
 
 static void __cpuinit generic_identify(struct cpuinfo_x86 *c)

commit d900329e20f4476db6461752accebcf7935a8055
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Sep 28 15:35:01 2010 -0700

    x86, cpu: After uncapping CPUID, re-run CPU feature detection
    
    After uncapping the CPUID level, we need to also re-run the CPU
    feature detection code.
    
    This resolves kernel bugzilla 16322.
    
    Reported-by: boris64 <bugzilla.kernel.org@boris64.net>
    Cc: <stable@kernel.org> v2.6.29..2.6.35
    LKML-Reference: <tip-@git.kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 490dac63c2d2..f2f9ac7da25c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -545,7 +545,7 @@ void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
 	}
 }
 
-static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
+void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
 	u32 ebx;

commit c2b9ff24a0df649d4d40947878b5b5ac39c7299e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Mon Sep 20 18:01:46 2010 -0700

    x86, cpu: Re-run get_cpu_cap() after adjusting the CPUID level
    
    At least on Intel, adjusting the max CPUID level can expose new CPUID
    features, so we need to re-run get_cpu_cap() after changing the CPUID
    level.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 490dac63c2d2..f2f9ac7da25c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -545,7 +545,7 @@ void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
 	}
 }
 
-static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
+void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
 	u32 ebx;

commit 6ac8bac2684235f4caf22a410549c582aa7327d6
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Sep 3 21:17:09 2010 -0400

    x86, fpu: Merge fpu_init()
    
    Make fpu_init() handle 32-bit setup.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1283563039-3466-3-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 490dac63c2d2..f9e23e8639eb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1264,13 +1264,6 @@ void __cpuinit cpu_init(void)
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
 
-	/*
-	 * Force FPU initialization:
-	 */
-	current_thread_info()->status = 0;
-	clear_used_math();
-	mxcsr_feature_mask_init();
-
 	fpu_init();
 	xsave_init();
 }

commit f6e9456c9272bb570df6e217cdbe007e270b1c4e
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jul 21 19:03:58 2010 +0200

    x86, cleanup: Remove obsolete boot_cpu_id variable
    
    boot_cpu_id is there for historical reasons and was renamed to
    boot_cpu_physical_apicid in patch:
    
     c70dcb7 x86: change boot_cpu_id to boot_cpu_physical_apicid
    
    However, there are some remaining occurrences of boot_cpu_id that are
    never touched in the kernel and thus its value is always 0.
    
    This patch removes boot_cpu_id completely.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1279731838-1522-8-git-send-email-robert.richter@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 490dac63c2d2..787b3c7c6625 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -665,7 +665,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		this_cpu->c_early_init(c);
 
 #ifdef CONFIG_SMP
-	c->cpu_index = boot_cpu_id;
+	c->cpu_index = 0;
 #endif
 	filter_cpuid_features(c, false);
 }

commit 4a386c3e177ca2fbc70c9283d0b46537844763a0
Merge: e8779776afbd 1cff92d8fdb2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 16:25:13 2010 -0700

    Merge branch 'x86-xsave-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-xsave-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, xsave: Make xstate_enable_boot_cpu() __init, protect on CPU 0
      x86, xsave: Add __init attribute to setup_xstate_features()
      x86, xsave: Make init_xstate_buf static
      x86, xsave: Check cpuid level for XSTATE_CPUID (0x0d)
      x86, xsave: Introduce xstate enable functions
      x86, xsave: Separate fpu and xsave initialization
      x86, xsave: Move boot cpu initialization to xsave_init()
      x86, xsave: 32/64 bit boot cpu check unification in initialization
      x86, xsave: Do not include asm/i387.h in asm/xsave.h
      x86, xsave: Use xsaveopt in context-switch path when supported
      x86, xsave: Sync xsave memory layout with its header for user handling
      x86, xsave: Track the offset, size of state in the xsave layout

commit f7ddc2b6cd880a259db338925d03bdc01f1d26c1
Merge: a5e11599da95 14671386dcba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 10:06:28 2010 -0700

    Merge branch 'x86-mrst-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mrst-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, mrst: make mrst_timer_options an enum
      x86, mrst: make mrst_identify_cpu() an inline returning enum
      x86, mrst: add more timer config options
      x86, mrst: add cpu type detection
      x86: detect scattered cpuid features earlier

commit 0e49bf66d2ca649b167428adddbbbe9d9bd4894c
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jul 21 19:03:52 2010 +0200

    x86, xsave: Separate fpu and xsave initialization
    
    As xsave also supports other than fpu features, it should be
    initialized independently of the fpu. This patch moves this out of fpu
    initialization.
    
    There is also a lot of cross referencing between fpu and xsave
    code. This patch reduces this by making xsave_cntxt_init() and
    init_thread_xstate() static functions.
    
    The patch moves the cpu_has_xsave check at the beginning of
    xsave_init(). All other checks may removed then.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1279731838-1522-2-git-send-email-robert.richter@amd.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 40561085d4f3..94c36c7ac183 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1210,6 +1210,7 @@ void __cpuinit cpu_init(void)
 	dbg_restore_debug_regs();
 
 	fpu_init();
+	xsave_init();
 
 	raw_local_save_flags(kernel_eflags);
 
@@ -1270,6 +1271,7 @@ void __cpuinit cpu_init(void)
 	clear_used_math();
 	mxcsr_feature_mask_init();
 
+	fpu_init();
 	xsave_init();
 }
 #endif

commit 82d4150cec83b9775f84810b39a1c0b91585d429
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 20 20:50:51 2010 +0200

    x86, xsave: Move boot cpu initialization to xsave_init()
    
    This patch moves boot cpu initialization to xsave_init(). Now all cpus
    are initialized in one single function.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1279651857-24639-5-git-send-email-robert.richter@amd.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 26804b2986b8..40561085d4f3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1270,12 +1270,6 @@ void __cpuinit cpu_init(void)
 	clear_used_math();
 	mxcsr_feature_mask_init();
 
-	/*
-	 * Boot processor to setup the FP and extended state context info.
-	 */
-	if (!smp_processor_id())
-		init_thread_xstate();
-
 	xsave_init();
 }
 #endif

commit db10db48b2c530def21bfd76d576702c7df7f620
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 20 20:50:49 2010 +0200

    x86, xsave: 32/64 bit boot cpu check unification in initialization
    
    Boot cpu id is always 0, thus simplifying and unifying boot cpu check.
    
    boot_cpu_id is there for historical reasons and was renamed to
    boot_cpu_physical_apicid in patch:
    
     c70dcb7 x86: change boot_cpu_id to boot_cpu_physical_apicid
    
    However, there are some remaining occurrences of boot_cpu_id that are
    never touched in the kernel and thus its value is always 0.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1279651857-24639-3-git-send-email-robert.richter@amd.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3f715efc594d..26804b2986b8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1273,7 +1273,7 @@ void __cpuinit cpu_init(void)
 	/*
 	 * Boot processor to setup the FP and extended state context info.
 	 */
-	if (smp_processor_id() == boot_cpu_id)
+	if (!smp_processor_id())
 		init_thread_xstate();
 
 	xsave_init();

commit 6bad06b768920e278c7cedfdda56a0b4c6a35ee9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Jul 19 16:05:52 2010 -0700

    x86, xsave: Use xsaveopt in context-switch path when supported
    
    xsaveopt is a more optimized form of xsave specifically designed
    for the context switch usage. xsaveopt doesn't save the state that's not
    modified from the prior xrstor. And if a specific feature state gets
    modified to the init state, then xsaveopt just updates the header bit
    in the xsave memory layout without updating the corresponding memory
    layout.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <20100719230205.604014179@sbs-t61.sc.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c7358303d8cd..3f715efc594d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -140,10 +140,18 @@ EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 static int __init x86_xsave_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
+	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
 	return 1;
 }
 __setup("noxsave", x86_xsave_setup);
 
+static int __init x86_xsaveopt_setup(char *s)
+{
+	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
+	return 1;
+}
+__setup("noxsaveopt", x86_xsaveopt_setup);
+
 #ifdef CONFIG_X86_32
 static int cachesize_override __cpuinitdata = -1;
 static int disable_x86_serial_nr __cpuinitdata = 1;

commit bdc802dcca1709b01988d57e91f9f35ce1609fcc
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Jul 7 17:29:18 2010 -0700

    x86, cpu: Support the features flags in new CPUID leaf 7
    
    Intel has defined CPUID leaf 7 as the next set of feature flags (see
    the AVX specification, version 007).  Add support for this new feature
    flags word.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    LKML-Reference: <tip-*@vger.kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 68e4a6f2211e..c7358303d8cd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -551,6 +551,16 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[4] = excap;
 	}
 
+	/* Additional Intel-defined flags: level 0x00000007 */
+	if (c->cpuid_level >= 0x00000007) {
+		u32 eax, ebx, ecx, edx;
+
+		cpuid_count(0x00000007, 0, &eax, &ebx, &ecx, &edx);
+
+		if (eax > 0)
+			c->x86_capability[9] = ebx;
+	}
+
 	/* AMD-defined flags: level 0x80000001 */
 	xlvl = cpuid_eax(0x80000000);
 	c->extended_cpuid_level = xlvl;

commit e534c7c5f8d6e9fc46f57fab067c7e48d8ceb172
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Wed May 26 14:44:58 2010 -0700

    numa: x86_64: use generic percpu var numa_node_id() implementation
    
    x86 arch specific changes to use generic numa_node_id() based on generic
    percpu variable infrastructure.  Back out x86's custom version of
    numa_node_id()
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cc83a002786e..68e4a6f2211e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1121,9 +1121,9 @@ void __cpuinit cpu_init(void)
 	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
-	if (cpu != 0 && percpu_read(node_number) == 0 &&
-	    cpu_to_node(cpu) != NUMA_NO_NODE)
-		percpu_write(node_number, cpu_to_node(cpu));
+	if (cpu != 0 && percpu_read(numa_node) == 0 &&
+	    early_cpu_to_node(cpu) != NUMA_NO_NODE)
+		set_numa_node(early_cpu_to_node(cpu));
 #endif
 
 	me = current;

commit 0bb9fef9134cf4fdcfce02f9adc22d3d0725cc29
Author: Jason Wessel <jason.wessel@windriver.com>
Date:   Thu May 20 21:04:30 2010 -0500

    x86,early dr regs,kgdb: Allow kernel debugger early dr register access
    
    If the kernel debugger was configured, attached and started with
    kgdbwait, the hardware breakpoint registers should get restored by the
    kgdb code which is managing the dr registers.
    
    CC: x86@kernel.org
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Ingo Molnar <mingo@redhat.com>
    CC: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c1c00d0b1692..cc83a002786e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1084,6 +1084,20 @@ static void clear_all_debug_regs(void)
 	}
 }
 
+#ifdef CONFIG_KGDB
+/*
+ * Restore debug regs if using kgdbwait and you have a kernel debugger
+ * connection established.
+ */
+static void dbg_restore_debug_regs(void)
+{
+	if (unlikely(kgdb_connected && arch_kgdb_ops.correct_hw_break))
+		arch_kgdb_ops.correct_hw_break();
+}
+#else /* ! CONFIG_KGDB */
+#define dbg_restore_debug_regs()
+#endif /* ! CONFIG_KGDB */
+
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT
@@ -1174,18 +1188,8 @@ void __cpuinit cpu_init(void)
 	load_TR_desc();
 	load_LDT(&init_mm.context);
 
-#ifdef CONFIG_KGDB
-	/*
-	 * If the kgdb is connected no debug regs should be altered.  This
-	 * is only applicable when KGDB and a KGDB I/O module are built
-	 * into the kernel and you are using early debugging with
-	 * kgdbwait. KGDB will control the kernel HW breakpoint registers.
-	 */
-	if (kgdb_connected && arch_kgdb_ops.correct_hw_break)
-		arch_kgdb_ops.correct_hw_break();
-	else
-#endif
-		clear_all_debug_regs();
+	clear_all_debug_regs();
+	dbg_restore_debug_regs();
 
 	fpu_init();
 
@@ -1239,6 +1243,7 @@ void __cpuinit cpu_init(void)
 #endif
 
 	clear_all_debug_regs();
+	dbg_restore_debug_regs();
 
 	/*
 	 * Force FPU initialization:

commit 1dedefd1a066a795a87afca9c0236e1a94de9bf6
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Wed May 19 12:01:23 2010 -0700

    x86: detect scattered cpuid features earlier
    
    Some extra CPU features such as ARAT is needed in early boot so
    that x86_init function pointers can be set up properly.
    http://lkml.org/lkml/2010/5/18/519
    At start_kernel() level, this patch moves init_scattered_cpuid_features()
    from check_bugs() to setup_arch() -> early_cpu_init() which is earlier than
    platform specific x86_init layer setup. Suggested by HPA.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    LKML-Reference: <1274295685-6774-2-git-send-email-jacob.jun.pan@linux.intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c1c00d0b1692..284bf89ddae2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -576,6 +576,7 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 	if (c->extended_cpuid_level >= 0x80000007)
 		c->x86_power = cpuid_edx(0x80000007);
 
+	init_scattered_cpuid_features(c);
 }
 
 static void __cpuinit identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
@@ -731,7 +732,6 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 
 	get_model_name(c); /* Default name */
 
-	init_scattered_cpuid_features(c);
 	detect_nopl(c);
 }
 

commit c9ad488289144ae5ef53b012e15895ef1f5e4bb6
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 6 11:45:45 2010 +0300

    x86: Eliminate TS_XSAVE
    
    The fpu code currently uses current->thread_info->status & TS_XSAVE as
    a way to distinguish between XSAVE capable processors and older processors.
    The decision is not really task specific; instead we use the task status to
    avoid a global memory reference - the value should be the same across all
    threads.
    
    Eliminate this tie-in into the task structure by using an alternative
    instruction keyed off the XSAVE cpu feature; this results in shorter and
    faster code, without introducing a global memory reference.
    
    [ hpa: in the future, this probably should use an asm jmp ]
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1273135546-29690-2-git-send-email-avi@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4868e4a951ee..c1c00d0b1692 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1243,10 +1243,7 @@ void __cpuinit cpu_init(void)
 	/*
 	 * Force FPU initialization:
 	 */
-	if (cpu_has_xsave)
-		current_thread_info()->status = TS_XSAVE;
-	else
-		current_thread_info()->status = 0;
+	current_thread_info()->status = 0;
 	clear_used_math();
 	mxcsr_feature_mask_init();
 

commit 75b08038ceb62f3bd8935346679920f97c3cf9f6
Merge: fb1beb29b5c5 70fe440718d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 14 12:36:46 2009 -0800

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, mce: Clean up thermal init by introducing intel_thermal_supported()
      x86, mce: Thermal monitoring depends on APIC being enabled
      x86: Gart: fix breakage due to IOMMU initialization cleanup
      x86: Move swiotlb initialization before dma32_free_bootmem
      x86: Fix build warning in arch/x86/mm/mmio-mod.c
      x86: Remove usedac in feature-removal-schedule.txt
      x86: Fix duplicated UV BAU interrupt vector
      nvram: Fix write beyond end condition; prove to gcc copy is safe
      mm: Adjust do_pages_stat() so gcc can see copy_from_user() is safe
      x86: Limit the number of processor bootup messages
      x86: Remove enabling x2apic message for every CPU
      doc: Add documentation for bootloader_{type,version}
      x86, msr: Add support for non-contiguous cpumasks
      x86: Use find_e820() instead of hard coded trampoline address
      x86, AMD: Fix stale cpuid4_info shared_map data in shared_cpu_map cpumasks
    
    Trivial percpu-naming-introduced conflicts in arch/x86/kernel/cpu/intel_cacheinfo.c

commit d0316554d3586cbea60592a41391b5def2553d6f
Merge: fb0bbb92d42d 51e99be00ce2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 14 09:58:24 2009 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (34 commits)
      m68k: rename global variable vmalloc_end to m68k_vmalloc_end
      percpu: add missing per_cpu_ptr_to_phys() definition for UP
      percpu: Fix kdump failure if booted with percpu_alloc=page
      percpu: make misc percpu symbols unique
      percpu: make percpu symbols in ia64 unique
      percpu: make percpu symbols in powerpc unique
      percpu: make percpu symbols in x86 unique
      percpu: make percpu symbols in xen unique
      percpu: make percpu symbols in cpufreq unique
      percpu: make percpu symbols in oprofile unique
      percpu: make percpu symbols in tracer unique
      percpu: make percpu symbols under kernel/ and mm/ unique
      percpu: remove some sparse warnings
      percpu: make alloc_percpu() handle array types
      vmalloc: fix use of non-existent percpu variable in put_cpu_var()
      this_cpu: Use this_cpu_xx in trace_functions_graph.c
      this_cpu: Use this_cpu_xx for ftrace
      this_cpu: Use this_cpu_xx in nmi handling
      this_cpu: Use this_cpu operations in RCU
      this_cpu: Use this_cpu ops for VM statistics
      ...
    
    Fix up trivial (famous last words) global per-cpu naming conflicts in
            arch/x86/kvm/svm.c
            mm/slab.c

commit 2eaad1fddd7450a48ad464229775f97fbfe8af36
Author: Mike Travis <travis@sgi.com>
Date:   Thu Dec 10 17:19:36 2009 -0800

    x86: Limit the number of processor bootup messages
    
    When there are a large number of processors in a system, there
    is an excessive amount of messages sent to the system console.
    It's estimated that with 4096 processors in a system, and the
    console baudrate set to 56K, the startup messages will take
    about 84 minutes to clear the serial port.
    
    This set of patches limits the number of repetitious messages
    which contain no additional information.  Much of this information
    is obtainable from the /proc and /sysfs.   Some of the messages
    are also sent to the kernel log buffer as KERN_DEBUG messages so
    dmesg can be used to examine more closely any details specific to
    a problem.
    
    The new cpu bootup sequence for system_state == SYSTEM_BOOTING:
    
    Booting Node   0, Processors  #1 #2 #3 #4 #5 #6 #7 Ok.
    Booting Node   1, Processors  #8 #9 #10 #11 #12 #13 #14 #15 Ok.
    ...
    Booting Node   3, Processors  #56 #57 #58 #59 #60 #61 #62 #63 Ok.
    Brought up 64 CPUs
    
    After the system is running, a single line boot message is displayed
    when CPU's are hotplugged on:
    
        Booting Node %d Processor %d APIC 0x%x
    
    Status of the following lines:
    
        CPU: Physical Processor ID:         printed once (for boot cpu)
        CPU: Processor Core ID:             printed once (for boot cpu)
        CPU: Hyper-Threading is disabled    printed once (for boot cpu)
        CPU: Thermal monitoring enabled     printed once (for boot cpu)
        CPU %d/0x%x -> Node %d:             removed
        CPU %d is now offline:              only if system_state == RUNNING
        Initializing CPU#%d:                KERN_DEBUG
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    LKML-Reference: <4B219E28.8080601@sgi.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c1afa990a6c8..0ee9a3254eec 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -427,6 +427,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 #ifdef CONFIG_X86_HT
 	u32 eax, ebx, ecx, edx;
 	int index_msb, core_bits;
+	static bool printed;
 
 	if (!cpu_has(c, X86_FEATURE_HT))
 		return;
@@ -442,7 +443,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 	smp_num_siblings = (ebx & 0xff0000) >> 16;
 
 	if (smp_num_siblings == 1) {
-		printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
+		printk_once(KERN_INFO "CPU0: Hyper-Threading is disabled\n");
 		goto out;
 	}
 
@@ -469,11 +470,12 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 				       ((1 << core_bits) - 1);
 
 out:
-	if ((c->x86_max_cores * smp_num_siblings) > 1) {
+	if (!printed && (c->x86_max_cores * smp_num_siblings) > 1) {
 		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
 		       c->phys_proc_id);
 		printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
 		       c->cpu_core_id);
+		printed = 1;
 	}
 #endif
 }
@@ -1115,7 +1117,7 @@ void __cpuinit cpu_init(void)
 	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask))
 		panic("CPU#%d already initialized!\n", cpu);
 
-	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
+	pr_debug("Initializing CPU#%d\n", cpu);
 
 	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 

commit e33c01972239fee4696679ae5f7d1f340f424999
Merge: 343036cea285 ccef086454d4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 13:27:33 2009 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (36 commits)
      x86, mm: Correct the implementation of is_untracked_pat_range()
      x86/pat: Trivial: don't create debugfs for memtype if pat is disabled
      x86, mtrr: Fix sorting of mtrr after subtracting
      x86: Move find_smp_config() earlier and avoid bootmem usage
      x86, platform: Change is_untracked_pat_range() to bool; cleanup init
      x86: Change is_ISA_range() into an inline function
      x86, mm: is_untracked_pat_range() takes a normal semiclosed range
      x86, mm: Call is_untracked_pat_range() rather than is_ISA_range()
      x86: UV SGI: Don't track GRU space in PAT
      x86: SGI UV: Fix BAU initialization
      x86, numa: Use near(er) online node instead of roundrobin for NUMA
      x86, numa, bootmem: Only free bootmem on NUMA failure path
      x86: Change crash kernel to reserve via reserve_early()
      x86: Eliminate redundant/contradicting cache line size config options
      x86: When cleaning MTRRs, do not fold WP into UC
      x86: remove "extern" from function prototypes in <asm/proto.h>
      x86, mm: Report state of NX protections during boot
      x86, mm: Clean up and simplify NX enablement
      x86, pageattr: Make set_memory_(x|nx) aware of NX support
      x86, sleep: Always save the value of EFER
      ...
    
    Fix up conflicts (added both iommu_shutdown and is_untracked_pat_range)
    to 'struct x86_platform_ops') in
            arch/x86/include/asm/x86_init.h
            arch/x86/kernel/x86_init.c

commit 6ec22f9b037fc0c2e00ddb7023fad279c365324d
Merge: 83be7d764dc4 9b3660a55a90
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 15:33:27 2009 -0800

    Merge branch 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Limit number of per cpu TSC sync messages
      x86: dumpstack, 64-bit: Disable preemption when walking the IRQ/exception stacks
      x86: dumpstack: Clean up the x86_stack_ids[][] initalization and other details
      x86, cpu: mv display_cacheinfo -> cpu_detect_cache_sizes
      x86: Suppress stack overrun message for init_task
      x86: Fix cpu_devs[] initialization in early_cpu_init()
      x86: Remove CPU cache size output for non-Intel too
      x86: Minimise printk spew from per-vendor init code
      x86: Remove the CPU cache size printk's
      cpumask: Avoid cpumask_t in arch/x86/kernel/apic/nmi.c
      x86: Make sure we also print a Code: line for show_regs()

commit 27c13ecec4d8856687b50b959e1146845b478f95
Author: Borislav Petkov <petkovbb@googlemail.com>
Date:   Sat Nov 21 14:01:45 2009 +0100

    x86, cpu: mv display_cacheinfo -> cpu_detect_cache_sizes
    
    display_cacheinfo() doesn't display anything anymore and it is used to
    detect CPU cache sizes. Rename it accordingly.
    
    Signed-off-by: Borislav Petkov <petkovbb@gmail.com>
    LKML-Reference: <20091121130145.GA31357@liondog.tnic>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 61242a56c2d6..9bf845dc8055 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -61,7 +61,7 @@ void __init setup_cpu_local_masks(void)
 static void __cpuinit default_init(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_64
-	display_cacheinfo(c);
+	cpu_detect_cache_sizes(c);
 #else
 	/* Not much we can do here... */
 	/* Check if at least it has cpuid */
@@ -383,7 +383,7 @@ static void __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	}
 }
 
-void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
+void __cpuinit cpu_detect_cache_sizes(struct cpuinfo_x86 *c)
 {
 	unsigned int n, dummy, ebx, ecx, edx, l2size;
 

commit 4763ed4d45522b876c97e1f7f4b659d211f75571
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri Nov 13 15:28:16 2009 -0800

    x86, mm: Clean up and simplify NX enablement
    
    The 32- and 64-bit code used very different mechanisms for enabling
    NX, but even the 32-bit code was enabling NX in head_32.S if it is
    available.  Furthermore, we had a bewildering collection of tests for
    the available of NX.
    
    This patch:
    
    a) merges the 32-bit set_nx() and the 64-bit check_efer() function
       into a single x86_configure_nx() function.  EFER control is left
       to the head code.
    
    b) eliminates the nx_enabled variable entirely.  Things that need to
       test for NX enablement can verify __supported_pte_mask directly,
       and cpu_has_nx gives the supported status of NX.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    LKML-Reference: <1258154897-6770-5-git-send-email-hpa@zytor.com>
    Acked-by: Kees Cook <kees.cook@canonical.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cc25c2b4a567..18346da8c594 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1136,7 +1136,7 @@ void __cpuinit cpu_init(void)
 	wrmsrl(MSR_KERNEL_GS_BASE, 0);
 	barrier();
 
-	check_efer();
+	x86_configure_nx();
 	if (cpu != 0)
 		enable_x2apic();
 

commit 31c997cac76e62918858a432fff6e43fd48425f9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 14 10:34:41 2009 +0100

    x86: Fix cpu_devs[] initialization in early_cpu_init()
    
    Yinghai Lu noticed that this commit:
    
      0388423: x86: Minimise printk spew from per-vendor init code
    
    mistakenly left out the initialization of cpu_devs[] in the
    !PROCESSOR_SELECT case. Fix it.
    
    Reported-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Dave Jones <davej@redhat.com>
    LKML-Reference: <20091113203000.GA19160@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9db1e2425c27..61242a56c2d6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -651,28 +651,34 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 void __init early_cpu_init(void)
 {
-#ifdef PROCESSOR_SELECT
 	const struct cpu_dev *const *cdev;
 	int count = 0;
 
+#ifdef PROCESSOR_SELECT
 	printk(KERN_INFO "KERNEL supported cpus:\n");
+#endif
+
 	for (cdev = __x86_cpu_dev_start; cdev < __x86_cpu_dev_end; cdev++) {
 		const struct cpu_dev *cpudev = *cdev;
-		unsigned int j;
 
 		if (count >= X86_VENDOR_NUM)
 			break;
 		cpu_devs[count] = cpudev;
 		count++;
 
-		for (j = 0; j < 2; j++) {
-			if (!cpudev->c_ident[j])
-				continue;
-			printk(KERN_INFO "  %s %s\n", cpudev->c_vendor,
-				cpudev->c_ident[j]);
+#ifdef PROCESSOR_SELECT
+		{
+			unsigned int j;
+
+			for (j = 0; j < 2; j++) {
+				if (!cpudev->c_ident[j])
+					continue;
+				printk(KERN_INFO "  %s %s\n", cpudev->c_vendor,
+					cpudev->c_ident[j]);
+			}
 		}
-	}
 #endif
+	}
 	early_identify_cpu(&boot_cpu_data);
 }
 

commit b01c845f0f2e3f9e54e6a78d5d56895f5b95e818
Author: Roland Dreier <rdreier@cisco.com>
Date:   Fri Nov 13 14:38:26 2009 -0800

    x86: Remove CPU cache size output for non-Intel too
    
    As Dave Jones said about the output in intel_cacheinfo.c: "They
    aren't useful, and pollute the dmesg output a lot (especially on
    machines with many cores).  Also the same information can be
    trivially found out from userspace."
    
    Give the generic display_cacheinfo() function the same treatment.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>
    Acked-by: Dave Jones <davej@redhat.com>
    Cc: Mike Travis <travis@sgi.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Randy Dunlap <rdunlap@xenotime.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@suse.de>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <adaocn6dp99.fsf_-_@roland-alpha.cisco.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 617a29f95b3c..9db1e2425c27 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -391,8 +391,6 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 
 	if (n >= 0x80000005) {
 		cpuid(0x80000005, &dummy, &ebx, &ecx, &edx);
-		printk(KERN_INFO "CPU: L1 I Cache: %dK (%d bytes/line), D cache %dK (%d bytes/line)\n",
-				edx>>24, edx&0xFF, ecx>>24, ecx&0xFF);
 		c->x86_cache_size = (ecx>>24) + (edx>>24);
 #ifdef CONFIG_X86_64
 		/* On K8 L1 TLB is inclusive, so don't count it */
@@ -422,9 +420,6 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 #endif
 
 	c->x86_cache_size = l2size;
-
-	printk(KERN_INFO "CPU: L2 Cache: %dK (%d bytes/line)\n",
-			l2size, ecx & 0xFF);
 }
 
 void __cpuinit detect_ht(struct cpuinfo_x86 *c)

commit 0388423dba2217b4e5b6c61690b0506d13b25a49
Author: Dave Jones <davej@redhat.com>
Date:   Fri Nov 13 15:30:00 2009 -0500

    x86: Minimise printk spew from per-vendor init code
    
    In the default case where the kernel supports all CPU vendors,
    we currently print out a bunch of not useful messages on every
    system.
    
    32-bit:
    KERNEL supported cpus:
      Intel GenuineIntel
      AMD AuthenticAMD
      NSC Geode by NSC
      Cyrix CyrixInstead
      Centaur CentaurHauls
      Transmeta GenuineTMx86
      Transmeta TransmetaCPU
      UMC UMC UMC UMC
    
    64-bit:
    KERNEL supported cpus:
      Intel GenuineIntel
      AMD AuthenticAMD
      Centaur CentaurHauls
    
    Given that "what CPUs does the kernel support" isn't useful for
    the "support everything" case, we can suppress these printk's.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    LKML-Reference: <20091113203000.GA19160@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cc25c2b4a567..617a29f95b3c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -656,6 +656,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 void __init early_cpu_init(void)
 {
+#ifdef PROCESSOR_SELECT
 	const struct cpu_dev *const *cdev;
 	int count = 0;
 
@@ -676,7 +677,7 @@ void __init early_cpu_init(void)
 				cpudev->c_ident[j]);
 		}
 	}
-
+#endif
 	early_identify_cpu(&boot_cpu_data);
 }
 

commit a2202aa29289db64ca7988b12343158b67b27f10
Author: Yong Wang <yong.y.wang@linux.intel.com>
Date:   Tue Nov 10 09:38:24 2009 +0800

    x86: Under BIOS control, restore AP's APIC_LVTTHMR to the BSP value
    
    On platforms where the BIOS handles the thermal monitor interrupt,
    APIC_LVTTHMR on each logical CPU is programmed to generate a SMI
    and OS must not touch it.
    
    Unfortunately AP bringup sequence using INIT-SIPI-SIPI clears all
    the LVT entries except the mask bit. Essentially this results in
    all LVT entries including the thermal monitoring interrupt set
    to masked (clearing the bios programmed value for APIC_LVTTHMR).
    
    And this leads to kernel take over the thermal monitoring
    interrupt on AP's but not on BSP (leaving the bios programmed
    value only on BSP).
    
    As a result of this, we have seen system hangs when the thermal
    monitoring interrupt is generated.
    
    Fix this by reading the initial value of thermal LVT entry on
    BSP and if bios has taken over the control, then program the
    same value on all AP's and leave the thermal monitoring
    interrupt control on all the logical cpu's to the bios.
    
    Signed-off-by: Yong Wang <yong.y.wang@intel.com>
    Reviewed-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Arjan van de Ven <arjan@infradead.org>
    LKML-Reference: <20091110013824.GA24940@ywang-moblin2.bj.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: stable@kernel.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4df69a38be57..9053be5d95cd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -837,10 +837,8 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
 	}
 
-#ifdef CONFIG_X86_MCE
 	/* Init Machine Check Exception if available. */
 	mcheck_cpu_init(c);
-#endif
 
 	select_idle_routine(c);
 

commit 0fe1e009541e925adc1748a605d8b66188e4b2ab
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 29 22:34:14 2009 +0900

    percpu: make percpu symbols in x86 unique
    
    This patch updates percpu related symbols in x86 such that percpu
    symbols are unique and don't clash with local symbols.  This serves
    two purposes of decreasing the possibility of global percpu symbol
    collision and allowing dropping per_cpu__ prefix from percpu symbols.
    
    * arch/x86/kernel/cpu/common.c: rename local variable to avoid collision
    
    * arch/x86/kvm/svm.c: s/svm_data/sd/ for local variables to avoid collision
    
    * arch/x86/kernel/cpu/cpu_debug.c: s/cpu_arr/cpud_arr/
                                       s/priv_arr/cpud_priv_arr/
                                       s/cpu_priv_count/cpud_priv_count/
    
    * arch/x86/kernel/cpu/intel_cacheinfo.c: s/cpuid4_info/ici_cpuid4_info/
                                             s/cache_kobject/ici_cache_kobject/
                                             s/index_kobject/ici_index_kobject/
    
    * arch/x86/kernel/ds.c: s/cpu_context/cpu_ds_context/
    
    Partly based on Rusty Russell's "alloc_percpu: rename percpu vars
    which cause name clashes" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: (kvm) Avi Kivity <avi@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: x86@kernel.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cc25c2b4a567..3192f22f2fdd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1093,7 +1093,7 @@ static void clear_all_debug_regs(void)
 
 void __cpuinit cpu_init(void)
 {
-	struct orig_ist *orig_ist;
+	struct orig_ist *oist;
 	struct task_struct *me;
 	struct tss_struct *t;
 	unsigned long v;
@@ -1102,7 +1102,7 @@ void __cpuinit cpu_init(void)
 
 	cpu = stack_smp_processor_id();
 	t = &per_cpu(init_tss, cpu);
-	orig_ist = &per_cpu(orig_ist, cpu);
+	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
 	if (cpu != 0 && percpu_read(node_number) == 0 &&
@@ -1143,12 +1143,12 @@ void __cpuinit cpu_init(void)
 	/*
 	 * set up and load the per-CPU TSS
 	 */
-	if (!orig_ist->ist[0]) {
+	if (!oist->ist[0]) {
 		char *estacks = per_cpu(exception_stacks, cpu);
 
 		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
 			estacks += exception_stack_sizes[v];
-			orig_ist->ist[v] = t->x86_tss.ist[v] =
+			oist->ist[v] = t->x86_tss.ist[v] =
 					(unsigned long)estacks;
 		}
 	}

commit 5e09954a9acc3b435ffe318b95afd3c02fae069f
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Fri Oct 16 12:31:32 2009 +0200

    x86, mce: Fix up MCE naming nomenclature
    
    Prefix global/setup routines with "mcheck_" thus differentiating
    from the internal facilities prefixed with "mce_". Also, prefix
    the per cpu calls with mcheck_cpu and rename them to reflect the
    MCE setup hierarchy of calls better.
    
    There should be no functionality change resulting from this
    patch.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    LKML-Reference: <1255689093-26921-1-git-send-email-borislav.petkov@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cc25c2b4a567..4df69a38be57 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -839,7 +839,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 #ifdef CONFIG_X86_MCE
 	/* Init Machine Check Exception if available. */
-	mcheck_init(c);
+	mcheck_cpu_init(c);
 #endif
 
 	select_idle_routine(c);

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2fea97eccf77..cc25c2b4a567 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -13,7 +13,7 @@
 #include <linux/io.h>
 
 #include <asm/stackprotector.h>
-#include <asm/perf_counter.h>
+#include <asm/perf_event.h>
 #include <asm/mmu_context.h>
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
@@ -869,7 +869,7 @@ void __init identify_boot_cpu(void)
 #else
 	vgetcpu_set_mode();
 #endif
-	init_hw_perf_counters();
+	init_hw_perf_events();
 }
 
 void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)

commit 5ac7687860dbfd3dd90e09d2c10dd31de91f20c2
Author: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
Date:   Sun Sep 20 16:34:24 2009 +0530

    includecheck fix: x86, cpu/common.c
    
    fix the following 'make includecheck' warning:
    
      arch/x86/kernel/cpu/common.c: linux/smp.h is included more than once.
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <1252087783.6385.10.camel@ht.satnam>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2055fc2b2e6b..2fea97eccf77 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -34,7 +34,6 @@
 #include <asm/mce.h>
 #include <asm/msr.h>
 #include <asm/pat.h>
-#include <linux/smp.h>
 
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/uv/uv.h>

commit 55e0715f612f19b44c17497929091df2f3357e5d
Merge: 7dfd54a905be bdf977b37418
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 08:01:28 2009 -0700

    Merge branch 'x86-percpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-percpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, percpu: Collect hot percpu variables into one cacheline
      x86, percpu: Fix DECLARE/DEFINE_PER_CPU_PAGE_ALIGNED()
      x86, percpu: Add 'percpu_read_stable()' interface for cacheable accesses

commit 15b0404272e1513940223cf9eefadfd22804a060
Merge: 9b74aec028e7 d535e4319a2e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 07:56:43 2009 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Make memtype_seq_ops const
      x86: uv: Clean up uv_ptc_init(), use proc_create()
      x86: Use printk_once()
      x86/cpu: Clean up various files a bit
      x86: Remove duplicated #include
      x86, ipi: Clean up safe_smp_processor_id() by using the cpu_has_apic() macro helper
      x86: Clean up idt_descr and idt_tableby using NR_VECTORS instead of hardcoded number
      x86: Further clean up of mtrr/generic.c
      x86: Clean up mtrr/main.c
      x86: Clean up mtrr/state.c
      x86: Clean up mtrr/mtrr.h
      x86: Clean up mtrr/if.c
      x86: Clean up mtrr/generic.c
      x86: Clean up mtrr/cyrix.c
      x86: Clean up mtrr/cleanup.c
      x86: Clean up mtrr/centaur.c
      x86: Clean up mtrr/amd.c:
      x86: ds.c fix invalid assignment

commit 53f824520b6d84ca5b4a8fd71addc91dbf64357e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Sep 3 14:31:44 2009 -0700

    x86/i386: Put aligned stack-canary in percpu shared_aligned section
    
    Pack aligned things together into a special section to minimize
    padding holes.
    
    Suggested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Tejun Heo <tj@kernel.org>
    LKML-Reference: <4AA035C0.9070202@goop.org>
    [ queued up in tip:x86/asm because it depends on this commit:
      x86/i386: Make sure stack-protector segment base is cache aligned ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7d84bc4c1188..f23e236391a3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1043,7 +1043,7 @@ DEFINE_PER_CPU(struct orig_ist, orig_ist);
 #else	/* CONFIG_X86_64 */
 
 #ifdef CONFIG_CC_STACKPROTECTOR
-DEFINE_PER_CPU(struct stack_canary, stack_canary) ____cacheline_aligned;
+DEFINE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif
 
 /* Make sure %fs and %gs are initialized properly in idle threads */

commit 1ea0d14e480c245683927eecc03a70faf06e80c8
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Sep 3 12:27:15 2009 -0700

    x86/i386: Make sure stack-protector segment base is cache aligned
    
    The Intel Optimization Reference Guide says:
    
            In Intel Atom microarchitecture, the address generation unit
            assumes that the segment base will be 0 by default. Non-zero
            segment base will cause load and store operations to experience
            a delay.
                    - If the segment base isn't aligned to a cache line
                      boundary, the max throughput of memory operations is
                      reduced to one [e]very 9 cycles.
            [...]
            Assembly/Compiler Coding Rule 15. (H impact, ML generality)
            For Intel Atom processors, use segments with base set to 0
            whenever possible; avoid non-zero segment base address that is
            not aligned to cache line boundary at all cost.
    
    We can't avoid having a non-zero base for the stack-protector
    segment, but we can make it cache-aligned.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: <stable@kernel.org>
    LKML-Reference: <4AA01893.6000507@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ced07ba5e937..7d84bc4c1188 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1043,7 +1043,7 @@ DEFINE_PER_CPU(struct orig_ist, orig_ist);
 #else	/* CONFIG_X86_64 */
 
 #ifdef CONFIG_CC_STACKPROTECTOR
-DEFINE_PER_CPU(unsigned long, stack_canary);
+DEFINE_PER_CPU(struct stack_canary, stack_canary) ____cacheline_aligned;
 #endif
 
 /* Make sure %fs and %gs are initialized properly in idle threads */

commit e8a2eb47e6ca03d4a4f98f0beef73720c5dddc0c
Merge: 8b5a10fc6fd0 c62e43202e7c
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Tue Aug 25 15:40:29 2009 -0700

    Merge commit 'origin/x86/urgent' into x86/asm

commit 5f9ece02401116b29eb04396b99ea092acb75dd8
Merge: 9f51e24ee8b5 422bef879e84
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 24 12:25:44 2009 +0200

    Merge commit 'v2.6.31-rc7' into x86/cleanups
    
    Merge reason: we were on -rc1 before - go up to -rc7
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e8055139d996e85722984968472868d6dccb1490
Author: Ondrej Zary <linux@rainbow-software.org>
Date:   Tue Aug 11 20:00:11 2009 +0200

    x86: Fix oops in identify_cpu() on CPUs without CPUID
    
    Kernel is broken for x86 CPUs without CPUID since 2.6.28. It
    crashes with NULL pointer dereference in identify_cpu():
    
    766        generic_identify(c);
    767
    768-->     if (this_cpu->c_identify)
    769               this_cpu->c_identify(c);
    
    this_cpu is NULL. This is because it's only initialized in
    get_cpu_vendor() function, which is not called if the CPU has
    no CPUID instruction.
    
    Signed-off-by: Ondrej Zary <linux@rainbow-software.org>
    LKML-Reference: <200908112000.15993.linux@rainbow-software.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f1961c07af9a..5ce60a88027b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -59,7 +59,30 @@ void __init setup_cpu_local_masks(void)
 	alloc_bootmem_cpumask_var(&cpu_sibling_setup_mask);
 }
 
-static const struct cpu_dev *this_cpu __cpuinitdata;
+static void __cpuinit default_init(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_64
+	display_cacheinfo(c);
+#else
+	/* Not much we can do here... */
+	/* Check if at least it has cpuid */
+	if (c->cpuid_level == -1) {
+		/* No cpuid. It must be an ancient CPU */
+		if (c->x86 == 4)
+			strcpy(c->x86_model_id, "486");
+		else if (c->x86 == 3)
+			strcpy(c->x86_model_id, "386");
+	}
+#endif
+}
+
+static const struct cpu_dev __cpuinitconst default_cpu = {
+	.c_init		= default_init,
+	.c_vendor	= "Unknown",
+	.c_x86_vendor	= X86_VENDOR_UNKNOWN,
+};
+
+static const struct cpu_dev *this_cpu __cpuinitdata = &default_cpu;
 
 DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 #ifdef CONFIG_X86_64
@@ -332,29 +355,6 @@ void switch_to_new_gdt(int cpu)
 
 static const struct cpu_dev *__cpuinitdata cpu_devs[X86_VENDOR_NUM] = {};
 
-static void __cpuinit default_init(struct cpuinfo_x86 *c)
-{
-#ifdef CONFIG_X86_64
-	display_cacheinfo(c);
-#else
-	/* Not much we can do here... */
-	/* Check if at least it has cpuid */
-	if (c->cpuid_level == -1) {
-		/* No cpuid. It must be an ancient CPU */
-		if (c->x86 == 4)
-			strcpy(c->x86_model_id, "486");
-		else if (c->x86 == 3)
-			strcpy(c->x86_model_id, "386");
-	}
-#endif
-}
-
-static const struct cpu_dev __cpuinitconst default_cpu = {
-	.c_init	= default_init,
-	.c_vendor = "Unknown",
-	.c_x86_vendor = X86_VENDOR_UNKNOWN,
-};
-
 static void __cpuinit get_model_name(struct cpuinfo_x86 *c)
 {
 	unsigned int *v;

commit 72c4d8530244264317a662de9a55cc47e6c8e9df
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 3 08:47:07 2009 +0200

    x86: Introduce GDT_ENTRY_INIT(), fix APM
    
    This crash:
    
    [    0.891983] calling  cache_sysfs_init+0x0/0x1ee @ 1
    [    0.897251] initcall cache_sysfs_init+0x0/0x1ee returned 0 after 405 usecs
    [    0.904019] calling  mce_init_device+0x0/0x242 @ 1
    [    0.909124] initcall mce_init_device+0x0/0x242 returned 0 after 347 usecs
    [    0.915815] calling  apm_init+0x0/0x38d @ 1
    [    0.919967] apm: BIOS version 1.2 Flags 0x07 (Driver version 1.16ac)
    [    0.926813] general protection fault: 0000 [#1]
    [    0.927269] last sysfs file:
    [    0.927269] Modules linked in:
    [    0.927269]
    [    0.927269] Pid: 271, comm: kapmd Not tainted (2.6.31-rc3-00100-gd520da1-dirty #311) System Product Name
    [    0.927269] EIP: 00c0:[<000082b2>] EFLAGS: 00010002 CPU: 0
    [    0.927269] EIP is at 0x82b2
    [    0.927269] EAX: 0000530e EBX: 00000000 ECX: 00000102 EDX: 00000000
    [    0.927269] ESI: 00000000 EDI: f6a4bf44 EBP: 67890000 ESP: f6a4beec
    [    0.927269]  DS: 00c8 ES: 0000 FS: 0000 GS: 0000 SS: 0068
    [    0.927269] Process kapmd (pid: 271, ti=f6a4a000 task=f7142280 task.ti=f6a4a000)
    [    0.927269] Stack:
    [    0.927269]  0000828d 02160000 00b88092 f6a4bf3c c102a63d 00000060 f6a4bf3c f6a4bf44
    [    0.927269] <0> 0000007b 0000007b 00000000 00000000 00000000 00000000 560aae9e 00000000
    [    0.927269] <0> 00000200 f705fd74 00000000 c102af70 f6a4bf60 c102a6ec 0000530e 00000000
    [    0.927269] Call Trace:
    [    0.927269]  [<c102a63d>] ? __apm_bios_call_simple+0x7d/0x110
    [    0.927269]  [<c102af70>] ? apm+0x0/0x6a0
    [    0.927269]  [<c102a6ec>] ? apm_bios_call_simple+0x1c/0x50
    [    0.927269]  [<c102b3f5>] ? apm+0x485/0x6a0
    [    0.927269]  [<c1038e7a>] ? finish_task_switch+0x2a/0xb0
    [    0.927269]  [<c164a69e>] ? schedule+0x31e/0x480
    [    0.927269]  [<c102af70>] ? apm+0x0/0x6a0
    [    0.927269]  [<c102af70>] ? apm+0x0/0x6a0
    [    0.927269]  [<c1052654>] ? kthread+0x74/0x80
    [    0.927269]  [<c10525e0>] ? kthread+0x0/0x80
    [    0.927269]  [<c101d627>] ? kernel_thread_helper+0x7/0x10
    [    0.927269] Code:  Bad EIP value.
    [    0.927269] EIP: [<000082b2>] 0x82b2 SS:ESP 0068:f6a4beec
    [    0.927269] ---[ end trace a7919e7f17c0a725 ]---
    [    0.927269] Kernel panic - not syncing: Fatal exception
    [    0.927269] Pid: 271, comm: kapmd Tainted: G      D    2.6.31-rc3-00100-gd520da1-dirty #311
    
    Is caused by an incorrect GDT_ENTRY_INIT() conversion in the apm
    code, as noticed by hpa.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Noticed-by: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    LKML-Reference: <20090808094905.GA2954@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8c9bc287f8fb..b5764a269645 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -106,7 +106,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	/* 16-bit code */
 	[GDT_ENTRY_APMBIOS_BASE+1]	= GDT_ENTRY_INIT(0x009a, 0, 0xffff),
 	/* data */
-	[GDT_ENTRY_APMBIOS_BASE+2]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
+	[GDT_ENTRY_APMBIOS_BASE+2]	= GDT_ENTRY_INIT(0x4092, 0, 0xffff),
 
 	[GDT_ENTRY_ESPFIX_SS]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
 	[GDT_ENTRY_PERCPU]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),

commit 1e5de18278e6862f4198412b5059a03770fa816a
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Jul 19 00:12:20 2009 +0900

    x86: Introduce GDT_ENTRY_INIT()
    
    GDT_ENTRY_INIT is static initializer of desc_struct.
    
    We already have similar macro GDT_ENTRY() but it's static
    initializer for u64 and it cannot be used for desc_struct.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    LKML-Reference: <20090718151219.GD11294@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f1961c07af9a..8c9bc287f8fb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -71,45 +71,45 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	 * TLS descriptors are currently at a different place compared to i386.
 	 * Hopefully nobody expects them at a fixed place (Wine?)
 	 */
-	[GDT_ENTRY_KERNEL32_CS]		= { { { 0x0000ffff, 0x00cf9b00 } } },
-	[GDT_ENTRY_KERNEL_CS]		= { { { 0x0000ffff, 0x00af9b00 } } },
-	[GDT_ENTRY_KERNEL_DS]		= { { { 0x0000ffff, 0x00cf9300 } } },
-	[GDT_ENTRY_DEFAULT_USER32_CS]	= { { { 0x0000ffff, 0x00cffb00 } } },
-	[GDT_ENTRY_DEFAULT_USER_DS]	= { { { 0x0000ffff, 0x00cff300 } } },
-	[GDT_ENTRY_DEFAULT_USER_CS]	= { { { 0x0000ffff, 0x00affb00 } } },
+	[GDT_ENTRY_KERNEL32_CS]		= GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),
+	[GDT_ENTRY_KERNEL_CS]		= GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),
+	[GDT_ENTRY_KERNEL_DS]		= GDT_ENTRY_INIT(0xc093, 0, 0xfffff),
+	[GDT_ENTRY_DEFAULT_USER32_CS]	= GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff),
+	[GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff),
+	[GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff),
 #else
-	[GDT_ENTRY_KERNEL_CS]		= { { { 0x0000ffff, 0x00cf9a00 } } },
-	[GDT_ENTRY_KERNEL_DS]		= { { { 0x0000ffff, 0x00cf9200 } } },
-	[GDT_ENTRY_DEFAULT_USER_CS]	= { { { 0x0000ffff, 0x00cffa00 } } },
-	[GDT_ENTRY_DEFAULT_USER_DS]	= { { { 0x0000ffff, 0x00cff200 } } },
+	[GDT_ENTRY_KERNEL_CS]		= GDT_ENTRY_INIT(0xc09a, 0, 0xfffff),
+	[GDT_ENTRY_KERNEL_DS]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
+	[GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff),
+	[GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff),
 	/*
 	 * Segments used for calling PnP BIOS have byte granularity.
 	 * They code segments and data segments have fixed 64k limits,
 	 * the transfer segment sizes are set at run time.
 	 */
 	/* 32-bit code */
-	[GDT_ENTRY_PNPBIOS_CS32]	= { { { 0x0000ffff, 0x00409a00 } } },
+	[GDT_ENTRY_PNPBIOS_CS32]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
 	/* 16-bit code */
-	[GDT_ENTRY_PNPBIOS_CS16]	= { { { 0x0000ffff, 0x00009a00 } } },
+	[GDT_ENTRY_PNPBIOS_CS16]	= GDT_ENTRY_INIT(0x009a, 0, 0xffff),
 	/* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_DS]		= { { { 0x0000ffff, 0x00009200 } } },
+	[GDT_ENTRY_PNPBIOS_DS]		= GDT_ENTRY_INIT(0x0092, 0, 0xffff),
 	/* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_TS1]		= { { { 0x00000000, 0x00009200 } } },
+	[GDT_ENTRY_PNPBIOS_TS1]		= GDT_ENTRY_INIT(0x0092, 0, 0),
 	/* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_TS2]		= { { { 0x00000000, 0x00009200 } } },
+	[GDT_ENTRY_PNPBIOS_TS2]		= GDT_ENTRY_INIT(0x0092, 0, 0),
 	/*
 	 * The APM segments have byte granularity and their bases
 	 * are set at run time.  All have 64k limits.
 	 */
 	/* 32-bit code */
-	[GDT_ENTRY_APMBIOS_BASE]	= { { { 0x0000ffff, 0x00409a00 } } },
+	[GDT_ENTRY_APMBIOS_BASE]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
 	/* 16-bit code */
-	[GDT_ENTRY_APMBIOS_BASE+1]	= { { { 0x0000ffff, 0x00009a00 } } },
+	[GDT_ENTRY_APMBIOS_BASE+1]	= GDT_ENTRY_INIT(0x009a, 0, 0xffff),
 	/* data */
-	[GDT_ENTRY_APMBIOS_BASE+2]	= { { { 0x0000ffff, 0x00409200 } } },
+	[GDT_ENTRY_APMBIOS_BASE+2]	= GDT_ENTRY_INIT(0x409a, 0, 0xffff),
 
-	[GDT_ENTRY_ESPFIX_SS]		= { { { 0x0000ffff, 0x00cf9200 } } },
-	[GDT_ENTRY_PERCPU]		= { { { 0x0000ffff, 0x00cf9200 } } },
+	[GDT_ENTRY_ESPFIX_SS]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
+	[GDT_ENTRY_PERCPU]		= GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
 	GDT_STACK_CANARY_INIT
 #endif
 } };

commit bdf977b37418cdf8a2252504779a7e12a09b7575
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 3 14:12:19 2009 +0900

    x86, percpu: Collect hot percpu variables into one cacheline
    
    On x86_64, percpu variables current_task and kernel_stack are used for
    get_current() and current_thread_info() respectively and thus are
    often used close to each other.  Move definition of current_task to
    kernel/cpu/common.c right above kernel_stack definition and align it
    to cacheline so that they always fall into the same cacheline.  Two
    percpu variables defined there together - irq_stack_ptr and irq_count
    - are also pretty hot and will benefit from sharing the cacheline.
    
    For consistency, current_task definition for x86_32 is also moved to
    kernel/cpu/common.c.
    
    Putting current_task and kernel_stack into the same cacheline was
    suggested by Linus Torvalds.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 12493c5485e5..1bd88ed978bd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -987,13 +987,21 @@ struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE);
 
-DEFINE_PER_CPU(char *, irq_stack_ptr) =
-	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
+/*
+ * The following four percpu variables are hot.  Align current_task to
+ * cacheline size such that all four fall in the same cacheline.
+ */
+DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =
+	&init_task;
+EXPORT_PER_CPU_SYMBOL(current_task);
 
 DEFINE_PER_CPU(unsigned long, kernel_stack) =
 	(unsigned long)&init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;
 EXPORT_PER_CPU_SYMBOL(kernel_stack);
 
+DEFINE_PER_CPU(char *, irq_stack_ptr) =
+	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
+
 DEFINE_PER_CPU(unsigned int, irq_count) = -1;
 
 /*
@@ -1041,6 +1049,9 @@ DEFINE_PER_CPU(struct orig_ist, orig_ist);
 
 #else	/* CONFIG_X86_64 */
 
+DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
+EXPORT_PER_CPU_SYMBOL(current_task);
+
 #ifdef CONFIG_CC_STACKPROTECTOR
 DEFINE_PER_CPU(unsigned long, stack_canary);
 #endif

commit 3e352aa8ee2bd48f1a19c7742810b3a4a7ba605e
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 3 14:10:11 2009 +0900

    x86, percpu: Fix DECLARE/DEFINE_PER_CPU_PAGE_ALIGNED()
    
    DECLARE/DEFINE_PER_CPU_PAGE_ALIGNED() put percpu variables in
    .page_aligned section without adding any alignment restrictions.
    Currently, this doesn't cause any problem because all users of the
    macros have explicit page alignment and page-sized but it's much safer
    to enforce page alignment from the macros.  After all, it's what they
    claim to do.
    
    Add __aligned(PAGE_SIZE) to DECLARE/DEFINE_PER_CPU_PAGE_ALIGNED() and
    drop explicit alignment from it users.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f1961c07af9a..12493c5485e5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1008,8 +1008,7 @@ static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {
 };
 
 static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
-	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ])
-	__aligned(PAGE_SIZE);
+	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
 
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)

commit 8bdbd962ecfcbdd96f9dbb02d780b4553afd2543
Author: Alan Cox <alan@linux.intel.com>
Date:   Sat Jul 4 00:35:45 2009 +0100

    x86/cpu: Clean up various files a bit
    
    No code changes except printk levels (although some of the K6
    mtrr code might be clearer if there were a few as would
    splitting out some of the intel cache code).
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d6f27c92854b..c96ea44928bf 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -18,8 +18,8 @@
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
 #include <asm/sections.h>
-#include <asm/topology.h>
-#include <asm/cpumask.h>
+#include <linux/topology.h>
+#include <linux/cpumask.h>
 #include <asm/pgtable.h>
 #include <asm/atomic.h>
 #include <asm/proto.h>
@@ -28,13 +28,13 @@
 #include <asm/desc.h>
 #include <asm/i387.h>
 #include <asm/mtrr.h>
-#include <asm/numa.h>
+#include <linux/numa.h>
 #include <asm/asm.h>
 #include <asm/cpu.h>
 #include <asm/mce.h>
 #include <asm/msr.h>
 #include <asm/pat.h>
-#include <asm/smp.h>
+#include <linux/smp.h>
 
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/uv/uv.h>

commit 9ff80942992cd5abd0779c815f310f65b7b83860
Author: Cyrill Gorcunov <gorcunov@openvz.org>
Date:   Wed Jul 8 22:03:53 2009 +0400

    x86: Clean up idt_descr and idt_tableby using NR_VECTORS instead of hardcoded number
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <20090708180353.GH5301@lenovo>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f1961c07af9a..d6f27c92854b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -982,7 +982,7 @@ static __init int setup_disablecpuid(char *arg)
 __setup("clearcpuid=", setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
-struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
+struct desc_ptr idt_descr = { NR_VECTORS * 16 - 1, (unsigned long) idt_table };
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE);

commit ff8a4bae459a9b6455504127fcb78fdbc8e50e4c
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sat Jun 27 12:22:27 2009 -0700

    Revert "x86: cap iomem_resource to addressable physical memory"
    
    This reverts commit 95ee14e4379c5e19c0897c872350570402014742.
    Mikael Petterson <mikepe@it.uu.se> reported that at least one of his
    systems will not boot as a result.  We have ruled out the detection
    algorithm malfunctioning, so it is not a matter of producing the
    incorrect bitmasks; rather, something in the application of them
    fails.
    
    Revert the commit until we can root cause and correct this problem.
    
    -stable team: this means the underlying commit should be rejected.
    
    Reported-and-isolated-by: Mikael Petterson <mikpe@it.uu.se>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    LKML-Reference: <200906261559.n5QFxJH8027336@pilspetsen.it.uu.se>
    Cc: stable@kernel.org
    Cc: Grant Grundler <grundler@parisc-linux.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6b26d4deada0..f1961c07af9a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -848,9 +848,6 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
 	numa_add_cpu(smp_processor_id());
 #endif
-
-	/* Cap the iomem address space to what is addressable on all CPUs */
-	iomem_resource.end &= (1ULL << c->x86_phys_bits) - 1;
 }
 
 #ifdef CONFIG_X86_64

commit c4c5ab3089c8a794eb0bdaa9794d0f055dd82412
Merge: 7fd5b632db00 1d99100120ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 20 10:49:48 2009 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (45 commits)
      x86, mce: fix error path in mce_create_device()
      x86: use zalloc_cpumask_var for mce_dev_initialized
      x86: fix duplicated sysfs attribute
      x86: de-assembler-ize asm/desc.h
      i386: fix/simplify espfix stack switching, move it into assembly
      i386: fix return to 16-bit stack from NMI handler
      x86, ioapic: Don't call disconnect_bsp_APIC if no APIC present
      x86: Remove duplicated #include's
      x86: msr.h linux/types.h is only required for __KERNEL__
      x86: nmi: Add Intel processor 0x6f4 to NMI perfctr1 workaround
      x86, mce: mce_intel.c needs <asm/apic.h>
      x86: apic/io_apic.c: dmar_msi_type should be static
      x86, io_apic.c: Work around compiler warning
      x86: mce: Don't touch THERMAL_APIC_VECTOR if no active APIC present
      x86: mce: Handle banks == 0 case in K7 quirk
      x86, boot: use .code16gcc instead of .code16
      x86: correct the conversion of EFI memory types
      x86: cap iomem_resource to addressable physical memory
      x86, mce: rename _64.c files which are no longer 64-bit-specific
      x86, mce: mce.h cleanup
      ...
    
    Manually fix up trivial conflict in arch/x86/mm/fault.c

commit dc4c2a0aed3b09f6e255bd5c3faa50fe6e0b2ded
Author: Alexander van Heukelum <heukelum@fastmail.fm>
Date:   Thu Jun 18 00:35:58 2009 +0200

    i386: fix/simplify espfix stack switching, move it into assembly
    
    The espfix code triggers if we have a protected mode userspace
    application with a 16-bit stack. On returning to userspace, with iret,
    the CPU doesn't restore the high word of the stack pointer. This is an
    "official" bug, and the work-around used in the kernel is to temporarily
    switch to a 32-bit stack segment/pointer pair where the high word of the
    pointer is equal to the high word of the userspace stackpointer.
    
    The current implementation uses THREAD_SIZE to determine the cut-off,
    but there is no good reason not to use the more natural 64kb... However,
    implementing this by simply substituting THREAD_SIZE with 65536 in
    patch_espfix_desc crashed the test application. patch_espfix_desc tries
    to do what is described above, but gets it subtly wrong if the userspace
    stack pointer is just below a multiple of THREAD_SIZE: an overflow
    occurs to bit 13... With a bit of luck, when the kernelspace
    stackpointer is just below a 64kb-boundary, the overflow then ripples
    trough to bit 16 and userspace will see its stack pointer changed by
    65536.
    
    This patch moves all espfix code into entry_32.S. Selecting a 16-bit
    cut-off simplifies the code. The game with changing the limit dynamically
    is removed too. It complicates matters and I see no value in it. Changing
    only the top 16-bit word of ESP is one instruction and it also implies
    that only two bytes of the ESPFIX GDT entry need to be changed and this
    can be implemented in just a handful simple to understand instructions.
    As a side effect, the operation to compute the original ESP from the
    ESPFIX ESP and the GDT entry simplifies a bit too, and the remaining
    three instructions have been expanded inline in entry_32.S.
    
    impact: can now reliably run userspace with ESP=xxxxfffc on 16-bit
    stack segment
    
    Signed-off-by: Alexander van Heukelum <heukelum@fastmail.fm>
    Acked-by: Stas Sergeev <stsp@aknet.ru>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 5b9cb8839cae..ba1131ac9434 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -108,7 +108,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	/* data */
 	[GDT_ENTRY_APMBIOS_BASE+2]	= { { { 0x0000ffff, 0x00409200 } } },
 
-	[GDT_ENTRY_ESPFIX_SS]		= { { { 0x00000000, 0x00c09200 } } },
+	[GDT_ENTRY_ESPFIX_SS]		= { { { 0x0000ffff, 0x00cf9200 } } },
 	[GDT_ENTRY_PERCPU]		= { { { 0x0000ffff, 0x00cf9200 } } },
 	GDT_STACK_CANARY_INIT
 #endif

commit a9c569539312cfd3c820b38036679a9d72c55331
Author: Minchan Kim <minchan.kim@gmail.com>
Date:   Tue Jun 16 15:33:44 2009 -0700

    use printk_once() in several places
    
    There are some places to be able to use printk_once instead of hard coding.
    
    Signed-off-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3ffdcfa9abdf..9fa33886c0d7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -487,7 +487,6 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 {
 	char *v = c->x86_vendor_id;
-	static int printed;
 	int i;
 
 	for (i = 0; i < X86_VENDOR_NUM; i++) {
@@ -504,13 +503,9 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 		}
 	}
 
-	if (!printed) {
-		printed++;
-		printk(KERN_ERR
-		    "CPU: vendor_id '%s' unknown, using generic init.\n", v);
-
-		printk(KERN_ERR "CPU: Your system may be unstable.\n");
-	}
+	printk_once(KERN_ERR
+			"CPU: vendor_id '%s' unknown, using generic init.\n" \
+			"CPU: Your system may be unstable.\n", v);
 
 	c->x86_vendor = X86_VENDOR_UNKNOWN;
 	this_cpu = &default_cpu;

commit 95ee14e4379c5e19c0897c872350570402014742
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Tue Jun 9 18:20:39 2009 -0700

    x86: cap iomem_resource to addressable physical memory
    
    iomem_resource is by default initialized to -1, which means 64 bits of
    physical address space if 64-bit resources are enabled.  However, x86
    CPUs cannot address 64 bits of physical address space.  Thus, we want
    to cap the physical address space to what the union of all CPU can
    actually address.
    
    Without this patch, we may end up assigning inaccessible values to
    uninitialized 64-bit PCI memory resources.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Matthew Wilcox <matthew@wil.cx>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: Martin Mares <mj@ucw.cz>
    Cc: stable@kernel.org

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3ffdcfa9abdf..5b9cb8839cae 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -853,6 +853,9 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
 	numa_add_cpu(smp_processor_id());
 #endif
+
+	/* Cap the iomem address space to what is addressable on all CPUs */
+	iomem_resource.end &= (1ULL << c->x86_phys_bits) - 1;
 }
 
 #ifdef CONFIG_X86_64

commit 940010c5a314a7bd9b498593bc6ba1718ac5aec5
Merge: 8dc8e5e8bc0c 991ec02cdca3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 17:55:42 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/irqinit.c
            arch/x86/kernel/irqinit_64.c
            arch/x86/kernel/traps.c
            arch/x86/mm/fault.c
            include/linux/sched.h
            kernel/exit.c

commit ee4c24a5c9b530481394132c8dbc10572d57c075
Merge: 3d58f48ba05c 3e0c373749d7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 1 22:29:35 2009 +0200

    Merge branch 'x86/cpufeature' into irq/numa
    
    Merge reason: irq/numa didnt build because this commit:
    
      2759c32: x86: don't call read_apic_id if !cpu_has_apic
    
    Had a dependency on x86/cpufeature changes. Pull in that
    (small) branch to fix the dependency.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 3d58f48ba05caed9118bce62b3047f8683438835
Merge: abfe0af98131 d9244b5d2fbf
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 1 21:06:21 2009 +0200

    Merge branch 'linus' into irq/numa
    
    Conflicts:
            arch/mips/sibyte/bcm1480/irq.c
            arch/mips/sibyte/sb1250/irq.c
    
    Merge reason: we gathered a few conflicts plus update to latest upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 23db9f430be9325a861c7762c1ffadad9ca528a8
Merge: 27b9613b7be3 3218911f839b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 1 10:01:03 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Merge reason: merge almost-rc8 into perfcounters/core, which was -rc6
                  based - to pick up the latest upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 0c752a93353d9b17dbe148312d732fbe06d235e1
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri May 22 12:17:45 2009 -0700

    x86: introduce noxsave boot parameter
    
    Introduce "noxsave" boot parameter which will disable the cpu's xsave/xrstor
    capabilities. Useful for debugging and working around xsave related issues.
    
    [ Impact: make it possible to debug problems in the field ]
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c1caefc82e62..77848d9fca68 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -114,6 +114,13 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
+static int __init x86_xsave_setup(char *s)
+{
+	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
+	return 1;
+}
+__setup("noxsave", x86_xsave_setup);
+
 #ifdef CONFIG_X86_32
 static int cachesize_override __cpuinitdata = -1;
 static int disable_x86_serial_nr __cpuinitdata = 1;

commit 2759c3287de27266e06f1f4e82cbd2d65f6a044c
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri May 15 13:05:16 2009 -0700

    x86: don't call read_apic_id if !cpu_has_apic
    
    should not call that if apic is disabled.
    
    [ Impact: fix crash on certain UP configs ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    LKML-Reference: <4A09CCBB.2000306@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c1caefc82e62..017c600e05ab 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -761,6 +761,12 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	if (this_cpu->c_identify)
 		this_cpu->c_identify(c);
 
+	/* Clear/Set all flags overriden by options, after probe */
+	for (i = 0; i < NCAPINTS; i++) {
+		c->x86_capability[i] &= ~cpu_caps_cleared[i];
+		c->x86_capability[i] |= cpu_caps_set[i];
+	}
+
 #ifdef CONFIG_X86_64
 	c->apicid = apic->phys_pkg_id(c->initial_apicid, 0);
 #endif

commit dc3f81b129b5439ba7bac265bbc6a51a39275dae
Merge: d2517a49d555 1406de8e11eb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 18 07:37:44 2009 +0200

    Merge commit 'v2.6.30-rc6' into perfcounters/core
    
    Merge reason: this branch was on an -rc4 base, merge it up to -rc6
                  to get the latest upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 3e0c373749d7eb5b354ac0b043f2b2cdf84eefef
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat May 9 23:47:42 2009 -0700

    x86: clean up and fix setup_clear/force_cpu_cap handling
    
    setup_force_cpu_cap() only have one user (Xen guest code),
    but it should not reuse cleared_cpu_cpus, otherwise it
    will have problems on SMP.
    
    Need to have a separate cpu_cpus_set array too, for forced-on
    flags, beyond the forced-off flags.
    
    Also need to setup handling before all cpus caps are combined.
    
    [ Impact: fix the forced-set CPU feature flag logic ]
    
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Yinghai Lu <yinghai.lu@kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c4f667896c28..e7fd5c4935a3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -292,7 +292,8 @@ static const char *__cpuinit table_lookup_model(struct cpuinfo_x86 *c)
 	return NULL;		/* Not found */
 }
 
-__u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
+__u32 cpu_caps_cleared[NCAPINTS] __cpuinitdata;
+__u32 cpu_caps_set[NCAPINTS] __cpuinitdata;
 
 void load_percpu_segment(int cpu)
 {
@@ -806,6 +807,16 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 #endif
 
 	init_hypervisor(c);
+
+	/*
+	 * Clear/Set all flags overriden by options, need do it
+	 * before following smp all cpus cap AND.
+	 */
+	for (i = 0; i < NCAPINTS; i++) {
+		c->x86_capability[i] &= ~cpu_caps_cleared[i];
+		c->x86_capability[i] |= cpu_caps_set[i];
+	}
+
 	/*
 	 * On SMP, boot_cpu_data holds the common feature set between
 	 * all CPUs; so make sure that we indicate which features are
@@ -818,10 +829,6 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
 	}
 
-	/* Clear all flags overriden by options */
-	for (i = 0; i < NCAPINTS; i++)
-		c->x86_capability[i] &= ~cleared_cpu_caps[i];
-
 #ifdef CONFIG_X86_MCE
 	/* Init Machine Check Exception if available. */
 	mcheck_init(c);

commit f9a196b8dceba3c1e5fe885b81e45043ad7c60fc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 1 20:59:25 2009 +0200

    x86: initialize io_bitmap_base on 32bit
    
    commit db949bba3c7cf2e664ac12e237c6d4c914f0c69d (x86-32: use non-lazy
    io bitmap context switching) broke ioperm for 32bit because it removed
    the lazy initialization of io_bitmap_base and did not set it to the
    real bitmap offset.
    
    [ Impact: fix non-working sys_ioperm() on 32-bit kernels ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c4f667896c28..c1caefc82e62 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1203,6 +1203,8 @@ void __cpuinit cpu_init(void)
 	load_TR_desc();
 	load_LDT(&init_mm.context);
 
+	t->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
+
 #ifdef CONFIG_DOUBLEFAULT
 	/* Set up doublefault TSS pointer in the GDT */
 	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);

commit f541ae326fa120fa5c57433e4d9a133df212ce41
Merge: e255357764f9 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 09:02:57 2009 +0200

    Merge branch 'linus' into perfcounters/core-v2
    
    Merge reason: we have gathered quite a few conflicts, need to merge upstream
    
    Conflicts:
            arch/powerpc/kernel/Makefile
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/hardirq.h
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/irq.c
            arch/x86/kernel/syscall_table_32.S
            arch/x86/mm/iomap_32.c
            include/linux/sched.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 65fb0d23fcddd8697c871047b700c78817bdaa43
Merge: 8c083f081d00 dfbbe89e197a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 30 23:53:32 2009 +0200

    Merge branch 'linus' into cpumask-for-linus
    
    Conflicts:
            arch/x86/kernel/cpu/common.c

commit 30e1e6d1af2b67558bccf322af2b3e0676b209ae
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Mar 17 14:50:34 2009 +1030

    cpumask: fix CONFIG_CPUMASK_OFFSTACK=y cpu hotunplug crash
    
    Impact: Fix cpu offline when CONFIG_MAXSMP=y
    
    Changeset bc9b83dd1f66402b870301c3c7117b9c1484abb4 "cpumask: convert
    c1e_mask in arch/x86/kernel/process.c to cpumask_var_t" contained a
    bug: c1e_mask is manipulated even if C1E isn't detected (and hence
    not allocated).
    
    This is simply fixed by checking for NULL (which gcc optimizes out
    anyway of CONFIG_CPUMASK_OFFSTACK=n, since it knows ce1_mask can never
    be NULL).
    
    In addition, fix a leak where select_idle_routine re-allocates
    (and re-clears) c1e_mask on every cpu init.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Mike Travis <travis@sgi.com>
    LKML-Reference: <200903171450.34549.rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 82f6cc045ad0..d7dd3c294e2a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -812,6 +812,7 @@ static void vgetcpu_set_mode(void)
 void __init identify_boot_cpu(void)
 {
 	identify_cpu(&boot_cpu_data);
+	init_c1e_mask();
 #ifdef CONFIG_X86_32
 	sysenter_setup();
 	enable_sep_cpu();

commit 0ca0f16fd17c5d880dd0abbe03595b0c7c5b3c95
Merge: c550033ced48 7a81d9a7da03 88200bc28da3 0f3fa48a7eaf 91219bcbdccc 063402356280 773e673de272 5a8ac9d28dae 16a6791934a1 895791dac694
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Mar 14 16:25:40 2009 +0100

    Merge branches 'x86/apic', 'x86/asm', 'x86/cleanups', 'x86/debug', 'x86/kconfig', 'x86/mm', 'x86/ptrace', 'x86/setup' and 'x86/urgent'; commit 'v2.6.29-rc8' into x86/core

commit 0f3fa48a7eaf5d1118cfda1650e8c759b2a116e4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Mar 14 08:46:17 2009 +0100

    x86: cpu/common.c more cleanups
    
    Complete/fix the cleanups of cpu/common.c:
    
     - fix ugly warning due to asm/topology.h -> linux/topology.h change
     - standardize the style across the file
     - simplify/refactor the code flow where possible
    
    Cc: Jaswinder Singh Rajput <jaswinder@kernel.org>
    LKML-Reference: <1237009789.4387.2.camel@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cad6878c88db..a9e3791ca098 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1,4 +1,3 @@
-#include <linux/topology.h>
 #include <linux/bootmem.h>
 #include <linux/linkage.h>
 #include <linux/bitops.h>
@@ -18,6 +17,7 @@
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
 #include <asm/sections.h>
+#include <asm/topology.h>
 #include <asm/cpumask.h>
 #include <asm/pgtable.h>
 #include <asm/atomic.h>
@@ -82,45 +82,45 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	 * TLS descriptors are currently at a different place compared to i386.
 	 * Hopefully nobody expects them at a fixed place (Wine?)
 	 */
-	[GDT_ENTRY_KERNEL32_CS] = { { { 0x0000ffff, 0x00cf9b00 } } },
-	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00af9b00 } } },
-	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9300 } } },
-	[GDT_ENTRY_DEFAULT_USER32_CS] = { { { 0x0000ffff, 0x00cffb00 } } },
-	[GDT_ENTRY_DEFAULT_USER_DS] = { { { 0x0000ffff, 0x00cff300 } } },
-	[GDT_ENTRY_DEFAULT_USER_CS] = { { { 0x0000ffff, 0x00affb00 } } },
+	[GDT_ENTRY_KERNEL32_CS]		= { { { 0x0000ffff, 0x00cf9b00 } } },
+	[GDT_ENTRY_KERNEL_CS]		= { { { 0x0000ffff, 0x00af9b00 } } },
+	[GDT_ENTRY_KERNEL_DS]		= { { { 0x0000ffff, 0x00cf9300 } } },
+	[GDT_ENTRY_DEFAULT_USER32_CS]	= { { { 0x0000ffff, 0x00cffb00 } } },
+	[GDT_ENTRY_DEFAULT_USER_DS]	= { { { 0x0000ffff, 0x00cff300 } } },
+	[GDT_ENTRY_DEFAULT_USER_CS]	= { { { 0x0000ffff, 0x00affb00 } } },
 #else
-	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00cf9a00 } } },
-	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9200 } } },
-	[GDT_ENTRY_DEFAULT_USER_CS] = { { { 0x0000ffff, 0x00cffa00 } } },
-	[GDT_ENTRY_DEFAULT_USER_DS] = { { { 0x0000ffff, 0x00cff200 } } },
+	[GDT_ENTRY_KERNEL_CS]		= { { { 0x0000ffff, 0x00cf9a00 } } },
+	[GDT_ENTRY_KERNEL_DS]		= { { { 0x0000ffff, 0x00cf9200 } } },
+	[GDT_ENTRY_DEFAULT_USER_CS]	= { { { 0x0000ffff, 0x00cffa00 } } },
+	[GDT_ENTRY_DEFAULT_USER_DS]	= { { { 0x0000ffff, 0x00cff200 } } },
 	/*
 	 * Segments used for calling PnP BIOS have byte granularity.
 	 * They code segments and data segments have fixed 64k limits,
 	 * the transfer segment sizes are set at run time.
 	 */
 	/* 32-bit code */
-	[GDT_ENTRY_PNPBIOS_CS32] = { { { 0x0000ffff, 0x00409a00 } } },
+	[GDT_ENTRY_PNPBIOS_CS32]	= { { { 0x0000ffff, 0x00409a00 } } },
 	/* 16-bit code */
-	[GDT_ENTRY_PNPBIOS_CS16] = { { { 0x0000ffff, 0x00009a00 } } },
+	[GDT_ENTRY_PNPBIOS_CS16]	= { { { 0x0000ffff, 0x00009a00 } } },
 	/* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_DS] = { { { 0x0000ffff, 0x00009200 } } },
+	[GDT_ENTRY_PNPBIOS_DS]		= { { { 0x0000ffff, 0x00009200 } } },
 	/* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_TS1] = { { { 0x00000000, 0x00009200 } } },
+	[GDT_ENTRY_PNPBIOS_TS1]		= { { { 0x00000000, 0x00009200 } } },
 	/* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_TS2] = { { { 0x00000000, 0x00009200 } } },
+	[GDT_ENTRY_PNPBIOS_TS2]		= { { { 0x00000000, 0x00009200 } } },
 	/*
 	 * The APM segments have byte granularity and their bases
 	 * are set at run time.  All have 64k limits.
 	 */
 	/* 32-bit code */
-	[GDT_ENTRY_APMBIOS_BASE] = { { { 0x0000ffff, 0x00409a00 } } },
+	[GDT_ENTRY_APMBIOS_BASE]	= { { { 0x0000ffff, 0x00409a00 } } },
 	/* 16-bit code */
-	[GDT_ENTRY_APMBIOS_BASE+1] = { { { 0x0000ffff, 0x00009a00 } } },
+	[GDT_ENTRY_APMBIOS_BASE+1]	= { { { 0x0000ffff, 0x00009a00 } } },
 	/* data */
-	[GDT_ENTRY_APMBIOS_BASE+2] = { { { 0x0000ffff, 0x00409200 } } },
+	[GDT_ENTRY_APMBIOS_BASE+2]	= { { { 0x0000ffff, 0x00409200 } } },
 
-	[GDT_ENTRY_ESPFIX_SS] = { { { 0x00000000, 0x00c09200 } } },
-	[GDT_ENTRY_PERCPU] = { { { 0x0000ffff, 0x00cf9200 } } },
+	[GDT_ENTRY_ESPFIX_SS]		= { { { 0x00000000, 0x00c09200 } } },
+	[GDT_ENTRY_PERCPU]		= { { { 0x0000ffff, 0x00cf9200 } } },
 	GDT_STACK_CANARY_INIT
 #endif
 } };
@@ -164,16 +164,17 @@ static inline int flag_is_changeable_p(u32 flag)
 	 * the CPUID. Add "volatile" to not allow gcc to
 	 * optimize the subsequent calls to this function.
 	 */
-	asm volatile ("pushfl\n\t"
-		      "pushfl\n\t"
-		      "popl %0\n\t"
-		      "movl %0,%1\n\t"
-		      "xorl %2,%0\n\t"
-		      "pushl %0\n\t"
-		      "popfl\n\t"
-		      "pushfl\n\t"
-		      "popl %0\n\t"
-		      "popfl\n\t"
+	asm volatile ("pushfl		\n\t"
+		      "pushfl		\n\t"
+		      "popl %0		\n\t"
+		      "movl %0, %1	\n\t"
+		      "xorl %2, %0	\n\t"
+		      "pushl %0		\n\t"
+		      "popfl		\n\t"
+		      "pushfl		\n\t"
+		      "popl %0		\n\t"
+		      "popfl		\n\t"
+
 		      : "=&r" (f1), "=&r" (f2)
 		      : "ir" (flag));
 
@@ -188,18 +189,22 @@ static int __cpuinit have_cpuid_p(void)
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 {
-	if (cpu_has(c, X86_FEATURE_PN) && disable_x86_serial_nr) {
-		/* Disable processor serial number */
-		unsigned long lo, hi;
-		rdmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
-		lo |= 0x200000;
-		wrmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
-		printk(KERN_NOTICE "CPU serial number disabled.\n");
-		clear_cpu_cap(c, X86_FEATURE_PN);
-
-		/* Disabling the serial number may affect the cpuid level */
-		c->cpuid_level = cpuid_eax(0);
-	}
+	unsigned long lo, hi;
+
+	if (!cpu_has(c, X86_FEATURE_PN) || !disable_x86_serial_nr)
+		return;
+
+	/* Disable processor serial number: */
+
+	rdmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
+	lo |= 0x200000;
+	wrmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
+
+	printk(KERN_NOTICE "CPU serial number disabled.\n");
+	clear_cpu_cap(c, X86_FEATURE_PN);
+
+	/* Disabling the serial number may affect the cpuid level */
+	c->cpuid_level = cpuid_eax(0);
 }
 
 static int __init x86_serial_nr_setup(char *s)
@@ -232,6 +237,7 @@ struct cpuid_dependent_feature {
 	u32 feature;
 	u32 level;
 };
+
 static const struct cpuid_dependent_feature __cpuinitconst
 cpuid_dependent_features[] = {
 	{ X86_FEATURE_MWAIT,		0x00000005 },
@@ -245,6 +251,9 @@ static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 	const struct cpuid_dependent_feature *df;
 
 	for (df = cpuid_dependent_features; df->feature; df++) {
+
+		if (!cpu_has(c, df->feature))
+			continue;
 		/*
 		 * Note: cpuid_level is set to -1 if unavailable, but
 		 * extended_extended_level is set to 0 if unavailable
@@ -252,26 +261,26 @@ static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 		 * when signed; hence the weird messing around with
 		 * signs here...
 		 */
-		if (cpu_has(c, df->feature) &&
-		    ((s32)df->level < 0 ?
+		if (!((s32)df->level < 0 ?
 		     (u32)df->level > (u32)c->extended_cpuid_level :
-		     (s32)df->level > (s32)c->cpuid_level)) {
-			clear_cpu_cap(c, df->feature);
-			if (warn)
-				printk(KERN_WARNING
-				       "CPU: CPU feature %s disabled "
-				       "due to lack of CPUID level 0x%x\n",
-				       x86_cap_flags[df->feature],
-				       df->level);
-		}
+		     (s32)df->level > (s32)c->cpuid_level))
+			continue;
+
+		clear_cpu_cap(c, df->feature);
+		if (!warn)
+			continue;
+
+		printk(KERN_WARNING
+		       "CPU: CPU feature %s disabled, no CPUID level 0x%x\n",
+				x86_cap_flags[df->feature], df->level);
 	}
 }
 
 /*
  * Naming convention should be: <Name> [(<Codename>)]
  * This table only is used unless init_<vendor>() below doesn't set it;
- * in particular, if CPUID levels 0x80000002..4 are supported, this isn't used
- *
+ * in particular, if CPUID levels 0x80000002..4 are supported, this
+ * isn't used
  */
 
 /* Look up CPU names by table lookup. */
@@ -308,8 +317,10 @@ void load_percpu_segment(int cpu)
 	load_stack_canary_segment();
 }
 
-/* Current gdt points %fs at the "master" per-cpu area: after this,
- * it's on the real one. */
+/*
+ * Current gdt points %fs at the "master" per-cpu area: after this,
+ * it's on the real one.
+ */
 void switch_to_new_gdt(int cpu)
 {
 	struct desc_ptr gdt_descr;
@@ -355,14 +366,16 @@ static void __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	if (c->extended_cpuid_level < 0x80000004)
 		return;
 
-	v = (unsigned int *) c->x86_model_id;
+	v = (unsigned int *)c->x86_model_id;
 	cpuid(0x80000002, &v[0], &v[1], &v[2], &v[3]);
 	cpuid(0x80000003, &v[4], &v[5], &v[6], &v[7]);
 	cpuid(0x80000004, &v[8], &v[9], &v[10], &v[11]);
 	c->x86_model_id[48] = 0;
 
-	/* Intel chips right-justify this string for some dumb reason;
-	   undo that brain damage */
+	/*
+	 * Intel chips right-justify this string for some dumb reason;
+	 * undo that brain damage:
+	 */
 	p = q = &c->x86_model_id[0];
 	while (*p == ' ')
 		p++;
@@ -439,28 +452,30 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 
 	if (smp_num_siblings == 1) {
 		printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
-	} else if (smp_num_siblings > 1) {
+		goto out;
+	}
 
-		if (smp_num_siblings > nr_cpu_ids) {
-			pr_warning("CPU: Unsupported number of siblings %d",
-				   smp_num_siblings);
-			smp_num_siblings = 1;
-			return;
-		}
+	if (smp_num_siblings <= 1)
+		goto out;
 
-		index_msb = get_count_order(smp_num_siblings);
-		c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid,
-						    index_msb);
+	if (smp_num_siblings > nr_cpu_ids) {
+		pr_warning("CPU: Unsupported number of siblings %d",
+			   smp_num_siblings);
+		smp_num_siblings = 1;
+		return;
+	}
 
-		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
+	index_msb = get_count_order(smp_num_siblings);
+	c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid, index_msb);
 
-		index_msb = get_count_order(smp_num_siblings);
+	smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
-		core_bits = get_count_order(c->x86_max_cores);
+	index_msb = get_count_order(smp_num_siblings);
 
-		c->cpu_core_id = apic->phys_pkg_id(c->initial_apicid, index_msb) &
-					       ((1 << core_bits) - 1);
-	}
+	core_bits = get_count_order(c->x86_max_cores);
+
+	c->cpu_core_id = apic->phys_pkg_id(c->initial_apicid, index_msb) &
+				       ((1 << core_bits) - 1);
 
 out:
 	if ((c->x86_max_cores * smp_num_siblings) > 1) {
@@ -475,8 +490,8 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 {
 	char *v = c->x86_vendor_id;
-	int i;
 	static int printed;
+	int i;
 
 	for (i = 0; i < X86_VENDOR_NUM; i++) {
 		if (!cpu_devs[i])
@@ -485,6 +500,7 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 		if (!strcmp(v, cpu_devs[i]->c_ident[0]) ||
 		    (cpu_devs[i]->c_ident[1] &&
 		     !strcmp(v, cpu_devs[i]->c_ident[1]))) {
+
 			this_cpu = cpu_devs[i];
 			c->x86_vendor = this_cpu->c_x86_vendor;
 			return;
@@ -493,8 +509,9 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 
 	if (!printed) {
 		printed++;
-		printk(KERN_ERR "CPU: vendor_id '%s'"
-			"unknown, using generic init.\n", v);
+		printk(KERN_ERR
+		    "CPU: vendor_id '%s' unknown, using generic init.\n", v);
+
 		printk(KERN_ERR "CPU: Your system may be unstable.\n");
 	}
 
@@ -514,14 +531,17 @@ void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
 	/* Intel-defined flags: level 0x00000001 */
 	if (c->cpuid_level >= 0x00000001) {
 		u32 junk, tfms, cap0, misc;
+
 		cpuid(0x00000001, &tfms, &misc, &junk, &cap0);
 		c->x86 = (tfms >> 8) & 0xf;
 		c->x86_model = (tfms >> 4) & 0xf;
 		c->x86_mask = tfms & 0xf;
+
 		if (c->x86 == 0xf)
 			c->x86 += (tfms >> 20) & 0xff;
 		if (c->x86 >= 0x6)
 			c->x86_model += ((tfms >> 16) & 0xf) << 4;
+
 		if (cap0 & (1<<19)) {
 			c->x86_clflush_size = ((misc >> 8) & 0xff) * 8;
 			c->x86_cache_alignment = c->x86_clflush_size;
@@ -537,6 +557,7 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 	/* Intel-defined flags: level 0x00000001 */
 	if (c->cpuid_level >= 0x00000001) {
 		u32 capability, excap;
+
 		cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
 		c->x86_capability[0] = capability;
 		c->x86_capability[4] = excap;
@@ -545,6 +566,7 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 	/* AMD-defined flags: level 0x80000001 */
 	xlvl = cpuid_eax(0x80000000);
 	c->extended_cpuid_level = xlvl;
+
 	if ((xlvl & 0xffff0000) == 0x80000000) {
 		if (xlvl >= 0x80000001) {
 			c->x86_capability[1] = cpuid_edx(0x80000001);
@@ -762,8 +784,8 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	squash_the_stupid_serial_number(c);
 
 	/*
-	 * The vendor-specific functions might have changed features.  Now
-	 * we do "generic changes."
+	 * The vendor-specific functions might have changed features.
+	 * Now we do "generic changes."
 	 */
 
 	/* Filter out anything that depends on CPUID levels we don't have */
@@ -846,8 +868,8 @@ void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)
 }
 
 struct msr_range {
-	unsigned min;
-	unsigned max;
+	unsigned	min;
+	unsigned	max;
 };
 
 static struct msr_range msr_range_array[] __cpuinitdata = {
@@ -859,14 +881,15 @@ static struct msr_range msr_range_array[] __cpuinitdata = {
 
 static void __cpuinit print_cpu_msr(void)
 {
+	unsigned index_min, index_max;
 	unsigned index;
 	u64 val;
 	int i;
-	unsigned index_min, index_max;
 
 	for (i = 0; i < ARRAY_SIZE(msr_range_array); i++) {
 		index_min = msr_range_array[i].min;
 		index_max = msr_range_array[i].max;
+
 		for (index = index_min; index < index_max; index++) {
 			if (rdmsrl_amd_safe(index, &val))
 				continue;
@@ -876,6 +899,7 @@ static void __cpuinit print_cpu_msr(void)
 }
 
 static int show_msr __cpuinitdata;
+
 static __init int setup_show_msr(char *arg)
 {
 	int num;
@@ -899,10 +923,12 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 {
 	char *vendor = NULL;
 
-	if (c->x86_vendor < X86_VENDOR_NUM)
+	if (c->x86_vendor < X86_VENDOR_NUM) {
 		vendor = this_cpu->c_vendor;
-	else if (c->cpuid_level >= 0)
-		vendor = c->x86_vendor_id;
+	} else {
+		if (c->cpuid_level >= 0)
+			vendor = c->x86_vendor_id;
+	}
 
 	if (vendor && !strstr(c->x86_model_id, vendor))
 		printk(KERN_CONT "%s ", vendor);
@@ -929,10 +955,12 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 static __init int setup_disablecpuid(char *arg)
 {
 	int bit;
+
 	if (get_option(&arg, &bit) && bit < NCAPINTS*32)
 		setup_clear_cpu_cap(bit);
 	else
 		return 0;
+
 	return 1;
 }
 __setup("clearcpuid=", setup_disablecpuid);
@@ -942,6 +970,7 @@ struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE);
+
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
 	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
 
@@ -951,6 +980,17 @@ EXPORT_PER_CPU_SYMBOL(kernel_stack);
 
 DEFINE_PER_CPU(unsigned int, irq_count) = -1;
 
+/*
+ * Special IST stacks which the CPU switches to when it calls
+ * an IST-marked descriptor entry. Up to 7 stacks (hardware
+ * limit), all of them are 4K, except the debug stack which
+ * is 8K.
+ */
+static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {
+	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,
+	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
+};
+
 static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ])
 	__aligned(PAGE_SIZE);
@@ -984,7 +1024,7 @@ unsigned long kernel_eflags;
  */
 DEFINE_PER_CPU(struct orig_ist, orig_ist);
 
-#else	/* x86_64 */
+#else	/* CONFIG_X86_64 */
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 DEFINE_PER_CPU(unsigned long, stack_canary);
@@ -996,9 +1036,10 @@ struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 	memset(regs, 0, sizeof(struct pt_regs));
 	regs->fs = __KERNEL_PERCPU;
 	regs->gs = __KERNEL_STACK_CANARY;
+
 	return regs;
 }
-#endif	/* x86_64 */
+#endif	/* CONFIG_X86_64 */
 
 /*
  * Clear all 6 debug registers:
@@ -1024,15 +1065,20 @@ static void clear_all_debug_regs(void)
  * A lot of state is already set up in PDA init for 64 bit
  */
 #ifdef CONFIG_X86_64
+
 void __cpuinit cpu_init(void)
 {
-	int cpu = stack_smp_processor_id();
-	struct tss_struct *t = &per_cpu(init_tss, cpu);
-	struct orig_ist *orig_ist = &per_cpu(orig_ist, cpu);
-	unsigned long v;
+	struct orig_ist *orig_ist;
 	struct task_struct *me;
+	struct tss_struct *t;
+	unsigned long v;
+	int cpu;
 	int i;
 
+	cpu = stack_smp_processor_id();
+	t = &per_cpu(init_tss, cpu);
+	orig_ist = &per_cpu(orig_ist, cpu);
+
 #ifdef CONFIG_NUMA
 	if (cpu != 0 && percpu_read(node_number) == 0 &&
 	    cpu_to_node(cpu) != NUMA_NO_NODE)
@@ -1073,19 +1119,17 @@ void __cpuinit cpu_init(void)
 	 * set up and load the per-CPU TSS
 	 */
 	if (!orig_ist->ist[0]) {
-		static const unsigned int sizes[N_EXCEPTION_STACKS] = {
-		  [0 ... N_EXCEPTION_STACKS - 1] = EXCEPTION_STKSZ,
-		  [DEBUG_STACK - 1] = DEBUG_STKSZ
-		};
 		char *estacks = per_cpu(exception_stacks, cpu);
+
 		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
-			estacks += sizes[v];
+			estacks += exception_stack_sizes[v];
 			orig_ist->ist[v] = t->x86_tss.ist[v] =
 					(unsigned long)estacks;
 		}
 	}
 
 	t->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
+
 	/*
 	 * <= is required because the CPU will access up to
 	 * 8 bits beyond the end of the IO permission bitmap.
@@ -1187,5 +1231,4 @@ void __cpuinit cpu_init(void)
 
 	xsave_init();
 }
-
 #endif

commit 9766cdbcb260389669e9679b2aa87c11832f479f
Author: Jaswinder Singh Rajput <jaswinder@kernel.org>
Date:   Sat Mar 14 11:19:49 2009 +0530

    x86: cpu/common.c cleanups
    
    - fix various style problems
     - declare varibles before they get used
     - introduced clear_all_debug_regs
     - fix header files issues
    
    LKML-Reference: <1237009789.4387.2.camel@localhost.localdomain>
    Signed-off-by: Jaswinder Singh Rajput <jaswinder@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 826d5c876278..cad6878c88db 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1,52 +1,52 @@
-#include <linux/init.h>
-#include <linux/kernel.h>
-#include <linux/sched.h>
-#include <linux/string.h>
+#include <linux/topology.h>
 #include <linux/bootmem.h>
+#include <linux/linkage.h>
 #include <linux/bitops.h>
+#include <linux/kernel.h>
 #include <linux/module.h>
-#include <linux/kgdb.h>
-#include <linux/topology.h>
+#include <linux/percpu.h>
+#include <linux/string.h>
 #include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <linux/kgdb.h>
 #include <linux/smp.h>
-#include <linux/percpu.h>
-#include <asm/i387.h>
-#include <asm/msr.h>
-#include <asm/io.h>
-#include <asm/linkage.h>
+#include <linux/io.h>
+
+#include <asm/stackprotector.h>
 #include <asm/mmu_context.h>
+#include <asm/hypervisor.h>
+#include <asm/processor.h>
+#include <asm/sections.h>
+#include <asm/cpumask.h>
+#include <asm/pgtable.h>
+#include <asm/atomic.h>
+#include <asm/proto.h>
+#include <asm/setup.h>
+#include <asm/apic.h>
+#include <asm/desc.h>
+#include <asm/i387.h>
 #include <asm/mtrr.h>
+#include <asm/numa.h>
+#include <asm/asm.h>
+#include <asm/cpu.h>
 #include <asm/mce.h>
+#include <asm/msr.h>
 #include <asm/pat.h>
-#include <asm/asm.h>
-#include <asm/numa.h>
 #include <asm/smp.h>
-#include <asm/cpu.h>
-#include <asm/cpumask.h>
-#include <asm/apic.h>
 
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/uv/uv.h>
 #endif
 
-#include <asm/pgtable.h>
-#include <asm/processor.h>
-#include <asm/desc.h>
-#include <asm/atomic.h>
-#include <asm/proto.h>
-#include <asm/sections.h>
-#include <asm/setup.h>
-#include <asm/hypervisor.h>
-#include <asm/stackprotector.h>
-
 #include "cpu.h"
 
 #ifdef CONFIG_X86_64
 
 /* all of these masks are initialized in setup_cpu_local_masks() */
-cpumask_var_t cpu_callin_mask;
-cpumask_var_t cpu_callout_mask;
 cpumask_var_t cpu_initialized_mask;
+cpumask_var_t cpu_callout_mask;
+cpumask_var_t cpu_callin_mask;
 
 /* representing cpus for which sibling maps can be computed */
 cpumask_var_t cpu_sibling_setup_mask;
@@ -62,10 +62,10 @@ void __init setup_cpu_local_masks(void)
 
 #else /* CONFIG_X86_32 */
 
-cpumask_t cpu_callin_map;
+cpumask_t cpu_sibling_setup_map;
 cpumask_t cpu_callout_map;
 cpumask_t cpu_initialized;
-cpumask_t cpu_sibling_setup_map;
+cpumask_t cpu_callin_map;
 
 #endif /* CONFIG_X86_32 */
 
@@ -79,7 +79,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	 * IRET will check the segment types  kkeil 2000/10/28
 	 * Also sysret mandates a special GDT layout
 	 *
-	 * The TLS descriptors are currently at a different place compared to i386.
+	 * TLS descriptors are currently at a different place compared to i386.
 	 * Hopefully nobody expects them at a fixed place (Wine?)
 	 */
 	[GDT_ENTRY_KERNEL32_CS] = { { { 0x0000ffff, 0x00cf9b00 } } },
@@ -243,6 +243,7 @@ cpuid_dependent_features[] = {
 static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 {
 	const struct cpuid_dependent_feature *df;
+
 	for (df = cpuid_dependent_features; df->feature; df++) {
 		/*
 		 * Note: cpuid_level is set to -1 if unavailable, but
@@ -364,12 +365,12 @@ static void __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	   undo that brain damage */
 	p = q = &c->x86_model_id[0];
 	while (*p == ' ')
-	     p++;
+		p++;
 	if (p != q) {
-	     while (*p)
-		  *q++ = *p++;
-	     while (q <= &c->x86_model_id[48])
-		  *q++ = '\0';	/* Zero-pad the rest */
+		while (*p)
+			*q++ = *p++;
+		while (q <= &c->x86_model_id[48])
+			*q++ = '\0';	/* Zero-pad the rest */
 	}
 }
 
@@ -441,14 +442,15 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 	} else if (smp_num_siblings > 1) {
 
 		if (smp_num_siblings > nr_cpu_ids) {
-			printk(KERN_WARNING "CPU: Unsupported number of siblings %d",
-					smp_num_siblings);
+			pr_warning("CPU: Unsupported number of siblings %d",
+				   smp_num_siblings);
 			smp_num_siblings = 1;
 			return;
 		}
 
 		index_msb = get_count_order(smp_num_siblings);
-		c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid, index_msb);
+		c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid,
+						    index_msb);
 
 		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
@@ -491,7 +493,8 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 
 	if (!printed) {
 		printed++;
-		printk(KERN_ERR "CPU: vendor_id '%s' unknown, using generic init.\n", v);
+		printk(KERN_ERR "CPU: vendor_id '%s'"
+			"unknown, using generic init.\n", v);
 		printk(KERN_ERR "CPU: Your system may be unstable.\n");
 	}
 
@@ -637,7 +640,7 @@ void __init early_cpu_init(void)
 	struct cpu_dev **cdev;
 	int count = 0;
 
-	printk("KERNEL supported cpus:\n");
+	printk(KERN_INFO "KERNEL supported cpus:\n");
 	for (cdev = __x86_cpu_dev_start; cdev < __x86_cpu_dev_end; cdev++) {
 		struct cpu_dev *cpudev = *cdev;
 		unsigned int j;
@@ -650,7 +653,7 @@ void __init early_cpu_init(void)
 		for (j = 0; j < 2; j++) {
 			if (!cpudev->c_ident[j])
 				continue;
-			printk("  %s %s\n", cpudev->c_vendor,
+			printk(KERN_INFO "  %s %s\n", cpudev->c_vendor,
 				cpudev->c_ident[j]);
 		}
 	}
@@ -952,8 +955,6 @@ static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ])
 	__aligned(PAGE_SIZE);
 
-extern asmlinkage void ignore_sysret(void);
-
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
@@ -999,6 +1000,22 @@ struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 }
 #endif	/* x86_64 */
 
+/*
+ * Clear all 6 debug registers:
+ */
+static void clear_all_debug_regs(void)
+{
+	int i;
+
+	for (i = 0; i < 8; i++) {
+		/* Ignore db4, db5 */
+		if ((i == 4) || (i == 5))
+			continue;
+
+		set_debugreg(0, i);
+	}
+}
+
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT
@@ -1098,17 +1115,7 @@ void __cpuinit cpu_init(void)
 		arch_kgdb_ops.correct_hw_break();
 	else
 #endif
-	{
-		/*
-		 * Clear all 6 debug registers:
-		 */
-		set_debugreg(0UL, 0);
-		set_debugreg(0UL, 1);
-		set_debugreg(0UL, 2);
-		set_debugreg(0UL, 3);
-		set_debugreg(0UL, 6);
-		set_debugreg(0UL, 7);
-	}
+		clear_all_debug_regs();
 
 	fpu_init();
 
@@ -1129,7 +1136,8 @@ void __cpuinit cpu_init(void)
 
 	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask)) {
 		printk(KERN_WARNING "CPU#%d already initialized!\n", cpu);
-		for (;;) local_irq_enable();
+		for (;;)
+			local_irq_enable();
 	}
 
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
@@ -1159,13 +1167,7 @@ void __cpuinit cpu_init(void)
 	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
 #endif
 
-	/* Clear all 6 debug registers: */
-	set_debugreg(0, 0);
-	set_debugreg(0, 1);
-	set_debugreg(0, 2);
-	set_debugreg(0, 3);
-	set_debugreg(0, 6);
-	set_debugreg(0, 7);
+	clear_all_debug_regs();
 
 	/*
 	 * Force FPU initialization:
@@ -1186,5 +1188,4 @@ void __cpuinit cpu_init(void)
 	xsave_init();
 }
 
-
 #endif

commit 3f76a183de8ad3aeb7425f3d9685bb6003abd1a5
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:54 2009 +1030

    x86: unify cpu_callin_mask/cpu_callout_mask/cpu_initialized_mask/cpu_sibling_setup_mask
    
    Impact: cleanup
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 826d5c876278..82f6cc045ad0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -41,8 +41,6 @@
 
 #include "cpu.h"
 
-#ifdef CONFIG_X86_64
-
 /* all of these masks are initialized in setup_cpu_local_masks() */
 cpumask_var_t cpu_callin_mask;
 cpumask_var_t cpu_callout_mask;
@@ -60,16 +58,6 @@ void __init setup_cpu_local_masks(void)
 	alloc_bootmem_cpumask_var(&cpu_sibling_setup_mask);
 }
 
-#else /* CONFIG_X86_32 */
-
-cpumask_t cpu_callin_map;
-cpumask_t cpu_callout_map;
-cpumask_t cpu_initialized;
-cpumask_t cpu_sibling_setup_map;
-
-#endif /* CONFIG_X86_32 */
-
-
 static struct cpu_dev *this_cpu __cpuinitdata;
 
 DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {

commit 13c6c53282d99c82e79b02477efd2c1e30a991ef
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Mar 12 12:37:34 2009 +0000

    x86, 32-bit: also use cpuinfo_x86's x86_{phys,virt}_bits members
    
    Impact: 32/64-bit consolidation
    
    In a first step, this allows fixing phys_addr_valid() for PAE (which
    until now reported all addresses to be valid). Subsequently, this will
    also allow simplifying some MTRR handling code.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <49B9101E.76E4.0078.0@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 826d5c876278..a95e9480bb9c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -549,13 +549,15 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 		}
 	}
 
-#ifdef CONFIG_X86_64
 	if (c->extended_cpuid_level >= 0x80000008) {
 		u32 eax = cpuid_eax(0x80000008);
 
 		c->x86_virt_bits = (eax >> 8) & 0xff;
 		c->x86_phys_bits = eax & 0xff;
 	}
+#ifdef CONFIG_X86_32
+	else if (cpu_has(c, X86_FEATURE_PAE) || cpu_has(c, X86_FEATURE_PSE36))
+		c->x86_phys_bits = 36;
 #endif
 
 	if (c->extended_cpuid_level >= 0x80000007)
@@ -602,8 +604,12 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_64
 	c->x86_clflush_size = 64;
+	c->x86_phys_bits = 36;
+	c->x86_virt_bits = 48;
 #else
 	c->x86_clflush_size = 32;
+	c->x86_phys_bits = 32;
+	c->x86_virt_bits = 32;
 #endif
 	c->x86_cache_alignment = c->x86_clflush_size;
 
@@ -726,9 +732,13 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	c->x86_coreid_bits = 0;
 #ifdef CONFIG_X86_64
 	c->x86_clflush_size = 64;
+	c->x86_phys_bits = 36;
+	c->x86_virt_bits = 48;
 #else
 	c->cpuid_level = -1;	/* CPUID not detected */
 	c->x86_clflush_size = 32;
+	c->x86_phys_bits = 32;
+	c->x86_virt_bits = 32;
 #endif
 	c->x86_cache_alignment = c->x86_clflush_size;
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);

commit 02dde8b45c5460794b9052d7c12939fe3eb63c2c
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Mar 12 12:08:49 2009 +0000

    x86: move various CPU initialization objects into .cpuinit.rodata
    
    Impact: debuggability and micro-optimization
    
    Putting whatever is possible into the (final) .rodata section increases
    the likelihood of catching memory corruption bugs early, and reduces
    false cache line sharing.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <49B90961.76E4.0078.0@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f8869978bbb7..54cbe7690f93 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -70,7 +70,7 @@ cpumask_t cpu_sibling_setup_map;
 #endif /* CONFIG_X86_32 */
 
 
-static struct cpu_dev *this_cpu __cpuinitdata;
+static const struct cpu_dev *this_cpu __cpuinitdata;
 
 DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 #ifdef CONFIG_X86_64
@@ -274,9 +274,9 @@ static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
  */
 
 /* Look up CPU names by table lookup. */
-static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
+static const char *__cpuinit table_lookup_model(struct cpuinfo_x86 *c)
 {
-	struct cpu_model_info *info;
+	const struct cpu_model_info *info;
 
 	if (c->x86_model >= 16)
 		return NULL;	/* Range check */
@@ -321,7 +321,7 @@ void switch_to_new_gdt(int cpu)
 	load_percpu_segment(cpu);
 }
 
-static struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
+static const struct cpu_dev *__cpuinitdata cpu_devs[X86_VENDOR_NUM] = {};
 
 static void __cpuinit default_init(struct cpuinfo_x86 *c)
 {
@@ -340,7 +340,7 @@ static void __cpuinit default_init(struct cpuinfo_x86 *c)
 #endif
 }
 
-static struct cpu_dev __cpuinitdata default_cpu = {
+static const struct cpu_dev __cpuinitconst default_cpu = {
 	.c_init	= default_init,
 	.c_vendor = "Unknown",
 	.c_x86_vendor = X86_VENDOR_UNKNOWN,
@@ -634,12 +634,12 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 void __init early_cpu_init(void)
 {
-	struct cpu_dev **cdev;
+	const struct cpu_dev *const *cdev;
 	int count = 0;
 
 	printk("KERNEL supported cpus:\n");
 	for (cdev = __x86_cpu_dev_start; cdev < __x86_cpu_dev_end; cdev++) {
-		struct cpu_dev *cpudev = *cdev;
+		const struct cpu_dev *cpudev = *cdev;
 		unsigned int j;
 
 		if (count >= X86_VENDOR_NUM)
@@ -768,7 +768,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 	/* If the model name is still unset, do table lookup. */
 	if (!c->x86_model_id[0]) {
-		char *p;
+		const char *p;
 		p = table_lookup_model(c);
 		if (p)
 			strcpy(c->x86_model_id, p);
@@ -847,7 +847,7 @@ struct msr_range {
 	unsigned max;
 };
 
-static struct msr_range msr_range_array[] __cpuinitdata = {
+static const struct msr_range msr_range_array[] __cpuinitconst = {
 	{ 0x00000000, 0x00000418},
 	{ 0xc0000000, 0xc000040b},
 	{ 0xc0010000, 0xc0010142},
@@ -894,7 +894,7 @@ __setup("noclflush", setup_noclflush);
 
 void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 {
-	char *vendor = NULL;
+	const char *vendor = NULL;
 
 	if (c->x86_vendor < X86_VENDOR_NUM)
 		vendor = this_cpu->c_vendor;

commit 8c5dfd25519bf302ba43daa59976c4d675a594a7
Author: Stoyan Gaydarov <stoyboyker@gmail.com>
Date:   Tue Mar 10 00:10:32 2009 -0500

    x86: BUG to BUG_ON changes
    
    Impact: cleanup
    
    Signed-off-by: Stoyan Gaydarov <stoyboyker@gmail.com>
    LKML-Reference: <1236661850-8237-8-git-send-email-stoyboyker@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 826d5c876278..f8869978bbb7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1078,8 +1078,7 @@ void __cpuinit cpu_init(void)
 
 	atomic_inc(&init_mm.mm_count);
 	me->active_mm = &init_mm;
-	if (me->mm)
-		BUG();
+	BUG_ON(me->mm);
 	enter_lazy_tlb(&init_mm, me);
 
 	load_sp0(t, &current->thread);
@@ -1145,8 +1144,7 @@ void __cpuinit cpu_init(void)
 	 */
 	atomic_inc(&init_mm.mm_count);
 	curr->active_mm = &init_mm;
-	if (curr->mm)
-		BUG();
+	BUG_ON(curr->mm);
 	enter_lazy_tlb(&init_mm, curr);
 
 	load_sp0(t, thread);

commit e641f5f525acb163ba71d92de79c9c7366deae03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 14:02:01 2009 +0100

    x86, apic: remove duplicate asm/apic.h inclusions
    
    Impact: cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 41f3788ec9b9..826d5c876278 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -23,11 +23,9 @@
 #include <asm/smp.h>
 #include <asm/cpu.h>
 #include <asm/cpumask.h>
-#ifdef CONFIG_X86_LOCAL_APIC
-#include <asm/mpspec.h>
-#include <asm/apic.h>
-#include <asm/apic.h>
 #include <asm/apic.h>
+
+#ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/uv/uv.h>
 #endif
 

commit 7b6aa335ca1a845c2262ec7a595b4521bca0f79d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 13:58:15 2009 +0100

    x86, apic: remove genapic.h
    
    Impact: cleanup
    
    Remove genapic.h and remove all references to it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4b5d13e472d6..41f3788ec9b9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -26,8 +26,8 @@
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
-#include <asm/genapic.h>
-#include <asm/genapic.h>
+#include <asm/apic.h>
+#include <asm/apic.h>
 #include <asm/uv/uv.h>
 #endif
 

commit 0b6de0092244c98b5ba1abda34c92470a20e0d0c
Merge: 37a25424252b f62bae5009c1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 17:20:11 2009 +0100

    Merge branch 'x86/apic' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/cpu/perfctr-watchdog.c

commit 06cd9a7dc8a58186060a91b6ddc031057435fd34
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Feb 16 17:29:58 2009 -0800

    x86: add x2apic config
    
    Impact: cleanup
    
    so could deselect x2apic
    and INTR_REMAP will select x2apic
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4db150ed446d..4b5d13e472d6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1051,7 +1051,7 @@ void __cpuinit cpu_init(void)
 	barrier();
 
 	check_efer();
-	if (cpu != 0 && x2apic)
+	if (cpu != 0)
 		enable_x2apic();
 
 	/*

commit 494df596f9c315e20523894caa2a2938db3e5d8d
Merge: 970ec1a8213c 98c061b6cf2e f6db44df5bd3 22796b157254 694aa960608d be716615fe59 c99608637eac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 12:07:00 2009 +0100

    Merge branches 'x86/acpi', 'x86/apic', 'x86/cpudetect', 'x86/headers', 'x86/paravirt', 'x86/urgent' and 'x86/xen'; commit 'v2.6.29-rc5' into x86/core

commit f6db44df5bd39ed33883786d35342759af796e4a
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat Feb 14 23:59:18 2009 -0800

    x86: fix typo in filter_cpuid_features()
    
    Impact: fix wrong disabling of cpu features
    
    an amd system got this strange output:
    
     CPU: CPU feature monitor disabled due to lack of CPUID level 0x5
    
    but in /proc/cpuinfo I have:
    
     cpuid level    : 5
    
    on intel system:
    
     CPU: CPU feature monitor disabled due to lack of CPUID level 0x5
     CPU: CPU feature dca disabled due to lack of CPUID level 0x9
    
    but in /proc/cpuinfo i have:
    
     cpuid level     : 11
    
    Tt turns out there is a typo, and we should use level member in df.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 21f086b4c1a8..32093d08d872 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -241,9 +241,9 @@ static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 		 * signs here...
 		 */
 		if (cpu_has(c, df->feature) &&
-		    ((s32)df->feature < 0 ?
-		     (u32)df->feature > (u32)c->extended_cpuid_level :
-		     (s32)df->feature > (s32)c->cpuid_level)) {
+		    ((s32)df->level < 0 ?
+		     (u32)df->level > (u32)c->extended_cpuid_level :
+		     (s32)df->level > (s32)c->cpuid_level)) {
 			clear_cpu_cap(c, df->feature);
 			if (warn)
 				printk(KERN_WARNING
@@ -253,7 +253,7 @@ static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 				       df->level);
 		}
 	}
-}	
+}
 
 /*
  * Naming convention should be: <Name> [(<Codename>)]

commit b1864e9a1afef41709886072c6e6248def0386f4
Merge: e9c4ffb11f0b 7032e8696726
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:49:38 2009 +0100

    Merge branch 'x86/core' into perfcounters/core
    
    Conflicts:
            arch/x86/Kconfig
            arch/x86/kernel/apic.c
            arch/x86/kernel/setup_percpu.c

commit ab639f3593f0b5e4439d549831442c18c3baf989
Merge: f8a6b2b9cee2 58105ef18571
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:45:09 2009 +0100

    Merge branch 'core/percpu' into x86/core

commit 60a5317ff0f42dd313094b88f809f63041568b08
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 9 22:17:40 2009 +0900

    x86: implement x86_32 stack protector
    
    Impact: stack protector for x86_32
    
    Implement stack protector for x86_32.  GDT entry 28 is used for it.
    It's set to point to stack_canary-20 and have the length of 24 bytes.
    CONFIG_CC_STACKPROTECTOR turns off CONFIG_X86_32_LAZY_GS and sets %gs
    to the stack canary segment on entry.  As %gs is otherwise unused by
    the kernel, the canary can be anywhere.  It's defined as a percpu
    variable.
    
    x86_32 exception handlers take register frame on stack directly as
    struct pt_regs.  With -fstack-protector turned on, gcc copies the
    whole structure after the stack canary and (of course) doesn't copy
    back on return thus losing all changed.  For now, -fno-stack-protector
    is added to all files which contain those functions.  We definitely
    need something better.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 41b0de6df873..260fe4cb2c82 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -39,6 +39,7 @@
 #include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/hypervisor.h>
+#include <asm/stackprotector.h>
 
 #include "cpu.h"
 
@@ -122,6 +123,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 
 	[GDT_ENTRY_ESPFIX_SS] = { { { 0x00000000, 0x00c09200 } } },
 	[GDT_ENTRY_PERCPU] = { { { 0x0000ffff, 0x00cf9200 } } },
+	GDT_STACK_CANARY_INIT
 #endif
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
@@ -261,6 +263,7 @@ void load_percpu_segment(int cpu)
 	loadsegment(gs, 0);
 	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
 #endif
+	load_stack_canary_segment();
 }
 
 /* Current gdt points %fs at the "master" per-cpu area: after this,
@@ -946,16 +949,21 @@ unsigned long kernel_eflags;
  */
 DEFINE_PER_CPU(struct orig_ist, orig_ist);
 
-#else
+#else	/* x86_64 */
+
+#ifdef CONFIG_CC_STACKPROTECTOR
+DEFINE_PER_CPU(unsigned long, stack_canary);
+#endif
 
-/* Make sure %fs is initialized properly in idle threads */
+/* Make sure %fs and %gs are initialized properly in idle threads */
 struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 {
 	memset(regs, 0, sizeof(struct pt_regs));
 	regs->fs = __KERNEL_PERCPU;
+	regs->gs = __KERNEL_STACK_CANARY;
 	return regs;
 }
-#endif
+#endif	/* x86_64 */
 
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
@@ -1120,9 +1128,6 @@ void __cpuinit cpu_init(void)
 	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
 #endif
 
-	/* Clear %gs. */
-	asm volatile ("mov %0, %%gs" : : "r" (0));
-
 	/* Clear all 6 debug registers: */
 	set_debugreg(0, 0);
 	set_debugreg(0, 1);

commit eca217b36e5d7d4377493d5cedd89105e66a5a72
Merge: 54a353a0f845 e4d0407185cd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 9 12:16:59 2009 +0100

    Merge branch 'x86/paravirt' into x86/apic
    
    Conflicts:
            arch/x86/mach-voyager/voyager_smp.c

commit 2add8e235cbe0dcd672c33fc322754e15500238c
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Feb 8 09:58:39 2009 -0500

    x86: use linker to offset symbols by __per_cpu_load
    
    Impact: cleanup and bug fix
    
    Use the linker to create symbols for certain per-cpu variables
    that are offset by __per_cpu_load.  This allows the removal of
    the runtime fixup of the GDT pointer, which fixes a bug with
    resume reported by Jiri Slaby.
    
    Reported-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0f73ea423089..41b0de6df873 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -902,12 +902,8 @@ struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE);
-#ifdef CONFIG_SMP
-DEFINE_PER_CPU(char *, irq_stack_ptr);	/* will be set during per cpu init */
-#else
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
-	per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
-#endif
+	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
 
 DEFINE_PER_CPU(unsigned long, kernel_stack) =
 	(unsigned long)&init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;

commit 11e3a840cd5b731cdd8f6f956dfae78a8046d09c
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Jan 30 17:47:54 2009 +0900

    x86: split loading percpu segments from loading gdt
    
    Impact: split out a function, no functional change
    
    Xen needs to be able to access percpu data from very early on.  For
    various reasons, it cannot also load the gdt at that time.   It does,
    however, have a pefectly functional gdt at that point, so there's no
    pressing need to reload the gdt.
    
    Split the function to load the segment registers off, so Xen can call
    it directly.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6eacd64b602e..0f73ea423089 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -253,6 +253,16 @@ static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
 
 __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 
+void load_percpu_segment(int cpu)
+{
+#ifdef CONFIG_X86_32
+	loadsegment(fs, __KERNEL_PERCPU);
+#else
+	loadsegment(gs, 0);
+	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
+#endif
+}
+
 /* Current gdt points %fs at the "master" per-cpu area: after this,
  * it's on the real one. */
 void switch_to_new_gdt(int cpu)
@@ -263,12 +273,8 @@ void switch_to_new_gdt(int cpu)
 	gdt_descr.size = GDT_SIZE - 1;
 	load_gdt(&gdt_descr);
 	/* Reload the per-cpu base */
-#ifdef CONFIG_X86_32
-	loadsegment(fs, __KERNEL_PERCPU);
-#else
-	loadsegment(gs, 0);
-	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
-#endif
+
+	load_percpu_segment(cpu);
 }
 
 static struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};

commit 552be871e67ff577ed36beb2f53d078b42304739
Author: Brian Gerst <brgerst@gmail.com>
Date:   Fri Jan 30 17:47:53 2009 +0900

    x86: pass in cpu number to switch_to_new_gdt()
    
    Impact: cleanup, prepare for xen boot fix.
    
    Xen needs to call this function very early to setup the GDT and
    per-cpu segments.  Remove the call to smp_processor_id() and just
    pass in the cpu number.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 652fdc9a757a..6eacd64b602e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -255,10 +255,9 @@ __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 
 /* Current gdt points %fs at the "master" per-cpu area: after this,
  * it's on the real one. */
-void switch_to_new_gdt(void)
+void switch_to_new_gdt(int cpu)
 {
 	struct desc_ptr gdt_descr;
-	int cpu = smp_processor_id();
 
 	gdt_descr.address = (long)get_cpu_gdt_table(cpu);
 	gdt_descr.size = GDT_SIZE - 1;
@@ -993,7 +992,7 @@ void __cpuinit cpu_init(void)
 	 * and set up the GDT descriptor:
 	 */
 
-	switch_to_new_gdt();
+	switch_to_new_gdt(cpu);
 	loadsegment(fs, 0);
 
 	load_idt((const struct desc_ptr *)&idt_descr);
@@ -1098,7 +1097,7 @@ void __cpuinit cpu_init(void)
 		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	load_idt(&idt_descr);
-	switch_to_new_gdt();
+	switch_to_new_gdt(cpu);
 
 	/*
 	 * Set up and load the per-CPU TSS and LDT

commit 1dcdd3d15ecea0c22a09d4d001a39d425fceff2c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 17:55:37 2009 +0100

    x86: remove mach_apic.h
    
    Spread mach_apic.h definitions into genapic.h. (with some knock-on effects
    on smp.h and apic.h.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 055b9c3a6600..c4bdc7f00207 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -26,7 +26,7 @@
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
-#include <mach_apic.h>
+#include <asm/genapic.h>
 #include <asm/genapic.h>
 #include <asm/uv/uv.h>
 #endif

commit cb8cc442dc7e07cb5438b357843ab4095ad73933
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 13:24:54 2009 +0100

    x86, apic: refactor ->phys_pkg_id()
    
    Refactor the ->phys_pkg_id() methods:
    
     - namespace separation
    
     - macro wrapper removal
    
     - open-coded calls to the methods in the generic code
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 93c491c4fe7f..055b9c3a6600 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -442,7 +442,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 		}
 
 		index_msb = get_count_order(smp_num_siblings);
-		c->phys_proc_id = phys_pkg_id(c->initial_apicid, index_msb);
+		c->phys_proc_id = apic->phys_pkg_id(c->initial_apicid, index_msb);
 
 		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
@@ -450,7 +450,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 
 		core_bits = get_count_order(c->x86_max_cores);
 
-		c->cpu_core_id = phys_pkg_id(c->initial_apicid, index_msb) &
+		c->cpu_core_id = apic->phys_pkg_id(c->initial_apicid, index_msb) &
 					       ((1 << core_bits) - 1);
 	}
 
@@ -686,7 +686,7 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		c->initial_apicid = (cpuid_ebx(1) >> 24) & 0xFF;
 #ifdef CONFIG_X86_32
 # ifdef CONFIG_X86_HT
-		c->apicid = phys_pkg_id(c->initial_apicid, 0);
+		c->apicid = apic->phys_pkg_id(c->initial_apicid, 0);
 # else
 		c->apicid = c->initial_apicid;
 # endif
@@ -733,7 +733,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 		this_cpu->c_identify(c);
 
 #ifdef CONFIG_X86_64
-	c->apicid = phys_pkg_id(c->initial_apicid, 0);
+	c->apicid = apic->phys_pkg_id(c->initial_apicid, 0);
 #endif
 
 	/*

commit d4c9a9f3d416cfa1f5ffbe09d864d069467fe693
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 13:31:22 2009 +0100

    x86, apic: unify phys_pkg_id()
    
    - unify the call signature of 64-bit to that of 32-bit
    
     - clean up the types all around
    
     - clean up namespace contamination
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 275e2cb43b91..93c491c4fe7f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -442,11 +442,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 		}
 
 		index_msb = get_count_order(smp_num_siblings);
-#ifdef CONFIG_X86_64
-		c->phys_proc_id = phys_pkg_id(index_msb);
-#else
 		c->phys_proc_id = phys_pkg_id(c->initial_apicid, index_msb);
-#endif
 
 		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
@@ -454,13 +450,8 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 
 		core_bits = get_count_order(c->x86_max_cores);
 
-#ifdef CONFIG_X86_64
-		c->cpu_core_id = phys_pkg_id(index_msb) &
-					       ((1 << core_bits) - 1);
-#else
 		c->cpu_core_id = phys_pkg_id(c->initial_apicid, index_msb) &
 					       ((1 << core_bits) - 1);
-#endif
 	}
 
 out:
@@ -742,7 +733,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 		this_cpu->c_identify(c);
 
 #ifdef CONFIG_X86_64
-	c->apicid = phys_pkg_id(0);
+	c->apicid = phys_pkg_id(c->initial_apicid, 0);
 #endif
 
 	/*

commit 74b6eb6b937df07d0757e8642b7538b07da4290f
Merge: 6a385db5ce7f 2d4d57db692e 8f6d86dc4178 b38b06659055 d5e397cb49b5 e56d0cfe7790 dbca1df48e89 fb746d0e1365 6522869c3466 d639bab8da86 042cbaf88ab4 5662a2f8e731 3b4b75700a24 30a0fb947a68
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 23:13:53 2009 +0100

    Merge branches 'x86/asm', 'x86/cleanups', 'x86/cpudetect', 'x86/debug', 'x86/doc', 'x86/header-fixes', 'x86/mm', 'x86/paravirt', 'x86/pat', 'x86/setup-v2', 'x86/subarch', 'x86/uaccess' and 'x86/urgent' into x86/core

commit 8f6d86dc4178957d9814b1784848012a927a3898
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jan 27 21:41:34 2009 +0100

    x86: cpu_init(): remove ugly #ifdef construct around debug register clear
    
    Impact: Cleanup
    
    While I was looking through the new and improved bootstrap code - great
    work that, thanks! I found the below a slight improvement.
    
    Remove unnecessary ugly #ifdef construct around debug register clear.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f00258462444..3f272d42d09a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1071,22 +1071,19 @@ void __cpuinit cpu_init(void)
 	 */
 	if (kgdb_connected && arch_kgdb_ops.correct_hw_break)
 		arch_kgdb_ops.correct_hw_break();
-	else {
+	else
 #endif
-	/*
-	 * Clear all 6 debug registers:
-	 */
-
-	set_debugreg(0UL, 0);
-	set_debugreg(0UL, 1);
-	set_debugreg(0UL, 2);
-	set_debugreg(0UL, 3);
-	set_debugreg(0UL, 6);
-	set_debugreg(0UL, 7);
-#ifdef CONFIG_KGDB
-	/* If the kgdb is connected no debug regs should be altered. */
+	{
+		/*
+		 * Clear all 6 debug registers:
+		 */
+		set_debugreg(0UL, 0);
+		set_debugreg(0UL, 1);
+		set_debugreg(0UL, 2);
+		set_debugreg(0UL, 3);
+		set_debugreg(0UL, 6);
+		set_debugreg(0UL, 7);
 	}
-#endif
 
 	fpu_init();
 

commit 4369f1fb7cd4cf777312f43e1cb9aa5504fc4125
Merge: 3ddeb51d9c83 cf3997f50762
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 27 12:03:24 2009 +0100

    Merge branch 'tj-percpu' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc into core/percpu
    
    Conflicts:
            arch/x86/kernel/setup_percpu.c
    
    Semantic conflict:
    
            arch/x86/kernel/cpu/common.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2697fbd5faf19c84c17441b1752bdcbdcfd1248c
Author: Brian Gerst <brgerst@gmail.com>
Date:   Tue Jan 27 12:56:48 2009 +0900

    x86: load new GDT after setting up boot cpu per-cpu area
    
    Impact: sync 32 and 64-bit code
    
    Merge load_gs_base() into switch_to_new_gdt().  Load the GDT and
    per-cpu state for the boot cpu when its new area is set up.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 67e30c8a282c..0c766b80d915 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -258,12 +258,17 @@ __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 void switch_to_new_gdt(void)
 {
 	struct desc_ptr gdt_descr;
+	int cpu = smp_processor_id();
 
-	gdt_descr.address = (long)get_cpu_gdt_table(smp_processor_id());
+	gdt_descr.address = (long)get_cpu_gdt_table(cpu);
 	gdt_descr.size = GDT_SIZE - 1;
 	load_gdt(&gdt_descr);
+	/* Reload the per-cpu base */
 #ifdef CONFIG_X86_32
-	asm("mov %0, %%fs" : : "r" (__KERNEL_PERCPU) : "memory");
+	loadsegment(fs, __KERNEL_PERCPU);
+#else
+	loadsegment(gs, 0);
+	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
 #endif
 }
 
@@ -968,10 +973,6 @@ void __cpuinit cpu_init(void)
 	struct task_struct *me;
 	int i;
 
-	loadsegment(fs, 0);
-	loadsegment(gs, 0);
-	load_gs_base(cpu);
-
 #ifdef CONFIG_NUMA
 	if (cpu != 0 && percpu_read(node_number) == 0 &&
 	    cpu_to_node(cpu) != NUMA_NO_NODE)
@@ -993,6 +994,8 @@ void __cpuinit cpu_init(void)
 	 */
 
 	switch_to_new_gdt();
+	loadsegment(fs, 0);
+
 	load_idt((const struct desc_ptr *)&idt_descr);
 
 	memset(me->thread.tls_array, 0, GDT_ENTRY_TLS_ENTRIES * 8);

commit 2f2f52bad72f5e1ca5d1b9ad00a7b57a8cbd9159
Author: Brian Gerst <brgerst@gmail.com>
Date:   Tue Jan 27 12:56:47 2009 +0900

    x86: move setup_cpu_local_masks()
    
    Impact: Code movement, no functional change.
    
    Move setup_cpu_local_masks() to kernel/cpu/common.c, where the
    masks are defined.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 99904f288d6a..67e30c8a282c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -52,6 +52,15 @@ cpumask_var_t cpu_initialized_mask;
 /* representing cpus for which sibling maps can be computed */
 cpumask_var_t cpu_sibling_setup_mask;
 
+/* correctly size the local cpu masks */
+void setup_cpu_local_masks(void)
+{
+	alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+	alloc_bootmem_cpumask_var(&cpu_callin_mask);
+	alloc_bootmem_cpumask_var(&cpu_callout_mask);
+	alloc_bootmem_cpumask_var(&cpu_sibling_setup_mask);
+}
+
 #else /* CONFIG_X86_32 */
 
 cpumask_t cpu_callin_map;

commit b38b0665905538e76e26f2a4c686179abb1f69f6
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Jan 23 17:20:50 2009 -0800

    x86: filter CPU features dependent on unavailable CPUID levels
    
    Impact: Fixes potential crashes on misconfigured systems.
    
    Some CPU features require specific CPUID levels to be available in
    order to function, as they contain information about the operation of
    a specific feature.  However, some BIOSes and virtualization software
    provide the ability to mask CPUID levels in order to support legacy
    operating systems.  We try to enable such CPUID levels when we know
    how to do it, but for the remaining cases, filter out such CPU
    features when there is no way for us to support them.
    
    Do this in one place, in the CPUID code, with a table-driven approach.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0f8656361e04..21f086b4c1a8 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -212,6 +212,49 @@ static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 }
 #endif
 
+/*
+ * Some CPU features depend on higher CPUID levels, which may not always
+ * be available due to CPUID level capping or broken virtualization
+ * software.  Add those features to this table to auto-disable them.
+ */
+struct cpuid_dependent_feature {
+	u32 feature;
+	u32 level;
+};
+static const struct cpuid_dependent_feature __cpuinitconst
+cpuid_dependent_features[] = {
+	{ X86_FEATURE_MWAIT,		0x00000005 },
+	{ X86_FEATURE_DCA,		0x00000009 },
+	{ X86_FEATURE_XSAVE,		0x0000000d },
+	{ 0, 0 }
+};
+
+static void __cpuinit filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
+{
+	const struct cpuid_dependent_feature *df;
+	for (df = cpuid_dependent_features; df->feature; df++) {
+		/*
+		 * Note: cpuid_level is set to -1 if unavailable, but
+		 * extended_extended_level is set to 0 if unavailable
+		 * and the legitimate extended levels are all negative
+		 * when signed; hence the weird messing around with
+		 * signs here...
+		 */
+		if (cpu_has(c, df->feature) &&
+		    ((s32)df->feature < 0 ?
+		     (u32)df->feature > (u32)c->extended_cpuid_level :
+		     (s32)df->feature > (s32)c->cpuid_level)) {
+			clear_cpu_cap(c, df->feature);
+			if (warn)
+				printk(KERN_WARNING
+				       "CPU: CPU feature %s disabled "
+				       "due to lack of CPUID level 0x%x\n",
+				       x86_cap_flags[df->feature],
+				       df->level);
+		}
+	}
+}	
+
 /*
  * Naming convention should be: <Name> [(<Codename>)]
  * This table only is used unless init_<vendor>() below doesn't set it;
@@ -573,6 +616,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 #ifdef CONFIG_SMP
 	c->cpu_index = boot_cpu_id;
 #endif
+	filter_cpuid_features(c, false);
 }
 
 void __init early_cpu_init(void)
@@ -706,6 +750,9 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	 * we do "generic changes."
 	 */
 
+	/* Filter out anything that depends on CPUID levels we don't have */
+	filter_cpuid_features(c, true);
+
 	/* If the model name is still unset, do table lookup. */
 	if (!c->x86_model_id[0]) {
 		char *p;

commit 75a048119e76540d73132cfc8e0fa0c0a8bb6c83
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Jan 22 16:17:05 2009 -0800

    x86: handle PAT more like other CPU features
    
    Impact: Cleanup
    
    When PAT was originally introduced, it was handled specially for a few
    reasons:
    
    - PAT bugs are hard to track down, so we wanted to maintain a
      whitelist of CPUs.
    - The i386 and x86-64 CPUID code was not yet unified.
    
    Both of these are now obsolete, so handle PAT like any other features,
    including ordinary feature blacklisting due to known bugs.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 83492b1f93b1..0f8656361e04 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -570,8 +570,6 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	if (this_cpu->c_early_init)
 		this_cpu->c_early_init(c);
 
-	validate_pat_support(c);
-
 #ifdef CONFIG_SMP
 	c->cpu_index = boot_cpu_id;
 #endif

commit bfe2a3c3b5bf479788d5d5c5561346be6b169043
Merge: 77835492ed48 35d266a24796
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 23 10:20:15 2009 +0100

    Merge branch 'core/percpu' into perfcounters/core
    
    Conflicts:
            arch/x86/include/asm/hardirq_32.h
            arch/x86/include/asm/hardirq_64.h
    
    Semantic merge:
            arch/x86/include/asm/hardirq.h
            [ added apic_perf_irqs field. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit bdbcdd48883940bbd8d17eb01172d58a261a413a
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Jan 21 17:26:06 2009 +0900

    x86: uv cleanup
    
    Impact: cleanup
    
    Make the following uv related cleanups.
    
    * collect visible uv related definitions and interfaces into uv/uv.h
      and use it.  this cleans up the messy situation where on 64bit, uv
      is defined properly, on 32bit generic it's dummy and on the rest
      undefined.  after this clean up, uv is defined on 64 and dummy on
      32.
    
    * update uv_flush_tlb_others() such that it takes cpumask of
      to-be-flushed cpus as argument, instead of that minus self, and
      returns yet-to-be-flushed cpumask, instead of modifying the passed
      in parameter.  this interface change will ease dummy implementation
      of uv_flush_tlb_others() and makes uv tlb flush related stuff
      defined in tlb_uv proper.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fbebbcec001b..99904f288d6a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -28,6 +28,7 @@
 #include <asm/apic.h>
 #include <mach_apic.h>
 #include <asm/genapic.h>
+#include <asm/uv/uv.h>
 #endif
 
 #include <asm/pgtable.h>

commit 0dd76d736eeb3e0ef86c5b103b47ae0e15edebad
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jan 21 17:26:05 2009 +0900

    x86: set %fs to __KERNEL_PERCPU unconditionally for x86_32
    
    Impact: cleanup
    
    %fs is currently set to __KERNEL_DS at boot, and conditionally
    switched to __KERNEL_PERCPU for secondary cpus.  Instead, initialize
    GDT_ENTRY_PERCPU to the same attributes as GDT_ENTRY_KERNEL_DS and
    set %fs to __KERNEL_PERCPU unconditionally.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a8f0dedcb0cc..fbebbcec001b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -111,7 +111,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	[GDT_ENTRY_APMBIOS_BASE+2] = { { { 0x0000ffff, 0x00409200 } } },
 
 	[GDT_ENTRY_ESPFIX_SS] = { { { 0x00000000, 0x00c09200 } } },
-	[GDT_ENTRY_PERCPU] = { { { 0x00000000, 0x00000000 } } },
+	[GDT_ENTRY_PERCPU] = { { { 0x0000ffff, 0x00cf9200 } } },
 #endif
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);

commit 06deef892c7327992434917fb6592c233430803d
Author: Brian Gerst <brgerst@gmail.com>
Date:   Wed Jan 21 17:26:05 2009 +0900

    x86: clean up gdt_page definition
    
    Impact: cleanup && more compact percpu area layout with future changes
    
    Move 64-bit GDT to page-aligned section and clean up comment
    formatting.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3887fcf6e519..a8f0dedcb0cc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -63,23 +63,23 @@ cpumask_t cpu_sibling_setup_map;
 
 static struct cpu_dev *this_cpu __cpuinitdata;
 
+DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 #ifdef CONFIG_X86_64
-/* We need valid kernel segments for data and code in long mode too
- * IRET will check the segment types  kkeil 2000/10/28
- * Also sysret mandates a special GDT layout
- */
-/* The TLS descriptors are currently at a different place compared to i386.
-   Hopefully nobody expects them at a fixed place (Wine?) */
-DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
+	/*
+	 * We need valid kernel segments for data and code in long mode too
+	 * IRET will check the segment types  kkeil 2000/10/28
+	 * Also sysret mandates a special GDT layout
+	 *
+	 * The TLS descriptors are currently at a different place compared to i386.
+	 * Hopefully nobody expects them at a fixed place (Wine?)
+	 */
 	[GDT_ENTRY_KERNEL32_CS] = { { { 0x0000ffff, 0x00cf9b00 } } },
 	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00af9b00 } } },
 	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9300 } } },
 	[GDT_ENTRY_DEFAULT_USER32_CS] = { { { 0x0000ffff, 0x00cffb00 } } },
 	[GDT_ENTRY_DEFAULT_USER_DS] = { { { 0x0000ffff, 0x00cff300 } } },
 	[GDT_ENTRY_DEFAULT_USER_CS] = { { { 0x0000ffff, 0x00affb00 } } },
-} };
 #else
-DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00cf9a00 } } },
 	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9200 } } },
 	[GDT_ENTRY_DEFAULT_USER_CS] = { { { 0x0000ffff, 0x00cffa00 } } },
@@ -112,8 +112,8 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 
 	[GDT_ENTRY_ESPFIX_SS] = { { { 0x00000000, 0x00c09200 } } },
 	[GDT_ENTRY_PERCPU] = { { { 0x00000000, 0x00000000 } } },
-} };
 #endif
+} };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
 #ifdef CONFIG_X86_32

commit 0d974d4592708f85044751817da4b7016e1b0602
Author: Brian Gerst <brgerst@gmail.com>
Date:   Sun Jan 18 19:52:25 2009 -0500

    x86: remove pda.h
    
    Impact: cleanup
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 098934e72a16..3887fcf6e519 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -30,7 +30,6 @@
 #include <asm/genapic.h>
 #endif
 
-#include <asm/pda.h>
 #include <asm/pgtable.h>
 #include <asm/processor.h>
 #include <asm/desc.h>

commit 947e76cdc34c782fc947313d4331380686eebbad
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 12:21:28 2009 +0900

    x86: move stack_canary into irq_stack
    
    Impact: x86_64 percpu area layout change, irq_stack now at the beginning
    
    Now that the PDA is empty except for the stack canary, it can be removed.
    The irqstack is moved to the start of the per-cpu section.  If the stack
    protector is enabled, the canary overlaps the bottom 48 bytes of the irqstack.
    
    tj: * updated subject
        * dropped asm relocation of irq_stack_ptr
        * updated comments a bit
        * rebased on top of stack canary changes
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f83a4d6160f0..098934e72a16 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -881,12 +881,13 @@ __setup("clearcpuid=", setup_disablecpuid);
 #ifdef CONFIG_X86_64
 struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
-DEFINE_PER_CPU_PAGE_ALIGNED(char[IRQ_STACK_SIZE], irq_stack);
+DEFINE_PER_CPU_FIRST(union irq_stack_union,
+		     irq_stack_union) __aligned(PAGE_SIZE);
 #ifdef CONFIG_SMP
 DEFINE_PER_CPU(char *, irq_stack_ptr);	/* will be set during per cpu init */
 #else
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
-	per_cpu_var(irq_stack) + IRQ_STACK_SIZE - 64;
+	per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
 #endif
 
 DEFINE_PER_CPU(unsigned long, kernel_stack) =
@@ -960,7 +961,7 @@ void __cpuinit cpu_init(void)
 
 	loadsegment(fs, 0);
 	loadsegment(gs, 0);
-	load_pda_offset(cpu);
+	load_gs_base(cpu);
 
 #ifdef CONFIG_NUMA
 	if (cpu != 0 && percpu_read(node_number) == 0 &&

commit 8ce031972b40da58c268caba8c5ea3c0856d7131
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 12:21:27 2009 +0900

    x86: remove pda_init()
    
    Impact: cleanup
    
    Copy the code to cpu_init() to satisfy the requirement that the cpu
    be reinitialized.  Remove all other calls, since the segments are
    already initialized in head_64.S.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7976a6a0f65c..f83a4d6160f0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -895,15 +895,6 @@ EXPORT_PER_CPU_SYMBOL(kernel_stack);
 
 DEFINE_PER_CPU(unsigned int, irq_count) = -1;
 
-void __cpuinit pda_init(int cpu)
-{
-	/* Setup up data that may be needed in __get_free_pages early */
-	loadsegment(fs, 0);
-	loadsegment(gs, 0);
-
-	load_pda_offset(cpu);
-}
-
 static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ])
 	__aligned(PAGE_SIZE);
@@ -967,9 +958,9 @@ void __cpuinit cpu_init(void)
 	struct task_struct *me;
 	int i;
 
-	/* CPU 0 is initialised in head64.c */
-	if (cpu != 0)
-		pda_init(cpu);
+	loadsegment(fs, 0);
+	loadsegment(gs, 0);
+	load_pda_offset(cpu);
 
 #ifdef CONFIG_NUMA
 	if (cpu != 0 && percpu_read(node_number) == 0 &&

commit af37501c792107c2bde1524bdae38d9a247b841a
Merge: d859e29fe34c 99937d6455ce
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 18 18:15:49 2009 +0100

    Merge branch 'core/percpu' into perfcounters/core
    
    Conflicts:
            arch/x86/include/asm/pda.h
    
    We merge tip/core/percpu into tip/perfcounters/core because of a
    semantic and contextual conflict: the former eliminates the PDA,
    while the latter extends it with apic_perf_irqs field.
    
    Resolve the conflict by moving the new field to the irq_cpustat
    structure on 64-bit too.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e7a22c1ebcc1caa8178df1819d05128bb5b45ab9
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:59 2009 +0900

    x86-64: Move nodenumber from PDA to per-cpu.
    
    tj: * s/nodenumber/node_number/
        * removed now unused pda variable from pda_init()
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e2323ecce1d3..7976a6a0f65c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -897,18 +897,11 @@ DEFINE_PER_CPU(unsigned int, irq_count) = -1;
 
 void __cpuinit pda_init(int cpu)
 {
-	struct x8664_pda *pda = cpu_pda(cpu);
-
 	/* Setup up data that may be needed in __get_free_pages early */
 	loadsegment(fs, 0);
 	loadsegment(gs, 0);
 
 	load_pda_offset(cpu);
-
-	if (cpu != 0) {
-		if (pda->nodenumber == 0 && cpu_to_node(cpu) != NUMA_NO_NODE)
-			pda->nodenumber = cpu_to_node(cpu);
-	}
 }
 
 static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
@@ -978,6 +971,12 @@ void __cpuinit cpu_init(void)
 	if (cpu != 0)
 		pda_init(cpu);
 
+#ifdef CONFIG_NUMA
+	if (cpu != 0 && percpu_read(node_number) == 0 &&
+	    cpu_to_node(cpu) != NUMA_NO_NODE)
+		percpu_write(node_number, cpu_to_node(cpu));
+#endif
+
 	me = current;
 
 	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask))

commit 5689553076c4a67b83426b076082c63085b7567a
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move irqcount from PDA to per-cpu.
    
    tj: s/irqcount/irq_count/
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 06b6290088f4..e2323ecce1d3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -893,6 +893,8 @@ DEFINE_PER_CPU(unsigned long, kernel_stack) =
 	(unsigned long)&init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;
 EXPORT_PER_CPU_SYMBOL(kernel_stack);
 
+DEFINE_PER_CPU(unsigned int, irq_count) = -1;
+
 void __cpuinit pda_init(int cpu)
 {
 	struct x8664_pda *pda = cpu_pda(cpu);
@@ -903,8 +905,6 @@ void __cpuinit pda_init(int cpu)
 
 	load_pda_offset(cpu);
 
-	pda->irqcount = -1;
-
 	if (cpu != 0) {
 		if (pda->nodenumber == 0 && cpu_to_node(cpu) != NUMA_NO_NODE)
 			pda->nodenumber = cpu_to_node(cpu);

commit 9af45651f1f7c89942e016a1a00a7ebddfa727f8
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move kernelstack from PDA to per-cpu.
    
    Also clean up PER_CPU_VAR usage in xen-asm_64.S
    
    tj: * remove now unused stack_thread_info()
        * s/kernelstack/kernel_stack/
        * added FIXME comment in xen-asm_64.S
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b50e38d16888..06b6290088f4 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -889,6 +889,10 @@ DEFINE_PER_CPU(char *, irq_stack_ptr) =
 	per_cpu_var(irq_stack) + IRQ_STACK_SIZE - 64;
 #endif
 
+DEFINE_PER_CPU(unsigned long, kernel_stack) =
+	(unsigned long)&init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;
+EXPORT_PER_CPU_SYMBOL(kernel_stack);
+
 void __cpuinit pda_init(int cpu)
 {
 	struct x8664_pda *pda = cpu_pda(cpu);
@@ -900,8 +904,6 @@ void __cpuinit pda_init(int cpu)
 	load_pda_offset(cpu);
 
 	pda->irqcount = -1;
-	pda->kernelstack = (unsigned long)stack_thread_info() -
-				 PDA_STACKOFFSET + THREAD_SIZE;
 
 	if (cpu != 0) {
 		if (pda->nodenumber == 0 && cpu_to_node(cpu) != NUMA_NO_NODE)

commit c6f5e0acd5d12ee23f701f15889872e67b47caa6
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move current task from PDA to per-cpu and consolidate with 32-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4221e920886d..b50e38d16888 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -903,10 +903,7 @@ void __cpuinit pda_init(int cpu)
 	pda->kernelstack = (unsigned long)stack_thread_info() -
 				 PDA_STACKOFFSET + THREAD_SIZE;
 
-	if (cpu == 0) {
-		/* others are initialized in smpboot.c */
-		pda->pcurrent = &init_task;
-	} else {
+	if (cpu != 0) {
 		if (pda->nodenumber == 0 && cpu_to_node(cpu) != NUMA_NO_NODE)
 			pda->nodenumber = cpu_to_node(cpu);
 	}

commit ea9279066de44053d0c20ea855bc9f4706652d84
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Move cpu number from PDA to per-cpu and consolidate with 32-bit.
    
    tj: moved cpu_number definition out of CONFIG_HAVE_SETUP_PER_CPU_AREA
        for voyager.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b6d7eec0be77..4221e920886d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -899,7 +899,6 @@ void __cpuinit pda_init(int cpu)
 
 	load_pda_offset(cpu);
 
-	pda->cpunumber = cpu;
 	pda->irqcount = -1;
 	pda->kernelstack = (unsigned long)stack_thread_info() -
 				 PDA_STACKOFFSET + THREAD_SIZE;

commit 92d65b2371d86d40807e1dbfdccadc4d501edcde
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Convert exception stacks to per-cpu
    
    Move the exception stacks to per-cpu, removing specific allocation code.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 496f0a01919b..b6d7eec0be77 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -913,8 +913,9 @@ void __cpuinit pda_init(int cpu)
 	}
 }
 
-static char boot_exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ +
-				  DEBUG_STKSZ] __page_aligned_bss;
+static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks
+	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ])
+	__aligned(PAGE_SIZE);
 
 extern asmlinkage void ignore_sysret(void);
 
@@ -972,15 +973,12 @@ void __cpuinit cpu_init(void)
 	struct tss_struct *t = &per_cpu(init_tss, cpu);
 	struct orig_ist *orig_ist = &per_cpu(orig_ist, cpu);
 	unsigned long v;
-	char *estacks = NULL;
 	struct task_struct *me;
 	int i;
 
 	/* CPU 0 is initialised in head64.c */
 	if (cpu != 0)
 		pda_init(cpu);
-	else
-		estacks = boot_exception_stacks;
 
 	me = current;
 
@@ -1014,18 +1012,13 @@ void __cpuinit cpu_init(void)
 	 * set up and load the per-CPU TSS
 	 */
 	if (!orig_ist->ist[0]) {
-		static const unsigned int order[N_EXCEPTION_STACKS] = {
-		  [0 ... N_EXCEPTION_STACKS - 1] = EXCEPTION_STACK_ORDER,
-		  [DEBUG_STACK - 1] = DEBUG_STACK_ORDER
+		static const unsigned int sizes[N_EXCEPTION_STACKS] = {
+		  [0 ... N_EXCEPTION_STACKS - 1] = EXCEPTION_STKSZ,
+		  [DEBUG_STACK - 1] = DEBUG_STKSZ
 		};
+		char *estacks = per_cpu(exception_stacks, cpu);
 		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
-			if (cpu) {
-				estacks = (char *)__get_free_pages(GFP_ATOMIC, order[v]);
-				if (!estacks)
-					panic("Cannot allocate exception "
-					      "stack %ld %d\n", v, cpu);
-			}
-			estacks += PAGE_SIZE << order[v];
+			estacks += sizes[v];
 			orig_ist->ist[v] = t->x86_tss.ist[v] =
 					(unsigned long)estacks;
 		}

commit 26f80bd6a9ab17bc8a60b6092e7c0d05c5927ce5
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:58 2009 +0900

    x86-64: Convert irqstacks to per-cpu
    
    Move the irqstackptr variable from the PDA to per-cpu.  Make the
    stacks themselves per-cpu, removing some specific allocation code.
    Add a seperate flag (is_boot_cpu) to simplify the per-cpu boot
    adjustments.
    
    tj: * sprinkle some underbars around.
    
        * irq_stack_ptr is not used till traps_init(), no reason to
          initialize it early.  On SMP, just leaving it NULL till proper
          initialization in setup_per_cpu_areas() works.  Dropped
          is_boot_cpu and early irq_stack_ptr initialization.
    
        * do DECLARE/DEFINE_PER_CPU(char[IRQ_STACK_SIZE], irq_stack)
          instead of (char, irq_stack[IRQ_STACK_SIZE]).
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3d0cc6f17116..496f0a01919b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -881,7 +881,13 @@ __setup("clearcpuid=", setup_disablecpuid);
 #ifdef CONFIG_X86_64
 struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
-static char boot_cpu_stack[IRQSTACKSIZE] __page_aligned_bss;
+DEFINE_PER_CPU_PAGE_ALIGNED(char[IRQ_STACK_SIZE], irq_stack);
+#ifdef CONFIG_SMP
+DEFINE_PER_CPU(char *, irq_stack_ptr);	/* will be set during per cpu init */
+#else
+DEFINE_PER_CPU(char *, irq_stack_ptr) =
+	per_cpu_var(irq_stack) + IRQ_STACK_SIZE - 64;
+#endif
 
 void __cpuinit pda_init(int cpu)
 {
@@ -901,18 +907,7 @@ void __cpuinit pda_init(int cpu)
 	if (cpu == 0) {
 		/* others are initialized in smpboot.c */
 		pda->pcurrent = &init_task;
-		pda->irqstackptr = boot_cpu_stack;
-		pda->irqstackptr += IRQSTACKSIZE - 64;
 	} else {
-		if (!pda->irqstackptr) {
-			pda->irqstackptr = (char *)
-				__get_free_pages(GFP_ATOMIC, IRQSTACK_ORDER);
-			if (!pda->irqstackptr)
-				panic("cannot allocate irqstack for cpu %d",
-				      cpu);
-			pda->irqstackptr += IRQSTACKSIZE - 64;
-		}
-
 		if (pda->nodenumber == 0 && cpu_to_node(cpu) != NUMA_NO_NODE)
 			pda->nodenumber = cpu_to_node(cpu);
 	}

commit 9eb912d1aa6b8106e06a73ea6702ec3dab0d6a1a
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 00:38:57 2009 +0900

    x86-64: Move TLB state from PDA to per-cpu and consolidate with 32-bit.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c49498d40830..3d0cc6f17116 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -897,8 +897,6 @@ void __cpuinit pda_init(int cpu)
 	pda->irqcount = -1;
 	pda->kernelstack = (unsigned long)stack_thread_info() -
 				 PDA_STACKOFFSET + THREAD_SIZE;
-	pda->active_mm = &init_mm;
-	pda->mmu_state = 0;
 
 	if (cpu == 0) {
 		/* others are initialized in smpboot.c */

commit b12d8db8fbfaed1e8222a15333a3645599636854
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: make pda a percpu variable
    
    [ Based on original patch from Christoph Lameter and Mike Travis. ]
    
    As pda is now allocated in percpu area, it can easily be made a proper
    percpu variable.  Make it so by defining per cpu symbol from linker
    script and declaring it in C code for SMP and simply defining it for
    UP.  This change cleans up code and brings SMP and UP closer a bit.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7041acdf5579..c49498d40830 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -879,9 +879,6 @@ static __init int setup_disablecpuid(char *arg)
 __setup("clearcpuid=", setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
-struct x8664_pda *_cpu_pda[NR_CPUS] __read_mostly;
-EXPORT_SYMBOL(_cpu_pda);
-
 struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
 static char boot_cpu_stack[IRQSTACKSIZE] __page_aligned_bss;

commit 1a51e3a0aed18767cf2762e95456ecfeb0bca5e6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: fold pda into percpu area on SMP
    
    [ Based on original patch from Christoph Lameter and Mike Travis. ]
    
    Currently pdas and percpu areas are allocated separately.  %gs points
    to local pda and percpu area can be reached using pda->data_offset.
    This patch folds pda into percpu area.
    
    Due to strange gcc requirement, pda needs to be at the beginning of
    the percpu area so that pda->stack_canary is at %gs:40.  To achieve
    this, a new percpu output section macro - PERCPU_VADDR_PREALLOC() - is
    added and used to reserve pda sized chunk at the start of the percpu
    area.
    
    After this change, for boot cpu, %gs first points to pda in the
    data.init area and later during setup_per_cpu_areas() gets updated to
    point to the actual pda.  This means that setup_per_cpu_areas() need
    to reload %gs for CPU0 while clearing pda area for other cpus as cpu0
    already has modified it when control reaches setup_per_cpu_areas().
    
    This patch also removes now unnecessary get_local_pda() and its call
    sites.
    
    A lot of this patch is taken from Mike Travis' "x86_64: Fold pda into
    per cpu area" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c116c599326e..7041acdf5579 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -893,10 +893,8 @@ void __cpuinit pda_init(int cpu)
 	/* Setup up data that may be needed in __get_free_pages early */
 	loadsegment(fs, 0);
 	loadsegment(gs, 0);
-	/* Memory clobbers used to order PDA accessed */
-	mb();
-	wrmsrl(MSR_GS_BASE, pda);
-	mb();
+
+	load_pda_offset(cpu);
 
 	pda->cpunumber = cpu;
 	pda->irqcount = -1;

commit c8f3329a0ddd751241e96b4100df7eda14b2cbc6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: use static _cpu_pda array
    
    _cpu_pda array first uses statically allocated storage in data.init
    and then switches to allocated bootmem to conserve space.  However,
    after folding pda area into percpu area, _cpu_pda array will be
    removed completely.  Drop the reallocation part to simplify the code
    for soon-to-follow changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f00258462444..c116c599326e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -879,7 +879,7 @@ static __init int setup_disablecpuid(char *arg)
 __setup("clearcpuid=", setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
-struct x8664_pda **_cpu_pda __read_mostly;
+struct x8664_pda *_cpu_pda[NR_CPUS] __read_mostly;
 EXPORT_SYMBOL(_cpu_pda);
 
 struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };

commit 506c10f26c481b7f8ef27c1c79290f68989b2e9e
Merge: e1df957670ae c59765042f53
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 02:42:53 2009 +0100

    Merge commit 'v2.6.29-rc1' into perfcounters/core
    
    Conflicts:
            include/linux/kernel_stat.h

commit 068790334cececc3d2d945617ccc585477da2e38
Author: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
Date:   Sat Jan 10 12:17:37 2009 +0530

    x86: smp.h move cpu_callin_mask and cpu_callin_map declartion to cpumask.h
    
    Impact: cleanup
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 14e543b6fd4f..f00258462444 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -22,6 +22,7 @@
 #include <asm/numa.h>
 #include <asm/smp.h>
 #include <asm/cpu.h>
+#include <asm/cpumask.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>

commit 1de8cd3cb9f61e854e743c7210df43db517d4832
Merge: 1eb1b3b65dc3 3d14bdad4031
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 10 23:56:42 2009 +0100

    Merge branch 'linus' into x86/cleanups

commit f472cdba849cc3d838f3788469316e8572463a8c
Author: Jaswinder Singh Rajput <jaswinder@infradead.org>
Date:   Wed Jan 7 21:34:25 2009 +0530

    x86: smp.h move stack_processor_id declartion to cpu.h
    
    Impact: cleanup
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3f95a40f718a..f7619a2eaffe 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -21,6 +21,7 @@
 #include <asm/asm.h>
 #include <asm/numa.h>
 #include <asm/smp.h>
+#include <asm/cpu.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>

commit c2d1cec1c77f7714672c1efeae075424c929e0d5
Author: Mike Travis <travis@sgi.com>
Date:   Sun Jan 4 05:18:03 2009 -0800

    x86: cleanup remaining cpumask_t ops in smpboot code
    
    Impact: use new cpumask API to reduce memory and stack usage
    
    Allocate the following local cpumasks based on the number of cpus that
    are present.  References will use new cpumask API.  (Currently only
    modified for x86_64, x86_32 continues to use the *_map variants.)
    
        cpu_callin_mask
        cpu_callout_mask
        cpu_initialized_mask
        cpu_sibling_setup_mask
    
    Provide the following accessor functions:
    
        struct cpumask *cpu_sibling_mask(int cpu)
        struct cpumask *cpu_core_mask(int cpu)
    
    Other changes are when setting or clearing the cpu online, possible
    or present maps, use the accessor functions.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3f95a40f718a..83492b1f93b1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -40,6 +40,26 @@
 
 #include "cpu.h"
 
+#ifdef CONFIG_X86_64
+
+/* all of these masks are initialized in setup_cpu_local_masks() */
+cpumask_var_t cpu_callin_mask;
+cpumask_var_t cpu_callout_mask;
+cpumask_var_t cpu_initialized_mask;
+
+/* representing cpus for which sibling maps can be computed */
+cpumask_var_t cpu_sibling_setup_mask;
+
+#else /* CONFIG_X86_32 */
+
+cpumask_t cpu_callin_map;
+cpumask_t cpu_callout_map;
+cpumask_t cpu_initialized;
+cpumask_t cpu_sibling_setup_map;
+
+#endif /* CONFIG_X86_32 */
+
+
 static struct cpu_dev *this_cpu __cpuinitdata;
 
 #ifdef CONFIG_X86_64
@@ -856,8 +876,6 @@ static __init int setup_disablecpuid(char *arg)
 }
 __setup("clearcpuid=", setup_disablecpuid);
 
-cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
-
 #ifdef CONFIG_X86_64
 struct x8664_pda **_cpu_pda __read_mostly;
 EXPORT_SYMBOL(_cpu_pda);
@@ -976,7 +994,7 @@ void __cpuinit cpu_init(void)
 
 	me = current;
 
-	if (cpu_test_and_set(cpu, cpu_initialized))
+	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask))
 		panic("CPU#%d already initialized!\n", cpu);
 
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
@@ -1085,7 +1103,7 @@ void __cpuinit cpu_init(void)
 	struct tss_struct *t = &per_cpu(init_tss, cpu);
 	struct thread_struct *thread = &curr->thread;
 
-	if (cpu_test_and_set(cpu, cpu_initialized)) {
+	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask)) {
 		printk(KERN_WARNING "CPU#%d already initialized!\n", cpu);
 		for (;;) local_irq_enable();
 	}

commit 9628937d5b37169151c5f6bbd40919c6ac958a46
Author: Mike Travis <travis@sgi.com>
Date:   Wed Dec 31 18:08:46 2008 -0800

    x86: cleanup some remaining usages of NR_CPUS where s/b nr_cpu_ids
    
    Impact: Reduce future system panics due to cpumask operations using NR_CPUS
    
    Insure that code does not look at bits >= nr_cpu_ids as when cpumasks are
    allocated based on nr_cpu_ids, these extra bits will not be defined.
    
    Also some other minor updates:
    
       * change in to use cpu accessor function set_cpu_present() instead of
         directly accessing cpu_present_map w/cpu_clear() [arch/x86/kernel/reboot.c]
    
       * use cpumask_of() instead of &cpumask_of_cpu() [arch/x86/kernel/reboot.c]
    
       * optimize some cpu_mask_to_apicid_and functions.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 42e0853030cb..3f95a40f718a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -355,7 +355,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 		printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
 	} else if (smp_num_siblings > 1) {
 
-		if (smp_num_siblings > NR_CPUS) {
+		if (smp_num_siblings > nr_cpu_ids) {
 			printk(KERN_WARNING "CPU: Unsupported number of siblings %d",
 					smp_num_siblings);
 			smp_num_siblings = 1;

commit e1df957670aef74ffd9a4ad93e6d2c90bf6b4845
Merge: 2b583d8bc8d7 3c92ec8ae91e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Dec 29 09:45:15 2008 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            fs/exec.c
            include/linux/init_task.h
    
    Simple context conflicts.

commit fa623d1b0222adbe8f822e53c08003b9679a410c
Merge: 3d44cc3e01ee 1ccedb7cdba6 34945ede3107 d43779740621 c415b3dce30d beeb4195cbc8 f269b07e862c 4e42ebd57b2e e1286f2c686f 878719e831d9 fd28a5b58ddd adf77bac052b 8f2466f45f75 93093d099e5d bb5574608a83 f34a10bd9f8c b6fd6f26733e 30604bb410b5 5b9a0e14eb4b 67bac792cd0c 7a9787e1eba9 f4166c54bfe0 69b88afa8d11 8daa19051e1c 3e1e9002aa8b 8403295e0fa4 4db646b1af8f 205516c12dbb c8182f0016fb ecbf29cdb399
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:27:23 2008 +0100

    Merge branches 'x86/apic', 'x86/cleanups', 'x86/cpufeature', 'x86/crashdump', 'x86/debug', 'x86/defconfig', 'x86/detect-hyper', 'x86/doc', 'x86/dumpstack', 'x86/early-printk', 'x86/fpu', 'x86/idle', 'x86/io', 'x86/memory-corruption-check', 'x86/microcode', 'x86/mm', 'x86/mtrr', 'x86/nmi-watchdog', 'x86/pat2', 'x86/pci-ioapic-boot-irq-quirks', 'x86/ptrace', 'x86/quirks', 'x86/reboot', 'x86/setup-memory', 'x86/signal', 'x86/sparse-fixes', 'x86/time', 'x86/uv' and 'x86/xen' into x86/core

commit 5c167b8585c8d91206b395d57011ead7711e322f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 17 09:02:19 2008 +0100

    x86, perfcounters: rename intel_arch_perfmon.h => perf_counter.h
    
    Impact: rename include file
    
    We'll be providing an asm/perf_counter.h to the generic perfcounter code,
    so use the already existing x86 file for this purpose and rename it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4461011db47c..ad331b4d6238 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -17,7 +17,7 @@
 #include <asm/mmu_context.h>
 #include <asm/mtrr.h>
 #include <asm/mce.h>
-#include <asm/intel_arch_perfmon.h>
+#include <asm/perf_counter.h>
 #include <asm/pat.h>
 #include <asm/asm.h>
 #include <asm/numa.h>

commit 34945ede31071ac7d72270cc6c1893323f392b3f
Author: Jaswinder Singh <jaswinder@infradead.org>
Date:   Fri Dec 19 22:33:52 2008 +0530

    x86: common.c boot_cpu_stack and boot_exception_stacks should be static
    
    Impact: cleanup, avoid sparse warnings, reduce kernel size a bit
    
    Fixes these sparse warnings:
    
     arch/x86/kernel/cpu/common.c:869:6: warning: symbol 'boot_cpu_stack' was not declared. Should it be static?
     arch/x86/kernel/cpu/common.c:910:6: warning: symbol 'boot_exception_stacks' was not declared. Should it be static?
    
    Signed-off-by: Jaswinder Singh <jaswinder@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b9c9ea0217a9..aba49c782fd6 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -862,7 +862,7 @@ EXPORT_SYMBOL(_cpu_pda);
 
 struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
-char boot_cpu_stack[IRQSTACKSIZE] __page_aligned_bss;
+static char boot_cpu_stack[IRQSTACKSIZE] __page_aligned_bss;
 
 void __cpuinit pda_init(int cpu)
 {
@@ -903,8 +903,8 @@ void __cpuinit pda_init(int cpu)
 	}
 }
 
-char boot_exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ +
-			   DEBUG_STKSZ] __page_aligned_bss;
+static char boot_exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ +
+				  DEBUG_STKSZ] __page_aligned_bss;
 
 extern asmlinkage void ignore_sysret(void);
 

commit 241771ef016b5c0c83cd7a4372a74321c973c1e6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 3 10:39:53 2008 +0100

    performance counters: x86 support
    
    Implement performance counters for x86 Intel CPUs.
    
    It's simplified right now: the PERFMON CPU feature is assumed,
    which is available in Core2 and later Intel CPUs.
    
    The design is flexible to be extended to more CPU types as well.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b9c9ea0217a9..4461011db47c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -17,6 +17,7 @@
 #include <asm/mmu_context.h>
 #include <asm/mtrr.h>
 #include <asm/mce.h>
+#include <asm/intel_arch_perfmon.h>
 #include <asm/pat.h>
 #include <asm/asm.h>
 #include <asm/numa.h>
@@ -750,6 +751,7 @@ void __init identify_boot_cpu(void)
 #else
 	vgetcpu_set_mode();
 #endif
+	init_hw_perf_counters();
 }
 
 void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)

commit 88b094fb8d4fe43b7025ea8d487059e8813e02cd
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Oct 27 10:41:46 2008 -0700

    x86: Hypervisor detection and get tsc_freq from hypervisor
    
    Impact: Changes timebase calibration on Vmware.
    
    v3->v2 : Abstract the hypervisor detection and feature (tsc_freq) request
             behind a hypervisor.c file
    v2->v1 : Add a x86_hyper_vendor field to the cpuinfo_x86 structure.
             This avoids multiple calls to the hypervisor detection function.
    
    This patch adds function to detect if we are running under VMware.
    The current way to check if we are on VMware is following,
    #  check if "hypervisor present bit" is set, if so read the 0x40000000
       cpuid leaf and check for "VMwareVMware" signature.
    #  if the above fails, check the DMI vendors name for "VMware" string
       if we find one we query the VMware hypervisor port to check if we are
       under VMware.
    
    The DMI + "VMware hypervisor port check" is needed for older VMware products,
    which don't implement the hypervisor signature cpuid leaf.
    Also note that since we are checking for the DMI signature the hypervisor
    port should never be accessed on native hardware.
    
    This patch also adds a hypervisor_get_tsc_freq function, instead of
    calibrating the frequency which can be error prone in virtualized
    environment, we ask the hypervisor for it. We get the frequency from
    the hypervisor by accessing the hypervisor port if we are running on VMware.
    Other hypervisors too can add code to the generic routine to get frequency on
    their platform.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    Signed-off-by: Dan Hecht <dhecht@vmware.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b9c9ea0217a9..b88595c36254 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -36,6 +36,7 @@
 #include <asm/proto.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
+#include <asm/hypervisor.h>
 
 #include "cpu.h"
 
@@ -703,6 +704,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	detect_ht(c);
 #endif
 
+	init_hypervisor(c);
 	/*
 	 * On SMP, boot_cpu_data holds the common feature set between
 	 * all CPUs; so make sure that we indicate which features are

commit b342797c1e5116a130841527b47dfaa462ed0968
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 31 09:31:38 2008 +0100

    x86: build fix
    
    Impact: build fix on certain UP configs
    
    fix:
    
     arch/x86/kernel/cpu/common.c: In function 'cpu_init':
     arch/x86/kernel/cpu/common.c:1141: error: 'boot_cpu_id' undeclared (first use in this function)
     arch/x86/kernel/cpu/common.c:1141: error: (Each undeclared identifier is reported only once
     arch/x86/kernel/cpu/common.c:1141: error: for each function it appears in.)
    
    Pull in asm/smp.h on UP, so that we get the definition of
    boot_cpu_id.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 003a65395bd5..b9c9ea0217a9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -20,6 +20,7 @@
 #include <asm/pat.h>
 #include <asm/asm.h>
 #include <asm/numa.h>
+#include <asm/smp.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>

commit 1c4acdb467f8a6704855a5670ff3d82e3c18eb0b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 31 00:43:03 2008 +0100

    x86: cpu_index build fix
    
    fix:
    
     arch/x86/kernel/cpu/common.c: In function 'early_identify_cpu':
     arch/x86/kernel/cpu/common.c:553: error: 'struct cpuinfo_x86' has no member named 'cpu_index'
    
    as cpu_index is only available on SMP.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index da8f15ac7a6d..003a65395bd5 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -550,7 +550,9 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	validate_pat_support(c);
 
+#ifdef CONFIG_SMP
 	c->cpu_index = boot_cpu_id;
+#endif
 }
 
 void __init early_cpu_init(void)

commit bfcb4c1becf93b1592f4a03a4d6e00a3ab89d5ec
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Thu Oct 30 16:13:37 2008 -0500

    x86/voyager: fix missing cpu_index initialisation
    
    Impact: fix /proc/cpuinfo output on x86/Voyager
    
    Ever since
    
    | commit 92cb7612aee39642d109b8d935ad265e602c0563
    | Author: Mike Travis <travis@sgi.com>
    | Date:   Fri Oct 19 20:35:04 2007 +0200
    |
    |     x86: convert cpuinfo_x86 array to a per_cpu array
    
    We've had an extra field in cpuinfo_x86 which is cpu_index.
    Unfortunately, voyager has never initialised this, although the only
    noticeable impact seems to be that /proc/cpuinfo shows all zeros for
    the processor ids.
    
    Anyway, fix this by initialising the boot CPU properly and setting the
    index when the secondaries update.
    
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 93e9393ea64a..da8f15ac7a6d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -549,6 +549,8 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		this_cpu->c_early_init(c);
 
 	validate_pat_support(c);
+
+	c->cpu_index = boot_cpu_id;
 }
 
 void __init early_cpu_init(void)

commit b3572e361b6b2ac5e724bc4bb932b7774b720b95
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Thu Oct 30 16:00:59 2008 -0500

    x86/voyager: fix compile breakage caused by dc1e35c6e95e8923cf1d3510438b63c600fee1e2
    
    Impact: build fix on x86/Voyager
    
    Given commits like this:
    
    | Author: Suresh Siddha <suresh.b.siddha@intel.com>
    | Date:   Tue Jul 29 10:29:19 2008 -0700
    |
    |     x86, xsave: enable xsave/xrstor on cpus with xsave support
    
    Which deliberately expose boot cpu dependence to pieces of the system,
    I think it's time to explicitly have a variable for it to prevent this
    continual misassumption that the boot CPU is zero.
    
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 25581dcb280e..93e9393ea64a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1134,7 +1134,7 @@ void __cpuinit cpu_init(void)
 	/*
 	 * Boot processor to setup the FP and extended state context info.
 	 */
-	if (!smp_processor_id())
+	if (smp_processor_id() == boot_cpu_id)
 		init_thread_xstate();
 
 	xsave_init();

commit 94f6bac1058fd59a8bd472d18c4b77f220d930b0
Author: Krzysztof Helt <krzysztof.h1@wp.pl>
Date:   Tue Sep 30 23:17:51 2008 +0200

    x86: do not allow to optimize flag_is_changeable_p() (rev. 2)
    
    The flag_is_changeable_p() is used by
    has_cpuid_p() which can return different results
    in the code sequence below:
    
     if (!have_cpuid_p())
          identify_cpu_without_cpuid(c);
    
      /* cyrix could have cpuid enabled via c_identify()*/
      if (!have_cpuid_p())
          return;
    
    Otherwise, the gcc 3.4.6 optimizes these two calls
    into one which make the code not working correctly.
    
    Cyrix cpus have the CPUID instruction enabled before
    the second call to the have_cpuid_p() but
    it is not detected due to the gcc optimization.
    Thus the ARR registers (mtrr like) are not detected
    on such a cpu.
    
    Signed-off-by: Krzysztof Helt <krzysztof.h1@wp.pl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f1af71851919..25581dcb280e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -124,18 +124,25 @@ static inline int flag_is_changeable_p(u32 flag)
 {
 	u32 f1, f2;
 
-	asm("pushfl\n\t"
-	    "pushfl\n\t"
-	    "popl %0\n\t"
-	    "movl %0,%1\n\t"
-	    "xorl %2,%0\n\t"
-	    "pushl %0\n\t"
-	    "popfl\n\t"
-	    "pushfl\n\t"
-	    "popl %0\n\t"
-	    "popfl\n\t"
-	    : "=&r" (f1), "=&r" (f2)
-	    : "ir" (flag));
+	/*
+	 * Cyrix and IDT cpus allow disabling of CPUID
+	 * so the code below may return different results
+	 * when it is executed before and after enabling
+	 * the CPUID. Add "volatile" to not allow gcc to
+	 * optimize the subsequent calls to this function.
+	 */
+	asm volatile ("pushfl\n\t"
+		      "pushfl\n\t"
+		      "popl %0\n\t"
+		      "movl %0,%1\n\t"
+		      "xorl %2,%0\n\t"
+		      "pushl %0\n\t"
+		      "popfl\n\t"
+		      "pushfl\n\t"
+		      "popl %0\n\t"
+		      "popfl\n\t"
+		      : "=&r" (f1), "=&r" (f2)
+		      : "ir" (flag));
 
 	return ((f1^f2) & flag) != 0;
 }

commit e04d645f326bc8e591bc4ae21c4ab535987a17c1
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Sep 22 14:35:08 2008 -0300

    x86: move vgetcpu mode probing to cpu detection
    
    Take it out of time initialization and move it to
    cpu detection time.
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 088fbdb6734e..f1af71851919 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -719,12 +719,24 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 #endif
 }
 
+#ifdef CONFIG_X86_64
+static void vgetcpu_set_mode(void)
+{
+	if (cpu_has(&boot_cpu_data, X86_FEATURE_RDTSCP))
+		vgetcpu_mode = VGETCPU_RDTSCP;
+	else
+		vgetcpu_mode = VGETCPU_LSL;
+}
+#endif
+
 void __init identify_boot_cpu(void)
 {
 	identify_cpu(&boot_cpu_data);
 #ifdef CONFIG_X86_32
 	sysenter_setup();
 	enable_sep_cpu();
+#else
+	vgetcpu_set_mode();
 #endif
 }
 

commit bd32a8cfa8f172bd943655a3663d8005e5c1d83c
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Fri Sep 19 18:41:16 2008 -0700

    x86: cpu don't print duplicated vendor string
    
    Some CPUs have vendor string in the middle of model_id instead of beginning
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fb789dd9e691..088fbdb6734e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -797,7 +797,7 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 	else if (c->cpuid_level >= 0)
 		vendor = c->x86_vendor_id;
 
-	if (vendor && strncmp(c->x86_model_id, vendor, strlen(vendor)))
+	if (vendor && !strstr(c->x86_model_id, vendor))
 		printk(KERN_CONT "%s ", vendor);
 
 	if (c->x86_model_id[0])

commit 365d46dc9be9b3c833990a06f3994b1987eda578
Merge: 5dc64a3442b9 fd0480883066
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 12 12:35:23 2008 +0200

    Merge branch 'linus' into x86/xen
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/process_64.c
            arch/x86/xen/enlighten.c

commit 0afe2db21394820d32646a695eccf3fbfe6ab5c7
Merge: d84705969f89 43603c8df97f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Oct 11 20:23:20 2008 +0200

    Merge branch 'x86/unify-cpu-detect' into x86-v28-for-linus-phase4-D
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/signal_64.c
            include/asm-x86/cpufeature.h

commit 43603c8df97f246e8be7b9cc92a8f968a85108bd
Author: Hans Schou <linux@schou.dk>
Date:   Thu Oct 9 20:47:24 2008 +0200

    x86, debug: print more information about unknown CPUs
    
    Write the name of the unknown vendor_id to output instead of just
    "unknown".
    
    Tag changed to 'vendor_id' as used in /proc/cpuinfo
    
    Signed-off-by: Hans Schou <linux@schou.dk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index cef3519b3bbb..84c4040efa80 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -406,7 +406,7 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 
 	if (!printed) {
 		printed++;
-		printk(KERN_ERR "CPU: Vendor unknown, using generic init.\n");
+		printk(KERN_ERR "CPU: vendor_id '%s' unknown, using generic init.\n", v);
 		printk(KERN_ERR "CPU: Your system may be unstable.\n");
 	}
 

commit 07bbc16a8676b06950a21f35b59f69b2fe763bbd
Merge: 6a9e91846bf5 f8e256c687eb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 23 23:26:42 2008 +0200

    Merge branch 'timers/urgent' into x86/xen
    
    Conflicts:
            arch/x86/kernel/process_32.c
            arch/x86/kernel/process_64.c
    
    Manual merge:
    
            arch/x86/kernel/smpboot.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ba0593bf553c450a03dbc5f8c1f0ff58b778a0c8
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Tue Sep 16 09:29:40 2008 -0700

    x86: completely disable NOPL on 32 bits
    
    Completely disable NOPL on 32 bits.  It turns out that Microsoft
    Virtual PC is so broken it can't even reliably *fail* in the presence
    of NOPL.
    
    This leaves the infrastructure in place but disables it
    unconditionally.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8aab8517642e..4e456bd955bb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -344,31 +344,15 @@ static void __init early_cpu_detect(void)
 
 /*
  * The NOPL instruction is supposed to exist on all CPUs with
- * family >= 6, unfortunately, that's not true in practice because
+ * family >= 6; unfortunately, that's not true in practice because
  * of early VIA chips and (more importantly) broken virtualizers that
- * are not easy to detect.  Hence, probe for it based on first
- * principles.
+ * are not easy to detect.  In the latter case it doesn't even *fail*
+ * reliably, so probing for it doesn't even work.  Disable it completely
+ * unless we can find a reliable way to detect all the broken cases.
  */
 static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
 {
-	const u32 nopl_signature = 0x888c53b1; /* Random number */
-	u32 has_nopl = nopl_signature;
-
 	clear_cpu_cap(c, X86_FEATURE_NOPL);
-	if (c->x86 >= 6) {
-		asm volatile("\n"
-			     "1:      .byte 0x0f,0x1f,0xc0\n" /* nopl %eax */
-			     "2:\n"
-			     "        .section .fixup,\"ax\"\n"
-			     "3:      xor %0,%0\n"
-			     "        jmp 2b\n"
-			     "        .previous\n"
-			     _ASM_EXTABLE(1b,3b)
-			     : "+a" (has_nopl));
-
-		if (has_nopl == nopl_signature)
-			set_cpu_cap(c, X86_FEATURE_NOPL);
-	}
 }
 
 static void __cpuinit generic_identify(struct cpuinfo_x86 *c)

commit a9853dd6d285c30a3ddeb3cce8c05e1678400bef
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 14 14:46:58 2008 +0200

    x86: cpuid, fix typo
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3e2ce4d33d3a..cef3519b3bbb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -529,7 +529,7 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 		identify_cpu_without_cpuid(c);
 
 	/* cyrix could have cpuid enabled via c_identify()*/
-	if (!have_cpuid())
+	if (!have_cpuid_p())
 		return;
 
 	cpu_detect(c);
@@ -611,7 +611,7 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		identify_cpu_without_cpuid(c);
 
 	/* cyrix could have cpuid enabled via c_identify()*/
-	if (!have_cpuid())
+	if (!have_cpuid_p())
 		return;
 
 	cpu_detect(c);

commit afae865613c8292e8bdcce4f0b161988d761f033
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 14 02:33:16 2008 -0700

    x86: move transmeta cap read to early_init_transmeta()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f5f25b85be95..3e2ce4d33d3a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -465,14 +465,6 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 	}
 
 #ifdef CONFIG_X86_64
-	/* Transmeta-defined flags: level 0x80860001 */
-	xlvl = cpuid_eax(0x80860000);
-	if ((xlvl & 0xffff0000) == 0x80860000) {
-		/* Don't set x86_cpuid_level here for now to not confuse. */
-		if (xlvl >= 0x80860001)
-			c->x86_capability[2] = cpuid_edx(0x80860001);
-	}
-
 	if (c->extended_cpuid_level >= 0x80000008) {
 		u32 eax = cpuid_eax(0x80000008);
 

commit aef93c8bd506a78983d91cd576a97fee6e934ac1
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 14 02:33:15 2008 -0700

    x86: identify_cpu_without_cpuid v2
    
    Krzysztof found some old cyrix cpu where an mtrr-alike cpu feature was
    not detected properly.
    
    this one is based on Krzysztof' patch, and we call ->c_identify() in
    early_identify_cpu.
    
    need to call c_identify() for cpus without cpuid even earlier ...
    
    v2: Krzysztof point out need to give cyrix another chance about cpuid
        checking again, after ->c_identify() enables cpuid for it
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: Krzysztof Helt <krzysztof.h1@wp.pl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9a57106c1995..f5f25b85be95 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -485,6 +485,33 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_power = cpuid_edx(0x80000007);
 
 }
+
+static void __cpuinit identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_32
+	int i;
+
+	/*
+	 * First of all, decide if this is a 486 or higher
+	 * It's a 486 if we can modify the AC flag
+	 */
+	if (flag_is_changeable_p(X86_EFLAGS_AC))
+		c->x86 = 4;
+	else
+		c->x86 = 3;
+
+	for (i = 0; i < X86_VENDOR_NUM; i++)
+		if (cpu_devs[i] && cpu_devs[i]->c_identify) {
+			c->x86_vendor_id[0] = 0;
+			cpu_devs[i]->c_identify(c);
+			if (c->x86_vendor_id[0]) {
+				get_cpu_vendor(c);
+				break;
+			}
+		}
+#endif
+}
+
 /*
  * Do minimum CPU detection early.
  * Fields really needed: vendor, cpuid_level, family, model, mask,
@@ -503,13 +530,16 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 #endif
 	c->x86_cache_alignment = c->x86_clflush_size;
 
-	if (!have_cpuid_p())
-		return;
-
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
-
 	c->extended_cpuid_level = 0;
 
+	if (!have_cpuid_p())
+		identify_cpu_without_cpuid(c);
+
+	/* cyrix could have cpuid enabled via c_identify()*/
+	if (!have_cpuid())
+		return;
+
 	cpu_detect(c);
 
 	get_cpu_vendor(c);
@@ -583,10 +613,14 @@ static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
 
 static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 {
+	c->extended_cpuid_level = 0;
+
 	if (!have_cpuid_p())
-		return;
+		identify_cpu_without_cpuid(c);
 
-	c->extended_cpuid_level = 0;
+	/* cyrix could have cpuid enabled via c_identify()*/
+	if (!have_cpuid())
+		return;
 
 	cpu_detect(c);
 
@@ -639,17 +673,6 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	c->x86_cache_alignment = c->x86_clflush_size;
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
 
-	if (!have_cpuid_p()) {
-		/*
-		 * First of all, decide if this is a 486 or higher
-		 * It's a 486 if we can modify the AC flag
-		 */
-		if (flag_is_changeable_p(X86_EFLAGS_AC))
-			c->x86 = 4;
-		else
-			c->x86 = 3;
-	}
-
 	generic_identify(c);
 
 	if (this_cpu->c_identify)

commit 3ce9bcb583536c45a46c7302747029450e22279c
Merge: 26fd10517e81 f7d0b926ac8c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 10 14:05:45 2008 +0200

    Merge branch 'core/xen' into x86/xen

commit 11fdd252bb5b461289fc5c21dc8fc87dc66f3284
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 7 17:58:50 2008 -0700

    x86: cpu make amd.c more like amd_64.c v2
    
    1. make 32bit have early_init_amd_mc and amd_detect_cmp
    2. seperate init_amd_k5/k6/k7 ...
    
    v2: fix compiling for !CONFIG_SMP
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 5cde4b18448c..9a57106c1995 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -629,8 +629,8 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	c->x86_vendor_id[0] = '\0'; /* Unset */
 	c->x86_model_id[0] = '\0';  /* Unset */
 	c->x86_max_cores = 1;
-#ifdef CONFIG_X86_64
 	c->x86_coreid_bits = 0;
+#ifdef CONFIG_X86_64
 	c->x86_clflush_size = 64;
 #else
 	c->cpuid_level = -1;	/* CPUID not detected */

commit 2d9cd6c27f00958a31622b8e9909d5e35f069b28
Author: Jan Beulich <jbeulich@novell.com>
Date:   Fri Aug 29 13:15:04 2008 +0100

    x86-64: add two __cpuinit annotations
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e8045c4ef1c1..5cde4b18448c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -845,7 +845,7 @@ struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
 char boot_cpu_stack[IRQSTACKSIZE] __page_aligned_bss;
 
-void pda_init(int cpu)
+void __cpuinit pda_init(int cpu)
 {
 	struct x8664_pda *pda = cpu_pda(cpu);
 

commit 12cf105cd66d95cf32c73cfa847a50bd1b700f23
Author: Krzysztof Helt <krzysztof.h1@wp.pl>
Date:   Thu Sep 4 21:09:43 2008 +0200

    x86: delay early cpu initialization until cpuid is done
    
    Move early cpu initialization after cpu early get cap so the
    early cpu initialization can fix up cpu caps.
    
    Signed-off-by: Krzysztof Helt <krzysztof.h1@wp.pl>
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0785b3c8d043..8aab8517642e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -335,11 +335,11 @@ static void __init early_cpu_detect(void)
 
 	get_cpu_vendor(c, 1);
 
+	early_get_cap(c);
+
 	if (c->x86_vendor != X86_VENDOR_UNKNOWN &&
 	    cpu_devs[c->x86_vendor]->c_early_init)
 		cpu_devs[c->x86_vendor]->c_early_init(c);
-
-	early_get_cap(c);
 }
 
 /*

commit e3224234717b4228c235cee401af89212f17a3a4
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sat Sep 6 01:52:28 2008 -0700

    x86, cpu init: call early_init_xxx in init_xxx
    
    so we:
    
     1. could set some cap to ap
     2. restore some cap after memset in identify_cpu for boot cpu
    
    esp for CONSTANT_TSC this matters, as:
    
    before this patch:
     flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm 3dnowext 3dnow rep_good nopl pni monitor cx16 lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs
    
    after this patch:
     flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm 3dnowext 3dnow constant_tsc rep_good nopl pni monitor cx16 lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs
    
    so constant_tsc is back...
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a8b9b7242428..e8045c4ef1c1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -473,9 +473,6 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 			c->x86_capability[2] = cpuid_edx(0x80860001);
 	}
 
-	if (c->extended_cpuid_level >= 0x80000007)
-		c->x86_power = cpuid_edx(0x80000007);
-
 	if (c->extended_cpuid_level >= 0x80000008) {
 		u32 eax = cpuid_eax(0x80000008);
 
@@ -483,6 +480,10 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_phys_bits = eax & 0xff;
 	}
 #endif
+
+	if (c->extended_cpuid_level >= 0x80000007)
+		c->x86_power = cpuid_edx(0x80000007);
+
 }
 /*
  * Do minimum CPU detection early.

commit 1b05d60d60e81c6594da8298107a05b506f01797
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sat Sep 6 01:52:27 2008 -0700

    x86: remove duplicated get_model_name() calling
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 286c89a991bd..a8b9b7242428 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -252,13 +252,13 @@ static struct cpu_dev __cpuinitdata default_cpu = {
 	.c_x86_vendor = X86_VENDOR_UNKNOWN,
 };
 
-int __cpuinit get_model_name(struct cpuinfo_x86 *c)
+static void __cpuinit get_model_name(struct cpuinfo_x86 *c)
 {
 	unsigned int *v;
 	char *p, *q;
 
 	if (c->extended_cpuid_level < 0x80000004)
-		return 0;
+		return;
 
 	v = (unsigned int *) c->x86_model_id;
 	cpuid(0x80000002, &v[0], &v[1], &v[2], &v[3]);
@@ -277,8 +277,6 @@ int __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	     while (q <= &c->x86_model_id[48])
 		  *q++ = '\0';	/* Zero-pad the rest */
 	}
-
-	return 1;
 }
 
 void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
@@ -610,8 +608,7 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 #endif
 	}
 
-	if (c->extended_cpuid_level >= 0x80000004)
-		get_model_name(c); /* Default name */
+	get_model_name(c); /* Default name */
 
 	init_scattered_cpuid_features(c);
 	detect_nopl(c);

commit b6734c35af028f06772c0b2c836c7d579e6d4dad
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Aug 18 17:39:32 2008 -0700

    x86: add NOPL as a synthetic CPU feature bit
    
    The long noops ("NOPL") are supposed to be detected by family >= 6.
    Unfortunately, several non-Intel x86 implementations, both hardware
    and software, don't obey this dictum.  Instead, probe for NOPL
    directly by executing a NOPL instruction and see if we get #UD.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 80ab20d4fa39..0785b3c8d043 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -13,6 +13,7 @@
 #include <asm/mtrr.h>
 #include <asm/mce.h>
 #include <asm/pat.h>
+#include <asm/asm.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
@@ -341,6 +342,35 @@ static void __init early_cpu_detect(void)
 	early_get_cap(c);
 }
 
+/*
+ * The NOPL instruction is supposed to exist on all CPUs with
+ * family >= 6, unfortunately, that's not true in practice because
+ * of early VIA chips and (more importantly) broken virtualizers that
+ * are not easy to detect.  Hence, probe for it based on first
+ * principles.
+ */
+static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
+{
+	const u32 nopl_signature = 0x888c53b1; /* Random number */
+	u32 has_nopl = nopl_signature;
+
+	clear_cpu_cap(c, X86_FEATURE_NOPL);
+	if (c->x86 >= 6) {
+		asm volatile("\n"
+			     "1:      .byte 0x0f,0x1f,0xc0\n" /* nopl %eax */
+			     "2:\n"
+			     "        .section .fixup,\"ax\"\n"
+			     "3:      xor %0,%0\n"
+			     "        jmp 2b\n"
+			     "        .previous\n"
+			     _ASM_EXTABLE(1b,3b)
+			     : "+a" (has_nopl));
+
+		if (has_nopl == nopl_signature)
+			set_cpu_cap(c, X86_FEATURE_NOPL);
+	}
+}
+
 static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
@@ -395,8 +425,8 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		}
 
 		init_scattered_cpuid_features(c);
+		detect_nopl(c);
 	}
-
 }
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)

commit 913da64b54b2b3bb212a59aba2e6f2b8294ca1fa
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Wed Sep 3 14:30:23 2008 +0100

    x86: build fix for !CONFIG_SMP
    
    Move reset_lazy_tlbstate into tlb_32.c, and define noop versions of
    play_dead() in process_{32,64}.c when !CONFIG_SMP.
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 18f5551b32dd..76d10d5c2fa7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -714,10 +714,3 @@ void __cpuinit cpu_init(void)
 	mxcsr_feature_mask_init();
 }
 
-void reset_lazy_tlbstate(void)
-{
-	int cpu = raw_smp_processor_id();
-
-	per_cpu(cpu_tlbstate, cpu).state = 0;
-	per_cpu(cpu_tlbstate, cpu).active_mm = &init_mm;
-}

commit bd220a24a9263eaa70d5e6821383085950b5a13c
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Fri Sep 5 00:58:28 2008 -0700

    x86: move nonx_setup etc from common.c to init_64.c
    
    like 32 bit put it in init_32.c
    
    Signed-off-by: Yinghai <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index eef868c97b89..286c89a991bd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -847,51 +847,6 @@ struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
 
 char boot_cpu_stack[IRQSTACKSIZE] __page_aligned_bss;
 
-unsigned long __supported_pte_mask __read_mostly = ~0UL;
-EXPORT_SYMBOL_GPL(__supported_pte_mask);
-
-static int do_not_nx __cpuinitdata;
-
-/* noexec=on|off
-Control non executable mappings for 64bit processes.
-
-on	Enable(default)
-off	Disable
-*/
-static int __init nonx_setup(char *str)
-{
-	if (!str)
-		return -EINVAL;
-	if (!strncmp(str, "on", 2)) {
-		__supported_pte_mask |= _PAGE_NX;
-		do_not_nx = 0;
-	} else if (!strncmp(str, "off", 3)) {
-		do_not_nx = 1;
-		__supported_pte_mask &= ~_PAGE_NX;
-	}
-	return 0;
-}
-early_param("noexec", nonx_setup);
-
-int force_personality32;
-
-/* noexec32=on|off
-Control non executable heap for 32bit processes.
-To control the stack too use noexec=off
-
-on	PROT_READ does not imply PROT_EXEC for 32bit processes (default)
-off	PROT_READ implies PROT_EXEC
-*/
-static int __init nonx32_setup(char *str)
-{
-	if (!strcmp(str, "on"))
-		force_personality32 &= ~READ_IMPLIES_EXEC;
-	else if (!strcmp(str, "off"))
-		force_personality32 |= READ_IMPLIES_EXEC;
-	return 1;
-}
-__setup("noexec32=", nonx32_setup);
-
 void pda_init(int cpu)
 {
 	struct x8664_pda *pda = cpu_pda(cpu);
@@ -957,15 +912,6 @@ void syscall_init(void)
 	       X86_EFLAGS_TF|X86_EFLAGS_DF|X86_EFLAGS_IF|X86_EFLAGS_IOPL);
 }
 
-void __cpuinit check_efer(void)
-{
-	unsigned long efer;
-
-	rdmsrl(MSR_EFER, efer);
-	if (!(efer & EFER_NX) || do_not_nx)
-		__supported_pte_mask &= ~_PAGE_NX;
-}
-
 unsigned long kernel_eflags;
 
 /*

commit 102bbe3ab83c7ac04461d277df23cd5acdb49892
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:13 2008 -0700

    x86: cpu/common*.c, merge identify_cpu()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f35baa7f3036..eef868c97b89 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -104,34 +104,6 @@ static int __init cachesize_setup(char *str)
 }
 __setup("cachesize=", cachesize_setup);
 
-/*
- * Naming convention should be: <Name> [(<Codename>)]
- * This table only is used unless init_<vendor>() below doesn't set it;
- * in particular, if CPUID levels 0x80000002..4 are supported, this isn't used
- *
- */
-
-/* Look up CPU names by table lookup. */
-static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
-{
-	struct cpu_model_info *info;
-
-	if (c->x86_model >= 16)
-		return NULL;	/* Range check */
-
-	if (!this_cpu)
-		return NULL;
-
-	info = this_cpu->c_models;
-
-	while (info && info->family) {
-		if (info->family == c->x86)
-			return info->model_names[c->x86_model];
-		info++;
-	}
-	return NULL;		/* Not found */
-}
-
 static int __init x86_fxsr_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_FXSR);
@@ -197,13 +169,48 @@ static int __init x86_serial_nr_setup(char *s)
 }
 __setup("serialnumber", x86_serial_nr_setup);
 #else
+static inline int flag_is_changeable_p(u32 flag)
+{
+	return 1;
+}
 /* Probe for the CPUID instruction */
 static inline int have_cpuid_p(void)
 {
 	return 1;
 }
+static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
+{
+}
 #endif
 
+/*
+ * Naming convention should be: <Name> [(<Codename>)]
+ * This table only is used unless init_<vendor>() below doesn't set it;
+ * in particular, if CPUID levels 0x80000002..4 are supported, this isn't used
+ *
+ */
+
+/* Look up CPU names by table lookup. */
+static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
+{
+	struct cpu_model_info *info;
+
+	if (c->x86_model >= 16)
+		return NULL;	/* Range check */
+
+	if (!this_cpu)
+		return NULL;
+
+	info = this_cpu->c_models;
+
+	while (info && info->family) {
+		if (info->family == c->x86)
+			return info->model_names[c->x86_model];
+		info++;
+	}
+	return NULL;		/* Not found */
+}
+
 __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 
 /* Current gdt points %fs at the "master" per-cpu area: after this,
@@ -620,12 +627,18 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	c->loops_per_jiffy = loops_per_jiffy;
 	c->x86_cache_size = -1;
 	c->x86_vendor = X86_VENDOR_UNKNOWN;
-	c->cpuid_level = -1;	/* CPUID not detected */
 	c->x86_model = c->x86_mask = 0;	/* So far unknown... */
 	c->x86_vendor_id[0] = '\0'; /* Unset */
 	c->x86_model_id[0] = '\0';  /* Unset */
 	c->x86_max_cores = 1;
+#ifdef CONFIG_X86_64
+	c->x86_coreid_bits = 0;
+	c->x86_clflush_size = 64;
+#else
+	c->cpuid_level = -1;	/* CPUID not detected */
 	c->x86_clflush_size = 32;
+#endif
+	c->x86_cache_alignment = c->x86_clflush_size;
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
 
 	if (!have_cpuid_p()) {
@@ -644,6 +657,10 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	if (this_cpu->c_identify)
 		this_cpu->c_identify(c);
 
+#ifdef CONFIG_X86_64
+	c->apicid = phys_pkg_id(0);
+#endif
+
 	/*
 	 * Vendor-specific initialization.  In this section we
 	 * canonicalize the feature flags, meaning if there are
@@ -677,6 +694,10 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 				c->x86, c->x86_model);
 	}
 
+#ifdef CONFIG_X86_64
+	detect_ht(c);
+#endif
+
 	/*
 	 * On SMP, boot_cpu_data holds the common feature set between
 	 * all CPUs; so make sure that we indicate which features are
@@ -693,24 +714,34 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	for (i = 0; i < NCAPINTS; i++)
 		c->x86_capability[i] &= ~cleared_cpu_caps[i];
 
+#ifdef CONFIG_X86_MCE
 	/* Init Machine Check Exception if available. */
 	mcheck_init(c);
+#endif
 
 	select_idle_routine(c);
+
+#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+	numa_add_cpu(smp_processor_id());
+#endif
 }
 
 void __init identify_boot_cpu(void)
 {
 	identify_cpu(&boot_cpu_data);
+#ifdef CONFIG_X86_32
 	sysenter_setup();
 	enable_sep_cpu();
+#endif
 }
 
 void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)
 {
 	BUG_ON(c == &boot_cpu_data);
 	identify_cpu(c);
+#ifdef CONFIG_X86_32
 	enable_sep_cpu();
+#endif
 	mtrr_ap_init();
 }
 

commit b89d3b3e2caeab3d895301d1aee3cd2a91eddb79
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:12 2008 -0700

    x86: cpu/common*.c, merge generic_identify()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 61a70748213e..f35baa7f3036 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -548,6 +548,10 @@ void __init early_cpu_init(void)
  * of early VIA chips and (more importantly) broken virtualizers that
  * are not easy to detect.  Hence, probe for it based on first
  * principles.
+ *
+ * Note: no 64-bit chip is known to lack these, but put the code here
+ * for consistency with 32 bits, and to make it utterly trivial to
+ * diagnose the problem should it ever surface.
  */
 static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
 {
@@ -586,11 +590,16 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 
 	if (c->cpuid_level >= 0x00000001) {
 		c->initial_apicid = (cpuid_ebx(1) >> 24) & 0xFF;
-#ifdef CONFIG_X86_HT
+#ifdef CONFIG_X86_32
+# ifdef CONFIG_X86_HT
 		c->apicid = phys_pkg_id(c->initial_apicid, 0);
-		c->phys_proc_id = c->initial_apicid;
-#else
+# else
 		c->apicid = c->initial_apicid;
+# endif
+#endif
+
+#ifdef CONFIG_X86_HT
+		c->phys_proc_id = c->initial_apicid;
 #endif
 	}
 

commit 6627d2423067f2c6eedb422a59fba9270a3c5e36
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:10 2008 -0700

    x86: cpu/common*.c, merge early_identify_cpu()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ffc2f5ed09a3..61a70748213e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -490,7 +490,11 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
  */
 static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 {
+#ifdef CONFIG_X86_64
+	c->x86_clflush_size = 64;
+#else
 	c->x86_clflush_size = 32;
+#endif
 	c->x86_cache_alignment = c->x86_clflush_size;
 
 	if (!have_cpuid_p())

commit 5122c890ba969e6efeaba9ed45f5936e62d3c6d5
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:09 2008 -0700

    x86: cpu/common.c: merge get_cpu_cap()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b2018f76c94d..ffc2f5ed09a3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -458,6 +458,26 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 			c->x86_capability[6] = cpuid_ecx(0x80000001);
 		}
 	}
+
+#ifdef CONFIG_X86_64
+	/* Transmeta-defined flags: level 0x80860001 */
+	xlvl = cpuid_eax(0x80860000);
+	if ((xlvl & 0xffff0000) == 0x80860000) {
+		/* Don't set x86_cpuid_level here for now to not confuse. */
+		if (xlvl >= 0x80860001)
+			c->x86_capability[2] = cpuid_edx(0x80860001);
+	}
+
+	if (c->extended_cpuid_level >= 0x80000007)
+		c->x86_power = cpuid_edx(0x80000007);
+
+	if (c->extended_cpuid_level >= 0x80000008) {
+		u32 eax = cpuid_eax(0x80000008);
+
+		c->x86_virt_bits = (eax >> 8) & 0xff;
+		c->x86_phys_bits = eax & 0xff;
+	}
+#endif
 }
 /*
  * Do minimum CPU detection early.

commit 1cd78776c7d5487e3000426d0ce1ff203c5d60cd
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:08 2008 -0700

    x86: cpu/common*.c, merge detect_ht()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f9191207718b..b2018f76c94d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -330,6 +330,9 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_CMP_LEGACY))
 		goto out;
 
+	if (cpu_has(c, X86_FEATURE_XTOPOLOGY))
+		return;
+
 	cpuid(1, &eax, &ebx, &ecx, &edx);
 
 	smp_num_siblings = (ebx & 0xff0000) >> 16;
@@ -346,8 +349,11 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 		}
 
 		index_msb = get_count_order(smp_num_siblings);
+#ifdef CONFIG_X86_64
+		c->phys_proc_id = phys_pkg_id(index_msb);
+#else
 		c->phys_proc_id = phys_pkg_id(c->initial_apicid, index_msb);
-
+#endif
 
 		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
@@ -355,8 +361,13 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 
 		core_bits = get_count_order(c->x86_max_cores);
 
+#ifdef CONFIG_X86_64
+		c->cpu_core_id = phys_pkg_id(index_msb) &
+					       ((1 << core_bits) - 1);
+#else
 		c->cpu_core_id = phys_pkg_id(c->initial_apicid, index_msb) &
 					       ((1 << core_bits) - 1);
+#endif
 	}
 
 out:

commit 140fc72709278989f08eb756d16a70008bdcc409
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:07 2008 -0700

    x86: cpu/common*.c, merge display_cacheinfo()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 2c4bfa2e56ad..f9191207718b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -285,6 +285,10 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 		printk(KERN_INFO "CPU: L1 I Cache: %dK (%d bytes/line), D cache %dK (%d bytes/line)\n",
 				edx>>24, edx&0xFF, ecx>>24, ecx&0xFF);
 		c->x86_cache_size = (ecx>>24) + (edx>>24);
+#ifdef CONFIG_X86_64
+		/* On K8 L1 TLB is inclusive, so don't count it */
+		c->x86_tlbsize = 0;
+#endif
 	}
 
 	if (n < 0x80000006)	/* Some chips just has a large L1. */
@@ -293,6 +297,9 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 	cpuid(0x80000006, &dummy, &ebx, &ecx, &edx);
 	l2size = ecx >> 16;
 
+#ifdef CONFIG_X86_64
+	c->x86_tlbsize += ((ebx >> 16) & 0xfff) + (ebx & 0xfff);
+#else
 	/* do processor-specific cache resizing */
 	if (this_cpu->c_size_cache)
 		l2size = this_cpu->c_size_cache(c, l2size);
@@ -303,6 +310,7 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 
 	if (l2size == 0)
 		return;		/* Again, no L2 cache is possible */
+#endif
 
 	c->x86_cache_size = l2size;
 

commit b9e67f00424e164dcd29391eb48dc941db8691ad
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:06 2008 -0700

    x86: cpu/common.c, merge default_init()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 43d5287bb2a4..2c4bfa2e56ad 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -224,6 +224,9 @@ static struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 
 static void __cpuinit default_init(struct cpuinfo_x86 *c)
 {
+#ifdef CONFIG_X86_64
+	display_cacheinfo(c);
+#else
 	/* Not much we can do here... */
 	/* Check if at least it has cpuid */
 	if (c->cpuid_level == -1) {
@@ -233,6 +236,7 @@ static void __cpuinit default_init(struct cpuinfo_x86 *c)
 		else if (c->x86 == 3)
 			strcpy(c->x86_model_id, "386");
 	}
+#endif
 }
 
 static struct cpu_dev __cpuinitdata default_cpu = {

commit fab334c1d5f24d23d12f98ad652d399279cd03ce
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:05 2008 -0700

    x86: cpu/common*.c, merge switch_to_new_gdt()
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f44678db1616..43d5287bb2a4 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -215,7 +215,9 @@ void switch_to_new_gdt(void)
 	gdt_descr.address = (long)get_cpu_gdt_table(smp_processor_id());
 	gdt_descr.size = GDT_SIZE - 1;
 	load_gdt(&gdt_descr);
+#ifdef CONFIG_X86_32
 	asm("mov %0, %%fs" : : "r" (__KERNEL_PERCPU) : "memory");
+#endif
 }
 
 static struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};

commit 1ba76586f778be327e452180d8378e40ee70f066
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:04 2008 -0700

    x86: cpu/common*.c have same cpu_init(), with copying and #ifdef
    
    hard to merge by lines... (as here we have material differences between
    32-bit and 64-bit mode) - will try to do it later.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index dcd3ebd5ba62..f44678db1616 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -901,7 +901,129 @@ struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
  * initialized (naturally) in the bootstrap process, such as the GDT
  * and IDT. We reload them nevertheless, this function acts as a
  * 'CPU state barrier', nothing should get across.
+ * A lot of state is already set up in PDA init for 64 bit
  */
+#ifdef CONFIG_X86_64
+void __cpuinit cpu_init(void)
+{
+	int cpu = stack_smp_processor_id();
+	struct tss_struct *t = &per_cpu(init_tss, cpu);
+	struct orig_ist *orig_ist = &per_cpu(orig_ist, cpu);
+	unsigned long v;
+	char *estacks = NULL;
+	struct task_struct *me;
+	int i;
+
+	/* CPU 0 is initialised in head64.c */
+	if (cpu != 0)
+		pda_init(cpu);
+	else
+		estacks = boot_exception_stacks;
+
+	me = current;
+
+	if (cpu_test_and_set(cpu, cpu_initialized))
+		panic("CPU#%d already initialized!\n", cpu);
+
+	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
+
+	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+
+	/*
+	 * Initialize the per-CPU GDT with the boot GDT,
+	 * and set up the GDT descriptor:
+	 */
+
+	switch_to_new_gdt();
+	load_idt((const struct desc_ptr *)&idt_descr);
+
+	memset(me->thread.tls_array, 0, GDT_ENTRY_TLS_ENTRIES * 8);
+	syscall_init();
+
+	wrmsrl(MSR_FS_BASE, 0);
+	wrmsrl(MSR_KERNEL_GS_BASE, 0);
+	barrier();
+
+	check_efer();
+	if (cpu != 0 && x2apic)
+		enable_x2apic();
+
+	/*
+	 * set up and load the per-CPU TSS
+	 */
+	if (!orig_ist->ist[0]) {
+		static const unsigned int order[N_EXCEPTION_STACKS] = {
+		  [0 ... N_EXCEPTION_STACKS - 1] = EXCEPTION_STACK_ORDER,
+		  [DEBUG_STACK - 1] = DEBUG_STACK_ORDER
+		};
+		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
+			if (cpu) {
+				estacks = (char *)__get_free_pages(GFP_ATOMIC, order[v]);
+				if (!estacks)
+					panic("Cannot allocate exception "
+					      "stack %ld %d\n", v, cpu);
+			}
+			estacks += PAGE_SIZE << order[v];
+			orig_ist->ist[v] = t->x86_tss.ist[v] =
+					(unsigned long)estacks;
+		}
+	}
+
+	t->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
+	/*
+	 * <= is required because the CPU will access up to
+	 * 8 bits beyond the end of the IO permission bitmap.
+	 */
+	for (i = 0; i <= IO_BITMAP_LONGS; i++)
+		t->io_bitmap[i] = ~0UL;
+
+	atomic_inc(&init_mm.mm_count);
+	me->active_mm = &init_mm;
+	if (me->mm)
+		BUG();
+	enter_lazy_tlb(&init_mm, me);
+
+	load_sp0(t, &current->thread);
+	set_tss_desc(cpu, t);
+	load_TR_desc();
+	load_LDT(&init_mm.context);
+
+#ifdef CONFIG_KGDB
+	/*
+	 * If the kgdb is connected no debug regs should be altered.  This
+	 * is only applicable when KGDB and a KGDB I/O module are built
+	 * into the kernel and you are using early debugging with
+	 * kgdbwait. KGDB will control the kernel HW breakpoint registers.
+	 */
+	if (kgdb_connected && arch_kgdb_ops.correct_hw_break)
+		arch_kgdb_ops.correct_hw_break();
+	else {
+#endif
+	/*
+	 * Clear all 6 debug registers:
+	 */
+
+	set_debugreg(0UL, 0);
+	set_debugreg(0UL, 1);
+	set_debugreg(0UL, 2);
+	set_debugreg(0UL, 3);
+	set_debugreg(0UL, 6);
+	set_debugreg(0UL, 7);
+#ifdef CONFIG_KGDB
+	/* If the kgdb is connected no debug regs should be altered. */
+	}
+#endif
+
+	fpu_init();
+
+	raw_local_save_flags(kernel_eflags);
+
+	if (is_uv_system())
+		uv_cpu_init();
+}
+
+#else
+
 void __cpuinit cpu_init(void)
 {
 	int cpu = smp_processor_id();
@@ -982,3 +1104,5 @@ void __cpuinit cpu_uninit(void)
 	per_cpu(cpu_tlbstate, cpu).active_mm = &init_mm;
 }
 #endif
+
+#endif

commit d5494d4f517158b1d2eea1ae33f6c264eb12675f
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:03 2008 -0700

    x86: cpu/common*.c, make 32-bit have 64-bit only functions
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 9128ba0c8d33..dcd3ebd5ba62 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -750,6 +750,143 @@ __setup("clearcpuid=", setup_disablecpuid);
 
 cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
 
+#ifdef CONFIG_X86_64
+struct x8664_pda **_cpu_pda __read_mostly;
+EXPORT_SYMBOL(_cpu_pda);
+
+struct desc_ptr idt_descr = { 256 * 16 - 1, (unsigned long) idt_table };
+
+char boot_cpu_stack[IRQSTACKSIZE] __page_aligned_bss;
+
+unsigned long __supported_pte_mask __read_mostly = ~0UL;
+EXPORT_SYMBOL_GPL(__supported_pte_mask);
+
+static int do_not_nx __cpuinitdata;
+
+/* noexec=on|off
+Control non executable mappings for 64bit processes.
+
+on	Enable(default)
+off	Disable
+*/
+static int __init nonx_setup(char *str)
+{
+	if (!str)
+		return -EINVAL;
+	if (!strncmp(str, "on", 2)) {
+		__supported_pte_mask |= _PAGE_NX;
+		do_not_nx = 0;
+	} else if (!strncmp(str, "off", 3)) {
+		do_not_nx = 1;
+		__supported_pte_mask &= ~_PAGE_NX;
+	}
+	return 0;
+}
+early_param("noexec", nonx_setup);
+
+int force_personality32;
+
+/* noexec32=on|off
+Control non executable heap for 32bit processes.
+To control the stack too use noexec=off
+
+on	PROT_READ does not imply PROT_EXEC for 32bit processes (default)
+off	PROT_READ implies PROT_EXEC
+*/
+static int __init nonx32_setup(char *str)
+{
+	if (!strcmp(str, "on"))
+		force_personality32 &= ~READ_IMPLIES_EXEC;
+	else if (!strcmp(str, "off"))
+		force_personality32 |= READ_IMPLIES_EXEC;
+	return 1;
+}
+__setup("noexec32=", nonx32_setup);
+
+void pda_init(int cpu)
+{
+	struct x8664_pda *pda = cpu_pda(cpu);
+
+	/* Setup up data that may be needed in __get_free_pages early */
+	loadsegment(fs, 0);
+	loadsegment(gs, 0);
+	/* Memory clobbers used to order PDA accessed */
+	mb();
+	wrmsrl(MSR_GS_BASE, pda);
+	mb();
+
+	pda->cpunumber = cpu;
+	pda->irqcount = -1;
+	pda->kernelstack = (unsigned long)stack_thread_info() -
+				 PDA_STACKOFFSET + THREAD_SIZE;
+	pda->active_mm = &init_mm;
+	pda->mmu_state = 0;
+
+	if (cpu == 0) {
+		/* others are initialized in smpboot.c */
+		pda->pcurrent = &init_task;
+		pda->irqstackptr = boot_cpu_stack;
+		pda->irqstackptr += IRQSTACKSIZE - 64;
+	} else {
+		if (!pda->irqstackptr) {
+			pda->irqstackptr = (char *)
+				__get_free_pages(GFP_ATOMIC, IRQSTACK_ORDER);
+			if (!pda->irqstackptr)
+				panic("cannot allocate irqstack for cpu %d",
+				      cpu);
+			pda->irqstackptr += IRQSTACKSIZE - 64;
+		}
+
+		if (pda->nodenumber == 0 && cpu_to_node(cpu) != NUMA_NO_NODE)
+			pda->nodenumber = cpu_to_node(cpu);
+	}
+}
+
+char boot_exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ +
+			   DEBUG_STKSZ] __page_aligned_bss;
+
+extern asmlinkage void ignore_sysret(void);
+
+/* May not be marked __init: used by software suspend */
+void syscall_init(void)
+{
+	/*
+	 * LSTAR and STAR live in a bit strange symbiosis.
+	 * They both write to the same internal register. STAR allows to
+	 * set CS/DS but only a 32bit target. LSTAR sets the 64bit rip.
+	 */
+	wrmsrl(MSR_STAR,  ((u64)__USER32_CS)<<48  | ((u64)__KERNEL_CS)<<32);
+	wrmsrl(MSR_LSTAR, system_call);
+	wrmsrl(MSR_CSTAR, ignore_sysret);
+
+#ifdef CONFIG_IA32_EMULATION
+	syscall32_cpu_init();
+#endif
+
+	/* Flags to clear on syscall */
+	wrmsrl(MSR_SYSCALL_MASK,
+	       X86_EFLAGS_TF|X86_EFLAGS_DF|X86_EFLAGS_IF|X86_EFLAGS_IOPL);
+}
+
+void __cpuinit check_efer(void)
+{
+	unsigned long efer;
+
+	rdmsrl(MSR_EFER, efer);
+	if (!(efer & EFER_NX) || do_not_nx)
+		__supported_pte_mask &= ~_PAGE_NX;
+}
+
+unsigned long kernel_eflags;
+
+/*
+ * Copies of the original ist values from the tss are only accessed during
+ * debugging, no special alignment required.
+ */
+DEFINE_PER_CPU(struct orig_ist, orig_ist);
+
+#else
+
 /* Make sure %fs is initialized properly in idle threads */
 struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 {
@@ -757,6 +894,7 @@ struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 	regs->fs = __KERNEL_PERCPU;
 	return regs;
 }
+#endif
 
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already

commit ba51dced0b55eb62874e1646d5eeff344c3d4e67
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:02 2008 -0700

    x86: cpu/common.c, let 64-bit code have 32-bit only functions
    
    No effect on 64-bit.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e0ca51f4f2d3..9128ba0c8d33 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -93,6 +93,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 #endif
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
+#ifdef CONFIG_X86_32
 static int cachesize_override __cpuinitdata = -1;
 static int disable_x86_serial_nr __cpuinitdata = 1;
 
@@ -195,6 +196,13 @@ static int __init x86_serial_nr_setup(char *s)
 	return 1;
 }
 __setup("serialnumber", x86_serial_nr_setup);
+#else
+/* Probe for the CPUID instruction */
+static inline int have_cpuid_p(void)
+{
+	return 1;
+}
+#endif
 
 __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 

commit 950ad7ff6ec17fc1b47abc95c5c74628eb1adf8b
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:01 2008 -0700

    x86: same gdt_page with macro
    
    Move the 32-bit and 64-bit gdt_page definitions next to each
    other, separated with an #ifdef.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6bbdbc24f2ba..e0ca51f4f2d3 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -40,6 +40,22 @@
 
 static struct cpu_dev *this_cpu __cpuinitdata;
 
+#ifdef CONFIG_X86_64
+/* We need valid kernel segments for data and code in long mode too
+ * IRET will check the segment types  kkeil 2000/10/28
+ * Also sysret mandates a special GDT layout
+ */
+/* The TLS descriptors are currently at a different place compared to i386.
+   Hopefully nobody expects them at a fixed place (Wine?) */
+DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
+	[GDT_ENTRY_KERNEL32_CS] = { { { 0x0000ffff, 0x00cf9b00 } } },
+	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00af9b00 } } },
+	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9300 } } },
+	[GDT_ENTRY_DEFAULT_USER32_CS] = { { { 0x0000ffff, 0x00cffb00 } } },
+	[GDT_ENTRY_DEFAULT_USER_DS] = { { { 0x0000ffff, 0x00cff300 } } },
+	[GDT_ENTRY_DEFAULT_USER_CS] = { { { 0x0000ffff, 0x00affb00 } } },
+} };
+#else
 DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00cf9a00 } } },
 	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9200 } } },
@@ -74,6 +90,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	[GDT_ENTRY_ESPFIX_SS] = { { { 0x00000000, 0x00c09200 } } },
 	[GDT_ENTRY_PERCPU] = { { { 0x00000000, 0x00000000 } } },
 } };
+#endif
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
 static int cachesize_override __cpuinitdata = -1;

commit f0fc4aff1fa4db1c201593422dcbf85c9897ec4f
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:09:00 2008 -0700

    x86: make header file the same in arch/x86/kernel/cpu/common_xx.c
    
    Make the files more similar in preparation to unification, no
    code changed.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 1f80ce07daa2..6bbdbc24f2ba 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1,25 +1,41 @@
 #include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
 #include <linux/string.h>
+#include <linux/bootmem.h>
+#include <linux/bitops.h>
+#include <linux/module.h>
+#include <linux/kgdb.h>
+#include <linux/topology.h>
 #include <linux/delay.h>
 #include <linux/smp.h>
-#include <linux/module.h>
 #include <linux/percpu.h>
-#include <linux/bootmem.h>
-#include <asm/processor.h>
 #include <asm/i387.h>
 #include <asm/msr.h>
 #include <asm/io.h>
+#include <asm/linkage.h>
 #include <asm/mmu_context.h>
 #include <asm/mtrr.h>
 #include <asm/mce.h>
 #include <asm/pat.h>
 #include <asm/asm.h>
+#include <asm/numa.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
 #include <mach_apic.h>
+#include <asm/genapic.h>
 #endif
 
+#include <asm/pda.h>
+#include <asm/pgtable.h>
+#include <asm/processor.h>
+#include <asm/desc.h>
+#include <asm/atomic.h>
+#include <asm/proto.h>
+#include <asm/sections.h>
+#include <asm/setup.h>
+
 #include "cpu.h"
 
 static struct cpu_dev *this_cpu __cpuinitdata;

commit 97e4db7c8719f67c52793eeca5ec4df4c3407f2a
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 20:08:59 2008 -0700

    x86: make detect_ht depend on CONFIG_X86_HT
    
    64-bit has X86_HT set too, so use that instead of SMP.
    
    This also removes a include/asm-x86/processor.h ifdef.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7d5a07f0fd24..1f80ce07daa2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -263,9 +263,9 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 			l2size, ecx & 0xFF);
 }
 
-#ifdef CONFIG_X86_HT
 void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 {
+#ifdef CONFIG_X86_HT
 	u32 eax, ebx, ecx, edx;
 	int index_msb, core_bits;
 
@@ -311,8 +311,8 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 		printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
 		       c->cpu_core_id);
 	}
-}
 #endif
+}
 
 static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 {

commit d3d0ba7b8fb8f57c33207adcb41f40c176148c03
Merge: 9042763808c5 63cc8c751564
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 5 09:24:30 2008 +0200

    Merge commit '63cc8c75156462d4b42cbdd76c293b7eee7ddbfe':
    
      "percpu: introduce DEFINE_PER_CPU_PAGE_ALIGNED() macro"
    
    into x86/core
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 446d27338d3b422dd3dfe496d0f362230994d059
Merge: accf0fa697ee 0a488a53d7ca
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 5 09:19:50 2008 +0200

    Merge branch 'x86/cpu' into x86/core

commit 0a488a53d7ca46ac638c30079072c57e50cfcc7b
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:47 2008 +0200

    x86: move 32bit related functions together
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a79cf5c52b6a..2b2c170e8d69 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -22,6 +22,8 @@
 
 #include "cpu.h"
 
+static struct cpu_dev *this_cpu __cpuinitdata;
+
 DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
 	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00cf9a00 } } },
 	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9200 } } },
@@ -58,6 +60,109 @@ DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
+static int cachesize_override __cpuinitdata = -1;
+static int disable_x86_serial_nr __cpuinitdata = 1;
+
+static int __init cachesize_setup(char *str)
+{
+	get_option(&str, &cachesize_override);
+	return 1;
+}
+__setup("cachesize=", cachesize_setup);
+
+/*
+ * Naming convention should be: <Name> [(<Codename>)]
+ * This table only is used unless init_<vendor>() below doesn't set it;
+ * in particular, if CPUID levels 0x80000002..4 are supported, this isn't used
+ *
+ */
+
+/* Look up CPU names by table lookup. */
+static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
+{
+	struct cpu_model_info *info;
+
+	if (c->x86_model >= 16)
+		return NULL;	/* Range check */
+
+	if (!this_cpu)
+		return NULL;
+
+	info = this_cpu->c_models;
+
+	while (info && info->family) {
+		if (info->family == c->x86)
+			return info->model_names[c->x86_model];
+		info++;
+	}
+	return NULL;		/* Not found */
+}
+
+static int __init x86_fxsr_setup(char *s)
+{
+	setup_clear_cpu_cap(X86_FEATURE_FXSR);
+	setup_clear_cpu_cap(X86_FEATURE_XMM);
+	return 1;
+}
+__setup("nofxsr", x86_fxsr_setup);
+
+static int __init x86_sep_setup(char *s)
+{
+	setup_clear_cpu_cap(X86_FEATURE_SEP);
+	return 1;
+}
+__setup("nosep", x86_sep_setup);
+
+/* Standard macro to see if a specific flag is changeable */
+static inline int flag_is_changeable_p(u32 flag)
+{
+	u32 f1, f2;
+
+	asm("pushfl\n\t"
+	    "pushfl\n\t"
+	    "popl %0\n\t"
+	    "movl %0,%1\n\t"
+	    "xorl %2,%0\n\t"
+	    "pushl %0\n\t"
+	    "popfl\n\t"
+	    "pushfl\n\t"
+	    "popl %0\n\t"
+	    "popfl\n\t"
+	    : "=&r" (f1), "=&r" (f2)
+	    : "ir" (flag));
+
+	return ((f1^f2) & flag) != 0;
+}
+
+/* Probe for the CPUID instruction */
+static int __cpuinit have_cpuid_p(void)
+{
+	return flag_is_changeable_p(X86_EFLAGS_ID);
+}
+
+static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
+{
+	if (cpu_has(c, X86_FEATURE_PN) && disable_x86_serial_nr) {
+		/* Disable processor serial number */
+		unsigned long lo, hi;
+		rdmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
+		lo |= 0x200000;
+		wrmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
+		printk(KERN_NOTICE "CPU serial number disabled.\n");
+		clear_cpu_cap(c, X86_FEATURE_PN);
+
+		/* Disabling the serial number may affect the cpuid level */
+		c->cpuid_level = cpuid_eax(0);
+	}
+}
+
+static int __init x86_serial_nr_setup(char *s)
+{
+	disable_x86_serial_nr = 0;
+	return 1;
+}
+__setup("serialnumber", x86_serial_nr_setup);
+
 __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 
 /* Current gdt points %fs at the "master" per-cpu area: after this,
@@ -72,9 +177,6 @@ void switch_to_new_gdt(void)
 	asm("mov %0, %%fs" : : "r" (__KERNEL_PERCPU) : "memory");
 }
 
-static int cachesize_override __cpuinitdata = -1;
-static int disable_x86_serial_nr __cpuinitdata = 1;
-
 static struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 
 static void __cpuinit default_init(struct cpuinfo_x86 *c)
@@ -95,14 +197,6 @@ static struct cpu_dev __cpuinitdata default_cpu = {
 	.c_vendor = "Unknown",
 	.c_x86_vendor = X86_VENDOR_UNKNOWN,
 };
-static struct cpu_dev *this_cpu __cpuinitdata;
-
-static int __init cachesize_setup(char *str)
-{
-	get_option(&str, &cachesize_override);
-	return 1;
-}
-__setup("cachesize=", cachesize_setup);
 
 int __cpuinit get_model_name(struct cpuinfo_x86 *c)
 {
@@ -133,7 +227,6 @@ int __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	return 1;
 }
 
-
 void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 {
 	unsigned int n, dummy, ebx, ecx, edx, l2size;
@@ -150,7 +243,7 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 	if (n < 0x80000006)	/* Some chips just has a large L1. */
 		return;
 
-	ecx = cpuid_ecx(0x80000006);
+	cpuid(0x80000006, &dummy, &ebx, &ecx, &edx);
 	l2size = ecx >> 16;
 
 	/* do processor-specific cache resizing */
@@ -167,48 +260,23 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 	c->x86_cache_size = l2size;
 
 	printk(KERN_INFO "CPU: L2 Cache: %dK (%d bytes/line)\n",
-	       l2size, ecx & 0xFF);
-}
-
-/*
- * Naming convention should be: <Name> [(<Codename>)]
- * This table only is used unless init_<vendor>() below doesn't set it;
- * in particular, if CPUID levels 0x80000002..4 are supported, this isn't used
- *
- */
-
-/* Look up CPU names by table lookup. */
-static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
-{
-	struct cpu_model_info *info;
-
-	if (c->x86_model >= 16)
-		return NULL;	/* Range check */
-
-	if (!this_cpu)
-		return NULL;
-
-	info = this_cpu->c_models;
-
-	while (info && info->family) {
-		if (info->family == c->x86)
-			return info->model_names[c->x86_model];
-		info++;
-	}
-	return NULL;		/* Not found */
+			l2size, ecx & 0xFF);
 }
 
 #ifdef CONFIG_X86_HT
 void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 {
-	u32 	eax, ebx, ecx, edx;
-	int 	index_msb, core_bits;
-
-	cpuid(1, &eax, &ebx, &ecx, &edx);
+	u32 eax, ebx, ecx, edx;
+	int index_msb, core_bits;
 
-	if (!cpu_has(c, X86_FEATURE_HT) || cpu_has(c, X86_FEATURE_CMP_LEGACY))
+	if (!cpu_has(c, X86_FEATURE_HT))
 		return;
 
+	if (cpu_has(c, X86_FEATURE_CMP_LEGACY))
+		goto out;
+
+	cpuid(1, &eax, &ebx, &ecx, &edx);
+
 	smp_num_siblings = (ebx & 0xff0000) >> 16;
 
 	if (smp_num_siblings == 1) {
@@ -225,8 +293,6 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 		index_msb = get_count_order(smp_num_siblings);
 		c->phys_proc_id = phys_pkg_id(c->initial_apicid, index_msb);
 
-		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
-		       c->phys_proc_id);
 
 		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
@@ -236,10 +302,14 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 
 		c->cpu_core_id = phys_pkg_id(c->initial_apicid, index_msb) &
 					       ((1 << core_bits) - 1);
+	}
 
-		if (c->x86_max_cores > 1)
-			printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
-			       c->cpu_core_id);
+out:
+	if ((c->x86_max_cores * smp_num_siblings) > 1) {
+		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
+		       c->phys_proc_id);
+		printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
+		       c->cpu_core_id);
 	}
 }
 #endif
@@ -273,52 +343,6 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 	this_cpu = &default_cpu;
 }
 
-
-static int __init x86_fxsr_setup(char *s)
-{
-	setup_clear_cpu_cap(X86_FEATURE_FXSR);
-	setup_clear_cpu_cap(X86_FEATURE_XMM);
-	return 1;
-}
-__setup("nofxsr", x86_fxsr_setup);
-
-
-static int __init x86_sep_setup(char *s)
-{
-	setup_clear_cpu_cap(X86_FEATURE_SEP);
-	return 1;
-}
-__setup("nosep", x86_sep_setup);
-
-
-/* Standard macro to see if a specific flag is changeable */
-static inline int flag_is_changeable_p(u32 flag)
-{
-	u32 f1, f2;
-
-	asm("pushfl\n\t"
-	    "pushfl\n\t"
-	    "popl %0\n\t"
-	    "movl %0,%1\n\t"
-	    "xorl %2,%0\n\t"
-	    "pushl %0\n\t"
-	    "popfl\n\t"
-	    "pushfl\n\t"
-	    "popl %0\n\t"
-	    "popfl\n\t"
-	    : "=&r" (f1), "=&r" (f2)
-	    : "ir" (flag));
-
-	return ((f1^f2) & flag) != 0;
-}
-
-
-/* Probe for the CPUID instruction */
-static int __cpuinit have_cpuid_p(void)
-{
-	return flag_is_changeable_p(X86_EFLAGS_ID);
-}
-
 void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
 {
 	/* Get vendor name */
@@ -380,16 +404,16 @@ static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
  */
 static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 {
-	c->x86_cache_alignment = 32;
 	c->x86_clflush_size = 32;
+	c->x86_cache_alignment = c->x86_clflush_size;
 
 	if (!have_cpuid_p())
 		return;
 
-	c->extended_cpuid_level = 0;
-
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
 
+	c->extended_cpuid_level = 0;
+
 	cpu_detect(c);
 
 	get_cpu_vendor(c);
@@ -487,31 +511,6 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 	detect_nopl(c);
 }
 
-static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
-{
-	if (cpu_has(c, X86_FEATURE_PN) && disable_x86_serial_nr) {
-		/* Disable processor serial number */
-		unsigned long lo, hi;
-		rdmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
-		lo |= 0x200000;
-		wrmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
-		printk(KERN_NOTICE "CPU serial number disabled.\n");
-		clear_cpu_cap(c, X86_FEATURE_PN);
-
-		/* Disabling the serial number may affect the cpuid level */
-		c->cpuid_level = cpuid_eax(0);
-	}
-}
-
-static int __init x86_serial_nr_setup(char *s)
-{
-	disable_x86_serial_nr = 0;
-	return 1;
-}
-__setup("serialnumber", x86_serial_nr_setup);
-
-
-
 /*
  * This does the hard work of actually picking apart the CPU stuff...
  */

commit a0854a46c5eb82588126421a9cbceb973384a644
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:46 2008 +0200

    x86: make 32bit support show_msr like 64 bit
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ee1044ca481d..a79cf5c52b6a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -616,6 +616,49 @@ void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)
 	mtrr_ap_init();
 }
 
+struct msr_range {
+	unsigned min;
+	unsigned max;
+};
+
+static struct msr_range msr_range_array[] __cpuinitdata = {
+	{ 0x00000000, 0x00000418},
+	{ 0xc0000000, 0xc000040b},
+	{ 0xc0010000, 0xc0010142},
+	{ 0xc0011000, 0xc001103b},
+};
+
+static void __cpuinit print_cpu_msr(void)
+{
+	unsigned index;
+	u64 val;
+	int i;
+	unsigned index_min, index_max;
+
+	for (i = 0; i < ARRAY_SIZE(msr_range_array); i++) {
+		index_min = msr_range_array[i].min;
+		index_max = msr_range_array[i].max;
+		for (index = index_min; index < index_max; index++) {
+			if (rdmsrl_amd_safe(index, &val))
+				continue;
+			printk(KERN_INFO " MSR%08x: %016llx\n", index, val);
+		}
+	}
+}
+
+static int show_msr __cpuinitdata;
+static __init int setup_show_msr(char *arg)
+{
+	int num;
+
+	get_option(&arg, &num);
+
+	if (num > 0)
+		show_msr = num;
+	return 1;
+}
+__setup("show_msr=", setup_show_msr);
+
 static __init int setup_noclflush(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_CLFLSH);
@@ -644,6 +687,14 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 		printk(KERN_CONT " stepping %02x\n", c->x86_mask);
 	else
 		printk(KERN_CONT "\n");
+
+#ifdef CONFIG_SMP
+	if (c->cpu_index < show_msr)
+		print_cpu_msr();
+#else
+	if (show_msr)
+		print_cpu_msr();
+#endif
 }
 
 static __init int setup_disablecpuid(char *arg)

commit 10a434fcb23a57c385177a0086955fae01003f64
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:45 2008 +0200

    x86: remove cpu_vendor_dev
    
    1. add c_x86_vendor into cpu_dev
    2. change cpu_devs to static
    3. check c_x86_vendor before put that cpu_dev into array
    4. remove alignment for 64bit
    5. order the sequence in cpu_devs according to link sequence...
       so could put intel at first, then amd...
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 10e89ae5a600..ee1044ca481d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -75,7 +75,7 @@ void switch_to_new_gdt(void)
 static int cachesize_override __cpuinitdata = -1;
 static int disable_x86_serial_nr __cpuinitdata = 1;
 
-struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
+static struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 
 static void __cpuinit default_init(struct cpuinfo_x86 *c)
 {
@@ -93,8 +93,9 @@ static void __cpuinit default_init(struct cpuinfo_x86 *c)
 static struct cpu_dev __cpuinitdata default_cpu = {
 	.c_init	= default_init,
 	.c_vendor = "Unknown",
+	.c_x86_vendor = X86_VENDOR_UNKNOWN,
 };
-static struct cpu_dev *this_cpu __cpuinitdata = &default_cpu;
+static struct cpu_dev *this_cpu __cpuinitdata;
 
 static int __init cachesize_setup(char *str)
 {
@@ -250,21 +251,24 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 	static int printed;
 
 	for (i = 0; i < X86_VENDOR_NUM; i++) {
-		if (cpu_devs[i]) {
-			if (!strcmp(v, cpu_devs[i]->c_ident[0]) ||
-			    (cpu_devs[i]->c_ident[1] &&
-			     !strcmp(v, cpu_devs[i]->c_ident[1]))) {
-				c->x86_vendor = i;
-				this_cpu = cpu_devs[i];
-				return;
-			}
+		if (!cpu_devs[i])
+			break;
+
+		if (!strcmp(v, cpu_devs[i]->c_ident[0]) ||
+		    (cpu_devs[i]->c_ident[1] &&
+		     !strcmp(v, cpu_devs[i]->c_ident[1]))) {
+			this_cpu = cpu_devs[i];
+			c->x86_vendor = this_cpu->c_x86_vendor;
+			return;
 		}
 	}
+
 	if (!printed) {
 		printed++;
 		printk(KERN_ERR "CPU: Vendor unknown, using generic init.\n");
 		printk(KERN_ERR "CPU: Your system may be unstable.\n");
 	}
+
 	c->x86_vendor = X86_VENDOR_UNKNOWN;
 	this_cpu = &default_cpu;
 }
@@ -315,25 +319,6 @@ static int __cpuinit have_cpuid_p(void)
 	return flag_is_changeable_p(X86_EFLAGS_ID);
 }
 
-static void __init early_cpu_support_print(void)
-{
-	int i,j;
-	struct cpu_dev *cpu_devx;
-
-	printk("KERNEL supported cpus:\n");
-	for (i = 0; i < X86_VENDOR_NUM; i++) {
-		cpu_devx = cpu_devs[i];
-		if (!cpu_devx)
-			continue;
-		for (j = 0; j < 2; j++) {
-			if (!cpu_devx->c_ident[j])
-				continue;
-			printk("  %s %s\n", cpu_devx->c_vendor,
-				cpu_devx->c_ident[j]);
-		}
-	}
-}
-
 void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
 {
 	/* Get vendor name */
@@ -411,21 +396,35 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 
 	get_cpu_cap(c);
 
-	if (c->x86_vendor != X86_VENDOR_UNKNOWN &&
-	    cpu_devs[c->x86_vendor]->c_early_init)
-		cpu_devs[c->x86_vendor]->c_early_init(c);
+	if (this_cpu->c_early_init)
+		this_cpu->c_early_init(c);
 
 	validate_pat_support(c);
 }
 
 void __init early_cpu_init(void)
 {
-	struct cpu_vendor_dev *cvdev;
+	struct cpu_dev **cdev;
+	int count = 0;
+
+	printk("KERNEL supported cpus:\n");
+	for (cdev = __x86_cpu_dev_start; cdev < __x86_cpu_dev_end; cdev++) {
+		struct cpu_dev *cpudev = *cdev;
+		unsigned int j;
 
-	for (cvdev = __x86cpuvendor_start; cvdev < __x86cpuvendor_end; cvdev++)
-		cpu_devs[cvdev->vendor] = cvdev->cpu_dev;
+		if (count >= X86_VENDOR_NUM)
+			break;
+		cpu_devs[count] = cpudev;
+		count++;
+
+		for (j = 0; j < 2; j++) {
+			if (!cpudev->c_ident[j])
+				continue;
+			printk("  %s %s\n", cpudev->c_vendor,
+				cpudev->c_ident[j]);
+		}
+	}
 
-	early_cpu_support_print();
 	early_identify_cpu(&boot_cpu_data);
 }
 

commit 9d31d35b5f9d619bb2482235cc889326de049e29
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:44 2008 +0200

    x86: order functions in cpu/common.c and cpu/common_64.c v2
    
    v2: make 64 bit get c->x86_cache_alignment = c->x86_clfush_size
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 96e1b8698d3a..10e89ae5a600 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -60,6 +60,18 @@ EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
 __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 
+/* Current gdt points %fs at the "master" per-cpu area: after this,
+ * it's on the real one. */
+void switch_to_new_gdt(void)
+{
+	struct desc_ptr gdt_descr;
+
+	gdt_descr.address = (long)get_cpu_gdt_table(smp_processor_id());
+	gdt_descr.size = GDT_SIZE - 1;
+	load_gdt(&gdt_descr);
+	asm("mov %0, %%fs" : : "r" (__KERNEL_PERCPU) : "memory");
+}
+
 static int cachesize_override __cpuinitdata = -1;
 static int disable_x86_serial_nr __cpuinitdata = 1;
 
@@ -123,15 +135,15 @@ int __cpuinit get_model_name(struct cpuinfo_x86 *c)
 
 void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 {
-	unsigned int n, dummy, ecx, edx, l2size;
+	unsigned int n, dummy, ebx, ecx, edx, l2size;
 
 	n = c->extended_cpuid_level;
 
 	if (n >= 0x80000005) {
-		cpuid(0x80000005, &dummy, &dummy, &ecx, &edx);
+		cpuid(0x80000005, &dummy, &ebx, &ecx, &edx);
 		printk(KERN_INFO "CPU: L1 I Cache: %dK (%d bytes/line), D cache %dK (%d bytes/line)\n",
-			edx>>24, edx&0xFF, ecx>>24, ecx&0xFF);
-		c->x86_cache_size = (ecx>>24)+(edx>>24);
+				edx>>24, edx&0xFF, ecx>>24, ecx&0xFF);
+		c->x86_cache_size = (ecx>>24) + (edx>>24);
 	}
 
 	if (n < 0x80000006)	/* Some chips just has a large L1. */
@@ -185,6 +197,51 @@ static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
 	return NULL;		/* Not found */
 }
 
+#ifdef CONFIG_X86_HT
+void __cpuinit detect_ht(struct cpuinfo_x86 *c)
+{
+	u32 	eax, ebx, ecx, edx;
+	int 	index_msb, core_bits;
+
+	cpuid(1, &eax, &ebx, &ecx, &edx);
+
+	if (!cpu_has(c, X86_FEATURE_HT) || cpu_has(c, X86_FEATURE_CMP_LEGACY))
+		return;
+
+	smp_num_siblings = (ebx & 0xff0000) >> 16;
+
+	if (smp_num_siblings == 1) {
+		printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
+	} else if (smp_num_siblings > 1) {
+
+		if (smp_num_siblings > NR_CPUS) {
+			printk(KERN_WARNING "CPU: Unsupported number of siblings %d",
+					smp_num_siblings);
+			smp_num_siblings = 1;
+			return;
+		}
+
+		index_msb = get_count_order(smp_num_siblings);
+		c->phys_proc_id = phys_pkg_id(c->initial_apicid, index_msb);
+
+		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
+		       c->phys_proc_id);
+
+		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
+
+		index_msb = get_count_order(smp_num_siblings);
+
+		core_bits = get_count_order(c->x86_max_cores);
+
+		c->cpu_core_id = phys_pkg_id(c->initial_apicid, index_msb) &
+					       ((1 << core_bits) - 1);
+
+		if (c->x86_max_cores > 1)
+			printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
+			       c->cpu_core_id);
+	}
+}
+#endif
 
 static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 {
@@ -258,7 +315,26 @@ static int __cpuinit have_cpuid_p(void)
 	return flag_is_changeable_p(X86_EFLAGS_ID);
 }
 
-void __init cpu_detect(struct cpuinfo_x86 *c)
+static void __init early_cpu_support_print(void)
+{
+	int i,j;
+	struct cpu_dev *cpu_devx;
+
+	printk("KERNEL supported cpus:\n");
+	for (i = 0; i < X86_VENDOR_NUM; i++) {
+		cpu_devx = cpu_devs[i];
+		if (!cpu_devx)
+			continue;
+		for (j = 0; j < 2; j++) {
+			if (!cpu_devx->c_ident[j])
+				continue;
+			printk("  %s %s\n", cpu_devx->c_vendor,
+				cpu_devx->c_ident[j]);
+		}
+	}
+}
+
+void __cpuinit cpu_detect(struct cpuinfo_x86 *c)
 {
 	/* Get vendor name */
 	cpuid(0x00000000, (unsigned int *)&c->cpuid_level,
@@ -267,19 +343,20 @@ void __init cpu_detect(struct cpuinfo_x86 *c)
 	      (unsigned int *)&c->x86_vendor_id[4]);
 
 	c->x86 = 4;
+	/* Intel-defined flags: level 0x00000001 */
 	if (c->cpuid_level >= 0x00000001) {
 		u32 junk, tfms, cap0, misc;
 		cpuid(0x00000001, &tfms, &misc, &junk, &cap0);
-		c->x86 = (tfms >> 8) & 15;
-		c->x86_model = (tfms >> 4) & 15;
+		c->x86 = (tfms >> 8) & 0xf;
+		c->x86_model = (tfms >> 4) & 0xf;
+		c->x86_mask = tfms & 0xf;
 		if (c->x86 == 0xf)
 			c->x86 += (tfms >> 20) & 0xff;
 		if (c->x86 >= 0x6)
-			c->x86_model += ((tfms >> 16) & 0xF) << 4;
-		c->x86_mask = tfms & 15;
+			c->x86_model += ((tfms >> 16) & 0xf) << 4;
 		if (cap0 & (1<<19)) {
-			c->x86_cache_alignment = ((misc >> 8) & 0xff) * 8;
 			c->x86_clflush_size = ((misc >> 8) & 0xff) * 8;
+			c->x86_cache_alignment = c->x86_clflush_size;
 		}
 	}
 }
@@ -341,6 +418,17 @@ static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 	validate_pat_support(c);
 }
 
+void __init early_cpu_init(void)
+{
+	struct cpu_vendor_dev *cvdev;
+
+	for (cvdev = __x86cpuvendor_start; cvdev < __x86cpuvendor_end; cvdev++)
+		cpu_devs[cvdev->vendor] = cvdev->cpu_dev;
+
+	early_cpu_support_print();
+	early_identify_cpu(&boot_cpu_data);
+}
+
 /*
  * The NOPL instruction is supposed to exist on all CPUs with
  * family >= 6, unfortunately, that's not true in practice because
@@ -500,7 +588,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	 */
 	if (c != &boot_cpu_data) {
 		/* AND the already accumulated flags with these */
-		for (i = 0 ; i < NCAPINTS ; i++)
+		for (i = 0; i < NCAPINTS; i++)
 			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
 	}
 
@@ -529,52 +617,6 @@ void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)
 	mtrr_ap_init();
 }
 
-#ifdef CONFIG_X86_HT
-void __cpuinit detect_ht(struct cpuinfo_x86 *c)
-{
-	u32 	eax, ebx, ecx, edx;
-	int 	index_msb, core_bits;
-
-	cpuid(1, &eax, &ebx, &ecx, &edx);
-
-	if (!cpu_has(c, X86_FEATURE_HT) || cpu_has(c, X86_FEATURE_CMP_LEGACY))
-		return;
-
-	smp_num_siblings = (ebx & 0xff0000) >> 16;
-
-	if (smp_num_siblings == 1) {
-		printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
-	} else if (smp_num_siblings > 1) {
-
-		if (smp_num_siblings > NR_CPUS) {
-			printk(KERN_WARNING "CPU: Unsupported number of the "
-					"siblings %d", smp_num_siblings);
-			smp_num_siblings = 1;
-			return;
-		}
-
-		index_msb = get_count_order(smp_num_siblings);
-		c->phys_proc_id = phys_pkg_id(c->initial_apicid, index_msb);
-
-		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
-		       c->phys_proc_id);
-
-		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
-
-		index_msb = get_count_order(smp_num_siblings) ;
-
-		core_bits = get_count_order(c->x86_max_cores);
-
-		c->cpu_core_id = phys_pkg_id(c->initial_apicid, index_msb) &
-					       ((1 << core_bits) - 1);
-
-		if (c->x86_max_cores > 1)
-			printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
-			       c->cpu_core_id);
-	}
-}
-#endif
-
 static __init int setup_noclflush(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_CLFLSH);
@@ -592,17 +634,17 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 		vendor = c->x86_vendor_id;
 
 	if (vendor && strncmp(c->x86_model_id, vendor, strlen(vendor)))
-		printk("%s ", vendor);
+		printk(KERN_CONT "%s ", vendor);
 
-	if (!c->x86_model_id[0])
-		printk("%d86", c->x86);
+	if (c->x86_model_id[0])
+		printk(KERN_CONT "%s", c->x86_model_id);
 	else
-		printk("%s", c->x86_model_id);
+		printk(KERN_CONT "%d86", c->x86);
 
 	if (c->x86_mask || c->cpuid_level >= 0)
-		printk(" stepping %02x\n", c->x86_mask);
+		printk(KERN_CONT " stepping %02x\n", c->x86_mask);
 	else
-		printk("\n");
+		printk(KERN_CONT "\n");
 }
 
 static __init int setup_disablecpuid(char *arg)
@@ -618,16 +660,6 @@ __setup("clearcpuid=", setup_disablecpuid);
 
 cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
 
-void __init early_cpu_init(void)
-{
-	struct cpu_vendor_dev *cvdev;
-
-	for (cvdev = __x86cpuvendor_start; cvdev < __x86cpuvendor_end; cvdev++)
-		cpu_devs[cvdev->vendor] = cvdev->cpu_dev;
-
-	early_identify_cpu(&boot_cpu_data);
-}
-
 /* Make sure %fs is initialized properly in idle threads */
 struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 {
@@ -636,18 +668,6 @@ struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 	return regs;
 }
 
-/* Current gdt points %fs at the "master" per-cpu area: after this,
- * it's on the real one. */
-void switch_to_new_gdt(void)
-{
-	struct desc_ptr gdt_descr;
-
-	gdt_descr.address = (long)get_cpu_gdt_table(smp_processor_id());
-	gdt_descr.size = GDT_SIZE - 1;
-	load_gdt(&gdt_descr);
-	asm("mov %0, %%fs" : : "r" (__KERNEL_PERCPU) : "memory");
-}
-
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
  * initialized (naturally) in the bootstrap process, such as the GDT

commit 3da99c97763703b23cbf2bd6e96252256d4e4617
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:44 2008 +0200

    x86: make (early)_identify_cpu more the same between 32bit and 64 bit
    
    1. add extended_cpuid_level for 32bit
     2. add generic_identify for 64bit
     3. add early_identify_cpu for 32bit
     4. early_identify_cpu not be called by identify_cpu
     5. remove early in get_cpu_vendor for 32bit
     6. add get_cpu_cap
     7. add cpu_detect for 64bit
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 8aab8517642e..96e1b8698d3a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -96,7 +96,7 @@ int __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	unsigned int *v;
 	char *p, *q;
 
-	if (cpuid_eax(0x80000000) < 0x80000004)
+	if (c->extended_cpuid_level < 0x80000004)
 		return 0;
 
 	v = (unsigned int *) c->x86_model_id;
@@ -125,7 +125,7 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 {
 	unsigned int n, dummy, ecx, edx, l2size;
 
-	n = cpuid_eax(0x80000000);
+	n = c->extended_cpuid_level;
 
 	if (n >= 0x80000005) {
 		cpuid(0x80000005, &dummy, &dummy, &ecx, &edx);
@@ -186,7 +186,7 @@ static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
 }
 
 
-static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c, int early)
+static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c)
 {
 	char *v = c->x86_vendor_id;
 	int i;
@@ -198,8 +198,7 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c, int early)
 			    (cpu_devs[i]->c_ident[1] &&
 			     !strcmp(v, cpu_devs[i]->c_ident[1]))) {
 				c->x86_vendor = i;
-				if (!early)
-					this_cpu = cpu_devs[i];
+				this_cpu = cpu_devs[i];
 				return;
 			}
 		}
@@ -284,34 +283,30 @@ void __init cpu_detect(struct cpuinfo_x86 *c)
 		}
 	}
 }
-static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
+
+static void __cpuinit get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
-	unsigned int ebx;
+	u32 ebx;
 
-	memset(&c->x86_capability, 0, sizeof c->x86_capability);
-	if (have_cpuid_p()) {
-		/* Intel-defined flags: level 0x00000001 */
-		if (c->cpuid_level >= 0x00000001) {
-			u32 capability, excap;
-			cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
-			c->x86_capability[0] = capability;
-			c->x86_capability[4] = excap;
-		}
+	/* Intel-defined flags: level 0x00000001 */
+	if (c->cpuid_level >= 0x00000001) {
+		u32 capability, excap;
+		cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
+		c->x86_capability[0] = capability;
+		c->x86_capability[4] = excap;
+	}
 
-		/* AMD-defined flags: level 0x80000001 */
-		xlvl = cpuid_eax(0x80000000);
-		if ((xlvl & 0xffff0000) == 0x80000000) {
-			if (xlvl >= 0x80000001) {
-				c->x86_capability[1] = cpuid_edx(0x80000001);
-				c->x86_capability[6] = cpuid_ecx(0x80000001);
-			}
+	/* AMD-defined flags: level 0x80000001 */
+	xlvl = cpuid_eax(0x80000000);
+	c->extended_cpuid_level = xlvl;
+	if ((xlvl & 0xffff0000) == 0x80000000) {
+		if (xlvl >= 0x80000001) {
+			c->x86_capability[1] = cpuid_edx(0x80000001);
+			c->x86_capability[6] = cpuid_ecx(0x80000001);
 		}
-
 	}
-
 }
-
 /*
  * Do minimum CPU detection early.
  * Fields really needed: vendor, cpuid_level, family, model, mask,
@@ -321,25 +316,29 @@ static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
  * WARNING: this function is only called on the BP.  Don't add code here
  * that is supposed to run on all CPUs.
  */
-static void __init early_cpu_detect(void)
+static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 {
-	struct cpuinfo_x86 *c = &boot_cpu_data;
-
 	c->x86_cache_alignment = 32;
 	c->x86_clflush_size = 32;
 
 	if (!have_cpuid_p())
 		return;
 
+	c->extended_cpuid_level = 0;
+
+	memset(&c->x86_capability, 0, sizeof c->x86_capability);
+
 	cpu_detect(c);
 
-	get_cpu_vendor(c, 1);
+	get_cpu_vendor(c);
 
-	early_get_cap(c);
+	get_cpu_cap(c);
 
 	if (c->x86_vendor != X86_VENDOR_UNKNOWN &&
 	    cpu_devs[c->x86_vendor]->c_early_init)
 		cpu_devs[c->x86_vendor]->c_early_init(c);
+
+	validate_pat_support(c);
 }
 
 /*
@@ -373,60 +372,32 @@ static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
 
 static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 {
-	u32 tfms, xlvl;
-	unsigned int ebx;
-
-	if (have_cpuid_p()) {
-		/* Get vendor name */
-		cpuid(0x00000000, (unsigned int *)&c->cpuid_level,
-		      (unsigned int *)&c->x86_vendor_id[0],
-		      (unsigned int *)&c->x86_vendor_id[8],
-		      (unsigned int *)&c->x86_vendor_id[4]);
-
-		get_cpu_vendor(c, 0);
-		/* Initialize the standard set of capabilities */
-		/* Note that the vendor-specific code below might override */
-		/* Intel-defined flags: level 0x00000001 */
-		if (c->cpuid_level >= 0x00000001) {
-			u32 capability, excap;
-			cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
-			c->x86_capability[0] = capability;
-			c->x86_capability[4] = excap;
-			c->x86 = (tfms >> 8) & 15;
-			c->x86_model = (tfms >> 4) & 15;
-			if (c->x86 == 0xf)
-				c->x86 += (tfms >> 20) & 0xff;
-			if (c->x86 >= 0x6)
-				c->x86_model += ((tfms >> 16) & 0xF) << 4;
-			c->x86_mask = tfms & 15;
-			c->initial_apicid = (ebx >> 24) & 0xFF;
+	if (!have_cpuid_p())
+		return;
+
+	c->extended_cpuid_level = 0;
+
+	cpu_detect(c);
+
+	get_cpu_vendor(c);
+
+	get_cpu_cap(c);
+
+	if (c->cpuid_level >= 0x00000001) {
+		c->initial_apicid = (cpuid_ebx(1) >> 24) & 0xFF;
 #ifdef CONFIG_X86_HT
-			c->apicid = phys_pkg_id(c->initial_apicid, 0);
-			c->phys_proc_id = c->initial_apicid;
+		c->apicid = phys_pkg_id(c->initial_apicid, 0);
+		c->phys_proc_id = c->initial_apicid;
 #else
-			c->apicid = c->initial_apicid;
+		c->apicid = c->initial_apicid;
 #endif
-			if (test_cpu_cap(c, X86_FEATURE_CLFLSH))
-				c->x86_clflush_size = ((ebx >> 8) & 0xff) * 8;
-		} else {
-			/* Have CPUID level 0 only - unheard of */
-			c->x86 = 4;
-		}
+	}
 
-		/* AMD-defined flags: level 0x80000001 */
-		xlvl = cpuid_eax(0x80000000);
-		if ((xlvl & 0xffff0000) == 0x80000000) {
-			if (xlvl >= 0x80000001) {
-				c->x86_capability[1] = cpuid_edx(0x80000001);
-				c->x86_capability[6] = cpuid_ecx(0x80000001);
-			}
-			if (xlvl >= 0x80000004)
-				get_model_name(c); /* Default name */
-		}
+	if (c->extended_cpuid_level >= 0x80000004)
+		get_model_name(c); /* Default name */
 
-		init_scattered_cpuid_features(c);
-		detect_nopl(c);
-	}
+	init_scattered_cpuid_features(c);
+	detect_nopl(c);
 }
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
@@ -651,13 +622,10 @@ void __init early_cpu_init(void)
 {
 	struct cpu_vendor_dev *cvdev;
 
-	for (cvdev = __x86cpuvendor_start ;
-	     cvdev < __x86cpuvendor_end   ;
-	     cvdev++)
+	for (cvdev = __x86cpuvendor_start; cvdev < __x86cpuvendor_end; cvdev++)
 		cpu_devs[cvdev->vendor] = cvdev->cpu_dev;
 
-	early_cpu_detect();
-	validate_pat_support(&boot_cpu_data);
+	early_identify_cpu(&boot_cpu_data);
 }
 
 /* Make sure %fs is initialized properly in idle threads */

commit 5031088dbc0cd30e893eb27d53f0e0eee6eb1c00
Author: Krzysztof Helt <krzysztof.h1@wp.pl>
Date:   Thu Sep 4 21:09:43 2008 +0200

    x86: delay early cpu initialization until cpuid is done
    
    Move early cpu initialization after cpu early get cap so the
    early cpu initialization can fix up cpu caps.
    
    Signed-off-by: Krzysztof Helt <krzysztof.h1@wp.pl>
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0785b3c8d043..8aab8517642e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -335,11 +335,11 @@ static void __init early_cpu_detect(void)
 
 	get_cpu_vendor(c, 1);
 
+	early_get_cap(c);
+
 	if (c->x86_vendor != X86_VENDOR_UNKNOWN &&
 	    cpu_devs[c->x86_vendor]->c_early_init)
 		cpu_devs[c->x86_vendor]->c_early_init(c);
-
-	early_get_cap(c);
 }
 
 /*

commit fe47784ba5cbb6b713c013e046859946789b45e4
Merge: 83b8e28b14d6 af2e1f276ff0
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Sep 4 09:04:45 2008 -0700

    Merge branch 'x86/cpu' into x86/xsave
    
    Conflicts:
    
            arch/x86/kernel/cpu/feature_names.c
            include/asm-x86/cpufeature.h

commit 379002586368ae22916f668011c9118c8ce8189c
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:12 2008 +0100

    x86_32: clean up play_dead
    
    The removal of the CPU from the various maps was redundant as it already
    happened in cpu_disable.
    
    After cleaning this up, cpu_uninit only resets the tlb state, so rename
    it and create a noop version for the X86_64 case (so the two play_deads
    can be unified later).
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 80ab20d4fa39..18f5551b32dd 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -714,14 +714,10 @@ void __cpuinit cpu_init(void)
 	mxcsr_feature_mask_init();
 }
 
-#ifdef CONFIG_HOTPLUG_CPU
-void __cpuinit cpu_uninit(void)
+void reset_lazy_tlbstate(void)
 {
 	int cpu = raw_smp_processor_id();
-	cpu_clear(cpu, cpu_initialized);
 
-	/* lazy TLB state */
 	per_cpu(cpu_tlbstate, cpu).state = 0;
 	per_cpu(cpu_tlbstate, cpu).active_mm = &init_mm;
 }
-#endif

commit 7e00df5818964298c9821365a6cb7a8304227c5c
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Aug 18 17:39:32 2008 -0700

    x86: add NOPL as a synthetic CPU feature bit
    
    The long noops ("NOPL") are supposed to be detected by family >= 6.
    Unfortunately, several non-Intel x86 implementations, both hardware
    and software, don't obey this dictum.  Instead, probe for NOPL
    directly by executing a NOPL instruction and see if we get #UD.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 80ab20d4fa39..0785b3c8d043 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -13,6 +13,7 @@
 #include <asm/mtrr.h>
 #include <asm/mce.h>
 #include <asm/pat.h>
+#include <asm/asm.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
@@ -341,6 +342,35 @@ static void __init early_cpu_detect(void)
 	early_get_cap(c);
 }
 
+/*
+ * The NOPL instruction is supposed to exist on all CPUs with
+ * family >= 6, unfortunately, that's not true in practice because
+ * of early VIA chips and (more importantly) broken virtualizers that
+ * are not easy to detect.  Hence, probe for it based on first
+ * principles.
+ */
+static void __cpuinit detect_nopl(struct cpuinfo_x86 *c)
+{
+	const u32 nopl_signature = 0x888c53b1; /* Random number */
+	u32 has_nopl = nopl_signature;
+
+	clear_cpu_cap(c, X86_FEATURE_NOPL);
+	if (c->x86 >= 6) {
+		asm volatile("\n"
+			     "1:      .byte 0x0f,0x1f,0xc0\n" /* nopl %eax */
+			     "2:\n"
+			     "        .section .fixup,\"ax\"\n"
+			     "3:      xor %0,%0\n"
+			     "        jmp 2b\n"
+			     "        .previous\n"
+			     _ASM_EXTABLE(1b,3b)
+			     : "+a" (has_nopl));
+
+		if (has_nopl == nopl_signature)
+			set_cpu_cap(c, X86_FEATURE_NOPL);
+	}
+}
+
 static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
@@ -395,8 +425,8 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		}
 
 		init_scattered_cpuid_features(c);
+		detect_nopl(c);
 	}
-
 }
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)

commit b359e8a434cc3d09847010fc4aeccf48d69740e4
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Jul 29 10:29:20 2008 -0700

    x86, xsave: context switch support using xsave/xrstor
    
    Uses xsave/xrstor (instead of traditional fxsave/fxrstor) in context switch
    when available.
    
    Introduces TS_XSAVE flag, which determine the need to use xsave/xrstor
    instructions during context switch instead of the legacy fxsave/fxrstor
    instructions. Thread-synchronous status word is already in L1 cache during
    this code patch and thus minimizes the performance penality compared to
    (cpu_has_xsave) checks.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fabbcb7020fb..6c2b9e756db2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -709,7 +709,10 @@ void __cpuinit cpu_init(void)
 	/*
 	 * Force FPU initialization:
 	 */
-	current_thread_info()->status = 0;
+	if (cpu_has_xsave)
+		current_thread_info()->status = TS_XSAVE;
+	else
+		current_thread_info()->status = 0;
 	clear_used_math();
 	mxcsr_feature_mask_init();
 

commit dc1e35c6e95e8923cf1d3510438b63c600fee1e2
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Jul 29 10:29:19 2008 -0700

    x86, xsave: enable xsave/xrstor on cpus with xsave support
    
    Enables xsave/xrstor by turning on cr4.osxsave on cpu's which have
    the xsave support. For now, features that OS supports/enabled are
    FP and SSE.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 80ab20d4fa39..fabbcb7020fb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -712,6 +712,14 @@ void __cpuinit cpu_init(void)
 	current_thread_info()->status = 0;
 	clear_used_math();
 	mxcsr_feature_mask_init();
+
+	/*
+	 * Boot processor to setup the FP and extended state context info.
+	 */
+	if (!smp_processor_id())
+		init_thread_xstate();
+
+	xsave_init();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 9a250347591da3e60b5ee53dd1d341732f081117
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sat Jun 21 03:24:00 2008 -0700

    x86: change identify_cpu to static
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d0463a946247..80ab20d4fa39 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -427,7 +427,7 @@ __setup("serialnumber", x86_serial_nr_setup);
 /*
  * This does the hard work of actually picking apart the CPU stuff...
  */
-void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
+static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 {
 	int i;
 

commit 63cc8c75156462d4b42cbdd76c293b7eee7ddbfe
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Mon May 12 15:44:40 2008 +0200

    percpu: introduce DEFINE_PER_CPU_PAGE_ALIGNED() macro
    
    While examining holes in percpu section I found this :
    
    c05f5000 D per_cpu__current_task
    c05f5000 D __per_cpu_start
    c05f5004 D per_cpu__cpu_number
    c05f5008 D per_cpu__irq_regs
    c05f500c d per_cpu__cpu_devices
    c05f5040 D per_cpu__cyc2ns
    
    <Big Hole of about 4000 bytes>
    
    c05f6000 d per_cpu__cpuid4_info
    c05f6004 d per_cpu__cache_kobject
    c05f6008 d per_cpu__index_kobject
    
    <Big Hole of about 4000 bytes>
    
    c05f7000 D per_cpu__gdt_page
    
    This is because gdt_page is a percpu variable, defined with
    a page alignement, and linker is doing its job, two times because of .o
    nesting in the build process.
    
    I introduced a new macro DEFINE_PER_CPU_PAGE_ALIGNED() to avoid
    wasting this space. All page aligned variables (only one at this time)
    are put in a separate
    subsection .data.percpu.page_aligned, at the very begining of percpu zone.
    
    Before patch , on a x86_32 machine :
    
    .data.percpu                30232   3227471872
    .data.percpu                22168   3227471872
    
    Thats 8064 bytes saved for each CPU.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d0463a946247..b2f54fafb8bc 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -21,7 +21,7 @@
 
 #include "cpu.h"
 
-DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
+DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00cf9a00 } } },
 	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9200 } } },
 	[GDT_ENTRY_DEFAULT_USER_CS] = { { { 0x0000ffff, 0x00cffa00 } } },

commit 8d4a4300854f3971502e81dacd930704cb88f606
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 8 09:18:43 2008 +0200

    x86: cleanup PAT cpu validation
    
    Move the scattered checks for PAT support to a single function. Its
    moved to addon_cpuid_features.c as this file is shared between 32 and
    64 bit.
    
    Remove the manipulation of the PAT feature bit and just disable PAT in
    the PAT layer, based on the PAT bit provided by the CPU and the
    current CPU version/model white list.
    
    Change the boot CPU check so it works on Voyager somewhere in the
    future as well :) Also panic, when a secondary has PAT disabled but
    the primary one has alrady switched to PAT. We have no way to undo
    that.
    
    The white list is kept for now to ensure that we can rely on known to
    work CPU types and concentrate on the software induced problems
    instead of fighthing CPU erratas and subtle wreckage caused by not yet
    verified CPUs. Once the PAT code has stabilized enough, we can remove
    the white list and open the can of worms.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 35b4f6a9c8ef..d0463a946247 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -12,6 +12,7 @@
 #include <asm/mmu_context.h>
 #include <asm/mtrr.h>
 #include <asm/mce.h>
+#include <asm/pat.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
@@ -308,19 +309,6 @@ static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
 
 	}
 
-	clear_cpu_cap(c, X86_FEATURE_PAT);
-
-	switch (c->x86_vendor) {
-	case X86_VENDOR_AMD:
-		if (c->x86 >= 0xf && c->x86 <= 0x11)
-			set_cpu_cap(c, X86_FEATURE_PAT);
-		break;
-	case X86_VENDOR_INTEL:
-		if (c->x86 == 0xF || (c->x86 == 6 && c->x86_model >= 15))
-			set_cpu_cap(c, X86_FEATURE_PAT);
-		break;
-	}
-
 }
 
 /*
@@ -409,18 +397,6 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		init_scattered_cpuid_features(c);
 	}
 
-	clear_cpu_cap(c, X86_FEATURE_PAT);
-
-	switch (c->x86_vendor) {
-	case X86_VENDOR_AMD:
-		if (c->x86 >= 0xf && c->x86 <= 0x11)
-			set_cpu_cap(c, X86_FEATURE_PAT);
-		break;
-	case X86_VENDOR_INTEL:
-		if (c->x86 == 0xF || (c->x86 == 6 && c->x86_model >= 15))
-			set_cpu_cap(c, X86_FEATURE_PAT);
-		break;
-	}
 }
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
@@ -651,6 +627,7 @@ void __init early_cpu_init(void)
 		cpu_devs[cvdev->vendor] = cvdev->cpu_dev;
 
 	early_cpu_detect();
+	validate_pat_support(&boot_cpu_data);
 }
 
 /* Make sure %fs is initialized properly in idle threads */

commit 950e4da32426859ee4b37b2c95026d4f1efa5d05
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Tue Feb 26 09:55:29 2008 -0500

    arch: Remove unnecessary inclusions of asm/semaphore.h
    
    None of these files use any of the functionality promised by
    asm/semaphore.h.  It's possible that they rely on it dragging in some
    unrelated header file, but I can't build all these files, so we'll have
    fix any build failures as they come up.
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d999d7833bc2..35b4f6a9c8ef 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -5,7 +5,6 @@
 #include <linux/module.h>
 #include <linux/percpu.h>
 #include <linux/bootmem.h>
-#include <asm/semaphore.h>
 #include <asm/processor.h>
 #include <asm/i387.h>
 #include <asm/msr.h>

commit 9307cacad0dfe3749f00303125c6f7f0523e5616
Author: Yinghai Lu <yhlu.kernel.send@gmail.com>
Date:   Mon Mar 24 23:24:34 2008 -0700

    x86: pat cpu feature bit setting for known cpus
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0dd87b8d6707..d999d7833bc2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -309,6 +309,19 @@ static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
 
 	}
 
+	clear_cpu_cap(c, X86_FEATURE_PAT);
+
+	switch (c->x86_vendor) {
+	case X86_VENDOR_AMD:
+		if (c->x86 >= 0xf && c->x86 <= 0x11)
+			set_cpu_cap(c, X86_FEATURE_PAT);
+		break;
+	case X86_VENDOR_INTEL:
+		if (c->x86 == 0xF || (c->x86 == 6 && c->x86_model >= 15))
+			set_cpu_cap(c, X86_FEATURE_PAT);
+		break;
+	}
+
 }
 
 /*
@@ -397,6 +410,18 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		init_scattered_cpuid_features(c);
 	}
 
+	clear_cpu_cap(c, X86_FEATURE_PAT);
+
+	switch (c->x86_vendor) {
+	case X86_VENDOR_AMD:
+		if (c->x86 >= 0xf && c->x86 <= 0x11)
+			set_cpu_cap(c, X86_FEATURE_PAT);
+		break;
+	case X86_VENDOR_INTEL:
+		if (c->x86 == 0xF || (c->x86 == 6 && c->x86_model >= 15))
+			set_cpu_cap(c, X86_FEATURE_PAT);
+		break;
+	}
 }
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)

commit 01aaea1afbcdb7c49fe4a567ebe3e295db9f720d
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Mar 6 13:46:39 2008 -0800

    x86: introduce initial apicid
    
    store initial_apicid from early identify. it is could be different from
    phys_proc_id later.
    
    also print it out in /proc/cpuinfo.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 57a46c36fa23..0dd87b8d6707 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -369,10 +369,12 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 			if (c->x86 >= 0x6)
 				c->x86_model += ((tfms >> 16) & 0xF) << 4;
 			c->x86_mask = tfms & 15;
+			c->initial_apicid = (ebx >> 24) & 0xFF;
 #ifdef CONFIG_X86_HT
-			c->apicid = phys_pkg_id((ebx >> 24) & 0xFF, 0);
+			c->apicid = phys_pkg_id(c->initial_apicid, 0);
+			c->phys_proc_id = c->initial_apicid;
 #else
-			c->apicid = (ebx >> 24) & 0xFF;
+			c->apicid = c->initial_apicid;
 #endif
 			if (test_cpu_cap(c, X86_FEATURE_CLFLSH))
 				c->x86_clflush_size = ((ebx >> 8) & 0xff) * 8;
@@ -395,9 +397,6 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 		init_scattered_cpuid_features(c);
 	}
 
-#ifdef CONFIG_X86_HT
-	c->phys_proc_id = (cpuid_ebx(1) >> 24) & 0xff;
-#endif
 }
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
@@ -554,7 +553,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 		}
 
 		index_msb = get_count_order(smp_num_siblings);
-		c->phys_proc_id = phys_pkg_id((ebx >> 24) & 0xFF, index_msb);
+		c->phys_proc_id = phys_pkg_id(c->initial_apicid, index_msb);
 
 		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
 		       c->phys_proc_id);
@@ -565,7 +564,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 
 		core_bits = get_count_order(c->x86_max_cores);
 
-		c->cpu_core_id = phys_pkg_id((ebx >> 24) & 0xFF, index_msb) &
+		c->cpu_core_id = phys_pkg_id(c->initial_apicid, index_msb) &
 					       ((1 << core_bits) - 1);
 
 		if (c->x86_max_cores > 1)

commit 4cbe668add030a35e0592a9bb292e0f2a1bcea88
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 26 08:51:32 2008 +0100

    x86: clean up cpu capabilities accesses, common.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index bd111ce8f605..57a46c36fa23 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -409,7 +409,7 @@ static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 		lo |= 0x200000;
 		wrmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
 		printk(KERN_NOTICE "CPU serial number disabled.\n");
-		clear_bit(X86_FEATURE_PN, c->x86_capability);
+		clear_cpu_cap(c, X86_FEATURE_PN);
 
 		/* Disabling the serial number may affect the cpuid level */
 		c->cpuid_level = cpuid_eax(0);

commit 9716951efd98ada69c417adddc85d9bbe1d7835a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 26 08:54:01 2008 +0100

    x86: clean up cpu capabilities accesses, generic
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ef9e31b89b35..bd111ce8f605 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -374,7 +374,7 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 #else
 			c->apicid = (ebx >> 24) & 0xFF;
 #endif
-			if (c->x86_capability[0] & (1<<19))
+			if (test_cpu_cap(c, X86_FEATURE_CLFLSH))
 				c->x86_clflush_size = ((ebx >> 8) & 0xff) * 8;
 		} else {
 			/* Have CPUID level 0 only - unheard of */

commit 34048c9e927d5ae29c6ba802c826370de2a046d2
Author: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
Date:   Sun Feb 24 11:58:13 2008 +0100

    x86: coding style fixes to arch/x86/kernel/cpu/common.c
    
    Before:
       total: 55 errors, 6 warnings, 727 lines checked
    After:
       total: 0 errors, 3 warnings, 734 lines checked
    
    No code changed:
    
    arch/x86/kernel/cpu/common.o:
    
       text    data     bss     dec     hex filename
       3500    4611      44    8155    1fdb common.o.before
       3500    4611      44    8155    1fdb common.o.after
    
    md5:
       e37091f11fbeb682c0db152ac3022a38  common.o.before.asm
       e37091f11fbeb682c0db152ac3022a38  common.o.after.asm
    
    Signed-off-by: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0fd6be154d5d..ef9e31b89b35 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -62,9 +62,9 @@ __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 static int cachesize_override __cpuinitdata = -1;
 static int disable_x86_serial_nr __cpuinitdata = 1;
 
-struct cpu_dev * cpu_devs[X86_VENDOR_NUM] = {};
+struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 
-static void __cpuinit default_init(struct cpuinfo_x86 * c)
+static void __cpuinit default_init(struct cpuinfo_x86 *c)
 {
 	/* Not much we can do here... */
 	/* Check if at least it has cpuid */
@@ -81,11 +81,11 @@ static struct cpu_dev __cpuinitdata default_cpu = {
 	.c_init	= default_init,
 	.c_vendor = "Unknown",
 };
-static struct cpu_dev * this_cpu __cpuinitdata = &default_cpu;
+static struct cpu_dev *this_cpu __cpuinitdata = &default_cpu;
 
 static int __init cachesize_setup(char *str)
 {
-	get_option (&str, &cachesize_override);
+	get_option(&str, &cachesize_override);
 	return 1;
 }
 __setup("cachesize=", cachesize_setup);
@@ -107,12 +107,12 @@ int __cpuinit get_model_name(struct cpuinfo_x86 *c)
 	/* Intel chips right-justify this string for some dumb reason;
 	   undo that brain damage */
 	p = q = &c->x86_model_id[0];
-	while ( *p == ' ' )
+	while (*p == ' ')
 	     p++;
-	if ( p != q ) {
-	     while ( *p )
+	if (p != q) {
+	     while (*p)
 		  *q++ = *p++;
-	     while ( q <= &c->x86_model_id[48] )
+	     while (q <= &c->x86_model_id[48])
 		  *q++ = '\0';	/* Zero-pad the rest */
 	}
 
@@ -130,7 +130,7 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 		cpuid(0x80000005, &dummy, &dummy, &ecx, &edx);
 		printk(KERN_INFO "CPU: L1 I Cache: %dK (%d bytes/line), D cache %dK (%d bytes/line)\n",
 			edx>>24, edx&0xFF, ecx>>24, ecx&0xFF);
-		c->x86_cache_size=(ecx>>24)+(edx>>24);	
+		c->x86_cache_size = (ecx>>24)+(edx>>24);
 	}
 
 	if (n < 0x80000006)	/* Some chips just has a large L1. */
@@ -138,16 +138,16 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 
 	ecx = cpuid_ecx(0x80000006);
 	l2size = ecx >> 16;
-	
+
 	/* do processor-specific cache resizing */
 	if (this_cpu->c_size_cache)
-		l2size = this_cpu->c_size_cache(c,l2size);
+		l2size = this_cpu->c_size_cache(c, l2size);
 
 	/* Allow user to override all this if necessary. */
 	if (cachesize_override != -1)
 		l2size = cachesize_override;
 
-	if ( l2size == 0 )
+	if (l2size == 0)
 		return;		/* Again, no L2 cache is possible */
 
 	c->x86_cache_size = l2size;
@@ -156,16 +156,19 @@ void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
 	       l2size, ecx & 0xFF);
 }
 
-/* Naming convention should be: <Name> [(<Codename>)] */
-/* This table only is used unless init_<vendor>() below doesn't set it; */
-/* in particular, if CPUID levels 0x80000002..4 are supported, this isn't used */
+/*
+ * Naming convention should be: <Name> [(<Codename>)]
+ * This table only is used unless init_<vendor>() below doesn't set it;
+ * in particular, if CPUID levels 0x80000002..4 are supported, this isn't used
+ *
+ */
 
 /* Look up CPU names by table lookup. */
 static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
 {
 	struct cpu_model_info *info;
 
-	if ( c->x86_model >= 16 )
+	if (c->x86_model >= 16)
 		return NULL;	/* Range check */
 
 	if (!this_cpu)
@@ -190,9 +193,9 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c, int early)
 
 	for (i = 0; i < X86_VENDOR_NUM; i++) {
 		if (cpu_devs[i]) {
-			if (!strcmp(v,cpu_devs[i]->c_ident[0]) ||
-			    (cpu_devs[i]->c_ident[1] && 
-			     !strcmp(v,cpu_devs[i]->c_ident[1]))) {
+			if (!strcmp(v, cpu_devs[i]->c_ident[0]) ||
+			    (cpu_devs[i]->c_ident[1] &&
+			     !strcmp(v, cpu_devs[i]->c_ident[1]))) {
 				c->x86_vendor = i;
 				if (!early)
 					this_cpu = cpu_devs[i];
@@ -210,7 +213,7 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c, int early)
 }
 
 
-static int __init x86_fxsr_setup(char * s)
+static int __init x86_fxsr_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_FXSR);
 	setup_clear_cpu_cap(X86_FEATURE_XMM);
@@ -219,7 +222,7 @@ static int __init x86_fxsr_setup(char * s)
 __setup("nofxsr", x86_fxsr_setup);
 
 
-static int __init x86_sep_setup(char * s)
+static int __init x86_sep_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_SEP);
 	return 1;
@@ -308,12 +311,15 @@ static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
 
 }
 
-/* Do minimum CPU detection early.
-   Fields really needed: vendor, cpuid_level, family, model, mask, cache alignment.
-   The others are not touched to avoid unwanted side effects.
-
-   WARNING: this function is only called on the BP.  Don't add code here
-   that is supposed to run on all CPUs. */
+/*
+ * Do minimum CPU detection early.
+ * Fields really needed: vendor, cpuid_level, family, model, mask,
+ * cache alignment.
+ * The others are not touched to avoid unwanted side effects.
+ *
+ * WARNING: this function is only called on the BP.  Don't add code here
+ * that is supposed to run on all CPUs.
+ */
 static void __init early_cpu_detect(void)
 {
 	struct cpuinfo_x86 *c = &boot_cpu_data;
@@ -335,7 +341,7 @@ static void __init early_cpu_detect(void)
 	early_get_cap(c);
 }
 
-static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
+static void __cpuinit generic_identify(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
 	unsigned int ebx;
@@ -346,13 +352,12 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
 		      (unsigned int *)&c->x86_vendor_id[0],
 		      (unsigned int *)&c->x86_vendor_id[8],
 		      (unsigned int *)&c->x86_vendor_id[4]);
-		
+
 		get_cpu_vendor(c, 0);
 		/* Initialize the standard set of capabilities */
 		/* Note that the vendor-specific code below might override */
-	
 		/* Intel-defined flags: level 0x00000001 */
-		if ( c->cpuid_level >= 0x00000001 ) {
+		if (c->cpuid_level >= 0x00000001) {
 			u32 capability, excap;
 			cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
 			c->x86_capability[0] = capability;
@@ -378,12 +383,12 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
 
 		/* AMD-defined flags: level 0x80000001 */
 		xlvl = cpuid_eax(0x80000000);
-		if ( (xlvl & 0xffff0000) == 0x80000000 ) {
-			if ( xlvl >= 0x80000001 ) {
+		if ((xlvl & 0xffff0000) == 0x80000000) {
+			if (xlvl >= 0x80000001) {
 				c->x86_capability[1] = cpuid_edx(0x80000001);
 				c->x86_capability[6] = cpuid_ecx(0x80000001);
 			}
-			if ( xlvl >= 0x80000004 )
+			if (xlvl >= 0x80000004)
 				get_model_name(c); /* Default name */
 		}
 
@@ -397,12 +402,12 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
 
 static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 {
-	if (cpu_has(c, X86_FEATURE_PN) && disable_x86_serial_nr ) {
+	if (cpu_has(c, X86_FEATURE_PN) && disable_x86_serial_nr) {
 		/* Disable processor serial number */
-		unsigned long lo,hi;
-		rdmsr(MSR_IA32_BBL_CR_CTL,lo,hi);
+		unsigned long lo, hi;
+		rdmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
 		lo |= 0x200000;
-		wrmsr(MSR_IA32_BBL_CR_CTL,lo,hi);
+		wrmsr(MSR_IA32_BBL_CR_CTL, lo, hi);
 		printk(KERN_NOTICE "CPU serial number disabled.\n");
 		clear_bit(X86_FEATURE_PN, c->x86_capability);
 
@@ -439,9 +444,11 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
 
 	if (!have_cpuid_p()) {
-		/* First of all, decide if this is a 486 or higher */
-		/* It's a 486 if we can modify the AC flag */
-		if ( flag_is_changeable_p(X86_EFLAGS_AC) )
+		/*
+		 * First of all, decide if this is a 486 or higher
+		 * It's a 486 if we can modify the AC flag
+		 */
+		if (flag_is_changeable_p(X86_EFLAGS_AC))
 			c->x86 = 4;
 		else
 			c->x86 = 3;
@@ -474,10 +481,10 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	 */
 
 	/* If the model name is still unset, do table lookup. */
-	if ( !c->x86_model_id[0] ) {
+	if (!c->x86_model_id[0]) {
 		char *p;
 		p = table_lookup_model(c);
-		if ( p )
+		if (p)
 			strcpy(c->x86_model_id, p);
 		else
 			/* Last resort... */
@@ -491,9 +498,9 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	 * common between the CPUs.  The first time this routine gets
 	 * executed, c == &boot_cpu_data.
 	 */
-	if ( c != &boot_cpu_data ) {
+	if (c != &boot_cpu_data) {
 		/* AND the already accumulated flags with these */
-		for ( i = 0 ; i < NCAPINTS ; i++ )
+		for (i = 0 ; i < NCAPINTS ; i++)
 			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
 	}
 
@@ -537,7 +544,7 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 
 	if (smp_num_siblings == 1) {
 		printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
-	} else if (smp_num_siblings > 1 ) {
+	} else if (smp_num_siblings > 1) {
 
 		if (smp_num_siblings > NR_CPUS) {
 			printk(KERN_WARNING "CPU: Unsupported number of the "
@@ -592,7 +599,7 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 	else
 		printk("%s", c->x86_model_id);
 
-	if (c->x86_mask || c->cpuid_level >= 0) 
+	if (c->x86_mask || c->cpuid_level >= 0)
 		printk(" stepping %02x\n", c->x86_mask);
 	else
 		printk("\n");
@@ -653,7 +660,7 @@ void __cpuinit cpu_init(void)
 {
 	int cpu = smp_processor_id();
 	struct task_struct *curr = current;
-	struct tss_struct * t = &per_cpu(init_tss, cpu);
+	struct tss_struct *t = &per_cpu(init_tss, cpu);
 	struct thread_struct *thread = &curr->thread;
 
 	if (cpu_test_and_set(cpu, cpu_initialized)) {
@@ -679,7 +686,7 @@ void __cpuinit cpu_init(void)
 	enter_lazy_tlb(&init_mm, curr);
 
 	load_sp0(t, thread);
-	set_tss_desc(cpu,t);
+	set_tss_desc(cpu, t);
 	load_TR_desc();
 	load_LDT(&init_mm.context);
 

commit 03ae5768b6110ebaa97dc3e7abf1c3d8bec5f874
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri Feb 15 12:00:23 2008 +0100

    x86: use ELF section to list CPU vendor specific code
    
    Replace the hardcoded list of initialization functions for each CPU
    vendor by a list in an ELF section, which is read at initialization in
    arch/x86/kernel/cpu/cpu.c to fill the cpu_devs[] array. The ELF
    section, named .x86cpuvendor.init, is reclaimed after boot, and
    contains entries of type "struct cpu_vendor_dev" which associates a
    vendor number with a pointer to a "struct cpu_dev" structure.
    
    This first modification allows to remove all the VENDOR_init_cpu()
    functions.
    
    This patch also removes the hardcoded calls to early_init_amd() and
    early_init_intel(). Instead, we add a "c_early_init" member to the
    cpu_dev structure, which is then called if not NULL by the generic CPU
    initialization code. Unfortunately, in early_cpu_detect(), this_cpu is
    not yet set, so we have to use the cpu_devs[] array directly.
    
    This patch is part of the Linux Tiny project, and is needed for
    further patch that will allow to disable compilation of unused CPU
    support code.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a38aafaefc23..0fd6be154d5d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -328,14 +328,9 @@ static void __init early_cpu_detect(void)
 
 	get_cpu_vendor(c, 1);
 
-	switch (c->x86_vendor) {
-	case X86_VENDOR_AMD:
-		early_init_amd(c);
-		break;
-	case X86_VENDOR_INTEL:
-		early_init_intel(c);
-		break;
-	}
+	if (c->x86_vendor != X86_VENDOR_UNKNOWN &&
+	    cpu_devs[c->x86_vendor]->c_early_init)
+		cpu_devs[c->x86_vendor]->c_early_init(c);
 
 	early_get_cap(c);
 }
@@ -616,23 +611,15 @@ __setup("clearcpuid=", setup_disablecpuid);
 
 cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
 
-/* This is hacky. :)
- * We're emulating future behavior.
- * In the future, the cpu-specific init functions will be called implicitly
- * via the magic of initcalls.
- * They will insert themselves into the cpu_devs structure.
- * Then, when cpu_init() is called, we can just iterate over that array.
- */
 void __init early_cpu_init(void)
 {
-	intel_cpu_init();
-	cyrix_init_cpu();
-	nsc_init_cpu();
-	amd_init_cpu();
-	centaur_init_cpu();
-	transmeta_init_cpu();
-	nexgen_init_cpu();
-	umc_init_cpu();
+	struct cpu_vendor_dev *cvdev;
+
+	for (cvdev = __x86cpuvendor_start ;
+	     cvdev < __x86cpuvendor_end   ;
+	     cvdev++)
+		cpu_devs[cvdev->vendor] = cvdev->cpu_dev;
+
 	early_cpu_detect();
 }
 

commit 12c247a6719987aad65f83158d2bb3e73c75c1f5
Author: Mikael Pettersson <mikpe@it.uu.se>
Date:   Sun Feb 24 18:27:03 2008 +0100

    x86: fix boot failure on 486 due to TSC breakage
    
     > Diffing dmesg between git7 and git8 doesn't sched any light since
     > git8 also removed the printouts of the x86 caps as they were being
     > initialised and updated. I'm currently adding those printouts back
     > in the hope of seeing where and when the caps get broken.
    
    That turned out to be very illuminating:
    
     --- dmesg-2.6.24-git7  2008-02-24 18:01:25.295851000 +0100
     +++ dmesg-2.6.24-git8  2008-02-24 18:01:25.530358000 +0100
     ...
     CPU: After generic identify, caps: 00000003 00000000 00000000 00000000 00000000 00000000 00000000 00000000
    
     CPU: After all inits, caps: 00000003 00000000 00000000 00000000 00000000 00000000 00000000 00000000
    +CPU: After applying cleared_cpu_caps, caps: 00000013 00000000 00000000 00000000 00000000 00000000 00000000 00000000
    
    Notice how the TSC cap bit goes from Off to On.
    
    (The first two lines are printout loops from -git7 forward-ported
    to -git8, the third line is the same printout loop added just after
    the xor-with-cleared_cpu_caps[] loop.)
    
    Here's how the breakage occurs:
    1. arch/x86/kernel/tsc_32.c:tsc_init() sees !cpu_has_tsc,
       so bails and calls setup_clear_cpu_cap(X86_FEATURE_TSC).
    2. include/asm-x86/cpufeature.h:setup_clear_cpu_cap(bit) clears
       the bit in boot_cpu_data and sets it in cleared_cpu_caps
    3. arch/x86/kernel/cpu/common.c:identify_cpu() XORs all caps
       in with cleared_cpu_caps
       HOWEVER, at this point c->x86_capability correctly has TSC
       Off, cleared_cpu_caps has TSC On, so the XOR incorrectly
       sets TSC to On in c->x86_capability, with disastrous results.
    
    The real bug is that clearing bits with XOR only works if the
    bits are known to be 1 prior to the XOR, and that's not true here.
    
    A simple fix is to convert the XOR to AND-NOT instead. The following
    patch does that, and allows my 486 to boot 2.6.25-rc kernels again.
    
    [ mingo@elte.hu: fixed a similar bug in setup_64.c as well. ]
    
    The breakage was introduced via commit 7d851c8d3db0.
    
    Signed-off-by: Mikael Pettersson <mikpe@it.uu.se>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f86a3c4a2669..a38aafaefc23 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -504,7 +504,7 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 	/* Clear all flags overriden by options */
 	for (i = 0; i < NCAPINTS; i++)
-		c->x86_capability[i] ^= cleared_cpu_caps[i];
+		c->x86_capability[i] &= ~cleared_cpu_caps[i];
 
 	/* Init Machine Check Exception if available. */
 	mcheck_init(c);

commit 6b2fb3c65844452bb9e8b449d50863d1b36c5dc0
Author: Adrian Bunk <bunk@kernel.org>
Date:   Wed Feb 6 01:37:55 2008 -0800

    idle_regs() must be __cpuinit
    
    Fix the following section mismatch with CONFIG_HOTPLUG=n,
    CONFIG_HOTPLUG_CPU=y:
    
    WARNING: vmlinux.o(.text+0x399a6): Section mismatch: reference to .init.text.5:idle_regs (between 'fork_idle' and 'get_task_mm')
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d9313d9adced..f86a3c4a2669 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -637,7 +637,7 @@ void __init early_cpu_init(void)
 }
 
 /* Make sure %fs is initialized properly in idle threads */
-struct pt_regs * __devinit idle_regs(struct pt_regs *regs)
+struct pt_regs * __cpuinit idle_regs(struct pt_regs *regs)
 {
 	memset(regs, 0, sizeof(struct pt_regs));
 	regs->fs = __KERNEL_PERCPU;

commit b6d549a2967881af4f02d02062acbfeb807d44b4
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Mon Feb 4 16:48:04 2008 +0100

    x86: add cpu init function defintions to cpu.h
    
    cpu.h was already included everywhere needed.
    
    Fixes following sparse warnings:
    
    arch/x86/kernel/cpu/amd.c:343:12: warning: symbol 'amd_init_cpu' was not declared. Should it be static?
    arch/x86/kernel/cpu/cyrix.c:444:12: warning: symbol 'cyrix_init_cpu' was not declared. Should it be static?
    arch/x86/kernel/cpu/cyrix.c:456:12: warning: symbol 'nsc_init_cpu' was not declared. Should it be static?
    arch/x86/kernel/cpu/centaur.c:467:12: warning: symbol 'centaur_init_cpu' was not declared. Should it be static?
    arch/x86/kernel/cpu/transmeta.c:112:12: warning: symbol 'transmeta_init_cpu' was not declared. Should it be static?
    arch/x86/kernel/cpu/intel.c:296:12: warning: symbol 'intel_cpu_init' was not declared. Should it be static?
    arch/x86/kernel/cpu/nexgen.c:56:12: warning: symbol 'nexgen_init_cpu' was not declared. Should it be static?
    arch/x86/kernel/cpu/umc.c:22:12: warning: symbol 'umc_init_cpu' was not declared. Should it be static?
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index b7b2142b58e7..d9313d9adced 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -623,16 +623,6 @@ cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
  * They will insert themselves into the cpu_devs structure.
  * Then, when cpu_init() is called, we can just iterate over that array.
  */
-
-extern int intel_cpu_init(void);
-extern int cyrix_init_cpu(void);
-extern int nsc_init_cpu(void);
-extern int amd_init_cpu(void);
-extern int centaur_init_cpu(void);
-extern int transmeta_init_cpu(void);
-extern int nexgen_init_cpu(void);
-extern int umc_init_cpu(void);
-
 void __init early_cpu_init(void)
 {
 	intel_cpu_init();

commit 4a1485131a6038ba5382f407db48badc332672c4
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Fri Feb 1 17:49:43 2008 +0100

    x86: fix sparse warnings in cpu/common.c
    
    The casts will always be needed, may as well make them the right
    signedness.  The ebx variables can easily be unsigned, may as well.
    
    arch/x86/kernel/cpu/common.c:261:21: warning: incorrect type in argument 2 (different signedness)
    arch/x86/kernel/cpu/common.c:261:21:    expected unsigned int *eax
    arch/x86/kernel/cpu/common.c:261:21:    got int *<noident>
    arch/x86/kernel/cpu/common.c:262:9: warning: incorrect type in argument 3 (different signedness)
    arch/x86/kernel/cpu/common.c:262:9:    expected unsigned int *ebx
    arch/x86/kernel/cpu/common.c:262:9:    got int *<noident>
    arch/x86/kernel/cpu/common.c:263:9: warning: incorrect type in argument 4 (different signedness)
    arch/x86/kernel/cpu/common.c:263:9:    expected unsigned int *ecx
    arch/x86/kernel/cpu/common.c:263:9:    got int *<noident>
    arch/x86/kernel/cpu/common.c:264:9: warning: incorrect type in argument 5 (different signedness)
    arch/x86/kernel/cpu/common.c:264:9:    expected unsigned int *edx
    arch/x86/kernel/cpu/common.c:264:9:    got int *<noident>
    arch/x86/kernel/cpu/common.c:293:30: warning: incorrect type in argument 3 (different signedness)
    arch/x86/kernel/cpu/common.c:293:30:    expected unsigned int *ebx
    arch/x86/kernel/cpu/common.c:293:30:    got int *<noident>
    arch/x86/kernel/cpu/common.c:350:22: warning: incorrect type in argument 2 (different signedness)
    arch/x86/kernel/cpu/common.c:350:22:    expected unsigned int *eax
    arch/x86/kernel/cpu/common.c:350:22:    got int *<noident>
    arch/x86/kernel/cpu/common.c:351:10: warning: incorrect type in argument 3 (different signedness)
    arch/x86/kernel/cpu/common.c:351:10:    expected unsigned int *ebx
    arch/x86/kernel/cpu/common.c:351:10:    got int *<noident>
    arch/x86/kernel/cpu/common.c:352:10: warning: incorrect type in argument 4 (different signedness)
    arch/x86/kernel/cpu/common.c:352:10:    expected unsigned int *ecx
    arch/x86/kernel/cpu/common.c:352:10:    got int *<noident>
    arch/x86/kernel/cpu/common.c:353:10: warning: incorrect type in argument 5 (different signedness)
    arch/x86/kernel/cpu/common.c:353:10:    expected unsigned int *edx
    arch/x86/kernel/cpu/common.c:353:10:    got int *<noident>
    arch/x86/kernel/cpu/common.c:362:30: warning: incorrect type in argument 3 (different signedness)
    arch/x86/kernel/cpu/common.c:362:30:    expected unsigned int *ebx
    arch/x86/kernel/cpu/common.c:362:30:    got int *<noident>
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d608c9ebbfe2..b7b2142b58e7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -258,10 +258,10 @@ static int __cpuinit have_cpuid_p(void)
 void __init cpu_detect(struct cpuinfo_x86 *c)
 {
 	/* Get vendor name */
-	cpuid(0x00000000, &c->cpuid_level,
-	      (int *)&c->x86_vendor_id[0],
-	      (int *)&c->x86_vendor_id[8],
-	      (int *)&c->x86_vendor_id[4]);
+	cpuid(0x00000000, (unsigned int *)&c->cpuid_level,
+	      (unsigned int *)&c->x86_vendor_id[0],
+	      (unsigned int *)&c->x86_vendor_id[8],
+	      (unsigned int *)&c->x86_vendor_id[4]);
 
 	c->x86 = 4;
 	if (c->cpuid_level >= 0x00000001) {
@@ -283,7 +283,7 @@ void __init cpu_detect(struct cpuinfo_x86 *c)
 static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
 {
 	u32 tfms, xlvl;
-	int ebx;
+	unsigned int ebx;
 
 	memset(&c->x86_capability, 0, sizeof c->x86_capability);
 	if (have_cpuid_p()) {
@@ -343,14 +343,14 @@ static void __init early_cpu_detect(void)
 static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
 {
 	u32 tfms, xlvl;
-	int ebx;
+	unsigned int ebx;
 
 	if (have_cpuid_p()) {
 		/* Get vendor name */
-		cpuid(0x00000000, &c->cpuid_level,
-		      (int *)&c->x86_vendor_id[0],
-		      (int *)&c->x86_vendor_id[8],
-		      (int *)&c->x86_vendor_id[4]);
+		cpuid(0x00000000, (unsigned int *)&c->cpuid_level,
+		      (unsigned int *)&c->x86_vendor_id[0],
+		      (unsigned int *)&c->x86_vendor_id[8],
+		      (unsigned int *)&c->x86_vendor_id[4]);
 		
 		get_cpu_vendor(c, 0);
 		/* Initialize the standard set of capabilities */

commit d4387bd3fa1d27e03bc87533c1650e24417c8016
Author: Huang, Ying <ying.huang@intel.com>
Date:   Thu Jan 31 22:05:45 2008 +0100

    x86: c_p_a clflush_cache_range fix
    
    Because in i386 early boot stage, boot_cpu_data may be not available,
    which makes clflush_cach_range() into infinite loop, which is called
    by change_page_attr(). This patch fixes this by setting
    boot_cpu_data.x86_clflush_size in early_cpu_detect().
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index db28aa9e2f69..d608c9ebbfe2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -274,8 +274,10 @@ void __init cpu_detect(struct cpuinfo_x86 *c)
 		if (c->x86 >= 0x6)
 			c->x86_model += ((tfms >> 16) & 0xF) << 4;
 		c->x86_mask = tfms & 15;
-		if (cap0 & (1<<19))
+		if (cap0 & (1<<19)) {
 			c->x86_cache_alignment = ((misc >> 8) & 0xff) * 8;
+			c->x86_clflush_size = ((misc >> 8) & 0xff) * 8;
+		}
 	}
 }
 static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
@@ -317,6 +319,7 @@ static void __init early_cpu_detect(void)
 	struct cpuinfo_x86 *c = &boot_cpu_data;
 
 	c->x86_cache_alignment = 32;
+	c->x86_clflush_size = 32;
 
 	if (!have_cpuid_p())
 		return;

commit 12d6f21eacc21d84a809829543f2fe45c7e37319
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 30 13:33:58 2008 +0100

    x86: do not PSE on CONFIG_DEBUG_PAGEALLOC=y
    
    get more testing of the c_p_a() code done by not turning off
    PSE on DEBUG_PAGEALLOC.
    
    this simplifies the early pagetable setup code, and tests
    the largepage-splitup code quite heavily.
    
    In the end, all the largepages will be split up pretty quickly,
    so there's no difference to how DEBUG_PAGEALLOC worked before.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index bba850b05d0e..db28aa9e2f69 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -641,13 +641,6 @@ void __init early_cpu_init(void)
 	nexgen_init_cpu();
 	umc_init_cpu();
 	early_cpu_detect();
-
-#ifdef CONFIG_DEBUG_PAGEALLOC
-	/* pse is not compatible with on-the-fly unmapping,
-	 * disable it even if the cpus claim to support it.
-	 */
-	setup_clear_cpu_cap(X86_FEATURE_PSE);
-#endif
 }
 
 /* Make sure %fs is initialized properly in idle threads */

commit 093af8d7f0ba3c6be1485973508584ef081e9f93
Author: Yinghai Lu <Yinghai.Lu@Sun.COM>
Date:   Wed Jan 30 13:33:32 2008 +0100

    x86_32: trim memory by updating e820
    
    when MTRRs are not covering the whole e820 table, we need to trim the
    RAM and need to update e820.
    
    reuse some code on 64-bit as well.
    
    here need to add early_get_cap and use it in early_cpu_detect, and move
    mtrr_bp_init early.
    
    The code successfully trimmed the memory map on Justin's system:
    
    from:
    
     [    0.000000]  BIOS-e820: 0000000100000000 - 000000022c000000 (usable)
    
    to:
    
     [    0.000000]   modified: 0000000100000000 - 0000000228000000 (usable)
     [    0.000000]   modified: 0000000228000000 - 000000022c000000 (reserved)
    
    According to Justin it makes quite a difference:
    
    |  When I boot the box without any trimming it acts like a 286 or 386,
    |  takes about 10 minutes to boot (using raptor disks).
    
    Signed-off-by: Yinghai Lu <yinghai.lu@sun.com>
    Tested-by: Justin Piszcz <jpiszcz@lucidpixels.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 56cc341cc586..bba850b05d0e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -278,6 +278,33 @@ void __init cpu_detect(struct cpuinfo_x86 *c)
 			c->x86_cache_alignment = ((misc >> 8) & 0xff) * 8;
 	}
 }
+static void __cpuinit early_get_cap(struct cpuinfo_x86 *c)
+{
+	u32 tfms, xlvl;
+	int ebx;
+
+	memset(&c->x86_capability, 0, sizeof c->x86_capability);
+	if (have_cpuid_p()) {
+		/* Intel-defined flags: level 0x00000001 */
+		if (c->cpuid_level >= 0x00000001) {
+			u32 capability, excap;
+			cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
+			c->x86_capability[0] = capability;
+			c->x86_capability[4] = excap;
+		}
+
+		/* AMD-defined flags: level 0x80000001 */
+		xlvl = cpuid_eax(0x80000000);
+		if ((xlvl & 0xffff0000) == 0x80000000) {
+			if (xlvl >= 0x80000001) {
+				c->x86_capability[1] = cpuid_edx(0x80000001);
+				c->x86_capability[6] = cpuid_ecx(0x80000001);
+			}
+		}
+
+	}
+
+}
 
 /* Do minimum CPU detection early.
    Fields really needed: vendor, cpuid_level, family, model, mask, cache alignment.
@@ -306,6 +333,8 @@ static void __init early_cpu_detect(void)
 		early_init_intel(c);
 		break;
 	}
+
+	early_get_cap(c);
 }
 
 static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
@@ -485,7 +514,6 @@ void __init identify_boot_cpu(void)
 	identify_cpu(&boot_cpu_data);
 	sysenter_setup();
 	enable_sep_cpu();
-	mtrr_bp_init();
 }
 
 void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)

commit ac72e7888a612dccfbc15b34698aad441bdfda10
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:21 2008 +0100

    x86: add generic clearcpuid=... option
    
    Add a generic option to clear any cpuid bit. I added it because it was
    very easy to add with the new generic cpuid disable bitmap and perhaps
    it will be useful in the future.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 56b7ea8e6c79..56cc341cc586 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -572,6 +572,17 @@ void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 		printk("\n");
 }
 
+static __init int setup_disablecpuid(char *arg)
+{
+	int bit;
+	if (get_option(&arg, &bit) && bit < NCAPINTS*32)
+		setup_clear_cpu_cap(bit);
+	else
+		return 0;
+	return 1;
+}
+__setup("clearcpuid=", setup_disablecpuid);
+
 cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
 
 /* This is hacky. :)

commit 191679fdfa63342752ff6a094a2522ae939b8d0c
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:21 2008 +0100

    x86: add noclflush option
    
    To disable CLFLUSH usage, especially in change_page_attr().
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index dfc9563fc4f0..56b7ea8e6c79 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -542,6 +542,13 @@ void __cpuinit detect_ht(struct cpuinfo_x86 *c)
 }
 #endif
 
+static __init int setup_noclflush(char *arg)
+{
+	setup_clear_cpu_cap(X86_FEATURE_CLFLSH);
+	return 1;
+}
+__setup("noclflush", setup_noclflush);
+
 void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
 {
 	char *vendor = NULL;

commit 404ee5b14b68d3cba287c2596588b83790c49f7b
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:20 2008 +0100

    x86: convert TSC disabling to generic cpuid disable bitmap
    
    Fix from: Ian Campbell <ijc@hellion.org.uk>
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index c66991a04a8a..dfc9563fc4f0 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -446,10 +446,6 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	 * we do "generic changes."
 	 */
 
-	/* TSC disabled? */
-	if ( tsc_disable )
-		clear_bit(X86_FEATURE_TSC, c->x86_capability);
-
 	/* If the model name is still unset, do table lookup. */
 	if ( !c->x86_model_id[0] ) {
 		char *p;
@@ -650,11 +646,6 @@ void __cpuinit cpu_init(void)
 
 	if (cpu_has_vme || cpu_has_tsc || cpu_has_de)
 		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
-	if (tsc_disable && cpu_has_tsc) {
-		printk(KERN_NOTICE "Disabling TSC...\n");
-		/**** FIX-HPA: DOES THIS REALLY BELONG HERE? ****/
-		clear_bit(X86_FEATURE_TSC, boot_cpu_data.x86_capability);
-	}
 
 	load_idt(&idt_descr);
 	switch_to_new_gdt();

commit 8424950b5e85543a494b5d940bb2f5f9f16f56a9
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:20 2008 +0100

    x86: don't disable RDTSC in userland for 32bit notsc
    
    Modern 32bit userland doesn't even boot when the TSC is disabled
    because ld.so tends to contain RDTSCs.  So make notsc only effective for the
    kernel, similar to 64bit.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 5f9c8e3a3e0f..c66991a04a8a 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -654,7 +654,6 @@ void __cpuinit cpu_init(void)
 		printk(KERN_NOTICE "Disabling TSC...\n");
 		/**** FIX-HPA: DOES THIS REALLY BELONG HERE? ****/
 		clear_bit(X86_FEATURE_TSC, boot_cpu_data.x86_capability);
-		set_in_cr4(X86_CR4_TSD);
 	}
 
 	load_idt(&idt_descr);

commit 135302577bb964ebf23376e2d991405ef4ff0457
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:20 2008 +0100

    x86: convert some existing cpuid disable options to new generic bitmap
    
    This convers nofxsr, mem=nopentium and nosep to use the new
    generic cpuid disable bitmap instead of using own variables.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index f0f29ddf33a2..5f9c8e3a3e0f 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -60,14 +60,10 @@ EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 __u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
 
 static int cachesize_override __cpuinitdata = -1;
-static int disable_x86_fxsr __cpuinitdata;
 static int disable_x86_serial_nr __cpuinitdata = 1;
-static int disable_x86_sep __cpuinitdata;
 
 struct cpu_dev * cpu_devs[X86_VENDOR_NUM] = {};
 
-extern int disable_pse;
-
 static void __cpuinit default_init(struct cpuinfo_x86 * c)
 {
 	/* Not much we can do here... */
@@ -216,16 +212,8 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c, int early)
 
 static int __init x86_fxsr_setup(char * s)
 {
-	/* Tell all the other CPUs to not use it... */
-	disable_x86_fxsr = 1;
-
-	/*
-	 * ... and clear the bits early in the boot_cpu_data
-	 * so that the bootup process doesn't try to do this
-	 * either.
-	 */
-	clear_bit(X86_FEATURE_FXSR, boot_cpu_data.x86_capability);
-	clear_bit(X86_FEATURE_XMM, boot_cpu_data.x86_capability);
+	setup_clear_cpu_cap(X86_FEATURE_FXSR);
+	setup_clear_cpu_cap(X86_FEATURE_XMM);
 	return 1;
 }
 __setup("nofxsr", x86_fxsr_setup);
@@ -233,7 +221,7 @@ __setup("nofxsr", x86_fxsr_setup);
 
 static int __init x86_sep_setup(char * s)
 {
-	disable_x86_sep = 1;
+	setup_clear_cpu_cap(X86_FEATURE_SEP);
 	return 1;
 }
 __setup("nosep", x86_sep_setup);
@@ -462,19 +450,6 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 	if ( tsc_disable )
 		clear_bit(X86_FEATURE_TSC, c->x86_capability);
 
-	/* FXSR disabled? */
-	if (disable_x86_fxsr) {
-		clear_bit(X86_FEATURE_FXSR, c->x86_capability);
-		clear_bit(X86_FEATURE_XMM, c->x86_capability);
-	}
-
-	/* SEP disabled? */
-	if (disable_x86_sep)
-		clear_bit(X86_FEATURE_SEP, c->x86_capability);
-
-	if (disable_pse)
-		clear_bit(X86_FEATURE_PSE, c->x86_capability);
-
 	/* If the model name is still unset, do table lookup. */
 	if ( !c->x86_model_id[0] ) {
 		char *p;
@@ -629,8 +604,7 @@ void __init early_cpu_init(void)
 	/* pse is not compatible with on-the-fly unmapping,
 	 * disable it even if the cpus claim to support it.
 	 */
-	clear_bit(X86_FEATURE_PSE, boot_cpu_data.x86_capability);
-	disable_pse = 1;
+	setup_clear_cpu_cap(X86_FEATURE_PSE);
 #endif
 }
 

commit 7d851c8d3db0f79b92c8b14361779ede8acd2488
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:20 2008 +0100

    x86: add framework to disable CPUID bits on the command line
    
    There are already various options to disable specific cpuid bits
    on the command line. They all use their own variable. Add a generic
    mask to make this easier in the future.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4bd326d0322c..f0f29ddf33a2 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -57,6 +57,8 @@ DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
+__u32 cleared_cpu_caps[NCAPINTS] __cpuinitdata;
+
 static int cachesize_override __cpuinitdata = -1;
 static int disable_x86_fxsr __cpuinitdata;
 static int disable_x86_serial_nr __cpuinitdata = 1;
@@ -497,6 +499,10 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
 	}
 
+	/* Clear all flags overriden by options */
+	for (i = 0; i < NCAPINTS; i++)
+		c->x86_capability[i] ^= cleared_cpu_caps[i];
+
 	/* Init Machine Check Exception if available. */
 	mcheck_init(c);
 

commit 30d432dfab2bcfd021d352e2058fae6b9405caeb
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:16 2008 +0100

    x86: move MWAIT idle check to generic CPU initialization on 32-bit
    
    Previously it was only run for Intel CPUs, but AMD Fam10h implements MWAIT too.
    
    This matches 64bit behaviour.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ed05c7a0ca9b..4bd326d0322c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -499,6 +499,8 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 	/* Init Machine Check Exception if available. */
 	mcheck_init(c);
+
+	select_idle_routine(c);
 }
 
 void __init identify_boot_cpu(void)

commit 3898534d85e2da8cedab1ceb6ab9328c61f2c1ce
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:32:49 2008 +0100

    x86: remove CPU capabitilites printks on 32-bit
    
    I don't know of any case where they have been useful and they look ugly.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index dbb9142a8241..ed05c7a0ca9b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -432,20 +432,9 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 	generic_identify(c);
 
-	printk(KERN_DEBUG "CPU: After generic identify, caps:");
-	for (i = 0; i < NCAPINTS; i++)
-		printk(" %08x", c->x86_capability[i]);
-	printk("\n");
-
-	if (this_cpu->c_identify) {
+	if (this_cpu->c_identify)
 		this_cpu->c_identify(c);
 
-		printk(KERN_DEBUG "CPU: After vendor identify, caps:");
-		for (i = 0; i < NCAPINTS; i++)
-			printk(" %08x", c->x86_capability[i]);
-		printk("\n");
-	}
-
 	/*
 	 * Vendor-specific initialization.  In this section we
 	 * canonicalize the feature flags, meaning if there are
@@ -496,13 +485,6 @@ void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 				c->x86, c->x86_model);
 	}
 
-	/* Now the feature flags better reflect actual CPU features! */
-
-	printk(KERN_DEBUG "CPU: After all inits, caps:");
-	for (i = 0; i < NCAPINTS; i++)
-		printk(" %08x", c->x86_capability[i]);
-	printk("\n");
-
 	/*
 	 * On SMP, boot_cpu_data holds the common feature set between
 	 * all CPUs; so make sure that we indicate which features are

commit 2b16a2353814a513cdb5c5c739b76a19d7ea39ce
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:32:40 2008 +0100

    x86: move X86_FEATURE_CONSTANT_TSC into early cpu feature detection
    
    Need this in the next patch in time_init and that happens early.
    
    This includes a minor fix on i386 where early_intel_workarounds()
    [which is now called early_init_intel] really executes early as
    the comments say.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e48832a6c2a9..dbb9142a8241 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -307,6 +307,15 @@ static void __init early_cpu_detect(void)
 	cpu_detect(c);
 
 	get_cpu_vendor(c, 1);
+
+	switch (c->x86_vendor) {
+	case X86_VENDOR_AMD:
+		early_init_amd(c);
+		break;
+	case X86_VENDOR_INTEL:
+		early_init_intel(c);
+		break;
+	}
 }
 
 static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
@@ -364,8 +373,6 @@ static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
 		init_scattered_cpuid_features(c);
 	}
 
-	early_intel_workaround(c);
-
 #ifdef CONFIG_X86_HT
 	c->phys_proc_id = (cpuid_ebx(1) >> 24) & 0xff;
 #endif

commit 1a53905adddf6cc6d795bd7e988c60a19773f72e
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:39 2008 +0100

    x86: move definitions to processor.h
    
    This patch moves definitions that are present in only one of the files
    (between processor_32.h and processor_64.h), to processor.h. They're mostly
    structures and function definitions.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ecd13c0e8542..e48832a6c2a9 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -399,7 +399,7 @@ __setup("serialnumber", x86_serial_nr_setup);
 /*
  * This does the hard work of actually picking apart the CPU stuff...
  */
-static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
+void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 {
 	int i;
 

commit 5300db887e251276eaab2801af297acf4b53b9eb
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:33 2008 +0100

    x86: unify x86_cpuinfo struct.
    
    x86_cpuinfo is one more to the family of "not fundamentally different"
    structs. It's unified in processor.h, with very specific fields enclosed
    around ifdefs.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 69507ae8a65b..ecd13c0e8542 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -427,7 +427,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 	printk(KERN_DEBUG "CPU: After generic identify, caps:");
 	for (i = 0; i < NCAPINTS; i++)
-		printk(" %08lx", c->x86_capability[i]);
+		printk(" %08x", c->x86_capability[i]);
 	printk("\n");
 
 	if (this_cpu->c_identify) {
@@ -435,7 +435,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 		printk(KERN_DEBUG "CPU: After vendor identify, caps:");
 		for (i = 0; i < NCAPINTS; i++)
-			printk(" %08lx", c->x86_capability[i]);
+			printk(" %08x", c->x86_capability[i]);
 		printk("\n");
 	}
 
@@ -493,7 +493,7 @@ static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
 
 	printk(KERN_DEBUG "CPU: After all inits, caps:");
 	for (i = 0; i < NCAPINTS; i++)
-		printk(" %08lx", c->x86_capability[i]);
+		printk(" %08x", c->x86_capability[i]);
 	printk("\n");
 
 	/*

commit 6b68f01baa810e9f63fbf39e9d5c3ef1d94a966f
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:12 2008 +0100

    x86: unify struct desc_ptr
    
    This patch unifies struct desc_ptr between i386 and x86_64.
    They can be expressed in the exact same way in C code, only
    having to change the name of one of them. As Xgt_desc_struct
    is ugly and big, this is the one that goes away.
    
    There's also a padding field in i386, but it is not really
    needed in the C structure definition.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 4f9e31912a25..69507ae8a65b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -649,7 +649,7 @@ struct pt_regs * __devinit idle_regs(struct pt_regs *regs)
  * it's on the real one. */
 void switch_to_new_gdt(void)
 {
-	struct Xgt_desc_struct gdt_descr;
+	struct desc_ptr gdt_descr;
 
 	gdt_descr.address = (long)get_cpu_gdt_table(smp_processor_id());
 	gdt_descr.size = GDT_SIZE - 1;

commit 6842ef0e85a9cc1295f3ef933a230f863b01eb0f
Author: Glauber de Oliveira Costa <gcosta@redhat.com>
Date:   Wed Jan 30 13:31:11 2008 +0100

    x86: unify desc_struct
    
    This patch aims to make the access of struct desc_struct variables
    equal across architectures. In this patch, I unify the i386 and x86_64
    versions under an anonymous union, keeping the way they are accessed
    untouched (a and b for 32-bit code, individual bit-fields for 64-bit).
    
    This solution is not beautiful, but will allow us to integrate common
    code that differed by the way descriptors were used. This is to be viewed
    incrementally. There's simply too much code to be fixed at once.
    
    In the future, goal is to set up in a single way of acessing
    the desc_struct fields.
    
    Signed-off-by: Glauber de Oliveira Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 235cd615b89d..4f9e31912a25 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -22,31 +22,38 @@
 #include "cpu.h"
 
 DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
-	[GDT_ENTRY_KERNEL_CS] = { 0x0000ffff, 0x00cf9a00 },
-	[GDT_ENTRY_KERNEL_DS] = { 0x0000ffff, 0x00cf9200 },
-	[GDT_ENTRY_DEFAULT_USER_CS] = { 0x0000ffff, 0x00cffa00 },
-	[GDT_ENTRY_DEFAULT_USER_DS] = { 0x0000ffff, 0x00cff200 },
+	[GDT_ENTRY_KERNEL_CS] = { { { 0x0000ffff, 0x00cf9a00 } } },
+	[GDT_ENTRY_KERNEL_DS] = { { { 0x0000ffff, 0x00cf9200 } } },
+	[GDT_ENTRY_DEFAULT_USER_CS] = { { { 0x0000ffff, 0x00cffa00 } } },
+	[GDT_ENTRY_DEFAULT_USER_DS] = { { { 0x0000ffff, 0x00cff200 } } },
 	/*
 	 * Segments used for calling PnP BIOS have byte granularity.
 	 * They code segments and data segments have fixed 64k limits,
 	 * the transfer segment sizes are set at run time.
 	 */
-	[GDT_ENTRY_PNPBIOS_CS32] = { 0x0000ffff, 0x00409a00 },/* 32-bit code */
-	[GDT_ENTRY_PNPBIOS_CS16] = { 0x0000ffff, 0x00009a00 },/* 16-bit code */
-	[GDT_ENTRY_PNPBIOS_DS] = { 0x0000ffff, 0x00009200 }, /* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_TS1] = { 0x00000000, 0x00009200 },/* 16-bit data */
-	[GDT_ENTRY_PNPBIOS_TS2] = { 0x00000000, 0x00009200 },/* 16-bit data */
+	/* 32-bit code */
+	[GDT_ENTRY_PNPBIOS_CS32] = { { { 0x0000ffff, 0x00409a00 } } },
+	/* 16-bit code */
+	[GDT_ENTRY_PNPBIOS_CS16] = { { { 0x0000ffff, 0x00009a00 } } },
+	/* 16-bit data */
+	[GDT_ENTRY_PNPBIOS_DS] = { { { 0x0000ffff, 0x00009200 } } },
+	/* 16-bit data */
+	[GDT_ENTRY_PNPBIOS_TS1] = { { { 0x00000000, 0x00009200 } } },
+	/* 16-bit data */
+	[GDT_ENTRY_PNPBIOS_TS2] = { { { 0x00000000, 0x00009200 } } },
 	/*
 	 * The APM segments have byte granularity and their bases
 	 * are set at run time.  All have 64k limits.
 	 */
-	[GDT_ENTRY_APMBIOS_BASE] = { 0x0000ffff, 0x00409a00 },/* 32-bit code */
+	/* 32-bit code */
+	[GDT_ENTRY_APMBIOS_BASE] = { { { 0x0000ffff, 0x00409a00 } } },
 	/* 16-bit code */
-	[GDT_ENTRY_APMBIOS_BASE+1] = { 0x0000ffff, 0x00009a00 },
-	[GDT_ENTRY_APMBIOS_BASE+2] = { 0x0000ffff, 0x00409200 }, /* data */
+	[GDT_ENTRY_APMBIOS_BASE+1] = { { { 0x0000ffff, 0x00009a00 } } },
+	/* data */
+	[GDT_ENTRY_APMBIOS_BASE+2] = { { { 0x0000ffff, 0x00409200 } } },
 
-	[GDT_ENTRY_ESPFIX_SS] = { 0x00000000, 0x00c09200 },
-	[GDT_ENTRY_PERCPU] = { 0x00000000, 0x00000000 },
+	[GDT_ENTRY_ESPFIX_SS] = { { { 0x00000000, 0x00c09200 } } },
+	[GDT_ENTRY_PERCPU] = { { { 0x00000000, 0x00000000 } } },
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 

commit faca62273b602ab482fb7d3d940dbf41ef08b00e
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Jan 30 13:31:02 2008 +0100

    x86: use generic register name in the thread and tss structures
    
    This changes size-specific register names (eip/rip, esp/rsp, etc.) to
    generic names in the thread and tss structures.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 5db2a163bf4b..235cd615b89d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -691,7 +691,7 @@ void __cpuinit cpu_init(void)
 		BUG();
 	enter_lazy_tlb(&init_mm, curr);
 
-	load_esp0(t, thread);
+	load_sp0(t, thread);
 	set_tss_desc(cpu,t);
 	load_TR_desc();
 	load_LDT(&init_mm.context);

commit 65ea5b0349903585bfed9720fa06f5edb4f1cd25
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Wed Jan 30 13:30:56 2008 +0100

    x86: rename the struct pt_regs members for 32/64-bit consistency
    
    We have a lot of code which differs only by the naming of specific
    members of structures that contain registers.  In order to enable
    additional unifications, this patch drops the e- or r- size prefix
    from the register names in struct pt_regs, and drops the x- prefixes
    for segment registers on the 32-bit side.
    
    This patch also performs the equivalent renames in some additional
    places that might be candidates for unification in the future.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e2fcf2051bdb..5db2a163bf4b 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -634,7 +634,7 @@ void __init early_cpu_init(void)
 struct pt_regs * __devinit idle_regs(struct pt_regs *regs)
 {
 	memset(regs, 0, sizeof(struct pt_regs));
-	regs->xfs = __KERNEL_PERCPU;
+	regs->fs = __KERNEL_PERCPU;
 	return regs;
 }
 

commit 27b46d7661dc720224813eb4f452e424f1bf3a9a
Author: Simon Arlott <simon@fire.lp0.eu>
Date:   Sat Oct 20 01:13:56 2007 +0200

    spelling fixes: arch/i386/
    
    Spelling fixes in arch/i386/.
    
    Signed-off-by: Simon Arlott <simon@fire.lp0.eu>
    Signed-off-by: Adrian Bunk <bunk@kernel.org>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index d506201d397c..e2fcf2051bdb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -207,7 +207,7 @@ static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c, int early)
 
 static int __init x86_fxsr_setup(char * s)
 {
-	/* Tell all the other CPU's to not use it... */
+	/* Tell all the other CPUs to not use it... */
 	disable_x86_fxsr = 1;
 
 	/*

commit f7627e2513987bb5d4e8cb13c4e0a478352141ac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:58 2007 +0200

    i386: move kernel/cpu
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
new file mode 100644
index 000000000000..d506201d397c
--- /dev/null
+++ b/arch/x86/kernel/cpu/common.c
@@ -0,0 +1,733 @@
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/module.h>
+#include <linux/percpu.h>
+#include <linux/bootmem.h>
+#include <asm/semaphore.h>
+#include <asm/processor.h>
+#include <asm/i387.h>
+#include <asm/msr.h>
+#include <asm/io.h>
+#include <asm/mmu_context.h>
+#include <asm/mtrr.h>
+#include <asm/mce.h>
+#ifdef CONFIG_X86_LOCAL_APIC
+#include <asm/mpspec.h>
+#include <asm/apic.h>
+#include <mach_apic.h>
+#endif
+
+#include "cpu.h"
+
+DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
+	[GDT_ENTRY_KERNEL_CS] = { 0x0000ffff, 0x00cf9a00 },
+	[GDT_ENTRY_KERNEL_DS] = { 0x0000ffff, 0x00cf9200 },
+	[GDT_ENTRY_DEFAULT_USER_CS] = { 0x0000ffff, 0x00cffa00 },
+	[GDT_ENTRY_DEFAULT_USER_DS] = { 0x0000ffff, 0x00cff200 },
+	/*
+	 * Segments used for calling PnP BIOS have byte granularity.
+	 * They code segments and data segments have fixed 64k limits,
+	 * the transfer segment sizes are set at run time.
+	 */
+	[GDT_ENTRY_PNPBIOS_CS32] = { 0x0000ffff, 0x00409a00 },/* 32-bit code */
+	[GDT_ENTRY_PNPBIOS_CS16] = { 0x0000ffff, 0x00009a00 },/* 16-bit code */
+	[GDT_ENTRY_PNPBIOS_DS] = { 0x0000ffff, 0x00009200 }, /* 16-bit data */
+	[GDT_ENTRY_PNPBIOS_TS1] = { 0x00000000, 0x00009200 },/* 16-bit data */
+	[GDT_ENTRY_PNPBIOS_TS2] = { 0x00000000, 0x00009200 },/* 16-bit data */
+	/*
+	 * The APM segments have byte granularity and their bases
+	 * are set at run time.  All have 64k limits.
+	 */
+	[GDT_ENTRY_APMBIOS_BASE] = { 0x0000ffff, 0x00409a00 },/* 32-bit code */
+	/* 16-bit code */
+	[GDT_ENTRY_APMBIOS_BASE+1] = { 0x0000ffff, 0x00009a00 },
+	[GDT_ENTRY_APMBIOS_BASE+2] = { 0x0000ffff, 0x00409200 }, /* data */
+
+	[GDT_ENTRY_ESPFIX_SS] = { 0x00000000, 0x00c09200 },
+	[GDT_ENTRY_PERCPU] = { 0x00000000, 0x00000000 },
+} };
+EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
+
+static int cachesize_override __cpuinitdata = -1;
+static int disable_x86_fxsr __cpuinitdata;
+static int disable_x86_serial_nr __cpuinitdata = 1;
+static int disable_x86_sep __cpuinitdata;
+
+struct cpu_dev * cpu_devs[X86_VENDOR_NUM] = {};
+
+extern int disable_pse;
+
+static void __cpuinit default_init(struct cpuinfo_x86 * c)
+{
+	/* Not much we can do here... */
+	/* Check if at least it has cpuid */
+	if (c->cpuid_level == -1) {
+		/* No cpuid. It must be an ancient CPU */
+		if (c->x86 == 4)
+			strcpy(c->x86_model_id, "486");
+		else if (c->x86 == 3)
+			strcpy(c->x86_model_id, "386");
+	}
+}
+
+static struct cpu_dev __cpuinitdata default_cpu = {
+	.c_init	= default_init,
+	.c_vendor = "Unknown",
+};
+static struct cpu_dev * this_cpu __cpuinitdata = &default_cpu;
+
+static int __init cachesize_setup(char *str)
+{
+	get_option (&str, &cachesize_override);
+	return 1;
+}
+__setup("cachesize=", cachesize_setup);
+
+int __cpuinit get_model_name(struct cpuinfo_x86 *c)
+{
+	unsigned int *v;
+	char *p, *q;
+
+	if (cpuid_eax(0x80000000) < 0x80000004)
+		return 0;
+
+	v = (unsigned int *) c->x86_model_id;
+	cpuid(0x80000002, &v[0], &v[1], &v[2], &v[3]);
+	cpuid(0x80000003, &v[4], &v[5], &v[6], &v[7]);
+	cpuid(0x80000004, &v[8], &v[9], &v[10], &v[11]);
+	c->x86_model_id[48] = 0;
+
+	/* Intel chips right-justify this string for some dumb reason;
+	   undo that brain damage */
+	p = q = &c->x86_model_id[0];
+	while ( *p == ' ' )
+	     p++;
+	if ( p != q ) {
+	     while ( *p )
+		  *q++ = *p++;
+	     while ( q <= &c->x86_model_id[48] )
+		  *q++ = '\0';	/* Zero-pad the rest */
+	}
+
+	return 1;
+}
+
+
+void __cpuinit display_cacheinfo(struct cpuinfo_x86 *c)
+{
+	unsigned int n, dummy, ecx, edx, l2size;
+
+	n = cpuid_eax(0x80000000);
+
+	if (n >= 0x80000005) {
+		cpuid(0x80000005, &dummy, &dummy, &ecx, &edx);
+		printk(KERN_INFO "CPU: L1 I Cache: %dK (%d bytes/line), D cache %dK (%d bytes/line)\n",
+			edx>>24, edx&0xFF, ecx>>24, ecx&0xFF);
+		c->x86_cache_size=(ecx>>24)+(edx>>24);	
+	}
+
+	if (n < 0x80000006)	/* Some chips just has a large L1. */
+		return;
+
+	ecx = cpuid_ecx(0x80000006);
+	l2size = ecx >> 16;
+	
+	/* do processor-specific cache resizing */
+	if (this_cpu->c_size_cache)
+		l2size = this_cpu->c_size_cache(c,l2size);
+
+	/* Allow user to override all this if necessary. */
+	if (cachesize_override != -1)
+		l2size = cachesize_override;
+
+	if ( l2size == 0 )
+		return;		/* Again, no L2 cache is possible */
+
+	c->x86_cache_size = l2size;
+
+	printk(KERN_INFO "CPU: L2 Cache: %dK (%d bytes/line)\n",
+	       l2size, ecx & 0xFF);
+}
+
+/* Naming convention should be: <Name> [(<Codename>)] */
+/* This table only is used unless init_<vendor>() below doesn't set it; */
+/* in particular, if CPUID levels 0x80000002..4 are supported, this isn't used */
+
+/* Look up CPU names by table lookup. */
+static char __cpuinit *table_lookup_model(struct cpuinfo_x86 *c)
+{
+	struct cpu_model_info *info;
+
+	if ( c->x86_model >= 16 )
+		return NULL;	/* Range check */
+
+	if (!this_cpu)
+		return NULL;
+
+	info = this_cpu->c_models;
+
+	while (info && info->family) {
+		if (info->family == c->x86)
+			return info->model_names[c->x86_model];
+		info++;
+	}
+	return NULL;		/* Not found */
+}
+
+
+static void __cpuinit get_cpu_vendor(struct cpuinfo_x86 *c, int early)
+{
+	char *v = c->x86_vendor_id;
+	int i;
+	static int printed;
+
+	for (i = 0; i < X86_VENDOR_NUM; i++) {
+		if (cpu_devs[i]) {
+			if (!strcmp(v,cpu_devs[i]->c_ident[0]) ||
+			    (cpu_devs[i]->c_ident[1] && 
+			     !strcmp(v,cpu_devs[i]->c_ident[1]))) {
+				c->x86_vendor = i;
+				if (!early)
+					this_cpu = cpu_devs[i];
+				return;
+			}
+		}
+	}
+	if (!printed) {
+		printed++;
+		printk(KERN_ERR "CPU: Vendor unknown, using generic init.\n");
+		printk(KERN_ERR "CPU: Your system may be unstable.\n");
+	}
+	c->x86_vendor = X86_VENDOR_UNKNOWN;
+	this_cpu = &default_cpu;
+}
+
+
+static int __init x86_fxsr_setup(char * s)
+{
+	/* Tell all the other CPU's to not use it... */
+	disable_x86_fxsr = 1;
+
+	/*
+	 * ... and clear the bits early in the boot_cpu_data
+	 * so that the bootup process doesn't try to do this
+	 * either.
+	 */
+	clear_bit(X86_FEATURE_FXSR, boot_cpu_data.x86_capability);
+	clear_bit(X86_FEATURE_XMM, boot_cpu_data.x86_capability);
+	return 1;
+}
+__setup("nofxsr", x86_fxsr_setup);
+
+
+static int __init x86_sep_setup(char * s)
+{
+	disable_x86_sep = 1;
+	return 1;
+}
+__setup("nosep", x86_sep_setup);
+
+
+/* Standard macro to see if a specific flag is changeable */
+static inline int flag_is_changeable_p(u32 flag)
+{
+	u32 f1, f2;
+
+	asm("pushfl\n\t"
+	    "pushfl\n\t"
+	    "popl %0\n\t"
+	    "movl %0,%1\n\t"
+	    "xorl %2,%0\n\t"
+	    "pushl %0\n\t"
+	    "popfl\n\t"
+	    "pushfl\n\t"
+	    "popl %0\n\t"
+	    "popfl\n\t"
+	    : "=&r" (f1), "=&r" (f2)
+	    : "ir" (flag));
+
+	return ((f1^f2) & flag) != 0;
+}
+
+
+/* Probe for the CPUID instruction */
+static int __cpuinit have_cpuid_p(void)
+{
+	return flag_is_changeable_p(X86_EFLAGS_ID);
+}
+
+void __init cpu_detect(struct cpuinfo_x86 *c)
+{
+	/* Get vendor name */
+	cpuid(0x00000000, &c->cpuid_level,
+	      (int *)&c->x86_vendor_id[0],
+	      (int *)&c->x86_vendor_id[8],
+	      (int *)&c->x86_vendor_id[4]);
+
+	c->x86 = 4;
+	if (c->cpuid_level >= 0x00000001) {
+		u32 junk, tfms, cap0, misc;
+		cpuid(0x00000001, &tfms, &misc, &junk, &cap0);
+		c->x86 = (tfms >> 8) & 15;
+		c->x86_model = (tfms >> 4) & 15;
+		if (c->x86 == 0xf)
+			c->x86 += (tfms >> 20) & 0xff;
+		if (c->x86 >= 0x6)
+			c->x86_model += ((tfms >> 16) & 0xF) << 4;
+		c->x86_mask = tfms & 15;
+		if (cap0 & (1<<19))
+			c->x86_cache_alignment = ((misc >> 8) & 0xff) * 8;
+	}
+}
+
+/* Do minimum CPU detection early.
+   Fields really needed: vendor, cpuid_level, family, model, mask, cache alignment.
+   The others are not touched to avoid unwanted side effects.
+
+   WARNING: this function is only called on the BP.  Don't add code here
+   that is supposed to run on all CPUs. */
+static void __init early_cpu_detect(void)
+{
+	struct cpuinfo_x86 *c = &boot_cpu_data;
+
+	c->x86_cache_alignment = 32;
+
+	if (!have_cpuid_p())
+		return;
+
+	cpu_detect(c);
+
+	get_cpu_vendor(c, 1);
+}
+
+static void __cpuinit generic_identify(struct cpuinfo_x86 * c)
+{
+	u32 tfms, xlvl;
+	int ebx;
+
+	if (have_cpuid_p()) {
+		/* Get vendor name */
+		cpuid(0x00000000, &c->cpuid_level,
+		      (int *)&c->x86_vendor_id[0],
+		      (int *)&c->x86_vendor_id[8],
+		      (int *)&c->x86_vendor_id[4]);
+		
+		get_cpu_vendor(c, 0);
+		/* Initialize the standard set of capabilities */
+		/* Note that the vendor-specific code below might override */
+	
+		/* Intel-defined flags: level 0x00000001 */
+		if ( c->cpuid_level >= 0x00000001 ) {
+			u32 capability, excap;
+			cpuid(0x00000001, &tfms, &ebx, &excap, &capability);
+			c->x86_capability[0] = capability;
+			c->x86_capability[4] = excap;
+			c->x86 = (tfms >> 8) & 15;
+			c->x86_model = (tfms >> 4) & 15;
+			if (c->x86 == 0xf)
+				c->x86 += (tfms >> 20) & 0xff;
+			if (c->x86 >= 0x6)
+				c->x86_model += ((tfms >> 16) & 0xF) << 4;
+			c->x86_mask = tfms & 15;
+#ifdef CONFIG_X86_HT
+			c->apicid = phys_pkg_id((ebx >> 24) & 0xFF, 0);
+#else
+			c->apicid = (ebx >> 24) & 0xFF;
+#endif
+			if (c->x86_capability[0] & (1<<19))
+				c->x86_clflush_size = ((ebx >> 8) & 0xff) * 8;
+		} else {
+			/* Have CPUID level 0 only - unheard of */
+			c->x86 = 4;
+		}
+
+		/* AMD-defined flags: level 0x80000001 */
+		xlvl = cpuid_eax(0x80000000);
+		if ( (xlvl & 0xffff0000) == 0x80000000 ) {
+			if ( xlvl >= 0x80000001 ) {
+				c->x86_capability[1] = cpuid_edx(0x80000001);
+				c->x86_capability[6] = cpuid_ecx(0x80000001);
+			}
+			if ( xlvl >= 0x80000004 )
+				get_model_name(c); /* Default name */
+		}
+
+		init_scattered_cpuid_features(c);
+	}
+
+	early_intel_workaround(c);
+
+#ifdef CONFIG_X86_HT
+	c->phys_proc_id = (cpuid_ebx(1) >> 24) & 0xff;
+#endif
+}
+
+static void __cpuinit squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
+{
+	if (cpu_has(c, X86_FEATURE_PN) && disable_x86_serial_nr ) {
+		/* Disable processor serial number */
+		unsigned long lo,hi;
+		rdmsr(MSR_IA32_BBL_CR_CTL,lo,hi);
+		lo |= 0x200000;
+		wrmsr(MSR_IA32_BBL_CR_CTL,lo,hi);
+		printk(KERN_NOTICE "CPU serial number disabled.\n");
+		clear_bit(X86_FEATURE_PN, c->x86_capability);
+
+		/* Disabling the serial number may affect the cpuid level */
+		c->cpuid_level = cpuid_eax(0);
+	}
+}
+
+static int __init x86_serial_nr_setup(char *s)
+{
+	disable_x86_serial_nr = 0;
+	return 1;
+}
+__setup("serialnumber", x86_serial_nr_setup);
+
+
+
+/*
+ * This does the hard work of actually picking apart the CPU stuff...
+ */
+static void __cpuinit identify_cpu(struct cpuinfo_x86 *c)
+{
+	int i;
+
+	c->loops_per_jiffy = loops_per_jiffy;
+	c->x86_cache_size = -1;
+	c->x86_vendor = X86_VENDOR_UNKNOWN;
+	c->cpuid_level = -1;	/* CPUID not detected */
+	c->x86_model = c->x86_mask = 0;	/* So far unknown... */
+	c->x86_vendor_id[0] = '\0'; /* Unset */
+	c->x86_model_id[0] = '\0';  /* Unset */
+	c->x86_max_cores = 1;
+	c->x86_clflush_size = 32;
+	memset(&c->x86_capability, 0, sizeof c->x86_capability);
+
+	if (!have_cpuid_p()) {
+		/* First of all, decide if this is a 486 or higher */
+		/* It's a 486 if we can modify the AC flag */
+		if ( flag_is_changeable_p(X86_EFLAGS_AC) )
+			c->x86 = 4;
+		else
+			c->x86 = 3;
+	}
+
+	generic_identify(c);
+
+	printk(KERN_DEBUG "CPU: After generic identify, caps:");
+	for (i = 0; i < NCAPINTS; i++)
+		printk(" %08lx", c->x86_capability[i]);
+	printk("\n");
+
+	if (this_cpu->c_identify) {
+		this_cpu->c_identify(c);
+
+		printk(KERN_DEBUG "CPU: After vendor identify, caps:");
+		for (i = 0; i < NCAPINTS; i++)
+			printk(" %08lx", c->x86_capability[i]);
+		printk("\n");
+	}
+
+	/*
+	 * Vendor-specific initialization.  In this section we
+	 * canonicalize the feature flags, meaning if there are
+	 * features a certain CPU supports which CPUID doesn't
+	 * tell us, CPUID claiming incorrect flags, or other bugs,
+	 * we handle them here.
+	 *
+	 * At the end of this section, c->x86_capability better
+	 * indicate the features this CPU genuinely supports!
+	 */
+	if (this_cpu->c_init)
+		this_cpu->c_init(c);
+
+	/* Disable the PN if appropriate */
+	squash_the_stupid_serial_number(c);
+
+	/*
+	 * The vendor-specific functions might have changed features.  Now
+	 * we do "generic changes."
+	 */
+
+	/* TSC disabled? */
+	if ( tsc_disable )
+		clear_bit(X86_FEATURE_TSC, c->x86_capability);
+
+	/* FXSR disabled? */
+	if (disable_x86_fxsr) {
+		clear_bit(X86_FEATURE_FXSR, c->x86_capability);
+		clear_bit(X86_FEATURE_XMM, c->x86_capability);
+	}
+
+	/* SEP disabled? */
+	if (disable_x86_sep)
+		clear_bit(X86_FEATURE_SEP, c->x86_capability);
+
+	if (disable_pse)
+		clear_bit(X86_FEATURE_PSE, c->x86_capability);
+
+	/* If the model name is still unset, do table lookup. */
+	if ( !c->x86_model_id[0] ) {
+		char *p;
+		p = table_lookup_model(c);
+		if ( p )
+			strcpy(c->x86_model_id, p);
+		else
+			/* Last resort... */
+			sprintf(c->x86_model_id, "%02x/%02x",
+				c->x86, c->x86_model);
+	}
+
+	/* Now the feature flags better reflect actual CPU features! */
+
+	printk(KERN_DEBUG "CPU: After all inits, caps:");
+	for (i = 0; i < NCAPINTS; i++)
+		printk(" %08lx", c->x86_capability[i]);
+	printk("\n");
+
+	/*
+	 * On SMP, boot_cpu_data holds the common feature set between
+	 * all CPUs; so make sure that we indicate which features are
+	 * common between the CPUs.  The first time this routine gets
+	 * executed, c == &boot_cpu_data.
+	 */
+	if ( c != &boot_cpu_data ) {
+		/* AND the already accumulated flags with these */
+		for ( i = 0 ; i < NCAPINTS ; i++ )
+			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
+	}
+
+	/* Init Machine Check Exception if available. */
+	mcheck_init(c);
+}
+
+void __init identify_boot_cpu(void)
+{
+	identify_cpu(&boot_cpu_data);
+	sysenter_setup();
+	enable_sep_cpu();
+	mtrr_bp_init();
+}
+
+void __cpuinit identify_secondary_cpu(struct cpuinfo_x86 *c)
+{
+	BUG_ON(c == &boot_cpu_data);
+	identify_cpu(c);
+	enable_sep_cpu();
+	mtrr_ap_init();
+}
+
+#ifdef CONFIG_X86_HT
+void __cpuinit detect_ht(struct cpuinfo_x86 *c)
+{
+	u32 	eax, ebx, ecx, edx;
+	int 	index_msb, core_bits;
+
+	cpuid(1, &eax, &ebx, &ecx, &edx);
+
+	if (!cpu_has(c, X86_FEATURE_HT) || cpu_has(c, X86_FEATURE_CMP_LEGACY))
+		return;
+
+	smp_num_siblings = (ebx & 0xff0000) >> 16;
+
+	if (smp_num_siblings == 1) {
+		printk(KERN_INFO  "CPU: Hyper-Threading is disabled\n");
+	} else if (smp_num_siblings > 1 ) {
+
+		if (smp_num_siblings > NR_CPUS) {
+			printk(KERN_WARNING "CPU: Unsupported number of the "
+					"siblings %d", smp_num_siblings);
+			smp_num_siblings = 1;
+			return;
+		}
+
+		index_msb = get_count_order(smp_num_siblings);
+		c->phys_proc_id = phys_pkg_id((ebx >> 24) & 0xFF, index_msb);
+
+		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
+		       c->phys_proc_id);
+
+		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
+
+		index_msb = get_count_order(smp_num_siblings) ;
+
+		core_bits = get_count_order(c->x86_max_cores);
+
+		c->cpu_core_id = phys_pkg_id((ebx >> 24) & 0xFF, index_msb) &
+					       ((1 << core_bits) - 1);
+
+		if (c->x86_max_cores > 1)
+			printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
+			       c->cpu_core_id);
+	}
+}
+#endif
+
+void __cpuinit print_cpu_info(struct cpuinfo_x86 *c)
+{
+	char *vendor = NULL;
+
+	if (c->x86_vendor < X86_VENDOR_NUM)
+		vendor = this_cpu->c_vendor;
+	else if (c->cpuid_level >= 0)
+		vendor = c->x86_vendor_id;
+
+	if (vendor && strncmp(c->x86_model_id, vendor, strlen(vendor)))
+		printk("%s ", vendor);
+
+	if (!c->x86_model_id[0])
+		printk("%d86", c->x86);
+	else
+		printk("%s", c->x86_model_id);
+
+	if (c->x86_mask || c->cpuid_level >= 0) 
+		printk(" stepping %02x\n", c->x86_mask);
+	else
+		printk("\n");
+}
+
+cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
+
+/* This is hacky. :)
+ * We're emulating future behavior.
+ * In the future, the cpu-specific init functions will be called implicitly
+ * via the magic of initcalls.
+ * They will insert themselves into the cpu_devs structure.
+ * Then, when cpu_init() is called, we can just iterate over that array.
+ */
+
+extern int intel_cpu_init(void);
+extern int cyrix_init_cpu(void);
+extern int nsc_init_cpu(void);
+extern int amd_init_cpu(void);
+extern int centaur_init_cpu(void);
+extern int transmeta_init_cpu(void);
+extern int nexgen_init_cpu(void);
+extern int umc_init_cpu(void);
+
+void __init early_cpu_init(void)
+{
+	intel_cpu_init();
+	cyrix_init_cpu();
+	nsc_init_cpu();
+	amd_init_cpu();
+	centaur_init_cpu();
+	transmeta_init_cpu();
+	nexgen_init_cpu();
+	umc_init_cpu();
+	early_cpu_detect();
+
+#ifdef CONFIG_DEBUG_PAGEALLOC
+	/* pse is not compatible with on-the-fly unmapping,
+	 * disable it even if the cpus claim to support it.
+	 */
+	clear_bit(X86_FEATURE_PSE, boot_cpu_data.x86_capability);
+	disable_pse = 1;
+#endif
+}
+
+/* Make sure %fs is initialized properly in idle threads */
+struct pt_regs * __devinit idle_regs(struct pt_regs *regs)
+{
+	memset(regs, 0, sizeof(struct pt_regs));
+	regs->xfs = __KERNEL_PERCPU;
+	return regs;
+}
+
+/* Current gdt points %fs at the "master" per-cpu area: after this,
+ * it's on the real one. */
+void switch_to_new_gdt(void)
+{
+	struct Xgt_desc_struct gdt_descr;
+
+	gdt_descr.address = (long)get_cpu_gdt_table(smp_processor_id());
+	gdt_descr.size = GDT_SIZE - 1;
+	load_gdt(&gdt_descr);
+	asm("mov %0, %%fs" : : "r" (__KERNEL_PERCPU) : "memory");
+}
+
+/*
+ * cpu_init() initializes state that is per-CPU. Some data is already
+ * initialized (naturally) in the bootstrap process, such as the GDT
+ * and IDT. We reload them nevertheless, this function acts as a
+ * 'CPU state barrier', nothing should get across.
+ */
+void __cpuinit cpu_init(void)
+{
+	int cpu = smp_processor_id();
+	struct task_struct *curr = current;
+	struct tss_struct * t = &per_cpu(init_tss, cpu);
+	struct thread_struct *thread = &curr->thread;
+
+	if (cpu_test_and_set(cpu, cpu_initialized)) {
+		printk(KERN_WARNING "CPU#%d already initialized!\n", cpu);
+		for (;;) local_irq_enable();
+	}
+
+	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
+
+	if (cpu_has_vme || cpu_has_tsc || cpu_has_de)
+		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+	if (tsc_disable && cpu_has_tsc) {
+		printk(KERN_NOTICE "Disabling TSC...\n");
+		/**** FIX-HPA: DOES THIS REALLY BELONG HERE? ****/
+		clear_bit(X86_FEATURE_TSC, boot_cpu_data.x86_capability);
+		set_in_cr4(X86_CR4_TSD);
+	}
+
+	load_idt(&idt_descr);
+	switch_to_new_gdt();
+
+	/*
+	 * Set up and load the per-CPU TSS and LDT
+	 */
+	atomic_inc(&init_mm.mm_count);
+	curr->active_mm = &init_mm;
+	if (curr->mm)
+		BUG();
+	enter_lazy_tlb(&init_mm, curr);
+
+	load_esp0(t, thread);
+	set_tss_desc(cpu,t);
+	load_TR_desc();
+	load_LDT(&init_mm.context);
+
+#ifdef CONFIG_DOUBLEFAULT
+	/* Set up doublefault TSS pointer in the GDT */
+	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
+#endif
+
+	/* Clear %gs. */
+	asm volatile ("mov %0, %%gs" : : "r" (0));
+
+	/* Clear all 6 debug registers: */
+	set_debugreg(0, 0);
+	set_debugreg(0, 1);
+	set_debugreg(0, 2);
+	set_debugreg(0, 3);
+	set_debugreg(0, 6);
+	set_debugreg(0, 7);
+
+	/*
+	 * Force FPU initialization:
+	 */
+	current_thread_info()->status = 0;
+	clear_used_math();
+	mxcsr_feature_mask_init();
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+void __cpuinit cpu_uninit(void)
+{
+	int cpu = raw_smp_processor_id();
+	cpu_clear(cpu, cpu_initialized);
+
+	/* lazy TLB state */
+	per_cpu(cpu_tlbstate, cpu).state = 0;
+	per_cpu(cpu_tlbstate, cpu).active_mm = &init_mm;
+}
+#endif
