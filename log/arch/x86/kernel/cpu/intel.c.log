commit 009bce1df0bb5eb970b9eb98d963861f7fe353c7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Jun 5 12:26:05 2020 -0700

    x86/split_lock: Don't write MSR_TEST_CTRL on CPUs that aren't whitelisted
    
    Choo! Choo!  All aboard the Split Lock Express, with direct service to
    Wreckage!
    
    Skip split_lock_verify_msr() if the CPU isn't whitelisted as a possible
    SLD-enabled CPU model to avoid writing MSR_TEST_CTRL.  MSR_TEST_CTRL
    exists, and is writable, on many generations of CPUs.  Writing the MSR,
    even with '0', can result in bizarre, undocumented behavior.
    
    This fixes a crash on Haswell when resuming from suspend with a live KVM
    guest.  Because APs use the standard SMP boot flow for resume, they will
    go through split_lock_init() and the subsequent RDMSR/WRMSR sequence,
    which runs even when sld_state==sld_off to ensure SLD is disabled.  On
    Haswell (at least, my Haswell), writing MSR_TEST_CTRL with '0' will
    succeed and _may_ take the SMT _sibling_ out of VMX root mode.
    
    When KVM has an active guest, KVM performs VMXON as part of CPU onlining
    (see kvm_starting_cpu()).  Because SMP boot is serialized, the resulting
    flow is effectively:
    
      on_each_ap_cpu() {
         WRMSR(MSR_TEST_CTRL, 0)
         VMXON
      }
    
    As a result, the WRMSR can disable VMX on a different CPU that has
    already done VMXON.  This ultimately results in a #UD on VMPTRLD when
    KVM regains control and attempt run its vCPUs.
    
    The above voodoo was confirmed by reworking KVM's VMXON flow to write
    MSR_TEST_CTRL prior to VMXON, and to serialize the sequence as above.
    Further verification of the insanity was done by redoing VMXON on all
    APs after the initial WRMSR->VMXON sequence.  The additional VMXON,
    which should VM-Fail, occasionally succeeded, and also eliminated the
    unexpected #UD on VMPTRLD.
    
    The damage done by writing MSR_TEST_CTRL doesn't appear to be limited
    to VMX, e.g. after suspend with an active KVM guest, subsequent reboots
    almost always hang (even when fudging VMXON), a #UD on a random Jcc was
    observed, suspend/resume stability is qualitatively poor, and so on and
    so forth.
    
      kernel BUG at arch/x86/kvm/x86.c:386!
      CPU: 1 PID: 2592 Comm: CPU 6/KVM Tainted: G      D
      Hardware name: ASUS Q87M-E/Q87M-E, BIOS 1102 03/03/2014
      RIP: 0010:kvm_spurious_fault+0xf/0x20
      Call Trace:
       vmx_vcpu_load_vmcs+0x1fb/0x2b0
       vmx_vcpu_load+0x3e/0x160
       kvm_arch_vcpu_load+0x48/0x260
       finish_task_switch+0x140/0x260
       __schedule+0x460/0x720
       _cond_resched+0x2d/0x40
       kvm_arch_vcpu_ioctl_run+0x82e/0x1ca0
       kvm_vcpu_ioctl+0x363/0x5c0
       ksys_ioctl+0x88/0xa0
       __x64_sys_ioctl+0x16/0x20
       do_syscall_64+0x4c/0x170
       entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    Fixes: dbaba47085b0c ("x86/split_lock: Rework the initialization flow of split lock detection")
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20200605192605.7439-1-sean.j.christopherson@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index c25a67a34bd3..0ab48f1cdf84 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -49,6 +49,13 @@ enum split_lock_detect_state {
 static enum split_lock_detect_state sld_state __ro_after_init = sld_off;
 static u64 msr_test_ctrl_cache __ro_after_init;
 
+/*
+ * With a name like MSR_TEST_CTL it should go without saying, but don't touch
+ * MSR_TEST_CTL unless the CPU is one of the whitelisted models.  Writing it
+ * on CPUs that do not support SLD can cause fireworks, even when writing '0'.
+ */
+static bool cpu_model_supports_sld __ro_after_init;
+
 /*
  * Processors which have self-snooping capability can handle conflicting
  * memory type across CPUs by snooping its own cache. However, there exists
@@ -1071,7 +1078,8 @@ static void sld_update_msr(bool on)
 
 static void split_lock_init(void)
 {
-	split_lock_verify_msr(sld_state != sld_off);
+	if (cpu_model_supports_sld)
+		split_lock_verify_msr(sld_state != sld_off);
 }
 
 static void split_lock_warn(unsigned long ip)
@@ -1177,5 +1185,6 @@ void __init cpu_set_core_cap_bits(struct cpuinfo_x86 *c)
 		return;
 	}
 
+	cpu_model_supports_sld = true;
 	split_lock_setup();
 }

commit 6a45a65888393eda692fce0851c40d9f5ce4ef66
Merge: 92ac971219a2 7778d8417b74
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 15:54:31 2020 -0700

    Merge tag 'x86-urgent-2020-06-11' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more x86 updates from Thomas Gleixner:
     "A set of fixes and updates for x86:
    
       - Unbreak paravirt VDSO clocks.
    
         While the VDSO code was moved into lib for sharing a subtle check
         for the validity of paravirt clocks got replaced. While the
         replacement works perfectly fine for bare metal as the update of
         the VDSO clock mode is synchronous, it fails for paravirt clocks
         because the hypervisor can invalidate them asynchronously.
    
         Bring it back as an optional function so it does not inflict this
         on architectures which are free of PV damage.
    
       - Fix the jiffies to jiffies64 mapping on 64bit so it does not
         trigger an ODR violation on newer compilers
    
       - Three fixes for the SSBD and *IB* speculation mitigation maze to
         ensure consistency, not disabling of some *IB* variants wrongly and
         to prevent a rogue cross process shutdown of SSBD. All marked for
         stable.
    
       - Add yet more CPU models to the splitlock detection capable list
         !@#%$!
    
       - Bring the pr_info() back which tells that TSC deadline timer is
         enabled.
    
       - Reboot quirk for MacBook6,1"
    
    * tag 'x86-urgent-2020-06-11' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vdso: Unbreak paravirt VDSO clocks
      lib/vdso: Provide sanity check for cycles (again)
      clocksource: Remove obsolete ifdef
      x86_64: Fix jiffies ODR violation
      x86/speculation: PR_SPEC_FORCE_DISABLE enforcement for indirect branches.
      x86/speculation: Prevent rogue cross-process SSBD shutdown
      x86/speculation: Avoid force-disabling IBPB based on STIBP and enhanced IBRS.
      x86/cpu: Add Sapphire Rapids CPU model number
      x86/split_lock: Add Icelake microserver and Tigerlake CPU models
      x86/apic: Make TSC deadline timer detection message visible
      x86/reboot/quirks: Add MacBook6,1 reboot quirk

commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1c00a443d6b9..63926c94eb5f 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1,5 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/kernel.h>
+#include <linux/pgtable.h>
 
 #include <linux/string.h>
 #include <linux/bitops.h>
@@ -11,7 +12,6 @@
 #include <linux/uaccess.h>
 
 #include <asm/cpufeature.h>
-#include <linux/pgtable.h>
 #include <asm/msr.h>
 #include <asm/bugs.h>
 #include <asm/cpu.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 166d7c355896..1c00a443d6b9 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -11,7 +11,7 @@
 #include <linux/uaccess.h>
 
 #include <asm/cpufeature.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/msr.h>
 #include <asm/bugs.h>
 #include <asm/cpu.h>

commit 429ac8b75a0b1c3478ffd584de8a63075cbe25e7
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu Apr 30 16:46:35 2020 -0700

    x86/split_lock: Add Icelake microserver and Tigerlake CPU models
    
    Icelake microserver CPU supports split lock detection while it doesn't
    have the split lock enumeration bit in IA32_CORE_CAPABILITIES. Tigerlake
    CPUs do enumerate the MSR.
    
     [ bp: Merge the two model-adding patches into one. ]
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/1588290395-2677-1-git-send-email-fenghua.yu@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a19a680542ce..6abbcc774b82 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1135,9 +1135,12 @@ void switch_to_sld(unsigned long tifn)
 static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,		0),
 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_L,		0),
+	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_D,		0),
 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT,	1),
 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_D,	1),
 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_L,	1),
+	X86_MATCH_INTEL_FAM6_MODEL(TIGERLAKE_L,		1),
+	X86_MATCH_INTEL_FAM6_MODEL(TIGERLAKE,		1),
 	{}
 };
 

commit 923f3a2b48bdccb6a1d1f0dd48de03de7ad936d9
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Tue May 5 15:36:15 2020 -0700

    x86/resctrl: Query LLC monitoring properties once during boot
    
    Cache and memory bandwidth monitoring are features that are part of
    x86 CPU resource control that is supported by the resctrl subsystem.
    The monitoring properties are obtained via CPUID from every CPU
    and only used within the resctrl subsystem where the properties are
    only read from boot_cpu_data.
    
    Obtain the monitoring properties once, placed in boot_cpu_data, via the
    ->c_bsp_init() helpers of the vendors that support X86_FEATURE_CQM_LLC.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/6d74a6ac3e69f4b7a8b4115835f9455faf0f468d.1588715690.git.reinette.chatre@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a19a680542ce..166d7c355896 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -22,6 +22,7 @@
 #include <asm/cpu_device_id.h>
 #include <asm/cmdline.h>
 #include <asm/traps.h>
+#include <asm/resctrl.h>
 
 #ifdef CONFIG_X86_64
 #include <linux/topology.h>
@@ -322,6 +323,11 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		detect_ht_early(c);
 }
 
+static void bsp_init_intel(struct cpuinfo_x86 *c)
+{
+	resctrl_cpu_detect(c);
+}
+
 #ifdef CONFIG_X86_32
 /*
  *	Early probe support logic for ppro memory erratum #50
@@ -961,6 +967,7 @@ static const struct cpu_dev intel_cpu_dev = {
 #endif
 	.c_detect_tlb	= intel_detect_tlb,
 	.c_early_init   = early_init_intel,
+	.c_bsp_init	= bsp_init_intel,
 	.c_init		= init_intel,
 	.c_x86_vendor	= X86_VENDOR_INTEL,
 };

commit 8b9a18a9f2494144fe23fe630d0734310fa65301
Author: Tony Luck <tony.luck@intel.com>
Date:   Thu Apr 16 13:57:54 2020 -0700

    x86/split_lock: Add Tremont family CPU models
    
    Tremont CPUs support IA32_CORE_CAPABILITIES bits to indicate whether
    specific SKUs have support for split lock detection.
    
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200416205754.21177-4-tony.luck@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index c23ad481347e..a19a680542ce 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1135,6 +1135,9 @@ void switch_to_sld(unsigned long tifn)
 static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,		0),
 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_L,		0),
+	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT,	1),
+	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_D,	1),
+	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_L,	1),
 	{}
 };
 

commit 48fd5b5ee714714f4cf9f9e1cba3b49b1fd40ed6
Author: Tony Luck <tony.luck@intel.com>
Date:   Thu Apr 16 13:57:53 2020 -0700

    x86/split_lock: Bits in IA32_CORE_CAPABILITIES are not architectural
    
    The Intel Software Developers' Manual erroneously listed bit 5 of the
    IA32_CORE_CAPABILITIES register as an architectural feature. It is not.
    
    Features enumerated by IA32_CORE_CAPABILITIES are model specific and
    implementation details may vary in different cpu models. Thus it is only
    safe to trust features after checking the CPU model.
    
    Icelake client and server models are known to implement the split lock
    detect feature even though they don't enumerate IA32_CORE_CAPABILITIES
    
    [ tglx: Use switch() for readability and massage comments ]
    
    Fixes: 6650cdd9a8cc ("x86/split_lock: Enable split lock detection by kernel")
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200416205754.21177-3-tony.luck@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ec0d8c74932f..c23ad481347e 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1120,10 +1120,17 @@ void switch_to_sld(unsigned long tifn)
 }
 
 /*
- * The following processors have the split lock detection feature. But
- * since they don't have the IA32_CORE_CAPABILITIES MSR, the feature cannot
- * be enumerated. Enable it by family and model matching on these
- * processors.
+ * Bits in the IA32_CORE_CAPABILITIES are not architectural, so they should
+ * only be trusted if it is confirmed that a CPU model implements a
+ * specific feature at a particular bit position.
+ *
+ * The possible driver data field values:
+ *
+ * - 0: CPU models that are known to have the per-core split-lock detection
+ *	feature even though they do not enumerate IA32_CORE_CAPABILITIES.
+ *
+ * - 1: CPU models which may enumerate IA32_CORE_CAPABILITIES and if so use
+ *      bit 5 to enumerate the per-core split-lock detection feature.
  */
 static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,		0),
@@ -1133,19 +1140,29 @@ static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
 
 void __init cpu_set_core_cap_bits(struct cpuinfo_x86 *c)
 {
-	u64 ia32_core_caps = 0;
+	const struct x86_cpu_id *m;
+	u64 ia32_core_caps;
+
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+		return;
 
-	if (c->x86_vendor != X86_VENDOR_INTEL)
+	m = x86_match_cpu(split_lock_cpu_ids);
+	if (!m)
 		return;
-	if (cpu_has(c, X86_FEATURE_CORE_CAPABILITIES)) {
-		/* Enumerate features reported in IA32_CORE_CAPABILITIES MSR. */
+
+	switch (m->driver_data) {
+	case 0:
+		break;
+	case 1:
+		if (!cpu_has(c, X86_FEATURE_CORE_CAPABILITIES))
+			return;
 		rdmsrl(MSR_IA32_CORE_CAPS, ia32_core_caps);
-	} else if (!boot_cpu_has(X86_FEATURE_HYPERVISOR)) {
-		/* Enumerate split lock detection by family and model. */
-		if (x86_match_cpu(split_lock_cpu_ids))
-			ia32_core_caps |= MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT;
+		if (!(ia32_core_caps & MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT))
+			return;
+		break;
+	default:
+		return;
 	}
 
-	if (ia32_core_caps & MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT)
-		split_lock_setup();
+	split_lock_setup();
 }

commit 3ab0762d1edfda6ccbc08f636acab42c103c299f
Author: Tony Luck <tony.luck@intel.com>
Date:   Thu Apr 16 13:57:52 2020 -0700

    x86/split_lock: Update to use X86_MATCH_INTEL_FAM6_MODEL()
    
    The SPLIT_LOCK_CPU() macro escaped the tree-wide sweep for old-style
    initialization. Update to use X86_MATCH_INTEL_FAM6_MODEL().
    
    Fixes: 6650cdd9a8cc ("x86/split_lock: Enable split lock detection by kernel")
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200416205754.21177-2-tony.luck@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index bf08d4508ecb..ec0d8c74932f 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1119,8 +1119,6 @@ void switch_to_sld(unsigned long tifn)
 	sld_update_msr(!(tifn & _TIF_SLD));
 }
 
-#define SPLIT_LOCK_CPU(model) {X86_VENDOR_INTEL, 6, model, X86_FEATURE_ANY}
-
 /*
  * The following processors have the split lock detection feature. But
  * since they don't have the IA32_CORE_CAPABILITIES MSR, the feature cannot
@@ -1128,8 +1126,8 @@ void switch_to_sld(unsigned long tifn)
  * processors.
  */
 static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
-	SPLIT_LOCK_CPU(INTEL_FAM6_ICELAKE_X),
-	SPLIT_LOCK_CPU(INTEL_FAM6_ICELAKE_L),
+	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,		0),
+	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_L,		0),
 	{}
 };
 

commit d7e94dbdac1a40924626b0efc7ff530c8baf5e4a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 10 13:54:00 2020 +0200

    x86/split_lock: Provide handle_guest_split_lock()
    
    Without at least minimal handling for split lock detection induced #AC,
    VMX will just run into the same problem as the VMWare hypervisor, which
    was reported by Kenneth.
    
    It will inject the #AC blindly into the guest whether the guest is
    prepared or not.
    
    Provide a function for guest mode which acts depending on the host
    SLD mode. If mode == sld_warn, treat it like user space, i.e. emit a
    warning, disable SLD and mark the task accordingly. Otherwise force
    SIGBUS.
    
     [ bp: Add a !CPU_SUP_INTEL stub for handle_guest_split_lock(). ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Link: https://lkml.kernel.org/r/20200410115516.978037132@linutronix.de
    Link: https://lkml.kernel.org/r/20200402123258.895628824@linutronix.de

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 9a26e972cdea..bf08d4508ecb 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -21,6 +21,7 @@
 #include <asm/elf.h>
 #include <asm/cpu_device_id.h>
 #include <asm/cmdline.h>
+#include <asm/traps.h>
 
 #ifdef CONFIG_X86_64
 #include <linux/topology.h>
@@ -1066,13 +1067,10 @@ static void split_lock_init(void)
 	split_lock_verify_msr(sld_state != sld_off);
 }
 
-bool handle_user_split_lock(struct pt_regs *regs, long error_code)
+static void split_lock_warn(unsigned long ip)
 {
-	if ((regs->flags & X86_EFLAGS_AC) || sld_state == sld_fatal)
-		return false;
-
 	pr_warn_ratelimited("#AC: %s/%d took a split_lock trap at address: 0x%lx\n",
-			    current->comm, current->pid, regs->ip);
+			    current->comm, current->pid, ip);
 
 	/*
 	 * Disable the split lock detection for this task so it can make
@@ -1081,6 +1079,31 @@ bool handle_user_split_lock(struct pt_regs *regs, long error_code)
 	 */
 	sld_update_msr(false);
 	set_tsk_thread_flag(current, TIF_SLD);
+}
+
+bool handle_guest_split_lock(unsigned long ip)
+{
+	if (sld_state == sld_warn) {
+		split_lock_warn(ip);
+		return true;
+	}
+
+	pr_warn_once("#AC: %s/%d %s split_lock trap at address: 0x%lx\n",
+		     current->comm, current->pid,
+		     sld_state == sld_fatal ? "fatal" : "bogus", ip);
+
+	current->thread.error_code = 0;
+	current->thread.trap_nr = X86_TRAP_AC;
+	force_sig_fault(SIGBUS, BUS_ADRALN, NULL);
+	return false;
+}
+EXPORT_SYMBOL_GPL(handle_guest_split_lock);
+
+bool handle_user_split_lock(struct pt_regs *regs, long error_code)
+{
+	if ((regs->flags & X86_EFLAGS_AC) || sld_state == sld_fatal)
+		return false;
+	split_lock_warn(regs->ip);
 	return true;
 }
 

commit a6a60741035bb48ca8d9f92a138958818148064c
Author: Xiaoyao Li <xiaoyao.li@intel.com>
Date:   Wed Mar 25 11:09:24 2020 +0800

    x86/split_lock: Avoid runtime reads of the TEST_CTRL MSR
    
    In a context switch from a task that is detecting split locks to one that
    is not (or vice versa) we need to update the TEST_CTRL MSR. Currently this
    is done with the common sequence:
    
            read the MSR
            flip the bit
            write the MSR
    in order to avoid changing the value of any reserved bits in the MSR.
    
    Cache unused and reserved bits of TEST_CTRL MSR with SPLIT_LOCK_DETECT bit
    cleared during initialization, so we can avoid an expensive RDMSR
    instruction during context switch.
    
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Originally-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Xiaoyao Li <xiaoyao.li@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200325030924.132881-3-xiaoyao.li@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 0c859c91d008..9a26e972cdea 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -45,6 +45,7 @@ enum split_lock_detect_state {
  * split lock detect, unless there is a command line override.
  */
 static enum split_lock_detect_state sld_state __ro_after_init = sld_off;
+static u64 msr_test_ctrl_cache __ro_after_init;
 
 /*
  * Processors which have self-snooping capability can handle conflicting
@@ -1034,6 +1035,8 @@ static void __init split_lock_setup(void)
 		break;
 	}
 
+	rdmsrl(MSR_TEST_CTRL, msr_test_ctrl_cache);
+
 	if (!split_lock_verify_msr(true)) {
 		pr_info("MSR access failed: Disabled\n");
 		return;
@@ -1050,14 +1053,10 @@ static void __init split_lock_setup(void)
  */
 static void sld_update_msr(bool on)
 {
-	u64 test_ctrl_val;
-
-	rdmsrl(MSR_TEST_CTRL, test_ctrl_val);
+	u64 test_ctrl_val = msr_test_ctrl_cache;
 
 	if (on)
 		test_ctrl_val |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
-	else
-		test_ctrl_val &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
 
 	wrmsrl(MSR_TEST_CTRL, test_ctrl_val);
 }

commit dbaba47085b0c2aa793ce849750164bd3765e163
Author: Xiaoyao Li <xiaoyao.li@intel.com>
Date:   Wed Mar 25 11:09:23 2020 +0800

    x86/split_lock: Rework the initialization flow of split lock detection
    
    Current initialization flow of split lock detection has following issues:
    
    1. It assumes the initial value of MSR_TEST_CTRL.SPLIT_LOCK_DETECT to be
       zero. However, it's possible that BIOS/firmware has set it.
    
    2. X86_FEATURE_SPLIT_LOCK_DETECT flag is unconditionally set even if
       there is a virtualization flaw that FMS indicates the existence while
       it's actually not supported.
    
    Rework the initialization flow to solve above issues. In detail, explicitly
    clear and set split_lock_detect bit to verify MSR_TEST_CTRL can be
    accessed, and rdmsr after wrmsr to ensure bit is cleared/set successfully.
    
    X86_FEATURE_SPLIT_LOCK_DETECT flag is set only when the feature does exist
    and the feature is not disabled with kernel param "split_lock_detect=off"
    
    On each processor, explicitly updating the SPLIT_LOCK_DETECT bit based on
    sld_sate in split_lock_init() since BIOS/firmware may touch it.
    
    Originally-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Xiaoyao Li <xiaoyao.li@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200325030924.132881-2-xiaoyao.li@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index db3e745e5d47..0c859c91d008 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -44,7 +44,7 @@ enum split_lock_detect_state {
  * split_lock_setup() will switch this to sld_warn on systems that support
  * split lock detect, unless there is a command line override.
  */
-static enum split_lock_detect_state sld_state = sld_off;
+static enum split_lock_detect_state sld_state __ro_after_init = sld_off;
 
 /*
  * Processors which have self-snooping capability can handle conflicting
@@ -984,78 +984,87 @@ static inline bool match_option(const char *arg, int arglen, const char *opt)
 	return len == arglen && !strncmp(arg, opt, len);
 }
 
+static bool split_lock_verify_msr(bool on)
+{
+	u64 ctrl, tmp;
+
+	if (rdmsrl_safe(MSR_TEST_CTRL, &ctrl))
+		return false;
+	if (on)
+		ctrl |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+	else
+		ctrl &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+	if (wrmsrl_safe(MSR_TEST_CTRL, ctrl))
+		return false;
+	rdmsrl(MSR_TEST_CTRL, tmp);
+	return ctrl == tmp;
+}
+
 static void __init split_lock_setup(void)
 {
+	enum split_lock_detect_state state = sld_warn;
 	char arg[20];
 	int i, ret;
 
-	setup_force_cpu_cap(X86_FEATURE_SPLIT_LOCK_DETECT);
-	sld_state = sld_warn;
+	if (!split_lock_verify_msr(false)) {
+		pr_info("MSR access failed: Disabled\n");
+		return;
+	}
 
 	ret = cmdline_find_option(boot_command_line, "split_lock_detect",
 				  arg, sizeof(arg));
 	if (ret >= 0) {
 		for (i = 0; i < ARRAY_SIZE(sld_options); i++) {
 			if (match_option(arg, ret, sld_options[i].option)) {
-				sld_state = sld_options[i].state;
+				state = sld_options[i].state;
 				break;
 			}
 		}
 	}
 
-	switch (sld_state) {
+	switch (state) {
 	case sld_off:
 		pr_info("disabled\n");
-		break;
-
+		return;
 	case sld_warn:
 		pr_info("warning about user-space split_locks\n");
 		break;
-
 	case sld_fatal:
 		pr_info("sending SIGBUS on user-space split_locks\n");
 		break;
 	}
+
+	if (!split_lock_verify_msr(true)) {
+		pr_info("MSR access failed: Disabled\n");
+		return;
+	}
+
+	sld_state = state;
+	setup_force_cpu_cap(X86_FEATURE_SPLIT_LOCK_DETECT);
 }
 
 /*
- * Locking is not required at the moment because only bit 29 of this
- * MSR is implemented and locking would not prevent that the operation
- * of one thread is immediately undone by the sibling thread.
- * Use the "safe" versions of rdmsr/wrmsr here because although code
- * checks CPUID and MSR bits to make sure the TEST_CTRL MSR should
- * exist, there may be glitches in virtualization that leave a guest
- * with an incorrect view of real h/w capabilities.
+ * MSR_TEST_CTRL is per core, but we treat it like a per CPU MSR. Locking
+ * is not implemented as one thread could undo the setting of the other
+ * thread immediately after dropping the lock anyway.
  */
-static bool __sld_msr_set(bool on)
+static void sld_update_msr(bool on)
 {
 	u64 test_ctrl_val;
 
-	if (rdmsrl_safe(MSR_TEST_CTRL, &test_ctrl_val))
-		return false;
+	rdmsrl(MSR_TEST_CTRL, test_ctrl_val);
 
 	if (on)
 		test_ctrl_val |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
 	else
 		test_ctrl_val &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
 
-	return !wrmsrl_safe(MSR_TEST_CTRL, test_ctrl_val);
+	wrmsrl(MSR_TEST_CTRL, test_ctrl_val);
 }
 
 static void split_lock_init(void)
 {
-	if (sld_state == sld_off)
-		return;
-
-	if (__sld_msr_set(true))
-		return;
-
-	/*
-	 * If this is anything other than the boot-cpu, you've done
-	 * funny things and you get to keep whatever pieces.
-	 */
-	pr_warn("MSR fail -- disabled\n");
-	sld_state = sld_off;
+	split_lock_verify_msr(sld_state != sld_off);
 }
 
 bool handle_user_split_lock(struct pt_regs *regs, long error_code)
@@ -1071,7 +1080,7 @@ bool handle_user_split_lock(struct pt_regs *regs, long error_code)
 	 * progress and set TIF_SLD so the detection is re-enabled via
 	 * switch_to_sld() when the task is scheduled out.
 	 */
-	__sld_msr_set(false);
+	sld_update_msr(false);
 	set_tsk_thread_flag(current, TIF_SLD);
 	return true;
 }
@@ -1085,7 +1094,7 @@ bool handle_user_split_lock(struct pt_regs *regs, long error_code)
  */
 void switch_to_sld(unsigned long tifn)
 {
-	__sld_msr_set(!(tifn & _TIF_SLD));
+	sld_update_msr(!(tifn & _TIF_SLD));
 }
 
 #define SPLIT_LOCK_CPU(model) {X86_VENDOR_INTEL, 6, model, X86_FEATURE_ANY}

commit 6650cdd9a8ccf00555dbbe743d58541ad8feb6a7
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Sun Jan 26 12:05:35 2020 -0800

    x86/split_lock: Enable split lock detection by kernel
    
    A split-lock occurs when an atomic instruction operates on data that spans
    two cache lines. In order to maintain atomicity the core takes a global bus
    lock.
    
    This is typically >1000 cycles slower than an atomic operation within a
    cache line. It also disrupts performance on other cores (which must wait
    for the bus lock to be released before their memory operations can
    complete). For real-time systems this may mean missing deadlines. For other
    systems it may just be very annoying.
    
    Some CPUs have the capability to raise an #AC trap when a split lock is
    attempted.
    
    Provide a command line option to give the user choices on how to handle
    this:
    
    split_lock_detect=
            off     - not enabled (no traps for split locks)
            warn    - warn once when an application does a
                      split lock, but allow it to continue
                      running.
            fatal   - Send SIGBUS to applications that cause split lock
    
    On systems that support split lock detection the default is "warn". Note
    that if the kernel hits a split lock in any mode other than "off" it will
    OOPs.
    
    One implementation wrinkle is that the MSR to control the split lock
    detection is per-core, not per thread. This might result in some short
    lived races on HT systems in "warn" mode if Linux tries to enable on one
    thread while disabling on the other. Race analysis by Sean Christopherson:
    
      - Toggling of split-lock is only done in "warn" mode.  Worst case
        scenario of a race is that a misbehaving task will generate multiple
        #AC exceptions on the same instruction.  And this race will only occur
        if both siblings are running tasks that generate split-lock #ACs, e.g.
        a race where sibling threads are writing different values will only
        occur if CPUx is disabling split-lock after an #AC and CPUy is
        re-enabling split-lock after *its* previous task generated an #AC.
      - Transitioning between off/warn/fatal modes at runtime isn't supported
        and disabling is tracked per task, so hardware will always reach a steady
        state that matches the configured mode.  I.e. split-lock is guaranteed to
        be enabled in hardware once all _TIF_SLD threads have been scheduled out.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Co-developed-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20200126200535.GB30377@agluck-desk2.amr.corp.intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index be82cd5841c3..db3e745e5d47 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -19,6 +19,8 @@
 #include <asm/microcode_intel.h>
 #include <asm/hwcap2.h>
 #include <asm/elf.h>
+#include <asm/cpu_device_id.h>
+#include <asm/cmdline.h>
 
 #ifdef CONFIG_X86_64
 #include <linux/topology.h>
@@ -31,6 +33,19 @@
 #include <asm/apic.h>
 #endif
 
+enum split_lock_detect_state {
+	sld_off = 0,
+	sld_warn,
+	sld_fatal,
+};
+
+/*
+ * Default to sld_off because most systems do not support split lock detection
+ * split_lock_setup() will switch this to sld_warn on systems that support
+ * split lock detect, unless there is a command line override.
+ */
+static enum split_lock_detect_state sld_state = sld_off;
+
 /*
  * Processors which have self-snooping capability can handle conflicting
  * memory type across CPUs by snooping its own cache. However, there exists
@@ -570,6 +585,8 @@ static void init_intel_misc_features(struct cpuinfo_x86 *c)
 	wrmsrl(MSR_MISC_FEATURES_ENABLES, msr);
 }
 
+static void split_lock_init(void);
+
 static void init_intel(struct cpuinfo_x86 *c)
 {
 	early_init_intel(c);
@@ -684,6 +701,8 @@ static void init_intel(struct cpuinfo_x86 *c)
 		tsx_enable();
 	if (tsx_ctrl_state == TSX_CTRL_DISABLE)
 		tsx_disable();
+
+	split_lock_init();
 }
 
 #ifdef CONFIG_X86_32
@@ -945,3 +964,159 @@ static const struct cpu_dev intel_cpu_dev = {
 };
 
 cpu_dev_register(intel_cpu_dev);
+
+#undef pr_fmt
+#define pr_fmt(fmt) "x86/split lock detection: " fmt
+
+static const struct {
+	const char			*option;
+	enum split_lock_detect_state	state;
+} sld_options[] __initconst = {
+	{ "off",	sld_off   },
+	{ "warn",	sld_warn  },
+	{ "fatal",	sld_fatal },
+};
+
+static inline bool match_option(const char *arg, int arglen, const char *opt)
+{
+	int len = strlen(opt);
+
+	return len == arglen && !strncmp(arg, opt, len);
+}
+
+static void __init split_lock_setup(void)
+{
+	char arg[20];
+	int i, ret;
+
+	setup_force_cpu_cap(X86_FEATURE_SPLIT_LOCK_DETECT);
+	sld_state = sld_warn;
+
+	ret = cmdline_find_option(boot_command_line, "split_lock_detect",
+				  arg, sizeof(arg));
+	if (ret >= 0) {
+		for (i = 0; i < ARRAY_SIZE(sld_options); i++) {
+			if (match_option(arg, ret, sld_options[i].option)) {
+				sld_state = sld_options[i].state;
+				break;
+			}
+		}
+	}
+
+	switch (sld_state) {
+	case sld_off:
+		pr_info("disabled\n");
+		break;
+
+	case sld_warn:
+		pr_info("warning about user-space split_locks\n");
+		break;
+
+	case sld_fatal:
+		pr_info("sending SIGBUS on user-space split_locks\n");
+		break;
+	}
+}
+
+/*
+ * Locking is not required at the moment because only bit 29 of this
+ * MSR is implemented and locking would not prevent that the operation
+ * of one thread is immediately undone by the sibling thread.
+ * Use the "safe" versions of rdmsr/wrmsr here because although code
+ * checks CPUID and MSR bits to make sure the TEST_CTRL MSR should
+ * exist, there may be glitches in virtualization that leave a guest
+ * with an incorrect view of real h/w capabilities.
+ */
+static bool __sld_msr_set(bool on)
+{
+	u64 test_ctrl_val;
+
+	if (rdmsrl_safe(MSR_TEST_CTRL, &test_ctrl_val))
+		return false;
+
+	if (on)
+		test_ctrl_val |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+	else
+		test_ctrl_val &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+
+	return !wrmsrl_safe(MSR_TEST_CTRL, test_ctrl_val);
+}
+
+static void split_lock_init(void)
+{
+	if (sld_state == sld_off)
+		return;
+
+	if (__sld_msr_set(true))
+		return;
+
+	/*
+	 * If this is anything other than the boot-cpu, you've done
+	 * funny things and you get to keep whatever pieces.
+	 */
+	pr_warn("MSR fail -- disabled\n");
+	sld_state = sld_off;
+}
+
+bool handle_user_split_lock(struct pt_regs *regs, long error_code)
+{
+	if ((regs->flags & X86_EFLAGS_AC) || sld_state == sld_fatal)
+		return false;
+
+	pr_warn_ratelimited("#AC: %s/%d took a split_lock trap at address: 0x%lx\n",
+			    current->comm, current->pid, regs->ip);
+
+	/*
+	 * Disable the split lock detection for this task so it can make
+	 * progress and set TIF_SLD so the detection is re-enabled via
+	 * switch_to_sld() when the task is scheduled out.
+	 */
+	__sld_msr_set(false);
+	set_tsk_thread_flag(current, TIF_SLD);
+	return true;
+}
+
+/*
+ * This function is called only when switching between tasks with
+ * different split-lock detection modes. It sets the MSR for the
+ * mode of the new task. This is right most of the time, but since
+ * the MSR is shared by hyperthreads on a physical core there can
+ * be glitches when the two threads need different modes.
+ */
+void switch_to_sld(unsigned long tifn)
+{
+	__sld_msr_set(!(tifn & _TIF_SLD));
+}
+
+#define SPLIT_LOCK_CPU(model) {X86_VENDOR_INTEL, 6, model, X86_FEATURE_ANY}
+
+/*
+ * The following processors have the split lock detection feature. But
+ * since they don't have the IA32_CORE_CAPABILITIES MSR, the feature cannot
+ * be enumerated. Enable it by family and model matching on these
+ * processors.
+ */
+static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
+	SPLIT_LOCK_CPU(INTEL_FAM6_ICELAKE_X),
+	SPLIT_LOCK_CPU(INTEL_FAM6_ICELAKE_L),
+	{}
+};
+
+void __init cpu_set_core_cap_bits(struct cpuinfo_x86 *c)
+{
+	u64 ia32_core_caps = 0;
+
+	if (c->x86_vendor != X86_VENDOR_INTEL)
+		return;
+	if (cpu_has(c, X86_FEATURE_CORE_CAPABILITIES)) {
+		/* Enumerate features reported in IA32_CORE_CAPABILITIES MSR. */
+		rdmsrl(MSR_IA32_CORE_CAPS, ia32_core_caps);
+	} else if (!boot_cpu_has(X86_FEATURE_HYPERVISOR)) {
+		/* Enumerate split lock detection by family and model. */
+		if (x86_match_cpu(split_lock_cpu_ids))
+			ia32_core_caps |= MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT;
+	}
+
+	if (ia32_core_caps & MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT)
+		split_lock_setup();
+}

commit ccaaaf6fe5a5e1fffca5cca0f3fc4ec84d7ae752
Merge: 35c222fd3236 45fc24e89b7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 30 16:11:50 2020 -0800

    Merge tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx
    
    Pull x86 MPX removal from Dave Hansen:
     "MPX requires recompiling applications, which requires compiler
      support. Unfortunately, GCC 9.1 is expected to be be released without
      support for MPX. This means that there was only a relatively small
      window where folks could have ever used MPX. It failed to gain wide
      adoption in the industry, and Linux was the only mainstream OS to ever
      support it widely.
    
      Support for the feature may also disappear on future processors.
    
      This set completes the process that we started during the 5.4 merge
      window when the MPX prctl()s were removed. XSAVE support is left in
      place, which allows MPX-using KVM guests to continue to function"
    
    * tag 'mpx-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/daveh/x86-mpx:
      x86/mpx: remove MPX from arch/x86
      mm: remove arch_bprm_mm_init() hook
      x86/mpx: remove bounds exception code
      x86/mpx: remove build infrastructure
      x86/alternatives: add missing insn.h include

commit 45fc24e89b7cc2e227b2f03d99dda0a2204bf383
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jan 23 10:41:20 2020 -0800

    x86/mpx: remove MPX from arch/x86
    
    From: Dave Hansen <dave.hansen@linux.intel.com>
    
    MPX is being removed from the kernel due to a lack of support
    in the toolchain going forward (gcc).
    
    This removes all the remaining (dead at this point) MPX handling
    code remaining in the tree.  The only remaining code is the XSAVE
    support for MPX state which is currently needd for KVM to handle
    VMs which might use MPX.
    
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: x86@kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 11d5c5950e2d..ea5899f79f36 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -31,41 +31,6 @@
 #include <asm/apic.h>
 #endif
 
-/*
- * Just in case our CPU detection goes bad, or you have a weird system,
- * allow a way to override the automatic disabling of MPX.
- */
-static int forcempx;
-
-static int __init forcempx_setup(char *__unused)
-{
-	forcempx = 1;
-
-	return 1;
-}
-__setup("intel-skd-046-workaround=disable", forcempx_setup);
-
-void check_mpx_erratum(struct cpuinfo_x86 *c)
-{
-	if (forcempx)
-		return;
-	/*
-	 * Turn off the MPX feature on CPUs where SMEP is not
-	 * available or disabled.
-	 *
-	 * Works around Intel Erratum SKD046: "Branch Instructions
-	 * May Initialize MPX Bound Registers Incorrectly".
-	 *
-	 * This might falsely disable MPX on systems without
-	 * SMEP, like Atom processors without SMEP.  But there
-	 * is no such hardware known at the moment.
-	 */
-	if (cpu_has(c, X86_FEATURE_MPX) && !cpu_has(c, X86_FEATURE_SMEP)) {
-		setup_clear_cpu_cap(X86_FEATURE_MPX);
-		pr_warn("x86/mpx: Disabling MPX since SMEP not present\n");
-	}
-}
-
 /*
  * Processors which have self-snooping capability can handle conflicting
  * memory type across CPUs by snooping its own cache. However, there exists
@@ -330,7 +295,6 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 			c->x86_coreid_bits = get_count_order((ebx >> 16) & 0xff);
 	}
 
-	check_mpx_erratum(c);
 	check_memory_type_self_snoop_errata(c);
 
 	/*

commit 167a4894c113ebe6a1f8b24fa6f9fca849c77f8a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 20 20:45:06 2019 -0800

    x86/cpu: Set synthetic VMX cpufeatures during init_ia32_feat_ctl()
    
    Set the synthetic VMX cpufeatures, which need to be kept to preserve
    /proc/cpuinfo's ABI, in the common IA32_FEAT_CTL initialization code.
    Remove the vendor code that manually sets the synthetic flags.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20191221044513.21680-13-sean.j.christopherson@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 9129c170ea11..57473e2c0869 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -494,52 +494,6 @@ static void srat_detect_node(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void detect_vmx_virtcap(struct cpuinfo_x86 *c)
-{
-	/* Intel VMX MSR indicated features */
-#define X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW	0x00200000
-#define X86_VMX_FEATURE_PROC_CTLS_VNMI		0x00400000
-#define X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS	0x80000000
-#define X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC	0x00000001
-#define X86_VMX_FEATURE_PROC_CTLS2_EPT		0x00000002
-#define X86_VMX_FEATURE_PROC_CTLS2_VPID		0x00000020
-#define x86_VMX_FEATURE_EPT_CAP_AD		0x00200000
-
-	u32 vmx_msr_low, vmx_msr_high, msr_ctl, msr_ctl2;
-	u32 msr_vpid_cap, msr_ept_cap;
-
-	clear_cpu_cap(c, X86_FEATURE_TPR_SHADOW);
-	clear_cpu_cap(c, X86_FEATURE_VNMI);
-	clear_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);
-	clear_cpu_cap(c, X86_FEATURE_EPT);
-	clear_cpu_cap(c, X86_FEATURE_VPID);
-	clear_cpu_cap(c, X86_FEATURE_EPT_AD);
-
-	rdmsr(MSR_IA32_VMX_PROCBASED_CTLS, vmx_msr_low, vmx_msr_high);
-	msr_ctl = vmx_msr_high | vmx_msr_low;
-	if (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW)
-		set_cpu_cap(c, X86_FEATURE_TPR_SHADOW);
-	if (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_VNMI)
-		set_cpu_cap(c, X86_FEATURE_VNMI);
-	if (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS) {
-		rdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,
-		      vmx_msr_low, vmx_msr_high);
-		msr_ctl2 = vmx_msr_high | vmx_msr_low;
-		if ((msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC) &&
-		    (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW))
-			set_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);
-		if (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_EPT) {
-			set_cpu_cap(c, X86_FEATURE_EPT);
-			rdmsr(MSR_IA32_VMX_EPT_VPID_CAP,
-			      msr_ept_cap, msr_vpid_cap);
-			if (msr_ept_cap & x86_VMX_FEATURE_EPT_CAP_AD)
-				set_cpu_cap(c, X86_FEATURE_EPT_AD);
-		}
-		if (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VPID)
-			set_cpu_cap(c, X86_FEATURE_VPID);
-	}
-}
-
 #define MSR_IA32_TME_ACTIVATE		0x982
 
 /* Helpers to access TME_ACTIVATE MSR */
@@ -757,9 +711,6 @@ static void init_intel(struct cpuinfo_x86 *c)
 
 	init_ia32_feat_ctl(c);
 
-	if (cpu_has(c, X86_FEATURE_VMX))
-		detect_vmx_virtcap(c);
-
 	if (cpu_has(c, X86_FEATURE_TME))
 		detect_tme(c);
 

commit 1db2a6e1e29ff994443a9eef7cf3d26104c777a7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 20 20:44:58 2019 -0800

    x86/intel: Initialize IA32_FEAT_CTL MSR at boot
    
    Opportunistically initialize IA32_FEAT_CTL to enable VMX when the MSR is
    left unlocked by BIOS.  Configuring feature control at boot time paves
    the way for similar enabling of other features, e.g. Software Guard
    Extensions (SGX).
    
    Temporarily leave equivalent KVM code in place in order to avoid
    introducing a regression on Centaur and Zhaoxin CPUs, e.g. removing
    KVM's code would leave the MSR unlocked on those CPUs and would break
    existing functionality if people are loading kvm_intel on Centaur and/or
    Zhaoxin.  Defer enablement of the boot-time configuration on Centaur and
    Zhaoxin to future patches to aid bisection.
    
    Note, Local Machine Check Exceptions (LMCE) are also supported by the
    kernel and enabled via feature control, but the kernel currently uses
    LMCE if and only if the feature is explicitly enabled by BIOS.  Keep
    the current behavior to avoid introducing bugs, future patches can opt
    in to opportunistic enabling if it's deemed desirable to do so.
    
    Always lock IA32_FEAT_CTL if it exists, even if the CPU doesn't support
    VMX, so that other existing and future kernel code that queries the MSR
    can assume it's locked.
    
    Start from a clean slate when constructing the value to write to
    IA32_FEAT_CTL, i.e. ignore whatever value BIOS left in the MSR so as not
    to enable random features or fault on the WRMSR.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20191221044513.21680-5-sean.j.christopherson@intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 4a900804a023..9129c170ea11 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -755,6 +755,8 @@ static void init_intel(struct cpuinfo_x86 *c)
 	/* Work around errata */
 	srat_detect_node(c);
 
+	init_ia32_feat_ctl(c);
+
 	if (cpu_has(c, X86_FEATURE_VMX))
 		detect_vmx_virtcap(c);
 

commit 9f4813b531a0b8cc502fcfb142937fe4e9104d77
Merge: 44e09568cf2d af42d3466bdc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 19 09:00:45 2019 +0100

    Merge tag 'v5.4-rc8' into WIP.x86/mm, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 95c5824f75f3ba4c9e8e5a4b1a623c95390ac266
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 11:01:53 2019 +0200

    x86/cpu: Add a "tsx=" cmdline option with TSX disabled by default
    
    Add a kernel cmdline parameter "tsx" to control the Transactional
    Synchronization Extensions (TSX) feature. On CPUs that support TSX
    control, use "tsx=on|off" to enable or disable TSX. Not specifying this
    option is equivalent to "tsx=off". This is because on certain processors
    TSX may be used as a part of a speculative side channel attack.
    
    Carve out the TSX controlling functionality into a separate compilation
    unit because TSX is a CPU feature while the TSX async abort control
    machinery will go to cpu/bugs.c.
    
     [ bp: - Massage, shorten and clear the arg buffer.
           - Clarifications of the tsx= possible options - Josh.
           - Expand on TSX_CTRL availability - Pawan. ]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index c2fdc00df163..11d5c5950e2d 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -762,6 +762,11 @@ static void init_intel(struct cpuinfo_x86 *c)
 		detect_tme(c);
 
 	init_intel_misc_features(c);
+
+	if (tsx_ctrl_state == TSX_CTRL_ENABLE)
+		tsx_enable();
+	if (tsx_ctrl_state == TSX_CTRL_DISABLE)
+		tsx_disable();
 }
 
 #ifdef CONFIG_X86_32

commit 77df779de742d6616d4ddd177cba152a75259104
Author: Sylvain 'ythier' Hitier <sylvain.hitier@gmail.com>
Date:   Sun Sep 15 11:09:25 2019 +0200

    x86/cpu: Clean up intel_tlb_table[]
    
    - Remove the unneeded backslash at EOL: that's not a macro.
      And let's please checkpatch by aligning to open parenthesis.
    
    - For 0x4f descriptor, remove " */" from the info field.
    
    - For 0xc2 descriptor, sync the beginning of info to match the tlb_type.
    
    (The value of info fields could be made more regular, but it's unused by
     the code and will be read only by developers, so don't bother.)
    
    Signed-off-by: Sylvain 'ythier' Hitier <sylvain.hitier@gmail.com>
    Cc: Alex Shi <alex.shi@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hans de Goede <hdegoede@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: trivial@kernel.org
    Link: https://lkml.kernel.org/r/20190915090917.GA5086@lilas
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 8d6d92ebeb54..24e619d1bf79 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -813,7 +813,7 @@ static const struct _tlb_table intel_tlb_table[] = {
 	{ 0x04, TLB_DATA_4M,		8,	" TLB_DATA 4 MByte pages, 4-way set associative" },
 	{ 0x05, TLB_DATA_4M,		32,	" TLB_DATA 4 MByte pages, 4-way set associative" },
 	{ 0x0b, TLB_INST_4M,		4,	" TLB_INST 4 MByte pages, 4-way set associative" },
-	{ 0x4f, TLB_INST_4K,		32,	" TLB_INST 4 KByte pages */" },
+	{ 0x4f, TLB_INST_4K,		32,	" TLB_INST 4 KByte pages" },
 	{ 0x50, TLB_INST_ALL,		64,	" TLB_INST 4 KByte and 2-MByte or 4-MByte pages" },
 	{ 0x51, TLB_INST_ALL,		128,	" TLB_INST 4 KByte and 2-MByte or 4-MByte pages" },
 	{ 0x52, TLB_INST_ALL,		256,	" TLB_INST 4 KByte and 2-MByte or 4-MByte pages" },
@@ -841,7 +841,7 @@ static const struct _tlb_table intel_tlb_table[] = {
 	{ 0xba, TLB_DATA_4K,		64,	" TLB_DATA 4 KByte pages, 4-way associative" },
 	{ 0xc0, TLB_DATA_4K_4M,		8,	" TLB_DATA 4 KByte and 4 MByte pages, 4-way associative" },
 	{ 0xc1, STLB_4K_2M,		1024,	" STLB 4 KByte and 2 MByte pages, 8-way associative" },
-	{ 0xc2, TLB_DATA_2M_4M,		16,	" DTLB 2 MByte/4MByte pages, 4-way associative" },
+	{ 0xc2, TLB_DATA_2M_4M,		16,	" TLB_DATA 2 MByte/4MByte pages, 4-way associative" },
 	{ 0xca, STLB_4K,		512,	" STLB 4 KByte pages, 4-way associative" },
 	{ 0x00, 0, 0 }
 };
@@ -853,8 +853,8 @@ static void intel_tlb_lookup(const unsigned char desc)
 		return;
 
 	/* look up this descriptor in the table */
-	for (k = 0; intel_tlb_table[k].descriptor != desc && \
-			intel_tlb_table[k].descriptor != 0; k++)
+	for (k = 0; intel_tlb_table[k].descriptor != desc &&
+	     intel_tlb_table[k].descriptor != 0; k++)
 		;
 
 	if (intel_tlb_table[k].tlb_type == 0)

commit 0cc5359d8fd45bc410906e009117e78e2b5b2322
Author: Rahul Tanwar <rahul.tanwar@linux.intel.com>
Date:   Thu Sep 5 12:30:20 2019 -0700

    x86/cpu: Update init data for new Airmont CPU model
    
    Update properties for newly added Airmont CPU variant.
    
    Signed-off-by: Rahul Tanwar <rahul.tanwar@linux.intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Cc: Gayatri Kammela <gayatri.kammela@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190905193020.14707-5-tony.luck@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index e2082ccccf13..c2fdc00df163 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -268,6 +268,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		case INTEL_FAM6_ATOM_SALTWELL_MID:
 		case INTEL_FAM6_ATOM_SALTWELL_TABLET:
 		case INTEL_FAM6_ATOM_SILVERMONT_MID:
+		case INTEL_FAM6_ATOM_AIRMONT_NP:
 			set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC_S3);
 			break;
 		default:

commit 9326011edfcb7f7c3ad70f8a1cde6b391604808e
Merge: 77e5517cb506 32b1cbe38041
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Sep 6 07:30:23 2019 +0200

    Merge branch 'x86/cleanups' into x86/cpu, to pick up dependent changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5ebb34edbefa8ea6a7e109179d5fc7b3529dbeba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 27 21:48:24 2019 +0200

    x86/intel: Aggregate microserver naming
    
    Currently big microservers have _XEON_D while small microservers have
    _X, Make it uniformly: _D.
    
    for i in `git grep -l "\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*_\(X\|XEON_D\)"`
    do
            sed -i -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*ATOM.*\)_X/\1_D/g' \
                   -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*\)_XEON_D/\1_D/g' ${i}
    done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20190827195122.677152989@infradead.org

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1d2c64bcf6ab..f4b795ab1a9b 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -151,8 +151,8 @@ static const struct sku_microcode spectre_bad_microcodes[] = {
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
 	{ INTEL_FAM6_BROADWELL,		0x04,	0x28 },
 	{ INTEL_FAM6_BROADWELL_G,	0x01,	0x1b },
-	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },
-	{ INTEL_FAM6_BROADWELL_XEON_D,	0x03,	0x07000011 },
+	{ INTEL_FAM6_BROADWELL_D,	0x02,	0x14 },
+	{ INTEL_FAM6_BROADWELL_D,	0x03,	0x07000011 },
 	{ INTEL_FAM6_BROADWELL_X,	0x01,	0x0b000025 },
 	{ INTEL_FAM6_HASWELL_L,		0x01,	0x21 },
 	{ INTEL_FAM6_HASWELL_G,		0x01,	0x18 },

commit 5e741407eab7c602ee5a2b06afb0070a02f4412f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 27 21:48:23 2019 +0200

    x86/intel: Aggregate big core graphics naming
    
    Currently big core clients with extra graphics on have:
    
     - _G
     - _GT3E
    
    Make it uniformly: _G
    
    for i in `git grep -l "\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*_GT3E"`
    do
            sed -i -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*\)_GT3E/\1_G/g' ${i}
    done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20190827195122.622802314@infradead.org

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index bafa2735f541..1d2c64bcf6ab 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -150,12 +150,12 @@ static const struct sku_microcode spectre_bad_microcodes[] = {
 	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
 	{ INTEL_FAM6_BROADWELL,		0x04,	0x28 },
-	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },
+	{ INTEL_FAM6_BROADWELL_G,	0x01,	0x1b },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x03,	0x07000011 },
 	{ INTEL_FAM6_BROADWELL_X,	0x01,	0x0b000025 },
 	{ INTEL_FAM6_HASWELL_L,		0x01,	0x21 },
-	{ INTEL_FAM6_HASWELL_GT3E,	0x01,	0x18 },
+	{ INTEL_FAM6_HASWELL_G,		0x01,	0x18 },
 	{ INTEL_FAM6_HASWELL,		0x03,	0x23 },
 	{ INTEL_FAM6_HASWELL_X,		0x02,	0x3b },
 	{ INTEL_FAM6_HASWELL_X,		0x04,	0x10 },

commit af239c44e3f976762e9bc052f0d5796b90ea530b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 27 21:48:22 2019 +0200

    x86/intel: Aggregate big core mobile naming
    
    Currently big core mobile chips have either:
    
     - _L
     - _ULT
     - _MOBILE
    
    Make it uniformly: _L.
    
    for i in `git grep -l "\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*_\(MOBILE\|ULT\)"`
    do
            sed -i -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*\)_\(MOBILE\|ULT\)/\1_L/g' ${i}
    done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190827195122.568978530@infradead.org

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ca62563c1bb3..bafa2735f541 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -145,8 +145,8 @@ static const struct sku_microcode spectre_bad_microcodes[] = {
 	{ INTEL_FAM6_KABYLAKE,		0x0B,	0x80 },
 	{ INTEL_FAM6_KABYLAKE,		0x0A,	0x80 },
 	{ INTEL_FAM6_KABYLAKE,		0x09,	0x80 },
-	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x80 },
-	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x80 },
+	{ INTEL_FAM6_KABYLAKE_L,	0x0A,	0x80 },
+	{ INTEL_FAM6_KABYLAKE_L,	0x09,	0x80 },
 	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
 	{ INTEL_FAM6_BROADWELL,		0x04,	0x28 },
@@ -154,7 +154,7 @@ static const struct sku_microcode spectre_bad_microcodes[] = {
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x03,	0x07000011 },
 	{ INTEL_FAM6_BROADWELL_X,	0x01,	0x0b000025 },
-	{ INTEL_FAM6_HASWELL_ULT,	0x01,	0x21 },
+	{ INTEL_FAM6_HASWELL_L,		0x01,	0x21 },
 	{ INTEL_FAM6_HASWELL_GT3E,	0x01,	0x18 },
 	{ INTEL_FAM6_HASWELL,		0x03,	0x23 },
 	{ INTEL_FAM6_HASWELL_X,		0x02,	0x3b },

commit c66f78a6de4de6cb520b15cf6a1b586617b9add5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 27 21:48:21 2019 +0200

    x86/intel: Aggregate big core client naming
    
    Currently the big core client models either have:
    
     - no OPTDIFF
     - _CORE
     - _DESKTOP
    
    Make it uniformly: 'no OPTDIFF'.
    
    for i in `git grep -l "\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*_\(CORE\|DESKTOP\)"`
    do
            sed -i -e 's/\(\(INTEL_FAM6_\|VULNWL_INTEL\|INTEL_CPU_FAM6\).*\)_\(CORE\|DESKTOP\)/\1/g' ${i}
    done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190827195122.513945586@infradead.org

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 8d6d92ebeb54..ca62563c1bb3 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -142,21 +142,21 @@ struct sku_microcode {
 	u32 microcode;
 };
 static const struct sku_microcode spectre_bad_microcodes[] = {
-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0B,	0x80 },
-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0A,	0x80 },
-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x09,	0x80 },
+	{ INTEL_FAM6_KABYLAKE,		0x0B,	0x80 },
+	{ INTEL_FAM6_KABYLAKE,		0x0A,	0x80 },
+	{ INTEL_FAM6_KABYLAKE,		0x09,	0x80 },
 	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x80 },
 	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x80 },
 	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
-	{ INTEL_FAM6_BROADWELL_CORE,	0x04,	0x28 },
+	{ INTEL_FAM6_BROADWELL,		0x04,	0x28 },
 	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x03,	0x07000011 },
 	{ INTEL_FAM6_BROADWELL_X,	0x01,	0x0b000025 },
 	{ INTEL_FAM6_HASWELL_ULT,	0x01,	0x21 },
 	{ INTEL_FAM6_HASWELL_GT3E,	0x01,	0x18 },
-	{ INTEL_FAM6_HASWELL_CORE,	0x03,	0x23 },
+	{ INTEL_FAM6_HASWELL,		0x03,	0x23 },
 	{ INTEL_FAM6_HASWELL_X,		0x02,	0x3b },
 	{ INTEL_FAM6_HASWELL_X,		0x04,	0x10 },
 	{ INTEL_FAM6_IVYBRIDGE_X,	0x04,	0x42a },

commit bba10c5cab4ddd8725a7998e064fc72c9770c667
Author: Rahul Tanwar <rahul.tanwar@linux.intel.com>
Date:   Fri Aug 16 16:18:57 2019 +0800

    x86/cpu: Use constant definitions for CPU models
    
    Replace model numbers with their respective macro definitions when
    comparing CPU models.
    
    Suggested-by: Andy Shevchenko <andriy.shevchenko@intel.com>
    Signed-off-by: Rahul Tanwar <rahul.tanwar@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: alan@linux.intel.com
    Cc: cheol.yong.kim@intel.com
    Cc: Hans de Goede <hdegoede@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: qi-ming.wu@intel.com
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/f7a0e142faa953a53d5f81f78055e1b3c793b134.1565940653.git.rahul.tanwar@linux.intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 8d6d92ebeb54..66de4b84c369 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -265,9 +265,9 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	/* Penwell and Cloverview have the TSC which doesn't sleep on S3 */
 	if (c->x86 == 6) {
 		switch (c->x86_model) {
-		case 0x27:	/* Penwell */
-		case 0x35:	/* Cloverview */
-		case 0x4a:	/* Merrifield */
+		case INTEL_FAM6_ATOM_SALTWELL_MID:
+		case INTEL_FAM6_ATOM_SALTWELL_TABLET:
+		case INTEL_FAM6_ATOM_SILVERMONT_MID:
 			set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC_S3);
 			break;
 		default:

commit 1e03bff3600101bd9158d005e4313132e55bdec8
Author: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Date:   Thu Jun 27 19:35:36 2019 -0700

    x86/cpu/intel: Clear cache self-snoop capability in CPUs with known errata
    
    Processors which have self-snooping capability can handle conflicting
    memory type across CPUs by snooping its own cache. However, there exists
    CPU models in which having conflicting memory types still leads to
    unpredictable behavior, machine check errors, or hangs.
    
    Clear this feature on affected CPUs to prevent its use.
    
    Suggested-by: Alan Cox <alan.cox@intel.com>
    Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Hans de Goede <hdegoede@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Jordan Borgner <mail@jordan-borgner.de>
    Cc: "Ravi V. Shankar" <ravi.v.shankar@intel.com>
    Cc: Mohammad Etemadi <mohammad.etemadi@intel.com>
    Cc: Ricardo Neri <ricardo.neri@intel.com>
    Cc: Andy Shevchenko <andriy.shevchenko@intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Peter Feiner <pfeiner@google.com>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/1561689337-19390-2-git-send-email-ricardo.neri-calderon@linux.intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index f17c1a714779..8d6d92ebeb54 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -66,6 +66,32 @@ void check_mpx_erratum(struct cpuinfo_x86 *c)
 	}
 }
 
+/*
+ * Processors which have self-snooping capability can handle conflicting
+ * memory type across CPUs by snooping its own cache. However, there exists
+ * CPU models in which having conflicting memory types still leads to
+ * unpredictable behavior, machine check errors, or hangs. Clear this
+ * feature to prevent its use on machines with known erratas.
+ */
+static void check_memory_type_self_snoop_errata(struct cpuinfo_x86 *c)
+{
+	switch (c->x86_model) {
+	case INTEL_FAM6_CORE_YONAH:
+	case INTEL_FAM6_CORE2_MEROM:
+	case INTEL_FAM6_CORE2_MEROM_L:
+	case INTEL_FAM6_CORE2_PENRYN:
+	case INTEL_FAM6_CORE2_DUNNINGTON:
+	case INTEL_FAM6_NEHALEM:
+	case INTEL_FAM6_NEHALEM_G:
+	case INTEL_FAM6_NEHALEM_EP:
+	case INTEL_FAM6_NEHALEM_EX:
+	case INTEL_FAM6_WESTMERE:
+	case INTEL_FAM6_WESTMERE_EP:
+	case INTEL_FAM6_SANDYBRIDGE:
+		setup_clear_cpu_cap(X86_FEATURE_SELFSNOOP);
+	}
+}
+
 static bool ring3mwait_disabled __read_mostly;
 
 static int __init ring3mwait_disable(char *__unused)
@@ -304,6 +330,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	}
 
 	check_mpx_erratum(c);
+	check_memory_type_self_snoop_errata(c);
 
 	/*
 	 * Get the number of SMT siblings early from the extended topology

commit 5861381d486601430cccf64849bd0a226154bc0d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 21 23:18:01 2019 +0100

    PM / arch: x86: Rework the MSR_IA32_ENERGY_PERF_BIAS handling
    
    The current handling of MSR_IA32_ENERGY_PERF_BIAS in the kernel is
    problematic, because it may cause changes made by user space to that
    MSR (with the help of the x86_energy_perf_policy tool, for example)
    to be lost every time a CPU goes offline and then back online as well
    as during system-wide power management transitions into sleep states
    and back into the working state.
    
    The first problem is that if the current EPB value for a CPU going
    online is 0 ('performance'), the kernel will change it to 6 ('normal')
    regardless of whether or not this is the first bring-up of that CPU.
    That also happens during system-wide resume from sleep states
    (including, but not limited to, hibernation).  However, the EPB may
    have been adjusted by user space this way and the kernel should not
    blindly override that setting.
    
    The second problem is that if the platform firmware resets the EPB
    values for any CPUs during system-wide resume from a sleep state,
    the kernel will not restore their previous EPB values that may
    have been set by user space before the preceding system-wide
    suspend transition.  Again, that behavior may at least be confusing
    from the user space perspective.
    
    In order to address these issues, rework the handling of
    MSR_IA32_ENERGY_PERF_BIAS so that the EPB value is saved on CPU
    offline and restored on CPU online as well as (for the boot CPU)
    during the syscore stages of system-wide suspend and resume
    transitions, respectively.
    
    However, retain the policy by which the EPB is set to 6 ('normal')
    on the first bring-up of each CPU if its initial value is 0, based
    on the observation that 0 may mean 'not initialized' just as well as
    'performance' in that case.
    
    While at it, move the MSR_IA32_ENERGY_PERF_BIAS handling code into
    a separate file and document it in Documentation/admin-guide.
    
    Fixes: abe48b108247 (x86, intel, power: Initialize MSR_IA32_ENERGY_PERF_BIAS)
    Fixes: b51ef52df71c (x86/cpu: Restore MSR_IA32_ENERGY_PERF_BIAS after resume)
    Reported-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index fc3c07fe7df5..f17c1a714779 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -596,36 +596,6 @@ static void detect_tme(struct cpuinfo_x86 *c)
 	c->x86_phys_bits -= keyid_bits;
 }
 
-static void init_intel_energy_perf(struct cpuinfo_x86 *c)
-{
-	u64 epb;
-
-	/*
-	 * Initialize MSR_IA32_ENERGY_PERF_BIAS if not already initialized.
-	 * (x86_energy_perf_policy(8) is available to change it at run-time.)
-	 */
-	if (!cpu_has(c, X86_FEATURE_EPB))
-		return;
-
-	rdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
-	if ((epb & 0xF) != ENERGY_PERF_BIAS_PERFORMANCE)
-		return;
-
-	pr_warn_once("ENERGY_PERF_BIAS: Set to 'normal', was 'performance'\n");
-	pr_warn_once("ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)\n");
-	epb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;
-	wrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
-}
-
-static void intel_bsp_resume(struct cpuinfo_x86 *c)
-{
-	/*
-	 * MSR_IA32_ENERGY_PERF_BIAS is lost across suspend/resume,
-	 * so reinitialize it properly like during bootup:
-	 */
-	init_intel_energy_perf(c);
-}
-
 static void init_cpuid_fault(struct cpuinfo_x86 *c)
 {
 	u64 msr;
@@ -763,8 +733,6 @@ static void init_intel(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_TME))
 		detect_tme(c);
 
-	init_intel_energy_perf(c);
-
 	init_intel_misc_features(c);
 }
 
@@ -1023,9 +991,7 @@ static const struct cpu_dev intel_cpu_dev = {
 	.c_detect_tlb	= intel_detect_tlb,
 	.c_early_init   = early_init_intel,
 	.c_init		= init_intel,
-	.c_bsp_resume	= intel_bsp_resume,
 	.c_x86_vendor	= X86_VENDOR_INTEL,
 };
 
 cpu_dev_register(intel_cpu_dev);
-

commit 1ab534e85c93945f7862378d8c8adcf408205b19
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Aug 24 10:03:51 2018 -0700

    x86/spectre: Add missing family 6 check to microcode check
    
    The check for Spectre microcodes does not check for family 6, only the
    model numbers.
    
    Add a family 6 check to avoid ambiguity with other families.
    
    Fixes: a5b296636453 ("x86/cpufeature: Blacklist SPEC_CTRL/PRED_CMD on early Spectre v2 microcodes")
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86@kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180824170351.34874-2-andi@firstfloor.org

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 401e8c133108..fc3c07fe7df5 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -150,6 +150,9 @@ static bool bad_spectre_microcode(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_HYPERVISOR))
 		return false;
 
+	if (c->x86 != 6)
+		return false;
+
 	for (i = 0; i < ARRAY_SIZE(spectre_bad_microcodes); i++) {
 		if (c->x86_model == spectre_bad_microcodes[i].model &&
 		    c->x86_stepping == spectre_bad_microcodes[i].stepping)

commit 958f338e96f874a0d29442396d6adf9c1e17aa2d
Merge: 781fca5b1046 07d981ad4cf1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 09:46:06 2018 -0700

    Merge branch 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Merge L1 Terminal Fault fixes from Thomas Gleixner:
     "L1TF, aka L1 Terminal Fault, is yet another speculative hardware
      engineering trainwreck. It's a hardware vulnerability which allows
      unprivileged speculative access to data which is available in the
      Level 1 Data Cache when the page table entry controlling the virtual
      address, which is used for the access, has the Present bit cleared or
      other reserved bits set.
    
      If an instruction accesses a virtual address for which the relevant
      page table entry (PTE) has the Present bit cleared or other reserved
      bits set, then speculative execution ignores the invalid PTE and loads
      the referenced data if it is present in the Level 1 Data Cache, as if
      the page referenced by the address bits in the PTE was still present
      and accessible.
    
      While this is a purely speculative mechanism and the instruction will
      raise a page fault when it is retired eventually, the pure act of
      loading the data and making it available to other speculative
      instructions opens up the opportunity for side channel attacks to
      unprivileged malicious code, similar to the Meltdown attack.
    
      While Meltdown breaks the user space to kernel space protection, L1TF
      allows to attack any physical memory address in the system and the
      attack works across all protection domains. It allows an attack of SGX
      and also works from inside virtual machines because the speculation
      bypasses the extended page table (EPT) protection mechanism.
    
      The assoicated CVEs are: CVE-2018-3615, CVE-2018-3620, CVE-2018-3646
    
      The mitigations provided by this pull request include:
    
       - Host side protection by inverting the upper address bits of a non
         present page table entry so the entry points to uncacheable memory.
    
       - Hypervisor protection by flushing L1 Data Cache on VMENTER.
    
       - SMT (HyperThreading) control knobs, which allow to 'turn off' SMT
         by offlining the sibling CPU threads. The knobs are available on
         the kernel command line and at runtime via sysfs
    
       - Control knobs for the hypervisor mitigation, related to L1D flush
         and SMT control. The knobs are available on the kernel command line
         and at runtime via sysfs
    
       - Extensive documentation about L1TF including various degrees of
         mitigations.
    
      Thanks to all people who have contributed to this in various ways -
      patches, review, testing, backporting - and the fruitful, sometimes
      heated, but at the end constructive discussions.
    
      There is work in progress to provide other forms of mitigations, which
      might be less horrible performance wise for a particular kind of
      workloads, but this is not yet ready for consumption due to their
      complexity and limitations"
    
    * 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (75 commits)
      x86/microcode: Allow late microcode loading with SMT disabled
      tools headers: Synchronise x86 cpufeatures.h for L1TF additions
      x86/mm/kmmio: Make the tracer robust against L1TF
      x86/mm/pat: Make set_memory_np() L1TF safe
      x86/speculation/l1tf: Make pmd/pud_mknotpresent() invert
      x86/speculation/l1tf: Invert all not present mappings
      cpu/hotplug: Fix SMT supported evaluation
      KVM: VMX: Tell the nested hypervisor to skip L1D flush on vmentry
      x86/speculation: Use ARCH_CAPABILITIES to skip L1D flush on vmentry
      x86/speculation: Simplify sysfs report of VMX L1TF vulnerability
      Documentation/l1tf: Remove Yonah processors from not vulnerable list
      x86/KVM/VMX: Don't set l1tf_flush_l1d from vmx_handle_external_intr()
      x86/irq: Let interrupt handlers set kvm_cpu_l1tf_flush_l1d
      x86: Don't include linux/irq.h from asm/hardirq.h
      x86/KVM/VMX: Introduce per-host-cpu analogue of l1tf_flush_l1d
      x86/irq: Demote irq_cpustat_t::__softirq_pending to u16
      x86/KVM/VMX: Move the l1tf_flush_l1d test to vmx_l1d_flush()
      x86/KVM/VMX: Replace 'vmx_l1d_flush_always' with 'vmx_l1d_flush_cond'
      x86/KVM/VMX: Don't set l1tf_flush_l1d to true from vmx_l1d_flush()
      cpu/hotplug: detect SMT disabled by BIOS
      ...

commit 301d328a6f8b53bb86c5ecf72db7bc178bcf1999
Author: Peter Feiner <pfeiner@google.com>
Date:   Wed Aug 1 11:06:57 2018 -0700

    x86/cpufeatures: Add EPT_AD feature bit
    
    Some Intel processors have an EPT feature whereby the accessed & dirty bits
    in EPT entries can be updated by HW. MSR IA32_VMX_EPT_VPID_CAP exposes the
    presence of this capability.
    
    There is no point in trying to use that new feature bit in the VMX code as
    VMX needs to read the MSR anyway to access other bits, but having the
    feature bit for EPT_AD in place helps virtualization management as it
    exposes "ept_ad" in /proc/cpuinfo/$proc/flags if the feature is present.
    
    [ tglx: Amended changelog ]
    
    Signed-off-by: Peter Feiner <pfeiner@google.com>
    Signed-off-by: Peter Shier <pshier@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Link: https://lkml.kernel.org/r/20180801180657.138051-1-pshier@google.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index eb75564f2d25..c050cd6066af 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -465,14 +465,17 @@ static void detect_vmx_virtcap(struct cpuinfo_x86 *c)
 #define X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC	0x00000001
 #define X86_VMX_FEATURE_PROC_CTLS2_EPT		0x00000002
 #define X86_VMX_FEATURE_PROC_CTLS2_VPID		0x00000020
+#define x86_VMX_FEATURE_EPT_CAP_AD		0x00200000
 
 	u32 vmx_msr_low, vmx_msr_high, msr_ctl, msr_ctl2;
+	u32 msr_vpid_cap, msr_ept_cap;
 
 	clear_cpu_cap(c, X86_FEATURE_TPR_SHADOW);
 	clear_cpu_cap(c, X86_FEATURE_VNMI);
 	clear_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);
 	clear_cpu_cap(c, X86_FEATURE_EPT);
 	clear_cpu_cap(c, X86_FEATURE_VPID);
+	clear_cpu_cap(c, X86_FEATURE_EPT_AD);
 
 	rdmsr(MSR_IA32_VMX_PROCBASED_CTLS, vmx_msr_low, vmx_msr_high);
 	msr_ctl = vmx_msr_high | vmx_msr_low;
@@ -487,8 +490,13 @@ static void detect_vmx_virtcap(struct cpuinfo_x86 *c)
 		if ((msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC) &&
 		    (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW))
 			set_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);
-		if (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_EPT)
+		if (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_EPT) {
 			set_cpu_cap(c, X86_FEATURE_EPT);
+			rdmsr(MSR_IA32_VMX_EPT_VPID_CAP,
+			      msr_ept_cap, msr_vpid_cap);
+			if (msr_ept_cap & x86_VMX_FEATURE_EPT_CAP_AD)
+				set_cpu_cap(c, X86_FEATURE_EPT_AD);
+		}
 		if (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VPID)
 			set_cpu_cap(c, X86_FEATURE_VPID);
 	}

commit 1910ad5624968f93be48e8e265513c54d66b897c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 6 01:00:55 2018 +0200

    x86/cpu/intel: Evaluate smp_num_siblings early
    
    Make use of the new early detection function to initialize smp_num_siblings
    on the boot cpu before the MP-Table or ACPI/MADT scan happens. That's
    required for force disabling SMT.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index eb75564f2d25..6602941cfebf 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -301,6 +301,13 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	}
 
 	check_mpx_erratum(c);
+
+	/*
+	 * Get the number of SMT siblings early from the extended topology
+	 * leaf, if available. Otherwise try the legacy SMT detection.
+	 */
+	if (detect_extended_topology_early(c) < 0)
+		detect_ht_early(c);
 }
 
 #ifdef CONFIG_X86_32

commit 5cef8c2a2289117b7f65de4313b7157578ec1a71
Merge: f7f4e7fc6c51 e4e961e36f06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 18:19:18 2018 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
    
     - Centaur CPU updates (David Wang)
    
     - AMD and other CPU topology enumeration improvements and fixes
       (Borislav Petkov, Thomas Gleixner, Suravee Suthikulpanit)
    
     - Continued 5-level paging work (Kirill A. Shutemov)
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Mark __pgtable_l5_enabled __initdata
      x86/mm: Mark p4d_offset() __always_inline
      x86/mm: Introduce the 'no5lvl' kernel parameter
      x86/mm: Stop pretending pgtable_l5_enabled is a variable
      x86/mm: Unify pgtable_l5_enabled usage in early boot code
      x86/boot/compressed/64: Fix trampoline page table address calculation
      x86/CPU: Move x86_cpuinfo::x86_max_cores assignment to detect_num_cpu_cores()
      x86/Centaur: Report correct CPU/cache topology
      x86/CPU: Move cpu_detect_cache_sizes() into init_intel_cacheinfo()
      x86/CPU: Make intel_num_cpu_cores() generic
      x86/CPU: Move cpu local function declarations to local header
      x86/CPU/AMD: Derive CPU topology from CPUID function 0xB when available
      x86/CPU: Modify detect_extended_topology() to return result
      x86/CPU/AMD: Calculate last level cache ID from number of sharing threads
      x86/CPU: Rename intel_cacheinfo.c to cacheinfo.c
      perf/events/amd/uncore: Fix amd_uncore_llc ID to use pre-defined cpu_llc_id
      x86/CPU/AMD: Have smp_num_siblings and cpu_llc_id always be present
      x86/Centaur: Initialize supported CPU features properly

commit 177bfd725bd1b67c7254248cf19f0465d493e631
Merge: 9305bd6ca7b4 fed71f7d9879 7dec80ccbe31
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat May 19 08:18:56 2018 +0200

    Merge branches 'x86/urgent' and 'core/urgent' into x86/boot, to pick up fixes and avoid conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 52817587e706686fcdb27f14c1b000c92f266c96
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 20:21:36 2018 +0200

    x86/cpufeatures: Disentangle SSBD enumeration
    
    The SSBD enumeration is similarly to the other bits magically shared
    between Intel and AMD though the mechanisms are different.
    
    Make X86_FEATURE_SSBD synthetic and set it depending on the vendor specific
    features or family dependent setup.
    
    Change the Intel bit to X86_FEATURE_SPEC_CTRL_SSBD to denote that SSBD is
    controlled via MSR_SPEC_CTRL and fix up the usage sites.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index dd37244c587a..577e7f7ae273 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -191,6 +191,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		setup_clear_cpu_cap(X86_FEATURE_MSR_SPEC_CTRL);
 		setup_clear_cpu_cap(X86_FEATURE_INTEL_STIBP);
 		setup_clear_cpu_cap(X86_FEATURE_SSBD);
+		setup_clear_cpu_cap(X86_FEATURE_SPEC_CTRL_SSBD);
 	}
 
 	/*

commit 7eb8956a7fec3c1f0abc2a5517dada99ccc8a961
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 19:13:18 2018 +0200

    x86/cpufeatures: Disentangle MSR_SPEC_CTRL enumeration from IBRS
    
    The availability of the SPEC_CTRL MSR is enumerated by a CPUID bit on
    Intel and implied by IBRS or STIBP support on AMD. That's just confusing
    and in case an AMD CPU has IBRS not supported because the underlying
    problem has been fixed but has another bit valid in the SPEC_CTRL MSR,
    the thing falls apart.
    
    Add a synthetic feature bit X86_FEATURE_MSR_SPEC_CTRL to denote the
    availability on both Intel and AMD.
    
    While at it replace the boot_cpu_has() checks with static_cpu_has() where
    possible. This prevents late microcode loading from exposing SPEC_CTRL, but
    late loading is already very limited as it does not reevaluate the
    mitigation options and other bits and pieces. Having static_cpu_has() is
    the simplest and least fragile solution.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 0eab6c89c8d9..dd37244c587a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -188,6 +188,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		setup_clear_cpu_cap(X86_FEATURE_IBPB);
 		setup_clear_cpu_cap(X86_FEATURE_STIBP);
 		setup_clear_cpu_cap(X86_FEATURE_SPEC_CTRL);
+		setup_clear_cpu_cap(X86_FEATURE_MSR_SPEC_CTRL);
 		setup_clear_cpu_cap(X86_FEATURE_INTEL_STIBP);
 		setup_clear_cpu_cap(X86_FEATURE_SSBD);
 	}

commit 9305bd6ca7b40fece04d7a7a02765e9e8349f146
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 13 11:43:53 2018 +0200

    x86/CPU: Move x86_cpuinfo::x86_max_cores assignment to detect_num_cpu_cores()
    
    No point to have it at the call sites.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ca141d159be1..6c414e2f3f5c 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -651,7 +651,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
 		 * detection.
 		 */
-		c->x86_max_cores = detect_num_cpu_cores(c);
+		detect_num_cpu_cores(c);
 #ifdef CONFIG_X86_32
 		detect_ht(c);
 #endif

commit 807e9bc8e2fe6b4907f9f77fd073f7ef5073af29
Author: David Wang <davidwang@zhaoxin.com>
Date:   Thu May 3 10:32:45 2018 +0800

    x86/CPU: Move cpu_detect_cache_sizes() into init_intel_cacheinfo()
    
    There is no point in having the conditional cpu_detect_cache_sizes() call
    at the callsite of init_intel_cacheinfo().
    
    Move it into init_intel_cacheinfo() and make init_intel_cacheinfo() void.
    
    [ tglx: Made the init_intel_cacheinfo() void as the return value was
            pointless. Adjust changelog accordingly ]
    
    Signed-off-by: David Wang <davidwang@zhaoxin.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: lukelin@viacpu.com
    Cc: qiyuanwang@zhaoxin.com
    Cc: gregkh@linuxfoundation.org
    Cc: brucechang@via-alliance.com
    Cc: timguo@zhaoxin.com
    Cc: cooperyan@zhaoxin.com
    Cc: hpa@zytor.com
    Cc: benjaminpan@viatech.com
    Link: https://lkml.kernel.org/r/1525314766-18910-3-git-send-email-davidwang@zhaoxin.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index b54535be254a..ca141d159be1 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -635,8 +635,6 @@ static void init_intel_misc_features(struct cpuinfo_x86 *c)
 
 static void init_intel(struct cpuinfo_x86 *c)
 {
-	unsigned int l2 = 0;
-
 	early_init_intel(c);
 
 	intel_workarounds(c);
@@ -659,13 +657,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 #endif
 	}
 
-	l2 = init_intel_cacheinfo(c);
-
-	/* Detect legacy cache sizes if init_intel_cacheinfo did not */
-	if (l2 == 0) {
-		cpu_detect_cache_sizes(c);
-		l2 = c->x86_cache_size;
-	}
+	init_intel_cacheinfo(c);
 
 	if (c->cpuid_level > 9) {
 		unsigned eax = cpuid_eax(10);
@@ -678,7 +670,8 @@ static void init_intel(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
 
 	if (boot_cpu_has(X86_FEATURE_DS)) {
-		unsigned int l1;
+		unsigned int l1, l2;
+
 		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);
 		if (!(l1 & (1<<11)))
 			set_cpu_cap(c, X86_FEATURE_BTS);
@@ -706,6 +699,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 	 * Dixon is NOT a Celeron.
 	 */
 	if (c->x86 == 6) {
+		unsigned int l2 = c->x86_cache_size;
 		char *p = NULL;
 
 		switch (c->x86_model) {

commit 2cc61be60e37b1856a97ccbdcca3e86e593bf06a
Author: David Wang <davidwang@zhaoxin.com>
Date:   Thu May 3 10:32:44 2018 +0800

    x86/CPU: Make intel_num_cpu_cores() generic
    
    intel_num_cpu_cores() is a static function in intel.c which can't be used
    by other files. Define another function called detect_num_cpu_cores() in
    common.c to replace this function so it can be reused.
    
    Signed-off-by: David Wang <davidwang@zhaoxin.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: lukelin@viacpu.com
    Cc: qiyuanwang@zhaoxin.com
    Cc: gregkh@linuxfoundation.org
    Cc: brucechang@via-alliance.com
    Cc: timguo@zhaoxin.com
    Cc: cooperyan@zhaoxin.com
    Cc: hpa@zytor.com
    Cc: benjaminpan@viatech.com
    Link: https://lkml.kernel.org/r/1525314766-18910-2-git-send-email-davidwang@zhaoxin.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index b9693b80fc21..b54535be254a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -453,24 +453,6 @@ static void srat_detect_node(struct cpuinfo_x86 *c)
 #endif
 }
 
-/*
- * find out the number of processor cores on the die
- */
-static int intel_num_cpu_cores(struct cpuinfo_x86 *c)
-{
-	unsigned int eax, ebx, ecx, edx;
-
-	if (!IS_ENABLED(CONFIG_SMP) || c->cpuid_level < 4)
-		return 1;
-
-	/* Intel has a non-standard dependency on %ecx for this CPUID level. */
-	cpuid_count(4, 0, &eax, &ebx, &ecx, &edx);
-	if (eax & 0x1f)
-		return (eax >> 26) + 1;
-	else
-		return 1;
-}
-
 static void detect_vmx_virtcap(struct cpuinfo_x86 *c)
 {
 	/* Intel VMX MSR indicated features */
@@ -671,7 +653,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
 		 * detection.
 		 */
-		c->x86_max_cores = intel_num_cpu_cores(c);
+		c->x86_max_cores = detect_num_cpu_cores(c);
 #ifdef CONFIG_X86_32
 		detect_ht(c);
 #endif

commit 9f65fb29374ee37856dbad847b4e121aab72b510
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 9 21:41:38 2018 +0200

    x86/bugs: Rename _RDS to _SSBD
    
    Intel collateral will reference the SSB mitigation bit in IA32_SPEC_CTL[2]
    as SSBD (Speculative Store Bypass Disable).
    
    Hence changing it.
    
    It is unclear yet what the MSR_IA32_ARCH_CAPABILITIES (0x10a) Bit(4) name
    is going to be. Following the rename it would be SSBD_NO but that rolls out
    to Speculative Store Bypass Disable No.
    
    Also fixed the missing space in X86_FEATURE_AMD_SSBD.
    
    [ tglx: Fixup x86_amd_rds_enable() and rds_tif_to_amd_ls_cfg() as well ]
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ef3f9c01c274..0eab6c89c8d9 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -189,7 +189,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		setup_clear_cpu_cap(X86_FEATURE_STIBP);
 		setup_clear_cpu_cap(X86_FEATURE_SPEC_CTRL);
 		setup_clear_cpu_cap(X86_FEATURE_INTEL_STIBP);
-		setup_clear_cpu_cap(X86_FEATURE_RDS);
+		setup_clear_cpu_cap(X86_FEATURE_SSBD);
 	}
 
 	/*

commit 772439717dbf703b39990be58d8d4e3e4ad0598a
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 25 22:04:22 2018 -0400

    x86/bugs/intel: Set proper CPU features and setup RDS
    
    Intel CPUs expose methods to:
    
     - Detect whether RDS capability is available via CPUID.7.0.EDX[31],
    
     - The SPEC_CTRL MSR(0x48), bit 2 set to enable RDS.
    
     - MSR_IA32_ARCH_CAPABILITIES, Bit(4) no need to enable RRS.
    
    With that in mind if spec_store_bypass_disable=[auto,on] is selected set at
    boot-time the SPEC_CTRL MSR to enable RDS if the platform requires it.
    
    Note that this does not fix the KVM case where the SPEC_CTRL is exposed to
    guests which can muck with it, see patch titled :
     KVM/SVM/VMX/x86/spectre_v2: Support the combination of guest and host IBRS.
    
    And for the firmware (IBRS to be set), see patch titled:
     x86/spectre_v2: Read SPEC_CTRL MSR during boot and re-use reserved bits
    
    [ tglx: Distangled it from the intel implementation and kept the call order ]
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 60d1897041da..ef3f9c01c274 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -189,6 +189,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		setup_clear_cpu_cap(X86_FEATURE_STIBP);
 		setup_clear_cpu_cap(X86_FEATURE_SPEC_CTRL);
 		setup_clear_cpu_cap(X86_FEATURE_INTEL_STIBP);
+		setup_clear_cpu_cap(X86_FEATURE_RDS);
 	}
 
 	/*

commit b837913fc2d9061bf9b8c0dd6bf2d24e2f98b84a
Author: jacek.tomaka@poczta.fm <jacek.tomaka@poczta.fm>
Date:   Tue Apr 24 00:14:25 2018 +0800

    x86/cpu/intel: Add missing TLB cpuid values
    
    Make kernel print the correct number of TLB entries on Intel Xeon Phi 7210
    (and others)
    
    Before:
    [ 0.320005] Last level dTLB entries: 4KB 0, 2MB 0, 4MB 0, 1GB 0
    After:
    [ 0.320005] Last level dTLB entries: 4KB 256, 2MB 128, 4MB 128, 1GB 16
    
    The entries do exist in the official Intel SMD but the type column there is
    incorrect (states "Cache" where it should read "TLB"), but the entries for
    the values 0x6B, 0x6C and 0x6D are correctly described as 'Data TLB'.
    
    Signed-off-by: Jacek Tomaka <jacek.tomaka@poczta.fm>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20180423161425.24366-1-jacekt@dugeo.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index b9693b80fc21..60d1897041da 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -835,6 +835,9 @@ static const struct _tlb_table intel_tlb_table[] = {
 	{ 0x5d, TLB_DATA_4K_4M,		256,	" TLB_DATA 4 KByte and 4 MByte pages" },
 	{ 0x61, TLB_INST_4K,		48,	" TLB_INST 4 KByte pages, full associative" },
 	{ 0x63, TLB_DATA_1G,		4,	" TLB_DATA 1 GByte pages, 4-way set associative" },
+	{ 0x6b, TLB_DATA_4K,		256,	" TLB_DATA 4 KByte pages, 8-way associative" },
+	{ 0x6c, TLB_DATA_2M_4M,		128,	" TLB_DATA 2 MByte or 4 MByte pages, 8-way associative" },
+	{ 0x6d, TLB_DATA_1G,		16,	" TLB_DATA 1 GByte pages, fully associative" },
 	{ 0x76, TLB_INST_2M_4M,		8,	" TLB_INST 2-MByte or 4-MByte pages, fully associative" },
 	{ 0xb0, TLB_INST_4K,		128,	" TLB_INST 4 KByte pages, 4-way set associative" },
 	{ 0xb1, TLB_INST_2M_4M,		4,	" TLB_INST 2M pages, 4-way, 8 entries or 4M pages, 4-way entries" },

commit eaeb8e76cd5751e805f6e4a3fcec91d283e3b0c2
Author: Colin Ian King <colin.king@canonical.com>
Date:   Tue Mar 13 15:47:09 2018 +0000

    x86/cpu/tme: Fix spelling: "configuation" -> "configuration"
    
    Trivial fix to spelling mistake in the pr_err_once() error message text.
    
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-janitors@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180313154709.1015-1-colin.king@canonical.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 6106d11ceb6b..b9693b80fc21 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -540,7 +540,7 @@ static void detect_tme(struct cpuinfo_x86 *c)
 	if (mktme_status != MKTME_UNINITIALIZED) {
 		if (tme_activate != tme_activate_cpu0) {
 			/* Broken BIOS? */
-			pr_err_once("x86/tme: configuation is inconsistent between CPUs\n");
+			pr_err_once("x86/tme: configuration is inconsistent between CPUs\n");
 			pr_err_once("x86/tme: MKTME is not usable\n");
 			mktme_status = MKTME_DISABLED;
 

commit 547edaca247abf910e32f0cd883ba83b8fc6d0ed
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 15 16:49:06 2018 +0300

    x86/mm: Update comment in detect_tme() regarding x86_phys_bits
    
    As Kai pointed out, the primary reason for adjusting x86_phys_bits is to
    reflect that the the address space is reduced and not the ability to
    communicate the available physical address space to virtual machines.
    
    Suggested-by: Kai Huang <kai.huang@linux.intel.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: linux-mm@kvack.org
    Link: https://lkml.kernel.org/r/20180315134907.9311-2-kirill.shutemov@linux.intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 3f8d7a3b6447..6106d11ceb6b 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -587,11 +587,8 @@ static void detect_tme(struct cpuinfo_x86 *c)
 	}
 
 	/*
-	 * Exclude KeyID bits from physical address bits.
-	 *
-	 * We have to do this even if we are not going to use KeyID bits
-	 * ourself. VM guests still have to know that these bits are not usable
-	 * for physical address.
+	 * KeyID bits effectively lower the number of physical address
+	 * bits.  Update cpuinfo_x86::x86_phys_bits accordingly.
 	 */
 	c->x86_phys_bits -= keyid_bits;
 }

commit 0bc91d4ba77156ae9217d25ed7c434540f950d05
Merge: 565977a3d929 3eb2ce825ea1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 27 08:43:39 2018 +0200

    Merge tag 'v4.16-rc7' into x86/mm, to fix up conflict
    
     Conflicts:
            arch/x86/mm/init_64.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e3b3121fa8da94cb20f9e0c64ab7981ae47fd085
Author: Alexander Sergeyev <sergeev917@gmail.com>
Date:   Tue Mar 13 22:38:56 2018 +0300

    x86/speculation: Remove Skylake C2 from Speculation Control microcode blacklist
    
    In accordance with Intel's microcode revision guidance from March 6 MCU
    rev 0xc2 is cleared on both Skylake H/S and Skylake Xeon E3 processors
    that share CPUID 506E3.
    
    Signed-off-by: Alexander Sergeyev <sergeev917@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jia Zhang <qianyue.zj@alibaba-inc.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Kyle Huey <me@kylehuey.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Link: https://lkml.kernel.org/r/20180313193856.GA8580@localhost.localdomain

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 4aa9fd379390..c3af167d0a70 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -105,7 +105,7 @@ static void probe_xeon_phi_r3mwait(struct cpuinfo_x86 *c)
 /*
  * Early microcode releases for the Spectre v2 mitigation were broken.
  * Information taken from;
- * - https://newsroom.intel.com/wp-content/uploads/sites/11/2018/01/microcode-update-guidance.pdf
+ * - https://newsroom.intel.com/wp-content/uploads/sites/11/2018/03/microcode-update-guidance.pdf
  * - https://kb.vmware.com/s/article/52345
  * - Microcode revisions observed in the wild
  * - Release note from 20180108 microcode release
@@ -123,7 +123,6 @@ static const struct sku_microcode spectre_bad_microcodes[] = {
 	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x80 },
 	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
-	{ INTEL_FAM6_SKYLAKE_DESKTOP,	0x03,	0xc2 },
 	{ INTEL_FAM6_BROADWELL_CORE,	0x04,	0x28 },
 	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },

commit cb06d8e3d020c30fe10ae711c925a5319ab82c88
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Mar 5 19:25:50 2018 +0300

    x86/tme: Detect if TME and MKTME is activated by BIOS
    
    IA32_TME_ACTIVATE MSR (0x982) can be used to check if BIOS has enabled
    TME and MKTME. It includes which encryption policy/algorithm is selected
    for TME or available for MKTME. For MKTME, the MSR also enumerates how
    many KeyIDs are available.
    
    We would need to exclude KeyID bits from physical address bits.
    detect_tme() would adjust cpuinfo_x86::x86_phys_bits accordingly.
    
    We have to do this even if we are not going to use KeyID bits
    ourself. VM guests still have to know that these bits are not usable
    for physical address.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Kai Huang <kai.huang@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20180305162610.37510-3-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 4aa9fd379390..b862067bb33c 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -510,6 +510,93 @@ static void detect_vmx_virtcap(struct cpuinfo_x86 *c)
 	}
 }
 
+#define MSR_IA32_TME_ACTIVATE		0x982
+
+/* Helpers to access TME_ACTIVATE MSR */
+#define TME_ACTIVATE_LOCKED(x)		(x & 0x1)
+#define TME_ACTIVATE_ENABLED(x)		(x & 0x2)
+
+#define TME_ACTIVATE_POLICY(x)		((x >> 4) & 0xf)	/* Bits 7:4 */
+#define TME_ACTIVATE_POLICY_AES_XTS_128	0
+
+#define TME_ACTIVATE_KEYID_BITS(x)	((x >> 32) & 0xf)	/* Bits 35:32 */
+
+#define TME_ACTIVATE_CRYPTO_ALGS(x)	((x >> 48) & 0xffff)	/* Bits 63:48 */
+#define TME_ACTIVATE_CRYPTO_AES_XTS_128	1
+
+/* Values for mktme_status (SW only construct) */
+#define MKTME_ENABLED			0
+#define MKTME_DISABLED			1
+#define MKTME_UNINITIALIZED		2
+static int mktme_status = MKTME_UNINITIALIZED;
+
+static void detect_tme(struct cpuinfo_x86 *c)
+{
+	u64 tme_activate, tme_policy, tme_crypto_algs;
+	int keyid_bits = 0, nr_keyids = 0;
+	static u64 tme_activate_cpu0 = 0;
+
+	rdmsrl(MSR_IA32_TME_ACTIVATE, tme_activate);
+
+	if (mktme_status != MKTME_UNINITIALIZED) {
+		if (tme_activate != tme_activate_cpu0) {
+			/* Broken BIOS? */
+			pr_err_once("x86/tme: configuation is inconsistent between CPUs\n");
+			pr_err_once("x86/tme: MKTME is not usable\n");
+			mktme_status = MKTME_DISABLED;
+
+			/* Proceed. We may need to exclude bits from x86_phys_bits. */
+		}
+	} else {
+		tme_activate_cpu0 = tme_activate;
+	}
+
+	if (!TME_ACTIVATE_LOCKED(tme_activate) || !TME_ACTIVATE_ENABLED(tme_activate)) {
+		pr_info_once("x86/tme: not enabled by BIOS\n");
+		mktme_status = MKTME_DISABLED;
+		return;
+	}
+
+	if (mktme_status != MKTME_UNINITIALIZED)
+		goto detect_keyid_bits;
+
+	pr_info("x86/tme: enabled by BIOS\n");
+
+	tme_policy = TME_ACTIVATE_POLICY(tme_activate);
+	if (tme_policy != TME_ACTIVATE_POLICY_AES_XTS_128)
+		pr_warn("x86/tme: Unknown policy is active: %#llx\n", tme_policy);
+
+	tme_crypto_algs = TME_ACTIVATE_CRYPTO_ALGS(tme_activate);
+	if (!(tme_crypto_algs & TME_ACTIVATE_CRYPTO_AES_XTS_128)) {
+		pr_err("x86/mktme: No known encryption algorithm is supported: %#llx\n",
+				tme_crypto_algs);
+		mktme_status = MKTME_DISABLED;
+	}
+detect_keyid_bits:
+	keyid_bits = TME_ACTIVATE_KEYID_BITS(tme_activate);
+	nr_keyids = (1UL << keyid_bits) - 1;
+	if (nr_keyids) {
+		pr_info_once("x86/mktme: enabled by BIOS\n");
+		pr_info_once("x86/mktme: %d KeyIDs available\n", nr_keyids);
+	} else {
+		pr_info_once("x86/mktme: disabled by BIOS\n");
+	}
+
+	if (mktme_status == MKTME_UNINITIALIZED) {
+		/* MKTME is usable */
+		mktme_status = MKTME_ENABLED;
+	}
+
+	/*
+	 * Exclude KeyID bits from physical address bits.
+	 *
+	 * We have to do this even if we are not going to use KeyID bits
+	 * ourself. VM guests still have to know that these bits are not usable
+	 * for physical address.
+	 */
+	c->x86_phys_bits -= keyid_bits;
+}
+
 static void init_intel_energy_perf(struct cpuinfo_x86 *c)
 {
 	u64 epb;
@@ -680,6 +767,9 @@ static void init_intel(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_VMX))
 		detect_vmx_virtcap(c);
 
+	if (cpu_has(c, X86_FEATURE_TME))
+		detect_tme(c);
+
 	init_intel_energy_perf(c);
 
 	init_intel_misc_features(c);

commit 36268223c1e9981d6cfc33aff8520b3bde4b8114
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Feb 26 09:35:01 2018 -0500

    x86/spectre_v2: Don't check microcode versions when running under hypervisors
    
    As:
    
     1) It's known that hypervisors lie about the environment anyhow (host
        mismatch)
    
     2) Even if the hypervisor (Xen, KVM, VMWare, etc) provided a valid
        "correct" value, it all gets to be very murky when migration happens
        (do you provide the "new" microcode of the machine?).
    
    And in reality the cloud vendors are the ones that should make sure that
    the microcode that is running is correct and we should just sing lalalala
    and trust them.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Cc: kvm <kvm@vger.kernel.org>
    Cc: Krm <rkrcmar@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    CC: "H. Peter Anvin" <hpa@zytor.com>
    CC: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180226213019.GE9497@char.us.oracle.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index d19e903214b4..4aa9fd379390 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -144,6 +144,13 @@ static bool bad_spectre_microcode(struct cpuinfo_x86 *c)
 {
 	int i;
 
+	/*
+	 * We know that the hypervisor lie to us on the microcode version so
+	 * we may as well hope that it is running the correct version.
+	 */
+	if (cpu_has(c, X86_FEATURE_HYPERVISOR))
+		return false;
+
 	for (i = 0; i < ARRAY_SIZE(spectre_bad_microcodes); i++) {
 		if (c->x86_model == spectre_bad_microcodes[i].model &&
 		    c->x86_stepping == spectre_bad_microcodes[i].stepping)

commit b399151cb48db30ad1e0e93dd40d68c6d007b637
Author: Jia Zhang <qianyue.zj@alibaba-inc.com>
Date:   Mon Jan 1 09:52:10 2018 +0800

    x86/cpu: Rename cpu_data.x86_mask to cpu_data.x86_stepping
    
    x86_mask is a confusing name which is hard to associate with the
    processor's stepping.
    
    Additionally, correct an indent issue in lib/cpu.c.
    
    Signed-off-by: Jia Zhang <qianyue.zj@alibaba-inc.com>
    [ Updated it to more recent kernels. ]
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: tony.luck@intel.com
    Link: http://lkml.kernel.org/r/1514771530-70829-1-git-send-email-qianyue.zj@alibaba-inc.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ef796f14f7ae..d19e903214b4 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -146,7 +146,7 @@ static bool bad_spectre_microcode(struct cpuinfo_x86 *c)
 
 	for (i = 0; i < ARRAY_SIZE(spectre_bad_microcodes); i++) {
 		if (c->x86_model == spectre_bad_microcodes[i].model &&
-		    c->x86_mask == spectre_bad_microcodes[i].stepping)
+		    c->x86_stepping == spectre_bad_microcodes[i].stepping)
 			return (c->microcode <= spectre_bad_microcodes[i].microcode);
 	}
 	return false;
@@ -193,7 +193,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	 * need the microcode to have already been loaded... so if it is
 	 * not, recommend a BIOS update and disable large pages.
 	 */
-	if (c->x86 == 6 && c->x86_model == 0x1c && c->x86_mask <= 2 &&
+	if (c->x86 == 6 && c->x86_model == 0x1c && c->x86_stepping <= 2 &&
 	    c->microcode < 0x20e) {
 		pr_warn("Atom PSE erratum detected, BIOS microcode update recommended\n");
 		clear_cpu_cap(c, X86_FEATURE_PSE);
@@ -209,7 +209,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 
 	/* CPUID workaround for 0F33/0F34 CPU */
 	if (c->x86 == 0xF && c->x86_model == 0x3
-	    && (c->x86_mask == 0x3 || c->x86_mask == 0x4))
+	    && (c->x86_stepping == 0x3 || c->x86_stepping == 0x4))
 		c->x86_phys_bits = 36;
 
 	/*
@@ -307,7 +307,7 @@ int ppro_with_ram_bug(void)
 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
 	    boot_cpu_data.x86 == 6 &&
 	    boot_cpu_data.x86_model == 1 &&
-	    boot_cpu_data.x86_mask < 8) {
+	    boot_cpu_data.x86_stepping < 8) {
 		pr_info("Pentium Pro with Errata#50 detected. Taking evasive action.\n");
 		return 1;
 	}
@@ -324,7 +324,7 @@ static void intel_smp_check(struct cpuinfo_x86 *c)
 	 * Mask B, Pentium, but not Pentium MMX
 	 */
 	if (c->x86 == 5 &&
-	    c->x86_mask >= 1 && c->x86_mask <= 4 &&
+	    c->x86_stepping >= 1 && c->x86_stepping <= 4 &&
 	    c->x86_model <= 3) {
 		/*
 		 * Remember we have B step Pentia with bugs
@@ -367,7 +367,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * SEP CPUID bug: Pentium Pro reports SEP but doesn't have it until
 	 * model 3 mask 3
 	 */
-	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
+	if ((c->x86<<8 | c->x86_model<<4 | c->x86_stepping) < 0x633)
 		clear_cpu_cap(c, X86_FEATURE_SEP);
 
 	/*
@@ -385,7 +385,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * P4 Xeon erratum 037 workaround.
 	 * Hardware prefetcher may cause stale data to be loaded into the cache.
 	 */
-	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
+	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_stepping == 1)) {
 		if (msr_set_bit(MSR_IA32_MISC_ENABLE,
 				MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE_BIT) > 0) {
 			pr_info("CPU: C0 stepping P4 Xeon detected.\n");
@@ -400,7 +400,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * Specification Update").
 	 */
 	if (boot_cpu_has(X86_FEATURE_APIC) && (c->x86<<8 | c->x86_model<<4) == 0x520 &&
-	    (c->x86_mask < 0x6 || c->x86_mask == 0xb))
+	    (c->x86_stepping < 0x6 || c->x86_stepping == 0xb))
 		set_cpu_bug(c, X86_BUG_11AP);
 
 
@@ -647,7 +647,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 		case 6:
 			if (l2 == 128)
 				p = "Celeron (Mendocino)";
-			else if (c->x86_mask == 0 || c->x86_mask == 5)
+			else if (c->x86_stepping == 0 || c->x86_stepping == 5)
 				p = "Celeron-A";
 			break;
 

commit d37fc6d360a404b208547ba112e7dabb6533c7fc
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Mon Feb 12 15:27:34 2018 +0000

    x86/speculation: Correct Speculation Control microcode blacklist again
    
    Arjan points out that the Intel document only clears the 0xc2 microcode
    on *some* parts with CPUID 506E3 (INTEL_FAM6_SKYLAKE_DESKTOP stepping 3).
    For the Skylake H/S platform it's OK but for Skylake E3 which has the
    same CPUID it isn't (yet) cleared.
    
    So removing it from the blacklist was premature. Put it back for now.
    
    Also, Arjan assures me that the 0x84 microcode for Kaby Lake which was
    featured in one of the early revisions of the Intel document was never
    released to the public, and won't be until/unless it is also validated
    as safe. So those can change to 0x80 which is what all *other* versions
    of the doc have identified.
    
    Once the retrospective testing of existing public microcodes is done, we
    should be back into a mode where new microcodes are only released in
    batches and we shouldn't even need to update the blacklist for those
    anyway, so this tweaking of the list isn't expected to be a thing which
    keeps happening.
    
    Requested-by: Arjan van de Ven <arjan.van.de.ven@intel.com>
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan.van.de.ven@intel.com
    Cc: dave.hansen@intel.com
    Cc: kvm@vger.kernel.org
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1518449255-2182-1-git-send-email-dwmw@amazon.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index f73b8148dd55..ef796f14f7ae 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -116,13 +116,14 @@ struct sku_microcode {
 	u32 microcode;
 };
 static const struct sku_microcode spectre_bad_microcodes[] = {
-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0B,	0x84 },
-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0A,	0x84 },
-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x09,	0x84 },
-	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x84 },
-	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x84 },
+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0B,	0x80 },
+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0A,	0x80 },
+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x09,	0x80 },
+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x80 },
+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x80 },
 	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
+	{ INTEL_FAM6_SKYLAKE_DESKTOP,	0x03,	0xc2 },
 	{ INTEL_FAM6_BROADWELL_CORE,	0x04,	0x28 },
 	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },

commit 1751342095f0d2b36fa8114d8e12c5688c455ac4
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Sat Feb 10 23:39:22 2018 +0000

    x86/speculation: Update Speculation Control microcode blacklist
    
    Intel have retroactively blessed the 0xc2 microcode on Skylake mobile
    and desktop parts, and the Gemini Lake 0x22 microcode is apparently fine
    too. We blacklisted the latter purely because it was present with all
    the other problematic ones in the 2018-01-08 release, but now it's
    explicitly listed as OK.
    
    We still list 0x84 for the various Kaby Lake / Coffee Lake parts, as
    that appeared in one version of the blacklist and then reverted to
    0x80 again. We can change it if 0x84 is actually announced to be safe.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan.van.de.ven@intel.com
    Cc: jmattson@google.com
    Cc: karahmed@amazon.de
    Cc: kvm@vger.kernel.org
    Cc: pbonzini@redhat.com
    Cc: rkrcmar@redhat.com
    Cc: sironi@amazon.de
    Link: http://lkml.kernel.org/r/1518305967-31356-2-git-send-email-dwmw@amazon.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 319bf989fad1..f73b8148dd55 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -123,8 +123,6 @@ static const struct sku_microcode spectre_bad_microcodes[] = {
 	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x84 },
 	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
-	{ INTEL_FAM6_SKYLAKE_MOBILE,	0x03,	0xc2 },
-	{ INTEL_FAM6_SKYLAKE_DESKTOP,	0x03,	0xc2 },
 	{ INTEL_FAM6_BROADWELL_CORE,	0x04,	0x28 },
 	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },
 	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },
@@ -136,8 +134,6 @@ static const struct sku_microcode spectre_bad_microcodes[] = {
 	{ INTEL_FAM6_HASWELL_X,		0x02,	0x3b },
 	{ INTEL_FAM6_HASWELL_X,		0x04,	0x10 },
 	{ INTEL_FAM6_IVYBRIDGE_X,	0x04,	0x42a },
-	/* Updated in the 20180108 release; blacklist until we know otherwise */
-	{ INTEL_FAM6_ATOM_GEMINI_LAKE,	0x01,	0x22 },
 	/* Observed in the wild */
 	{ INTEL_FAM6_SANDYBRIDGE_X,	0x06,	0x61b },
 	{ INTEL_FAM6_SANDYBRIDGE_X,	0x07,	0x712 },

commit 7fcae1118f5fd44a862aa5c3525248e35ee67c3b
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Tue Jan 30 14:30:23 2018 +0000

    x86/cpuid: Fix up "virtual" IBRS/IBPB/STIBP feature bits on Intel
    
    Despite the fact that all the other code there seems to be doing it, just
    using set_cpu_cap() in early_intel_init() doesn't actually work.
    
    For CPUs with PKU support, setup_pku() calls get_cpu_cap() after
    c->c_init() has set those feature bits. That resets those bits back to what
    was queried from the hardware.
    
    Turning the bits off for bad microcode is easy to fix. That can just use
    setup_clear_cpu_cap() to force them off for all CPUs.
    
    I was less keen on forcing the feature bits *on* that way, just in case
    of inconsistencies. I appreciate that the kernel is going to get this
    utterly wrong if CPU features are not consistent, because it has already
    applied alternatives by the time secondary CPUs are brought up.
    
    But at least if setup_force_cpu_cap() isn't being used, we might have a
    chance of *detecting* the lack of the corresponding bit and either
    panicking or refusing to bring the offending CPU online.
    
    So ensure that the appropriate feature bits are set within get_cpu_cap()
    regardless of how many extra times it's called.
    
    Fixes: 2961298e ("x86/cpufeatures: Clean up Spectre v2 related CPUID flags")
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: karahmed@amazon.de
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Link: https://lkml.kernel.org/r/1517322623-15261-1-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 6936d14d4c77..319bf989fad1 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -175,28 +175,17 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64))
 		c->microcode = intel_get_microcode_revision();
 
-	/*
-	 * The Intel SPEC_CTRL CPUID bit implies IBRS and IBPB support,
-	 * and they also have a different bit for STIBP support. Also,
-	 * a hypervisor might have set the individual AMD bits even on
-	 * Intel CPUs, for finer-grained selection of what's available.
-	 */
-	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {
-		set_cpu_cap(c, X86_FEATURE_IBRS);
-		set_cpu_cap(c, X86_FEATURE_IBPB);
-	}
-	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
-		set_cpu_cap(c, X86_FEATURE_STIBP);
-
 	/* Now if any of them are set, check the blacklist and clear the lot */
-	if ((cpu_has(c, X86_FEATURE_IBRS) || cpu_has(c, X86_FEATURE_IBPB) ||
+	if ((cpu_has(c, X86_FEATURE_SPEC_CTRL) ||
+	     cpu_has(c, X86_FEATURE_INTEL_STIBP) ||
+	     cpu_has(c, X86_FEATURE_IBRS) || cpu_has(c, X86_FEATURE_IBPB) ||
 	     cpu_has(c, X86_FEATURE_STIBP)) && bad_spectre_microcode(c)) {
 		pr_warn("Intel Spectre v2 broken microcode detected; disabling Speculation Control\n");
-		clear_cpu_cap(c, X86_FEATURE_IBRS);
-		clear_cpu_cap(c, X86_FEATURE_IBPB);
-		clear_cpu_cap(c, X86_FEATURE_STIBP);
-		clear_cpu_cap(c, X86_FEATURE_SPEC_CTRL);
-		clear_cpu_cap(c, X86_FEATURE_INTEL_STIBP);
+		setup_clear_cpu_cap(X86_FEATURE_IBRS);
+		setup_clear_cpu_cap(X86_FEATURE_IBPB);
+		setup_clear_cpu_cap(X86_FEATURE_STIBP);
+		setup_clear_cpu_cap(X86_FEATURE_SPEC_CTRL);
+		setup_clear_cpu_cap(X86_FEATURE_INTEL_STIBP);
 	}
 
 	/*

commit 7e86548e2cc8d308cb75439480f428137151b0de
Merge: 64e16720ea08 d8a5b80568a9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jan 30 15:08:27 2018 +0100

    Merge tag 'v4.15' into x86/pti, to be able to merge dependent changes
    
    Time has come to switch PTI development over to a v4.15 base - we'll still
    try to make sure that all PTI fixes backport cleanly to v4.14 and earlier.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2961298efe1ea1b6fc0d7ee8b76018fa6c0bcef2
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Sat Jan 27 16:24:32 2018 +0000

    x86/cpufeatures: Clean up Spectre v2 related CPUID flags
    
    We want to expose the hardware features simply in /proc/cpuinfo as "ibrs",
    "ibpb" and "stibp". Since AMD has separate CPUID bits for those, use them
    as the user-visible bits.
    
    When the Intel SPEC_CTRL bit is set which indicates both IBRS and IBPB
    capability, set those (AMD) bits accordingly. Likewise if the Intel STIBP
    bit is set, set the AMD STIBP that's used for the generic hardware
    capability.
    
    Hide the rest from /proc/cpuinfo by putting "" in the comments. Including
    RETPOLINE and RETPOLINE_AMD which shouldn't be visible there. There are
    patches to make the sysfs vulnerabilities information non-readable by
    non-root, and the same should apply to all information about which
    mitigations are actually in use. Those *shouldn't* appear in /proc/cpuinfo.
    
    The feature bit for whether IBPB is actually used, which is needed for
    ALTERNATIVEs, is renamed to X86_FEATURE_USE_IBPB.
    
    Originally-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ak@linux.intel.com
    Cc: dave.hansen@intel.com
    Cc: karahmed@amazon.de
    Cc: arjan@linux.intel.com
    Cc: torvalds@linux-foundation.org
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Cc: pbonzini@redhat.com
    Cc: tim.c.chen@linux.intel.com
    Cc: gregkh@linux-foundation.org
    Link: https://lkml.kernel.org/r/1517070274-12128-2-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 5faa487d0477..0c8b916abced 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -175,17 +175,28 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64))
 		c->microcode = intel_get_microcode_revision();
 
-	if ((cpu_has(c, X86_FEATURE_SPEC_CTRL) ||
-	     cpu_has(c, X86_FEATURE_STIBP) ||
-	     cpu_has(c, X86_FEATURE_AMD_SPEC_CTRL) ||
-	     cpu_has(c, X86_FEATURE_AMD_PRED_CMD) ||
-	     cpu_has(c, X86_FEATURE_AMD_STIBP)) && bad_spectre_microcode(c)) {
-		pr_warn("Intel Spectre v2 broken microcode detected; disabling SPEC_CTRL\n");
-		clear_cpu_cap(c, X86_FEATURE_SPEC_CTRL);
+	/*
+	 * The Intel SPEC_CTRL CPUID bit implies IBRS and IBPB support,
+	 * and they also have a different bit for STIBP support. Also,
+	 * a hypervisor might have set the individual AMD bits even on
+	 * Intel CPUs, for finer-grained selection of what's available.
+	 */
+	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {
+		set_cpu_cap(c, X86_FEATURE_IBRS);
+		set_cpu_cap(c, X86_FEATURE_IBPB);
+	}
+	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
+		set_cpu_cap(c, X86_FEATURE_STIBP);
+
+	/* Now if any of them are set, check the blacklist and clear the lot */
+	if ((cpu_has(c, X86_FEATURE_IBRS) || cpu_has(c, X86_FEATURE_IBPB) ||
+	     cpu_has(c, X86_FEATURE_STIBP)) && bad_spectre_microcode(c)) {
+		pr_warn("Intel Spectre v2 broken microcode detected; disabling Speculation Control\n");
+		clear_cpu_cap(c, X86_FEATURE_IBRS);
+		clear_cpu_cap(c, X86_FEATURE_IBPB);
 		clear_cpu_cap(c, X86_FEATURE_STIBP);
-		clear_cpu_cap(c, X86_FEATURE_AMD_SPEC_CTRL);
-		clear_cpu_cap(c, X86_FEATURE_AMD_PRED_CMD);
-		clear_cpu_cap(c, X86_FEATURE_AMD_STIBP);
+		clear_cpu_cap(c, X86_FEATURE_SPEC_CTRL);
+		clear_cpu_cap(c, X86_FEATURE_INTEL_STIBP);
 	}
 
 	/*

commit a5b2966364538a0e68c9fa29bc0a3a1651799035
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Thu Jan 25 16:14:14 2018 +0000

    x86/cpufeature: Blacklist SPEC_CTRL/PRED_CMD on early Spectre v2 microcodes
    
    This doesn't refuse to load the affected microcodes; it just refuses to
    use the Spectre v2 mitigation features if they're detected, by clearing
    the appropriate feature bits.
    
    The AMD CPUID bits are handled here too, because hypervisors *may* have
    been exposing those bits even on Intel chips, for fine-grained control
    of what's available.
    
    It is non-trivial to use x86_match_cpu() for this table because that
    doesn't handle steppings. And the approach taken in commit bd9240a18
    almost made me lose my lunch.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: ak@linux.intel.com
    Cc: ashok.raj@intel.com
    Cc: dave.hansen@intel.com
    Cc: karahmed@amazon.de
    Cc: arjan@linux.intel.com
    Cc: torvalds@linux-foundation.org
    Cc: peterz@infradead.org
    Cc: bp@alien8.de
    Cc: pbonzini@redhat.com
    Cc: tim.c.chen@linux.intel.com
    Cc: gregkh@linux-foundation.org
    Link: https://lkml.kernel.org/r/1516896855-7642-7-git-send-email-dwmw@amazon.co.uk

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index b720dacac051..5faa487d0477 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -102,6 +102,59 @@ static void probe_xeon_phi_r3mwait(struct cpuinfo_x86 *c)
 		ELF_HWCAP2 |= HWCAP2_RING3MWAIT;
 }
 
+/*
+ * Early microcode releases for the Spectre v2 mitigation were broken.
+ * Information taken from;
+ * - https://newsroom.intel.com/wp-content/uploads/sites/11/2018/01/microcode-update-guidance.pdf
+ * - https://kb.vmware.com/s/article/52345
+ * - Microcode revisions observed in the wild
+ * - Release note from 20180108 microcode release
+ */
+struct sku_microcode {
+	u8 model;
+	u8 stepping;
+	u32 microcode;
+};
+static const struct sku_microcode spectre_bad_microcodes[] = {
+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0B,	0x84 },
+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0A,	0x84 },
+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x09,	0x84 },
+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x84 },
+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x84 },
+	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
+	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
+	{ INTEL_FAM6_SKYLAKE_MOBILE,	0x03,	0xc2 },
+	{ INTEL_FAM6_SKYLAKE_DESKTOP,	0x03,	0xc2 },
+	{ INTEL_FAM6_BROADWELL_CORE,	0x04,	0x28 },
+	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },
+	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },
+	{ INTEL_FAM6_BROADWELL_XEON_D,	0x03,	0x07000011 },
+	{ INTEL_FAM6_BROADWELL_X,	0x01,	0x0b000025 },
+	{ INTEL_FAM6_HASWELL_ULT,	0x01,	0x21 },
+	{ INTEL_FAM6_HASWELL_GT3E,	0x01,	0x18 },
+	{ INTEL_FAM6_HASWELL_CORE,	0x03,	0x23 },
+	{ INTEL_FAM6_HASWELL_X,		0x02,	0x3b },
+	{ INTEL_FAM6_HASWELL_X,		0x04,	0x10 },
+	{ INTEL_FAM6_IVYBRIDGE_X,	0x04,	0x42a },
+	/* Updated in the 20180108 release; blacklist until we know otherwise */
+	{ INTEL_FAM6_ATOM_GEMINI_LAKE,	0x01,	0x22 },
+	/* Observed in the wild */
+	{ INTEL_FAM6_SANDYBRIDGE_X,	0x06,	0x61b },
+	{ INTEL_FAM6_SANDYBRIDGE_X,	0x07,	0x712 },
+};
+
+static bool bad_spectre_microcode(struct cpuinfo_x86 *c)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(spectre_bad_microcodes); i++) {
+		if (c->x86_model == spectre_bad_microcodes[i].model &&
+		    c->x86_mask == spectre_bad_microcodes[i].stepping)
+			return (c->microcode <= spectre_bad_microcodes[i].microcode);
+	}
+	return false;
+}
+
 static void early_init_intel(struct cpuinfo_x86 *c)
 {
 	u64 misc_enable;
@@ -122,6 +175,19 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64))
 		c->microcode = intel_get_microcode_revision();
 
+	if ((cpu_has(c, X86_FEATURE_SPEC_CTRL) ||
+	     cpu_has(c, X86_FEATURE_STIBP) ||
+	     cpu_has(c, X86_FEATURE_AMD_SPEC_CTRL) ||
+	     cpu_has(c, X86_FEATURE_AMD_PRED_CMD) ||
+	     cpu_has(c, X86_FEATURE_AMD_STIBP)) && bad_spectre_microcode(c)) {
+		pr_warn("Intel Spectre v2 broken microcode detected; disabling SPEC_CTRL\n");
+		clear_cpu_cap(c, X86_FEATURE_SPEC_CTRL);
+		clear_cpu_cap(c, X86_FEATURE_STIBP);
+		clear_cpu_cap(c, X86_FEATURE_AMD_SPEC_CTRL);
+		clear_cpu_cap(c, X86_FEATURE_AMD_PRED_CMD);
+		clear_cpu_cap(c, X86_FEATURE_AMD_STIBP);
+	}
+
 	/*
 	 * Atom erratum AAE44/AAF40/AAG38/AAH41:
 	 *

commit 4675ff05de2d76d167336b368bd07f3fef6ed5a6
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:36:02 2017 -0800

    kmemcheck: rip it out
    
    Fix up makefiles, remove references, and git rm kmemcheck.
    
    Link: http://lkml.kernel.org/r/20171007030159.22241-4-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index b720dacac051..b1af22073e28 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -187,21 +187,6 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86 == 6 && c->x86_model < 15)
 		clear_cpu_cap(c, X86_FEATURE_PAT);
 
-#ifdef CONFIG_KMEMCHECK
-	/*
-	 * P4s have a "fast strings" feature which causes single-
-	 * stepping REP instructions to only generate a #DB on
-	 * cache-line boundaries.
-	 *
-	 * Ingo Molnar reported a Pentium D (model 6) and a Xeon
-	 * (model 2) with the same problem.
-	 */
-	if (c->x86 == 15)
-		if (msr_clear_bit(MSR_IA32_MISC_ENABLE,
-				  MSR_IA32_MISC_ENABLE_FAST_STRING_BIT) > 0)
-			pr_info("kmemcheck: Disabling fast string operations\n");
-#endif
-
 	/*
 	 * If fast string is not enabled in IA32_MISC_ENABLE for any reason,
 	 * clear the fast string and enhanced fast string CPU capabilities.

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index dfa90a3a5145..b720dacac051 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include <linux/kernel.h>
 
 #include <linux/string.h>

commit e9ea1e7f53b852147cbd568b0568c7ad97ec21a3
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:26 2017 -0700

    x86/arch_prctl: Add ARCH_[GET|SET]_CPUID
    
    Intel supports faulting on the CPUID instruction beginning with Ivy Bridge.
    When enabled, the processor will fault on attempts to execute the CPUID
    instruction with CPL>0. Exposing this feature to userspace will allow a
    ptracer to trap and emulate the CPUID instruction.
    
    When supported, this feature is controlled by toggling bit 0 of
    MSR_MISC_FEATURES_ENABLES. It is documented in detail in Section 2.3.2 of
    https://bugzilla.kernel.org/attachment.cgi?id=243991
    
    Implement a new pair of arch_prctls, available on both x86-32 and x86-64.
    
    ARCH_GET_CPUID: Returns the current CPUID state, either 0 if CPUID faulting
        is enabled (and thus the CPUID instruction is not available) or 1 if
        CPUID faulting is not enabled.
    
    ARCH_SET_CPUID: Set the CPUID state to the second argument. If
        cpuid_enabled is 0 CPUID faulting will be activated, otherwise it will
        be deactivated. Returns ENODEV if CPUID faulting is not supported on
        this system.
    
    The state of the CPUID faulting flag is propagated across forks, but reset
    upon exec.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-9-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a07f8295c9ed..dfa90a3a5145 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -90,16 +90,12 @@ static void probe_xeon_phi_r3mwait(struct cpuinfo_x86 *c)
 		return;
 	}
 
-	if (ring3mwait_disabled) {
-		msr_clear_bit(MSR_MISC_FEATURES_ENABLES,
-			      MSR_MISC_FEATURES_ENABLES_RING3MWAIT_BIT);
+	if (ring3mwait_disabled)
 		return;
-	}
-
-	msr_set_bit(MSR_MISC_FEATURES_ENABLES,
-		    MSR_MISC_FEATURES_ENABLES_RING3MWAIT_BIT);
 
 	set_cpu_cap(c, X86_FEATURE_RING3MWAIT);
+	this_cpu_or(msr_misc_features_shadow,
+		    1UL << MSR_MISC_FEATURES_ENABLES_RING3MWAIT_BIT);
 
 	if (c == &boot_cpu_data)
 		ELF_HWCAP2 |= HWCAP2_RING3MWAIT;
@@ -505,9 +501,15 @@ static void init_intel_misc_features(struct cpuinfo_x86 *c)
 	if (rdmsrl_safe(MSR_MISC_FEATURES_ENABLES, &msr))
 		return;
 
-	/* Check features and update capabilities */
+	/* Clear all MISC features */
+	this_cpu_write(msr_misc_features_shadow, 0);
+
+	/* Check features and update capabilities and shadow control bits */
 	init_cpuid_fault(c);
 	probe_xeon_phi_r3mwait(c);
+
+	msr = this_cpu_read(msr_misc_features_shadow);
+	wrmsrl(MSR_MISC_FEATURES_ENABLES, msr);
 }
 
 static void init_intel(struct cpuinfo_x86 *c)

commit 90218ac77d0582eaf2d0872d8d900cbd5bf1f205
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:25 2017 -0700

    x86/cpufeature: Detect CPUID faulting support
    
    Intel supports faulting on the CPUID instruction beginning with Ivy Bridge.
    When enabled, the processor will fault on attempts to execute the CPUID
    instruction with CPL>0. This will allow a ptracer to emulate the CPUID
    instruction.
    
    Bit 31 of MSR_PLATFORM_INFO advertises support for this feature. It is
    documented in detail in Section 2.3.2 of
    https://bugzilla.kernel.org/attachment.cgi?id=243991
    
    Detect support for this feature and expose it as X86_FEATURE_CPUID_FAULT.
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-8-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index e229318d7230..a07f8295c9ed 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -488,6 +488,28 @@ static void intel_bsp_resume(struct cpuinfo_x86 *c)
 	init_intel_energy_perf(c);
 }
 
+static void init_cpuid_fault(struct cpuinfo_x86 *c)
+{
+	u64 msr;
+
+	if (!rdmsrl_safe(MSR_PLATFORM_INFO, &msr)) {
+		if (msr & MSR_PLATFORM_INFO_CPUID_FAULT)
+			set_cpu_cap(c, X86_FEATURE_CPUID_FAULT);
+	}
+}
+
+static void init_intel_misc_features(struct cpuinfo_x86 *c)
+{
+	u64 msr;
+
+	if (rdmsrl_safe(MSR_MISC_FEATURES_ENABLES, &msr))
+		return;
+
+	/* Check features and update capabilities */
+	init_cpuid_fault(c);
+	probe_xeon_phi_r3mwait(c);
+}
+
 static void init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int l2 = 0;
@@ -602,7 +624,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 
 	init_intel_energy_perf(c);
 
-	probe_xeon_phi_r3mwait(c);
+	init_intel_misc_features(c);
 }
 
 #ifdef CONFIG_X86_32

commit ab6d9468631a6e56e4c071c6ce6710956485fe08
Author: Kyle Huey <me@kylehuey.com>
Date:   Mon Mar 20 01:16:19 2017 -0700

    x86/msr: Rename MISC_FEATURE_ENABLES to MISC_FEATURES_ENABLES
    
    This matches the only public Intel documentation of this MSR, in the
    "Virtualization Technology FlexMigration Application Note"
    (preserved at https://bugzilla.kernel.org/attachment.cgi?id=243991)
    
    Signed-off-by: Kyle Huey <khuey@kylehuey.com>
    Cc: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: kvm@vger.kernel.org
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: linux-kselftest@vger.kernel.org
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Robert O'Callahan <robert@ocallahan.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: user-mode-linux-user@lists.sourceforge.net
    Cc: David Matlack <dmatlack@google.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170320081628.18952-2-khuey@kylehuey.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 063197771b8d..e229318d7230 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -91,13 +91,13 @@ static void probe_xeon_phi_r3mwait(struct cpuinfo_x86 *c)
 	}
 
 	if (ring3mwait_disabled) {
-		msr_clear_bit(MSR_MISC_FEATURE_ENABLES,
-			      MSR_MISC_FEATURE_ENABLES_RING3MWAIT_BIT);
+		msr_clear_bit(MSR_MISC_FEATURES_ENABLES,
+			      MSR_MISC_FEATURES_ENABLES_RING3MWAIT_BIT);
 		return;
 	}
 
-	msr_set_bit(MSR_MISC_FEATURE_ENABLES,
-		    MSR_MISC_FEATURE_ENABLES_RING3MWAIT_BIT);
+	msr_set_bit(MSR_MISC_FEATURES_ENABLES,
+		    MSR_MISC_FEATURES_ENABLES_RING3MWAIT_BIT);
 
 	set_cpu_cap(c, X86_FEATURE_RING3MWAIT);
 

commit 609b07b72d3caaa8eed3a238886467946b78fa5e
Merge: c3abcabe813b f94c8d116997
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 7 14:42:34 2017 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "A fix for KVM's scheduler clock which (erroneously) was always marked
      unstable, a fix for RT/DL load balancing, plus latency fixes"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
      sched/core: Fix pick_next_task() for RT,DL
      sched/fair: Make select_idle_cpu() more aggressive

commit f94c8d116997597fc00f0812b0ab9256e7b0c58f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 1 15:53:38 2017 +0100

    sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
    
    Wanpeng Li reported that since the following commit:
    
      acb04058de49 ("sched/clock: Fix hotplug crash")
    
    ... KVM always runs with unstable sched-clock even though KVM's
    kvm_clock _is_ stable.
    
    The problem is that we've tied clear_sched_clock_stable() to the TSC
    state, and overlooked that sched_clock() is a paravirt function.
    
    Solve this by doing two things:
    
     - tie the sched_clock() stable state more clearly to the TSC stable
       state for the normal (!paravirt) case.
    
     - only call clear_sched_clock_stable() when we mark TSC unstable
       when we use native_sched_clock().
    
    The first means we can actually run with stable sched_clock in more
    situations then before, which is good. And since commit:
    
      12907fbb1a69 ("sched/clock, clocksource: Add optional cs::mark_unstable() method")
    
    ... this should be reliable. Since any detection of TSC fail now results
    in marking the TSC unstable.
    
    Reported-by: Wanpeng Li <kernellwp@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Fixes: acb04058de49 ("sched/clock: Fix hotplug crash")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 017ecd3bb553..2388bafe5c37 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -161,10 +161,6 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
-		if (check_tsc_unstable())
-			clear_sched_clock_stable();
-	} else {
-		clear_sched_clock_stable();
 	}
 
 	/* Penwell and Cloverview have the TSC which doesn't sleep on S3 */

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 017ecd3bb553..fe0a615a051b 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -4,6 +4,7 @@
 #include <linux/bitops.h>
 #include <linux/smp.h>
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 #include <linux/thread_info.h>
 #include <linux/init.h>
 #include <linux/uaccess.h>

commit 8a9365a4725a4d6265a416dd63bff937e300308e
Merge: 2891e8e66787 3bba73b1b7a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 14:37:08 2017 -0800

    Merge branch 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpufeature updates from Ingo Molnar:
     "The main changes in this cycle were related to enable ring-3
      MONITOR/MWAIT instructions support on supported CPUs, by Grzegorz
      Andrejczuk and Piotr Luc"
    
    * 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpufeature: Move RING3MWAIT feature to avoid conflicts
      x86/cpufeature: Enable RING3MWAIT for Knights Mill
      x86/cpufeature: Enable RING3MWAIT for Knights Landing
      x86/cpufeature: Add RING3MWAIT to CPU features
      x86/elf: Add HWCAP2 to expose ring 3 MONITOR/MWAIT
      x86/msr: Add MSR_MISC_FEATURE_ENABLES and RING3MWAIT bit
      x86/cpufeature: Add AVX512_VPOPCNTDQ feature

commit 4d8bb00604b182b62e7786bae0e58e0befeeff85
Author: Piotr Luc <piotr.luc@intel.com>
Date:   Fri Jan 20 14:22:37 2017 +0100

    x86/cpufeature: Enable RING3MWAIT for Knights Mill
    
    Enable ring 3 MONITOR/MWAIT for Intel Xeon Phi codenamed Knights Mill. We
    can't guarantee that this (KNM) will be the last CPU model that needs this
    hack.  But, we do recognize that this is far from optimal, and there is an
    effort to ensure we don't keep doing extending this hack forever.
    
    Signed-off-by: Piotr Luc <piotr.luc@intel.com>
    Cc: Piotr.Luc@intel.com
    Cc: dave.hansen@linux.intel.com
    Link: http://lkml.kernel.org/r/1484918557-15481-6-git-send-email-grzegorz.andrejczuk@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index da2401a4b0f4..a4c4ff9b27e4 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -79,8 +79,15 @@ static void probe_xeon_phi_r3mwait(struct cpuinfo_x86 *c)
 	 * Ring 3 MONITOR/MWAIT feature cannot be detected without
 	 * cpu model and family comparison.
 	 */
-	if (c->x86 != 6 || c->x86_model != INTEL_FAM6_XEON_PHI_KNL)
+	if (c->x86 != 6)
 		return;
+	switch (c->x86_model) {
+	case INTEL_FAM6_XEON_PHI_KNL:
+	case INTEL_FAM6_XEON_PHI_KNM:
+		break;
+	default:
+		return;
+	}
 
 	if (ring3mwait_disabled) {
 		msr_clear_bit(MSR_MISC_FEATURE_ENABLES,

commit e16fd002afe2b16d828bbf738b8a81a185fe9272
Author: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
Date:   Fri Jan 20 14:22:36 2017 +0100

    x86/cpufeature: Enable RING3MWAIT for Knights Landing
    
    Enable ring 3 MONITOR/MWAIT for Intel Xeon Phi x200 codenamed Knights
    Landing.
    
    Presence of this feature cannot be detected automatically (by reading any
    other MSR) therefore it is required to explicitly check for the family and
    model of the CPU before attempting to enable it.
    
    Signed-off-by: Grzegorz Andrejczuk <grzegorz.andrejczuk@intel.com>
    Cc: Piotr.Luc@intel.com
    Cc: dave.hansen@linux.intel.com
    Link: http://lkml.kernel.org/r/1484918557-15481-5-git-send-email-grzegorz.andrejczuk@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 203f860d2ab3..da2401a4b0f4 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -15,6 +15,8 @@
 #include <asm/cpu.h>
 #include <asm/intel-family.h>
 #include <asm/microcode_intel.h>
+#include <asm/hwcap2.h>
+#include <asm/elf.h>
 
 #ifdef CONFIG_X86_64
 #include <linux/topology.h>
@@ -62,6 +64,39 @@ void check_mpx_erratum(struct cpuinfo_x86 *c)
 	}
 }
 
+static bool ring3mwait_disabled __read_mostly;
+
+static int __init ring3mwait_disable(char *__unused)
+{
+	ring3mwait_disabled = true;
+	return 0;
+}
+__setup("ring3mwait=disable", ring3mwait_disable);
+
+static void probe_xeon_phi_r3mwait(struct cpuinfo_x86 *c)
+{
+	/*
+	 * Ring 3 MONITOR/MWAIT feature cannot be detected without
+	 * cpu model and family comparison.
+	 */
+	if (c->x86 != 6 || c->x86_model != INTEL_FAM6_XEON_PHI_KNL)
+		return;
+
+	if (ring3mwait_disabled) {
+		msr_clear_bit(MSR_MISC_FEATURE_ENABLES,
+			      MSR_MISC_FEATURE_ENABLES_RING3MWAIT_BIT);
+		return;
+	}
+
+	msr_set_bit(MSR_MISC_FEATURE_ENABLES,
+		    MSR_MISC_FEATURE_ENABLES_RING3MWAIT_BIT);
+
+	set_cpu_cap(c, X86_FEATURE_RING3MWAIT);
+
+	if (c == &boot_cpu_data)
+		ELF_HWCAP2 |= HWCAP2_RING3MWAIT;
+}
+
 static void early_init_intel(struct cpuinfo_x86 *c)
 {
 	u64 misc_enable;
@@ -560,6 +595,8 @@ static void init_intel(struct cpuinfo_x86 *c)
 		detect_vmx_virtcap(c);
 
 	init_intel_energy_perf(c);
+
+	probe_xeon_phi_r3mwait(c);
 }
 
 #ifdef CONFIG_X86_32

commit ed5c8c854f2b990dfa4d85c4995d115768a05d3c
Merge: 619bd4a71874 a2ca3d617944
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 09:12:25 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes and refresh the branch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit acb04058de49458010c44bb35b849d45113fd668
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 19 14:36:33 2017 +0100

    sched/clock: Fix hotplug crash
    
    Mike reported that he could trigger the WARN_ON_ONCE() in
    set_sched_clock_stable() using hotplug.
    
    This exposed a fundamental problem with the interface, we should never
    mark the TSC stable if we ever find it to be unstable. Therefore
    set_sched_clock_stable() is a broken interface.
    
    The reason it existed is that not having it is a pain, it means all
    relevant architecture code needs to call clear_sched_clock_stable()
    where appropriate.
    
    Of the three architectures that select HAVE_UNSTABLE_SCHED_CLOCK ia64
    and parisc are trivial in that they never called
    set_sched_clock_stable(), so add an unconditional call to
    clear_sched_clock_stable() to them.
    
    For x86 the story is a lot more involved, and what this patch tries to
    do is ensure we preserve the status quo. So even is Cyrix or Transmeta
    have usable TSC they never called set_sched_clock_stable() so they now
    get an explicit mark unstable.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 9881b024b7d7 ("sched/clock: Delay switching sched_clock to stable")
    Link: http://lkml.kernel.org/r/20170119133633.GB6536@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index fcd484d2bb03..26eaff4907b2 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -124,8 +124,10 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
-		if (!check_tsc_unstable())
-			set_sched_clock_stable();
+		if (check_tsc_unstable())
+			clear_sched_clock_stable();
+	} else {
+		clear_sched_clock_stable();
 	}
 
 	/* Penwell and Cloverview have the TSC which doesn't sleep on S3 */

commit 4167709bbf826512a52ebd6aafda2be104adaec9
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Jan 9 12:41:45 2017 +0100

    x86/microcode/intel: Add a helper which gives the microcode revision
    
    Since on Intel we're required to do CPUID(1) first, before reading
    the microcode revision MSR, let's add a special helper which does the
    required steps so that we don't forget to do them next time, when we
    want to read the microcode revision.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/20170109114147.5082-4-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 2d49aa949fa1..203f860d2ab3 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -14,6 +14,7 @@
 #include <asm/bugs.h>
 #include <asm/cpu.h>
 #include <asm/intel-family.h>
+#include <asm/microcode_intel.h>
 
 #ifdef CONFIG_X86_64
 #include <linux/topology.h>
@@ -78,14 +79,8 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		(c->x86 == 0x6 && c->x86_model >= 0x0e))
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 
-	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64)) {
-		unsigned lower_word;
-
-		wrmsr(MSR_IA32_UCODE_REV, 0, 0);
-		/* Required by the SDM */
-		native_cpuid_eax(1);
-		rdmsr(MSR_IA32_UCODE_REV, lower_word, c->microcode);
-	}
+	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64))
+		c->microcode = intel_get_microcode_revision();
 
 	/*
 	 * Atom erratum AAE44/AAF40/AAG38/AAH41:

commit f3e2a51f568d9f33370f4e8bb05669a34223241a
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Jan 9 12:41:44 2017 +0100

    x86/microcode: Use native CPUID to tickle out microcode revision
    
    Intel supplies the microcode revision value in MSR 0x8b
    (IA32_BIOS_SIGN_ID) after CPUID(1) has been executed. Execute it each
    time before reading that MSR.
    
    It used to do sync_core() which did do CPUID but
    
      c198b121b1a1 ("x86/asm: Rewrite sync_core() to use IRET-to-self")
    
    changed the sync_core() implementation so we better make the microcode
    loading case explicit, as the SDM documents it.
    
    Reported-and-tested-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/20170109114147.5082-3-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index fcd484d2bb03..2d49aa949fa1 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -83,7 +83,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 
 		wrmsr(MSR_IA32_UCODE_REV, 0, 0);
 		/* Required by the SDM */
-		sync_core();
+		native_cpuid_eax(1);
 		rdmsr(MSR_IA32_UCODE_REV, lower_word, c->microcode);
 	}
 

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit 08e237fa56a1d95c1372033bc29c4a2517b3c0fa
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jul 18 11:41:10 2016 -0700

    x86/cpu: Add workaround for MONITOR instruction erratum on Goldmont based CPUs
    
    Monitored cached line may not wake up from mwait on certain
    Goldmont based CPUs. This patch will avoid calling
    current_set_polling_and_test() and thereby not set the TIF_ flag.
    The result is that we'll always send IPIs for wakeups.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1468867270-18493-1-git-send-email-jacob.jun.pan@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index c1a89bc026ac..abf601235b29 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -13,6 +13,7 @@
 #include <asm/msr.h>
 #include <asm/bugs.h>
 #include <asm/cpu.h>
+#include <asm/intel-family.h>
 
 #ifdef CONFIG_X86_64
 #include <linux/topology.h>
@@ -508,6 +509,10 @@ static void init_intel(struct cpuinfo_x86 *c)
 	    (c->x86_model == 29 || c->x86_model == 46 || c->x86_model == 47))
 		set_cpu_bug(c, X86_BUG_CLFLUSH_MONITOR);
 
+	if (c->x86 == 6 && boot_cpu_has(X86_FEATURE_MWAIT) &&
+		((c->x86_model == INTEL_FAM6_ATOM_GOLDMONT)))
+		set_cpu_bug(c, X86_BUG_MONITOR);
+
 #ifdef CONFIG_X86_64
 	if (c->x86 == 15)
 		c->x86_cache_alignment = c->x86_clflush_size * 2;

commit 186f43608a5c827f8284fe4559225b4dccaa49ef
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jul 13 20:18:56 2016 -0400

    x86/kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.  Build testing
    revealed some implicit header usage that was fixed up accordingly.
    
    Note that some bool/obj-y instances remain since module.h is
    the header for some exception table entry stuff, and for things
    like __init_or_module (code that is tossed when MODULES=n).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160714001901.31603-4-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index c1a89bc026ac..f380b61d9a7a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -5,7 +5,7 @@
 #include <linux/smp.h>
 #include <linux/sched.h>
 #include <linux/thread_info.h>
-#include <linux/module.h>
+#include <linux/init.h>
 #include <linux/uaccess.h>
 
 #include <asm/cpufeature.h>

commit 8e8c668927b029f6ccc350eb1aa936864cc4eb6f
Merge: f0133acc7d48 970442c599b2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jun 8 13:02:16 2016 +0200

    Merge branch 'x86/urgent' into x86/cpu, to pick up dependency
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 06cd3d8c14bdd06f49f1c6a06acf219749c5598e
Merge: 0f6ff2bce0d4 bc231d9ede99
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri May 20 09:09:26 2016 +0200

    Merge branch 'linus' into x86/urgent, to refresh the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0f6ff2bce0d4c3e4ff34f5d2ffb7329025b30844
Author: Dave Hansen <dave@sr71.net>
Date:   Thu May 12 15:04:00 2016 -0700

    x86/mm/mpx: Work around MPX erratum SKD046
    
    This erratum essentially causes the CPU to forget which privilege
    level it is operating on (kernel vs. user) for the purposes of MPX.
    
    This erratum can only be triggered when a system is not using
    Supervisor Mode Execution Prevention (SMEP).  Our workaround for
    the erratum is to ensure that MPX can only be used in cases where
    SMEP is present in the processor and is enabled.
    
    This erratum only affects Core processors.  Atom is unaffected.
    But, there is no architectural way to determine Atom vs. Core.
    So, we just apply this workaround to all processors.  It's
    possible that it will mistakenly disable MPX on some Atom
    processsors or future unaffected Core processors.  There are
    currently no processors that have MPX and not SMEP.  It would
    take something akin to a hypervisor masking SMEP out on an Atom
    processor for this to present itself on current hardware.
    
    More details can be found at:
    
      http://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/desktop-6th-gen-core-family-spec-update.pdf
    
    "
      SKD046 Branch Instructions May Initialize MPX Bound Registers Incorrectly
    
      Problem:
    
      Depending on the current Intel MPX (Memory Protection
      Extensions) configuration, execution of certain branch
      instructions (near CALL, near RET, near JMP, and Jcc
      instructions) without a BND prefix (F2H) initialize the MPX bound
      registers. Due to this erratum, such a branch instruction that is
      executed both with CPL = 3 and with CPL < 3 may not use the
      correct MPX configuration register (BNDCFGU or BNDCFGS,
      respectively) for determining whether to initialize the bound
      registers; it may thus initialize the bound registers when it
      should not, or fail to initialize them when it should.
    
      Implication:
    
      A branch instruction that has executed both in user mode and in
      supervisor mode (from the same linear address) may cause a #BR
      (bound range fault) when it should not have or may not cause a
      #BR when it should have.  Workaround An operating system can
      avoid this erratum by setting CR4.SMEP[bit 20] to enable
      supervisor-mode execution prevention (SMEP). When SMEP is
      enabled, no code can be executed both with CPL = 3 and with CPL < 3.
    "
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160512220400.3B35F1BC@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index e4393bfc7f0d..b47df99dc5d2 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -25,6 +25,41 @@
 #include <asm/apic.h>
 #endif
 
+/*
+ * Just in case our CPU detection goes bad, or you have a weird system,
+ * allow a way to override the automatic disabling of MPX.
+ */
+static int forcempx;
+
+static int __init forcempx_setup(char *__unused)
+{
+	forcempx = 1;
+
+	return 1;
+}
+__setup("intel-skd-046-workaround=disable", forcempx_setup);
+
+void check_mpx_erratum(struct cpuinfo_x86 *c)
+{
+	if (forcempx)
+		return;
+	/*
+	 * Turn off the MPX feature on CPUs where SMEP is not
+	 * available or disabled.
+	 *
+	 * Works around Intel Erratum SKD046: "Branch Instructions
+	 * May Initialize MPX Bound Registers Incorrectly".
+	 *
+	 * This might falsely disable MPX on systems without
+	 * SMEP, like Atom processors without SMEP.  But there
+	 * is no such hardware known at the moment.
+	 */
+	if (cpu_has(c, X86_FEATURE_MPX) && !cpu_has(c, X86_FEATURE_SMEP)) {
+		setup_clear_cpu_cap(X86_FEATURE_MPX);
+		pr_warn("x86/mpx: Disabling MPX since SMEP not present\n");
+	}
+}
+
 static void early_init_intel(struct cpuinfo_x86 *c)
 {
 	u64 misc_enable;
@@ -173,6 +208,8 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		if (edx & (1U << 28))
 			c->x86_coreid_bits = get_count_order((ebx >> 16) & 0xff);
 	}
+
+	check_mpx_erratum(c);
 }
 
 #ifdef CONFIG_X86_32

commit 9a45f036af363aec1efec08827c825d69c115a9a
Merge: 168f1a7163b3 d2d3462f9f08
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 15:54:01 2016 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - prepare for more KASLR related changes, by restructuring, cleaning
         up and fixing the existing boot code.  (Kees Cook, Baoquan He,
         Yinghai Lu)
    
       - simplifly/concentrate subarch handling code, eliminate
         paravirt_enabled() usage.  (Luis R Rodriguez)"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (50 commits)
      x86/KASLR: Clarify purpose of each get_random_long()
      x86/KASLR: Add virtual address choosing function
      x86/KASLR: Return earliest overlap when avoiding regions
      x86/KASLR: Add 'struct slot_area' to manage random_addr slots
      x86/boot: Add missing file header comments
      x86/KASLR: Initialize mapping_info every time
      x86/boot: Comment what finalize_identity_maps() does
      x86/KASLR: Build identity mappings on demand
      x86/boot: Split out kernel_ident_mapping_init()
      x86/boot: Clean up indenting for asm/boot.h
      x86/KASLR: Improve comments around the mem_avoid[] logic
      x86/boot: Simplify pointer casting in choose_random_location()
      x86/KASLR: Consolidate mem_avoid[] entries
      x86/boot: Clean up pointer casting
      x86/boot: Warn on future overlapping memcpy() use
      x86/boot: Extract error reporting functions
      x86/boot: Correctly bounds-check relocations
      x86/KASLR: Clean up unused code from old 'run_size' and rename it to 'kernel_total_size'
      x86/boot: Fix "run_size" calculation
      x86/boot: Calculate decompression size during boot not build
      ...

commit 168f1a7163b37294a0ef33829e1ed54d41e33c42
Merge: 825a3b2605c3 4afd0565552c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 15:15:17 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - MSR access API fixes and enhancements (Andy Lutomirski)
    
       - early exception handling improvements (Andy Lutomirski)
    
       - user-space FS/GS prctl usage fixes and improvements (Andy
         Lutomirski)
    
       - Remove the cpu_has_*() APIs and replace them with equivalents
         (Borislav Petkov)
    
       - task switch micro-optimization (Brian Gerst)
    
       - 32-bit entry code simplification (Denys Vlasenko)
    
       - enhance PAT handling in enumated CPUs (Toshi Kani)
    
      ... and lots of other cleanups/fixlets"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/arch_prctl/64: Restore accidentally removed put_cpu() in ARCH_SET_GS
      x86/entry/32: Remove asmlinkage_protect()
      x86/entry/32: Remove GET_THREAD_INFO() from entry code
      x86/entry, sched/x86: Don't save/restore EFLAGS on task switch
      x86/asm/entry/32: Simplify pushes of zeroed pt_regs->REGs
      selftests/x86/ldt_gdt: Test set_thread_area() deletion of an active segment
      x86/tls: Synchronize segment registers in set_thread_area()
      x86/asm/64: Rename thread_struct's fs and gs to fsbase and gsbase
      x86/arch_prctl/64: Remove FSBASE/GSBASE < 4G optimization
      x86/segments/64: When load_gs_index fails, clear the base
      x86/segments/64: When loadsegment(fs, ...) fails, clear the base
      x86/asm: Make asm/alternative.h safe from assembly
      x86/asm: Stop depending on ptrace.h in alternative.h
      x86/entry: Rename is_{ia32,x32}_task() to in_{ia32,x32}_syscall()
      x86/asm: Make sure verify_cpu() has a good stack
      x86/extable: Add a comment about early exception handlers
      x86/msr: Set the return value to zero when native_rdmsr_safe() fails
      x86/paravirt: Make "unsafe" MSR accesses unsafe even if PARAVIRT=y
      x86/paravirt: Add paravirt_{read,write}_msr()
      x86/msr: Carry on after a non-"safe" MSR access fails
      ...

commit f0133acc7d4835cfbb86393b7d2a4fba7519585b
Author: Borislav Petkov <bp@suse.de>
Date:   Sun May 8 20:58:40 2016 +0200

    x86/cpu: Correct comments and messages in P4 erratum 037 handling code
    
    Remove the linebreak in the conditional and s/errata/erratum/ as the
    singular is "erratum".
    
    No functionality change.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1462733920-7224-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index f71a34944b56..5354080f76c3 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -263,15 +263,14 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	}
 
 	/*
-	 * P4 Xeon errata 037 workaround.
+	 * P4 Xeon erratum 037 workaround.
 	 * Hardware prefetcher may cause stale data to be loaded into the cache.
 	 */
 	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
 		if (msr_set_bit(MSR_IA32_MISC_ENABLE,
-				MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE_BIT)
-		    > 0) {
+				MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE_BIT) > 0) {
 			pr_info("CPU: C0 stepping P4 Xeon detected.\n");
-			pr_info("CPU: Disabling hardware prefetching (Errata 037)\n");
+			pr_info("CPU: Disabling hardware prefetching (Erratum 037)\n");
 		}
 	}
 

commit 8d415ee225a3d15d3e3029524350e8237a4de7b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 10 09:20:33 2016 +0200

    x86/topology: Set x86_max_cores to 1 for CONFIG_SMP=n
    
    Josef reported that the uncore driver trips over with CONFIG_SMP=n because
    x86_max_cores is 16 instead of 12.
    
    The reason is, that for SMP=n the extended topology detection is a NOOP and
    the cache leaf is used to determine the number of cores. That's wrong in two
    aspects:
    
    1) The cache leaf enumerates the maximum addressable number of cores in the
       package, which is obviously not correct
    
    2) UP has no business with topology bits at all.
    
    Make intel_num_cpu_cores() return 1 for CONFIG_SMP=n
    
    Reported-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel-team <Kernel-team@fb.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Link: http://lkml.kernel.org/r/761b4a2a-0332-7954-f030-c6639f949612@fb.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1f7fdb91a818..e4393bfc7f0d 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -336,7 +336,7 @@ static int intel_num_cpu_cores(struct cpuinfo_x86 *c)
 {
 	unsigned int eax, ebx, ecx, edx;
 
-	if (c->cpuid_level < 4)
+	if (!IS_ENABLED(CONFIG_SMP) || c->cpuid_level < 4)
 		return 1;
 
 	/* Intel has a non-standard dependency on %ecx for this CPUID level. */

commit fa392794ed8329379f3f637da7c3c2f078309a77
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Wed Apr 13 17:04:40 2016 -0700

    x86/cpu/intel: Remove not needed paravirt_enabled() use for F00F work around
    
    The X86_BUG_F00F work around is responsible for fixing up the error
    generated on attempted F00F exploitation from an OOPS to a SIGILL.
    
    There is no reason why this code should not be allowed to run on
    PV guest on a F00F-affected CPU -- it would simply never trigger.
    The pv_enabled() check was there only to avoid printing the f00f
    workaround, so removing the check is purely a cosmetic change.
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrew.cooper3@citrix.com
    Cc: andriy.shevchenko@linux.intel.com
    Cc: bigeasy@linutronix.de
    Cc: boris.ostrovsky@oracle.com
    Cc: david.vrabel@citrix.com
    Cc: ffainelli@freebox.fr
    Cc: george.dunlap@citrix.com
    Cc: glin@suse.com
    Cc: jgross@suse.com
    Cc: jlee@suse.com
    Cc: josh@joshtriplett.org
    Cc: julien.grall@linaro.org
    Cc: konrad.wilk@oracle.com
    Cc: kozerkov@parallels.com
    Cc: lenb@kernel.org
    Cc: lguest@lists.ozlabs.org
    Cc: linux-acpi@vger.kernel.org
    Cc: lv.zheng@intel.com
    Cc: matt@codeblueprint.co.uk
    Cc: mbizon@freebox.fr
    Cc: rjw@rjwysocki.net
    Cc: robert.moore@intel.com
    Cc: rusty@rustcorp.com.au
    Cc: tiwai@suse.de
    Cc: toshi.kani@hp.com
    Cc: xen-devel@lists.xensource.com
    Link: http://lkml.kernel.org/r/1460592286-300-11-git-send-email-mcgrof@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1f7fdb91a818..016b3d9ffa7d 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -233,7 +233,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * The Quark is also family 5, but does not have the same bug.
 	 */
 	clear_cpu_bug(c, X86_BUG_F00F);
-	if (!paravirt_enabled() && c->x86 == 5 && c->x86_model < 9) {
+	if (c->x86 == 5 && c->x86_model < 9) {
 		static int f00f_workaround_enabled;
 
 		set_cpu_bug(c, X86_BUG_F00F);

commit 93984fbd4e33cc861d5b49caed02a02cbfb01340
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 4 22:25:00 2016 +0200

    x86/cpufeature: Replace cpu_has_apic with boot_cpu_has() usage
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: linux-pm@vger.kernel.org
    Cc: oprofile-list@lists.sf.net
    Link: http://lkml.kernel.org/r/1459801503-15600-8-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index f71a34944b56..1d5582259b20 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -281,7 +281,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * integrated APIC (see 11AP erratum in "Pentium Processor
 	 * Specification Update").
 	 */
-	if (cpu_has_apic && (c->x86<<8 | c->x86_model<<4) == 0x520 &&
+	if (boot_cpu_has(X86_FEATURE_APIC) && (c->x86<<8 | c->x86_model<<4) == 0x520 &&
 	    (c->x86_mask < 0x6 || c->x86_mask == 0xb))
 		set_cpu_bug(c, X86_BUG_11AP);
 

commit c109bf95992b391bb40bc37c5d309d13fead99b5
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:42:02 2016 +0200

    x86/cpufeature: Remove cpu_has_pge
    
    Use static_cpu_has() in __flush_tlb_all() due to the time-sensitivity of
    this one.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1459266123-21878-10-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1dba36fe73e5..f71a34944b56 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -152,9 +152,9 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	 *  the TLB when any changes are made to any of the page table entries.
 	 *  The operating system must reload CR3 to cause the TLB to be flushed"
 	 *
-	 * As a result cpu_has_pge() in arch/x86/include/asm/tlbflush.h should
-	 * be false so that __flush_tlb_all() causes CR3 insted of CR4.PGE
-	 * to be modified
+	 * As a result, boot_cpu_has(X86_FEATURE_PGE) in arch/x86/include/asm/tlbflush.h
+	 * should be false so that __flush_tlb_all() causes CR3 insted of CR4.PGE
+	 * to be modified.
 	 */
 	if (c->x86 == 5 && c->x86_model == 9) {
 		pr_info("Disabling PGE capability bit\n");

commit 054efb6467f84490bdf92afab6d9dbd5102e620a
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:42:00 2016 +0200

    x86/cpufeature: Remove cpu_has_xmm2
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-crypto@vger.kernel.org
    Link: http://lkml.kernel.org/r/1459266123-21878-8-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 628a9f853b84..1dba36fe73e5 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -456,7 +456,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
 	}
 
-	if (cpu_has_xmm2)
+	if (cpu_has(c, X86_FEATURE_XMM2))
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
 
 	if (boot_cpu_has(X86_FEATURE_DS)) {

commit 906bf7fda2c9cf5c1762ec607943ed54b6c5b203
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:41:59 2016 +0200

    x86/cpufeature: Remove cpu_has_clflush
    
    Use the fast variant in the DRM code.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dri-devel@lists.freedesktop.org
    Cc: intel-gfx@lists.freedesktop.org
    Link: http://lkml.kernel.org/r/1459266123-21878-7-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1f7fdb91a818..628a9f853b84 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -468,7 +468,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_PEBS);
 	}
 
-	if (c->x86 == 6 && cpu_has_clflush &&
+	if (c->x86 == 6 && boot_cpu_has(X86_FEATURE_CLFLUSH) &&
 	    (c->x86_model == 29 || c->x86_model == 46 || c->x86_model == 47))
 		set_cpu_bug(c, X86_BUG_CLFLUSH_MONITOR);
 

commit ba33ea811e1ff6726abb7f8f96df38c2d7b50304
Merge: e23604edac2a d05004944206
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 09:32:27 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "This is another big update. Main changes are:
    
       - lots of x86 system call (and other traps/exceptions) entry code
         enhancements.  In particular the complex parts of the 64-bit entry
         code have been migrated to C code as well, and a number of dusty
         corners have been refreshed.  (Andy Lutomirski)
    
       - vDSO special mapping robustification and general cleanups (Andy
         Lutomirski)
    
       - cpufeature refactoring, cleanups and speedups (Borislav Petkov)
    
       - lots of other changes ..."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (64 commits)
      x86/cpufeature: Enable new AVX-512 features
      x86/entry/traps: Show unhandled signal for i386 in do_trap()
      x86/entry: Call enter_from_user_mode() with IRQs off
      x86/entry/32: Change INT80 to be an interrupt gate
      x86/entry: Improve system call entry comments
      x86/entry: Remove TIF_SINGLESTEP entry work
      x86/entry/32: Add and check a stack canary for the SYSENTER stack
      x86/entry/32: Simplify and fix up the SYSENTER stack #DB/NMI fixup
      x86/entry: Only allocate space for tss_struct::SYSENTER_stack if needed
      x86/entry: Vastly simplify SYSENTER TF (single-step) handling
      x86/entry/traps: Clear DR6 early in do_debug() and improve the comment
      x86/entry/traps: Clear TIF_BLOCKSTEP on all debug exceptions
      x86/entry/32: Restore FLAGS on SYSEXIT
      x86/entry/32: Filter NT and speed up AC filtering in SYSENTER
      x86/entry/compat: In SYSENTER, sink AC clearing below the existing FLAGS test
      selftests/x86: In syscall_nt, test NT|TF as well
      x86/asm-offsets: Remove PARAVIRT_enabled
      x86/entry/32: Introduce and use X86_BUG_ESPFIX instead of paravirt_enabled
      uprobes: __create_xol_area() must nullify xol_mapping.fault
      x86/cpufeature: Create a new synthetic cpu capability for machine check recovery
      ...

commit 1f12e32f4cd5243ae46d8b933181be0d022c6793
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 22 22:19:15 2016 +0000

    x86/topology: Create logical package id
    
    For per package oriented services we must be able to rely on the number of CPU
    packages to be within bounds. Create a tracking facility, which
    
    - calculates the number of possible packages depending on nr_cpu_ids after boot
    
    - makes sure that the package id is within the number of possible packages. If
      the apic id is outside we map it to a logical package id if there is enough
      space available.
    
    Provide interfaces for drivers to query the mapping and do translations from
    physcial to logical ids.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Harish Chegondi <harish.chegondi@intel.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160222221011.541071755@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 05b9211ea0f7..38766c2b5b00 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -160,6 +160,19 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		pr_info("Disabling PGE capability bit\n");
 		setup_clear_cpu_cap(X86_FEATURE_PGE);
 	}
+
+	if (c->cpuid_level >= 0x00000001) {
+		u32 eax, ebx, ecx, edx;
+
+		cpuid(0x00000001, &eax, &ebx, &ecx, &edx);
+		/*
+		 * If HTT (EDX[28]) is set EBX[16:23] contain the number of
+		 * apicids which are reserved per package. Store the resulting
+		 * shift value for the package management code.
+		 */
+		if (edx & (1U << 28))
+			c->x86_coreid_bits = get_count_order((ebx >> 16) & 0xff);
+	}
 }
 
 #ifdef CONFIG_X86_32

commit 1b74dde7c47c19a73ea3e9fac95ac27b5d3d50c5
Author: Chen Yucong <slaoub@gmail.com>
Date:   Tue Feb 2 11:45:02 2016 +0800

    x86/cpu: Convert printk(KERN_<LEVEL> ...) to pr_<level>(...)
    
     - Use the more current logging style pr_<level>(...) instead of the old
       printk(KERN_<LEVEL> ...).
    
     - Convert pr_warning() to pr_warn().
    
    Signed-off-by: Chen Yucong <slaoub@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454384702-21707-1-git-send-email-slaoub@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 565648bc1a0a..05b9211ea0f7 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -61,7 +61,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	 */
 	if (c->x86 == 6 && c->x86_model == 0x1c && c->x86_mask <= 2 &&
 	    c->microcode < 0x20e) {
-		printk(KERN_WARNING "Atom PSE erratum detected, BIOS microcode update recommended\n");
+		pr_warn("Atom PSE erratum detected, BIOS microcode update recommended\n");
 		clear_cpu_cap(c, X86_FEATURE_PSE);
 	}
 
@@ -140,7 +140,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {
 		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
 		if (!(misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING)) {
-			printk(KERN_INFO "Disabled fast string operations\n");
+			pr_info("Disabled fast string operations\n");
 			setup_clear_cpu_cap(X86_FEATURE_REP_GOOD);
 			setup_clear_cpu_cap(X86_FEATURE_ERMS);
 		}
@@ -176,7 +176,7 @@ int ppro_with_ram_bug(void)
 	    boot_cpu_data.x86 == 6 &&
 	    boot_cpu_data.x86_model == 1 &&
 	    boot_cpu_data.x86_mask < 8) {
-		printk(KERN_INFO "Pentium Pro with Errata#50 detected. Taking evasive action.\n");
+		pr_info("Pentium Pro with Errata#50 detected. Taking evasive action.\n");
 		return 1;
 	}
 	return 0;
@@ -225,7 +225,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 
 		set_cpu_bug(c, X86_BUG_F00F);
 		if (!f00f_workaround_enabled) {
-			printk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");
+			pr_notice("Intel Pentium with F0 0F bug - workaround enabled.\n");
 			f00f_workaround_enabled = 1;
 		}
 	}
@@ -244,7 +244,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * Forcefully enable PAE if kernel parameter "forcepae" is present.
 	 */
 	if (forcepae) {
-		printk(KERN_WARNING "PAE forced!\n");
+		pr_warn("PAE forced!\n");
 		set_cpu_cap(c, X86_FEATURE_PAE);
 		add_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_NOW_UNRELIABLE);
 	}

commit cd4d09ec6f6c12a2cc3db5b7d8876a325a53545b
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 26 22:12:04 2016 +0100

    x86/cpufeature: Carve out X86_FEATURE_*
    
    Move them to a separate header and have the following
    dependency:
    
      x86/cpufeatures.h <- x86/processor.h <- x86/cpufeature.h
    
    This makes it easier to use the header in asm code and not
    include the whole cpufeature.h and add guards for asm.
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453842730-28463-5-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 565648bc1a0a..9299e3bdfad6 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -8,7 +8,7 @@
 #include <linux/module.h>
 #include <linux/uaccess.h>
 
-#include <asm/processor.h>
+#include <asm/cpufeature.h>
 #include <asm/pgtable.h>
 #include <asm/msr.h>
 #include <asm/bugs.h>

commit 362f924b64ba0f4be2ee0cb697690c33d40be721
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Dec 7 10:39:41 2015 +0100

    x86/cpufeature: Remove unused and seldomly used cpu_has_xx macros
    
    Those are stupid and code should use static_cpu_has_safe() or
    boot_cpu_has() instead. Kill the least used and unused ones.
    
    The remaining ones need more careful inspection before a conversion can
    happen. On the TODO.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1449481182-27541-4-git-send-email-bp@alien8.de
    Cc: David Sterba <dsterba@suse.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 209ac1e7d1f0..565648bc1a0a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -445,7 +445,8 @@ static void init_intel(struct cpuinfo_x86 *c)
 
 	if (cpu_has_xmm2)
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
-	if (cpu_has_ds) {
+
+	if (boot_cpu_has(X86_FEATURE_DS)) {
 		unsigned int l1;
 		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);
 		if (!(l1 & (1<<11)))

commit 354dbaa7ff5b53a0ed1c0f7a9773d5953b3a1bb9
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Thu Oct 8 18:56:26 2015 +0300

    x86/cpu/intel: Enable X86_FEATURE_NONSTOP_TSC_S3 for Merrifield
    
    The Intel Merrifield SoC is a successor of the Intel MID line of
    SoCs. Let's set the neccessary capability for that chip. See commit
    c54fdbb2823d (x86: Add cpu capability flag X86_FEATURE_NONSTOP_TSC_S3)
    for the details.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Link: http://lkml.kernel.org/r/1444319786-36125-1-git-send-email-andriy.shevchenko@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 98a13db5f4be..209ac1e7d1f0 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -97,6 +97,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		switch (c->x86_model) {
 		case 0x27:	/* Penwell */
 		case 0x35:	/* Cloverview */
+		case 0x4a:	/* Merrifield */
 			set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC_S3);
 			break;
 		default:

commit b51ef52df71cb28e9d90cd1d48b79bf19f0bab06
Author: Laura Abbott <labbott@fedoraproject.org>
Date:   Mon Jul 20 14:47:58 2015 -0700

    x86/cpu: Restore MSR_IA32_ENERGY_PERF_BIAS after resume
    
    MSR_IA32_ENERGY_PERF_BIAS is lost after suspend/resume:
    
            x86_energy_perf_policy -r before
    
            cpu0: 0x0000000000000006
            cpu1: 0x0000000000000006
            cpu2: 0x0000000000000006
            cpu3: 0x0000000000000006
            cpu4: 0x0000000000000006
            cpu5: 0x0000000000000006
            cpu6: 0x0000000000000006
            cpu7: 0x0000000000000006
    
            after
    
            cpu0: 0x0000000000000000
            cpu1: 0x0000000000000006
            cpu2: 0x0000000000000006
            cpu3: 0x0000000000000006
            cpu4: 0x0000000000000006
            cpu5: 0x0000000000000006
            cpu6: 0x0000000000000006
            cpu7: 0x0000000000000006
    
    Resulting in inconsistent energy policy settings across CPUs.
    
    This register is set via init_intel() at bootup. During resume,
    the secondary CPUs are brought online again and init_intel() is
    callled which re-initializes the register. The boot CPU however
    never reinitializes the register.
    
    Add a syscore callback to reinitialize the register for the boot CPU.
    
    Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437428878-4105-1-git-send-email-labbott@fedoraproject.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 50163fa9034f..98a13db5f4be 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -371,6 +371,36 @@ static void detect_vmx_virtcap(struct cpuinfo_x86 *c)
 	}
 }
 
+static void init_intel_energy_perf(struct cpuinfo_x86 *c)
+{
+	u64 epb;
+
+	/*
+	 * Initialize MSR_IA32_ENERGY_PERF_BIAS if not already initialized.
+	 * (x86_energy_perf_policy(8) is available to change it at run-time.)
+	 */
+	if (!cpu_has(c, X86_FEATURE_EPB))
+		return;
+
+	rdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
+	if ((epb & 0xF) != ENERGY_PERF_BIAS_PERFORMANCE)
+		return;
+
+	pr_warn_once("ENERGY_PERF_BIAS: Set to 'normal', was 'performance'\n");
+	pr_warn_once("ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)\n");
+	epb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;
+	wrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
+}
+
+static void intel_bsp_resume(struct cpuinfo_x86 *c)
+{
+	/*
+	 * MSR_IA32_ENERGY_PERF_BIAS is lost across suspend/resume,
+	 * so reinitialize it properly like during bootup:
+	 */
+	init_intel_energy_perf(c);
+}
+
 static void init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int l2 = 0;
@@ -478,21 +508,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 	if (cpu_has(c, X86_FEATURE_VMX))
 		detect_vmx_virtcap(c);
 
-	/*
-	 * Initialize MSR_IA32_ENERGY_PERF_BIAS if BIOS did not.
-	 * x86_energy_perf_policy(8) is available to change it at run-time
-	 */
-	if (cpu_has(c, X86_FEATURE_EPB)) {
-		u64 epb;
-
-		rdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
-		if ((epb & 0xF) == ENERGY_PERF_BIAS_PERFORMANCE) {
-			pr_warn_once("ENERGY_PERF_BIAS: Set to 'normal', was 'performance'\n");
-			pr_warn_once("ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)\n");
-			epb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;
-			wrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
-		}
-	}
+	init_intel_energy_perf(c);
 }
 
 #ifdef CONFIG_X86_32
@@ -747,6 +763,7 @@ static const struct cpu_dev intel_cpu_dev = {
 	.c_detect_tlb	= intel_detect_tlb,
 	.c_early_init   = early_init_intel,
 	.c_init		= init_intel,
+	.c_bsp_resume	= intel_bsp_resume,
 	.c_x86_vendor	= X86_VENDOR_INTEL,
 };
 

commit a927792c196f1c24410f3c12ccf45238a353783a
Author: Yannick Guerrini <yguerrini@tomshardware.fr>
Date:   Sat Feb 21 23:41:50 2015 +0100

    x86/cpu/intel: Fix trivial typo in intel_tlb_table[]
    
    Change 'ssociative' to 'associative'
    
    Signed-off-by: Yannick Guerrini <yguerrini@tomshardware.fr>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Bryan O'Donoghue <pure.logic@nexus-software.ie>
    Cc: Chris Bainbridge <chris.bainbridge@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Steven Honeyman <stevenhoneyman@gmail.com>
    Cc: trivial@kernel.org
    Link: http://lkml.kernel.org/r/1424558510-1420-1-git-send-email-yguerrini@tomshardware.fr
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 94d7dcb12145..50163fa9034f 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -565,8 +565,8 @@ static const struct _tlb_table intel_tlb_table[] = {
 	{ 0xb2, TLB_INST_4K,		64,	" TLB_INST 4KByte pages, 4-way set associative" },
 	{ 0xb3, TLB_DATA_4K,		128,	" TLB_DATA 4 KByte pages, 4-way set associative" },
 	{ 0xb4, TLB_DATA_4K,		256,	" TLB_DATA 4 KByte pages, 4-way associative" },
-	{ 0xb5, TLB_INST_4K,		64,	" TLB_INST 4 KByte pages, 8-way set ssociative" },
-	{ 0xb6, TLB_INST_4K,		128,	" TLB_INST 4 KByte pages, 8-way set ssociative" },
+	{ 0xb5, TLB_INST_4K,		64,	" TLB_INST 4 KByte pages, 8-way set associative" },
+	{ 0xb6, TLB_INST_4K,		128,	" TLB_INST 4 KByte pages, 8-way set associative" },
 	{ 0xba, TLB_DATA_4K,		64,	" TLB_DATA 4 KByte pages, 4-way associative" },
 	{ 0xc0, TLB_DATA_4K_4M,		8,	" TLB_DATA 4 KByte and 4 MByte pages, 4-way associative" },
 	{ 0xc1, STLB_4K_2M,		1024,	" STLB 4 KByte and 2 MByte pages, 8-way associative" },

commit f94fe119f2e53362a3038ee856fa58412f728bc9
Author: Steven Honeyman <stevenhoneyman@gmail.com>
Date:   Wed Nov 5 22:52:18 2014 +0000

    x86, CPU: Fix trivial printk formatting issues with dmesg
    
    dmesg (from util-linux) currently has two methods for reading the kernel
    message ring buffer: /dev/kmsg and syslog(2). Since kernel 3.5.0 kmsg
    has been the default, which escapes control characters (e.g. new lines)
    before they are shown.
    
    This change means that when dmesg is using /dev/kmsg, a 2 line printk
    makes the output messy, because the second line does not get a
    timestamp.
    
    For example:
    
    [    0.012863] CPU0: Thermal monitoring enabled (TM1)
    [    0.012869] Last level iTLB entries: 4KB 1024, 2MB 1024, 4MB 1024
    Last level dTLB entries: 4KB 1024, 2MB 1024, 4MB 1024, 1GB 4
    [    0.012958] Freeing SMP alternatives memory: 28K (ffffffff81d86000 - ffffffff81d8d000)
    [    0.014961] dmar: Host address width 39
    
    Because printk.c intentionally escapes control characters, they should
    not be there in the first place. This patch fixes two occurrences of
    this.
    
    Signed-off-by: Steven Honeyman <stevenhoneyman@gmail.com>
    Link: https://lkml.kernel.org/r/1414856696-8094-1-git-send-email-stevenhoneyman@gmail.com
    [ Boris: make cpu_detect_tlb() static, while at it. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 9cc6b6f25f42..94d7dcb12145 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -487,10 +487,8 @@ static void init_intel(struct cpuinfo_x86 *c)
 
 		rdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
 		if ((epb & 0xF) == ENERGY_PERF_BIAS_PERFORMANCE) {
-			printk_once(KERN_WARNING "ENERGY_PERF_BIAS:"
-				" Set to 'normal', was 'performance'\n"
-				"ENERGY_PERF_BIAS: View and update with"
-				" x86_energy_perf_policy(8)\n");
+			pr_warn_once("ENERGY_PERF_BIAS: Set to 'normal', was 'performance'\n");
+			pr_warn_once("ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)\n");
 			epb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;
 			wrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
 		}

commit d4e1a0af1d3a88cdfc8c954d3005eb8745ec518d
Author: Dave Jones <davej@redhat.com>
Date:   Tue Oct 28 13:57:53 2014 -0400

    x86: Don't enable F00F workaround on Intel Quark processors
    
    The Intel Quark processor is a part of family 5, but does not have the
    F00F bug present in Pentiums of the same family.
    
    Pentiums were models 0 through 8, Quark is model 9.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Cc: Bryan O'Donoghue <pure.logic@nexus-software.ie>
    Link: http://lkml.kernel.org/r/20141028175753.GA12743@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1ef456273172..9cc6b6f25f42 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -213,12 +213,13 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_F00F_BUG
 	/*
-	 * All current models of Pentium and Pentium with MMX technology CPUs
+	 * All models of Pentium and Pentium with MMX technology CPUs
 	 * have the F0 0F bug, which lets nonprivileged users lock up the
 	 * system. Announce that the fault handler will be checking for it.
+	 * The Quark is also family 5, but does not have the same bug.
 	 */
 	clear_cpu_bug(c, X86_BUG_F00F);
-	if (!paravirt_enabled() && c->x86 == 5) {
+	if (!paravirt_enabled() && c->x86 == 5 && c->x86_model < 9) {
 		static int f00f_workaround_enabled;
 
 		set_cpu_bug(c, X86_BUG_F00F);

commit aece118e487a744eafcdd0c77fe32b55ee2092a1
Author: Bryan O'Donoghue <pure.logic@nexus-software.ie>
Date:   Tue Oct 7 01:19:49 2014 +0100

    x86: Add cpu_detect_cache_sizes to init_intel() add Quark legacy_cache()
    
    Intel processors which don't report cache information via cpuid(2)
    or cpuid(4) need quirk code in the legacy_cache_size callback to
    report this data. For Intel that callback is is intel_size_cache().
    
    This patch enables calling of cpu_detect_cache_sizes() inside of
    init_intel() and hence the calling of the legacy_cache callback in
    intel_size_cache(). Adding this call will ensure that PIII Tualatin
    currently in intel_size_cache() and Quark SoC X1000 being added to
    intel_size_cache() in this patch will report their respective cache
    sizes.
    
    This model of calling cpu_detect_cache_sizes() is consistent with
    AMD/Via/Cirix/Transmeta and Centaur.
    
    Also added is a string to idenitfy the Quark as Quark SoC X1000
    giving better and more descriptive output via /proc/cpuinfo
    
    Adding cpu_detect_cache_sizes to init_intel() will enable calling
    of intel_size_cache() on Intel processors which currently no code
    can reach. Therefore this patch will also re-enable reporting
    of PIII Tualatin cache size information as well as add
    Quark SoC X1000 support.
    
    Comment text and cache flow logic suggested by Thomas Gleixner
    
    Signed-off-by: Bryan O'Donoghue <pure.logic@nexus-software.ie>
    Cc: davej@redhat.com
    Cc: hmh@hmh.eng.br
    Link: http://lkml.kernel.org/r/1412641189-12415-3-git-send-email-pure.logic@nexus-software.ie
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 50ce7519ccef..1ef456273172 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -397,6 +397,13 @@ static void init_intel(struct cpuinfo_x86 *c)
 	}
 
 	l2 = init_intel_cacheinfo(c);
+
+	/* Detect legacy cache sizes if init_intel_cacheinfo did not */
+	if (l2 == 0) {
+		cpu_detect_cache_sizes(c);
+		l2 = c->x86_cache_size;
+	}
+
 	if (c->cpuid_level > 9) {
 		unsigned eax = cpuid_eax(10);
 		/* Check for version and the number of counters */
@@ -500,6 +507,13 @@ static unsigned int intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 	 */
 	if ((c->x86 == 6) && (c->x86_model == 11) && (size == 0))
 		size = 256;
+
+	/*
+	 * Intel Quark SoC X1000 contains a 4-way set associative
+	 * 16K cache with a 16 byte cache line and 256 lines per tag
+	 */
+	if ((c->x86 == 5) && (c->x86_model == 9))
+		size = 16;
 	return size;
 }
 #endif
@@ -701,7 +715,8 @@ static const struct cpu_dev intel_cpu_dev = {
 			  [3] = "OverDrive PODP5V83",
 			  [4] = "Pentium MMX",
 			  [7] = "Mobile Pentium 75 - 200",
-			  [8] = "Mobile Pentium MMX"
+			  [8] = "Mobile Pentium MMX",
+			  [9] = "Quark SoC X1000",
 		  }
 		},
 		{ .family = 6, .model_names =

commit ee1b5b165c0a2f04d2107e634e51f05d0eb107de
Author: Bryan O'Donoghue <pure.logic@nexus-software.ie>
Date:   Wed Sep 24 00:26:24 2014 +0100

    x86/intel/quark: Switch off CR4.PGE so TLB flush uses CR3 instead
    
    Quark x1000 advertises PGE via the standard CPUID method
    PGE bits exist in Quark X1000's PTEs. In order to flush
    an individual PTE it is necessary to reload CR3 irrespective
    of the PTE.PGE bit.
    
    See Quark Core_DevMan_001.pdf section 6.4.11
    
    This bug was fixed in Galileo kernels, unfixed vanilla kernels are expected to
    crash and burn on this platform.
    
    Signed-off-by: Bryan O'Donoghue <pure.logic@nexus-software.ie>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: <stable@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1411514784-14885-1-git-send-email-pure.logic@nexus-software.ie
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 74e804ddc5c7..50ce7519ccef 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -144,6 +144,21 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 			setup_clear_cpu_cap(X86_FEATURE_ERMS);
 		}
 	}
+
+	/*
+	 * Intel Quark Core DevMan_001.pdf section 6.4.11
+	 * "The operating system also is required to invalidate (i.e., flush)
+	 *  the TLB when any changes are made to any of the page table entries.
+	 *  The operating system must reload CR3 to cause the TLB to be flushed"
+	 *
+	 * As a result cpu_has_pge() in arch/x86/include/asm/tlbflush.h should
+	 * be false so that __flush_tlb_all() causes CR3 insted of CR4.PGE
+	 * to be modified
+	 */
+	if (c->x86 == 5 && c->x86_model == 9) {
+		pr_info("Disabling PGE capability bit\n");
+		setup_clear_cpu_cap(X86_FEATURE_PGE);
+	}
 }
 
 #ifdef CONFIG_X86_32

commit ce4747963252a30613ebf1c1df3d83b9526a342e
Merge: 76f09aa464a1 a5102476a24b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 17:15:45 2014 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "The main change in this cycle is the rework of the TLB range flushing
      code, to simplify, fix and consolidate the code.  By Dave Hansen"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Set TLB flush tunable to sane value (33)
      x86/mm: New tunable for single vs full TLB flush
      x86/mm: Add tracepoints for TLB flushes
      x86/mm: Unify remote INVLPG code
      x86/mm: Fix missed global TLB flush stat
      x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
      x86/mm: Clean up the TLB flushing code
      x86/smep: Be more informative when signalling an SMEP fault

commit e9c9eecabaa898ff3fedd98813ee4ac1a00d006a
Merge: 19d402c1e750 af0fa6f6b585
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 17:12:45 2014 -0700

    Merge branch 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpufeature updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Continued cleanups of CPU bugs mis-marked as 'missing features', by
         Borislav Petkov.
    
       - Detect the xsaves/xrstors feature and releated cleanup, by Fenghua
         Yu"
    
    * 'x86-cpufeature-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpu: Kill cpu_has_mp
      x86, amd: Cleanup init_amd
      x86/cpufeature: Add bug flags to /proc/cpuinfo
      x86, cpufeature: Convert more "features" to bugs
      x86/xsaves: Detect xsaves/xrstors feature
      x86/cpufeature.h: Reformat x86 feature macros

commit e9f4e0a9fe2723078b7a1a1169828dd46a7b2f9e
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jul 31 08:40:55 2014 -0700

    x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
    
    I think the flush_tlb_mm_range() code that tries to tune the
    flush sizes based on the CPU needs to get ripped out for
    several reasons:
    
    1. It is obviously buggy.  It uses mm->total_vm to judge the
       task's footprint in the TLB.  It should certainly be using
       some measure of RSS, *NOT* ->total_vm since only resident
       memory can populate the TLB.
    2. Haswell, and several other CPUs are missing from the
       intel_tlb_flushall_shift_set() function.  Thus, it has been
       demonstrated to bitrot quickly in practice.
    3. It is plain wrong in my vm:
            [    0.037444] Last level iTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] Last level dTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] tlb_flushall_shift: 6
       Which leads to it to never use invlpg.
    4. The assumptions about TLB refill costs are wrong:
            http://lkml.kernel.org/r/1337782555-8088-3-git-send-email-alex.shi@intel.com
        (more on this in later patches)
    5. I can not reproduce the original data: https://lkml.org/lkml/2012/5/17/59
       I believe the sample times were too short.  Running the
       benchmark in a loop yields times that vary quite a bit.
    
    Note that this leaves us with a static ceiling of 1 page.  This
    is a conservative, dumb setting, and will be revised in a later
    patch.
    
    This also removes the code which attempts to predict whether we
    are flushing data or instructions.  We expect instruction flushes
    to be relatively rare and not worth tuning for explicitly.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20140731154055.ABC88E89@viggo.jf.intel.com
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a80029035bf2..cd61755de49b 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -634,31 +634,6 @@ static void intel_tlb_lookup(const unsigned char desc)
 	}
 }
 
-static void intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
-{
-	switch ((c->x86 << 8) + c->x86_model) {
-	case 0x60f: /* original 65 nm celeron/pentium/core2/xeon, "Merom"/"Conroe" */
-	case 0x616: /* single-core 65 nm celeron/core2solo "Merom-L"/"Conroe-L" */
-	case 0x617: /* current 45 nm celeron/core2/xeon "Penryn"/"Wolfdale" */
-	case 0x61d: /* six-core 45 nm xeon "Dunnington" */
-		tlb_flushall_shift = -1;
-		break;
-	case 0x63a: /* Ivybridge */
-		tlb_flushall_shift = 2;
-		break;
-	case 0x61a: /* 45 nm nehalem, "Bloomfield" */
-	case 0x61e: /* 45 nm nehalem, "Lynnfield" */
-	case 0x625: /* 32 nm nehalem, "Clarkdale" */
-	case 0x62c: /* 32 nm nehalem, "Gulftown" */
-	case 0x62e: /* 45 nm nehalem-ex, "Beckton" */
-	case 0x62f: /* 32 nm Xeon E7 */
-	case 0x62a: /* SandyBridge */
-	case 0x62d: /* SandyBridge, "Romely-EP" */
-	default:
-		tlb_flushall_shift = 6;
-	}
-}
-
 static void intel_detect_tlb(struct cpuinfo_x86 *c)
 {
 	int i, j, n;
@@ -683,7 +658,6 @@ static void intel_detect_tlb(struct cpuinfo_x86 *c)
 		for (j = 1 ; j < 16 ; j++)
 			intel_tlb_lookup(desc[j]);
 	}
-	intel_tlb_flushall_shift_set(c);
 }
 
 static const struct cpu_dev intel_cpu_dev = {

commit 2a2261553dd1472ca574acadbd93e12f44c4e6d5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jul 22 15:35:14 2014 +0200

    x86, cpu: Fix cache topology for early P4-SMT
    
    P4 systems with cpuid level < 4 can have SMT, but the cache topology
    description available (cpuid2) does not include SMP information.
    
    Now we know that SMT shares all cache levels, and therefore we can
    mark all available cache levels as shared.
    
    We do this by setting cpu_llc_id to ->phys_proc_id, since that's
    the same for each SMT thread. We can do this unconditional since if
    there's no SMT its still true, the one CPU shares cache with only
    itself.
    
    This fixes a problem where such CPUs report an incorrect LLC CPU mask.
    
    This in turn fixes a crash in the scheduler where the topology was
    build wrong, it assumes the LLC mask to include at least the SMT CPUs.
    
    Cc: Josh Boyer <jwboyer@redhat.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Tested-by: Bruno Wolff III <bruno@wolff.to>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140722133514.GM12054@laptop.lan
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a80029035bf2..f9e4fdd3b877 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -370,6 +370,17 @@ static void init_intel(struct cpuinfo_x86 *c)
 	 */
 	detect_extended_topology(c);
 
+	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
+		/*
+		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
+		 * detection.
+		 */
+		c->x86_max_cores = intel_num_cpu_cores(c);
+#ifdef CONFIG_X86_32
+		detect_ht(c);
+#endif
+	}
+
 	l2 = init_intel_cacheinfo(c);
 	if (c->cpuid_level > 9) {
 		unsigned eax = cpuid_eax(10);
@@ -438,17 +449,6 @@ static void init_intel(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_P3);
 #endif
 
-	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
-		/*
-		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
-		 * detection.
-		 */
-		c->x86_max_cores = intel_num_cpu_cores(c);
-#ifdef CONFIG_X86_32
-		detect_ht(c);
-#endif
-	}
-
 	/* Work around errata */
 	srat_detect_node(c);
 

commit 9b13a93df267af681a66a6a738bf1af10102da7d
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jun 18 00:06:23 2014 +0200

    x86, cpufeature: Convert more "features" to bugs
    
    X86_FEATURE_FXSAVE_LEAK, X86_FEATURE_11AP and
    X86_FEATURE_CLFLUSH_MONITOR are not really features but synthetic bits
    we use for applying different bug workarounds. Call them what they
    really are, and make sure they get the proper cross-CPU behavior (OR
    rather than AND).
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1403042783-23278-1-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a80029035bf2..0fd955778f35 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -253,7 +253,7 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 */
 	if (cpu_has_apic && (c->x86<<8 | c->x86_model<<4) == 0x520 &&
 	    (c->x86_mask < 0x6 || c->x86_mask == 0xb))
-		set_cpu_cap(c, X86_FEATURE_11AP);
+		set_cpu_bug(c, X86_BUG_11AP);
 
 
 #ifdef CONFIG_X86_INTEL_USERCOPY
@@ -391,7 +391,7 @@ static void init_intel(struct cpuinfo_x86 *c)
 
 	if (c->x86 == 6 && cpu_has_clflush &&
 	    (c->x86_model == 29 || c->x86_model == 46 || c->x86_model == 47))
-		set_cpu_cap(c, X86_FEATURE_CLFLUSH_MONITOR);
+		set_cpu_bug(c, X86_BUG_CLFLUSH_MONITOR);
 
 #ifdef CONFIG_X86_64
 	if (c->x86 == 15)

commit 467cbd207abdbfe29514b5804a22661ab6e80dc6
Merge: 7125764c5d1a b5660ba76b41
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 2 13:15:58 2014 -0700

    Merge branch 'x86-nuke-platforms-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 old platform removal from Peter Anvin:
     "This patchset removes support for several completely obsolete
      platforms, where the maintainers either have completely vanished or
      acked the removal.  For some of them it is questionable if there even
      exists functional specimens of the hardware"
    
    Geert Uytterhoeven apparently thought this was a April Fool's pull request ;)
    
    * 'x86-nuke-platforms-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, platforms: Remove NUMAQ
      x86, platforms: Remove SGI Visual Workstation
      x86, apic: Remove support for IBM Summit/EXA chipset
      x86, apic: Remove support for ia32-based Unisys ES7000

commit 69f2366c9456d0ce784cf5aba87ee77eeadc1d5e
Author: Chris Bainbridge <chris.bainbridge@gmail.com>
Date:   Fri Mar 7 18:40:42 2014 +0700

    x86, cpu: Add forcepae parameter for booting PAE kernels on PAE-disabled Pentium M
    
    Many Pentium M systems disable PAE but may have a functionally usable PAE
    implementation. This adds the "forcepae" parameter which bypasses the boot
    check for PAE, and sets the CPU as being PAE capable. Using this parameter
    will taint the kernel with TAINT_CPU_OUT_OF_SPEC.
    
    Signed-off-by: Chris Bainbridge <chris.bainbridge@gmail.com>
    Link: http://lkml.kernel.org/r/20140307114040.GA4997@localhost
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 34bbb555e269..897d6201ef10 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -186,6 +186,14 @@ static void intel_smp_check(struct cpuinfo_x86 *c)
 	}
 }
 
+static int forcepae;
+static int __init forcepae_setup(char *__unused)
+{
+	forcepae = 1;
+	return 1;
+}
+__setup("forcepae", forcepae_setup);
+
 static void intel_workarounds(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_F00F_BUG
@@ -213,6 +221,17 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
 		clear_cpu_cap(c, X86_FEATURE_SEP);
 
+	/*
+	 * PAE CPUID issue: many Pentium M report no PAE but may have a
+	 * functionally usable PAE implementation.
+	 * Forcefully enable PAE if kernel parameter "forcepae" is present.
+	 */
+	if (forcepae) {
+		printk(KERN_WARNING "PAE forced!\n");
+		set_cpu_cap(c, X86_FEATURE_PAE);
+		add_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_NOW_UNRELIABLE);
+	}
+
 	/*
 	 * P4 Xeon errata 037 workaround.
 	 * Hardware prefetcher may cause stale data to be loaded into the cache.

commit 0b131be8d4125b32eb5b94e84f4f9dee6a0ef797
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Mar 13 15:40:52 2014 -0700

    x86, intel: Make MSR_IA32_MISC_ENABLE bit constants systematic
    
    Replace somewhat arbitrary constants for bits in MSR_IA32_MISC_ENABLE
    with verbose but systematic ones.  Add _BIT defines for all the rest
    of them, too.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 44ca6317af43..34bbb555e269 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -31,7 +31,8 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 
 	/* Unmask CPUID levels if masked: */
 	if (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {
-		if (msr_clear_bit(MSR_IA32_MISC_ENABLE, MSR_BIT_LIMIT_CPUID) > 0) {
+		if (msr_clear_bit(MSR_IA32_MISC_ENABLE,
+				  MSR_IA32_MISC_ENABLE_LIMIT_CPUID_BIT) > 0) {
 			c->cpuid_level = cpuid_eax(0);
 			get_cpu_cap(c);
 		}
@@ -126,7 +127,8 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	 * (model 2) with the same problem.
 	 */
 	if (c->x86 == 15)
-		if (msr_clear_bit(MSR_IA32_MISC_ENABLE, MSR_BIT_FAST_STRING) > 0)
+		if (msr_clear_bit(MSR_IA32_MISC_ENABLE,
+				  MSR_IA32_MISC_ENABLE_FAST_STRING_BIT) > 0)
 			pr_info("kmemcheck: Disabling fast string operations\n");
 #endif
 
@@ -216,7 +218,9 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * Hardware prefetcher may cause stale data to be loaded into the cache.
 	 */
 	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
-		if (msr_set_bit(MSR_IA32_MISC_ENABLE, MSR_BIT_PRF_DIS) > 0) {
+		if (msr_set_bit(MSR_IA32_MISC_ENABLE,
+				MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE_BIT)
+		    > 0) {
 			pr_info("CPU: C0 stepping P4 Xeon detected.\n");
 			pr_info("CPU: Disabling hardware prefetching (Errata 037)\n");
 		}

commit c0a639ad0bc6b178b46996bd1f821a04643e2bde
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Mar 9 18:05:25 2014 +0100

    x86, Intel: Convert to the new bit access MSR accessors
    
    ... and save some lines of code.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1394384725-10796-4-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 5cd9bfabd645..44ca6317af43 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -31,11 +31,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 
 	/* Unmask CPUID levels if masked: */
 	if (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {
-		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
-
-		if (misc_enable & MSR_IA32_MISC_ENABLE_LIMIT_CPUID) {
-			misc_enable &= ~MSR_IA32_MISC_ENABLE_LIMIT_CPUID;
-			wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
+		if (msr_clear_bit(MSR_IA32_MISC_ENABLE, MSR_BIT_LIMIT_CPUID) > 0) {
 			c->cpuid_level = cpuid_eax(0);
 			get_cpu_cap(c);
 		}
@@ -129,16 +125,9 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 	 * Ingo Molnar reported a Pentium D (model 6) and a Xeon
 	 * (model 2) with the same problem.
 	 */
-	if (c->x86 == 15) {
-		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
-
-		if (misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING) {
-			printk(KERN_INFO "kmemcheck: Disabling fast string operations\n");
-
-			misc_enable &= ~MSR_IA32_MISC_ENABLE_FAST_STRING;
-			wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
-		}
-	}
+	if (c->x86 == 15)
+		if (msr_clear_bit(MSR_IA32_MISC_ENABLE, MSR_BIT_FAST_STRING) > 0)
+			pr_info("kmemcheck: Disabling fast string operations\n");
 #endif
 
 	/*
@@ -197,8 +186,6 @@ static void intel_smp_check(struct cpuinfo_x86 *c)
 
 static void intel_workarounds(struct cpuinfo_x86 *c)
 {
-	unsigned long lo, hi;
-
 #ifdef CONFIG_X86_F00F_BUG
 	/*
 	 * All current models of Pentium and Pentium with MMX technology CPUs
@@ -229,12 +216,9 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	 * Hardware prefetcher may cause stale data to be loaded into the cache.
 	 */
 	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
-		rdmsr(MSR_IA32_MISC_ENABLE, lo, hi);
-		if ((lo & MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE) == 0) {
-			printk (KERN_INFO "CPU: C0 stepping P4 Xeon detected.\n");
-			printk (KERN_INFO "CPU: Disabling hardware prefetching (Errata 037)\n");
-			lo |= MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE;
-			wrmsr(MSR_IA32_MISC_ENABLE, lo, hi);
+		if (msr_set_bit(MSR_IA32_MISC_ENABLE, MSR_BIT_PRF_DIS) > 0) {
+			pr_info("CPU: C0 stepping P4 Xeon detected.\n");
+			pr_info("CPU: Disabling hardware prefetching (Errata 037)\n");
 		}
 	}
 

commit b5660ba76b41af69a0c09d434927bb4b4cadd4b1
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Feb 25 12:14:06 2014 -0800

    x86, platforms: Remove NUMAQ
    
    The NUMAQ support seems to be unmaintained, remove it.
    
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: David Rientjes <rientjes@google.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/n/530CFD6C.7040705@zytor.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 5cd9bfabd645..ea56e7c9abe6 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -267,10 +267,6 @@ static void intel_workarounds(struct cpuinfo_x86 *c)
 	}
 #endif
 
-#ifdef CONFIG_X86_NUMAQ
-	numaq_tsc_disable();
-#endif
-
 	intel_smp_check(c);
 }
 #else

commit 2b45e0f9f34f718725e093f4e335600811d7105a
Merge: a85eba881463 15c81026204d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 25 09:16:14 2014 +0100

    Merge branch 'linus' into x86/urgent
    
    Merge in the x86 changes to apply a fix.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b9a3b4c976c1209957326537ad5c0bb633dfd764
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jan 21 14:33:22 2014 -0800

    mm, x86: Revisit tlb_flushall_shift tuning for page flushes except on IvyBridge
    
    There was a large ebizzy performance regression that was
    bisected to commit 611ae8e3 (x86/tlb: enable tlb flush range
    support for x86).  The problem was related to the
    tlb_flushall_shift tuning for IvyBridge which was altered.  The
    problem is that it is not clear if the tuning values for each
    CPU family is correct as the methodology used to tune the values
    is unclear.
    
    This patch uses a conservative tlb_flushall_shift value for all
    CPU families except IvyBridge so the decision can be revisited
    if any regression is found as a result of this change.
    IvyBridge is an exception as testing with one methodology
    determined that the value of 2 is acceptable.  Details are in
    the changelog for the patch "x86: mm: Change tlb_flushall_shift
    for IvyBridge".
    
    One important aspect of this to watch out for is Xen.  The
    original commit log mentioned large performance gains on Xen.
    It's possible Xen is more sensitive to this value if it flushes
    small ranges of pages more frequently than workloads on bare
    metal typically do.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-dyzMww3fqugnhbhgo6Gxmtkw@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index bbe1b8b1f1c4..d358a3928b8f 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -615,21 +615,17 @@ static void intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
 	case 0x61d: /* six-core 45 nm xeon "Dunnington" */
 		tlb_flushall_shift = -1;
 		break;
+	case 0x63a: /* Ivybridge */
+		tlb_flushall_shift = 2;
+		break;
 	case 0x61a: /* 45 nm nehalem, "Bloomfield" */
 	case 0x61e: /* 45 nm nehalem, "Lynnfield" */
 	case 0x625: /* 32 nm nehalem, "Clarkdale" */
 	case 0x62c: /* 32 nm nehalem, "Gulftown" */
 	case 0x62e: /* 45 nm nehalem-ex, "Beckton" */
 	case 0x62f: /* 32 nm Xeon E7 */
-		tlb_flushall_shift = 6;
-		break;
 	case 0x62a: /* SandyBridge */
 	case 0x62d: /* SandyBridge, "Romely-EP" */
-		tlb_flushall_shift = 5;
-		break;
-	case 0x63a: /* Ivybridge */
-		tlb_flushall_shift = 2;
-		break;
 	default:
 		tlb_flushall_shift = 6;
 	}

commit f98b7a772ab51b52ca4d2a14362fc0e0c8a2e0f3
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jan 21 14:33:21 2014 -0800

    x86: mm: change tlb_flushall_shift for IvyBridge
    
    There was a large performance regression that was bisected to
    commit 611ae8e3 ("x86/tlb: enable tlb flush range support for
    x86").  This patch simply changes the default balance point
    between a local and global flush for IvyBridge.
    
    In the interest of allowing the tests to be reproduced, this
    patch was tested using mmtests 0.15 with the following
    configurations
    
            configs/config-global-dhp__tlbflush-performance
            configs/config-global-dhp__scheduler-performance
            configs/config-global-dhp__network-performance
    
    Results are from two machines
    
    Ivybridge   4 threads:  Intel(R) Core(TM) i3-3240 CPU @ 3.40GHz
    Ivybridge   8 threads:  Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz
    
    Page fault microbenchmark showed nothing interesting.
    
    Ebizzy was configured to run multiple iterations and threads.
    Thread counts ranged from 1 to NR_CPUS*2. For each thread count,
    it ran 100 iterations and each iteration lasted 10 seconds.
    
    Ivybridge 4 threads
                        3.13.0-rc7            3.13.0-rc7
                           vanilla           altshift-v3
    Mean   1     6395.44 (  0.00%)     6789.09 (  6.16%)
    Mean   2     7012.85 (  0.00%)     8052.16 ( 14.82%)
    Mean   3     6403.04 (  0.00%)     6973.74 (  8.91%)
    Mean   4     6135.32 (  0.00%)     6582.33 (  7.29%)
    Mean   5     6095.69 (  0.00%)     6526.68 (  7.07%)
    Mean   6     6114.33 (  0.00%)     6416.64 (  4.94%)
    Mean   7     6085.10 (  0.00%)     6448.51 (  5.97%)
    Mean   8     6120.62 (  0.00%)     6462.97 (  5.59%)
    
    Ivybridge 8 threads
                         3.13.0-rc7            3.13.0-rc7
                            vanilla           altshift-v3
    Mean   1      7336.65 (  0.00%)     7787.02 (  6.14%)
    Mean   2      8218.41 (  0.00%)     9484.13 ( 15.40%)
    Mean   3      7973.62 (  0.00%)     8922.01 ( 11.89%)
    Mean   4      7798.33 (  0.00%)     8567.03 (  9.86%)
    Mean   5      7158.72 (  0.00%)     8214.23 ( 14.74%)
    Mean   6      6852.27 (  0.00%)     7952.45 ( 16.06%)
    Mean   7      6774.65 (  0.00%)     7536.35 ( 11.24%)
    Mean   8      6510.50 (  0.00%)     6894.05 (  5.89%)
    Mean   12     6182.90 (  0.00%)     6661.29 (  7.74%)
    Mean   16     6100.09 (  0.00%)     6608.69 (  8.34%)
    
    Ebizzy hits the worst case scenario for TLB range flushing every
    time and it shows for these Ivybridge CPUs at least that the
    default choice is a poor on.  The patch addresses the problem.
    
    Next was a tlbflush microbenchmark written by Alex Shi at
    http://marc.info/?l=linux-kernel&m=133727348217113 .  It
    measures access costs while the TLB is being flushed.  The
    expectation is that if there are always full TLB flushes that
    the benchmark would suffer and it benefits from range flushing
    
    There are 320 iterations of the test per thread count.  The
    number of entries is randomly selected with a min of 1 and max
    of 512.  To ensure a reasonably even spread of entries, the full
    range is broken up into 8 sections and a random number selected
    within that section.
    
    iteration 1, random number between 0-64
    iteration 2, random number between 64-128 etc
    
    This is still a very weak methodology.  When you do not know
    what are typical ranges, random is a reasonable choice but it
    can be easily argued that the opimisation was for smaller ranges
    and an even spread is not representative of any workload that
    matters.  To improve this, we'd need to know the probability
    distribution of TLB flush range sizes for a set of workloads
    that are considered "common", build a synthetic trace and feed
    that into this benchmark.  Even that is not perfect because it
    would not account for the time between flushes but there are
    limits of what can be reasonably done and still be doing
    something useful.  If a representative synthetic trace is
    provided then this benchmark could be revisited and the shift values retuned.
    
    Ivybridge 4 threads
                            3.13.0-rc7            3.13.0-rc7
                               vanilla           altshift-v3
    Mean       1       10.50 (  0.00%)       10.50 (  0.03%)
    Mean       2       17.59 (  0.00%)       17.18 (  2.34%)
    Mean       3       22.98 (  0.00%)       21.74 (  5.41%)
    Mean       5       47.13 (  0.00%)       46.23 (  1.92%)
    Mean       8       43.30 (  0.00%)       42.56 (  1.72%)
    
    Ivybridge 8 threads
                             3.13.0-rc7            3.13.0-rc7
                                vanilla           altshift-v3
    Mean       1         9.45 (  0.00%)        9.36 (  0.93%)
    Mean       2         9.37 (  0.00%)        9.70 ( -3.54%)
    Mean       3         9.36 (  0.00%)        9.29 (  0.70%)
    Mean       5        14.49 (  0.00%)       15.04 ( -3.75%)
    Mean       8        41.08 (  0.00%)       38.73 (  5.71%)
    Mean       13       32.04 (  0.00%)       31.24 (  2.49%)
    Mean       16       40.05 (  0.00%)       39.04 (  2.51%)
    
    For both CPUs, average access time is reduced which is good as
    this is the benchmark that was used to tune the shift values in
    the first place albeit it is now known *how* the benchmark was
    used.
    
    The scheduler benchmarks were somewhat inconclusive.  They
    showed gains and losses and makes me reconsider how stable those
    benchmarks really are or if something else might be interfering
    with the test results recently.
    
    Network benchmarks were inconclusive.  Almost all results were
    flat except for netperf-udp tests on the 4 thread machine.
    These results were unstable and showed large variations between
    reboots.  It is unknown if this is a recent problems but I've
    noticed before that netperf-udp results tend to vary.
    
    Based on these results, changing the default for Ivybridge seems
    like a logical choice.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Alex Shi <alex.shi@linaro.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-cqnadffh1tiqrshthRj3Esge@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ea04b342c026..bbe1b8b1f1c4 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -628,7 +628,7 @@ static void intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
 		tlb_flushall_shift = 5;
 		break;
 	case 0x63a: /* Ivybridge */
-		tlb_flushall_shift = 1;
+		tlb_flushall_shift = 2;
 		break;
 	default:
 		tlb_flushall_shift = 6;

commit 5d4863e4cc4dc12d1d5e42da3cb5d38c535e4ad6
Merge: 2a0fede97fd5 dd360393f4d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:04:45 2014 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 TLB detection update from Ingo Molnar:
     "A single change that extends our TLB cache size detection+reporting
      code"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpu: Detect more TLB configuration

commit 2a0fede97fd52a5c9789d1d54ebd3b46878151c3
Merge: 06bc0f4a2e7f d139336700a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:03:57 2014 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpu, amd: Fix a shadowed variable situation
      um, x86: Fix vDSO build
      x86: Delete non-required instances of include <linux/init.h>
      x86, realmode: Pointer walk cleanups, pull out invariant use of __pa()
      x86/traps: Clean up error exception handler definitions

commit 35af99e646c7f7ea46dc2977601e9e71a51dadd5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 28 19:38:42 2013 +0100

    sched/clock, x86: Use a static_key for sched_clock_stable
    
    In order to avoid the runtime condition and variable load turn
    sched_clock_stable into a static_key.
    
    Also provide a shorter implementation of local_clock() and
    cpu_clock(int) when sched_clock_stable==1.
    
                            MAINLINE   PRE       POST
    
        sched_clock_stable: 1          1         1
        (cold) sched_clock: 329841     221876    215295
        (cold) local_clock: 301773     234692    220773
        (warm) sched_clock: 38375      25602     25659
        (warm) local_clock: 100371     33265     27242
        (warm) rdtsc:       27340      24214     24208
        sched_clock_stable: 0          0         0
        (cold) sched_clock: 382634     235941    237019
        (cold) local_clock: 396890     297017    294819
        (warm) sched_clock: 38194      25233     25609
        (warm) local_clock: 143452     71234     71232
        (warm) rdtsc:       27345      24245     24243
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-eummbdechzz37mwmpags1gjr@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ea04b342c026..1a439c047ff3 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -93,7 +93,7 @@ static void early_init_intel(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
 		if (!check_tsc_unstable())
-			sched_clock_stable = 1;
+			set_sched_clock_stable();
 	}
 
 	/* Penwell and Cloverview have the TSC which doesn't sleep on S3 */

commit 663b55b9b39fa9c848cca273ca4e12bf29b32c71
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Jan 6 19:20:26 2014 -0500

    x86: Delete non-required instances of include <linux/init.h>
    
    None of these files are actually using any __init type directives
    and hence don't need to include <linux/init.h>.  Most are just a
    left over from __devinit and __cpuinit removal, or simply due to
    code getting copied from one driver to the next.
    
    [ hpa: undid incorrect removal from arch/x86/kernel/head_32.S ]
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Link: http://lkml.kernel.org/r/1389054026-12947-1-git-send-email-paul.gortmaker@windriver.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index dc1ec0dff939..53f5d3c7ac09 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1,4 +1,3 @@
-#include <linux/init.h>
 #include <linux/kernel.h>
 
 #include <linux/string.h>

commit dd360393f4d948eb518372316e52101cf3b44212
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Dec 23 14:16:58 2013 +0200

    x86, cpu: Detect more TLB configuration
    
    The Intel Software Developers Manual covers few more TLB
    configurations exposed as CPUID 2 descriptors:
    
    61H Instruction TLB: 4 KByte pages, fully associative, 48 entries
    63H Data TLB: 1 GByte pages, 4-way set associative, 4 entries
    76H Instruction TLB: 2M/4M pages, fully associative, 8 entries
    B5H Instruction TLB: 4KByte pages, 8-way set associative, 64 entries
    B6H Instruction TLB: 4KByte pages, 8-way set associative, 128 entries
    C1H Shared 2nd-Level TLB: 4 KByte/2MByte pages, 8-way associative, 1024 entries
    C2H DTLB DTLB: 2 MByte/$MByte pages, 4-way associative, 16 entries
    
    Let's detect them as well.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Link: http://lkml.kernel.org/r/1387801018-14499-1-git-send-email-kirill.shutemov@linux.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ea04b342c026..5eb7ea5fae15 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -506,6 +506,7 @@ static unsigned int intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 #define TLB_DATA0_2M_4M	0x23
 
 #define STLB_4K		0x41
+#define STLB_4K_2M	0x42
 
 static const struct _tlb_table intel_tlb_table[] = {
 	{ 0x01, TLB_INST_4K,		32,	" TLB_INST 4 KByte pages, 4-way set associative" },
@@ -526,13 +527,20 @@ static const struct _tlb_table intel_tlb_table[] = {
 	{ 0x5b, TLB_DATA_4K_4M,		64,	" TLB_DATA 4 KByte and 4 MByte pages" },
 	{ 0x5c, TLB_DATA_4K_4M,		128,	" TLB_DATA 4 KByte and 4 MByte pages" },
 	{ 0x5d, TLB_DATA_4K_4M,		256,	" TLB_DATA 4 KByte and 4 MByte pages" },
+	{ 0x61, TLB_INST_4K,		48,	" TLB_INST 4 KByte pages, full associative" },
+	{ 0x63, TLB_DATA_1G,		4,	" TLB_DATA 1 GByte pages, 4-way set associative" },
+	{ 0x76, TLB_INST_2M_4M,		8,	" TLB_INST 2-MByte or 4-MByte pages, fully associative" },
 	{ 0xb0, TLB_INST_4K,		128,	" TLB_INST 4 KByte pages, 4-way set associative" },
 	{ 0xb1, TLB_INST_2M_4M,		4,	" TLB_INST 2M pages, 4-way, 8 entries or 4M pages, 4-way entries" },
 	{ 0xb2, TLB_INST_4K,		64,	" TLB_INST 4KByte pages, 4-way set associative" },
 	{ 0xb3, TLB_DATA_4K,		128,	" TLB_DATA 4 KByte pages, 4-way set associative" },
 	{ 0xb4, TLB_DATA_4K,		256,	" TLB_DATA 4 KByte pages, 4-way associative" },
+	{ 0xb5, TLB_INST_4K,		64,	" TLB_INST 4 KByte pages, 8-way set ssociative" },
+	{ 0xb6, TLB_INST_4K,		128,	" TLB_INST 4 KByte pages, 8-way set ssociative" },
 	{ 0xba, TLB_DATA_4K,		64,	" TLB_DATA 4 KByte pages, 4-way associative" },
 	{ 0xc0, TLB_DATA_4K_4M,		8,	" TLB_DATA 4 KByte and 4 MByte pages, 4-way associative" },
+	{ 0xc1, STLB_4K_2M,		1024,	" STLB 4 KByte and 2 MByte pages, 8-way associative" },
+	{ 0xc2, TLB_DATA_2M_4M,		16,	" DTLB 2 MByte/4MByte pages, 4-way associative" },
 	{ 0xca, STLB_4K,		512,	" STLB 4 KByte pages, 4-way associative" },
 	{ 0x00, 0, 0 }
 };
@@ -558,6 +566,20 @@ static void intel_tlb_lookup(const unsigned char desc)
 		if (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)
 			tlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;
 		break;
+	case STLB_4K_2M:
+		if (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lli_2m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_2m[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lld_2m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_2m[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;
+		break;
 	case TLB_INST_ALL:
 		if (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)
 			tlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;
@@ -603,6 +625,10 @@ static void intel_tlb_lookup(const unsigned char desc)
 		if (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)
 			tlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;
 		break;
+	case TLB_DATA_1G:
+		if (tlb_lld_1g[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_1g[ENTRIES] = intel_tlb_table[k].entries;
+		break;
 	}
 }
 

commit 40e2d7f9b5dae048789c64672bf3027fbb663ffa
Author: Len Brown <len.brown@intel.com>
Date:   Wed Dec 18 16:44:57 2013 -0500

    x86 idle: Repair large-server 50-watt idle-power regression
    
    Linux 3.10 changed the timing of how thread_info->flags is touched:
    
            x86: Use generic idle loop
            (7d1a941731fabf27e5fb6edbebb79fe856edb4e5)
    
    This caused Intel NHM-EX and WSM-EX servers to experience a large number
    of immediate MONITOR/MWAIT break wakeups, which caused cpuidle to demote
    from deep C-states to shallow C-states, which caused these platforms
    to experience a significant increase in idle power.
    
    Note that this issue was already present before the commit above,
    however, it wasn't seen often enough to be noticed in power measurements.
    
    Here we extend an errata workaround from the Core2 EX "Dunnington"
    to extend to NHM-EX and WSM-EX, to prevent these immediate
    returns from MWAIT, reducing idle power on these platforms.
    
    While only acpi_idle ran on Dunnington, intel_idle
    may also run on these two newer systems.
    As of today, there are no other models that are known
    to need this tweak.
    
    Link: http://lkml.kernel.org/r/CAJvTdK=%2BaNN66mYpCGgbHGCHhYQAKx-vB0kJSWjVpsNb_hOAtQ@mail.gmail.com
    Signed-off-by: Len Brown <len.brown@intel.com>
    Link: http://lkml.kernel.org/r/baff264285f6e585df757d58b17788feabc68918.1387403066.git.len.brown@intel.com
    Cc: <stable@vger.kernel.org> # 3.12.x, 3.11.x, 3.10.x
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index dc1ec0dff939..ea04b342c026 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -387,7 +387,8 @@ static void init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_PEBS);
 	}
 
-	if (c->x86 == 6 && c->x86_model == 29 && cpu_has_clflush)
+	if (c->x86 == 6 && cpu_has_clflush &&
+	    (c->x86_model == 29 || c->x86_model == 46 || c->x86_model == 47))
 		set_cpu_cap(c, X86_FEATURE_CLFLUSH_MONITOR);
 
 #ifdef CONFIG_X86_64

commit 09dc68d958c67c76cf672ec78b7391af453010f8
Author: Jan Beulich <JBeulich@suse.com>
Date:   Mon Oct 21 09:35:20 2013 +0100

    x86/cpu: Track legacy CPU model data only on 32-bit kernels
    
    struct cpu_dev's c_models is only ever set inside CONFIG_X86_32
    conditionals (or code that's being built for 32-bit only), so
    there's no use of reserving the (empty) space for the model
    names in a 64-bit kernel.
    
    Similarly, c_size_cache is only used in the #else of a
    CONFIG_X86_64 conditional, so reserving space for (and in one
    case even initializing) that field is pointless for 64-bit
    kernels too.
    
    While moving both fields to the end of the structure, I also
    noticed that:
    
     - the c_models array size was one too small, potentially causing
       table_lookup_model() to return garbage on Intel CPUs (intel.c's
       instance was lacking the sentinel with family being zero), so the
       patch bumps that by one,
    
     - c_models' vendor sub-field was unused (and anyway redundant
       with the base structure's c_x86_vendor field), so the patch deletes it.
    
    Also rename the legacy fields so that their legacy nature stands out
    and comment their declarations.
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Link: http://lkml.kernel.org/r/5265036802000078000FC4DB@nat28.tlf.novell.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ec7299566f79..dc1ec0dff939 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -665,8 +665,8 @@ static const struct cpu_dev intel_cpu_dev = {
 	.c_vendor	= "Intel",
 	.c_ident	= { "GenuineIntel" },
 #ifdef CONFIG_X86_32
-	.c_models = {
-		{ .vendor = X86_VENDOR_INTEL, .family = 4, .model_names =
+	.legacy_models = {
+		{ .family = 4, .model_names =
 		  {
 			  [0] = "486 DX-25/33",
 			  [1] = "486 DX-50",
@@ -679,7 +679,7 @@ static const struct cpu_dev intel_cpu_dev = {
 			  [9] = "486 DX/4-WB"
 		  }
 		},
-		{ .vendor = X86_VENDOR_INTEL, .family = 5, .model_names =
+		{ .family = 5, .model_names =
 		  {
 			  [0] = "Pentium 60/66 A-step",
 			  [1] = "Pentium 60/66",
@@ -690,7 +690,7 @@ static const struct cpu_dev intel_cpu_dev = {
 			  [8] = "Mobile Pentium MMX"
 		  }
 		},
-		{ .vendor = X86_VENDOR_INTEL, .family = 6, .model_names =
+		{ .family = 6, .model_names =
 		  {
 			  [0] = "Pentium Pro A-step",
 			  [1] = "Pentium Pro",
@@ -704,7 +704,7 @@ static const struct cpu_dev intel_cpu_dev = {
 			  [11] = "Pentium III (Tualatin)",
 		  }
 		},
-		{ .vendor = X86_VENDOR_INTEL, .family = 15, .model_names =
+		{ .family = 15, .model_names =
 		  {
 			  [0] = "Pentium 4 (Unknown)",
 			  [1] = "Pentium 4 (Willamette)",
@@ -714,7 +714,7 @@ static const struct cpu_dev intel_cpu_dev = {
 		  }
 		},
 	},
-	.c_size_cache	= intel_size_cache,
+	.legacy_cache_size = intel_size_cache,
 #endif
 	.c_detect_tlb	= intel_detect_tlb,
 	.c_early_init   = early_init_intel,

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 9b0c441c03f5..ec7299566f79 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -26,7 +26,7 @@
 #include <asm/apic.h>
 #endif
 
-static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
+static void early_init_intel(struct cpuinfo_x86 *c)
 {
 	u64 misc_enable;
 
@@ -163,7 +163,7 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
  *	This is called before we do cpu ident work
  */
 
-int __cpuinit ppro_with_ram_bug(void)
+int ppro_with_ram_bug(void)
 {
 	/* Uses data from early_cpu_detect now */
 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
@@ -176,7 +176,7 @@ int __cpuinit ppro_with_ram_bug(void)
 	return 0;
 }
 
-static void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)
+static void intel_smp_check(struct cpuinfo_x86 *c)
 {
 	/* calling is from identify_secondary_cpu() ? */
 	if (!c->cpu_index)
@@ -196,7 +196,7 @@ static void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)
 	}
 }
 
-static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
+static void intel_workarounds(struct cpuinfo_x86 *c)
 {
 	unsigned long lo, hi;
 
@@ -275,12 +275,12 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 	intel_smp_check(c);
 }
 #else
-static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
+static void intel_workarounds(struct cpuinfo_x86 *c)
 {
 }
 #endif
 
-static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
+static void srat_detect_node(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_NUMA
 	unsigned node;
@@ -300,7 +300,7 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 /*
  * find out the number of processor cores on the die
  */
-static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
+static int intel_num_cpu_cores(struct cpuinfo_x86 *c)
 {
 	unsigned int eax, ebx, ecx, edx;
 
@@ -315,7 +315,7 @@ static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
 		return 1;
 }
 
-static void __cpuinit detect_vmx_virtcap(struct cpuinfo_x86 *c)
+static void detect_vmx_virtcap(struct cpuinfo_x86 *c)
 {
 	/* Intel VMX MSR indicated features */
 #define X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW	0x00200000
@@ -353,7 +353,7 @@ static void __cpuinit detect_vmx_virtcap(struct cpuinfo_x86 *c)
 	}
 }
 
-static void __cpuinit init_intel(struct cpuinfo_x86 *c)
+static void init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int l2 = 0;
 
@@ -472,7 +472,7 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 }
 
 #ifdef CONFIG_X86_32
-static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)
+static unsigned int intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 {
 	/*
 	 * Intel PIII Tualatin. This comes in two flavours.
@@ -506,7 +506,7 @@ static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned i
 
 #define STLB_4K		0x41
 
-static const struct _tlb_table intel_tlb_table[] __cpuinitconst = {
+static const struct _tlb_table intel_tlb_table[] = {
 	{ 0x01, TLB_INST_4K,		32,	" TLB_INST 4 KByte pages, 4-way set associative" },
 	{ 0x02, TLB_INST_4M,		2,	" TLB_INST 4 MByte pages, full associative" },
 	{ 0x03, TLB_DATA_4K,		64,	" TLB_DATA 4 KByte pages, 4-way set associative" },
@@ -536,7 +536,7 @@ static const struct _tlb_table intel_tlb_table[] __cpuinitconst = {
 	{ 0x00, 0, 0 }
 };
 
-static void __cpuinit intel_tlb_lookup(const unsigned char desc)
+static void intel_tlb_lookup(const unsigned char desc)
 {
 	unsigned char k;
 	if (desc == 0)
@@ -605,7 +605,7 @@ static void __cpuinit intel_tlb_lookup(const unsigned char desc)
 	}
 }
 
-static void __cpuinit intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
+static void intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
 {
 	switch ((c->x86 << 8) + c->x86_model) {
 	case 0x60f: /* original 65 nm celeron/pentium/core2/xeon, "Merom"/"Conroe" */
@@ -634,7 +634,7 @@ static void __cpuinit intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
 	}
 }
 
-static void __cpuinit intel_detect_tlb(struct cpuinfo_x86 *c)
+static void intel_detect_tlb(struct cpuinfo_x86 *c)
 {
 	int i, j, n;
 	unsigned int regs[4];
@@ -661,7 +661,7 @@ static void __cpuinit intel_detect_tlb(struct cpuinfo_x86 *c)
 	intel_tlb_flushall_shift_set(c);
 }
 
-static const struct cpu_dev __cpuinitconst intel_cpu_dev = {
+static const struct cpu_dev intel_cpu_dev = {
 	.c_vendor	= "Intel",
 	.c_ident	= { "GenuineIntel" },
 #ifdef CONFIG_X86_32

commit 01c7cd0ef5d98fdd007d8a04c9f834bead6e5ee1
Merge: 39b2f8656e2a c889ba801dc3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:37:24 2013 -0700

    Merge branch 'x86-kaslr-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perparatory x86 kasrl changes from Ingo Molnar:
     "This contains changes from the ongoing KASLR work, by Kees Cook.
    
      The main changes are the use of a read-only IDT on x86 (which
      decouples the userspace visible virtual IDT address from the physical
      address), and a rework of ELF relocation support, in preparation of
      random, boot-time kernel image relocation."
    
    * 'x86-kaslr-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, relocs: Refactor the relocs tool to merge 32- and 64-bit ELF
      x86, relocs: Build separate 32/64-bit tools
      x86, relocs: Add 64-bit ELF support to relocs tool
      x86, relocs: Consolidate processing logic
      x86, relocs: Generalize ELF structure names
      x86: Use a read-only IDT alias on all CPUs

commit df8edfa9af5b2160549ed1a79b72e3ed13b6c7e2
Merge: 874f6d1be769 1077c932db63
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:34:38 2013 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpuid changes from Ingo Molnar:
     "The biggest change is x86 CPU bug handling refactoring and cleanups,
      by Borislav Petkov"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, CPU, AMD: Drop useless label
      x86, AMD: Correct {rd,wr}msr_amd_safe warnings
      x86: Fold-in trivial check_config function
      x86, cpu: Convert AMD Erratum 400
      x86, cpu: Convert AMD Erratum 383
      x86, cpu: Convert Cyrix coma bug detection
      x86, cpu: Convert FDIV bug detection
      x86, cpu: Convert F00F bug detection
      x86, cpu: Expand cpufeature facility to include cpu bugs

commit 4eefbe792baedb474e256d35370849992fcf1c79
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Apr 10 12:24:22 2013 -0700

    x86: Use a read-only IDT alias on all CPUs
    
    Make a copy of the IDT (as seen via the "sidt" instruction) read-only.
    This primarily removes the IDT from being a target for arbitrary memory
    write attacks, and has the added benefit of also not leaking the kernel
    base offset, if it has been relocated.
    
    We already did this on vendor == Intel and family == 5 because of the
    F0 0F bug -- regardless of if a particular CPU had the F0 0F bug or
    not.  Since the workaround was so cheap, there simply was no reason to
    be very specific.  This patch extends the readonly alias to all CPUs,
    but does not activate the #PF to #UD conversion code needed to deliver
    the proper exception in the F0 0F case except on Intel family 5
    processors.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/20130410192422.GA17344@www.outflux.net
    Cc: Eric Northup <digitaleric@google.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1905ce98bee0..71700247a5d7 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -164,20 +164,6 @@ int __cpuinit ppro_with_ram_bug(void)
 	return 0;
 }
 
-#ifdef CONFIG_X86_F00F_BUG
-static void __cpuinit trap_init_f00f_bug(void)
-{
-	__set_fixmap(FIX_F00F_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);
-
-	/*
-	 * Update the IDT descriptor and reload the IDT so that
-	 * it uses the read-only mapped virtual address.
-	 */
-	idt_descr.address = fix_to_virt(FIX_F00F_IDT);
-	load_idt(&idt_descr);
-}
-#endif
-
 static void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)
 {
 	/* calling is from identify_secondary_cpu() ? */
@@ -206,8 +192,7 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 	/*
 	 * All current models of Pentium and Pentium with MMX technology CPUs
 	 * have the F0 0F bug, which lets nonprivileged users lock up the
-	 * system.
-	 * Note that the workaround only should be initialized once...
+	 * system. Announce that the fault handler will be checking for it.
 	 */
 	c->f00f_bug = 0;
 	if (!paravirt_enabled() && c->x86 == 5) {
@@ -215,7 +200,6 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 
 		c->f00f_bug = 1;
 		if (!f00f_workaround_enabled) {
-			trap_init_f00f_bug();
 			printk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");
 			f00f_workaround_enabled = 1;
 		}

commit e2604b49e8a8827e8bc5b61bc0c5a889e028fdad
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:24 2013 +0100

    x86, cpu: Convert F00F bug detection
    
    ... to using the new facility and drop the cpuinfo_x86 member.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-3-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1905ce98bee0..1acdd42d86d1 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -209,11 +209,11 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 	 * system.
 	 * Note that the workaround only should be initialized once...
 	 */
-	c->f00f_bug = 0;
+	clear_cpu_bug(c, X86_BUG_F00F);
 	if (!paravirt_enabled() && c->x86 == 5) {
 		static int f00f_workaround_enabled;
 
-		c->f00f_bug = 1;
+		set_cpu_bug(c, X86_BUG_F00F);
 		if (!f00f_workaround_enabled) {
 			trap_init_f00f_bug();
 			printk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");

commit c54fdbb2823d96b842d00c548e14dbc0dd37831d
Author: Feng Tang <feng.tang@intel.com>
Date:   Tue Mar 12 11:56:45 2013 +0800

    x86: Add cpu capability flag X86_FEATURE_NONSTOP_TSC_S3
    
    On some new Intel Atom processors (Penwell and Cloverview), there is
    a feature that the TSC won't stop in S3 state, say the TSC value
    won't be reset to 0 after resume. This feature makes TSC a more reliable
    clocksource and could benefit the timekeeping code during system
    suspend/resume cycle, so add a flag for it.
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    [jstultz: Fix checkpatch warning]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1905ce98bee0..e7ae0d89e7e0 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -96,6 +96,18 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 			sched_clock_stable = 1;
 	}
 
+	/* Penwell and Cloverview have the TSC which doesn't sleep on S3 */
+	if (c->x86 == 6) {
+		switch (c->x86_model) {
+		case 0x27:	/* Penwell */
+		case 0x35:	/* Cloverview */
+			set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC_S3);
+			break;
+		default:
+			break;
+		}
+	}
+
 	/*
 	 * There is a known erratum on Pentium III and Core Solo
 	 * and Core Duo CPUs.

commit 68d00bbebb5a48b7a9056a8c03476a71ecbc30a6
Merge: ac2cbab21f31 07f4207a305c
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Feb 1 02:25:06 2013 -0800

    Merge remote-tracking branch 'origin/x86/mm' into x86/mm2
    
    Explicitly merging these two branches due to nontrivial conflicts and
    to allow further work.
    
    Resolved Conflicts:
            arch/x86/kernel/head32.c
            arch/x86/kernel/head64.c
            arch/x86/mm/init_64.c
            arch/x86/realmode/init.c
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit de65d816aa44f9ddd79861ae21d75010cc1fd003
Merge: 9710f581bb4c 5dcd14ecd41e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Jan 29 14:59:09 2013 -0800

    Merge remote-tracking branch 'origin/x86/boot' into x86/mm2
    
    Coming patches to x86/mm2 require the changes and advanced baseline in
    x86/boot.
    
    Resolved Conflicts:
            arch/x86/kernel/setup.c
            mm/nobootmem.c
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 7b5c4a65cc27f017c170b025f8d6d75dabb11c6f
Merge: 3596f5bb0a6a 949db153b646
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Jan 25 16:31:21 2013 -0800

    Merge tag 'v3.8-rc5' into x86/mm
    
    The __pa() fixup series that follows touches KVM code that is not
    present in the existing branch based on v3.7-rc5, so merge in the
    current upstream from Linus.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 094ab1db7cb7833cd4c820acd868fc26acf3f08e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Nov 28 11:50:27 2012 -0800

    x86, 386 removal: Remove CONFIG_INVLPG
    
    All 486+ CPUs support INVLPG, so remove the fallback 386 support
    code.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1354132230-21854-6-git-send-email-hpa@linux.intel.com

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 198e019a531a..fcaabd0432c5 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -612,10 +612,6 @@ static void __cpuinit intel_tlb_lookup(const unsigned char desc)
 
 static void __cpuinit intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
 {
-	if (!cpu_has_invlpg) {
-		tlb_flushall_shift = -1;
-		return;
-	}
 	switch ((c->x86 << 8) + c->x86_model) {
 	case 0x60f: /* original 65 nm celeron/pentium/core2/xeon, "Merom"/"Conroe" */
 	case 0x616: /* single-core 65 nm celeron/core2solo "Merom-L"/"Conroe-L" */

commit c074eaac2ab264c94520efff7e896b771de885ae
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Nov 16 19:39:20 2012 -0800

    x86, mm: kill numa_64.h
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1353123563-3103-44-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 198e019a531a..3b547cc4bd03 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -17,7 +17,6 @@
 
 #ifdef CONFIG_X86_64
 #include <linux/topology.h>
-#include <asm/numa_64.h>
 #endif
 
 #include "cpu.h"

commit fc8d782677f163dee76427fdd8a92bebd2b50b23
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Fri Nov 16 13:57:13 2012 -0800

    x86: Use __pa_symbol instead of __pa on C visible symbols
    
    When I made an attempt at separating __pa_symbol and __pa I found that there
    were a number of cases where __pa was used on an obvious symbol.
    
    I also caught one non-obvious case as _brk_start and _brk_end are based on the
    address of __brk_base which is a C visible symbol.
    
    In mark_rodata_ro I was able to reduce the overhead of kernel symbol to
    virtual memory translation by using a combination of __va(__pa_symbol())
    instead of page_address(virt_to_page()).
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Link: http://lkml.kernel.org/r/20121116215640.8521.80483.stgit@ahduyck-cp1.jf.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 198e019a531a..2249e7e44521 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -168,7 +168,7 @@ int __cpuinit ppro_with_ram_bug(void)
 #ifdef CONFIG_X86_F00F_BUG
 static void __cpuinit trap_init_f00f_bug(void)
 {
-	__set_fixmap(FIX_F00F_IDT, __pa(&idt_table), PAGE_KERNEL_RO);
+	__set_fixmap(FIX_F00F_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);
 
 	/*
 	 * Update the IDT descriptor and reload the IDT so that

commit 5b556332c3ab19e6375836d35ca658776e9ba0f6
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon Aug 6 19:00:37 2012 +0200

    x86, cpu: Push TLB detection CPUID check down
    
    Push the max CPUID leaf check into the ->detect_tlb function and remove
    general test case from the generic path.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1344272439-29080-3-git-send-email-bp@amd64.org
    Acked-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 0a4ce2980a5a..198e019a531a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -648,6 +648,10 @@ static void __cpuinit intel_detect_tlb(struct cpuinfo_x86 *c)
 	int i, j, n;
 	unsigned int regs[4];
 	unsigned char *desc = (unsigned char *)regs;
+
+	if (c->cpuid_level < 2)
+		return;
+
 	/* Number of times to iterate */
 	n = cpuid_eax(2) & 0xFF;
 

commit c4211f42d3e66875298a5e26a75109878c80f15b
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 28 09:02:19 2012 +0800

    x86/tlb: add tlb_flushall_shift for specific CPU
    
    Testing show different CPU type(micro architectures and NUMA mode) has
    different balance points between the TLB flush all and multiple invlpg.
    And there also has cases the tlb flush change has no any help.
    
    This patch give a interface to let x86 vendor developers have a chance
    to set different shift for different CPU type.
    
    like some machine in my hands, balance points is 16 entries on
    Romely-EP; while it is at 8 entries on Bloomfield NHM-EP; and is 256 on
    IVB mobile CPU. but on model 15 core2 Xeon using invlpg has nothing
    help.
    
    For untested machine, do a conservative optimization, same as NHM CPU.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Link: http://lkml.kernel.org/r/1340845344-27557-5-git-send-email-alex.shi@intel.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ed0d512cf51b..0a4ce2980a5a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -610,6 +610,39 @@ static void __cpuinit intel_tlb_lookup(const unsigned char desc)
 	}
 }
 
+static void __cpuinit intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)
+{
+	if (!cpu_has_invlpg) {
+		tlb_flushall_shift = -1;
+		return;
+	}
+	switch ((c->x86 << 8) + c->x86_model) {
+	case 0x60f: /* original 65 nm celeron/pentium/core2/xeon, "Merom"/"Conroe" */
+	case 0x616: /* single-core 65 nm celeron/core2solo "Merom-L"/"Conroe-L" */
+	case 0x617: /* current 45 nm celeron/core2/xeon "Penryn"/"Wolfdale" */
+	case 0x61d: /* six-core 45 nm xeon "Dunnington" */
+		tlb_flushall_shift = -1;
+		break;
+	case 0x61a: /* 45 nm nehalem, "Bloomfield" */
+	case 0x61e: /* 45 nm nehalem, "Lynnfield" */
+	case 0x625: /* 32 nm nehalem, "Clarkdale" */
+	case 0x62c: /* 32 nm nehalem, "Gulftown" */
+	case 0x62e: /* 45 nm nehalem-ex, "Beckton" */
+	case 0x62f: /* 32 nm Xeon E7 */
+		tlb_flushall_shift = 6;
+		break;
+	case 0x62a: /* SandyBridge */
+	case 0x62d: /* SandyBridge, "Romely-EP" */
+		tlb_flushall_shift = 5;
+		break;
+	case 0x63a: /* Ivybridge */
+		tlb_flushall_shift = 1;
+		break;
+	default:
+		tlb_flushall_shift = 6;
+	}
+}
+
 static void __cpuinit intel_detect_tlb(struct cpuinfo_x86 *c)
 {
 	int i, j, n;
@@ -630,6 +663,7 @@ static void __cpuinit intel_detect_tlb(struct cpuinfo_x86 *c)
 		for (j = 1 ; j < 16 ; j++)
 			intel_tlb_lookup(desc[j]);
 	}
+	intel_tlb_flushall_shift_set(c);
 }
 
 static const struct cpu_dev __cpuinitconst intel_cpu_dev = {

commit e0ba94f14f747c2661c4d21f8c44e5b0b8cd8e48
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 28 09:02:16 2012 +0800

    x86/tlb_info: get last level TLB entry number of CPU
    
    For 4KB pages, x86 CPU has 2 or 1 level TLB, first level is data TLB and
    instruction TLB, second level is shared TLB for both data and instructions.
    
    For hupe page TLB, usually there is just one level and seperated by 2MB/4MB
    and 1GB.
    
    Although each levels TLB size is important for performance tuning, but for
    genernal and rude optimizing, last level TLB entry number is suitable. And
    in fact, last level TLB always has the biggest entry number.
    
    This patch will get the biggest TLB entry number and use it in furture TLB
    optimizing.
    
    Accroding Borislav's suggestion, except tlb_ll[i/d]_* array, other
    function and data will be released after system boot up.
    
    For all kinds of x86 vendor friendly, vendor specific code was moved to its
    specific files.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Link: http://lkml.kernel.org/r/1340845344-27557-2-git-send-email-alex.shi@intel.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 3e6ff6cbf42a..ed0d512cf51b 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -491,6 +491,147 @@ static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned i
 }
 #endif
 
+#define TLB_INST_4K	0x01
+#define TLB_INST_4M	0x02
+#define TLB_INST_2M_4M	0x03
+
+#define TLB_INST_ALL	0x05
+#define TLB_INST_1G	0x06
+
+#define TLB_DATA_4K	0x11
+#define TLB_DATA_4M	0x12
+#define TLB_DATA_2M_4M	0x13
+#define TLB_DATA_4K_4M	0x14
+
+#define TLB_DATA_1G	0x16
+
+#define TLB_DATA0_4K	0x21
+#define TLB_DATA0_4M	0x22
+#define TLB_DATA0_2M_4M	0x23
+
+#define STLB_4K		0x41
+
+static const struct _tlb_table intel_tlb_table[] __cpuinitconst = {
+	{ 0x01, TLB_INST_4K,		32,	" TLB_INST 4 KByte pages, 4-way set associative" },
+	{ 0x02, TLB_INST_4M,		2,	" TLB_INST 4 MByte pages, full associative" },
+	{ 0x03, TLB_DATA_4K,		64,	" TLB_DATA 4 KByte pages, 4-way set associative" },
+	{ 0x04, TLB_DATA_4M,		8,	" TLB_DATA 4 MByte pages, 4-way set associative" },
+	{ 0x05, TLB_DATA_4M,		32,	" TLB_DATA 4 MByte pages, 4-way set associative" },
+	{ 0x0b, TLB_INST_4M,		4,	" TLB_INST 4 MByte pages, 4-way set associative" },
+	{ 0x4f, TLB_INST_4K,		32,	" TLB_INST 4 KByte pages */" },
+	{ 0x50, TLB_INST_ALL,		64,	" TLB_INST 4 KByte and 2-MByte or 4-MByte pages" },
+	{ 0x51, TLB_INST_ALL,		128,	" TLB_INST 4 KByte and 2-MByte or 4-MByte pages" },
+	{ 0x52, TLB_INST_ALL,		256,	" TLB_INST 4 KByte and 2-MByte or 4-MByte pages" },
+	{ 0x55, TLB_INST_2M_4M,		7,	" TLB_INST 2-MByte or 4-MByte pages, fully associative" },
+	{ 0x56, TLB_DATA0_4M,		16,	" TLB_DATA0 4 MByte pages, 4-way set associative" },
+	{ 0x57, TLB_DATA0_4K,		16,	" TLB_DATA0 4 KByte pages, 4-way associative" },
+	{ 0x59, TLB_DATA0_4K,		16,	" TLB_DATA0 4 KByte pages, fully associative" },
+	{ 0x5a, TLB_DATA0_2M_4M,	32,	" TLB_DATA0 2-MByte or 4 MByte pages, 4-way set associative" },
+	{ 0x5b, TLB_DATA_4K_4M,		64,	" TLB_DATA 4 KByte and 4 MByte pages" },
+	{ 0x5c, TLB_DATA_4K_4M,		128,	" TLB_DATA 4 KByte and 4 MByte pages" },
+	{ 0x5d, TLB_DATA_4K_4M,		256,	" TLB_DATA 4 KByte and 4 MByte pages" },
+	{ 0xb0, TLB_INST_4K,		128,	" TLB_INST 4 KByte pages, 4-way set associative" },
+	{ 0xb1, TLB_INST_2M_4M,		4,	" TLB_INST 2M pages, 4-way, 8 entries or 4M pages, 4-way entries" },
+	{ 0xb2, TLB_INST_4K,		64,	" TLB_INST 4KByte pages, 4-way set associative" },
+	{ 0xb3, TLB_DATA_4K,		128,	" TLB_DATA 4 KByte pages, 4-way set associative" },
+	{ 0xb4, TLB_DATA_4K,		256,	" TLB_DATA 4 KByte pages, 4-way associative" },
+	{ 0xba, TLB_DATA_4K,		64,	" TLB_DATA 4 KByte pages, 4-way associative" },
+	{ 0xc0, TLB_DATA_4K_4M,		8,	" TLB_DATA 4 KByte and 4 MByte pages, 4-way associative" },
+	{ 0xca, STLB_4K,		512,	" STLB 4 KByte pages, 4-way associative" },
+	{ 0x00, 0, 0 }
+};
+
+static void __cpuinit intel_tlb_lookup(const unsigned char desc)
+{
+	unsigned char k;
+	if (desc == 0)
+		return;
+
+	/* look up this descriptor in the table */
+	for (k = 0; intel_tlb_table[k].descriptor != desc && \
+			intel_tlb_table[k].descriptor != 0; k++)
+		;
+
+	if (intel_tlb_table[k].tlb_type == 0)
+		return;
+
+	switch (intel_tlb_table[k].tlb_type) {
+	case STLB_4K:
+		if (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_INST_ALL:
+		if (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lli_2m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_2m[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_INST_4K:
+		if (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_INST_4M:
+		if (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_INST_2M_4M:
+		if (tlb_lli_2m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_2m[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_DATA_4K:
+	case TLB_DATA0_4K:
+		if (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_DATA_4M:
+	case TLB_DATA0_4M:
+		if (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_DATA_2M_4M:
+	case TLB_DATA0_2M_4M:
+		if (tlb_lld_2m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_2m[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	case TLB_DATA_4K_4M:
+		if (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;
+		if (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)
+			tlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;
+		break;
+	}
+}
+
+static void __cpuinit intel_detect_tlb(struct cpuinfo_x86 *c)
+{
+	int i, j, n;
+	unsigned int regs[4];
+	unsigned char *desc = (unsigned char *)regs;
+	/* Number of times to iterate */
+	n = cpuid_eax(2) & 0xFF;
+
+	for (i = 0 ; i < n ; i++) {
+		cpuid(2, &regs[0], &regs[1], &regs[2], &regs[3]);
+
+		/* If bit 31 is set, this is an unknown format */
+		for (j = 0 ; j < 3 ; j++)
+			if (regs[j] & (1 << 31))
+				regs[j] = 0;
+
+		/* Byte 0 is level count, not a descriptor */
+		for (j = 1 ; j < 16 ; j++)
+			intel_tlb_lookup(desc[j]);
+	}
+}
+
 static const struct cpu_dev __cpuinitconst intel_cpu_dev = {
 	.c_vendor	= "Intel",
 	.c_ident	= { "GenuineIntel" },
@@ -546,6 +687,7 @@ static const struct cpu_dev __cpuinitconst intel_cpu_dev = {
 	},
 	.c_size_cache	= intel_size_cache,
 #endif
+	.c_detect_tlb	= intel_detect_tlb,
 	.c_early_init   = early_init_intel,
 	.c_init		= init_intel,
 	.c_x86_vendor	= X86_VENDOR_INTEL,

commit 141168c36cdee3ff23d9c7700b0edc47cb65479f
Author: Kevin Winchester <kjwinchester@gmail.com>
Date:   Tue Dec 20 20:52:22 2011 -0400

    x86: Simplify code by removing a !SMP #ifdefs from 'struct cpuinfo_x86'
    
    Several fields in struct cpuinfo_x86 were not defined for the
    !SMP case, likely to save space.  However, those fields still
    have some meaning for UP, and keeping them allows some #ifdef
    removal from other files.  The additional size of the UP kernel
    from this change is not significant enough to worry about
    keeping up the distinction:
    
               text    data     bss     dec     hex filename
            4737168  506459  972040 6215667  5ed7f3 vmlinux.o.before
            4737444  506459  972040 6215943  5ed907 vmlinux.o.after
    
    for a difference of 276 bytes for an example UP config.
    
    If someone wants those 276 bytes back badly then it should
    be implemented in a cleaner way.
    
    Signed-off-by: Kevin Winchester <kjwinchester@gmail.com>
    Cc: Steffen Persvold <sp@numascale.com>
    Link: http://lkml.kernel.org/r/1324428742-12498-1-git-send-email-kjwinchester@gmail.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 523131213f08..3e6ff6cbf42a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -181,7 +181,6 @@ static void __cpuinit trap_init_f00f_bug(void)
 
 static void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_SMP
 	/* calling is from identify_secondary_cpu() ? */
 	if (!c->cpu_index)
 		return;
@@ -198,7 +197,6 @@ static void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)
 		WARN_ONCE(1, "WARNING: SMP operation may be unreliable"
 				    "with B stepping processors.\n");
 	}
-#endif
 }
 
 static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)

commit 30963c0ac721f70aea0352ed1cd0fc3cfbaef730
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Oct 12 17:46:34 2011 -0700

    x86, intel: Use c->microcode for Atom errata check
    
    Now that the cpu update level is available the Atom PSE errata
    check can use it directly without reading the MSR again.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1318466795-7393-2-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 26627a3a7148..523131213f08 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -64,17 +64,10 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 	 * need the microcode to have already been loaded... so if it is
 	 * not, recommend a BIOS update and disable large pages.
 	 */
-	if (c->x86 == 6 && c->x86_model == 0x1c && c->x86_mask <= 2) {
-		u32 ucode, junk;
-
-		wrmsr(MSR_IA32_UCODE_REV, 0, 0);
-		sync_core();
-		rdmsr(MSR_IA32_UCODE_REV, junk, ucode);
-
-		if (ucode < 0x20e) {
-			printk(KERN_WARNING "Atom PSE erratum detected, BIOS microcode update recommended\n");
-			clear_cpu_cap(c, X86_FEATURE_PSE);
-		}
+	if (c->x86 == 6 && c->x86_model == 0x1c && c->x86_mask <= 2 &&
+	    c->microcode < 0x20e) {
+		printk(KERN_WARNING "Atom PSE erratum detected, BIOS microcode update recommended\n");
+		clear_cpu_cap(c, X86_FEATURE_PSE);
 	}
 
 #ifdef CONFIG_X86_64

commit 506ed6b53e00ba303ad778122f08e1fca7cf5efb
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Oct 12 17:46:33 2011 -0700

    x86, intel: Output microcode revision in /proc/cpuinfo
    
    I got a request to make it easier to determine the microcode
    update level on Intel CPUs. This patch adds a new "microcode"
    field to /proc/cpuinfo.
    
    The microcode level is also outputed on fatal machine checks
    together with the other CPUID model information.
    
    I removed the respective code from the microcode update driver,
    it just reads the field from cpu_data. Also when the microcode
    is updated it fills in the new values too.
    
    I had to add a memory barrier to native_cpuid to prevent it
    being optimized away when the result is not used.
    
    This turns out to clean up further code which already got this
    information manually. This is done in followon patches.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1318466795-7393-1-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index ed6086eedf1d..26627a3a7148 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -47,6 +47,15 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 		(c->x86 == 0x6 && c->x86_model >= 0x0e))
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 
+	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64)) {
+		unsigned lower_word;
+
+		wrmsr(MSR_IA32_UCODE_REV, 0, 0);
+		/* Required by the SDM */
+		sync_core();
+		rdmsr(MSR_IA32_UCODE_REV, lower_word, c->microcode);
+	}
+
 	/*
 	 * Atom erratum AAE44/AAF40/AAG38/AAH41:
 	 *

commit 17edf2d79f1ea6dfdb4c444801d928953b9f98d6
Author: Len Brown <len.brown@intel.com>
Date:   Fri Jul 15 17:37:15 2011 -0400

    x86, intel, power: Correct the MSR_IA32_ENERGY_PERF_BIAS message
    
    Fix the printk_once() so that it actually prints (didn't print before
    due to a stray comma.)
    
    [ hpa: changed to an incremental patch and adjusted the description
      accordingly. ]
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1107151732480.18606@x980
    Cc: <table@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index da0d779ecd9e..ed6086eedf1d 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -465,11 +465,11 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		u64 epb;
 
 		rdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
-		if ((epb & 0xF) == 0) {
-			printk_once(KERN_WARNING, "x86: updated energy_perf_bias"
-				" to 'normal' from 'performance'\n"
-				"You can view and update epb via utility,"
-				" such as x86_energy_perf_policy(8)\n");
+		if ((epb & 0xF) == ENERGY_PERF_BIAS_PERFORMANCE) {
+			printk_once(KERN_WARNING "ENERGY_PERF_BIAS:"
+				" Set to 'normal', was 'performance'\n"
+				"ENERGY_PERF_BIAS: View and update with"
+				" x86_energy_perf_policy(8)\n");
 			epb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;
 			wrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
 		}

commit abe48b108247e9b90b4c6739662a2e5c765ed114
Author: Len Brown <len.brown@intel.com>
Date:   Thu Jul 14 00:53:24 2011 -0400

    x86, intel, power: Initialize MSR_IA32_ENERGY_PERF_BIAS
    
    Since 2.6.36 (23016bf0d25), Linux prints the existence of "epb" in /proc/cpuinfo,
    Since 2.6.38 (d5532ee7b40), the x86_energy_perf_policy(8) utility has
    been available in-tree to update MSR_IA32_ENERGY_PERF_BIAS.
    
    However, the typical BIOS fails to initialize the MSR, presumably
    because this is handled by high-volume shrink-wrap operating systems...
    
    Linux distros, on the other hand, do not yet invoke x86_energy_perf_policy(8).
    As a result, WSM-EP, SNB, and later hardware from Intel will run in its
    default hardware power-on state (performance), which assumes that users
    care for performance at all costs and not for energy efficiency.
    While that is fine for performance benchmarks, the hardware's intended default
    operating point is "normal" mode...
    
    Initialize the MSR to the "normal" by default during kernel boot.
    
    x86_energy_perf_policy(8) is available to change the default after boot,
    should the user have a different preference.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1107140051020.18606@x980
    Acked-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: <stable@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1edf5ba4fb2b..da0d779ecd9e 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -456,6 +456,24 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 
 	if (cpu_has(c, X86_FEATURE_VMX))
 		detect_vmx_virtcap(c);
+
+	/*
+	 * Initialize MSR_IA32_ENERGY_PERF_BIAS if BIOS did not.
+	 * x86_energy_perf_policy(8) is available to change it at run-time
+	 */
+	if (cpu_has(c, X86_FEATURE_EPB)) {
+		u64 epb;
+
+		rdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
+		if ((epb & 0xF) == 0) {
+			printk_once(KERN_WARNING, "x86: updated energy_perf_bias"
+				" to 'normal' from 'performance'\n"
+				"You can view and update epb via utility,"
+				" such as x86_energy_perf_policy(8)\n");
+			epb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;
+			wrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);
+		}
+	}
 }
 
 #ifdef CONFIG_X86_32

commit 016281880439a8665ecf37514865742da58131d4
Merge: 17b141803c6c 865be7a81071
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 17:55:12 2011 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, cpu: Fix detection of Celeron Covington stepping A1 and B0
      Documentation, ABI: Update L3 cache index disable text
      x86, AMD, cacheinfo: Fix L3 cache index disable checks
      x86, AMD, cacheinfo: Fix fallout caused by max3 conversion
      x86, cpu: Change NOP selection for certain Intel CPUs
      x86, cpu: Clean up and unify the NOP selection infrastructure
      x86, percpu: Use ASM_NOP4 instead of hardcoding P6_NOP4
      x86, cpu: Move AMD Elan Kconfig under "Processor family"
    
    Fix up trivial conflicts in alternative handling (commit dc326fca2b64
    "x86, cpu: Clean up and unify the NOP selection infrastructure" removed
    some hacky 5-byte instruction stuff, while commit d430d3d7e646 "jump
    label: Introduce static_branch() interface" renamed HAVE_JUMP_LABEL to
    CONFIG_JUMP_LABEL in the code that went away)

commit 161ec53c702ce9df2f439804dfb9331807066daa
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue May 17 15:29:11 2011 -0700

    x86, mem, intel: Initialize Enhanced REP MOVSB/STOSB
    
    If kernel intends to use enhanced REP MOVSB/STOSB, it must ensure
    IA32_MISC_ENABLE.Fast_String_Enable (bit 0) is set and CPUID.(EAX=07H, ECX=0H):
    EBX[bit 9] also reports 1.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1305671358-14478-3-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index df86bc8c859d..fc73a34ba8c9 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -29,10 +29,10 @@
 
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
+	u64 misc_enable;
+
 	/* Unmask CPUID levels if masked: */
 	if (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {
-		u64 misc_enable;
-
 		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
 
 		if (misc_enable & MSR_IA32_MISC_ENABLE_LIMIT_CPUID) {
@@ -118,8 +118,6 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 	 * (model 2) with the same problem.
 	 */
 	if (c->x86 == 15) {
-		u64 misc_enable;
-
 		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
 
 		if (misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING) {
@@ -130,6 +128,19 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 		}
 	}
 #endif
+
+	/*
+	 * If fast string is not enabled in IA32_MISC_ENABLE for any reason,
+	 * clear the fast string and enhanced fast string CPU capabilities.
+	 */
+	if (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {
+		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
+		if (!(misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING)) {
+			printk(KERN_INFO "Disabled fast string operations\n");
+			setup_clear_cpu_cap(X86_FEATURE_REP_GOOD);
+			setup_clear_cpu_cap(X86_FEATURE_ERMS);
+		}
+	}
 }
 
 #ifdef CONFIG_X86_32

commit 865be7a81071a77014c83cd01536c989eed362b4
Author: Ondrej Zary <linux@rainbow-software.org>
Date:   Mon May 16 21:38:08 2011 +0200

    x86, cpu: Fix detection of Celeron Covington stepping A1 and B0
    
    Steppings A1 and B0 of Celeron Covington are currently misdetected as
    Pentium II (Dixon). Fix it by removing the stepping check.
    
    [ hpa: this fixes this specific bug... the CPUID documentation
      specifies that the L2 cache size can disambiguate additional CPUs;
      this patch does not fix that. ]
    
    Signed-off-by: Ondrej Zary <linux@rainbow-software.org>
    Link: http://lkml.kernel.org/r/201105162138.15416.linux@rainbow-software.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index df86bc8c859d..32e86aa52743 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -400,12 +400,10 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 
 		switch (c->x86_model) {
 		case 5:
-			if (c->x86_mask == 0) {
-				if (l2 == 0)
-					p = "Celeron (Covington)";
-				else if (l2 == 256)
-					p = "Mobile Pentium II (Dixon)";
-			}
+			if (l2 == 0)
+				p = "Celeron (Covington)";
+			else if (l2 == 256)
+				p = "Mobile Pentium II (Dixon)";
 			break;
 
 		case 6:

commit 645a79195f66eb68ef3ab2b21d9829ac3aa085a9
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:40 2011 +0100

    x86: Unify CPU -> NUMA node mapping between 32 and 64bit
    
    Unlike 64bit, 32bit has been using its own cpu_to_node_map[] for
    CPU -> NUMA node mapping.  Replace it with early_percpu variable
    x86_cpu_to_node_map and share the mapping code with 64bit.
    
    * USE_PERCPU_NUMA_NODE_ID is now enabled for 32bit too.
    
    * x86_cpu_to_node_map and numa_set/clear_node() are moved from
      numa_64 to numa.  For now, on 32bit, x86_cpu_to_node_map is initialized
      with 0 instead of NUMA_NO_NODE.  This is to avoid introducing unexpected
      behavior change and will be updated once init path is unified.
    
    * srat_detect_node() is now enabled for x86_32 too.  It calls
      numa_set_node() and initializes the mapping making explicit
      cpu_to_node_map[] updates from map/unmap_cpu_to_node() unnecessary.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: penberg@kernel.org
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-15-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 6052004bf4f4..df86bc8c859d 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -276,7 +276,7 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 
 static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 {
-#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+#ifdef CONFIG_NUMA
 	unsigned node;
 	int cpu = smp_processor_id();
 

commit bbc9e2f452d9c4b166d1f9a78d941d80173312fe
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:39 2011 +0100

    x86: Unify cpu/apicid <-> NUMA node mapping between 32 and 64bit
    
    The mapping between cpu/apicid and node is done via
    apicid_to_node[] on 64bit and apicid_2_node[] +
    apic->x86_32_numa_cpu_node() on 32bit. This difference makes it
    difficult to further unify 32 and 64bit NUMA handling.
    
    This patch unifies it by replacing both apicid_to_node[] and
    apicid_2_node[] with __apicid_to_node[] array, which is accessed
    by two accessors - set_apicid_to_node() and numa_cpu_node().  On
    64bit, numa_cpu_node() always consults __apicid_to_node[]
    directly while 32bit goes through apic->numa_cpu_node() method
    to allow apic implementations to override it.
    
    srat_detect_node() for amd cpus contains workaround for broken
    NUMA configuration which assumes relationship between APIC ID,
    HT node ID and NUMA topology.  Leave it to access
    __apicid_to_node[] directly as mapping through CPU might result
    in undesirable behavior change.  The comment is reformatted and
    updated to note the ugliness.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-14-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index d16c2c53d6bf..6052004bf4f4 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -279,11 +279,10 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
 	unsigned node;
 	int cpu = smp_processor_id();
-	int apicid = cpu_has_apic ? hard_smp_processor_id() : c->apicid;
 
 	/* Don't do the funky fallback heuristics the AMD version employs
 	   for now. */
-	node = apicid_to_node[apicid];
+	node = numa_cpu_node(cpu);
 	if (node == NUMA_NO_NODE || !node_online(node)) {
 		/* reuse the value from init_cpu_to_node() */
 		node = cpu_to_node(cpu);

commit 02f36038c568111ad4fc433f6fa760ff5e38fab4
Merge: 6c2754c28f23 676cb02dc32a 07bd8516a2f9 50f2d7f682f9 892df7f81c31 68f4d5a00ada b365a85c6816 6554287b1de0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 23 08:25:36 2010 -0700

    Merge branches 'softirq-for-linus', 'x86-debug-for-linus', 'x86-numa-for-linus', 'x86-quirks-for-linus', 'x86-setup-for-linus', 'x86-uv-for-linus' and 'x86-vm86-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'softirq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      softirqs: Make wakeup_softirqd static
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, asm: Restore parentheses around one pushl_cfi argument
      x86, asm: Fix ancient-GAS workaround
      x86, asm: Fix CFI macro invocations to deal with shortcomings in gas
    
    * 'x86-numa-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, numa: Assign CPUs to nodes in round-robin manner on fake NUMA
    
    * 'x86-quirks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: HPET force enable for CX700 / VIA Epia LT
    
    * 'x86-setup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, setup: Use string copy operation to optimze copy in kernel compression
    
    * 'x86-uv-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, UV: Use allocated buffer in tlb_uv.c:tunables_read()
    
    * 'x86-vm86-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, vm86: Fix preemption bug for int1 debug and int3 breakpoint handlers.

commit d60a2793ba562c6ea9bbf62112da3e6342adcf83
Merge: 781c5a67f152 40ffa9379198
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:18:06 2010 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Remove stale pmtimer_64.c
      x86, cleanups: Use clear_page/copy_page rather than memset/memcpy
      x86: Remove unnecessary #ifdef ACPI/X86_IO_ACPI
      x86, cleanup: Remove obsolete boot_cpu_id variable

commit 50f2d7f682f9c0ed58191d0982fe77888d59d162
Author: Nikanth Karthikesan <knikanth@suse.de>
Date:   Thu Sep 30 17:34:10 2010 +0530

    x86, numa: Assign CPUs to nodes in round-robin manner on fake NUMA
    
    commit d9c2d5ac6af87b4491bff107113aaf16f6c2b2d9 "x86, numa: Use near(er)
    online node instead of roundrobin for NUMA" changed NUMA initialization on
    Intel to choose the nearest online node or first node.  Fake NUMA would be
    better of with round-robin initialization, instead of the all CPUS on
    first node.  Change the choice of first node, back to round-robin.
    
    For testing NUMA kernel behaviour without cpusets and NUMA aware
    applications, it would be better to have cpus in different nodes, rather
    than all in a single node.  With cpusets migration of tasks scenarios
    cannot not be tested.
    
    I guess having it round-robin shouldn't affect the use cases for all cpus
    on the first node.
    
    The code comments in arch/x86/mm/numa_64.c:759 indicate that this used to
    be the case, which was changed by commit d9c2d5ac6.  It changed from
    roundrobin to nearer or first node.  And I couldn't find any reason for
    this change in its changelog.
    
    Signed-off-by: Nikanth Karthikesan <knikanth@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index b4389441efbb..6d61786c2dd5 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -284,9 +284,7 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 	/* Don't do the funky fallback heuristics the AMD version employs
 	   for now. */
 	node = apicid_to_node[apicid];
-	if (node == NUMA_NO_NODE)
-		node = first_node(node_online_map);
-	else if (!node_online(node)) {
+	if (node == NUMA_NO_NODE || !node_online(node)) {
 		/* reuse the value from init_cpu_to_node() */
 		node = cpu_to_node(cpu);
 	}

commit d900329e20f4476db6461752accebcf7935a8055
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Sep 28 15:35:01 2010 -0700

    x86, cpu: After uncapping CPUID, re-run CPU feature detection
    
    After uncapping the CPUID level, we need to also re-run the CPU
    feature detection code.
    
    This resolves kernel bugzilla 16322.
    
    Reported-by: boris64 <bugzilla.kernel.org@boris64.net>
    Cc: <stable@kernel.org> v2.6.29..2.6.35
    LKML-Reference: <tip-@git.kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 85f69cdeae10..b4389441efbb 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -39,6 +39,7 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 			misc_enable &= ~MSR_IA32_MISC_ENABLE_LIMIT_CPUID;
 			wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
 			c->cpuid_level = cpuid_eax(0);
+			get_cpu_cap(c);
 		}
 	}
 

commit f6e9456c9272bb570df6e217cdbe007e270b1c4e
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jul 21 19:03:58 2010 +0200

    x86, cleanup: Remove obsolete boot_cpu_id variable
    
    boot_cpu_id is there for historical reasons and was renamed to
    boot_cpu_physical_apicid in patch:
    
     c70dcb7 x86: change boot_cpu_id to boot_cpu_physical_apicid
    
    However, there are some remaining occurrences of boot_cpu_id that are
    never touched in the kernel and thus its value is always 0.
    
    This patch removes boot_cpu_id completely.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1279731838-1522-8-git-send-email-robert.richter@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 85f69cdeae10..3a683ea5267e 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -169,7 +169,7 @@ static void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
 	/* calling is from identify_secondary_cpu() ? */
-	if (c->cpu_index == boot_cpu_id)
+	if (!c->cpu_index)
 		return;
 
 	/*

commit 07d77759c95d899b84f8e473a01cff001019dd5f
Merge: b7723f9d21d8 3998d095354d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:49:13 2010 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hypervisor: add missing <linux/module.h>
      Modify the VMware balloon driver for the new x86_hyper API
      x86, hypervisor: Export the x86_hyper* symbols
      x86: Clean up the hypervisor layer
      x86, HyperV: fix up the license to mshyperv.c
      x86: Detect running on a Microsoft HyperV system
      x86, cpu: Make APERF/MPERF a normal table-driven flag
      x86, k8: Fix build error when K8_NB is disabled
      x86, cacheinfo: Disable index in all four subcaches
      x86, cacheinfo: Make L3 cache info per node
      x86, cacheinfo: Reorganize AMD L3 cache structure
      x86, cacheinfo: Turn off L3 cache index disable feature in virtualized environments
      x86, cacheinfo: Unify AMD L3 cache index disable checking
      cpufreq: Unify sysfs attribute definition macros
      powernow-k8: Fix frequency reporting
      x86, cpufreq: Add APERF/MPERF support for AMD processors
      x86: Unify APERF/MPERF support
      powernow-k8: Add core performance boost support
      x86, cpu: Add AMD core boosting feature flag to /proc/cpuinfo
    
    Fix up trivial conflicts in arch/x86/kernel/cpu/intel_cacheinfo.c and
    drivers/cpufreq/cpufreq_ondemand.c

commit d7be0ce6afb1df60bc786f57410407ceae92b994
Merge: e08cae4181af 66f41d4c5c8a
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Sat May 8 14:59:58 2010 -0700

    Merge commit 'v2.6.34-rc6' into x86/cpu

commit 3ca50496c2677a2b3fdd3ede86660fd1433beac6
Merge: 462b04e28a7e 66f41d4c5c8a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 30 09:56:41 2010 +0200

    Merge commit 'v2.6.34-rc6' into perf/core
    
    Merge reason: update to the latest -rc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 7a0fc404ae663776e96db43879a0fa24fec1fa3a
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Apr 13 14:40:54 2010 -0700

    x86: Disable large pages on CPUs with Atom erratum AAE44
    
    Atom erratum AAE44/AAF40/AAG38/AAH41:
    
    "If software clears the PS (page size) bit in a present PDE (page
    directory entry), that will cause linear addresses mapped through this
    PDE to use 4-KByte pages instead of using a large page after old TLB
    entries are invalidated. Due to this erratum, if a code fetch uses
    this PDE before the TLB entry for the large page is invalidated then
    it may fetch from a different physical address than specified by
    either the old large page translation or the new 4-KByte page
    translation. This erratum may also cause speculative code fetches from
    incorrect addresses."
    
    [http://download.intel.com/design/processor/specupdt/319536.pdf]
    
    Where as commit 211b3d03c7400f48a781977a50104c9d12f4e229 seems to
    workaround errata AAH41 (mixed 4K TLBs) it reduces the window of
    opportunity for the bug to occur and does not totally remove it.  This
    patch disables mixed 4K/4MB page tables totally avoiding the page
    splitting and not tripping this processor issue.
    
    This is based on an original patch by Colin King.
    
    Originally-by: Colin Ian King <colin.king@canonical.com>
    Cc: Colin Ian King <colin.king@canonical.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    LKML-Reference: <1269271251-19775-1-git-send-email-colin.king@canonical.com>
    Cc: <stable@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 7e1cca13af35..1366c7cfd483 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -47,6 +47,27 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 		(c->x86 == 0x6 && c->x86_model >= 0x0e))
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 
+	/*
+	 * Atom erratum AAE44/AAF40/AAG38/AAH41:
+	 *
+	 * A race condition between speculative fetches and invalidating
+	 * a large page.  This is worked around in microcode, but we
+	 * need the microcode to have already been loaded... so if it is
+	 * not, recommend a BIOS update and disable large pages.
+	 */
+	if (c->x86 == 6 && c->x86_model == 0x1c && c->x86_mask <= 2) {
+		u32 ucode, junk;
+
+		wrmsr(MSR_IA32_UCODE_REV, 0, 0);
+		sync_core();
+		rdmsr(MSR_IA32_UCODE_REV, junk, ucode);
+
+		if (ucode < 0x20e) {
+			printk(KERN_WARNING "Atom PSE erratum detected, BIOS microcode update recommended\n");
+			clear_cpu_cap(c, X86_FEATURE_PSE);
+		}
+	}
+
 #ifdef CONFIG_X86_64
 	set_cpu_cap(c, X86_FEATURE_SYSENTER32);
 #else

commit d65ad45cd82a0db9544469b8c54f5dc5cafbb2d8
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Wed Mar 31 21:56:43 2010 +0200

    x86: Unify APERF/MPERF support
    
    Initialize this CPUID flag feature in common code. It could be made a
    standalone function later, maybe, if more functionality is duplicated.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <1270065406-1814-4-git-send-email-bp@amd64.org>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 7e1cca13af35..3830258a5f53 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -352,12 +352,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
 	}
 
-	if (c->cpuid_level > 6) {
-		unsigned ecx = cpuid_ecx(6);
-		if (ecx & 0x01)
-			set_cpu_cap(c, X86_FEATURE_APERFMPERF);
-	}
-
 	if (cpu_has_xmm2)
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
 	if (cpu_has_ds) {

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 7e1cca13af35..d72377c41c76 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -12,7 +12,6 @@
 #include <asm/processor.h>
 #include <asm/pgtable.h>
 #include <asm/msr.h>
-#include <asm/ds.h>
 #include <asm/bugs.h>
 #include <asm/cpu.h>
 
@@ -367,7 +366,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_BTS);
 		if (!(l1 & (1<<12)))
 			set_cpu_cap(c, X86_FEATURE_PEBS);
-		ds_init_intel(c);
 	}
 
 	if (c->x86 == 6 && c->x86_model == 29 && cpu_has_clflush)

commit 14be1f7454ea96ee614467a49cf018a1a383b189
Author: Dimitri Sivanich <sivanich@sgi.com>
Date:   Mon Mar 1 11:48:15 2010 -0600

    x86: Fix sched_clock_cpu for systems with unsynchronized TSC
    
    On UV systems, the TSC is not synchronized across blades.  The
    sched_clock_cpu() function is returning values that can go
    backwards  (I've seen as much as 8 seconds) when switching
    between cpus.
    
    As each cpu comes up, early_init_intel() will currently set the
    sched_clock_stable flag true.  When mark_tsc_unstable() runs, it
    clears the flag, but this only occurs once (the first time a cpu
    comes up whose TSC is not synchronized with cpu 0).  After this,
    early_init_intel() will set the flag again as the next cpu comes
    up.
    
    Only set sched_clock_stable if tsc has not been marked unstable.
    
    Signed-off-by: Dimitri Sivanich <sivanich@sgi.com>
    Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100301174815.GC8224@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 879666f4d871..7e1cca13af35 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -70,7 +70,8 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
-		sched_clock_stable = 1;
+		if (!check_tsc_unstable())
+			sched_clock_stable = 1;
 	}
 
 	/*

commit 6c56ccecf05fafe100ab4ea94f6fccbf5ff00db7
Author: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
Date:   Thu Dec 17 12:27:02 2009 -0800

    x86: Reenable TSC sync check at boot, even with NONSTOP_TSC
    
    Commit 83ce4009 did the following change
    If the TSC is constant and non-stop, also set it reliable.
    
    But, there seems to be few systems that will end up with TSC warp across
    sockets, depending on how the cpus come out of reset. Skipping TSC sync
    test on such systems may result in time inconsistency later.
    
    So, reenable TSC sync test even on constant and non-stop TSC systems.
    Set, sched_clock_stable to 1 by default and reset it in
    mark_tsc_unstable, if TSC sync fails.
    
    This change still gives perf benefit mentioned in 83ce4009 for systems
    where TSC is reliable.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <20091217202702.GA18015@linux-os.sc.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 9c31e8b09d2c..879666f4d871 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -70,7 +70,6 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
-		set_cpu_cap(c, X86_FEATURE_TSC_RELIABLE);
 		sched_clock_stable = 1;
 	}
 

commit 2eaad1fddd7450a48ad464229775f97fbfe8af36
Author: Mike Travis <travis@sgi.com>
Date:   Thu Dec 10 17:19:36 2009 -0800

    x86: Limit the number of processor bootup messages
    
    When there are a large number of processors in a system, there
    is an excessive amount of messages sent to the system console.
    It's estimated that with 4096 processors in a system, and the
    console baudrate set to 56K, the startup messages will take
    about 84 minutes to clear the serial port.
    
    This set of patches limits the number of repetitious messages
    which contain no additional information.  Much of this information
    is obtainable from the /proc and /sysfs.   Some of the messages
    are also sent to the kernel log buffer as KERN_DEBUG messages so
    dmesg can be used to examine more closely any details specific to
    a problem.
    
    The new cpu bootup sequence for system_state == SYSTEM_BOOTING:
    
    Booting Node   0, Processors  #1 #2 #3 #4 #5 #6 #7 Ok.
    Booting Node   1, Processors  #8 #9 #10 #11 #12 #13 #14 #15 Ok.
    ...
    Booting Node   3, Processors  #56 #57 #58 #59 #60 #61 #62 #63 Ok.
    Brought up 64 CPUs
    
    After the system is running, a single line boot message is displayed
    when CPU's are hotplugged on:
    
        Booting Node %d Processor %d APIC 0x%x
    
    Status of the following lines:
    
        CPU: Physical Processor ID:         printed once (for boot cpu)
        CPU: Processor Core ID:             printed once (for boot cpu)
        CPU: Hyper-Threading is disabled    printed once (for boot cpu)
        CPU: Thermal monitoring enabled     printed once (for boot cpu)
        CPU %d/0x%x -> Node %d:             removed
        CPU %d is now offline:              only if system_state == RUNNING
        Initializing CPU#%d:                KERN_DEBUG
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    LKML-Reference: <4B219E28.8080601@sgi.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index c900b73f9224..9c31e8b09d2c 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -270,8 +270,6 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 		node = cpu_to_node(cpu);
 	}
 	numa_set_node(cpu, node);
-
-	printk(KERN_INFO "CPU %d/0x%x -> Node %d\n", cpu, apicid, node);
 #endif
 }
 

commit d9c2d5ac6af87b4491bff107113aaf16f6c2b2d9
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat Nov 21 00:23:37 2009 -0800

    x86, numa: Use near(er) online node instead of roundrobin for NUMA
    
    CPU to node mapping is set via the following sequence:
    
     1. numa_init_array(): Set up roundrobin from cpu to online node
    
     2. init_cpu_to_node(): Set that according to apicid_to_node[]
                            according to srat only handle the node that
                            is online, and leave other cpu on node
                            without ram (aka not online) to still
                            roundrobin.
    
    3. later call srat_detect_node for Intel/AMD, will use first_online
       node or nearby node.
    
    Problem is that setup_per_cpu_areas() is not called between 2 and 3,
    the per_cpu for cpu on node with ram is on different node, and could
    put that on node with two hops away.
    
    So try to optimize this and add find_near_online_node() and call
    init_cpu_to_node().
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <4B07A739.3030104@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 40e1835b35e8..c900b73f9224 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -263,8 +263,12 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 	/* Don't do the funky fallback heuristics the AMD version employs
 	   for now. */
 	node = apicid_to_node[apicid];
-	if (node == NUMA_NO_NODE || !node_online(node))
+	if (node == NUMA_NO_NODE)
 		node = first_node(node_online_map);
+	else if (!node_online(node)) {
+		/* reuse the value from init_cpu_to_node() */
+		node = cpu_to_node(cpu);
+	}
 	numa_set_node(cpu, node);
 
 	printk(KERN_INFO "CPU %d/0x%x -> Node %d\n", cpu, apicid, node);

commit a8303aaf2b2f74714db6d204ab4fcb810942664e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 2 10:56:56 2009 +0200

    x86: Move APERF/MPERF into a X86_FEATURE
    
    Move the APERFMPERF capacility into a X86_FEATURE flag so that it
    can be used outside of the acpi cpufreq driver.
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: cpufreq@vger.kernel.org
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 80a722a071b5..40e1835b35e8 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -350,6 +350,12 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
 	}
 
+	if (c->cpuid_level > 6) {
+		unsigned ecx = cpuid_ecx(6);
+		if (ecx & 0x01)
+			set_cpu_cap(c, X86_FEATURE_APERFMPERF);
+	}
+
 	if (cpu_has_xmm2)
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
 	if (cpu_has_ds) {

commit 8bdbd962ecfcbdd96f9dbb02d780b4553afd2543
Author: Alan Cox <alan@linux.intel.com>
Date:   Sat Jul 4 00:35:45 2009 +0100

    x86/cpu: Clean up various files a bit
    
    No code changes except printk levels (although some of the K6
    mtrr code might be clearer if there were a few as would
    splitting out some of the intel cache code).
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 3260ab044996..80a722a071b5 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -7,17 +7,17 @@
 #include <linux/sched.h>
 #include <linux/thread_info.h>
 #include <linux/module.h>
+#include <linux/uaccess.h>
 
 #include <asm/processor.h>
 #include <asm/pgtable.h>
 #include <asm/msr.h>
-#include <asm/uaccess.h>
 #include <asm/ds.h>
 #include <asm/bugs.h>
 #include <asm/cpu.h>
 
 #ifdef CONFIG_X86_64
-#include <asm/topology.h>
+#include <linux/topology.h>
 #include <asm/numa_64.h>
 #endif
 
@@ -174,7 +174,8 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 #ifdef CONFIG_X86_F00F_BUG
 	/*
 	 * All current models of Pentium and Pentium with MMX technology CPUs
-	 * have the F0 0F bug, which lets nonprivileged users lock up the system.
+	 * have the F0 0F bug, which lets nonprivileged users lock up the
+	 * system.
 	 * Note that the workaround only should be initialized once...
 	 */
 	c->f00f_bug = 0;
@@ -207,7 +208,7 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 			printk (KERN_INFO "CPU: C0 stepping P4 Xeon detected.\n");
 			printk (KERN_INFO "CPU: Disabling hardware prefetching (Errata 037)\n");
 			lo |= MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE;
-			wrmsr (MSR_IA32_MISC_ENABLE, lo, hi);
+			wrmsr(MSR_IA32_MISC_ENABLE, lo, hi);
 		}
 	}
 
@@ -283,7 +284,7 @@ static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
 	/* Intel has a non-standard dependency on %ecx for this CPUID level. */
 	cpuid_count(4, 0, &eax, &ebx, &ecx, &edx);
 	if (eax & 0x1f)
-		return ((eax >> 26) + 1);
+		return (eax >> 26) + 1;
 	else
 		return 1;
 }

commit f85612967c93b67b10dd240e3e8bf8a0eee9def7
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Fri Apr 4 00:53:23 2008 +0200

    x86: add hooks for kmemcheck
    
    The hooks that we modify are:
    - Page fault handler (to handle kmemcheck faults)
    - Debug exception handler (to hide pages after single-stepping
      the instruction that caused the page fault)
    
    Also redefine memset() to use the optimized version if kmemcheck is
    enabled.
    
    (Thanks to Pekka Enberg for minimizing the impact on the page fault
    handler.)
    
    As kmemcheck doesn't handle MMX/SSE instructions (yet), we also disable
    the optimized xor code, and rely instead on the generic C implementation
    in order to avoid false-positive warnings.
    
    Signed-off-by: Vegard Nossum <vegardno@ifi.uio.no>
    
    [whitespace fixlet]
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    [rebased for mainline inclusion]
    Signed-off-by: Vegard Nossum <vegardno@ifi.uio.no>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index daed39ba2614..3260ab044996 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -86,6 +86,29 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 	 */
 	if (c->x86 == 6 && c->x86_model < 15)
 		clear_cpu_cap(c, X86_FEATURE_PAT);
+
+#ifdef CONFIG_KMEMCHECK
+	/*
+	 * P4s have a "fast strings" feature which causes single-
+	 * stepping REP instructions to only generate a #DB on
+	 * cache-line boundaries.
+	 *
+	 * Ingo Molnar reported a Pentium D (model 6) and a Xeon
+	 * (model 2) with the same problem.
+	 */
+	if (c->x86 == 15) {
+		u64 misc_enable;
+
+		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
+
+		if (misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING) {
+			printk(KERN_INFO "kmemcheck: Disabling fast string operations\n");
+
+			misc_enable &= ~MSR_IA32_MISC_ENABLE_FAST_STRING;
+			wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
+		}
+	}
+#endif
 }
 
 #ifdef CONFIG_X86_32

commit 2759c3287de27266e06f1f4e82cbd2d65f6a044c
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri May 15 13:05:16 2009 -0700

    x86: don't call read_apic_id if !cpu_has_apic
    
    should not call that if apic is disabled.
    
    [ Impact: fix crash on certain UP configs ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    LKML-Reference: <4A09CCBB.2000306@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 7437fa133c02..daed39ba2614 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -229,12 +229,12 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 }
 #endif
 
-static void __cpuinit srat_detect_node(void)
+static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 {
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
 	unsigned node;
 	int cpu = smp_processor_id();
-	int apicid = hard_smp_processor_id();
+	int apicid = cpu_has_apic ? hard_smp_processor_id() : c->apicid;
 
 	/* Don't do the funky fallback heuristics the AMD version employs
 	   for now. */
@@ -400,7 +400,7 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	}
 
 	/* Work around errata */
-	srat_detect_node();
+	srat_detect_node(c);
 
 	if (cpu_has(c, X86_FEATURE_VMX))
 		detect_vmx_virtcap(c);

commit 3fab191002b184e4390aa07c7149c6cc7b638ec7
Merge: 93394a761d78 7c730ccdc118
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Mar 28 22:27:45 2009 +0100

    Merge branch 'linus' into x86/core

commit 6e15cf04860074ad032e88c306bea656bbdd0f22
Merge: be0ea69674ed 60db56422043
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 26 21:39:17 2009 +0100

    Merge branch 'core/percpu' into percpu-cpumask-x86-for-linus-2
    
    Conflicts:
            arch/parisc/kernel/irq.c
            arch/x86/include/asm/fixmap_64.h
            arch/x86/include/asm/setup.h
            kernel/irq/handle.c
    
    Semantic merge:
            arch/x86/include/asm/fixmap.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 0ca0f16fd17c5d880dd0abbe03595b0c7c5b3c95
Merge: c550033ced48 7a81d9a7da03 88200bc28da3 0f3fa48a7eaf 91219bcbdccc 063402356280 773e673de272 5a8ac9d28dae 16a6791934a1 895791dac694
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Mar 14 16:25:40 2009 +0100

    Merge branches 'x86/apic', 'x86/asm', 'x86/cleanups', 'x86/debug', 'x86/kconfig', 'x86/mm', 'x86/ptrace', 'x86/setup' and 'x86/urgent'; commit 'v2.6.29-rc8' into x86/core

commit 13c6c53282d99c82e79b02477efd2c1e30a991ef
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Mar 12 12:37:34 2009 +0000

    x86, 32-bit: also use cpuinfo_x86's x86_{phys,virt}_bits members
    
    Impact: 32/64-bit consolidation
    
    In a first step, this allows fixing phys_addr_valid() for PAE (which
    until now reported all addresses to be valid). Subsequently, this will
    also allow simplifying some MTRR handling code.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <49B9101E.76E4.0078.0@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 191117f1ad51..ae769471042e 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -54,6 +54,11 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 		c->x86_cache_alignment = 128;
 #endif
 
+	/* CPUID workaround for 0F33/0F34 CPU */
+	if (c->x86 == 0xF && c->x86_model == 0x3
+	    && (c->x86_mask == 0x3 || c->x86_mask == 0x4))
+		c->x86_phys_bits = 36;
+
 	/*
 	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate
 	 * with P/T states and does not stop in deep C-states

commit 02dde8b45c5460794b9052d7c12939fe3eb63c2c
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Mar 12 12:08:49 2009 +0000

    x86: move various CPU initialization objects into .cpuinit.rodata
    
    Impact: debuggability and micro-optimization
    
    Putting whatever is possible into the (final) .rodata section increases
    the likelihood of catching memory corruption bugs early, and reduces
    false cache line sharing.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <49B90961.76E4.0078.0@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 191117f1ad51..968f15129ed8 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -410,7 +410,7 @@ static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned i
 }
 #endif
 
-static struct cpu_dev intel_cpu_dev __cpuinitdata = {
+static const struct cpu_dev __cpuinitconst intel_cpu_dev = {
 	.c_vendor	= "Intel",
 	.c_ident	= { "GenuineIntel" },
 #ifdef CONFIG_X86_32

commit 1f442d70c84aa798e243e721eba728a98434cd86
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat Mar 7 23:46:26 2009 -0800

    x86: remove smp_apply_quirks()/smp_checks()
    
    Impact: cleanup and code size reduction on 64-bit
    
    This code is only applied to Intel Pentium and AMD K7 32-bit cpus.
    
    Move those checks to intel_init()/amd_init() for 32-bit
    so 64-bit will not build this code.
    
    Also change to use cpu_index check to see if we need to emit warning.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <49B377D2.8030108@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 25c559ba8d54..191117f1ad51 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -13,6 +13,7 @@
 #include <asm/uaccess.h>
 #include <asm/ds.h>
 #include <asm/bugs.h>
+#include <asm/cpu.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/topology.h>
@@ -110,6 +111,28 @@ static void __cpuinit trap_init_f00f_bug(void)
 }
 #endif
 
+static void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_SMP
+	/* calling is from identify_secondary_cpu() ? */
+	if (c->cpu_index == boot_cpu_id)
+		return;
+
+	/*
+	 * Mask B, Pentium, but not Pentium MMX
+	 */
+	if (c->x86 == 5 &&
+	    c->x86_mask >= 1 && c->x86_mask <= 4 &&
+	    c->x86_model <= 3) {
+		/*
+		 * Remember we have B step Pentia with bugs
+		 */
+		WARN_ONCE(1, "WARNING: SMP operation may be unreliable"
+				    "with B stepping processors.\n");
+	}
+#endif
+}
+
 static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 {
 	unsigned long lo, hi;
@@ -186,6 +209,8 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 #ifdef CONFIG_X86_NUMAQ
 	numaq_tsc_disable();
 #endif
+
+	intel_smp_check(c);
 }
 #else
 static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)

commit 83ce400928680a6c8123d492684b27857f5a2d95
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 26 20:16:58 2009 +0100

    x86: set X86_FEATURE_TSC_RELIABLE
    
    If the TSC is constant and non-stop, also set it reliable.
    
    (We will turn this off in DMI quirks for multi-chassis systems)
    
    The performance number on a 16-way Nehalem system running
    32 tasks that context-switch between each other is significant:
    
       sched_clock_stable=0         sched_clock_stable=1
       ....................         ....................
       22.456925 million/sec        24.306972 million/sec   [+8.2%]
    
    lmbench's "lat_ctx -s 0 2" goes from 0.63 microseconds to
    0.59 microseconds - a 6.7% increase in context-switching
    performance.
    
    Perfstat of 1 million pipe context switches between two tasks:
    
     Performance counter stats for './pipe-test-1m':
    
           [before]           [after]
       ............      ............
       37621.421089      36436.848378    task clock ticks     (msecs)
    
                  0                 0    CPU migrations       (events)
            2000274           2000189    context switches     (events)
                194               193    pagefaults           (events)
         8433799643        8171016416    CPU cycles           (events) -3.21%
         8370133368        8180999694    instructions         (events) -2.31%
            4158565           3895941    cache references     (events) -6.74%
              44312             46264    cache misses         (events)
    
        2349.287976       2279.362465    wall-time            (msecs)  -3.06%
    
    The speedup comes straight from the reduction in the instruction
    count. sched_clock_cpu() got simpler and the whole workload thus
    executes faster.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 24ff26a38ade..5fff00c70de0 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -4,6 +4,7 @@
 #include <linux/string.h>
 #include <linux/bitops.h>
 #include <linux/smp.h>
+#include <linux/sched.h>
 #include <linux/thread_info.h>
 #include <linux/module.h>
 
@@ -56,11 +57,16 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 
 	/*
 	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate
-	 * with P/T states and does not stop in deep C-states
+	 * with P/T states and does not stop in deep C-states.
+	 *
+	 * It is also reliable across cores and sockets. (but not across
+	 * cabinets - we turn it off in that case explicitly.)
 	 */
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
+		set_cpu_cap(c, X86_FEATURE_TSC_RELIABLE);
+		sched_clock_stable = 1;
 	}
 
 }

commit 609162850db11dfe3b23f43b36888aa8b3ad7685
Merge: 3b6f7b9beb55 cb425afd2183 ecab22aa6dc9 e9ce0c37c2cb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 20 17:40:50 2009 +0100

    Merge branches 'x86/asm', 'x86/cleanups' and 'x86/headers' into x86/core

commit ecab22aa6dc9d42ca52de2cad0854b4c6bd85ac9
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Fri Feb 20 11:56:38 2009 +0100

    x86: use symbolic constants for MSR_IA32_MISC_ENABLE bits
    
    Impact: Cleanup. No functional changes.
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1f137a87d4bd..c8ff69a46681 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -147,10 +147,10 @@ static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 	 */
 	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
 		rdmsr(MSR_IA32_MISC_ENABLE, lo, hi);
-		if ((lo & (1<<9)) == 0) {
+		if ((lo & MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE) == 0) {
 			printk (KERN_INFO "CPU: C0 stepping P4 Xeon detected.\n");
 			printk (KERN_INFO "CPU: Disabling hardware prefetching (Errata 037)\n");
-			lo |= (1<<9);	/* Disable hw prefetching */
+			lo |= MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE;
 			wrmsr (MSR_IA32_MISC_ENABLE, lo, hi);
 		}
 	}

commit e641f5f525acb163ba71d92de79c9c7366deae03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 14:02:01 2009 +0100

    x86, apic: remove duplicate asm/apic.h inclusions
    
    Impact: cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 290f92e2b7c6..7aeef1d327b1 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -24,7 +24,6 @@
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
-#include <asm/apic.h>
 #endif
 
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)

commit 7b6aa335ca1a845c2262ec7a595b4521bca0f79d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 13:58:15 2009 +0100

    x86, apic: remove genapic.h
    
    Impact: cleanup
    
    Remove genapic.h and remove all references to it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 1f137a87d4bd..290f92e2b7c6 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -24,7 +24,7 @@
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
-#include <asm/genapic.h>
+#include <asm/apic.h>
 #endif
 
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)

commit f8a6b2b9cee298a9663cbe38ce1eb5240987cb62
Merge: ba1511bf7fbd 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:44:22 2009 +0100

    Merge branch 'linus' into x86/apic
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c
            arch/x86/mm/fault.c

commit e736ad548db152776de61d7a26805cfae77ce5ce
Author: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
Date:   Fri Feb 6 16:52:05 2009 -0800

    x86: add clflush before monitor for Intel 7400 series
    
    For Intel 7400 series CPUs, the recommendation is to use a clflush on the
    monitored address just before monitor and mwait pair [1].
    
    This clflush makes sure that there are no false wakeups from mwait when the
    monitored address was recently written to.
    
    [1] "MONITOR/MWAIT Recommendations for Intel Xeon Processor 7400 series"
        section in specification update document of 7400 series
        http://download.intel.com/design/xeon/specupdt/32033601.pdf
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 430e5c38a544..24ff26a38ade 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -291,6 +291,9 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		ds_init_intel(c);
 	}
 
+	if (c->x86 == 6 && c->x86_model == 29 && cpu_has_clflush)
+		set_cpu_cap(c, X86_FEATURE_CLFLUSH_MONITOR);
+
 #ifdef CONFIG_X86_64
 	if (c->x86 == 15)
 		c->x86_cache_alignment = c->x86_clflush_size * 2;

commit 1dcdd3d15ecea0c22a09d4d001a39d425fceff2c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 17:55:37 2009 +0100

    x86: remove mach_apic.h
    
    Spread mach_apic.h definitions into genapic.h. (with some knock-on effects
    on smp.h and apic.h.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 5deefae9064d..1cef0aa5e5dc 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -24,7 +24,7 @@
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
-#include <mach_apic.h>
+#include <asm/genapic.h>
 #endif
 
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)

commit 74b6eb6b937df07d0757e8642b7538b07da4290f
Merge: 6a385db5ce7f 2d4d57db692e 8f6d86dc4178 b38b06659055 d5e397cb49b5 e56d0cfe7790 dbca1df48e89 fb746d0e1365 6522869c3466 d639bab8da86 042cbaf88ab4 5662a2f8e731 3b4b75700a24 30a0fb947a68
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 23:13:53 2009 +0100

    Merge branches 'x86/asm', 'x86/cleanups', 'x86/cpudetect', 'x86/debug', 'x86/doc', 'x86/header-fixes', 'x86/mm', 'x86/paravirt', 'x86/pat', 'x86/setup-v2', 'x86/subarch', 'x86/uaccess' and 'x86/urgent' into x86/core

commit 30a0fb947a68ad3ab8a7184e3b3d79dce10e3688
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Mon Jan 26 09:40:58 2009 -0800

    x86: correct the CPUID pattern for MSR_IA32_MISC_ENABLE availability
    
    Impact: re-enable CPUID unmasking on affected processors
    
    As far as I am capable of discerning from the documentation,
    MSR_IA32_MISC_ENABLE should be available for all family 0xf CPUs, as
    well as family 6 for model >= 0xd (newer Pentium M).
    
    The documentation on this isn't ideal, so we need to be on the lookout
    for errors, still.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 549f2ada55f5..430e5c38a544 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -30,7 +30,7 @@
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
 	/* Unmask CPUID levels if masked: */
-	if (c->x86 == 6 && c->x86_model >= 15) {
+	if (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {
 		u64 misc_enable;
 
 		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);

commit 99fb4d349db7e7dacb2099c5cc320a9e2d31c1ef
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jan 26 04:30:41 2009 +0100

    x86: unmask CPUID levels on Intel CPUs, fix
    
    Impact: fix boot hang on pre-model-15 Intel CPUs
    
    rdmsrl_safe() does not work in very early bootup code yet, because we
    dont have the pagefault handler installed yet so exception section
    does not get parsed. rdmsr_safe() will just crash and hang the bootup.
    
    So limit the MSR_IA32_MISC_ENABLE MSR read to those CPU types that
    support it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 43c1dcf0bec7..549f2ada55f5 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -29,14 +29,17 @@
 
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
-	u64 misc_enable;
-
-	/* Unmask CPUID levels if masked */
-	if (!rdmsrl_safe(MSR_IA32_MISC_ENABLE, &misc_enable) &&
-	    (misc_enable & MSR_IA32_MISC_ENABLE_LIMIT_CPUID)) {
-		misc_enable &= ~MSR_IA32_MISC_ENABLE_LIMIT_CPUID;
-		wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
-		c->cpuid_level = cpuid_eax(0);
+	/* Unmask CPUID levels if masked: */
+	if (c->x86 == 6 && c->x86_model >= 15) {
+		u64 misc_enable;
+
+		rdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
+
+		if (misc_enable & MSR_IA32_MISC_ENABLE_LIMIT_CPUID) {
+			misc_enable &= ~MSR_IA32_MISC_ENABLE_LIMIT_CPUID;
+			wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
+			c->cpuid_level = cpuid_eax(0);
+		}
 	}
 
 	if ((c->x86 == 0xf && c->x86_model >= 0x03) ||

commit 75a048119e76540d73132cfc8e0fa0c0a8bb6c83
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Jan 22 16:17:05 2009 -0800

    x86: handle PAT more like other CPU features
    
    Impact: Cleanup
    
    When PAT was originally introduced, it was handled specially for a few
    reasons:
    
    - PAT bugs are hard to track down, so we wanted to maintain a
      whitelist of CPUs.
    - The i386 and x86-64 CPUID code was not yet unified.
    
    Both of these are now obsolete, so handle PAT like any other features,
    including ordinary feature blacklisting due to known bugs.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 8ea6929e974c..20ce03acf04b 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -50,6 +50,18 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
 	}
 
+	/*
+	 * There is a known erratum on Pentium III and Core Solo
+	 * and Core Duo CPUs.
+	 * " Page with PAT set to WC while associated MTRR is UC
+	 *   may consolidate to UC "
+	 * Because of this erratum, it is better to stick with
+	 * setting WC in MTRR rather than using PAT on these CPUs.
+	 *
+	 * Enable PAT WC only on P4, Core 2 or later CPUs.
+	 */
+	if (c->x86 == 6 && c->x86_model < 15)
+		clear_cpu_cap(c, X86_FEATURE_PAT);
 }
 
 #ifdef CONFIG_X86_32

commit 066941bd4eeb159307a5d7d795100d0887c00442
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Jan 21 15:04:32 2009 -0800

    x86: unmask CPUID levels on Intel CPUs
    
    Impact: Fixes crashes with misconfigured BIOSes on XSAVE hardware
    
    Avuton Olrich reported early boot crashes with v2.6.28 and
    bisected it down to dc1e35c6e95e8923cf1d3510438b63c600fee1e2
    ("x86, xsave: enable xsave/xrstor on cpus with xsave support").
    
    If the CPUID limit bit in MSR_IA32_MISC_ENABLE is set, clear it to
    make all CPUID information available.  This is required for some
    features to work, in particular XSAVE.
    
    Reported-and-bisected-by: Avuton Olrich <avuton@gmail.com>
    Tested-by: Avuton Olrich <avuton@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 8ea6929e974c..43c1dcf0bec7 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -29,6 +29,16 @@
 
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
+	u64 misc_enable;
+
+	/* Unmask CPUID levels if masked */
+	if (!rdmsrl_safe(MSR_IA32_MISC_ENABLE, &misc_enable) &&
+	    (misc_enable & MSR_IA32_MISC_ENABLE_LIMIT_CPUID)) {
+		misc_enable &= ~MSR_IA32_MISC_ENABLE_LIMIT_CPUID;
+		wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
+		c->cpuid_level = cpuid_eax(0);
+	}
+
 	if ((c->x86 == 0xf && c->x86_model >= 0x03) ||
 		(c->x86 == 0x6 && c->x86_model >= 0x0e))
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);

commit a3eeeefbf1cd1d142c52238cc19c75d14c3bc8d5
Merge: 30cd324e9787 7e3cbc3f774f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 25 12:48:18 2008 +0100

    Merge branch 'x86/tsc' into tracing/core
    
    Merge it to resolve this incidental conflict between the BTS fixes/cleanups
    and changes in x86/tsc:
    
    Conflicts:
            arch/x86/kernel/cpu/intel.c

commit 7e3cbc3f774f31ecd88a51edae3d9377f60a4c00
Merge: 345077cd98ff f4166c54bfe0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:29:31 2008 +0100

    Merge branch 'x86/ptrace' into x86/tsc
    
    Conflicts:
            arch/x86/kernel/cpu/intel.c

commit 345077cd98ff5532b2d1158013c3fec7b1ae85ec
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Dec 18 18:09:21 2008 -0800

    x86: fix intel x86_64 llc_shared_map/cpu_llc_id anomolies
    
    Impact: fix wrong cache sharing detection on platforms supporting > 8 bit apicid's
    
    In the presence of extended topology eumeration leaf 0xb provided
    by cpuid, 32bit extended initial_apicid in cpuinfo_x86 struct will be
    updated by detect_extended_topology(). At this instance, we should also
    reinit the apicid (which could also potentially be extended to 32bit).
    
    With out this there will potentially be duplicate apicid's populated in the
    per cpu's cpuinfo_x86 struct, resulting in wrong cache sharing topology etc
    detected by init_intel_cacheinfo().
    
    Reported-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index caec59437a22..b21c37c060a2 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -252,6 +252,13 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 
 	intel_workarounds(c);
 
+	/*
+	 * Detect the extended topology information if available. This
+	 * will reinitialise the initial_apicid which will be used
+	 * in init_intel_cacheinfo()
+	 */
+	detect_extended_topology(c);
+
 	l2 = init_intel_cacheinfo(c);
 	if (c->cpuid_level > 9) {
 		unsigned eax = cpuid_eax(10);
@@ -323,7 +330,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 
 #endif
 
-	detect_extended_topology(c);
 	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
 		/*
 		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology

commit 40fb17152c50a69dc304dd632131c2f41281ce44
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Mon Nov 17 16:11:37 2008 -0800

    x86: support always running TSC on Intel CPUs
    
    Impact: reward non-stop TSCs with good TSC-based clocksources, etc.
    
    Add support for CPUID_0x80000007_Bit8 on Intel CPUs as well. This bit means
    that the TSC is invariant with C/P/T states and always runs at constant
    frequency.
    
    With Intel CPUs, we have 3 classes
    * CPUs where TSC runs at constant rate and does not stop n C-states
    * CPUs where TSC runs at constant rate, but will stop in deep C-states
    * CPUs where TSC rate will vary based on P/T-states and TSC will stop in deep
      C-states.
    
    To cover these 3, one feature bit (CONSTANT_TSC) is not enough. So, add a
    second bit (NONSTOP_TSC). CONSTANT_TSC indicates that the TSC runs at
    constant frequency irrespective of P/T-states, and NONSTOP_TSC indicates
    that TSC does not stop in deep C-states.
    
    CPUID_0x8000000_Bit8 indicates both these feature bit can be set.
    We still have CONSTANT_TSC _set_ and NONSTOP_TSC _not_set_ on some older Intel
    CPUs, based on model checks. We can use TSC on such CPUs for time, as long as
    those CPUs do not support/enter deep C-states.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index cce0b6118d55..caec59437a22 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -41,6 +41,16 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 	if (c->x86 == 15 && c->x86_cache_alignment == 64)
 		c->x86_cache_alignment = 128;
 #endif
+
+	/*
+	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate
+	 * with P/T states and does not stop in deep C-states
+	 */
+	if (c->x86_power & (1 << 8)) {
+		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
+		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
+	}
+
 }
 
 #ifdef CONFIG_X86_32

commit c2724775ce57c98b8af9694857b941dc61056516
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Thu Dec 11 13:49:59 2008 +0100

    x86, bts: provide in-kernel branch-trace interface
    
    Impact: cleanup
    
    Move the BTS bits from ptrace.c into ds.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 816f27f289b1..cd413d9a0218 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -11,7 +11,6 @@
 #include <asm/pgtable.h>
 #include <asm/msr.h>
 #include <asm/uaccess.h>
-#include <asm/ptrace.h>
 #include <asm/ds.h>
 #include <asm/bugs.h>
 
@@ -309,9 +308,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_P3);
 #endif
 
-	if (cpu_has_bts)
-		ptrace_bts_init_intel(c);
-
 	detect_extended_topology(c);
 	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
 		/*

commit f4166c54bfe04f64603974058e44fbd7cfef0ccc
Author: Markus Metzger <markus.t.metzger@googlemail.com>
Date:   Sun Nov 9 14:29:21 2008 +0100

    x86, bts: DS and BTS initialization
    
    Impact: widen BTS/PEBS ptrace enablement to more CPU models
    
    Move BTS initialisation out of an #ifdef CONFIG_X86_64 guard.
    
    Assume core2 BTS and DS layout for future models of family 6 processors.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index cce0b6118d55..816f27f289b1 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -307,12 +307,11 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_P4);
 	if (c->x86 == 6)
 		set_cpu_cap(c, X86_FEATURE_P3);
+#endif
 
 	if (cpu_has_bts)
 		ptrace_bts_init_intel(c);
 
-#endif
-
 	detect_extended_topology(c);
 	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
 		/*

commit 823b259b80158a5fb694f6784e18b5bae669c599
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Wed Sep 10 21:56:46 2008 -0700

    x86: print out apic id in hex format
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 99468dbd08da..cce0b6118d55 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -174,7 +174,7 @@ static void __cpuinit srat_detect_node(void)
 		node = first_node(node_online_map);
 	numa_set_node(cpu, node);
 
-	printk(KERN_INFO "CPU %d/%x -> Node %d\n", cpu, apicid, node);
+	printk(KERN_INFO "CPU %d/0x%x -> Node %d\n", cpu, apicid, node);
 #endif
 }
 

commit e38e05a85828dac23540cd007df5f20985388afc
Author: Sheng Yang <sheng.yang@intel.com>
Date:   Wed Sep 10 18:53:34 2008 +0800

    x86: extended "flags" to show virtualization HW feature in /proc/cpuinfo
    
    The hardware virtualization technology evolves very fast. But currently
    it's hard to tell if your CPU support a certain kind of HW technology
    without digging into the source code.
    
    The patch add a new catagory in "flags" under /proc/cpuinfo. Now "flags"
    can indicate the (important) HW virtulization features the CPU supported
    as well.
    
    Current implementation just cover Intel VMX side.
    
    Signed-off-by: Sheng Yang <sheng.yang@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 5f76bf139fda..99468dbd08da 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -196,6 +196,44 @@ static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
 		return 1;
 }
 
+static void __cpuinit detect_vmx_virtcap(struct cpuinfo_x86 *c)
+{
+	/* Intel VMX MSR indicated features */
+#define X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW	0x00200000
+#define X86_VMX_FEATURE_PROC_CTLS_VNMI		0x00400000
+#define X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS	0x80000000
+#define X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC	0x00000001
+#define X86_VMX_FEATURE_PROC_CTLS2_EPT		0x00000002
+#define X86_VMX_FEATURE_PROC_CTLS2_VPID		0x00000020
+
+	u32 vmx_msr_low, vmx_msr_high, msr_ctl, msr_ctl2;
+
+	clear_cpu_cap(c, X86_FEATURE_TPR_SHADOW);
+	clear_cpu_cap(c, X86_FEATURE_VNMI);
+	clear_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);
+	clear_cpu_cap(c, X86_FEATURE_EPT);
+	clear_cpu_cap(c, X86_FEATURE_VPID);
+
+	rdmsr(MSR_IA32_VMX_PROCBASED_CTLS, vmx_msr_low, vmx_msr_high);
+	msr_ctl = vmx_msr_high | vmx_msr_low;
+	if (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW)
+		set_cpu_cap(c, X86_FEATURE_TPR_SHADOW);
+	if (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_VNMI)
+		set_cpu_cap(c, X86_FEATURE_VNMI);
+	if (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS) {
+		rdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,
+		      vmx_msr_low, vmx_msr_high);
+		msr_ctl2 = vmx_msr_high | vmx_msr_low;
+		if ((msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC) &&
+		    (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW))
+			set_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);
+		if (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_EPT)
+			set_cpu_cap(c, X86_FEATURE_EPT);
+		if (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VPID)
+			set_cpu_cap(c, X86_FEATURE_VPID);
+	}
+}
+
 static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int l2 = 0;
@@ -289,6 +327,9 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 
 	/* Work around errata */
 	srat_detect_node();
+
+	if (cpu_has(c, X86_FEATURE_VMX))
+		detect_vmx_virtcap(c);
 }
 
 #ifdef CONFIG_X86_32

commit 4052704d92c3172ebc13cc0cc8df8ba2bd446a5c
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Tue Sep 9 16:40:38 2008 -0700

    x86: intel.c put workaround for old cpus together
    
    consolidate the code some more.
    
    No change in functionality intended.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 365a008080c2..5f76bf139fda 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -63,15 +63,54 @@ int __cpuinit ppro_with_ram_bug(void)
 	return 0;
 }
 
+#ifdef CONFIG_X86_F00F_BUG
+static void __cpuinit trap_init_f00f_bug(void)
+{
+	__set_fixmap(FIX_F00F_IDT, __pa(&idt_table), PAGE_KERNEL_RO);
 
-/*
- * P4 Xeon errata 037 workaround.
- * Hardware prefetcher may cause stale data to be loaded into the cache.
- */
-static void __cpuinit Intel_errata_workarounds(struct cpuinfo_x86 *c)
+	/*
+	 * Update the IDT descriptor and reload the IDT so that
+	 * it uses the read-only mapped virtual address.
+	 */
+	idt_descr.address = fix_to_virt(FIX_F00F_IDT);
+	load_idt(&idt_descr);
+}
+#endif
+
+static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
 {
 	unsigned long lo, hi;
 
+#ifdef CONFIG_X86_F00F_BUG
+	/*
+	 * All current models of Pentium and Pentium with MMX technology CPUs
+	 * have the F0 0F bug, which lets nonprivileged users lock up the system.
+	 * Note that the workaround only should be initialized once...
+	 */
+	c->f00f_bug = 0;
+	if (!paravirt_enabled() && c->x86 == 5) {
+		static int f00f_workaround_enabled;
+
+		c->f00f_bug = 1;
+		if (!f00f_workaround_enabled) {
+			trap_init_f00f_bug();
+			printk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");
+			f00f_workaround_enabled = 1;
+		}
+	}
+#endif
+
+	/*
+	 * SEP CPUID bug: Pentium Pro reports SEP but doesn't have it until
+	 * model 3 mask 3
+	 */
+	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
+		clear_cpu_cap(c, X86_FEATURE_SEP);
+
+	/*
+	 * P4 Xeon errata 037 workaround.
+	 * Hardware prefetcher may cause stale data to be loaded into the cache.
+	 */
 	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
 		rdmsr(MSR_IA32_MISC_ENABLE, lo, hi);
 		if ((lo & (1<<9)) == 0) {
@@ -81,23 +120,44 @@ static void __cpuinit Intel_errata_workarounds(struct cpuinfo_x86 *c)
 			wrmsr (MSR_IA32_MISC_ENABLE, lo, hi);
 		}
 	}
-}
-
 
+	/*
+	 * See if we have a good local APIC by checking for buggy Pentia,
+	 * i.e. all B steppings and the C2 stepping of P54C when using their
+	 * integrated APIC (see 11AP erratum in "Pentium Processor
+	 * Specification Update").
+	 */
+	if (cpu_has_apic && (c->x86<<8 | c->x86_model<<4) == 0x520 &&
+	    (c->x86_mask < 0x6 || c->x86_mask == 0xb))
+		set_cpu_cap(c, X86_FEATURE_11AP);
 
-#ifdef CONFIG_X86_F00F_BUG
-static void __cpuinit trap_init_f00f_bug(void)
-{
-	__set_fixmap(FIX_F00F_IDT, __pa(&idt_table), PAGE_KERNEL_RO);
 
+#ifdef CONFIG_X86_INTEL_USERCOPY
 	/*
-	 * Update the IDT descriptor and reload the IDT so that
-	 * it uses the read-only mapped virtual address.
+	 * Set up the preferred alignment for movsl bulk memory moves
 	 */
-	idt_descr.address = fix_to_virt(FIX_F00F_IDT);
-	load_idt(&idt_descr);
-}
+	switch (c->x86) {
+	case 4:		/* 486: untested */
+		break;
+	case 5:		/* Old Pentia: untested */
+		break;
+	case 6:		/* PII/PIII only like movsl with 8-byte alignment */
+		movsl_mask.mask = 7;
+		break;
+	case 15:	/* P4 is OK down to 8-byte alignment */
+		movsl_mask.mask = 7;
+		break;
+	}
 #endif
+
+#ifdef CONFIG_X86_NUMAQ
+	numaq_tsc_disable();
+#endif
+}
+#else
+static void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)
+{
+}
 #endif
 
 static void __cpuinit srat_detect_node(void)
@@ -139,28 +199,10 @@ static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
 static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int l2 = 0;
-	char *p = NULL;
 
 	early_init_intel(c);
 
-#ifdef CONFIG_X86_F00F_BUG
-	/*
-	 * All current models of Pentium and Pentium with MMX technology CPUs
-	 * have the F0 0F bug, which lets nonprivileged users lock up the system.
-	 * Note that the workaround only should be initialized once...
-	 */
-	c->f00f_bug = 0;
-	if (!paravirt_enabled() && c->x86 == 5) {
-		static int f00f_workaround_enabled;
-
-		c->f00f_bug = 1;
-		if (!f00f_workaround_enabled) {
-			trap_init_f00f_bug();
-			printk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");
-			f00f_workaround_enabled = 1;
-		}
-	}
-#endif
+	intel_workarounds(c);
 
 	l2 = init_intel_cacheinfo(c);
 	if (c->cpuid_level > 9) {
@@ -170,17 +212,32 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
 	}
 
-#ifdef CONFIG_X86_32
-	/* SEP CPUID bug: Pentium Pro reports SEP but doesn't have it until model 3 mask 3 */
-	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
-		clear_cpu_cap(c, X86_FEATURE_SEP);
+	if (cpu_has_xmm2)
+		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
+	if (cpu_has_ds) {
+		unsigned int l1;
+		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);
+		if (!(l1 & (1<<11)))
+			set_cpu_cap(c, X86_FEATURE_BTS);
+		if (!(l1 & (1<<12)))
+			set_cpu_cap(c, X86_FEATURE_PEBS);
+		ds_init_intel(c);
+	}
 
+#ifdef CONFIG_X86_64
+	if (c->x86 == 15)
+		c->x86_cache_alignment = c->x86_clflush_size * 2;
+	if (c->x86 == 6)
+		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
+#else
 	/*
 	 * Names for the Pentium II/Celeron processors
 	 * detectable only by also checking the cache size.
 	 * Dixon is NOT a Celeron.
 	 */
 	if (c->x86 == 6) {
+		char *p = NULL;
+
 		switch (c->x86_model) {
 		case 5:
 			if (c->x86_mask == 0) {
@@ -203,51 +260,11 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 				p = "Celeron (Coppermine)";
 			break;
 		}
-	}
 
-	if (p)
-		strcpy(c->x86_model_id, p);
-
-	Intel_errata_workarounds(c);
-
-#ifdef CONFIG_X86_INTEL_USERCOPY
-	/*
-	 * Set up the preferred alignment for movsl bulk memory moves
-	 */
-	switch (c->x86) {
-	case 4:		/* 486: untested */
-		break;
-	case 5:		/* Old Pentia: untested */
-		break;
-	case 6:		/* PII/PIII only like movsl with 8-byte alignment */
-		movsl_mask.mask = 7;
-		break;
-	case 15:	/* P4 is OK down to 8-byte alignment */
-		movsl_mask.mask = 7;
-		break;
+		if (p)
+			strcpy(c->x86_model_id, p);
 	}
-#endif
-
-#endif
 
-	if (cpu_has_xmm2)
-		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
-	if (cpu_has_ds) {
-		unsigned int l1;
-		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);
-		if (!(l1 & (1<<11)))
-			set_cpu_cap(c, X86_FEATURE_BTS);
-		if (!(l1 & (1<<12)))
-			set_cpu_cap(c, X86_FEATURE_PEBS);
-		ds_init_intel(c);
-	}
-
-#ifdef CONFIG_X86_64
-	if (c->x86 == 15)
-		c->x86_cache_alignment = c->x86_clflush_size * 2;
-	if (c->x86 == 6)
-		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
-#else
 	if (c->x86 == 15)
 		set_cpu_cap(c, X86_FEATURE_P4);
 	if (c->x86 == 6)
@@ -256,19 +273,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	if (cpu_has_bts)
 		ptrace_bts_init_intel(c);
 
-	/*
-	 * See if we have a good local APIC by checking for buggy Pentia,
-	 * i.e. all B steppings and the C2 stepping of P54C when using their
-	 * integrated APIC (see 11AP erratum in "Pentium Processor
-	 * Specification Update").
-	 */
-	if (cpu_has_apic && (c->x86<<8 | c->x86_model<<4) == 0x520 &&
-	    (c->x86_mask < 0x6 || c->x86_mask == 0xb))
-		set_cpu_cap(c, X86_FEATURE_11AP);
-
-#ifdef CONFIG_X86_NUMAQ
-	numaq_tsc_disable();
-#endif
 #endif
 
 	detect_extended_topology(c);

commit 185f3b9da24c09c26b784591ed354fe57998a7b1
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Tue Sep 9 16:40:35 2008 -0700

    x86: make intel.c have 64-bit support code
    
    prepare for unification.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a66989586a84..365a008080c2 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -15,6 +15,11 @@
 #include <asm/ds.h>
 #include <asm/bugs.h>
 
+#ifdef CONFIG_X86_64
+#include <asm/topology.h>
+#include <asm/numa_64.h>
+#endif
+
 #include "cpu.h"
 
 #ifdef CONFIG_X86_LOCAL_APIC
@@ -25,14 +30,20 @@
 
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
-	/* Netburst reports 64 bytes clflush size, but does IO in 128 bytes */
-	if (c->x86 == 15 && c->x86_cache_alignment == 64)
-		c->x86_cache_alignment = 128;
 	if ((c->x86 == 0xf && c->x86_model >= 0x03) ||
 		(c->x86 == 0x6 && c->x86_model >= 0x0e))
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
+
+#ifdef CONFIG_X86_64
+	set_cpu_cap(c, X86_FEATURE_SYSENTER32);
+#else
+	/* Netburst reports 64 bytes clflush size, but does IO in 128 bytes */
+	if (c->x86 == 15 && c->x86_cache_alignment == 64)
+		c->x86_cache_alignment = 128;
+#endif
 }
 
+#ifdef CONFIG_X86_32
 /*
  *	Early probe support logic for ppro memory erratum #50
  *
@@ -73,6 +84,40 @@ static void __cpuinit Intel_errata_workarounds(struct cpuinfo_x86 *c)
 }
 
 
+
+#ifdef CONFIG_X86_F00F_BUG
+static void __cpuinit trap_init_f00f_bug(void)
+{
+	__set_fixmap(FIX_F00F_IDT, __pa(&idt_table), PAGE_KERNEL_RO);
+
+	/*
+	 * Update the IDT descriptor and reload the IDT so that
+	 * it uses the read-only mapped virtual address.
+	 */
+	idt_descr.address = fix_to_virt(FIX_F00F_IDT);
+	load_idt(&idt_descr);
+}
+#endif
+#endif
+
+static void __cpuinit srat_detect_node(void)
+{
+#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+	unsigned node;
+	int cpu = smp_processor_id();
+	int apicid = hard_smp_processor_id();
+
+	/* Don't do the funky fallback heuristics the AMD version employs
+	   for now. */
+	node = apicid_to_node[apicid];
+	if (node == NUMA_NO_NODE || !node_online(node))
+		node = first_node(node_online_map);
+	numa_set_node(cpu, node);
+
+	printk(KERN_INFO "CPU %d/%x -> Node %d\n", cpu, apicid, node);
+#endif
+}
+
 /*
  * find out the number of processor cores on the die
  */
@@ -91,20 +136,6 @@ static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
 		return 1;
 }
 
-#ifdef CONFIG_X86_F00F_BUG
-static void __cpuinit trap_init_f00f_bug(void)
-{
-	__set_fixmap(FIX_F00F_IDT, __pa(&idt_table), PAGE_KERNEL_RO);
-
-	/*
-	 * Update the IDT descriptor and reload the IDT so that
-	 * it uses the read-only mapped virtual address.
-	 */
-	idt_descr.address = fix_to_virt(FIX_F00F_IDT);
-	load_idt(&idt_descr);
-}
-#endif
-
 static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int l2 = 0;
@@ -139,6 +170,7 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
 	}
 
+#ifdef CONFIG_X86_32
 	/* SEP CPUID bug: Pentium Pro reports SEP but doesn't have it until model 3 mask 3 */
 	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
 		clear_cpu_cap(c, X86_FEATURE_SEP);
@@ -176,18 +208,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	if (p)
 		strcpy(c->x86_model_id, p);
 
-	detect_extended_topology(c);
-
-	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
-		/*
-		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
-		 * detection.
-		 */
-		c->x86_max_cores = intel_num_cpu_cores(c);
-		detect_ht(c);
-	}
-
-	/* Work around errata */
 	Intel_errata_workarounds(c);
 
 #ifdef CONFIG_X86_INTEL_USERCOPY
@@ -206,14 +226,12 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		movsl_mask.mask = 7;
 		break;
 	}
+#endif
+
 #endif
 
 	if (cpu_has_xmm2)
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
-	if (c->x86 == 15)
-		set_cpu_cap(c, X86_FEATURE_P4);
-	if (c->x86 == 6)
-		set_cpu_cap(c, X86_FEATURE_P3);
 	if (cpu_has_ds) {
 		unsigned int l1;
 		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);
@@ -224,6 +242,17 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		ds_init_intel(c);
 	}
 
+#ifdef CONFIG_X86_64
+	if (c->x86 == 15)
+		c->x86_cache_alignment = c->x86_clflush_size * 2;
+	if (c->x86 == 6)
+		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
+#else
+	if (c->x86 == 15)
+		set_cpu_cap(c, X86_FEATURE_P4);
+	if (c->x86 == 6)
+		set_cpu_cap(c, X86_FEATURE_P3);
+
 	if (cpu_has_bts)
 		ptrace_bts_init_intel(c);
 
@@ -240,8 +269,25 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 #ifdef CONFIG_X86_NUMAQ
 	numaq_tsc_disable();
 #endif
+#endif
+
+	detect_extended_topology(c);
+	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
+		/*
+		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
+		 * detection.
+		 */
+		c->x86_max_cores = intel_num_cpu_cores(c);
+#ifdef CONFIG_X86_32
+		detect_ht(c);
+#endif
+	}
+
+	/* Work around errata */
+	srat_detect_node();
 }
 
+#ifdef CONFIG_X86_32
 static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 {
 	/*
@@ -254,10 +300,12 @@ static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned i
 		size = 256;
 	return size;
 }
+#endif
 
 static struct cpu_dev intel_cpu_dev __cpuinitdata = {
 	.c_vendor	= "Intel",
 	.c_ident	= { "GenuineIntel" },
+#ifdef CONFIG_X86_32
 	.c_models = {
 		{ .vendor = X86_VENDOR_INTEL, .family = 4, .model_names =
 		  {
@@ -307,13 +355,12 @@ static struct cpu_dev intel_cpu_dev __cpuinitdata = {
 		  }
 		},
 	},
+	.c_size_cache	= intel_size_cache,
+#endif
 	.c_early_init   = early_init_intel,
 	.c_init		= init_intel,
-	.c_size_cache	= intel_size_cache,
 	.c_x86_vendor	= X86_VENDOR_INTEL,
 };
 
 cpu_dev_register(intel_cpu_dev);
 
-/* arch_initcall(intel_cpu_init); */
-

commit 81faaae45701484bd7368336e02f2a846153b22f
Merge: f69feff72049 3c9339049df5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 10 08:20:51 2008 +0200

    Merge branch 'x86/pebs' into x86/unify-cpu-detect
    
    Conflicts:
            arch/x86/Kconfig.cpu
            include/asm-x86/ds.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f69feff720497237ae9dd2f4604921bd3080c421
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 7 17:58:58 2008 -0700

    x86: little clean up of intel.c/intel_64.c
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 959417b8cd64..4a8ac9d4ff50 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -76,7 +76,7 @@ static void __cpuinit Intel_errata_workarounds(struct cpuinfo_x86 *c)
 /*
  * find out the number of processor cores on the die
  */
-static int __cpuinit num_cpu_cores(struct cpuinfo_x86 *c)
+static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
 {
 	unsigned int eax, ebx, ecx, edx;
 
@@ -183,7 +183,7 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
 		 * detection.
 		 */
-		c->x86_max_cores = num_cpu_cores(c);
+		c->x86_max_cores = intel_num_cpu_cores(c);
 		detect_ht(c);
 	}
 
@@ -210,9 +210,8 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 
 	if (cpu_has_xmm2)
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
-	if (c->x86 == 15) {
+	if (c->x86 == 15)
 		set_cpu_cap(c, X86_FEATURE_P4);
-	}
 	if (c->x86 == 6)
 		set_cpu_cap(c, X86_FEATURE_P3);
 	if (cpu_has_ds) {

commit 9042763808c5285a1a61b45b0fe98a710a4c903c
Merge: 446d27338d3b aa3341a16888
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 5 09:21:21 2008 +0200

    Merge branch 'x86/x2apic' into x86/core
    
    Conflicts:
            arch/x86/kernel/cpu/common_64.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 10a434fcb23a57c385177a0086955fae01003f64
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:45 2008 +0200

    x86: remove cpu_vendor_dev
    
    1. add c_x86_vendor into cpu_dev
    2. change cpu_devs to static
    3. check c_x86_vendor before put that cpu_dev into array
    4. remove alignment for 64bit
    5. order the sequence in cpu_devs according to link sequence...
       so could put intel at first, then amd...
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 77618c717d76..c5ac08124adc 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -303,9 +303,10 @@ static struct cpu_dev intel_cpu_dev __cpuinitdata = {
 	.c_early_init   = early_init_intel,
 	.c_init		= init_intel,
 	.c_size_cache	= intel_size_cache,
+	.c_x86_vendor	= X86_VENDOR_INTEL,
 };
 
-cpu_vendor_dev_register(X86_VENDOR_INTEL, &intel_cpu_dev);
+cpu_dev_register(intel_cpu_dev);
 
 /* arch_initcall(intel_cpu_init); */
 

commit bbb65d2d365efe9951290e61678dcf81ec60add4
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Sat Aug 23 17:47:10 2008 +0200

    x86: use cpuid vector 0xb when available for detecting cpu topology
    
    cpuid leaf 0xb provides extended topology enumeration. This interface provides
    the 32-bit x2APIC id of the logical processor and it also provides a new
    mechanism to detect SMT and core siblings (which provides increased
    addressability).
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 77618c717d76..58a6f1a0b297 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -176,9 +176,16 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	if (p)
 		strcpy(c->x86_model_id, p);
 
-	c->x86_max_cores = num_cpu_cores(c);
-
-	detect_ht(c);
+	detect_extended_topology(c);
+
+	if (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {
+		/*
+		 * let's use the legacy cpuid vector 0x1 and 0x4 for topology
+		 * detection.
+		 */
+		c->x86_max_cores = num_cpu_cores(c);
+		detect_ht(c);
+	}
 
 	/* Work around errata */
 	Intel_errata_workarounds(c);

commit 774400a3ba23b63f4de39e67ce6c4e48935809dc
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Mon Aug 18 12:33:21 2008 +0200

    x86: move cmpxchg fallbacks to a generic place
    
    arch/x86/kernel/cpu/intel.c defines a few fallback functions
    (cmpxchg_*()) that are used when the CPU doesn't support cmpxchg
    and/or cmpxchg64 natively. However, while defined in an Intel-specific
    file, these functions are also used for CPUs from other vendors when
    they don't support cmpxchg and/or cmpxchg64. This breaks the
    compilation when support for Intel CPUs is disabled.
    
    This patch moves these functions to a new
    arch/x86/kernel/cpu/cmpxchg.c file, unconditionally compiled when
    X86_32 is enabled.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Cc: michael@free-electrons.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 5c8959b8a42e..77618c717d76 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -307,69 +307,5 @@ static struct cpu_dev intel_cpu_dev __cpuinitdata = {
 
 cpu_vendor_dev_register(X86_VENDOR_INTEL, &intel_cpu_dev);
 
-#ifndef CONFIG_X86_CMPXCHG
-unsigned long cmpxchg_386_u8(volatile void *ptr, u8 old, u8 new)
-{
-	u8 prev;
-	unsigned long flags;
-
-	/* Poor man's cmpxchg for 386. Unsuitable for SMP */
-	local_irq_save(flags);
-	prev = *(u8 *)ptr;
-	if (prev == old)
-		*(u8 *)ptr = new;
-	local_irq_restore(flags);
-	return prev;
-}
-EXPORT_SYMBOL(cmpxchg_386_u8);
-
-unsigned long cmpxchg_386_u16(volatile void *ptr, u16 old, u16 new)
-{
-	u16 prev;
-	unsigned long flags;
-
-	/* Poor man's cmpxchg for 386. Unsuitable for SMP */
-	local_irq_save(flags);
-	prev = *(u16 *)ptr;
-	if (prev == old)
-		*(u16 *)ptr = new;
-	local_irq_restore(flags);
-	return prev;
-}
-EXPORT_SYMBOL(cmpxchg_386_u16);
-
-unsigned long cmpxchg_386_u32(volatile void *ptr, u32 old, u32 new)
-{
-	u32 prev;
-	unsigned long flags;
-
-	/* Poor man's cmpxchg for 386. Unsuitable for SMP */
-	local_irq_save(flags);
-	prev = *(u32 *)ptr;
-	if (prev == old)
-		*(u32 *)ptr = new;
-	local_irq_restore(flags);
-	return prev;
-}
-EXPORT_SYMBOL(cmpxchg_386_u32);
-#endif
-
-#ifndef CONFIG_X86_CMPXCHG64
-unsigned long long cmpxchg_486_u64(volatile void *ptr, u64 old, u64 new)
-{
-	u64 prev;
-	unsigned long flags;
-
-	/* Poor man's cmpxchg8b for 386 and 486. Unsuitable for SMP */
-	local_irq_save(flags);
-	prev = *(u64 *)ptr;
-	if (prev == old)
-		*(u64 *)ptr = new;
-	local_irq_restore(flags);
-	return prev;
-}
-EXPORT_SYMBOL(cmpxchg_486_u64);
-#endif
-
 /* arch_initcall(intel_cpu_init); */
 

commit 8bfcb3960fde049b863266dab8c3617bb5a541aa
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Mon Aug 18 12:33:20 2008 +0200

    x86: make movsl_mask definition non-CPU specific
    
    movsl_mask is currently defined in arch/x86/kernel/cpu/intel.c, which
    contains code specific to Intel CPUs. However, movsl_mask is used in
    the non-CPU specific code in arch/x86/lib/usercopy_32.c, which breaks
    the compilation when support for Intel CPUs is compiled out.
    
    This patch solves this problem by moving movsl_mask's definition close
    to its users in arch/x86/lib/usercopy_32.c.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Cc: michael@free-electrons.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index b75f2569b8f8..5c8959b8a42e 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -23,13 +23,6 @@
 #include <mach_apic.h>
 #endif
 
-#ifdef CONFIG_X86_INTEL_USERCOPY
-/*
- * Alignment at which movsl is preferred for bulk memory copies.
- */
-struct movsl_mask movsl_mask __read_mostly;
-#endif
-
 static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
 	/* Netburst reports 64 bytes clflush size, but does IO in 128 bytes */

commit 0e2f65ee30eee2db054f7fd73f462c5da33ec963
Merge: da7878d75b85 fb2e405fc1fc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 25 11:37:07 2008 +0200

    Merge branch 'linus' into x86/pebs
    
    Conflicts:
    
            arch/x86/Kconfig.cpu
            arch/x86/kernel/cpu/intel.c
            arch/x86/kernel/setup_64.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 593f4a788e5d09e9f00182561437461b0b564de4
Author: Maciej W. Rozycki <macro@linux-mips.org>
Date:   Wed Jul 16 19:15:30 2008 +0100

    x86: APIC: remove apic_write_around(); use alternatives
    
    Use alternatives to select the workaround for the 11AP Pentium erratum
    for the affected steppings on the fly rather than build time.  Remove the
    X86_GOOD_APIC configuration option and replace all the calls to
    apic_write_around() with plain apic_write(), protecting accesses to the
    ESR as appropriate due to the 3AP Pentium erratum.  Remove
    apic_read_around() and all its invocations altogether as not needed.
    Remove apic_write_atomic() and all its implementing backends.  The use of
    ASM_OUTPUT2() is not strictly needed for input constraints, but I have
    used it for readability's sake.
    
    I had the feeling no one else was brave enough to do it, so I went ahead
    and here it is.  Verified by checking the generated assembly and tested
    with both a 32-bit and a 64-bit configuration, also with the 11AP
    "feature" forced on and verified with gdb on /proc/kcore to work as
    expected (as an 11AP machines are quite hard to get hands on these days).
    Some script complained about the use of "volatile", but apic_write() needs
    it for the same reason and is effectively a replacement for writel(), so I
    have disregarded it.
    
    I am not sure what the policy wrt defconfig files is, they are generated
    and there is risk of a conflict resulting from an unrelated change, so I
    have left changes to them out.  The option will get removed from them at
    the next run.
    
    Some testing with machines other than mine will be needed to avoid some
    stupid mistake, but despite its volume, the change is not really that
    intrusive, so I am fairly confident that because it works for me, it will
    everywhere.
    
    Signed-off-by: Maciej W. Rozycki <macro@linux-mips.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 70609efdf1da..b75f2569b8f8 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -227,6 +227,16 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	if (cpu_has_bts)
 		ds_init_intel(c);
 
+	/*
+	 * See if we have a good local APIC by checking for buggy Pentia,
+	 * i.e. all B steppings and the C2 stepping of P54C when using their
+	 * integrated APIC (see 11AP erratum in "Pentium Processor
+	 * Specification Update").
+	 */
+	if (cpu_has_apic && (c->x86<<8 | c->x86_model<<4) == 0x520 &&
+	    (c->x86_mask < 0x6 || c->x86_mask == 0xb))
+		set_cpu_cap(c, X86_FEATURE_11AP);
+
 #ifdef CONFIG_X86_NUMAQ
 	numaq_tsc_disable();
 #endif

commit 3d88cca7085cffce077f808f36551e9050eb9e3a
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sat Jul 12 22:52:55 2008 -0700

    x86: fix numaq_tsc_disable calling
    
    got this on a test-system:
    
     calling  numaq_tsc_disable+0x0/0x39
     NUMAQ: disabling TSC
     initcall numaq_tsc_disable+0x0/0x39 returned 0 after 0 msecs
    
    that's because we should not be using arch_initcall to call numaq_tsc_disable.
    
    need to call it in setup_arch before time_init()/tsc_init()
    and call it in init_intel() to make the cpu feature bits right.
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index fe9224c51d37..70609efdf1da 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -226,6 +226,10 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 
 	if (cpu_has_bts)
 		ds_init_intel(c);
+
+#ifdef CONFIG_X86_NUMAQ
+	numaq_tsc_disable();
+#endif
 }
 
 static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)

commit 93fa7636dfdc059b25df148f230c0991096afdef
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Tue Apr 8 11:01:58 2008 +0200

    x86, ptrace: PEBS support
    
    Polish the ds.h interface and add support for PEBS.
    
    Ds.c is meant to be the resource allocator for per-thread and per-cpu
    BTS and PEBS recording.
    It is used by ptrace/utrace to provide execution tracing of debugged tasks.
    It will be used by profilers (e.g. perfmon2).
    It may be used by kernel debuggers to provide a kernel execution trace.
    
    Changes in detail:
    - guard DS and ptrace by CONFIG macros
    - separate DS and BTS more clearly
    - simplify field accesses
    - add functions to manage PEBS buffers
    - add simple protection/allocation mechanism
    - added support for Atom
    
    Opens:
    - buffer overflow handling
      Currently, only circular buffers are supported. This is all we need
      for debugging. Profilers would want an overflow notification.
      This is planned to be added when perfmon2 is made to use the ds.h
      interface.
    - utrace intermediate layer
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index fe9224c51d37..cbffa2a25a13 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -222,10 +222,11 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_BTS);
 		if (!(l1 & (1<<12)))
 			set_cpu_cap(c, X86_FEATURE_PEBS);
+		ds_init_intel(c);
 	}
 
 	if (cpu_has_bts)
-		ds_init_intel(c);
+		ptrace_bts_init_intel(c);
 }
 
 static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)

commit d0e95ebdc5cf5fe6fa29f2e0a5c6a0fe5c5aa50f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 26 08:52:33 2008 +0100

    x86: clean up cpu capabilities in arch/x86/kernel/cpu/intel.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index c9ecf378cc41..fe9224c51d37 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -143,12 +143,12 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		unsigned eax = cpuid_eax(10);
 		/* Check for version and the number of counters */
 		if ((eax & 0xff) && (((eax>>8) & 0xff) > 1))
-			set_bit(X86_FEATURE_ARCH_PERFMON, c->x86_capability);
+			set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
 	}
 
 	/* SEP CPUID bug: Pentium Pro reports SEP but doesn't have it until model 3 mask 3 */
 	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
-		clear_bit(X86_FEATURE_SEP, c->x86_capability);
+		clear_cpu_cap(c, X86_FEATURE_SEP);
 
 	/*
 	 * Names for the Pentium II/Celeron processors
@@ -209,19 +209,19 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 #endif
 
 	if (cpu_has_xmm2)
-		set_bit(X86_FEATURE_LFENCE_RDTSC, c->x86_capability);
+		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
 	if (c->x86 == 15) {
-		set_bit(X86_FEATURE_P4, c->x86_capability);
+		set_cpu_cap(c, X86_FEATURE_P4);
 	}
 	if (c->x86 == 6)
-		set_bit(X86_FEATURE_P3, c->x86_capability);
+		set_cpu_cap(c, X86_FEATURE_P3);
 	if (cpu_has_ds) {
 		unsigned int l1;
 		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);
 		if (!(l1 & (1<<11)))
-			set_bit(X86_FEATURE_BTS, c->x86_capability);
+			set_cpu_cap(c, X86_FEATURE_BTS);
 		if (!(l1 & (1<<12)))
-			set_bit(X86_FEATURE_PEBS, c->x86_capability);
+			set_cpu_cap(c, X86_FEATURE_PEBS);
 	}
 
 	if (cpu_has_bts)

commit 65eb6b4326daddd1cccd003bd4df3fd75b06f0e1
Author: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
Date:   Fri Feb 22 23:09:42 2008 +0100

    x86: coding style fixes to arch/x86/kernel/cpu/intel.c
    
    Before:
       total: 37 errors, 16 warnings, 366 lines checked
    After:
       total: 0 errors, 15 warnings, 369 lines checked
    
    No code changed:
    
    arch/x86/kernel/cpu/intel.o:
    
       text    data     bss     dec     hex filename
       1534     452       0    1986     7c2 intel.o.before
       1534     452       0    1986     7c2 intel.o.after
    
    md5:
       1ca348a06de6eb354c4b6ea715a57db5  intel.o.before.asm
       1ca348a06de6eb354c4b6ea715a57db5  intel.o.after.asm
    
    Signed-off-by: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 34468b2e2507..c9ecf378cc41 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -45,7 +45,7 @@ static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
  *
  *	This is called before we do cpu ident work
  */
- 
+
 int __cpuinit ppro_with_ram_bug(void)
 {
 	/* Uses data from early_cpu_detect now */
@@ -58,7 +58,7 @@ int __cpuinit ppro_with_ram_bug(void)
 	}
 	return 0;
 }
-	
+
 
 /*
  * P4 Xeon errata 037 workaround.
@@ -69,7 +69,7 @@ static void __cpuinit Intel_errata_workarounds(struct cpuinfo_x86 *c)
 	unsigned long lo, hi;
 
 	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
-		rdmsr (MSR_IA32_MISC_ENABLE, lo, hi);
+		rdmsr(MSR_IA32_MISC_ENABLE, lo, hi);
 		if ((lo & (1<<9)) == 0) {
 			printk (KERN_INFO "CPU: C0 stepping P4 Xeon detected.\n");
 			printk (KERN_INFO "CPU: Disabling hardware prefetching (Errata 037)\n");
@@ -127,10 +127,10 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	 */
 	c->f00f_bug = 0;
 	if (!paravirt_enabled() && c->x86 == 5) {
-		static int f00f_workaround_enabled = 0;
+		static int f00f_workaround_enabled;
 
 		c->f00f_bug = 1;
-		if ( !f00f_workaround_enabled ) {
+		if (!f00f_workaround_enabled) {
 			trap_init_f00f_bug();
 			printk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");
 			f00f_workaround_enabled = 1;
@@ -139,7 +139,7 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 #endif
 
 	l2 = init_intel_cacheinfo(c);
-	if (c->cpuid_level > 9 ) {
+	if (c->cpuid_level > 9) {
 		unsigned eax = cpuid_eax(10);
 		/* Check for version and the number of counters */
 		if ((eax & 0xff) && (((eax>>8) & 0xff) > 1))
@@ -150,9 +150,11 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
 		clear_bit(X86_FEATURE_SEP, c->x86_capability);
 
-	/* Names for the Pentium II/Celeron processors 
-	   detectable only by also checking the cache size.
-	   Dixon is NOT a Celeron. */
+	/*
+	 * Names for the Pentium II/Celeron processors
+	 * detectable only by also checking the cache size.
+	 * Dixon is NOT a Celeron.
+	 */
 	if (c->x86 == 6) {
 		switch (c->x86_model) {
 		case 5:
@@ -163,14 +165,14 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 					p = "Mobile Pentium II (Dixon)";
 			}
 			break;
-			
+
 		case 6:
 			if (l2 == 128)
 				p = "Celeron (Mendocino)";
 			else if (c->x86_mask == 0 || c->x86_mask == 5)
 				p = "Celeron-A";
 			break;
-			
+
 		case 8:
 			if (l2 == 128)
 				p = "Celeron (Coppermine)";
@@ -178,9 +180,9 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		}
 	}
 
-	if ( p )
+	if (p)
 		strcpy(c->x86_model_id, p);
-	
+
 	c->x86_max_cores = num_cpu_cores(c);
 
 	detect_ht(c);
@@ -211,7 +213,7 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	if (c->x86 == 15) {
 		set_bit(X86_FEATURE_P4, c->x86_capability);
 	}
-	if (c->x86 == 6) 
+	if (c->x86 == 6)
 		set_bit(X86_FEATURE_P3, c->x86_capability);
 	if (cpu_has_ds) {
 		unsigned int l1;
@@ -226,9 +228,10 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		ds_init_intel(c);
 }
 
-static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 * c, unsigned int size)
+static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 {
-	/* Intel PIII Tualatin. This comes in two flavours.
+	/*
+	 * Intel PIII Tualatin. This comes in two flavours.
 	 * One has 256kb of cache, the other 512. We have no way
 	 * to determine which, so we use a boottime override
 	 * for the 512kb model, and assume 256 otherwise.
@@ -240,42 +243,42 @@ static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 * c, unsigned
 
 static struct cpu_dev intel_cpu_dev __cpuinitdata = {
 	.c_vendor	= "Intel",
-	.c_ident 	= { "GenuineIntel" },
+	.c_ident	= { "GenuineIntel" },
 	.c_models = {
-		{ .vendor = X86_VENDOR_INTEL, .family = 4, .model_names = 
-		  { 
-			  [0] = "486 DX-25/33", 
-			  [1] = "486 DX-50", 
-			  [2] = "486 SX", 
-			  [3] = "486 DX/2", 
-			  [4] = "486 SL", 
-			  [5] = "486 SX/2", 
-			  [7] = "486 DX/2-WB", 
-			  [8] = "486 DX/4", 
+		{ .vendor = X86_VENDOR_INTEL, .family = 4, .model_names =
+		  {
+			  [0] = "486 DX-25/33",
+			  [1] = "486 DX-50",
+			  [2] = "486 SX",
+			  [3] = "486 DX/2",
+			  [4] = "486 SL",
+			  [5] = "486 SX/2",
+			  [7] = "486 DX/2-WB",
+			  [8] = "486 DX/4",
 			  [9] = "486 DX/4-WB"
 		  }
 		},
 		{ .vendor = X86_VENDOR_INTEL, .family = 5, .model_names =
-		  { 
-			  [0] = "Pentium 60/66 A-step", 
-			  [1] = "Pentium 60/66", 
+		  {
+			  [0] = "Pentium 60/66 A-step",
+			  [1] = "Pentium 60/66",
 			  [2] = "Pentium 75 - 200",
-			  [3] = "OverDrive PODP5V83", 
+			  [3] = "OverDrive PODP5V83",
 			  [4] = "Pentium MMX",
-			  [7] = "Mobile Pentium 75 - 200", 
+			  [7] = "Mobile Pentium 75 - 200",
 			  [8] = "Mobile Pentium MMX"
 		  }
 		},
 		{ .vendor = X86_VENDOR_INTEL, .family = 6, .model_names =
-		  { 
+		  {
 			  [0] = "Pentium Pro A-step",
-			  [1] = "Pentium Pro", 
-			  [3] = "Pentium II (Klamath)", 
-			  [4] = "Pentium II (Deschutes)", 
-			  [5] = "Pentium II (Deschutes)", 
+			  [1] = "Pentium Pro",
+			  [3] = "Pentium II (Klamath)",
+			  [4] = "Pentium II (Deschutes)",
+			  [5] = "Pentium II (Deschutes)",
 			  [6] = "Mobile Pentium II",
-			  [7] = "Pentium III (Katmai)", 
-			  [8] = "Pentium III (Coppermine)", 
+			  [7] = "Pentium III (Katmai)",
+			  [8] = "Pentium III (Coppermine)",
 			  [10] = "Pentium III (Cascades)",
 			  [11] = "Pentium III (Tualatin)",
 		  }
@@ -361,5 +364,5 @@ unsigned long long cmpxchg_486_u64(volatile void *ptr, u64 old, u64 new)
 EXPORT_SYMBOL(cmpxchg_486_u64);
 #endif
 
-// arch_initcall(intel_cpu_init);
+/* arch_initcall(intel_cpu_init); */
 

commit 03ae5768b6110ebaa97dc3e7abf1c3d8bec5f874
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri Feb 15 12:00:23 2008 +0100

    x86: use ELF section to list CPU vendor specific code
    
    Replace the hardcoded list of initialization functions for each CPU
    vendor by a list in an ELF section, which is read at initialization in
    arch/x86/kernel/cpu/cpu.c to fill the cpu_devs[] array. The ELF
    section, named .x86cpuvendor.init, is reclaimed after boot, and
    contains entries of type "struct cpu_vendor_dev" which associates a
    vendor number with a pointer to a "struct cpu_dev" structure.
    
    This first modification allows to remove all the VENDOR_init_cpu()
    functions.
    
    This patch also removes the hardcoded calls to early_init_amd() and
    early_init_intel(). Instead, we add a "c_early_init" member to the
    cpu_dev structure, which is then called if not NULL by the generic CPU
    initialization code. Unfortunately, in early_cpu_detect(), this_cpu is
    not yet set, so we have to use the cpu_devs[] array directly.
    
    This patch is part of the Linux Tiny project, and is needed for
    further patch that will allow to disable compilation of unused CPU
    support code.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index fae31ce747bd..34468b2e2507 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -30,7 +30,7 @@
 struct movsl_mask movsl_mask __read_mostly;
 #endif
 
-void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
+static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
 	/* Netburst reports 64 bytes clflush size, but does IO in 128 bytes */
 	if (c->x86 == 15 && c->x86_cache_alignment == 64)
@@ -290,15 +290,12 @@ static struct cpu_dev intel_cpu_dev __cpuinitdata = {
 		  }
 		},
 	},
+	.c_early_init   = early_init_intel,
 	.c_init		= init_intel,
 	.c_size_cache	= intel_size_cache,
 };
 
-__init int intel_cpu_init(void)
-{
-	cpu_devs[X86_VENDOR_INTEL] = &intel_cpu_dev;
-	return 0;
-}
+cpu_vendor_dev_register(X86_VENDOR_INTEL, &intel_cpu_dev);
 
 #ifndef CONFIG_X86_CMPXCHG
 unsigned long cmpxchg_386_u8(volatile void *ptr, u8 old, u8 new)

commit 73bdb73f6666228289af4be55a77e2ed978061a7
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Mon Feb 4 16:48:04 2008 +0100

    x86: add include to cpu/intel.c
    
    Fixes sparse warning:
    
    arch/x86/kernel/cpu/intel.c:48:15: warning: symbol 'ppro_with_ram_bug' was not declared. Should it be static?
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index d1c372b018db..fae31ce747bd 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -13,6 +13,7 @@
 #include <asm/uaccess.h>
 #include <asm/ptrace.h>
 #include <asm/ds.h>
+#include <asm/bugs.h>
 
 #include "cpu.h"
 

commit 30d432dfab2bcfd021d352e2058fae6b9405caeb
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:16 2008 +0100

    x86: move MWAIT idle check to generic CPU initialization on 32-bit
    
    Previously it was only run for Intel CPUs, but AMD Fam10h implements MWAIT too.
    
    This matches 64bit behaviour.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index f1136115279a..d1c372b018db 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -137,7 +137,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	}
 #endif
 
-	select_idle_routine(c);
 	l2 = init_intel_cacheinfo(c);
 	if (c->cpuid_level > 9 ) {
 		unsigned eax = cpuid_eax(10);

commit 2b16a2353814a513cdb5c5c739b76a19d7ea39ce
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:32:40 2008 +0100

    x86: move X86_FEATURE_CONSTANT_TSC into early cpu feature detection
    
    Need this in the next patch in time_init and that happens early.
    
    This includes a minor fix on i386 where early_intel_workarounds()
    [which is now called early_init_intel] really executes early as
    the comments say.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 5731de3e1bd1..f1136115279a 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -29,13 +29,14 @@
 struct movsl_mask movsl_mask __read_mostly;
 #endif
 
-void __cpuinit early_intel_workaround(struct cpuinfo_x86 *c)
+void __cpuinit early_init_intel(struct cpuinfo_x86 *c)
 {
-	if (c->x86_vendor != X86_VENDOR_INTEL)
-		return;
 	/* Netburst reports 64 bytes clflush size, but does IO in 128 bytes */
 	if (c->x86 == 15 && c->x86_cache_alignment == 64)
 		c->x86_cache_alignment = 128;
+	if ((c->x86 == 0xf && c->x86_model >= 0x03) ||
+		(c->x86 == 0x6 && c->x86_model >= 0x0e))
+		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 }
 
 /*
@@ -115,6 +116,8 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	unsigned int l2 = 0;
 	char *p = NULL;
 
+	early_init_intel(c);
+
 #ifdef CONFIG_X86_F00F_BUG
 	/*
 	 * All current models of Pentium and Pentium with MMX technology CPUs
@@ -210,10 +213,6 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	}
 	if (c->x86 == 6) 
 		set_bit(X86_FEATURE_P3, c->x86_capability);
-	if ((c->x86 == 0xf && c->x86_model >= 0x03) ||
-		(c->x86 == 0x6 && c->x86_model >= 0x0e))
-		set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);
-
 	if (cpu_has_ds) {
 		unsigned int l1;
 		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);

commit 6d5f718a497375f853d90247f5f6963368e89803
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 30 13:32:38 2008 +0100

    x86: lfence fix
    
    LFENCE is available on XMM2 or higher Intel CPUs - not XMM or higher...
    
    this caused boot failures on XMM1 & !XMM1 capable CPUs.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 0a4abdb61ae4..5731de3e1bd1 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -203,7 +203,7 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	}
 #endif
 
-	if (cpu_has_xmm)
+	if (cpu_has_xmm2)
 		set_bit(X86_FEATURE_LFENCE_RDTSC, c->x86_capability);
 	if (c->x86 == 15) {
 		set_bit(X86_FEATURE_P4, c->x86_capability);

commit 707fa8ed923b1b6a3d7af0d386b0b3abad28ed19
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:32:37 2008 +0100

    x86: Implement support to synchronize RDTSC with LFENCE on Intel CPUs
    
    According to Intel RDTSC can be always synchronized with LFENCE
    on all current CPUs. Implement the necessary CPUID bit for that.
    
    It is unclear yet if that is true for all future CPUs too,
    but if there's another way the kernel can be always updated.
    
    Cc: asit.k.mallick@intel.com
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index e4b7e73e9024..0a4abdb61ae4 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -203,9 +203,10 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 	}
 #endif
 
+	if (cpu_has_xmm)
+		set_bit(X86_FEATURE_LFENCE_RDTSC, c->x86_capability);
 	if (c->x86 == 15) {
 		set_bit(X86_FEATURE_P4, c->x86_capability);
-		set_bit(X86_FEATURE_SYNC_RDTSC, c->x86_capability);
 	}
 	if (c->x86 == 6) 
 		set_bit(X86_FEATURE_P3, c->x86_capability);

commit eee3af4a2c83a97fff107ddc445d9df6fded9ce4
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Wed Jan 30 13:31:09 2008 +0100

    x86, ptrace: support for branch trace store(BTS)
    
    Resend using different mail client
    
    Changes to the last version:
    - split implementation into two layers: ds/bts and ptrace
    - renamed TIF's
    - save/restore ds save area msr in __switch_to_xtra()
    - make block-stepping only look at BTF bit
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index 867ff94579be..e4b7e73e9024 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -11,6 +11,8 @@
 #include <asm/pgtable.h>
 #include <asm/msr.h>
 #include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/ds.h>
 
 #include "cpu.h"
 
@@ -219,6 +221,9 @@ static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 		if (!(l1 & (1<<12)))
 			set_bit(X86_FEATURE_PEBS, c->x86_capability);
 	}
+
+	if (cpu_has_bts)
+		ds_init_intel(c);
 }
 
 static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 * c, unsigned int size)

commit 2c0b8a7578f7653e1e5312a5232e8ead563cf477
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Wed Jan 30 13:30:47 2008 +0100

    x86: fall back on interrupt disable in cmpxchg8b on 80386 and 80486
    
    Actually, on 386, cmpxchg and cmpxchg_local fall back on
    cmpxchg_386_u8/16/32: it disables interruptions around non atomic
    updates to mimic the cmpxchg behavior.
    
    The comment:
    /* Poor man's cmpxchg for 386. Unsuitable for SMP */
    
    already present in cmpxchg_386_u32 tells much about how this cmpxchg
    implementation should not be used in a SMP context. However, the cmpxchg_local
    can perfectly use this fallback, since it only needs to be atomic wrt the local
    cpu.
    
    This patch adds a cmpxchg_486_u64 and uses it as a fallback for cmpxchg64
    and cmpxchg64_local on 80386 and 80486.
    
    Q:
    but why is it called cmpxchg_486 when the other functions are called
    
    A:
    Because the standard cmpxchg is missing only on 386, but cmpxchg8b is
    missing both on 386 and 486.
    
    Citing Intel's Instruction set reference:
    
    cmpxchg:
    This instruction is not supported on Intel processors earlier than the
    Intel486 processors.
    
    cmpxchg8b:
    This instruction encoding is not supported on Intel processors earlier
    than the Pentium processors.
    
    Q:
    What's the reason to have cmpxchg64_local on 32 bit architectures?
    Without that need all this would just be a few simple defines.
    
    A:
    cmpxchg64_local on 32 bits architectures takes unsigned long long
    parameters, but cmpxchg_local only takes longs. Since we have cmpxchg8b
    to execute a 8 byte cmpxchg atomically on pentium and +, it makes sense
    to provide a flavor of cmpxchg and cmpxchg_local using this instruction.
    
    Also, for 32 bits architectures lacking the 64 bits atomic cmpxchg, it
    makes sense _not_ to define cmpxchg64 while cmpxchg could still be
    available.
    
    Moreover, the fallback for cmpxchg8b on i386 for 386 and 486 is a
    
    However, cmpxchg64_local will be emulated by disabling interrupts on all
    architectures where it is not supported atomically.
    
    Therefore, we *could* turn cmpxchg64_local into a cmpxchg_local, but it
    would make the 386/486 fallbacks ugly, make its design different from
    cmpxchg/cmpxchg64 (which really depends on atomic operations and cannot
    be emulated) and require the __cmpxchg_local to be expressed as a macro
    rather than an inline function so the parameters would not be fixed to
    unsigned long long in every case.
    
    So I think cmpxchg64_local makes sense there, but I am open to
    suggestions.
    
    Q:
    Are there any callers?
    
    A:
    I am actually using it in LTTng in my timestamping code. I use it to
    work around CPUs with asynchronous TSCs. I need to update 64 bits
    values atomically on this 32 bits architecture.
    
    Changelog:
    - Ran though checkpatch.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index cc8c501b9f39..867ff94579be 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -342,5 +342,22 @@ unsigned long cmpxchg_386_u32(volatile void *ptr, u32 old, u32 new)
 EXPORT_SYMBOL(cmpxchg_386_u32);
 #endif
 
+#ifndef CONFIG_X86_CMPXCHG64
+unsigned long long cmpxchg_486_u64(volatile void *ptr, u64 old, u64 new)
+{
+	u64 prev;
+	unsigned long flags;
+
+	/* Poor man's cmpxchg8b for 386 and 486. Unsuitable for SMP */
+	local_irq_save(flags);
+	prev = *(u64 *)ptr;
+	if (prev == old)
+		*(u64 *)ptr = new;
+	local_irq_restore(flags);
+	return prev;
+}
+EXPORT_SYMBOL(cmpxchg_486_u64);
+#endif
+
 // arch_initcall(intel_cpu_init);
 

commit d72b1b4f41b5159d2d0e54e54c794d500197572e
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Wed Oct 17 18:04:33 2007 +0200

    i386: fix section mismatch warning in intel.c
    
    Fix following section mismatch warning:
    WARNING: vmlinux.o(.text+0xc88c): Section mismatch: reference to .init.text:trap_init_f00f_bug (between 'init_intel' and 'cpuid4_cache_lookup')
    
    init_intel are __cpuint where trap_init_f00f_bug is __init.
    Fixed by declaring trap_init_f00f_bug __cpuinit.
    
    Moved the defintion of trap_init_f00f_bug to the sole user in init.c
    so the ugly prototype in intel.c could get killed.
    
    Frank van Maarseveen <frankvm@frankvm.com> supplied the .config used
    to reproduce the warning.
    
    [ tglx: arch/x86 adaptation ]
    
    Cc: Frank van Maarseveen <frankvm@frankvm.com>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index dc4e08147b1f..cc8c501b9f39 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -8,6 +8,7 @@
 #include <linux/module.h>
 
 #include <asm/processor.h>
+#include <asm/pgtable.h>
 #include <asm/msr.h>
 #include <asm/uaccess.h>
 
@@ -19,8 +20,6 @@
 #include <mach_apic.h>
 #endif
 
-extern int trap_init_f00f_bug(void);
-
 #ifdef CONFIG_X86_INTEL_USERCOPY
 /*
  * Alignment at which movsl is preferred for bulk memory copies.
@@ -95,6 +94,20 @@ static int __cpuinit num_cpu_cores(struct cpuinfo_x86 *c)
 		return 1;
 }
 
+#ifdef CONFIG_X86_F00F_BUG
+static void __cpuinit trap_init_f00f_bug(void)
+{
+	__set_fixmap(FIX_F00F_IDT, __pa(&idt_table), PAGE_KERNEL_RO);
+
+	/*
+	 * Update the IDT descriptor and reload the IDT so that
+	 * it uses the read-only mapped virtual address.
+	 */
+	idt_descr.address = fix_to_virt(FIX_F00F_IDT);
+	load_idt(&idt_descr);
+}
+#endif
+
 static void __cpuinit init_intel(struct cpuinfo_x86 *c)
 {
 	unsigned int l2 = 0;

commit f7627e2513987bb5d4e8cb13c4e0a478352141ac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:58 2007 +0200

    i386: move kernel/cpu
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
new file mode 100644
index 000000000000..dc4e08147b1f
--- /dev/null
+++ b/arch/x86/kernel/cpu/intel.c
@@ -0,0 +1,333 @@
+#include <linux/init.h>
+#include <linux/kernel.h>
+
+#include <linux/string.h>
+#include <linux/bitops.h>
+#include <linux/smp.h>
+#include <linux/thread_info.h>
+#include <linux/module.h>
+
+#include <asm/processor.h>
+#include <asm/msr.h>
+#include <asm/uaccess.h>
+
+#include "cpu.h"
+
+#ifdef CONFIG_X86_LOCAL_APIC
+#include <asm/mpspec.h>
+#include <asm/apic.h>
+#include <mach_apic.h>
+#endif
+
+extern int trap_init_f00f_bug(void);
+
+#ifdef CONFIG_X86_INTEL_USERCOPY
+/*
+ * Alignment at which movsl is preferred for bulk memory copies.
+ */
+struct movsl_mask movsl_mask __read_mostly;
+#endif
+
+void __cpuinit early_intel_workaround(struct cpuinfo_x86 *c)
+{
+	if (c->x86_vendor != X86_VENDOR_INTEL)
+		return;
+	/* Netburst reports 64 bytes clflush size, but does IO in 128 bytes */
+	if (c->x86 == 15 && c->x86_cache_alignment == 64)
+		c->x86_cache_alignment = 128;
+}
+
+/*
+ *	Early probe support logic for ppro memory erratum #50
+ *
+ *	This is called before we do cpu ident work
+ */
+ 
+int __cpuinit ppro_with_ram_bug(void)
+{
+	/* Uses data from early_cpu_detect now */
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+	    boot_cpu_data.x86 == 6 &&
+	    boot_cpu_data.x86_model == 1 &&
+	    boot_cpu_data.x86_mask < 8) {
+		printk(KERN_INFO "Pentium Pro with Errata#50 detected. Taking evasive action.\n");
+		return 1;
+	}
+	return 0;
+}
+	
+
+/*
+ * P4 Xeon errata 037 workaround.
+ * Hardware prefetcher may cause stale data to be loaded into the cache.
+ */
+static void __cpuinit Intel_errata_workarounds(struct cpuinfo_x86 *c)
+{
+	unsigned long lo, hi;
+
+	if ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {
+		rdmsr (MSR_IA32_MISC_ENABLE, lo, hi);
+		if ((lo & (1<<9)) == 0) {
+			printk (KERN_INFO "CPU: C0 stepping P4 Xeon detected.\n");
+			printk (KERN_INFO "CPU: Disabling hardware prefetching (Errata 037)\n");
+			lo |= (1<<9);	/* Disable hw prefetching */
+			wrmsr (MSR_IA32_MISC_ENABLE, lo, hi);
+		}
+	}
+}
+
+
+/*
+ * find out the number of processor cores on the die
+ */
+static int __cpuinit num_cpu_cores(struct cpuinfo_x86 *c)
+{
+	unsigned int eax, ebx, ecx, edx;
+
+	if (c->cpuid_level < 4)
+		return 1;
+
+	/* Intel has a non-standard dependency on %ecx for this CPUID level. */
+	cpuid_count(4, 0, &eax, &ebx, &ecx, &edx);
+	if (eax & 0x1f)
+		return ((eax >> 26) + 1);
+	else
+		return 1;
+}
+
+static void __cpuinit init_intel(struct cpuinfo_x86 *c)
+{
+	unsigned int l2 = 0;
+	char *p = NULL;
+
+#ifdef CONFIG_X86_F00F_BUG
+	/*
+	 * All current models of Pentium and Pentium with MMX technology CPUs
+	 * have the F0 0F bug, which lets nonprivileged users lock up the system.
+	 * Note that the workaround only should be initialized once...
+	 */
+	c->f00f_bug = 0;
+	if (!paravirt_enabled() && c->x86 == 5) {
+		static int f00f_workaround_enabled = 0;
+
+		c->f00f_bug = 1;
+		if ( !f00f_workaround_enabled ) {
+			trap_init_f00f_bug();
+			printk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");
+			f00f_workaround_enabled = 1;
+		}
+	}
+#endif
+
+	select_idle_routine(c);
+	l2 = init_intel_cacheinfo(c);
+	if (c->cpuid_level > 9 ) {
+		unsigned eax = cpuid_eax(10);
+		/* Check for version and the number of counters */
+		if ((eax & 0xff) && (((eax>>8) & 0xff) > 1))
+			set_bit(X86_FEATURE_ARCH_PERFMON, c->x86_capability);
+	}
+
+	/* SEP CPUID bug: Pentium Pro reports SEP but doesn't have it until model 3 mask 3 */
+	if ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)
+		clear_bit(X86_FEATURE_SEP, c->x86_capability);
+
+	/* Names for the Pentium II/Celeron processors 
+	   detectable only by also checking the cache size.
+	   Dixon is NOT a Celeron. */
+	if (c->x86 == 6) {
+		switch (c->x86_model) {
+		case 5:
+			if (c->x86_mask == 0) {
+				if (l2 == 0)
+					p = "Celeron (Covington)";
+				else if (l2 == 256)
+					p = "Mobile Pentium II (Dixon)";
+			}
+			break;
+			
+		case 6:
+			if (l2 == 128)
+				p = "Celeron (Mendocino)";
+			else if (c->x86_mask == 0 || c->x86_mask == 5)
+				p = "Celeron-A";
+			break;
+			
+		case 8:
+			if (l2 == 128)
+				p = "Celeron (Coppermine)";
+			break;
+		}
+	}
+
+	if ( p )
+		strcpy(c->x86_model_id, p);
+	
+	c->x86_max_cores = num_cpu_cores(c);
+
+	detect_ht(c);
+
+	/* Work around errata */
+	Intel_errata_workarounds(c);
+
+#ifdef CONFIG_X86_INTEL_USERCOPY
+	/*
+	 * Set up the preferred alignment for movsl bulk memory moves
+	 */
+	switch (c->x86) {
+	case 4:		/* 486: untested */
+		break;
+	case 5:		/* Old Pentia: untested */
+		break;
+	case 6:		/* PII/PIII only like movsl with 8-byte alignment */
+		movsl_mask.mask = 7;
+		break;
+	case 15:	/* P4 is OK down to 8-byte alignment */
+		movsl_mask.mask = 7;
+		break;
+	}
+#endif
+
+	if (c->x86 == 15) {
+		set_bit(X86_FEATURE_P4, c->x86_capability);
+		set_bit(X86_FEATURE_SYNC_RDTSC, c->x86_capability);
+	}
+	if (c->x86 == 6) 
+		set_bit(X86_FEATURE_P3, c->x86_capability);
+	if ((c->x86 == 0xf && c->x86_model >= 0x03) ||
+		(c->x86 == 0x6 && c->x86_model >= 0x0e))
+		set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);
+
+	if (cpu_has_ds) {
+		unsigned int l1;
+		rdmsr(MSR_IA32_MISC_ENABLE, l1, l2);
+		if (!(l1 & (1<<11)))
+			set_bit(X86_FEATURE_BTS, c->x86_capability);
+		if (!(l1 & (1<<12)))
+			set_bit(X86_FEATURE_PEBS, c->x86_capability);
+	}
+}
+
+static unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 * c, unsigned int size)
+{
+	/* Intel PIII Tualatin. This comes in two flavours.
+	 * One has 256kb of cache, the other 512. We have no way
+	 * to determine which, so we use a boottime override
+	 * for the 512kb model, and assume 256 otherwise.
+	 */
+	if ((c->x86 == 6) && (c->x86_model == 11) && (size == 0))
+		size = 256;
+	return size;
+}
+
+static struct cpu_dev intel_cpu_dev __cpuinitdata = {
+	.c_vendor	= "Intel",
+	.c_ident 	= { "GenuineIntel" },
+	.c_models = {
+		{ .vendor = X86_VENDOR_INTEL, .family = 4, .model_names = 
+		  { 
+			  [0] = "486 DX-25/33", 
+			  [1] = "486 DX-50", 
+			  [2] = "486 SX", 
+			  [3] = "486 DX/2", 
+			  [4] = "486 SL", 
+			  [5] = "486 SX/2", 
+			  [7] = "486 DX/2-WB", 
+			  [8] = "486 DX/4", 
+			  [9] = "486 DX/4-WB"
+		  }
+		},
+		{ .vendor = X86_VENDOR_INTEL, .family = 5, .model_names =
+		  { 
+			  [0] = "Pentium 60/66 A-step", 
+			  [1] = "Pentium 60/66", 
+			  [2] = "Pentium 75 - 200",
+			  [3] = "OverDrive PODP5V83", 
+			  [4] = "Pentium MMX",
+			  [7] = "Mobile Pentium 75 - 200", 
+			  [8] = "Mobile Pentium MMX"
+		  }
+		},
+		{ .vendor = X86_VENDOR_INTEL, .family = 6, .model_names =
+		  { 
+			  [0] = "Pentium Pro A-step",
+			  [1] = "Pentium Pro", 
+			  [3] = "Pentium II (Klamath)", 
+			  [4] = "Pentium II (Deschutes)", 
+			  [5] = "Pentium II (Deschutes)", 
+			  [6] = "Mobile Pentium II",
+			  [7] = "Pentium III (Katmai)", 
+			  [8] = "Pentium III (Coppermine)", 
+			  [10] = "Pentium III (Cascades)",
+			  [11] = "Pentium III (Tualatin)",
+		  }
+		},
+		{ .vendor = X86_VENDOR_INTEL, .family = 15, .model_names =
+		  {
+			  [0] = "Pentium 4 (Unknown)",
+			  [1] = "Pentium 4 (Willamette)",
+			  [2] = "Pentium 4 (Northwood)",
+			  [4] = "Pentium 4 (Foster)",
+			  [5] = "Pentium 4 (Foster)",
+		  }
+		},
+	},
+	.c_init		= init_intel,
+	.c_size_cache	= intel_size_cache,
+};
+
+__init int intel_cpu_init(void)
+{
+	cpu_devs[X86_VENDOR_INTEL] = &intel_cpu_dev;
+	return 0;
+}
+
+#ifndef CONFIG_X86_CMPXCHG
+unsigned long cmpxchg_386_u8(volatile void *ptr, u8 old, u8 new)
+{
+	u8 prev;
+	unsigned long flags;
+
+	/* Poor man's cmpxchg for 386. Unsuitable for SMP */
+	local_irq_save(flags);
+	prev = *(u8 *)ptr;
+	if (prev == old)
+		*(u8 *)ptr = new;
+	local_irq_restore(flags);
+	return prev;
+}
+EXPORT_SYMBOL(cmpxchg_386_u8);
+
+unsigned long cmpxchg_386_u16(volatile void *ptr, u16 old, u16 new)
+{
+	u16 prev;
+	unsigned long flags;
+
+	/* Poor man's cmpxchg for 386. Unsuitable for SMP */
+	local_irq_save(flags);
+	prev = *(u16 *)ptr;
+	if (prev == old)
+		*(u16 *)ptr = new;
+	local_irq_restore(flags);
+	return prev;
+}
+EXPORT_SYMBOL(cmpxchg_386_u16);
+
+unsigned long cmpxchg_386_u32(volatile void *ptr, u32 old, u32 new)
+{
+	u32 prev;
+	unsigned long flags;
+
+	/* Poor man's cmpxchg for 386. Unsuitable for SMP */
+	local_irq_save(flags);
+	prev = *(u32 *)ptr;
+	if (prev == old)
+		*(u32 *)ptr = new;
+	local_irq_restore(flags);
+	return prev;
+}
+EXPORT_SYMBOL(cmpxchg_386_u32);
+#endif
+
+// arch_initcall(intel_cpu_init);
+
