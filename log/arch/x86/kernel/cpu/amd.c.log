commit eff5ddadab04dc7ae8c4588dd3eef2ecad770fd7
Merge: 17e0a7cb6a25 3d81b3d1e55a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 13:57:51 2020 -0700

    Merge tag 'x86-cpu-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu updates from Ingo Molnar:
     "Misc updates:
    
       - Extend the x86 family/model macros with a steppings dimension,
         because x86 life isn't complex enough and Intel uses steppings to
         differentiate between different CPUs. :-/
    
       - Convert the TSC deadline timer quirks to the steppings macros.
    
       - Clean up asm mnemonics.
    
       - Fix the handling of an AMD erratum, or in other words, fix a kernel
         erratum"
    
    * tag 'x86-cpu-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/cpu: Use RDRAND and RDSEED mnemonics in archrandom.h
      x86/cpu: Use INVPCID mnemonic in invpcid.h
      x86/cpu/amd: Make erratum #1054 a legacy erratum
      x86/apic: Convert the TSC deadline timer matching to steppings macro
      x86/cpu: Add a X86_MATCH_INTEL_FAM6_MODEL_STEPPINGS() macro
      x86/cpu: Add a steppings field to struct x86_cpu_id

commit e2abfc0448a46d8a137505aa180caf14070ec535
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Fri Apr 17 09:33:56 2020 -0500

    x86/cpu/amd: Make erratum #1054 a legacy erratum
    
    Commit
    
      21b5ee59ef18 ("x86/cpu/amd: Enable the fixed Instructions Retired
                     counter IRPERF")
    
    mistakenly added erratum #1054 as an OS Visible Workaround (OSVW) ID 0.
    Erratum #1054 is not OSVW ID 0 [1], so make it a legacy erratum.
    
    There would never have been a false positive on older hardware that
    has OSVW bit 0 set, since the IRPERF feature was not available.
    
    However, save a couple of RDMSR executions per thread, on modern
    system configurations that correctly set non-zero values in their
    OSVW_ID_Length MSRs.
    
    [1] Revision Guide for AMD Family 17h Models 00h-0Fh Processors. The
    revision guide is available from the bugzilla link below.
    
    Fixes: 21b5ee59ef18 ("x86/cpu/amd: Enable the fixed Instructions Retired counter IRPERF")
    Reported-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20200417143356.26054-1-kim.phillips@amd.com
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=206537

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 547ad7bbf0e0..8a1bdda895a4 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -1142,8 +1142,7 @@ static const int amd_erratum_383[] =
 
 /* #1054: Instructions Retired Performance Counter May Be Inaccurate */
 static const int amd_erratum_1054[] =
-	AMD_OSVW_ERRATUM(0, AMD_MODEL_RANGE(0x17, 0, 0, 0x2f, 0xf));
-
+	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x17, 0, 0, 0x2f, 0xf));
 
 static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
 {

commit 923f3a2b48bdccb6a1d1f0dd48de03de7ad936d9
Author: Reinette Chatre <reinette.chatre@intel.com>
Date:   Tue May 5 15:36:15 2020 -0700

    x86/resctrl: Query LLC monitoring properties once during boot
    
    Cache and memory bandwidth monitoring are features that are part of
    x86 CPU resource control that is supported by the resctrl subsystem.
    The monitoring properties are obtained via CPUID from every CPU
    and only used within the resctrl subsystem where the properties are
    only read from boot_cpu_data.
    
    Obtain the monitoring properties once, placed in boot_cpu_data, via the
    ->c_bsp_init() helpers of the vendors that support X86_FEATURE_CQM_LLC.
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/6d74a6ac3e69f4b7a8b4115835f9455faf0f468d.1588715690.git.reinette.chatre@intel.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 547ad7bbf0e0..c36e89930965 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -18,6 +18,7 @@
 #include <asm/pci-direct.h>
 #include <asm/delay.h>
 #include <asm/debugreg.h>
+#include <asm/resctrl.h>
 
 #ifdef CONFIG_X86_64
 # include <asm/mmconfig.h>
@@ -597,6 +598,8 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 			x86_amd_ls_cfg_ssbd_mask = 1ULL << bit;
 		}
 	}
+
+	resctrl_cpu_detect(c);
 }
 
 static void early_detect_mem_encrypt(struct cpuinfo_x86 *c)

commit 9b82f05f869a823d43ea4186f5f732f2924d3693
Merge: 4b9fd8a829a1 629b3df7ecb0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 16:40:08 2020 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "The main changes in this cycle were:
    
      Kernel side changes:
    
       - A couple of x86/cpu cleanups and changes were grandfathered in due
         to patch dependencies. These clean up the set of CPU model/family
         matching macros with a consistent namespace and C99 initializer
         style.
    
       - A bunch of updates to various low level PMU drivers:
           * AMD Family 19h L3 uncore PMU
           * Intel Tiger Lake uncore support
           * misc fixes to LBR TOS sampling
    
       - optprobe fixes
    
       - perf/cgroup: optimize cgroup event sched-in processing
    
       - misc cleanups and fixes
    
      Tooling side changes are to:
    
       - perf {annotate,expr,record,report,stat,test}
    
       - perl scripting
    
       - libapi, libperf and libtraceevent
    
       - vendor events on Intel and S390, ARM cs-etm
    
       - Intel PT updates
    
       - Documentation changes and updates to core facilities
    
       - misc cleanups, fixes and other enhancements"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (89 commits)
      cpufreq/intel_pstate: Fix wrong macro conversion
      x86/cpu: Cleanup the now unused CPU match macros
      hwrng: via_rng: Convert to new X86 CPU match macros
      crypto: Convert to new CPU match macros
      ASoC: Intel: Convert to new X86 CPU match macros
      powercap/intel_rapl: Convert to new X86 CPU match macros
      PCI: intel-mid: Convert to new X86 CPU match macros
      mmc: sdhci-acpi: Convert to new X86 CPU match macros
      intel_idle: Convert to new X86 CPU match macros
      extcon: axp288: Convert to new X86 CPU match macros
      thermal: Convert to new X86 CPU match macros
      hwmon: Convert to new X86 CPU match macros
      platform/x86: Convert to new CPU match macros
      EDAC: Convert to new X86 CPU match macros
      cpufreq: Convert to new X86 CPU match macros
      ACPI: Convert to new X86 CPU match macros
      x86/platform: Convert to new CPU match macros
      x86/kernel: Convert to new CPU match macros
      x86/kvm: Convert to new CPU match macros
      x86/perf/events: Convert to new CPU match macros
      ...

commit ff7b862a4c354793580545afa64c56fafa18952b
Merge: aaf985e21a4a 077168e241ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 13:17:50 2020 -0700

    Merge tag 'ras_updates_for_5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RAS updates from Borislav Petkov:
    
     - Do not report spurious MCEs on some Intel platforms caused by errata;
       by Prarit Bhargava.
    
     - Change dev-mcelog's hardcoded limit of 32 error records to a dynamic
       one, controlled by the number of logical CPUs, by Tony Luck.
    
     - Add support for the processor identification number (PPIN) on AMD, by
       Wei Huang.
    
    * tag 'ras_updates_for_5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mce/amd: Add PPIN support for AMD MCE
      x86/mce/dev-mcelog: Dynamically allocate space for machine check records
      x86/mce: Do not log spurious corrected mce errors

commit 629b3df7ecb01fddfdf71cb5d3c563d143117c33
Merge: 3442a9ecb8e7 d97828072d0b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Mar 25 15:20:44 2020 +0100

    Merge branch 'x86/cpu' into perf/core, to resolve conflict
    
    Conflicts:
            arch/x86/events/intel/uncore.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 077168e241ec5a3b273652acb1e85f8bc1dc2d81
Author: Wei Huang <wei.huang2@amd.com>
Date:   Sat Mar 21 14:38:00 2020 -0500

    x86/mce/amd: Add PPIN support for AMD MCE
    
    Newer AMD CPUs support a feature called protected processor
    identification number (PPIN). This feature can be detected via
    CPUID_Fn80000008_EBX[23].
    
    However, CPUID alone is not enough to read the processor identification
    number - MSR_AMD_PPIN_CTL also needs to be configured properly. If, for
    any reason, MSR_AMD_PPIN_CTL[PPIN_EN] can not be turned on, such as
    disabled in BIOS, the CPU capability bit X86_FEATURE_AMD_PPIN needs to
    be cleared.
    
    When the X86_FEATURE_AMD_PPIN capability is available, the
    identification number is issued together with the MCE error info in
    order to keep track of the source of MCE errors.
    
     [ bp: Massage. ]
    
    Co-developed-by: Smita Koralahalli Channabasappa <smita.koralahallichannabasappa@amd.com>
    Signed-off-by: Smita Koralahalli Channabasappa <smita.koralahallichannabasappa@amd.com>
    Signed-off-by: Wei Huang <wei.huang2@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/20200321193800.3666964-1-wei.huang2@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ac83a0fef628..a2cd870af7f5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -393,6 +393,35 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
 }
 
+static void amd_detect_ppin(struct cpuinfo_x86 *c)
+{
+	unsigned long long val;
+
+	if (!cpu_has(c, X86_FEATURE_AMD_PPIN))
+		return;
+
+	/* When PPIN is defined in CPUID, still need to check PPIN_CTL MSR */
+	if (rdmsrl_safe(MSR_AMD_PPIN_CTL, &val))
+		goto clear_ppin;
+
+	/* PPIN is locked in disabled mode, clear feature bit */
+	if ((val & 3UL) == 1UL)
+		goto clear_ppin;
+
+	/* If PPIN is disabled, try to enable it */
+	if (!(val & 2UL)) {
+		wrmsrl_safe(MSR_AMD_PPIN_CTL,  val | 2UL);
+		rdmsrl_safe(MSR_AMD_PPIN_CTL, &val);
+	}
+
+	/* If PPIN_EN bit is 1, return from here; otherwise fall through */
+	if (val & 2UL)
+		return;
+
+clear_ppin:
+	clear_cpu_cap(c, X86_FEATURE_AMD_PPIN);
+}
+
 u16 amd_get_nb_id(int cpu)
 {
 	return per_cpu(cpu_llc_id, cpu);
@@ -940,6 +969,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 	amd_detect_cmp(c);
 	amd_get_topology(c);
 	srat_detect_node(c);
+	amd_detect_ppin(c);
 
 	init_amd_cacheinfo(c);
 

commit 753039ef8b2f1078e5bff8cd42f80578bf6385b0
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Wed Mar 11 14:14:51 2020 -0500

    x86/cpu/amd: Call init_amd_zn() om Family 19h processors too
    
    Family 19h CPUs are Zen-based and still share most architectural
    features with Family 17h CPUs, and therefore still need to call
    init_amd_zn() e.g., to set the RECLAIM_DISTANCE override.
    
    init_amd_zn() also sets X86_FEATURE_ZEN, which today is only used
    in amd_set_core_ssb_state(), which isn't called on some late
    model Family 17h CPUs, nor on any Family 19h CPUs:
    X86_FEATURE_AMD_SSBD replaces X86_FEATURE_LS_CFG_SSBD on those
    later model CPUs, where the SSBD mitigation is done via the
    SPEC_CTRL MSR instead of the LS_CFG MSR.
    
    Family 19h CPUs also don't have the erratum where the CPB feature
    bit isn't set, but that code can stay unchanged and run safely
    on Family 19h.
    
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20200311191451.13221-1-kim.phillips@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ac83a0fef628..dc6894a3f22d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -925,7 +925,8 @@ static void init_amd(struct cpuinfo_x86 *c)
 	case 0x12: init_amd_ln(c); break;
 	case 0x15: init_amd_bd(c); break;
 	case 0x16: init_amd_jg(c); break;
-	case 0x17: init_amd_zn(c); break;
+	case 0x17: fallthrough;
+	case 0x19: init_amd_zn(c); break;
 	}
 
 	/*

commit 21b5ee59ef18e27d85810584caf1f7ddc705ea83
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Wed Feb 19 18:52:43 2020 +0100

    x86/cpu/amd: Enable the fixed Instructions Retired counter IRPERF
    
    Commit
    
      aaf248848db50 ("perf/x86/msr: Add AMD IRPERF (Instructions Retired)
                      performance counter")
    
    added support for access to the free-running counter via 'perf -e
    msr/irperf/', but when exercised, it always returns a 0 count:
    
    BEFORE:
    
      $ perf stat -e instructions,msr/irperf/ true
    
       Performance counter stats for 'true':
    
                 624,833      instructions
                       0      msr/irperf/
    
    Simply set its enable bit - HWCR bit 30 - to make it start counting.
    
    Enablement is restricted to all machines advertising IRPERF capability,
    except those susceptible to an erratum that makes the IRPERF return
    bad values.
    
    That erratum occurs in Family 17h models 00-1fh [1], but not in F17h
    models 20h and above [2].
    
    AFTER (on a family 17h model 31h machine):
    
      $ perf stat -e instructions,msr/irperf/ true
    
       Performance counter stats for 'true':
    
                 621,690      instructions
                 622,490      msr/irperf/
    
    [1] Revision Guide for AMD Family 17h Models 00h-0Fh Processors
    [2] Revision Guide for AMD Family 17h Models 30h-3Fh Processors
    
    The revision guides are available from the bugzilla Link below.
    
     [ bp: Massage commit message. ]
    
    Fixes: aaf248848db50 ("perf/x86/msr: Add AMD IRPERF (Instructions Retired) performance counter")
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=206537
    Link: http://lkml.kernel.org/r/20200214201805.13830-1-kim.phillips@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ac83a0fef628..1f875fbe1384 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -28,6 +28,7 @@
 
 static const int amd_erratum_383[];
 static const int amd_erratum_400[];
+static const int amd_erratum_1054[];
 static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum);
 
 /*
@@ -972,6 +973,15 @@ static void init_amd(struct cpuinfo_x86 *c)
 	/* AMD CPUs don't reset SS attributes on SYSRET, Xen does. */
 	if (!cpu_has(c, X86_FEATURE_XENPV))
 		set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
+
+	/*
+	 * Turn on the Instructions Retired free counter on machines not
+	 * susceptible to erratum #1054 "Instructions Retired Performance
+	 * Counter May Be Inaccurate".
+	 */
+	if (cpu_has(c, X86_FEATURE_IRPERF) &&
+	    !cpu_has_amd_erratum(c, amd_erratum_1054))
+		msr_set_bit(MSR_K7_HWCR, MSR_K7_HWCR_IRPERF_EN_BIT);
 }
 
 #ifdef CONFIG_X86_32
@@ -1099,6 +1109,10 @@ static const int amd_erratum_400[] =
 static const int amd_erratum_383[] =
 	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
 
+/* #1054: Instructions Retired Performance Counter May Be Inaccurate */
+static const int amd_erratum_1054[] =
+	AMD_OSVW_ERRATUM(0, AMD_MODEL_RANGE(0x17, 0, 0, 0x2f, 0xf));
+
 
 static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
 {

commit 6da49d1abd2c2b70063f3606e3974c13d22497a1
Merge: 4244057c3da1 3c749b81ee99
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 28 12:11:23 2020 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups all around the map"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/CPU/AMD: Remove amd_get_topology_early()
      x86/tsc: Remove redundant assignment
      x86/crash: Use resource_size()
      x86/cpu: Add a missing prototype for arch_smt_update()
      x86/nospec: Remove unused RSB_FILL_LOOPS
      x86/vdso: Provide missing include file
      x86/Kconfig: Correct spelling and punctuation
      Documentation/x86/boot: Fix typo
      x86/boot: Fix a comment's incorrect file reference
      x86/process: Remove set but not used variables prev and next
      x86/Kconfig: Fix Kconfig indentation

commit 3c749b81ee99ef1a01d342ee5e4bc01e4332eb75
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jan 23 17:54:33 2020 +0100

    x86/CPU/AMD: Remove amd_get_topology_early()
    
    ... and fold its function body into its single call site.
    
    No functional changes:
    
      # arch/x86/kernel/cpu/amd.o:
    
       text    data     bss     dec     hex filename
       5994     385       1    6380    18ec amd.o.before
       5994     385       1    6380    18ec amd.o.after
    
    md5:
       99ec6daa095b502297884e949c520f90  amd.o.before.asm
       99ec6daa095b502297884e949c520f90  amd.o.after.asm
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20200123165811.5288-1-bp@alien8.de

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 90f75e515876..8a88d3f01476 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -319,13 +319,6 @@ static void legacy_fixup_core_id(struct cpuinfo_x86 *c)
 	c->cpu_core_id %= cus_per_node;
 }
 
-
-static void amd_get_topology_early(struct cpuinfo_x86 *c)
-{
-	if (cpu_has(c, X86_FEATURE_TOPOEXT))
-		smp_num_siblings = ((cpuid_ebx(0x8000001e) >> 8) & 0xff) + 1;
-}
-
 /*
  * Fixup core topology information for
  * (1) AMD multi-node processors
@@ -717,7 +710,8 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 		}
 	}
 
-	amd_get_topology_early(c);
+	if (cpu_has(c, X86_FEATURE_TOPOEXT))
+		smp_num_siblings = ((cpuid_ebx(0x8000001e) >> 8) & 0xff) + 1;
 }
 
 static void init_amd_k8(struct cpuinfo_x86 *c)

commit a006483b2f97af685f0e60f3a547c9ad4c9b9e94
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Wed Jan 15 16:05:16 2020 -0600

    x86/CPU/AMD: Ensure clearing of SME/SEV features is maintained
    
    If the SME and SEV features are present via CPUID, but memory encryption
    support is not enabled (MSR 0xC001_0010[23]), the feature flags are cleared
    using clear_cpu_cap(). However, if get_cpu_cap() is later called, these
    feature flags will be reset back to present, which is not desired.
    
    Change from using clear_cpu_cap() to setup_clear_cpu_cap() so that the
    clearing of the flags is maintained.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # 4.16.x-
    Link: https://lkml.kernel.org/r/226de90a703c3c0be5a49565047905ac4e94e8f3.1579125915.git.thomas.lendacky@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 90f75e515876..62c30279be77 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -615,9 +615,9 @@ static void early_detect_mem_encrypt(struct cpuinfo_x86 *c)
 		return;
 
 clear_all:
-		clear_cpu_cap(c, X86_FEATURE_SME);
+		setup_clear_cpu_cap(X86_FEATURE_SME);
 clear_sev:
-		clear_cpu_cap(c, X86_FEATURE_SEV);
+		setup_clear_cpu_cap(X86_FEATURE_SEV);
 	}
 }
 

commit 22331f895298bd23ca9f99f6a237aae883c9e1c7
Merge: fc6fd1392a8f 0cc5359d8fd4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 18:47:53 2019 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu-feature updates from Ingo Molnar:
    
     - Rework the Intel model names symbols/macros, which were decades of
       ad-hoc extensions and added random noise. It's now a coherent, easy
       to follow nomenclature.
    
     - Add new Intel CPU model IDs:
        - "Tiger Lake" desktop and mobile models
        - "Elkhart Lake" model ID
        - and the "Lightning Mountain" variant of Airmont, plus support code
    
     - Add the new AVX512_VP2INTERSECT instruction to cpufeatures
    
     - Remove Intel MPX user-visible APIs and the self-tests, because the
       toolchain (gcc) is not supporting it going forward. This is the
       first, lowest-risk phase of MPX removal.
    
     - Remove X86_FEATURE_MFENCE_RDTSC
    
     - Various smaller cleanups and fixes
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (25 commits)
      x86/cpu: Update init data for new Airmont CPU model
      x86/cpu: Add new Airmont variant to Intel family
      x86/cpu: Add Elkhart Lake to Intel family
      x86/cpu: Add Tiger Lake to Intel family
      x86: Correct misc typos
      x86/intel: Add common OPTDIFFs
      x86/intel: Aggregate microserver naming
      x86/intel: Aggregate big core graphics naming
      x86/intel: Aggregate big core mobile naming
      x86/intel: Aggregate big core client naming
      x86/cpufeature: Explain the macro duplication
      x86/ftrace: Remove mcount() declaration
      x86/PCI: Remove superfluous returns from void functions
      x86/msr-index: Move AMD MSRs where they belong
      x86/cpu: Use constant definitions for CPU models
      lib: Remove redundant ftrace flag removal
      x86/crash: Remove unnecessary comparison
      x86/bitops: Use __builtin_constant_p() directly instead of IS_IMMEDIATE()
      x86: Remove X86_FEATURE_MFENCE_RDTSC
      x86/mpx: Remove MPX APIs
      ...

commit 7e67a859997aad47727aff9c5a32e160da079ce3
Merge: 772c1d06bd40 563c4f85f9f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 17:25:49 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - MAINTAINERS: Add Mark Rutland as perf submaintainer, Juri Lelli and
       Vincent Guittot as scheduler submaintainers. Add Dietmar Eggemann,
       Steven Rostedt, Ben Segall and Mel Gorman as scheduler reviewers.
    
       As perf and the scheduler is getting bigger and more complex,
       document the status quo of current responsibilities and interests,
       and spread the review pain^H^H^H^H fun via an increase in the Cc:
       linecount generated by scripts/get_maintainer.pl. :-)
    
     - Add another series of patches that brings the -rt (PREEMPT_RT) tree
       closer to mainline: split the monolithic CONFIG_PREEMPT dependencies
       into a new CONFIG_PREEMPTION category that will allow the eventual
       introduction of CONFIG_PREEMPT_RT. Still a few more hundred patches
       to go though.
    
     - Extend the CPU cgroup controller with uclamp.min and uclamp.max to
       allow the finer shaping of CPU bandwidth usage.
    
     - Micro-optimize energy-aware wake-ups from O(CPUS^2) to O(CPUS).
    
     - Improve the behavior of high CPU count, high thread count
       applications running under cpu.cfs_quota_us constraints.
    
     - Improve balancing with SCHED_IDLE (SCHED_BATCH) tasks present.
    
     - Improve CPU isolation housekeeping CPU allocation NUMA locality.
    
     - Fix deadline scheduler bandwidth calculations and logic when cpusets
       rebuilds the topology, or when it gets deadline-throttled while it's
       being offlined.
    
     - Convert the cpuset_mutex to percpu_rwsem, to allow it to be used from
       setscheduler() system calls without creating global serialization.
       Add new synchronization between cpuset topology-changing events and
       the deadline acceptance tests in setscheduler(), which were broken
       before.
    
     - Rework the active_mm state machine to be less confusing and more
       optimal.
    
     - Rework (simplify) the pick_next_task() slowpath.
    
     - Improve load-balancing on AMD EPYC systems.
    
     - ... and misc cleanups, smaller fixes and improvements - please see
       the Git log for more details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (53 commits)
      sched/psi: Correct overly pessimistic size calculation
      sched/fair: Speed-up energy-aware wake-ups
      sched/uclamp: Always use 'enum uclamp_id' for clamp_id values
      sched/uclamp: Update CPU's refcount on TG's clamp changes
      sched/uclamp: Use TG's clamps to restrict TASK's clamps
      sched/uclamp: Propagate system defaults to the root group
      sched/uclamp: Propagate parent clamps
      sched/uclamp: Extend CPU's cgroup controller
      sched/topology: Improve load balancing on AMD EPYC systems
      arch, ia64: Make NUMA select SMP
      sched, perf: MAINTAINERS update, add submaintainers and reviewers
      sched/fair: Use rq_lock/unlock in online_fair_sched_group
      cpufreq: schedutil: fix equation in comment
      sched: Rework pick_next_task() slow-path
      sched: Allow put_prev_task() to drop rq->lock
      sched/fair: Expose newidle_balance()
      sched: Add task_struct pointer to sched_class::set_curr_task
      sched: Rework CPU hotplug task selection
      sched/{rt,deadline}: Fix set_next_task vs pick_next_task
      sched: Fix kerneldoc comment for ia64_set_curr_task
      ...

commit a55c7454a8c887b226a01d7eed088ccb5374d81e
Author: Matt Fleming <matt@codeblueprint.co.uk>
Date:   Thu Aug 8 20:53:01 2019 +0100

    sched/topology: Improve load balancing on AMD EPYC systems
    
    SD_BALANCE_{FORK,EXEC} and SD_WAKE_AFFINE are stripped in sd_init()
    for any sched domains with a NUMA distance greater than 2 hops
    (RECLAIM_DISTANCE). The idea being that it's expensive to balance
    across domains that far apart.
    
    However, as is rather unfortunately explained in:
    
      commit 32e45ff43eaf ("mm: increase RECLAIM_DISTANCE to 30")
    
    the value for RECLAIM_DISTANCE is based on node distance tables from
    2011-era hardware.
    
    Current AMD EPYC machines have the following NUMA node distances:
    
     node distances:
     node   0   1   2   3   4   5   6   7
       0:  10  16  16  16  32  32  32  32
       1:  16  10  16  16  32  32  32  32
       2:  16  16  10  16  32  32  32  32
       3:  16  16  16  10  32  32  32  32
       4:  32  32  32  32  10  16  16  16
       5:  32  32  32  32  16  10  16  16
       6:  32  32  32  32  16  16  10  16
       7:  32  32  32  32  16  16  16  10
    
    where 2 hops is 32.
    
    The result is that the scheduler fails to load balance properly across
    NUMA nodes on different sockets -- 2 hops apart.
    
    For example, pinning 16 busy threads to NUMA nodes 0 (CPUs 0-7) and 4
    (CPUs 32-39) like so,
    
      $ numactl -C 0-7,32-39 ./spinner 16
    
    causes all threads to fork and remain on node 0 until the active
    balancer kicks in after a few seconds and forcibly moves some threads
    to node 4.
    
    Override node_reclaim_distance for AMD Zen.
    
    Signed-off-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Suravee.Suthikulpanit@amd.com
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas.Lendacky@amd.com
    Cc: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/20190808195301.13222-3-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 8d4e50428b68..ceeb8afc7cf3 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -8,6 +8,7 @@
 #include <linux/sched.h>
 #include <linux/sched/clock.h>
 #include <linux/random.h>
+#include <linux/topology.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cacheinfo.h>
@@ -824,6 +825,10 @@ static void init_amd_zn(struct cpuinfo_x86 *c)
 {
 	set_cpu_cap(c, X86_FEATURE_ZEN);
 
+#ifdef CONFIG_NUMA
+	node_reclaim_distance = 32;
+#endif
+
 	/*
 	 * Fix erratum 1076: CPB feature bit not being set in CPUID.
 	 * Always set it, except when running under a hypervisor.

commit b3e30c9884407599353e690a4eb36d0c4671bf62
Merge: 342061c53a04 a55aa89aab90
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Aug 26 11:20:55 2019 +0200

    Merge tag 'v5.3-rc6' into x86/cpu, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c49a0a80137c7ca7d6ced4c812c9e07a949f6f24
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Aug 19 15:52:35 2019 +0000

    x86/CPU/AMD: Clear RDRAND CPUID bit on AMD family 15h/16h
    
    There have been reports of RDRAND issues after resuming from suspend on
    some AMD family 15h and family 16h systems. This issue stems from a BIOS
    not performing the proper steps during resume to ensure RDRAND continues
    to function properly.
    
    RDRAND support is indicated by CPUID Fn00000001_ECX[30]. This bit can be
    reset by clearing MSR C001_1004[62]. Any software that checks for RDRAND
    support using CPUID, including the kernel, will believe that RDRAND is
    not supported.
    
    Update the CPU initialization to clear the RDRAND CPUID bit for any family
    15h and 16h processor that supports RDRAND. If it is known that the family
    15h or family 16h system does not have an RDRAND resume issue or that the
    system will not be placed in suspend, the "rdrand=force" kernel parameter
    can be used to stop the clearing of the RDRAND CPUID bit.
    
    Additionally, update the suspend and resume path to save and restore the
    MSR C001_1004 value to ensure that the RDRAND CPUID setting remains in
    place after resuming from suspend.
    
    Note, that clearing the RDRAND CPUID bit does not prevent a processor
    that normally supports the RDRAND instruction from executing it. So any
    code that determined the support based on family and model won't #UD.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Cooper <andrew.cooper3@citrix.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chen Yu <yu.c.chen@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: "linux-doc@vger.kernel.org" <linux-doc@vger.kernel.org>
    Cc: "linux-pm@vger.kernel.org" <linux-pm@vger.kernel.org>
    Cc: Nathan Chancellor <natechancellor@gmail.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: <stable@vger.kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "x86@kernel.org" <x86@kernel.org>
    Link: https://lkml.kernel.org/r/7543af91666f491547bd86cebb1e17c66824ab9f.1566229943.git.thomas.lendacky@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 8d4e50428b68..68c363c341bf 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -804,6 +804,64 @@ static void init_amd_ln(struct cpuinfo_x86 *c)
 	msr_set_bit(MSR_AMD64_DE_CFG, 31);
 }
 
+static bool rdrand_force;
+
+static int __init rdrand_cmdline(char *str)
+{
+	if (!str)
+		return -EINVAL;
+
+	if (!strcmp(str, "force"))
+		rdrand_force = true;
+	else
+		return -EINVAL;
+
+	return 0;
+}
+early_param("rdrand", rdrand_cmdline);
+
+static void clear_rdrand_cpuid_bit(struct cpuinfo_x86 *c)
+{
+	/*
+	 * Saving of the MSR used to hide the RDRAND support during
+	 * suspend/resume is done by arch/x86/power/cpu.c, which is
+	 * dependent on CONFIG_PM_SLEEP.
+	 */
+	if (!IS_ENABLED(CONFIG_PM_SLEEP))
+		return;
+
+	/*
+	 * The nordrand option can clear X86_FEATURE_RDRAND, so check for
+	 * RDRAND support using the CPUID function directly.
+	 */
+	if (!(cpuid_ecx(1) & BIT(30)) || rdrand_force)
+		return;
+
+	msr_clear_bit(MSR_AMD64_CPUID_FN_1, 62);
+
+	/*
+	 * Verify that the CPUID change has occurred in case the kernel is
+	 * running virtualized and the hypervisor doesn't support the MSR.
+	 */
+	if (cpuid_ecx(1) & BIT(30)) {
+		pr_info_once("BIOS may not properly restore RDRAND after suspend, but hypervisor does not support hiding RDRAND via CPUID.\n");
+		return;
+	}
+
+	clear_cpu_cap(c, X86_FEATURE_RDRAND);
+	pr_info_once("BIOS may not properly restore RDRAND after suspend, hiding RDRAND via CPUID. Use rdrand=force to reenable.\n");
+}
+
+static void init_amd_jg(struct cpuinfo_x86 *c)
+{
+	/*
+	 * Some BIOS implementations do not restore proper RDRAND support
+	 * across suspend and resume. Check on whether to hide the RDRAND
+	 * instruction support via CPUID.
+	 */
+	clear_rdrand_cpuid_bit(c);
+}
+
 static void init_amd_bd(struct cpuinfo_x86 *c)
 {
 	u64 value;
@@ -818,6 +876,13 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 			wrmsrl_safe(MSR_F15H_IC_CFG, value);
 		}
 	}
+
+	/*
+	 * Some BIOS implementations do not restore proper RDRAND support
+	 * across suspend and resume. Check on whether to hide the RDRAND
+	 * instruction support via CPUID.
+	 */
+	clear_rdrand_cpuid_bit(c);
 }
 
 static void init_amd_zn(struct cpuinfo_x86 *c)
@@ -860,6 +925,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 	case 0x10: init_amd_gh(c); break;
 	case 0x12: init_amd_ln(c); break;
 	case 0x15: init_amd_bd(c); break;
+	case 0x16: init_amd_jg(c); break;
 	case 0x17: init_amd_zn(c); break;
 	}
 

commit be261ffce6f13229dad50f59c5e491f933d3167f
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Jul 4 10:46:37 2019 -0500

    x86: Remove X86_FEATURE_MFENCE_RDTSC
    
    AMD and Intel both have serializing lfence (X86_FEATURE_LFENCE_RDTSC).
    They've both had it for a long time, and AMD has had it enabled in Linux
    since Spectre v1 was announced.
    
    Back then, there was a proposal to remove the serializing mfence feature
    bit (X86_FEATURE_MFENCE_RDTSC), since both AMD and Intel have
    serializing lfence.  At the time, it was (ahem) speculated that some
    hypervisors might not yet support its removal, so it remained for the
    time being.
    
    Now a year-and-a-half later, it should be safe to remove.
    
    I asked Andrew Cooper about whether it's still needed:
    
      So if you're virtualised, you've got no choice in the matter.  lfence
      is either dispatch-serialising or not on AMD, and you won't be able to
      change it.
    
      Furthermore, you can't accurately tell what state the bit is in, because
      the MSR might not be virtualised at all, or may not reflect the true
      state in hardware.  Worse still, attempting to set the bit may not be
      successful even if there isn't a fault for doing so.
    
      Xen sets the DE_CFG bit unconditionally, as does Linux by the looks of
      things (see MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT).  ISTR other hypervisor
      vendors saying the same, but I don't have any information to hand.
    
      If you are running under a hypervisor which has been updated, then
      lfence will almost certainly be dispatch-serialising in practice, and
      you'll almost certainly see the bit already set in DE_CFG.  If you're
      running under a hypervisor which hasn't been patched since Spectre,
      you've already lost in many more ways.
    
      I'd argue that X86_FEATURE_MFENCE_RDTSC is not worth keeping.
    
    So remove it.  This will reduce some code rot, and also make it easier
    to hook barrier_nospec() up to a cmdline disable for performance
    raisins, without having to need an alternative_3() macro.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/d990aa51e40063acb9888e8c1b688e41355a9588.1562255067.git.jpoimboe@redhat.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 8d4e50428b68..3afe07d602dd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -879,12 +879,8 @@ static void init_amd(struct cpuinfo_x86 *c)
 	init_amd_cacheinfo(c);
 
 	if (cpu_has(c, X86_FEATURE_XMM2)) {
-		unsigned long long val;
-		int ret;
-
 		/*
-		 * A serializing LFENCE has less overhead than MFENCE, so
-		 * use it for execution serialization.  On families which
+		 * Use LFENCE for execution serialization.  On families which
 		 * don't have that MSR, LFENCE is already serializing.
 		 * msr_set_bit() uses the safe accessors, too, even if the MSR
 		 * is not present.
@@ -892,19 +888,8 @@ static void init_amd(struct cpuinfo_x86 *c)
 		msr_set_bit(MSR_F10H_DECFG,
 			    MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT);
 
-		/*
-		 * Verify that the MSR write was successful (could be running
-		 * under a hypervisor) and only then assume that LFENCE is
-		 * serializing.
-		 */
-		ret = rdmsrl_safe(MSR_F10H_DECFG, &val);
-		if (!ret && (val & MSR_F10H_DECFG_LFENCE_SERIALIZE)) {
-			/* A serializing LFENCE stops RDTSC speculation */
-			set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
-		} else {
-			/* MFENCE stops RDTSC speculation */
-			set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
-		}
+		/* A serializing LFENCE stops RDTSC speculation */
+		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
 	}
 
 	/*

commit 2ac44ab608705948564791ce1d15d43ba81a1e38
Author: Frank van der Linden <fllinden@amazon.com>
Date:   Wed May 22 22:17:45 2019 +0000

    x86/CPU/AMD: Don't force the CPB cap when running under a hypervisor
    
    For F17h AMD CPUs, the CPB capability ('Core Performance Boost') is forcibly set,
    because some versions of that chip incorrectly report that they do not have it.
    
    However, a hypervisor may filter out the CPB capability, for good
    reasons. For example, KVM currently does not emulate setting the CPB
    bit in MSR_K7_HWCR, and unchecked MSR access errors will be thrown
    when trying to set it as a guest:
    
            unchecked MSR access error: WRMSR to 0xc0010015 (tried to write 0x0000000001000011) at rIP: 0xffffffff890638f4 (native_write_msr+0x4/0x20)
    
            Call Trace:
            boost_set_msr+0x50/0x80 [acpi_cpufreq]
            cpuhp_invoke_callback+0x86/0x560
            sort_range+0x20/0x20
            cpuhp_thread_fun+0xb0/0x110
            smpboot_thread_fn+0xef/0x160
            kthread+0x113/0x130
            kthread_create_worker_on_cpu+0x70/0x70
            ret_from_fork+0x35/0x40
    
    To avoid this issue, don't forcibly set the CPB capability for a CPU
    when running under a hypervisor.
    
    Signed-off-by: Frank van der Linden <fllinden@amazon.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: jiaxun.yang@flygoat.com
    Fixes: 0237199186e7 ("x86/CPU/AMD: Set the CPB bit unconditionally on F17h")
    Link: http://lkml.kernel.org/r/20190522221745.GA15789@dev-dsk-fllinden-2c-c1893d73.us-west-2.amazon.com
    [ Minor edits to the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 80a405c2048a..8d4e50428b68 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -824,8 +824,11 @@ static void init_amd_zn(struct cpuinfo_x86 *c)
 {
 	set_cpu_cap(c, X86_FEATURE_ZEN);
 
-	/* Fix erratum 1076: CPB feature bit not being set in CPUID. */
-	if (!cpu_has(c, X86_FEATURE_CPB))
+	/*
+	 * Fix erratum 1076: CPB feature bit not being set in CPUID.
+	 * Always set it, except when running under a hypervisor.
+	 */
+	if (!cpu_has(c, X86_FEATURE_HYPERVISOR) && !cpu_has(c, X86_FEATURE_CPB))
 		set_cpu_cap(c, X86_FEATURE_CPB);
 }
 

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index fb6a64bd765f..80a405c2048a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 #include <linux/export.h>
 #include <linux/bitops.h>
 #include <linux/elf.h>

commit 26b31f46f036ad89de20cbbb732b76289411eddb
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Mar 29 17:47:36 2019 -0700

    x86/cpu/amd: Exclude 32bit only assembler from 64bit build
    
    The "vide" inline assembler is only needed on 32bit kernels for old
    32bit only CPUs.
    
    Guard it with an #ifdef so it's not included in 64bit builds.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190330004743.29541-2-andi@firstfloor.org

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1bcb489e07e7..fb6a64bd765f 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -82,12 +82,14 @@ static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
  *	performance at the same time..
  */
 
+#ifdef CONFIG_X86_32
 extern __visible void vide(void);
 __asm__(".text\n"
 	".globl vide\n"
 	".type vide, @function\n"
 	".align 4\n"
 	"vide: ret\n");
+#endif
 
 static void init_amd_k5(struct cpuinfo_x86 *c)
 {

commit c03e27506a564ec7db1b179e7464835901f49751
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Mar 29 17:47:35 2019 -0700

    x86/asm: Mark all top level asm statements as .text
    
    With gcc toplevel assembler statements that do not mark themselves as .text
    may end up in other sections. This causes LTO boot crashes because various
    assembler statements ended up in the middle of the initcall section. It's
    also a latent problem without LTO, although it's currently not known to
    cause any real problems.
    
    According to the gcc team it's expected behavior.
    
    Always mark all the top level assembler statements as text so that they
    switch to the right section.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190330004743.29541-1-andi@firstfloor.org

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 01004bfb1a1b..1bcb489e07e7 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -83,7 +83,8 @@ static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
  */
 
 extern __visible void vide(void);
-__asm__(".globl vide\n"
+__asm__(".text\n"
+	".globl vide\n"
 	".type vide, @function\n"
 	".align 4\n"
 	"vide: ret\n");

commit 0237199186e7a4aa5310741f0a6498a20c820fd7
Author: Jiaxun Yang <jiaxun.yang@flygoat.com>
Date:   Tue Nov 20 11:00:18 2018 +0800

    x86/CPU/AMD: Set the CPB bit unconditionally on F17h
    
    Some F17h models do not have CPB set in CPUID even though the CPU
    supports it. Set the feature bit unconditionally on all F17h.
    
     [ bp: Rewrite commit message and patch. ]
    
    Signed-off-by: Jiaxun Yang <jiaxun.yang@flygoat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Sherry Hurwitz <sherry.hurwitz@amd.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20181120030018.5185-1-jiaxun.yang@flygoat.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 69f6bbb41be0..01004bfb1a1b 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -819,11 +819,9 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 static void init_amd_zn(struct cpuinfo_x86 *c)
 {
 	set_cpu_cap(c, X86_FEATURE_ZEN);
-	/*
-	 * Fix erratum 1076: CPB feature bit not being set in CPUID. It affects
-	 * all up to and including B1.
-	 */
-	if (c->x86_model <= 1 && c->x86_stepping <= 1)
+
+	/* Fix erratum 1076: CPB feature bit not being set in CPUID. */
+	if (!cpu_has(c, X86_FEATURE_CPB))
 		set_cpu_cap(c, X86_FEATURE_CPB);
 }
 

commit ad3bc25a320742f42b3015115384f5aec69c7ce2
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Dec 5 00:34:56 2018 +0100

    x86/kernel: Fix more -Wmissing-prototypes warnings
    
    ... with the goal of eventually enabling -Wmissing-prototypes by
    default. At least on x86.
    
    Make functions static where possible, otherwise add prototypes or make
    them visible through includes.
    
    asm/trace/ changes courtesy of Steven Rostedt <rostedt@goodmis.org>.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com> # ACPI + cpufreq bits
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Dimitri Sivanich <dimitri.sivanich@hpe.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Mike Travis <mike.travis@hpe.com>
    Cc: "Steven Rostedt (VMware)" <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Cc: linux-acpi@vger.kernel.org

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index eeea634bee0a..69f6bbb41be0 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -15,6 +15,7 @@
 #include <asm/smp.h>
 #include <asm/pci-direct.h>
 #include <asm/delay.h>
+#include <asm/debugreg.h>
 
 #ifdef CONFIG_X86_64
 # include <asm/mmconfig.h>

commit 88296bd42b4e9d24e138a68b337c235b5cac89a7
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Tue Oct 2 15:45:11 2018 -0700

    x86/cpu/amd: Remove unnecessary parentheses
    
    Clang warns when multiple pairs of parentheses are used for a single
    conditional statement.
    
    arch/x86/kernel/cpu/amd.c:925:14: warning: equality comparison with
    extraneous parentheses [-Wparentheses-equality]
            if ((c->x86 == 6)) {
                 ~~~~~~~^~~~
    arch/x86/kernel/cpu/amd.c:925:14: note: remove extraneous parentheses
    around the comparison to silence this warning
            if ((c->x86 == 6)) {
                ~       ^   ~
    arch/x86/kernel/cpu/amd.c:925:14: note: use '=' to turn this equality
    comparison into an assignment
            if ((c->x86 == 6)) {
                        ^~
                        =
    1 warning generated.
    
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20181002224511.14929-1-natechancellor@gmail.com
    Link: https://github.com/ClangBuiltLinux/linux/issues/187
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 22ab408177b2..eeea634bee0a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -922,7 +922,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 static unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 {
 	/* AMD errata T13 (order #21922) */
-	if ((c->x86 == 6)) {
+	if (c->x86 == 6) {
 		/* Duron Rev A0 */
 		if (c->x86_model == 3 && c->x86_stepping == 0)
 			size = 64;

commit 958f338e96f874a0d29442396d6adf9c1e17aa2d
Merge: 781fca5b1046 07d981ad4cf1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 09:46:06 2018 -0700

    Merge branch 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Merge L1 Terminal Fault fixes from Thomas Gleixner:
     "L1TF, aka L1 Terminal Fault, is yet another speculative hardware
      engineering trainwreck. It's a hardware vulnerability which allows
      unprivileged speculative access to data which is available in the
      Level 1 Data Cache when the page table entry controlling the virtual
      address, which is used for the access, has the Present bit cleared or
      other reserved bits set.
    
      If an instruction accesses a virtual address for which the relevant
      page table entry (PTE) has the Present bit cleared or other reserved
      bits set, then speculative execution ignores the invalid PTE and loads
      the referenced data if it is present in the Level 1 Data Cache, as if
      the page referenced by the address bits in the PTE was still present
      and accessible.
    
      While this is a purely speculative mechanism and the instruction will
      raise a page fault when it is retired eventually, the pure act of
      loading the data and making it available to other speculative
      instructions opens up the opportunity for side channel attacks to
      unprivileged malicious code, similar to the Meltdown attack.
    
      While Meltdown breaks the user space to kernel space protection, L1TF
      allows to attack any physical memory address in the system and the
      attack works across all protection domains. It allows an attack of SGX
      and also works from inside virtual machines because the speculation
      bypasses the extended page table (EPT) protection mechanism.
    
      The assoicated CVEs are: CVE-2018-3615, CVE-2018-3620, CVE-2018-3646
    
      The mitigations provided by this pull request include:
    
       - Host side protection by inverting the upper address bits of a non
         present page table entry so the entry points to uncacheable memory.
    
       - Hypervisor protection by flushing L1 Data Cache on VMENTER.
    
       - SMT (HyperThreading) control knobs, which allow to 'turn off' SMT
         by offlining the sibling CPU threads. The knobs are available on
         the kernel command line and at runtime via sysfs
    
       - Control knobs for the hypervisor mitigation, related to L1D flush
         and SMT control. The knobs are available on the kernel command line
         and at runtime via sysfs
    
       - Extensive documentation about L1TF including various degrees of
         mitigations.
    
      Thanks to all people who have contributed to this in various ways -
      patches, review, testing, backporting - and the fruitful, sometimes
      heated, but at the end constructive discussions.
    
      There is work in progress to provide other forms of mitigations, which
      might be less horrible performance wise for a particular kind of
      workloads, but this is not yet ready for consumption due to their
      complexity and limitations"
    
    * 'l1tf-final' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (75 commits)
      x86/microcode: Allow late microcode loading with SMT disabled
      tools headers: Synchronise x86 cpufeatures.h for L1TF additions
      x86/mm/kmmio: Make the tracer robust against L1TF
      x86/mm/pat: Make set_memory_np() L1TF safe
      x86/speculation/l1tf: Make pmd/pud_mknotpresent() invert
      x86/speculation/l1tf: Invert all not present mappings
      cpu/hotplug: Fix SMT supported evaluation
      KVM: VMX: Tell the nested hypervisor to skip L1D flush on vmentry
      x86/speculation: Use ARCH_CAPABILITIES to skip L1D flush on vmentry
      x86/speculation: Simplify sysfs report of VMX L1TF vulnerability
      Documentation/l1tf: Remove Yonah processors from not vulnerable list
      x86/KVM/VMX: Don't set l1tf_flush_l1d from vmx_handle_external_intr()
      x86/irq: Let interrupt handlers set kvm_cpu_l1tf_flush_l1d
      x86: Don't include linux/irq.h from asm/hardirq.h
      x86/KVM/VMX: Introduce per-host-cpu analogue of l1tf_flush_l1d
      x86/irq: Demote irq_cpustat_t::__softirq_pending to u16
      x86/KVM/VMX: Move the l1tf_flush_l1d test to vmx_l1d_flush()
      x86/KVM/VMX: Replace 'vmx_l1d_flush_always' with 'vmx_l1d_flush_cond'
      x86/KVM/VMX: Don't set l1tf_flush_l1d to true from vmx_l1d_flush()
      cpu/hotplug: detect SMT disabled by BIOS
      ...

commit f2701b77bbd992f3df4631de8493f21db0830452
Merge: 18b57ce2eb8c acb1872577b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Aug 5 16:39:29 2018 +0200

    Merge 4.18-rc7 into master to pick up the KVM dependcy
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 8990cac6e5ea7fa57607736019fe8dca961b998f
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:28 2018 -0400

    x86/jump_label: Initialize static branching early
    
    Static branching is useful to runtime patch branches that are used in hot
    path, but are infrequently changed.
    
    The x86 clock framework is one example that uses static branches to setup
    the best clock during boot and never changes it again.
    
    It is desired to enable the TSC based sched clock early to allow fine
    grained boot time analysis early on. That requires the static branching
    functionality to be functional early as well.
    
    Static branching requires patching nop instructions, thus,
    arch_init_ideal_nops() must be called prior to jump_label_init().
    
    Do all the necessary steps to call arch_init_ideal_nops() right after
    early_cpu_init(), which also allows to insert a call to jump_label_init()
    right after that. jump_label_init() will be called again from the generic
    init code, but the code is protected against reinitialization already.
    
    [ tglx: Massaged changelog ]
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-10-pasha.tatashin@oracle.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 38915fbfae73..b732438c1a1e 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -232,8 +232,6 @@ static void init_amd_k7(struct cpuinfo_x86 *c)
 		}
 	}
 
-	set_cpu_cap(c, X86_FEATURE_K7);
-
 	/* calling is from identify_secondary_cpu() ? */
 	if (!c->cpu_index)
 		return;
@@ -617,6 +615,14 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 
 	early_init_amd_mc(c);
 
+#ifdef CONFIG_X86_32
+	if (c->x86 == 6)
+		set_cpu_cap(c, X86_FEATURE_K7);
+#endif
+
+	if (c->x86 >= 0xf)
+		set_cpu_cap(c, X86_FEATURE_K8);
+
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
 
 	/*
@@ -863,9 +869,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 
 	init_amd_cacheinfo(c);
 
-	if (c->x86 >= 0xf)
-		set_cpu_cap(c, X86_FEATURE_K8);
-
 	if (cpu_has(c, X86_FEATURE_XMM2)) {
 		unsigned long long val;
 		int ret;

commit 845d382bb15c6e7dc5026c0ff919c5b13fc7e11b
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 2 16:35:53 2018 -0500

    x86/bugs: Update when to check for the LS_CFG SSBD mitigation
    
    If either the X86_FEATURE_AMD_SSBD or X86_FEATURE_VIRT_SSBD features are
    present, then there is no need to perform the check for the LS_CFG SSBD
    mitigation support.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180702213553.29202.21089.stgit@tlendack-t1.amdoffice.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 082d7875cef8..38915fbfae73 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -543,7 +543,9 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 		nodes_per_socket = ((value >> 3) & 7) + 1;
 	}
 
-	if (c->x86 >= 0x15 && c->x86 <= 0x17) {
+	if (!boot_cpu_has(X86_FEATURE_AMD_SSBD) &&
+	    !boot_cpu_has(X86_FEATURE_VIRT_SSBD) &&
+	    c->x86 >= 0x15 && c->x86 <= 0x17) {
 		unsigned int bit;
 
 		switch (c->x86) {

commit 7ce2f0393ea2396142b7faf6ee9b1f3676d08a5f
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Jun 22 11:34:11 2018 +0200

    x86/CPU/AMD: Move TOPOEXT reenablement before reading smp_num_siblings
    
    The TOPOEXT reenablement is a workaround for broken BIOSen which didn't
    enable the CPUID bit. amd_get_topology_early(), however, relies on
    that bit being set so that it can read out the CPUID leaf and set
    smp_num_siblings properly.
    
    Move the reenablement up to early_init_amd(). While at it, simplify
    amd_get_topology_early().
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b20dee982a53..74061e421e62 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -318,12 +318,8 @@ static void legacy_fixup_core_id(struct cpuinfo_x86 *c)
 
 static void amd_get_topology_early(struct cpuinfo_x86 *c)
 {
-	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
-		u32 eax, ebx, ecx, edx;
-
-		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
-		smp_num_siblings = ((ebx >> 8) & 0xff) + 1;
-	}
+	if (cpu_has(c, X86_FEATURE_TOPOEXT))
+		smp_num_siblings = ((cpuid_ebx(0x8000001e) >> 8) & 0xff) + 1;
 }
 
 /*
@@ -345,7 +341,6 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
 
 		node_id  = ecx & 0xff;
-		smp_num_siblings = ((ebx >> 8) & 0xff) + 1;
 
 		if (c->x86 == 0x15)
 			c->cu_id = ebx & 0xff;
@@ -622,6 +617,7 @@ static void early_detect_mem_encrypt(struct cpuinfo_x86 *c)
 
 static void early_init_amd(struct cpuinfo_x86 *c)
 {
+	u64 value;
 	u32 dummy;
 
 	early_init_amd_mc(c);
@@ -693,6 +689,20 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 
 	early_detect_mem_encrypt(c);
 
+	/* Re-enable TopologyExtensions if switched off by BIOS */
+	if (c->x86 == 0x15 &&
+	    (c->x86_model >= 0x10 && c->x86_model <= 0x6f) &&
+	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
+
+		if (msr_set_bit(0xc0011005, 54) > 0) {
+			rdmsrl(0xc0011005, value);
+			if (value & BIT_64(54)) {
+				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
+				pr_info_once(FW_INFO "CPU: Re-enabling disabled Topology Extensions Support.\n");
+			}
+		}
+	}
+
 	amd_get_topology_early(c);
 }
 
@@ -785,19 +795,6 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 {
 	u64 value;
 
-	/* re-enable TopologyExtensions if switched off by BIOS */
-	if ((c->x86_model >= 0x10) && (c->x86_model <= 0x6f) &&
-	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
-
-		if (msr_set_bit(0xc0011005, 54) > 0) {
-			rdmsrl(0xc0011005, value);
-			if (value & BIT_64(54)) {
-				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
-				pr_info_once(FW_INFO "CPU: Re-enabling disabled Topology Extensions Support.\n");
-			}
-		}
-	}
-
 	/*
 	 * The way access filter has a performance penalty on some workloads.
 	 * Disable it on the affected CPUs.

commit 1e1d7e25fd759eddf96d8ab39d0a90a1979b2d8c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 6 00:57:38 2018 +0200

    x86/cpu/AMD: Evaluate smp_num_siblings early
    
    To support force disabling of SMT it's required to know the number of
    thread siblings early. amd_get_topology() cannot be called before the APIC
    driver is selected, so split out the part which initializes
    smp_num_siblings and invoke it from amd_early_init().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index d120e5cefca9..b20dee982a53 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -315,6 +315,17 @@ static void legacy_fixup_core_id(struct cpuinfo_x86 *c)
 	c->cpu_core_id %= cus_per_node;
 }
 
+
+static void amd_get_topology_early(struct cpuinfo_x86 *c)
+{
+	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
+		u32 eax, ebx, ecx, edx;
+
+		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
+		smp_num_siblings = ((ebx >> 8) & 0xff) + 1;
+	}
+}
+
 /*
  * Fixup core topology information for
  * (1) AMD multi-node processors
@@ -681,6 +692,8 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 		set_cpu_bug(c, X86_BUG_AMD_E400);
 
 	early_detect_mem_encrypt(c);
+
+	amd_get_topology_early(c);
 }
 
 static void init_amd_k8(struct cpuinfo_x86 *c)

commit 119bff8a9c9bb00116a844ec68be7bc4b1c768f5
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Jun 15 20:48:39 2018 +0200

    x86/CPU/AMD: Do not check CPUID max ext level before parsing SMP info
    
    Old code used to check whether CPUID ext max level is >= 0x80000008 because
    that last leaf contains the number of cores of the physical CPU.  The three
    functions called there now do not depend on that leaf anymore so the check
    can go.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 9fb1c681b63a..d120e5cefca9 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -848,12 +848,9 @@ static void init_amd(struct cpuinfo_x86 *c)
 
 	cpu_detect_cache_sizes(c);
 
-	/* Multi core CPU? */
-	if (c->extended_cpuid_level >= 0x80000008) {
-		amd_detect_cmp(c);
-		amd_get_topology(c);
-		srat_detect_node(c);
-	}
+	amd_detect_cmp(c);
+	amd_get_topology(c);
+	srat_detect_node(c);
 
 	init_amd_cacheinfo(c);
 

commit 44ca36de56d1bf196dca2eb67cd753a46961ffe6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 6 00:47:10 2018 +0200

    x86/cpu/AMD: Remove the pointless detect_ht() call
    
    Real 32bit AMD CPUs do not have SMT and the only value of the call was to
    reach the magic printout which got removed.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 082d7875cef8..9fb1c681b63a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -855,10 +855,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 		srat_detect_node(c);
 	}
 
-#ifdef CONFIG_X86_32
-	detect_ht(c);
-#endif
-
 	init_amd_cacheinfo(c);
 
 	if (c->x86 >= 0xf)

commit 5cef8c2a2289117b7f65de4313b7157578ec1a71
Merge: f7f4e7fc6c51 e4e961e36f06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 18:19:18 2018 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
    
     - Centaur CPU updates (David Wang)
    
     - AMD and other CPU topology enumeration improvements and fixes
       (Borislav Petkov, Thomas Gleixner, Suravee Suthikulpanit)
    
     - Continued 5-level paging work (Kirill A. Shutemov)
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Mark __pgtable_l5_enabled __initdata
      x86/mm: Mark p4d_offset() __always_inline
      x86/mm: Introduce the 'no5lvl' kernel parameter
      x86/mm: Stop pretending pgtable_l5_enabled is a variable
      x86/mm: Unify pgtable_l5_enabled usage in early boot code
      x86/boot/compressed/64: Fix trampoline page table address calculation
      x86/CPU: Move x86_cpuinfo::x86_max_cores assignment to detect_num_cpu_cores()
      x86/Centaur: Report correct CPU/cache topology
      x86/CPU: Move cpu_detect_cache_sizes() into init_intel_cacheinfo()
      x86/CPU: Make intel_num_cpu_cores() generic
      x86/CPU: Move cpu local function declarations to local header
      x86/CPU/AMD: Derive CPU topology from CPUID function 0xB when available
      x86/CPU: Modify detect_extended_topology() to return result
      x86/CPU/AMD: Calculate last level cache ID from number of sharing threads
      x86/CPU: Rename intel_cacheinfo.c to cacheinfo.c
      perf/events/amd/uncore: Fix amd_uncore_llc ID to use pre-defined cpu_llc_id
      x86/CPU/AMD: Have smp_num_siblings and cpu_llc_id always be present
      x86/Centaur: Initialize supported CPU features properly

commit d1035d971829dcf80e8686ccde26f94b0a069472
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 16:26:00 2018 +0200

    x86/cpufeatures: Add FEATURE_ZEN
    
    Add a ZEN feature bit so family-dependent static_cpu_has() optimizations
    can be built for ZEN.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 2d2d8985654b..1b18be3f35a8 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -812,6 +812,7 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 
 static void init_amd_zn(struct cpuinfo_x86 *c)
 {
+	set_cpu_cap(c, X86_FEATURE_ZEN);
 	/*
 	 * Fix erratum 1076: CPB feature bit not being set in CPUID. It affects
 	 * all up to and including B1.

commit 52817587e706686fcdb27f14c1b000c92f266c96
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 10 20:21:36 2018 +0200

    x86/cpufeatures: Disentangle SSBD enumeration
    
    The SSBD enumeration is similarly to the other bits magically shared
    between Intel and AMD though the mechanisms are different.
    
    Make X86_FEATURE_SSBD synthetic and set it depending on the vendor specific
    features or family dependent setup.
    
    Change the Intel bit to X86_FEATURE_SPEC_CTRL_SSBD to denote that SSBD is
    controlled via MSR_SPEC_CTRL and fix up the usage sites.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7bde990b0385..2d2d8985654b 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -570,8 +570,8 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 		 * avoid RMW. If that faults, do not enable SSBD.
 		 */
 		if (!rdmsrl_safe(MSR_AMD64_LS_CFG, &x86_amd_ls_cfg_base)) {
+			setup_force_cpu_cap(X86_FEATURE_LS_CFG_SSBD);
 			setup_force_cpu_cap(X86_FEATURE_SSBD);
-			setup_force_cpu_cap(X86_FEATURE_AMD_SSBD);
 			x86_amd_ls_cfg_ssbd_mask = 1ULL << bit;
 		}
 	}
@@ -919,11 +919,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 	/* AMD CPUs don't reset SS attributes on SYSRET, Xen does. */
 	if (!cpu_has(c, X86_FEATURE_XENPV))
 		set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
-
-	if (boot_cpu_has(X86_FEATURE_AMD_SSBD)) {
-		set_cpu_cap(c, X86_FEATURE_SSBD);
-		set_cpu_cap(c, X86_FEATURE_AMD_SSBD);
-	}
 }
 
 #ifdef CONFIG_X86_32

commit 9f65fb29374ee37856dbad847b4e121aab72b510
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 9 21:41:38 2018 +0200

    x86/bugs: Rename _RDS to _SSBD
    
    Intel collateral will reference the SSB mitigation bit in IA32_SPEC_CTL[2]
    as SSBD (Speculative Store Bypass Disable).
    
    Hence changing it.
    
    It is unclear yet what the MSR_IA32_ARCH_CAPABILITIES (0x10a) Bit(4) name
    is going to be. Following the rename it would be SSBD_NO but that rolls out
    to Speculative Store Bypass Disable No.
    
    Also fixed the missing space in X86_FEATURE_AMD_SSBD.
    
    [ tglx: Fixup x86_amd_rds_enable() and rds_tif_to_amd_ls_cfg() as well ]
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 18efc33a8d2e..7bde990b0385 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -567,12 +567,12 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 		}
 		/*
 		 * Try to cache the base value so further operations can
-		 * avoid RMW. If that faults, do not enable RDS.
+		 * avoid RMW. If that faults, do not enable SSBD.
 		 */
 		if (!rdmsrl_safe(MSR_AMD64_LS_CFG, &x86_amd_ls_cfg_base)) {
-			setup_force_cpu_cap(X86_FEATURE_RDS);
-			setup_force_cpu_cap(X86_FEATURE_AMD_RDS);
-			x86_amd_ls_cfg_rds_mask = 1ULL << bit;
+			setup_force_cpu_cap(X86_FEATURE_SSBD);
+			setup_force_cpu_cap(X86_FEATURE_AMD_SSBD);
+			x86_amd_ls_cfg_ssbd_mask = 1ULL << bit;
 		}
 	}
 }
@@ -920,9 +920,9 @@ static void init_amd(struct cpuinfo_x86 *c)
 	if (!cpu_has(c, X86_FEATURE_XENPV))
 		set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
 
-	if (boot_cpu_has(X86_FEATURE_AMD_RDS)) {
-		set_cpu_cap(c, X86_FEATURE_RDS);
-		set_cpu_cap(c, X86_FEATURE_AMD_RDS);
+	if (boot_cpu_has(X86_FEATURE_AMD_SSBD)) {
+		set_cpu_cap(c, X86_FEATURE_SSBD);
+		set_cpu_cap(c, X86_FEATURE_AMD_SSBD);
 	}
 }
 

commit 3986a0a805e668a63fac0ca2cdfa8db951f87c4b
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Fri Apr 27 16:48:01 2018 -0500

    x86/CPU/AMD: Derive CPU topology from CPUID function 0xB when available
    
    Derive topology information from Extended Topology Enumeration (CPUID
    function 0xB) when the information is available.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1524865681-112110-3-git-send-email-suravee.suthikulpanit@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bf27246bb7bd..55361ee04cc5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -327,6 +327,7 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 
 	/* get information required for multi-node processors */
 	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
+		int err;
 		u32 eax, ebx, ecx, edx;
 
 		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
@@ -344,6 +345,14 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 				c->x86_max_cores /= smp_num_siblings;
 		}
 
+		/*
+		 * In case leaf B is available, use it to derive
+		 * topology information.
+		 */
+		err = detect_extended_topology(c);
+		if (!err)
+			c->x86_coreid_bits = get_count_order(c->x86_max_cores);
+
 		cacheinfo_amd_init_llc_id(c, cpu, node_id);
 
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
@@ -378,7 +387,6 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 	c->phys_proc_id = c->initial_apicid >> bits;
 	/* use socket ID also for last level cache */
 	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
-	amd_get_topology(c);
 }
 
 u16 amd_get_nb_id(int cpu)
@@ -821,6 +829,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 	/* Multi core CPU? */
 	if (c->extended_cpuid_level >= 0x80000008) {
 		amd_detect_cmp(c);
+		amd_get_topology(c);
 		srat_detect_node(c);
 	}
 

commit 68091ee7ac3c1a8786fe1bebbd616b14236efb99
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Fri Apr 27 16:34:37 2018 -0500

    x86/CPU/AMD: Calculate last level cache ID from number of sharing threads
    
    Last Level Cache ID can be calculated from the number of threads sharing
    the cache, which is available from CPUID Fn0x8000001D (Cache Properties).
    This is used to left-shift the APIC ID to derive LLC ID.
    
    Therefore, default to this method unless the APIC ID enumeration does not
    follow the scheme.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1524864877-111962-5-git-send-email-suravee.suthikulpanit@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a37a83809665..bf27246bb7bd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -9,6 +9,7 @@
 #include <linux/random.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
+#include <asm/cacheinfo.h>
 #include <asm/cpu.h>
 #include <asm/smp.h>
 #include <asm/pci-direct.h>
@@ -343,22 +344,8 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 				c->x86_max_cores /= smp_num_siblings;
 		}
 
-		/*
-		 * We may have multiple LLCs if L3 caches exist, so check if we
-		 * have an L3 cache by looking at the L3 cache CPUID leaf.
-		 */
-		if (cpuid_edx(0x80000006)) {
-			if (c->x86 == 0x17) {
-				/*
-				 * LLC is at the core complex level.
-				 * Core complex id is ApicId[3].
-				 */
-				per_cpu(cpu_llc_id, cpu) = c->apicid >> 3;
-			} else {
-				/* LLC is at the node level. */
-				per_cpu(cpu_llc_id, cpu) = node_id;
-			}
-		}
+		cacheinfo_amd_init_llc_id(c, cpu, node_id);
+
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
 		u64 value;
 

commit f8b64d08dde2714c62751d18ba77f4aeceb161d3
Author: Borislav Petkov <bpetkov@suse.de>
Date:   Fri Apr 27 16:34:34 2018 -0500

    x86/CPU/AMD: Have smp_num_siblings and cpu_llc_id always be present
    
    Move smp_num_siblings and cpu_llc_id to cpu/common.c so that they're
    always present as symbols and not only in the CONFIG_SMP case. Then,
    other code using them doesn't need ugly ifdeffery anymore. Get rid of
    some ifdeffery.
    
    Signed-off-by: Borislav Petkov <bpetkov@suse.de>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1524864877-111962-2-git-send-email-suravee.suthikulpanit@amd.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 12bc0a1139da..a37a83809665 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -297,7 +297,6 @@ static int nearby_node(int apicid)
 }
 #endif
 
-#ifdef CONFIG_SMP
 /*
  * Fix up cpu_core_id for pre-F17h systems to be in the
  * [0 .. cores_per_node - 1] range. Not really needed but
@@ -375,7 +374,6 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		legacy_fixup_core_id(c);
 	}
 }
-#endif
 
 /*
  * On a AMD dual core setup the lower bits of the APIC id distinguish the cores.
@@ -383,7 +381,6 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
  */
 static void amd_detect_cmp(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_SMP
 	unsigned bits;
 	int cpu = smp_processor_id();
 
@@ -395,16 +392,11 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 	/* use socket ID also for last level cache */
 	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
 	amd_get_topology(c);
-#endif
 }
 
 u16 amd_get_nb_id(int cpu)
 {
-	u16 id = 0;
-#ifdef CONFIG_SMP
-	id = per_cpu(cpu_llc_id, cpu);
-#endif
-	return id;
+	return per_cpu(cpu_llc_id, cpu);
 }
 EXPORT_SYMBOL_GPL(amd_get_nb_id);
 

commit 28a2775217b17208811fa43a9e96bd1fdf417b86
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 29 15:01:37 2018 +0200

    x86/speculation: Create spec-ctrl.h to avoid include hell
    
    Having everything in nospec-branch.h creates a hell of dependencies when
    adding the prctl based switching mechanism. Move everything which is not
    required in nospec-branch.h to spec-ctrl.h and fix up the includes in the
    relevant files.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 3d553fa5075d..18efc33a8d2e 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -10,7 +10,7 @@
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cpu.h>
-#include <asm/nospec-branch.h>
+#include <asm/spec-ctrl.h>
 #include <asm/smp.h>
 #include <asm/pci-direct.h>
 #include <asm/delay.h>

commit 764f3c21588a059cd783c6ba0734d4db2d72822d
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 25 22:04:24 2018 -0400

    x86/bugs/AMD: Add support to disable RDS on Fam[15,16,17]h if requested
    
    AMD does not need the Speculative Store Bypass mitigation to be enabled.
    
    The parameters for this are already available and can be done via MSR
    C001_1020. Each family uses a different bit in that MSR for this.
    
    [ tglx: Expose the bit mask via a variable and move the actual MSR fiddling
            into the bugs code as that's the right thing to do and also required
            to prepare for dynamic enable/disable ]
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 12bc0a1139da..3d553fa5075d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -10,6 +10,7 @@
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cpu.h>
+#include <asm/nospec-branch.h>
 #include <asm/smp.h>
 #include <asm/pci-direct.h>
 #include <asm/delay.h>
@@ -554,6 +555,26 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 		rdmsrl(MSR_FAM10H_NODE_ID, value);
 		nodes_per_socket = ((value >> 3) & 7) + 1;
 	}
+
+	if (c->x86 >= 0x15 && c->x86 <= 0x17) {
+		unsigned int bit;
+
+		switch (c->x86) {
+		case 0x15: bit = 54; break;
+		case 0x16: bit = 33; break;
+		case 0x17: bit = 10; break;
+		default: return;
+		}
+		/*
+		 * Try to cache the base value so further operations can
+		 * avoid RMW. If that faults, do not enable RDS.
+		 */
+		if (!rdmsrl_safe(MSR_AMD64_LS_CFG, &x86_amd_ls_cfg_base)) {
+			setup_force_cpu_cap(X86_FEATURE_RDS);
+			setup_force_cpu_cap(X86_FEATURE_AMD_RDS);
+			x86_amd_ls_cfg_rds_mask = 1ULL << bit;
+		}
+	}
 }
 
 static void early_detect_mem_encrypt(struct cpuinfo_x86 *c)
@@ -898,6 +919,11 @@ static void init_amd(struct cpuinfo_x86 *c)
 	/* AMD CPUs don't reset SS attributes on SYSRET, Xen does. */
 	if (!cpu_has(c, X86_FEATURE_XENPV))
 		set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
+
+	if (boot_cpu_has(X86_FEATURE_AMD_RDS)) {
+		set_cpu_cap(c, X86_FEATURE_RDS);
+		set_cpu_cap(c, X86_FEATURE_AMD_RDS);
+	}
 }
 
 #ifdef CONFIG_X86_32

commit 8364e1f8379c7f9d3e63f127a585f889906b3e10
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Wed Mar 7 08:39:17 2018 +0100

    x86/jailhouse: Allow to use PCI_MMCONFIG without ACPI
    
    Jailhouse does not use ACPI, but it does support MMCONFIG. Make sure the
    latter can be built without having to enable ACPI as well. Primarily, its
    required to make the AMD mmconf-fam10h_64 depend upon MMCONFIG and
    ACPI, instead of just the former.
    
    Saves some bytes in the Jailhouse non-root kernel.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: jailhouse-dev@googlegroups.com
    Cc: linux-pci@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Link: https://lkml.kernel.org/r/788bbd5325d1922235e9562c213057425fbc548c.1520408357.git.jan.kiszka@siemens.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f0e6456ca7d3..12bc0a1139da 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -716,7 +716,7 @@ static void init_amd_k8(struct cpuinfo_x86 *c)
 
 static void init_amd_gh(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_X86_64
+#ifdef CONFIG_MMCONF_FAM10H
 	/* do this for boot cpu */
 	if (c == &boot_cpu_data)
 		check_enable_amd_mmconf_dmi();

commit d4667ca142610961c89ae7c41a823b3358fcdd0e
Merge: 6556677a8040 e48657573481
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 14 17:02:15 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 PTI and Spectre related fixes and updates from Ingo Molnar:
     "Here's the latest set of Spectre and PTI related fixes and updates:
    
      Spectre:
       - Add entry code register clearing to reduce the Spectre attack
         surface
       - Update the Spectre microcode blacklist
       - Inline the KVM Spectre helpers to get close to v4.14 performance
         again.
       - Fix indirect_branch_prediction_barrier()
       - Fix/improve Spectre related kernel messages
       - Fix array_index_nospec_mask() asm constraint
       - KVM: fix two MSR handling bugs
    
      PTI:
       - Fix a paranoid entry PTI CR3 handling bug
       - Fix comments
    
      objtool:
       - Fix paranoid_entry() frame pointer warning
       - Annotate WARN()-related UD2 as reachable
       - Various fixes
       - Add Add Peter Zijlstra as objtool co-maintainer
    
      Misc:
       - Various x86 entry code self-test fixes
       - Improve/simplify entry code stack frame generation and handling
         after recent heavy-handed PTI and Spectre changes. (There's two
         more WIP improvements expected here.)
       - Type fix for cache entries
    
      There's also some low risk non-fix changes I've included in this
      branch to reduce backporting conflicts:
    
       - rename a confusing x86_cpu field name
       - de-obfuscate the naming of single-TLB flushing primitives"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (41 commits)
      x86/entry/64: Fix CR3 restore in paranoid_exit()
      x86/cpu: Change type of x86_cache_size variable to unsigned int
      x86/spectre: Fix an error message
      x86/cpu: Rename cpu_data.x86_mask to cpu_data.x86_stepping
      selftests/x86/mpx: Fix incorrect bounds with old _sigfault
      x86/mm: Rename flush_tlb_single() and flush_tlb_one() to __flush_tlb_one_[user|kernel]()
      x86/speculation: Add <asm/msr-index.h> dependency
      nospec: Move array_index_nospec() parameter checking into separate macro
      x86/speculation: Fix up array_index_nospec_mask() asm constraint
      x86/debug: Use UD2 for WARN()
      x86/debug, objtool: Annotate WARN()-related UD2 as reachable
      objtool: Fix segfault in ignore_unreachable_insn()
      selftests/x86: Disable tests requiring 32-bit support on pure 64-bit systems
      selftests/x86: Do not rely on "int $0x80" in single_step_syscall.c
      selftests/x86: Do not rely on "int $0x80" in test_mremap_vdso.c
      selftests/x86: Fix build bug caused by the 5lvl test which has been moved to the VM directory
      selftests/x86/pkeys: Remove unused functions
      selftests/x86: Clean up and document sscanf() usage
      selftests/x86: Fix vDSO selftest segfault for vsyscall=none
      x86/entry/64: Remove the unused 'icebp' macro
      ...

commit b399151cb48db30ad1e0e93dd40d68c6d007b637
Author: Jia Zhang <qianyue.zj@alibaba-inc.com>
Date:   Mon Jan 1 09:52:10 2018 +0800

    x86/cpu: Rename cpu_data.x86_mask to cpu_data.x86_stepping
    
    x86_mask is a confusing name which is hard to associate with the
    processor's stepping.
    
    Additionally, correct an indent issue in lib/cpu.c.
    
    Signed-off-by: Jia Zhang <qianyue.zj@alibaba-inc.com>
    [ Updated it to more recent kernels. ]
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: tony.luck@intel.com
    Link: http://lkml.kernel.org/r/1514771530-70829-1-git-send-email-qianyue.zj@alibaba-inc.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ea831c858195..e7d5a7883632 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -119,7 +119,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 		return;
 	}
 
-	if (c->x86_model == 6 && c->x86_mask == 1) {
+	if (c->x86_model == 6 && c->x86_stepping == 1) {
 		const int K6_BUG_LOOP = 1000000;
 		int n;
 		void (*f_vide)(void);
@@ -149,7 +149,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 
 	/* K6 with old style WHCR */
 	if (c->x86_model < 8 ||
-	   (c->x86_model == 8 && c->x86_mask < 8)) {
+	   (c->x86_model == 8 && c->x86_stepping < 8)) {
 		/* We can only write allocate on the low 508Mb */
 		if (mbytes > 508)
 			mbytes = 508;
@@ -168,7 +168,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 		return;
 	}
 
-	if ((c->x86_model == 8 && c->x86_mask > 7) ||
+	if ((c->x86_model == 8 && c->x86_stepping > 7) ||
 	     c->x86_model == 9 || c->x86_model == 13) {
 		/* The more serious chips .. */
 
@@ -221,7 +221,7 @@ static void init_amd_k7(struct cpuinfo_x86 *c)
 	 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
 	 * As per AMD technical note 27212 0.2
 	 */
-	if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
+	if ((c->x86_model == 8 && c->x86_stepping >= 1) || (c->x86_model > 8)) {
 		rdmsr(MSR_K7_CLK_CTL, l, h);
 		if ((l & 0xfff00000) != 0x20000000) {
 			pr_info("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n",
@@ -241,12 +241,12 @@ static void init_amd_k7(struct cpuinfo_x86 *c)
 	 * but they are not certified as MP capable.
 	 */
 	/* Athlon 660/661 is valid. */
-	if ((c->x86_model == 6) && ((c->x86_mask == 0) ||
-	    (c->x86_mask == 1)))
+	if ((c->x86_model == 6) && ((c->x86_stepping == 0) ||
+	    (c->x86_stepping == 1)))
 		return;
 
 	/* Duron 670 is valid */
-	if ((c->x86_model == 7) && (c->x86_mask == 0))
+	if ((c->x86_model == 7) && (c->x86_stepping == 0))
 		return;
 
 	/*
@@ -256,8 +256,8 @@ static void init_amd_k7(struct cpuinfo_x86 *c)
 	 * See http://www.heise.de/newsticker/data/jow-18.10.01-000 for
 	 * more.
 	 */
-	if (((c->x86_model == 6) && (c->x86_mask >= 2)) ||
-	    ((c->x86_model == 7) && (c->x86_mask >= 1)) ||
+	if (((c->x86_model == 6) && (c->x86_stepping >= 2)) ||
+	    ((c->x86_model == 7) && (c->x86_stepping >= 1)) ||
 	     (c->x86_model > 7))
 		if (cpu_has(c, X86_FEATURE_MP))
 			return;
@@ -583,7 +583,7 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	/*  Set MTRR capability flag if appropriate */
 	if (c->x86 == 5)
 		if (c->x86_model == 13 || c->x86_model == 9 ||
-		    (c->x86_model == 8 && c->x86_mask >= 8))
+		    (c->x86_model == 8 && c->x86_stepping >= 8))
 			set_cpu_cap(c, X86_FEATURE_K6_MTRR);
 #endif
 #if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_PCI)
@@ -769,7 +769,7 @@ static void init_amd_zn(struct cpuinfo_x86 *c)
 	 * Fix erratum 1076: CPB feature bit not being set in CPUID. It affects
 	 * all up to and including B1.
 	 */
-	if (c->x86_model <= 1 && c->x86_mask <= 1)
+	if (c->x86_model <= 1 && c->x86_stepping <= 1)
 		set_cpu_cap(c, X86_FEATURE_CPB);
 }
 
@@ -880,11 +880,11 @@ static unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 	/* AMD errata T13 (order #21922) */
 	if ((c->x86 == 6)) {
 		/* Duron Rev A0 */
-		if (c->x86_model == 3 && c->x86_mask == 0)
+		if (c->x86_model == 3 && c->x86_stepping == 0)
 			size = 64;
 		/* Tbird rev A1/A2 */
 		if (c->x86_model == 4 &&
-			(c->x86_mask == 0 || c->x86_mask == 1))
+			(c->x86_stepping == 0 || c->x86_stepping == 1))
 			size = 256;
 	}
 	return size;
@@ -1021,7 +1021,7 @@ static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
 	}
 
 	/* OSVW unavailable or ID unknown, match family-model-stepping range */
-	ms = (cpu->x86_model << 4) | cpu->x86_mask;
+	ms = (cpu->x86_model << 4) | cpu->x86_stepping;
 	while ((range = *erratum++))
 		if ((cpu->x86 == AMD_MODEL_RANGE_FAMILY(range)) &&
 		    (ms >= AMD_MODEL_RANGE_START(range)) &&

commit 7bf14c28ee776be567855bd39ed8ff795ea19f55
Merge: 87cedc6be559 5fa4ec9cb2e6
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 1 15:04:17 2018 +0100

    Merge branch 'x86/hyperv' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Topic branch for stable KVM clockource under Hyper-V.
    
    Thanks to Christoffer Dall for resolving the ARM conflict.

commit 65e38583c3bbbba78a081c808e2d58a8454a821e
Merge: 476b7adaa327 00b10fe1046c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jan 16 16:34:48 2018 +0100

    Merge branch 'sev-v9-p2' of https://github.com/codomania/kvm
    
    This part of Secure Encrypted Virtualization (SEV) patch series focuses on KVM
    changes required to create and manage SEV guests.
    
    SEV is an extension to the AMD-V architecture which supports running encrypted
    virtual machine (VMs) under the control of a hypervisor. Encrypted VMs have their
    pages (code and data) secured such that only the guest itself has access to
    unencrypted version. Each encrypted VM is associated with a unique encryption key;
    if its data is accessed to a different entity using a different key the encrypted
    guest's data will be incorrectly decrypted, leading to unintelligible data.
    This security model ensures that hypervisor will no longer able to inspect or
    alter any guest code or data.
    
    The key management of this feature is handled by a separate processor known as
    the AMD Secure Processor (AMD-SP) which is present on AMD SOCs. The SEV Key
    Management Specification (see below) provides a set of commands which can be
    used by hypervisor to load virtual machine keys through the AMD-SP driver.
    
    The patch series adds a new ioctl in KVM driver (KVM_MEMORY_ENCRYPT_OP). The
    ioctl will be used by qemu to issue SEV guest-specific commands defined in Key
    Management Specification.
    
    The following links provide additional details:
    
    AMD Memory Encryption white paper:
    http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2013/12/AMD_Memory_Encryption_Whitepaper_v7-Public.pdf
    
    AMD64 Architecture Programmer's Manual:
        http://support.amd.com/TechDocs/24593.pdf
        SME is section 7.10
        SEV is section 15.34
    
    SEV Key Management:
    http://support.amd.com/TechDocs/55766_SEV-KM API_Specification.pdf
    
    KVM Forum Presentation:
    http://www.linux-kvm.org/images/7/74/02x08A-Thomas_Lendacky-AMDs_Virtualizatoin_Memory_Encryption_Technology.pdf
    
    SEV Guest BIOS support:
      SEV support has been add to EDKII/OVMF BIOS
      https://github.com/tianocore/edk2
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 9c6a73c75864ad9fa49e5fa6513e4c4071c0e29f
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jan 8 16:09:32 2018 -0600

    x86/cpu/AMD: Use LFENCE_RDTSC in preference to MFENCE_RDTSC
    
    With LFENCE now a serializing instruction, use LFENCE_RDTSC in preference
    to MFENCE_RDTSC.  However, since the kernel could be running under a
    hypervisor that does not support writing that MSR, read the MSR back and
    verify that the bit has been set successfully.  If the MSR can be read
    and the bit is set, then set the LFENCE_RDTSC feature, otherwise set the
    MFENCE_RDTSC feature.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Paul Turner <pjt@google.com>
    Link: https://lkml.kernel.org/r/20180108220932.12580.52458.stgit@tlendack-t1.amdoffice.net

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 5b438d81beb2..ea831c858195 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -829,6 +829,9 @@ static void init_amd(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_K8);
 
 	if (cpu_has(c, X86_FEATURE_XMM2)) {
+		unsigned long long val;
+		int ret;
+
 		/*
 		 * A serializing LFENCE has less overhead than MFENCE, so
 		 * use it for execution serialization.  On families which
@@ -839,8 +842,19 @@ static void init_amd(struct cpuinfo_x86 *c)
 		msr_set_bit(MSR_F10H_DECFG,
 			    MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT);
 
-		/* MFENCE stops RDTSC speculation */
-		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
+		/*
+		 * Verify that the MSR write was successful (could be running
+		 * under a hypervisor) and only then assume that LFENCE is
+		 * serializing.
+		 */
+		ret = rdmsrl_safe(MSR_F10H_DECFG, &val);
+		if (!ret && (val & MSR_F10H_DECFG_LFENCE_SERIALIZE)) {
+			/* A serializing LFENCE stops RDTSC speculation */
+			set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
+		} else {
+			/* MFENCE stops RDTSC speculation */
+			set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
+		}
 	}
 
 	/*

commit e4d0e84e490790798691aaa0f2e598637f1867ec
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jan 8 16:09:21 2018 -0600

    x86/cpu/AMD: Make LFENCE a serializing instruction
    
    To aid in speculation control, make LFENCE a serializing instruction
    since it has less overhead than MFENCE.  This is done by setting bit 1
    of MSR 0xc0011029 (DE_CFG).  Some families that support LFENCE do not
    have this MSR.  For these families, the LFENCE instruction is already
    serializing.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Paul Turner <pjt@google.com>
    Link: https://lkml.kernel.org/r/20180108220921.12580.71694.stgit@tlendack-t1.amdoffice.net

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bcb75dc97d44..5b438d81beb2 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -829,6 +829,16 @@ static void init_amd(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_K8);
 
 	if (cpu_has(c, X86_FEATURE_XMM2)) {
+		/*
+		 * A serializing LFENCE has less overhead than MFENCE, so
+		 * use it for execution serialization.  On families which
+		 * don't have that MSR, LFENCE is already serializing.
+		 * msr_set_bit() uses the safe accessors, too, even if the MSR
+		 * is not present.
+		 */
+		msr_set_bit(MSR_F10H_DECFG,
+			    MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT);
+
 		/* MFENCE stops RDTSC speculation */
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
 	}

commit f2dbad36c55e5d3a91dccbde6e8cae345fe5632f
Author: Rudolf Marek <r.marek@assembler.cz>
Date:   Tue Nov 28 22:01:06 2017 +0100

    x86: Make X86_BUG_FXSAVE_LEAK detectable in CPUID on AMD
    
    [ Note, this is a Git cherry-pick of the following commit:
    
        2b67799bdf25 ("x86: Make X86_BUG_FXSAVE_LEAK detectable in CPUID on AMD")
    
      ... for easier x86 PTI code testing and back-porting. ]
    
    The latest AMD AMD64 Architecture Programmer's Manual
    adds a CPUID feature XSaveErPtr (CPUID_Fn80000008_EBX[2]).
    
    If this feature is set, the FXSAVE, XSAVE, FXSAVEOPT, XSAVEC, XSAVES
    / FXRSTOR, XRSTOR, XRSTORS always save/restore error pointers,
    thus making the X86_BUG_FXSAVE_LEAK workaround obsolete on such CPUs.
    
    Signed-Off-By: Rudolf Marek <r.marek@assembler.cz>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Tested-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Link: https://lkml.kernel.org/r/bdcebe90-62c5-1f05-083c-eba7f08b2540@assembler.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index d58184b7cd44..bcb75dc97d44 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -804,8 +804,11 @@ static void init_amd(struct cpuinfo_x86 *c)
 	case 0x17: init_amd_zn(c); break;
 	}
 
-	/* Enable workaround for FXSAVE leak */
-	if (c->x86 >= 6)
+	/*
+	 * Enable workaround for FXSAVE leak on CPUs
+	 * without a XSaveErPtr feature
+	 */
+	if ((c->x86 >= 6) && (!cpu_has(c, X86_FEATURE_XSAVEERPTR)))
 		set_cpu_bug(c, X86_BUG_FXSAVE_LEAK);
 
 	cpu_detect_cache_sizes(c);

commit e3811a3f74bd1ad773667b78323f396166891f3a
Author: Rudolf Marek <r.marek@assembler.cz>
Date:   Tue Nov 28 22:01:06 2017 +0100

    x86/cpufeatures: Make X86_BUG_FXSAVE_LEAK detectable in CPUID on AMD
    
    The latest AMD AMD64 Architecture Programmer's Manual
    adds a CPUID feature XSaveErPtr (CPUID_Fn80000008_EBX[2]).
    
    If this feature is set, the FXSAVE, XSAVE, FXSAVEOPT, XSAVEC, XSAVES
    / FXRSTOR, XRSTOR, XRSTORS always save/restore error pointers,
    thus making the X86_BUG_FXSAVE_LEAK workaround obsolete on such CPUs.
    
    Signed-off-by: Rudolf Marek <r.marek@assembler.cz>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Tested-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Link: https://lkml.kernel.org/r/bdcebe90-62c5-1f05-083c-eba7f08b2540@assembler.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index d58184b7cd44..bcb75dc97d44 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -804,8 +804,11 @@ static void init_amd(struct cpuinfo_x86 *c)
 	case 0x17: init_amd_zn(c); break;
 	}
 
-	/* Enable workaround for FXSAVE leak */
-	if (c->x86 >= 6)
+	/*
+	 * Enable workaround for FXSAVE leak on CPUs
+	 * without a XSaveErPtr feature
+	 */
+	if ((c->x86 >= 6) && (!cpu_has(c, X86_FEATURE_XSAVEERPTR)))
 		set_cpu_bug(c, X86_BUG_FXSAVE_LEAK);
 
 	cpu_detect_cache_sizes(c);

commit 18c71ce9c8822d48d2b4c50242051535d46082ac
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Dec 4 10:57:23 2017 -0600

    x86/CPU/AMD: Add the Secure Encrypted Virtualization CPU feature
    
    Update the CPU features to include identifying and reporting on the
    Secure Encrypted Virtualization (SEV) feature.  SEV is identified by
    CPUID 0x8000001f, but requires BIOS support to enable it (set bit 23 of
    MSR_K8_SYSCFG and set bit 0 of MSR_K7_HWCR).  Only show the SEV feature
    as available if reported by CPUID and enabled by BIOS.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: kvm@vger.kernel.org
    Cc: x86@kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index d58184b7cd44..c1234aa0550c 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -556,6 +556,51 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 	}
 }
 
+static void early_detect_mem_encrypt(struct cpuinfo_x86 *c)
+{
+	u64 msr;
+
+	/*
+	 * BIOS support is required for SME and SEV.
+	 *   For SME: If BIOS has enabled SME then adjust x86_phys_bits by
+	 *	      the SME physical address space reduction value.
+	 *	      If BIOS has not enabled SME then don't advertise the
+	 *	      SME feature (set in scattered.c).
+	 *   For SEV: If BIOS has not enabled SEV then don't advertise the
+	 *            SEV feature (set in scattered.c).
+	 *
+	 *   In all cases, since support for SME and SEV requires long mode,
+	 *   don't advertise the feature under CONFIG_X86_32.
+	 */
+	if (cpu_has(c, X86_FEATURE_SME) || cpu_has(c, X86_FEATURE_SEV)) {
+		/* Check if memory encryption is enabled */
+		rdmsrl(MSR_K8_SYSCFG, msr);
+		if (!(msr & MSR_K8_SYSCFG_MEM_ENCRYPT))
+			goto clear_all;
+
+		/*
+		 * Always adjust physical address bits. Even though this
+		 * will be a value above 32-bits this is still done for
+		 * CONFIG_X86_32 so that accurate values are reported.
+		 */
+		c->x86_phys_bits -= (cpuid_ebx(0x8000001f) >> 6) & 0x3f;
+
+		if (IS_ENABLED(CONFIG_X86_32))
+			goto clear_all;
+
+		rdmsrl(MSR_K7_HWCR, msr);
+		if (!(msr & MSR_K7_HWCR_SMMLOCK))
+			goto clear_sev;
+
+		return;
+
+clear_all:
+		clear_cpu_cap(c, X86_FEATURE_SME);
+clear_sev:
+		clear_cpu_cap(c, X86_FEATURE_SEV);
+	}
+}
+
 static void early_init_amd(struct cpuinfo_x86 *c)
 {
 	u32 dummy;
@@ -627,26 +672,7 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	if (cpu_has_amd_erratum(c, amd_erratum_400))
 		set_cpu_bug(c, X86_BUG_AMD_E400);
 
-	/*
-	 * BIOS support is required for SME. If BIOS has enabled SME then
-	 * adjust x86_phys_bits by the SME physical address space reduction
-	 * value. If BIOS has not enabled SME then don't advertise the
-	 * feature (set in scattered.c). Also, since the SME support requires
-	 * long mode, don't advertise the feature under CONFIG_X86_32.
-	 */
-	if (cpu_has(c, X86_FEATURE_SME)) {
-		u64 msr;
-
-		/* Check if SME is enabled */
-		rdmsrl(MSR_K8_SYSCFG, msr);
-		if (msr & MSR_K8_SYSCFG_MEM_ENCRYPT) {
-			c->x86_phys_bits -= (cpuid_ebx(0x8000001f) >> 6) & 0x3f;
-			if (IS_ENABLED(CONFIG_X86_32))
-				clear_cpu_cap(c, X86_FEATURE_SME);
-		} else {
-			clear_cpu_cap(c, X86_FEATURE_SME);
-		}
-	}
+	early_detect_mem_encrypt(c);
 }
 
 static void init_amd_k8(struct cpuinfo_x86 *c)

commit f7f3dc00f61261cdc9ccd8b886f21bc4dffd6fd9
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Sep 7 19:08:21 2017 +0200

    x86/cpu/AMD: Fix erratum 1076 (CPB bit)
    
    CPUID Fn8000_0007_EDX[CPB] is wrongly 0 on models up to B1. But they do
    support CPB (AMD's Core Performance Boosting cpufreq CPU feature), so fix that.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sherry Hurwitz <sherry.hurwitz@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170907170821.16021-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 9862e2cd6d93..d58184b7cd44 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -763,6 +763,16 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 	}
 }
 
+static void init_amd_zn(struct cpuinfo_x86 *c)
+{
+	/*
+	 * Fix erratum 1076: CPB feature bit not being set in CPUID. It affects
+	 * all up to and including B1.
+	 */
+	if (c->x86_model <= 1 && c->x86_mask <= 1)
+		set_cpu_cap(c, X86_FEATURE_CPB);
+}
+
 static void init_amd(struct cpuinfo_x86 *c)
 {
 	early_init_amd(c);
@@ -791,6 +801,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 	case 0x10: init_amd_gh(c); break;
 	case 0x12: init_amd_ln(c); break;
 	case 0x15: init_amd_bd(c); break;
+	case 0x17: init_amd_zn(c); break;
 	}
 
 	/* Enable workaround for FXSAVE leak */

commit b1b6f83ac938d176742c85757960dec2cf10e468
Merge: 5f82e71a001d 9e52fc2b50de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 12:21:28 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "PCID support, 5-level paging support, Secure Memory Encryption support
    
      The main changes in this cycle are support for three new, complex
      hardware features of x86 CPUs:
    
       - Add 5-level paging support, which is a new hardware feature on
         upcoming Intel CPUs allowing up to 128 PB of virtual address space
         and 4 PB of physical RAM space - a 512-fold increase over the old
         limits. (Supercomputers of the future forecasting hurricanes on an
         ever warming planet can certainly make good use of more RAM.)
    
         Many of the necessary changes went upstream in previous cycles,
         v4.14 is the first kernel that can enable 5-level paging.
    
         This feature is activated via CONFIG_X86_5LEVEL=y - disabled by
         default.
    
         (By Kirill A. Shutemov)
    
       - Add 'encrypted memory' support, which is a new hardware feature on
         upcoming AMD CPUs ('Secure Memory Encryption', SME) allowing system
         RAM to be encrypted and decrypted (mostly) transparently by the
         CPU, with a little help from the kernel to transition to/from
         encrypted RAM. Such RAM should be more secure against various
         attacks like RAM access via the memory bus and should make the
         radio signature of memory bus traffic harder to intercept (and
         decrypt) as well.
    
         This feature is activated via CONFIG_AMD_MEM_ENCRYPT=y - disabled
         by default.
    
         (By Tom Lendacky)
    
       - Enable PCID optimized TLB flushing on newer Intel CPUs: PCID is a
         hardware feature that attaches an address space tag to TLB entries
         and thus allows to skip TLB flushing in many cases, even if we
         switch mm's.
    
         (By Andy Lutomirski)
    
      All three of these features were in the works for a long time, and
      it's coincidence of the three independent development paths that they
      are all enabled in v4.14 at once"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (65 commits)
      x86/mm: Enable RCU based page table freeing (CONFIG_HAVE_RCU_TABLE_FREE=y)
      x86/mm: Use pr_cont() in dump_pagetable()
      x86/mm: Fix SME encryption stack ptr handling
      kvm/x86: Avoid clearing the C-bit in rsvd_bits()
      x86/CPU: Align CR3 defines
      x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages
      acpi, x86/mm: Remove encryption mask from ACPI page protection type
      x86/mm, kexec: Fix memory corruption with SME on successive kexecs
      x86/mm/pkeys: Fix typo in Documentation/x86/protection-keys.txt
      x86/mm/dump_pagetables: Speed up page tables dump for CONFIG_KASAN=y
      x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID
      x86: Enable 5-level paging support via CONFIG_X86_5LEVEL=y
      x86/mm: Allow userspace have mappings above 47-bit
      x86/mm: Prepare to expose larger address space to userspace
      x86/mpx: Do not allow MPX if we have mappings above 47-bit
      x86/mm: Rename tasksize_32bit/64bit to task_size_32bit/64bit()
      x86/xen: Redefine XEN_ELFNOTE_INIT_P2M using PUD_SIZE * PTRS_PER_PUD
      x86/mm/dump_pagetables: Fix printout of p4d level
      x86/mm/dump_pagetables: Generalize address normalization
      x86/boot: Fix memremap() related build failure
      ...

commit 413d63d71b222108d19703f3fd5cf9108652a730
Merge: d6c8103b0265 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Aug 26 09:19:13 2017 +0200

    Merge branch 'linus' into x86/mm to pick up fixes and to fix conflicts
    
    Conflicts:
            arch/x86/kernel/head64.c
            arch/x86/mm/mmap.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit aac64f7de999d5a7fff55f49434fdd87df919829
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Aug 11 13:19:11 2017 +0200

    x86/cpu/amd: Hide unused legacy_fixup_core_id() function
    
    The newly introduced function is only used when CONFIG_SMP is set:
    
      arch/x86/kernel/cpu/amd.c:305:13: warning: 'legacy_fixup_core_id' defined but not used
    
    This moves the existing #ifdef around the caller so it covers
    legacy_fixup_core_id() as well.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Emanuel Czirai <icanrealizeum@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Yazen Ghannam <Yazen.Ghannam@amd.com>
    Fixes: b89b41d0b841 ("x86/cpu/amd: Limit cpu_core_id fixup to families older than F17h")
    Link: http://lkml.kernel.org/r/20170811111937.2006128-1-arnd@arndb.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 82d571c9923e..e44338dd62dd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -297,6 +297,7 @@ static int nearby_node(int apicid)
 }
 #endif
 
+#ifdef CONFIG_SMP
 /*
  * Fix up cpu_core_id for pre-F17h systems to be in the
  * [0 .. cores_per_node - 1] range. Not really needed but
@@ -319,7 +320,6 @@ static void legacy_fixup_core_id(struct cpuinfo_x86 *c)
  *     Assumption: Number of cores in each internal node is the same.
  * (2) AMD processors supporting compute units
  */
-#ifdef CONFIG_SMP
 static void amd_get_topology(struct cpuinfo_x86 *c)
 {
 	u8 node_id;

commit b89b41d0b8414690ec0030c134b8bde209e6d06c
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Mon Jul 31 10:51:58 2017 +0200

    x86/cpu/amd: Limit cpu_core_id fixup to families older than F17h
    
    Current cpu_core_id fixup causes downcored F17h configurations to be
    incorrect:
    
      NODE: 0
      processor  0 core id : 0
      processor  1 core id : 1
      processor  2 core id : 2
      processor  3 core id : 4
      processor  4 core id : 5
      processor  5 core id : 0
    
      NODE: 1
      processor  6 core id : 2
      processor  7 core id : 3
      processor  8 core id : 4
      processor  9 core id : 0
      processor 10 core id : 1
      processor 11 core id : 2
    
    Code that relies on the cpu_core_id, like match_smt(), for example,
    which builds the thread siblings masks used by the scheduler, is
    mislead.
    
    So, limit the fixup to pre-F17h machines. The new value for cpu_core_id
    for F17h and later will represent the CPUID_Fn8000001E_EBX[CoreId],
    which is guaranteed to be unique for each core within a socket.
    
    This way we have:
    
      NODE: 0
      processor  0 core id : 0
      processor  1 core id : 1
      processor  2 core id : 2
      processor  3 core id : 4
      processor  4 core id : 5
      processor  5 core id : 6
    
      NODE: 1
      processor  6 core id : 8
      processor  7 core id : 9
      processor  8 core id : 10
      processor  9 core id : 12
      processor 10 core id : 13
      processor 11 core id : 14
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    [ Heavily massaged. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <Yazen.Ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20170731085159.9455-2-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 3b9e220621f8..82d571c9923e 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -297,6 +297,22 @@ static int nearby_node(int apicid)
 }
 #endif
 
+/*
+ * Fix up cpu_core_id for pre-F17h systems to be in the
+ * [0 .. cores_per_node - 1] range. Not really needed but
+ * kept so as not to break existing setups.
+ */
+static void legacy_fixup_core_id(struct cpuinfo_x86 *c)
+{
+	u32 cus_per_node;
+
+	if (c->x86 >= 0x17)
+		return;
+
+	cus_per_node = c->x86_max_cores / nodes_per_socket;
+	c->cpu_core_id %= cus_per_node;
+}
+
 /*
  * Fixup core topology information for
  * (1) AMD multi-node processors
@@ -354,15 +370,9 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 	} else
 		return;
 
-	/* fixup multi-node processor information */
 	if (nodes_per_socket > 1) {
-		u32 cus_per_node;
-
 		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
-		cus_per_node = c->x86_max_cores / nodes_per_socket;
-
-		/* core id has to be in the [0 .. cores_per_node - 1] range */
-		c->cpu_core_id %= cus_per_node;
+		legacy_fixup_core_id(c);
 	}
 }
 #endif

commit f655e6e6b992a2fb0d0334db2620607b98df39e7
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:23 2017 -0500

    x86/cpu/AMD: Make the microcode level available earlier in the boot
    
    Move the setting of the cpuinfo_x86.microcode field from amd_init() to
    early_amd_init() so that it is available earlier in the boot process. This
    avoids having to read MSR_AMD64_PATCH_LEVEL directly during early boot.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/7b7525fa12593dac5f4b01fcc25c95f97e93862f.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 4d87950fde30..f22fd4ea8858 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -547,8 +547,12 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 
 static void early_init_amd(struct cpuinfo_x86 *c)
 {
+	u32 dummy;
+
 	early_init_amd_mc(c);
 
+	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
+
 	/*
 	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate
 	 * with P/T states and does not stop in deep C-states
@@ -750,8 +754,6 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 
 static void init_amd(struct cpuinfo_x86 *c)
 {
-	u32 dummy;
-
 	early_init_amd(c);
 
 	/*
@@ -813,8 +815,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 	if (c->x86 > 0x11)
 		set_cpu_cap(c, X86_FEATURE_ARAT);
 
-	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
-
 	/* 3DNow or LM implies PREFETCHW */
 	if (!cpu_has(c, X86_FEATURE_3DNOWPREFETCH))
 		if (cpu_has(c, X86_FEATURE_3DNOW) || cpu_has(c, X86_FEATURE_LM))

commit 9af9b94068fb1ea3206a700fc222075966fbef14
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:02 2017 -0500

    x86/cpu/AMD: Handle SME reduction in physical address size
    
    When System Memory Encryption (SME) is enabled, the physical address
    space is reduced. Adjust the x86_phys_bits value to reflect this
    reduction.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/593c037a3cad85ba92f3d061ffa7462e9ce3531d.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 5ccc7b2e63bb..4d87950fde30 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -613,21 +613,23 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 		set_cpu_bug(c, X86_BUG_AMD_E400);
 
 	/*
-	 * BIOS support is required for SME. If BIOS has not enabled SME
-	 * then don't advertise the feature (set in scattered.c). Also,
-	 * since the SME support requires long mode, don't advertise the
-	 * feature under CONFIG_X86_32.
+	 * BIOS support is required for SME. If BIOS has enabled SME then
+	 * adjust x86_phys_bits by the SME physical address space reduction
+	 * value. If BIOS has not enabled SME then don't advertise the
+	 * feature (set in scattered.c). Also, since the SME support requires
+	 * long mode, don't advertise the feature under CONFIG_X86_32.
 	 */
 	if (cpu_has(c, X86_FEATURE_SME)) {
-		if (IS_ENABLED(CONFIG_X86_32)) {
-			clear_cpu_cap(c, X86_FEATURE_SME);
-		} else {
-			u64 msr;
+		u64 msr;
 
-			/* Check if SME is enabled */
-			rdmsrl(MSR_K8_SYSCFG, msr);
-			if (!(msr & MSR_K8_SYSCFG_MEM_ENCRYPT))
+		/* Check if SME is enabled */
+		rdmsrl(MSR_K8_SYSCFG, msr);
+		if (msr & MSR_K8_SYSCFG_MEM_ENCRYPT) {
+			c->x86_phys_bits -= (cpuid_ebx(0x8000001f) >> 6) & 0x3f;
+			if (IS_ENABLED(CONFIG_X86_32))
 				clear_cpu_cap(c, X86_FEATURE_SME);
+		} else {
+			clear_cpu_cap(c, X86_FEATURE_SME);
 		}
 	}
 }

commit 872cbefd2d9c52bd0b1e2c7942c4369e98a5a5ae
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:01 2017 -0500

    x86/cpu/AMD: Add the Secure Memory Encryption CPU feature
    
    Update the CPU features to include identifying and reporting on the
    Secure Memory Encryption (SME) feature.  SME is identified by CPUID
    0x8000001f, but requires BIOS support to enable it (set bit 23 of
    MSR_K8_SYSCFG).  Only show the SME feature as available if reported by
    CPUID, enabled by BIOS and not configured as CONFIG_X86_32=y.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/85c17ff450721abccddc95e611ae8df3f4d9718b.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bb5abe8f5fd4..5ccc7b2e63bb 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -611,6 +611,25 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	 */
 	if (cpu_has_amd_erratum(c, amd_erratum_400))
 		set_cpu_bug(c, X86_BUG_AMD_E400);
+
+	/*
+	 * BIOS support is required for SME. If BIOS has not enabled SME
+	 * then don't advertise the feature (set in scattered.c). Also,
+	 * since the SME support requires long mode, don't advertise the
+	 * feature under CONFIG_X86_32.
+	 */
+	if (cpu_has(c, X86_FEATURE_SME)) {
+		if (IS_ENABLED(CONFIG_X86_32)) {
+			clear_cpu_cap(c, X86_FEATURE_SME);
+		} else {
+			u64 msr;
+
+			/* Check if SME is enabled */
+			rdmsrl(MSR_K8_SYSCFG, msr);
+			if (!(msr & MSR_K8_SYSCFG_MEM_ENCRYPT))
+				clear_cpu_cap(c, X86_FEATURE_SME);
+		}
+	}
 }
 
 static void init_amd_k8(struct cpuinfo_x86 *c)

commit 5f8a16156aa1b2d0223eaee9dacdfb9bc096f610
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Tue Jul 11 07:44:05 2017 -0400

    x86/cpu: Use indirect call to measure performance in init_amd_k6()
    
    This old piece of code is supposed to measure the performance of indirect
    calls to determine if the processor is buggy or not, however the compiler
    optimizer turns it into a direct call.
    
    Use the OPTIMIZER_HIDE_VAR() macro to thwart the optimization, so that a real
    indirect call is generated.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/alpine.LRH.2.02.1707110737530.8746@file01.intranet.prod.int.rdu2.redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bb5abe8f5fd4..3b9e220621f8 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -134,6 +134,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 
 		n = K6_BUG_LOOP;
 		f_vide = vide;
+		OPTIMIZER_HIDE_VAR(f_vide);
 		d = rdtsc();
 		while (n--)
 			f_vide();

commit 5836e422e5ca6d36d552f700916436e01cbd2066
Merge: dc2a24816637 69861e0a52f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 12 10:09:14 2017 -0700

    Merge tag 'for-linus-4.12b-rc0c-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen fixes from Juergen Gross:
     "This contains two fixes for booting under Xen introduced during this
      merge window and two fixes for older problems, where one is just much
      more probable due to another merge window change"
    
    * tag 'for-linus-4.12b-rc0c-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen: adjust early dom0 p2m handling to xen hypervisor behavior
      x86/amd: don't set X86_BUG_SYSRET_SS_ATTRS when running under Xen
      xen/x86: Do not call xen_init_time_ops() until shared_info is initialized
      x86/xen: fix xsave capability setting

commit def9331a12977770cc6132d79f8e6565871e8e38
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Apr 27 07:01:20 2017 +0200

    x86/amd: don't set X86_BUG_SYSRET_SS_ATTRS when running under Xen
    
    When running as Xen pv guest X86_BUG_SYSRET_SS_ATTRS must not be set
    on AMD cpus.
    
    This bug/feature bit is kind of special as it will be used very early
    when switching threads. Setting the bit and clearing it a little bit
    later leaves a critical window where things can go wrong. This time
    window has enlarged a little bit by using setup_clear_cpu_cap() instead
    of the hypervisor's set_cpu_features callback. It seems this larger
    window now makes it rather easy to hit the problem.
    
    The proper solution is to never set the bit in case of Xen.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c36140d788fe..b6da6e75e3a8 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -799,8 +799,9 @@ static void init_amd(struct cpuinfo_x86 *c)
 		if (cpu_has(c, X86_FEATURE_3DNOW) || cpu_has(c, X86_FEATURE_LM))
 			set_cpu_cap(c, X86_FEATURE_3DNOWPREFETCH);
 
-	/* AMD CPUs don't reset SS attributes on SYSRET */
-	set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
+	/* AMD CPUs don't reset SS attributes on SYSRET, Xen does. */
+	if (!cpu_has(c, X86_FEATURE_XENPV))
+		set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
 }
 
 #ifdef CONFIG_X86_32

commit d11636511ed97ceda66a08ecff99f100e1107b76
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon May 8 15:58:11 2017 -0700

    x86: use set_memory.h header
    
    set_memory_* functions have moved to set_memory.h.  Switch to this
    explicitly.
    
    Link: http://lkml.kernel.org/r/1488920133-27229-6-git-send-email-labbott@redhat.com
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c36140d788fe..ee8f11800295 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -16,7 +16,7 @@
 
 #ifdef CONFIG_X86_64
 # include <asm/mmconfig.h>
-# include <asm/cacheflush.h>
+# include <asm/set_memory.h>
 #endif
 
 #include "cpu.h"

commit 609b07b72d3caaa8eed3a238886467946b78fa5e
Merge: c3abcabe813b f94c8d116997
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 7 14:42:34 2017 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "A fix for KVM's scheduler clock which (erroneously) was always marked
      unstable, a fix for RT/DL load balancing, plus latency fixes"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
      sched/core: Fix pick_next_task() for RT,DL
      sched/fair: Make select_idle_cpu() more aggressive

commit f94c8d116997597fc00f0812b0ab9256e7b0c58f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 1 15:53:38 2017 +0100

    sched/clock, x86/tsc: Rework the x86 'unstable' sched_clock() interface
    
    Wanpeng Li reported that since the following commit:
    
      acb04058de49 ("sched/clock: Fix hotplug crash")
    
    ... KVM always runs with unstable sched-clock even though KVM's
    kvm_clock _is_ stable.
    
    The problem is that we've tied clear_sched_clock_stable() to the TSC
    state, and overlooked that sched_clock() is a paravirt function.
    
    Solve this by doing two things:
    
     - tie the sched_clock() stable state more clearly to the TSC stable
       state for the normal (!paravirt) case.
    
     - only call clear_sched_clock_stable() when we mark TSC unstable
       when we use native_sched_clock().
    
    The first means we can actually run with stable sched_clock in more
    situations then before, which is good. And since commit:
    
      12907fbb1a69 ("sched/clock, clocksource: Add optional cs::mark_unstable() method")
    
    ... this should be reliable. Since any detection of TSC fail now results
    in marking the TSC unstable.
    
    Reported-by: Wanpeng Li <kernellwp@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Fixes: acb04058de49 ("sched/clock: Fix hotplug crash")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 4e95b2e0d95f..30d924ae5c34 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -555,10 +555,6 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
-		if (check_tsc_unstable())
-			clear_sched_clock_stable();
-	} else {
-		clear_sched_clock_stable();
 	}
 
 	/* Bit 12 of 8000_0007 edx is accumulated power mechanism. */

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 4e95b2e0d95f..35a5d5dca2fa 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -5,6 +5,7 @@
 
 #include <linux/io.h>
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 #include <linux/random.h>
 #include <asm/processor.h>
 #include <asm/apic.h>

commit 828cad8ea05d194d8a9452e0793261c2024c23a2
Merge: 60c906bab124 bb3bac2ca9a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 12:52:55 2017 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this (fairly busy) cycle were:
    
       - There was a class of scheduler bugs related to forgetting to update
         the rq-clock timestamp which can cause weird and hard to debug
         problems, so there's a new debug facility for this: which uncovered
         a whole lot of bugs which convinced us that we want to keep the
         debug facility.
    
         (Peter Zijlstra, Matt Fleming)
    
       - Various cputime related updates: eliminate cputime and use u64
         nanoseconds directly, simplify and improve the arch interfaces,
         implement delayed accounting more widely, etc. - (Frederic
         Weisbecker)
    
       - Move code around for better structure plus cleanups (Ingo Molnar)
    
       - Move IO schedule accounting deeper into the scheduler plus related
         changes to improve the situation (Tejun Heo)
    
       - ... plus a round of sched/rt and sched/deadline fixes, plus other
         fixes, updats and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (85 commits)
      sched/core: Remove unlikely() annotation from sched_move_task()
      sched/autogroup: Rename auto_group.[ch] to autogroup.[ch]
      sched/topology: Split out scheduler topology code from core.c into topology.c
      sched/core: Remove unnecessary #include headers
      sched/rq_clock: Consolidate the ordering of the rq_clock methods
      delayacct: Include <uapi/linux/taskstats.h>
      sched/core: Clean up comments
      sched/rt: Show the 'sched_rr_timeslice' SCHED_RR timeslice tuning knob in milliseconds
      sched/clock: Add dummy clear_sched_clock_stable() stub function
      sched/cputime: Remove generic asm headers
      sched/cputime: Remove unused nsec_to_cputime()
      s390, sched/cputime: Remove unused cputime definitions
      powerpc, sched/cputime: Remove unused cputime definitions
      s390, sched/cputime: Make arch_cpu_idle_time() to return nsecs
      ia64, sched/cputime: Remove unused cputime definitions
      ia64: Convert vtime to use nsec units directly
      ia64, sched/cputime: Move the nsecs based cputime headers to the last arch using it
      sched/cputime: Remove jiffies based cputime
      sched/cputime, vtime: Return nsecs instead of cputime_t to account
      sched/cputime: Complete nsec conversion of tick based accounting
      ...

commit 08b259631b5a1d912af4832847b5642f377d9101
Author: Yazen Ghannam <Yazen.Ghannam@amd.com>
Date:   Sun Feb 5 11:50:22 2017 +0100

    x86/CPU/AMD: Fix Zen SMT topology
    
    After:
    
      a33d331761bc ("x86/CPU/AMD: Fix Bulldozer topology")
    
    our  SMT scheduling topology for Fam17h systems is broken, because
    the ThreadId is included in the ApicId when SMT is enabled.
    
    So, without further decoding cpu_core_id is unique for each thread
    rather than the same for threads on the same core. This didn't affect
    systems with SMT disabled. Make cpu_core_id be what it is defined to be.
    
    Signed-off-by: Yazen Ghannam <Yazen.Ghannam@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # 4.9
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170205105022.8705-2-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 20dc44d1e6be..2b4cf04239b6 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -319,6 +319,13 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		if (c->x86 == 0x15)
 			c->cu_id = ebx & 0xff;
 
+		if (c->x86 >= 0x17) {
+			c->cpu_core_id = ebx & 0xff;
+
+			if (smp_num_siblings > 1)
+				c->x86_max_cores /= smp_num_siblings;
+		}
+
 		/*
 		 * We may have multiple LLCs if L3 caches exist, so check if we
 		 * have an L3 cache by looking at the L3 cache CPUID leaf.

commit 79a8b9aa388b0620cc1d525d7c0f0d9a8a85e08e
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Feb 5 11:50:21 2017 +0100

    x86/CPU/AMD: Bring back Compute Unit ID
    
    Commit:
    
      a33d331761bc ("x86/CPU/AMD: Fix Bulldozer topology")
    
    restored the initial approach we had with the Fam15h topology of
    enumerating CU (Compute Unit) threads as cores. And this is still
    correct - they're beefier than HT threads but still have some
    shared functionality.
    
    Our current approach has a problem with the Mad Max Steam game, for
    example. Yves Dionne reported a certain "choppiness" while playing on
    v4.9.5.
    
    That problem stems most likely from the fact that the CU threads share
    resources within one CU and when we schedule to a thread of a different
    compute unit, this incurs latency due to migrating the working set to a
    different CU through the caches.
    
    When the thread siblings mask mirrors that aspect of the CUs and
    threads, the scheduler pays attention to it and tries to schedule within
    one CU first. Which takes care of the latency, of course.
    
    Reported-by: Yves Dionne <yves.dionne@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # 4.9
    Cc: Brice Goglin <Brice.Goglin@inria.fr>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20170205105022.8705-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1d3167269a67..20dc44d1e6be 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -309,8 +309,15 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 
 	/* get information required for multi-node processors */
 	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
+		u32 eax, ebx, ecx, edx;
 
-		node_id = cpuid_ecx(0x8000001e) & 7;
+		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
+
+		node_id  = ecx & 0xff;
+		smp_num_siblings = ((ebx >> 8) & 0xff) + 1;
+
+		if (c->x86 == 0x15)
+			c->cu_id = ebx & 0xff;
 
 		/*
 		 * We may have multiple LLCs if L3 caches exist, so check if we

commit ed5c8c854f2b990dfa4d85c4995d115768a05d3c
Merge: 619bd4a71874 a2ca3d617944
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 09:12:25 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes and refresh the branch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit acb04058de49458010c44bb35b849d45113fd668
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 19 14:36:33 2017 +0100

    sched/clock: Fix hotplug crash
    
    Mike reported that he could trigger the WARN_ON_ONCE() in
    set_sched_clock_stable() using hotplug.
    
    This exposed a fundamental problem with the interface, we should never
    mark the TSC stable if we ever find it to be unstable. Therefore
    set_sched_clock_stable() is a broken interface.
    
    The reason it existed is that not having it is a pain, it means all
    relevant architecture code needs to call clear_sched_clock_stable()
    where appropriate.
    
    Of the three architectures that select HAVE_UNSTABLE_SCHED_CLOCK ia64
    and parisc are trivial in that they never called
    set_sched_clock_stable(), so add an unconditional call to
    clear_sched_clock_stable() to them.
    
    For x86 the story is a lot more involved, and what this patch tries to
    do is ensure we preserve the status quo. So even is Cyrix or Transmeta
    have usable TSC they never called set_sched_clock_stable() so they now
    get an explicit mark unstable.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 9881b024b7d7 ("sched/clock: Delay switching sched_clock to stable")
    Link: http://lkml.kernel.org/r/20170119133633.GB6536@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 71cae73a5076..1bb253a6ee4d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -548,8 +548,10 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
-		if (!check_tsc_unstable())
-			set_sched_clock_stable();
+		if (check_tsc_unstable())
+			clear_sched_clock_stable();
+	} else {
+		clear_sched_clock_stable();
 	}
 
 	/* Bit 12 of 8000_0007 edx is accumulated power mechanism. */

commit a33d331761bc5dd330499ca5ceceb67f0640a8e6
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jan 5 10:26:38 2017 +0100

    x86/CPU/AMD: Fix Bulldozer topology
    
    The following commit:
    
      8196dab4fc15 ("x86/cpu: Get rid of compute_unit_id")
    
    ... broke the initial strategy for Bulldozer-based cores' topology,
    where we consider each thread of a compute unit a standalone core
    and not a HT or SMT thread.
    
    Revert to the firmware-supplied core_id numbering and do not make
    them thread siblings as we don't consider them for such even if they
    technically are, more or less.
    
    Reported-and-tested-by: Brice Goglin <Brice.Goglin@inria.fr>
    Tested-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # v4.6+
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 8196dab4fc15 ("x86/cpu: Get rid of compute_unit_id")
    Link: http://lkml.kernel.org/r/20170105092638.5247-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 71cae73a5076..1d3167269a67 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -309,15 +309,8 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 
 	/* get information required for multi-node processors */
 	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
-		u32 eax, ebx, ecx, edx;
 
-		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
-		node_id = ecx & 7;
-
-		/* get compute unit information */
-		smp_num_siblings = ((ebx >> 8) & 3) + 1;
-		c->x86_max_cores /= smp_num_siblings;
-		c->cpu_core_id = ebx & 0xff;
+		node_id = cpuid_ecx(0x8000001e) & 7;
 
 		/*
 		 * We may have multiple LLCs if L3 caches exist, so check if we

commit 212f30008a284a9312d95dad6cc237ff81173d73
Merge: 6f3be0f04354 34bc3560c657
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:55:04 2016 -0800

    Merge branch 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 idle updates from Ingo Molnar:
     "There were two bigger changes in this development cycle:
    
       - remove idle notifiers:
    
           32 files changed, 74 insertions(+), 803 deletions(-)
    
         These notifiers were of questionable value and the main usecase,
         the i7300 driver, was essentially unmaintained and can be removed,
         plus modern power management concepts don't need the callback - so
         use this golden opportunity and get rid of this opaque and fragile
         callback from a latency sensitive code path.
    
         (Len Brown, Thomas Gleixner)
    
       - improve the AMD Erratum 400 workaround that used high overhead MSR
         polling in the idle loop (Borisla Petkov, Thomas Gleixner)"
    
    * 'x86-idle-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Remove empty idle.h header
      x86/amd: Simplify AMD E400 aware idle routine
      x86/amd: Check for the C1E bug post ACPI subsystem init
      x86/bugs: Separate AMD E400 erratum and C1E bug
      x86/cpufeature: Provide helper to set bugs bits
      x86/idle: Remove enter_idle(), exit_idle()
      x86: Remove x86_test_and_clear_bit_percpu()
      x86/idle: Remove is_idle flag
      x86/idle: Remove idle_notifier
      i7300_idle: Remove this driver

commit 3344ed30791af66dbbad5f375008f3d1863b6c99
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 9 19:29:09 2016 +0100

    x86/bugs: Separate AMD E400 erratum and C1E bug
    
    The workaround for the AMD Erratum E400 (Local APIC timer stops in C1E
    state) is a two step process:
    
     - Selection of the E400 aware idle routine
    
     - Detection whether the platform is affected
    
    The idle routine selection happens for possibly affected CPUs depending on
    family/model/stepping information. These range of CPUs is not necessarily
    affected as the decision whether to enable the C1E feature is made by the
    firmware. Unfortunately there is no way to query this at early boot.
    
    The current implementation polls a MSR in the E400 aware idle routine to
    detect whether the CPU is affected. This is inefficient on non affected
    CPUs because every idle entry has to do the MSR read.
    
    There is a better way to detect this before going idle for the first time
    which requires to seperate the bug flags:
    
      X86_BUG_AMD_E400      - Selects the E400 aware idle routine and
                              enables the detection
    
      X86_BUG_AMD_APIC_C1E  - Set when the platform is affected by E400
    
    Replace the current X86_BUG_AMD_APIC_C1E usage by the new X86_BUG_AMD_E400
    bug bit to select the idle routine which currently does an unconditional
    detection poll. X86_BUG_AMD_APIC_C1E is going to be used in later patches
    to remove the MSR polling and simplify the handling of this misfeature.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/20161209182912.2726-3-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b81fe2d63e15..ae26c282f5d3 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -20,6 +20,10 @@
 
 #include "cpu.h"
 
+static const int amd_erratum_383[];
+static const int amd_erratum_400[];
+static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum);
+
 /*
  * nodes_per_socket: Stores the number of nodes per socket.
  * Refer to Fam15h Models 00-0fh BKDG - CPUID Fn8000_001E_ECX
@@ -589,11 +593,16 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	/* F16h erratum 793, CVE-2013-6885 */
 	if (c->x86 == 0x16 && c->x86_model <= 0xf)
 		msr_set_bit(MSR_AMD64_LS_CFG, 15);
-}
 
-static const int amd_erratum_383[];
-static const int amd_erratum_400[];
-static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum);
+	/*
+	 * Check whether the machine is affected by erratum 400. This is
+	 * used to select the proper idle routine and to enable the check
+	 * whether the machine is affected in arch_post_acpi_init(), which
+	 * sets the X86_BUG_AMD_APIC_C1E bug depending on the MSR check.
+	 */
+	if (cpu_has_amd_erratum(c, amd_erratum_400))
+		set_cpu_bug(c, X86_BUG_AMD_E400);
+}
 
 static void init_amd_k8(struct cpuinfo_x86 *c)
 {
@@ -774,9 +783,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 	if (c->x86 > 0x11)
 		set_cpu_cap(c, X86_FEATURE_ARAT);
 
-	if (cpu_has_amd_erratum(c, amd_erratum_400))
-		set_cpu_bug(c, X86_BUG_AMD_APIC_C1E);
-
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
 
 	/* 3DNow or LM implies PREFETCHW */

commit b6a50cddbcbda7105355898ead18f1a647c22520
Author: Yazen Ghannam <Yazen.Ghannam@amd.com>
Date:   Tue Nov 8 16:30:54 2016 +0100

    x86/cpu/AMD: Clean up cpu_llc_id assignment per topology feature
    
    These changes do not affect current hw - just a cleanup:
    
    Currently, we assume that a system has a single Last Level Cache (LLC)
    per node, and that the cpu_llc_id is thus equal to the node_id. This no
    longer applies since Fam17h can have multiple last level caches within a
    node.
    
    So group the cpu_llc_id assignment by topology feature and family in
    order to make the computation of cpu_llc_id on the different families
    more clear.
    
    Here is how the LLC ID is being computed on the different families:
    
    The NODEID_MSR feature only applies to Fam10h in which case the LLC is
    at the node level.
    
    The TOPOEXT feature is used on families 15h, 16h and 17h. So far we only
    see multiple last level caches if L3 caches are available. Otherwise,
    the cpu_llc_id will default to be the phys_proc_id.
    
    We have L3 caches only on families 15h and 17h:
    
     - on Fam15h, the LLC is at the node level.
    
     - on Fam17h, the LLC is at the core complex level and can be found by
       right shifting the APIC ID. Also, keep the family checks explicit so that
       new families will fall back to the default, which will be node_id for
       TOPOEXT systems.
    
    Single node systems in families 10h and 15h will have a Node ID of 0
    which will be the same as the phys_proc_id, so we don't need to check
    for multiple nodes before using the node_id.
    
    Tested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Yazen Ghannam <Yazen.Ghannam@amd.com>
    [ Rewrote the commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Aravind Gopalakrishnan <aravindksg.lkml@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20161108153054.bs3sajbyevq6a6uu@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1e81a37c034e..4daad1e39352 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -314,11 +314,30 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		smp_num_siblings = ((ebx >> 8) & 3) + 1;
 		c->x86_max_cores /= smp_num_siblings;
 		c->cpu_core_id = ebx & 0xff;
+
+		/*
+		 * We may have multiple LLCs if L3 caches exist, so check if we
+		 * have an L3 cache by looking at the L3 cache CPUID leaf.
+		 */
+		if (cpuid_edx(0x80000006)) {
+			if (c->x86 == 0x17) {
+				/*
+				 * LLC is at the core complex level.
+				 * Core complex id is ApicId[3].
+				 */
+				per_cpu(cpu_llc_id, cpu) = c->apicid >> 3;
+			} else {
+				/* LLC is at the node level. */
+				per_cpu(cpu_llc_id, cpu) = node_id;
+			}
+		}
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
 		u64 value;
 
 		rdmsrl(MSR_FAM10H_NODE_ID, value);
 		node_id = value & 7;
+
+		per_cpu(cpu_llc_id, cpu) = node_id;
 	} else
 		return;
 
@@ -329,9 +348,6 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
 		cus_per_node = c->x86_max_cores / nodes_per_socket;
 
-		/* store NodeID, use llc_shared_map to store sibling info */
-		per_cpu(cpu_llc_id, cpu) = node_id;
-
 		/* core id has to be in the [0 .. cores_per_node - 1] range */
 		c->cpu_core_id %= cus_per_node;
 	}
@@ -356,15 +372,6 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 	/* use socket ID also for last level cache */
 	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
 	amd_get_topology(c);
-
-	/*
-	 * Fix percpu cpu_llc_id here as LLC topology is different
-	 * for Fam17h systems.
-	 */
-	 if (c->x86 != 0x17 || !cpuid_edx(0x80000006))
-		return;
-
-	per_cpu(cpu_llc_id, cpu) = c->apicid >> 3;
 #endif
 }
 

commit b0b6e86846093c5f8820386bc01515f857dd8faa
Author: Yazen Ghannam <Yazen.Ghannam@amd.com>
Date:   Tue Nov 8 09:35:06 2016 +0100

    x86/cpu/AMD: Fix cpu_llc_id for AMD Fam17h systems
    
    cpu_llc_id (Last Level Cache ID) derivation on AMD Fam17h has an
    underflow bug when extracting the socket_id value. It starts from 0
    so subtracting 1 from it will result in an invalid value. This breaks
    scheduling topology later on since the cpu_llc_id will be incorrect.
    
    For example, the the cpu_llc_id of the *other* CPU in the loops in
    set_cpu_sibling_map() underflows and we're generating the funniest
    thread_siblings masks and then when I run 8 threads of nbench, they get
    spread around the LLC domains in a very strange pattern which doesn't
    give you the normal scheduling spread one would expect for performance.
    
    Other things like EDAC use cpu_llc_id so they will be b0rked too.
    
    So, the APIC ID is preset in APICx020 for bits 3 and above: they contain
    the core complex, node and socket IDs.
    
    The LLC is at the core complex level so we can find a unique cpu_llc_id
    by right shifting the APICID by 3 because then the least significant bit
    will be the Core Complex ID.
    
    Tested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Yazen Ghannam <Yazen.Ghannam@amd.com>
    [ Cleaned up and extended the commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <stable@vger.kernel.org> # v4.4..
    Cc: Aravind Gopalakrishnan <aravindksg.lkml@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: 3849e91f571d ("x86/AMD: Fix last level cache topology for AMD Fam17h systems")
    Link: http://lkml.kernel.org/r/20161108083506.rvqb5h4chrcptj7d@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b81fe2d63e15..1e81a37c034e 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -347,7 +347,6 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 #ifdef CONFIG_SMP
 	unsigned bits;
 	int cpu = smp_processor_id();
-	unsigned int socket_id, core_complex_id;
 
 	bits = c->x86_coreid_bits;
 	/* Low order bits define the core id (index of core in socket) */
@@ -365,10 +364,7 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 	 if (c->x86 != 0x17 || !cpuid_edx(0x80000006))
 		return;
 
-	socket_id	= (c->apicid >> bits) - 1;
-	core_complex_id	= (c->apicid & ((1 << bits) - 1)) >> 3;
-
-	per_cpu(cpu_llc_id, cpu) = (socket_id << 3) | core_complex_id;
+	per_cpu(cpu_llc_id, cpu) = c->apicid >> 3;
 #endif
 }
 

commit d1992996753132e2dafe955cccb2fb0714d3cfc4
Author: Emanuel Czirai <icanrealizeum@gmail.com>
Date:   Fri Sep 2 07:35:50 2016 +0200

    x86/AMD: Apply erratum 665 on machines without a BIOS fix
    
    AMD F12h machines have an erratum which can cause DIV/IDIV to behave
    unpredictably. The workaround is to set MSRC001_1029[31] but sometimes
    there is no BIOS update containing that workaround so let's do it
    ourselves unconditionally. It is simple enough.
    
    [ Borislav: Wrote commit message. ]
    
    Signed-off-by: Emanuel Czirai <icanrealizeum@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Yaowu Xu <yaowu@google.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160902053550.18097-1-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f5c69d8974e1..b81fe2d63e15 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -669,6 +669,17 @@ static void init_amd_gh(struct cpuinfo_x86 *c)
 		set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);
 }
 
+#define MSR_AMD64_DE_CFG	0xC0011029
+
+static void init_amd_ln(struct cpuinfo_x86 *c)
+{
+	/*
+	 * Apply erratum 665 fix unconditionally so machines without a BIOS
+	 * fix work.
+	 */
+	msr_set_bit(MSR_AMD64_DE_CFG, 31);
+}
+
 static void init_amd_bd(struct cpuinfo_x86 *c)
 {
 	u64 value;
@@ -726,6 +737,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 	case 6:	   init_amd_k7(c); break;
 	case 0xf:  init_amd_k8(c); break;
 	case 0x10: init_amd_gh(c); break;
+	case 0x12: init_amd_ln(c); break;
 	case 0x15: init_amd_bd(c); break;
 	}
 

commit 96685a55a82c383cbba7ef1d4a636acf708cf17f
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jun 1 12:04:28 2016 +0200

    x86/cpu/AMD: Extend X86_FEATURE_TOPOEXT workaround to newer models
    
    We need to reenable the topology extensions CPUID leafs on newer models
    too, if BIOS has disabled them, as we rely on them to get proper compute
    unit topology.
    
    Make the printk a once thing, while at it.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rui Huang <ray.huang@amd.com>
    Cc: Sherry Hurwitz <sherry.hurwitz@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-hwmon@vger.kernel.org
    Link: http://lkml.kernel.org/r/1464775468-23355-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c343a54bed39..f5c69d8974e1 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -674,14 +674,14 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 	u64 value;
 
 	/* re-enable TopologyExtensions if switched off by BIOS */
-	if ((c->x86_model >= 0x10) && (c->x86_model <= 0x1f) &&
+	if ((c->x86_model >= 0x10) && (c->x86_model <= 0x6f) &&
 	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
 
 		if (msr_set_bit(0xc0011005, 54) > 0) {
 			rdmsrl(0xc0011005, value);
 			if (value & BIT_64(54)) {
 				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
-				pr_info(FW_INFO "CPU: Re-enabling disabled Topology Extensions Support.\n");
+				pr_info_once(FW_INFO "CPU: Re-enabling disabled Topology Extensions Support.\n");
 			}
 		}
 	}

commit 425d8c2fc5e6dddbad083502bb77c7beae545620
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Apr 5 08:29:51 2016 +0200

    x86/cpu: Simplify extended APIC ID detection on AMD
    
    Both if-branches are under if (boot_cpu_has(X86_FEATURE_APIC)), unify
    them.
    
    Also, simplify the test for bits:
    
    - 17 ("ApicExtBrdCst: APIC extended broadcast enable") and
    - 18 ("ApicExtId: APIC extended ID enable.")
    
    in "D18F0x68 Link Transaction Control."
    
    No functionality change.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1459837795-2588-3-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 54f7b44dcf01..c343a54bed39 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -565,14 +565,17 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	 * can safely set X86_FEATURE_EXTD_APICID unconditionally for families
 	 * after 16h.
 	 */
-	if (boot_cpu_has(X86_FEATURE_APIC) && c->x86 > 0x16) {
-		set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
-	} else if (boot_cpu_has(X86_FEATURE_APIC) && c->x86 >= 0xf) {
-		/* check CPU config space for extended APIC ID */
-		unsigned int val;
-		val = read_pci_config(0, 24, 0, 0x68);
-		if ((val & ((1 << 17) | (1 << 18))) == ((1 << 17) | (1 << 18)))
+	if (boot_cpu_has(X86_FEATURE_APIC)) {
+		if (c->x86 > 0x16)
 			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
+		else if (c->x86 >= 0xf) {
+			/* check CPU config space for extended APIC ID */
+			unsigned int val;
+
+			val = read_pci_config(0, 24, 0, 0x68);
+			if ((val >> 17 & 0x3) == 0x3)
+				set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
+		}
 	}
 #endif
 

commit 93984fbd4e33cc861d5b49caed02a02cbfb01340
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 4 22:25:00 2016 +0200

    x86/cpufeature: Replace cpu_has_apic with boot_cpu_has() usage
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: linux-pm@vger.kernel.org
    Cc: oprofile-list@lists.sf.net
    Link: http://lkml.kernel.org/r/1459801503-15600-8-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 19d7dcfc8b3e..54f7b44dcf01 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -565,9 +565,9 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	 * can safely set X86_FEATURE_EXTD_APICID unconditionally for families
 	 * after 16h.
 	 */
-	if (cpu_has_apic && c->x86 > 0x16) {
+	if (boot_cpu_has(X86_FEATURE_APIC) && c->x86 > 0x16) {
 		set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
-	} else if (cpu_has_apic && c->x86 >= 0xf) {
+	} else if (boot_cpu_has(X86_FEATURE_APIC) && c->x86 >= 0xf) {
 		/* check CPU config space for extended APIC ID */
 		unsigned int val;
 		val = read_pci_config(0, 24, 0, 0x68);

commit 95a8e746f82bdda5d8a443b6557c930782d65b86
Merge: d8d1c3513948 a3125494cff0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 13 11:36:44 2016 +0200

    Merge branch 'x86/urgent' into x86/asm to pick up dependent fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit cb44d0cfc2969999a4d9e20e4fd8749fec6c5498
Merge: 482dd2ef1244 d7847a7017b2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 13 11:15:39 2016 +0200

    Merge branch 'x86/cpu' into x86/asm, to merge more patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 96e5d28ae7a5250f3deb2434f1895c9daf48b1bd
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Apr 7 17:31:49 2016 -0700

    x86/cpu: Add Erratum 88 detection on AMD
    
    Erratum 88 affects old AMD K8s, where a SWAPGS fails to cause an input
    dependency on GS. Therefore, we need to MFENCE before it.
    
    But that MFENCE is expensive and unnecessary on the remaining x86 CPUs
    out there so patch it out on the CPUs which don't require it.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@kernel.org
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rudolf Marek <r.marek@assembler.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/aec6b2df1bfc56101d4e9e2e5d5d570bf41663c6.1460075211.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 6e47e3a916f1..b7cc9efe08b5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -632,6 +632,7 @@ static void init_amd_k8(struct cpuinfo_x86 *c)
 	 */
 	msr_set_bit(MSR_K7_HWCR, 6);
 #endif
+	set_cpu_bug(c, X86_BUG_SWAPGS_FENCE);
 }
 
 static void init_amd_gh(struct cpuinfo_x86 *c)

commit 054efb6467f84490bdf92afab6d9dbd5102e620a
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Mar 29 17:42:00 2016 +0200

    x86/cpufeature: Remove cpu_has_xmm2
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-crypto@vger.kernel.org
    Link: http://lkml.kernel.org/r/1459266123-21878-8-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 6e47e3a916f1..ea8f88a2a688 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -750,7 +750,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 	if (c->x86 >= 0xf)
 		set_cpu_cap(c, X86_FEATURE_K8);
 
-	if (cpu_has_xmm2) {
+	if (cpu_has(c, X86_FEATURE_XMM2)) {
 		/* MFENCE stops RDTSC speculation */
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
 	}

commit 8196dab4fc159943df6baaac04973bb1accb7100
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 25 15:52:36 2016 +0100

    x86/cpu: Get rid of compute_unit_id
    
    It is cpu_core_id anyway.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1458917557-8757-3-git-send-email-bp@alien8.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 4d0087f94ee5..7b76eb67a9b3 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -300,7 +300,6 @@ static int nearby_node(int apicid)
 #ifdef CONFIG_SMP
 static void amd_get_topology(struct cpuinfo_x86 *c)
 {
-	u32 cores_per_cu = 1;
 	u8 node_id;
 	int cpu = smp_processor_id();
 
@@ -312,9 +311,9 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		node_id = ecx & 7;
 
 		/* get compute unit information */
-		cores_per_cu = smp_num_siblings = ((ebx >> 8) & 3) + 1;
+		smp_num_siblings = ((ebx >> 8) & 3) + 1;
 		c->x86_max_cores /= smp_num_siblings;
-		c->compute_unit_id = ebx & 0xff;
+		c->cpu_core_id = ebx & 0xff;
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
 		u64 value;
 
@@ -325,19 +324,16 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 
 	/* fixup multi-node processor information */
 	if (nodes_per_socket > 1) {
-		u32 cores_per_node;
 		u32 cus_per_node;
 
 		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
 		cus_per_node = c->x86_max_cores / nodes_per_socket;
-		cores_per_node = cus_per_node * cores_per_cu;
 
 		/* store NodeID, use llc_shared_map to store sibling info */
 		per_cpu(cpu_llc_id, cpu) = node_id;
 
 		/* core id has to be in the [0 .. cores_per_node - 1] range */
-		c->cpu_core_id %= cores_per_node;
-		c->compute_unit_id %= cus_per_node;
+		c->cpu_core_id %= cus_per_node;
 	}
 }
 #endif

commit ee6825c80e870fff1a370c718ec77022ade0889b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 25 15:52:34 2016 +0100

    x86/topology: Fix AMD core count
    
    It turns out AMD gets x86_max_cores wrong when there are compute
    units.
    
    The issue is that Linux assumes:
    
            nr_logical_cpus = nr_cores * nr_siblings
    
    But AMD reports its CU unit as 2 cores, but then sets num_smp_siblings
    to 2 as well.
    
    Boris: fixup ras/mce_amd_inj.c too, to compute the Node Base Core
    properly, according to the new nomenclature.
    
    Fixes: 1f12e32f4cd5 ("x86/topology: Create logical package id")
    Reported-by: Xiong Zhou <jencce.kernel@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andreas Herrmann <aherrmann@suse.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: http://lkml.kernel.org/r/20160317095220.GO6344@twins.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 6e47e3a916f1..4d0087f94ee5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -312,9 +312,9 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		node_id = ecx & 7;
 
 		/* get compute unit information */
-		smp_num_siblings = ((ebx >> 8) & 3) + 1;
+		cores_per_cu = smp_num_siblings = ((ebx >> 8) & 3) + 1;
+		c->x86_max_cores /= smp_num_siblings;
 		c->compute_unit_id = ebx & 0xff;
-		cores_per_cu += ((ebx >> 8) & 3);
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
 		u64 value;
 
@@ -329,8 +329,8 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		u32 cus_per_node;
 
 		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
-		cores_per_node = c->x86_max_cores / nodes_per_socket;
-		cus_per_node = cores_per_node / cores_per_cu;
+		cus_per_node = c->x86_max_cores / nodes_per_socket;
+		cores_per_node = cus_per_node * cores_per_cu;
 
 		/* store NodeID, use llc_shared_map to store sibling info */
 		per_cpu(cpu_llc_id, cpu) = node_id;

commit 3fa2fe2ce09c5a16be69c5319eb3347271a99735
Merge: d88f48e12821 05f5ece76a88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 10:02:14 2016 -0700

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Ingo Molnar:
     "This tree contains various perf fixes on the kernel side, plus three
      hw/event-enablement late additions:
    
       - Intel Memory Bandwidth Monitoring events and handling
       - the AMD Accumulated Power Mechanism reporting facility
       - more IOMMU events
    
      ... and a final round of perf tooling updates/fixes"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      perf llvm: Use strerror_r instead of the thread unsafe strerror one
      perf llvm: Use realpath to canonicalize paths
      perf tools: Unexport some methods unused outside strbuf.c
      perf probe: No need to use formatting strbuf method
      perf help: Use asprintf instead of adhoc equivalents
      perf tools: Remove unused perf_pathdup, xstrdup functions
      perf tools: Do not include stringify.h from the kernel sources
      tools include: Copy linux/stringify.h from the kernel
      tools lib traceevent: Remove redundant CPU output
      perf tools: Remove needless 'extern' from function prototypes
      perf tools: Simplify die() mechanism
      perf tools: Remove unused DIE_IF macro
      perf script: Remove lots of unused arguments
      perf thread: Rename perf_event__preprocess_sample_addr to thread__resolve
      perf machine: Rename perf_event__preprocess_sample to machine__resolve
      perf tools: Add cpumode to struct perf_sample
      perf tests: Forward the perf_sample in the dwarf unwind test
      perf tools: Remove misplaced __maybe_unused
      perf list: Fix documentation of :ppp
      perf bench numa: Fix assertion for nodes bitfield
      ...

commit d88f48e12821ab4b2244124d50ac094568f48db5
Merge: be53f58fa0fc 9da77666d697
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 09:47:32 2016 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes:
    
       - fix hotplug bugs
       - fix irq live lock
       - fix various topology handling bugs
       - fix APIC ACK ordering
       - fix PV iopl handling
       - fix speling
       - fix/tweak memcpy_mcsafe() return value
       - fix fbcon bug
       - remove stray prototypes"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/msr: Remove unused native_read_tscp()
      x86/apic: Remove declaration of unused hw_nmi_is_cpu_stuck
      x86/oprofile/nmi: Add missing hotplug FROZEN handling
      x86/hpet: Use proper mask to modify hotplug action
      x86/apic/uv: Fix the hotplug notifier
      x86/apb/timer: Use proper mask to modify hotplug action
      x86/topology: Use total_cpus not nr_cpu_ids for logical packages
      x86/topology: Fix Intel HT disable
      x86/topology: Fix logical package mapping
      x86/irq: Cure live lock in fixup_irqs()
      x86/tsc: Prevent NULL pointer deref in calibrate_delay_is_known()
      x86/apic: Fix suspicious RCU usage in smp_trace_call_function_interrupt()
      x86/iopl: Fix iopl capability check on Xen PV
      x86/iopl/64: Properly context-switch IOPL on Xen PV
      selftests/x86: Add an iopl test
      x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe()
      x86/video: Don't assume all FB devices are PCI devices
      arch/x86/irq: Purge useless handler declarations from hw_irq.h
      x86: Fix misspellings in comments

commit 01fe03ff1c7ce6b6e2212cb6171a49c2858fbc7c
Author: Huang Rui <ray.huang@amd.com>
Date:   Thu Jan 14 10:50:06 2016 +0800

    x86/cpufeature, perf/x86: Add AMD Accumulated Power Mechanism feature flag
    
    AMD CPU family 15h model 0x60 introduces a mechanism for measuring
    accumulated power. It is used to report the processor power consumption
    and support for it is indicated by CPUID Fn8000_0007_EDX[12].
    
    Signed-off-by: Huang Rui <ray.huang@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andreas Herrmann <herrmann.der.user@googlemail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hector Marco-Gisbert <hecmargi@upv.es>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Kristen Carlson Accardi <kristen@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wan Zongshun <Vincent.Wan@amd.com>
    Cc: spg_linux_kernel@amd.com
    Link: http://lkml.kernel.org/r/1452739808-11871-4-git-send-email-ray.huang@amd.com
    [ Resolved conflict and moved the synthetic CPUID slot to 19. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index d4b06e8cd77c..68fe8d3bed56 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -546,6 +546,10 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 			set_sched_clock_stable();
 	}
 
+	/* Bit 12 of 8000_0007 edx is accumulated power mechanism. */
+	if (c->x86_power & BIT(12))
+		set_cpu_cap(c, X86_FEATURE_ACC_POWER);
+
 #ifdef CONFIG_X86_64
 	set_cpu_cap(c, X86_FEATURE_SYSCALL32);
 #else

commit 8dfeae0d73bf803be1a533e147b3b0ea69375596
Author: Huang Rui <ray.huang@amd.com>
Date:   Thu Jan 14 10:50:04 2016 +0800

    perf/x86/amd: Move nodes_per_socket into bsp_init_amd()
    
    nodes_per_socket is static and it needn't be initialized many
    times during every CPU core init. So move its initialization into
    bsp_init_amd().
    
    Signed-off-by: Huang Rui <ray.huang@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andreas Herrmann <herrmann.der.user@googlemail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hector Marco-Gisbert <hecmargi@upv.es>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: spg_linux_kernel@amd.com
    Link: http://lkml.kernel.org/r/1452739808-11871-2-git-send-email-ray.huang@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b39338c4b260..d4b06e8cd77c 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -306,7 +306,6 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		u32 eax, ebx, ecx, edx;
 
 		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
-		nodes_per_socket = ((ecx >> 8) & 7) + 1;
 		node_id = ecx & 7;
 
 		/* get compute unit information */
@@ -317,7 +316,6 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		u64 value;
 
 		rdmsrl(MSR_FAM10H_NODE_ID, value);
-		nodes_per_socket = ((value >> 3) & 7) + 1;
 		node_id = value & 7;
 	} else
 		return;
@@ -519,6 +517,18 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 
 	if (cpu_has(c, X86_FEATURE_MWAITX))
 		use_mwaitx_delay();
+
+	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
+		u32 ecx;
+
+		ecx = cpuid_ecx(0x8000001e);
+		nodes_per_socket = ((ecx >> 8) & 7) + 1;
+	} else if (boot_cpu_has(X86_FEATURE_NODEID_MSR)) {
+		u64 value;
+
+		rdmsrl(MSR_FAM10H_NODE_ID, value);
+		nodes_per_socket = ((value >> 3) & 7) + 1;
+	}
 }
 
 static void early_init_amd(struct cpuinfo_x86 *c)

commit 26660a4046b171a752e72a1dd32153230234fe3a
Merge: 46e595a17dcf 1bcb58a09993
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 20 18:23:21 2016 -0700

    Merge branch 'core-objtool-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull 'objtool' stack frame validation from Ingo Molnar:
     "This tree adds a new kernel build-time object file validation feature
      (ONFIG_STACK_VALIDATION=y): kernel stack frame correctness validation.
      It was written by and is maintained by Josh Poimboeuf.
    
      The motivation: there's a category of hard to find kernel bugs, most
      of them in assembly code (but also occasionally in C code), that
      degrades the quality of kernel stack dumps/backtraces.  These bugs are
      hard to detect at the source code level.  Such bugs result in
      incorrect/incomplete backtraces most of time - but can also in some
      rare cases result in crashes or other undefined behavior.
    
      The build time correctness checking is done via the new 'objtool'
      user-space utility that was written for this purpose and which is
      hosted in the kernel repository in tools/objtool/.  The tool's (very
      simple) UI and source code design is shaped after Git and perf and
      shares quite a bit of infrastructure with tools/perf (which tooling
      infrastructure sharing effort got merged via perf and is already
      upstream).  Objtool follows the well-known kernel coding style.
    
      Objtool does not try to check .c or .S files, it instead analyzes the
      resulting .o generated machine code from first principles: it decodes
      the instruction stream and interprets it.  (Right now objtool supports
      the x86-64 architecture.)
    
      From tools/objtool/Documentation/stack-validation.txt:
    
       "The kernel CONFIG_STACK_VALIDATION option enables a host tool named
        objtool which runs at compile time.  It has a "check" subcommand
        which analyzes every .o file and ensures the validity of its stack
        metadata.  It enforces a set of rules on asm code and C inline
        assembly code so that stack traces can be reliable.
    
        Currently it only checks frame pointer usage, but there are plans to
        add CFI validation for C files and CFI generation for asm files.
    
        For each function, it recursively follows all possible code paths
        and validates the correct frame pointer state at each instruction.
    
        It also follows code paths involving special sections, like
        .altinstructions, __jump_table, and __ex_table, which can add
        alternative execution paths to a given instruction (or set of
        instructions).  Similarly, it knows how to follow switch statements,
        for which gcc sometimes uses jump tables."
    
      When this new kernel option is enabled (it's disabled by default), the
      tool, if it finds any suspicious assembly code pattern, outputs
      warnings in compiler warning format:
    
        warning: objtool: rtlwifi_rate_mapping()+0x2e7: frame pointer state mismatch
        warning: objtool: cik_tiling_mode_table_init()+0x6ce: call without frame pointer save/setup
        warning: objtool:__schedule()+0x3c0: duplicate frame pointer save
        warning: objtool:__schedule()+0x3fd: sibling call from callable instruction with changed frame pointer
    
      ... so that scripts that pick up compiler warnings will notice them.
      All known warnings triggered by the tool are fixed by the tree, most
      of the commits in fact prepare the kernel to be warning-free.  Most of
      them are bugfixes or cleanups that stand on their own, but there are
      also some annotations of 'special' stack frames for justified cases
      such entries to JIT-ed code (BPF) or really special boot time code.
    
      There are two other long-term motivations behind this tool as well:
    
       - To improve the quality and reliability of kernel stack frames, so
         that they can be used for optimized live patching.
    
       - To create independent infrastructure to check the correctness of
         CFI stack frames at build time.  CFI debuginfo is notoriously
         unreliable and we cannot use it in the kernel as-is without extra
         checking done both on the kernel side and on the build side.
    
      The quality of kernel stack frames matters to debuggability as well,
      so IMO we can merge this without having to consider the live patching
      or CFI debuginfo angle"
    
    * 'core-objtool-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (52 commits)
      objtool: Only print one warning per function
      objtool: Add several performance improvements
      tools: Copy hashtable.h into tools directory
      objtool: Fix false positive warnings for functions with multiple switch statements
      objtool: Rename some variables and functions
      objtool: Remove superflous INIT_LIST_HEAD
      objtool: Add helper macros for traversing instructions
      objtool: Fix false positive warnings related to sibling calls
      objtool: Compile with debugging symbols
      objtool: Detect infinite recursion
      objtool: Prevent infinite recursion in noreturn detection
      objtool: Detect and warn if libelf is missing and don't break the build
      tools: Support relative directory path for 'O='
      objtool: Support CROSS_COMPILE
      x86/asm/decoder: Use explicitly signed chars
      objtool: Enable stack metadata validation on 64-bit x86
      objtool: Add CONFIG_STACK_VALIDATION option
      objtool: Add tool to perform compile-time stack metadata validation
      x86/kprobes: Mark kretprobe_trampoline() stack frame as non-standard
      sched: Always inline context_switch()
      ...

commit 6a6256f9e0ebaabf7ded1fef8977a4352dbe7784
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Tue Feb 23 15:34:30 2016 -0800

    x86: Fix misspellings in comments
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: trivial@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 97c59fd60702..b39338c4b260 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -82,7 +82,7 @@ static void init_amd_k5(struct cpuinfo_x86 *c)
 #ifdef CONFIG_X86_32
 /*
  * General Systems BIOSen alias the cpu frequency registers
- * of the Elan at 0x000df000. Unfortuantly, one of the Linux
+ * of the Elan at 0x000df000. Unfortunately, one of the Linux
  * drivers subsequently pokes it, and changes the CPU speed.
  * Workaround : Remove the unneeded alias.
  */

commit de642faf48670c3c8eae5899177f786c624f4894
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Jan 21 16:49:14 2016 -0600

    x86/amd: Set ELF function type for vide()
    
    vide() is a callable function, but is missing the ELF function type,
    which confuses tools like stacktool.
    
    Properly annotate it to be a callable function.  The generated code is
    unchanged.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Bernd Petrovitsch <bernd@petrovitsch.priv.at>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Chris J Arges <chris.j.arges@canonical.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Marek <mmarek@suse.cz>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Pedro Alves <palves@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: live-patching@vger.kernel.org
    Link: http://lkml.kernel.org/r/a324095f5c9390ff39b15b4562ea1bbeda1a8282.1453405861.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a07956a08936..fe2f089f03d2 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -75,7 +75,10 @@ static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
  */
 
 extern __visible void vide(void);
-__asm__(".globl vide\n\t.align 4\nvide: ret");
+__asm__(".globl vide\n"
+	".type vide, @function\n"
+	".align 4\n"
+	"vide: ret\n");
 
 static void init_amd_k5(struct cpuinfo_x86 *c)
 {

commit 1b74dde7c47c19a73ea3e9fac95ac27b5d3d50c5
Author: Chen Yucong <slaoub@gmail.com>
Date:   Tue Feb 2 11:45:02 2016 +0800

    x86/cpu: Convert printk(KERN_<LEVEL> ...) to pr_<level>(...)
    
     - Use the more current logging style pr_<level>(...) instead of the old
       printk(KERN_<LEVEL> ...).
    
     - Convert pr_warning() to pr_warn().
    
    Signed-off-by: Chen Yucong <slaoub@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454384702-21707-1-git-send-email-slaoub@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a07956a08936..97c59fd60702 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -117,7 +117,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 		void (*f_vide)(void);
 		u64 d, d2;
 
-		printk(KERN_INFO "AMD K6 stepping B detected - ");
+		pr_info("AMD K6 stepping B detected - ");
 
 		/*
 		 * It looks like AMD fixed the 2.6.2 bug and improved indirect
@@ -133,10 +133,9 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 		d = d2-d;
 
 		if (d > 20*K6_BUG_LOOP)
-			printk(KERN_CONT
-				"system stability may be impaired when more than 32 MB are used.\n");
+			pr_cont("system stability may be impaired when more than 32 MB are used.\n");
 		else
-			printk(KERN_CONT "probably OK (after B9730xxxx).\n");
+			pr_cont("probably OK (after B9730xxxx).\n");
 	}
 
 	/* K6 with old style WHCR */
@@ -154,7 +153,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 			wbinvd();
 			wrmsr(MSR_K6_WHCR, l, h);
 			local_irq_restore(flags);
-			printk(KERN_INFO "Enabling old style K6 write allocation for %d Mb\n",
+			pr_info("Enabling old style K6 write allocation for %d Mb\n",
 				mbytes);
 		}
 		return;
@@ -175,7 +174,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 			wbinvd();
 			wrmsr(MSR_K6_WHCR, l, h);
 			local_irq_restore(flags);
-			printk(KERN_INFO "Enabling new style K6 write allocation for %d Mb\n",
+			pr_info("Enabling new style K6 write allocation for %d Mb\n",
 				mbytes);
 		}
 
@@ -202,7 +201,7 @@ static void init_amd_k7(struct cpuinfo_x86 *c)
 	 */
 	if (c->x86_model >= 6 && c->x86_model <= 10) {
 		if (!cpu_has(c, X86_FEATURE_XMM)) {
-			printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
+			pr_info("Enabling disabled K7/SSE Support.\n");
 			msr_clear_bit(MSR_K7_HWCR, 15);
 			set_cpu_cap(c, X86_FEATURE_XMM);
 		}
@@ -216,9 +215,8 @@ static void init_amd_k7(struct cpuinfo_x86 *c)
 	if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
 		rdmsr(MSR_K7_CLK_CTL, l, h);
 		if ((l & 0xfff00000) != 0x20000000) {
-			printk(KERN_INFO
-			    "CPU: CLK_CTL MSR was %x. Reprogramming to %x\n",
-					l, ((l & 0x000fffff)|0x20000000));
+			pr_info("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n",
+				l, ((l & 0x000fffff)|0x20000000));
 			wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
 		}
 	}
@@ -485,7 +483,7 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 		if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &tseg)) {
 			unsigned long pfn = tseg >> PAGE_SHIFT;
 
-			printk(KERN_DEBUG "tseg: %010llx\n", tseg);
+			pr_debug("tseg: %010llx\n", tseg);
 			if (pfn_range_is_mapped(pfn, pfn + 1))
 				set_memory_4k((unsigned long)__va(tseg), 1);
 		}
@@ -500,8 +498,7 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 
 			rdmsrl(MSR_K7_HWCR, val);
 			if (!(val & BIT(24)))
-				printk(KERN_WARNING FW_BUG "TSC doesn't count "
-					"with P0 frequency!\n");
+				pr_warn(FW_BUG "TSC doesn't count with P0 frequency!\n");
 		}
 	}
 

commit 7030a7e9321166eef44c811fe4af4d460360d424
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Wed Jan 13 15:39:40 2016 +0300

    x86/cpu/amd: Remove an unneeded condition in srat_detect_node()
    
    Originally we calculated ht_nodeid as "ht_nodeid = apicid -
    boot_cpu_id;" so presumably it could be negative.
    
    But after commit:
    
      01aaea1afbcd ('x86: introduce initial apicid')
    
    we use c->initial_apicid which is an unsigned short and thus always >= 0.
    
    It causes a static checker warning to test for impossible
    conditions so let's remove it.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Hector Marco-Gisbert <hecmargi@upv.es>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Link: http://lkml.kernel.org/r/20160113123940.GE19993@mwanda
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e678ddeed030..a07956a08936 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -434,8 +434,7 @@ static void srat_detect_node(struct cpuinfo_x86 *c)
 		 */
 		int ht_nodeid = c->initial_apicid;
 
-		if (ht_nodeid >= 0 &&
-		    __apicid_to_node[ht_nodeid] != NUMA_NO_NODE)
+		if (__apicid_to_node[ht_nodeid] != NUMA_NO_NODE)
 			node = __apicid_to_node[ht_nodeid];
 		/* Pick a nearby node */
 		if (!node_online(node))

commit 671d5532aaad777782b66eff71bc4dfad25f942d
Merge: 67c707e451e1 0007bccc3cfd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 11 16:46:20 2016 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Improved CPU ID handling code and related enhancements (Borislav
         Petkov)
    
       - RDRAND fix (Len Brown)"
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Replace RDRAND forced-reseed with simple sanity check
      x86/MSR: Chop off lower 32-bit value
      x86/cpu: Fix MSR value truncation issue
      x86/cpu/amd, kvm: Satisfy guest kernel reads of IC_CFG MSR
      kvm: Add accessors for guest CPU's family, model, stepping
      x86/cpu: Unify CPU family, model, stepping calculation

commit 362f924b64ba0f4be2ee0cb697690c33d40be721
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Dec 7 10:39:41 2015 +0100

    x86/cpufeature: Remove unused and seldomly used cpu_has_xx macros
    
    Those are stupid and code should use static_cpu_has_safe() or
    boot_cpu_has() instead. Kill the least used and unused ones.
    
    The remaining ones need more careful inspection before a conversion can
    happen. On the TODO.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1449481182-27541-4-git-send-email-bp@alien8.de
    Cc: David Sterba <dsterba@suse.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a8816b325162..34c3ad608dd4 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -304,7 +304,7 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 	int cpu = smp_processor_id();
 
 	/* get information required for multi-node processors */
-	if (cpu_has_topoext) {
+	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
 		u32 eax, ebx, ecx, edx;
 
 		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
@@ -922,7 +922,7 @@ static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
 
 void set_dr_addr_mask(unsigned long mask, int dr)
 {
-	if (!cpu_has_bpext)
+	if (!boot_cpu_has(X86_FEATURE_BPEXT))
 		return;
 
 	switch (dr) {

commit ae8b787543d872cf89a7f9ef8aa302f3ef9bcbd7
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Nov 23 11:12:23 2015 +0100

    x86/cpu/amd, kvm: Satisfy guest kernel reads of IC_CFG MSR
    
    The kernel accesses IC_CFG MSR (0xc0011021) on AMD because it
    checks whether the way access filter is enabled on some F15h
    models, and, if so, disables it.
    
    kvm doesn't handle that MSR access and complains about it, which
    can get really noisy in dmesg when one starts kvm guests all the
    time for testing. And it is useless anyway - guest kernel
    shouldn't be doing such changes anyway so tell it that that
    filter is disabled.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1448273546-2567-4-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a8816b325162..e229640c19ab 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -678,9 +678,9 @@ static void init_amd_bd(struct cpuinfo_x86 *c)
 	 * Disable it on the affected CPUs.
 	 */
 	if ((c->x86_model >= 0x02) && (c->x86_model < 0x20)) {
-		if (!rdmsrl_safe(0xc0011021, &value) && !(value & 0x1E)) {
+		if (!rdmsrl_safe(MSR_F15H_IC_CFG, &value) && !(value & 0x1E)) {
 			value |= 0x1E;
-			wrmsrl_safe(0xc0011021, value);
+			wrmsrl_safe(MSR_F15H_IC_CFG, value);
 		}
 	}
 }

commit 3849e91f571dcb48cf2c8143480c59137d44d6bc
Author: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
Date:   Wed Nov 4 12:49:42 2015 +0100

    x86/AMD: Fix last level cache topology for AMD Fam17h systems
    
    On AMD Fam17h systems, the last level cache is not resident in the
    northbridge. Therefore, we cannot assign cpu_llc_id to the same value as
    Node ID as we have been doing until now.
    
    We should rather look at the ApicID bits of the core to provide us the
    last level cache ID info.
    
    Signed-off-by: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Link: http://lkml.kernel.org/r/1446582899-9378-1-git-send-email-Aravind.Gopalakrishnan@amd.com
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 4a70fc6d400a..a8816b325162 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -352,6 +352,7 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 #ifdef CONFIG_SMP
 	unsigned bits;
 	int cpu = smp_processor_id();
+	unsigned int socket_id, core_complex_id;
 
 	bits = c->x86_coreid_bits;
 	/* Low order bits define the core id (index of core in socket) */
@@ -361,6 +362,18 @@ static void amd_detect_cmp(struct cpuinfo_x86 *c)
 	/* use socket ID also for last level cache */
 	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
 	amd_get_topology(c);
+
+	/*
+	 * Fix percpu cpu_llc_id here as LLC topology is different
+	 * for Fam17h systems.
+	 */
+	 if (c->x86 != 0x17 || !cpuid_edx(0x80000006))
+		return;
+
+	socket_id	= (c->apicid >> bits) - 1;
+	core_complex_id	= (c->apicid & ((1 << bits) - 1)) >> 3;
+
+	per_cpu(cpu_llc_id, cpu) = (socket_id << 3) | core_complex_id;
 #endif
 }
 

commit b466bdb614823aaaa7188e85516177d2850f4782
Author: Huang Rui <ray.huang@amd.com>
Date:   Mon Aug 10 12:19:54 2015 +0200

    x86/asm/delay: Introduce an MWAITX-based delay with a configurable timer
    
    MWAITX can enable a timer and a corresponding timer value
    specified in SW P0 clocks. The SW P0 frequency is the same as
    TSC. The timer provides an upper bound on how long the
    instruction waits before exiting.
    
    This way, a delay function in the kernel can leverage that
    MWAITX timer of MWAITX.
    
    When a CPU core executes MWAITX, it will be quiesced in a
    waiting phase, diminishing its power consumption. This way, we
    can save power in comparison to our default TSC-based delays.
    
    A simple test shows that:
    
            $ cat /sys/bus/pci/devices/0000\:00\:18.4/hwmon/hwmon0/power1_acc
            $ sleep 10000s
            $ cat /sys/bus/pci/devices/0000\:00\:18.4/hwmon/hwmon0/power1_acc
    
    Results:
    
            * TSC-based default delay:      485115 uWatts average power
            * MWAITX-based delay:           252738 uWatts average power
    
    Thus, that's about 240 milliWatts less power consumption. The
    test method relies on the support of AMD CPU accumulated power
    algorithm in fam15h_power for which patches are forthcoming.
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Suggested-by: Borislav Petkov <bp@suse.de>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Huang Rui <ray.huang@amd.com>
    [ Fix delay truncation. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Andreas Herrmann <herrmann.der.user@gmail.com>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hector Marco-Gisbert <hecmargi@upv.es>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Li <tony.li@amd.com>
    Link: http://lkml.kernel.org/r/1438744732-1459-3-git-send-email-ray.huang@amd.com
    Link: http://lkml.kernel.org/r/1439201994-28067-4-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 51ad2af84a72..4a70fc6d400a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -11,6 +11,7 @@
 #include <asm/cpu.h>
 #include <asm/smp.h>
 #include <asm/pci-direct.h>
+#include <asm/delay.h>
 
 #ifdef CONFIG_X86_64
 # include <asm/mmconfig.h>
@@ -506,6 +507,9 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 		/* A random value per boot for bit slice [12:upper_bit) */
 		va_align.bits = get_random_int() & va_align.mask;
 	}
+
+	if (cpu_has(c, X86_FEATURE_MWAITX))
+		use_mwaitx_delay();
 }
 
 static void early_init_amd(struct cpuinfo_x86 *c)

commit 4ea1636b04dbd66536fa387bae2eea463efc705b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:44:07 2015 +0200

    x86/asm/tsc: Rename native_read_tsc() to rdtsc()
    
    Now that there is no paravirt TSC, the "native" is
    inappropriate. The function does RDTSC, so give it the obvious
    name: rdtsc().
    
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/fd43e16281991f096c1e4d21574d9e1402c62d39.1434501121.git.luto@kernel.org
    [ Ported it to v4.2-rc1. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a69710db6112..51ad2af84a72 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -125,10 +125,10 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 
 		n = K6_BUG_LOOP;
 		f_vide = vide;
-		d = native_read_tsc();
+		d = rdtsc();
 		while (n--)
 			f_vide();
-		d2 = native_read_tsc();
+		d2 = rdtsc();
 		d = d2-d;
 
 		if (d > 20*K6_BUG_LOOP)

commit 3796366614598e48edf0561b86f18c230a7debc8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jun 25 18:44:01 2015 +0200

    x86/asm/tsc, x86/cpu/amd: Use the full 64-bit TSC to detect the 2.6.2 bug
    
    This code is timing 100k indirect calls, so the added overhead
    of counting the number of cycles elapsed as a 64-bit number
    should be insignificant.  Drop the optimization of using a
    32-bit count.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Huang Rui <ray.huang@amd.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm ML <kvm@vger.kernel.org>
    Link: http://lkml.kernel.org/r/d58f339a9c0dd8352b50d2f7a216f67ec2844f20.1434501121.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index dd3a4baffe50..a69710db6112 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -114,7 +114,7 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 		const int K6_BUG_LOOP = 1000000;
 		int n;
 		void (*f_vide)(void);
-		unsigned long d, d2;
+		u64 d, d2;
 
 		printk(KERN_INFO "AMD K6 stepping B detected - ");
 
@@ -125,10 +125,10 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 
 		n = K6_BUG_LOOP;
 		f_vide = vide;
-		rdtscl(d);
+		d = native_read_tsc();
 		while (n--)
 			f_vide();
-		rdtscl(d2);
+		d2 = native_read_tsc();
 		d = d2-d;
 
 		if (d > 20*K6_BUG_LOOP)

commit d70b3ef54ceaf1c7c92209f5a662a670d04cbed9
Merge: 650ec5a6bd5d 7ef3d7d58d9d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 17:59:09 2015 -0700

    Merge branch 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "There were so many changes in the x86/asm, x86/apic and x86/mm topics
      in this cycle that the topical separation of -tip broke down somewhat -
      so the result is a more traditional architecture pull request,
      collected into the 'x86/core' topic.
    
      The topics were still maintained separately as far as possible, so
      bisectability and conceptual separation should still be pretty good -
      but there were a handful of merge points to avoid excessive
      dependencies (and conflicts) that would have been poorly tested in the
      end.
    
      The next cycle will hopefully be much more quiet (or at least will
      have fewer dependencies).
    
      The main changes in this cycle were:
    
       * x86/apic changes, with related IRQ core changes: (Jiang Liu, Thomas
         Gleixner)
    
         - This is the second and most intrusive part of changes to the x86
           interrupt handling - full conversion to hierarchical interrupt
           domains:
    
              [IOAPIC domain]   -----
                                     |
              [MSI domain]      --------[Remapping domain] ----- [ Vector domain ]
                                     |   (optional)          |
              [HPET MSI domain] -----                        |
                                                             |
              [DMAR domain]     -----------------------------
                                                             |
              [Legacy domain]   -----------------------------
    
           This now reflects the actual hardware and allowed us to distangle
           the domain specific code from the underlying parent domain, which
           can be optional in the case of interrupt remapping.  It's a clear
           separation of functionality and removes quite some duct tape
           constructs which plugged the remap code between ioapic/msi/hpet
           and the vector management.
    
         - Intel IOMMU IRQ remapping enhancements, to allow direct interrupt
           injection into guests (Feng Wu)
    
       * x86/asm changes:
    
         - Tons of cleanups and small speedups, micro-optimizations.  This
           is in preparation to move a good chunk of the low level entry
           code from assembly to C code (Denys Vlasenko, Andy Lutomirski,
           Brian Gerst)
    
         - Moved all system entry related code to a new home under
           arch/x86/entry/ (Ingo Molnar)
    
         - Removal of the fragile and ugly CFI dwarf debuginfo annotations.
           Conversion to C will reintroduce many of them - but meanwhile
           they are only getting in the way, and the upstream kernel does
           not rely on them (Ingo Molnar)
    
         - NOP handling refinements. (Borislav Petkov)
    
       * x86/mm changes:
    
         - Big PAT and MTRR rework: making the code more robust and
           preparing to phase out exposing direct MTRR interfaces to drivers -
           in favor of using PAT driven interfaces (Toshi Kani, Luis R
           Rodriguez, Borislav Petkov)
    
         - New ioremap_wt()/set_memory_wt() interfaces to support
           Write-Through cached memory mappings.  This is especially
           important for good performance on NVDIMM hardware (Toshi Kani)
    
       * x86/ras changes:
    
         - Add support for deferred errors on AMD (Aravind Gopalakrishnan)
    
           This is an important RAS feature which adds hardware support for
           poisoned data.  That means roughly that the hardware marks data
           which it has detected as corrupted but wasn't able to correct, as
           poisoned data and raises an APIC interrupt to signal that in the
           form of a deferred error.  It is the OS's responsibility then to
           take proper recovery action and thus prolonge system lifetime as
           far as possible.
    
         - Add support for Intel "Local MCE"s: upcoming CPUs will support
           CPU-local MCE interrupts, as opposed to the traditional system-
           wide broadcasted MCE interrupts (Ashok Raj)
    
         - Misc cleanups (Borislav Petkov)
    
       * x86/platform changes:
    
         - Intel Atom SoC updates
    
      ... and lots of other cleanups, fixlets and other changes - see the
      shortlog and the Git log for details"
    
    * 'x86-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (222 commits)
      x86/hpet: Use proper hpet device number for MSI allocation
      x86/hpet: Check for irq==0 when allocating hpet MSI interrupts
      x86/mm/pat, drivers/infiniband/ipath: Use arch_phys_wc_add() and require PAT disabled
      x86/mm/pat, drivers/media/ivtv: Use arch_phys_wc_add() and require PAT disabled
      x86/platform/intel/baytrail: Add comments about why we disabled HPET on Baytrail
      genirq: Prevent crash in irq_move_irq()
      genirq: Enhance irq_data_to_desc() to support hierarchy irqdomain
      iommu, x86: Properly handle posted interrupts for IOMMU hotplug
      iommu, x86: Provide irq_remapping_cap() interface
      iommu, x86: Setup Posted-Interrupts capability for Intel iommu
      iommu, x86: Add cap_pi_support() to detect VT-d PI capability
      iommu, x86: Avoid migrating VT-d posted interrupts
      iommu, x86: Save the mode (posted or remapped) of an IRTE
      iommu, x86: Implement irq_set_vcpu_affinity for intel_ir_chip
      iommu: dmar: Provide helper to copy shared irte fields
      iommu: dmar: Extend struct irte for VT-d Posted-Interrupts
      iommu: Add new member capability to struct irq_remap_ops
      x86/asm/entry/64: Disentangle error_entry/exit gsbase/ebx/usermode code
      x86/asm/entry/32: Shorten __audit_syscall_entry() args preparation
      x86/asm/entry/32: Explain reloading of registers after __audit_syscall_entry()
      ...

commit cc2749e4095cbbcb35518fb2db5e926b85c3f25f
Author: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
Date:   Mon Jun 15 10:28:15 2015 +0200

    x86/cpu/amd: Give access to the number of nodes in a physical package
    
    Stash the number of nodes in a physical processor package
    locally and add an accessor to be called by interested parties.
    The first user is the MCE injection module which uses it to find
    the node base core in a package for injecting a certain type of
    errors.
    
    Signed-off-by: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    [ Rewrote the commit message, merged it with the accessor patch and unified naming. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-edac <linux-edac@vger.kernel.org>
    Cc: mchehab@osg.samsung.com
    Link: http://lkml.kernel.org/r/1433868317-18417-2-git-send-email-Aravind.Gopalakrishnan@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 94e7051fba1a..56cae1964a81 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -19,6 +19,13 @@
 
 #include "cpu.h"
 
+/*
+ * nodes_per_socket: Stores the number of nodes per socket.
+ * Refer to Fam15h Models 00-0fh BKDG - CPUID Fn8000_001E_ECX
+ * Node Identifiers[10:8]
+ */
+static u32 nodes_per_socket = 1;
+
 static inline int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)
 {
 	u32 gprs[8] = { 0 };
@@ -291,7 +298,7 @@ static int nearby_node(int apicid)
 #ifdef CONFIG_X86_HT
 static void amd_get_topology(struct cpuinfo_x86 *c)
 {
-	u32 nodes, cores_per_cu = 1;
+	u32 cores_per_cu = 1;
 	u8 node_id;
 	int cpu = smp_processor_id();
 
@@ -300,7 +307,7 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		u32 eax, ebx, ecx, edx;
 
 		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
-		nodes = ((ecx >> 8) & 7) + 1;
+		nodes_per_socket = ((ecx >> 8) & 7) + 1;
 		node_id = ecx & 7;
 
 		/* get compute unit information */
@@ -311,18 +318,18 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 		u64 value;
 
 		rdmsrl(MSR_FAM10H_NODE_ID, value);
-		nodes = ((value >> 3) & 7) + 1;
+		nodes_per_socket = ((value >> 3) & 7) + 1;
 		node_id = value & 7;
 	} else
 		return;
 
 	/* fixup multi-node processor information */
-	if (nodes > 1) {
+	if (nodes_per_socket > 1) {
 		u32 cores_per_node;
 		u32 cus_per_node;
 
 		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
-		cores_per_node = c->x86_max_cores / nodes;
+		cores_per_node = c->x86_max_cores / nodes_per_socket;
 		cus_per_node = cores_per_node / cores_per_cu;
 
 		/* store NodeID, use llc_shared_map to store sibling info */
@@ -366,6 +373,12 @@ u16 amd_get_nb_id(int cpu)
 }
 EXPORT_SYMBOL_GPL(amd_get_nb_id);
 
+u32 amd_get_nodes_per_socket(void)
+{
+	return nodes_per_socket;
+}
+EXPORT_SYMBOL_GPL(amd_get_nodes_per_socket);
+
 static void srat_detect_node(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_NUMA

commit c8e56d20f2d190d54c0615775dcb6a23c1091681
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jun 4 18:55:25 2015 +0200

    x86: Kill CONFIG_X86_HT
    
    In talking to Aravind recently about making certain AMD topology
    attributes available to the MCE injection module, it seemed like
    that CONFIG_X86_HT thing is more or less superfluous. It is
    def_bool y, depends on SMP and gets enabled in the majority of
    .configs - distro and otherwise - out there.
    
    So let's kill it and make code behind it depend directly on SMP.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Daniel Walter <dwalter@google.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1433436928-31903-18-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e4cf63301ff4..eb4f01269b5d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -288,7 +288,7 @@ static int nearby_node(int apicid)
  *     Assumption: Number of cores in each internal node is the same.
  * (2) AMD processors supporting compute units
  */
-#ifdef CONFIG_X86_HT
+#ifdef CONFIG_SMP
 static void amd_get_topology(struct cpuinfo_x86 *c)
 {
 	u32 nodes, cores_per_cu = 1;
@@ -341,7 +341,7 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
  */
 static void amd_detect_cmp(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_X86_HT
+#ifdef CONFIG_SMP
 	unsigned bits;
 	int cpu = smp_processor_id();
 
@@ -420,7 +420,7 @@ static void srat_detect_node(struct cpuinfo_x86 *c)
 
 static void early_init_amd_mc(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_X86_HT
+#ifdef CONFIG_SMP
 	unsigned bits, ecx;
 
 	/* Multi core CPU? */

commit b9d16a2a21aa9c264a29dd84d6f7b03581517a03
Author: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
Date:   Mon Apr 27 10:25:51 2015 -0500

    x86/cpu/amd: Set X86_FEATURE_EXTD_APICID for future processors
    
    Decision to use a 4-bit mask or 8-bit mask in default_get_apic_id()
    is controlled by setting capability bit X86_FEATURE_EXTD_APICID.
    
    Currently, we detect extended APIC ID support by accessing Link
    Transaction Control register D18F0x68 in PCI config space.
    
    But, not even that is needed as we can safely postulate that future
    AMD processors will support 8-bit APIC IDs and we can simply set that
    feature bit on them, without the PCI access.
    
    Signed-off-by: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jacob Shin <jacob.w.shin@gmail.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave.hansen@linux.intel.com
    Cc: hecmargi@upv.es
    Cc: mgorman@suse.de
    Link: http://lkml.kernel.org/r/1430148351-9013-1-git-send-email-Aravind.Gopalakrishnan@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e4cf63301ff4..94e7051fba1a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -520,8 +520,16 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_K6_MTRR);
 #endif
 #if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_PCI)
-	/* check CPU config space for extended APIC ID */
-	if (cpu_has_apic && c->x86 >= 0xf) {
+	/*
+	 * ApicID can always be treated as an 8-bit value for AMD APIC versions
+	 * >= 0x10, but even old K8s came out of reset with version 0x10. So, we
+	 * can safely set X86_FEATURE_EXTD_APICID unconditionally for families
+	 * after 16h.
+	 */
+	if (cpu_has_apic && c->x86 > 0x16) {
+		set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
+	} else if (cpu_has_apic && c->x86 >= 0xf) {
+		/* check CPU config space for extended APIC ID */
 		unsigned int val;
 		val = read_pci_config(0, 24, 0, 0x68);
 		if ((val & ((1 << 17) | (1 << 18))) == ((1 << 17) | (1 << 18)))

commit 61f01dd941ba9e06d2bf05994450ecc3d61b6b8b
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Apr 26 16:47:59 2015 -0700

    x86_64, asm: Work around AMD SYSRET SS descriptor attribute issue
    
    AMD CPUs don't reinitialize the SS descriptor on SYSRET, so SYSRET with
    SS == 0 results in an invalid usermode state in which SS is apparently
    equal to __USER_DS but causes #SS if used.
    
    Work around the issue by setting SS to __KERNEL_DS __switch_to, thus
    ensuring that SYSRET never happens with SS set to NULL.
    
    This was exposed by a recent vDSO cleanup.
    
    Fixes: e7d6eefaaa44 x86/vdso32/syscall.S: Do not load __USER32_DS to %ss
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Anvin <hpa@zytor.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <vda.linux@googlemail.com>
    Cc: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index fd470ebf924e..e4cf63301ff4 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -720,6 +720,9 @@ static void init_amd(struct cpuinfo_x86 *c)
 	if (!cpu_has(c, X86_FEATURE_3DNOWPREFETCH))
 		if (cpu_has(c, X86_FEATURE_3DNOW) || cpu_has(c, X86_FEATURE_LM))
 			set_cpu_cap(c, X86_FEATURE_3DNOWPREFETCH);
+
+	/* AMD CPUs don't reset SS attributes on SYSRET */
+	set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
 }
 
 #ifdef CONFIG_X86_32

commit 6cf78d4b3766bcd25348d72377796f9566ac8e1a
Merge: 0ad5c6b3c2d1 4e26d11f5268
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 12:31:32 2015 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - reduce the x86/32 PAE per task PGD allocation overhead from 4K to
         0.032k (Fenghua Yu)
    
       - early_ioremap/memunmap() usage cleanups (Juergen Gross)
    
       - gbpages support cleanups (Luis R Rodriguez)
    
       - improve AMD Bulldozer (family 0x15) ASLR I$ aliasing workaround to
         increase randomization by 3 bits (per bootup) (Hector
         Marco-Gisbert)
    
       - misc fixlets"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Improve AMD Bulldozer ASLR workaround
      x86/mm/pat: Initialize __cachemode2pte_tbl[] and __pte2cachemode_tbl[] in a bit more readable fashion
      init.h: Clean up the __setup()/early_param() macros
      x86/mm: Simplify probe_page_size_mask()
      x86/mm: Further simplify 1 GB kernel linear mappings handling
      x86/mm: Use early_param_on_off() for direct_gbpages
      init.h: Add early_param_on_off()
      x86/mm: Simplify enabling direct_gbpages
      x86/mm: Use IS_ENABLED() for direct_gbpages
      x86/mm: Unexport set_memory_ro() and set_memory_rw()
      x86/mm, efi: Use early_ioremap() in arch/x86/platform/efi/efi-bgrt.c
      x86/mm: Use early_memunmap() instead of early_iounmap()
      x86/mm/pat: Ensure different messages in STRICT_DEVMEM and PAT cases
      x86/mm: Reduce PAE-mode per task pgd allocation overhead from 4K to 32 bytes

commit 4e26d11f52684dc8b1632a8cfe450cb5197a8464
Author: Hector Marco-Gisbert <hecmargi@upv.es>
Date:   Fri Mar 27 12:38:21 2015 +0100

    x86/mm: Improve AMD Bulldozer ASLR workaround
    
    The ASLR implementation needs to special-case AMD F15h processors by
    clearing out bits [14:12] of the virtual address in order to avoid I$
    cross invalidations and thus performance penalty for certain workloads.
    For details, see:
    
      dfb09f9b7ab0 ("x86, amd: Avoid cache aliasing penalties on AMD family 15h")
    
    This special case reduces the mmapped file's entropy by 3 bits.
    
    The following output is the run on an AMD Opteron 62xx class CPU
    processor under x86_64 Linux 4.0.0:
    
      $ for i in `seq 1 10`; do cat /proc/self/maps | grep "r-xp.*libc" ; done
      b7588000-b7736000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b7570000-b771e000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b75d0000-b777e000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b75b0000-b775e000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      b7578000-b7726000 r-xp 00000000 00:01 4924       /lib/i386-linux-gnu/libc.so.6
      ...
    
    Bits [12:14] are always 0, i.e. the address always ends in 0x8000 or
    0x0000.
    
    32-bit systems, as in the example above, are especially sensitive
    to this issue because 32-bit randomness for VA space is 8 bits (see
    mmap_rnd()). With the Bulldozer special case, this diminishes to only 32
    different slots of mmap virtual addresses.
    
    This patch randomizes per boot the three affected bits rather than
    setting them to zero. Since all the shared pages have the same value
    at bits [12..14], there is no cache aliasing problems. This value gets
    generated during system boot and it is thus not known to a potential
    remote attacker. Therefore, the impact from the Bulldozer workaround
    gets diminished and ASLR randomness increased.
    
    More details at:
    
      http://hmarco.org/bugs/AMD-Bulldozer-linux-ASLR-weakness-reducing-mmaped-files-by-eight.html
    
    Original white paper by AMD dealing with the issue:
    
      http://developer.amd.com/wordpress/media/2012/10/SharedL1InstructionCacheonAMD15hCPU.pdf
    
    Mentored-by: Ismael Ripoll <iripoll@disca.upv.es>
    Signed-off-by: Hector Marco-Gisbert <hecmargi@upv.es>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jan-Simon <dl9pf@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-fsdevel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1427456301-3764-1-git-send-email-hecmargi@upv.es
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a220239cea65..ec6a61b21b41 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -5,6 +5,7 @@
 
 #include <linux/io.h>
 #include <linux/sched.h>
+#include <linux/random.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cpu.h>
@@ -488,6 +489,9 @@ static void bsp_init_amd(struct cpuinfo_x86 *c)
 
 		va_align.mask	  = (upperbit - 1) & PAGE_MASK;
 		va_align.flags    = ALIGN_VA_32 | ALIGN_VA_64;
+
+		/* A random value per boot for bit slice [12:upper_bit) */
+		va_align.bits = get_random_int() & va_align.mask;
 	}
 }
 

commit a930dc4543a2b213deb9fde12682716edff8a4a6
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Jan 18 17:48:18 2015 +0100

    x86/asm: Cleanup prefetch primitives
    
    This is based on a patch originally by hpa.
    
    With the current improvements to the alternatives, we can simply use %P1
    as a mem8 operand constraint and rely on the toolchain to generate the
    proper instruction sizes. For example, on 32-bit, where we use an empty
    old instruction we get:
    
      apply_alternatives: feat: 6*32+8, old: (c104648b, len: 4), repl: (c195566c, len: 4)
      c104648b: alt_insn: 90 90 90 90
      c195566c: rpl_insn: 0f 0d 4b 5c
    
      ...
    
      apply_alternatives: feat: 6*32+8, old: (c18e09b4, len: 3), repl: (c1955948, len: 3)
      c18e09b4: alt_insn: 90 90 90
      c1955948: rpl_insn: 0f 0d 08
    
      ...
    
      apply_alternatives: feat: 6*32+8, old: (c1190cf9, len: 7), repl: (c1955a79, len: 7)
      c1190cf9: alt_insn: 90 90 90 90 90 90 90
      c1955a79: rpl_insn: 0f 0d 0d a0 d4 85 c1
    
    all with the proper padding done depending on the size of the
    replacement instruction the compiler generates.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a220239cea65..dd9e50500297 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -711,6 +711,11 @@ static void init_amd(struct cpuinfo_x86 *c)
 		set_cpu_bug(c, X86_BUG_AMD_APIC_C1E);
 
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
+
+	/* 3DNow or LM implies PREFETCHW */
+	if (!cpu_has(c, X86_FEATURE_3DNOWPREFETCH))
+		if (cpu_has(c, X86_FEATURE_3DNOW) || cpu_has(c, X86_FEATURE_LM))
+			set_cpu_cap(c, X86_FEATURE_3DNOWPREFETCH);
 }
 
 #ifdef CONFIG_X86_32

commit b3890e4704594fa23abe1395d1fafc97d3214be8
Merge: 29bf4dbc9841 2a2662bf88e6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jan 28 15:48:59 2015 +0100

    Merge branch 'perf/hw_breakpoints' into perf/core
    
    The new hw_breakpoint bits are now ready for v3.20, merge them
    into the main branch, to avoid conflicts.
    
    Conflicts:
            tools/perf/Documentation/perf-record.txt
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d6d55f0b9d900673548515614b56ab55aa2c51f8
Author: Jacob Shin <jacob.w.shin@gmail.com>
Date:   Thu May 29 17:26:50 2014 +0200

    perf/x86/amd: AMD support for bp_len > HW_BREAKPOINT_LEN_8
    
    Implement hardware breakpoint address mask for AMD Family 16h and
    above processors. CPUID feature bit indicates hardware support for
    DRn_ADDR_MASK MSRs. These masks further qualify DRn/DR7 hardware
    breakpoint addresses to allow matching of larger addresses ranges.
    
    Valuable advice and pseudo code from Oleg Nesterov <oleg@redhat.com>
    
    Signed-off-by: Jacob Shin <jacob.w.shin@gmail.com>
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: xiakaixu <xiakaixu@huawei.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 813d29d00a17..abe4ec760db3 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -870,3 +870,22 @@ static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
 
 	return false;
 }
+
+void set_dr_addr_mask(unsigned long mask, int dr)
+{
+	if (!cpu_has_bpext)
+		return;
+
+	switch (dr) {
+	case 0:
+		wrmsr(MSR_F16H_DR0_ADDR_MASK, mask, 0);
+		break;
+	case 1:
+	case 2:
+	case 3:
+		wrmsr(MSR_F16H_DR1_ADDR_MASK - 1 + dr, mask, 0);
+		break;
+	default:
+		break;
+	}
+}

commit 6f9b63a0ae0d694e3d8e6f673e1e8e2638526b97
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jul 29 17:41:23 2014 +0200

    x86, CPU, AMD: Move K8 TLB flush filter workaround to K8 code
    
    This belongs with the rest of the code in init_amd_k8() which gets
    executed on family 0xf.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 813d29d00a17..15c5df92f74e 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -566,6 +566,17 @@ static void init_amd_k8(struct cpuinfo_x86 *c)
 
 	if (!c->x86_model_id[0])
 		strcpy(c->x86_model_id, "Hammer");
+
+#ifdef CONFIG_SMP
+	/*
+	 * Disable TLB flush filter by setting HWCR.FFDIS on K8
+	 * bit 6 of msr C001_0015
+	 *
+	 * Errata 63 for SH-B3 steppings
+	 * Errata 122 for all steppings (F+ have it disabled by default)
+	 */
+	msr_set_bit(MSR_K7_HWCR, 6);
+#endif
 }
 
 static void init_amd_gh(struct cpuinfo_x86 *c)
@@ -636,18 +647,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 {
 	u32 dummy;
 
-#ifdef CONFIG_SMP
-	/*
-	 * Disable TLB flush filter by setting HWCR.FFDIS on K8
-	 * bit 6 of msr C001_0015
-	 *
-	 * Errata 63 for SH-B3 steppings
-	 * Errata 122 for all steppings (F+ have it disabled by default)
-	 */
-	if (c->x86 == 0xf)
-		msr_set_bit(MSR_K7_HWCR, 6);
-#endif
-
 	early_init_amd(c);
 
 	/*

commit c1118b3602c2329671ad5ec8bdf8e374323d6343
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Sep 22 13:17:48 2014 +0200

    x86: kvm: use alternatives for VMCALL vs. VMMCALL if kernel text is read-only
    
    On x86_64, kernel text mappings are mapped read-only with CONFIG_DEBUG_RODATA.
    In that case, KVM will fail to patch VMCALL instructions to VMMCALL
    as required on AMD processors.
    
    The failure mode is currently a divide-by-zero exception, which obviously
    is a KVM bug that has to be fixed.  However, picking the right instruction
    between VMCALL and VMMCALL will be faster and will help if you cannot upgrade
    the hypervisor.
    
    Reported-by: Chris Webb <chris@arachsys.com>
    Tested-by: Chris Webb <chris@arachsys.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 60e5497681f5..813d29d00a17 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -525,6 +525,13 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 	}
 #endif
 
+	/*
+	 * This is only needed to tell the kernel whether to use VMCALL
+	 * and VMMCALL.  VMMCALL is never executed except under virt, so
+	 * we can set it unconditionally.
+	 */
+	set_cpu_cap(c, X86_FEATURE_VMMCALL);
+
 	/* F16h erratum 793, CVE-2013-6885 */
 	if (c->x86 == 0x16 && c->x86_model <= 0xf)
 		msr_set_bit(MSR_AMD64_LS_CFG, 15);

commit ce4747963252a30613ebf1c1df3d83b9526a342e
Merge: 76f09aa464a1 a5102476a24b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 17:15:45 2014 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "The main change in this cycle is the rework of the TLB range flushing
      code, to simplify, fix and consolidate the code.  By Dave Hansen"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Set TLB flush tunable to sane value (33)
      x86/mm: New tunable for single vs full TLB flush
      x86/mm: Add tracepoints for TLB flushes
      x86/mm: Unify remote INVLPG code
      x86/mm: Fix missed global TLB flush stat
      x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
      x86/mm: Clean up the TLB flushing code
      x86/smep: Be more informative when signalling an SMEP fault

commit e9f4e0a9fe2723078b7a1a1169828dd46a7b2f9e
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jul 31 08:40:55 2014 -0700

    x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
    
    I think the flush_tlb_mm_range() code that tries to tune the
    flush sizes based on the CPU needs to get ripped out for
    several reasons:
    
    1. It is obviously buggy.  It uses mm->total_vm to judge the
       task's footprint in the TLB.  It should certainly be using
       some measure of RSS, *NOT* ->total_vm since only resident
       memory can populate the TLB.
    2. Haswell, and several other CPUs are missing from the
       intel_tlb_flushall_shift_set() function.  Thus, it has been
       demonstrated to bitrot quickly in practice.
    3. It is plain wrong in my vm:
            [    0.037444] Last level iTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] Last level dTLB entries: 4KB 0, 2MB 0, 4MB 0
            [    0.037444] tlb_flushall_shift: 6
       Which leads to it to never use invlpg.
    4. The assumptions about TLB refill costs are wrong:
            http://lkml.kernel.org/r/1337782555-8088-3-git-send-email-alex.shi@intel.com
        (more on this in later patches)
    5. I can not reproduce the original data: https://lkml.org/lkml/2012/5/17/59
       I believe the sample times were too short.  Running the
       benchmark in a loop yields times that vary quite a bit.
    
    Note that this leaves us with a static ceiling of 1 page.  This
    is a conservative, dumb setting, and will be revised in a later
    patch.
    
    This also removes the code which attempts to predict whether we
    are flushing data or instructions.  We expect instruction flushes
    to be relatively rare and not worth tuning for explicitly.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20140731154055.ABC88E89@viggo.jf.intel.com
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ce8b8ff0e0ef..a1a53d094987 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -741,11 +741,6 @@ static unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 }
 #endif
 
-static void cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)
-{
-	tlb_flushall_shift = 6;
-}
-
 static void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 {
 	u32 ebx, eax, ecx, edx;
@@ -793,8 +788,6 @@ static void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 		tlb_lli_2m[ENTRIES] = eax & mask;
 
 	tlb_lli_4m[ENTRIES] = tlb_lli_2m[ENTRIES] >> 1;
-
-	cpu_set_tlb_flushall_shift(c);
 }
 
 static const struct cpu_dev amd_cpu_dev = {

commit 26bfa5f89486a8926cd4d4ca81a04d3f0f174934
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jun 24 13:25:04 2014 +0200

    x86, amd: Cleanup init_amd
    
    Distribute family-specific code to corresponding functions.
    
    Also,
    
    * move the direct mapping splitting around the TSEG SMM area to
    bsp_init_amd().
    
    * kill ancient comment about what we should do for K5.
    
    * merge amd_k7_smp_check() into its only caller init_amd_k7 and drop
    cpu_has_mp macro.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1403609105-8332-3-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 8714a78414bf..bc360d3df60e 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -8,6 +8,7 @@
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cpu.h>
+#include <asm/smp.h>
 #include <asm/pci-direct.h>
 
 #ifdef CONFIG_X86_64
@@ -50,7 +51,6 @@ static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
 	return wrmsr_safe_regs(gprs);
 }
 
-#ifdef CONFIG_X86_32
 /*
  *	B step AMD K6 before B 9730xxxx have hardware bugs that can cause
  *	misexecution of code under Linux. Owners of such processors should
@@ -70,6 +70,7 @@ __asm__(".globl vide\n\t.align 4\nvide: ret");
 
 static void init_amd_k5(struct cpuinfo_x86 *c)
 {
+#ifdef CONFIG_X86_32
 /*
  * General Systems BIOSen alias the cpu frequency registers
  * of the Elan at 0x000df000. Unfortuantly, one of the Linux
@@ -83,11 +84,12 @@ static void init_amd_k5(struct cpuinfo_x86 *c)
 		if (inl(CBAR) & CBAR_ENB)
 			outl(0 | CBAR_KEY, CBAR);
 	}
+#endif
 }
 
-
 static void init_amd_k6(struct cpuinfo_x86 *c)
 {
+#ifdef CONFIG_X86_32
 	u32 l, h;
 	int mbytes = get_num_physpages() >> (20-PAGE_SHIFT);
 
@@ -176,10 +178,44 @@ static void init_amd_k6(struct cpuinfo_x86 *c)
 		/* placeholder for any needed mods */
 		return;
 	}
+#endif
 }
 
-static void amd_k7_smp_check(struct cpuinfo_x86 *c)
+static void init_amd_k7(struct cpuinfo_x86 *c)
 {
+#ifdef CONFIG_X86_32
+	u32 l, h;
+
+	/*
+	 * Bit 15 of Athlon specific MSR 15, needs to be 0
+	 * to enable SSE on Palomino/Morgan/Barton CPU's.
+	 * If the BIOS didn't enable it already, enable it here.
+	 */
+	if (c->x86_model >= 6 && c->x86_model <= 10) {
+		if (!cpu_has(c, X86_FEATURE_XMM)) {
+			printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
+			msr_clear_bit(MSR_K7_HWCR, 15);
+			set_cpu_cap(c, X86_FEATURE_XMM);
+		}
+	}
+
+	/*
+	 * It's been determined by AMD that Athlons since model 8 stepping 1
+	 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
+	 * As per AMD technical note 27212 0.2
+	 */
+	if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
+		rdmsr(MSR_K7_CLK_CTL, l, h);
+		if ((l & 0xfff00000) != 0x20000000) {
+			printk(KERN_INFO
+			    "CPU: CLK_CTL MSR was %x. Reprogramming to %x\n",
+					l, ((l & 0x000fffff)|0x20000000));
+			wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
+		}
+	}
+
+	set_cpu_cap(c, X86_FEATURE_K7);
+
 	/* calling is from identify_secondary_cpu() ? */
 	if (!c->cpu_index)
 		return;
@@ -207,7 +243,7 @@ static void amd_k7_smp_check(struct cpuinfo_x86 *c)
 	if (((c->x86_model == 6) && (c->x86_mask >= 2)) ||
 	    ((c->x86_model == 7) && (c->x86_mask >= 1)) ||
 	     (c->x86_model > 7))
-		if (cpu_has_mp)
+		if (cpu_has(c, X86_FEATURE_MP))
 			return;
 
 	/* If we get here, not a certified SMP capable AMD system. */
@@ -219,45 +255,8 @@ static void amd_k7_smp_check(struct cpuinfo_x86 *c)
 	WARN_ONCE(1, "WARNING: This combination of AMD"
 		" processors is not suitable for SMP.\n");
 	add_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_NOW_UNRELIABLE);
-}
-
-static void init_amd_k7(struct cpuinfo_x86 *c)
-{
-	u32 l, h;
-
-	/*
-	 * Bit 15 of Athlon specific MSR 15, needs to be 0
-	 * to enable SSE on Palomino/Morgan/Barton CPU's.
-	 * If the BIOS didn't enable it already, enable it here.
-	 */
-	if (c->x86_model >= 6 && c->x86_model <= 10) {
-		if (!cpu_has(c, X86_FEATURE_XMM)) {
-			printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
-			msr_clear_bit(MSR_K7_HWCR, 15);
-			set_cpu_cap(c, X86_FEATURE_XMM);
-		}
-	}
-
-	/*
-	 * It's been determined by AMD that Athlons since model 8 stepping 1
-	 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
-	 * As per AMD technical note 27212 0.2
-	 */
-	if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
-		rdmsr(MSR_K7_CLK_CTL, l, h);
-		if ((l & 0xfff00000) != 0x20000000) {
-			printk(KERN_INFO
-			    "CPU: CLK_CTL MSR was %x. Reprogramming to %x\n",
-					l, ((l & 0x000fffff)|0x20000000));
-			wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
-		}
-	}
-
-	set_cpu_cap(c, X86_FEATURE_K7);
-
-	amd_k7_smp_check(c);
-}
 #endif
+}
 
 #ifdef CONFIG_NUMA
 /*
@@ -446,6 +445,26 @@ static void early_init_amd_mc(struct cpuinfo_x86 *c)
 
 static void bsp_init_amd(struct cpuinfo_x86 *c)
 {
+
+#ifdef CONFIG_X86_64
+	if (c->x86 >= 0xf) {
+		unsigned long long tseg;
+
+		/*
+		 * Split up direct mapping around the TSEG SMM area.
+		 * Don't do it for gbpages because there seems very little
+		 * benefit in doing so.
+		 */
+		if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &tseg)) {
+			unsigned long pfn = tseg >> PAGE_SHIFT;
+
+			printk(KERN_DEBUG "tseg: %010llx\n", tseg);
+			if (pfn_range_is_mapped(pfn, pfn + 1))
+				set_memory_4k((unsigned long)__va(tseg), 1);
+		}
+	}
+#endif
+
 	if (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {
 
 		if (c->x86 > 0x10 ||
@@ -515,10 +534,100 @@ static const int amd_erratum_383[];
 static const int amd_erratum_400[];
 static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum);
 
+static void init_amd_k8(struct cpuinfo_x86 *c)
+{
+	u32 level;
+	u64 value;
+
+	/* On C+ stepping K8 rep microcode works well for copy/memset */
+	level = cpuid_eax(1);
+	if ((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)
+		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
+
+	/*
+	 * Some BIOSes incorrectly force this feature, but only K8 revision D
+	 * (model = 0x14) and later actually support it.
+	 * (AMD Erratum #110, docId: 25759).
+	 */
+	if (c->x86_model < 0x14 && cpu_has(c, X86_FEATURE_LAHF_LM)) {
+		clear_cpu_cap(c, X86_FEATURE_LAHF_LM);
+		if (!rdmsrl_amd_safe(0xc001100d, &value)) {
+			value &= ~BIT_64(32);
+			wrmsrl_amd_safe(0xc001100d, value);
+		}
+	}
+
+	if (!c->x86_model_id[0])
+		strcpy(c->x86_model_id, "Hammer");
+}
+
+static void init_amd_gh(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_64
+	/* do this for boot cpu */
+	if (c == &boot_cpu_data)
+		check_enable_amd_mmconf_dmi();
+
+	fam10h_check_enable_mmcfg();
+#endif
+
+	/*
+	 * Disable GART TLB Walk Errors on Fam10h. We do this here because this
+	 * is always needed when GART is enabled, even in a kernel which has no
+	 * MCE support built in. BIOS should disable GartTlbWlk Errors already.
+	 * If it doesn't, we do it here as suggested by the BKDG.
+	 *
+	 * Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=33012
+	 */
+	msr_set_bit(MSR_AMD64_MCx_MASK(4), 10);
+
+	/*
+	 * On family 10h BIOS may not have properly enabled WC+ support, causing
+	 * it to be converted to CD memtype. This may result in performance
+	 * degradation for certain nested-paging guests. Prevent this conversion
+	 * by clearing bit 24 in MSR_AMD64_BU_CFG2.
+	 *
+	 * NOTE: we want to use the _safe accessors so as not to #GP kvm
+	 * guests on older kvm hosts.
+	 */
+	msr_clear_bit(MSR_AMD64_BU_CFG2, 24);
+
+	if (cpu_has_amd_erratum(c, amd_erratum_383))
+		set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);
+}
+
+static void init_amd_bd(struct cpuinfo_x86 *c)
+{
+	u64 value;
+
+	/* re-enable TopologyExtensions if switched off by BIOS */
+	if ((c->x86_model >= 0x10) && (c->x86_model <= 0x1f) &&
+	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
+
+		if (msr_set_bit(0xc0011005, 54) > 0) {
+			rdmsrl(0xc0011005, value);
+			if (value & BIT_64(54)) {
+				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
+				pr_info(FW_INFO "CPU: Re-enabling disabled Topology Extensions Support.\n");
+			}
+		}
+	}
+
+	/*
+	 * The way access filter has a performance penalty on some workloads.
+	 * Disable it on the affected CPUs.
+	 */
+	if ((c->x86_model >= 0x02) && (c->x86_model < 0x20)) {
+		if (!rdmsrl_safe(0xc0011021, &value) && !(value & 0x1E)) {
+			value |= 0x1E;
+			wrmsrl_safe(0xc0011021, value);
+		}
+	}
+}
+
 static void init_amd(struct cpuinfo_x86 *c)
 {
 	u32 dummy;
-	unsigned long long value;
 
 #ifdef CONFIG_SMP
 	/*
@@ -540,100 +649,29 @@ static void init_amd(struct cpuinfo_x86 *c)
 	 */
 	clear_cpu_cap(c, 0*32+31);
 
-#ifdef CONFIG_X86_64
-	/* On C+ stepping K8 rep microcode works well for copy/memset */
-	if (c->x86 == 0xf) {
-		u32 level;
-
-		level = cpuid_eax(1);
-		if ((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)
-			set_cpu_cap(c, X86_FEATURE_REP_GOOD);
-
-		/*
-		 * Some BIOSes incorrectly force this feature, but only K8
-		 * revision D (model = 0x14) and later actually support it.
-		 * (AMD Erratum #110, docId: 25759).
-		 */
-		if (c->x86_model < 0x14 && cpu_has(c, X86_FEATURE_LAHF_LM)) {
-			clear_cpu_cap(c, X86_FEATURE_LAHF_LM);
-			if (!rdmsrl_amd_safe(0xc001100d, &value)) {
-				value &= ~(1ULL << 32);
-				wrmsrl_amd_safe(0xc001100d, value);
-			}
-		}
-
-	}
 	if (c->x86 >= 0x10)
 		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
 
 	/* get apicid instead of initial apic id from cpuid */
 	c->apicid = hard_smp_processor_id();
-#else
-
-	/*
-	 *	FIXME: We should handle the K5 here. Set up the write
-	 *	range and also turn on MSR 83 bits 4 and 31 (write alloc,
-	 *	no bus pipeline)
-	 */
-
-	switch (c->x86) {
-	case 4:
-		init_amd_k5(c);
-		break;
-	case 5:
-		init_amd_k6(c);
-		break;
-	case 6: /* An Athlon/Duron */
-		init_amd_k7(c);
-		break;
-	}
 
 	/* K6s reports MCEs but don't actually have all the MSRs */
 	if (c->x86 < 6)
 		clear_cpu_cap(c, X86_FEATURE_MCE);
-#endif
+
+	switch (c->x86) {
+	case 4:    init_amd_k5(c); break;
+	case 5:    init_amd_k6(c); break;
+	case 6:	   init_amd_k7(c); break;
+	case 0xf:  init_amd_k8(c); break;
+	case 0x10: init_amd_gh(c); break;
+	case 0x15: init_amd_bd(c); break;
+	}
 
 	/* Enable workaround for FXSAVE leak */
 	if (c->x86 >= 6)
 		set_cpu_bug(c, X86_BUG_FXSAVE_LEAK);
 
-	if (!c->x86_model_id[0]) {
-		switch (c->x86) {
-		case 0xf:
-			/* Should distinguish Models here, but this is only
-			   a fallback anyways. */
-			strcpy(c->x86_model_id, "Hammer");
-			break;
-		}
-	}
-
-	/* re-enable TopologyExtensions if switched off by BIOS */
-	if ((c->x86 == 0x15) &&
-	    (c->x86_model >= 0x10) && (c->x86_model <= 0x1f) &&
-	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
-
-		if (msr_set_bit(0xc0011005, 54) > 0) {
-			rdmsrl(0xc0011005, value);
-			if (value & BIT_64(54)) {
-				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
-				pr_info(FW_INFO "CPU: Re-enabling disabled Topology Extensions Support.\n");
-			}
-		}
-	}
-
-	/*
-	 * The way access filter has a performance penalty on some workloads.
-	 * Disable it on the affected CPUs.
-	 */
-	if ((c->x86 == 0x15) &&
-	    (c->x86_model >= 0x02) && (c->x86_model < 0x20)) {
-
-		if (!rdmsrl_safe(0xc0011021, &value) && !(value & 0x1E)) {
-			value |= 0x1E;
-			wrmsrl_safe(0xc0011021, value);
-		}
-	}
-
 	cpu_detect_cache_sizes(c);
 
 	/* Multi core CPU? */
@@ -656,33 +694,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
 	}
 
-#ifdef CONFIG_X86_64
-	if (c->x86 == 0x10) {
-		/* do this for boot cpu */
-		if (c == &boot_cpu_data)
-			check_enable_amd_mmconf_dmi();
-
-		fam10h_check_enable_mmcfg();
-	}
-
-	if (c == &boot_cpu_data && c->x86 >= 0xf) {
-		unsigned long long tseg;
-
-		/*
-		 * Split up direct mapping around the TSEG SMM area.
-		 * Don't do it for gbpages because there seems very little
-		 * benefit in doing so.
-		 */
-		if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &tseg)) {
-			unsigned long pfn = tseg >> PAGE_SHIFT;
-
-			printk(KERN_DEBUG "tseg: %010llx\n", tseg);
-			if (pfn_range_is_mapped(pfn, pfn + 1))
-				set_memory_4k((unsigned long)__va(tseg), 1);
-		}
-	}
-#endif
-
 	/*
 	 * Family 0x12 and above processors have APIC timer
 	 * running in deep C states.
@@ -690,34 +701,6 @@ static void init_amd(struct cpuinfo_x86 *c)
 	if (c->x86 > 0x11)
 		set_cpu_cap(c, X86_FEATURE_ARAT);
 
-	if (c->x86 == 0x10) {
-		/*
-		 * Disable GART TLB Walk Errors on Fam10h. We do this here
-		 * because this is always needed when GART is enabled, even in a
-		 * kernel which has no MCE support built in.
-		 * BIOS should disable GartTlbWlk Errors already. If
-		 * it doesn't, do it here as suggested by the BKDG.
-		 *
-		 * Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=33012
-		 */
-		msr_set_bit(MSR_AMD64_MCx_MASK(4), 10);
-
-		/*
-		 * On family 10h BIOS may not have properly enabled WC+ support,
-		 * causing it to be converted to CD memtype. This may result in
-		 * performance degradation for certain nested-paging guests.
-		 * Prevent this conversion by clearing bit 24 in
-		 * MSR_AMD64_BU_CFG2.
-		 *
-		 * NOTE: we want to use the _safe accessors so as not to #GP kvm
-		 * guests on older kvm hosts.
-		 */
-		msr_clear_bit(MSR_AMD64_BU_CFG2, 24);
-
-		if (cpu_has_amd_erratum(c, amd_erratum_383))
-			set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);
-	}
-
 	if (cpu_has_amd_erratum(c, amd_erratum_400))
 		set_cpu_bug(c, X86_BUG_AMD_APIC_C1E);
 

commit 9b13a93df267af681a66a6a738bf1af10102da7d
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jun 18 00:06:23 2014 +0200

    x86, cpufeature: Convert more "features" to bugs
    
    X86_FEATURE_FXSAVE_LEAK, X86_FEATURE_11AP and
    X86_FEATURE_CLFLUSH_MONITOR are not really features but synthetic bits
    we use for applying different bug workarounds. Call them what they
    really are, and make sure they get the proper cross-CPU behavior (OR
    rather than AND).
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1403042783-23278-1-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ce8b8ff0e0ef..8714a78414bf 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -595,7 +595,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 
 	/* Enable workaround for FXSAVE leak */
 	if (c->x86 >= 6)
-		set_cpu_cap(c, X86_FEATURE_FXSAVE_LEAK);
+		set_cpu_bug(c, X86_BUG_FXSAVE_LEAK);
 
 	if (!c->x86_model_id[0]) {
 		switch (c->x86) {

commit 8c90487cdc64847b4fdd812ab3047f426fec4d13
Author: Dave Jones <davej@redhat.com>
Date:   Wed Feb 26 10:49:49 2014 -0500

    Rename TAINT_UNSAFE_SMP to TAINT_CPU_OUT_OF_SPEC
    
    Rename TAINT_UNSAFE_SMP to TAINT_CPU_OUT_OF_SPEC, so we can repurpose
    the flag to encompass a wider range of pushing the CPU beyond its
    warrany.
    
    Signed-off-by: Dave Jones <davej@fedoraproject.org>
    Link: http://lkml.kernel.org/r/20140226154949.GA770@redhat.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b85e43a5a462..ce8b8ff0e0ef 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -218,7 +218,7 @@ static void amd_k7_smp_check(struct cpuinfo_x86 *c)
 	 */
 	WARN_ONCE(1, "WARNING: This combination of AMD"
 		" processors is not suitable for SMP.\n");
-	add_taint(TAINT_UNSAFE_SMP, LOCKDEP_NOW_UNRELIABLE);
+	add_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_NOW_UNRELIABLE);
 }
 
 static void init_amd_k7(struct cpuinfo_x86 *c)

commit 8f86a7373a1c8ee52d3cc64adf7f2ace13fd24ed
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Mar 9 18:05:24 2014 +0100

    x86, AMD: Convert to the new bit access MSR accessors
    
    ... and save us a bunch of code.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1394384725-10796-3-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c67ffa686064..b85e43a5a462 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -233,9 +233,7 @@ static void init_amd_k7(struct cpuinfo_x86 *c)
 	if (c->x86_model >= 6 && c->x86_model <= 10) {
 		if (!cpu_has(c, X86_FEATURE_XMM)) {
 			printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
-			rdmsr(MSR_K7_HWCR, l, h);
-			l &= ~0x00008000;
-			wrmsr(MSR_K7_HWCR, l, h);
+			msr_clear_bit(MSR_K7_HWCR, 15);
 			set_cpu_cap(c, X86_FEATURE_XMM);
 		}
 	}
@@ -509,14 +507,8 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 #endif
 
 	/* F16h erratum 793, CVE-2013-6885 */
-	if (c->x86 == 0x16 && c->x86_model <= 0xf) {
-		u64 val;
-
-		rdmsrl(MSR_AMD64_LS_CFG, val);
-		if (!(val & BIT(15)))
-			wrmsrl(MSR_AMD64_LS_CFG, val | BIT(15));
-	}
-
+	if (c->x86 == 0x16 && c->x86_model <= 0xf)
+		msr_set_bit(MSR_AMD64_LS_CFG, 15);
 }
 
 static const int amd_erratum_383[];
@@ -536,11 +528,8 @@ static void init_amd(struct cpuinfo_x86 *c)
 	 * Errata 63 for SH-B3 steppings
 	 * Errata 122 for all steppings (F+ have it disabled by default)
 	 */
-	if (c->x86 == 0xf) {
-		rdmsrl(MSR_K7_HWCR, value);
-		value |= 1 << 6;
-		wrmsrl(MSR_K7_HWCR, value);
-	}
+	if (c->x86 == 0xf)
+		msr_set_bit(MSR_K7_HWCR, 6);
 #endif
 
 	early_init_amd(c);
@@ -623,14 +612,11 @@ static void init_amd(struct cpuinfo_x86 *c)
 	    (c->x86_model >= 0x10) && (c->x86_model <= 0x1f) &&
 	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
 
-		if (!rdmsrl_safe(0xc0011005, &value)) {
-			value |= 1ULL << 54;
-			wrmsrl_safe(0xc0011005, value);
+		if (msr_set_bit(0xc0011005, 54) > 0) {
 			rdmsrl(0xc0011005, value);
-			if (value & (1ULL << 54)) {
+			if (value & BIT_64(54)) {
 				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
-				printk(KERN_INFO FW_INFO "CPU: Re-enabling "
-				  "disabled Topology Extensions Support\n");
+				pr_info(FW_INFO "CPU: Re-enabling disabled Topology Extensions Support.\n");
 			}
 		}
 	}
@@ -709,19 +695,12 @@ static void init_amd(struct cpuinfo_x86 *c)
 		 * Disable GART TLB Walk Errors on Fam10h. We do this here
 		 * because this is always needed when GART is enabled, even in a
 		 * kernel which has no MCE support built in.
-		 * BIOS should disable GartTlbWlk Errors themself. If
-		 * it doesn't do it here as suggested by the BKDG.
+		 * BIOS should disable GartTlbWlk Errors already. If
+		 * it doesn't, do it here as suggested by the BKDG.
 		 *
 		 * Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=33012
 		 */
-		u64 mask;
-		int err;
-
-		err = rdmsrl_safe(MSR_AMD64_MCx_MASK(4), &mask);
-		if (err == 0) {
-			mask |= (1 << 10);
-			wrmsrl_safe(MSR_AMD64_MCx_MASK(4), mask);
-		}
+		msr_set_bit(MSR_AMD64_MCx_MASK(4), 10);
 
 		/*
 		 * On family 10h BIOS may not have properly enabled WC+ support,
@@ -733,10 +712,7 @@ static void init_amd(struct cpuinfo_x86 *c)
 		 * NOTE: we want to use the _safe accessors so as not to #GP kvm
 		 * guests on older kvm hosts.
 		 */
-
-		rdmsrl_safe(MSR_AMD64_BU_CFG2, &value);
-		value &= ~(1ULL << 24);
-		wrmsrl_safe(MSR_AMD64_BU_CFG2, value);
+		msr_clear_bit(MSR_AMD64_BU_CFG2, 24);
 
 		if (cpu_has_amd_erratum(c, amd_erratum_383))
 			set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);

commit 2b45e0f9f34f718725e093f4e335600811d7105a
Merge: a85eba881463 15c81026204d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 25 09:16:14 2014 +0100

    Merge branch 'linus' into x86/urgent
    
    Merge in the x86 changes to apply a fix.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b9a3b4c976c1209957326537ad5c0bb633dfd764
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jan 21 14:33:22 2014 -0800

    mm, x86: Revisit tlb_flushall_shift tuning for page flushes except on IvyBridge
    
    There was a large ebizzy performance regression that was
    bisected to commit 611ae8e3 (x86/tlb: enable tlb flush range
    support for x86).  The problem was related to the
    tlb_flushall_shift tuning for IvyBridge which was altered.  The
    problem is that it is not clear if the tuning values for each
    CPU family is correct as the methodology used to tune the values
    is unclear.
    
    This patch uses a conservative tlb_flushall_shift value for all
    CPU families except IvyBridge so the decision can be revisited
    if any regression is found as a result of this change.
    IvyBridge is an exception as testing with one methodology
    determined that the value of 2 is acceptable.  Details are in
    the changelog for the patch "x86: mm: Change tlb_flushall_shift
    for IvyBridge".
    
    One important aspect of this to watch out for is Xen.  The
    original commit log mentioned large performance gains on Xen.
    It's possible Xen is more sensitive to this value if it flushes
    small ranges of pages more frequently than workloads on bare
    metal typically do.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-dyzMww3fqugnhbhgo6Gxmtkw@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 59bfebc8c805..96abccaada33 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -768,10 +768,7 @@ static unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 
 static void cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)
 {
-	tlb_flushall_shift = 5;
-
-	if (c->x86 <= 0x11)
-		tlb_flushall_shift = 4;
+	tlb_flushall_shift = 6;
 }
 
 static void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)

commit 7fe67a1180db49d41a3f764c379a08f8e31580ec
Merge: fab5669d5562 da6139e49c7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:11:41 2014 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull leftover x86 fixes from Ingo Molnar:
     "Two leftover fixes that did not make it into v3.13"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Add check for number of available vectors before CPU down
      x86, cpu, amd: Add workaround for family 16h, erratum 793

commit 2a0fede97fd52a5c9789d1d54ebd3b46878151c3
Merge: 06bc0f4a2e7f d139336700a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 12:03:57 2014 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cpu, amd: Fix a shadowed variable situation
      um, x86: Fix vDSO build
      x86: Delete non-required instances of include <linux/init.h>
      x86, realmode: Pointer walk cleanups, pull out invariant use of __pa()
      x86/traps: Clean up error exception handler definitions

commit d139336700a5f3a560da235e4dfcd286773025d4
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jan 15 12:52:15 2014 +0100

    x86, cpu, amd: Fix a shadowed variable situation
    
    Having u32 and struct cpuinfo_x86 * by the same name is not very smart,
    although it was ok in this case due to the limited scope of u32 c and it
    being used only once in there.
    
    Fix this.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1389786735-16751-1-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 39bc78dad377..e5647ab5fc23 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -789,14 +789,10 @@ static void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 	}
 
 	/* Handle DTLB 2M and 4M sizes, fall back to L1 if L2 is disabled */
-	if (!((eax >> 16) & mask)) {
-		u32 a, b, c, d;
-
-		cpuid(0x80000005, &a, &b, &c, &d);
-		tlb_lld_2m[ENTRIES] = (a >> 16) & 0xff;
-	} else {
+	if (!((eax >> 16) & mask))
+		tlb_lld_2m[ENTRIES] = (cpuid_eax(0x80000005) >> 16) & 0xff;
+	else
 		tlb_lld_2m[ENTRIES] = (eax >> 16) & mask;
-	}
 
 	/* a 4M entry uses two 2M entries */
 	tlb_lld_4m[ENTRIES] = tlb_lld_2m[ENTRIES] >> 1;

commit 3b56496865f9f7d9bcb2f93b44c63f274f08e3b6
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Jan 15 00:07:11 2014 +0100

    x86, cpu, amd: Add workaround for family 16h, erratum 793
    
    This adds the workaround for erratum 793 as a precaution in case not
    every BIOS implements it.  This addresses CVE-2013-6885.
    
    Erratum text:
    
    [Revision Guide for AMD Family 16h Models 00h-0Fh Processors,
    document 51810 Rev. 3.04 November 2013]
    
    793 Specific Combination of Writes to Write Combined Memory Types and
    Locked Instructions May Cause Core Hang
    
    Description
    
    Under a highly specific and detailed set of internal timing
    conditions, a locked instruction may trigger a timing sequence whereby
    the write to a write combined memory type is not flushed, causing the
    locked instruction to stall indefinitely.
    
    Potential Effect on System
    
    Processor core hang.
    
    Suggested Workaround
    
    BIOS should set MSR
    C001_1020[15] = 1b.
    
    Fix Planned
    
    No fix planned
    
    [ hpa: updated description, fixed typo in MSR name ]
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/20140114230711.GS29865@pd.tnic
    Tested-by: Aravind Gopalakrishnan <aravind.gopalakrishnan@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bca023bdd6b2..59bfebc8c805 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -508,6 +508,16 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
 	}
 #endif
+
+	/* F16h erratum 793, CVE-2013-6885 */
+	if (c->x86 == 0x16 && c->x86_model <= 0xf) {
+		u64 val;
+
+		rdmsrl(MSR_AMD64_LS_CFG, val);
+		if (!(val & BIT(15)))
+			wrmsrl(MSR_AMD64_LS_CFG, val | BIT(15));
+	}
+
 }
 
 static const int amd_erratum_383[];

commit 35af99e646c7f7ea46dc2977601e9e71a51dadd5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 28 19:38:42 2013 +0100

    sched/clock, x86: Use a static_key for sched_clock_stable
    
    In order to avoid the runtime condition and variable load turn
    sched_clock_stable into a static_key.
    
    Also provide a shorter implementation of local_clock() and
    cpu_clock(int) when sched_clock_stable==1.
    
                            MAINLINE   PRE       POST
    
        sched_clock_stable: 1          1         1
        (cold) sched_clock: 329841     221876    215295
        (cold) local_clock: 301773     234692    220773
        (warm) sched_clock: 38375      25602     25659
        (warm) local_clock: 100371     33265     27242
        (warm) rdtsc:       27340      24214     24208
        sched_clock_stable: 0          0         0
        (cold) sched_clock: 382634     235941    237019
        (cold) local_clock: 396890     297017    294819
        (warm) sched_clock: 38194      25233     25609
        (warm) local_clock: 143452     71234     71232
        (warm) rdtsc:       27345      24245     24243
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-eummbdechzz37mwmpags1gjr@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bca023bdd6b2..8bc79cddd9a2 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -487,7 +487,7 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
 		if (!check_tsc_unstable())
-			sched_clock_stable = 1;
+			set_sched_clock_stable();
 	}
 
 #ifdef CONFIG_X86_64

commit 663b55b9b39fa9c848cca273ca4e12bf29b32c71
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Jan 6 19:20:26 2014 -0500

    x86: Delete non-required instances of include <linux/init.h>
    
    None of these files are actually using any __init type directives
    and hence don't need to include <linux/init.h>.  Most are just a
    left over from __devinit and __cpuinit removal, or simply due to
    code getting copied from one driver to the next.
    
    [ hpa: undid incorrect removal from arch/x86/kernel/head_32.S ]
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Link: http://lkml.kernel.org/r/1389054026-12947-1-git-send-email-paul.gortmaker@windriver.com
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bca023bdd6b2..39bc78dad377 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -1,5 +1,4 @@
 #include <linux/export.h>
-#include <linux/init.h>
 #include <linux/bitops.h>
 #include <linux/elf.h>
 #include <linux/mm.h>

commit 9073e1a804c3096eda84ee7cbf11d1f174236c75
Merge: 4937e2a6f939 2bb9936beac2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 15 16:47:22 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial tree updates from Jiri Kosina:
     "Usual earth-shaking, news-breaking, rocket science pile from
      trivial.git"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (23 commits)
      doc: usb: Fix typo in Documentation/usb/gadget_configs.txt
      doc: add missing files to timers/00-INDEX
      timekeeping: Fix some trivial typos in comments
      mm: Fix some trivial typos in comments
      irq: Fix some trivial typos in comments
      NUMA: fix typos in Kconfig help text
      mm: update 00-INDEX
      doc: Documentation/DMA-attributes.txt fix typo
      DRM: comment: `halve' -> `half'
      Docs: Kconfig: `devlopers' -> `developers'
      doc: typo on word accounting in kprobes.c in mutliple architectures
      treewide: fix "usefull" typo
      treewide: fix "distingush" typo
      mm/Kconfig: Grammar s/an/a/
      kexec: Typo s/the/then/
      Documentation/kvm: Update cpuid documentation for steal time and pv eoi
      treewide: Fix common typo in "identify"
      __page_to_pfn: Fix typo in comment
      Correct some typos for word frequency
      clk: fixed-factor: Fix a trivial typo
      ...

commit 09dc68d958c67c76cf672ec78b7391af453010f8
Author: Jan Beulich <JBeulich@suse.com>
Date:   Mon Oct 21 09:35:20 2013 +0100

    x86/cpu: Track legacy CPU model data only on 32-bit kernels
    
    struct cpu_dev's c_models is only ever set inside CONFIG_X86_32
    conditionals (or code that's being built for 32-bit only), so
    there's no use of reserving the (empty) space for the model
    names in a 64-bit kernel.
    
    Similarly, c_size_cache is only used in the #else of a
    CONFIG_X86_64 conditional, so reserving space for (and in one
    case even initializing) that field is pointless for 64-bit
    kernels too.
    
    While moving both fields to the end of the structure, I also
    noticed that:
    
     - the c_models array size was one too small, potentially causing
       table_lookup_model() to return garbage on Intel CPUs (intel.c's
       instance was lacking the sentinel with family being zero), so the
       patch bumps that by one,
    
     - c_models' vendor sub-field was unused (and anyway redundant
       with the base structure's c_x86_vendor field), so the patch deletes it.
    
    Also rename the legacy fields so that their legacy nature stands out
    and comment their declarations.
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Link: http://lkml.kernel.org/r/5265036802000078000FC4DB@nat28.tlf.novell.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 903a264af981..3daece79a142 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -823,8 +823,8 @@ static const struct cpu_dev amd_cpu_dev = {
 	.c_vendor	= "AMD",
 	.c_ident	= { "AuthenticAMD" },
 #ifdef CONFIG_X86_32
-	.c_models = {
-		{ .vendor = X86_VENDOR_AMD, .family = 4, .model_names =
+	.legacy_models = {
+		{ .family = 4, .model_names =
 		  {
 			  [3] = "486 DX/2",
 			  [7] = "486 DX/2-WB",
@@ -835,7 +835,7 @@ static const struct cpu_dev amd_cpu_dev = {
 		  }
 		},
 	},
-	.c_size_cache	= amd_size_cache,
+	.legacy_cache_size = amd_size_cache,
 #endif
 	.c_early_init   = early_init_amd,
 	.c_detect_tlb	= cpu_detect_tlb_amd,

commit aa5e5dc2a8878ecf1a94819d889939023fd576c9
Author: Michael Opdenacker <michael.opdenacker@free-electrons.com>
Date:   Wed Sep 18 06:00:43 2013 +0200

    treewide: fix "distingush" typo
    
    Signed-off-by: Michael Opdenacker <michael.opdenacker@free-electrons.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 903a264af981..3538a1b0aeac 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -339,7 +339,7 @@ static void amd_get_topology(struct cpuinfo_x86 *c)
 #endif
 
 /*
- * On a AMD dual core setup the lower bits of the APIC id distingush the cores.
+ * On a AMD dual core setup the lower bits of the APIC id distinguish the cores.
  * Assumes number of cores is a power of two.
  */
 static void amd_detect_cmp(struct cpuinfo_x86 *c)

commit 2a475501b81f06f64c474cfad66f8807294b4534
Merge: 3d7e5fc37f91 eb86b5fd505c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 08:42:44 2013 -0700

    Merge branch 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/asmlinkage changes from Ingo Molnar:
     "As a preparation for Andi Kleen's LTO patchset (link time
      optimizations using GCC's -flto which build time optimization has
      steadily increased in quality over the past few years and might
      eventually be usable for the kernel too) this tree includes a handful
      of preparatory patches that make function calling convention
      annotations consistent again:
    
       - Mark every function without arguments (or 64bit only) that is used
         by assembly code with asmlinkage()
    
       - Mark every function with parameters or variables that is used by
         assembly code as __visible.
    
      For the vanilla kernel this has documentation, consistency and
      debuggability advantages, for the time being"
    
    * 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/asmlinkage: Fix warning in xen asmlinkage change
      x86, asmlinkage, vdso: Mark vdso variables __visible
      x86, asmlinkage, power: Make various symbols used by the suspend asm code visible
      x86, asmlinkage: Make dump_stack visible
      x86, asmlinkage: Make 64bit checksum functions visible
      x86, asmlinkage, paravirt: Add __visible/asmlinkage to xen paravirt ops
      x86, asmlinkage, apm: Make APM data structure used from assembler visible
      x86, asmlinkage: Make syscall tables visible
      x86, asmlinkage: Make several variables used from assembler/linker script visible
      x86, asmlinkage: Make kprobes code visible and fix assembler code
      x86, asmlinkage: Make various syscalls asmlinkage
      x86, asmlinkage: Make 32bit/64bit __switch_to visible
      x86, asmlinkage: Make _*_start_kernel visible
      x86, asmlinkage: Make all interrupt handlers asmlinkage / __visible
      x86, asmlinkage: Change dotraplinkage into __visible on 32bit
      x86: Fix sys_call_table type in asm/syscall.h

commit 8c6b79bb1211d91fb31bcbc2a1eea8d6963d3ad9
Author: Torsten Kaiser <just.for.lkml@googlemail.com>
Date:   Tue Jul 23 19:40:49 2013 +0200

    x86, microcode, AMD: Make cpu_has_amd_erratum() use the correct struct cpuinfo_x86
    
    cpu_has_amd_erratum() is buggy, because it uses the per-cpu cpu_info
    before it is filled by smp_store_boot_cpu_info() / smp_store_cpu_info().
    
    If early microcode loading is enabled its collect_cpu_info_amd_early()
    will fill ->x86 and so the fallback to boot_cpu_data is not used. But
    ->x86_vendor was not filled and is still X86_VENDOR_INTEL resulting in
    no errata fixes getting applied and my system hangs on boot.
    
    Using cpu_info in cpu_has_amd_erratum() is wrong anyway: its only
    caller init_amd() will have a struct cpuinfo_x86 as parameter and the
    set_cpu_bug() that is controlled by cpu_has_amd_erratum() also only uses
    that struct.
    
    So pass the struct cpuinfo_x86 from init_amd() to cpu_has_amd_erratum()
    and the broken fallback can be dropped.
    
    [ Boris: Drop WARN_ON() since we're called only from init_amd() ]
    
    Signed-off-by: Torsten Kaiser <just.for.lkml@googlemail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f654ecefea5b..08a089043ccf 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -512,7 +512,7 @@ static void early_init_amd(struct cpuinfo_x86 *c)
 
 static const int amd_erratum_383[];
 static const int amd_erratum_400[];
-static bool cpu_has_amd_erratum(const int *erratum);
+static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum);
 
 static void init_amd(struct cpuinfo_x86 *c)
 {
@@ -729,11 +729,11 @@ static void init_amd(struct cpuinfo_x86 *c)
 		value &= ~(1ULL << 24);
 		wrmsrl_safe(MSR_AMD64_BU_CFG2, value);
 
-		if (cpu_has_amd_erratum(amd_erratum_383))
+		if (cpu_has_amd_erratum(c, amd_erratum_383))
 			set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);
 	}
 
-	if (cpu_has_amd_erratum(amd_erratum_400))
+	if (cpu_has_amd_erratum(c, amd_erratum_400))
 		set_cpu_bug(c, X86_BUG_AMD_APIC_C1E);
 
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
@@ -878,23 +878,13 @@ static const int amd_erratum_400[] =
 static const int amd_erratum_383[] =
 	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
 
-static bool cpu_has_amd_erratum(const int *erratum)
+
+static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
 {
-	struct cpuinfo_x86 *cpu = __this_cpu_ptr(&cpu_info);
 	int osvw_id = *erratum++;
 	u32 range;
 	u32 ms;
 
-	/*
-	 * If called early enough that current_cpu_data hasn't been initialized
-	 * yet, fall back to boot_cpu_data.
-	 */
-	if (cpu->x86 == 0)
-		cpu = &boot_cpu_data;
-
-	if (cpu->x86_vendor != X86_VENDOR_AMD)
-		return false;
-
 	if (osvw_id >= 0 && osvw_id < 65536 &&
 	    cpu_has(cpu, X86_FEATURE_OSVW)) {
 		u64 osvw_len;

commit 277d5b40b7bf495d2d4193746181b17dd98441b2
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:43 2013 -0700

    x86, asmlinkage: Make several variables used from assembler/linker script visible
    
    Plus one function, load_gs_index().
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-10-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f654ecefea5b..466e3d15de12 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -66,8 +66,8 @@ static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
  *	performance at the same time..
  */
 
-extern void vide(void);
-__asm__(".align 4\nvide: ret");
+extern __visible void vide(void);
+__asm__(".globl vide\n\t.align 4\nvide: ret");
 
 static void init_amd_k5(struct cpuinfo_x86 *c)
 {

commit 148f9bb87745ed45f7a11b2cbd3bc0f017d5d257
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 18:23:59 2013 -0400

    x86: delete __cpuinit usage from all x86 files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/x86 uses of the __cpuinit macros from
    all C files.  x86 only had the one __CPUINIT used in assembly files,
    and it wasn't paired off with a .previous or a __FINIT, so we can
    delete it directly w/o any corresponding additional change there.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c587a8757227..f654ecefea5b 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -69,7 +69,7 @@ static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
 extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
-static void __cpuinit init_amd_k5(struct cpuinfo_x86 *c)
+static void init_amd_k5(struct cpuinfo_x86 *c)
 {
 /*
  * General Systems BIOSen alias the cpu frequency registers
@@ -87,7 +87,7 @@ static void __cpuinit init_amd_k5(struct cpuinfo_x86 *c)
 }
 
 
-static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
+static void init_amd_k6(struct cpuinfo_x86 *c)
 {
 	u32 l, h;
 	int mbytes = get_num_physpages() >> (20-PAGE_SHIFT);
@@ -179,7 +179,7 @@ static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
 	}
 }
 
-static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
+static void amd_k7_smp_check(struct cpuinfo_x86 *c)
 {
 	/* calling is from identify_secondary_cpu() ? */
 	if (!c->cpu_index)
@@ -222,7 +222,7 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 	add_taint(TAINT_UNSAFE_SMP, LOCKDEP_NOW_UNRELIABLE);
 }
 
-static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
+static void init_amd_k7(struct cpuinfo_x86 *c)
 {
 	u32 l, h;
 
@@ -267,7 +267,7 @@ static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
  * To workaround broken NUMA config.  Read the comment in
  * srat_detect_node().
  */
-static int __cpuinit nearby_node(int apicid)
+static int nearby_node(int apicid)
 {
 	int i, node;
 
@@ -292,7 +292,7 @@ static int __cpuinit nearby_node(int apicid)
  * (2) AMD processors supporting compute units
  */
 #ifdef CONFIG_X86_HT
-static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
+static void amd_get_topology(struct cpuinfo_x86 *c)
 {
 	u32 nodes, cores_per_cu = 1;
 	u8 node_id;
@@ -342,7 +342,7 @@ static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
  * On a AMD dual core setup the lower bits of the APIC id distingush the cores.
  * Assumes number of cores is a power of two.
  */
-static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
+static void amd_detect_cmp(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_HT
 	unsigned bits;
@@ -369,7 +369,7 @@ u16 amd_get_nb_id(int cpu)
 }
 EXPORT_SYMBOL_GPL(amd_get_nb_id);
 
-static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
+static void srat_detect_node(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_NUMA
 	int cpu = smp_processor_id();
@@ -421,7 +421,7 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void __cpuinit early_init_amd_mc(struct cpuinfo_x86 *c)
+static void early_init_amd_mc(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_HT
 	unsigned bits, ecx;
@@ -447,7 +447,7 @@ static void __cpuinit early_init_amd_mc(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void __cpuinit bsp_init_amd(struct cpuinfo_x86 *c)
+static void bsp_init_amd(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {
 
@@ -475,7 +475,7 @@ static void __cpuinit bsp_init_amd(struct cpuinfo_x86 *c)
 	}
 }
 
-static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
+static void early_init_amd(struct cpuinfo_x86 *c)
 {
 	early_init_amd_mc(c);
 
@@ -514,7 +514,7 @@ static const int amd_erratum_383[];
 static const int amd_erratum_400[];
 static bool cpu_has_amd_erratum(const int *erratum);
 
-static void __cpuinit init_amd(struct cpuinfo_x86 *c)
+static void init_amd(struct cpuinfo_x86 *c)
 {
 	u32 dummy;
 	unsigned long long value;
@@ -740,8 +740,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 }
 
 #ifdef CONFIG_X86_32
-static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c,
-							unsigned int size)
+static unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 {
 	/* AMD errata T13 (order #21922) */
 	if ((c->x86 == 6)) {
@@ -757,7 +756,7 @@ static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c,
 }
 #endif
 
-static void __cpuinit cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)
+static void cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)
 {
 	tlb_flushall_shift = 5;
 
@@ -765,7 +764,7 @@ static void __cpuinit cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)
 		tlb_flushall_shift = 4;
 }
 
-static void __cpuinit cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
+static void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 {
 	u32 ebx, eax, ecx, edx;
 	u16 mask = 0xfff;
@@ -820,7 +819,7 @@ static void __cpuinit cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 	cpu_set_tlb_flushall_shift(c);
 }
 
-static const struct cpu_dev __cpuinitconst amd_cpu_dev = {
+static const struct cpu_dev amd_cpu_dev = {
 	.c_vendor	= "AMD",
 	.c_ident	= { "AuthenticAMD" },
 #ifdef CONFIG_X86_32

commit 46a841329a6cd6298e131afd82e7d58130b19025
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:04:19 2013 -0700

    mm/x86: prepare for removing num_physpages and simplify mem_init()
    
    Prepare for removing num_physpages and simplify mem_init().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 5013a48d1aff..c587a8757227 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -90,7 +90,7 @@ static void __cpuinit init_amd_k5(struct cpuinfo_x86 *c)
 static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
 {
 	u32 l, h;
-	int mbytes = num_physpages >> (20-PAGE_SHIFT);
+	int mbytes = get_num_physpages() >> (20-PAGE_SHIFT);
 
 	if (c->x86_model < 6) {
 		/* Based on AMD doc 20734R - June 2000 */

commit 1077c932db63ecc571c31df1c24d4a44e30928e5
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 8 17:57:46 2013 +0200

    x86, CPU, AMD: Drop useless label
    
    All we want to do is return from this function so stop jumping around
    like a flea for no good reason.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1365436666-9837-5-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index cea02d703bca..5013a48d1aff 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -192,11 +192,11 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 	/* Athlon 660/661 is valid. */
 	if ((c->x86_model == 6) && ((c->x86_mask == 0) ||
 	    (c->x86_mask == 1)))
-		goto valid_k7;
+		return;
 
 	/* Duron 670 is valid */
 	if ((c->x86_model == 7) && (c->x86_mask == 0))
-		goto valid_k7;
+		return;
 
 	/*
 	 * Athlon 662, Duron 671, and Athlon >model 7 have capability
@@ -209,7 +209,7 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 	    ((c->x86_model == 7) && (c->x86_mask >= 1)) ||
 	     (c->x86_model > 7))
 		if (cpu_has_mp)
-			goto valid_k7;
+			return;
 
 	/* If we get here, not a certified SMP capable AMD system. */
 
@@ -220,9 +220,6 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 	WARN_ONCE(1, "WARNING: This combination of AMD"
 		" processors is not suitable for SMP.\n");
 	add_taint(TAINT_UNSAFE_SMP, LOCKDEP_NOW_UNRELIABLE);
-
-valid_k7:
-	;
 }
 
 static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)

commit 682469a5db6fade318a72406935b5000186e5643
Author: Borislav Petkov <bp@suse.de>
Date:   Mon Apr 8 17:57:45 2013 +0200

    x86, AMD: Correct {rd,wr}msr_amd_safe warnings
    
    The idea with those routines is to slowly phase them out and not call
    them on anything else besides K8. They even have a check for that which,
    when called too early, fails. Let me explain:
    
    It gets the cpuinfo_x86 pointer from the per_cpu array and when this
    happens for cpu0, before its boot_cpu_data has been copied back to the
    per_cpu array in smp_store_boot_cpu_info(), we get an empty struct and
    thus the check fails.
    
    Use boot_cpu_data directly instead.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1365436666-9837-4-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 9a2a71669c5d..cea02d703bca 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -20,11 +20,11 @@
 
 static inline int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)
 {
-	struct cpuinfo_x86 *c = &cpu_data(smp_processor_id());
 	u32 gprs[8] = { 0 };
 	int err;
 
-	WARN_ONCE((c->x86 != 0xf), "%s should only be used on K8!\n", __func__);
+	WARN_ONCE((boot_cpu_data.x86 != 0xf),
+		  "%s should only be used on K8!\n", __func__);
 
 	gprs[1] = msr;
 	gprs[7] = 0x9c5a203a;
@@ -38,10 +38,10 @@ static inline int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)
 
 static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
 {
-	struct cpuinfo_x86 *c = &cpu_data(smp_processor_id());
 	u32 gprs[8] = { 0 };
 
-	WARN_ONCE((c->x86 != 0xf), "%s should only be used on K8!\n", __func__);
+	WARN_ONCE((boot_cpu_data.x86 != 0xf),
+		  "%s should only be used on K8!\n", __func__);
 
 	gprs[0] = (u32)val;
 	gprs[1] = msr;

commit 7d7dc116e56c8a1ba4beb36d06a77a48fe5f750b
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:28 2013 +0100

    x86, cpu: Convert AMD Erratum 400
    
    Convert AMD erratum 400 to the bug infrastructure. Then, retract all
    exports for modules since they're not needed now and make the AMD
    erratum checking machinery local to amd.c. Use forward declarations to
    avoid shuffling too much code around needlessly.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-7-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 85f84e13d2dd..9a2a71669c5d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -514,6 +514,8 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 }
 
 static const int amd_erratum_383[];
+static const int amd_erratum_400[];
+static bool cpu_has_amd_erratum(const int *erratum);
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 {
@@ -734,6 +736,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 			set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);
 	}
 
+	if (cpu_has_amd_erratum(amd_erratum_400))
+		set_cpu_bug(c, X86_BUG_AMD_APIC_C1E);
+
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
 }
 
@@ -852,8 +857,7 @@ cpu_dev_register(amd_cpu_dev);
  * AMD_OSVW_ERRATUM() macros. The latter is intended for newer errata that
  * have an OSVW id assigned, which it takes as first argument. Both take a
  * variable number of family-specific model-stepping ranges created by
- * AMD_MODEL_RANGE(). Each erratum also has to be declared as extern const
- * int[] in arch/x86/include/asm/processor.h.
+ * AMD_MODEL_RANGE().
  *
  * Example:
  *
@@ -863,15 +867,22 @@ cpu_dev_register(amd_cpu_dev);
  *			   AMD_MODEL_RANGE(0x10, 0x9, 0x0, 0x9, 0x0));
  */
 
-const int amd_erratum_400[] =
+#define AMD_LEGACY_ERRATUM(...)		{ -1, __VA_ARGS__, 0 }
+#define AMD_OSVW_ERRATUM(osvw_id, ...)	{ osvw_id, __VA_ARGS__, 0 }
+#define AMD_MODEL_RANGE(f, m_start, s_start, m_end, s_end) \
+	((f << 24) | (m_start << 16) | (s_start << 12) | (m_end << 4) | (s_end))
+#define AMD_MODEL_RANGE_FAMILY(range)	(((range) >> 24) & 0xff)
+#define AMD_MODEL_RANGE_START(range)	(((range) >> 12) & 0xfff)
+#define AMD_MODEL_RANGE_END(range)	((range) & 0xfff)
+
+static const int amd_erratum_400[] =
 	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
 			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
-EXPORT_SYMBOL_GPL(amd_erratum_400);
 
 static const int amd_erratum_383[] =
 	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
 
-bool cpu_has_amd_erratum(const int *erratum)
+static bool cpu_has_amd_erratum(const int *erratum)
 {
 	struct cpuinfo_x86 *cpu = __this_cpu_ptr(&cpu_info);
 	int osvw_id = *erratum++;
@@ -912,5 +923,3 @@ bool cpu_has_amd_erratum(const int *erratum)
 
 	return false;
 }
-
-EXPORT_SYMBOL_GPL(cpu_has_amd_erratum);

commit e6ee94d58dfd06ec64c55f91581f00d4f98bf1f6
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Mar 20 15:07:27 2013 +0100

    x86, cpu: Convert AMD Erratum 383
    
    Convert the AMD erratum 383 testing code to the bug infrastructure. This
    allows keeping the AMD-specific erratum testing machinery private to
    amd.c and not export symbols to modules needlessly.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1363788448-31325-6-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index fa96eb0d02fb..85f84e13d2dd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -513,6 +513,8 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 #endif
 }
 
+static const int amd_erratum_383[];
+
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 {
 	u32 dummy;
@@ -727,6 +729,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		rdmsrl_safe(MSR_AMD64_BU_CFG2, &value);
 		value &= ~(1ULL << 24);
 		wrmsrl_safe(MSR_AMD64_BU_CFG2, value);
+
+		if (cpu_has_amd_erratum(amd_erratum_383))
+			set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);
 	}
 
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
@@ -863,9 +868,8 @@ const int amd_erratum_400[] =
 			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
 EXPORT_SYMBOL_GPL(amd_erratum_400);
 
-const int amd_erratum_383[] =
+static const int amd_erratum_383[] =
 	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
-EXPORT_SYMBOL_GPL(amd_erratum_383);
 
 bool cpu_has_amd_erratum(const int *erratum)
 {

commit 9043a2650cd21f96f831a97f516c2c302e21fb70
Merge: ab7826595e9e d9d8d7ed498e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 25 15:41:43 2013 -0800

    Merge tag 'modules-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux
    
    Pull module update from Rusty Russell:
     "The sweeping change is to make add_taint() explicitly indicate whether
      to disable lockdep, but it's a mechanical change."
    
    * tag 'modules-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux:
      MODSIGN: Add option to not sign modules during modules_install
      MODSIGN: Add -s <signature> option to sign-file
      MODSIGN: Specify the hash algorithm on sign-file command line
      MODSIGN: Simplify Makefile with a Kconfig helper
      module: clean up load_module a little more.
      modpost: Ignore ARC specific non-alloc sections
      module: constify within_module_*
      taint: add explicit flag to show whether lock dep is still OK.
      module: printk message when module signature fail taints kernel.

commit 2ef14f465b9e096531343f5b734cffc5f759f4a6
Merge: cb715a836642 0da3e7f526fd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 21 18:06:55 2013 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Peter Anvin:
     "This is a huge set of several partly interrelated (and concurrently
      developed) changes, which is why the branch history is messier than
      one would like.
    
      The *really* big items are two humonguous patchsets mostly developed
      by Yinghai Lu at my request, which completely revamps the way we
      create initial page tables.  In particular, rather than estimating how
      much memory we will need for page tables and then build them into that
      memory -- a calculation that has shown to be incredibly fragile -- we
      now build them (on 64 bits) with the aid of a "pseudo-linear mode" --
      a #PF handler which creates temporary page tables on demand.
    
      This has several advantages:
    
      1. It makes it much easier to support things that need access to data
         very early (a followon patchset uses this to load microcode way
         early in the kernel startup).
    
      2. It allows the kernel and all the kernel data objects to be invoked
         from above the 4 GB limit.  This allows kdump to work on very large
         systems.
    
      3. It greatly reduces the difference between Xen and native (Xen's
         equivalent of the #PF handler are the temporary page tables created
         by the domain builder), eliminating a bunch of fragile hooks.
    
      The patch series also gets us a bit closer to W^X.
    
      Additional work in this pull is the 64-bit get_user() work which you
      were also involved with, and a bunch of cleanups/speedups to
      __phys_addr()/__pa()."
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (105 commits)
      x86, mm: Move reserving low memory later in initialization
      x86, doc: Clarify the use of asm("%edx") in uaccess.h
      x86, mm: Redesign get_user with a __builtin_choose_expr hack
      x86: Be consistent with data size in getuser.S
      x86, mm: Use a bitfield to mask nuisance get_user() warnings
      x86/kvm: Fix compile warning in kvm_register_steal_time()
      x86-32: Add support for 64bit get_user()
      x86-32, mm: Remove reference to alloc_remap()
      x86-32, mm: Remove reference to resume_map_numa_kva()
      x86-32, mm: Rip out x86_32 NUMA remapping code
      x86/numa: Use __pa_nodebug() instead
      x86: Don't panic if can not alloc buffer for swiotlb
      mm: Add alloc_bootmem_low_pages_nopanic()
      x86, 64bit, mm: hibernate use generic mapping_init
      x86, 64bit, mm: Mark data/bss/brk to nx
      x86: Merge early kernel reserve for 32bit and 64bit
      x86: Add Crash kernel low reservation
      x86, kdump: Remove crashkernel range find limit for 64bit
      memblock: Add memblock_mem_size()
      x86, boot: Not need to check setup_header version for setup_data
      ...

commit cb715a836642e0ec69350670d1c2f800f3e2d2e4
Merge: 27ea6dfdc23e 2e32b7190641
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 21 18:03:39 2013 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cpu updates from Peter Anvin:
     "This is a corrected attempt at the x86/cpu branch, this time with the
      fixes in that makes it not break on KVM (current or past), or any
      other virtualizer which traps on this configuration.
    
      Again, the biggest change here is enabling the WC+ memory type on AMD
      processors, if the BIOS doesn't."
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, kvm: Add MSR_AMD64_BU_CFG2 to the list of ignored MSRs
      x86, cpu, amd: Fix WC+ workaround for older virtual hosts
      x86, AMD: Enable WC+ memory type on family 10 processors
      x86, AMD: Clean up init_amd()
      x86/process: Change %8s to %s for pr_warn() in release_thread()
      x86/cpu/hotplug: Remove CONFIG_EXPERIMENTAL dependency

commit 52d3d06e706bdde3d6c5c386deb065c3b4c51618
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Feb 19 19:33:12 2013 +0100

    x86, cpu, amd: Fix WC+ workaround for older virtual hosts
    
    The WC+ workaround for F10h introduces a new MSR and kvm host #GPs
    on accesses to unknown MSRs if paravirt is not compiled in. Use the
    exception-handling MSR accessors so as not to break 3.8 and later guests
    booting on older hosts.
    
    Remove a redundant family check while at it.
    
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1361298793-31834-1-git-send-email-bp@alien8.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 721ef3208eb5..163af4a91d09 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -723,12 +723,14 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		 * performance degradation for certain nested-paging guests.
 		 * Prevent this conversion by clearing bit 24 in
 		 * MSR_AMD64_BU_CFG2.
+		 *
+		 * NOTE: we want to use the _safe accessors so as not to #GP kvm
+		 * guests on older kvm hosts.
 		 */
-		if (c->x86 == 0x10) {
-			rdmsrl(MSR_AMD64_BU_CFG2, value);
-			value &= ~(1ULL << 24);
-			wrmsrl(MSR_AMD64_BU_CFG2, value);
-		}
+
+		rdmsrl_safe(MSR_AMD64_BU_CFG2, &value);
+		value &= ~(1ULL << 24);
+		wrmsrl_safe(MSR_AMD64_BU_CFG2, value);
 	}
 
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);

commit f0322bd341fd63261527bf84afd3272bcc2e8dd3
Author: Boris Ostrovsky <boris.ostrovsky@amd.com>
Date:   Tue Jan 29 16:32:49 2013 -0500

    x86, AMD: Enable WC+ memory type on family 10 processors
    
    In some cases BIOS may not enable WC+ memory type on family 10
    processors, instead converting what would be WC+ memory to CD type.
    On guests using nested pages this could result in performance
    degradation. This patch enables WC+.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@amd.com>
    Link: http://lkml.kernel.org/r/1359495169-23278-1-git-send-email-ostr@amd64.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index dd4a5b685a00..721ef3208eb5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -698,13 +698,11 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	if (c->x86 > 0x11)
 		set_cpu_cap(c, X86_FEATURE_ARAT);
 
-	/*
-	 * Disable GART TLB Walk Errors on Fam10h. We do this here
-	 * because this is always needed when GART is enabled, even in a
-	 * kernel which has no MCE support built in.
-	 */
 	if (c->x86 == 0x10) {
 		/*
+		 * Disable GART TLB Walk Errors on Fam10h. We do this here
+		 * because this is always needed when GART is enabled, even in a
+		 * kernel which has no MCE support built in.
 		 * BIOS should disable GartTlbWlk Errors themself. If
 		 * it doesn't do it here as suggested by the BKDG.
 		 *
@@ -718,6 +716,19 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 			mask |= (1 << 10);
 			wrmsrl_safe(MSR_AMD64_MCx_MASK(4), mask);
 		}
+
+		/*
+		 * On family 10h BIOS may not have properly enabled WC+ support,
+		 * causing it to be converted to CD memtype. This may result in
+		 * performance degradation for certain nested-paging guests.
+		 * Prevent this conversion by clearing bit 24 in
+		 * MSR_AMD64_BU_CFG2.
+		 */
+		if (c->x86 == 0x10) {
+			rdmsrl(MSR_AMD64_BU_CFG2, value);
+			value &= ~(1ULL << 24);
+			wrmsrl(MSR_AMD64_BU_CFG2, value);
+		}
 	}
 
 	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);

commit 6bf08a8dcd1ef13e542f08fc3b1ce6cf64ae63b6
Author: Boris Ostrovsky <boris.ostrovsky@amd.com>
Date:   Tue Jan 29 16:32:16 2013 -0500

    x86, AMD: Clean up init_amd()
    
    Clean up multiple declarations of variable used for rd/wrmsr.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@amd.com>
    Link: http://lkml.kernel.org/r/1359495136-23244-1-git-send-email-ostr@amd64.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 15239fffd6fe..dd4a5b685a00 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -518,10 +518,9 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 {
 	u32 dummy;
-
-#ifdef CONFIG_SMP
 	unsigned long long value;
 
+#ifdef CONFIG_SMP
 	/*
 	 * Disable TLB flush filter by setting HWCR.FFDIS on K8
 	 * bit 6 of msr C001_0015
@@ -559,12 +558,10 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		 * (AMD Erratum #110, docId: 25759).
 		 */
 		if (c->x86_model < 0x14 && cpu_has(c, X86_FEATURE_LAHF_LM)) {
-			u64 val;
-
 			clear_cpu_cap(c, X86_FEATURE_LAHF_LM);
-			if (!rdmsrl_amd_safe(0xc001100d, &val)) {
-				val &= ~(1ULL << 32);
-				wrmsrl_amd_safe(0xc001100d, val);
+			if (!rdmsrl_amd_safe(0xc001100d, &value)) {
+				value &= ~(1ULL << 32);
+				wrmsrl_amd_safe(0xc001100d, value);
 			}
 		}
 
@@ -617,13 +614,12 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	if ((c->x86 == 0x15) &&
 	    (c->x86_model >= 0x10) && (c->x86_model <= 0x1f) &&
 	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
-		u64 val;
 
-		if (!rdmsrl_safe(0xc0011005, &val)) {
-			val |= 1ULL << 54;
-			wrmsrl_safe(0xc0011005, val);
-			rdmsrl(0xc0011005, val);
-			if (val & (1ULL << 54)) {
+		if (!rdmsrl_safe(0xc0011005, &value)) {
+			value |= 1ULL << 54;
+			wrmsrl_safe(0xc0011005, value);
+			rdmsrl(0xc0011005, value);
+			if (value & (1ULL << 54)) {
 				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
 				printk(KERN_INFO FW_INFO "CPU: Re-enabling "
 				  "disabled Topology Extensions Support\n");
@@ -637,11 +633,10 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	 */
 	if ((c->x86 == 0x15) &&
 	    (c->x86_model >= 0x02) && (c->x86_model < 0x20)) {
-		u64 val;
 
-		if (!rdmsrl_safe(0xc0011021, &val) && !(val & 0x1E)) {
-			val |= 0x1E;
-			wrmsrl_safe(0xc0011021, val);
+		if (!rdmsrl_safe(0xc0011021, &value) && !(value & 0x1E)) {
+			value |= 0x1E;
+			wrmsrl_safe(0xc0011021, value);
 		}
 	}
 

commit de65d816aa44f9ddd79861ae21d75010cc1fd003
Merge: 9710f581bb4c 5dcd14ecd41e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Tue Jan 29 14:59:09 2013 -0800

    Merge remote-tracking branch 'origin/x86/boot' into x86/mm2
    
    Coming patches to x86/mm2 require the changes and advanced baseline in
    x86/boot.
    
    Resolved Conflicts:
            arch/x86/kernel/setup.c
            mm/nobootmem.c
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 373d4d099761cb1f637bed488ab3871945882273
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Jan 21 17:17:39 2013 +1030

    taint: add explicit flag to show whether lock dep is still OK.
    
    Fix up all callers as they were before, with make one change: an
    unsigned module taints the kernel, but doesn't turn off lockdep.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 15239fffd6fe..5853e57523e5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -220,8 +220,7 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 	 */
 	WARN_ONCE(1, "WARNING: This combination of AMD"
 		" processors is not suitable for SMP.\n");
-	if (!test_taint(TAINT_UNSAFE_SMP))
-		add_taint(TAINT_UNSAFE_SMP);
+	add_taint(TAINT_UNSAFE_SMP, LOCKDEP_NOW_UNRELIABLE);
 
 valid_k7:
 	;

commit 8b84c8df38d5796da2e8cd051666d203ddabcb62
Author: Daniel J Blueman <daniel@numascale-asia.com>
Date:   Tue Nov 27 14:32:10 2012 +0800

    x86, AMD, NB: Use u16 for northbridge IDs in amd_get_nb_id
    
    Change amd_get_nb_id to return u16 to support >255 memory controllers,
    and related consistency fixes.
    
    Signed-off-by: Daniel J Blueman <daniel@numascale-asia.com>
    Link: http://lkml.kernel.org/r/1353997932-8475-2-git-send-email-daniel@numascale-asia.com
    Signed-off-by: Borislav Petkov <bp@alien8.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 15239fffd6fe..782c456eaa01 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -364,9 +364,9 @@ static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
 #endif
 }
 
-int amd_get_nb_id(int cpu)
+u16 amd_get_nb_id(int cpu)
 {
-	int id = 0;
+	u16 id = 0;
 #ifdef CONFIG_SMP
 	id = per_cpu(cpu_llc_id, cpu);
 #endif

commit 743aa456c1834f76982af44e8b71d1a0b2a82e21
Merge: a05a4e24dcd7 11af32b69ef7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 19:59:32 2012 -0800

    Merge branch 'x86-nuke386-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull "Nuke 386-DX/SX support" from Ingo Molnar:
     "This tree removes ancient-386-CPUs support and thus zaps quite a bit
      of complexity:
    
        24 files changed, 56 insertions(+), 425 deletions(-)
    
      ... which complexity has plagued us with extra work whenever we wanted
      to change SMP primitives, for years.
    
      Unfortunately there's a nostalgic cost: your old original 386 DX33
      system from early 1991 won't be able to boot modern Linux kernels
      anymore.  Sniff."
    
    I'm not sentimental.  Good riddance.
    
    * 'x86-nuke386-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, 386 removal: Document Nx586 as a 386 and thus unsupported
      x86, cleanups: Simplify sync_core() in the case of no CPUID
      x86, 386 removal: Remove CONFIG_X86_POPAD_OK
      x86, 386 removal: Remove CONFIG_X86_WP_WORKS_OK
      x86, 386 removal: Remove CONFIG_INVLPG
      x86, 386 removal: Remove CONFIG_BSWAP
      x86, 386 removal: Remove CONFIG_XADD
      x86, 386 removal: Remove CONFIG_CMPXCHG
      x86, 386 removal: Remove CONFIG_M386 from Kconfig

commit a05a4e24dcd73c2de4ef3f8d520b8bbb44570c60
Merge: e9a5a9197196 27d3a8a26ada
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 19:58:29 2012 -0800

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 topology discovery improvements from Ingo Molnar:
     "These changes improve topology discovery on AMD CPUs.
    
      Right now this feeds information displayed in
      /sys/devices/system/cpu/cpuX/cache/indexY/* - but in the future we
      could use this to set up a better scheduling topology."
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, cacheinfo: Base cache sharing info on CPUID 0x8000001d on AMD
      x86, cacheinfo: Make use of CPUID 0x8000001d for cache information on AMD
      x86, cacheinfo: Determine number of cache leafs using CPUID 0x8000001d on AMD
      x86: Add cpu_has_topoext

commit 094ab1db7cb7833cd4c820acd868fc26acf3f08e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Nov 28 11:50:27 2012 -0800

    x86, 386 removal: Remove CONFIG_INVLPG
    
    All 486+ CPUs support INVLPG, so remove the fallback 386 support
    code.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1354132230-21854-6-git-send-email-hpa@linux.intel.com

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1b7d1656a042..a025d8cc4577 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -753,9 +753,6 @@ static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c,
 
 static void __cpuinit cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)
 {
-	if (!cpu_has_invlpg)
-		return;
-
 	tlb_flushall_shift = 5;
 
 	if (c->x86 <= 0x11)

commit c074eaac2ab264c94520efff7e896b771de885ae
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri Nov 16 19:39:20 2012 -0800

    x86, mm: kill numa_64.h
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/1353123563-3103-44-git-send-email-yinghai@kernel.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 9619ba6528ca..913f94f9e8d9 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -12,7 +12,6 @@
 #include <asm/pci-direct.h>
 
 #ifdef CONFIG_X86_64
-# include <asm/numa_64.h>
 # include <asm/mmconfig.h>
 # include <asm/cacheflush.h>
 #endif

commit dda56e134059b840631fdfd034784056b627c2a6
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Fri Nov 16 19:38:48 2012 -0800

    x86, mm: Fixup code testing if a pfn is direct mapped
    
    Update code that previously assumed pfns [ 0 - max_low_pfn_mapped ) and
    [ 4GB - max_pfn_mapped ) were always direct mapped, to now look up
    pfn_mapped ranges instead.
    
    -v2: change applying sequence to keep git bisecting working.
         so add dummy pfn_range_is_mapped(). - Yinghai Lu
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Link: http://lkml.kernel.org/r/1353123563-3103-12-git-send-email-yinghai@kernel.org
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f7e98a2c0d12..9619ba6528ca 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -676,12 +676,10 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		 * benefit in doing so.
 		 */
 		if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &tseg)) {
+			unsigned long pfn = tseg >> PAGE_SHIFT;
+
 			printk(KERN_DEBUG "tseg: %010llx\n", tseg);
-			if ((tseg>>PMD_SHIFT) <
-				(max_low_pfn_mapped>>(PMD_SHIFT-PAGE_SHIFT)) ||
-				((tseg>>PMD_SHIFT) <
-				(max_pfn_mapped>>(PMD_SHIFT-PAGE_SHIFT)) &&
-				(tseg>>PMD_SHIFT) >= (1ULL<<(32 - PMD_SHIFT))))
+			if (pfn_range_is_mapped(pfn, pfn + 1))
 				set_memory_4k((unsigned long)__va(tseg), 1);
 		}
 	}

commit 04a1541828ea223169eb44a336bfad8ec0dfb46a
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Fri Oct 19 10:59:33 2012 +0200

    x86, cacheinfo: Determine number of cache leafs using CPUID 0x8000001d on AMD
    
    CPUID 0x8000001d works quite similar to Intels' CPUID function 4.
    Use it to determine number of cache leafs.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/20121019085933.GE26718@alberich
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 64e9ad4e49a0..a8538e6d2ff2 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -643,12 +643,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	detect_ht(c);
 #endif
 
-	if (c->extended_cpuid_level >= 0x80000006) {
-		if (cpuid_edx(0x80000006) & 0xf000)
-			num_cache_leaves = 4;
-		else
-			num_cache_leaves = 3;
-	}
+	init_amd_cacheinfo(c);
 
 	if (c->x86 >= 0xf)
 		set_cpu_cap(c, X86_FEATURE_K8);

commit 193f3fcb3ab769bab4a2b9fa181eef3e5699a352
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Fri Oct 19 10:58:13 2012 +0200

    x86: Add cpu_has_topoext
    
    Introduce cpu_has_topoext to check for AMD's CPUID topology extensions
    support. It indicates support for
    CPUID Fn8000_001D_EAX_x[N:0]-CPUID Fn8000_001E_EDX
    
    See AMD's CPUID Specification, Publication # 25481
    (as of Rev. 2.34 September 2010)
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/20121019085813.GD26718@alberich
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f7e98a2c0d12..64e9ad4e49a0 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -304,7 +304,7 @@ static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 	int cpu = smp_processor_id();
 
 	/* get information required for multi-node processors */
-	if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
+	if (cpu_has_topoext) {
 		u32 eax, ebx, ecx, edx;
 
 		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);

commit 2bbf0a1427c377350f001fbc6260995334739ad7
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Wed Oct 31 17:20:50 2012 +0100

    x86, amd: Disable way access filter on Piledriver CPUs
    
    The Way Access Filter in recent AMD CPUs may hurt the performance of
    some workloads, caused by aliasing issues in the L1 cache.
    This patch disables it on the affected CPUs.
    
    The issue is similar to that one of last year:
    http://lkml.indiana.edu/hypermail/linux/kernel/1107.3/00041.html
    This new patch does not replace the old one, we just need another
    quirk for newer CPUs.
    
    The performance penalty without the patch depends on the
    circumstances, but is a bit less than the last year's 3%.
    
    The workloads affected would be those that access code from the same
    physical page under different virtual addresses, so different
    processes using the same libraries with ASLR or multiple instances of
    PIE-binaries. The code needs to be accessed simultaneously from both
    cores of the same compute unit.
    
    More details can be found here:
    http://developer.amd.com/Assets/SharedL1InstructionCacheonAMD15hCPU.pdf
    
    CPUs affected are anything with the core known as Piledriver.
    That includes the new parts of the AMD A-Series (aka Trinity) and the
    just released new CPUs of the FX-Series (aka Vishera).
    The model numbering is a bit odd here: FX CPUs have model 2,
    A-Series has model 10h, with possible extensions to 1Fh. Hence the
    range of model ids.
    
    Signed-off-by: Andre Przywara <osp@andrep.de>
    Link: http://lkml.kernel.org/r/1351700450-9277-1-git-send-email-osp@andrep.de
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f7e98a2c0d12..1b7d1656a042 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -631,6 +631,20 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		}
 	}
 
+	/*
+	 * The way access filter has a performance penalty on some workloads.
+	 * Disable it on the affected CPUs.
+	 */
+	if ((c->x86 == 0x15) &&
+	    (c->x86_model >= 0x02) && (c->x86_model < 0x20)) {
+		u64 val;
+
+		if (!rdmsrl_safe(0xc0011021, &val) && !(val & 0x1E)) {
+			val |= 0x1E;
+			wrmsrl_safe(0xc0011021, val);
+		}
+	}
+
 	cpu_detect_cache_sizes(c);
 
 	/* Multi core CPU? */

commit 057237bb35a605d795fd787868a1088705f26ee5
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon Aug 6 19:00:39 2012 +0200

    x86, cpu: Preset default tlb_flushall_shift on AMD
    
    Run the mprotect.c microbenchmark on all our families >= K8 and preset
    the flushall shift variable accordingly.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1344272439-29080-5-git-send-email-bp@amd64.org
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bcd200839c90..f7e98a2c0d12 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -737,6 +737,17 @@ static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c,
 }
 #endif
 
+static void __cpuinit cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)
+{
+	if (!cpu_has_invlpg)
+		return;
+
+	tlb_flushall_shift = 5;
+
+	if (c->x86 <= 0x11)
+		tlb_flushall_shift = 4;
+}
+
 static void __cpuinit cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 {
 	u32 ebx, eax, ecx, edx;
@@ -788,6 +799,8 @@ static void __cpuinit cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 		tlb_lli_2m[ENTRIES] = eax & mask;
 
 	tlb_lli_4m[ENTRIES] = tlb_lli_2m[ENTRIES] >> 1;
+
+	cpu_set_tlb_flushall_shift(c);
 }
 
 static const struct cpu_dev __cpuinitconst amd_cpu_dev = {

commit b46882e4c4de4813947fce940fe74af794a1eb72
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon Aug 6 19:00:38 2012 +0200

    x86, cpu: Add AMD TLB size detection
    
    Read I- and DTLB entries count from CPUID on AMD. Handle all the
    different family-specific cases.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1344272439-29080-4-git-send-email-bp@amd64.org
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 9d92e19039f0..bcd200839c90 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -737,6 +737,59 @@ static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c,
 }
 #endif
 
+static void __cpuinit cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
+{
+	u32 ebx, eax, ecx, edx;
+	u16 mask = 0xfff;
+
+	if (c->x86 < 0xf)
+		return;
+
+	if (c->extended_cpuid_level < 0x80000006)
+		return;
+
+	cpuid(0x80000006, &eax, &ebx, &ecx, &edx);
+
+	tlb_lld_4k[ENTRIES] = (ebx >> 16) & mask;
+	tlb_lli_4k[ENTRIES] = ebx & mask;
+
+	/*
+	 * K8 doesn't have 2M/4M entries in the L2 TLB so read out the L1 TLB
+	 * characteristics from the CPUID function 0x80000005 instead.
+	 */
+	if (c->x86 == 0xf) {
+		cpuid(0x80000005, &eax, &ebx, &ecx, &edx);
+		mask = 0xff;
+	}
+
+	/* Handle DTLB 2M and 4M sizes, fall back to L1 if L2 is disabled */
+	if (!((eax >> 16) & mask)) {
+		u32 a, b, c, d;
+
+		cpuid(0x80000005, &a, &b, &c, &d);
+		tlb_lld_2m[ENTRIES] = (a >> 16) & 0xff;
+	} else {
+		tlb_lld_2m[ENTRIES] = (eax >> 16) & mask;
+	}
+
+	/* a 4M entry uses two 2M entries */
+	tlb_lld_4m[ENTRIES] = tlb_lld_2m[ENTRIES] >> 1;
+
+	/* Handle ITLB 2M and 4M sizes, fall back to L1 if L2 is disabled */
+	if (!(eax & mask)) {
+		/* Erratum 658 */
+		if (c->x86 == 0x15 && c->x86_model <= 0x1f) {
+			tlb_lli_2m[ENTRIES] = 1024;
+		} else {
+			cpuid(0x80000005, &eax, &ebx, &ecx, &edx);
+			tlb_lli_2m[ENTRIES] = eax & 0xff;
+		}
+	} else
+		tlb_lli_2m[ENTRIES] = eax & mask;
+
+	tlb_lli_4m[ENTRIES] = tlb_lli_2m[ENTRIES] >> 1;
+}
+
 static const struct cpu_dev __cpuinitconst amd_cpu_dev = {
 	.c_vendor	= "AMD",
 	.c_ident	= { "AuthenticAMD" },
@@ -756,6 +809,7 @@ static const struct cpu_dev __cpuinitconst amd_cpu_dev = {
 	.c_size_cache	= amd_size_cache,
 #endif
 	.c_early_init   = early_init_amd,
+	.c_detect_tlb	= cpu_detect_tlb_amd,
 	.c_bsp_init	= bsp_init_amd,
 	.c_init		= init_amd,
 	.c_x86_vendor	= X86_VENDOR_AMD,

commit 715c85b1fc824e9cd0ea07d6ceb80d2262f32e90
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Jun 7 13:32:04 2012 -0700

    x86, cpu: Rename checking_wrmsrl() to wrmsrl_safe()
    
    Rename checking_wrmsrl() to wrmsrl_safe(), to match the naming
    convention used by all the other MSR access functions/macros.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c928eb26ada6..9d92e19039f0 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -621,7 +621,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 		if (!rdmsrl_safe(0xc0011005, &val)) {
 			val |= 1ULL << 54;
-			checking_wrmsrl(0xc0011005, val);
+			wrmsrl_safe(0xc0011005, val);
 			rdmsrl(0xc0011005, val);
 			if (val & (1ULL << 54)) {
 				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
@@ -712,7 +712,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		err = rdmsrl_safe(MSR_AMD64_MCx_MASK(4), &mask);
 		if (err == 0) {
 			mask |= (1 << 10);
-			checking_wrmsrl(MSR_AMD64_MCx_MASK(4), mask);
+			wrmsrl_safe(MSR_AMD64_MCx_MASK(4), mask);
 		}
 	}
 

commit 2c929ce6f1ed1302be225512b433e6a6554f71a4
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Fri Jun 1 16:52:38 2012 +0200

    x86, cpu, amd: Deprecate AMD-specific MSR variants
    
    Now that all users of {rd,wr}msr_amd_safe have been fixed, deprecate its
    use by making them private to amd.c and adding warnings when used on
    anything else beside K8.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1338562358-28182-5-git-send-email-bp@amd64.org
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 80ccd99542e6..c928eb26ada6 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -19,6 +19,39 @@
 
 #include "cpu.h"
 
+static inline int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)
+{
+	struct cpuinfo_x86 *c = &cpu_data(smp_processor_id());
+	u32 gprs[8] = { 0 };
+	int err;
+
+	WARN_ONCE((c->x86 != 0xf), "%s should only be used on K8!\n", __func__);
+
+	gprs[1] = msr;
+	gprs[7] = 0x9c5a203a;
+
+	err = rdmsr_safe_regs(gprs);
+
+	*p = gprs[0] | ((u64)gprs[2] << 32);
+
+	return err;
+}
+
+static inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
+{
+	struct cpuinfo_x86 *c = &cpu_data(smp_processor_id());
+	u32 gprs[8] = { 0 };
+
+	WARN_ONCE((c->x86 != 0xf), "%s should only be used on K8!\n", __func__);
+
+	gprs[0] = (u32)val;
+	gprs[1] = msr;
+	gprs[2] = val >> 32;
+	gprs[7] = 0x9c5a203a;
+
+	return wrmsr_safe_regs(gprs);
+}
+
 #ifdef CONFIG_X86_32
 /*
  *	B step AMD K6 before B 9730xxxx have hardware bugs that can cause

commit 169e9cbd77db23fe50bc8ba68bf081adb67b4220
Author: Andre Przywara <andre.przywara@amd.com>
Date:   Fri Jun 1 16:52:37 2012 +0200

    x86, cpu, amd: Fix crash as Xen Dom0 on AMD Trinity systems
    
    f7f286a910221 ("x86/amd: Re-enable CPU topology extensions in case BIOS
    has disabled it") wrongfully added code which used the AMD-specific
    {rd,wr}msr variants for no real reason.
    
    This caused boot panics on xen which wasn't initializing the
    {rd,wr}msr_safe_regs pv_ops members properly.
    
    This, in turn, caused a heated discussion leading to us reviewing all
    uses of the AMD-specific variants and removing them where unneeded
    (almost everywhere except an obscure K8 BIOS fix, see 6b0f43ddfa358).
    
    Finally, this patch switches to the standard {rd,wr}msr*_safe* variants
    which should've been used in the first place anyway and avoided unneeded
    excitation with xen.
    
    Signed-off-by: Andre Przywara <andre.przywara@amd.com>
    Link: http://lkml.kernel.org/r/1338562358-28182-4-git-send-email-bp@amd64.org
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: <http://lkml.kernel.org/r/1338383402-3838-1-git-send-email-andre.przywara@amd.com>
    [Boris: correct and expand commit message]
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 146bb6218eec..80ccd99542e6 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -586,9 +586,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
 		u64 val;
 
-		if (!rdmsrl_amd_safe(0xc0011005, &val)) {
+		if (!rdmsrl_safe(0xc0011005, &val)) {
 			val |= 1ULL << 54;
-			wrmsrl_amd_safe(0xc0011005, val);
+			checking_wrmsrl(0xc0011005, val);
 			rdmsrl(0xc0011005, val);
 			if (val & (1ULL << 54)) {
 				set_cpu_cap(c, X86_FEATURE_TOPOEXT);

commit f7f286a910221ae18b21c18d9d0f4cd88965829f
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Tue Apr 3 12:13:07 2012 +0200

    x86/amd: Re-enable CPU topology extensions in case BIOS has disabled it
    
    BIOS will switch off the corresponding feature flag on family
    15h models 10h-1fh non-desktop CPUs.
    
    The topology extension CPUID leafs are required to detect which
    cores belong to the same compute unit. (thread siblings mask is
    set accordingly and also correct information about L1i and L2
    cache sharing depends on this).
    
    W/o this patch we wouldn't see which cores belong to the same
    compute unit and also cache sharing information for L1i and L2
    would be incorrect on such systems.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1c67ca100e4c..146bb6218eec 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -580,6 +580,24 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		}
 	}
 
+	/* re-enable TopologyExtensions if switched off by BIOS */
+	if ((c->x86 == 0x15) &&
+	    (c->x86_model >= 0x10) && (c->x86_model <= 0x1f) &&
+	    !cpu_has(c, X86_FEATURE_TOPOEXT)) {
+		u64 val;
+
+		if (!rdmsrl_amd_safe(0xc0011005, &val)) {
+			val |= 1ULL << 54;
+			wrmsrl_amd_safe(0xc0011005, val);
+			rdmsrl(0xc0011005, val);
+			if (val & (1ULL << 54)) {
+				set_cpu_cap(c, X86_FEATURE_TOPOEXT);
+				printk(KERN_INFO FW_INFO "CPU: Re-enabling "
+				  "disabled Topology Extensions Support\n");
+			}
+		}
+	}
+
 	cpu_detect_cache_sizes(c);
 
 	/* Multi core CPU? */

commit 68894632afb2729a1d8785c877840953894c7283
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Mon Apr 2 18:06:48 2012 +0200

    x86/platform: Remove incorrect error message in x86_default_fixup_cpu_id()
    
    It's only called from amd.c:srat_detect_node(). The introduced
    condition for calling the fixup code is true for all AMD
    multi-node processors, e.g. Magny-Cours and Interlagos. There we
    have 2 NUMA nodes on one socket. Thus there are cores having
    different numa-node-id but with equal phys_proc_id.
    
    There is no point to print error messages in such a situation.
    
    The confusing/misleading error message was introduced with
    commit 64be4c1c2428e148de6081af235e2418e6a66dda ("x86: Add
    x86_init platform override to fix up NUMA core numbering").
    
    Remove the default fixup function (especially the error message)
    and replace it by a NULL pointer check, move the
    Numascale-specific condition for calling the fixup into the
    fixup-function itself and slightly adapt the comment.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: <stable@kernel.org>
    Cc: <sp@numascale.com>
    Cc: <bp@amd64.org>
    Cc: <daniel@numascale-asia.com>
    Link: http://lkml.kernel.org/r/20120402160648.GR27684@alberich.amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1248f9ceabc1..1c67ca100e4c 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -353,10 +353,11 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 		node = per_cpu(cpu_llc_id, cpu);
 
 	/*
-	 * If core numbers are inconsistent, it's likely a multi-fabric platform,
-	 * so invoke platform-specific handler
+	 * On multi-fabric platform (e.g. Numascale NumaChip) a
+	 * platform-specific handler needs to be called to fixup some
+	 * IDs of the CPU.
 	 */
-	if (c->phys_proc_id != node)
+	if (x86_cpuinit.fixup_cpu_id)
 		x86_cpuinit.fixup_cpu_id(c, node);
 
 	if (!node_online(node)) {

commit d7de8649f34d45041409d1af4ba4a521971a9075
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Wed Apr 11 17:12:38 2012 +0200

    x86/amd: Remove broken links from comment and kernel message
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/20120411151238.GA4794@alberich.amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 0a44b90602b0..1248f9ceabc1 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -26,7 +26,8 @@
  *	contact AMD for precise details and a CPU swap.
  *
  *	See	http://www.multimania.com/poulot/k6bug.html
- *		http://www.amd.com/K6/k6docs/revgd.html
+ *	and	section 2.6.2 of "AMD-K6 Processor Revision Guide - Model 6"
+ *		(Publication # 21266  Issue Date: August 1998)
  *
  *	The following test is erm.. interesting. AMD neglected to up
  *	the chip setting when fixing the bug but they also tweaked some
@@ -94,7 +95,6 @@ static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
 				"system stability may be impaired when more than 32 MB are used.\n");
 		else
 			printk(KERN_CONT "probably OK (after B9730xxxx).\n");
-		printk(KERN_INFO "Please see http://membres.lycos.fr/poulot/k6bug.html\n");
 	}
 
 	/* K6 with old style WHCR */

commit c98fdeaa92731308ed80386261fa2589addefa47
Author: Borislav Petkov <bp@alien8.de>
Date:   Tue Feb 7 13:08:52 2012 +0100

    x86/sched/perf/AMD: Set sched_clock_stable
    
    Stephane Eranian reported that doing a scheduler latency
    measurements with perf on AMD doesn't work out as expected due
    to the fact that the sched_clock() granularity is too coarse,
    i.e. done in jiffies due to the sched_clock_stable not set,
    which, if set, would mean that we get to use the TSC as sample
    source which would give us much higher precision.
    
    However, there's no reason not to set sched_clock_stable on AMD
    because all families from F10h and upwards do have an invariant
    TSC and have the CPUID flag to prove (CPUID_8000_0007_EDX[8]).
    
    Make it so, #1.
    
    Signed-off-by: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@amd64.org>
    Cc: Venki Pallipadi <venki@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/20120206132546.GA30854@quad
    [ Should any non-standard system break the TSC, we should
      mark them so explicitly, in their platform init handler, or
      in a DMI quirk. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f4773f4aae35..0a44b90602b0 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -5,6 +5,7 @@
 #include <linux/mm.h>
 
 #include <linux/io.h>
+#include <linux/sched.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cpu.h>
@@ -456,6 +457,8 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
+		if (!check_tsc_unstable())
+			sched_clock_stable = 1;
 	}
 
 #ifdef CONFIG_X86_64

commit 141168c36cdee3ff23d9c7700b0edc47cb65479f
Author: Kevin Winchester <kjwinchester@gmail.com>
Date:   Tue Dec 20 20:52:22 2011 -0400

    x86: Simplify code by removing a !SMP #ifdefs from 'struct cpuinfo_x86'
    
    Several fields in struct cpuinfo_x86 were not defined for the
    !SMP case, likely to save space.  However, those fields still
    have some meaning for UP, and keeping them allows some #ifdef
    removal from other files.  The additional size of the UP kernel
    from this change is not significant enough to worry about
    keeping up the distinction:
    
               text    data     bss     dec     hex filename
            4737168  506459  972040 6215667  5ed7f3 vmlinux.o.before
            4737444  506459  972040 6215943  5ed907 vmlinux.o.after
    
    for a difference of 276 bytes for an example UP config.
    
    If someone wants those 276 bytes back badly then it should
    be implemented in a cleaner way.
    
    Signed-off-by: Kevin Winchester <kjwinchester@gmail.com>
    Cc: Steffen Persvold <sp@numascale.com>
    Link: http://lkml.kernel.org/r/1324428742-12498-1-git-send-email-kjwinchester@gmail.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ef21bdccd674..f4773f4aae35 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -148,7 +148,6 @@ static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
 
 static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_SMP
 	/* calling is from identify_secondary_cpu() ? */
 	if (!c->cpu_index)
 		return;
@@ -192,7 +191,6 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 
 valid_k7:
 	;
-#endif
 }
 
 static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)

commit 64be4c1c2428e148de6081af235e2418e6a66dda
Author: Daniel J Blueman <daniel@numascale-asia.com>
Date:   Mon Dec 5 16:20:37 2011 +0800

    x86: Add x86_init platform override to fix up NUMA core numbering
    
    Add an x86_init vector for handling inconsistent core numbering.
    This is useful for multi-fabric platforms, such as Numascale
    NumaConnect.
    
    v2:
     - use struct x86_cpuinit_ops
     - provide default fall-back function to warn
    
    Signed-off-by: Daniel J Blueman <daniel@numascale-asia.com>
    Cc: Steffen Persvold <sp@numascale.com>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Link: http://lkml.kernel.org/r/1323073238-32686-2-git-send-email-daniel@numascale-asia.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 0bab2b18bb20..ef21bdccd674 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -353,6 +353,13 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 	if (node == NUMA_NO_NODE)
 		node = per_cpu(cpu_llc_id, cpu);
 
+	/*
+	 * If core numbers are inconsistent, it's likely a multi-fabric platform,
+	 * so invoke platform-specific handler
+	 */
+	if (c->phys_proc_id != node)
+		x86_cpuinit.fixup_cpu_id(c, node);
+
 	if (!node_online(node)) {
 		/*
 		 * Two possibilities here:

commit 8e8da023f5af71662867729db5547dc54786093c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 4 11:57:09 2011 -0800

    x86: Fix boot failures on older AMD CPU's
    
    People with old AMD chips are getting hung boots, because commit
    bcb80e53877c ("x86, microcode, AMD: Add microcode revision to
    /proc/cpuinfo") moved the microcode detection too early into
    "early_init_amd()".
    
    At that point we are *so* early in the booth that the exception tables
    haven't even been set up yet, so the whole
    
            rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
    
    doesn't actually work: if the rdmsr does a GP fault (due to non-existant
    MSR register on older CPU's), we can't fix it up yet, and the boot fails.
    
    Fix it by simply moving the code to a slightly later point in the boot
    (init_amd() instead of early_init_amd()), since the kernel itself
    doesn't even really care about the microcode patchlevel at this point
    (or really ever: it's made available to user space in /proc/cpuinfo, and
    updated if you do a microcode load).
    
    Reported-tested-and-bisected-by:  Larry Finger <Larry.Finger@lwfinger.net>
    Tested-by: Bob Tracy <rct@gherkin.frus.com>
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c7e46cb35327..0bab2b18bb20 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -442,8 +442,6 @@ static void __cpuinit bsp_init_amd(struct cpuinfo_x86 *c)
 
 static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
-	u32 dummy;
-
 	early_init_amd_mc(c);
 
 	/*
@@ -473,12 +471,12 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
 	}
 #endif
-
-	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
 }
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 {
+	u32 dummy;
+
 #ifdef CONFIG_SMP
 	unsigned long long value;
 
@@ -657,6 +655,8 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 			checking_wrmsrl(MSR_AMD64_MCx_MASK(4), mask);
 		}
 	}
+
+	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
 }
 
 #ifdef CONFIG_X86_32

commit 69c60c88eeb364ebf58432f9bc38033522d58767
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu May 26 12:22:53 2011 -0400

    x86: Fix files explicitly requiring export.h for EXPORT_SYMBOL/THIS_MODULE
    
    These files were implicitly getting EXPORT_SYMBOL via device.h
    which was including module.h, but that will be fixed up shortly.
    
    By fixing these now, we can avoid seeing things like:
    
    arch/x86/kernel/rtc.c:29: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL’
    arch/x86/kernel/pci-dma.c:20: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL’
    arch/x86/kernel/e820.c:69: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL_GPL’
    
    [ with input from Randy Dunlap <rdunlap@xenotime.net> and also
      from Stephen Rothwell <sfr@canb.auug.org.au> ]
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 46ae4f65fc7f..c7e46cb35327 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -1,3 +1,4 @@
+#include <linux/export.h>
 #include <linux/init.h>
 #include <linux/bitops.h>
 #include <linux/elf.h>

commit 8237eb946a1a23c600fb289cf8dd3b399b10604e
Merge: cc21fe518a97 bcb80e53877c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 28 05:14:48 2011 -0700

    Merge branch 'x86-microcode-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'x86-microcode-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, microcode, AMD: Add microcode revision to /proc/cpuinfo
      x86, microcode: Correct microcode revision format
      coretemp: Get microcode revision from cpu_data
      x86, intel: Use c->microcode for Atom errata check
      x86, intel: Output microcode revision in /proc/cpuinfo
      x86, microcode: Don't request microcode from userspace unnecessarily
    
    Fix up trivial conflicts in arch/x86/kernel/cpu/amd.c (conflict between
    moving AMD BSP code to cpu_dev helper function and adding AMD microcode
    revision to /proc/cpuinfo code)

commit bcb80e53877c2045d9e52f4a71372c3fe6501f6f
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon Oct 17 16:34:36 2011 +0200

    x86, microcode, AMD: Add microcode revision to /proc/cpuinfo
    
    Enable microcode revision output for AMD after 506ed6b53e00 ("x86,
    intel: Output microcode revision in /proc/cpuinfo") did it for Intel.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b13ed393dfce..d898fab0e125 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -412,6 +412,8 @@ static void __cpuinit early_init_amd_mc(struct cpuinfo_x86 *c)
 
 static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
+	u32 dummy;
+
 	early_init_amd_mc(c);
 
 	/*
@@ -442,6 +444,8 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 	}
 #endif
 
+	rdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);
+
 	/* We need to do the following only once */
 	if (c != &boot_cpu_data)
 		return;

commit 5cdd174feab1b4a74afc494c447906274aed4a20
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Wed Aug 10 11:49:56 2011 +1000

    x86, amd: Include elf.h explicitly, prepare the code for the module.h split
    
    When the moduleu.h splitting tree is merged to the latest
    tip:x86/cpu tree, the x86_64 allmodconfig build fails like this:
    
     arch/x86/kernel/cpu/amd.c: In function 'bsp_init_amd':
     arch/x86/kernel/cpu/amd.c:437:3: error: 'va_align' undeclared (first use in this function)
     arch/x86/kernel/cpu/amd.c:438:23: error: 'ALIGN_VA_32' undeclared (first use in this function)
     arch/x86/kernel/cpu/amd.c:438:37: error: 'ALIGN_VA_64' undeclared (first use in this function)
    
    This is caused by the module.h split up intreacting with commit
    dfb09f9b7ab0 ("x86, amd: Avoid cache aliasing penalties on AMD
    family 15h") from the tip:x86/cpu tree.
    
    I have added the following patch for today (this, or something
    similar, could be applied to the tip tree directly - the
    export.h include below was added by the module.h splitup).
    
    So include elf.h to use va_align and remove this implicit
    dependency on module.h doing it for us.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Link: http://lkml.kernel.org/r/20110810114956.238d66772883636e3040d29f@canb.auug.org.au
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b6e3e87d25fb..13c6ec812545 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -1,5 +1,6 @@
 #include <linux/init.h>
 #include <linux/bitops.h>
+#include <linux/elf.h>
 #include <linux/mm.h>
 
 #include <linux/io.h>

commit 8fa8b035085e7320c15875c1f6b03b290ca2dd66
Author: Borislav Petkov <bp@amd64.org>
Date:   Fri Aug 5 20:04:09 2011 +0200

    x86, amd: Move BSP code to cpu_dev helper
    
    Move code which is run once on the BSP during boot into the cpu_dev
    helper.
    
    [ hpa: removed bogus cpu_has -> static_cpu_has conversion ]
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/20110805180409.GC26217@aftab
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b0234bcbd32a..b6e3e87d25fb 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -410,6 +410,34 @@ static void __cpuinit early_init_amd_mc(struct cpuinfo_x86 *c)
 #endif
 }
 
+static void __cpuinit bsp_init_amd(struct cpuinfo_x86 *c)
+{
+	if (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {
+
+		if (c->x86 > 0x10 ||
+		    (c->x86 == 0x10 && c->x86_model >= 0x2)) {
+			u64 val;
+
+			rdmsrl(MSR_K7_HWCR, val);
+			if (!(val & BIT(24)))
+				printk(KERN_WARNING FW_BUG "TSC doesn't count "
+					"with P0 frequency!\n");
+		}
+	}
+
+	if (c->x86 == 0x15) {
+		unsigned long upperbit;
+		u32 cpuid, assoc;
+
+		cpuid	 = cpuid_edx(0x80000005);
+		assoc	 = cpuid >> 16 & 0xff;
+		upperbit = ((cpuid >> 24) << 10) / assoc;
+
+		va_align.mask	  = (upperbit - 1) & PAGE_MASK;
+		va_align.flags    = ALIGN_VA_32 | ALIGN_VA_64;
+	}
+}
+
 static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
 	early_init_amd_mc(c);
@@ -441,36 +469,6 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
 	}
 #endif
-
-	/* We need to do the following only once */
-	if (c != &boot_cpu_data)
-		return;
-
-	if (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {
-
-		if (c->x86 > 0x10 ||
-		    (c->x86 == 0x10 && c->x86_model >= 0x2)) {
-			u64 val;
-
-			rdmsrl(MSR_K7_HWCR, val);
-			if (!(val & BIT(24)))
-				printk(KERN_WARNING FW_BUG "TSC doesn't count "
-					"with P0 frequency!\n");
-		}
-	}
-
-	if (c->x86 == 0x15) {
-		unsigned long upperbit;
-		u32 cpuid, assoc;
-
-		cpuid	 = cpuid_edx(0x80000005);
-		assoc	 = cpuid >> 16 & 0xff;
-		upperbit = ((cpuid >> 24) << 10) / assoc;
-
-		va_align.mask	  = (upperbit - 1) & PAGE_MASK;
-		va_align.flags    = ALIGN_VA_32 | ALIGN_VA_64;
-
-	}
 }
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
@@ -692,6 +690,7 @@ static const struct cpu_dev __cpuinitconst amd_cpu_dev = {
 	.c_size_cache	= amd_size_cache,
 #endif
 	.c_early_init   = early_init_amd,
+	.c_bsp_init	= bsp_init_amd,
 	.c_init		= init_amd,
 	.c_x86_vendor	= X86_VENDOR_AMD,
 };

commit dfb09f9b7ab03fd367740e541a5caf830ed56726
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Fri Aug 5 15:15:08 2011 +0200

    x86, amd: Avoid cache aliasing penalties on AMD family 15h
    
    This patch provides performance tuning for the "Bulldozer" CPU. With its
    shared instruction cache there is a chance of generating an excessive
    number of cache cross-invalidates when running specific workloads on the
    cores of a compute module.
    
    This excessive amount of cross-invalidations can be observed if cache
    lines backed by shared physical memory alias in bits [14:12] of their
    virtual addresses, as those bits are used for the index generation.
    
    This patch addresses the issue by clearing all the bits in the [14:12]
    slice of the file mapping's virtual address at generation time, thus
    forcing those bits the same for all mappings of a single shared library
    across processes and, in doing so, avoids instruction cache aliases.
    
    It also adds the command line option "align_va_addr=(32|64|on|off)" with
    which virtual address alignment can be enabled for 32-bit or 64-bit x86
    individually, or both, or be completely disabled.
    
    This change leaves virtual region address allocation on other families
    and/or vendors unaffected.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1312550110-24160-2-git-send-email-bp@amd64.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index b13ed393dfce..b0234bcbd32a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -458,6 +458,19 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 					"with P0 frequency!\n");
 		}
 	}
+
+	if (c->x86 == 0x15) {
+		unsigned long upperbit;
+		u32 cpuid, assoc;
+
+		cpuid	 = cpuid_edx(0x80000005);
+		assoc	 = cpuid >> 16 & 0xff;
+		upperbit = ((cpuid >> 24) << 10) / assoc;
+
+		va_align.mask	  = (upperbit - 1) & PAGE_MASK;
+		va_align.flags    = ALIGN_VA_32 | ALIGN_VA_64;
+
+	}
 }
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)

commit e9cdd343a5e42c43bcda01e609fa23089e026470
Author: Boris Ostrovsky <ostr@amd64.org>
Date:   Thu May 26 11:19:52 2011 -0400

    x86, amd: Do not enable ARAT feature on AMD processors below family 0x12
    
    Commit b87cf80af3ba4b4c008b4face3c68d604e1715c6 added support for
    ARAT (Always Running APIC timer) on AMD processors that are not
    affected by erratum 400. This erratum is present on certain processor
    families and prevents APIC timer from waking up the CPU when it
    is in a deep C state, including C1E state.
    
    Determining whether a processor is affected by this erratum may
    have some corner cases and handling these cases is somewhat
    complicated. In the interest of simplicity we won't claim ARAT
    support on processor families below 0x12 and will go back to
    broadcasting timer when going idle.
    
    Signed-off-by: Boris Ostrovsky <ostr@amd64.org>
    Link: http://lkml.kernel.org/r/1306423192-19774-1-git-send-email-ostr@amd64.org
    Tested-by: Boris Petkov <borislav.petkov@amd.com>
    Cc: Hans Rosenfeld <Hans.Rosenfeld@amd.com>
    Cc: Andreas Herrmann <Andreas.Herrmann3@amd.com>
    Cc: Chuck Ebbert <cebbert@redhat.com>
    Cc: stable@kernel.org # 32.x, 38.x, 39.x
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 8f5cabb3c5b0..b13ed393dfce 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -612,8 +612,11 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	}
 #endif
 
-	/* As a rule processors have APIC timer running in deep C states */
-	if (c->x86 > 0xf && !cpu_has_amd_erratum(amd_erratum_400))
+	/*
+	 * Family 0x12 and above processors have APIC timer
+	 * running in deep C states.
+	 */
+	if (c->x86 > 0x11)
 		set_cpu_cap(c, X86_FEATURE_ARAT);
 
 	/*

commit d47cc0db8fd6011de2248df505fc34990b7451bf
Author: Roedel, Joerg <Joerg.Roedel@amd.com>
Date:   Thu May 19 11:13:39 2011 +0200

    x86, amd: Use _safe() msr access for GartTlbWlk disable code
    
    The workaround for Bugzilla:
    
            https://bugzilla.kernel.org/show_bug.cgi?id=33012
    
    introduced a read and a write to the MC4 mask msr.
    
    Unfortunatly this MSR is not emulated by the KVM hypervisor
    so that the kernel will get a #GP and crashes when applying
    this workaround when running inside KVM.
    
    This issue was reported as:
    
            https://bugzilla.kernel.org/show_bug.cgi?id=35132
    
    and is fixed with this patch. The change just let the kernel
    ignore any #GP it gets while accessing this MSR by using the
    _safe msr access methods.
    
    Reported-by: Török Edwin <edwintorok@gmail.com>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Maciej Rutecki <maciej.rutecki@gmail.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: <stable@kernel.org> # .39.x
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 6f9d1f6063e9..8f5cabb3c5b0 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -629,10 +629,13 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		 * Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=33012
 		 */
 		u64 mask;
+		int err;
 
-		rdmsrl(MSR_AMD64_MCx_MASK(4), mask);
-		mask |= (1 << 10);
-		wrmsrl(MSR_AMD64_MCx_MASK(4), mask);
+		err = rdmsrl_safe(MSR_AMD64_MCx_MASK(4), &mask);
+		if (err == 0) {
+			mask |= (1 << 10);
+			checking_wrmsrl(MSR_AMD64_MCx_MASK(4), mask);
+		}
 	}
 }
 

commit 14fb57dccb6e1defe9f89a66f548fcb24c374c1d
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Tue May 17 14:55:19 2011 +0200

    x86, AMD: Fix ARAT feature setting again
    
    Trying to enable the local APIC timer on early K8 revisions
    uncovers a number of other issues with it, in conjunction with
    the C1E enter path on AMD. Fixing those causes much more churn
    and troubles than the benefit of using that timer brings so
    don't enable it on K8 at all, falling back to the original
    functionality the kernel had wrt to that.
    
    Reported-and-bisected-by: Nick Bowler <nbowler@elliptictech.com>
    Cc: Boris Ostrovsky <Boris.Ostrovsky@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Greg Kroah-Hartman <greg@kroah.com>
    Cc: Hans Rosenfeld <hans.rosenfeld@amd.com>
    Cc: Nick Bowler <nbowler@elliptictech.com>
    Cc: Joerg-Volker-Peetz <jvpeetz@web.de>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Link: http://lkml.kernel.org/r/1305636919-31165-3-git-send-email-bp@amd64.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 3532d3bf8105..6f9d1f6063e9 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -613,7 +613,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 #endif
 
 	/* As a rule processors have APIC timer running in deep C states */
-	if (c->x86 >= 0xf && !cpu_has_amd_erratum(amd_erratum_400))
+	if (c->x86 > 0xf && !cpu_has_amd_erratum(amd_erratum_400))
 		set_cpu_cap(c, X86_FEATURE_ARAT);
 
 	/*

commit 328935e6348c6a7cb34798a68c326f4b8372e68a
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Tue May 17 14:55:18 2011 +0200

    Revert "x86, AMD: Fix APIC timer erratum 400 affecting K8 Rev.A-E processors"
    
    This reverts commit e20a2d205c05cef6b5783df339a7d54adeb50962, as it crashes
    certain boxes with specific AMD CPU models.
    
    Moving the lower endpoint of the Erratum 400 check to accomodate
    earlier K8 revisions (A-E) opens a can of worms which is simply
    not worth to fix properly by tweaking the errata checking
    framework:
    
    * missing IntPenging MSR on revisions < CG cause #GP:
    
    http://marc.info/?l=linux-kernel&m=130541471818831
    
    * makes earlier revisions use the LAPIC timer instead of the C1E
    idle routine which switches to HPET, thus not waking up in
    deeper C-states:
    
    http://lkml.org/lkml/2011/4/24/20
    
    Therefore, leave the original boundary starting with K8-revF.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index bb9eb29a52dd..3532d3bf8105 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -698,7 +698,7 @@ cpu_dev_register(amd_cpu_dev);
  */
 
 const int amd_erratum_400[] =
-	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0x0f, 0x4, 0x2, 0xff, 0xf),
+	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
 			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
 EXPORT_SYMBOL_GPL(amd_erratum_400);
 

commit e20a2d205c05cef6b5783df339a7d54adeb50962
Author: Boris Ostrovsky <ostr@amd64.org>
Date:   Fri Apr 29 17:47:43 2011 -0400

    x86, AMD: Fix APIC timer erratum 400 affecting K8 Rev.A-E processors
    
    Older AMD K8 processors (Revisions A-E) are affected by erratum
    400 (APIC timer interrupts don't occur in C states greater than
    C1). This, for example, means that X86_FEATURE_ARAT flag should
    not be set for these parts.
    
    This addresses regression introduced by commit
    b87cf80af3ba4b4c008b4face3c68d604e1715c6 ("x86, AMD: Set ARAT
    feature on AMD processors") where the system may become
    unresponsive until external interrupt (such as keyboard input)
    occurs. This results, for example, in time not being reported
    correctly, lack of progress on the system and other lockups.
    
    Reported-by: Joerg-Volker Peetz <jvpeetz@web.de>
    Tested-by: Joerg-Volker Peetz <jvpeetz@web.de>
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: Boris Ostrovsky <Boris.Ostrovsky@amd.com>
    Cc: stable@kernel.org
    Link: http://lkml.kernel.org/r/1304113663-6586-1-git-send-email-ostr@amd64.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 3532d3bf8105..bb9eb29a52dd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -698,7 +698,7 @@ cpu_dev_register(amd_cpu_dev);
  */
 
 const int amd_erratum_400[] =
-	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
+	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0x0f, 0x4, 0x2, 0xff, 0xf),
 			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
 EXPORT_SYMBOL_GPL(amd_erratum_400);
 

commit 5bbc097d890409d8eff4e3f1d26f11a9d6b7c07e
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Apr 15 14:47:40 2011 +0200

    x86, amd: Disable GartTlbWlkErr when BIOS forgets it
    
    This patch disables GartTlbWlk errors on AMD Fam10h CPUs if
    the BIOS forgets to do is (or is just too old). Letting
    these errors enabled can cause a sync-flood on the CPU
    causing a reboot.
    
    The AMD BKDG recommends disabling GART TLB Wlk Error completely.
    
    This patch is the fix for
    
            https://bugzilla.kernel.org/show_bug.cgi?id=33012
    
    on my machine.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Link: http://lkml.kernel.org/r/20110415131152.GJ18463@8bytes.org
    Tested-by: Alexandre Demers <alexandre.f.demers@gmail.com>
    Cc: <stable@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 3ecece0217ef..3532d3bf8105 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -615,6 +615,25 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	/* As a rule processors have APIC timer running in deep C states */
 	if (c->x86 >= 0xf && !cpu_has_amd_erratum(amd_erratum_400))
 		set_cpu_cap(c, X86_FEATURE_ARAT);
+
+	/*
+	 * Disable GART TLB Walk Errors on Fam10h. We do this here
+	 * because this is always needed when GART is enabled, even in a
+	 * kernel which has no MCE support built in.
+	 */
+	if (c->x86 == 0x10) {
+		/*
+		 * BIOS should disable GartTlbWlk Errors themself. If
+		 * it doesn't do it here as suggested by the BKDG.
+		 *
+		 * Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=33012
+		 */
+		u64 mask;
+
+		rdmsrl(MSR_AMD64_MCx_MASK(4), mask);
+		mask |= (1 << 10);
+		wrmsrl(MSR_AMD64_MCx_MASK(4), mask);
+	}
 }
 
 #ifdef CONFIG_X86_32

commit 41e0e0738cf864f4f49b11aac804496999b311d9
Merge: e7fd3b4669f5 344c21c32287
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 10:14:56 2011 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, AMD: Set ARAT feature on AMD processors
      x86, quirk: Fix SB600 revision check
      x86: stop_machine_text_poke() should issue sync_core()
      x86, amd-nb: Misc cleanliness fixes

commit 344c21c32287755b5cda0eeb84adb4546a57c1db
Merge: b87cf80af3ba 84fd1d35cc86
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 16 16:33:56 2011 +0100

    Merge branch 'x86/amd-nb' into x86/urgent
    
    Merge reason: This is one followup commit that was not in x86/mm - merge it via the urgent path
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit b87cf80af3ba4b4c008b4face3c68d604e1715c6
Author: Boris Ostrovsky <boris.ostrovsky@amd.com>
Date:   Tue Mar 15 12:13:44 2011 -0400

    x86, AMD: Set ARAT feature on AMD processors
    
    Support for Always Running APIC timer (ARAT) was introduced in
    commit db954b5898dd3ef3ef93f4144158ea8f97deb058. This feature
    allows us to avoid switching timers from LAPIC to something else
    (e.g. HPET) and go into timer broadcasts when entering deep
    C-states.
    
    AMD processors don't provide a CPUID bit for that feature but
    they also keep APIC timers running in deep C-states (except for
    cases when the processor is affected by erratum 400). Therefore
    we should set ARAT feature bit on AMD CPUs.
    
    Tested-by: Borislav Petkov <borislav.petkov@amd.com>
    Acked-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@amd.com>
    LKML-Reference: <1300205624-4813-1-git-send-email-ostr@amd64.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7c7bedb83c5a..48eaa1b6fc46 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -594,6 +594,10 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		}
 	}
 #endif
+
+	/* As a rule processors have APIC timer running in deep C states */
+	if (c->x86 >= 0xf && !cpu_has_amd_erratum(amd_erratum_400))
+		set_cpu_cap(c, X86_FEATURE_ARAT);
 }
 
 #ifdef CONFIG_X86_32

commit 275a88d3cf0e2f08a98dc5ce9494af0cb6ed2092
Merge: 52b8b8d7251f 9e81509efc4f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 16 09:45:33 2011 +0100

    Merge branch 'x86/amd-nb' into x86/mm
    
    Merge reason: consolidate it into the more generic x86/mm tree to prevent conflicts
                  with ongoing NUMA work.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9e81509efc4fefcdd75cc6a4121672fa71ae8745
Author: Borislav Petkov <bp@amd64.org>
Date:   Mon Feb 14 18:14:51 2011 +0100

    x86, amd: Initialize variable properly
    
    Commit d518573de63f ("x86, amd: Normalize compute unit IDs on
    multi-node processors") introduced compute unit normalization
    but causes a compiler warning:
    
     arch/x86/kernel/cpu/amd.c: In function 'amd_detect_cmp':
     arch/x86/kernel/cpu/amd.c:268: warning: 'cores_per_cu' may be used uninitialized in this function
     arch/x86/kernel/cpu/amd.c:268: note: 'cores_per_cu' was declared here
    
    The compiler is right - initialize it with a proper value.
    
    Also, fix up a comment while at it.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20110214171451.GB10076@kryptos.osrc.amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 990cc4861586..589bdd7a4cff 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -261,7 +261,7 @@ static int __cpuinit nearby_node(int apicid)
 #ifdef CONFIG_X86_HT
 static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 {
-	u32 nodes, cores_per_cu;
+	u32 nodes, cores_per_cu = 1;
 	u8 node_id;
 	int cpu = smp_processor_id();
 
@@ -276,7 +276,7 @@ static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 		/* get compute unit information */
 		smp_num_siblings = ((ebx >> 8) & 3) + 1;
 		c->compute_unit_id = ebx & 0xff;
-		cores_per_cu = ((ebx >> 8) & 3) + 1;
+		cores_per_cu += ((ebx >> 8) & 3);
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
 		u64 value;
 
@@ -298,7 +298,7 @@ static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 		/* store NodeID, use llc_shared_map to store sibling info */
 		per_cpu(cpu_llc_id, cpu) = node_id;
 
-		/* core id to be in range from 0 to (cores_per_node - 1) */
+		/* core id has to be in the [0 .. cores_per_node - 1] range */
 		c->cpu_core_id %= cores_per_node;
 		c->compute_unit_id %= cus_per_node;
 	}

commit 645a79195f66eb68ef3ab2b21d9829ac3aa085a9
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:40 2011 +0100

    x86: Unify CPU -> NUMA node mapping between 32 and 64bit
    
    Unlike 64bit, 32bit has been using its own cpu_to_node_map[] for
    CPU -> NUMA node mapping.  Replace it with early_percpu variable
    x86_cpu_to_node_map and share the mapping code with 64bit.
    
    * USE_PERCPU_NUMA_NODE_ID is now enabled for 32bit too.
    
    * x86_cpu_to_node_map and numa_set/clear_node() are moved from
      numa_64 to numa.  For now, on 32bit, x86_cpu_to_node_map is initialized
      with 0 instead of NUMA_NO_NODE.  This is to avoid introducing unexpected
      behavior change and will be updated once init path is unified.
    
    * srat_detect_node() is now enabled for x86_32 too.  It calls
      numa_set_node() and initializes the mapping making explicit
      cpu_to_node_map[] updates from map/unmap_cpu_to_node() unnecessary.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: penberg@kernel.org
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-15-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 3cce8f2bb2e1..77858fd64620 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -233,7 +233,7 @@ static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
 }
 #endif
 
-#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+#ifdef CONFIG_NUMA
 /*
  * To workaround broken NUMA config.  Read the comment in
  * srat_detect_node().
@@ -338,7 +338,7 @@ EXPORT_SYMBOL_GPL(amd_get_nb_id);
 
 static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 {
-#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+#ifdef CONFIG_NUMA
 	int cpu = smp_processor_id();
 	int node;
 	unsigned apicid = c->apicid;

commit bbc9e2f452d9c4b166d1f9a78d941d80173312fe
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 23 14:37:39 2011 +0100

    x86: Unify cpu/apicid <-> NUMA node mapping between 32 and 64bit
    
    The mapping between cpu/apicid and node is done via
    apicid_to_node[] on 64bit and apicid_2_node[] +
    apic->x86_32_numa_cpu_node() on 32bit. This difference makes it
    difficult to further unify 32 and 64bit NUMA handling.
    
    This patch unifies it by replacing both apicid_to_node[] and
    apicid_2_node[] with __apicid_to_node[] array, which is accessed
    by two accessors - set_apicid_to_node() and numa_cpu_node().  On
    64bit, numa_cpu_node() always consults __apicid_to_node[]
    directly while 32bit goes through apic->numa_cpu_node() method
    to allow apic implementations to override it.
    
    srat_detect_node() for amd cpus contains workaround for broken
    NUMA configuration which assumes relationship between APIC ID,
    HT node ID and NUMA topology.  Leave it to access
    __apicid_to_node[] directly as mapping through CPU might result
    in undesirable behavior change.  The comment is reformatted and
    updated to note the ugliness.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Cc: eric.dumazet@gmail.com
    Cc: yinghai@kernel.org
    Cc: brgerst@gmail.com
    Cc: gorcunov@gmail.com
    Cc: shaohui.zheng@intel.com
    Cc: rientjes@google.com
    LKML-Reference: <1295789862-25482-14-git-send-email-tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Rientjes <rientjes@google.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7c7bedb83c5a..3cce8f2bb2e1 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -234,17 +234,21 @@ static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
 #endif
 
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+/*
+ * To workaround broken NUMA config.  Read the comment in
+ * srat_detect_node().
+ */
 static int __cpuinit nearby_node(int apicid)
 {
 	int i, node;
 
 	for (i = apicid - 1; i >= 0; i--) {
-		node = apicid_to_node[i];
+		node = __apicid_to_node[i];
 		if (node != NUMA_NO_NODE && node_online(node))
 			return node;
 	}
 	for (i = apicid + 1; i < MAX_LOCAL_APIC; i++) {
-		node = apicid_to_node[i];
+		node = __apicid_to_node[i];
 		if (node != NUMA_NO_NODE && node_online(node))
 			return node;
 	}
@@ -339,26 +343,35 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 	int node;
 	unsigned apicid = c->apicid;
 
-	node = per_cpu(cpu_llc_id, cpu);
+	node = numa_cpu_node(cpu);
+	if (node == NUMA_NO_NODE)
+		node = per_cpu(cpu_llc_id, cpu);
 
-	if (apicid_to_node[apicid] != NUMA_NO_NODE)
-		node = apicid_to_node[apicid];
 	if (!node_online(node)) {
-		/* Two possibilities here:
-		   - The CPU is missing memory and no node was created.
-		   In that case try picking one from a nearby CPU
-		   - The APIC IDs differ from the HyperTransport node IDs
-		   which the K8 northbridge parsing fills in.
-		   Assume they are all increased by a constant offset,
-		   but in the same order as the HT nodeids.
-		   If that doesn't result in a usable node fall back to the
-		   path for the previous case.  */
-
+		/*
+		 * Two possibilities here:
+		 *
+		 * - The CPU is missing memory and no node was created.  In
+		 *   that case try picking one from a nearby CPU.
+		 *
+		 * - The APIC IDs differ from the HyperTransport node IDs
+		 *   which the K8 northbridge parsing fills in.  Assume
+		 *   they are all increased by a constant offset, but in
+		 *   the same order as the HT nodeids.  If that doesn't
+		 *   result in a usable node fall back to the path for the
+		 *   previous case.
+		 *
+		 * This workaround operates directly on the mapping between
+		 * APIC ID and NUMA node, assuming certain relationship
+		 * between APIC ID, HT node ID and NUMA topology.  As going
+		 * through CPU mapping may alter the outcome, directly
+		 * access __apicid_to_node[].
+		 */
 		int ht_nodeid = c->initial_apicid;
 
 		if (ht_nodeid >= 0 &&
-		    apicid_to_node[ht_nodeid] != NUMA_NO_NODE)
-			node = apicid_to_node[ht_nodeid];
+		    __apicid_to_node[ht_nodeid] != NUMA_NO_NODE)
+			node = __apicid_to_node[ht_nodeid];
 		/* Pick a nearby node */
 		if (!node_online(node))
 			node = nearby_node(apicid);

commit d518573de63fb119e5e9a3137386544671387681
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Mon Jan 24 16:05:40 2011 +0100

    x86, amd: Normalize compute unit IDs on multi-node processors
    
    On multi-node CPUs we don't need the socket wide compute unit ID
    but the node-wide compute unit ID. Thus we need to normalize the
    value. This is similar to what we do with cpu_core_id.
    
    A compute unit is then identified by physical_package_id,
    node_id, and compute_unit_id.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <1295881543-572552-2-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7c7bedb83c5a..990cc4861586 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -261,7 +261,7 @@ static int __cpuinit nearby_node(int apicid)
 #ifdef CONFIG_X86_HT
 static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 {
-	u32 nodes;
+	u32 nodes, cores_per_cu;
 	u8 node_id;
 	int cpu = smp_processor_id();
 
@@ -276,6 +276,7 @@ static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 		/* get compute unit information */
 		smp_num_siblings = ((ebx >> 8) & 3) + 1;
 		c->compute_unit_id = ebx & 0xff;
+		cores_per_cu = ((ebx >> 8) & 3) + 1;
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
 		u64 value;
 
@@ -288,15 +289,18 @@ static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 	/* fixup multi-node processor information */
 	if (nodes > 1) {
 		u32 cores_per_node;
+		u32 cus_per_node;
 
 		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
 		cores_per_node = c->x86_max_cores / nodes;
+		cus_per_node = cores_per_node / cores_per_cu;
 
 		/* store NodeID, use llc_shared_map to store sibling info */
 		per_cpu(cpu_llc_id, cpu) = node_id;
 
 		/* core id to be in range from 0 to (cores_per_node - 1) */
-		c->cpu_core_id = c->cpu_core_id % cores_per_node;
+		c->cpu_core_id %= cores_per_node;
+		c->compute_unit_id %= cus_per_node;
 	}
 }
 #endif

commit 7b543a5334ff4ea2e3ad3b777fc23cdb8072a988
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Dec 18 16:30:05 2010 +0100

    x86: Replace uses of current_cpu_data with this_cpu ops
    
    Replace all uses of current_cpu_data with this_cpu operations on the
    per cpu structure cpu_info.  The scala accesses are replaced with the
    matching this_cpu ops which results in smaller and more efficient
    code.
    
    In the long run, it might be a good idea to remove cpu_data() macro
    too and use per_cpu macro directly.
    
    tj: updated description
    
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 9e093f8fe78c..7c7bedb83c5a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -668,7 +668,7 @@ EXPORT_SYMBOL_GPL(amd_erratum_383);
 
 bool cpu_has_amd_erratum(const int *erratum)
 {
-	struct cpuinfo_x86 *cpu = &current_cpu_data;
+	struct cpuinfo_x86 *cpu = __this_cpu_ptr(&cpu_info);
 	int osvw_id = *erratum++;
 	u32 range;
 	u32 ms;

commit d60a2793ba562c6ea9bbf62112da3e6342adcf83
Merge: 781c5a67f152 40ffa9379198
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:18:06 2010 -0700

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Remove stale pmtimer_64.c
      x86, cleanups: Use clear_page/copy_page rather than memset/memcpy
      x86: Remove unnecessary #ifdef ACPI/X86_IO_ACPI
      x86, cleanup: Remove obsolete boot_cpu_id variable

commit 6057b4d331f19a3ea51aec463ea7839c128b3227
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 30 14:38:57 2010 +0200

    x86, amd: Extract compute unit information for AMD CPUs
    
    Get compute unit information from CPUID Fn8000_001E_EBX.
    (See AMD CPUID Specification - publication # 25481, revision 2.34,
    September 2010.)
    
    Note that each core on a compute unit still has a core_id of its own.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20100930123857.GE20545@loge.amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7e6a37d24253..70168ab88b7f 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -256,21 +256,29 @@ static int __cpuinit nearby_node(int apicid)
  * Fixup core topology information for
  * (1) AMD multi-node processors
  *     Assumption: Number of cores in each internal node is the same.
+ * (2) AMD processors supporting compute units
  */
 #ifdef CONFIG_X86_HT
 static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 {
-	u32 nodes, cores_per_node;
+	u32 nodes;
 	u8 node_id;
-	unsigned long long value;
 	int cpu = smp_processor_id();
 
 	/* get information required for multi-node processors */
 	if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
-		value = cpuid_ecx(0x8000001e);
-		nodes = ((value >> 8) & 7) + 1;
-		node_id = value & 7;
+		u32 eax, ebx, ecx, edx;
+
+		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
+		nodes = ((ecx >> 8) & 7) + 1;
+		node_id = ecx & 7;
+
+		/* get compute unit information */
+		smp_num_siblings = ((ebx >> 8) & 3) + 1;
+		c->compute_unit_id = ebx & 0xff;
 	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
+		u64 value;
+
 		rdmsrl(MSR_FAM10H_NODE_ID, value);
 		nodes = ((value >> 3) & 7) + 1;
 		node_id = value & 7;
@@ -279,6 +287,8 @@ static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 
 	/* fixup multi-node processor information */
 	if (nodes > 1) {
+		u32 cores_per_node;
+
 		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
 		cores_per_node = c->x86_max_cores / nodes;
 

commit 23588c38a84c9175c6668789b64ffba4651e5c6a
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 30 14:36:28 2010 +0200

    x86, amd: Add support for CPUID topology extension of AMD CPUs
    
    Node information (ID, number of internal nodes) is provided via
    CPUID Fn8000_001e_ECX.
    
    See AMD CPUID Specification (Publication # 25481, Revision 2.34,
    September 2010).
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20100930123628.GD20545@loge.amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 0f0ace5d7db5..7e6a37d24253 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -253,37 +253,41 @@ static int __cpuinit nearby_node(int apicid)
 #endif
 
 /*
- * Fixup core topology information for AMD multi-node processors.
- * Assumption: Number of cores in each internal node is the same.
+ * Fixup core topology information for
+ * (1) AMD multi-node processors
+ *     Assumption: Number of cores in each internal node is the same.
  */
 #ifdef CONFIG_X86_HT
-static void __cpuinit amd_fixup_dcm(struct cpuinfo_x86 *c)
+static void __cpuinit amd_get_topology(struct cpuinfo_x86 *c)
 {
-	unsigned long long value;
 	u32 nodes, cores_per_node;
+	u8 node_id;
+	unsigned long long value;
 	int cpu = smp_processor_id();
 
-	if (!cpu_has(c, X86_FEATURE_NODEID_MSR))
-		return;
-
-	/* fixup topology information only once for a core */
-	if (cpu_has(c, X86_FEATURE_AMD_DCM))
+	/* get information required for multi-node processors */
+	if (cpu_has(c, X86_FEATURE_TOPOEXT)) {
+		value = cpuid_ecx(0x8000001e);
+		nodes = ((value >> 8) & 7) + 1;
+		node_id = value & 7;
+	} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {
+		rdmsrl(MSR_FAM10H_NODE_ID, value);
+		nodes = ((value >> 3) & 7) + 1;
+		node_id = value & 7;
+	} else
 		return;
 
-	rdmsrl(MSR_FAM10H_NODE_ID, value);
+	/* fixup multi-node processor information */
+	if (nodes > 1) {
+		set_cpu_cap(c, X86_FEATURE_AMD_DCM);
+		cores_per_node = c->x86_max_cores / nodes;
 
-	nodes = ((value >> 3) & 7) + 1;
-	if (nodes == 1)
-		return;
-
-	set_cpu_cap(c, X86_FEATURE_AMD_DCM);
-	cores_per_node = c->x86_max_cores / nodes;
+		/* store NodeID, use llc_shared_map to store sibling info */
+		per_cpu(cpu_llc_id, cpu) = node_id;
 
-	/* store NodeID, use llc_shared_map to store sibling info */
-	per_cpu(cpu_llc_id, cpu) = value & 7;
-
-	/* fixup core id to be in range from 0 to (cores_per_node - 1) */
-	c->cpu_core_id = c->cpu_core_id % cores_per_node;
+		/* core id to be in range from 0 to (cores_per_node - 1) */
+		c->cpu_core_id = c->cpu_core_id % cores_per_node;
+	}
 }
 #endif
 
@@ -304,9 +308,7 @@ static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
 	c->phys_proc_id = c->initial_apicid >> bits;
 	/* use socket ID also for last level cache */
 	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
-	/* fixup topology information on multi-node processors */
-	if ((c->x86 == 0x10) && (c->x86_model == 9))
-		amd_fixup_dcm(c);
+	amd_get_topology(c);
 #endif
 }
 

commit d9fadd7ba99a67030783a212bcb17d11f0678433
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 2 15:37:10 2010 +0200

    x86, AMD: Remove needless CPU family check (for L3 cache info)
    
    Old 32-bit AMD CPUs (all w/o L3 cache) should always return 0
    for cpuid_edx(0x80000006).
    
    For unknown reason the 32-bit implementation differed from the
    64-bit implementation. See commit 67cddd94799 ("i386: Add L3 cache
    support to AMD CPUID4 emulation"). The current check is the
    result of the x86 merge.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    LKML-Reference: <20100902133710.GA5449@loge.amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index fc563fabde67..0f0ace5d7db5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -540,7 +540,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 #endif
 
 	if (c->extended_cpuid_level >= 0x80000006) {
-		if ((c->x86 >= 0x0f) && (cpuid_edx(0x80000006) & 0xf000))
+		if (cpuid_edx(0x80000006) & 0xf000)
 			num_cache_leaves = 4;
 		else
 			num_cache_leaves = 3;

commit acf01734b1747b1ec4be6f159aff579ea5f7f8e2
Author: Borislav Petkov <bp@amd64.org>
Date:   Wed Aug 25 18:28:23 2010 +0200

    x86, tsc: Remove CPU frequency calibration on AMD
    
    6b37f5a20c0e5c334c010a587058354215433e92 introduced the CPU frequency
    calibration code for AMD CPUs whose TSCs didn't increment with the
    core's P0 frequency. From F10h, revB onward, however, the TSC increment
    rate is denoted by MSRC001_0015[24] and when this bit is set (which
    should be done by the BIOS) the TSC increments with the P0 frequency
    so the calibration is not needed and booting can be a couple of mcecs
    faster on those machines.
    
    Besides, there should be virtually no machines out there which don't
    have this bit set, therefore this calibration can be safely removed. It
    is a shaky hack anyway since it assumes implicitly that the core is in
    P0 when BIOS hands off to the OS, which might not always be the case.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100825162823.GE26438@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ba5f62f45f01..fc563fabde67 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -412,6 +412,23 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
 	}
 #endif
+
+	/* We need to do the following only once */
+	if (c != &boot_cpu_data)
+		return;
+
+	if (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {
+
+		if (c->x86 > 0x10 ||
+		    (c->x86 == 0x10 && c->x86_model >= 0x2)) {
+			u64 val;
+
+			rdmsrl(MSR_K7_HWCR, val);
+			if (!(val & BIT(24)))
+				printk(KERN_WARNING FW_BUG "TSC doesn't count "
+					"with P0 frequency!\n");
+		}
+	}
 }
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)

commit 07a7795ca2e6e66d00b184efb46bd0e23d90d3fe
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Aug 18 16:19:50 2010 +0200

    x86, cpu: Fix regression in AMD errata checking code
    
    A bug in the family-model-stepping matching code caused the presence of
    errata to go undetected when OSVW was not used. This causes hangs on
    some K8 systems because the E400 workaround is not enabled.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1282141190-930137-1-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 60a57b13082d..ba5f62f45f01 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -669,7 +669,7 @@ bool cpu_has_amd_erratum(const int *erratum)
 	}
 
 	/* OSVW unavailable or ID unknown, match family-model-stepping range */
-	ms = (cpu->x86_model << 8) | cpu->x86_mask;
+	ms = (cpu->x86_model << 4) | cpu->x86_mask;
 	while ((range = *erratum++))
 		if ((cpu->x86 == AMD_MODEL_RANGE_FAMILY(range)) &&
 		    (ms >= AMD_MODEL_RANGE_START(range)) &&

commit f6e9456c9272bb570df6e217cdbe007e270b1c4e
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jul 21 19:03:58 2010 +0200

    x86, cleanup: Remove obsolete boot_cpu_id variable
    
    boot_cpu_id is there for historical reasons and was renamed to
    boot_cpu_physical_apicid in patch:
    
     c70dcb7 x86: change boot_cpu_id to boot_cpu_physical_apicid
    
    However, there are some remaining occurrences of boot_cpu_id that are
    never touched in the kernel and thus its value is always 0.
    
    This patch removes boot_cpu_id completely.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    LKML-Reference: <1279731838-1522-8-git-send-email-robert.richter@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 60a57b13082d..3a7c852f021d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -148,7 +148,7 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
 	/* calling is from identify_secondary_cpu() ? */
-	if (c->cpu_index == boot_cpu_id)
+	if (!c->cpu_index)
 		return;
 
 	/*

commit a5b91606bdc9d0a0d036d2d829a22921c705573e
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Wed Jul 28 16:23:20 2010 -0700

    x86, cpu: Export AMD errata definitions
    
    Exprot the AMD errata definitions, since they are needed by kvm_amd.ko
    if that is built as a module.  Doing "make allmodconfig" during
    testing would have caught this.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-1-git-send-email-hans.rosenfeld@amd.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 30f30dcbdb82..60a57b13082d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -631,9 +631,11 @@ cpu_dev_register(amd_cpu_dev);
 const int amd_erratum_400[] =
 	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
 			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
+EXPORT_SYMBOL_GPL(amd_erratum_400);
 
 const int amd_erratum_383[] =
 	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
+EXPORT_SYMBOL_GPL(amd_erratum_383);
 
 bool cpu_has_amd_erratum(const int *erratum)
 {
@@ -676,3 +678,5 @@ bool cpu_has_amd_erratum(const int *erratum)
 
 	return false;
 }
+
+EXPORT_SYMBOL_GPL(cpu_has_amd_erratum);

commit 1be85a6d93f4207d8c2c6238c4a96895e28cefba
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Jul 28 19:09:32 2010 +0200

    x86, cpu: Use AMD errata checking framework for erratum 383
    
    Use the AMD errata checking framework instead of open-coding the test.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-3-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a62a4ae7a11a..30f30dcbdb82 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -632,6 +632,8 @@ const int amd_erratum_400[] =
 	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
 			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
 
+const int amd_erratum_383[] =
+	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
 
 bool cpu_has_amd_erratum(const int *erratum)
 {

commit 9d8888c2a214aece2494a49e699a097c2ba9498b
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Jul 28 19:09:31 2010 +0200

    x86, cpu: Clean up AMD erratum 400 workaround
    
    Remove check_c1e_idle() and use the new AMD errata checking framework
    instead.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-2-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 80665410b064..a62a4ae7a11a 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -628,6 +628,11 @@ cpu_dev_register(amd_cpu_dev);
  *			   AMD_MODEL_RANGE(0x10, 0x9, 0x0, 0x9, 0x0));
  */
 
+const int amd_erratum_400[] =
+	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
+			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
+
+
 bool cpu_has_amd_erratum(const int *erratum)
 {
 	struct cpuinfo_x86 *cpu = &current_cpu_data;

commit d78d671db478eb8b14c78501c0cee1cc7baf6967
Author: Hans Rosenfeld <hans.rosenfeld@amd.com>
Date:   Wed Jul 28 19:09:30 2010 +0200

    x86, cpu: AMD errata checking framework
    
    Errata are defined using the AMD_LEGACY_ERRATUM() or AMD_OSVW_ERRATUM()
    macros. The latter is intended for newer errata that have an OSVW id
    assigned, which it takes as first argument. Both take a variable number
    of family-specific model-stepping ranges created by AMD_MODEL_RANGE().
    
    Iff an erratum has an OSVW id, OSVW is available on the CPU, and the
    OSVW id is known to the hardware, it is used to determine whether an
    erratum is present. Otherwise, the model-stepping ranges are matched
    against the current CPU to find out whether the erratum applies.
    
    For certain special errata, the code using this framework might have to
    conduct further checks to make sure an erratum is really (not) present.
    
    Signed-off-by: Hans Rosenfeld <hans.rosenfeld@amd.com>
    LKML-Reference: <1280336972-865982-1-git-send-email-hans.rosenfeld@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 12b9cff047c1..80665410b064 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -609,3 +609,63 @@ static const struct cpu_dev __cpuinitconst amd_cpu_dev = {
 };
 
 cpu_dev_register(amd_cpu_dev);
+
+/*
+ * AMD errata checking
+ *
+ * Errata are defined as arrays of ints using the AMD_LEGACY_ERRATUM() or
+ * AMD_OSVW_ERRATUM() macros. The latter is intended for newer errata that
+ * have an OSVW id assigned, which it takes as first argument. Both take a
+ * variable number of family-specific model-stepping ranges created by
+ * AMD_MODEL_RANGE(). Each erratum also has to be declared as extern const
+ * int[] in arch/x86/include/asm/processor.h.
+ *
+ * Example:
+ *
+ * const int amd_erratum_319[] =
+ *	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0x4, 0x2),
+ *			   AMD_MODEL_RANGE(0x10, 0x8, 0x0, 0x8, 0x0),
+ *			   AMD_MODEL_RANGE(0x10, 0x9, 0x0, 0x9, 0x0));
+ */
+
+bool cpu_has_amd_erratum(const int *erratum)
+{
+	struct cpuinfo_x86 *cpu = &current_cpu_data;
+	int osvw_id = *erratum++;
+	u32 range;
+	u32 ms;
+
+	/*
+	 * If called early enough that current_cpu_data hasn't been initialized
+	 * yet, fall back to boot_cpu_data.
+	 */
+	if (cpu->x86 == 0)
+		cpu = &boot_cpu_data;
+
+	if (cpu->x86_vendor != X86_VENDOR_AMD)
+		return false;
+
+	if (osvw_id >= 0 && osvw_id < 65536 &&
+	    cpu_has(cpu, X86_FEATURE_OSVW)) {
+		u64 osvw_len;
+
+		rdmsrl(MSR_AMD64_OSVW_ID_LENGTH, osvw_len);
+		if (osvw_id < osvw_len) {
+			u64 osvw_bits;
+
+			rdmsrl(MSR_AMD64_OSVW_STATUS + (osvw_id >> 6),
+			    osvw_bits);
+			return osvw_bits & (1ULL << (osvw_id & 0x3f));
+		}
+	}
+
+	/* OSVW unavailable or ID unknown, match family-model-stepping range */
+	ms = (cpu->x86_model << 8) | cpu->x86_mask;
+	while ((range = *erratum++))
+		if ((cpu->x86 == AMD_MODEL_RANGE_FAMILY(range)) &&
+		    (ms >= AMD_MODEL_RANGE_START(range)) &&
+		    (ms <= AMD_MODEL_RANGE_END(range)))
+			return true;
+
+	return false;
+}

commit 12d8a961289644d265d8b3e88201878837c3b814
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Wed Jun 2 20:29:21 2010 +0200

    x86, AMD: Extend support to future families
    
    Extend support to future families, and in particular:
    
    * extend direct mapping split of Tseg SMM area.
    * extend K8 flavored alternatives (NOPS).
    * rep movs* prefix is fast in ucode.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100602182921.GA21557@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e485825130d2..12b9cff047c1 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -466,7 +466,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		}
 
 	}
-	if (c->x86 == 0x10 || c->x86 == 0x11)
+	if (c->x86 >= 0x10)
 		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
 
 	/* get apicid instead of initial apic id from cpuid */
@@ -529,7 +529,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 			num_cache_leaves = 3;
 	}
 
-	if (c->x86 >= 0xf && c->x86 <= 0x11)
+	if (c->x86 >= 0xf)
 		set_cpu_cap(c, X86_FEATURE_K8);
 
 	if (cpu_has_xmm2) {
@@ -546,7 +546,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		fam10h_check_enable_mmcfg();
 	}
 
-	if (c == &boot_cpu_data && c->x86 >= 0xf && c->x86 <= 0x11) {
+	if (c == &boot_cpu_data && c->x86 >= 0xf) {
 		unsigned long long tseg;
 
 		/*

commit 9d260ebc09a0ad6b5c73e17676df42c7bc75ff64
Author: Andreas Herrmann <herrmann.der.user@googlemail.com>
Date:   Wed Dec 16 15:43:55 2009 +0100

    x86, amd: Get multi-node CPU info from NodeId MSR instead of PCI config space
    
    Use NodeId MSR to get NodeId and number of nodes per processor.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20091216144355.GB28798@alberich.amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 8dc3ea145c97..e485825130d2 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -254,59 +254,36 @@ static int __cpuinit nearby_node(int apicid)
 
 /*
  * Fixup core topology information for AMD multi-node processors.
- * Assumption 1: Number of cores in each internal node is the same.
- * Assumption 2: Mixed systems with both single-node and dual-node
- *               processors are not supported.
+ * Assumption: Number of cores in each internal node is the same.
  */
 #ifdef CONFIG_X86_HT
 static void __cpuinit amd_fixup_dcm(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_PCI
-	u32 t, cpn;
-	u8 n, n_id;
+	unsigned long long value;
+	u32 nodes, cores_per_node;
 	int cpu = smp_processor_id();
 
+	if (!cpu_has(c, X86_FEATURE_NODEID_MSR))
+		return;
+
 	/* fixup topology information only once for a core */
 	if (cpu_has(c, X86_FEATURE_AMD_DCM))
 		return;
 
-	/* check for multi-node processor on boot cpu */
-	t = read_pci_config(0, 24, 3, 0xe8);
-	if (!(t & (1 << 29)))
+	rdmsrl(MSR_FAM10H_NODE_ID, value);
+
+	nodes = ((value >> 3) & 7) + 1;
+	if (nodes == 1)
 		return;
 
 	set_cpu_cap(c, X86_FEATURE_AMD_DCM);
+	cores_per_node = c->x86_max_cores / nodes;
 
-	/* cores per node: each internal node has half the number of cores */
-	cpn = c->x86_max_cores >> 1;
+	/* store NodeID, use llc_shared_map to store sibling info */
+	per_cpu(cpu_llc_id, cpu) = value & 7;
 
-	/* even-numbered NB_id of this dual-node processor */
-	n = c->phys_proc_id << 1;
-
-	/*
-	 * determine internal node id and assign cores fifty-fifty to
-	 * each node of the dual-node processor
-	 */
-	t = read_pci_config(0, 24 + n, 3, 0xe8);
-	n = (t>>30) & 0x3;
-	if (n == 0) {
-		if (c->cpu_core_id < cpn)
-			n_id = 0;
-		else
-			n_id = 1;
-	} else {
-		if (c->cpu_core_id < cpn)
-			n_id = 1;
-		else
-			n_id = 0;
-	}
-
-	/* compute entire NodeID, use llc_shared_map to store sibling info */
-	per_cpu(cpu_llc_id, cpu) = (c->phys_proc_id << 1) + n_id;
-
-	/* fixup core id to be in range from 0 to cpn */
-	c->cpu_core_id = c->cpu_core_id % cpn;
-#endif
+	/* fixup core id to be in range from 0 to (cores_per_node - 1) */
+	c->cpu_core_id = c->cpu_core_id % cores_per_node;
 }
 #endif
 

commit 2eaad1fddd7450a48ad464229775f97fbfe8af36
Author: Mike Travis <travis@sgi.com>
Date:   Thu Dec 10 17:19:36 2009 -0800

    x86: Limit the number of processor bootup messages
    
    When there are a large number of processors in a system, there
    is an excessive amount of messages sent to the system console.
    It's estimated that with 4096 processors in a system, and the
    console baudrate set to 56K, the startup messages will take
    about 84 minutes to clear the serial port.
    
    This set of patches limits the number of repetitious messages
    which contain no additional information.  Much of this information
    is obtainable from the /proc and /sysfs.   Some of the messages
    are also sent to the kernel log buffer as KERN_DEBUG messages so
    dmesg can be used to examine more closely any details specific to
    a problem.
    
    The new cpu bootup sequence for system_state == SYSTEM_BOOTING:
    
    Booting Node   0, Processors  #1 #2 #3 #4 #5 #6 #7 Ok.
    Booting Node   1, Processors  #8 #9 #10 #11 #12 #13 #14 #15 Ok.
    ...
    Booting Node   3, Processors  #56 #57 #58 #59 #60 #61 #62 #63 Ok.
    Brought up 64 CPUs
    
    After the system is running, a single line boot message is displayed
    when CPU's are hotplugged on:
    
        Booting Node %d Processor %d APIC 0x%x
    
    Status of the following lines:
    
        CPU: Physical Processor ID:         printed once (for boot cpu)
        CPU: Processor Core ID:             printed once (for boot cpu)
        CPU: Hyper-Threading is disabled    printed once (for boot cpu)
        CPU: Thermal monitoring enabled     printed once (for boot cpu)
        CPU %d/0x%x -> Node %d:             removed
        CPU %d is now offline:              only if system_state == RUNNING
        Initializing CPU#%d:                KERN_DEBUG
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    LKML-Reference: <4B219E28.8080601@sgi.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7128b3799cec..8dc3ea145c97 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -375,8 +375,6 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 			node = nearby_node(apicid);
 	}
 	numa_set_node(cpu, node);
-
-	printk(KERN_INFO "CPU %d/0x%x -> Node %d\n", cpu, apicid, node);
 #endif
 }
 

commit 27c13ecec4d8856687b50b959e1146845b478f95
Author: Borislav Petkov <petkovbb@googlemail.com>
Date:   Sat Nov 21 14:01:45 2009 +0100

    x86, cpu: mv display_cacheinfo -> cpu_detect_cache_sizes
    
    display_cacheinfo() doesn't display anything anymore and it is used to
    detect CPU cache sizes. Rename it accordingly.
    
    Signed-off-by: Borislav Petkov <petkovbb@gmail.com>
    LKML-Reference: <20091121130145.GA31357@liondog.tnic>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c910a716a71c..7128b3799cec 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -535,7 +535,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		}
 	}
 
-	display_cacheinfo(c);
+	cpu_detect_cache_sizes(c);
 
 	/* Multi core CPU? */
 	if (c->extended_cpuid_level >= 0x80000008) {

commit 7da8b6ddc7a03346f825925e0d981ca2bd1ed617
Author: Michael Tokarev <mjt@tls.msk.ru>
Date:   Wed Jul 22 17:50:23 2009 +0400

    trivial: fix missing printk space in amd_k7_smp_check
    
    This trivial patch fixes one missing space in printk.
    
    I already fixed it about half a year ago or more, but the change (in
    arch/x86/kernel/cpu/smpboot.c at that time) didn't made into
    mainline yet.
    
    Signed-off-by: Michael Tokarev <mjt@tls.msk.ru>
    
    index 28e5f59..6c139ed 100644
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f32fa71ccf97..c910a716a71c 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -184,7 +184,7 @@ static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
 	 * approved Athlon
 	 */
 	WARN_ONCE(1, "WARNING: This combination of AMD"
-		"processors is not suitable for SMP.\n");
+		" processors is not suitable for SMP.\n");
 	if (!test_taint(TAINT_UNSAFE_SMP))
 		add_taint(TAINT_UNSAFE_SMP);
 

commit 6a8126911a5ab167783fce18ae9cc70ec9b84fe2
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Wed Sep 16 11:33:40 2009 +0200

    x86, EDAC: Provide function to return NodeId of a CPU
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 22a47c82f3c0..f32fa71ccf97 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -333,6 +333,16 @@ static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
 #endif
 }
 
+int amd_get_nb_id(int cpu)
+{
+	int id = 0;
+#ifdef CONFIG_SMP
+	id = per_cpu(cpu_llc_id, cpu);
+#endif
+	return id;
+}
+EXPORT_SYMBOL_GPL(amd_get_nb_id);
+
 static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 {
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)

commit c7208de304ac335d5d58db346bb773a471fc636b
Merge: 15b0404272e1 5367b6887e7d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 07:57:32 2009 -0700

    Merge branch 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-cpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (22 commits)
      x86: Fix code patching for paravirt-alternatives on 486
      x86, msr: change msr-reg.o to obj-y, and export its symbols
      x86: Use hard_smp_processor_id() to get apic id for AMD K8 cpus
      x86, sched: Workaround broken sched domain creation for AMD Magny-Cours
      x86, mcheck: Use correct cpumask for shared bank4
      x86, cacheinfo: Fixup L3 cache information for AMD multi-node processors
      x86: Fix CPU llc_shared_map information for AMD Magny-Cours
      x86, msr: Fix msr-reg.S compilation with gas 2.16.1, on 32-bit too
      x86: Move kernel_fpu_using to irq_fpu_usable in asm/i387.h
      x86, msr: fix msr-reg.S compilation with gas 2.16.1
      x86, msr: Export the register-setting MSR functions via /dev/*/msr
      x86, msr: Create _on_cpu helpers for {rw,wr}msr_safe_regs()
      x86, msr: Have the _safe MSR functions return -EIO, not -EFAULT
      x86, msr: CFI annotations, cleanups for msr-reg.S
      x86, asm: Make _ASM_EXTABLE() usable from assembly code
      x86, asm: Add 32-bit versions of the combined CFI macros
      x86, AMD: Disable wrongly set X86_FEATURE_LAHF_LM CPUID bit
      x86, msr: Rewrite AMD rd/wrmsr variants
      x86, msr: Add rd/wrmsr interfaces with preset registers
      x86: add specific support for Intel Atom architecture
      ...

commit 0d96b9ff748b5f57d6f1d6d21209f5745245aadc
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat Aug 29 13:17:14 2009 -0700

    x86: Use hard_smp_processor_id() to get apic id for AMD K8 cpus
    
    Otherwise, system with apci id lifting will have wrong apicid in
    /proc/cpuinfo.
    
    and use that in srat_detect_node().
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    LKML-Reference: <4A998CCA.1040407@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a76d2c18f150..e1600c775e1d 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -336,7 +336,7 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
 	int cpu = smp_processor_id();
 	int node;
-	unsigned apicid = cpu_has_apic ? hard_smp_processor_id() : c->apicid;
+	unsigned apicid = c->apicid;
 
 	node = per_cpu(cpu_llc_id, cpu);
 
@@ -481,6 +481,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	}
 	if (c->x86 == 0x10 || c->x86 == 0x11)
 		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
+
+	/* get apicid instead of initial apic id from cpuid */
+	c->apicid = hard_smp_processor_id();
 #else
 
 	/*

commit 4a376ec3a2599c02207cd4cbd5dbf73783548463
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Thu Sep 3 09:40:21 2009 +0200

    x86: Fix CPU llc_shared_map information for AMD Magny-Cours
    
    Construct entire NodeID and use it as cpu_llc_id. Thus internal node
    siblings are stored in llc_shared_map.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 0a717fc6aeb6..a76d2c18f150 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -250,6 +250,64 @@ static int __cpuinit nearby_node(int apicid)
 }
 #endif
 
+/*
+ * Fixup core topology information for AMD multi-node processors.
+ * Assumption 1: Number of cores in each internal node is the same.
+ * Assumption 2: Mixed systems with both single-node and dual-node
+ *               processors are not supported.
+ */
+#ifdef CONFIG_X86_HT
+static void __cpuinit amd_fixup_dcm(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_PCI
+	u32 t, cpn;
+	u8 n, n_id;
+	int cpu = smp_processor_id();
+
+	/* fixup topology information only once for a core */
+	if (cpu_has(c, X86_FEATURE_AMD_DCM))
+		return;
+
+	/* check for multi-node processor on boot cpu */
+	t = read_pci_config(0, 24, 3, 0xe8);
+	if (!(t & (1 << 29)))
+		return;
+
+	set_cpu_cap(c, X86_FEATURE_AMD_DCM);
+
+	/* cores per node: each internal node has half the number of cores */
+	cpn = c->x86_max_cores >> 1;
+
+	/* even-numbered NB_id of this dual-node processor */
+	n = c->phys_proc_id << 1;
+
+	/*
+	 * determine internal node id and assign cores fifty-fifty to
+	 * each node of the dual-node processor
+	 */
+	t = read_pci_config(0, 24 + n, 3, 0xe8);
+	n = (t>>30) & 0x3;
+	if (n == 0) {
+		if (c->cpu_core_id < cpn)
+			n_id = 0;
+		else
+			n_id = 1;
+	} else {
+		if (c->cpu_core_id < cpn)
+			n_id = 1;
+		else
+			n_id = 0;
+	}
+
+	/* compute entire NodeID, use llc_shared_map to store sibling info */
+	per_cpu(cpu_llc_id, cpu) = (c->phys_proc_id << 1) + n_id;
+
+	/* fixup core id to be in range from 0 to cpn */
+	c->cpu_core_id = c->cpu_core_id % cpn;
+#endif
+}
+#endif
+
 /*
  * On a AMD dual core setup the lower bits of the APIC id distingush the cores.
  * Assumes number of cores is a power of two.
@@ -267,6 +325,9 @@ static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
 	c->phys_proc_id = c->initial_apicid >> bits;
 	/* use socket ID also for last level cache */
 	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
+	/* fixup topology information on multi-node processors */
+	if ((c->x86 == 0x10) && (c->x86_model == 9))
+		amd_fixup_dcm(c);
 #endif
 }
 
@@ -277,7 +338,8 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 	int node;
 	unsigned apicid = cpu_has_apic ? hard_smp_processor_id() : c->apicid;
 
-	node = c->phys_proc_id;
+	node = per_cpu(cpu_llc_id, cpu);
+
 	if (apicid_to_node[apicid] != NUMA_NO_NODE)
 		node = apicid_to_node[apicid];
 	if (!node_online(node)) {

commit 6b0f43ddfa358dc71ad2a2d57bce5906c1c5dc1a
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon Aug 31 09:50:11 2009 +0200

    x86, AMD: Disable wrongly set X86_FEATURE_LAHF_LM CPUID bit
    
    fbd8b1819e80ac5a176d085fdddc3a34d1499318 turns off the bit for
    /proc/cpuinfo. However, a proper/full fix would be to additionally
    turn off the bit in the CPUID output so that future callers get
    correct CPU features info.
    
    Do that by basically reversing what the BIOS wrongfully does at boot.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <1251705011-18636-3-git-send-email-petkovbb@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 63fddcd082cd..0a717fc6aeb6 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -404,9 +404,18 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		/*
 		 * Some BIOSes incorrectly force this feature, but only K8
 		 * revision D (model = 0x14) and later actually support it.
+		 * (AMD Erratum #110, docId: 25759).
 		 */
-		if (c->x86_model < 0x14)
+		if (c->x86_model < 0x14 && cpu_has(c, X86_FEATURE_LAHF_LM)) {
+			u64 val;
+
 			clear_cpu_cap(c, X86_FEATURE_LAHF_LM);
+			if (!rdmsrl_amd_safe(0xc001100d, &val)) {
+				val &= ~(1ULL << 32);
+				wrmsrl_amd_safe(0xc001100d, val);
+			}
+		}
+
 	}
 	if (c->x86 == 0x10 || c->x86 == 0x11)
 		set_cpu_cap(c, X86_FEATURE_REP_GOOD);

commit 5f9ece02401116b29eb04396b99ea092acb75dd8
Merge: 9f51e24ee8b5 422bef879e84
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 24 12:25:44 2009 +0200

    Merge commit 'v2.6.31-rc7' into x86/cleanups
    
    Merge reason: we were on -rc1 before - go up to -rc7
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit fbd8b1819e80ac5a176d085fdddc3a34d1499318
Author: Kevin Winchester <kjwinchester@gmail.com>
Date:   Mon Aug 10 19:56:45 2009 -0300

    x86: Clear incorrectly forced X86_FEATURE_LAHF_LM flag
    
    Due to an erratum with certain AMD Athlon 64 processors, the
    BIOS may need to force enable the LAHF_LM capability.
    Unfortunately, in at least one case, the BIOS does this even
    for processors that do not support the functionality.
    
    Add a specific check that will clear the feature bit for
    processors known not to support the LAHF/SAHF instructions.
    
    Signed-off-by: Kevin Winchester <kjwinchester@gmail.com>
    Acked-by: Borislav Petkov <petkovbb@googlemail.com>
    LKML-Reference: <4A80A5AD.2000209@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e2485b03f1cf..63fddcd082cd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -400,6 +400,13 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		level = cpuid_eax(1);
 		if((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)
 			set_cpu_cap(c, X86_FEATURE_REP_GOOD);
+
+		/*
+		 * Some BIOSes incorrectly force this feature, but only K8
+		 * revision D (model = 0x14) and later actually support it.
+		 */
+		if (c->x86_model < 0x14)
+			clear_cpu_cap(c, X86_FEATURE_LAHF_LM);
 	}
 	if (c->x86 == 0x10 || c->x86 == 0x11)
 		set_cpu_cap(c, X86_FEATURE_REP_GOOD);

commit 2cb078603abb612e3bcd428fb8122c3d39e08832
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed Jul 22 09:59:35 2009 -0700

    x86, amd: Don't probe for extended APIC ID if APICs are disabled
    
    If we've logically disabled apics, don't probe the PCI space for the
    AMD extended APIC ID.
    
    [ Impact: prevent boot crash under Xen. ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reported-by: Bastian Blank <bastian@waldi.eu.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 28e5f5956042..e2485b03f1cf 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -356,7 +356,7 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 #endif
 #if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_PCI)
 	/* check CPU config space for extended APIC ID */
-	if (c->x86 >= 0xf) {
+	if (cpu_has_apic && c->x86 >= 0xf) {
 		unsigned int val;
 		val = read_pci_config(0, 24, 0, 0x68);
 		if ((val & ((1 << 17) | (1 << 18))) == ((1 << 17) | (1 << 18)))

commit 8bdbd962ecfcbdd96f9dbb02d780b4553afd2543
Author: Alan Cox <alan@linux.intel.com>
Date:   Sat Jul 4 00:35:45 2009 +0100

    x86/cpu: Clean up various files a bit
    
    No code changes except printk levels (although some of the K6
    mtrr code might be clearer if there were a few as would
    splitting out some of the intel cache code).
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 28e5f5956042..c6eb02e69875 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -2,7 +2,7 @@
 #include <linux/bitops.h>
 #include <linux/mm.h>
 
-#include <asm/io.h>
+#include <linux/io.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cpu.h>
@@ -45,8 +45,8 @@ static void __cpuinit init_amd_k5(struct cpuinfo_x86 *c)
 #define CBAR_ENB	(0x80000000)
 #define CBAR_KEY	(0X000000CB)
 	if (c->x86_model == 9 || c->x86_model == 10) {
-		if (inl (CBAR) & CBAR_ENB)
-			outl (0 | CBAR_KEY, CBAR);
+		if (inl(CBAR) & CBAR_ENB)
+			outl(0 | CBAR_KEY, CBAR);
 	}
 }
 
@@ -87,9 +87,10 @@ static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
 		d = d2-d;
 
 		if (d > 20*K6_BUG_LOOP)
-			printk("system stability may be impaired when more than 32 MB are used.\n");
+			printk(KERN_CONT
+				"system stability may be impaired when more than 32 MB are used.\n");
 		else
-			printk("probably OK (after B9730xxxx).\n");
+			printk(KERN_CONT "probably OK (after B9730xxxx).\n");
 		printk(KERN_INFO "Please see http://membres.lycos.fr/poulot/k6bug.html\n");
 	}
 
@@ -219,8 +220,9 @@ static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
 	if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
 		rdmsr(MSR_K7_CLK_CTL, l, h);
 		if ((l & 0xfff00000) != 0x20000000) {
-			printk ("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n", l,
-				((l & 0x000fffff)|0x20000000));
+			printk(KERN_INFO
+			    "CPU: CLK_CTL MSR was %x. Reprogramming to %x\n",
+					l, ((l & 0x000fffff)|0x20000000));
 			wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
 		}
 	}
@@ -398,7 +400,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		u32 level;
 
 		level = cpuid_eax(1);
-		if((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)
+		if ((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)
 			set_cpu_cap(c, X86_FEATURE_REP_GOOD);
 	}
 	if (c->x86 == 0x10 || c->x86 == 0x11)
@@ -487,27 +489,30 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		 * benefit in doing so.
 		 */
 		if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &tseg)) {
-		    printk(KERN_DEBUG "tseg: %010llx\n", tseg);
-		    if ((tseg>>PMD_SHIFT) <
+			printk(KERN_DEBUG "tseg: %010llx\n", tseg);
+			if ((tseg>>PMD_SHIFT) <
 				(max_low_pfn_mapped>>(PMD_SHIFT-PAGE_SHIFT)) ||
-			((tseg>>PMD_SHIFT) <
+				((tseg>>PMD_SHIFT) <
 				(max_pfn_mapped>>(PMD_SHIFT-PAGE_SHIFT)) &&
-			 (tseg>>PMD_SHIFT) >= (1ULL<<(32 - PMD_SHIFT))))
-			set_memory_4k((unsigned long)__va(tseg), 1);
+				(tseg>>PMD_SHIFT) >= (1ULL<<(32 - PMD_SHIFT))))
+				set_memory_4k((unsigned long)__va(tseg), 1);
 		}
 	}
 #endif
 }
 
 #ifdef CONFIG_X86_32
-static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
+static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c,
+							unsigned int size)
 {
 	/* AMD errata T13 (order #21922) */
 	if ((c->x86 == 6)) {
-		if (c->x86_model == 3 && c->x86_mask == 0)	/* Duron Rev A0 */
+		/* Duron Rev A0 */
+		if (c->x86_model == 3 && c->x86_mask == 0)
 			size = 64;
+		/* Tbird rev A1/A2 */
 		if (c->x86_model == 4 &&
-		    (c->x86_mask == 0 || c->x86_mask == 1))	/* Tbird rev A1/A2 */
+			(c->x86_mask == 0 || c->x86_mask == 1))
 			size = 256;
 	}
 	return size;

commit 99bd0c0fc4b04da54cb311953ef9489931c19c63
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Fri Jun 19 10:59:09 2009 +0200

    x86: Set cpu_llc_id on AMD CPUs
    
    This counts when building sched domains in case NUMA information
    is not available.
    
    ( See cpu_coregroup_mask() which uses llc_shared_map which in turn is
      created based on cpu_llc_id. )
    
    Currently Linux builds domains as follows:
    (example from a dual socket quad-core system)
    
     CPU0 attaching sched-domain:
      domain 0: span 0-7 level CPU
       groups: 0 1 2 3 4 5 6 7
    
      ...
    
     CPU7 attaching sched-domain:
      domain 0: span 0-7 level CPU
       groups: 7 0 1 2 3 4 5 6
    
    Ever since that is borked for multi-core AMD CPU systems.
    This patch fixes that and now we get a proper:
    
     CPU0 attaching sched-domain:
      domain 0: span 0-3 level MC
       groups: 0 1 2 3
       domain 1: span 0-7 level CPU
        groups: 0-3 4-7
    
      ...
    
     CPU7 attaching sched-domain:
      domain 0: span 4-7 level MC
       groups: 7 4 5 6
       domain 1: span 0-7 level CPU
        groups: 4-7 0-3
    
    This allows scheduler to assign tasks to cores on different sockets
    (i.e. that don't share last level cache) for performance reasons.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    LKML-Reference: <20090619085909.GJ5218@alberich.amd.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e5b27d8f1b47..28e5f5956042 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -258,13 +258,15 @@ static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_HT
 	unsigned bits;
+	int cpu = smp_processor_id();
 
 	bits = c->x86_coreid_bits;
-
 	/* Low order bits define the core id (index of core in socket) */
 	c->cpu_core_id = c->initial_apicid & ((1 << bits)-1);
 	/* Convert the initial APIC ID into the socket ID */
 	c->phys_proc_id = c->initial_apicid >> bits;
+	/* use socket ID also for last level cache */
+	per_cpu(cpu_llc_id, cpu) = c->phys_proc_id;
 #endif
 }
 

commit 9b29e8228a5c2a169436a1a90a60b1f88cb35cd1
Merge: bec706838ec2 0b8c3d5ab000
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 16:15:14 2009 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: Clear TS in irq_ts_save() when in an atomic section
      x86: Detect use of extended APIC ID for AMD CPUs
      x86: memtest: remove 64-bit division
      x86, UV: Fix macros for multiple coherency domains
      x86: Fix non-lazy GS handling in sys_vm86()
      x86: Add quirk for reboot stalls on a Dell Optiplex 360
      x86: Fix UV BAU activation descriptor init

commit 42937e81a82b6bbc51a309c83da140b3a7ca5945
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Mon Jun 8 15:55:09 2009 +0200

    x86: Detect use of extended APIC ID for AMD CPUs
    
    Booting a 32-bit kernel on Magny-Cours results in the following panic:
    
      ...
      Using APIC driver default
      ...
      Overriding APIC driver with bigsmp
      ...
      Getting VERSION: 80050010
      Getting VERSION: 80050010
      Getting ID: 10000000
      Getting ID: ef000000
      Getting LVT0: 700
      Getting LVT1: 10000
      Kernel panic - not syncing: Boot APIC ID in local APIC unexpected (16 vs 0)
      Pid: 1, comm: swapper Not tainted 2.6.30-rcX #2
      Call Trace:
       [<c05194da>] ? panic+0x38/0xd3
       [<c0743102>] ? native_smp_prepare_cpus+0x259/0x31f
       [<c073b19d>] ? kernel_init+0x3e/0x141
       [<c073b15f>] ? kernel_init+0x0/0x141
       [<c020325f>] ? kernel_thread_helper+0x7/0x10
    
    The reason is that default_get_apic_id handled extension of local APIC
    ID field just in case of XAPIC.
    
    Thus for this AMD CPU, default_get_apic_id() returns 0 and
    bigsmp_get_apic_id() returns 16 which leads to the respective kernel
    panic.
    
    This patch introduces a Linux specific feature flag to indicate
    support for extended APIC id (8 bits instead of 4 bits width) and sets
    the flag on AMD CPUs if applicable.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: <stable@kernel.org>
    LKML-Reference: <20090608135509.GA12431@alberich.amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7e4a459daa64..0802e151c2c9 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -6,6 +6,7 @@
 #include <asm/processor.h>
 #include <asm/apic.h>
 #include <asm/cpu.h>
+#include <asm/pci-direct.h>
 
 #ifdef CONFIG_X86_64
 # include <asm/numa_64.h>
@@ -351,6 +352,15 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 		    (c->x86_model == 8 && c->x86_mask >= 8))
 			set_cpu_cap(c, X86_FEATURE_K6_MTRR);
 #endif
+#if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_PCI)
+	/* check CPU config space for extended APIC ID */
+	if (c->x86 >= 0xf) {
+		unsigned int val;
+		val = read_pci_config(0, 24, 0, 0x68);
+		if ((val & ((1 << 17) | (1 << 18))) == ((1 << 17) | (1 << 18)))
+			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
+	}
+#endif
 }
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)

commit 2759c3287de27266e06f1f4e82cbd2d65f6a044c
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Fri May 15 13:05:16 2009 -0700

    x86: don't call read_apic_id if !cpu_has_apic
    
    should not call that if apic is disabled.
    
    [ Impact: fix crash on certain UP configs ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    LKML-Reference: <4A09CCBB.2000306@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7e4a459daa64..728b3750a3e8 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -272,7 +272,7 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 #if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
 	int cpu = smp_processor_id();
 	int node;
-	unsigned apicid = hard_smp_processor_id();
+	unsigned apicid = cpu_has_apic ? hard_smp_processor_id() : c->apicid;
 
 	node = c->phys_proc_id;
 	if (apicid_to_node[apicid] != NUMA_NO_NODE)

commit 02dde8b45c5460794b9052d7c12939fe3eb63c2c
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Mar 12 12:08:49 2009 +0000

    x86: move various CPU initialization objects into .cpuinit.rodata
    
    Impact: debuggability and micro-optimization
    
    Putting whatever is possible into the (final) .rodata section increases
    the likelihood of catching memory corruption bugs early, and reduces
    false cache line sharing.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <49B90961.76E4.0078.0@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index f47df59016c5..7e4a459daa64 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -502,7 +502,7 @@ static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int
 }
 #endif
 
-static struct cpu_dev amd_cpu_dev __cpuinitdata = {
+static const struct cpu_dev __cpuinitconst amd_cpu_dev = {
 	.c_vendor	= "AMD",
 	.c_ident	= { "AuthenticAMD" },
 #ifdef CONFIG_X86_32

commit 1f442d70c84aa798e243e721eba728a98434cd86
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Sat Mar 7 23:46:26 2009 -0800

    x86: remove smp_apply_quirks()/smp_checks()
    
    Impact: cleanup and code size reduction on 64-bit
    
    This code is only applied to Intel Pentium and AMD K7 32-bit cpus.
    
    Move those checks to intel_init()/amd_init() for 32-bit
    so 64-bit will not build this code.
    
    Also change to use cpu_index check to see if we need to emit warning.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <49B377D2.8030108@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 25423a5b80ed..f47df59016c5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -5,6 +5,7 @@
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
+#include <asm/cpu.h>
 
 #ifdef CONFIG_X86_64
 # include <asm/numa_64.h>
@@ -141,6 +142,55 @@ static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
 	}
 }
 
+static void __cpuinit amd_k7_smp_check(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_SMP
+	/* calling is from identify_secondary_cpu() ? */
+	if (c->cpu_index == boot_cpu_id)
+		return;
+
+	/*
+	 * Certain Athlons might work (for various values of 'work') in SMP
+	 * but they are not certified as MP capable.
+	 */
+	/* Athlon 660/661 is valid. */
+	if ((c->x86_model == 6) && ((c->x86_mask == 0) ||
+	    (c->x86_mask == 1)))
+		goto valid_k7;
+
+	/* Duron 670 is valid */
+	if ((c->x86_model == 7) && (c->x86_mask == 0))
+		goto valid_k7;
+
+	/*
+	 * Athlon 662, Duron 671, and Athlon >model 7 have capability
+	 * bit. It's worth noting that the A5 stepping (662) of some
+	 * Athlon XP's have the MP bit set.
+	 * See http://www.heise.de/newsticker/data/jow-18.10.01-000 for
+	 * more.
+	 */
+	if (((c->x86_model == 6) && (c->x86_mask >= 2)) ||
+	    ((c->x86_model == 7) && (c->x86_mask >= 1)) ||
+	     (c->x86_model > 7))
+		if (cpu_has_mp)
+			goto valid_k7;
+
+	/* If we get here, not a certified SMP capable AMD system. */
+
+	/*
+	 * Don't taint if we are running SMP kernel on a single non-MP
+	 * approved Athlon
+	 */
+	WARN_ONCE(1, "WARNING: This combination of AMD"
+		"processors is not suitable for SMP.\n");
+	if (!test_taint(TAINT_UNSAFE_SMP))
+		add_taint(TAINT_UNSAFE_SMP);
+
+valid_k7:
+	;
+#endif
+}
+
 static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
 {
 	u32 l, h;
@@ -175,6 +225,8 @@ static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
 	}
 
 	set_cpu_cap(c, X86_FEATURE_K7);
+
+	amd_k7_smp_check(c);
 }
 #endif
 

commit e641f5f525acb163ba71d92de79c9c7366deae03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 14:02:01 2009 +0100

    x86, apic: remove duplicate asm/apic.h inclusions
    
    Impact: cleanup
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c94ba9311e65..25423a5b80ed 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -12,8 +12,6 @@
 # include <asm/cacheflush.h>
 #endif
 
-#include <asm/apic.h>
-
 #include "cpu.h"
 
 #ifdef CONFIG_X86_32

commit 7b6aa335ca1a845c2262ec7a595b4521bca0f79d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 13:58:15 2009 +0100

    x86, apic: remove genapic.h
    
    Impact: cleanup
    
    Remove genapic.h and remove all references to it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index ff4d7b9e32e4..c94ba9311e65 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -12,7 +12,7 @@
 # include <asm/cacheflush.h>
 #endif
 
-#include <asm/genapic.h>
+#include <asm/apic.h>
 
 #include "cpu.h"
 

commit 1dcdd3d15ecea0c22a09d4d001a39d425fceff2c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 17:55:37 2009 +0100

    x86: remove mach_apic.h
    
    Spread mach_apic.h definitions into genapic.h. (with some knock-on effects
    on smp.h and apic.h.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 7c878f6aa919..ff4d7b9e32e4 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -12,7 +12,7 @@
 # include <asm/cacheflush.h>
 #endif
 
-#include <mach_apic.h>
+#include <asm/genapic.h>
 
 #include "cpu.h"
 

commit 40fb17152c50a69dc304dd632131c2f41281ce44
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Mon Nov 17 16:11:37 2008 -0800

    x86: support always running TSC on Intel CPUs
    
    Impact: reward non-stop TSCs with good TSC-based clocksources, etc.
    
    Add support for CPUID_0x80000007_Bit8 on Intel CPUs as well. This bit means
    that the TSC is invariant with C/P/T states and always runs at constant
    frequency.
    
    With Intel CPUs, we have 3 classes
    * CPUs where TSC runs at constant rate and does not stop n C-states
    * CPUs where TSC runs at constant rate, but will stop in deep C-states
    * CPUs where TSC rate will vary based on P/T-states and TSC will stop in deep
      C-states.
    
    To cover these 3, one feature bit (CONSTANT_TSC) is not enough. So, add a
    second bit (NONSTOP_TSC). CONSTANT_TSC indicates that the TSC runs at
    constant frequency irrespective of P/T-states, and NONSTOP_TSC indicates
    that TSC does not stop in deep C-states.
    
    CPUID_0x8000000_Bit8 indicates both these feature bit can be set.
    We still have CONSTANT_TSC _set_ and NONSTOP_TSC _not_set_ on some older Intel
    CPUs, based on model checks. We can use TSC on such CPUs for time, as long as
    those CPUs do not support/enter deep C-states.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 8f1e31db2ad5..7c878f6aa919 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -283,9 +283,14 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
 	early_init_amd_mc(c);
 
-	/* c->x86_power is 8000_0007 edx. Bit 8 is constant TSC */
-	if (c->x86_power & (1<<8))
+	/*
+	 * c->x86_power is 8000_0007 edx. Bit 8 is TSC runs at constant rate
+	 * with P/T states and does not stop in deep C-states
+	 */
+	if (c->x86_power & (1 << 8)) {
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
+		set_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);
+	}
 
 #ifdef CONFIG_X86_64
 	set_cpu_cap(c, X86_FEATURE_SYSCALL32);

commit 823b259b80158a5fb694f6784e18b5bae669c599
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Wed Sep 10 21:56:46 2008 -0700

    x86: print out apic id in hex format
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 32e73520adf7..8f1e31db2ad5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -249,7 +249,7 @@ static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
 	}
 	numa_set_node(cpu, node);
 
-	printk(KERN_INFO "CPU %d/%x -> Node %d\n", cpu, apicid, node);
+	printk(KERN_INFO "CPU %d/0x%x -> Node %d\n", cpu, apicid, node);
 #endif
 }
 

commit 6c62aa4a3c12989a1f1fcbbe6f1ee5d4de4b2300
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 7 17:58:54 2008 -0700

    x86: make amd.c have 64bit support code
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 3c8090d10053..32e73520adf7 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -16,6 +16,7 @@
 
 #include "cpu.h"
 
+#ifdef CONFIG_X86_32
 /*
  *	B step AMD K6 before B 9730xxxx have hardware bugs that can cause
  *	misexecution of code under Linux. Owners of such processors should
@@ -177,6 +178,26 @@ static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
 
 	set_cpu_cap(c, X86_FEATURE_K7);
 }
+#endif
+
+#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+static int __cpuinit nearby_node(int apicid)
+{
+	int i, node;
+
+	for (i = apicid - 1; i >= 0; i--) {
+		node = apicid_to_node[i];
+		if (node != NUMA_NO_NODE && node_online(node))
+			return node;
+	}
+	for (i = apicid + 1; i < MAX_LOCAL_APIC; i++) {
+		node = apicid_to_node[i];
+		if (node != NUMA_NO_NODE && node_online(node))
+			return node;
+	}
+	return first_node(node_online_map); /* Shouldn't happen */
+}
+#endif
 
 /*
  * On a AMD dual core setup the lower bits of the APIC id distingush the cores.
@@ -196,6 +217,42 @@ static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
 #endif
 }
 
+static void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)
+{
+#if defined(CONFIG_NUMA) && defined(CONFIG_X86_64)
+	int cpu = smp_processor_id();
+	int node;
+	unsigned apicid = hard_smp_processor_id();
+
+	node = c->phys_proc_id;
+	if (apicid_to_node[apicid] != NUMA_NO_NODE)
+		node = apicid_to_node[apicid];
+	if (!node_online(node)) {
+		/* Two possibilities here:
+		   - The CPU is missing memory and no node was created.
+		   In that case try picking one from a nearby CPU
+		   - The APIC IDs differ from the HyperTransport node IDs
+		   which the K8 northbridge parsing fills in.
+		   Assume they are all increased by a constant offset,
+		   but in the same order as the HT nodeids.
+		   If that doesn't result in a usable node fall back to the
+		   path for the previous case.  */
+
+		int ht_nodeid = c->initial_apicid;
+
+		if (ht_nodeid >= 0 &&
+		    apicid_to_node[ht_nodeid] != NUMA_NO_NODE)
+			node = apicid_to_node[ht_nodeid];
+		/* Pick a nearby node */
+		if (!node_online(node))
+			node = nearby_node(apicid);
+	}
+	numa_set_node(cpu, node);
+
+	printk(KERN_INFO "CPU %d/%x -> Node %d\n", cpu, apicid, node);
+#endif
+}
+
 static void __cpuinit early_init_amd_mc(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_HT
@@ -226,13 +283,19 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
 	early_init_amd_mc(c);
 
+	/* c->x86_power is 8000_0007 edx. Bit 8 is constant TSC */
 	if (c->x86_power & (1<<8))
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 
+#ifdef CONFIG_X86_64
+	set_cpu_cap(c, X86_FEATURE_SYSCALL32);
+#else
 	/*  Set MTRR capability flag if appropriate */
-	if (c->x86_model == 13 || c->x86_model == 9 ||
-	   (c->x86_model == 8 && c->x86_mask >= 8))
-		set_cpu_cap(c, X86_FEATURE_K6_MTRR);
+	if (c->x86 == 5)
+		if (c->x86_model == 13 || c->x86_model == 9 ||
+		    (c->x86_model == 8 && c->x86_mask >= 8))
+			set_cpu_cap(c, X86_FEATURE_K6_MTRR);
+#endif
 }
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
@@ -256,18 +319,31 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 	early_init_amd(c);
 
-	/*
-	 *	FIXME: We should handle the K5 here. Set up the write
-	 *	range and also turn on MSR 83 bits 4 and 31 (write alloc,
-	 *	no bus pipeline)
-	 */
-
 	/*
 	 * Bit 31 in normal CPUID used for nonstandard 3DNow ID;
 	 * 3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway
 	 */
 	clear_cpu_cap(c, 0*32+31);
 
+#ifdef CONFIG_X86_64
+	/* On C+ stepping K8 rep microcode works well for copy/memset */
+	if (c->x86 == 0xf) {
+		u32 level;
+
+		level = cpuid_eax(1);
+		if((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)
+			set_cpu_cap(c, X86_FEATURE_REP_GOOD);
+	}
+	if (c->x86 == 0x10 || c->x86 == 0x11)
+		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
+#else
+
+	/*
+	 *	FIXME: We should handle the K5 here. Set up the write
+	 *	range and also turn on MSR 83 bits 4 and 31 (write alloc,
+	 *	no bus pipeline)
+	 */
+
 	switch (c->x86) {
 	case 4:
 		init_amd_k5(c);
@@ -283,7 +359,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	/* K6s reports MCEs but don't actually have all the MSRs */
 	if (c->x86 < 6)
 		clear_cpu_cap(c, X86_FEATURE_MCE);
+#endif
 
+	/* Enable workaround for FXSAVE leak */
 	if (c->x86 >= 6)
 		set_cpu_cap(c, X86_FEATURE_FXSAVE_LEAK);
 
@@ -300,10 +378,14 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	display_cacheinfo(c);
 
 	/* Multi core CPU? */
-	if (c->extended_cpuid_level >= 0x80000008)
+	if (c->extended_cpuid_level >= 0x80000008) {
 		amd_detect_cmp(c);
+		srat_detect_node(c);
+	}
 
+#ifdef CONFIG_X86_32
 	detect_ht(c);
+#endif
 
 	if (c->extended_cpuid_level >= 0x80000006) {
 		if ((c->x86 >= 0x0f) && (cpuid_edx(0x80000006) & 0xf000))
@@ -319,8 +401,38 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		/* MFENCE stops RDTSC speculation */
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
 	}
+
+#ifdef CONFIG_X86_64
+	if (c->x86 == 0x10) {
+		/* do this for boot cpu */
+		if (c == &boot_cpu_data)
+			check_enable_amd_mmconf_dmi();
+
+		fam10h_check_enable_mmcfg();
+	}
+
+	if (c == &boot_cpu_data && c->x86 >= 0xf && c->x86 <= 0x11) {
+		unsigned long long tseg;
+
+		/*
+		 * Split up direct mapping around the TSEG SMM area.
+		 * Don't do it for gbpages because there seems very little
+		 * benefit in doing so.
+		 */
+		if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &tseg)) {
+		    printk(KERN_DEBUG "tseg: %010llx\n", tseg);
+		    if ((tseg>>PMD_SHIFT) <
+				(max_low_pfn_mapped>>(PMD_SHIFT-PAGE_SHIFT)) ||
+			((tseg>>PMD_SHIFT) <
+				(max_pfn_mapped>>(PMD_SHIFT-PAGE_SHIFT)) &&
+			 (tseg>>PMD_SHIFT) >= (1ULL<<(32 - PMD_SHIFT))))
+			set_memory_4k((unsigned long)__va(tseg), 1);
+		}
+	}
+#endif
 }
 
+#ifdef CONFIG_X86_32
 static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 {
 	/* AMD errata T13 (order #21922) */
@@ -333,10 +445,12 @@ static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int
 	}
 	return size;
 }
+#endif
 
 static struct cpu_dev amd_cpu_dev __cpuinitdata = {
 	.c_vendor	= "AMD",
 	.c_ident	= { "AuthenticAMD" },
+#ifdef CONFIG_X86_32
 	.c_models = {
 		{ .vendor = X86_VENDOR_AMD, .family = 4, .model_names =
 		  {
@@ -349,9 +463,10 @@ static struct cpu_dev amd_cpu_dev __cpuinitdata = {
 		  }
 		},
 	},
+	.c_size_cache	= amd_size_cache,
+#endif
 	.c_early_init   = early_init_amd,
 	.c_init		= init_amd,
-	.c_size_cache	= amd_size_cache,
 	.c_x86_vendor	= X86_VENDOR_AMD,
 };
 

commit 8d71a2ea0ad4ef9b9076ffd44726bad1f0ccf59b
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 7 17:58:53 2008 -0700

    x86: merge header in amd_64.c
    
    Singed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a3a9e3cbdea9..3c8090d10053 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -1,11 +1,19 @@
 #include <linux/init.h>
 #include <linux/bitops.h>
 #include <linux/mm.h>
+
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
 
+#ifdef CONFIG_X86_64
+# include <asm/numa_64.h>
+# include <asm/mmconfig.h>
+# include <asm/cacheflush.h>
+#endif
+
 #include <mach_apic.h>
+
 #include "cpu.h"
 
 /*

commit 11fdd252bb5b461289fc5c21dc8fc87dc66f3284
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Sep 7 17:58:50 2008 -0700

    x86: cpu make amd.c more like amd_64.c v2
    
    1. make 32bit have early_init_amd_mc and amd_detect_cmp
    2. seperate init_amd_k5/k6/k7 ...
    
    v2: fix compiling for !CONFIG_SMP
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index c3175da7bc69..a3a9e3cbdea9 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -24,8 +24,200 @@
 extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
+static void __cpuinit init_amd_k5(struct cpuinfo_x86 *c)
+{
+/*
+ * General Systems BIOSen alias the cpu frequency registers
+ * of the Elan at 0x000df000. Unfortuantly, one of the Linux
+ * drivers subsequently pokes it, and changes the CPU speed.
+ * Workaround : Remove the unneeded alias.
+ */
+#define CBAR		(0xfffc) /* Configuration Base Address  (32-bit) */
+#define CBAR_ENB	(0x80000000)
+#define CBAR_KEY	(0X000000CB)
+	if (c->x86_model == 9 || c->x86_model == 10) {
+		if (inl (CBAR) & CBAR_ENB)
+			outl (0 | CBAR_KEY, CBAR);
+	}
+}
+
+
+static void __cpuinit init_amd_k6(struct cpuinfo_x86 *c)
+{
+	u32 l, h;
+	int mbytes = num_physpages >> (20-PAGE_SHIFT);
+
+	if (c->x86_model < 6) {
+		/* Based on AMD doc 20734R - June 2000 */
+		if (c->x86_model == 0) {
+			clear_cpu_cap(c, X86_FEATURE_APIC);
+			set_cpu_cap(c, X86_FEATURE_PGE);
+		}
+		return;
+	}
+
+	if (c->x86_model == 6 && c->x86_mask == 1) {
+		const int K6_BUG_LOOP = 1000000;
+		int n;
+		void (*f_vide)(void);
+		unsigned long d, d2;
+
+		printk(KERN_INFO "AMD K6 stepping B detected - ");
+
+		/*
+		 * It looks like AMD fixed the 2.6.2 bug and improved indirect
+		 * calls at the same time.
+		 */
+
+		n = K6_BUG_LOOP;
+		f_vide = vide;
+		rdtscl(d);
+		while (n--)
+			f_vide();
+		rdtscl(d2);
+		d = d2-d;
+
+		if (d > 20*K6_BUG_LOOP)
+			printk("system stability may be impaired when more than 32 MB are used.\n");
+		else
+			printk("probably OK (after B9730xxxx).\n");
+		printk(KERN_INFO "Please see http://membres.lycos.fr/poulot/k6bug.html\n");
+	}
+
+	/* K6 with old style WHCR */
+	if (c->x86_model < 8 ||
+	   (c->x86_model == 8 && c->x86_mask < 8)) {
+		/* We can only write allocate on the low 508Mb */
+		if (mbytes > 508)
+			mbytes = 508;
+
+		rdmsr(MSR_K6_WHCR, l, h);
+		if ((l&0x0000FFFF) == 0) {
+			unsigned long flags;
+			l = (1<<0)|((mbytes/4)<<1);
+			local_irq_save(flags);
+			wbinvd();
+			wrmsr(MSR_K6_WHCR, l, h);
+			local_irq_restore(flags);
+			printk(KERN_INFO "Enabling old style K6 write allocation for %d Mb\n",
+				mbytes);
+		}
+		return;
+	}
+
+	if ((c->x86_model == 8 && c->x86_mask > 7) ||
+	     c->x86_model == 9 || c->x86_model == 13) {
+		/* The more serious chips .. */
+
+		if (mbytes > 4092)
+			mbytes = 4092;
+
+		rdmsr(MSR_K6_WHCR, l, h);
+		if ((l&0xFFFF0000) == 0) {
+			unsigned long flags;
+			l = ((mbytes>>2)<<22)|(1<<16);
+			local_irq_save(flags);
+			wbinvd();
+			wrmsr(MSR_K6_WHCR, l, h);
+			local_irq_restore(flags);
+			printk(KERN_INFO "Enabling new style K6 write allocation for %d Mb\n",
+				mbytes);
+		}
+
+		return;
+	}
+
+	if (c->x86_model == 10) {
+		/* AMD Geode LX is model 10 */
+		/* placeholder for any needed mods */
+		return;
+	}
+}
+
+static void __cpuinit init_amd_k7(struct cpuinfo_x86 *c)
+{
+	u32 l, h;
+
+	/*
+	 * Bit 15 of Athlon specific MSR 15, needs to be 0
+	 * to enable SSE on Palomino/Morgan/Barton CPU's.
+	 * If the BIOS didn't enable it already, enable it here.
+	 */
+	if (c->x86_model >= 6 && c->x86_model <= 10) {
+		if (!cpu_has(c, X86_FEATURE_XMM)) {
+			printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
+			rdmsr(MSR_K7_HWCR, l, h);
+			l &= ~0x00008000;
+			wrmsr(MSR_K7_HWCR, l, h);
+			set_cpu_cap(c, X86_FEATURE_XMM);
+		}
+	}
+
+	/*
+	 * It's been determined by AMD that Athlons since model 8 stepping 1
+	 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
+	 * As per AMD technical note 27212 0.2
+	 */
+	if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
+		rdmsr(MSR_K7_CLK_CTL, l, h);
+		if ((l & 0xfff00000) != 0x20000000) {
+			printk ("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n", l,
+				((l & 0x000fffff)|0x20000000));
+			wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
+		}
+	}
+
+	set_cpu_cap(c, X86_FEATURE_K7);
+}
+
+/*
+ * On a AMD dual core setup the lower bits of the APIC id distingush the cores.
+ * Assumes number of cores is a power of two.
+ */
+static void __cpuinit amd_detect_cmp(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_HT
+	unsigned bits;
+
+	bits = c->x86_coreid_bits;
+
+	/* Low order bits define the core id (index of core in socket) */
+	c->cpu_core_id = c->initial_apicid & ((1 << bits)-1);
+	/* Convert the initial APIC ID into the socket ID */
+	c->phys_proc_id = c->initial_apicid >> bits;
+#endif
+}
+
+static void __cpuinit early_init_amd_mc(struct cpuinfo_x86 *c)
+{
+#ifdef CONFIG_X86_HT
+	unsigned bits, ecx;
+
+	/* Multi core CPU? */
+	if (c->extended_cpuid_level < 0x80000008)
+		return;
+
+	ecx = cpuid_ecx(0x80000008);
+
+	c->x86_max_cores = (ecx & 0xff) + 1;
+
+	/* CPU telling us the core id bits shift? */
+	bits = (ecx >> 12) & 0xF;
+
+	/* Otherwise recompute */
+	if (bits == 0) {
+		while ((1 << bits) < c->x86_max_cores)
+			bits++;
+	}
+
+	c->x86_coreid_bits = bits;
+#endif
+}
+
 static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
+	early_init_amd_mc(c);
+
 	if (c->x86_power & (1<<8))
 		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 
@@ -37,9 +229,6 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 {
-	u32 l, h;
-	int mbytes = num_physpages >> (20-PAGE_SHIFT);
-
 #ifdef CONFIG_SMP
 	unsigned long long value;
 
@@ -50,7 +239,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	 * Errata 63 for SH-B3 steppings
 	 * Errata 122 for all steppings (F+ have it disabled by default)
 	 */
-	if (c->x86 == 15) {
+	if (c->x86 == 0xf) {
 		rdmsrl(MSR_K7_HWCR, value);
 		value |= 1 << 6;
 		wrmsrl(MSR_K7_HWCR, value);
@@ -73,192 +262,55 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 	switch (c->x86) {
 	case 4:
-		/*
-		 * General Systems BIOSen alias the cpu frequency registers
-		 * of the Elan at 0x000df000. Unfortuantly, one of the Linux
-		 * drivers subsequently pokes it, and changes the CPU speed.
-		 * Workaround : Remove the unneeded alias.
-		 */
-#define CBAR		(0xfffc) /* Configuration Base Address  (32-bit) */
-#define CBAR_ENB	(0x80000000)
-#define CBAR_KEY	(0X000000CB)
-			if (c->x86_model == 9 || c->x86_model == 10) {
-				if (inl (CBAR) & CBAR_ENB)
-					outl (0 | CBAR_KEY, CBAR);
-			}
-			break;
+		init_amd_k5(c);
+		break;
 	case 5:
-			if (c->x86_model < 6) {
-				/* Based on AMD doc 20734R - June 2000 */
-				if (c->x86_model == 0) {
-					clear_cpu_cap(c, X86_FEATURE_APIC);
-					set_cpu_cap(c, X86_FEATURE_PGE);
-				}
-				break;
-			}
-
-			if (c->x86_model == 6 && c->x86_mask == 1) {
-				const int K6_BUG_LOOP = 1000000;
-				int n;
-				void (*f_vide)(void);
-				unsigned long d, d2;
-
-				printk(KERN_INFO "AMD K6 stepping B detected - ");
-
-				/*
-				 * It looks like AMD fixed the 2.6.2 bug and improved indirect
-				 * calls at the same time.
-				 */
-
-				n = K6_BUG_LOOP;
-				f_vide = vide;
-				rdtscl(d);
-				while (n--)
-					f_vide();
-				rdtscl(d2);
-				d = d2-d;
-
-				if (d > 20*K6_BUG_LOOP)
-					printk("system stability may be impaired when more than 32 MB are used.\n");
-				else
-					printk("probably OK (after B9730xxxx).\n");
-				printk(KERN_INFO "Please see http://membres.lycos.fr/poulot/k6bug.html\n");
-			}
-
-			/* K6 with old style WHCR */
-			if (c->x86_model < 8 ||
-			   (c->x86_model == 8 && c->x86_mask < 8)) {
-				/* We can only write allocate on the low 508Mb */
-				if (mbytes > 508)
-					mbytes = 508;
-
-				rdmsr(MSR_K6_WHCR, l, h);
-				if ((l&0x0000FFFF) == 0) {
-					unsigned long flags;
-					l = (1<<0)|((mbytes/4)<<1);
-					local_irq_save(flags);
-					wbinvd();
-					wrmsr(MSR_K6_WHCR, l, h);
-					local_irq_restore(flags);
-					printk(KERN_INFO "Enabling old style K6 write allocation for %d Mb\n",
-						mbytes);
-				}
-				break;
-			}
-
-			if ((c->x86_model == 8 && c->x86_mask > 7) ||
-			     c->x86_model == 9 || c->x86_model == 13) {
-				/* The more serious chips .. */
-
-				if (mbytes > 4092)
-					mbytes = 4092;
-
-				rdmsr(MSR_K6_WHCR, l, h);
-				if ((l&0xFFFF0000) == 0) {
-					unsigned long flags;
-					l = ((mbytes>>2)<<22)|(1<<16);
-					local_irq_save(flags);
-					wbinvd();
-					wrmsr(MSR_K6_WHCR, l, h);
-					local_irq_restore(flags);
-					printk(KERN_INFO "Enabling new style K6 write allocation for %d Mb\n",
-						mbytes);
-				}
-
-				break;
-			}
-
-			if (c->x86_model == 10) {
-				/* AMD Geode LX is model 10 */
-				/* placeholder for any needed mods */
-				break;
-			}
-			break;
-	case 6: /* An Athlon/Duron */
-
-			/*
-			 * Bit 15 of Athlon specific MSR 15, needs to be 0
-			 * to enable SSE on Palomino/Morgan/Barton CPU's.
-			 * If the BIOS didn't enable it already, enable it here.
-			 */
-			if (c->x86_model >= 6 && c->x86_model <= 10) {
-				if (!cpu_has(c, X86_FEATURE_XMM)) {
-					printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
-					rdmsr(MSR_K7_HWCR, l, h);
-					l &= ~0x00008000;
-					wrmsr(MSR_K7_HWCR, l, h);
-					set_cpu_cap(c, X86_FEATURE_XMM);
-				}
-			}
-
-			/*
-			 * It's been determined by AMD that Athlons since model 8 stepping 1
-			 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
-			 * As per AMD technical note 27212 0.2
-			 */
-			if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
-				rdmsr(MSR_K7_CLK_CTL, l, h);
-				if ((l & 0xfff00000) != 0x20000000) {
-					printk ("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n", l,
-						((l & 0x000fffff)|0x20000000));
-					wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
-				}
-			}
-			break;
-	}
-
-	switch (c->x86) {
-	case 15:
-	/* Use K8 tuning for Fam10h and Fam11h */
-	case 0x10:
-	case 0x11:
-		set_cpu_cap(c, X86_FEATURE_K8);
+		init_amd_k6(c);
 		break;
-	case 6:
-		set_cpu_cap(c, X86_FEATURE_K7);
+	case 6: /* An Athlon/Duron */
+		init_amd_k7(c);
 		break;
 	}
+
+	/* K6s reports MCEs but don't actually have all the MSRs */
+	if (c->x86 < 6)
+		clear_cpu_cap(c, X86_FEATURE_MCE);
+
 	if (c->x86 >= 6)
 		set_cpu_cap(c, X86_FEATURE_FXSAVE_LEAK);
 
-	display_cacheinfo(c);
+	if (!c->x86_model_id[0]) {
+		switch (c->x86) {
+		case 0xf:
+			/* Should distinguish Models here, but this is only
+			   a fallback anyways. */
+			strcpy(c->x86_model_id, "Hammer");
+			break;
+		}
+	}
 
-	if (cpuid_eax(0x80000000) >= 0x80000008)
-		c->x86_max_cores = (cpuid_ecx(0x80000008) & 0xff) + 1;
+	display_cacheinfo(c);
 
-#ifdef CONFIG_X86_HT
-	/*
-	 * On a AMD multi core setup the lower bits of the APIC id
-	 * distinguish the cores.
-	 */
-	if (c->x86_max_cores > 1) {
-		int cpu = smp_processor_id();
-		unsigned bits = (cpuid_ecx(0x80000008) >> 12) & 0xf;
+	/* Multi core CPU? */
+	if (c->extended_cpuid_level >= 0x80000008)
+		amd_detect_cmp(c);
 
-		if (bits == 0) {
-			while ((1 << bits) < c->x86_max_cores)
-				bits++;
-		}
-		c->cpu_core_id = c->phys_proc_id & ((1<<bits)-1);
-		c->phys_proc_id >>= bits;
-		printk(KERN_INFO "CPU %d(%d) -> Core %d\n",
-		       cpu, c->x86_max_cores, c->cpu_core_id);
-	}
-#endif
+	detect_ht(c);
 
-	if (cpuid_eax(0x80000000) >= 0x80000006) {
-		if ((c->x86 == 0x10) && (cpuid_edx(0x80000006) & 0xf000))
+	if (c->extended_cpuid_level >= 0x80000006) {
+		if ((c->x86 >= 0x0f) && (cpuid_edx(0x80000006) & 0xf000))
 			num_cache_leaves = 4;
 		else
 			num_cache_leaves = 3;
 	}
 
-	/* K6s reports MCEs but don't actually have all the MSRs */
-	if (c->x86 < 6)
-		clear_cpu_cap(c, X86_FEATURE_MCE);
+	if (c->x86 >= 0xf && c->x86 <= 0x11)
+		set_cpu_cap(c, X86_FEATURE_K8);
 
-	if (cpu_has_xmm2)
+	if (cpu_has_xmm2) {
+		/* MFENCE stops RDTSC speculation */
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
+	}
 }
 
 static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)

commit e3224234717b4228c235cee401af89212f17a3a4
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sat Sep 6 01:52:28 2008 -0700

    x86, cpu init: call early_init_xxx in init_xxx
    
    so we:
    
     1. could set some cap to ap
     2. restore some cap after memset in identify_cpu for boot cpu
    
    esp for CONSTANT_TSC this matters, as:
    
    before this patch:
     flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm 3dnowext 3dnow rep_good nopl pni monitor cx16 lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs
    
    after this patch:
     flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm 3dnowext 3dnow constant_tsc rep_good nopl pni monitor cx16 lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs
    
    so constant_tsc is back...
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e0ba2c7c5a18..c3175da7bc69 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -26,11 +26,8 @@ __asm__(".align 4\nvide: ret");
 
 static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
-	if (cpuid_eax(0x80000000) >= 0x80000007) {
-		c->x86_power = cpuid_edx(0x80000007);
-		if (c->x86_power & (1<<8))
-			set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
-	}
+	if (c->x86_power & (1<<8))
+		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 
 	/*  Set MTRR capability flag if appropriate */
 	if (c->x86_model == 13 || c->x86_model == 9 ||

commit 1b05d60d60e81c6594da8298107a05b506f01797
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sat Sep 6 01:52:27 2008 -0700

    x86: remove duplicated get_model_name() calling
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index d64ea6097ca7..e0ba2c7c5a18 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -42,7 +42,6 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 {
 	u32 l, h;
 	int mbytes = num_physpages >> (20-PAGE_SHIFT);
-	int r;
 
 #ifdef CONFIG_SMP
 	unsigned long long value;
@@ -75,8 +74,6 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	 */
 	clear_cpu_cap(c, 0*32+31);
 
-	r = get_model_name(c);
-
 	switch (c->x86) {
 	case 4:
 		/*

commit 10a434fcb23a57c385177a0086955fae01003f64
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:45 2008 +0200

    x86: remove cpu_vendor_dev
    
    1. add c_x86_vendor into cpu_dev
    2. change cpu_devs to static
    3. check c_x86_vendor before put that cpu_dev into array
    4. remove alignment for 64bit
    5. order the sequence in cpu_devs according to link sequence...
       so could put intel at first, then amd...
    
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 18514ed26104..d64ea6097ca7 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -298,6 +298,7 @@ static struct cpu_dev amd_cpu_dev __cpuinitdata = {
 	.c_early_init   = early_init_amd,
 	.c_init		= init_amd,
 	.c_size_cache	= amd_size_cache,
+	.c_x86_vendor	= X86_VENDOR_AMD,
 };
 
-cpu_vendor_dev_register(X86_VENDOR_AMD, &amd_cpu_dev);
+cpu_dev_register(amd_cpu_dev);

commit 5fef55fddb7317585cabc1eae38dbd57f1c59729
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Thu Sep 4 21:09:43 2008 +0200

    x86: move mtrr cpu cap setting early in early_init_xxxx
    
    Krzysztof Helt found MTRR is not detected on k6-2
    
    root cause:
            we moved mtrr_bp_init() early for mtrr trimming,
    and in early_detect we only read the CPU capability from cpuid,
    so some cpu doesn't have that bit in cpuid.
    
    So we need to add early_init_xxxx to preset those bit before mtrr_bp_init
    for those earlier cpus.
    
    this patch is for v2.6.27
    
    Reported-by: Krzysztof Helt <krzysztof.h1@wp.pl>
    Signed-off-by: Yinghai Lu <yhlu.kernel@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index cae9cabc3031..18514ed26104 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -31,6 +31,11 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 		if (c->x86_power & (1<<8))
 			set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 	}
+
+	/*  Set MTRR capability flag if appropriate */
+	if (c->x86_model == 13 || c->x86_model == 9 ||
+	   (c->x86_model == 8 && c->x86_mask >= 8))
+		set_cpu_cap(c, X86_FEATURE_K6_MTRR);
 }
 
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
@@ -166,10 +171,6 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 						mbytes);
 				}
 
-				/*  Set MTRR capability flag if appropriate */
-				if (c->x86_model == 13 || c->x86_model == 9 ||
-				   (c->x86_model == 8 && c->x86_mask >= 8))
-					set_cpu_cap(c, X86_FEATURE_K6_MTRR);
 				break;
 			}
 

commit 08ad8afaa0f7343e9c64eec5dbbb178e390e03a2
Author: Jan Beulich <jbeulich@novell.com>
Date:   Fri Jul 18 13:45:20 2008 +0100

    x86: reduce force_mwait visibility
    
    It's not used anywhere outside its single referencing file.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 81a07ca65d44..cae9cabc3031 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -24,8 +24,6 @@
 extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
-int force_mwait __cpuinitdata;
-
 static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
 	if (cpuid_eax(0x80000000) >= 0x80000007) {

commit 3a27dd1ce5de08e21e0266ddf00e6f1f843bfe8b
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Jun 12 20:19:23 2008 +0200

    x86: Move PCI IO ECS code to x86/pci
    
    "Form follows function". Code is now where it belongs to.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index acc891ae5901..81a07ca65d44 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -266,9 +266,6 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 	if (cpu_has_xmm2)
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
-
-	if (c->x86 == 0x10)
-		amd_enable_pci_ext_cfg(c);
 }
 
 static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)

commit aa276e1cafb3ce9d01d1e837bcd67e92616013ac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 19:15:00 2008 +0200

    x86, clockevents: add C1E aware idle function
    
    C1E on AMD machines is like C3 but without control from the OS. Up to
    now we disabled the local apic timer for those machines as it stops
    when the CPU goes into C1E. This excludes those machines from high
    resolution timers / dynamic ticks, which hurts especially X2 based
    laptops.
    
    The current boot time C1E detection has another, more serious flaw
    as well: some BIOSes do not enable C1E until the ACPI processor module
    is loaded. This causes systems to stop working after that point.
    
    To work nicely with C1E enabled machines we use a separate idle
    function, which checks on idle entry whether C1E was enabled in the
    Interrupt Pending Message MSR. This allows us to do timer broadcasting
    for C1E and covers the late enablement of C1E as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index e76b49e7a916..acc891ae5901 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -24,31 +24,6 @@
 extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
-#ifdef CONFIG_X86_LOCAL_APIC
-
-/* AMD systems with C1E don't have a working lAPIC timer. Check for that. */
-static __cpuinit int amd_apic_timer_broken(struct cpuinfo_x86 *c)
-{
-	u32 lo, hi;
-
-	if (c->x86 < 0x0F)
-		return 0;
-
-	/* Family 0x0f models < rev F do not have this MSR */
-	if (c->x86 == 0x0f && c->x86_model < 0x40)
-		return 0;
-
-	rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
-	if (lo & K8_INTP_C1E_ACTIVE_MASK) {
-		if (smp_processor_id() != boot_cpu_physical_apicid)
-			printk(KERN_INFO "AMD C1E detected late. "
-			       "Force timer broadcast.\n");
-		return 1;
-	}
-	return 0;
-}
-#endif
-
 int force_mwait __cpuinitdata;
 
 static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
@@ -285,11 +260,6 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 			num_cache_leaves = 3;
 	}
 
-#ifdef CONFIG_X86_LOCAL_APIC
-	if (amd_apic_timer_broken(c))
-		local_apic_timer_disabled = 1;
-#endif
-
 	/* K6s reports MCEs but don't actually have all the MSRs */
 	if (c->x86 < 6)
 		clear_cpu_cap(c, X86_FEATURE_MCE);

commit 732d7be17b98ebfd59e5864c3490f19856fa832c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 17:27:20 2008 +0200

    x86: use cpuinfo to check for interrupt pending message msr
    
    Simplify code: no need to do a cpuid(1) again. The cpuinfo structure
    has all necessary information already.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 30b5055be355..e76b49e7a916 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -25,35 +25,24 @@ extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
 #ifdef CONFIG_X86_LOCAL_APIC
-#define CPUID_PROCESSOR_SIGNATURE       1
-#define CPUID_XFAM              0x0ff00000
-#define CPUID_XFAM_K8           0x00000000
-#define CPUID_XFAM_10H          0x00100000
-#define CPUID_XFAM_11H          0x00200000
-#define CPUID_XMOD              0x000f0000
-#define CPUID_XMOD_REV_F        0x00040000
 
 /* AMD systems with C1E don't have a working lAPIC timer. Check for that. */
-static __cpuinit int amd_apic_timer_broken(void)
+static __cpuinit int amd_apic_timer_broken(struct cpuinfo_x86 *c)
 {
 	u32 lo, hi;
-	u32 eax = cpuid_eax(CPUID_PROCESSOR_SIGNATURE);
-	switch (eax & CPUID_XFAM) {
-	case CPUID_XFAM_K8:
-		if ((eax & CPUID_XMOD) < CPUID_XMOD_REV_F)
-			break;
-	case CPUID_XFAM_10H:
-	case CPUID_XFAM_11H:
-		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
-		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
-			if (smp_processor_id() != boot_cpu_physical_apicid)
-				printk(KERN_INFO "AMD C1E detected late. "
-				       "	Force timer broadcast.\n");
-			return 1;
-		}
-		break;
-	default:
-		/* err on the side of caution */
+
+	if (c->x86 < 0x0F)
+		return 0;
+
+	/* Family 0x0f models < rev F do not have this MSR */
+	if (c->x86 == 0x0f && c->x86_model < 0x40)
+		return 0;
+
+	rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
+	if (lo & K8_INTP_C1E_ACTIVE_MASK) {
+		if (smp_processor_id() != boot_cpu_physical_apicid)
+			printk(KERN_INFO "AMD C1E detected late. "
+			       "Force timer broadcast.\n");
 		return 1;
 	}
 	return 0;
@@ -297,7 +286,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	}
 
 #ifdef CONFIG_X86_LOCAL_APIC
-	if (amd_apic_timer_broken())
+	if (amd_apic_timer_broken(c))
 		local_apic_timer_disabled = 1;
 #endif
 

commit aa83f3f2cfc74d66d01b1d2eb1485ea1103a0f4e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 17:11:13 2008 +0200

    x86: cleanup C1E enabled detection
    
    Rename the "MSR_K8_ENABLE_C1E" MSR to INT_PENDING_MSG, which is the
    name in the data sheet as well. Move the C1E mask to the header file.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index a38d54f4ff25..30b5055be355 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -25,7 +25,6 @@ extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
 #ifdef CONFIG_X86_LOCAL_APIC
-#define ENABLE_C1E_MASK         0x18000000
 #define CPUID_PROCESSOR_SIGNATURE       1
 #define CPUID_XFAM              0x0ff00000
 #define CPUID_XFAM_K8           0x00000000
@@ -45,8 +44,8 @@ static __cpuinit int amd_apic_timer_broken(void)
 			break;
 	case CPUID_XFAM_10H:
 	case CPUID_XFAM_11H:
-		rdmsr(MSR_K8_ENABLE_C1E, lo, hi);
-		if (lo & ENABLE_C1E_MASK) {
+		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
+		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
 			if (smp_processor_id() != boot_cpu_physical_apicid)
 				printk(KERN_INFO "AMD C1E detected late. "
 				       "	Force timer broadcast.\n");

commit 9e26d84273541a8c6c2efb705457ca8e6245fb73
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Jun 6 12:01:13 2008 +0200

    fix build bug in "x86: add PCI extended config space access for AMD Barcelona"
    
    Also much less code now.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 656b40aed647..a38d54f4ff25 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -4,10 +4,8 @@
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
-#include <asm/mmconfig.h>
 
 #include <mach_apic.h>
-#include "../setup.h"
 #include "cpu.h"
 
 /*

commit 1a5726528a70bb239bdd149aef7f2155cd2b1699
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 2 12:21:36 2008 +0200

    fix build bug in "x86: add PCI extended config space access for AMD Barcelona"

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 99221f9834e4..656b40aed647 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -4,6 +4,7 @@
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
+#include <asm/mmconfig.h>
 
 #include <mach_apic.h>
 #include "../setup.h"

commit 831d991821daedd4839073dbca55514432ef1768
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Sep 3 10:17:39 2007 +0200

    x86: add PCI extended config space access for AMD Barcelona
    
    This patch implements PCI extended configuration space access for
    AMD's Barcelona CPUs. It extends the method using CF8/CFC IO
    addresses. An x86 capability bit has been introduced that is set for
    CPUs supporting PCI extended config space accesses.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 245866828294..99221f9834e4 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -6,6 +6,7 @@
 #include <asm/apic.h>
 
 #include <mach_apic.h>
+#include "../setup.h"
 #include "cpu.h"
 
 /*
@@ -308,6 +309,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 	if (cpu_has_xmm2)
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
+
+	if (c->x86 == 0x10)
+		amd_enable_pci_ext_cfg(c);
 }
 
 static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)

commit f3b14a32db9a74f2bbda980bc95cd4b1f136d80a
Author: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
Date:   Sun Apr 20 06:54:31 2008 +0400

    x86: remove unused function amd_init_cpu()
    
    There are no users for the function amd_init_cpu() defined in
    arch/x86/kernel/cpu/amd.c. This patch removes this routine.
    
    This patch was build-tested using defconfigs for i386 and x86_64,
    and a few randconfig instances. Runtime tests were performed by
    booting 32- and 64-bit x86 boxen up to the shell prompt.
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 0173065dc3b7..245866828294 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -343,10 +343,4 @@ static struct cpu_dev amd_cpu_dev __cpuinitdata = {
 	.c_size_cache	= amd_size_cache,
 };
 
-int __init amd_init_cpu(void)
-{
-	cpu_devs[X86_VENDOR_AMD] = &amd_cpu_dev;
-	return 0;
-}
-
 cpu_vendor_dev_register(X86_VENDOR_AMD, &amd_cpu_dev);

commit dd46e3ca73d136aa7f9f1813e4cbb6934c3611cc
Author: Glauber Costa <gcosta@redhat.com>
Date:   Tue Mar 25 18:10:46 2008 -0300

    x86: move apic declarations to mach_apic.h
    
    take them out of the x86_64-specific asm/mach_apic.h
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 33d38f8305ee..0173065dc3b7 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -4,8 +4,8 @@
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
-#include <asm/mach_apic.h>
 
+#include <mach_apic.h>
 #include "cpu.h"
 
 /*

commit 16282a8e25f1783f296e5116dcef810a8e68d1a0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 26 08:49:57 2008 +0100

    x86: clean up cpu capabilities accesses, amd.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1a3e1bb4d758..33d38f8305ee 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -68,7 +68,7 @@ static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 	if (cpuid_eax(0x80000000) >= 0x80000007) {
 		c->x86_power = cpuid_edx(0x80000007);
 		if (c->x86_power & (1<<8))
-			set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);
+			set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
 	}
 }
 
@@ -105,9 +105,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 	/*
 	 * Bit 31 in normal CPUID used for nonstandard 3DNow ID;
-	 * DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway
+	 * 3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway
 	 */
-	clear_bit(0*32+31, c->x86_capability);
+	clear_cpu_cap(c, 0*32+31);
 
 	r = get_model_name(c);
 
@@ -131,8 +131,8 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 			if (c->x86_model < 6) {
 				/* Based on AMD doc 20734R - June 2000 */
 				if (c->x86_model == 0) {
-					clear_bit(X86_FEATURE_APIC, c->x86_capability);
-					set_bit(X86_FEATURE_PGE, c->x86_capability);
+					clear_cpu_cap(c, X86_FEATURE_APIC);
+					set_cpu_cap(c, X86_FEATURE_PGE);
 				}
 				break;
 			}
@@ -208,7 +208,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 				/*  Set MTRR capability flag if appropriate */
 				if (c->x86_model == 13 || c->x86_model == 9 ||
 				   (c->x86_model == 8 && c->x86_mask >= 8))
-					set_bit(X86_FEATURE_K6_MTRR, c->x86_capability);
+					set_cpu_cap(c, X86_FEATURE_K6_MTRR);
 				break;
 			}
 
@@ -231,7 +231,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 					rdmsr(MSR_K7_HWCR, l, h);
 					l &= ~0x00008000;
 					wrmsr(MSR_K7_HWCR, l, h);
-					set_bit(X86_FEATURE_XMM, c->x86_capability);
+					set_cpu_cap(c, X86_FEATURE_XMM);
 				}
 			}
 
@@ -256,14 +256,14 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	/* Use K8 tuning for Fam10h and Fam11h */
 	case 0x10:
 	case 0x11:
-		set_bit(X86_FEATURE_K8, c->x86_capability);
+		set_cpu_cap(c, X86_FEATURE_K8);
 		break;
 	case 6:
-		set_bit(X86_FEATURE_K7, c->x86_capability);
+		set_cpu_cap(c, X86_FEATURE_K7);
 		break;
 	}
 	if (c->x86 >= 6)
-		set_bit(X86_FEATURE_FXSAVE_LEAK, c->x86_capability);
+		set_cpu_cap(c, X86_FEATURE_FXSAVE_LEAK);
 
 	display_cacheinfo(c);
 
@@ -304,10 +304,10 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 	/* K6s reports MCEs but don't actually have all the MSRs */
 	if (c->x86 < 6)
-		clear_bit(X86_FEATURE_MCE, c->x86_capability);
+		clear_cpu_cap(c, X86_FEATURE_MCE);
 
 	if (cpu_has_xmm2)
-		set_bit(X86_FEATURE_MFENCE_RDTSC, c->x86_capability);
+		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
 }
 
 static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)

commit fb87a298fb79357fa5b27e6916ae1c45bf94dac7
Author: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
Date:   Fri Feb 22 23:10:33 2008 +0100

    x86: coding style fixes to arch/x86/kernel/cpu/amd.c
    
    Before:
       total: 42 errors, 26 warnings, 350 lines checked
    After:
       total: 0 errors, 26 warnings, 352 lines checked
    
    No code changed:
    
    arch/x86/kernel/cpu/amd.o:
    
       text    data     bss     dec     hex filename
       1936     328       0    2264     8d8 amd.o.before
       1936     328       0    2264     8d8 amd.o.after
    
    md5:
       873430a88faaf31bb4bbfe3a2a691e45  amd.o.before.asm
       873430a88faaf31bb4bbfe3a2a691e45  amd.o.after.asm
    
    Signed-off-by: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index cab4e562b5cb..1a3e1bb4d758 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -20,7 +20,7 @@
  *	the chip setting when fixing the bug but they also tweaked some
  *	performance at the same time..
  */
- 
+
 extern void vide(void);
 __asm__(".align 4\nvide: ret");
 
@@ -81,7 +81,8 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 #ifdef CONFIG_SMP
 	unsigned long long value;
 
-	/* Disable TLB flush filter by setting HWCR.FFDIS on K8
+	/*
+	 * Disable TLB flush filter by setting HWCR.FFDIS on K8
 	 * bit 6 of msr C001_0015
 	 *
 	 * Errata 63 for SH-B3 steppings
@@ -102,15 +103,16 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	 *	no bus pipeline)
 	 */
 
-	/* Bit 31 in normal CPUID used for nonstandard 3DNow ID;
-	   3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway */
+	/*
+	 * Bit 31 in normal CPUID used for nonstandard 3DNow ID;
+	 * DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway
+	 */
 	clear_bit(0*32+31, c->x86_capability);
-	
+
 	r = get_model_name(c);
 
-	switch(c->x86)
-	{
-		case 4:
+	switch (c->x86) {
+	case 4:
 		/*
 		 * General Systems BIOSen alias the cpu frequency registers
 		 * of the Elan at 0x000df000. Unfortuantly, one of the Linux
@@ -120,61 +122,60 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 #define CBAR		(0xfffc) /* Configuration Base Address  (32-bit) */
 #define CBAR_ENB	(0x80000000)
 #define CBAR_KEY	(0X000000CB)
-			if (c->x86_model==9 || c->x86_model == 10) {
+			if (c->x86_model == 9 || c->x86_model == 10) {
 				if (inl (CBAR) & CBAR_ENB)
 					outl (0 | CBAR_KEY, CBAR);
 			}
 			break;
-		case 5:
-			if( c->x86_model < 6 )
-			{
+	case 5:
+			if (c->x86_model < 6) {
 				/* Based on AMD doc 20734R - June 2000 */
-				if ( c->x86_model == 0 ) {
+				if (c->x86_model == 0) {
 					clear_bit(X86_FEATURE_APIC, c->x86_capability);
 					set_bit(X86_FEATURE_PGE, c->x86_capability);
 				}
 				break;
 			}
-			
-			if ( c->x86_model == 6 && c->x86_mask == 1 ) {
+
+			if (c->x86_model == 6 && c->x86_mask == 1) {
 				const int K6_BUG_LOOP = 1000000;
 				int n;
 				void (*f_vide)(void);
 				unsigned long d, d2;
-				
+
 				printk(KERN_INFO "AMD K6 stepping B detected - ");
-				
+
 				/*
-				 * It looks like AMD fixed the 2.6.2 bug and improved indirect 
+				 * It looks like AMD fixed the 2.6.2 bug and improved indirect
 				 * calls at the same time.
 				 */
 
 				n = K6_BUG_LOOP;
 				f_vide = vide;
 				rdtscl(d);
-				while (n--) 
+				while (n--)
 					f_vide();
 				rdtscl(d2);
 				d = d2-d;
 
-				if (d > 20*K6_BUG_LOOP) 
+				if (d > 20*K6_BUG_LOOP)
 					printk("system stability may be impaired when more than 32 MB are used.\n");
-				else 
+				else
 					printk("probably OK (after B9730xxxx).\n");
 				printk(KERN_INFO "Please see http://membres.lycos.fr/poulot/k6bug.html\n");
 			}
 
 			/* K6 with old style WHCR */
 			if (c->x86_model < 8 ||
-			   (c->x86_model== 8 && c->x86_mask < 8)) {
+			   (c->x86_model == 8 && c->x86_mask < 8)) {
 				/* We can only write allocate on the low 508Mb */
-				if(mbytes>508)
-					mbytes=508;
+				if (mbytes > 508)
+					mbytes = 508;
 
 				rdmsr(MSR_K6_WHCR, l, h);
-				if ((l&0x0000FFFF)==0) {
+				if ((l&0x0000FFFF) == 0) {
 					unsigned long flags;
-					l=(1<<0)|((mbytes/4)<<1);
+					l = (1<<0)|((mbytes/4)<<1);
 					local_irq_save(flags);
 					wbinvd();
 					wrmsr(MSR_K6_WHCR, l, h);
@@ -185,17 +186,17 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 				break;
 			}
 
-			if ((c->x86_model == 8 && c->x86_mask >7) ||
+			if ((c->x86_model == 8 && c->x86_mask > 7) ||
 			     c->x86_model == 9 || c->x86_model == 13) {
 				/* The more serious chips .. */
 
-				if(mbytes>4092)
-					mbytes=4092;
+				if (mbytes > 4092)
+					mbytes = 4092;
 
 				rdmsr(MSR_K6_WHCR, l, h);
-				if ((l&0xFFFF0000)==0) {
+				if ((l&0xFFFF0000) == 0) {
 					unsigned long flags;
-					l=((mbytes>>2)<<22)|(1<<16);
+					l = ((mbytes>>2)<<22)|(1<<16);
 					local_irq_save(flags);
 					wbinvd();
 					wrmsr(MSR_K6_WHCR, l, h);
@@ -217,10 +218,11 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 				break;
 			}
 			break;
-		case 6: /* An Athlon/Duron */
- 
-			/* Bit 15 of Athlon specific MSR 15, needs to be 0
- 			 * to enable SSE on Palomino/Morgan/Barton CPU's.
+	case 6: /* An Athlon/Duron */
+
+			/*
+			 * Bit 15 of Athlon specific MSR 15, needs to be 0
+			 * to enable SSE on Palomino/Morgan/Barton CPU's.
 			 * If the BIOS didn't enable it already, enable it here.
 			 */
 			if (c->x86_model >= 6 && c->x86_model <= 10) {
@@ -233,11 +235,12 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 				}
 			}
 
-			/* It's been determined by AMD that Athlons since model 8 stepping 1
+			/*
+			 * It's been determined by AMD that Athlons since model 8 stepping 1
 			 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
 			 * As per AMD technical note 27212 0.2
 			 */
-			if ((c->x86_model == 8 && c->x86_mask>=1) || (c->x86_model > 8)) {
+			if ((c->x86_model == 8 && c->x86_mask >= 1) || (c->x86_model > 8)) {
 				rdmsr(MSR_K7_CLK_CTL, l, h);
 				if ((l & 0xfff00000) != 0x20000000) {
 					printk ("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n", l,
@@ -256,7 +259,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		set_bit(X86_FEATURE_K8, c->x86_capability);
 		break;
 	case 6:
-		set_bit(X86_FEATURE_K7, c->x86_capability); 
+		set_bit(X86_FEATURE_K7, c->x86_capability);
 		break;
 	}
 	if (c->x86 >= 6)
@@ -264,9 +267,8 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 
 	display_cacheinfo(c);
 
-	if (cpuid_eax(0x80000000) >= 0x80000008) {
+	if (cpuid_eax(0x80000000) >= 0x80000008)
 		c->x86_max_cores = (cpuid_ecx(0x80000008) & 0xff) + 1;
-	}
 
 #ifdef CONFIG_X86_HT
 	/*
@@ -308,14 +310,14 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		set_bit(X86_FEATURE_MFENCE_RDTSC, c->x86_capability);
 }
 
-static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 * c, unsigned int size)
+static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)
 {
 	/* AMD errata T13 (order #21922) */
 	if ((c->x86 == 6)) {
 		if (c->x86_model == 3 && c->x86_mask == 0)	/* Duron Rev A0 */
 			size = 64;
 		if (c->x86_model == 4 &&
-		    (c->x86_mask==0 || c->x86_mask==1))	/* Tbird rev A1/A2 */
+		    (c->x86_mask == 0 || c->x86_mask == 1))	/* Tbird rev A1/A2 */
 			size = 256;
 	}
 	return size;
@@ -323,16 +325,16 @@ static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 * c, unsigned in
 
 static struct cpu_dev amd_cpu_dev __cpuinitdata = {
 	.c_vendor	= "AMD",
-	.c_ident 	= { "AuthenticAMD" },
+	.c_ident	= { "AuthenticAMD" },
 	.c_models = {
 		{ .vendor = X86_VENDOR_AMD, .family = 4, .model_names =
 		  {
 			  [3] = "486 DX/2",
 			  [7] = "486 DX/2-WB",
-			  [8] = "486 DX/4", 
-			  [9] = "486 DX/4-WB", 
+			  [8] = "486 DX/4",
+			  [9] = "486 DX/4-WB",
 			  [14] = "Am5x86-WT",
-			  [15] = "Am5x86-WB" 
+			  [15] = "Am5x86-WB"
 		  }
 		},
 	},

commit 03ae5768b6110ebaa97dc3e7abf1c3d8bec5f874
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri Feb 15 12:00:23 2008 +0100

    x86: use ELF section to list CPU vendor specific code
    
    Replace the hardcoded list of initialization functions for each CPU
    vendor by a list in an ELF section, which is read at initialization in
    arch/x86/kernel/cpu/cpu.c to fill the cpu_devs[] array. The ELF
    section, named .x86cpuvendor.init, is reclaimed after boot, and
    contains entries of type "struct cpu_vendor_dev" which associates a
    vendor number with a pointer to a "struct cpu_dev" structure.
    
    This first modification allows to remove all the VENDOR_init_cpu()
    functions.
    
    This patch also removes the hardcoded calls to early_init_amd() and
    early_init_intel(). Instead, we add a "c_early_init" member to the
    cpu_dev structure, which is then called if not NULL by the generic CPU
    initialization code. Unfortunately, in early_cpu_detect(), this_cpu is
    not yet set, so we have to use the cpu_devs[] array directly.
    
    This patch is part of the Linux Tiny project, and is needed for
    further patch that will allow to disable compilation of unused CPU
    support code.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 693e353999cd..cab4e562b5cb 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -63,7 +63,7 @@ static __cpuinit int amd_apic_timer_broken(void)
 
 int force_mwait __cpuinitdata;
 
-void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
+static void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
 {
 	if (cpuid_eax(0x80000000) >= 0x80000007) {
 		c->x86_power = cpuid_edx(0x80000007);
@@ -336,6 +336,7 @@ static struct cpu_dev amd_cpu_dev __cpuinitdata = {
 		  }
 		},
 	},
+	.c_early_init   = early_init_amd,
 	.c_init		= init_amd,
 	.c_size_cache	= amd_size_cache,
 };
@@ -345,3 +346,5 @@ int __init amd_init_cpu(void)
 	cpu_devs[X86_VENDOR_AMD] = &amd_cpu_dev;
 	return 0;
 }
+
+cpu_vendor_dev_register(X86_VENDOR_AMD, &amd_cpu_dev);

commit aa6299926950c8dfe2fea638276cad6def092bc9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 1 23:45:18 2008 +0100

    x86: fix bootup crash in native_read_tsc()
    
    fix bootup crash in native_read_tsc() that was reported on an Athlon-XP
    and bisected. The correct feature boundary for X86_FEATURE_MFENCE_RDTSC
    is not XMM but XMM2.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 06fa159232fd..693e353999cd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -304,7 +304,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	if (c->x86 < 6)
 		clear_bit(X86_FEATURE_MCE, c->x86_capability);
 
-	if (cpu_has_xmm)
+	if (cpu_has_xmm2)
 		set_bit(X86_FEATURE_MFENCE_RDTSC, c->x86_capability);
 }
 

commit 0c07ee38c9d4eb081758f5ad14bbffa7197e1aec
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:16 2008 +0100

    x86: use the correct cpuid method to detect MWAIT support for C states
    
    Previously there was a AMD specific quirk to handle the case of
    AMD Fam10h MWAIT not supporting any C states. But it turns out
    that CPUID already has ways to detectly detect that without
    using special quirks.
    
    The new code simply checks if MWAIT supports at least C1 and doesn't
    use it if it doesn't. No more vendor specific code.
    
    Note this is does not simply clear MWAIT because MWAIT can be still
    useful even without C states.
    
    Credit goes to Ben Serebrin for pointing out the (nearly) obvious.
    
    Cc: "Andreas Herrmann" <andreas.herrmann3@amd.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index cd2fe15ff4b5..06fa159232fd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -300,9 +300,6 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		local_apic_timer_disabled = 1;
 #endif
 
-	if (c->x86 == 0x10 && !force_mwait)
-		clear_bit(X86_FEATURE_MWAIT, c->x86_capability);
-
 	/* K6s reports MCEs but don't actually have all the MSRs */
 	if (c->x86 < 6)
 		clear_bit(X86_FEATURE_MCE, c->x86_capability);

commit 2b16a2353814a513cdb5c5c739b76a19d7ea39ce
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:32:40 2008 +0100

    x86: move X86_FEATURE_CONSTANT_TSC into early cpu feature detection
    
    Need this in the next patch in time_init and that happens early.
    
    This includes a minor fix on i386 where early_intel_workarounds()
    [which is now called early_init_intel] really executes early as
    the comments say.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index aaa8101d3d80..cd2fe15ff4b5 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -63,6 +63,15 @@ static __cpuinit int amd_apic_timer_broken(void)
 
 int force_mwait __cpuinitdata;
 
+void __cpuinit early_init_amd(struct cpuinfo_x86 *c)
+{
+	if (cpuid_eax(0x80000000) >= 0x80000007) {
+		c->x86_power = cpuid_edx(0x80000007);
+		if (c->x86_power & (1<<8))
+			set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);
+	}
+}
+
 static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 {
 	u32 l, h;
@@ -85,6 +94,8 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	}
 #endif
 
+	early_init_amd(c);
+
 	/*
 	 *	FIXME: We should handle the K5 here. Set up the write
 	 *	range and also turn on MSR 83 bits 4 and 31 (write alloc,
@@ -257,12 +268,6 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 		c->x86_max_cores = (cpuid_ecx(0x80000008) & 0xff) + 1;
 	}
 
-	if (cpuid_eax(0x80000000) >= 0x80000007) {
-		c->x86_power = cpuid_edx(0x80000007);
-		if (c->x86_power & (1<<8))
-			set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);
-	}
-
 #ifdef CONFIG_X86_HT
 	/*
 	 * On a AMD multi core setup the lower bits of the APIC id

commit de4218634e3df6d73a3e6cdfdf3a17fa3bc7e013
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:32:37 2008 +0100

    x86: implement support to synchronize RDTSC through MFENCE on AMD CPUs
    
    According to AMD RDTSC can be synchronized through MFENCE.
    Implement the necessary CPUID bit for that.
    
    Cc: andreas.herrmann3@amd.com
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1ff88c7f45cf..aaa8101d3d80 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -301,6 +301,9 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 	/* K6s reports MCEs but don't actually have all the MSRs */
 	if (c->x86 < 6)
 		clear_bit(X86_FEATURE_MCE, c->x86_capability);
+
+	if (cpu_has_xmm)
+		set_bit(X86_FEATURE_MFENCE_RDTSC, c->x86_capability);
 }
 
 static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 * c, unsigned int size)

commit 27b46d7661dc720224813eb4f452e424f1bf3a9a
Author: Simon Arlott <simon@fire.lp0.eu>
Date:   Sat Oct 20 01:13:56 2007 +0200

    spelling fixes: arch/i386/
    
    Spelling fixes in arch/i386/.
    
    Signed-off-by: Simon Arlott <simon@fire.lp0.eu>
    Signed-off-by: Adrian Bunk <bunk@kernel.org>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 5f8af875f457..1ff88c7f45cf 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -266,7 +266,7 @@ static void __cpuinit init_amd(struct cpuinfo_x86 *c)
 #ifdef CONFIG_X86_HT
 	/*
 	 * On a AMD multi core setup the lower bits of the APIC id
-	 * distingush the cores.
+	 * distinguish the cores.
 	 */
 	if (c->x86_max_cores > 1) {
 		int cpu = smp_processor_id();

commit c1e3619edd2b3e17450d745e27e335490cafd78d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 17 18:04:40 2007 +0200

    x86: print info about late C1E detection on 32bit as well
    
    Some BIOSes set the C1E flag only on the second core. Print a warning so
    the Firmware Toolkit can check for it.
    
    mingo: fix C1E build bug on 32-bit
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index dcf6bbb1c7c0..5f8af875f457 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -4,6 +4,7 @@
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/apic.h>
+#include <asm/mach_apic.h>
 
 #include "cpu.h"
 
@@ -45,13 +46,17 @@ static __cpuinit int amd_apic_timer_broken(void)
 	case CPUID_XFAM_10H:
 	case CPUID_XFAM_11H:
 		rdmsr(MSR_K8_ENABLE_C1E, lo, hi);
-		if (lo & ENABLE_C1E_MASK)
+		if (lo & ENABLE_C1E_MASK) {
+			if (smp_processor_id() != boot_cpu_physical_apicid)
+				printk(KERN_INFO "AMD C1E detected late. "
+				       "	Force timer broadcast.\n");
 			return 1;
-                break;
-        default:
-                /* err on the side of caution */
+		}
+		break;
+	default:
+		/* err on the side of caution */
 		return 1;
-        }
+	}
 	return 0;
 }
 #endif

commit f7627e2513987bb5d4e8cb13c4e0a478352141ac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 11 11:16:58 2007 +0200

    i386: move kernel/cpu
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
new file mode 100644
index 000000000000..dcf6bbb1c7c0
--- /dev/null
+++ b/arch/x86/kernel/cpu/amd.c
@@ -0,0 +1,337 @@
+#include <linux/init.h>
+#include <linux/bitops.h>
+#include <linux/mm.h>
+#include <asm/io.h>
+#include <asm/processor.h>
+#include <asm/apic.h>
+
+#include "cpu.h"
+
+/*
+ *	B step AMD K6 before B 9730xxxx have hardware bugs that can cause
+ *	misexecution of code under Linux. Owners of such processors should
+ *	contact AMD for precise details and a CPU swap.
+ *
+ *	See	http://www.multimania.com/poulot/k6bug.html
+ *		http://www.amd.com/K6/k6docs/revgd.html
+ *
+ *	The following test is erm.. interesting. AMD neglected to up
+ *	the chip setting when fixing the bug but they also tweaked some
+ *	performance at the same time..
+ */
+ 
+extern void vide(void);
+__asm__(".align 4\nvide: ret");
+
+#ifdef CONFIG_X86_LOCAL_APIC
+#define ENABLE_C1E_MASK         0x18000000
+#define CPUID_PROCESSOR_SIGNATURE       1
+#define CPUID_XFAM              0x0ff00000
+#define CPUID_XFAM_K8           0x00000000
+#define CPUID_XFAM_10H          0x00100000
+#define CPUID_XFAM_11H          0x00200000
+#define CPUID_XMOD              0x000f0000
+#define CPUID_XMOD_REV_F        0x00040000
+
+/* AMD systems with C1E don't have a working lAPIC timer. Check for that. */
+static __cpuinit int amd_apic_timer_broken(void)
+{
+	u32 lo, hi;
+	u32 eax = cpuid_eax(CPUID_PROCESSOR_SIGNATURE);
+	switch (eax & CPUID_XFAM) {
+	case CPUID_XFAM_K8:
+		if ((eax & CPUID_XMOD) < CPUID_XMOD_REV_F)
+			break;
+	case CPUID_XFAM_10H:
+	case CPUID_XFAM_11H:
+		rdmsr(MSR_K8_ENABLE_C1E, lo, hi);
+		if (lo & ENABLE_C1E_MASK)
+			return 1;
+                break;
+        default:
+                /* err on the side of caution */
+		return 1;
+        }
+	return 0;
+}
+#endif
+
+int force_mwait __cpuinitdata;
+
+static void __cpuinit init_amd(struct cpuinfo_x86 *c)
+{
+	u32 l, h;
+	int mbytes = num_physpages >> (20-PAGE_SHIFT);
+	int r;
+
+#ifdef CONFIG_SMP
+	unsigned long long value;
+
+	/* Disable TLB flush filter by setting HWCR.FFDIS on K8
+	 * bit 6 of msr C001_0015
+	 *
+	 * Errata 63 for SH-B3 steppings
+	 * Errata 122 for all steppings (F+ have it disabled by default)
+	 */
+	if (c->x86 == 15) {
+		rdmsrl(MSR_K7_HWCR, value);
+		value |= 1 << 6;
+		wrmsrl(MSR_K7_HWCR, value);
+	}
+#endif
+
+	/*
+	 *	FIXME: We should handle the K5 here. Set up the write
+	 *	range and also turn on MSR 83 bits 4 and 31 (write alloc,
+	 *	no bus pipeline)
+	 */
+
+	/* Bit 31 in normal CPUID used for nonstandard 3DNow ID;
+	   3DNow is IDd by bit 31 in extended CPUID (1*32+31) anyway */
+	clear_bit(0*32+31, c->x86_capability);
+	
+	r = get_model_name(c);
+
+	switch(c->x86)
+	{
+		case 4:
+		/*
+		 * General Systems BIOSen alias the cpu frequency registers
+		 * of the Elan at 0x000df000. Unfortuantly, one of the Linux
+		 * drivers subsequently pokes it, and changes the CPU speed.
+		 * Workaround : Remove the unneeded alias.
+		 */
+#define CBAR		(0xfffc) /* Configuration Base Address  (32-bit) */
+#define CBAR_ENB	(0x80000000)
+#define CBAR_KEY	(0X000000CB)
+			if (c->x86_model==9 || c->x86_model == 10) {
+				if (inl (CBAR) & CBAR_ENB)
+					outl (0 | CBAR_KEY, CBAR);
+			}
+			break;
+		case 5:
+			if( c->x86_model < 6 )
+			{
+				/* Based on AMD doc 20734R - June 2000 */
+				if ( c->x86_model == 0 ) {
+					clear_bit(X86_FEATURE_APIC, c->x86_capability);
+					set_bit(X86_FEATURE_PGE, c->x86_capability);
+				}
+				break;
+			}
+			
+			if ( c->x86_model == 6 && c->x86_mask == 1 ) {
+				const int K6_BUG_LOOP = 1000000;
+				int n;
+				void (*f_vide)(void);
+				unsigned long d, d2;
+				
+				printk(KERN_INFO "AMD K6 stepping B detected - ");
+				
+				/*
+				 * It looks like AMD fixed the 2.6.2 bug and improved indirect 
+				 * calls at the same time.
+				 */
+
+				n = K6_BUG_LOOP;
+				f_vide = vide;
+				rdtscl(d);
+				while (n--) 
+					f_vide();
+				rdtscl(d2);
+				d = d2-d;
+
+				if (d > 20*K6_BUG_LOOP) 
+					printk("system stability may be impaired when more than 32 MB are used.\n");
+				else 
+					printk("probably OK (after B9730xxxx).\n");
+				printk(KERN_INFO "Please see http://membres.lycos.fr/poulot/k6bug.html\n");
+			}
+
+			/* K6 with old style WHCR */
+			if (c->x86_model < 8 ||
+			   (c->x86_model== 8 && c->x86_mask < 8)) {
+				/* We can only write allocate on the low 508Mb */
+				if(mbytes>508)
+					mbytes=508;
+
+				rdmsr(MSR_K6_WHCR, l, h);
+				if ((l&0x0000FFFF)==0) {
+					unsigned long flags;
+					l=(1<<0)|((mbytes/4)<<1);
+					local_irq_save(flags);
+					wbinvd();
+					wrmsr(MSR_K6_WHCR, l, h);
+					local_irq_restore(flags);
+					printk(KERN_INFO "Enabling old style K6 write allocation for %d Mb\n",
+						mbytes);
+				}
+				break;
+			}
+
+			if ((c->x86_model == 8 && c->x86_mask >7) ||
+			     c->x86_model == 9 || c->x86_model == 13) {
+				/* The more serious chips .. */
+
+				if(mbytes>4092)
+					mbytes=4092;
+
+				rdmsr(MSR_K6_WHCR, l, h);
+				if ((l&0xFFFF0000)==0) {
+					unsigned long flags;
+					l=((mbytes>>2)<<22)|(1<<16);
+					local_irq_save(flags);
+					wbinvd();
+					wrmsr(MSR_K6_WHCR, l, h);
+					local_irq_restore(flags);
+					printk(KERN_INFO "Enabling new style K6 write allocation for %d Mb\n",
+						mbytes);
+				}
+
+				/*  Set MTRR capability flag if appropriate */
+				if (c->x86_model == 13 || c->x86_model == 9 ||
+				   (c->x86_model == 8 && c->x86_mask >= 8))
+					set_bit(X86_FEATURE_K6_MTRR, c->x86_capability);
+				break;
+			}
+
+			if (c->x86_model == 10) {
+				/* AMD Geode LX is model 10 */
+				/* placeholder for any needed mods */
+				break;
+			}
+			break;
+		case 6: /* An Athlon/Duron */
+ 
+			/* Bit 15 of Athlon specific MSR 15, needs to be 0
+ 			 * to enable SSE on Palomino/Morgan/Barton CPU's.
+			 * If the BIOS didn't enable it already, enable it here.
+			 */
+			if (c->x86_model >= 6 && c->x86_model <= 10) {
+				if (!cpu_has(c, X86_FEATURE_XMM)) {
+					printk(KERN_INFO "Enabling disabled K7/SSE Support.\n");
+					rdmsr(MSR_K7_HWCR, l, h);
+					l &= ~0x00008000;
+					wrmsr(MSR_K7_HWCR, l, h);
+					set_bit(X86_FEATURE_XMM, c->x86_capability);
+				}
+			}
+
+			/* It's been determined by AMD that Athlons since model 8 stepping 1
+			 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
+			 * As per AMD technical note 27212 0.2
+			 */
+			if ((c->x86_model == 8 && c->x86_mask>=1) || (c->x86_model > 8)) {
+				rdmsr(MSR_K7_CLK_CTL, l, h);
+				if ((l & 0xfff00000) != 0x20000000) {
+					printk ("CPU: CLK_CTL MSR was %x. Reprogramming to %x\n", l,
+						((l & 0x000fffff)|0x20000000));
+					wrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);
+				}
+			}
+			break;
+	}
+
+	switch (c->x86) {
+	case 15:
+	/* Use K8 tuning for Fam10h and Fam11h */
+	case 0x10:
+	case 0x11:
+		set_bit(X86_FEATURE_K8, c->x86_capability);
+		break;
+	case 6:
+		set_bit(X86_FEATURE_K7, c->x86_capability); 
+		break;
+	}
+	if (c->x86 >= 6)
+		set_bit(X86_FEATURE_FXSAVE_LEAK, c->x86_capability);
+
+	display_cacheinfo(c);
+
+	if (cpuid_eax(0x80000000) >= 0x80000008) {
+		c->x86_max_cores = (cpuid_ecx(0x80000008) & 0xff) + 1;
+	}
+
+	if (cpuid_eax(0x80000000) >= 0x80000007) {
+		c->x86_power = cpuid_edx(0x80000007);
+		if (c->x86_power & (1<<8))
+			set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);
+	}
+
+#ifdef CONFIG_X86_HT
+	/*
+	 * On a AMD multi core setup the lower bits of the APIC id
+	 * distingush the cores.
+	 */
+	if (c->x86_max_cores > 1) {
+		int cpu = smp_processor_id();
+		unsigned bits = (cpuid_ecx(0x80000008) >> 12) & 0xf;
+
+		if (bits == 0) {
+			while ((1 << bits) < c->x86_max_cores)
+				bits++;
+		}
+		c->cpu_core_id = c->phys_proc_id & ((1<<bits)-1);
+		c->phys_proc_id >>= bits;
+		printk(KERN_INFO "CPU %d(%d) -> Core %d\n",
+		       cpu, c->x86_max_cores, c->cpu_core_id);
+	}
+#endif
+
+	if (cpuid_eax(0x80000000) >= 0x80000006) {
+		if ((c->x86 == 0x10) && (cpuid_edx(0x80000006) & 0xf000))
+			num_cache_leaves = 4;
+		else
+			num_cache_leaves = 3;
+	}
+
+#ifdef CONFIG_X86_LOCAL_APIC
+	if (amd_apic_timer_broken())
+		local_apic_timer_disabled = 1;
+#endif
+
+	if (c->x86 == 0x10 && !force_mwait)
+		clear_bit(X86_FEATURE_MWAIT, c->x86_capability);
+
+	/* K6s reports MCEs but don't actually have all the MSRs */
+	if (c->x86 < 6)
+		clear_bit(X86_FEATURE_MCE, c->x86_capability);
+}
+
+static unsigned int __cpuinit amd_size_cache(struct cpuinfo_x86 * c, unsigned int size)
+{
+	/* AMD errata T13 (order #21922) */
+	if ((c->x86 == 6)) {
+		if (c->x86_model == 3 && c->x86_mask == 0)	/* Duron Rev A0 */
+			size = 64;
+		if (c->x86_model == 4 &&
+		    (c->x86_mask==0 || c->x86_mask==1))	/* Tbird rev A1/A2 */
+			size = 256;
+	}
+	return size;
+}
+
+static struct cpu_dev amd_cpu_dev __cpuinitdata = {
+	.c_vendor	= "AMD",
+	.c_ident 	= { "AuthenticAMD" },
+	.c_models = {
+		{ .vendor = X86_VENDOR_AMD, .family = 4, .model_names =
+		  {
+			  [3] = "486 DX/2",
+			  [7] = "486 DX/2-WB",
+			  [8] = "486 DX/4", 
+			  [9] = "486 DX/4-WB", 
+			  [14] = "Am5x86-WT",
+			  [15] = "Am5x86-WB" 
+		  }
+		},
+	},
+	.c_init		= init_amd,
+	.c_size_cache	= amd_size_cache,
+};
+
+int __init amd_init_cpu(void)
+{
+	cpu_devs[X86_VENDOR_AMD] = &amd_cpu_dev;
+	return 0;
+}
