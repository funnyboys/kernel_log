commit 13cad9851ef1d004640991d45227dd35c08f45fc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:45 2020 +0200

    x86/entry: Convert reschedule interrupt to IDTENTRY_SYSVEC_SIMPLE
    
    The scheduler IPI does not need the full interrupt entry handling logic
    when the entry is from kernel mode. Use IDTENTRY_SYSVEC_SIMPLE and spare
    all the overhead.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20200521202119.835425642@linutronix.de

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index e5647daa7e96..eff4ce3b10da 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -220,26 +220,15 @@ static void native_stop_other_cpus(int wait)
 
 /*
  * Reschedule call back. KVM uses this interrupt to force a cpu out of
- * guest mode
+ * guest mode.
  */
-__visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
+DEFINE_IDTENTRY_SYSVEC_SIMPLE(sysvec_reschedule_ipi)
 {
 	ack_APIC_irq();
+	trace_reschedule_entry(RESCHEDULE_VECTOR);
 	inc_irq_stat(irq_resched_count);
-
-	if (trace_resched_ipi_enabled()) {
-		/*
-		 * scheduler_ipi() might call irq_enter() as well, but
-		 * nested calls are fine.
-		 */
-		irq_enter();
-		trace_reschedule_entry(RESCHEDULE_VECTOR);
-		scheduler_ipi();
-		trace_reschedule_exit(RESCHEDULE_VECTOR);
-		irq_exit();
-		return;
-	}
 	scheduler_ipi();
+	trace_reschedule_exit(RESCHEDULE_VECTOR);
 }
 
 DEFINE_IDTENTRY_SYSVEC(sysvec_call_function)

commit 582f9191231b994582ad5349a7b06b3255c926fb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:40 2020 +0200

    x86/entry: Convert SMP system vectors to IDTENTRY_SYSVEC
    
    Convert SMP system vectors to IDTENTRY_SYSVEC:
    
      - Implement the C entry point with DEFINE_IDTENTRY_SYSVEC
      - Emit the ASM stub with DECLARE_IDTENTRY_SYSVEC
      - Remove the ASM idtentries in 64-bit
      - Remove the BUILD_INTERRUPT entries in 32-bit
      - Remove the old prototypes
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20200521202119.372234635@linutronix.de

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index b8d4e9c3c070..e5647daa7e96 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -27,6 +27,7 @@
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
 #include <asm/apic.h>
+#include <asm/idtentry.h>
 #include <asm/nmi.h>
 #include <asm/mce.h>
 #include <asm/trace/irq_vectors.h>
@@ -130,13 +131,11 @@ static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
 /*
  * this function calls the 'stop' function on all other CPUs in the system.
  */
-
-asmlinkage __visible void smp_reboot_interrupt(void)
+DEFINE_IDTENTRY_SYSVEC(sysvec_reboot)
 {
-	ipi_entering_ack_irq();
+	ack_APIC_irq();
 	cpu_emergency_vmxoff();
 	stop_this_cpu(NULL);
-	irq_exit();
 }
 
 static int register_stop_handler(void)
@@ -227,7 +226,6 @@ __visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
-	kvm_set_cpu_l1tf_flush_l1d();
 
 	if (trace_resched_ipi_enabled()) {
 		/*
@@ -244,24 +242,22 @@ __visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
 	scheduler_ipi();
 }
 
-__visible void __irq_entry smp_call_function_interrupt(struct pt_regs *regs)
+DEFINE_IDTENTRY_SYSVEC(sysvec_call_function)
 {
-	ipi_entering_ack_irq();
+	ack_APIC_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
 	inc_irq_stat(irq_call_count);
 	generic_smp_call_function_interrupt();
 	trace_call_function_exit(CALL_FUNCTION_VECTOR);
-	exiting_irq();
 }
 
-__visible void __irq_entry smp_call_function_single_interrupt(struct pt_regs *r)
+DEFINE_IDTENTRY_SYSVEC(sysvec_call_function_single)
 {
-	ipi_entering_ack_irq();
+	ack_APIC_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);
 	inc_irq_stat(irq_call_count);
 	generic_smp_call_function_single_interrupt();
 	trace_call_function_single_exit(CALL_FUNCTION_SINGLE_VECTOR);
-	exiting_irq();
 }
 
 static int __init nonmi_ipi_setup(char *str)

commit d0a7166bc7ac4feac5c482ebe8b2417aa3302ef4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 22 20:47:25 2019 +0200

    x86/smp: Move smp_function_call implementations into IPI code
    
    Move it where it belongs. That allows to keep all the shorthand logic in
    one place.
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20190722105220.677835995@linutronix.de

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index b8ad1876a081..b8d4e9c3c070 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -115,46 +115,6 @@
 static atomic_t stopping_cpu = ATOMIC_INIT(-1);
 static bool smp_no_nmi_ipi = false;
 
-/*
- * this function sends a 'reschedule' IPI to another CPU.
- * it goes straight through and wastes no time serializing
- * anything. Worst case is that we lose a reschedule ...
- */
-static void native_smp_send_reschedule(int cpu)
-{
-	if (unlikely(cpu_is_offline(cpu))) {
-		WARN(1, "sched: Unexpected reschedule of offline CPU#%d!\n", cpu);
-		return;
-	}
-	apic->send_IPI(cpu, RESCHEDULE_VECTOR);
-}
-
-void native_send_call_func_single_ipi(int cpu)
-{
-	apic->send_IPI(cpu, CALL_FUNCTION_SINGLE_VECTOR);
-}
-
-void native_send_call_func_ipi(const struct cpumask *mask)
-{
-	cpumask_var_t allbutself;
-
-	if (!alloc_cpumask_var(&allbutself, GFP_ATOMIC)) {
-		apic->send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
-		return;
-	}
-
-	cpumask_copy(allbutself, cpu_online_mask);
-	__cpumask_clear_cpu(smp_processor_id(), allbutself);
-
-	if (cpumask_equal(mask, allbutself) &&
-	    cpumask_equal(cpu_online_mask, cpu_callout_mask))
-		apic->send_IPI_allbutself(CALL_FUNCTION_VECTOR);
-	else
-		apic->send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
-
-	free_cpumask_var(allbutself);
-}
-
 static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
 {
 	/* We are registered on stopping cpu too, avoid spurious NMI */

commit 22ca7ee933a39f542ff6f81fc64f8036eff56519
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 22 20:47:23 2019 +0200

    x86/apic: Provide and use helper for send_IPI_allbutself()
    
    To support IPI shorthands wrap invocations of apic->send_IPI_allbutself()
    in a helper function, so the static key controlling the shorthand mode is
    only in one place.
    
    Fixup all callers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20190722105220.492691679@linutronix.de

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 231fa230ebc7..b8ad1876a081 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -215,7 +215,7 @@ static void native_stop_other_cpus(int wait)
 		/* sync above data before sending IRQ */
 		wmb();
 
-		apic->send_IPI_allbutself(REBOOT_VECTOR);
+		apic_send_IPI_allbutself(REBOOT_VECTOR);
 
 		/*
 		 * Don't wait longer than a second for IPI completion. The
@@ -241,7 +241,7 @@ static void native_stop_other_cpus(int wait)
 
 			pr_emerg("Shutting down cpus with NMI\n");
 
-			apic->send_IPI_allbutself(NMI_VECTOR);
+			apic_send_IPI_allbutself(NMI_VECTOR);
 		}
 		/*
 		 * Don't wait longer than 10 ms if the caller didn't

commit 747d5a1bf293dcb33af755a6d285d41b8c1ea010
Author: Grzegorz Halat <ghalat@redhat.com>
Date:   Fri Jun 28 14:28:13 2019 +0200

    x86/reboot: Always use NMI fallback when shutdown via reboot vector IPI fails
    
    A reboot request sends an IPI via the reboot vector and waits for all other
    CPUs to stop. If one or more CPUs are in critical regions with interrupts
    disabled then the IPI is not handled on those CPUs and the shutdown hangs
    if native_stop_other_cpus() is called with the wait argument set.
    
    Such a situation can happen when one CPU was stopped within a lock held
    section and another CPU is trying to acquire that lock with interrupts
    disabled. There are other scenarios which can cause such a lockup as well.
    
    In theory the shutdown should be attempted by an NMI IPI after the timeout
    period elapsed. Though the wait loop after sending the reboot vector IPI
    prevents this. It checks the wait request argument and the timeout. If wait
    is set, which is true for sys_reboot() then it won't fall through to the
    NMI shutdown method after the timeout period has finished.
    
    This was an oversight when the NMI shutdown mechanism was added to handle
    the 'reboot IPI is not working' situation. The mechanism was added to deal
    with stuck panic shutdowns, which do not have the wait request set, so the
    'wait request' case was probably not considered.
    
    Remove the wait check from the post reboot vector IPI wait loop and enforce
    that the wait loop in the NMI fallback path is invoked even if NMI IPIs are
    disabled or the registration of the NMI handler fails. That second wait
    loop will then hang if not all CPUs shutdown and the wait argument is set.
    
    [ tglx: Avoid the hard to parse line break in the NMI fallback path,
            add comments and massage the changelog ]
    
    Fixes: 7d007d21e539 ("x86/reboot: Use NMI to assist in shutting down if IRQ fails")
    Signed-off-by: Grzegorz Halat <ghalat@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Don Zickus <dzickus@redhat.com>
    Link: https://lkml.kernel.org/r/20190628122813.15500-1-ghalat@redhat.com

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 96421f97e75c..231fa230ebc7 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -179,6 +179,12 @@ asmlinkage __visible void smp_reboot_interrupt(void)
 	irq_exit();
 }
 
+static int register_stop_handler(void)
+{
+	return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+				    NMI_FLAG_FIRST, "smp_stop");
+}
+
 static void native_stop_other_cpus(int wait)
 {
 	unsigned long flags;
@@ -212,39 +218,41 @@ static void native_stop_other_cpus(int wait)
 		apic->send_IPI_allbutself(REBOOT_VECTOR);
 
 		/*
-		 * Don't wait longer than a second if the caller
-		 * didn't ask us to wait.
+		 * Don't wait longer than a second for IPI completion. The
+		 * wait request is not checked here because that would
+		 * prevent an NMI shutdown attempt in case that not all
+		 * CPUs reach shutdown state.
 		 */
 		timeout = USEC_PER_SEC;
-		while (num_online_cpus() > 1 && (wait || timeout--))
+		while (num_online_cpus() > 1 && timeout--)
 			udelay(1);
 	}
-	
-	/* if the REBOOT_VECTOR didn't work, try with the NMI */
-	if ((num_online_cpus() > 1) && (!smp_no_nmi_ipi))  {
-		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
-					 NMI_FLAG_FIRST, "smp_stop"))
-			/* Note: we ignore failures here */
-			/* Hope the REBOOT_IRQ is good enough */
-			goto finish;
-
-		/* sync above data before sending IRQ */
-		wmb();
 
-		pr_emerg("Shutting down cpus with NMI\n");
+	/* if the REBOOT_VECTOR didn't work, try with the NMI */
+	if (num_online_cpus() > 1) {
+		/*
+		 * If NMI IPI is enabled, try to register the stop handler
+		 * and send the IPI. In any case try to wait for the other
+		 * CPUs to stop.
+		 */
+		if (!smp_no_nmi_ipi && !register_stop_handler()) {
+			/* Sync above data before sending IRQ */
+			wmb();
 
-		apic->send_IPI_allbutself(NMI_VECTOR);
+			pr_emerg("Shutting down cpus with NMI\n");
 
+			apic->send_IPI_allbutself(NMI_VECTOR);
+		}
 		/*
-		 * Don't wait longer than a 10 ms if the caller
-		 * didn't ask us to wait.
+		 * Don't wait longer than 10 ms if the caller didn't
+		 * reqeust it. If wait is true, the machine hangs here if
+		 * one or more CPUs do not reach shutdown state.
 		 */
 		timeout = USEC_PER_MSEC * 10;
 		while (num_online_cpus() > 1 && (wait || timeout--))
 			udelay(1);
 	}
 
-finish:
 	local_irq_save(flags);
 	disable_local_APIC();
 	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));

commit 0902d5011cfaabd6a09326299ef77e1c8735fb89
Merge: 927ba67a63c7 f8a8fe61fec8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 11:22:57 2019 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x96 apic updates from Thomas Gleixner:
     "Updates for the x86 APIC interrupt handling and APIC timer:
    
       - Fix a long standing issue with spurious interrupts which was caused
         by the big vector management rework a few years ago. Robert Hodaszi
         provided finally enough debug data and an excellent initial failure
         analysis which allowed to understand the underlying issues.
    
         This contains a change to the core interrupt management code which
         is required to handle this correctly for the APIC/IO_APIC. The core
         changes are NOOPs for most architectures except ARM64. ARM64 is not
         impacted by the change as confirmed by Marc Zyngier.
    
       - Newer systems allow to disable the PIT clock for power saving
         causing panic in the timer interrupt delivery check of the IO/APIC
         when the HPET timer is not enabled either. While the clock could be
         turned on this would cause an endless whack a mole game to chase
         the proper register in each affected chipset.
    
         These systems provide the relevant frequencies for TSC, CPU and the
         local APIC timer via CPUID and/or MSRs, which allows to avoid the
         PIT/HPET based calibration. As the calibration code is the only
         usage of the legacy timers on modern systems and is skipped anyway
         when the frequencies are known already, there is no point in
         setting up the PIT and actually checking for the interrupt delivery
         via IO/APIC.
    
         To achieve this on a wide variety of platforms, the CPUID/MSR based
         frequency readout has been made more robust, which also allowed to
         remove quite some workarounds which turned out to be not longer
         required. Thanks to Daniel Drake for analysis, patches and
         verification"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/irq: Seperate unused system vectors from spurious entry again
      x86/irq: Handle spurious interrupt after shutdown gracefully
      x86/ioapic: Implement irq_get_irqchip_state() callback
      genirq: Add optional hardware synchronization for shutdown
      genirq: Fix misleading synchronize_irq() documentation
      genirq: Delay deactivation in free_irq()
      x86/timer: Skip PIT initialization on modern chipsets
      x86/apic: Use non-atomic operations when possible
      x86/apic: Make apic_bsp_setup() static
      x86/tsc: Set LAPIC timer period to crystal clock frequency
      x86/apic: Rename 'lapic_timer_frequency' to 'lapic_timer_period'
      x86/tsc: Use CPUID.0x16 to calculate missing crystal frequency

commit dde3626f815e38bbf96fddd5185038c4b4d395a8
Author: Nadav Amit <namit@vmware.com>
Date:   Wed Jun 12 23:48:13 2019 -0700

    x86/apic: Use non-atomic operations when possible
    
    Using __clear_bit() and __cpumask_clear_cpu() is more efficient than using
    their atomic counterparts.
    
    Use them when atomicity is not needed, such as when manipulating bitmasks
    that are on the stack.
    
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Link: https://lkml.kernel.org/r/20190613064813.8102-10-namit@vmware.com

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 04adc8d60aed..acddd988602d 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -146,7 +146,7 @@ void native_send_call_func_ipi(const struct cpumask *mask)
 	}
 
 	cpumask_copy(allbutself, cpu_online_mask);
-	cpumask_clear_cpu(smp_processor_id(), allbutself);
+	__cpumask_clear_cpu(smp_processor_id(), allbutself);
 
 	if (cpumask_equal(mask, allbutself) &&
 	    cpumask_equal(cpu_online_mask, cpu_callout_mask))

commit 9ff554e9be1f78d47cc77ad563e57e6c0612dbb3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 22 09:51:28 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 82
    
    Based on 1 normalized pattern(s):
    
      this code is released under the gnu general public license version 2
      or later
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190520075211.232210963@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 04adc8d60aed..4693e2f3a03e 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  *	Intel SMP support routines.
  *
@@ -6,9 +7,6 @@
  *      (c) 2002,2003 Andi Kleen, SuSE Labs.
  *
  *	i386 and x86_64 integration by Glauber Costa <gcosta@redhat.com>
- *
- *	This code is released under the GNU General Public License version 2 or
- *	later.
  */
 
 #include <linux/init.h>

commit ffcba43ff66c7dab34ec700debd491d2a4d319b4
Author: Nicolai Stange <nstange@suse.de>
Date:   Sun Jul 29 13:06:04 2018 +0200

    x86/irq: Let interrupt handlers set kvm_cpu_l1tf_flush_l1d
    
    The last missing piece to having vmx_l1d_flush() take interrupts after
    VMEXIT into account is to set the kvm_cpu_l1tf_flush_l1d per-cpu flag on
    irq entry.
    
    Issue calls to kvm_set_cpu_l1tf_flush_l1d() from entering_irq(),
    ipi_entering_ack_irq(), smp_reschedule_interrupt() and
    uv_bau_message_interrupt().
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Nicolai Stange <nstange@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 5c574dff4c1a..04adc8d60aed 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -261,6 +261,7 @@ __visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
+	kvm_set_cpu_l1tf_flush_l1d();
 
 	if (trace_resched_ipi_enabled()) {
 		/*

commit 809547472edae0bc68f2b5abc37b92c8a988bc8a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:33 2017 +0200

    x86/tracing: Disentangle pagefault and resched IPI tracing key
    
    The pagefault and the resched IPI handler are the only ones where it is
    worth to optimize the code further in case tracepoints are disabled. But it
    makes no sense to have a single static key for both.
    
    Seperate the static keys so the facilities are handled seperately.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064957.536699116@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index cfe865b85bc5..5c574dff4c1a 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -262,7 +262,7 @@ __visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
 	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
 
-	if (trace_irqvectors_enabled()) {
+	if (trace_resched_ipi_enabled()) {
 		/*
 		 * scheduler_ipi() might call irq_enter() as well, but
 		 * nested calls are fine.

commit 3cd788c1eec4b3659671aa13d335a15102ac4d06
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:30 2017 +0200

    x86/smp: Use static key for reschedule interrupt tracing
    
    It's worth to avoid the extra irq_enter()/irq_exit() pair in the case that
    the reschedule interrupt tracepoints are disabled.
    
    Use the static key which indicates that exception tracing is enabled. For
    now this key is global. It will be optimized in a later step.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170828064957.299808677@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index fb49e10cc30a..cfe865b85bc5 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -254,37 +254,27 @@ static void native_stop_other_cpus(int wait)
 }
 
 /*
- * Reschedule call back.
+ * Reschedule call back. KVM uses this interrupt to force a cpu out of
+ * guest mode
  */
-static inline void __smp_reschedule_interrupt(void)
-{
-	inc_irq_stat(irq_resched_count);
-	scheduler_ipi();
-}
-
 __visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
-	__smp_reschedule_interrupt();
-	/*
-	 * KVM uses this interrupt to force a cpu out of guest mode
-	 */
-}
-
-__visible void __irq_entry smp_trace_reschedule_interrupt(struct pt_regs *regs)
-{
-	/*
-	 * Need to call irq_enter() before calling the trace point.
-	 * __smp_reschedule_interrupt() calls irq_enter/exit() too (in
-	 * scheduler_ipi(). This is OK, since those functions are allowed
-	 * to nest.
-	 */
-	ipi_entering_ack_irq();
-	trace_reschedule_entry(RESCHEDULE_VECTOR);
 	inc_irq_stat(irq_resched_count);
+
+	if (trace_irqvectors_enabled()) {
+		/*
+		 * scheduler_ipi() might call irq_enter() as well, but
+		 * nested calls are fine.
+		 */
+		irq_enter();
+		trace_reschedule_entry(RESCHEDULE_VECTOR);
+		scheduler_ipi();
+		trace_reschedule_exit(RESCHEDULE_VECTOR);
+		irq_exit();
+		return;
+	}
 	scheduler_ipi();
-	trace_reschedule_exit(RESCHEDULE_VECTOR);
-	exiting_irq();
 }
 
 __visible void __irq_entry smp_call_function_interrupt(struct pt_regs *regs)

commit 85b77cdd8fbd163d65f340e3f6578c50031af960
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Aug 28 08:47:29 2017 +0200

    x86/smp: Remove pointless duplicated interrupt code
    
    Two NOP5s are really a good tradeoff vs. the unholy IDT switching mess,
    which duplicates code all over the place. The rescheduling interrupt gets
    optimized in a later step.
    
    Make the ordering of function call and statistics increment the same as in
    other places. Calculate stats first, then do the function call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170828064957.222101344@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index d798c0da451c..fb49e10cc30a 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -281,57 +281,28 @@ __visible void __irq_entry smp_trace_reschedule_interrupt(struct pt_regs *regs)
 	 */
 	ipi_entering_ack_irq();
 	trace_reschedule_entry(RESCHEDULE_VECTOR);
-	__smp_reschedule_interrupt();
+	inc_irq_stat(irq_resched_count);
+	scheduler_ipi();
 	trace_reschedule_exit(RESCHEDULE_VECTOR);
 	exiting_irq();
-	/*
-	 * KVM uses this interrupt to force a cpu out of guest mode
-	 */
-}
-
-static inline void __smp_call_function_interrupt(void)
-{
-	generic_smp_call_function_interrupt();
-	inc_irq_stat(irq_call_count);
 }
 
 __visible void __irq_entry smp_call_function_interrupt(struct pt_regs *regs)
-{
-	ipi_entering_ack_irq();
-	__smp_call_function_interrupt();
-	exiting_irq();
-}
-
-__visible void __irq_entry
-smp_trace_call_function_interrupt(struct pt_regs *regs)
 {
 	ipi_entering_ack_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
-	__smp_call_function_interrupt();
-	trace_call_function_exit(CALL_FUNCTION_VECTOR);
-	exiting_irq();
-}
-
-static inline void __smp_call_function_single_interrupt(void)
-{
-	generic_smp_call_function_single_interrupt();
 	inc_irq_stat(irq_call_count);
-}
-
-__visible void __irq_entry
-smp_call_function_single_interrupt(struct pt_regs *regs)
-{
-	ipi_entering_ack_irq();
-	__smp_call_function_single_interrupt();
+	generic_smp_call_function_interrupt();
+	trace_call_function_exit(CALL_FUNCTION_VECTOR);
 	exiting_irq();
 }
 
-__visible void __irq_entry
-smp_trace_call_function_single_interrupt(struct pt_regs *regs)
+__visible void __irq_entry smp_call_function_single_interrupt(struct pt_regs *r)
 {
 	ipi_entering_ack_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);
-	__smp_call_function_single_interrupt();
+	inc_irq_stat(irq_call_count);
+	generic_smp_call_function_single_interrupt();
 	trace_call_function_single_exit(CALL_FUNCTION_SINGLE_VECTOR);
 	exiting_irq();
 }

commit 16b76293c5c81e6345323d7aef41b26e8390f62d
Merge: 3dee9fb2a4ce da63b6b20077
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 20:51:12 2017 -0700

    Merge branch 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 boot updates from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - reworking of the e820 code: separate in-kernel and boot-ABI data
         structures and apply a whole range of cleanups to the kernel side.
    
         No change in functionality.
    
       - enable KASLR by default: it's used by all major distros and it's
         out of the experimental stage as well.
    
       - ... misc fixes and cleanups"
    
    * 'x86-boot-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (63 commits)
      x86/KASLR: Fix kexec kernel boot crash when KASLR randomization fails
      x86/reboot: Turn off KVM when halting a CPU
      x86/boot: Fix BSS corruption/overwrite bug in early x86 kernel startup
      x86: Enable KASLR by default
      boot/param: Move next_arg() function to lib/cmdline.c for later reuse
      x86/boot: Fix Sparse warning by including required header file
      x86/boot/64: Rename start_cpu()
      x86/xen: Update e820 table handling to the new core x86 E820 code
      x86/boot: Fix pr_debug() API braindamage
      xen, x86/headers: Add <linux/device.h> dependency to <asm/xen/page.h>
      x86/boot/e820: Simplify e820__update_table()
      x86/boot/e820: Separate the E820 ABI structures from the in-kernel structures
      x86/boot/e820: Fix and clean up e820_type switch() statements
      x86/boot/e820: Rename the remaining E820 APIs to the e820__*() prefix
      x86/boot/e820: Remove unnecessary #include's
      x86/boot/e820: Rename e820_mark_nosave_regions() to e820__register_nosave_regions()
      x86/boot/e820: Rename e820_reserve_resources*() to e820__reserve_resources*()
      x86/boot/e820: Use bool in query APIs
      x86/boot/e820: Document e820__reserve_setup_data()
      x86/boot/e820: Clean up __e820__update_table() et al
      ...

commit 21173d0b4d2a0b9e9e5f3155cf2cfc5781a6f4b1
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Tue Apr 18 08:25:05 2017 -0400

    sched/x86: Update reschedule warning text
    
    Modify the reschedule warning to output the offline CPU number and
    use a better debug message.
    
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1492518305-3808-1-git-send-email-prarit@redhat.com
    [ Tweaked the warning message. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index d3c66a15bbde..3cab8415389a 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -124,7 +124,7 @@ static bool smp_no_nmi_ipi = false;
 static void native_smp_send_reschedule(int cpu)
 {
 	if (unlikely(cpu_is_offline(cpu))) {
-		WARN_ON(1);
+		WARN(1, "sched: Unexpected reschedule of offline CPU#%d!\n", cpu);
 		return;
 	}
 	apic->send_IPI(cpu, RESCHEDULE_VECTOR);

commit fba4f472b33aa81ca1836f57d005455261e9126f
Author: Tiantian Feng <fengtiantian@huawei.com>
Date:   Wed Apr 19 18:18:39 2017 +0200

    x86/reboot: Turn off KVM when halting a CPU
    
    A CPU in VMX root mode will ignore INIT signals and will fail to bring
    up the APs after reboot.  Therefore, on a panic we disable VMX on all
    CPUs before rebooting or triggering kdump.
    
    Do this when halting the machine as well, in case a firmware-level reboot
    does not perform a cold reset for all processors.  Without doing this,
    rebooting the host may hang.
    
    Signed-off-by: Tiantian Feng <fengtiantian@huawei.com>
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    [ Rewritten commit message. ]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kvm@vger.kernel.org
    Link: http://lkml.kernel.org/r/20170419161839.30550-1-pbonzini@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index d3c66a15bbde..9d7223cad389 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -33,6 +33,7 @@
 #include <asm/mce.h>
 #include <asm/trace/irq_vectors.h>
 #include <asm/kexec.h>
+#include <asm/virtext.h>
 
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
@@ -162,6 +163,7 @@ static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
 	if (raw_smp_processor_id() == atomic_read(&stopping_cpu))
 		return NMI_HANDLED;
 
+	cpu_emergency_vmxoff();
 	stop_this_cpu(NULL);
 
 	return NMI_HANDLED;
@@ -174,6 +176,7 @@ static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
 asmlinkage __visible void smp_reboot_interrupt(void)
 {
 	ipi_entering_ack_irq();
+	cpu_emergency_vmxoff();
 	stop_this_cpu(NULL);
 	irq_exit();
 }

commit c4158ff536439619fa342810cc575ae2c809f03f
Author: Daniel Bristot de Oliveira <bristot@redhat.com>
Date:   Wed Jan 4 12:20:33 2017 +0100

    x86/irq, trace: Add __irq_entry annotation to x86's platform IRQ handlers
    
    This patch adds the __irq_entry annotation to the default x86
    platform IRQ handlers. ftrace's function_graph tracer uses the
    __irq_entry annotation to notify the entry and return of IRQ
    handlers.
    
    For example, before the patch:
      354549.667252 |   3)  d..1              |  default_idle_call() {
      354549.667252 |   3)  d..1              |    arch_cpu_idle() {
      354549.667253 |   3)  d..1              |      default_idle() {
      354549.696886 |   3)  d..1              |        smp_trace_reschedule_interrupt() {
      354549.696886 |   3)  d..1              |          irq_enter() {
      354549.696886 |   3)  d..1              |            rcu_irq_enter() {
    
    After the patch:
      366416.254476 |   3)  d..1              |    arch_cpu_idle() {
      366416.254476 |   3)  d..1              |      default_idle() {
      366416.261566 |   3)  d..1  ==========> |
      366416.261566 |   3)  d..1              |        smp_trace_reschedule_interrupt() {
      366416.261566 |   3)  d..1              |          irq_enter() {
      366416.261566 |   3)  d..1              |            rcu_irq_enter() {
    
    KASAN also uses this annotation. The smp_apic_timer_interrupt()
    was already annotated.
    
    Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Claudio Fontana <claudio.fontana@huawei.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Cc: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicolai Stange <nicstange@gmail.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Cc: linux-edac@vger.kernel.org
    Link: http://lkml.kernel.org/r/059fdf437c2f0c09b13c18c8fe4e69999d3ffe69.1483528431.git.bristot@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 68f8cc222f25..d3c66a15bbde 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -259,7 +259,7 @@ static inline void __smp_reschedule_interrupt(void)
 	scheduler_ipi();
 }
 
-__visible void smp_reschedule_interrupt(struct pt_regs *regs)
+__visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	__smp_reschedule_interrupt();
@@ -268,7 +268,7 @@ __visible void smp_reschedule_interrupt(struct pt_regs *regs)
 	 */
 }
 
-__visible void smp_trace_reschedule_interrupt(struct pt_regs *regs)
+__visible void __irq_entry smp_trace_reschedule_interrupt(struct pt_regs *regs)
 {
 	/*
 	 * Need to call irq_enter() before calling the trace point.
@@ -292,14 +292,15 @@ static inline void __smp_call_function_interrupt(void)
 	inc_irq_stat(irq_call_count);
 }
 
-__visible void smp_call_function_interrupt(struct pt_regs *regs)
+__visible void __irq_entry smp_call_function_interrupt(struct pt_regs *regs)
 {
 	ipi_entering_ack_irq();
 	__smp_call_function_interrupt();
 	exiting_irq();
 }
 
-__visible void smp_trace_call_function_interrupt(struct pt_regs *regs)
+__visible void __irq_entry
+smp_trace_call_function_interrupt(struct pt_regs *regs)
 {
 	ipi_entering_ack_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
@@ -314,14 +315,16 @@ static inline void __smp_call_function_single_interrupt(void)
 	inc_irq_stat(irq_call_count);
 }
 
-__visible void smp_call_function_single_interrupt(struct pt_regs *regs)
+__visible void __irq_entry
+smp_call_function_single_interrupt(struct pt_regs *regs)
 {
 	ipi_entering_ack_irq();
 	__smp_call_function_single_interrupt();
 	exiting_irq();
 }
 
-__visible void smp_trace_call_function_single_interrupt(struct pt_regs *regs)
+__visible void __irq_entry
+smp_trace_call_function_single_interrupt(struct pt_regs *regs)
 {
 	ipi_entering_ack_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);

commit 8ca225520e278e41396dab0524989f4848626f83
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Nov 7 11:13:40 2016 +0800

    x86/apic: Prevent tracing on apic_msr_write_eoi()
    
    The following RCU lockdep warning led to adding irq_enter()/irq_exit() into
    smp_reschedule_interrupt():
    
     RCU used illegally from idle CPU!
     rcu_scheduler_active = 1, debug_locks = 0
     RCU used illegally from extended quiescent state!
     no locks held by swapper/1/0.
    
      do_trace_write_msr
      native_write_msr
      native_apic_msr_eoi_write
      smp_reschedule_interrupt
      reschedule_interrupt
    
    As Peterz pointed out:
    
    | So now we're making a very frequent interrupt slower because of debug
    | code.
    |
    | The thing is, many many smp_reschedule_interrupt() invocations don't
    | actually execute anything much at all and are only sent to tickle the
    | return to user path (which does the actual preemption).
    |
    | Having to do the whole irq_enter/irq_exit dance just for this unlikely
    | debug case totally blows.
    
    Use the wrmsr_notrace() variant in native_apic_msr_write_eoi, annotate the
    kvm variant with notrace and add a native_apic_eoi callback to the apic
    structure so KVM guests are covered as well.
    
    This allows to revert the irq_enter/irq_exit dance in
    smp_reschedule_interrupt().
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/1478488420-5982-3-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index c00cb64bc0a1..68f8cc222f25 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -261,10 +261,8 @@ static inline void __smp_reschedule_interrupt(void)
 
 __visible void smp_reschedule_interrupt(struct pt_regs *regs)
 {
-	irq_enter();
 	ack_APIC_irq();
 	__smp_reschedule_interrupt();
-	irq_exit();
 	/*
 	 * KVM uses this interrupt to force a cpu out of guest mode
 	 */

commit 1d33369db25eb7f37b7a8bd22d736888b4501a9c
Merge: 23446cb66c07 1001354ca341
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Oct 16 11:31:39 2016 +0200

    Merge tag 'v4.9-rc1' into x86/urgent, to pick up updates
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1ec6ec14a2943f6f611fc1d5fb2d4eaa85bd9d72
Author: Wanpeng Li <kernellwp@gmail.com>
Date:   Fri Oct 14 09:48:53 2016 +0800

    x86/smp: Add irq_enter/exit() in smp_reschedule_interrupt()
    
     ===============================
     [ INFO: suspicious RCU usage. ]
     4.8.0+ #24 Not tainted
     -------------------------------
     ./arch/x86/include/asm/msr-trace.h:47 suspicious rcu_dereference_check() usage!
    
     other info that might help us debug this:
    
     RCU used illegally from idle CPU!
     rcu_scheduler_active = 1, debug_locks = 0
     RCU used illegally from extended quiescent state!
     no locks held by swapper/1/0.
    
      [<ffffffff9d492b95>] do_trace_write_msr+0x135/0x140
      [<ffffffff9d06f860>] native_write_msr+0x20/0x30
      [<ffffffff9d065fad>] native_apic_msr_eoi_write+0x1d/0x30
      [<ffffffff9d05bd1d>] smp_reschedule_interrupt+0x1d/0x30
      [<ffffffff9d8daec6>] reschedule_interrupt+0x96/0xa0
    
    Reschedule interrupt may be called in cpu idle state. This causes lockdep
    check warning above.
    
    Add irq_enter/exit() in smp_reschedule_interrupt(), irq_enter() tells the RCU
    subsystems to end the extended quiescent state, so the following trace call in
    ack_APIC_irq() works correctly.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Link: http://lkml.kernel.org/r/1476409733-5133-1-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 658777cf3851..ac2ee87deb55 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -259,8 +259,10 @@ static inline void __smp_reschedule_interrupt(void)
 
 __visible void smp_reschedule_interrupt(struct pt_regs *regs)
 {
+	irq_enter();
 	ack_APIC_irq();
 	__smp_reschedule_interrupt();
+	irq_exit();
 	/*
 	 * KVM uses this interrupt to force a cpu out of guest mode
 	 */

commit 0ee59413c967c35a6dd2dbdab605b4cd42025ee5
Author: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
Date:   Tue Oct 11 13:54:23 2016 -0700

    x86/panic: replace smp_send_stop() with kdump friendly version in panic path
    
    Daniel Walker reported problems which happens when
    crash_kexec_post_notifiers kernel option is enabled
    (https://lkml.org/lkml/2015/6/24/44).
    
    In that case, smp_send_stop() is called before entering kdump routines
    which assume other CPUs are still online.  As the result, for x86, kdump
    routines fail to save other CPUs' registers and disable virtualization
    extensions.
    
    To fix this problem, call a new kdump friendly function,
    crash_smp_send_stop(), instead of the smp_send_stop() when
    crash_kexec_post_notifiers is enabled.  crash_smp_send_stop() is a weak
    function, and it just call smp_send_stop().  Architecture codes should
    override it so that kdump can work appropriately.  This patch only
    provides x86-specific version.
    
    For Xen's PV kernel, just keep the current behavior.
    
    NOTES:
    
    - Right solution would be to place crash_smp_send_stop() before
      __crash_kexec() invocation in all cases and remove smp_send_stop(), but
      we can't do that until all architectures implement own
      crash_smp_send_stop()
    
    - crash_smp_send_stop()-like work is still needed by
      machine_crash_shutdown() because crash_kexec() can be called without
      entering panic()
    
    Fixes: f06e5153f4ae (kernel/panic.c: add "crash_kexec_post_notifiers" option)
    Link: http://lkml.kernel.org/r/20160810080948.11028.15344.stgit@sysi4-13.yrl.intra.hitachi.co.jp
    Signed-off-by: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Reported-by: Daniel Walker <dwalker@fifo99.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Daniel Walker <dwalker@fifo99.com>
    Cc: Xunlei Pang <xpang@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Aaro Koskinen <aaro.koskinen@iki.fi>
    Cc: "Steven J. Hill" <steven.hill@cavium.com>
    Cc: Corey Minyard <cminyard@mvista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 658777cf3851..68f8cc222f25 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -32,6 +32,8 @@
 #include <asm/nmi.h>
 #include <asm/mce.h>
 #include <asm/trace/irq_vectors.h>
+#include <asm/kexec.h>
+
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *
@@ -342,6 +344,9 @@ struct smp_ops smp_ops = {
 	.smp_cpus_done		= native_smp_cpus_done,
 
 	.stop_other_cpus	= native_stop_other_cpus,
+#if defined(CONFIG_KEXEC_CORE)
+	.crash_stop_other_cpus	= kdump_nmi_shootdown_cpus,
+#endif
 	.smp_send_reschedule	= native_smp_send_reschedule,
 
 	.cpu_up			= native_cpu_up,

commit 72613184a1f076659e8a902d64351f50d3f9c990
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 4 22:57:09 2015 +0000

    x86/smp: Remove single IPI wrapper
    
    All APIC implementation have send_IPI now. Remove the conditional in
    the calling code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Travis <travis@sgi.com>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/20151104220849.807817097@linutronix.de

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 1dbf590cdd89..658777cf3851 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -114,18 +114,6 @@
 static atomic_t stopping_cpu = ATOMIC_INIT(-1);
 static bool smp_no_nmi_ipi = false;
 
-/*
- * Helper wrapper: not all apic definitions support sending to
- * a single CPU, so we fall back to sending to a mask.
- */
-static void send_IPI_cpu(int cpu, int vector)
-{
-	if (apic->send_IPI)
-		apic->send_IPI(cpu, vector);
-	else
-		apic->send_IPI_mask(cpumask_of(cpu), vector);
-}
-
 /*
  * this function sends a 'reschedule' IPI to another CPU.
  * it goes straight through and wastes no time serializing
@@ -137,12 +125,12 @@ static void native_smp_send_reschedule(int cpu)
 		WARN_ON(1);
 		return;
 	}
-	send_IPI_cpu(cpu, RESCHEDULE_VECTOR);
+	apic->send_IPI(cpu, RESCHEDULE_VECTOR);
 }
 
 void native_send_call_func_single_ipi(int cpu)
 {
-	send_IPI_cpu(cpu, CALL_FUNCTION_SINGLE_VECTOR);
+	apic->send_IPI(cpu, CALL_FUNCTION_SINGLE_VECTOR);
 }
 
 void native_send_call_func_ipi(const struct cpumask *mask)

commit 539da7877275edb21a76aa02fb2c147eff02c559
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 22:57:00 2015 +0000

    x86/apic: Add a single-target IPI function to the apic
    
    We still fall back on the "send mask" versions if an apic definition
    doesn't have the single-target version, but at least this allows the
    (trivial) case for the common clustered x2apic case.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Borislav Petkov <bp@alien.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Travis <travis@sgi.com>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/20151104220848.737120838@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 12c8286206ce..1dbf590cdd89 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -114,6 +114,18 @@
 static atomic_t stopping_cpu = ATOMIC_INIT(-1);
 static bool smp_no_nmi_ipi = false;
 
+/*
+ * Helper wrapper: not all apic definitions support sending to
+ * a single CPU, so we fall back to sending to a mask.
+ */
+static void send_IPI_cpu(int cpu, int vector)
+{
+	if (apic->send_IPI)
+		apic->send_IPI(cpu, vector);
+	else
+		apic->send_IPI_mask(cpumask_of(cpu), vector);
+}
+
 /*
  * this function sends a 'reschedule' IPI to another CPU.
  * it goes straight through and wastes no time serializing
@@ -125,12 +137,12 @@ static void native_smp_send_reschedule(int cpu)
 		WARN_ON(1);
 		return;
 	}
-	apic->send_IPI_mask(cpumask_of(cpu), RESCHEDULE_VECTOR);
+	send_IPI_cpu(cpu, RESCHEDULE_VECTOR);
 }
 
 void native_send_call_func_single_ipi(int cpu)
 {
-	apic->send_IPI_mask(cpumask_of(cpu), CALL_FUNCTION_SINGLE_VECTOR);
+	send_IPI_cpu(cpu, CALL_FUNCTION_SINGLE_VECTOR);
 }
 
 void native_send_call_func_ipi(const struct cpumask *mask)

commit 8838eb6c0bf3b6a6494a163947ab3d1700ab45d2
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Wed Aug 12 18:29:40 2015 +0200

    x86/mce: Clear Local MCE opt-in before kexec
    
    kexec could boot a kernel that could be legacy with no knowledge
    of LMCE. Hence we should make sure we clear LMCE optin before
    kexec reboot.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Aravind Gopalakrishnan <Aravind.Gopalakrishnan@amd.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-edac <linux-edac@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1439396985-12812-9-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 15aaa69bbb5e..12c8286206ce 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -30,6 +30,7 @@
 #include <asm/proto.h>
 #include <asm/apic.h>
 #include <asm/nmi.h>
+#include <asm/mce.h>
 #include <asm/trace/irq_vectors.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
@@ -243,6 +244,7 @@ static void native_stop_other_cpus(int wait)
 finish:
 	local_irq_save(flags);
 	disable_local_APIC();
+	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));
 	local_irq_restore(flags);
 }
 

commit 6dc178760553605c58d78bd403dfcb4e042c5b72
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 15 15:50:45 2015 +0200

    x86: Consolidate irq entering inlines
    
    smp.c and irq_work.c implement the same inline helper. Move it to
    apic.h and use it everywhere.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index be8e1bde07aa..15aaa69bbb5e 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -170,8 +170,7 @@ static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
 
 asmlinkage __visible void smp_reboot_interrupt(void)
 {
-	ack_APIC_irq();
-	irq_enter();
+	ipi_entering_ack_irq();
 	stop_this_cpu(NULL);
 	irq_exit();
 }
@@ -265,12 +264,6 @@ __visible void smp_reschedule_interrupt(struct pt_regs *regs)
 	 */
 }
 
-static inline void smp_entering_irq(void)
-{
-	ack_APIC_irq();
-	irq_enter();
-}
-
 __visible void smp_trace_reschedule_interrupt(struct pt_regs *regs)
 {
 	/*
@@ -279,7 +272,7 @@ __visible void smp_trace_reschedule_interrupt(struct pt_regs *regs)
 	 * scheduler_ipi(). This is OK, since those functions are allowed
 	 * to nest.
 	 */
-	smp_entering_irq();
+	ipi_entering_ack_irq();
 	trace_reschedule_entry(RESCHEDULE_VECTOR);
 	__smp_reschedule_interrupt();
 	trace_reschedule_exit(RESCHEDULE_VECTOR);
@@ -297,14 +290,14 @@ static inline void __smp_call_function_interrupt(void)
 
 __visible void smp_call_function_interrupt(struct pt_regs *regs)
 {
-	smp_entering_irq();
+	ipi_entering_ack_irq();
 	__smp_call_function_interrupt();
 	exiting_irq();
 }
 
 __visible void smp_trace_call_function_interrupt(struct pt_regs *regs)
 {
-	smp_entering_irq();
+	ipi_entering_ack_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
 	__smp_call_function_interrupt();
 	trace_call_function_exit(CALL_FUNCTION_VECTOR);
@@ -319,14 +312,14 @@ static inline void __smp_call_function_single_interrupt(void)
 
 __visible void smp_call_function_single_interrupt(struct pt_regs *regs)
 {
-	smp_entering_irq();
+	ipi_entering_ack_irq();
 	__smp_call_function_single_interrupt();
 	exiting_irq();
 }
 
 __visible void smp_trace_call_function_single_interrupt(struct pt_regs *regs)
 {
-	smp_entering_irq();
+	ipi_entering_ack_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);
 	__smp_call_function_single_interrupt();
 	trace_call_function_single_exit(CALL_FUNCTION_SINGLE_VECTOR);

commit 2605fc216fa492f9e7c488bdc7f687cd6dcc703b
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri May 2 00:44:37 2014 +0200

    asmlinkage, x86: Add explicit __visible to arch/x86/*
    
    As requested by Linus add explicit __visible to the asmlinkage users.
    This marks all functions visible to assembler.
    
    Tree sweep for arch/x86/*
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398984278-29319-3-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 7c3a5a61f2e4..be8e1bde07aa 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -168,7 +168,7 @@ static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
  * this function calls the 'stop' function on all other CPUs in the system.
  */
 
-asmlinkage void smp_reboot_interrupt(void)
+asmlinkage __visible void smp_reboot_interrupt(void)
 {
 	ack_APIC_irq();
 	irq_enter();

commit 1d9090e2fb32c84277cef6e72a21be7f78c929f4
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 5 15:02:37 2013 -0700

    x86, asmlinkage: Make all interrupt handlers asmlinkage / __visible
    
    These handlers are all referenced from assembler stubs, so need
    to be visible.
    
    The handlers without arguments become asmlinkage, the others __visible
    to not force regparms(0) on x86-32.
    
    I put it all into a single patch, please let me know if you want
    it it split up.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1375740170-7446-4-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index cdaa347dfcad..7c3a5a61f2e4 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -256,7 +256,7 @@ static inline void __smp_reschedule_interrupt(void)
 	scheduler_ipi();
 }
 
-void smp_reschedule_interrupt(struct pt_regs *regs)
+__visible void smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	__smp_reschedule_interrupt();
@@ -271,7 +271,7 @@ static inline void smp_entering_irq(void)
 	irq_enter();
 }
 
-void smp_trace_reschedule_interrupt(struct pt_regs *regs)
+__visible void smp_trace_reschedule_interrupt(struct pt_regs *regs)
 {
 	/*
 	 * Need to call irq_enter() before calling the trace point.
@@ -295,14 +295,14 @@ static inline void __smp_call_function_interrupt(void)
 	inc_irq_stat(irq_call_count);
 }
 
-void smp_call_function_interrupt(struct pt_regs *regs)
+__visible void smp_call_function_interrupt(struct pt_regs *regs)
 {
 	smp_entering_irq();
 	__smp_call_function_interrupt();
 	exiting_irq();
 }
 
-void smp_trace_call_function_interrupt(struct pt_regs *regs)
+__visible void smp_trace_call_function_interrupt(struct pt_regs *regs)
 {
 	smp_entering_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
@@ -317,14 +317,14 @@ static inline void __smp_call_function_single_interrupt(void)
 	inc_irq_stat(irq_call_count);
 }
 
-void smp_call_function_single_interrupt(struct pt_regs *regs)
+__visible void smp_call_function_single_interrupt(struct pt_regs *regs)
 {
 	smp_entering_irq();
 	__smp_call_function_single_interrupt();
 	exiting_irq();
 }
 
-void smp_trace_call_function_single_interrupt(struct pt_regs *regs)
+__visible void smp_trace_call_function_single_interrupt(struct pt_regs *regs)
 {
 	smp_entering_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);

commit 4787c368a9bca39e173d702389ee2eaf0520abc1
Author: Seiji Aguchi <seiji.aguchi@hds.com>
Date:   Fri Jun 28 14:02:11 2013 -0400

    x86/tracing: Add irq_enter/exit() in smp_trace_reschedule_interrupt()
    
    Reschedule vector tracepoints may be called in cpu idle state.
    This causes lockdep check warning below.
    
    The tracepoint requires rcu but for accuracy it also
    requires irq_enter() (tracepoints record the irq context), thus,
    the tracepoint interrupt handler should be calling irq_enter()
    and not rcu_irq_enter() (irq_enter() calls rcu_irq_enter()).
    
    So, add irq_enter/exit() to smp_trace_reschedule_interrupt()
    with common pre/post processing functions, smp_entering_irq()
    and exiting_irq() (exiting_irq() calls just irq_exit()
     in arch/x86/include/asm/apic.h),
    because these can be shared among reschedule, call_function,
    and call_function_single vectors.
    
    [   50.720557] Testing event reschedule_exit:
    [   50.721349]
    [   50.721502] ===============================
    [   50.721835] [ INFO: suspicious RCU usage. ]
    [   50.722169] 3.10.0-rc6-00004-gcf910e8 #190 Not tainted
    [   50.722582] -------------------------------
    [   50.722915] /c/kernel-tests/src/linux/arch/x86/include/asm/trace/irq_vectors.h:50 suspicious rcu_dereference_check() usage!
    [   50.723770]
    [   50.723770] other info that might help us debug this:
    [   50.723770]
    [   50.724385]
    [   50.724385] RCU used illegally from idle CPU!
    [   50.724385] rcu_scheduler_active = 1, debug_locks = 0
    [   50.725232] RCU used illegally from extended quiescent state!
    [   50.725690] no locks held by swapper/0/0.
    [   50.726010]
    [   50.726010] stack backtrace:
    [...]
    
    Signed-off-by: Seiji Aguchi <seiji.aguchi@hds.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/51CDCFA3.9080101@hds.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index f4fe0b8879e0..cdaa347dfcad 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -265,23 +265,30 @@ void smp_reschedule_interrupt(struct pt_regs *regs)
 	 */
 }
 
-void smp_trace_reschedule_interrupt(struct pt_regs *regs)
+static inline void smp_entering_irq(void)
 {
 	ack_APIC_irq();
+	irq_enter();
+}
+
+void smp_trace_reschedule_interrupt(struct pt_regs *regs)
+{
+	/*
+	 * Need to call irq_enter() before calling the trace point.
+	 * __smp_reschedule_interrupt() calls irq_enter/exit() too (in
+	 * scheduler_ipi(). This is OK, since those functions are allowed
+	 * to nest.
+	 */
+	smp_entering_irq();
 	trace_reschedule_entry(RESCHEDULE_VECTOR);
 	__smp_reschedule_interrupt();
 	trace_reschedule_exit(RESCHEDULE_VECTOR);
+	exiting_irq();
 	/*
 	 * KVM uses this interrupt to force a cpu out of guest mode
 	 */
 }
 
-static inline void call_function_entering_irq(void)
-{
-	ack_APIC_irq();
-	irq_enter();
-}
-
 static inline void __smp_call_function_interrupt(void)
 {
 	generic_smp_call_function_interrupt();
@@ -290,14 +297,14 @@ static inline void __smp_call_function_interrupt(void)
 
 void smp_call_function_interrupt(struct pt_regs *regs)
 {
-	call_function_entering_irq();
+	smp_entering_irq();
 	__smp_call_function_interrupt();
 	exiting_irq();
 }
 
 void smp_trace_call_function_interrupt(struct pt_regs *regs)
 {
-	call_function_entering_irq();
+	smp_entering_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
 	__smp_call_function_interrupt();
 	trace_call_function_exit(CALL_FUNCTION_VECTOR);
@@ -312,14 +319,14 @@ static inline void __smp_call_function_single_interrupt(void)
 
 void smp_call_function_single_interrupt(struct pt_regs *regs)
 {
-	call_function_entering_irq();
+	smp_entering_irq();
 	__smp_call_function_single_interrupt();
 	exiting_irq();
 }
 
 void smp_trace_call_function_single_interrupt(struct pt_regs *regs)
 {
-	call_function_entering_irq();
+	smp_entering_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);
 	__smp_call_function_single_interrupt();
 	trace_call_function_single_exit(CALL_FUNCTION_SINGLE_VECTOR);

commit cf910e83ae23692fdeefc7e506e504c4c468d38a
Author: Seiji Aguchi <seiji.aguchi@hds.com>
Date:   Thu Jun 20 11:46:53 2013 -0400

    x86, trace: Add irq vector tracepoints
    
    [Purpose of this patch]
    
    As Vaibhav explained in the thread below, tracepoints for irq vectors
    are useful.
    
    http://www.spinics.net/lists/mm-commits/msg85707.html
    
    <snip>
    The current interrupt traces from irq_handler_entry and irq_handler_exit
    provide when an interrupt is handled.  They provide good data about when
    the system has switched to kernel space and how it affects the currently
    running processes.
    
    There are some IRQ vectors which trigger the system into kernel space,
    which are not handled in generic IRQ handlers.  Tracing such events gives
    us the information about IRQ interaction with other system events.
    
    The trace also tells where the system is spending its time.  We want to
    know which cores are handling interrupts and how they are affecting other
    processes in the system.  Also, the trace provides information about when
    the cores are idle and which interrupts are changing that state.
    <snip>
    
    On the other hand, my usecase is tracing just local timer event and
    getting a value of instruction pointer.
    
    I suggested to add an argument local timer event to get instruction pointer before.
    But there is another way to get it with external module like systemtap.
    So, I don't need to add any argument to irq vector tracepoints now.
    
    [Patch Description]
    
    Vaibhav's patch shared a trace point ,irq_vector_entry/irq_vector_exit, in all events.
    But there is an above use case to trace specific irq_vector rather than tracing all events.
    In this case, we are concerned about overhead due to unwanted events.
    
    So, add following tracepoints instead of introducing irq_vector_entry/exit.
    so that we can enable them independently.
       - local_timer_vector
       - reschedule_vector
       - call_function_vector
       - call_function_single_vector
       - irq_work_entry_vector
       - error_apic_vector
       - thermal_apic_vector
       - threshold_apic_vector
       - spurious_apic_vector
       - x86_platform_ipi_vector
    
    Also, introduce a logic switching IDT at enabling/disabling time so that a time penalty
    makes a zero when tracepoints are disabled. Detailed explanations are as follows.
     - Create trace irq handlers with entering_irq()/exiting_irq().
     - Create a new IDT, trace_idt_table, at boot time by adding a logic to
       _set_gate(). It is just a copy of original idt table.
     - Register the new handlers for tracpoints to the new IDT by introducing
       macros to alloc_intr_gate() called at registering time of irq_vector handlers.
     - Add checking, whether irq vector tracing is on/off, into load_current_idt().
       This has to be done below debug checking for these reasons.
       - Switching to debug IDT may be kicked while tracing is enabled.
       - On the other hands, switching to trace IDT is kicked only when debugging
         is disabled.
    
    In addition, the new IDT is created only when CONFIG_TRACING is enabled to avoid being
    used for other purposes.
    
    Signed-off-by: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/51C323ED.5050708@hds.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index d85837574a79..f4fe0b8879e0 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -30,6 +30,7 @@
 #include <asm/proto.h>
 #include <asm/apic.h>
 #include <asm/nmi.h>
+#include <asm/trace/irq_vectors.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *
@@ -264,6 +265,17 @@ void smp_reschedule_interrupt(struct pt_regs *regs)
 	 */
 }
 
+void smp_trace_reschedule_interrupt(struct pt_regs *regs)
+{
+	ack_APIC_irq();
+	trace_reschedule_entry(RESCHEDULE_VECTOR);
+	__smp_reschedule_interrupt();
+	trace_reschedule_exit(RESCHEDULE_VECTOR);
+	/*
+	 * KVM uses this interrupt to force a cpu out of guest mode
+	 */
+}
+
 static inline void call_function_entering_irq(void)
 {
 	ack_APIC_irq();
@@ -283,6 +295,15 @@ void smp_call_function_interrupt(struct pt_regs *regs)
 	exiting_irq();
 }
 
+void smp_trace_call_function_interrupt(struct pt_regs *regs)
+{
+	call_function_entering_irq();
+	trace_call_function_entry(CALL_FUNCTION_VECTOR);
+	__smp_call_function_interrupt();
+	trace_call_function_exit(CALL_FUNCTION_VECTOR);
+	exiting_irq();
+}
+
 static inline void __smp_call_function_single_interrupt(void)
 {
 	generic_smp_call_function_single_interrupt();
@@ -296,6 +317,15 @@ void smp_call_function_single_interrupt(struct pt_regs *regs)
 	exiting_irq();
 }
 
+void smp_trace_call_function_single_interrupt(struct pt_regs *regs)
+{
+	call_function_entering_irq();
+	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);
+	__smp_call_function_single_interrupt();
+	trace_call_function_single_exit(CALL_FUNCTION_SINGLE_VECTOR);
+	exiting_irq();
+}
+
 static int __init nonmi_ipi_setup(char *str)
 {
 	smp_no_nmi_ipi = true;

commit eddc0e922a3530e0f22cef170229bcae3a7d5e31
Author: Seiji Aguchi <seiji.aguchi@hds.com>
Date:   Thu Jun 20 11:45:17 2013 -0400

    x86, trace: Introduce entering/exiting_irq()
    
    When implementing tracepoints in interrupt handers, if the tracepoints are
    simply added in the performance sensitive path of interrupt handers,
    it may cause potential performance problem due to the time penalty.
    
    To solve the problem, an idea is to prepare non-trace/trace irq handers and
    switch their IDTs at the enabling/disabling time.
    
    So, let's introduce entering_irq()/exiting_irq() for pre/post-
    processing of each irq handler.
    
    A way to use them is as follows.
    
    Non-trace irq handler:
    smp_irq_handler()
    {
            entering_irq();         /* pre-processing of this handler */
            __smp_irq_handler();    /*
                                     * common logic between non-trace and trace handlers
                                     * in a vector.
                                     */
            exiting_irq();          /* post-processing of this handler */
    
    }
    
    Trace irq_handler:
    smp_trace_irq_handler()
    {
            entering_irq();         /* pre-processing of this handler */
            trace_irq_entry();      /* tracepoint for irq entry */
            __smp_irq_handler();    /*
                                     * common logic between non-trace and trace handlers
                                     * in a vector.
                                     */
            trace_irq_exit();       /* tracepoint for irq exit */
            exiting_irq();          /* post-processing of this handler */
    
    }
    
    If tracepoints can place outside entering_irq()/exiting_irq() as follows,
    it looks cleaner.
    
    smp_trace_irq_handler()
    {
            trace_irq_entry();
            smp_irq_handler();
            trace_irq_exit();
    }
    
    But it doesn't work.
    The problem is with irq_enter/exit() being called. They must be called before
    trace_irq_enter/exit(),  because of the rcu_irq_enter() must be called before
    any tracepoints are used, as tracepoints use  rcu to synchronize.
    
    As a possible alternative, we may be able to call irq_enter() first as follows
    if irq_enter() can nest.
    
    smp_trace_irq_hander()
    {
            irq_entry();
            trace_irq_entry();
            smp_irq_handler();
            trace_irq_exit();
            irq_exit();
    }
    
    But it doesn't work, either.
    If irq_enter() is nested, it may have a time penalty because it has to check if it
    was already called or not. The time penalty is not desired in performance sensitive
    paths even if it is tiny.
    
    Signed-off-by: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/51C3238D.9040706@hds.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 48d2b7ded422..d85837574a79 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -249,32 +249,51 @@ static void native_stop_other_cpus(int wait)
 /*
  * Reschedule call back.
  */
-void smp_reschedule_interrupt(struct pt_regs *regs)
+static inline void __smp_reschedule_interrupt(void)
 {
-	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
 	scheduler_ipi();
+}
+
+void smp_reschedule_interrupt(struct pt_regs *regs)
+{
+	ack_APIC_irq();
+	__smp_reschedule_interrupt();
 	/*
 	 * KVM uses this interrupt to force a cpu out of guest mode
 	 */
 }
 
-void smp_call_function_interrupt(struct pt_regs *regs)
+static inline void call_function_entering_irq(void)
 {
 	ack_APIC_irq();
 	irq_enter();
+}
+
+static inline void __smp_call_function_interrupt(void)
+{
 	generic_smp_call_function_interrupt();
 	inc_irq_stat(irq_call_count);
-	irq_exit();
 }
 
-void smp_call_function_single_interrupt(struct pt_regs *regs)
+void smp_call_function_interrupt(struct pt_regs *regs)
+{
+	call_function_entering_irq();
+	__smp_call_function_interrupt();
+	exiting_irq();
+}
+
+static inline void __smp_call_function_single_interrupt(void)
 {
-	ack_APIC_irq();
-	irq_enter();
 	generic_smp_call_function_single_interrupt();
 	inc_irq_stat(irq_call_count);
-	irq_exit();
+}
+
+void smp_call_function_single_interrupt(struct pt_regs *regs)
+{
+	call_function_entering_irq();
+	__smp_call_function_single_interrupt();
+	exiting_irq();
 }
 
 static int __init nonmi_ipi_setup(char *str)

commit 3aac27aba79b7c52e709ef6de0f7d8139caedc01
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri May 11 14:41:15 2012 -0400

    x86/reboot: Update nonmi_ipi parameter
    
    Update the nonmi_ipi parameter to reflect the simple change
    instead of the previous complicated one.  There should be less
    of a need to use it but there may still be corner cases on older
    hardware that stumble into NMI issues.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1336761675-24296-4-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 228e7405511a..48d2b7ded422 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -110,6 +110,7 @@
  */
 
 static atomic_t stopping_cpu = ATOMIC_INIT(-1);
+static bool smp_no_nmi_ipi = false;
 
 /*
  * this function sends a 'reschedule' IPI to another CPU.
@@ -216,7 +217,7 @@ static void native_stop_other_cpus(int wait)
 	}
 	
 	/* if the REBOOT_VECTOR didn't work, try with the NMI */
-	if ((num_online_cpus() > 1))  {
+	if ((num_online_cpus() > 1) && (!smp_no_nmi_ipi))  {
 		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
 					 NMI_FLAG_FIRST, "smp_stop"))
 			/* Note: we ignore failures here */
@@ -245,11 +246,6 @@ static void native_stop_other_cpus(int wait)
 	local_irq_restore(flags);
 }
 
-static void native_smp_disable_nmi_ipi(void)
-{
-	smp_ops.stop_other_cpus = native_irq_stop_other_cpus;
-}
-
 /*
  * Reschedule call back.
  */
@@ -283,8 +279,8 @@ void smp_call_function_single_interrupt(struct pt_regs *regs)
 
 static int __init nonmi_ipi_setup(char *str)
 {
-        native_smp_disable_nmi_ipi();
-        return 1;
+	smp_no_nmi_ipi = true;
+	return 1;
 }
 
 __setup("nonmi_ipi", nonmi_ipi_setup);

commit 7d007d21e539dbecb6942c5734e6649f720982cf
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri May 11 14:41:14 2012 -0400

    x86/reboot: Use NMI to assist in shutting down if IRQ fails
    
    For v3.3, I added code to use the NMI to stop other cpus in the
    panic case.  The idea was to make sure all cpus on the system
    were definitely halted to help serialize the panic path to
    execute the rest of the code on a single cpu.
    
    The main problem it was trying to solve was how to stop a cpu
    that was spinning with its irqs disabled.  A IPI irq would be
    stuck and couldn't get in there, but an NMI could.
    
    Things were great until we had another conversation about some
    pstore changes.  Because some of the backend pstore still uses
    spinlocks to protect the device access, things could get ugly if
    a panic happened and we were stuck spinning on a lock.
    
    Now with the NMI shutting down cpus, we could assume no other
    cpus were running and just bust the spin lock and proceed.
    
    The counter argument was, well if you do that the backend could
    be in a screwed up state and you might not be able to save
    anything as a result. If we could have just given the cpu a
    little more time to finish things, we could have grabbed the
    spin lock cleanly and everything would have been fine.
    
    Well, how do give a cpu a 'little more time' in the panic case?
    For the most part you can't without spinning on the lock and
    even in that case, how long do you spin for?
    
    So instead of making it ugly in the pstore code, just mimic the
    idea that stop_machine had, which is block on an IRQ IPI until
    the remote cpu has re-enabled interrupts and left the critical
    region.  Which is what happens now using REBOOT_IRQ.
    
    Then leave the NMI case for those cpus that are truly stuck
    after a short time.  This leaves the current behaviour alone and
    just handle a corner case.  Most systems should never have to
    enter the NMI code and if they do, print out a message in case
    the NMI itself causes another issue.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1336761675-24296-3-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 6d20f523bc4e..228e7405511a 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -29,6 +29,7 @@
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
 #include <asm/apic.h>
+#include <asm/nmi.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *
@@ -108,6 +109,8 @@
  *	about nothing of note with C stepping upwards.
  */
 
+static atomic_t stopping_cpu = ATOMIC_INIT(-1);
+
 /*
  * this function sends a 'reschedule' IPI to another CPU.
  * it goes straight through and wastes no time serializing
@@ -148,6 +151,17 @@ void native_send_call_func_ipi(const struct cpumask *mask)
 	free_cpumask_var(allbutself);
 }
 
+static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
+{
+	/* We are registered on stopping cpu too, avoid spurious NMI */
+	if (raw_smp_processor_id() == atomic_read(&stopping_cpu))
+		return NMI_HANDLED;
+
+	stop_this_cpu(NULL);
+
+	return NMI_HANDLED;
+}
+
 /*
  * this function calls the 'stop' function on all other CPUs in the system.
  */
@@ -171,13 +185,25 @@ static void native_stop_other_cpus(int wait)
 	/*
 	 * Use an own vector here because smp_call_function
 	 * does lots of things not suitable in a panic situation.
-	 * On most systems we could also use an NMI here,
-	 * but there are a few systems around where NMI
-	 * is problematic so stay with an non NMI for now
-	 * (this implies we cannot stop CPUs spinning with irq off
-	 * currently)
+	 */
+
+	/*
+	 * We start by using the REBOOT_VECTOR irq.
+	 * The irq is treated as a sync point to allow critical
+	 * regions of code on other cpus to release their spin locks
+	 * and re-enable irqs.  Jumping straight to an NMI might
+	 * accidentally cause deadlocks with further shutdown/panic
+	 * code.  By syncing, we give the cpus up to one second to
+	 * finish their work before we force them off with the NMI.
 	 */
 	if (num_online_cpus() > 1) {
+		/* did someone beat us here? */
+		if (atomic_cmpxchg(&stopping_cpu, -1, safe_smp_processor_id()) != -1)
+			return;
+
+		/* sync above data before sending IRQ */
+		wmb();
+
 		apic->send_IPI_allbutself(REBOOT_VECTOR);
 
 		/*
@@ -188,7 +214,32 @@ static void native_stop_other_cpus(int wait)
 		while (num_online_cpus() > 1 && (wait || timeout--))
 			udelay(1);
 	}
+	
+	/* if the REBOOT_VECTOR didn't work, try with the NMI */
+	if ((num_online_cpus() > 1))  {
+		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+					 NMI_FLAG_FIRST, "smp_stop"))
+			/* Note: we ignore failures here */
+			/* Hope the REBOOT_IRQ is good enough */
+			goto finish;
+
+		/* sync above data before sending IRQ */
+		wmb();
+
+		pr_emerg("Shutting down cpus with NMI\n");
+
+		apic->send_IPI_allbutself(NMI_VECTOR);
+
+		/*
+		 * Don't wait longer than a 10 ms if the caller
+		 * didn't ask us to wait.
+		 */
+		timeout = USEC_PER_MSEC * 10;
+		while (num_online_cpus() > 1 && (wait || timeout--))
+			udelay(1);
+	}
 
+finish:
 	local_irq_save(flags);
 	disable_local_APIC();
 	local_irq_restore(flags);

commit 5d2b86d90f7cc4a41316cef3d41560da6141f45c
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri May 11 14:41:13 2012 -0400

    Revert "x86, reboot: Use NMI instead of REBOOT_VECTOR to stop cpus"
    
    This reverts commit 3603a2512f9e69dc87914ba922eb4a0812b21cd6.
    
    Originally I wanted a better hammer to shutdown cpus during
    panic. However, this really steps on the toes of various
    spinlocks in the panic path.  Sometimes it is easier to wait for
    the IRQ to become re-enabled to indictate the cpu left the
    critical region and then shutdown the cpu.
    
    The next patch moves the NMI addition after the IRQ part.  To
    make it easier to see the logic of everything, revert this patch
    and apply the next simpler patch.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1336761675-24296-2-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 66c74f481cab..6d20f523bc4e 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -29,7 +29,6 @@
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
 #include <asm/apic.h>
-#include <asm/nmi.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *
@@ -149,60 +148,6 @@ void native_send_call_func_ipi(const struct cpumask *mask)
 	free_cpumask_var(allbutself);
 }
 
-static atomic_t stopping_cpu = ATOMIC_INIT(-1);
-
-static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
-{
-	/* We are registered on stopping cpu too, avoid spurious NMI */
-	if (raw_smp_processor_id() == atomic_read(&stopping_cpu))
-		return NMI_HANDLED;
-
-	stop_this_cpu(NULL);
-
-	return NMI_HANDLED;
-}
-
-static void native_nmi_stop_other_cpus(int wait)
-{
-	unsigned long flags;
-	unsigned long timeout;
-
-	if (reboot_force)
-		return;
-
-	/*
-	 * Use an own vector here because smp_call_function
-	 * does lots of things not suitable in a panic situation.
-	 */
-	if (num_online_cpus() > 1) {
-		/* did someone beat us here? */
-		if (atomic_cmpxchg(&stopping_cpu, -1, safe_smp_processor_id()) != -1)
-			return;
-
-		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
-					 NMI_FLAG_FIRST, "smp_stop"))
-			/* Note: we ignore failures here */
-			return;
-
-		/* sync above data before sending NMI */
-		wmb();
-
-		apic->send_IPI_allbutself(NMI_VECTOR);
-
-		/*
-		 * Don't wait longer than a second if the caller
-		 * didn't ask us to wait.
-		 */
-		timeout = USEC_PER_SEC;
-		while (num_online_cpus() > 1 && (wait || timeout--))
-			udelay(1);
-	}
-
-	local_irq_save(flags);
-	disable_local_APIC();
-	local_irq_restore(flags);
-}
-
 /*
  * this function calls the 'stop' function on all other CPUs in the system.
  */
@@ -215,7 +160,7 @@ asmlinkage void smp_reboot_interrupt(void)
 	irq_exit();
 }
 
-static void native_irq_stop_other_cpus(int wait)
+static void native_stop_other_cpus(int wait)
 {
 	unsigned long flags;
 	unsigned long timeout;
@@ -298,7 +243,7 @@ struct smp_ops smp_ops = {
 	.smp_prepare_cpus	= native_smp_prepare_cpus,
 	.smp_cpus_done		= native_smp_cpus_done,
 
-	.stop_other_cpus	= native_nmi_stop_other_cpus,
+	.stop_other_cpus	= native_stop_other_cpus,
 	.smp_send_reschedule	= native_smp_send_reschedule,
 
 	.cpu_up			= native_cpu_up,

commit e58d429209105e698e9d0357481d62b37fe9a7dd
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri Jan 6 11:17:51 2012 -0500

    x86, reboot: Fix typo in nmi reboot path
    
    It was brought to my attention that my x86 change to use NMI in
    the reboot path broke Intel Nehalem and Westmere boxes when
    using kexec.
    
    I realized I had mistyped the if statement in commit
    3603a2512f9e69dc87914ba922eb4a0812b21cd6 and stuck the ')' in
    the wrong spot.  Putting it in the right spot fixes kexec again.
    
    Doh.
    
    Reported-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Link: http://lkml.kernel.org/r/1325866671-9797-1-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 113acda5879e..66c74f481cab 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -176,7 +176,7 @@ static void native_nmi_stop_other_cpus(int wait)
 	 */
 	if (num_online_cpus() > 1) {
 		/* did someone beat us here? */
-		if (atomic_cmpxchg(&stopping_cpu, -1, safe_smp_processor_id() != -1))
+		if (atomic_cmpxchg(&stopping_cpu, -1, safe_smp_processor_id()) != -1)
 			return;
 
 		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,

commit bda62633983f9db49ce0b1a9235b3709c1cda5f0
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Oct 13 15:14:27 2011 -0400

    x86, NMI: Add knob to disable using NMI IPIs to stop cpus
    
    Some machines may exhibit problems using the NMI to stop other
    cpus. This knob just allows one to revert back to the original
    behaviour to help diagnose the problem.
    
    V2:
      make function static
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: seiji.aguchi@hds.com
    Cc: vgoyal@redhat.com
    Cc: mjg@redhat.com
    Cc: tony.luck@intel.com
    Cc: gong.chen@intel.com
    Cc: satoru.moriya@hds.com
    Cc: avi@redhat.com
    Cc: Andi Kleen <andi@firstfloor.org>
    Link: http://lkml.kernel.org/r/1318533267-18880-4-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index e72b1754a2d7..113acda5879e 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -249,6 +249,11 @@ static void native_irq_stop_other_cpus(int wait)
 	local_irq_restore(flags);
 }
 
+static void native_smp_disable_nmi_ipi(void)
+{
+	smp_ops.stop_other_cpus = native_irq_stop_other_cpus;
+}
+
 /*
  * Reschedule call back.
  */
@@ -280,6 +285,14 @@ void smp_call_function_single_interrupt(struct pt_regs *regs)
 	irq_exit();
 }
 
+static int __init nonmi_ipi_setup(char *str)
+{
+        native_smp_disable_nmi_ipi();
+        return 1;
+}
+
+__setup("nonmi_ipi", nonmi_ipi_setup);
+
 struct smp_ops smp_ops = {
 	.smp_prepare_boot_cpu	= native_smp_prepare_boot_cpu,
 	.smp_prepare_cpus	= native_smp_prepare_cpus,

commit 3603a2512f9e69dc87914ba922eb4a0812b21cd6
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Oct 13 15:14:25 2011 -0400

    x86, reboot: Use NMI instead of REBOOT_VECTOR to stop cpus
    
    A recent discussion started talking about the locking on the
    pstore fs and how it relates to the kmsg infrastructure.  We
    noticed it was possible for userspace to r/w to the pstore fs
    (grabbing the locks in the process) and block the panic path
    from r/w to the same fs.
    
    The reason was the cpu with the lock could be doing work while
    the crashing cpu is panic'ing.  Busting those spinlocks might
    cause those cpus to step on each other's data.  Fine, fair
    enough.
    
    It was suggested it would be nice to serialize the panic path
    (ie stop the other cpus) and have only one cpu running.  This
    would allow us to bust the spinlocks and not worry about another
    cpu stepping on the data.
    
    Of course, smp_send_stop() does this in the panic case.
    kmsg_dump() would have to be moved to be called after it.  Easy
    enough.
    
    The only problem is on x86 the smp_send_stop() function calls
    the REBOOT_VECTOR.  Any cpu with irqs disabled (which pstore and
    its backend ERST would do), block this IPI and thus do not stop.
     This makes it difficult to reliably log data to the pstore fs.
    
    The patch below switches from the REBOOT_VECTOR to NMI (and
    mimics what kdump does).  Switching to NMI allows us to deliver
    the IPI when irqs are disabled, increasing the reliability of
    this function.
    
    However, Andi carefully noted that on some machines this
    approach does not work because of broken BIOSes or whatever.
    
    To help accomodate this, the next couple of patches will run a
    selftest and provide a knob to disable.
    
    V2:
      uses atomic ops to serialize the cpu that shuts everyone down
    V3:
      comment cleanup
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: seiji.aguchi@hds.com
    Cc: vgoyal@redhat.com
    Cc: mjg@redhat.com
    Cc: tony.luck@intel.com
    Cc: gong.chen@intel.com
    Cc: satoru.moriya@hds.com
    Cc: avi@redhat.com
    Cc: Andi Kleen <andi@firstfloor.org>
    Link: http://lkml.kernel.org/r/1318533267-18880-2-git-send-email-dzickus@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 16204dc15484..e72b1754a2d7 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -29,6 +29,7 @@
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
 #include <asm/apic.h>
+#include <asm/nmi.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *
@@ -148,6 +149,60 @@ void native_send_call_func_ipi(const struct cpumask *mask)
 	free_cpumask_var(allbutself);
 }
 
+static atomic_t stopping_cpu = ATOMIC_INIT(-1);
+
+static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
+{
+	/* We are registered on stopping cpu too, avoid spurious NMI */
+	if (raw_smp_processor_id() == atomic_read(&stopping_cpu))
+		return NMI_HANDLED;
+
+	stop_this_cpu(NULL);
+
+	return NMI_HANDLED;
+}
+
+static void native_nmi_stop_other_cpus(int wait)
+{
+	unsigned long flags;
+	unsigned long timeout;
+
+	if (reboot_force)
+		return;
+
+	/*
+	 * Use an own vector here because smp_call_function
+	 * does lots of things not suitable in a panic situation.
+	 */
+	if (num_online_cpus() > 1) {
+		/* did someone beat us here? */
+		if (atomic_cmpxchg(&stopping_cpu, -1, safe_smp_processor_id() != -1))
+			return;
+
+		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+					 NMI_FLAG_FIRST, "smp_stop"))
+			/* Note: we ignore failures here */
+			return;
+
+		/* sync above data before sending NMI */
+		wmb();
+
+		apic->send_IPI_allbutself(NMI_VECTOR);
+
+		/*
+		 * Don't wait longer than a second if the caller
+		 * didn't ask us to wait.
+		 */
+		timeout = USEC_PER_SEC;
+		while (num_online_cpus() > 1 && (wait || timeout--))
+			udelay(1);
+	}
+
+	local_irq_save(flags);
+	disable_local_APIC();
+	local_irq_restore(flags);
+}
+
 /*
  * this function calls the 'stop' function on all other CPUs in the system.
  */
@@ -160,7 +215,7 @@ asmlinkage void smp_reboot_interrupt(void)
 	irq_exit();
 }
 
-static void native_stop_other_cpus(int wait)
+static void native_irq_stop_other_cpus(int wait)
 {
 	unsigned long flags;
 	unsigned long timeout;
@@ -230,7 +285,7 @@ struct smp_ops smp_ops = {
 	.smp_prepare_cpus	= native_smp_prepare_cpus,
 	.smp_cpus_done		= native_smp_cpus_done,
 
-	.stop_other_cpus	= native_stop_other_cpus,
+	.stop_other_cpus	= native_nmi_stop_other_cpus,
 	.smp_send_reschedule	= native_smp_send_reschedule,
 
 	.cpu_up			= native_cpu_up,

commit 69c60c88eeb364ebf58432f9bc38033522d58767
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu May 26 12:22:53 2011 -0400

    x86: Fix files explicitly requiring export.h for EXPORT_SYMBOL/THIS_MODULE
    
    These files were implicitly getting EXPORT_SYMBOL via device.h
    which was including module.h, but that will be fixed up shortly.
    
    By fixing these now, we can avoid seeing things like:
    
    arch/x86/kernel/rtc.c:29: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL’
    arch/x86/kernel/pci-dma.c:20: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL’
    arch/x86/kernel/e820.c:69: warning: type defaults to ‘int’ in declaration of ‘EXPORT_SYMBOL_GPL’
    
    [ with input from Randy Dunlap <rdunlap@xenotime.net> and also
      from Stephen Rothwell <sfr@canb.auug.org.au> ]
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 013e7eba83bb..16204dc15484 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -16,6 +16,7 @@
 #include <linux/mm.h>
 #include <linux/delay.h>
 #include <linux/spinlock.h>
+#include <linux/export.h>
 #include <linux/kernel_stat.h>
 #include <linux/mc146818rtc.h>
 #include <linux/cache.h>

commit 184748cc50b2dceb8287f9fb657eda48ff8fcfe7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:39 2011 +0200

    sched: Provide scheduler_ipi() callback in response to smp_send_reschedule()
    
    For future rework of try_to_wake_up() we'd like to push part of that
    function onto the CPU the task is actually going to run on.
    
    In order to do so we need a generic callback from the existing scheduler IPI.
    
    This patch introduces such a generic callback: scheduler_ipi() and
    implements it as a NOP.
    
    BenH notes: PowerPC might use this IPI on offline CPUs under rare conditions!
    
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110405152728.744338123@chello.nl

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 513deac7228d..013e7eba83bb 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -194,14 +194,13 @@ static void native_stop_other_cpus(int wait)
 }
 
 /*
- * Reschedule call back. Nothing to do,
- * all the work is done automatically when
- * we return from the interrupt.
+ * Reschedule call back.
  */
 void smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
+	scheduler_ipi();
 	/*
 	 * KVM uses this interrupt to force a cpu out of guest mode
 	 */

commit 76fac077db6b34e2c6383a7b4f3f4f7b7d06d8ce
Author: Alok Kataria <akataria@vmware.com>
Date:   Mon Oct 11 14:37:08 2010 -0700

    x86, kexec: Make sure to stop all CPUs before exiting the kernel
    
    x86 smp_ops now has a new op, stop_other_cpus which takes a parameter
    "wait" this allows the caller to specify if it wants to stop until all
    the cpus have processed the stop IPI.  This is required specifically
    for the kexec case where we should wait for all the cpus to be stopped
    before starting the new kernel.  We now wait for the cpus to stop in
    all cases except for panic/kdump where we expect things to be broken
    and we are doing our best to make things work anyway.
    
    This patch fixes a legitimate regression, which was introduced during
    2.6.30, by commit id 4ef702c10b5df18ab04921fc252c26421d4d6c75.
    
    Signed-off-by: Alok N Kataria <akataria@vmware.com>
    LKML-Reference: <1286833028.1372.20.camel@ank32.eng.vmware.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: <stable@kernel.org> v2.6.30-36
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index d801210945d6..513deac7228d 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -159,10 +159,10 @@ asmlinkage void smp_reboot_interrupt(void)
 	irq_exit();
 }
 
-static void native_smp_send_stop(void)
+static void native_stop_other_cpus(int wait)
 {
 	unsigned long flags;
-	unsigned long wait;
+	unsigned long timeout;
 
 	if (reboot_force)
 		return;
@@ -179,9 +179,12 @@ static void native_smp_send_stop(void)
 	if (num_online_cpus() > 1) {
 		apic->send_IPI_allbutself(REBOOT_VECTOR);
 
-		/* Don't wait longer than a second */
-		wait = USEC_PER_SEC;
-		while (num_online_cpus() > 1 && wait--)
+		/*
+		 * Don't wait longer than a second if the caller
+		 * didn't ask us to wait.
+		 */
+		timeout = USEC_PER_SEC;
+		while (num_online_cpus() > 1 && (wait || timeout--))
 			udelay(1);
 	}
 
@@ -227,7 +230,7 @@ struct smp_ops smp_ops = {
 	.smp_prepare_cpus	= native_smp_prepare_cpus,
 	.smp_cpus_done		= native_smp_cpus_done,
 
-	.smp_send_stop		= native_smp_send_stop,
+	.stop_other_cpus	= native_stop_other_cpus,
 	.smp_send_reschedule	= native_smp_send_reschedule,
 
 	.cpu_up			= native_cpu_up,

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index ec1de97600e7..d801210945d6 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -21,6 +21,7 @@
 #include <linux/cache.h>
 #include <linux/interrupt.h>
 #include <linux/cpu.h>
+#include <linux/gfp.h>
 
 #include <asm/mtrr.h>
 #include <asm/tlbflush.h>

commit e7ab0f7b50bc4688fb5cf65de5d42e3b882fb8d1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 9 15:58:20 2009 +0200

    Revert "x86, timers: Check for pending timers after (device) interrupts"
    
    This reverts commit 9bcbdd9c58617f1301dd4f17c738bb9bc73aca70.
    
    The real bug producing LatencyTop latencies has been fixed in:
    
      f5dc375: sched: Update the clock of runqueue select_task_rq() selected
    
    And the commit being reverted here triggers local timer processing
    from every device IRQ. If device IRQs come in at a high frequency,
    this could cause a performance regression.
    
    The commit being reverted here purely 'fixed' the reported latency
    as a side effect, because CPUs were being moved out of idle more
    often.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Frans Pop <elendil@planet.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <20091008064041.67219b13@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index d915d956e66d..ec1de97600e7 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -198,7 +198,6 @@ void smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
-	run_local_timers();
 	/*
 	 * KVM uses this interrupt to force a cpu out of guest mode
 	 */

commit 9bcbdd9c58617f1301dd4f17c738bb9bc73aca70
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Thu Oct 8 06:40:41 2009 -0700

    x86, timers: Check for pending timers after (device) interrupts
    
    Now that range timers and deferred timers are common, I found a
    problem with these using the "perf timechart" tool. Frans Pop also
    reported high scheduler latencies via LatencyTop, when using
    iwlagn.
    
    It turns out that on x86, these two 'opportunistic' timers only get
    checked when another "real" timer happens. These opportunistic
    timers have the objective to save power by hitchhiking on other
    wakeups, as to avoid CPU wakeups by themselves as much as possible.
    
    The change in this patch runs this check not only at timer
    interrupts, but at all (device) interrupts. The effect is that:
    
     1) the deferred timers/range timers get delayed less
    
     2) the range timers cause less wakeups by themselves because
        the percentage of hitchhiking on existing wakeup events goes up.
    
    I've verified the working of the patch using "perf timechart", the
    original exposed bug is gone with this patch. Frans also reported
    success - the latencies are now down in the expected ~10 msec
    range.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Tested-by: Frans Pop <elendil@planet.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <20091008064041.67219b13@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index ec1de97600e7..d915d956e66d 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -198,6 +198,7 @@ void smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
+	run_local_timers();
 	/*
 	 * KVM uses this interrupt to force a cpu out of guest mode
 	 */

commit 0d5959723e1db3fd7323c198a50c16cecf96c7a9
Merge: 62fdac5913f7 512626a04e72
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 23:31:52 2009 +0200

    Merge branch 'linus' into x86/mce3
    
    Conflicts:
            arch/x86/kernel/cpu/mcheck/mce_64.c
            arch/x86/kernel/irq.c
    
    Merge reason: Resolve the conflicts above.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 6cd8e300b49332eb9eeda45816c711c198d31505
Merge: ddbb868493ab 09f8ca74ae6c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 10:03:30 2009 -0700

    Merge branch 'kvm-updates/2.6.31' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.31' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (138 commits)
      KVM: Prevent overflow in largepages calculation
      KVM: Disable large pages on misaligned memory slots
      KVM: Add VT-x machine check support
      KVM: VMX: Rename rmode.active to rmode.vm86_active
      KVM: Move "exit due to NMI" handling into vmx_complete_interrupts()
      KVM: Disable CR8 intercept if tpr patching is active
      KVM: Do not migrate pending software interrupts.
      KVM: inject NMI after IRET from a previous NMI, not before.
      KVM: Always request IRQ/NMI window if an interrupt is pending
      KVM: Do not re-execute INTn instruction.
      KVM: skip_emulated_instruction() decode instruction if size is not known
      KVM: Remove irq_pending bitmap
      KVM: Do not allow interrupt injection from userspace if there is a pending event.
      KVM: Unprotect a page if #PF happens during NMI injection.
      KVM: s390: Verify memory in kvm run
      KVM: s390: Sanity check on validity intercept
      KVM: s390: Unlink vcpu on destroy - v2
      KVM: s390: optimize float int lock: spin_lock_bh --> spin_lock
      KVM: s390: use hrtimer for clock wakeup from idle - v2
      KVM: s390: Fix memory slot versus run - v3
      ...

commit 32f8840064d88cc3f6e85203aec7b6b57bebcb97
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu May 7 17:55:12 2009 -0300

    KVM: use smp_send_reschedule in kvm_vcpu_kick
    
    KVM uses a function call IPI to cause the exit of a guest running on a
    physical cpu. For virtual interrupt notification there is no need to
    wait on IPI receival, or to execute any function.
    
    This is exactly what the reschedule IPI does, without the overhead
    of function IPI. So use it instead of smp_call_function_single in
    kvm_vcpu_kick.
    
    Also change the "guest_mode" variable to a bit in vcpu->requests, and
    use that to collapse multiple IPI's that would be issued between the
    first one and zeroing of guest mode.
    
    This allows kvm_vcpu_kick to called with interrupts disabled.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 13f33ea8ccaa..3b2e55e8ad2b 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -172,6 +172,9 @@ void smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	inc_irq_stat(irq_resched_count);
+	/*
+	 * KVM uses this interrupt to force a cpu out of guest mode
+	 */
 }
 
 void smp_call_function_interrupt(struct pt_regs *regs)

commit 4ef702c10b5df18ab04921fc252c26421d4d6c75
Author: Andi Kleen <andi@firstfloor.org>
Date:   Wed May 27 21:56:52 2009 +0200

    x86: fix panic with interrupts off (needed for MCE)
    
    For some time each panic() called with interrupts disabled
    triggered the !irqs_disabled() WARN_ON in smp_call_function(),
    producing ugly backtraces and confusing users.
    
    This is a common situation with machine checks for example which
    tend to call panic with interrupts disabled, but will also hit
    in other situations e.g. panic during early boot.  In fact it
    means that panic cannot be called in many circumstances, which
    would be bad.
    
    This all started with the new fancy queued smp_call_function,
    which is then used by the shutdown path to shut down the other
    CPUs.
    
    On closer examination it turned out that the fancy RCU
    smp_call_function() does lots of things not suitable in a panic
    situation anyways, like allocating memory and relying on complex
    system state.
    
    I originally tried to patch this over by checking for panic
    there, but it was quite complicated and the original patch
    was also not very popular.  This also didn't fix some of the
    underlying complexity problems.
    
    The new code in post 2.6.29 tries to patch around this by
    checking for oops_in_progress, but that is not enough to make
    this fully safe and I don't think that's a real solution
    because panic has to be reliable.
    
    So instead use an own vector to reboot.  This makes the reboot
    code extremly straight forward, which is definitely a big plus
    in a panic situation where it is important to avoid relying on
    too much kernel state.  The new simple code is also safe to be
    called from interupts off region because it is very very simple.
    
    There can be situations where it is important that panic
    is reliable.  For example on a fatal machine check the panic
    is needed to get the system up again and running as quickly
    as possible.  So it's important that panic is reliable and
    all function it calls simple.
    
    This is why I came up with this simple vector scheme.
    It's very hard to beat in simplicity.  Vectors are not
    particularly precious anymore since all big systems are
    using per CPU vectors.
    
    Another possibility would have been to use an NMI similar
    to kdump, but there is still the problem that NMIs don't
    work reliably on some systems due to BIOS issues.  NMIs
    would have been able to stop CPUs running with interrupts
    off too.  In the sake of universal reliability I opted for
    using a non NMI vector for now.
    
    I put the reboot vector into the highest priority bucket of
    the APIC vectors and moved the 64bit UV_BAU message down
    instead into the next lower priority.
    
    [ Impact: bug fix, fixes an old regression ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index f6db48c405b8..bf1831aa14fa 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -150,14 +150,40 @@ void native_send_call_func_ipi(const struct cpumask *mask)
  * this function calls the 'stop' function on all other CPUs in the system.
  */
 
+asmlinkage void smp_reboot_interrupt(void)
+{
+	ack_APIC_irq();
+	irq_enter();
+	stop_this_cpu(NULL);
+	irq_exit();
+}
+
 static void native_smp_send_stop(void)
 {
 	unsigned long flags;
+	unsigned long wait;
 
 	if (reboot_force)
 		return;
 
-	smp_call_function(stop_this_cpu, NULL, 0);
+	/*
+	 * Use an own vector here because smp_call_function
+	 * does lots of things not suitable in a panic situation.
+	 * On most systems we could also use an NMI here,
+	 * but there are a few systems around where NMI
+	 * is problematic so stay with an non NMI for now
+	 * (this implies we cannot stop CPUs spinning with irq off
+	 * currently)
+	 */
+	if (num_online_cpus() > 1) {
+		apic->send_IPI_allbutself(REBOOT_VECTOR);
+
+		/* Don't wait longer than a second */
+		wait = USEC_PER_SEC;
+		while (num_online_cpus() > 1 && wait--)
+			udelay(1);
+	}
+
 	local_irq_save(flags);
 	disable_local_APIC();
 	local_irq_restore(flags);

commit b9b34f24b23ba9e79e07c0980e7fff16af2a67d1
Author: Cyrill Gorcunov <gorcunov@openvz.org>
Date:   Sun Apr 12 20:47:42 2009 +0400

    x86: smp.c - align smp_ops assignments
    
    Impact: cleanup
    
    It's a bit hard to parse by eyes without
    them being aligned.
    
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    LKML-Reference: <20090412165058.924175574@openvz.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 13f33ea8ccaa..f6db48c405b8 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -193,19 +193,19 @@ void smp_call_function_single_interrupt(struct pt_regs *regs)
 }
 
 struct smp_ops smp_ops = {
-	.smp_prepare_boot_cpu = native_smp_prepare_boot_cpu,
-	.smp_prepare_cpus = native_smp_prepare_cpus,
-	.smp_cpus_done = native_smp_cpus_done,
+	.smp_prepare_boot_cpu	= native_smp_prepare_boot_cpu,
+	.smp_prepare_cpus	= native_smp_prepare_cpus,
+	.smp_cpus_done		= native_smp_cpus_done,
 
-	.smp_send_stop = native_smp_send_stop,
-	.smp_send_reschedule = native_smp_send_reschedule,
+	.smp_send_stop		= native_smp_send_stop,
+	.smp_send_reschedule	= native_smp_send_reschedule,
 
-	.cpu_up = native_cpu_up,
-	.cpu_die = native_cpu_die,
-	.cpu_disable = native_cpu_disable,
-	.play_dead = native_play_dead,
+	.cpu_up			= native_cpu_up,
+	.cpu_die		= native_cpu_die,
+	.cpu_disable		= native_cpu_disable,
+	.play_dead		= native_play_dead,
 
-	.send_call_func_ipi = native_send_call_func_ipi,
+	.send_call_func_ipi	= native_send_call_func_ipi,
 	.send_call_func_single_ipi = native_send_call_func_single_ipi,
 };
 EXPORT_SYMBOL_GPL(smp_ops);

commit 7b6aa335ca1a845c2262ec7a595b4521bca0f79d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 17 13:58:15 2009 +0100

    x86, apic: remove genapic.h
    
    Impact: cleanup
    
    Remove genapic.h and remove all references to it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index eaaffae31cc0..13f33ea8ccaa 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -26,7 +26,7 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
-#include <asm/genapic.h>
+#include <asm/apic.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *

commit 8f47e16348e8e25eedf639092a8a2f10a66aba34
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 31 02:03:42 2009 +0100

    x86: update copyrights
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 0eb32ae9bf1f..eaaffae31cc0 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -2,7 +2,7 @@
  *	Intel SMP support routines.
  *
  *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>
- *	(c) 1998-99, 2000 Ingo Molnar <mingo@redhat.com>
+ *	(c) 1998-99, 2000, 2009 Ingo Molnar <mingo@redhat.com>
  *      (c) 2002,2003 Andi Kleen, SuSE Labs.
  *
  *	i386 and x86_64 integration by Glauber Costa <gcosta@redhat.com>

commit d53e2f2855f1c7c2725d550c1ae6b26f4d671c50
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 19:14:52 2009 +0100

    x86, smp: remove mach_ipi.h
    
    Move mach_ipi.h definitions into genapic.h.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 892e7c389be1..0eb32ae9bf1f 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -26,7 +26,6 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
-#include <mach_ipi.h>
 #include <asm/genapic.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:

commit 1dcdd3d15ecea0c22a09d4d001a39d425fceff2c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 17:55:37 2009 +0100

    x86: remove mach_apic.h
    
    Spread mach_apic.h definitions into genapic.h. (with some knock-on effects
    on smp.h and apic.h.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index c48ba6cc32aa..892e7c389be1 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -27,7 +27,7 @@
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
 #include <mach_ipi.h>
-#include <mach_apic.h>
+#include <asm/genapic.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *

commit dac5f4121df3c39fdb2ea57acd669a0ae19e46f8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 28 15:42:24 2009 +0100

    x86, apic: untangle the send_IPI_*() jungle
    
    Our send_IPI_*() methods and definitions are a twisted mess: the same
    symbol is defined to different things depending on .config details,
    in a non-transparent way.
    
     - spread out the quirks into separately named per apic driver methods
    
     - prefix the standard PC methods with default_
    
     - get rid of wrapper macro obfuscation
    
     - clean up various details
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index e6faa3316bd2..c48ba6cc32aa 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -118,12 +118,12 @@ static void native_smp_send_reschedule(int cpu)
 		WARN_ON(1);
 		return;
 	}
-	send_IPI_mask(cpumask_of(cpu), RESCHEDULE_VECTOR);
+	apic->send_IPI_mask(cpumask_of(cpu), RESCHEDULE_VECTOR);
 }
 
 void native_send_call_func_single_ipi(int cpu)
 {
-	send_IPI_mask(cpumask_of(cpu), CALL_FUNCTION_SINGLE_VECTOR);
+	apic->send_IPI_mask(cpumask_of(cpu), CALL_FUNCTION_SINGLE_VECTOR);
 }
 
 void native_send_call_func_ipi(const struct cpumask *mask)
@@ -131,7 +131,7 @@ void native_send_call_func_ipi(const struct cpumask *mask)
 	cpumask_var_t allbutself;
 
 	if (!alloc_cpumask_var(&allbutself, GFP_ATOMIC)) {
-		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
+		apic->send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
 		return;
 	}
 
@@ -140,9 +140,9 @@ void native_send_call_func_ipi(const struct cpumask *mask)
 
 	if (cpumask_equal(mask, allbutself) &&
 	    cpumask_equal(cpu_online_mask, cpu_callout_mask))
-		send_IPI_allbutself(CALL_FUNCTION_VECTOR);
+		apic->send_IPI_allbutself(CALL_FUNCTION_VECTOR);
 	else
-		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
+		apic->send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
 
 	free_cpumask_var(allbutself);
 }

commit 3d14bdad40315b54470cb7812293d14c8af2bf7d
Merge: 4e9b1c184cad 51d7a1398d18
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 10 06:13:09 2009 -0800

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (36 commits)
      x86: fix section mismatch warnings in mcheck/mce_amd_64.c
      x86: offer frame pointers in all build modes
      x86: remove duplicated #include's
      x86: k8 numa register active regions later
      x86: update Alan Cox's email addresses
      x86: rename all fields of mpc_table mpc_X to X
      x86: rename all fields of mpc_oemtable oem_X to X
      x86: rename all fields of mpc_bus mpc_X to X
      x86: rename all fields of mpc_cpu mpc_X to X
      x86: rename all fields of mpc_intsrc mpc_X to X
      x86: rename all fields of mpc_lintsrc mpc_X to X
      x86: rename all fields of mpc_iopic mpc_X to X
      x86: irqinit_64.c init_ISA_irqs should be static
      Documentation/x86/boot.txt: payload length was changed to payload_length
      x86: setup_percpu.c fix style problems
      x86: irqinit_64.c fix style problems
      x86: irqinit_32.c fix style problems
      x86: i8259.c fix style problems
      x86: irq_32.c fix style problems
      x86: ioport.c fix style problems
      ...

commit 87c6fe26186d734e932426cc8ab9fd8cf9aeed94
Author: Alan Cox <alan@lxorguk.ukuu.org.uk>
Date:   Mon Jan 5 14:08:04 2009 +0000

    x86: update Alan Cox's email addresses
    
    Signed-off-by: Alan Cox <alan@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index beea2649a240..cf1f075886b4 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -1,7 +1,7 @@
 /*
  *	Intel SMP support routines.
  *
- *	(c) 1995 Alan Cox, Building #3 <alan@redhat.com>
+ *	(c) 1995 Alan Cox, Building #3 <alan@lxorguk.ukuu.org.uk>
  *	(c) 1998-99, 2000 Ingo Molnar <mingo@redhat.com>
  *      (c) 2002,2003 Andi Kleen, SuSE Labs.
  *

commit c2d1cec1c77f7714672c1efeae075424c929e0d5
Author: Mike Travis <travis@sgi.com>
Date:   Sun Jan 4 05:18:03 2009 -0800

    x86: cleanup remaining cpumask_t ops in smpboot code
    
    Impact: use new cpumask API to reduce memory and stack usage
    
    Allocate the following local cpumasks based on the number of cpus that
    are present.  References will use new cpumask API.  (Currently only
    modified for x86_64, x86_32 continues to use the *_map variants.)
    
        cpu_callin_mask
        cpu_callout_mask
        cpu_initialized_mask
        cpu_sibling_setup_mask
    
    Provide the following accessor functions:
    
        struct cpumask *cpu_sibling_mask(int cpu)
        struct cpumask *cpu_core_mask(int cpu)
    
    Other changes are when setting or clearing the cpu online, possible
    or present maps, use the accessor functions.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index beea2649a240..182135ba1eaf 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -128,16 +128,23 @@ void native_send_call_func_single_ipi(int cpu)
 
 void native_send_call_func_ipi(const struct cpumask *mask)
 {
-	cpumask_t allbutself;
+	cpumask_var_t allbutself;
 
-	allbutself = cpu_online_map;
-	cpu_clear(smp_processor_id(), allbutself);
+	if (!alloc_cpumask_var(&allbutself, GFP_ATOMIC)) {
+		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
+		return;
+	}
 
-	if (cpus_equal(*mask, allbutself) &&
-	    cpus_equal(cpu_online_map, cpu_callout_map))
+	cpumask_copy(allbutself, cpu_online_mask);
+	cpumask_clear_cpu(smp_processor_id(), allbutself);
+
+	if (cpumask_equal(mask, allbutself) &&
+	    cpumask_equal(cpu_online_mask, cpu_callout_mask))
 		send_IPI_allbutself(CALL_FUNCTION_VECTOR);
 	else
 		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
+
+	free_cpumask_var(allbutself);
 }
 
 /*

commit b840d79631c882786925303c2b0f4fefc31845ed
Merge: 597b0d21626d c3d80000e3a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 2 11:44:09 2009 -0800

    Merge branch 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (66 commits)
      x86: export vector_used_by_percpu_irq
      x86: use logical apicid in x2apic_cluster's x2apic_cpu_mask_to_apicid_and()
      sched: nominate preferred wakeup cpu, fix
      x86: fix lguest used_vectors breakage, -v2
      x86: fix warning in arch/x86/kernel/io_apic.c
      sched: fix warning in kernel/sched.c
      sched: move test_sd_parent() to an SMP section of sched.h
      sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
      sched: activate active load balancing in new idle cpus
      sched: bias task wakeups to preferred semi-idle packages
      sched: nominate preferred wakeup cpu
      sched: favour lower logical cpu number for sched_mc balance
      sched: framework for sched_mc/smt_power_savings=N
      sched: convert BALANCE_FOR_xx_POWER to inline functions
      x86: use possible_cpus=NUM to extend the possible cpus allowed
      x86: fix cpu_mask_to_apicid_and to include cpu_online_mask
      x86: update io_apic.c to the new cpumask code
      x86: Introduce topology_core_cpumask()/topology_thread_cpumask()
      x86: xen: use smp_call_function_many()
      x86: use work_on_cpu in x86/kernel/cpu/mcheck/mce_amd_64.c
      ...
    
    Fixed up trivial conflict in kernel/time/tick-sched.c manually

commit bed4f13065b520e564adffbfcd1c1a764a9c887e
Merge: 3e5621edb339 bf8bd66d0580
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 23 16:30:31 2008 +0100

    Merge branch 'x86/irq' into x86/core

commit bcda016eddd7a8b374bb371473c821a91ff1d8cc
Author: Mike Travis <travis@sgi.com>
Date:   Tue Dec 16 17:33:59 2008 -0800

    x86: cosmetic changes apic-related files.
    
    This patch simply changes cpumask_t to struct cpumask and similar
    trivial modernizations.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 341df946f9a9..49ed667b06f3 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -118,15 +118,15 @@ static void native_smp_send_reschedule(int cpu)
 		WARN_ON(1);
 		return;
 	}
-	send_IPI_mask(&cpumask_of_cpu(cpu), RESCHEDULE_VECTOR);
+	send_IPI_mask(cpumask_of(cpu), RESCHEDULE_VECTOR);
 }
 
 void native_send_call_func_single_ipi(int cpu)
 {
-	send_IPI_mask(&cpumask_of_cpu(cpu), CALL_FUNCTION_SINGLE_VECTOR);
+	send_IPI_mask(cpumask_of(cpu), CALL_FUNCTION_SINGLE_VECTOR);
 }
 
-void native_send_call_func_ipi(const cpumask_t *mask)
+void native_send_call_func_ipi(const struct cpumask *mask)
 {
 	cpumask_t allbutself;
 

commit e7986739a76cde5079da08809d8bbc6878387ae0
Author: Mike Travis <travis@sgi.com>
Date:   Tue Dec 16 17:33:52 2008 -0800

    x86 smp: modify send_IPI_mask interface to accept cpumask_t pointers
    
    Impact: cleanup, change parameter passing
    
      * Change genapic interfaces to accept cpumask_t pointers where possible.
    
      * Modify external callers to use cpumask_t pointers in function calls.
    
      * Create new send_IPI_mask_allbutself which is the same as the
        send_IPI_mask functions but removes smp_processor_id() from list.
        This removes another common need for a temporary cpumask_t variable.
    
      * Functions that used a temp cpumask_t variable for:
    
            cpumask_t allbutme = cpu_online_map;
    
            cpu_clear(smp_processor_id(), allbutme);
            if (!cpus_empty(allbutme))
                    ...
    
        become:
    
            if (!cpus_equal(cpu_online_map, cpumask_of_cpu(cpu)))
                    ...
    
      * Other minor code optimizations (like using cpus_clear instead of
        CPU_MASK_NONE, etc.)
    
    Applies to linux-2.6.tip/master.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 3f92b134ab90..341df946f9a9 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -118,22 +118,22 @@ static void native_smp_send_reschedule(int cpu)
 		WARN_ON(1);
 		return;
 	}
-	send_IPI_mask(cpumask_of_cpu(cpu), RESCHEDULE_VECTOR);
+	send_IPI_mask(&cpumask_of_cpu(cpu), RESCHEDULE_VECTOR);
 }
 
 void native_send_call_func_single_ipi(int cpu)
 {
-	send_IPI_mask(cpumask_of_cpu(cpu), CALL_FUNCTION_SINGLE_VECTOR);
+	send_IPI_mask(&cpumask_of_cpu(cpu), CALL_FUNCTION_SINGLE_VECTOR);
 }
 
-void native_send_call_func_ipi(cpumask_t mask)
+void native_send_call_func_ipi(const cpumask_t *mask)
 {
 	cpumask_t allbutself;
 
 	allbutself = cpu_online_map;
 	cpu_clear(smp_processor_id(), allbutself);
 
-	if (cpus_equal(mask, allbutself) &&
+	if (cpus_equal(*mask, allbutself) &&
 	    cpus_equal(cpu_online_map, cpu_callout_map))
 		send_IPI_allbutself(CALL_FUNCTION_VECTOR);
 	else

commit 915b0d0104b72fd36af088ba4b11b5690bc96a6c
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Mon Dec 8 19:19:26 2008 -0800

    x86: hardirq: introduce inc_irq_stat()
    
    Impact: cleanup
    
    Introduce inc_irq_stat() macro and unify irq_stat accounting code.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 18f9b19f5f8f..d18537ce2c79 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -178,11 +178,7 @@ static void native_smp_send_stop(void)
 void smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
-#ifdef CONFIG_X86_32
-	__get_cpu_var(irq_stat).irq_resched_count++;
-#else
-	add_pda(irq_resched_count, 1);
-#endif
+	inc_irq_stat(irq_resched_count);
 }
 
 void smp_call_function_interrupt(struct pt_regs *regs)
@@ -190,11 +186,7 @@ void smp_call_function_interrupt(struct pt_regs *regs)
 	ack_APIC_irq();
 	irq_enter();
 	generic_smp_call_function_interrupt();
-#ifdef CONFIG_X86_32
-	__get_cpu_var(irq_stat).irq_call_count++;
-#else
-	add_pda(irq_call_count, 1);
-#endif
+	inc_irq_stat(irq_call_count);
 	irq_exit();
 }
 
@@ -203,11 +195,7 @@ void smp_call_function_single_interrupt(struct pt_regs *regs)
 	ack_APIC_irq();
 	irq_enter();
 	generic_smp_call_function_single_interrupt();
-#ifdef CONFIG_X86_32
-	__get_cpu_var(irq_stat).irq_call_count++;
-#else
-	add_pda(irq_call_count, 1);
-#endif
+	inc_irq_stat(irq_call_count);
 	irq_exit();
 }
 

commit d3ec5cae0921611ceae06464ef6291012dd9849f
Author: Ivan Vecera <ivecera@redhat.com>
Date:   Tue Nov 11 14:33:44 2008 +0100

    x86: call machine_shutdown and stop all CPUs in native_machine_halt
    
    Impact: really halt all CPUs on halt
    
    Function machine_halt (resp. native_machine_halt) is empty for x86
    architectures. When command 'halt -f' is invoked, the message "System
    halted." is displayed but this is not really true because all CPUs are
    still running.
    
    There are also similar inconsistencies for other arches (some uses
    power-off for halt or forever-loop with IRQs enabled/disabled).
    
    IMO there should be used the same approach for all architectures OR
    what does the message "System halted" really mean?
    
    This patch fixes it for x86.
    
    Signed-off-by: Ivan Vecera <ivecera@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 18f9b19f5f8f..3f92b134ab90 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -140,19 +140,6 @@ void native_send_call_func_ipi(cpumask_t mask)
 		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
 }
 
-static void stop_this_cpu(void *dummy)
-{
-	local_irq_disable();
-	/*
-	 * Remove this CPU:
-	 */
-	cpu_clear(smp_processor_id(), cpu_online_map);
-	disable_local_APIC();
-	if (hlt_works(smp_processor_id()))
-		for (;;) halt();
-	for (;;);
-}
-
 /*
  * this function calls the 'stop' function on all other CPUs in the system.
  */

commit 93be71b672f167b1e8c23725114f86305354f0ac
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:11 2008 +0100

    x86: add cpu hotplug hooks into smp_ops
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 361b7a4c640c..18f9b19f5f8f 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -214,12 +214,16 @@ void smp_call_function_single_interrupt(struct pt_regs *regs)
 struct smp_ops smp_ops = {
 	.smp_prepare_boot_cpu = native_smp_prepare_boot_cpu,
 	.smp_prepare_cpus = native_smp_prepare_cpus,
-	.cpu_up = native_cpu_up,
 	.smp_cpus_done = native_smp_cpus_done,
 
 	.smp_send_stop = native_smp_send_stop,
 	.smp_send_reschedule = native_smp_send_reschedule,
 
+	.cpu_up = native_cpu_up,
+	.cpu_die = native_cpu_die,
+	.cpu_disable = native_cpu_disable,
+	.play_dead = native_play_dead,
+
 	.send_call_func_ipi = native_send_call_func_ipi,
 	.send_call_func_single_ipi = native_send_call_func_single_ipi,
 };

commit 5e374fb62621aca9522f76c2317c9acda75a8e88
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jul 1 13:12:04 2008 +0200

    generic-ipi: fixlet
    
    create proper stackframe.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 56546e8a13ac..361b7a4c640c 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -198,7 +198,7 @@ void smp_call_function_interrupt(struct pt_regs *regs)
 	irq_exit();
 }
 
-void smp_call_function_single_interrupt(void)
+void smp_call_function_single_interrupt(struct pt_regs *regs)
 {
 	ack_APIC_irq();
 	irq_enter();

commit 8691e5a8f691cc2a4fda0651e8d307aaba0e7d68
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 6 11:18:06 2008 +0200

    smp_call_function: get rid of the unused nonatomic/retry argument
    
    It's never used and the comments refer to nonatomic and retry
    interchangably. So get rid of it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 575aa3d7248a..56546e8a13ac 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -164,7 +164,7 @@ static void native_smp_send_stop(void)
 	if (reboot_force)
 		return;
 
-	smp_call_function(stop_this_cpu, NULL, 0, 0);
+	smp_call_function(stop_this_cpu, NULL, 0);
 	local_irq_save(flags);
 	disable_local_APIC();
 	local_irq_restore(flags);

commit 3b16cf874861436725c43ba0b68bdd799297be7c
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jun 26 11:21:54 2008 +0200

    x86: convert to generic helpers for IPI function calls
    
    This converts x86, x86-64, and xen to use the new helpers for
    smp_call_function() and friends, and adds support for
    smp_call_function_single().
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 0cb7aadc87cd..575aa3d7248a 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -121,132 +121,23 @@ static void native_smp_send_reschedule(int cpu)
 	send_IPI_mask(cpumask_of_cpu(cpu), RESCHEDULE_VECTOR);
 }
 
-/*
- * Structure and data for smp_call_function(). This is designed to minimise
- * static memory requirements. It also looks cleaner.
- */
-static DEFINE_SPINLOCK(call_lock);
-
-struct call_data_struct {
-	void (*func) (void *info);
-	void *info;
-	atomic_t started;
-	atomic_t finished;
-	int wait;
-};
-
-void lock_ipi_call_lock(void)
+void native_send_call_func_single_ipi(int cpu)
 {
-	spin_lock_irq(&call_lock);
-}
-
-void unlock_ipi_call_lock(void)
-{
-	spin_unlock_irq(&call_lock);
-}
-
-static struct call_data_struct *call_data;
-
-static void __smp_call_function(void (*func) (void *info), void *info,
-				int nonatomic, int wait)
-{
-	struct call_data_struct data;
-	int cpus = num_online_cpus() - 1;
-
-	if (!cpus)
-		return;
-
-	data.func = func;
-	data.info = info;
-	atomic_set(&data.started, 0);
-	data.wait = wait;
-	if (wait)
-		atomic_set(&data.finished, 0);
-
-	call_data = &data;
-	mb();
-
-	/* Send a message to all other CPUs and wait for them to respond */
-	send_IPI_allbutself(CALL_FUNCTION_VECTOR);
-
-	/* Wait for response */
-	while (atomic_read(&data.started) != cpus)
-		cpu_relax();
-
-	if (wait)
-		while (atomic_read(&data.finished) != cpus)
-			cpu_relax();
+	send_IPI_mask(cpumask_of_cpu(cpu), CALL_FUNCTION_SINGLE_VECTOR);
 }
 
-
-/**
- * smp_call_function_mask(): Run a function on a set of other CPUs.
- * @mask: The set of cpus to run on.  Must not include the current cpu.
- * @func: The function to run. This must be fast and non-blocking.
- * @info: An arbitrary pointer to pass to the function.
- * @wait: If true, wait (atomically) until function has completed on other CPUs.
- *
-  * Returns 0 on success, else a negative status code.
- *
- * If @wait is true, then returns once @func has returned; otherwise
- * it returns just before the target cpu calls @func.
- *
- * You must not call this function with disabled interrupts or from a
- * hardware interrupt handler or from a bottom half handler.
- */
-static int
-native_smp_call_function_mask(cpumask_t mask,
-			      void (*func)(void *), void *info,
-			      int wait)
+void native_send_call_func_ipi(cpumask_t mask)
 {
-	struct call_data_struct data;
 	cpumask_t allbutself;
-	int cpus;
-
-	/* Can deadlock when called with interrupts disabled */
-	WARN_ON(irqs_disabled());
-
-	/* Holding any lock stops cpus from going down. */
-	spin_lock(&call_lock);
 
 	allbutself = cpu_online_map;
 	cpu_clear(smp_processor_id(), allbutself);
 
-	cpus_and(mask, mask, allbutself);
-	cpus = cpus_weight(mask);
-
-	if (!cpus) {
-		spin_unlock(&call_lock);
-		return 0;
-	}
-
-	data.func = func;
-	data.info = info;
-	atomic_set(&data.started, 0);
-	data.wait = wait;
-	if (wait)
-		atomic_set(&data.finished, 0);
-
-	call_data = &data;
-	wmb();
-
-	/* Send a message to other CPUs */
 	if (cpus_equal(mask, allbutself) &&
 	    cpus_equal(cpu_online_map, cpu_callout_map))
 		send_IPI_allbutself(CALL_FUNCTION_VECTOR);
 	else
 		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
-
-	/* Wait for response */
-	while (atomic_read(&data.started) != cpus)
-		cpu_relax();
-
-	if (wait)
-		while (atomic_read(&data.finished) != cpus)
-			cpu_relax();
-	spin_unlock(&call_lock);
-
-	return 0;
 }
 
 static void stop_this_cpu(void *dummy)
@@ -268,18 +159,13 @@ static void stop_this_cpu(void *dummy)
 
 static void native_smp_send_stop(void)
 {
-	int nolock;
 	unsigned long flags;
 
 	if (reboot_force)
 		return;
 
-	/* Don't deadlock on the call lock in panic */
-	nolock = !spin_trylock(&call_lock);
+	smp_call_function(stop_this_cpu, NULL, 0, 0);
 	local_irq_save(flags);
-	__smp_call_function(stop_this_cpu, NULL, 0, 0);
-	if (!nolock)
-		spin_unlock(&call_lock);
 	disable_local_APIC();
 	local_irq_restore(flags);
 }
@@ -301,33 +187,28 @@ void smp_reschedule_interrupt(struct pt_regs *regs)
 
 void smp_call_function_interrupt(struct pt_regs *regs)
 {
-	void (*func) (void *info) = call_data->func;
-	void *info = call_data->info;
-	int wait = call_data->wait;
-
 	ack_APIC_irq();
-	/*
-	 * Notify initiating CPU that I've grabbed the data and am
-	 * about to execute the function
-	 */
-	mb();
-	atomic_inc(&call_data->started);
-	/*
-	 * At this point the info structure may be out of scope unless wait==1
-	 */
 	irq_enter();
-	(*func)(info);
+	generic_smp_call_function_interrupt();
 #ifdef CONFIG_X86_32
 	__get_cpu_var(irq_stat).irq_call_count++;
 #else
 	add_pda(irq_call_count, 1);
 #endif
 	irq_exit();
+}
 
-	if (wait) {
-		mb();
-		atomic_inc(&call_data->finished);
-	}
+void smp_call_function_single_interrupt(void)
+{
+	ack_APIC_irq();
+	irq_enter();
+	generic_smp_call_function_single_interrupt();
+#ifdef CONFIG_X86_32
+	__get_cpu_var(irq_stat).irq_call_count++;
+#else
+	add_pda(irq_call_count, 1);
+#endif
+	irq_exit();
 }
 
 struct smp_ops smp_ops = {
@@ -338,7 +219,8 @@ struct smp_ops smp_ops = {
 
 	.smp_send_stop = native_smp_send_stop,
 	.smp_send_reschedule = native_smp_send_reschedule,
-	.smp_call_function_mask = native_smp_call_function_mask,
+
+	.send_call_func_ipi = native_send_call_func_ipi,
+	.send_call_func_single_ipi = native_send_call_func_single_ipi,
 };
 EXPORT_SYMBOL_GPL(smp_ops);
-

commit 61165d7a035f6571c7576e7f51e7230157724c8d
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue May 13 14:26:57 2008 +0100

    x86: fix app crashes after SMP resume
    
    After resume on a 2cpu laptop, kernel builds collapse with a sed hang,
    sh or make segfault (often on 20295564), real-time signal to cc1 etc.
    
    Several hurdles to jump, but a manually-assisted bisect led to -rc1's
    d2bcbad5f3ad38a1c09861bca7e252dde7bb8259 x86: do not zap_low_mappings
    in __smp_prepare_cpus.  Though the low mappings were removed at bootup,
    they were left behind (with Global flags helping to keep them in TLB)
    after resume or cpu online, causing the crashes seen.
    
    Reinstate zap_low_mappings (with local __flush_tlb_all) for each cpu_up
    on x86_32.  This used to be serialized by smp_commenced_mask: that's now
    gone, but a low_mappings flag will do.  No need for native_smp_cpus_done
    to repeat the zap: let mem_init zap BSP's low mappings just like on UP.
    
    (In passing, fix error code from native_cpu_up: do_boot_cpu returns a
    variety of diagnostic values, Dprintk what it says but convert to -EIO.
    And save_pg_dir separately before zap_low_mappings: doesn't matter now,
    but zapping twice in succession wiped out resume's swsusp_pg_dir.)
    
    That worked well on the duo and one quad, but wouldn't boot 3rd or 4th
    cpu on P4 Xeon, oopsing just after unlock_ipi_call_lock.  The TLB flush
    IPI now being sent reveals a long-standing bug: the booting cpu has its
    APIC readied in smp_callin at the top of start_secondary, but isn't put
    into the cpu_online_map until just before that unlock_ipi_call_lock.
    
    So native_smp_call_function_mask to online cpus would send_IPI_allbutself,
    including the cpu just coming up, though it has been excluded from the
    count to wait for: by the time it handles the IPI, the call data on
    native_smp_call_function_mask's stack may well have been overwritten.
    
    So fall back to send_IPI_mask while cpu_online_map does not match
    cpu_callout_map: perhaps there's a better APICological fix to be
    made at the start_secondary end, but I wouldn't know that.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 8f75893a6467..0cb7aadc87cd 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -231,7 +231,8 @@ native_smp_call_function_mask(cpumask_t mask,
 	wmb();
 
 	/* Send a message to other CPUs */
-	if (cpus_equal(mask, allbutself))
+	if (cpus_equal(mask, allbutself) &&
+	    cpus_equal(cpu_online_map, cpu_callout_map))
 		send_IPI_allbutself(CALL_FUNCTION_VECTOR);
 	else
 		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);

commit 5af5573ee06c361378e22a9dd71dae0320e841f7
Author: Glauber Costa <gcosta@redhat.com>
Date:   Tue Mar 25 13:28:56 2008 -0300

    x86: move ipi definitions to mach_ipi.h
    
    take them out of the x86_64-only asm/mach_apic.h
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 16c52aaaca35..8f75893a6467 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -26,12 +26,8 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
 #include <asm/proto.h>
-#ifdef CONFIG_X86_32
-#include <mach_apic.h>
 #include <mach_ipi.h>
-#else
-#include <asm/mach_apic.h>
-#endif
+#include <mach_apic.h>
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *

commit f694010185c429629ad5a65245da08103e611852
Author: Gautham R Shenoy <ego@in.ibm.com>
Date:   Mon Mar 10 17:44:03 2008 +0530

    x86: Don't send RESCHEDULE_VECTOR to offlined cpus
    
    In the x86 native_smp_send_reschedule_function(), don't send the IPI if the
    cpu has gone offline already. Warn nevertheless!!
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 88c1e518a203..16c52aaaca35 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -118,7 +118,10 @@
  */
 static void native_smp_send_reschedule(int cpu)
 {
-	WARN_ON(cpu_is_offline(cpu));
+	if (unlikely(cpu_is_offline(cpu))) {
+		WARN_ON(1);
+		return;
+	}
 	send_IPI_mask(cpumask_of_cpu(cpu), RESCHEDULE_VECTOR);
 }
 

commit 0941ecb55fbfd2d8bcc62dfd2fcaba1b35f2f196
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:12:55 2008 -0300

    x86: get rid of smp_32.c and smp_64.c
    
    This patch merges the copyright notices, and valuable
    comments that were left back on smp_{32,64}.c. With that,
    files are empty, and are deleted
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index b662300a88f3..88c1e518a203 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -1,3 +1,16 @@
+/*
+ *	Intel SMP support routines.
+ *
+ *	(c) 1995 Alan Cox, Building #3 <alan@redhat.com>
+ *	(c) 1998-99, 2000 Ingo Molnar <mingo@redhat.com>
+ *      (c) 2002,2003 Andi Kleen, SuSE Labs.
+ *
+ *	i386 and x86_64 integration by Glauber Costa <gcosta@redhat.com>
+ *
+ *	This code is released under the GNU General Public License version 2 or
+ *	later.
+ */
+
 #include <linux/init.h>
 
 #include <linux/mm.h>
@@ -19,6 +32,84 @@
 #else
 #include <asm/mach_apic.h>
 #endif
+/*
+ *	Some notes on x86 processor bugs affecting SMP operation:
+ *
+ *	Pentium, Pentium Pro, II, III (and all CPUs) have bugs.
+ *	The Linux implications for SMP are handled as follows:
+ *
+ *	Pentium III / [Xeon]
+ *		None of the E1AP-E3AP errata are visible to the user.
+ *
+ *	E1AP.	see PII A1AP
+ *	E2AP.	see PII A2AP
+ *	E3AP.	see PII A3AP
+ *
+ *	Pentium II / [Xeon]
+ *		None of the A1AP-A3AP errata are visible to the user.
+ *
+ *	A1AP.	see PPro 1AP
+ *	A2AP.	see PPro 2AP
+ *	A3AP.	see PPro 7AP
+ *
+ *	Pentium Pro
+ *		None of 1AP-9AP errata are visible to the normal user,
+ *	except occasional delivery of 'spurious interrupt' as trap #15.
+ *	This is very rare and a non-problem.
+ *
+ *	1AP.	Linux maps APIC as non-cacheable
+ *	2AP.	worked around in hardware
+ *	3AP.	fixed in C0 and above steppings microcode update.
+ *		Linux does not use excessive STARTUP_IPIs.
+ *	4AP.	worked around in hardware
+ *	5AP.	symmetric IO mode (normal Linux operation) not affected.
+ *		'noapic' mode has vector 0xf filled out properly.
+ *	6AP.	'noapic' mode might be affected - fixed in later steppings
+ *	7AP.	We do not assume writes to the LVT deassering IRQs
+ *	8AP.	We do not enable low power mode (deep sleep) during MP bootup
+ *	9AP.	We do not use mixed mode
+ *
+ *	Pentium
+ *		There is a marginal case where REP MOVS on 100MHz SMP
+ *	machines with B stepping processors can fail. XXX should provide
+ *	an L1cache=Writethrough or L1cache=off option.
+ *
+ *		B stepping CPUs may hang. There are hardware work arounds
+ *	for this. We warn about it in case your board doesn't have the work
+ *	arounds. Basically that's so I can tell anyone with a B stepping
+ *	CPU and SMP problems "tough".
+ *
+ *	Specific items [From Pentium Processor Specification Update]
+ *
+ *	1AP.	Linux doesn't use remote read
+ *	2AP.	Linux doesn't trust APIC errors
+ *	3AP.	We work around this
+ *	4AP.	Linux never generated 3 interrupts of the same priority
+ *		to cause a lost local interrupt.
+ *	5AP.	Remote read is never used
+ *	6AP.	not affected - worked around in hardware
+ *	7AP.	not affected - worked around in hardware
+ *	8AP.	worked around in hardware - we get explicit CS errors if not
+ *	9AP.	only 'noapic' mode affected. Might generate spurious
+ *		interrupts, we log only the first one and count the
+ *		rest silently.
+ *	10AP.	not affected - worked around in hardware
+ *	11AP.	Linux reads the APIC between writes to avoid this, as per
+ *		the documentation. Make sure you preserve this as it affects
+ *		the C stepping chips too.
+ *	12AP.	not affected - worked around in hardware
+ *	13AP.	not affected - worked around in hardware
+ *	14AP.	we always deassert INIT during bootup
+ *	15AP.	not affected - worked around in hardware
+ *	16AP.	not affected - worked around in hardware
+ *	17AP.	not affected - worked around in hardware
+ *	18AP.	not affected - worked around in hardware
+ *	19AP.	not affected - worked around in BIOS
+ *
+ *	If this sounds worrying believe me these bugs are either ___RARE___,
+ *	or are signal timing bugs worked around in hardware and there's
+ *	about nothing of note with C stepping upwards.
+ */
 
 /*
  * this function sends a 'reschedule' IPI to another CPU.

commit f9e47a126be2eaabf04a1a5c71ca7b23a473d0d8
Author: Glauber Costa <gcosta@redhat.com>
Date:   Mon Mar 3 14:12:52 2008 -0300

    x86: create smp.c
    
    this patch moves all the functions and data structures that look
    like exactly the same from smp_{32,64}.c to smp.c
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
new file mode 100644
index 000000000000..b662300a88f3
--- /dev/null
+++ b/arch/x86/kernel/smp.c
@@ -0,0 +1,253 @@
+#include <linux/init.h>
+
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/spinlock.h>
+#include <linux/kernel_stat.h>
+#include <linux/mc146818rtc.h>
+#include <linux/cache.h>
+#include <linux/interrupt.h>
+#include <linux/cpu.h>
+
+#include <asm/mtrr.h>
+#include <asm/tlbflush.h>
+#include <asm/mmu_context.h>
+#include <asm/proto.h>
+#ifdef CONFIG_X86_32
+#include <mach_apic.h>
+#include <mach_ipi.h>
+#else
+#include <asm/mach_apic.h>
+#endif
+
+/*
+ * this function sends a 'reschedule' IPI to another CPU.
+ * it goes straight through and wastes no time serializing
+ * anything. Worst case is that we lose a reschedule ...
+ */
+static void native_smp_send_reschedule(int cpu)
+{
+	WARN_ON(cpu_is_offline(cpu));
+	send_IPI_mask(cpumask_of_cpu(cpu), RESCHEDULE_VECTOR);
+}
+
+/*
+ * Structure and data for smp_call_function(). This is designed to minimise
+ * static memory requirements. It also looks cleaner.
+ */
+static DEFINE_SPINLOCK(call_lock);
+
+struct call_data_struct {
+	void (*func) (void *info);
+	void *info;
+	atomic_t started;
+	atomic_t finished;
+	int wait;
+};
+
+void lock_ipi_call_lock(void)
+{
+	spin_lock_irq(&call_lock);
+}
+
+void unlock_ipi_call_lock(void)
+{
+	spin_unlock_irq(&call_lock);
+}
+
+static struct call_data_struct *call_data;
+
+static void __smp_call_function(void (*func) (void *info), void *info,
+				int nonatomic, int wait)
+{
+	struct call_data_struct data;
+	int cpus = num_online_cpus() - 1;
+
+	if (!cpus)
+		return;
+
+	data.func = func;
+	data.info = info;
+	atomic_set(&data.started, 0);
+	data.wait = wait;
+	if (wait)
+		atomic_set(&data.finished, 0);
+
+	call_data = &data;
+	mb();
+
+	/* Send a message to all other CPUs and wait for them to respond */
+	send_IPI_allbutself(CALL_FUNCTION_VECTOR);
+
+	/* Wait for response */
+	while (atomic_read(&data.started) != cpus)
+		cpu_relax();
+
+	if (wait)
+		while (atomic_read(&data.finished) != cpus)
+			cpu_relax();
+}
+
+
+/**
+ * smp_call_function_mask(): Run a function on a set of other CPUs.
+ * @mask: The set of cpus to run on.  Must not include the current cpu.
+ * @func: The function to run. This must be fast and non-blocking.
+ * @info: An arbitrary pointer to pass to the function.
+ * @wait: If true, wait (atomically) until function has completed on other CPUs.
+ *
+  * Returns 0 on success, else a negative status code.
+ *
+ * If @wait is true, then returns once @func has returned; otherwise
+ * it returns just before the target cpu calls @func.
+ *
+ * You must not call this function with disabled interrupts or from a
+ * hardware interrupt handler or from a bottom half handler.
+ */
+static int
+native_smp_call_function_mask(cpumask_t mask,
+			      void (*func)(void *), void *info,
+			      int wait)
+{
+	struct call_data_struct data;
+	cpumask_t allbutself;
+	int cpus;
+
+	/* Can deadlock when called with interrupts disabled */
+	WARN_ON(irqs_disabled());
+
+	/* Holding any lock stops cpus from going down. */
+	spin_lock(&call_lock);
+
+	allbutself = cpu_online_map;
+	cpu_clear(smp_processor_id(), allbutself);
+
+	cpus_and(mask, mask, allbutself);
+	cpus = cpus_weight(mask);
+
+	if (!cpus) {
+		spin_unlock(&call_lock);
+		return 0;
+	}
+
+	data.func = func;
+	data.info = info;
+	atomic_set(&data.started, 0);
+	data.wait = wait;
+	if (wait)
+		atomic_set(&data.finished, 0);
+
+	call_data = &data;
+	wmb();
+
+	/* Send a message to other CPUs */
+	if (cpus_equal(mask, allbutself))
+		send_IPI_allbutself(CALL_FUNCTION_VECTOR);
+	else
+		send_IPI_mask(mask, CALL_FUNCTION_VECTOR);
+
+	/* Wait for response */
+	while (atomic_read(&data.started) != cpus)
+		cpu_relax();
+
+	if (wait)
+		while (atomic_read(&data.finished) != cpus)
+			cpu_relax();
+	spin_unlock(&call_lock);
+
+	return 0;
+}
+
+static void stop_this_cpu(void *dummy)
+{
+	local_irq_disable();
+	/*
+	 * Remove this CPU:
+	 */
+	cpu_clear(smp_processor_id(), cpu_online_map);
+	disable_local_APIC();
+	if (hlt_works(smp_processor_id()))
+		for (;;) halt();
+	for (;;);
+}
+
+/*
+ * this function calls the 'stop' function on all other CPUs in the system.
+ */
+
+static void native_smp_send_stop(void)
+{
+	int nolock;
+	unsigned long flags;
+
+	if (reboot_force)
+		return;
+
+	/* Don't deadlock on the call lock in panic */
+	nolock = !spin_trylock(&call_lock);
+	local_irq_save(flags);
+	__smp_call_function(stop_this_cpu, NULL, 0, 0);
+	if (!nolock)
+		spin_unlock(&call_lock);
+	disable_local_APIC();
+	local_irq_restore(flags);
+}
+
+/*
+ * Reschedule call back. Nothing to do,
+ * all the work is done automatically when
+ * we return from the interrupt.
+ */
+void smp_reschedule_interrupt(struct pt_regs *regs)
+{
+	ack_APIC_irq();
+#ifdef CONFIG_X86_32
+	__get_cpu_var(irq_stat).irq_resched_count++;
+#else
+	add_pda(irq_resched_count, 1);
+#endif
+}
+
+void smp_call_function_interrupt(struct pt_regs *regs)
+{
+	void (*func) (void *info) = call_data->func;
+	void *info = call_data->info;
+	int wait = call_data->wait;
+
+	ack_APIC_irq();
+	/*
+	 * Notify initiating CPU that I've grabbed the data and am
+	 * about to execute the function
+	 */
+	mb();
+	atomic_inc(&call_data->started);
+	/*
+	 * At this point the info structure may be out of scope unless wait==1
+	 */
+	irq_enter();
+	(*func)(info);
+#ifdef CONFIG_X86_32
+	__get_cpu_var(irq_stat).irq_call_count++;
+#else
+	add_pda(irq_call_count, 1);
+#endif
+	irq_exit();
+
+	if (wait) {
+		mb();
+		atomic_inc(&call_data->finished);
+	}
+}
+
+struct smp_ops smp_ops = {
+	.smp_prepare_boot_cpu = native_smp_prepare_boot_cpu,
+	.smp_prepare_cpus = native_smp_prepare_cpus,
+	.cpu_up = native_cpu_up,
+	.smp_cpus_done = native_smp_cpus_done,
+
+	.smp_send_stop = native_smp_send_stop,
+	.smp_send_reschedule = native_smp_send_reschedule,
+	.smp_call_function_mask = native_smp_call_function_mask,
+};
+EXPORT_SYMBOL_GPL(smp_ops);
+
